# Usability Functionality

[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

## Overview

In our pursuit to understand why researchers in Artificial Intelligence may prefer PyTorch over TensorFlow, our investigation focuses on assessing the readability of code and the seamless usability of APIs within each framework. We conducted measurements on the API density of six models implemented in both TensorFlow and PyTorch (client code), API usability documentation for both PyTorch and TensorFlow, and code readability of the six models implemented in TensorFlow and PyTorch (client code).

### Project Structure

```bash
Usability Functionality
├─ Data 
│  ├─ Documentation : Contains HTML and produced CSV files.
│  ├─ Models        : Holds the seven models implemented in both PyTorch and TensorFlow.
│  └─ json_packages : The most popular substring terms in APIs of both PyTorch and TensorFlow used for our code to count the number of APIs.
├─ API_Density_Metrics: Automated script for collecting the unique number of API calls in each model. Some counts were manually conducted.
│  └─ Final_Results   : Results produced from the CSV file generated by collect_Unique_API_Call.py.
├─ API_Usability_Metrics: Contains all implemented metrics used for quantitative comparison of results.
├─ Code_Readability_Metrics: Metrics on the six models.
│  ├─ model_result: Metric results for each file and average per file.
│  └─ gomean_all: Uses Geometric Mean to average all files for each model if applicable.
├─ generating_resultGraphs.ipynb : Responsible for generating all graphs in this section of the paper.
├─ pytorch_html_scraper.py : Produces the PyTorch CSV file of APIs from the documentation.
└─ tensorflow_html_scraper.py : Produces the TensorFlow CSV file of APIs from the documentation.
```

## How could we Scrape the Documentation for API Usability? 

Scraping PyTorch documentation is relatively straightforward. This is because all parameter documentations follow the format:
```bash
parameter_name (parameter_type(s)) - parameter descriptions
```

The **return** type is either specified in the function description like this: <br>
```bash
_function_name(parameters) → return_type
```
Or it is specified in the **Returns** section of the documentation [torchdoc](link-to-torchdoc). <br>

However, scraping TensorFlow has proven to be challenging. While TensorFlow documentation does provide an "**Args**" and "**Returns**" table that specifies the parameters, the parameter types and return types, it does so in a natural language way. For example, consider a parameter that is expected to be the type "tensor"; TensorFlow documentation will describe it as "A `tensor` of any type and shape" [tfdoc](link-to-tfdoc). While this may improve readability and human understandability, it is an obstacle in obtaining the correct types for the given method. After testing out a variety of methods, including relying on external services such as OpenAI, we have developed our own method that is quick to execute and fairly accurate. <br>

First, we tokenize the type description using nltk's word tokenize [nltkdoc](link-to-nltkdoc). Next, we perform Part of Speech (POS) Tagging on the tokenized words. POS Tagging allows us to identify nouns within the type description, as most Python types are nouns [pythondatatypes](link-to-pythondatatypes). We then extract the tokens that are either tagged as a noun or are quoted in back-tick (`). The justification for extracting back-tick quoted tokens is that we discovered that most parameter and return types are quoted in back-tick in TensorFlow's Documentation [tfdoc](link-to-tfdoc). This is so that the type will be rendered in code-like font when displayed on the website. However, this is not true for all types.<br><br>

Once we have extracted all the relevant tokens, we score each token based on the following criteria:

- If the token is the word "tensor," it has a score of 4 (highly likely).
- If the token is back-tick quoted, it has a score of 3. 1 bonus point if it is the first occurrence.
- If the token is a Python or TensorFlow built-in type [pythondatatypes](link-to-pythondatatypes) [tftypes](link-to-tftypes), or it can be translated into such (e.g., the word "string" represents the type "str"), it has 2 points.
- If the token is a valid Python identifier, it has a score of 1. <br>
Once each token is scored, we will return the token of the highest score, and that token will be presumed to be the parameter type or the return type of the API.<br>

We **evaluated** this method by randomly selecting 25 API Documentations and compared its type identification result to our manual type identification. We discovered that it is able to achieve** 89.43%** accuracy when compared to our manual tagging result. Given our time and resource constraints, we believe this method does a satisfactory job in identifying the parameter and return types of TensorFlow APIs.




## Reproducing The Results
We have three sections: (1) **API Density**, (2) **API Usability**, and (3) **Code Readability**

### 1. API Density 

To obtain API density for each model, run the following command:
```bash
python3 collect_Unique_API_Calls.py --data_paths FolderForEachModel --json_paths Json_PackageFile --state Model_Type(TF or Torch)
```
**Note**: Generally, provide the folder path for each model, and the script will yield results directly.

### 2. API Usability 

To generate API usability documentation results, run the following command:
```bash
python3 report_results.py --data_path API_CSV_FILE
```
To produce the documentation results, run the code two times, each time you send the data_path for each csv file. 


### 3. Code Readability

To generate result for Identifier Terms in Dictionary or Textual Coherance, run the `metric_runner.py` in `Code_Readability_Metrics`.

```bash
python3 Code_Readability_Metrics/metric_runner.py -m [tc, itid] -l [tf, torch] MODEL_DATA_LOCATION
```
Where:
* `MODEL_DATA_LOCATION` is the location of the raw Models. It is located in `Data/Models`.
* `-m` specify the type of metric. Can be either `itid` for Identifier Terms in Dictionary, or `tc` for Textual Coherance.
* `l` specify the library type. Can be either `tf` for Tensorflow or `torch` for PyTorch.
 
For example, to run ITID on PyTorch:
```bash
python3 Code_Readability_Metrics/metric_runner.py -m itid -l torch -o Code_Readability_Metrics/model_results/torch/ Data/Models/
```

To calculate the geometric mean for each model, depending on the metric / model, run:
```bash
python3 Code_Readability_Metrics/result_tabulator.py -m [tc, itid] -l [tf, torch] Code_Readability_Metrics/model_results/[torch/, tf/] -o Code_Readability_Metrics/model_results/summerized_permodel_result
```
The tabulator is able to auto-detect the relevant result csv.

To tabluate the total score for each framework, run the result tabulator again, and replace with the summerized permodel result:
```bash
python3 Code_Readability_Metrics/result_tabulator.py -m [tc, itid] -l [tf, torch] Code_Readability_Metrics/model_results/summerized_permodel_result
```
#### Additional Information.
You can reproduce the results in this sheet (https://docs.google.com/spreadsheets/d/1cvkvzK7qmmBwF825iB4jhYmMa-C5ubVbUPCEmv1OLck/edit?usp=sharing) in sheet 2, 3, and 4
