Issue Number,Issue Title,Issue Body
27983,TFLite crash on Android running a model for NNPI,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution: **n/a**
- Mobile device: **Android Q Emulator** and **Xiaomi A2**
- TensorFlow installed from: **binary**
- TensorFlow version: **1.13.1**
- GPU model and memory: **?**

**Describe the current behavior**
Simple program wroks with CPU, but crashes when NNAPI is turned on, with words:

    E/tflite: Op code 67 is currently not delegated to NNAPI
    E/tflite: Returning error since TFLite returned failure nnapi_delegate.cc:739.

**Expected behavior**
I would expect TFLite to fall back to CPU for the opcodes that cannot work with NNAPI.

**Code to reproduce the issue**
GitHub repo: https://github.com/alexcohn/tflite-nnapi"
27981,Wrong document for dynamic_rnn,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: TF r1.13 (and many other versions)
- Doc Link: [tf.nn.dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)


**Describe the documentation issue**

It describes the argument `sequence_length` as ""it's more for performance than correctness"", but this is wrong:

- This argument enables this API to extract the last **VALID** state of RNN instead of a **PADDED** time step, **so it IS for correctness.**
- This argument CANNOT reduce computation, because even with this argument, computation of new states of RNN cell still occurs. The only difference is that `tf.nn.dynamic_rnn` chooses to copy through old states instead of keeping the new states, and this leads to even larger computation. **So it is NOT for performance.**

The initial document is correct, see [this commit](https://github.com/tensorflow/tensorflow/commit/855d3b56014780a90143b3e0c0865334b188c2df).

> it's more for correctness than performance""

I don't really understand why you replaced the correct document with a wrong one.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**"
27980,"why would I got ""tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file"" when I use pretrain model download from https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models","Hi,
I download resnet_v2_2017_04_14 from https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models,
and rename it as ""resnet_v2_2017_04_14.ckpt"".
But when I ""tf.train.init_from_checkpoint "" to initialize it ,I got ""tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file""
why would I got ""tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file"" ?
This also happened when I use slim.assign_from_checkpoint_fn()
### Source code / logs
#Here is the code
def get_init_fn(checkpoint_exclude_scopes=None):
    checkpoint_path =  './ini_checkpoints/resnet_v1_101.ckpt'
    if checkpoint_path is None:
        return None

    # Warn the user if a checkpoint exists in the train_dir. Then we'll be
    # ignoring the checkpoint anyway.
    if tf.train.latest_checkpoint('./training'):
        tf.logging.info(
            'Ignoring --checkpoint_path because a checkpoint already exists ' +
            'in ./training')
        return None

    exclusions = []
    if checkpoint_exclude_scopes:
        exclusions = [scope.strip() for scope in
                      checkpoint_exclude_scopes.split(',')]
    variables_to_restore = []
    for var in slim.get_model_variables():
        excluded = False
        for exclusion in exclusions:
            if var.op.name.startswith(exclusion):
                excluded = True
        if not excluded:
            variables_to_restore.append(var)

    if tf.gfile.IsDirectory(checkpoint_path):
        checkpoint_path = tf.train.latest_checkpoint(checkpoint_path)
    else:
        checkpoint_path = checkpoint_path

    tf.logging.info('Fine-tuning from %s' % checkpoint_path)

    return slim.assign_from_checkpoint_fn(
        checkpoint_path,
        variables_to_restore,
        ignore_missing_vars=True)

init_fn = get_init_fn()"
27979,optimizer weights not saved into keras.model.save() via TPU training,"While I need train over 12h on colab TPU, I should train the model, save and reload the model, and continue training. But [optimizer_weights] are not saved into my_model.h5. Here I have my demo code to examine this:


<em>import tensorflow as tf
from tensorflow.keras.layers import Input,Dense
from tensorflow.keras.models import Model
import numpy as np
from keras import backend as K
import os
np.random.seed(1000)
X, y = np.random.rand(200, 50), np.random.randint(2, size=200)
x = Input((50,))
out = Dense(1, activation='sigmoid')(x)
model100 = Model(x, out)
model100.compile(optimizer='adam', loss='binary_crossentropy')
use_tpu=True # @param {type:""boolean""}
if use_tpu:
  assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; Maybe you should switch hardware accelerator to TPU for TPU support'
  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']
  strategy = tf.contrib.tpu.TPUDistributionStrategy(
          tf.contrib.cluster_resolver.TPUClusterResolver(tpu=tpu_address)
  )
  model3 = tf.contrib.tpu.keras_to_tpu_model(model100, strategy=strategy)
model3.fit(X, y, epochs=1,batch_size=8)
model3.save('weights.h5')

import h5py
filename = 'weights.h5'
h555 = h5py.File(filename,'r')
print(list(h555.keys()))</em>


Finally I get the keys as ['model_weights'] but not ['model_weights','optimizer_weights']
I am not sure there are other ways to get optimizer_weights, if yes, please dont hesitate to tell.


"
27978,"ValueError: Incompatible shapes between op input and calculated input gradient. Forward operation: Dec_Conv5/dcon1. Input index: 2. Original input shape: (?, 256, 256, 64). Calculated input gradient shape: (?, 512, 512, 64) #12700","Please help me to solve this issue. Encountered error is pasted below:
Code File: [File.zip](https://github.com/tensorflow/tensorflow/files/3097435/File.zip)

**### ERROR**
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in set_shape(self, shape)
    536           dim_list,
--> 537           unknown_shape)
    538     except errors.InvalidArgumentError as e:

InvalidArgumentError: Dimension 1 in both shapes must be equal, but are 512 and 256. Shapes are [?,512,512,64] and [?,256,256,64].

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    991               try:
--> 992                 in_grad.set_shape(t_in.get_shape())
    993               except ValueError:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in set_shape(self, shape)
    539       # Convert to ValueError for backwards compatibility.
--> 540       raise ValueError(str(e))
    541 

ValueError: Dimension 1 in both shapes must be equal, but are 512 and 256. Shapes are [?,512,512,64] and [?,256,256,64].

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-103-e50856e0f1bb> in <module>()
     58   g_Loss = tf.losses.mean_squared_error(inp, d_con5)
     59   mean = tf.reduce_mean(g_Loss)
---> 60   optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(mean)
     61 
     62   #train = tf.reduced_mean()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)
    401         aggregation_method=aggregation_method,
    402         colocate_gradients_with_ops=colocate_gradients_with_ops,
--> 403         grad_loss=grad_loss)
    404 
    405     vars_with_grad = [v for g, v in grads_and_vars if g is not None]

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py in compute_gradients(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)
    510         gate_gradients=(gate_gradients == Optimizer.GATE_OP),
    511         aggregation_method=aggregation_method,
--> 512         colocate_gradients_with_ops=colocate_gradients_with_ops)
    513     if gate_gradients == Optimizer.GATE_GRAPH:
    514       grads = control_flow_ops.tuple(grads)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)
    662     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,
    663                             gate_gradients, aggregation_method, stop_gradients,
--> 664                             unconnected_gradients)
    665 
    666 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    997                     ""Original input shape: %s.  ""
    998                     ""Calculated input gradient shape: %s"" %
--> 999                     (op.name, i, t_in.shape, in_grad.shape))
   1000             _SetGrad(grads, t_in, in_grad)
   1001         if loop_state:

ValueError: Incompatible shapes between op input and calculated input gradient.  Forward operation: Dec_Conv5/dcon1.  Input index: 2. Original input shape: (?, 256, 256, 64).  Calculated input gradient shape: (?, 512, 512, 64)




"
27977,increase throughput for `ResizeBilinearKernel`,"`ResizeBilinearKernel` in [cuda file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/resize_bilinear_op_gpu.cu.cc) there are 3 major changes for improvement:

1. `images` is `NHWC` which makes reads and writes to global memory uncoalesced. By changing the input and output to `NCHW`, we can increase the overall speed by a few folds
2. instead of spawning each thread for every element, spawn only as many threads as `output_height * output_width`, and have each thread iterate over `channel ` and `batch` dimension. this has two benefits: it increases throughput roughly 30%, and it allows smaller kernel launches which allows more number of concurrent kernels to be present on the device.
3. use `__restrict__` modifier for both input and output. This alone increase throughput 2~4%

The above improvements is something that can be applied to general kernels that perform element-wise operation to a vector. I have a working code of `ResizeBilinearKernel` with increased throughput which I can push in a couple days as soon as I get a few uncertainties straightened out. I would really appreciate your help.

Thanks :)
"
27975,CUDA Kernel throughput optimization,"I want to go through Tensorflow's CUDA kernels and optimize them. It seems like all the CUDA kernels are defined in `tensorflow/core/kernels` and `tensorflow/contrib` with some helper functions in `tensorflow/core/util`.

1. Is it okay for me to optimize kernels in tensorflow/core/kernels and send pull request?

2. now that tensorflow is moving to 2.0, are the CUDA kernels going to also change so dramatically that any improvements to them now could be rendered useless?

3. what are some conventions or practices when writing CUDA kernels? I have read the `contrib.md`, but there was nothing special on CUDA kernels ( ex. "" Do not change kernel launch configurations "" or "" must support devices of compute capability >= 3.0 "" )

4. must I use `CudaGridRangeX` over regular for loops?

5. Is there way to test and profile each .cu file individually, instead of having to rebuild the entire tensorflow or even having to test all kernels?"
27972,Tensorflow 2.0's eager_execution for keras is not real.,"For normal tensorflow codes, it works well.
However, for tf.keras, it actually performs as a static graph, which is terrible.

An good example is tensorflow 1.12.0, which enables users easily debug keras models.
I just wonder why not inherit the 1.12.0's mode in tf2.0? the former is much better."
27970,piecewise_constant_decay not supported by xla: Segmentation fault (core dumped),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CENTOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below): 1.13
- Python version: 3.6
- Bazel version (if compiling from source): 0.21
- GCC/Compiler version (if compiling from source): 6.3.0
- CUDA/cuDNN version: NA
- GPU model and memory: NA

 I am trying to run a model which uses `tf.train.piecewise_constant_decay` with xla.  Previously this was supported in tf1.10 when I last tried. I am now trying the same script in tf1.13. 

I receive a segmentation fault when I run it now. After digging around it seems to be because of type `DT_STRING` which the `const` kernel doesn't seem to support. 

When I see what operations are supported by the `Const` kernel, 
(`bazel run -c opt -- tensorflow/compiler/tf2xla:tf2xla_supported_ops --device=XLA_CPU_JIT`) it shows 
``` 
Const | dtype={bool,complex64,double,float,half,int32,int64,int8,qint32,qint8,quint8,string,uint32,uint64,uint8}
```  

To reproduce this bug, a simple example is:
```
import tensorflow as tf
from tensorflow.contrib.compiler.xla import compile
import numpy as np

LABELS = 10
HIDDEN_SIZE = 256
BATCH_SIZE = 64
FEAT_DIM = 784

xla_flag = True


def model_fn(features, labels):
    with tf.variable_scope(""fc"", use_resource=True):
        net = tf.keras.layers.Dense(units=HIDDEN_SIZE,
                                    activation=tf.nn.relu)(features)
        logits = tf.keras.layers.Dense(units=LABELS)(net)
        cross_entropy = tf.reduce_mean(
            tf.nn.softmax_cross_entropy_with_logits(labels=labels,
                                                    logits=logits))
        global_step = tf.train.get_or_create_global_step()
        boundaries = [100000, 110000]
        values = [1.0, 0.5, 0.1]
        learning_rate = tf.train.piecewise_constant_decay(global_step, boundaries, values)
        train_step = tf.train.GradientDescentOptimizer(
            learning_rate, name=""final_node"").minimize(cross_entropy)
        with tf.control_dependencies([train_step]):
            return tf.identity(cross_entropy, name=""results"")

x = tf.placeholder(tf.float32, [BATCH_SIZE, FEAT_DIM], name='x')
y = tf.placeholder(tf.float32, [BATCH_SIZE, LABELS], name='y')

if xla_flag:
    (xla_loss,) = compile(model_fn, [x,y])
else:
    xla_loss = model_fn(x,y)

with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())
    ans = sess.run(xla_loss,feed_dict={x:np.ones((BATCH_SIZE,FEAT_DIM)),y:np.ones((BATCH_SIZE,LABELS))})
```

While looking at the implementation, it seems like it could be because of the name_scope defined at https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/training/learning_rate_decay_v2.py#L169 

Please let me know if there is a solution, or if I am doing something wrong from my side. "
27967,cannot find symbol class Fillwhere T is a type-variable:T extends Object declared in class Zeros,"**System information**
- Have I written custom code: no
- OS Platform and Distribution: Windows 10
- Mobile device: Huawei P9 Lite (Android 6.0)
- TensorFlow installed from: source but i did not build it, read below
- TensorFlow version: 1.13.1 but on anaconda and in my situation i'm not using any installation read below.
- Python version: 3.6.8 same thing.
- Bazel version (if compiling from source): 0.21.0
- CUDA/cuDNN version: 10.0 / 7.3.1
- GPU model and memory: GEFORCE 840M 4GB

**Current behavior**
I'm using Tensorflow's android example but it would fail to compile unless i changed nativeBuildSystem from 'bazel' to 'none' in build.gradle.

**The expected behavior**
I must use a compiler because if i don't, then Object Tracking feature wouldn't be available in the app.

**Code to reproduce the issue**
I cloned the repo (but did not build it) and got bazel up and running, i then opened the android example in Android Studio, that is all to reproduce this i guess. I'm only using the android example do i need to build tensorflow from source because i've struggled with that for months.

**All Errors**
cannot find symbol class Fillwhere T is a type-variable:T extends Object declared in class Zeros
cannot find symbol class Fillwhere T is a type-variable:T extends Object declared in class Zeros
cannot find symbol variable Fillwhere T is a type-variable:T extends Object declared in class Zeros

"
27965,bug,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27961,Multi level variable scopes not working with tf.layers,"I am trying to access tf.layers of one scope from another scope but I guess the scopes are not working properly.  


```
import tf.layers as layers
def mlp_model(input, num_outputs, scope, reuse=False, num_units=64, rnn_cell=None):
    # This model takes as input an observation and returns values of all actions
    with tf.variable_scope(scope, reuse=reuse):
        out = input
        out = layers.dense(out, units=num_units, activation=tf.nn.relu)
        out = layers.dense(out, units=num_units, activation=tf.nn.relu)
        out = layers.dense(out, units=num_outputs, activation=None)
        return out

input_placeholder = tf.placeholder(tf.float32, shape=(None, 64), name=""input"")

with tf.variable_scope(""agent_0"") as agent_scope:
    q_func = mlp_model(input_placeholder, 2, ""q_func"", num_units=64)

 with tf.variable_scope(""agent_1""):
     with tf.variable_scope(agent_scope, reuse=True):
         q_func_2=mlp_model(input_placeholder, 2, ""q_func"", num_units=64, reuse=True)

print(q_func == q_func_2)
```

As far as I know the agent_scope should have cleared the ""agent_1""  and q_func == q_func_2 should be true.  

I also tried the following code that works so I am assuming, there is a bug

```
with tf.variable_scope('agent_0') as agent_scope:      
w1 = tf.get_variable('test', [1], dtype=tf.float32)

with tf.variable_scope('agent_1'):
  with tf.variable_scope(agent_scope, reuse=True):  
    w2 = tf.get_variable('test', [1], dtype=tf.float32)

print(w1==w2)
```
I am using TF 1.13.1 on Ubuntu 18.04 GeForce GTX 1070/PCIe/SSE2
"
27956,All Diagrams Missing,"**System information**
- TensorFlow version: N/A
- Doc Link: https://github.com/tensorflow/docs/blob/master/site/en/guide/premade_estimators.md
https://github.com/tensorflow/docs/blob/master/site/en/guide/custom_estimators.md

**Describe the documentation issue**
All of the diagrams / images are missing from both of the links above. 

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
No.  I don't know what the diagrams were supposed to be."
27955,TF Nightly 20190418 Crashes on Estimator Import,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 16.04**
- TensorFlow installed from (source or binary): **Binary**
- TensorFlow version (use command below): **tf-nightly-20190418**
- Python version: **3.6**

**Describe the current behavior**
Nightly tensorflow crashes on importing estimator-nightly

**Describe the expected behavior**
Able to import

**Code to reproduce the issue**
```
pip install tf-nightly==1.14.1.dev20190418
import tensorflow
```
**Other info / logs**
https://colab.research.google.com/drive/1mvVTcAWHwJY1Kko49Ae93iQum6lT9W8D

Crash:
```
TypeError                                 Traceback (most recent call last)
<ipython-input-2-64156d691fe5> in <module>()
----> 1 import tensorflow as tf

/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py in <module>()
     32 from tensorflow._api.v1 import autograph
     33 from tensorflow._api.v1 import bitwise
---> 34 from tensorflow._api.v1 import compat
     35 from tensorflow._api.v1 import config
     36 from tensorflow._api.v1 import data

/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v1/compat/__init__.py in <module>()
     19 from __future__ import print_function as _print_function
     20 
---> 21 from tensorflow._api.v1.compat import v1
     22 from tensorflow._api.v1.compat import v2
     23 from tensorflow.python.compat.compat import forward_compatibility_horizon

/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v1/compat/v1/__init__.py in <module>()
    641     parent_package_str=__name__,
    642     child_package_str=(
--> 643         'tensorflow_estimator.python.estimator.api._v1.estimator'))
    644 _component_api_helper.package_hook(
    645     parent_package_str=__name__,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/component_api_helper.py in package_hook(parent_package_str, child_package_str, error_msg)
     54   parent_pkg = importlib.import_module(parent_package_str)
     55   try:
---> 56     child_pkg = importlib.import_module(child_package_str)
     57   except ImportError:
     58     if error_msg:

/usr/lib/python3.6/importlib/__init__.py in import_module(name, package)
    124                 break
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 
    128 

/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/__init__.py in <module>()
      6 from __future__ import print_function as _print_function
      7 
----> 8 from tensorflow_estimator._api.v1 import estimator
      9 _names_with_underscore = []
     10 __all__ = [_s for _s in dir() if not _s.startswith('_')]

/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/_api/v1/estimator/__init__.py in <module>()
      6 from __future__ import print_function as _print_function
      7 
----> 8 from tensorflow_estimator._api.v1.estimator import experimental
      9 from tensorflow_estimator._api.v1.estimator import export
     10 from tensorflow_estimator._api.v1.estimator import inputs

/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py in <module>()
     33 if not isinstance(_sys.modules[__name__], _deprecation_wrapper.DeprecationWrapper):
     34   _sys.modules[__name__] = _deprecation_wrapper.DeprecationWrapper(
---> 35       _sys.modules[__name__], ""estimator.experimental"", _DEPRECATED_TO_CANONICAL)

TypeError: __init__() takes 3 positional arguments but 4 were given
```"
27954,Tensorflow 2.0 has missing features related to checkpointing Dataset,"**System information**
- TensorFlow version: 2.0-alpha.0
- Doc Link:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/make_saveable_from_iterator

**Describe the documentation issue**
With 2.0-alpha.0, tf.data.Dataset does not have any make_xx_iterator() methods, so I can't save/restore iterate state while training by using tf.data.experimental.make_saveable_from_iterator().

How to save/restore internal state of Dataset?

"
27953,tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'TFLite_Detection_PostProcess' in binary,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version: 1.9
- Python version: python 3
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 
- GPU model and memory: GeForce RTX 2080 Ti/PCIe/SSE2  memory:15.6 GiB

I am trying to convert tflite_graph.pb to .tflite from terminal. Earlier I was using tensorflow 1.3 and used the below command to convert:

`tflite_convert  --graph_def_file=/home/user/Desktop/FaceDetection/frozen_tflite/tflite_graph.pb --output_file=/home/user/Desktop/FaceDetection/frozen_tflite/coverted_frozen_graph.tflite --input_arrays=normalized_input_image_tensor --output_arrays=""detection_boxes"",""detection_scores"",""detection_classes"",""num_detections"" --input_shape=1,300,300,3 --allow_custom_ops=True
` 

That time I got an error that tflite_convert command not found. After digging few git issues and stackover flow, I found that this will work with **tensorflow 1.9**

I used the pip install -upgrade tensorflow==1.9 to upgrade the tf.

It ran with the following error:
`2019-04-18 17:24:39.216037: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""/usr/local/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 320, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 316, in run_main
    _convert_model(tflite_flags)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 91, in _convert_model
    converter = _get_toco_converter(flags)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 81, in _get_toco_converter
    return converter_fn(**converter_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/lite/python/lite.py"", line 204, in from_frozen_graph
    import_graph_def(graph_def, name="""")
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py"", line 418, in import_graph_def
    graph._c_graph, serialized, options)  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'TFLite_Detection_PostProcess' in binary running on user. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`

**tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'TFLite_Detection_PostProcess' in binary**

Can anyone help me to solve this issue?

Thank in adv.
"
27952,"Array conv_23/Conv_1/BatchNorm/FusedBatchNorm, which is an input to the Conv operator producing the output array conv_3//0/Conv/Relu6, is lacking min/max data, which is necessary for quantization. ","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):conda install tensorflow-gpu==1.13
- TensorFlow version (use command below):1.13
- Python version: 3.6
- Bazel version (if compiling from source):NO
- GCC/Compiler version (if compiling from source):NO
- CUDA/cuDNN version: CUDA9.0 cuDNN7.1
- GPU model and memory: p40


**Describe the current behavior**
I quantized training mobilefacenet,   and want to conver pb file to tflite file but failed

toco --output_file=face_freeze.tflite --graph_def_file=face_freeze.pb --input_arrays=x_i
nput  --output_arrays=BatchNorm/Reshape_1 --output_format=TFLITE --inference_typ
e=QUANTIZED_UINT8 --mean_values=127  --std_dev_values=128

**Code to reproduce the issue**
`def model(is_training , x, y, is_quantize ):

    with slim.arg_scope([slim.conv2d, slim.fully_connected],
                        activation_fn = tf.nn.relu6,
                        weights_initializer=slim.variance_scaling_initializer(factor=3.0, mode='FAN_IN', uniform=True),					
                        biases_initializer = tf.constant_initializer(0.0), 						
                        normalizer_fn=slim.batch_norm,    
                        normalizer_params={'is_training': is_training, 'epsilon':0.001, 'scale': True, 'decay':0.9, 'updates_collections': tf.GraphKeys.UPDATE_OPS}):

        print('x',x.get_shape())        
        # conv_1        
        conv_1 = slim.conv2d(x, 64, [3, 3], stride=2, padding='SAME',  scope='conv_1')
        print('conv_1',conv_1.get_shape())

        # conv_2
        conv_2 = Residual(conv_1, num_block=2, num_out=64, _kernel=3,  _stride=1, pad='SAME', num_group=64, name='conv_2')         
        print('conv_2',conv_2.get_shape())

       # conv_23
        conv_23 = DResidual(conv_2, num_out=128, _kernel=3 , _stride=2, pad='SAME', num_group=128, name='conv_23')
        print('conv_23',conv_23.get_shape())

       # conv_3
        conv_3 = Residual(conv_23, num_block=6, num_out=128, _kernel=3, _stride=1, pad='SAME', num_group=128, name='conv_3')
        print('conv_3',conv_3.get_shape())
        .....
        .....
		
        #softmax loss
            cross_entropy__ = tf.nn.sparse_softmax_cross_entropy_with_logits(
                labels=y, logits=fc7, name='cross_entropy_per_example')
        cross_entropy = tf.reduce_mean(cross_entropy__, name='cross_entropy')

        all_variable_decay = _all_variables_decapy(weight_decay,1.0)		
        total_loss = cross_entropy + all_variable_decay 
			
        tf.contrib.quantize.create_training_graph()`

I use tf.contrib.quantize.create_eval_graph() to create eval graph pb file


**Other info / logs**
2019-04-18 19:21:14.747016: F tensorflow/lite/toco/tooling_util.cc:1702] Array c
onv_23/Conv_1/BatchNorm/FusedBatchNorm, which is an input to the Conv operator p
roducing the output array conv_3//0/Conv/Relu6, is lacking min/max data, which i
s necessary for quantization. If accuracy matters, either target a non-quantized
 output format, or run quantized training with your model from a floating point
checkpoint to change the input graph to contain min/max information. If you don'
t care about accuracy, you can pass --default_ranges_min= and --default_ranges_m
ax= for easy experimentation.

Looking forward to receive your reply,  thanks very much"
27951,tf.nn.batch_normalization unexpected behavior,"tf.VERSION :  1.13.0-rc2
OS              : MacOS Mojave 10.14.4 on MacBook Pro (13-inch, 2017)

I encountered a problem trying to implement tf.nn.batch_normalization working on the mnist digits dataset.
For testing purposes, I implemented a very simple network
L1 : 100 neurons fully connected layer + Batch norm + Sigmoid
L2:   10 neurons fully connected layer + softmax

When I use tf.nn.batch_normalization 
`Ybn1   = tf.nn.batch_normalization(Yl1, m1, v1, O1, S1, 1e-5)` it leads to divergence:

![image](https://user-images.githubusercontent.com/44782534/56353537-df10f380-61d1-11e9-9060-749a04f55cb4.png)

If I do the math by myself it does converge:
`Yhat1 = (Yl1 - m1) / tf.sqrt(v1 + 1e-5)`
` Ybn1  = S1 * Yhat1 + O1`

![image](https://user-images.githubusercontent.com/44782534/56353771-6eb6a200-61d2-11e9-9d4f-08a6096091ea.png)

If I implement the content of batch_normalization function within my code, it does not work either.
`inv = math_ops.rsqrt(v1 + 1e-5)`
`inv *= S1`
`Ybn1 =  Yl1 * math_ops.cast(inv, Yl1.dtype) + math_ops.cast(O1 - m1 * inv, Yl1.dtype)`

but if I combine the 2 last lines it works correctly:
 `inv = math_ops.rsqrt(v1 + 1e-5)`
`Ybn1 =  (Yl1 - m1) * inv * S1+ O1`

I have certainly done something wrong, but I cannot figure out what, and if it is a real bug I just wanted to let you know.

Here is the entire code if you want to reproduce the issue:
https://github.com/neodelphis/tensorflow-without-a-phd-french/blob/master/mnist_test.ipynb

"
27949,Tensorflow 2.0: No gradients provided for any variable but only when tf.math.square AND tf.function is used?,"I posted this in SO

https://stackoverflow.com/questions/55730930/tensorflow-2-0-no-gradients-provided-for-any-variable-but-only-when-tf-math-squ

but now am wondering if it is actually a bug. 

```
In [176]: sys.version
Out[176]: '3.7.3 (default, Mar 27 2019, 16:54:48) \n[Clang 4.0.1 (tags/RELEASE_401/final)]'

In [177]: tf.__version__
Out[177]: '2.0.0-alpha0'
```


``` python
import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.backend as K
import numpy as np

m = 1000
n = 1
X = np.random.randn(m, n).astype(np.float32)
y = (3 + 0 * np.random.randn(m)).astype(np.float32)

def create_model():
    a_input = keras.layers.Input(shape=(n,), dtype=np.float32)
    a = K.expand_dims(a_input, axis=2)
    q = keras.layers.Conv1D(1, 1)(a)
    q = - tf.math.square(q) # this breaks things, but only when using tf.function
    model = keras.models.Model(inputs=a_input, outputs=q)
    return model

model = create_model()
model.predict(X)

class Trainer():
    def __init__(self, epochs=10):
        self.epochs = epochs
        self.model = create_model()
        self.optimizer = tf.optimizers.Adam()
        self.step = 0
    def train(self, X, y, epochs=10):
        X = tf.convert_to_tensor(X, dtype=tf.float32)
        y = tf.convert_to_tensor(y, dtype=tf.float32)
        for epoch in range(epochs):
            l = self._train_one_step(X, y)
        return l
    @tf.function
    def _train_one_step(self, X, y):
        with tf.GradientTape() as tape:
            yp = self.model(X)
            loss = tf.reduce_mean(tf.math.square(y - yp))
        gradients = tape.gradient(loss, self.model.trainable_variables)
        l = self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))
        d = dict(loss=loss)
        tf.print(yp[0], loss)
        self.step += 1

trainer = Trainer()
l = trainer.train(X, y, epochs=100)
```


```
lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in _filter_grads(grads_and_vars)
    922   if not filtered:
    923     raise ValueError(""No gradients provided for any variable: %s."" %
--> 924                      ([v.name for _, v in grads_and_vars],))
    925   if vars_with_empty_grads:
    926     logging.warning(

ValueError: No gradients provided for any variable: ['conv1d_47/kernel:0', 'conv1d_47/bias:0'].
```"
27948,Automatic conversion from CONV_2D to DEPTHWISE_CONV_2D when input channl is 1 in tflite,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
macos 10.14.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
None
- TensorFlow installed from (source or binary):
conda install
- TensorFlow version (use command below):
tensorflow 1.13
- Python version:
python 3.6.8
- Bazel version (if compiling from source):
None
- GCC/Compiler version (if compiling from source):
None
- CUDA/cuDNN version:
None
- GPU model and memory:
None


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

I was trying to implement some super resolution models in tensorflow and tflite, and evaluate their speed using tflite benchmark tool. when i use 1 channel input, e.g. input = tf.placeholder(tf.float32, [1, img_h, img_w, 1]), the first convolution layer automatically became DEPTHWISE_CONV_2D in the benchmark, even if i called ""slim.convolution2d"" function, and the speed was extremely slow. Then i simply changed the input channel to be 3, e.g. input = tf.placeholder(tf.float32, [1, img_h, img_w, 3]), the first convolution in benchmark became CONV_2D as normal and the speed became much faster.

below are some information printed out by the benchmark.

first time, input was ""inputs = tf.placeholder(tf.float32, [1, img_h, img_w, 1])""

```
Initialized session in 253.8ms
Running benchmark for at least 1 iterations and at least 0.5 seconds
count=1 curr=980043

Running benchmark for at least 50 iterations and at least 1 seconds
count=50 first=948543 curr=944078 min=944078 max=955061 avg=947350 std=2060

============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	       DEPTHWISE_CONV_2D	            0.000	  538.139	  536.830	 56.668%	 56.668%	     0.000	        1	[srnetwork/Conv/Relu]
	                 CONV_2D	          536.833	   20.081	   20.083	  2.120%	 58.788%	     0.000	        1	[srnetwork/Conv_1/Relu]
	                 CONV_2D	          556.918	   36.430	   36.419	  3.844%	 62.632%	     0.000	        1	[srnetwork/Conv_2/Relu]
	                 CONV_2D	          593.339	   36.507	   36.473	  3.850%	 66.482%	     0.000	        1	[srnetwork/Conv_3/Relu]
	                 CONV_2D	          629.814	   36.657	   36.581	  3.862%	 70.344%	     0.000	        1	[srnetwork/Conv_4/Relu]
	                 CONV_2D	          666.397	   36.532	   36.718	  3.876%	 74.220%	     0.000	        1	[srnetwork/Conv_5/Relu]
	                 CONV_2D	          703.117	   55.629	   55.648	  5.874%	 80.094%	     0.000	        1	[srnetwork/Conv_6/Relu]
	                 CONV_2D	          758.768	  185.664	  185.694	 19.602%	 99.696%	     0.000	        1	[srnetwork/Conv_7/Conv2D]
	                   SPLIT	          944.465	    2.726	    2.722	  0.287%	 99.983%	     0.000	        1	[srnetwork/split, srnetwork/split:1]
	           CONCATENATION	          947.188	    0.089	    0.092	  0.010%	 99.993%	     0.000	        1	[srnetwork/concat]
	                 RESHAPE	          947.280	    0.063	    0.065	  0.007%	100.000%	     0.000	        1	[srnetwork/Reshape]

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	       DEPTHWISE_CONV_2D	            0.000	  538.139	  536.830	 56.668%	 56.668%	     0.000	        1	[srnetwork/Conv/Relu]
	                 CONV_2D	          758.768	  185.664	  185.694	 19.602%	 76.270%	     0.000	        1	[srnetwork/Conv_7/Conv2D]
	                 CONV_2D	          703.117	   55.629	   55.648	  5.874%	 82.144%	     0.000	        1	[srnetwork/Conv_6/Relu]
	                 CONV_2D	          666.397	   36.532	   36.718	  3.876%	 86.020%	     0.000	        1	[srnetwork/Conv_5/Relu]
	                 CONV_2D	          629.814	   36.657	   36.581	  3.862%	 89.882%	     0.000	        1	[srnetwork/Conv_4/Relu]
	                 CONV_2D	          593.339	   36.507	   36.473	  3.850%	 93.732%	     0.000	        1	[srnetwork/Conv_3/Relu]
	                 CONV_2D	          556.918	   36.430	   36.419	  3.844%	 97.576%	     0.000	        1	[srnetwork/Conv_2/Relu]
	                 CONV_2D	          536.833	   20.081	   20.083	  2.120%	 99.696%	     0.000	        1	[srnetwork/Conv_1/Relu]
	                   SPLIT	          944.465	    2.726	    2.722	  0.287%	 99.983%	     0.000	        1	[srnetwork/split, srnetwork/split:1]
	           CONCATENATION	          947.188	    0.089	    0.092	  0.010%	 99.993%	     0.000	        1	[srnetwork/concat]

Number of nodes executed: 11
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	       DEPTHWISE_CONV_2D	        1	   536.830	    56.668%	    56.668%	     0.000	        1
	                 CONV_2D	        7	   407.612	    43.028%	    99.696%	     0.000	        7
	                   SPLIT	        1	     2.722	     0.287%	    99.984%	     0.000	        1
	           CONCATENATION	        1	     0.091	     0.010%	    99.993%	     0.000	        1
	                 RESHAPE	        1	     0.065	     0.007%	   100.000%	     0.000	        1

Timings (microseconds): count=50 first=948517 curr=944053 min=944053 max=955032 avg=947325 std=2060
Memory (bytes): count=0
11 nodes observed
```


second time, input was ""inputs = tf.placeholder(tf.float32, [1, img_h, img_w, 3])""
please note that nothing else except the input channel was changed.
```
Initialized session in 257.992ms
Running benchmark for at least 1 iterations and at least 0.5 seconds
count=1 curr=559357

Running benchmark for at least 50 iterations and at least 1 seconds
count=50 first=510275 curr=520996 min=501413 max=591696 avg=519391 std=16622

============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	            0.000	   96.923	   97.608	 18.794%	 18.794%	     0.000	        1	[srnetwork/Conv/Relu6]
	                 CONV_2D	           97.611	   19.989	   20.090	  3.868%	 22.662%	     0.000	        1	[srnetwork/Conv_1/Relu6]
	                 CONV_2D	          117.703	   37.846	   38.136	  7.343%	 30.005%	     0.000	        1	[srnetwork/Conv_2/Relu6]
	                 CONV_2D	          155.844	   37.351	   38.488	  7.411%	 37.416%	     0.000	        1	[srnetwork/Conv_3/Relu6]
	                 CONV_2D	          194.335	   37.632	   37.826	  7.283%	 44.699%	     0.000	        1	[srnetwork/Conv_4/Relu6]
	                 CONV_2D	          232.163	   38.839	   37.950	  7.307%	 52.006%	     0.000	        1	[srnetwork/Conv_5/Relu6]
	                 CONV_2D	          270.116	   54.885	   55.838	 10.751%	 62.757%	     0.000	        1	[srnetwork/Conv_6/Relu6]
	                 CONV_2D	          325.956	  183.861	  190.501	 36.680%	 99.437%	     0.000	        1	[srnetwork/Conv_7/Conv2D]
	                   SPLIT	          516.461	    2.701	    2.727	  0.525%	 99.962%	     0.000	        1	[srnetwork/split, srnetwork/split:1]
	           CONCATENATION	          519.189	    0.135	    0.102	  0.020%	 99.982%	     0.000	        1	[srnetwork/concat]
	                 RESHAPE	          519.292	    0.082	    0.094	  0.018%	100.000%	     0.000	        1	[srnetwork/Reshape]

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	          325.956	  183.861	  190.501	 36.680%	 36.680%	     0.000	        1	[srnetwork/Conv_7/Conv2D]
	                 CONV_2D	            0.000	   96.923	   97.608	 18.794%	 55.474%	     0.000	        1	[srnetwork/Conv/Relu6]
	                 CONV_2D	          270.116	   54.885	   55.838	 10.751%	 66.225%	     0.000	        1	[srnetwork/Conv_6/Relu6]
	                 CONV_2D	          155.844	   37.351	   38.488	  7.411%	 73.636%	     0.000	        1	[srnetwork/Conv_3/Relu6]
	                 CONV_2D	          117.703	   37.846	   38.136	  7.343%	 80.979%	     0.000	        1	[srnetwork/Conv_2/Relu6]
	                 CONV_2D	          232.163	   38.839	   37.950	  7.307%	 88.286%	     0.000	        1	[srnetwork/Conv_5/Relu6]
	                 CONV_2D	          194.335	   37.632	   37.826	  7.283%	 95.569%	     0.000	        1	[srnetwork/Conv_4/Relu6]
	                 CONV_2D	           97.611	   19.989	   20.090	  3.868%	 99.437%	     0.000	        1	[srnetwork/Conv_1/Relu6]
	                   SPLIT	          516.461	    2.701	    2.727	  0.525%	 99.962%	     0.000	        1	[srnetwork/split, srnetwork/split:1]
	           CONCATENATION	          519.189	    0.135	    0.102	  0.020%	 99.982%	     0.000	        1	[srnetwork/concat]

Number of nodes executed: 11
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	        8	   516.432	    99.438%	    99.438%	     0.000	        8
	                   SPLIT	        1	     2.727	     0.525%	    99.963%	     0.000	        1
	           CONCATENATION	        1	     0.101	     0.019%	    99.982%	     0.000	        1
	                 RESHAPE	        1	     0.093	     0.018%	   100.000%	     0.000	        1

Timings (microseconds): count=50 first=510244 curr=520968 min=501382 max=591666 avg=519359 std=16622
Memory (bytes): count=0
11 nodes observed
```

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27944,The kernel appears to have died. It will restart automatically. with memory problem?,"1


kernel died after running some code 
I try to run the code to generate a sample image with the generator I tried to update the conda and Jupiter but none of them worked 

I keep watching the memory usage of GPU but it does not use the GPU that much 

tensorflow2.0 , ubuntu 18.10, cuda 10.0 
python 3.5,

def make_generator_model():
    model = tf.keras.Sequential()
    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Reshape((7, 7, 256)))
    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size

    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 7, 7, 128)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 14, 14, 64)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 28, 28, 1)

    return model
generator = make_generator_model()

noise = tf.random.normal([1, 100])
generated_image = generator(noise, training=False)
[I 10:20:06.664 NotebookApp] KernelRestarter: restarting kernel (1/5), keep random ports WARNING:root:kernel 4406ce3b-1b5b-4ef8-aba9-d5fd9ed129e7 restarted 2019-04-18 10:20:21.002451: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1 2019-04-18 10:20:21.081020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1589] Found device 0 with properties: name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582 pciBusID: 0000:42:00.0 totalMemory: 11.91GiB freeMemory: 340.69MiB 2019-04-18 10:20:21.081054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1712] Adding visible gpu devices: 0 2019-04-18 10:20:21.081382: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA 2019-04-18 10:20:21.107510: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55de6ead0990 executing computations on platform CUDA. Devices: 2019-04-18 10:20:21.107562: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): TITAN Xp, Compute Capability 6.1 2019-04-18 10:20:21.127890: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3493050000 Hz 2019-04-18 10:20:21.129460: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55de6eed7eb0 executing computations on platform Host. Devices: 2019-04-18 10:20:21.129503: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): , 2019-04-18 10:20:21.129616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1712] Adding visible gpu devices: 0 2019-04-18 10:20:21.129722: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0 2019-04-18 10:20:21.130785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Device interconnect StreamExecutor with strength 1 edge matrix: 2019-04-18 10:20:21.130807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1126] 0 2019-04-18 10:20:21.130819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1139] 0: N 2019-04-18 10:20:21.131090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1260] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 115 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:42:00.0, compute capability: 6.1) 2019-04-18 10:20:24.168083: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0 2019-04-18 10:20:24.331094: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7 2019-04-18 10:20:24.789774: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 2019-04-18 10:20:24.791468: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 2019-04-18 10:20:24.791484: F tensorflow/core/kernels/conv_grad_input_ops.cc:949] Check failed: stream->parent()->GetConvolveBackwardDataAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo(stream->parent()), &algorithms) [I 10:20:27.669 NotebookApp] KernelRestarter: restarting kernel (1/5), keep random ports WARNING:root:kernel 4406ce3b-1b5b-4ef8-aba9-d5fd9ed129e7 restarted"
27943,`GetCudaLaunchConfig` configuration of number of blocks,"the definition is:
```
inline CudaLaunchConfig GetCudaLaunchConfig(int work_element_count,
                                            const Eigen::GpuDevice& d) {
  CHECK_GT(work_element_count, 0);
  CudaLaunchConfig config;
  const int virtual_thread_count = work_element_count;
  const int physical_thread_count = std::min(
      d.getNumGpuMultiProcessors() * d.maxGpuThreadsPerMultiProcessor(),
      virtual_thread_count);
  const int thread_per_block = std::min(1024, d.maxGpuThreadsPerBlock());
  const int block_count =
      std::min(DivUp(physical_thread_count, thread_per_block),
               d.getNumGpuMultiProcessors());

  config.virtual_thread_count = virtual_thread_count;
  config.thread_per_block = thread_per_block;
  config.block_count = block_count;
  return config;
}
```

why is the number of blocks is capped at `getNumGpuMultiProcessors`?"
27942,"Why ""Don't build XLA with -ffast-math""?","The most problem is not as title.

It is ridiculous that the error is thrown out after thousands of files were compiled."
27941,RNN compatibility issue on unknown batch_size,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google colaboratory
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0-alpha0 or 1.13.1
- Python version: 3.6.7
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: not using GPU
- GPU model and memory: not using GPU

**Describe the current behavior**

I sent some data from one model's loss function to another model. The other model starts with a RNN layer.
in tensorflow 1.12.0 and before, the variable y_pred is shape (32, 10, 10) or (20, 10, 10) as first dimension is batch_size, and the model trained normally
in tensorflow 1.13.1 and 2.0.0-alpha0, it shows that the shape is (None, 10, 10) then throws this exception

```
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in <listcomp>(.0)
   3217     outputs = self._graph_fn(*converted_inputs)
   3218     return nest.pack_sequence_as(self._outputs_structure,
-> 3219                                  [x.numpy() for x in outputs])
   3220 
   3221 

AttributeError: 'Tensor' object has no attribute 'numpy'
```

probably because it changed the behavior to use a dynamic batch size tensor, then tensors of unknown batch size don't have the numpy method?

If the RNN layer is not present, and the data is 2d (like (None, 10) ), the model can train normally.
CNN and Flatten seem to also have the problem.

**Describe the expected behavior**
I think it should not use $.numpy() there in the backend, probably(?)

**Code to reproduce the issue**
```python
import tensorflow as tf
import numpy as np
from tensorflow import keras

# try-except eager trigger to prevent trouble from ipynb
try:
    tf.enable_eager_execution();
except:
    pass

def build_classifier_model():
    model = keras.Sequential([
        keras.layers.SimpleRNN(64, input_shape=(10, 10)),
        keras.layers.Dense(64, activation=tf.nn.relu),
        keras.layers.Dense(1, activation=tf.nn.tanh)
    ])

    optimizer = tf.optimizers.Adam(0.001) #tf.train.AdamOptimizer(0.001)

    model.compile(loss='mse',
                optimizer=optimizer,
                metrics=[keras.metrics.mae])
    return model

some_model = build_classifier_model();
  
def build_model():
    def custom_loss(y_true, y_pred):
      # simply tile y_pred as an example
      y_cred = tf.tile(tf.expand_dims(y_pred, axis=2), (1, 1, 10))
      print(y_cred.shape)
      return tf.reduce_sum(some_model(y_cred) ** 2)
    
    model = keras.Sequential([
        keras.layers.Dense(64, input_shape=(12,)),
        keras.layers.Dense(10, activation=tf.nn.tanh)
    ])

    optimizer = tf.optimizers.Adam(0.001) #tf.train.AdamOptimizer(0.001)

    model.compile(loss=custom_loss,
                optimizer=optimizer,
                metrics=[keras.metrics.mae])
    return model
        
m = build_model();
# fitting randomness as example
x = np.random.random((500, 12));
y = tf.cast(np.random.randint(0, 2, size=(500,)), tf.float32);
m.fit(x, y, epochs=10);
```

**Other info / logs**

full exception log
```python
(None, 10, 10)

---------------------------------------------------------------------------

AttributeError                            Traceback (most recent call last)

<ipython-input-3-de77aceb3f93> in <module>()
     38     return model
     39 
---> 40 m = build_model();
     41 # fitting randomness as example
     42 x = np.random.random((500, 12));

<ipython-input-3-de77aceb3f93> in build_model()
     35     model.compile(loss=custom_loss,
     36                 optimizer=optimizer,
---> 37                 metrics=[keras.metrics.mae])
     38     return model
     39 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    454     self._setattr_tracking = False  # pylint: disable=protected-access
    455     try:
--> 456       result = method(self, *args, **kwargs)
    457     finally:
    458       self._setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)
    428       #                   loss_weight_2 * output_2_loss_fn(...) +
    429       #                   layer losses.
--> 430       self.total_loss = self._prepare_total_loss(skip_target_indices, masks)
    431 
    432       # Functions for train, test and predict will

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _prepare_total_loss(self, skip_target_indices, masks)
   1684             loss_fn.reduction = losses_utils.ReductionV2.NONE
   1685             weighted_losses = loss_fn(
-> 1686                 y_true, y_pred, sample_weight=sample_weight)
   1687             loss_fn.reduction = current_loss_reduction
   1688 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py in __call__(self, y_true, y_pred, sample_weight)
     94     with ops.name_scope(scope_name, format(self.__class__.__name__),
     95                         (y_pred, y_true, sample_weight)):
---> 96       losses = self.call(y_true, y_pred)
     97       return losses_utils.compute_weighted_loss(
     98           losses, sample_weight, reduction=self.reduction)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py in call(self, y_true, y_pred)
    156       Loss values per sample.
    157     """"""
--> 158     return self.fn(y_true, y_pred, **self._fn_kwargs)
    159 
    160   def get_config(self):

<ipython-input-3-de77aceb3f93> in custom_loss(y_true, y_pred)
     24       y_cred = tf.tile(tf.expand_dims(y_pred, axis=2), (1, 1, 10))
     25       print(y_cred.shape)
---> 26       return tf.reduce_sum(some_model(y_cred) ** 2)
     27 
     28     model = keras.Sequential([

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    559       # framework.
    560       if base_layer_utils.needs_keras_history(inputs):
--> 561         base_layer_utils.create_keras_history(inputs)
    562 
    563     # Handle Keras mask propagation from previous layer to current layer.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py in create_keras_history(tensors)
    249       operations and need to have Keras metadata assigned to them.
    250   """"""
--> 251   _create_keras_history_helper(tensors, set())
    252 
    253 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py in _create_keras_history_helper(tensors, processed_ops)
    286           # when originating from an eager context
    287           # so can't be supported.
--> 288           constants[i] = backend.function([], op_input)([])
    289       processed_ops = _create_keras_history_helper(layer_inputs, processed_ops)
    290       name = op.name

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)
   3217     outputs = self._graph_fn(*converted_inputs)
   3218     return nest.pack_sequence_as(self._outputs_structure,
-> 3219                                  [x.numpy() for x in outputs])
   3220 
   3221 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in <listcomp>(.0)
   3217     outputs = self._graph_fn(*converted_inputs)
   3218     return nest.pack_sequence_as(self._outputs_structure,
-> 3219                                  [x.numpy() for x in outputs])
   3220 
   3221 

AttributeError: 'Tensor' object has no attribute 'numpy'
```"
27940,__name__ becomes 'tensorflow.keras.layers' when import tf.keras.layers,"**System information**
Google Colaboratory with tensorflow nightly build 2.0.0.dev20190417.
Any runtime

**Code to reproduce**
```
!pip install tf-nightly-2.0-preview
import tensorflow as tf
from tensorflow.keras.layers import *
print(__name__)  # output is 'tensorflow.keras.layers'
```

**Code to avoid the problem**
```
!pip install tf-nightly-2.0-preview
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense # Import layers explicitly
print(__name__)  # output is '__main__'
```"
27939,[CI] Is TensorFlow-jenkins bot died?,"Has Tensoflow-Jenkins bot been running now? We could  get the some messages from the bot when PR was submitted in the past (e.g., PR #13557). However, It seems that we could not see the any activity of the Tensoflow-Jenkins bot recently. What is the CI infrastructure  (e.g., Jenkins, Kokoro, Travis-CI, Circle-CI, and so on)of the Tensorflow deep-learning framework?


* Example: PR #24314 (On Dec-13-2019)
![image](https://user-images.githubusercontent.com/82404/56335313-f6061480-61d6-11e9-9118-c8d4ff11c39e.png)

* Example: PR #20809 (On Jun-4-2019)
![image](https://user-images.githubusercontent.com/82404/56335253-b5a69680-61d6-11e9-9795-045c7b0fb94e.png)


* Example: PR #13557 (On Oct-8-2017)
![image](https://user-images.githubusercontent.com/82404/56335169-652f3900-61d6-11e9-9d5f-a6ea642bb606.png)
"
27938,Read timed out and referenced by '//tensorflow/tools/pip_package:licenses',"bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package   
 Fetching @jpeg; fetching 167s
ERROR: /mnt/d/Project/TensorFlow/tensorflow-master/tensorflow/tools/pip_package/BUILD:145:1: no such package '@grpc//third_party/address_sorting': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/grpc/grpc/archive/4566c2a29ebec0835643b972eb99f4306c4234a3.tar.gz, https://github.com/grpc/grpc/archive/4566c2a29ebec0835643b972eb99f4306c4234a3.tar.gz] to /home/qili/.cache/bazel/_bazel_qili/f937d3405457c6b857b73c6deab45175/external/grpc/4566c2a29ebec0835643b972eb99f4306c4234a3.tar.gz: Read timed out and referenced by '//tensorflow/tools/pip_package:licenses'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@grpc//third_party/address_sorting': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/grpc/grpc/archive/4566c2a29ebec0835643b972eb99f4306c4234a3.tar.gz, https://github.com/grpc/grpc/archive/4566c2a29ebec0835643b972eb99f4306c4234a3.tar.gz] to /home/qili/.cache/bazel/_bazel_qili/f937d3405457c6b857b73c6deab45175/external/grpc/4566c2a29ebec0835643b972eb99f4306c4234a3.tar.gz: Read timed out
INFO: Elapsed time: 168.452s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)"
27937,Restoring from checkpoints are broken in TF 1.13.1,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Binary, pip3 install tensorflow-gpu
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: V10.0.130
- GPU model and memory: GTX 1060m, 6GB

**Describe the current behavior**
I am unable to restore the weights of any of my tf.keras models ONLY when restoring from a new initialization of the model. If I change the weights then restore without reinitializing the model, it will properly restore. Furthermore, a SILENT error is being thrown when this happens, requiring me to print the status of the restore to see it.

**Describe the expected behavior**
The weights should restore and not run into an error. And if an error would occur, it should be logged without me having to print it myself.

**Code to reproduce the issue**
```
import os
import tensorflow as tf
import numpy as np

# Disable logging
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
tf.logging.set_verbosity(tf.logging.ERROR)
tf.enable_eager_execution()

# Create model
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(256, 3, padding=""same""),
    tf.keras.layers.Conv2D(3, 3, padding=""same"")
])
print(""Are weights empty before training?"", model.weights == [])

# Create optim, checkpoint
optimizer = tf.train.AdamOptimizer(0.001)
checkpoint = tf.train.Checkpoint(model=model)

# Make fake data
img = np.random.uniform(0, 255, (32, 32, 3)).astype(np.float32)
truth = np.random.uniform(0, 255, (32, 32, 3)).astype(np.float32)
# Train
with tf.GradientTape() as tape:
    logits = model(img[None])
    loss = tf.losses.mean_squared_error(truth[None], logits)

# Compute/apply gradients
grads = tape.gradient(loss, model.trainable_weights)
grads_and_vars = zip(grads, model.trainable_weights)
optimizer.apply_gradients(grads_and_vars)

# Save model
checkpoint_path = './ckpt/'
checkpoint.save('./ckpt/')

# Check if weights update
print(""Are weights empty after training?"", model.weights == [])

# Reset model
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(256, 3, padding=""same""),
    tf.keras.layers.Conv2D(3, 3, padding=""same"")
])
print(""Are weights empty when resetting model?"", model.weights == [])

# Update checkpoint pointer
checkpoint = tf.train.Checkpoint(model=model)
# Restore values from the checkpoint
status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))

print(""Are weights empty after restoring from checkpoint?"", model.weights == [])
print(status)
status.assert_existing_objects_matched()
status.assert_consumed()
```

**Other info / logs**
```
Are weights empty before training? True
Are weights empty after training? False
Are weights empty when resetting model? True
Are weights empty after restoring from checkpoint? True
<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus object at 0x7f6ac9691f98>
Traceback (most recent call last):
  File ""test.py"", line 56, in <module>
    status.assert_consumed()
  File ""/home/jpatts/Documents/alpha-doom/env/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/util.py"", line 1025, in assert_consumed
    raise AssertionError(""Unresolved object in checkpoint: %s"" % (node,))
AssertionError: Unresolved object in checkpoint: attributes {
  name: ""VARIABLE_VALUE""
  full_name: ""sequential/conv2d/kernel""
  checkpoint_key: ""model/layer-0/kernel/.ATTRIBUTES/VARIABLE_VALUE""
}
```"
27936,Following object-based saving guide on TensorFlow website leads to error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary? pip3 install tensorflow-gpu
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: V10.0.130
- GPU model and memory: GTX 1060m, 6gb

**Describe the current behavior**
I am having issues saving and loading my models, so I tried the MVP example on the tensorflow website: https://www.tensorflow.org/guide/eager#object-based_saving
It has an error.
**Describe the expected behavior**
It should work.
**Code to reproduce the issue**
```
import tensorflow as tf

x = tf.Variable(10.)
checkpoint = tf.train.Checkpoint(x=x)

x.assign(2.)   # Assign a new value to the variables and save.
checkpoint_path = './ckpt/'
checkpoint.save('./ckpt/')

x.assign(11.)  # Change the variable after saving.

# Restore values from the checkpoint
checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))

print(x)  # => 2.0
```
**Other info / logs**
```
python3 test.py 
WARNING:tensorflow:From /home/jpatts/Documents/alpha-doom/env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Traceback (most recent call last):
  File ""test.py"", line 8, in <module>
    checkpoint.save('./ckpt/')
  File ""/home/jpatts/Documents/alpha-doom/env/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/util.py"", line 1856, in save
    session.run(self.save_counter.initializer)
AttributeError: 'NoneType' object has no attribute 'run'
```"
27935,ModuleNotFoundError: No module named 'tensorflow'  in spyder | windows + anaconda,"**System information**
- OS Platform and Distribution : Windows 10 64-bits
- TensorFlow installed from (source or binary): Python 3.6 CPU-only | https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.13.1-cp36-cp36m-win_amd64.whl
- TensorFlow version: 1.13.1
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: conda


**Describe the problem**

Hi,
I have struggled for hours to install tensorflow to Anaconda and finally succeeded. Hope my experience can help those who have same problem with installation.

I have tried a lot of methods before, such as:
```
pip install tensorflow
conda install tensorflow
```
etc,.
These methods might act as it installed properly but when I tested it in spyder, there were some problems, such as:

> ImportError: cannot import name 'abs'
> attributeerror: module 'tensorflow' has no attribute '__version__'

I  just installed tensorflow successfully in Anaconda following the instructions from: https://www.tensorflow.org/install/pip
2. Create a virtual environment (recommended)
``` 
(base) C:\Users\XXXX>conda create -n venv pip python=3.6
(base) C:\Users\XXXX>conda activate venv
(venv) C:\Users\XXXX>pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.13.1-cp36-cp36m-win_amd64.whl
```

Then I test it in terminal:
```
(venv) C:\Users\XXXX>python
Python 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> tf.__version__
'1.13.1'
```

It works!
But when I tried to import tensorflow in spyder:
> ModuleNotFoundError: No module named 'tensorflow'


**Solution**

This problem might cause by using virtual environment, and in Anaconda, your spyder or Jupyter Notebook works in default root, but tensorflow is installed in an isolated virtual environment 'venv'. So just install a new spyder or Jupyter Notebook under the virtual enviroment.
- 1. Open Anaconda Navigation
![image](https://user-images.githubusercontent.com/15613656/56325165-c273ad80-6136-11e9-9438-ec8f8eb759c9.png)
- 2. Change 'Applications on' to the virtual environment you just created ('venv')
- 3. Install a new spyder under 'venv'
- 4. Run the new spyder and test
![image](https://user-images.githubusercontent.com/15613656/56325929-c228e180-6139-11e9-9135-9d00715e03bf.png)


"
27932,NameError: name 'MyCapper' is not defined,"For gardient clipping in case of Adam optimizer, I'm using below code. 
```
# Create an optimizer.
opt = AdamOptimizer(learning_rate=0.1)

# Compute the gradients for a list of variables.
grads_and_vars = opt.compute_gradients(loss)

# grads_and_vars is a list of tuples (gradient, variable).  Do whatever you
# need to the 'gradient' part, for example cap them, etc.
capped_grads_and_vars = [(MyCapper(gv[0]), gv[1]) for gv in grads_and_vars]

# Ask the optimizer to apply the capped gradients.
opt.apply_gradients(capped_grads_and_vars)
```
But I'm getting error 

> NameError: name 'MyCapper' is not defined

**Fulltraceback:**

`grads = [(MyCapper(gv[0]), gv[1]) for gv in grads_and_vars]

NameError: name 'MyCapper' is not defined`

Pleae help.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.10.0
- Python version: 3.6.5
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): 7.2.0
- CUDA/cuDNN version: Cuda compilation tools, release 9.0, V9.0.176
- GPU model and memory: Tesla V100 


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27931,GPU build on ARM-64 failing,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04 running on Nvidia Jetson Nano board
Linux jetson-01 4.9.140-tegra #1 SMP PREEMPT Wed Mar 13 00:32:22 PDT 2019 aarch64 aarch64 aarch64 GNU/Linux

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A

- TensorFlow installed from (source or binary):
source

- TensorFlow version:
v1.13.1

- Python version:
Python 2.7.15rc1

- Installed using virtualenv? pip? conda?:
N/A

- Bazel version (if compiling from source):
```
INFO: Invocation ID: e7fbbd06-6d2d-4fee-9260-a96c2efc12e9
Build label: 0.21.0- (@non-git)
Build target: bazel-out/aarch64-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Apr 17 11:17:18 2019 (1555499838)
Build timestamp: 1555499838
Build timestamp as int: 1555499838

```
- GCC/Compiler version (if compiling from source):
gcc (Ubuntu/Linaro 7.3.0-27ubuntu1~18.04) 7.3.0

- CUDA/cuDNN version:
cuda 10, cuDNN 7

- GPU model and memory:
Tegra based GPU on Jetson Nano bord



**Describe the problem**
I am trying to build TensorFlow on ARM64 (Jetson Nano board) for use with GPU. Build is failing.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

- Enabled 6GB swap space per: https://www.jetsonhacks.com/2019/04/14/jetson-nano-use-more-memory/
- Built bazel 0.21 from source.
- Checked out v1.13.1 from TF repo
- Applied these changes to build files (diff attached below): https://github.com/tensorflow/tensorflow/issues/21852#issuecomment-477885516
- ran `./configure` saying no to most items except yes for CUDA support

```
Ran following command to start the build
nohup bazel build -c opt --jobs 1 --local_resources 5000,1.0,1.0 --verbose_failures --config=opt --config=cuda --config=noaws --config=nogcp --config=nohdfs --config=noignite --config=nokafka //tensorflow:libtensorflow.so &

```
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
[nohup.txt](https://github.com/tensorflow/tensorflow/files/3091084/nohup.txt)


[tf_jetson_nano_build.diff.txt](https://github.com/tensorflow/tensorflow/files/3092069/tf_jetson_nano_build.diff.txt)
"
27929,tf-2.0: AttributeError: module 'tensorflow' has no attribute 'optimizers',"**System information**
- OS Platform and Distribution Ubuntu 18.04 LTS
- TensorFlow installed from source or binary): source
- TensorFlow version (use command below): v2.0.0-alpha0-4-g2c2d508 2.0.0-alpha0
- Python version: 3.5.7
- Bazel version (if compiling from source): From docker image
- GCC/Compiler version (if compiling from source): From docker image
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
This documentation suggests there are 4 ways to load an optimizer:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers

However:

(tf_35) mark@science:~$ python tf_optimizers.py 
Traceback (most recent call last):
  File ""tf_optimizers.py"", line 7, in <module>
    opt4 = tf.optimizers.Adagrad
AttributeError: module 'tensorflow' has no attribute 'optimizers'

**Describe the expected behavior**

opt = tf.optimizers.Adagrad doesn't work. The first 3 methods do.

**MINIMAL code to reproduce the issue**

import tensorflow as tf
tf.__version__

opt1 = tf.compat.v2.keras.optimizers.Adagrad
opt2 = tf.compat.v2.optimizers.Adagrad
opt3 = tf.keras.optimizers.Adagrad
opt4 = tf.optimizers.Adagrad


I don't know if this is a documentation issue or a tensorflow bug.
"
27927,Under MirroredStrategy and ParameterStrategy tf.trainable_variables() doesn't return the correct wrapped variables,"Under MirroredStrategy and ParameterStrategy tf.trainable_variables() returns the normal unwrapped variables.

But according to the comments in mirrored_strategy.py and parameter_server_strategy.py, tf.trainable_variables() should return the wrapped variables.

Here tf 1.13.1 mirrored_strategy.py code segment,
```
 if not context.executing_eagerly():
    g = ops.get_default_graph()
    # If ""trainable"" is True, next_creator() will add the member variables                                                                                                                    
    # to the TRAINABLE_VARIABLES collection, so we manually remove                                                                                                                            
    # them and replace with the MirroredVariable. We can't set                                                                                                                                
    # ""trainable"" to False for next_creator() since that causes functions                                                                                                                     
    # like implicit_gradients to skip those variables.                                                                                                                                        
    if kwargs.get(""trainable"", True) or kwargs.get(""trainable"", True) is None:
      collections.append(ops.GraphKeys.TRAINABLE_VARIABLES)
      l = g.get_collection_ref(ops.GraphKeys.TRAINABLE_VARIABLES)
      for v in index.values():
        if v in l:
          l.remove(v)
    g.add_to_collections(collections, result)
```
The problem is that in kwargs past from up stream API, ""trainable"" is set but the value is None, so just ckeck

    if kwargs.get(""trainable"", True): 

is not enough, it should be

    if kwargs.get(""trainable"", True) or  kwargs.get(""trainable"", True) is None:

This bug exists in all version of tensorflow. For tf 1.13.1, here is the fix for mirred_strategy.py
```
diff -u mirrored_strategy.py-org mirrored_strategy.py
--- mirrored_strategy.py-org    2019-03-15 08:56:17.677027702 -0400
+++ mirrored_strategy.py        2019-04-17 12:21:06.339507273 -0400
@@ -256,7 +256,7 @@
     # them and replace with the MirroredVariable. We can't set
     # ""trainable"" to False for next_creator() since that causes functions
     # like implicit_gradients to skip those variables.
-    if kwargs.get(""trainable"", True):
+    if kwargs.get(""trainable"", True) or kwargs.get(""trainable"", True) is None:
       collections.append(ops.GraphKeys.TRAINABLE_VARIABLES)
       l = g.get_collection_ref(ops.GraphKeys.TRAINABLE_VARIABLES)
       for v in index.values():
```"
27926,Autodiff creates stack for loop-invariant data,"In file `control_flow_ops.py`, `GetRealValue()` implements the functionality to get the value of an operation from each forward loop iteration to be used in the backward loop.
This function checks if the operation is a constant and skips creating a stack for these. Otherwise it creates a stack and pushes the value at each loop iteration.
The issue is that `GetRealValue()` doesn't check if the operation is loop invariant, or simply, if it's computed outside the loop, and still creates the stack. I see significant memory consumption because of this.

I tried to find a way of checking if an operation is loop invariant or if it's computed outside the loop, but failed to find such a function (I'm new in TF source-code). If you could point me out to such a function, I would be happy to try to produce a patch.
Thanks!"
27924,Estimator from Keras Model fails to learn when original keras Model learns fine,"I have created a keras `Model` that learns a simple embedding from a deterministic function.  When I convert to an Estimator using ` tf.keras.estimator.model_to_estimator` learning fails to converge and the learned embeddings are incorrect.
  
I would expect that an Estimator constructed from a keras Model would have similar performance to the original keras Model.  However the keras model loss decays to zero quickly:

![image](https://user-images.githubusercontent.com/10658783/56299648-760e7a80-6102-11e9-92d0-600480fe8659.png)

But the Estimator does not learn at all:
![Screenshot from 2019-04-17 11-30-24](https://user-images.githubusercontent.com/10658783/56300844-a820dc00-6104-11e9-8fe8-fd4d7d70cbfa.png)


**Code to reproduce the issue**
```python
import numpy as np
import pandas as pd
import tensorflow as tf

# Generate Some Data
np.random.seed(1234)
n_obs = 200
n_sensors = 50
n_sources  = 5

Z = np.random.randn(n_obs, n_sources)
b = np.random.randn(n_sources, n_sensors)
y = Z.dot(b)

data = []
for t in range(n_obs):
    betas = pd.DataFrame(b).rename(lambda x: 'f_{}'.format(x)).T
    data.append(
        pd.DataFrame({'target': y[t,:], 't': t}).join(betas)
    )

data = pd.concat(data)

def build_keras_model():
""""""
A model to learn `Z` in f(b, t) = Z[t].dot(b)
""""""
    betas = tf.keras.layers.Input(shape=(n_sources,), name='betas')
    t = tf.keras.layers.Input(shape=(1,), name='t')

    sources = tf.keras.layers.Embedding(
        input_dim=n_obs,
        output_dim=n_sources,
        name='sources')(t)

    fitted = tf.keras.layers.Dot(axes=-1)([sources, betas])
    net = tf.keras.Model([betas, t], fitted)
    net.compile(optimizer='adadelta', loss='mse')
    return net

# Train Keras Model 
net = build_keras_model()
loss = []

hist = net.fit(
    x=[data.filter(like='f_'), data.t],
    y=data.target,
    batch_size=100,
    epochs=200)
loss += hist.history['loss']

#  get embedding weights and average squared different to ground truth
Z_hat_keras = net.get_layer('sources').get_weights()
print ((Z_hat_keras - Z) ** 2).mean()
# Out: 8.794945836110652e-05

# convert to a Estimator object
estimator = tf.keras.estimator.model_to_estimator(net)

# construct input functions for Estimator
def input_fn(data, batch_size, num_epochs, shuffle):
    ds = tf.data.Dataset.from_tensor_slices((
        {'betas': data.filter(like='f_'), 't': data.t}, data.target
    ))

    if shuffle:
        ds = ds.shuffle(len(data))

    return ds.repeat(num_epochs).batch(batch_size)

train_fn = lambda: input_fn(data, 100, 200, True)
eval_fn = lambda: input_fn(data, 500,1,False)

# run training on estimator
estimator.train(train_fn)
estimator.evaluate(eval_fn)
# Out: {'global_step': 20001, 'loss': 5.0131807}

#  get embedding weights and average squared different to ground truth
Z_hat_tensorflow = estimator.get_variable_value('sources/embeddings')
print ((Z_hat_tensorflow - Z) ** 2).mean()
# Out 0.928678746751657
```

**Other info / logs**
I have also posted on StackOverflow with a bit more detail:
https://stackoverflow.com/questions/55712219/are-keras-embedding-layers-getting-added-regularization-when-converted-to-tensor
"
27923,The kernel appears to have died. It will restart automatically.,"def make_generator_model():
    model = tf.keras.Sequential()
    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Reshape((7, 7, 256)))
    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size

    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 7, 7, 128)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 14, 14, 64)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 28, 28, 1)

    return model


generator = make_generator_model()

noise = tf.random.normal([1, 100])
generated_image = generator(noise, training=False)

#plt.imshow(generated_image[0, :, :, 0], cmap='gray')

==============================
tensorflow2 
cuda 10.0 
"
27921,This company RANNED !!!!!!,THIS  company  is    By  Liberals .... For  You  a will see   Why U Think    Its a Good Compay . Your.   Wrong !!!!!!!!    Learn More.......
27920,How to compare two DimensionHandle objects in shape inference function?,"I am writing some ops in C++.
My question is: how to compare two DimensionHandle objects in shape inference function?"
27918,TensorFlowLite: failed to load native library: Library tensorflowlite_jni not found; at  /system/lib64,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  downloaded .aar file from https://mvnrepository.com/artifact/org.tensorflow/tensorflow-lite/1.13.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy A5
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: NA
- GPU model and memory: NA


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior** Trying to load .tflite file on Android device. Getting error ""TensorFlowLite: failed to load native library: Library tensorflowlite_jni not found; at  /system/lib64"". It is not able to find the tensorflowlite_jni.so at /system/lib64 path. If I push the file to /system/lib64 path in device it works

**Describe the expected behavior**: It should work without the requirement of external push

**Code to reproduce the issue**: 1. Generate a .tflite file after model training on desktop
2. download .aar file from https://mvnrepository.com/artifact/org.tensorflow/tensorflow-lite/1.13.1
3. Build Android binary using downloaded .aar file and .tflite file along with Android Application trying to load .tflite file
4. Run time error while app is trying to load the .tflite file
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

04-17 17:50:00.518  4470  4470 W **System.err: TensorFlowLite: failed to load native library: Library tensorflowlite_jni not found; tried [/system/lib64/libtensorflowlite_jni.so]** 04-17 17:50:00.519  4470  4470 W System.err: TensorFlowLite: failed to load native library: Library tensorflowlite_jni not found; tried [/system/lib64/libtensorflowlite_jni.so] 04-17 17:50:00.528  8077  8077 D OpenGLRenderer: Skia GL Pipeline 04-17 17:50:00.539  5932  5959 V ContactsProvider_EventLog: contents_sample_query: [ uri = content://com.android.contacts/contacts(usr:0), pkg = com.samsung.android.bixby.service(6596), prj = [name_raw_contact_id, contact_last_updated_timestamp], sel = name_raw_contact_id <= 1 AND ( contact_last_updated_timestamp > 1555500415060 OR (contact_last_updated_timestamp = 1555500415060 AND name_raw_contact_id > 1 )), selArg = null, so = contact_last_updated_timestamp ASC LIMIT 300, cv = null, dur = 27, ret = 0, trw = false ] 04-17 17:50:00.541  8077  8077 D EmergencyMode: [EmergencyManager] android createPackageContext successful 04-17 17:50:00.543  4470  4470 E system_server: No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I) 04-17 17:50:00.544  4470  4470 E Zygote  : System zygote died with exception 04-17 17:50:00.544  4470  4470 E Zygote  : java.lang.UnsatisfiedLinkError: No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I) 04-17 17:50:00.544  4470  4470 E Zygote  :     at org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(Native Method) 04-17 17:50:00.544  4470  4470 E Zygote  :     at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:73) 04-17 17:50:00.544  4470  4470 E Zygote  :     at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:54) 04-17 17:50:00.544  4470  4470 E Zygote  :     at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:114) 04-17 17:50:00.544  4470  4470 E Zygote  :     at android.hardware.VirtualProximityService.initTflite(VirtualProximityService.java:184) 04-17 17:50:00.544  4470  4470 E Zygote  :     at android.hardware.VirtualProximityService.onCreate(VirtualProximityService.java:99) 04-17 17:50:00.544  4470  4470 E Zygote  :     at android.app.ActivityThread.handleCreateService(ActivityThread.java:3757) 04-17 17:50:00.544  4470  4470 E Zygote  :     at android.app.ActivityThread.access$1400(ActivityThread.java:237) 04-17 17:50:00.544  4470  4470 E Zygote  :     at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1808) 04-17 17:50:00.544  4470  4470 E Zygote  :     at android.os.Handler.dispatchMessage(Handler.java:106) 04-17 17:50:00.544  4470  4470 E Zygote  :     at android.os.Looper.loop(Looper.java:214) 04-17 17:50:00.544  4470  4470 E Zygote  :     at com.android.server.SystemServer.run(SystemServer.java:802) 04-17 17:50:00.544  4470  4470 E Zygote  :     at com.android.server.SystemServer.main(SystemServer.java:624) 04-17 17:50:00.544  44
04-17 17:50:00.544  4470  4470 E Zygote  :     at com.android.server.SystemServer.main(SystemServer.java:624) 04-17 17:50:00.544  4470  4470 E Zygote  :     at java.lang.reflect.Method.invoke(Native Method) 04-17 17:50:00.544  4470  4470 E Zygote  :     at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:493) 04-17 17:50:00.544  4470  4470 E Zygote  :     at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:945) 04-17 17:50:00.545  4470  4470 D AndroidRuntime: Shutting down VM
"
27917,are the same tensorflow serving .pb file  with frozen graph .pb file,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
27916,"[CI] Ubuntu CC  is still ""Expected  Waiting for status to be reported"" after 14 days.","It is strange. Let's see the operation status of Tensorflow CI on  PR #27466.  The work status of **Ubuntu CC**  is still ""**Expected  Waiting for status to be reported**"" even though 14 days is passed after the contributor submitted PR #27466. Why does not `Ubuntu CC`  report the result either PASS or FAILURE? Why Ubuntu CC is still running for 14 days?

* https://github.com/tensorflow/tensorflow/pull/27466
![image](https://user-images.githubusercontent.com/82404/56286270-50aa5c80-6154-11e9-993c-e2c6188d823b.png)


Above all, lots of PRs do not have got the execution result of Tensorflow CI.  Please see the red circles below. Why these situations have repeatedly generated?

![image](https://user-images.githubusercontent.com/82404/56286707-5fddda00-6155-11e9-846a-177ccaa3e9ee.png)
"
27915,[feature request]check whether a data iterator is initialized,"**System information**
- TensorFlow version (you are using): r1.13
- Are you willing to contribute it (Yes/No): yes

**Describe the feature and the current behavior/state.**
When a dataset iterator is shared among sessions by ""shared_name"", non-chief sessions need to wait for its initialization on the chief session. And what is the proper way to check the state of iterators currently
Ideally, iterators should be initialized by init_ops of monitoredsession, and checked in model_ready function.

**Will this change the current api? How?**
No changes on current api. add one new api like ""is_initialized"".

**Who will benefit with this feature?**
all who use ""shared_name"" feature."
27913,ptr bug tensorflow,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>
wrong ptr 
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):1.13
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:7.0/9.0
- GPU model and memory: 1080ti


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
project deeplab v3 +.When I open the memory, the network input is a pointer to mat, not a pointer to the raw file I opened, so the input is wrong. And remove the pointer input of mat, although the input pointer is correct,  the content read by tensor is wrong, resulting in the final output map is all 2 (label).This may be related to defining the location of the tensor.
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
`    Mat img=imread(image_path);
    cvtColor(img, img, CV_BGR2RGB);
    unsigned char * q=img.data;
         FILE *out; 
         out = fopen(""./a011.raw"", ""rb"");
    unsigned char *buf_tempx;
    Tensor resized_tensor(DT_UINT8, TensorShape({1,input_height,input_width,3}));
    buf_tempx = (unsigned char *)malloc(input_width*input_height*3);
        fread(buf_tempx, 1, input_width*input_height*3, out);
        fclose(out);
    Point_to_Tensor(buf_tempx,&resized_tensor,input_height,input_width);
    struct  timeval start;
    struct  timeval end;
    unsigned  long diff;
    gettimeofday(&start,NULL);
    cout<<endl<<""<-------------Running the model with test_image--------------->""<<endl;
    vector<tensorflow::Tensor> outputs;
    string output_node = output_tensor_name;
    Status status_run = session->Run({{input_tensor_name, resized_tensor}}, {output_node}, {}, &outputs);
`"
27912,There is still bug with function tf.keras.layers.Conv3DTranspose when its input shape is None,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7




You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Hi, here's my code:

input_img = keras.layers.Input( shape=( None, len(params2), len(params2), 1 ) ) # adapt this if using `channels_first` image data format

conv1 = keras.layers.Conv3D(filters=32, kernel_size=(1, 3, 3), strides=(1, 1, 1), activation='selu', padding='same', data_format='channels_last', name='conv1')(input_img)
conv2 = keras.layers.Conv3D(filters=64, kernel_size=(1, 3, 3), strides=(1, 2, 2), activation='selu', padding='same', data_format='channels_last', name='conv2')(conv1)
conv3 = keras.layers.Conv3D(filters=128,kernel_size=(1, 3, 3), strides=(1, 2, 2), activation='selu', padding='same', data_format='channels_last', name='conv3')(conv2)
conv4 = keras.layers.Conv3D(filters=256,kernel_size=(1, 3, 3), strides=(1, 2, 2), activation='selu', padding='same', data_format='channels_last', name='conv4')(conv3)

convlstm1 = keras.layers.ConvLSTM2D(filters=32, return_sequences=True, kernel_size=(3, 3), strides=(1, 1), activation='selu', padding='same', data_format='channels_last', name='convlstm1')(conv1)
convlstm2 = keras.layers.ConvLSTM2D(filters=64, return_sequences=True, kernel_size=(3, 3), strides=(1, 1), activation='selu', padding='same', data_format='channels_last', name='convlstm2')(conv2)
convlstm3 = keras.layers.ConvLSTM2D(filters=128,return_sequences=True, kernel_size=(3, 3), strides=(1, 1), activation='selu', padding='same', data_format='channels_last', name='convlstm3')(conv3)
convlstm4 = keras.layers.ConvLSTM2D(filters=256,return_sequences=True, kernel_size=(3, 3), strides=(1, 1), activation='selu', padding='same', data_format='channels_last', name='convlstm4')(conv4)

deconv4 = keras.layers.Conv3DTranspose(filters=128, kernel_size=(1, 2, 2), strides=(1, 2, 2), activation='selu', padding='valid', output_padding=(0, -1, -1), data_format='channels_last', name='deconv4')(convlstm4)
concat4 = keras.layers.Concatenate(axis=4, name='concat4')([convlstm3, deconv4])
deconv3 = keras.layers.Conv3DTranspose(filters=64 , kernel_size=(1, 2, 2), strides=(1, 2, 2), activation='selu', padding='valid', output_padding=(0, -1, -1), data_format='channels_last', name='deconv3')(concat4)
concat3 = keras.layers.Concatenate(axis=4, name='concat3')([convlstm2, deconv3])
deconv2 = keras.layers.Conv3DTranspose(filters=32 , kernel_size=(1, 3, 3), strides=(1, 2, 2), activation='selu', padding='same' , data_format='channels_last', name='deconv2')(concat3)
concat2 = keras.layers.Concatenate(axis=4, name='concat2')([convlstm1, deconv2])
deconv1 = keras.layers.Conv3DTranspose(filters=1  , kernel_size=(1, 3, 3), strides=(1, 1, 1), activation='selu', padding='same' , data_format='channels_last', name='deconv1')(concat2)

autoencoder = keras.models.Model(input_img, deconv1)
autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')
autoencoder.summary()

and here's the error message:

TypeError                                 Traceback (most recent call last)
<ipython-input-150-83622ca7a77b> in <module>
     28     return autoencoder
     29 
---> 30 autoencoder1(1700)

<ipython-input-150-83622ca7a77b> in autoencoder1(file_length)
     14     convlstm4 = keras.layers.ConvLSTM2D(filters=256,return_sequences=True, kernel_size=(3, 3), strides=(1, 1), activation='selu', padding='same', data_format='channels_last', name='convlstm4')(conv4)
     15 
---> 16     deconv4 = keras.layers.Conv3DTranspose(filters=128, kernel_size=(1, 2, 2), strides=(1, 2, 2), activation='selu', padding='valid', output_padding=(0, -1, -1), data_format='channels_last', name='deconv4')(convlstm4)
     17     concat4 = keras.layers.Concatenate(axis=4, name='concat4')([convlstm3, deconv4])
     18     deconv3 = keras.layers.Conv3DTranspose(filters=64 , kernel_size=(1, 2, 2), strides=(1, 2, 2), activation='selu', padding='valid', output_padding=(0, -1, -1), data_format='channels_last', name='deconv3')(concat4)

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    552             # In graph mode, failure to build the layer's graph
    553             # implies a user-side bug. We don't catch exceptions.
--> 554             outputs = self.call(inputs, *args, **kwargs)
    555           else:
    556             try:

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py in call(self, inputs)
   1138       else:
   1139         outputs_4d = array_ops.reshape(outputs, [
-> 1140             outputs_shape[0], outputs_shape[1] * outputs_shape[2],
   1141             outputs_shape[3], outputs_shape[4]
   1142         ])

TypeError: unsupported operand type(s) for *: 'NoneType' and 'int'

You just said that this problem was solved. But I'm really not sure why I still have this problem today on April 2019.
"
27910,Want to optimize Tensorflow CUDA kernels,"I want to go through Tensorflow's CUDA kernels and optimize them.  It seems like all the CUDA kernels are defined in `tensorflow/core/kernels` and `tensorflow/contrib` with some helper functions in `tensorflow/core/util`. 

1. Is it okay for me to optimize kernels in `tensorflow/core/kernels` and send pull request?
2. now that tensorflow is moving to 2.0, are the CUDA kernels going to also change so dramatically that any improvements to them now could be rendered useless?
3. what are some conventions or practices when writing CUDA kernels? I have read the [contrib.md](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md), but there was nothing special on CUDA kernels ( ex. "" Do not change kernel launch configurations "" or "" must support devices of compute capability >= 3.0 "" )
4. must I use `CudaGridRangeX` over regular for loops?
5. Is there way to test and profile each .cu file individually, instead of having to rebuild the entire tensorflow or even having to test all kernels?"
27909,ModelCheckpoint callback error,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Anaconda
- TensorFlow version (use command below): 1.13.1
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 7.3.4
- GPU model and memory: GTX 2080 TI

**Describe the current behavior**
Using tf.keras, when I fit the model with train dataset and validation dataset created from the tf.dataset, and use ModelCheckpoint with default policy, an error showed up. The error seemed to happen because the callbacks tried to save the model with the same name twice, one at the end of each training epoch, one at the end of each validation epoch twice(using validation dataset). This should not happen because I think the source code already set overwrite = true, but it still happened.

The error information:

```
Traceback (most recent call last):
  File ""C:/Users/nones/iCloudDrive/Courses/DL/Ass8-CNN-MNIST/eager_tf_keras.py"", line 180, in <module>
    fit_model_and_evaluate(model)
  File ""C:/Users/nones/iCloudDrive/Courses/DL/Ass8-CNN-MNIST/eager_tf_keras.py"", line 172, in fit_model_and_evaluate
    validation_steps=40, verbose=verbose)
  File ""C:\Users\nones\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 851, in fit
    initial_epoch=initial_epoch)
  File ""C:\Users\nones\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\engine\training_generator.py"", line 232, in model_iteration
    callbacks.on_epoch_end(epoch, epoch_logs, mode=mode)
  File ""C:\Users\nones\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 251, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""C:\Users\nones\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 624, in on_epoch_end
    self.model.save(filepath, overwrite=True)
  File ""C:\Users\nones\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\engine\network.py"", line 1334, in save
    save_model(self, filepath, overwrite, include_optimizer)
  File ""C:\Users\nones\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\engine\saving.py"", line 152, in save_model
    name, val.shape, dtype=val.dtype)
  File ""C:\Users\nones\Anaconda3\envs\tensorflow\lib\site-packages\h5py\_hl\group.py"", line 119, in create_dataset
    self[name] = dset
  File ""C:\Users\nones\Anaconda3\envs\tensorflow\lib\site-packages\h5py\_hl\group.py"", line 287, in __setitem__
    h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl)
  File ""h5py\_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper
  File ""h5py\_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper
  File ""h5py\h5o.pyx"", line 202, in h5py.h5o.link
RuntimeError: Unable to create link (name already exists)

```

**Describe the expected behavior**
At least the error should not happen when saving model with the same name twice. Better if we can explicitly control when a model is saved.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import os

import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.model_selection import train_test_split

tf.enable_eager_execution()

# Mnist dataset
IMAGE_ROW, IMAGE_COLS = 28, 28
NUM_CLASSES = 10
BATCH_SIZE = 32

temp_dir = './temp'


def get_input_datasets(use_bfloat16=False):
    """"""Creates train and test dataset objects for mnist dataset.
    Args:
      use_bfloat16: Boolean, to determine if input should be cast to bfloat16
    Returns:
      Train dataset, test dataset and input shape, and class names.
    """"""

    cast_dtype = tf.bfloat16 if use_bfloat16 else tf.float32
    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

    # the data, split between train and test sets
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

    if tf.keras.backend.image_data_format() == 'channels_first':
        x_train = x_train.reshape(x_train.shape[0], 1, IMAGE_ROW, IMAGE_COLS)
        x_test = x_test.reshape(x_test.shape[0], 1, IMAGE_ROW, IMAGE_COLS)
        input_shape = (1, IMAGE_ROW, IMAGE_COLS)
    else:
        x_train = x_train.reshape(x_train.shape[0], IMAGE_ROW, IMAGE_COLS, 1)
        x_test = x_test.reshape(x_test.shape[0], IMAGE_ROW, IMAGE_COLS, 1)
        input_shape = (IMAGE_ROW, IMAGE_COLS, 1)

    # Preprocess
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    x_train /= 255
    x_test /= 255

    y_train = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)
    y_test = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)

    # build dataset
    # ds_train = tf.data.Dataset.from_tensor_slices({'images': x_train, 'labels': y_train})
    # ds_test = tf.data.Dataset.from_tensor_slices({'images': x_test, 'labels': y_test})

    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1)

    ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))
    ds_valid = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))
    ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test))

    # Preprocess dataset
    ds_train = preprocess_dataset(ds_train, cast_dtype)
    ds_valid = preprocess_dataset(ds_valid, cast_dtype)
    ds_test = preprocess_dataset(ds_test, cast_dtype)

    return ds_train, ds_valid, ds_test, input_shape, class_names


def preprocess_dataset(dataset, cast_dtype):
    dataset = dataset.map(lambda x, y: (tf.cast(x, cast_dtype), y))
    dataset = dataset.shuffle(buffer_size=6000)
    dataset = dataset.repeat()
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.prefetch(buffer_size=1000)
    return dataset


def plot_image(image):
    plt.figure()
    plt.imshow(image)
    plt.colorbar()
    plt.grid(False)


def test_ds(dataset, fname):
    save_dir = os.path.join(temp_dir, ""%s.png"" % fname)
    os.makedirs(temp_dir, exist_ok=True)
    plt.figure()
    for image, label in dataset.take(1):
        for index in range(4):
            plt.subplot(2, 2, index + 1)
            plt.imshow(image[index].numpy().reshape(28, 28))
            plt.xlabel(label[index].numpy())
            plt.grid(False)
    plt.savefig(save_dir, bbox_inches=""tight"")
    plt.clf()

def get_optimizer(optimizer_choice='SGD', learning_rate=0.01, momentum=0.9):
    return {
        'SGD':tf.keras.optimizers.SGD(lr=learning_rate, momentum=momentum),
        'Adam': tf.keras.optimizers.Adam(lr=learning_rate)
    }.get(optimizer_choice, 'SGD')
    #
    # return {
    #     'SGD':tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum),
    #     'Adam': tf.train.AdamOptimizer(learning_rate=learning_rate)
    # }.get(optimizer_choice, 'SGD')


def create_model(input_shapes):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=input_shapes))
    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(tf.keras.layers.Dropout(0.25))
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(128, activation='relu'))
    model.add(tf.keras.layers.Dropout(0.5))
    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'))
    return model


def create_model_functional(input_shapes, kernel_size=(3, 3), dropout_rate=0, l2_regularizer=0.1):
    input_tensor = tf.keras.Input(shape=input_shapes)
    layer = tf.keras.layers.Conv2D(filters=64, kernel_size=kernel_size, activation='relu')(input_tensor)
    layer = tf.keras.layers.Conv2D(filters=64, kernel_size=kernel_size, activation='relu')(layer)
    layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(layer)
    layer = tf.keras.layers.Dropout(rate=dropout_rate)(layer)
    layer = tf.keras.layers.Flatten()(layer)
    layer = tf.keras.layers.Dense(128, activation='relu')(layer)
    layer = tf.keras.layers.Dropout(rate=dropout_rate)(layer)
    predictions = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax',
                                        kernel_regularizer=tf.keras.regularizers.l2(l=l2_regularizer))(layer)
    model = tf.keras.models.Model(inputs=input_tensor, outputs=predictions)
    return model

def fit_model_and_evaluate(model, optimizer_choice='SGD', learning_rate=0.01, verbose=1):
    optimizer = get_optimizer(optimizer_choice=optimizer_choice, learning_rate=learning_rate)
    os.makedirs('graph', exist_ok=True)
    os.makedirs('checkpoint', exist_ok=True)
    file_path = 'checkpoint/model.{epoch:02d}-{val_loss:.4f}-{val_acc:.4f}.hdf5'
    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(file_path)
    # log_dir = os.path.join()
    board = tf.keras.callbacks.TensorBoard(log_dir='./graph', histogram_freq=0,
                                           write_graph=True, write_images=True)
    model.compile(loss=tf.keras.losses.categorical_crossentropy,
                  optimizer=optimizer,
                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
    model.fit(x=ds_train, validation_data=ds_valid, epochs=20, steps_per_epoch=468,
              callbacks=[board, model_checkpoint],
              validation_steps=40, verbose=verbose)
    score = model.evaluate(ds_test, steps=10, verbose=0)
    print('Test loss:', score[0])
    print('Test accuracy:', score[1])

if __name__ == '__main__':
    ds_train, ds_valid, ds_test, input_shapes, class_names = get_input_datasets()
    model = create_model_functional(input_shapes)
    fit_model_and_evaluate(model)
    # test_ds(ds_train, 'train')
    # test_ds(ds_test, 'test')
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27906,"could not create a dilated convolution forward descriptor, in file tensorflow/core/kernels/mkl_conv_ops.cc:1111","When I tried to run some yolov3 codes, I always got the following error:
`Traceback (most recent call last):

  File ""<ipython-input-16-cd2eeb60d39b>"", line 1, in <module>
    runfile('C:/Users/10137/Desktop/learn/python/yolov3_keras_qwe/train.py', wdir='C:/Users/10137/Desktop/learn/python/yolov3_keras_qwe')

  File ""C:\Users\10137\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 786, in runfile
    execfile(filename, namespace)

  File ""C:\Users\10137\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/10137/Desktop/learn/python/yolov3_keras_qwe/train.py"", line 190, in <module>
    _main()

  File ""C:/Users/10137/Desktop/learn/python/yolov3_keras_qwe/train.py"", line 65, in _main
    callbacks=[logging, checkpoint])

  File ""C:\Users\10137\Anaconda3\lib\site-packages\keras\legacy\interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)

  File ""C:\Users\10137\Anaconda3\lib\site-packages\keras\engine\training.py"", line 1418, in fit_generator
    initial_epoch=initial_epoch)

  File ""C:\Users\10137\Anaconda3\lib\site-packages\keras\engine\training_generator.py"", line 217, in fit_generator
    class_weight=class_weight)

  File ""C:\Users\10137\Anaconda3\lib\site-packages\keras\engine\training.py"", line 1217, in train_on_batch
    outputs = self.train_function(ins)

  File ""C:\Users\10137\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 2715, in __call__
    return self._call(inputs)

  File ""C:\Users\10137\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 2675, in _call
    fetched = self._callable_fn(*array_vals)

  File ""C:\Users\10137\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1439, in __call__
    run_metadata_ptr)

  File ""C:\Users\10137\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))

AbortedError: Operation received an exception:Status: 3, message: could not create a dilated convolution forward descriptor, in file tensorflow/core/kernels/mkl_conv_ops.cc:1111
	 [[{{node conv2d_2/convolution}}]]`
"
27904,saved_model.load gives KeyError in 2.0.0 Same model loads fine in 1.13,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.5


**Describe the current behavior**
I am trying to load a model saved in 1.x in 2.0.0. Here is the model: https://storage.googleapis.com/marco-168219-model/savedmodel.zip

This is the error I am getting: KeyError: <tf.Tensor 'map/while/Switch_1:0' shape=() dtype=float32>

The same model loads fine in 1.13

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

model = tf.saved_model.load(model_dir)


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27903,Pylint checks from ci_sanity.sh `--incremental`,"Currently we kind of support `--incremental` to run incremental sanity checks. In reality, we still run it for all files:

https://github.com/tensorflow/tensorflow/blob/0e4117f67192a140466ba66513035e24bbd1a474/tensorflow/tools/ci_build/ci_sanity.sh#L137

```
# For incremental builds, we still check all Python files in cases there
# are function signature changes that affect unchanged Python files.
```

Would it make sense to just clean it up or what is the current idea? 

I started to add an incremental check for since a specific commit, so one can run checks for changes across multiple commits.

"
27902,ResizeBilinear FP16 CUDA Kernel Support,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10/7.4.1
- GPU model and memory: NVIDIA Titan V


**Describe the current behavior**
The resize bilinear CUDA kernel omits FP16 support. The implementation accepts a templated tensor type, but then internally performs a hard-cast to a float during filtering, which is then written to a float tensor. Couldn't the templated type be propagated to the output as in resize nearest neighbor?

This causes a significant and anomalous perf drop when converting from FP32 to FP16 in networks that utilize this operation.


**Describe the expected behavior**
An FP16 variant would be implemented alongside the FP32 CUDA implementation, with performance scaling appropriately between the two.
"
27900,pydoc.ErrorDuringImport: problem in tensorflow.keras - ImportError: cannot import name 'Layer',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: 1.13.1
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: Conda
- Bazel version (if compiling from source): Nope
- GCC/Compiler version (if compiling from source): Nope
- CUDA/cuDNN version: CUDA Version 10.1.105 /7

- GPU model and memory:  GeForce GTX 1080 Ti and  11178 MiB


**Describe the problem**
I get the error when I was importing keras.


**Provide the exact sequence of commands / steps that you executed before running into the problem**
    `from tensorflow.python.estimator.keras import Layer`


**Any other info / logs**
`Traceback (most recent call last):
  File ""/home/bjayakumar/.conda/envs/py36/lib/python3.6/pydoc.py"", line 343, in safeimport
    module = __import__(path)
  File ""/home/bjayakumar/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/keras/__init__.py"", line 20, in <module>
    from . import layers
  File ""/home/bjayakumar/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/keras/layers/__init__.py"", line 8, in <module>
    from tensorflow.python.estimator.keras import Layer
ImportError: cannot import name 'Layer'`


This solution is not working for me.
**https://github.com/tensorflow/tensorflow/issues/24847**"
27899,Can't easily write custom C++ ops which update resource variables,"**System information**
- TensorFlow version (you are using): HEAD
- Are you willing to contribute it (Yes/No): No (moving headers is best done inside Google)

**Describe the feature and the current behavior/state.**

We're trying to update optimizers like the ones at https://github.com/openai/blocksparse/blob/master/blocksparse/optimize.py to support parameters stored as resource variables. Looking at the TensorFlow source code for the built-in optimizer ops, it seems necessary to have relatively complicated locking and copy-on-write logic to handle the different states a resource variable could be in. TensorFlow uses helper functions in [`core/kernels/training_op_helpers.h`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_op_helpers.h) to do this, calling `MaybeLockVariableInputMutexesInOrder` and `GetInputTensorFromVariable`.

Unfortunately, `training_op_helpers.h` is not in the public TensorFlow headers, so we can't just `#include` it. Worse, it depends on internal functions that `libtensorflow_framework.so` doesn't even export (notably, `tensorflow::functor::DenseUpdate` for the copy-on-write), so copying the headers also doesn't suffice.

Can we move this file and the relevant dependencies into `framework` or a similarly public location?

**Will this change the current api? How?**

Yes.  Those helpers would become public.

**Who will benefit with this feature?**

People writing new C++ optimizers.

**Any Other info.**

Related StackOverflow question (cc @daniel-ziegler): https://stackoverflow.com/questions/55661246/how-to-write-custom-c-ops-which-update-resource-variables"
27897,Spurious deprecation warnings,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): tf-nightly-2.0-preview-20190416-py2.7
- TensorFlow version (use command below): 2.0.0-dev20190416
- Python version: 2.7.16rc1
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

When using `keras.callbacks.TensorBoard` or `keras.models.Sequential`, a
deprecation warning is emitted instructing the user to use the same
symbol that theyre already using:

>   - The name keras.callbacks.TensorBoard is deprecated. Please use keras.callbacks.TensorBoard instead.
>   - The name keras.models.Sequential is deprecated. Please use keras.models.Sequential instead.

**Describe the expected behavior**

No deprecation warning should be emitted.

**Code to reproduce the issue**

```
$ cat /tmp/repro.py; echo
import tensorflow as tf
print(tf.__version__)
tf.keras.callbacks.TensorBoard
tf.keras.models.Sequential

$ python /tmp/repro.py
2.0.0-dev20190416
WARNING: Logging before flag parsing goes to stderr.
W0416 11:29:00.042920 139891507525376 deprecation_wrapper.py:76] From /tmp/repro.py:3: The name keras.callbacks.TensorBoard is deprecated. Please use keras.callbacks.TensorBoard instead.

W0416 11:29:00.043119 139891507525376 deprecation_wrapper.py:76] From /tmp/repro.py:4: The name keras.models.Sequential is deprecated. Please use keras.models.Sequential instead.

```
"
27896,Changing the tf_random_seed (RunConfig) doesn't change results/parameters for LinearClassifier,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): N/A
- TensorFlow version (use command below): v1.13.1-0-g6612da8951
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Using different `tf_random_seed` values for `tf.estimator.RunConfig` produces the same model performance and parameters for `tf.estimator.LinearClassifier`.

**Describe the expected behavior**
Different seeds to produce different performance/parameters, specially if the dataset is small or if the optimizer only takes a step for a small batch size.

**Code to reproduce the issue**
```python
import pandas as pd
import numpy as np

from sklearn.datasets import make_classification
from sklearn.metrics import roc_auc_score

import tensorflow as tf
from tensorflow.logging import set_verbosity
from tensorflow.estimator.inputs import pandas_input_fn
from tensorflow.estimator import LinearClassifier
from tensorflow.feature_column import numeric_column


X, y = make_classification(n_samples=50000, n_classes=2, 
                           n_clusters_per_class=4, class_sep=1.0, 
                           random_state=10, weights=[0.8, 0.2], 
                           n_features=10, n_informative=5)

X = pd.DataFrame(X, columns=[""f{}"".format(i) for i in range(10)])
y = pd.Series(y)

input_fn = pandas_input_fn(X, y=y, batch_size=5, num_epochs=1, shuffle=False)

feature_columns = [numeric_column(key=c) for c in X.columns]

classifier = LinearClassifier(feature_columns, config=tf.estimator.RunConfig(tf_random_seed=10))

classifier.train(input_fn, steps=1)

print('\n\n\n')

for c in X.columns:
    print(c, classifier.get_variable_value(""linear/linear_model/{}/weights"".format(c))[0][0])

feature_columns = [numeric_column(key=c) for c in X.columns]

classifier = LinearClassifier(feature_columns, config=tf.estimator.RunConfig(tf_random_seed=31231))

classifier.train(input_fn, steps=1)

print('\n\n\n')

for c in X.columns:
    print(c, classifier.get_variable_value(""linear/linear_model/{}/weights"".format(c))[0][0])
```

**Other info / logs**
```
f0 -0.1945913
f1 -0.19702944
f2 0.1967695
f3 0.19892408
f4 -0.18479808
f5 -0.19904605
f6 0.1991331
f7 -0.19970222
f8 0.186639
f9 -0.1857145




f0 -0.1945913
f1 -0.19702944
f2 0.1967695
f3 0.19892408
f4 -0.18479808
f5 -0.19904605
f6 0.1991331
f7 -0.19970222
f8 0.186639
f9 -0.1857145
```"
27895,"[TF 2.0] documentation in guides for feature columns, in particular `numeric_column`","
**System information**
- TensorFlow version: 2.0.0-alpha0
- Doc Link: https://www.tensorflow.org/alpha

Guides/examples that use feature columns follow a bad pattern for numeric features. In particular, the examples in the guides always create a list of scalar `numeric_column`s instead of one `numeric_column` with all numeric features. This results in really bad performance when training. See [this thread](https://groups.google.com/a/tensorflow.org/d/msg/discuss/Vt0JGKF_Bno/S5XXmdaoCQAJ).
Locations in the docs (there may be others I missed): 
* https://www.tensorflow.org/alpha/tutorials/estimators/linear#base_feature_columns
* https://www.tensorflow.org/guide/estimators#structure_of_a_pre-made_estimators_program
* https://www.tensorflow.org/alpha/tutorials/keras/feature_columns#choose_which_columns_to_use
* https://www.tensorflow.org/alpha/tutorials/estimators/boosted_trees#create_feature_columns_and_input_functions
* https://www.tensorflow.org/alpha/tutorials/estimators/boosted_trees_model_understanding#create_feature_columns_input_fn_and_the_train_the_estimator


The '[Feature Columns guide](https://www.tensorflow.org/guide/feature_columns)' from TF 1 or anything analogous is not available in the [TF 2 docs](https://www.tensorflow.org/alpha). That guide does mention the shape parameter, but does not discuss performance at all, and also describes creating a separate `numeric_column` for each feature in the Iris dataset in [this section](https://www.tensorflow.org/guide/feature_columns#numeric_column)."
27894,TypeError: The `train_input_config` must be a input_reader_pb2.InputReader.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
linux ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
n\a
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
v1.12.0-10232-g9a43dfe 1.13.1
- Python version:
Python 3.7.1
- Bazel version (if compiling from source):
n/a
- GCC/Compiler version (if compiling from source):
n/a
- CUDA/cuDNN version:
n/a
- GPU model and memory:
n/a

**Describe the current behavior**
I've been following examples located here:
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_pets.md

Instead of running it in the Google cloud, I wanted to run it locally. I will attach training configs files below. So when i run this command:
`object_detection/model_main.py --pipeline_config_path=/home/konsof01/work/models/research/pipiline_config.proto
--model_dir=/home/konsof01/tmp
--alsologtostderr`

I get the following error:

> Traceback (most recent call last):
>   File ""object_detection/model_main.py"", line 109, in <module>
>     tf.app.run()
>   File ""/home/konsof01/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
>     _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
>   File ""/home/konsof01/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 300, in run
>     _run_main(main, args)
>   File ""/home/konsof01/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 251, in _run_main
>     sys.exit(main(argv))
>   File ""object_detection/model_main.py"", line 105, in main
>     tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
>   File ""/home/konsof01/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py"", line 471, in train_and_evaluate
>     return executor.run()
>   File ""/home/konsof01/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py"", line 611, in run
>     return self.run_local()
>   File ""/home/konsof01/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py"", line 712, in run_local
>     saving_listeners=saving_listeners)
>   File ""/home/konsof01/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 358, in train
>     loss = self._train_model(input_fn, hooks, saving_listeners)
>   File ""/home/konsof01/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1124, in _train_model
>     return self._train_model_default(input_fn, hooks, saving_listeners)
>   File ""/home/konsof01/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1151, in _train_model_default
>     input_fn, model_fn_lib.ModeKeys.TRAIN))
>   File ""/home/konsof01/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 992, in _get_features_and_labels_from_input_fn
>     self._call_input_fn(input_fn, mode))
>   File ""/home/konsof01/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1079, in _call_input_fn
>     return input_fn(**kwargs)
>   File ""/home/konsof01/work/models/research/object_detection/inputs.py"", line 486, in _train_input_fn
>     raise TypeError('The `train_input_config` must be a '
> TypeError: The `train_input_config` must be a input_reader_pb2.InputReader.

The problem occurs in this piece of code in `inputs.py`:

`
    if not isinstance(train_input_config, input_reader_pb2.InputReader):
      raise TypeError('The `train_input_config` must be a '
                      'input_reader_pb2.InputReader.')
`
However:

> model_pb2.DetectionModel
> <class 'model_pb2.DetectionModel'>
> type(model_config)
> <class 'model_pb2.DetectionModel'>
> 

but `isinstance(train_input_config, input_reader_pb2.InputReader) is False`
Pipeline config file: 
[pipiline_config.proto.zip](https://github.com/tensorflow/tensorflow/files/3085417/pipiline_config.proto.zip)



Why is that?
Thank you


**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27893,Transforming date values,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.13.1
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
Hi.

I have a use case where I want to use date features as input values for a predictive model. I need to transform the date features to be useful.
For example, I need to know the difference between two dates (for example, just the difference in days between 01-04-2019 and 16-04-2019, but the dates can also be months or years apart).
Or just getting the day of the month, the month itself or the year (i.e. for 16-04-2019, getting 16, 4 and 2019 as seperate values).

My question is if it is possible to do this within TFX and if not, is this a feature that is coming up?
It would be important for my use case because the transform needs to be done in the graph format so that I can serve the model with the transformations inside the pipeline.
Otherwise I would need to add something that can do this for me outside of TFX.

Thanks in advance!

Martijn"
27892,Crash on TF 1.13 when custom `RNNCell which has `Template`. ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): v1.13.0-rc2-5-g6612da8 / 1.13.1
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1 / 7.5.0
- GPU model and memory: GTX 1080 / 8G


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
A code below works well on Python 3.6 with TF 1.12. On Python 3.7 with TF 1.13, however, the code crashes.

**Describe the expected behavior**
Do not crash

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
import tensorflow as tf


class Cell(tf.nn.rnn_cell.RNNCell):
  
  def __init__(self, state_size, reuse=None):
    super(Cell, self).__init__(_reuse=reuse)
    self.__state_size = state_size
    self.__encoder = tf.make_template(""encoder"", self.encoder)
  
  @property
  def state_size(self):
    return self.__state_size
  
  @property
  def output_size(self):
    return self.state_size

  def zero_state(self, batch_size, dtype):
    return tf.zeros([batch_size, self.state_size])

  def encoder(self, prev_state, obs):
    inputs = tf.concat([prev_state, obs], -1)
    return tf.layers.dense(inputs, self.state_size, None)

  def call(self, inputs, prev_state):
    state = self.__encoder(prev_state, inputs)
    return state, state


inputs = tf.placeholder(tf.float32, [32, 2, 20])

rnn_cell = Cell(20)
outputs, state = tf.nn.dynamic_rnn(rnn_cell, inputs, dtype=tf.float32)
```

Traceback

```
WARNING:tensorflow:From test.py:33: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:From [PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From test.py:24: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
Traceback (most recent call last):
  File ""test.py"", line 33, in <module>
    outputs, state = tf.nn.dynamic_rnn(rnn_cell, inputs, dtype=tf.float32)
  File ""[PYTHONDIRECTORY]/site-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""[PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/rnn.py"", line 671, in dynamic_rnn
    dtype=dtype)
  File ""[PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/rnn.py"", line 879, in _dynamic_rnn_loop
    swap_memory=swap_memory)
  File ""[PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3556, in while_loop
    return_same_structure)
  File ""[PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3087, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""[PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3022, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""[PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3525, in <lambda>
    body = lambda i, lv: (i + 1, orig_body(*lv))
  File ""[PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/rnn.py"", line 847, in _time_step
    (output, new_state) = call_cell()
  File ""[PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/rnn.py"", line 833, in <lambda>
    call_cell = lambda: cell(input_t, state)
  File ""[PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 234, in __call__
    return super(RNNCell, self).__call__(inputs, state)
  File ""[PYTHONDIRECTORY]/site-packages/tensorflow/python/layers/base.py"", line 534, in __call__
    _add_elements_to_collection(self.updates, ops.GraphKeys.UPDATE_OPS)
  File ""[PYTHONDIRECTORY]/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 653, in updates
    return self._updates + self._gather_children_attribute('updates')
  File ""[PYTHONDIRECTORY]/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1647, in _gather_children_attribute
    getattr(layer, attribute) for layer in self._layers))
  File ""[PYTHONDIRECTORY]/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1647, in <genexpr>
    getattr(layer, attribute) for layer in self._layers))
AttributeError: 'Template' object has no attribute 'updates'
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

"
27890,DLL load failed while importing tensorflow cpu 2.0,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 x64
- TensorFlow installed from (source or binary): 
cmd cd C:\Users\Admin\AppData\Local\Programs\Python\Python37\Scripts
Then  pip install tensorflow==2.0.0-alpha0 
Np here
- Python version: 3.7.3

**Describe the current behavior**
I have an error while import tensorflow (cf screen)

**Describe the expected behavior**
tensorflow should import properly

**Code to reproduce the issue**
I install tensorflow as I tell and I try :
import tensorflow

**Other info / logs**
I tried w/ conda, same thing
![clean](https://user-images.githubusercontent.com/30508173/56212750-f4174680-605a-11e9-9b89-a622371760e5.png)

"
27889,Error: Can't build Tensorflow Lite for Raspberry Pi 3b+.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**Raspbian GNU/Linux 9.8 (stretch)**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary):N/A
- TensorFlow version:**the latest**
- Python version: N/A
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source):N/A
- GCC/Compiler version (if compiling from source):**gcc version 6.3.0 20170516 (Raspbian 6.3.0-18+rpi1+deb9u1)**
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

trying to build Tensorflow Lite for  Raspberry Pi 3b+
followed the instructions on https://tensorflow.google.cn/lite/guide/build_rpi 


 **at the last step got lots of errors such as:** 

`In file included from ./tensorflow/lite/experimental/c/c_api.h:24:0,
                 from ./tensorflow/lite/experimental/c/c_api_experimental.h:19,
                 from tensorflow/lite/experimental/c/c_api_experimental.cc:16:
./tensorflow/lite/experimental/c/c_api_types.h:1:1: error: expected unqualified-id before . token
 ../../c/c_api_internal.h
 ^
In file included from tensorflow/lite/experimental/c/c_api_experimental.cc:16:0:
./tensorflow/lite/experimental/c/c_api_experimental.h:28:24: error: TFL_Status does not name a type
 TFL_CAPI_EXPORT extern TFL_Status TFL_InterpreterResetVariableTensors(
                        ^~~~~~~~~~
./tensorflow/lite/experimental/c/c_api_experimental.h:38:5: error: variable or field TFL_InterpreterOptionsAddBuiltinOp declared void
     TFL_InterpreterOptions* options, TFL_BuiltinOperator op,
     ^~~~~~~~~~~~~~~~~~~~~~
./tensorflow/lite/experimental/c/c_api_experimental.h:38:5: error: TFL_InterpreterOptions was not declared in this scope
./tensorflow/lite/experimental/c/c_api_experimental.h:38:29: error: options was not declared in this scope
     TFL_InterpreterOptions* options, TFL_BuiltinOperator op,
                             ^~~~~~~
./tensorflow/lite/experimental/c/c_api_experimental.h:38:58: error: expected primary-expression before op
     TFL_InterpreterOptions* options, TFL_BuiltinOperator op,
                                                          ^~
./tensorflow/lite/experimental/c/c_api_experimental.h:39:5: error: expected primary-expression before const
     const TFL_Registration* registration, int min_version, int max_version);
     ^~~~~
./tensorflow/lite/experimental/c/c_api_experimental.h:39:43: error: expected primary-expression before int
     const TFL_Registration* registration, int min_version, int max_version);
                                           ^~~
./tensorflow/lite/experimental/c/c_api_experimental.h:39:60: error: expected primary-expression before int
     const TFL_Registration* registration, int min_version, int max_version);
                                                            ^~~
./tensorflow/lite/experimental/c/c_api_experimental.h:48:5: error: variable or field TFL_InterpreterOptionsAddCustomOp declared void
     TFL_InterpreterOptions* options, const char* name,
     ^~~~~~~~~~~~~~~~~~~~~~
./tensorflow/lite/experimental/c/c_api_experimental.h:48:5: error: TFL_InterpreterOptions was not declared in this scope
./tensorflow/lite/experimental/c/c_api_experimental.h:48:29: error: options was not declared in this scope
     TFL_InterpreterOptions* options, const char* name,
                             ^~~~~~~
./tensorflow/lite/experimental/c/c_api_experimental.h:48:38: error: expected primary-expression before const
     TFL_InterpreterOptions* options, const char* name,
                                      ^~~~~
./tensorflow/lite/experimental/c/c_api_experimental.h:49:5: error: expected primary-expression before const
     const TFL_Registration* registration, int min_version, int max_version);
     ^~~~~
./tensorflow/lite/experimental/c/c_api_experimental.h:49:43: error: expected primary-expression before int
     const TFL_Registration* registration, int min_version, int max_version);
                                           ^~~
./tensorflow/lite/experimental/c/c_api_experimental.h:49:60: error: expected primary-expression before int
     const TFL_Registration* registration, int min_version, int max_version);
                                                            ^~~
In file included from tensorflow/lite/experimental/c/c_api.cc:21:0:
./tensorflow/lite/experimental/c/c_api_types.h:1:1: error: expected unqualified-id before . token
 ../../c/c_api_internal.h

`



"
27888,tensorflow.multinomial performance ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
tensorflow.multinomial and pytorch.multinomial,the performance gap is huge.In particular I think tensorflow.multinomial is very problematic

I wrote the following code to compare the two functions,I set the first number to 10 and the rest to 0.0001, followed by softmax as the probability of selection.

**Describe the expected behavior**
I think tensorflow's performance is very unusual


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
[https://stackoverflow.com/questions/55705625/how-to-get-the-same-effect-and-what-makes-a-differencei-between-tensorflow-multi](url)


import torch

import numpy as np

import  tensorflow as tf

single_data = np.array([10,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001])


single_data = np.expand_dims(single_data, axis=0)

data =np.repeat(single_data, 50, axis=0)

data = tf.nn.softmax(data,axis=-1)

sampled_word = tf.multinomial(data,1)

sample = tf.reshape(sampled_word,[-1])

with tf.Session() as sess:

    a_data=sess.run(data)

    print(""prob:"",a_data)

    print(""tensorflow.multinomial"",sess.run(sample))

    a_data = torch.from_numpy(a_data)

    idx = torch.multinomial(a_data, num_samples=1)

    print(""tensorflow.multinomial"",np.reshape(idx.data.numpy(),[-1]))
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
but I get the result 
prob: [[9.93907740e-01 4.51278541e-05 4.51278541e-05 ... 4.51278541e-05
  4.51278541e-05 4.51278541e-05]
 [9.93907740e-01 4.51278541e-05 4.51278541e-05 ... 4.51278541e-05
  4.51278541e-05 4.51278541e-05]
 [9.93907740e-01 4.51278541e-05 4.51278541e-05 ... 4.51278541e-05
  4.51278541e-05 4.51278541e-05]
 ...
 [9.93907740e-01 4.51278541e-05 4.51278541e-05 ... 4.51278541e-05
  4.51278541e-05 4.51278541e-05]
 [9.93907740e-01 4.51278541e-05 4.51278541e-05 ... 4.51278541e-05
  4.51278541e-05 4.51278541e-05]
 [9.93907740e-01 4.51278541e-05 4.51278541e-05 ... 4.51278541e-05
  4.51278541e-05 4.51278541e-05]]
tensorflow.multinomial [121 129  35 104   4 133  60  92 104 129   4  49  35  99 109 111  62  87
  23   5 109  63 103  61  78  43 101  85   2 127   0  36  53   0  74  44
  64  55  51  59 108   0 112  32  36  24  68 135  72  22]
tensorflow.multinomial [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0]"
27887,Saving model and loading model on a separate session gives wrong predictions,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: using pip install tensorflow-gpu
- **TensorFlow version (use command below)**:1.12.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: - 
- **GCC/Compiler version (if compiling from source)**: - 
- **CUDA/cuDNN version**: 9
- **GPU model and memory**: GeForce 940MX
- **Exact command to reproduce**: As described below

### Describe the problem

As soon as training is finished, generating predictions on the testing dataset gives an accuracy > 70.

Then I save the model using model.save(...) method.

When Trying to load the model in a separate session, the predictions are around 20s.

I can confirm that i'm using the same testing dataset and same preprocessing steps that I've done when predicting after training.

To get over with the doubt on the different tokenizers, I've saved the tokenizer to a tokenizer.pickle file which will be loaded before conducting texts_to_sequences and pad_sequences on the testing text data (this didn't make any difference).

I've tried compiling the model after loading before making predictions, this didn't work as well.

### Source code / logs

`model.save('model.h5') #for saving the model`

---------------------------------------------------------------------------

`#Predicting after saving on a different session`

`trained_model = load_model('model.h5') #for loading the model in a different session.`

`# load tokenizer`
`tokenizer = Tokenizer()`
`with open('trained_model/tokenizer.pickle', 'rb') as handle:`
`  tokenizer = pickle.load(handle)`

`test_revs = pd.read_csv('test_dataset.csv')`

`test_revs.loc[:, 'rating'] = test_revs['rating'].apply(points_to_class) #converting decimal ratings to integer classes `

`actual_texts = test_revs['text']`
`actual_ratings = test_revs['rating']`

`final_Y_test = to_categorical(actual_ratings, 5)`

`actual_text_tokens = add_doc_to_vocab(actual_texts.tolist()) # preprocessing method`

`sequences_test = tokenizer.texts_to_sequences(actual_text_tokens)`

`X_test = pad_sequences(sequences_test, maxlen=1939, padding='post')`

`# Predictions`
`pred_test = trained_model.predict(X_test)`
`pred_test = [np.argmax(x) for x in pred_test]`

`# Actual`
`true_test = final_Y_test`
`true_test = [np.argmax(x) for x in true_test]`

`# Find accuracies`
`accuracy = accuracy_score(true_test, pred_test)`

`print(""The total accuracy is : "", accuracy)`

I'm new to TensorFlow, if anyone has any suggestions, please raise, cheers !"
27886,[TF2.0] TypeError: Cannot convert provided value to EagerTensor when applying constraint on variable,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0.0 alpha
- Python version: 3.7.3
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.0/7.5
- GPU model and memory: V100, 16GB

**Describe the current behavior**
Cannot convert provided value to EagerTensor when applying keras constraint on variable in TF2.0 eager mode.

**Describe the expected behavior**
Variable should be converted to EagerTensor, operation should return constrained variable.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
v = tf.Variable([[1, 2], [3, 4]])   
tf.keras.constraints.UnitNorm(axis=1)(v)
```

**Other info / logs**
> TypeError                                 Traceback (most recent call last)
> <ipython-input-35-760885333ab7> in <module>
> ----> 1 tf.keras.constraints.UnitNorm(axis=1)(v)
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/constraints.py in __call__(self, w)
>     110         K.epsilon() + K.sqrt(
>     111             math_ops.reduce_sum(
> --> 112                 math_ops.square(w), axis=self.axis, keepdims=True)))
>     113 
>     114   def get_config(self):
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in sqrt(x)
>    1878       A tensor.
>    1879   """"""
> -> 1880   zero = _to_tensor(0., x.dtype.base_dtype)
>    1881   inf = _to_tensor(np.inf, x.dtype.base_dtype)
>    1882   x = clip_ops.clip_by_value(x, zero, inf)
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in _to_tensor(x, dtype)
>     612       A tensor.
>     613   """"""
> --> 614   return ops.convert_to_tensor(x, dtype=dtype)
>     615 
>     616 
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)
>    1048   preferred_dtype = deprecation.deprecated_argument_lookup(
>    1049       ""dtype_hint"", dtype_hint, ""preferred_dtype"", preferred_dtype)
> -> 1050   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
>    1051 
>    1052 
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)
>    1106       name=name,
>    1107       preferred_dtype=dtype_hint,
> -> 1108       as_ref=False)
>    1109 
>    1110 
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)
>    1184 
>    1185     if ret is None:
> -> 1186       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
>    1187 
>    1188     if ret is NotImplemented:
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
>     302                                          as_ref=False):
>     303   _ = as_ref
> --> 304   return constant(v, dtype=dtype, name=name)
>     305 
>     306 
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
>     243   """"""
>     244   return _constant_impl(value, dtype, shape, name, verify_shape=False,
> --> 245                         allow_broadcast=True)
>     246 
>     247 
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
>     251   ctx = context.context()
>     252   if ctx.executing_eagerly():
> --> 253     t = convert_to_eager_tensor(value, ctx, dtype)
>     254     if shape is None:
>     255       return t
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
>     108       return ops.EagerTensor(
>     109           value, handle, device, dtype, tensor)
> --> 110     t = ops.EagerTensor(value, handle, device, dtype)
>     111     scalar_cache[cache_key] = t
>     112     return t
> 
> TypeError: Cannot convert provided value to EagerTensor. Provided value: 0.0 Requested dtype: int32"
27885,[Bug]Error when use ExponentialMovingAverage with distribute.Strategy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MS Windows10 X64 1809 build17763
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary):binary(use pip)
- TensorFlow version (use command below):v1.13.1-0-g6612da8951
- Python version:3.6.7
- Bazel version (if compiling from source):N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version:10.0/7.5.1
- GPU model and memory:NVIDIA Geforce RTX2080TI 11GB x2

**Describe the current behavior**
```python
def _model_fn(...):
  logits, _ = _models.build_model(
          features,
          model_name=FLAGS.model_name,
          training=is_training,
          override_params=override_params)
......
  global_step = tf.train.get_global_step()
  if has_moving_average_decay:
    ema = tf.train.ExponentialMovingAverage(
        decay=FLAGS.moving_average_decay, num_updates=global_step)
    ema_vars = tf.trainable_variables() + tf.get_collection('moving_vars')
    for v in tf.global_variables():
      if 'moving_mean' in v.name or 'moving_variance' in v.name:
        ema_vars.append(v)
    ema_vars = list(set(ema_vars))
......
  if has_moving_average_decay:
      with tf.control_dependencies([train_op]):
        train_op = ema.apply(ema_vars)
......
def main():
......
  if FLAGS.num_gpus <= 1:
    distribution_strategy = None
  else:
    distribution_strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=FLAGS.num_gpus,                                                                   cross_device_ops=tf.contrib.distribute.AllReduceCrossDeviceOps('hierarchical_copy', num_packs=2))
  config = tf.estimator.RunConfig(
      model_dir=FLAGS.model_dir,
      train_distribute=distribution_strategy,
      save_checkpoints_steps=save_checkpoints_steps,
      log_step_count_steps=FLAGS.log_step_count_steps,
      session_config=tf.ConfigProto(
          allow_soft_placement=True,
          graph_options=tf.GraphOptions(
              rewrite_options=rewriter_config_pb2.RewriterConfig(
                  disable_meta_optimizer=True))),)
  model_est = tf.estimator.Estimator(
                        model_fn=_model_fn,
                        config=config,
                        params=params
  )
......
```
I have find a bug when use ExponentialMovingAverage with distribute.Strategy.These codes work fine without using a distribution strategy.But when using a distribution strategy (such as MirroredStrategy), an error is reported:
```
Traceback (most recent call last):
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\distribute\shared_variable_creator.py"", line 90, in reuse_variable
    v = shared_variable_store[canonical_name][variable_index]
KeyError: 'mnasnet-a1/mnasnet/mnas_stem/conv2d/kernel/replica/ExponentialMovingAverage/'
```
By debugging I can see that it works fine when ema.apply() is executed on the first GPU.But when executed on the second GPU, it will report an error.This is because MirroredVariable/ReplicaLocalVariable has been created when building the model in _model_fn().So on the second GPU, TF tries to create the variable 'conv2d/kernel/replica/ExponentialMovingAverage/'.But it doesn't exist at all, 'replica' should appear at the end of the variable name.
I found a temporary solution by modifying the shared_variable_creator:
```python
  def reuse_variable(next_creator, *args, **kwargs):
    """"""Re-use existing variable from store with same name (in order).""""""
    del next_creator, args
    name = kwargs.get(""name"")
    canonical_name = _canonicalize_variable_name(name)
    replica_index = canonical_name.find('replica/')
    if replica_index != -1:
      canonical_name = canonical_name[0:replica_index] + canonical_name[replica_index+8:]
    try:
      variable_index = variable_scope_access_index.get(canonical_name, 0)
      v = shared_variable_store[canonical_name][variable_index]
      # TODO(priyag): Make this variable re-use more robust by adding checks
      # that the requested shape and dtype match the existing variable.
      variable_scope_access_index[canonical_name] = variable_index + 1
      return v
    except (KeyError, IndexError):
      raise RuntimeError(
          ""Tried to create variable {} with mismatching name on device {}"".
          format(name, device_id))

  if device_id == 0:
    return create_new_variable
  else:
    return reuse_variable
```
But this looks more like a patch than a solution.I hope someone can review this bug and fix it.
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27883,build error on latest master ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7.4.1708
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA
- TensorFlow installed from (source or binary): source
- TensorFlow version: commit 0e4117f67192a140466ba66513035e24bbd1a474
- Python version: 3.6
- Installed using virtualenv? pip? conda?: source
- Bazel version (if compiling from source):0.22.0
- GCC/Compiler version (if compiling from source): gcc 6.3.0
- CUDA/cuDNN version: NA
- GPU model and memory:NA



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
ERROR: /home/guizili/.cache/bazel/_bazel_guizili/fb08bc8aff974002b12a4e85119bd120/external/com_github_googlecloudplatform_google_cloud_cpp/google/cloud/BUILD:27:1: no such target '@bazel_tools//tools/cpp:cc_flags': target 'cc_flags' not declared in package 'tools/cpp' defined by /home/guizili/.cache/bazel/_bazel_guizili/fb08bc8aff974002b12a4e85119bd120/external/bazel_tools/tools/cpp/BUILD and referenced by '@com_github_googlecloudplatform_google_cloud_cpp//google/cloud:generate_build_info'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27882,Converting InceptionV3 to TfLite missing IdentityN,"**System information**
Google Colab
- TensorFlow installed from (source or binary):
Installed from https://storage.googleapis.com/download.tensorflow.org/data/tensorflow_hub-0.4.0.dev0-py2.py3-none-any.whl
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CONCATENATION, CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, MEAN, RESHAPE, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.

Also, please include a link to a GraphDef or the model if possible.
Google Colab Notebook: https://colab.research.google.com/drive/1idF4ZZysBnfcGwfoj0DovQb8C4NVA42w

I updated the Google Colab Tutorial and updated it to use InceptionV3(based on the link included in the notebook and modified the file size to be [299,299].
"
27880,Quantization-Aware Training support in Keras,"**System information**
- TensorFlow version (you are using): 1.13.1 (but willing to use 2.0.0-alpha0 if there is a good reason)
- Are you willing to contribute it (Yes/No): Yes (given some pointers on how to best go about it)

**Describe the feature and the current behavior/state.**
Currently there is no obvious way to apply `tf.contrib.quantize.create_training_graph` to a keras model. The keras API only allows access to the graph after it has already created a session. Attempting to modify the graph at this point does not work:
https://stackoverflow.com/questions/55123417/quantization-aware-retraining-a-keras-model
https://stackoverflow.com/questions/52259343/quantize-a-keras-neural-network-model

I have also tried to create a new session after rewriting the graph, without success:
```
tf.contrib.quantize.create_training_graph(input_graph=tf.keras.backend.get_session().graph, quant_delay=0)
# create a new session after rewriting the graph
new_session = tf.Session()
tf.keras.backend.set_session(new_session)
```

Results in this error when I try to fit the model:
```
tensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable dense_5/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/dense_5/bias/class tensorflow::Var does not exist.
        [[{{node dense_5/BiasAdd/ReadVariableOp}}]]
```

**Will this change the current api? How?**
Probably, but in a backwards-compatible way. I imagine some kind of graph rewriting hook would probably be necessary in the tf.keras API.

**Who will benefit with this feature?** Users of TF Lite / Edge TPU wishing to easily train quantized models using the keras API (which is being pushed as the new ""one true API"" for tensorflow).

**Any Other info.**
Related issue on the main keras project https://github.com/keras-team/keras/issues/11105"
27879,Which latest python is supported by the latest version?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27877,tf.keras with dataset operates much slower without persistence mode,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1 (from Anaconda)
- Python version: 3.6.8 (Anaconda)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: Titan Xp 12 Gb


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I am training a straightforward Keras stacked LSTM model, using model.compile and model.fit. When I train using nvidia-smi with persistence mode off, GPU utilization is at ~5-10% and training ETA goes up 3x. When I turn on persistence mode, GPU utilization goes to 60% and stays there.
**Describe the expected behavior**
I would like this code to work without needing to enable persistence mode.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
I can if deemed necessary. 
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27875,Second Order derivative of sparse_softmax_cross_entropy_with_logits,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Non
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): PyPI
- TensorFlow version (use command below): 2.0-alpha
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behaviour**
Currently trying to take second order derivatives of `sparse_softmax_cross_entropy_with_logits` leads to the error:
```
LookupError: Gradient explicitly disabled. Reason: b""Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation's interaction with tf.gradients()""
```

**Describe the expected behavior**
This should work.
"
27874,Having trouble running tensorflow with gpu,"

**System information**
- Ubuntu 18.04
- TensorFlow installed from source (not sure what this means)
- TensorFlow version 1.12.0
- Python version 3.6
- Installed using pip
- CUDA/cuDNN version 9.1
- GPU model and memory: GV100 


I'm new to tensorflow, and I'm trying to run it using a GPU. I've installed tensorflow in the past, but not the gpu version. Today, I uninstalled the old version, and installed the gpu version (1.12.0). I'm getting the following error when I try to run basic commands: 

"" ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory
Failed to load the native TensorFlow runtime. ""

I've done some troubleshooting, including installing an older version of tensorflow, but nothing seems to work. Has anyone dealt with this problem before?


"
27872,examples/get_started/regression/dnn_regression.py failed with ValueError (nightly build),"The above example has not been working for awhile now. (I don't believe it worked with 1.13.1) the code looks valid, what is wrong with the example code?


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): x86 Linux Ubuntu 18.04 container
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.0-12503-g5e52b70188 1.14.1-dev20190415
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
The example https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/get_started/regression/dnn_regression.py, fails with the following error:
ValueError: Tensor(""buffer_size:0"", shape=(), dtype=int64, device=/device:CPU:0) must be from the same graph as Tensor(""MapDataset_2:0"", shape=(), dtype=variant).

(Actually all 3 examples in that folder fail with the same error)

**Describe the expected behavior**
Provided examples should run without failure

**Code to reproduce the issue**
pip3 install tf-nightly
git clone http://github.com/tensorflow/tensorflow
cd tensorflow/tensorflow/examples/get_started/regression/
python3 dnn_regression.py

**Other info / logs**
```
root@c12a5b5766c5:~/tensorflow/tensorflow/examples/get_started/regression# python3 dnn_regression.py

WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data
32768/25936 [=====================================] - 0s 1us/step
W0415 20:14:20.850975 140624720754496 deprecation.py:237] From /root/tensorflow/tensorflow/examples/get_started/regression/imports85.py:123: The name tf.string_to_hash_bucket_fast is deprecated. Please use tf.strings.to_hash_bucket_fast instead.

I0415 20:14:20.987369 140624720754496 estimator.py:1761] Using default config.
W0415 20:14:20.987863 140624720754496 estimator.py:1782] Using temporary folder as model directory: /tmp/tmpn4983vfs
I0415 20:14:20.988089 140624720754496 estimator.py:203] Using config: {'_model_dir': '/tmp/tmpn4983vfs', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe593747048>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0415 20:14:20.993491 140624720754496 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Traceback (most recent call last):
  File ""dnn_regression.py"", line 105, in <module>
    tf.app.run(main=main)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""dnn_regression.py"", line 85, in main
    model.train(input_fn=input_train, steps=STEPS)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 360, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1152, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1179, in _train_model_default
    input_fn, ModeKeys.TRAIN))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1016, in _get_features_and_labels_from_input_fn
    self._call_input_fn(input_fn, mode))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1107, in _call_input_fn
    return input_fn(**kwargs)
  File ""dnn_regression.py"", line 46, in input_train
    train.shuffle(1000).batch(128)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1634, in shuffle
    buffer_size, seed, reshuffle_each_iteration))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 751, in shuffle
    return ShuffleDataset(self, buffer_size, seed, reshuffle_each_iteration)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2743, in __init__
    **flat_structure(self))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 4976, in shuffle_dataset
    name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 366, in _apply_op_helper
    g = ops._get_graph_from_inputs(_Flatten(keywords.values()))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 6098, in _get_graph_from_inputs
    _assert_same_graph(original_graph_element, graph_element)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 6034, in _assert_same_graph
    original_item))
ValueError: Tensor(""buffer_size:0"", shape=(), dtype=int64, device=/device:CPU:0) must be from the same graph as Tensor(""MapDataset_2:0"", shape=(), dtype=variant).
root@c12a5b5766c5:~/tensorflow/tensorflow/examples/get_started/regression#

```
"
27868,Possible wrong assumption for how a dense flow filed is stored,"In `dense_image_warp` function it is assumed that flow vectors are stored in source location of flow vectors while for dense optical flow datasets like **Middlebury**, **MPI Sinte**, **Flying Chairs**it is not the case.

In the following simple example a black dot moves from the location (2,6) in image one to (5,10) in image two. So, flow vector for that point is (3,4). In Tensorflow code it assumes that flow field is stored in **Format A**. That is why [here](https://github.com/tensorflow/tensorflow/blob/79d8779069a154a74786c28bf9d8af3d0cbd9905/tensorflow/contrib/image/python/ops/dense_image_warp.py#L172) it is explained:

> ... where the warp is specified by a dense
>   flow field of offset vectors that define the correspondences of pixel values
>   in the output image back to locations in the  source image. Specifically, the
>   pixel value at output[b, j, i, c] is
>   images[b, j - flow[b, j, i, 0], i - flow[b, j, i, 1], c].


![FlowExplained-tensorflow](https://user-images.githubusercontent.com/39167815/56152784-3ae13f80-5f7a-11e9-8cb0-a26d097f062d.png)

Can anybody explain the reason to implement this function in this way? While this format is useful for some specific tasks, it is not consistent with standard datasets.

I would suggest changing the assumption to **Format B** and modify the code to do backward warping (given image 2 and flow field, it will reconstruct image 1). Specifically, the pixel value at 
output[b, j, i, c] would be  images[b, j + flow[b, j, i, 0], i + flow[b, j, i, 1], c]

In terms of implementation, line [210](https://github.com/tensorflow/tensorflow/blob/79d8779069a154a74786c28bf9d8af3d0cbd9905/tensorflow/contrib/image/python/ops/dense_image_warp.py#L210) can be changed to:

```python
query_points_on_grid = batched_grid + flow
```
"
27862,Dimension Mismatch error in [image_captioning.ipynb],"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:2.2.0 alpha
- Doc Link: [https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/image_captioning.ipynb](url)

I am running this example with the Google colab and getting this issue: 
```
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)
   1818   try:
-> 1819     c_op = c_api.TF_FinishOperation(op_desc)
   1820   except errors.InvalidArgumentError as e:

InvalidArgumentError: Dimension 0 in both shapes must be equal, but are 1 and 32. Shapes are [1,1] and [32,1]. for 'rnn__decoder/concat' (op: 'ConcatV2') with input shapes: [1,1,256], [32,1,256], [] and with computed input tensors: input[2] = <-1>.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-40-bdc59db794fd> in <module>()
      6 
      7     for (batch, (img_tensor, target)) in enumerate(dataset):
----> 8         batch_loss, t_loss = train_step(img_tensor, target)
      9         total_loss += t_loss
     10 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    412       # In this case we have created variables on the first call, so we run the
    413       # defunned version which is guaranteed to never create variables.
--> 414       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
    415     elif self._stateful_fn is not None:
    416       # In this case we have not created variables on the first call. So we can

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   1285   def __call__(self, *args, **kwargs):
   1286     """"""Calls a graph function specialized to the inputs.""""""
-> 1287     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
   1288     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1289 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   1609           relaxed_arg_shapes)
   1610       graph_function = self._create_graph_function(
-> 1611           args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)
   1612       self._function_cache.arg_relaxed[rank_only_cache_key] = graph_function
   1613 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   1510             arg_names=arg_names,
   1511             override_flat_arg_shapes=override_flat_arg_shapes,
-> 1512             capture_by_value=self._capture_by_value),
   1513         self._function_attributes)
   1514 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    692                                           converted_func)
    693 
--> 694       func_outputs = python_func(*func_args, **func_kwargs)
    695 
    696       # invariant: `func_outputs` contains only Tensors, IndexedSlices,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    315         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    316         # the function a weak reference to itself to avoid a reference cycle.
--> 317         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    318     weak_wrapped_fn = weakref.ref(wrapped_fn)
    319 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    684                   optional_features=autograph_options,
    685                   force_conversion=True,
--> 686               ), args, kwargs)
    687 
    688         # Wrapping around a decorator allows checks like tf_inspect.getargspec

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)
    390     return _call_unconverted(f, args, kwargs)
    391 
--> 392   result = converted_f(*effective_args, **kwargs)
    393 
    394   # The converted function's closure is simply inserted into the function's

/tmp/tmp3uq18dwt.py in tf__train_step(img_tensor, target)
     15       dec_input_1 = ag__.converted_call('expand_dims', tf, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_6, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (target[:, (i)], 1), {})
     16       return dec_input_1, loss_1, hidden_1
---> 17     dec_input, loss, hidden = ag__.for_stmt(ag__.converted_call(range, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_3, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (1, target.shape[1]), {}), None, loop_body, (dec_input, loss, hidden))
     18   total_loss = loss / ag__.converted_call(int, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_7, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (target.shape[1],), {})
     19   trainable_variables = encoder.trainable_variables + decoder.trainable_variables

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/operators/control_flow.py in for_stmt(iter_, extra_test, body, init_state)
     79     return _dataset_for_stmt(iter_, extra_test, body, init_state)
     80   else:
---> 81     return _py_for_stmt(iter_, extra_test, body, init_state)
     82 
     83 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/operators/control_flow.py in _py_for_stmt(iter_, extra_test, body, init_state)
     88     if extra_test is not None and not extra_test(*state):
     89       break
---> 90     state = body(target, *state)
     91   return state
     92 

/tmp/tmp3uq18dwt.py in loop_body(loop_vars, dec_input_1, loss_1, hidden_1)
     11     def loop_body(loop_vars, dec_input_1, loss_1, hidden_1):
     12       i = loop_vars
---> 13       predictions, hidden_1, _ = ag__.converted_call(decoder, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_4, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (dec_input_1, features, hidden_1), {})
     14       loss_1 += ag__.converted_call(loss_function, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_5, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (target[:, (i)], predictions), {})
     15       dec_input_1 = ag__.converted_call('expand_dims', tf, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_6, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (target[:, (i)], 1), {})

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)
    265 
    266   if not options.force_conversion and conversion.is_whitelisted_for_graph(f):
--> 267     return _call_unconverted(f, args, kwargs)
    268 
    269   # internal_convert_user_code is for example turned off when issuing a dynamic

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs)
    186     return f.__self__.call(args, kwargs)
    187 
--> 188   return f(*args, **kwargs)
    189 
    190 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    610                       base_layer_utils.AutoAddUpdates(self,
    611                                                       inputs)) as auto_updater:
--> 612                 outputs = self.call(inputs, *args, **kwargs)
    613                 auto_updater.set_outputs(outputs)
    614 

<ipython-input-33-dc6f8641457a> in call(self, x, features, hidden)
     22 
     23     # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)
---> 24     x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)
     25 
     26     # passing the concatenated vector to the GRU

/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    178     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    179     try:
--> 180       return target(*args, **kwargs)
    181     except (TypeError, ValueError):
    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in concat(values, axis, name)
   1269               tensor_shape.scalar())
   1270       return identity(values[0], name=scope)
-> 1271   return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
   1272 
   1273 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in concat_v2(values, axis, name)
   1215   _attr_N = len(values)
   1216   _, _, _op = _op_def_lib._apply_op_helper(
-> 1217         ""ConcatV2"", values=values, axis=axis, name=name)
   1218   _result = _op.outputs[:]
   1219   _inputs_flat = _op.inputs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    798         op = g.create_op(op_type_name, inputs, output_types, name=scope,
    799                          input_types=input_types, attrs=attr_protos,
--> 800                          op_def=op_def)
    801       return output_structure, op_def.is_stateful, op
    802 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)
    450     return super(FuncGraph, self).create_op(
    451         op_type, inputs, dtypes, input_types, name, attrs, op_def,
--> 452         compute_device=compute_device)
    453 
    454   def capture(self, tensor, name=None):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    505                 'in a future version' if date is None else ('after %s' % date),
    506                 instructions)
--> 507       return func(*args, **kwargs)
    508 
    509     doc = _add_deprecated_arg_notice_to_docstring(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in create_op(***failed resolving arguments***)
   3477           input_types=input_types,
   3478           original_op=self._default_original_op,
-> 3479           op_def=op_def)
   3480       self._create_op_helper(ret, compute_device=compute_device)
   3481     return ret

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)
   1981           op_def, inputs, node_def.attr)
   1982       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,
-> 1983                                 control_input_ops)
   1984 
   1985     # Initialize self._outputs.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)
   1820   except errors.InvalidArgumentError as e:
   1821     # Convert to ValueError for backwards compatibility.
-> 1822     raise ValueError(str(e))
   1823 
   1824   return c_op

ValueError: Dimension 0 in both shapes must be equal, but are 1 and 32. Shapes are [1,1] and [32,1]. for 'rnn__decoder/concat' (op: 'ConcatV2') with input shapes: [1,1,256], [32,1,256], [] and with computed input tensors: input[2] = <-1>.
```

**I checked different thing but can't able to solve this issue**

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
27861,Tensorflow 2.0 keras load_model does not restore step / epoch counters,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, see example below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0 alpha 0 - gpu
- Python version: 3.5.3
- CUDA/cuDNN version: Cuda 10
- GPU model and memory: GeForce 960M 

**Describe the current behavior**
I train a keras model and let it be saved by the `keras.callbacks.ModelCheckpoint`. After the training is finished, I want to continue it from the saved checkpoint. Therefore, I load the model with `keras.models.load_model` and run `fit` again. Unfortunately, it seems that the steps/epochs counters are not restored. 

**Describe the expected behavior**
The steps /epochs counters should be restored to allow continuation of training. If they are not restored, e.g. tensorboard cannot be used properly, since the new training will write its values to the same steps as the first training, instead of appending them. The result looks like this: 

![Unbenannt](https://user-images.githubusercontent.com/9267365/56139393-12b50900-5f99-11e9-9c4e-b553ab2c1589.PNG)


**Code to reproduce the issue**
Run the following code multiple times and have a look at the tensorboard results by running `tensorboard --logdir=test_outputs`.

```
import os
from tensorflow import keras

log_dir = 'test_outputs'
model_file = os.path.join(log_dir, ""model.hdf5"")

(x_train, y_train), _ = keras.datasets.mnist.load_data()


def create_model():
	model = keras.models.Sequential([
		keras.layers.Flatten(input_shape=(28, 28), name=""flatten""),
		keras.layers.Dense(128, activation='relu', name=""dense1""),
		keras.layers.Dropout(0.2, name=""dropout""),
		keras.layers.Dense(10, activation='softmax', name=""dense2"")
	])

	model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
	return model


if os.path.exists(model_file):
	model = keras.models.load_model(model_file)
else:
	model = create_model()

model.fit(x_train, y_train, epochs=5, callbacks=[keras.callbacks.TensorBoard(log_dir=log_dir), keras.callbacks.ModelCheckpoint(model_file)])
```
"
27860,AttributeError: module 'tensorflow.tools.api.generator.api.estimator' has no attribute 'SessionRunHook',"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
27858,"tf.summary.tensor_summary no longer appears in the REPL, see it in the code? Docs need update? ","Not sure what the intention is as I see a lot of v1/v2 name changing.

tf.summary.scalar is there but not tf.summary.tensor_summary

In [80]: tf.__version__
Out[80]: '2.0.0-alpha0'"
27857,tf.estimator.train_and_evaluate not run evaluate when distribute strategy is CollectiveAllReduceStrategy,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
CentOS Linux release 7.3.1611
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
1.13.1
- Python version:
2.7.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.13.1-0-g6612da8951', '1.13.1')

**Describe the current behavior**
When I try to run estimator in distribute with CollectiveAllReduceStrategy strategy, the train_and_evaluate api do not run evaluation after model save checkpoint.

**Describe the expected behavior**
train_and_evaluate should run evaluation after model save checkpoint

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
import tensorflow as tf
import random
import os
import json

class Generator:
    def __init__(self, mode, batch_size=100):
        self._i = 0
        self._mode = mode
        self._batch_size = batch_size

    def _get_random(self):
        return random.uniform(0, 100)

    def next_batch(self):
        self._i += 1
        if self._mode != tf.estimator.ModeKeys.TRAIN and self._i > 200:
            raise StopIteration
        features = {'a': [], 'b': []}
        labels = []
        for _ in xrange(self._batch_size):
            label = 0.0
            for key in features:
                r = self._get_random()
                features[key].append(r)
                label += r
            labels.append(label)
        return features, labels

    def output_types(self):
        return ({'a': tf.float32, 'b': tf.float32}, tf.float32)

    def output_shapes(self):
        return ({'a': [None], 'b': [None]}, [None])

def _dataset(mode):
    generator = Generator(mode)

    def generate_data():
        while True:
            yield generator.next_batch()

    return tf.data.Dataset.from_generator(
            generator=generate_data,
            output_types=generator.output_types(),
            output_shapes=generator.output_shapes(),
            args=[])

def _my_model_fn(features, labels, mode, params):
    learning_rate = params['learning_rate']
    keep_prob = params['keep_prob']
    feature_columns = [tf.feature_column.numeric_column('a'),
            tf.feature_column.numeric_column('b')]
    dense_tensor = tf.feature_column.input_layer(features, feature_columns)
    dense_tensor = tf.nn.dropout(dense_tensor, keep_prob=keep_prob)
    for units in [64, 32]:
        dense_tensor = tf.layers.dense(dense_tensor, units, tf.nn.relu)
    predictions = tf.layers.dense(dense_tensor, 1)
    predictions = tf.squeeze(predictions, [1])
    loss = tf.losses.absolute_difference(labels=labels, predictions=predictions)
    if mode == tf.estimator.ModeKeys.EVAL:
        accuracy_op = tf.metrics.accuracy(
                labels=labels,
                predictions=predictions,
                name='accuracy_op')
        eval_metric_ops = {'accuracy': accuracy_op}
        spec = tf.estimator.EstimatorSpec(tf.estimator.ModeKeys.EVAL,
                loss=loss,
                eval_metric_ops=eval_metric_ops)
    else:
        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())
        spec = tf.estimator.EstimatorSpec(tf.estimator.ModeKeys.TRAIN,
                loss=loss,
                train_op=train_op)
    return spec

def _cluster():
    return {'worker': ['localhost:2222', 'localhost:2223', 'localhost:2224']}

def _set_tf_config(index):
    tf_config = {
            'cluster': _cluster(),
            'task': {'type': 'worker', 'index': index}}
    os.environ['TF_CONFIG'] = json.dumps(tf_config)

def main(argv):
    distribution = tf.contrib.distribute.CollectiveAllReduceStrategy()
    config = tf.estimator.RunConfig(
            save_checkpoints_steps=2000,
            keep_checkpoint_max=1,
            train_distribute=distribution,
            eval_distribute=distribution)
    model_dir = './model'
    learning_rate = 1e-6
    keep_prob = 0.75
    estimator = tf.estimator.Estimator(
            model_fn=_my_model_fn,
            model_dir=model_dir,
            config=config,
            params={
                'learning_rate': learning_rate,
                'keep_prob': keep_prob
            })
    train_spec = tf.estimator.TrainSpec(
            input_fn=lambda : _dataset(tf.estimator.ModeKeys.TRAIN),
            max_steps=4000)
    eval_spec = tf.estimator.EvalSpec(
            input_fn=lambda : _dataset(tf.estimator.ModeKeys.EVAL))
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)

if __name__ == '__main__':
    FLAGS = tf.app.flags.FLAGS
    tf.app.flags.DEFINE_integer(
        'index', 0, 'input task index')
    _set_tf_config(FLAGS.index)
    tf.logging.set_verbosity(tf.logging.INFO)
    tf.app.run()
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27856,get error message when use tf.summary.scalar() with TF 2.0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf-nightly-gpu-2.0-preview: 2.0.0-dev20190413
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.0
- GPU model and memory: Yes, 11GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

I get the following error message when I use tensorflow in GPU mode. When I use tensorflow in CPU mode, such error message doesn't occur.

>AttributeError: module 'tensorboard.summary._tf.summary' has no attribute 'summary_scope'

**Describe the expected behavior**

I expect no error when I call tf.summary.scalar() in TF2.0 in both GPU and CPU mode.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```Python
#!/usr/bin/python3

import tensorflow as tf;

def main():

    log = tf.summary.create_file_writer('checkpoints');
    with log.as_default():
        tf.summary.scalar('loss',1., step = 0);

if __name__ == ""__main__"":

    main();
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27855,How to use libtensorflow-lite.a with C++ on Raspberry Pi 3?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>


**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Rasbian
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi 3b+
- TensorFlow installed from (source or binary):No
- TensorFlow version:No
- Python version:3.5
- Installed using virtualenv? pip? conda?:No
- Bazel version (if compiling from source):No
- GCC/Compiler version (if compiling from source):No
- CUDA/cuDNN version:No
- GPU model and memory:No


I have trained own model and converted to lite model. Then I have generated libtensorflow-lite.a with cross-compiling on Ubuntu 16.04. I am trying to find the header file of all the objects in the libtensorflow-lite.a. Is this correct ? How to use it on Raspberry Pi 3 ? 

In addition, the path to the libtensorflow-lite.a generated when using cross-compilation is /tensorflow/lite/tools/make/gen/rpi_armv7l/lib is not the same as the official teaching path.
Does this compile fail?

"
27854,Problem with Running GANEstimator: Type Error in eval_utils_impl.py,"```

---> 55   if grid_shape[0] * grid_shape[1] != int(input_tensor.shape[0]):
     56     raise ValueError(""Grid shape %s incompatible with minibatch size %i."" %
     57                      (grid_shape, int(input_tensor.shape[0])))

TypeError: __int__ returned non-int (type NoneType)
```
Source: tensorflow/contrib/gan/python/eval/python/eval_utils_impl.py

I am rebuilding GAN Estimator with MNIST Dataset:

input function is defined as:

```python
def preprocess(img, label):
    img = tf.cast(img, dtype=tf.float32)
    img = tf.divide(img, 255.0)
    if len(img.shape) == 2:
        img = tf.expand_dims(img, 2)
    prior = tf.random_normal([64])
    return prior, img


def get_input_fn(is_train):
    if is_train:
        dataset = train
    else:
        dataset = test
    tf_dataset = tf.data.Dataset.from_tensor_slices(dataset)

    tf_dataset = tf_dataset.map(preprocess)
        
    def input_fn():
        data = tf_dataset.batch(BATCH_SIZE)
        dA_iterator = data.make_one_shot_iterator()
        prior, d_data = dA_iterator.get_next()
        
        return (prior, d_data)
    return input_fn

input_fn = get_input_fn(True)
```

GAN generators and discriminators:
```python

def generator_fn(prior):
    output = tf.keras.layers.Dense(1024)(prior)
    output = tf.keras.layers.Dense(7*7*128)(output)
    output = tf.reshape(output, (-1, 7, 7, 128))
    output = tf.keras.layers.Conv2DTranspose(64, 4, 2, padding=""same"")(output)
    output = tf.keras.layers.Conv2DTranspose(32, 4, 2, padding=""same"")(output)
    output = tf.keras.layers.Conv2DTranspose(1, 4, padding=""same"", activation=tf.nn.tanh)(output)
    return output

def discriminator_fn(img, unused_conditioning):
    output = tf.keras.layers.Conv2D(64, 4, 2)(img)
    output = tf.keras.layers.Conv2D(128, 4, 2)(output)
    output = tf.layers.flatten(output)
    output = tf.layers.Dense(1024)(output)
    output = tf.contrib.layers.layer_norm(output)
    output = tf.layers.Dense(1)(output)
    return output
```

Estimator is created as:

```python
gan_estimator = tfgan.estimator.GANEstimator(generator_fn=generator_fn, discriminator_fn=discriminator_fn,\
                             generator_loss_fn=tfgan.losses.wasserstein_generator_loss, \
                             discriminator_loss_fn=tfgan.losses.wasserstein_discriminator_loss,\
                             generator_optimizer=tf.train.AdamOptimizer(0.001, 0.5),\
                             discriminator_optimizer=tf.train.AdamOptimizer(0.0001, 0.5),\
                             add_summaries=tfgan.estimator.SummaryType.IMAGES)

gan_estimator.train(input_fn, max_steps=1000)
```

The environment I am running on is:

Ubuntu 16.01
Python 3.6.6
Tensorflow 1.13"
27853,Tensorflow variables not casting to ref type [BUG][TF 2.0],"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.0.0-alpha
- Python version: 3.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
**Describe the current behavior**
When passed to different Function, tf.Variable dtype doesn't cast to _ref type.
For example:
```python3
import tensorflow as tf
tf.compat.v1.disable_eager_execution()
a = tf.Variable(1.0, dtype=tf.float32)
def f(x):
  print(x.dtype)
```
Output: `<dtype: 'float32'>`
**Describe the expected behavior** (according to tf r1.13)
```python3
import tensorflow as tf
a = tf.Variable(1.0, dtype=tf.float32)
def f(x):
  print(x.dtype)
```
Output: `<dtype: 'float32_ref'>`

**Code to reproduce the issue**
Given Above

**Other info / logs**
For this reason, a lot Function Calls like `tensorflow.python.ops.gen_state_ops.assign_sub` are failing. This Leads to Failing of Keras Backend Calls, like `tensorflow.python.keras.backend.moving_average_update`, even in Graph Mode.
Traceback created after Appending `tf.compat.v1.disable_eager_execution()` to the issue https://github.com/tensorflow/tensorflow/issues/27739, shows that assign_sub is failing as Tensor Object have no assign_sub. Upon Further Investigation, this error was found here:
https://github.com/tensorflow/tensorflow/blob/0c464c70cef2369b6ef5c5e17dbd2cda2a6107fb/tensorflow/python/ops/state_ops.py#L159-L162"
27852,Problem building tensorflow on Ubuntu system with Bazel gpu flavor,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: master
- Python version: Python 3.6.7
- Installed using virtualenv? pip? conda?: NA
- Bazel version (if compiling from source): 0.23.2
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 6.5.0-2ubuntu1~18.04) 6.5.0 20181026
- CUDA/cuDNN version: 9.2
- GPU model and memory: GeForce GTX 1060 Max-Q GPU with 6GB of VRAM



Faced tensorflow compilation problem with below details

> user@user:/opt/work/work_tf/tensbazel build -j 8 --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
> Extracting Bazel installation...
> Starting local Bazel server and connecting to it...
> WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
> DEBUG: /home/user/.cache/bazel/_bazel_user/f793b012987ffd570584b4a3e97b9108/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: 
> Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
> ERROR: /home/user/.cache/bazel/_bazel_user/f793b012987ffd570584b4a3e97b9108/external/com_github_googlecloudplatform_google_cloud_cpp/google/cloud/BUILD:27:1: no such target '@bazel_tools//tools/cpp:cc_flags': target 'cc_flags' not declared in package 'tools/cpp' defined by /home/user/.cache/bazel/_bazel_user/f793b012987ffd570584b4a3e97b9108/external/bazel_tools/tools/cpp/BUILD and referenced by '@com_github_googlecloudplatform_google_cloud_cpp//google/cloud:generate_build_info'
> ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed
> INFO: Elapsed time: 43.643s
> INFO: 0 processes.
> FAILED: Build did NOT complete successfully (327 packages loaded, 8419 targets configured)
>     Fetching @grpc; fetching 18s
>     Fetching @com_google_absl; fetching 18s
>     Fetching @aws; fetching 17s
>     Fetching @llvm; fetching 15s
>     Fetching @nasm; fetching 9s


**Any other info / logs**
This problem is reported after merge ""Update Google Cloud C++ Client to the v0.8.1 release."" (commit id : 0602e366015a85899e3f2cc645667dd308f4fbb5) changes."
27850,"broadcast_to op: ""invalid shape to broadcast"" on 6D tensors","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: x
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: CUDA 10, cudnn 7.5
- GPU model and memory: Pascal Xp

**Describe the current behavior**

```python
a = tf.zeros([2, 100, 32, 32, 3])
b = tf.broadcast_to( tf.expand_dims(a, 1), [2, 10, 100, 32, 32, 3] )

sess = tf.InteractiveSession()     # or whatever session
sess.run(b).shape
```

gives the error:

```
InvalidArgumentError (see above for traceback): invalid shape to broadcast from [2,1,100,32,32,3] to [2,10,100,32,32,3]
         [[node BroadcastTo_1 (defined at <ipython-input-10-49d2a4dce7c3>:1) ]]
```

**Describe the expected behavior**

Should run normally, giving the `[2, 10, 100, 32, 32, 3]` tensor.



**Other Information**

It works for 5D tensor.

```python
>>> a = tf.zeros([2,100,32,32])
>>> b = tf.broadcast_to( tf.expand_dims(a, 1), [2,10,100,32,32] )

>>> sess.run(b).shape
(2, 10, 100, 32, 32)
```"
27848,Error filename logging with tf.logging.warn,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7.0



You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
stdout prints 
```
tf_logging.py:warn - test warn
<stdin>:<module> - test warning
```
**Describe the expected behavior**
stdout prints
```
<stdin>:<module> - test warn
<stdin>:<module> - test warning
```
**Code to reproduce the issue**
``` python
import logging
import sys
h = logging.StreamHandler(sys.stdout)
formatter = logging.Formatter(""%(filename)s:%(funcName)s - %(message)s"")
h.setFormatter(formatter)
log = logging.getLogger()
log.addHandler(h)
import tensorflow as tf
tf.logging.warn(""test warn"")
tf.logging.warning(""test warning"")
```
**Other info / logs**
since the _logging.warn_ of python3 is the wrapper of _logging.warning_, code is [here](https://github.com/python/cpython/blob/master/Lib/logging/__init__.py#L1459), 
this adds one frame making the [get_caller(4)](https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/python/platform/tf_logging.py#L49) of tf.logging miss the _warn_ method.
"
27847,BUG: tfdbg session cannot be used with SessionRunHooks,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ArchLinux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):b'v1.13.0-rc2-5-g6612da8' 1.13.1
- Python version:3.7
- Bazel version (if compiling from source):n/a
- GCC/Compiler version (if compiling from source):n/a
- CUDA/cuDNN version:n/a
- GPU model and memory:n/a

The following code:
```python
#-*- coding: utf-8 -*-
#File:


import numpy as np
import tensorflow as tf
from tensorflow.python import debug as tf_debug

a = tf.placeholder(tf.float32, [10])
b = a + 1
c = b * 2

class Hook(tf.train.SessionRunHook):
    def before_run(self, _):
        return tf.train.SessionRunArgs(fetches=c)

class Hook2(tf.train.SessionRunHook):
    def before_run(self, _):
        return tf.train.SessionRunArgs(fetches=b)

sess = tf.Session()
sess = tf_debug.LocalCLIDebugWrapperSession(sess)

class SessionCreator():
    def create_session(self):
        return sess
final_sess = tf.train.MonitoredSession(session_creator=SessionCreator(), hooks=[Hook(), Hook2()])

final_sess.run(b, feed_dict={a:np.arange(10)})
```

Throws:
```
Traceback (most recent call last):
  File ""tfdbg.py"", line 30, in <module>
    final_sess.run(b, feed_dict={a:np.arange(10)})
  File ""/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 676, in run                                                                            
    run_metadata=run_metadata)
  File ""/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1171, in run                                                                           
    run_metadata=run_metadata)
  File ""/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1270, in run                                                                           
    raise six.reraise(*original_exc_info)
  File ""/usr/lib/python3.7/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1255, in run                                                                           
    return self._sess.run(*args, **kwargs)
  File ""/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1327, in run                                                                           
    run_metadata=run_metadata)
  File ""/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1091, in run                                                                           
    return self._sess.run(*args, **kwargs)
  File ""/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/debug/wrappers/framework.py"", line 463, in run                                                                              
    empty_fetches = not nest.flatten(fetches)
  File ""/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 2156, in Flatten                                                                       
    return _pywrap_tensorflow_internal.Flatten(nested)
TypeError: '<' not supported between instances of 'Hook' and 'str'
```

I believe this issue was introduced in https://github.com/tensorflow/tensorflow/commit/1f26c65254268730b7409f517d1ed1b554d01e50 a year ago. `flatten` cannot handle fetches created by hooks.
The fix will be to obtain `empty_fetches` in a smarter way."
27846,quantize_graph incompatible shapes,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 9 cuDNN 7.1
- GPU model and memory:GTX 1080ti


**Source code:**

```
 if FLAGS.base_architecture not in ['resnet_v2_50', 'resnet_v2_101']:
  raise ValueError(""'base_architrecture' must be either 'resnet_v2_50' or 'resnet_v2_101'."")

  if FLAGS.base_architecture == 'resnet_v2_50':
  base_model = resnet_v2.resnet_v2_50
  else:
  base_model = resnet_v2.resnet_v2_101

  with tf.contrib.slim.arg_scope(resnet_v2.resnet_arg_scope(batch_norm_decay=_BATCH_NORM_DECAY)):
  logits, end_points = base_model(inputs,
              num_classes=None,
              is_training=is_train,
              global_pool=False,
              output_stride=FLAGS.output_stride) # output_stride=16
 inputs_size = tf.shape(inputs)[1:3]
  net = end_points[ FLAGS.base_architecture + '/block4']# [None, 16, 16, 2048]
```


**problem description:**
I was trying to use tf.contrib.quantize.experimental_create_training_graph to conduct fake quantize training, then convert to tflite model. Firstly, i load the resnet_v2_101 pre-trained model (output_stride is set to 16), forward-pass is ok, then when i use  tf.contrib.quantize.experimental_create_training_graph to create fake quantize nodes, the error happened:

**log:**
  File ""D:/tensorflow_project/pocketflow-deeplabv3/nets/deeplabv3_at_pascal2012_run.py"", line 72, in <module>
    tf.app.run()
  File ""D:\anaconda\install_path\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:/tensorflow_project/pocketflow-deeplabv3/nets/deeplabv3_at_pascal2012_run.py"", line 53, in main
    learner = create_learner(sm_writer, model_helper)
  File ""D:\tensorflow_project\pocketflow-deeplabv3\learners\learner_utils.py"", line 60, in create_learner
    learner = UniformQuantTFLearner(sm_writer, model_helper)
  File ""D:\tensorflow_project\pocketflow-deeplabv3\learners\uniform_quantization_tf\learner.py"", line 98, in __init__
    self.__build_train()
  File ""D:\tensorflow_project\pocketflow-deeplabv3\learners\uniform_quantization_tf\learner.py"", line 199, in __build_train
    scope=self.model_scope_quan)   # rewrite the graph
  File ""D:\anaconda\install_path\envs\tensorflow\lib\site-packages\tensorflow\contrib\quantize\python\quantize_graph.py"", line 197, in experimental_create_training_graph
    scope=scope)
  File ""D:\anaconda\install_path\envs\tensorflow\lib\site-packages\tensorflow\contrib\quantize\python\quantize_graph.py"", line 70, in _create_graph
    is_training=is_training)
  File ""D:\anaconda\install_path\envs\tensorflow\lib\site-packages\tensorflow\contrib\quantize\python\fold_batch_norms.py"", line 53, in FoldBatchNorms
    graph, is_training, freeze_batch_norm_delay=freeze_batch_norm_delay)
  File ""D:\anaconda\install_path\envs\tensorflow\lib\site-packages\tensorflow\contrib\quantize\python\fold_batch_norms.py"", line 136, in _FoldFusedBatchNorms
    match.output_tensor)
  File ""D:\anaconda\install_path\envs\tensorflow\lib\site-packages\tensorflow\contrib\quantize\python\common.py"", line 157, in RerouteTensor
    c._update_input(i, t0)  # pylint: disable=protected-access
  File ""D:\anaconda\install_path\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 2061, in _update_input
    self._tf_input(index))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot update edge, incompatible shapes: [?,8,8,512] and [?,16,16,512].


"
27845,Wrong derivatives for complex second order derivatives.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tensorflow==1.12.0
- Python version: 3.6.8

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

Derivatives of non-holomorphic functions are incorrect when compared both against AD and finite differences.

**Describe the expected behavior**

Derivatives of non-holomorphic functions should becorrect.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

``` python
import numpy as onp
import autograd as ag
import autograd.numpy as anp
import numpy as onp
import tensorflow as tf

inp = anp.array(2.0)

print(""input"", inp)

def ag_fn(x):
    real = anp.cos(x+2)
    imag = anp.sin(x-1)
    return anp.abs(real+1j*imag)

ag_hess = ag.hessian(ag_fn)

print(""ag val:"", ag_fn(inp))
print(""ag hess:"", ag_hess(inp))

def tf_fn(x):
    real = tf.cos(x+2)
    imag = tf.sin(x-1)
    return tf.abs(tf.complex(real, imag))

# tf_inp = tf.convert_to_tensor(inp)
tf_inp = tf.placeholder(shape=tuple(), dtype=onp.float64)

out_op = tf_fn(tf_inp)

tf_grad = tf.gradients(out_op, tf_inp)[0]
tf_hess = tf.hessians(out_op, tf_inp)[0]

sess = tf.Session()
delta = 1e-7

_, d0, tf_ad = sess.run([out_op, tf_grad, tf_hess], feed_dict={tf_inp: inp})
_, d1, _ = sess.run([out_op, tf_grad, tf_hess], feed_dict={tf_inp: inp+delta})

print(""tf_numerical derivative:"", (d1-d0)/delta)
print(""tf_autodiff derivative:"", tf_ad)
```

```
input 2.0
ag val: 1.0655155566059393
ag hess: -0.25533014019223726
2019-04-14 22:55:43.481283: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
tf_numerical derivative: -0.25533013481293665
tf_autodiff derivative: -1.0655155566059389
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Additional information: https://github.com/google/jax/issues/603"
27844,quantization with multiple gpu training,"when i try to quantization with multiple gpu training,   a error happened

KeyError: ""The name 'tower_0/gradients/tower_0/MobileNet/conv_ds_13/pointwise_conv/BatchNorm/batchnorm_1/Rsqrt' refers to an Operation not in the graph.""

my multiple gpu code
 with tf.variable_scope(tf.get_variable_scope()):
            for i in range(2):
                with tf.device('/gpu:%d' % i):
                    with tf.name_scope('tower_%d' % i) as scope1:
                        X = x[int(i*BATCH_SIZE*0.5):int((i+1)*BATCH_SIZE*0.5)]
                        Y = y_[int(i*BATCH_SIZE*0.5):int((i+1)*BATCH_SIZE*0.5)]

                        with slim.arg_scope(mobilenet.mobilenet_arg_scope(weight_decay=0.00004)):
                            logits, end_points = mobilenet.mobilenetv1(
                                inputs=X,
                                num_classes=136,
                                is_training=is_training,
                                width_multiplier=0.5,
                                scope='MobileNet'
                            )

                        with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):
                               tf.contrib.quantize.create_training_graph(quant_delay=get_quant_delay())

                        loss = tf.losses.huber_loss(labels=Y, predictions=logits, delta=1.0, scope=scope1)
                        tf.get_variable_scope().reuse_variables()

                        if QUANTIZE:
                            tf.contrib.quantize.create_training_graph(quant_delay=get_quant_delay())

                        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)

                        with tf.control_dependencies(update_ops):
                            grads = train_steps.compute_gradients(loss)
                            tower_grads.append(grads)

        grads = average_gradients(tower_grads)
        train_op = train_steps.apply_gradients(grads, global_step=global_step)

"
27843,ImportError: cannot import name 'abs',"os: ubuntu 16.04 LTS
processor : Intel Xeon(R) CPU E5-1620 v4 @ 3.50GHz  8 
GPU : GeForce GTX 1080 Ti/PCIe/SSE2 
python : 3.6 
tensorflow 1.13.1

https://github.com/hellochick/PSPNet-tensorflow

runfile('/data/segmentation-master/run_on_sequence.py', wdir='/data/segmentation-master')
Traceback (most recent call last):

  File ""<ipython-input-3-914d8c6a3fac>"", line 1, in <module>
    runfile('/data/segmentation-master/run_on_sequence.py', wdir='/data/segmentation-master')

  File ""/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/spyder_kernels/customize/spydercustomize.py"", line 786, in runfile
    execfile(filename, namespace)

  File ""/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/spyder_kernels/customize/spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""/data/segmentation-master/run_on_sequence.py"", line 3, in <module>
    import tensorflow as tf

  File ""/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import

  File ""/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 88, in <module>
    from tensorflow.python import keras

  File ""/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/__init__.py"", line 24, in <module>
    from tensorflow.python.keras import activations

  File ""/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/activations/__init__.py"", line 22, in <module>
    from tensorflow.python.keras._impl.keras.activations import elu

  File ""/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/__init__.py"", line 21, in <module>
    from tensorflow.python.keras._impl.keras import activations

  File ""/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/activations.py"", line 23, in <module>
    from tensorflow.python.keras._impl.keras import backend as K

  File ""/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py"", line 37, in <module>
    from tensorflow.python.layers import base as tf_base_layers

  File ""/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 25, in <module>
    from tensorflow.python.keras.engine import base_layer

  File ""/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/__init__.py"", line 23, in <module>
    from tensorflow.python.keras.engine.base_layer import InputSpec

  File ""/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 35, in <module>
    from tensorflow.python.keras import backend

  File ""/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/backend/__init__.py"", line 22, in <module>
    from tensorflow.python.keras._impl.keras.backend import abs

ImportError: cannot import name 'abs'



"
27842,2.0 alpha build from source version failed when call model.fit(),"**System information**
- OS: windows10
- TensorFlow installed from source
- TensorFlow version: 2.0.0-alpha0
- Python version: 3.6.6
- Installed using virtualenv? pip? conda?: venv
- Bazel version (if compiling from source): Bazel 22.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: GTX 1050 4G



**Describe the problem**

I had compiled TensorFlow 2.0.0-alpha0 GPU version on windows 10. It said build is completely sucessful. I installed the whl file to my python virtual environment. The print(tf.__version__) can print correct version info. But when I run this simple program it failed unexpectly.
The simple program as below:
`
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

celsius_q = np.array([-40, -10, 0, 8, 15, 22, 38], dtype=float)
fahrenfeit_a = np.array([-40, 14, 32, 46, 59, 72, 100], dtype=float)
for i, c in enumerate(celsius_q):
    print('{0}C = {1}F'.format(c, fahrenfeit_a[i]))
    
layer1 = tf.keras.layers.Dense(units=1, input_shape=[1])
model = tf.keras.Sequential([layer1])
model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.1))
history = model.fit(celsius_q, fahrenfeit_a, epochs=500, verbose=False)
`

**Any other info / logs**
The error log as below:
`

(tfg) (base) D:\adb>python tf2_demo.py
-40.0C = -40.0F
-10.0C = 14.0F
0.0C = 32.0F
8.0C = 46.0F
15.0C = 59.0F
22.0C = 72.0F
38.0C = 100.0F
WARNING: Logging before flag parsing goes to stderr.
W0415 09:24:58.782853  1912 deprecation.py:506] From D:\abiz\mnf\dev\tfg\lib\site-packages\tensorflow\python\ops\init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
2019-04-15 09:24:58.942776: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll
2019-04-15 09:24:59.943075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties:
name: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493
pciBusID: 0000:01:00.0
totalMemory: 4.00GiB freeMemory: 3.30GiB
2019-04-15 09:24:59.948936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0
2019-04-15 09:25:00.987639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-04-15 09:25:00.991733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0
2019-04-15 09:25:00.993541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N
2019-04-15 09:25:00.995511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3011 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)                                                                                2019-04-15 09:25:11.523151: W tensorflow/core/common_runtime/bfc_allocator.cc:288] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.68GiB.  Current allocation summary follows.
2019-04-15 09:25:11.528008: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (256):   Total Chunks: 36, Chunks in use: 34. 9.0KiB allocated for chunks. 8.5KiB in use in bin. 232B client-requested in use in bin.
2019-04-15 09:25:11.533611: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (512):   Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-04-15 09:25:11.538796: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (1024):  Total Chunks: 1, Chunks in use: 1. 1.3KiB allocated for chunks. 1.3KiB in use in bin. 1.0KiB client-requested in use in bin.                    2019-04-15 09:25:11.545015: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (2048):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-04-15 09:25:11.549985: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (4096):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-04-15 09:25:11.555897: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (8192):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-04-15 09:25:11.562697: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (16384):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.                        2019-04-15 09:25:11.568183: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (32768):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-04-15 09:25:11.577286: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (65536):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-04-15 09:25:11.584167: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (131072):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-04-15 09:25:11.594069: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (262144):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-04-15 09:25:11.600881: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (524288):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-04-15 09:25:11.608271: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (1048576):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-04-15 09:25:11.615063: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (2097152):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-04-15 09:25:11.622202: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (4194304):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-04-15 09:25:11.629692: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (8388608):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-04-15 09:25:11.636440: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (16777216):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-04-15 09:25:11.643003: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (33554432):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-04-15 09:25:11.649099: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (67108864):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-04-15 09:25:11.655134: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (134217728):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-04-15 09:25:11.663212: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (268435456):     Total Chunks: 1, Chunks in use: 0. 2.94GiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-04-15 09:25:11.668643: I tensorflow/core/common_runtime/bfc_allocator.cc:649] Bin for 5.68GiB was 256.00MiB, Chunk State:
2019-04-15 09:25:11.675501: I tensorflow/core/common_runtime/bfc_allocator.cc:655]   Size: 2.94GiB | Requested Size: 4B | in_use: 0, prev:   Size: 256B | Requested Size: 28B | in_use: 1
2019-04-15 09:25:11.679701: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400000 of size 1280
2019-04-15 09:25:11.683459: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400500 of size 256
2019-04-15 09:25:11.689566: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400600 of size 256
2019-04-15 09:25:11.693736: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400700 of size 256
2019-04-15 09:25:11.696694: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400800 of size 256
2019-04-15 09:25:11.700230: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400900 of size 256
2019-04-15 09:25:11.703917: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400A00 of size 256
2019-04-15 09:25:11.707864: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400B00 of size 256
2019-04-15 09:25:11.711760: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400C00 of size 256
2019-04-15 09:25:11.717932: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400D00 of size 256
2019-04-15 09:25:11.723400: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400E00 of size 256
2019-04-15 09:25:11.727774: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400F00 of size 256
2019-04-15 09:25:11.731836: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401000 of size 256
2019-04-15 09:25:11.735367: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401100 of size 256
2019-04-15 09:25:11.741123: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401200 of size 256
2019-04-15 09:25:11.745619: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401300 of size 256
2019-04-15 09:25:11.749705: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401400 of size 256
2019-04-15 09:25:11.753128: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401500 of size 256
2019-04-15 09:25:11.758698: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401600 of size 256
2019-04-15 09:25:11.762093: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401700 of size 256
2019-04-15 09:25:11.766516: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401800 of size 256
2019-04-15 09:25:11.774200: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401900 of size 256
2019-04-15 09:25:11.780786: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401A00 of size 256
2019-04-15 09:25:11.785347: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401B00 of size 256
2019-04-15 09:25:11.793098: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401C00 of size 256
2019-04-15 09:25:11.799916: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401D00 of size 256
2019-04-15 09:25:11.804645: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401E00 of size 256
2019-04-15 09:25:11.812418: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401F00 of size 256
2019-04-15 09:25:11.819120: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402000 of size 256
2019-04-15 09:25:11.824766: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402100 of size 256
2019-04-15 09:25:11.828505: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402200 of size 256
2019-04-15 09:25:11.833054: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402300 of size 256
2019-04-15 09:25:11.841134: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Free  at 0000000502402400 of size 256
2019-04-15 09:25:11.848579: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402500 of size 256
2019-04-15 09:25:11.855357: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Free  at 0000000502402600 of size 256
2019-04-15 09:25:11.860341: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402700 of size 256
2019-04-15 09:25:11.867202: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402800 of size 256
2019-04-15 09:25:11.874567: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Free  at 0000000502402900 of size 3157422080
2019-04-15 09:25:11.879528: I tensorflow/core/common_runtime/bfc_allocator.cc:677]      Summary of in-use Chunks by size:
2019-04-15 09:25:11.883032: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 34 Chunks of size 256 totalling 8.5KiB
2019-04-15 09:25:11.887446: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 1280 totalling 1.3KiB
2019-04-15 09:25:11.895103: I tensorflow/core/common_runtime/bfc_allocator.cc:684] Sum Total of in-use chunks: 9.8KiB
2019-04-15 09:25:11.901440: I tensorflow/core/common_runtime/bfc_allocator.cc:686] Stats:
Limit:                  3157432729
InUse:                        9984
MaxInUse:                    10752
NumAllocs:                      44
MaxAllocSize:                 1280

2019-04-15 09:25:11.916657: W tensorflow/core/common_runtime/bfc_allocator.cc:292] *___________________________________________________________________________________________________
2019-04-15 09:25:11.923443: W tensorflow/core/framework/op_kernel.cc:1431] OP_REQUIRES failed at dynamic_stitch_op.cc:132 : Resource exhausted: OOM when allocating tensor with shape[1525343649] and type int32 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File ""tf2_demo.py"", line 13, in <module>
    history = model.fit(celsius_q, fahrenfeit_a, epochs=500, verbose=False)
  File ""D:\abiz\mnf\dev\tfg\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 873, in fit
    steps_name='steps_per_epoch')
  File ""D:\abiz\mnf\dev\tfg\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 352, in model_iteration
    batch_outs = f(ins_batch)
  File ""D:\abiz\mnf\dev\tfg\lib\site-packages\tensorflow\python\keras\backend.py"", line 3096, in __call__
    run_metadata=self.run_metadata)
  File ""D:\abiz\mnf\dev\tfg\lib\site-packages\tensorflow\python\client\session.py"", line 1440, in __call__
    run_metadata_ptr)
  File ""D:\abiz\mnf\dev\tfg\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 548, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1525343649] and type int32 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[{{node training/Adam/gradients/loss/dense_loss/mean_squared_error/Mean_grad/DynamicStitch}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[GroupCrossDeviceControlEdges_0/training/Adam/Adam/Const/_72]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


(tfg) (base) D:\adb>import numpy as npimport matplotlib.pyplot as pltimport tensorflow as tfcelsius_q = np.array([-40, -10, 0, 8, 15, 22, 38], dtype=float)fahrenfeit_a = np.array([-40, 14, 32, 46, 59, 72, 100], dtype=float)for i, c in enumerate(celsius_q):    print('{0}C = {1}F'.format(c, fahrenfeit_a[i]))    layer1 = tf.keras.layers.Dense(units=1, input_shape=[1])model = tf.keras.Sequential([layer1])model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.1))history = model.fit(celsius_q, fahrenfeit_a, epochs=500, verbose=False)
`
By the way this simple program I run successfully in TensorFlow 2.0.0-alpha0 binary version.
"
27841,Cannot initialize two different models with the same checkpoint.,"I'm trying to use TF Slim ResNet-v1-101 pre-trained model to initialize two input branches of model, with different inputs (RGB features[0] and depth map features[1] respectively) to further combine them and feed their joint result to some spatial convolution layers. In order to achieve that, I create separate scope for each of the branches and initialize the models with pre-trained checkpoint, excluding the last fully connected layer (`logits`) :

**System information**
- OS Platform and Distribution : Linux Ubuntu 16.04:
- TensorFlow version 1.13:
- Python version: 3.5
- CUDA/cuDNN version: 10.1
- GPU model and memory: GV100 32GB

**Describe the current behavior**
```
with tf.variable_scope(""rgb_branch""):
    with tf.contrib.slim.arg_scope(resnet_v1.resnet_arg_scope()):
        rgb_logits, end_points = resnet_v1.resnet_v1_101(features[0], self.num_classes, is_training=is_training)        

        rgb_variables_to_restore = tf.contrib.slim.get_variables_to_restore(exclude=['rgb_branch/resnet_v1_101/logits'])
                
        # strip scope name
        rgb_assignment_map = { rgb_variables_to_restore[0].name.split(':')[0] : rgb_variables_to_restore[0]}
        rgb_assignment_map.update({ v.name.split(':')[0].split('/', 1)[1] : v for v in rgb_variables_to_restore[1:] })
      
        tf.train.init_from_checkpoint(self.pre_trained_model_path, rgb_assignment_map)
        
        
with tf.variable_scope(""depth_branch""):
    with tf.contrib.slim.arg_scope(resnet_v1.resnet_arg_scope()):     
        depth_logits, end_points = resnet_v1.resnet_v1_101(features[1], self.num_classes, is_training=is_training)

        depth_variables_to_restore = tf.contrib.slim.get_variables_to_restore(exclude=['depth_branch/resnet_v1_101/logits'])
                
        depth_assignment_map = { depth_variables_to_restore[0].name.split(':')[0] : depth_variables_to_restore[0]}
        depth_assignment_map.update({ v.name.split(':')[0].split('/', 1)[1] : v for v in depth_variables_to_restore[1:] })
      
        tf.train.init_from_checkpoint(self.pre_trained_model_path, depth_assignment_map)
```
During the second initialization (the depth branch), TF complains about shape of the logits layer as if it was never removed:

`ValueError: Shape of variable rgb_branch/resnet_v1_101/logits/biases:0 ((3,)) doesn't match with shape of tensor resnet_v1_101/logits/biases ([1000]) from checkpoint reader.`

The problem doesn't occur when I initialize only one branch, and is tied only to the first of the branches defined - if I change the order of the branches, the above error would change to `depth_branch/resnet_v1_101/logits/biases:0`

**Describe the expected behavior**

Expected - have two separate graphs, both initialized by the same checkpoint"
27840,Proper way to build Tensorflow for arbitrary CUDA Compute Capability,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **docker image** (_tensorflow/tensorflow:devel-gpu-py3_) for building tensorflow
- TensorFlow version: **1.13.0**
- Python version: **3.5**
- Installed using virtualenv? pip? conda?: **docker image**
- Bazel version (if compiling from source): **0.23.2** _(default for the docker image)_
- GCC/Compiler version (if compiling from source): **5.4.0 20160609** _(default for the docker image)_
- CUDA/cuDNN version: **10.0**
- GPU model and memory: [NVIDIA QUADRO K4200](https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/quadro-product-literature/DS-NV-Quadro-K4200-JUL24-US-NV-r-HR.pdf)



**Describe the problem**

Following the [build instructions for docker with GPU support with CUDA](https://www.tensorflow.org/install/source#gpu_support_2) leads to a pip wheel with support for CUDA Compute Capability 3.5, 5.2, 6.0, 6.1, 7.0.

However, the GPU I am trying to set up Tensorflow for supports only CUDA Compute Capability 3.0. Going through the documentation I did not manage to find a clear instruction on how to enable support for non-default CUDA Compute Capability during the build process. Despite trying a couple of different approaches, I always got the following error:

```python
2019-04-14 22:58:01.822303: I tensorflow/compiler/xla/service/platform_util.cc:197] StreamExecutor cuda device (0) is of insufficient compute capability: 3.5 required, device is 3.0
```

when trying to execute the following command:
```python
python -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I tried to edit `/tensorflow_src/.tf_configure.bazelrc` in the docker container by having `TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""` before building, but this did not solve the problem.
I set the respective shell variable `TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""` before configuring with bazel, but this also did not result in a working image.


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27839,"com.ai.tensorflow A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x28 in tid 17491 (m.ai.tensorflow)","<em>it has a crash in tensorflow lite android App
 **--------- beginning of crash
com.ai.tensorflow A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x28 in tid 17491
 (m.ai.tensorflow)**
</em>

**System information**
- Have I use tensorflow demo app
**tensorflow/tensorflow/lite/examples/android/**
 [lite android](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/android)
- the issue happens on mobile device:android 5.0.2



** How this error is happen**
TFLiteObjectDetectionAPIModel.java

1.`tfLite = new Interpreter(loadModelFile(assetManager, modelFilename));`
after get the tfLite, and then to detect the image
2.when it wants to stop the tflite by:
`tfLite.close();`

3.if it's running image detect by : `tfLite.runForMultipleInputsOutputs(inputArray, outputMap);`
  and this timing I call the`tfLite.close();`
then this error happened

**all the error information**
04-15 03:34:43.581 17441-17491/com.ai.tensorflow A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x28 in tid 17491 (m.ai.tensorflow)
04-15 03:34:43.586 17441-17488/com.ai.tensorflow A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x28 in tid 17488 (m.ai.tensorflow)
04-15 03:34:43.596 17441-17490/com.ai.tensorflow A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x10 in tid 17490 (m.ai.tensorflow)
04-15 03:34:43.685 11849-11849/? I/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
04-15 03:34:43.685 11849-11849/? I/DEBUG: Revision: '0'
04-15 03:34:43.685 11849-11849/? I/DEBUG: ABI: 'arm'
04-15 03:34:43.686 11849-11849/? I/DEBUG: pid: 17441, tid: 17491, name: m.ai.tensorflow  >>> com.ai.tensorflow <<<
04-15 03:34:43.686 11849-11849/? I/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x28
04-15 03:34:43.708 11849-11849/? I/DEBUG:     r0 00000028  r1 0000000c  r2 0000000f  r3 00000000
04-15 03:34:43.709 11849-11849/? I/DEBUG:     r4 0000000f  r5 b908ef30  r6 9f26b1f4  r7 9f565cf8
04-15 03:34:43.709 11849-11849/? I/DEBUG:     r8 b910d888  r9 0000000c  sl 00000001  fp 00000000
04-15 03:34:43.710 11849-11849/? I/DEBUG:     ip 00000001  sp 9f565cc0  lr a42ee2af  pc a42ed98c  cpsr 200d0030
04-15 03:34:43.710 11849-11849/? I/DEBUG: backtrace:
04-15 03:34:43.710 11849-11849/? I/DEBUG:     #00 pc 0005198c  /data/app/com.ai.tensorflow-1/lib/arm/libtensorflowlite_jni.so
04-15 03:34:43.710 11849-11849/? I/DEBUG:     #01 pc 000522ab  /data/app/com.ai.tensorflow-1/lib/arm/libtensorflowlite_jni.so
04-15 03:34:43.710 11849-11849/? I/DEBUG:     #02 pc 000d6bd3  /data/app/com.ai.tensorflow-1/lib/arm/libtensorflowlite_jni.so
04-15 03:34:43.710 11849-11849/? I/DEBUG:     #03 pc 000f5a7f  /data/app/com.ai.tensorflow-1/lib/arm/libtensorflowlite_jni.so
04-15 03:34:43.710 11849-11849/? I/DEBUG:     #04 pc 00012ff7  /system/lib/libc.so (__pthread_start(void*)+30)
04-15 03:34:43.710 11849-11849/? I/DEBUG:     #05 pc 00010fcf  /system/lib/libc.so (__start_thread+6)


"
27836,XLA: specify num_partitions in xla_cpu_runtime_ParallelForkJoin,"I am interested in the LLVM IR obtained by launching a Tensorflow test program with the `--xla_dump_ir_to` flag. In particular, I would like XLA to create a certain number of copies of a function that can run in parallel on multiple CPUs. 
(I know this sounds like a support question rather than an issue, but nobody could answer me on StackOverflow until now, so I figured that maybe what I am looking for is a feature that still does not exist)

I have a test program that includes these lines
`with tf.device(""device:XLA_CPU:0""):`
`       y = tf.nn.conv2d(X, weights_tensor, strides=[1, 1, 1, 1], padding='SAME')`
`    sess.run(tf.global_variables_initializer())`

And in the output `.ll` files XLA generates a call to
`call void @__xla_cpu_runtime_ParallelForkJoin(i8* %6, i8* %run_options, i8** null, i8** %buffer_table, i64* %prof_counters, i32 56, i64* getelementptr inbounds ([224 x i64], [224 x i64]* @parallel_convolution_parallel_dimension_partitions, i32 0, i32 0), i32 2, i8* bitcast (void (i8*, i8*, i8**, i8**, i64*, i64*)* @parallel_convolution to i8*))`
which is a good starting point.

But now I want to specify how many copies of that function I want. What I have tried up to now is to use these options
`config = tf.ConfigProto()`
`config.device_count={""CPU"": 5}`
`config.intra_op_parallelism_threads=5`
`config.inter_op_parallelism_threads=5`
and they still lead to the same call, with the `num_partitions` argument set to 56 (number of CPUs on my machine).

Moreover, a different test program with
`with tf.device(""device:XLA_CPU:0""):`
`                y=tf.layers.max_pooling2d(inputs=x, pool_size=[3, 3], strides=3)`
`        sess.run(y,{x: reshaped_input_array})`
does not generate any call to `xla_cpu_runtime_ParallelForkJoin` at all.

Is it possible to force XLA in any way to produce parallel copies of a function, and to specify how many copies?


### System information
- OS Platform and Distribution: Linux 4.19.0-4-amd64 #1 SMP Debian 4.19.28-2 (2019-03-15)
- TensorFlow installed from: source
- TensorFlow version: 1.13.1
- Python version: 2.7.16
- Bazel version: 0.21.0
- GCC version: 8.3.0
"
27833,Problem building libtensorflow.so on Debian system with Bazel,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Stretch (9.8, latest)
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.13.1
- Python version: Python 2.7.13
- Installed using virtualenv? pip? conda?: Official Debian repository
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516
- GPU model and memory: Intel HD 4000

I am trying to compile Tensorflow release 1.13 from source in order to obtain the shared library libtensorflow.so using bazel. I do this by invoking
`bazel build //tensorflow:libtensorflow.so`

After some time the build stops with the following error message
`ERROR: ~/tensorflow/tensorflow_r1.13.1/tensorflow/core/kernels/BUILD:4312:1: C++ compilation of rule '//tensorflow/core/kernels:sparse_reduce_op' failed (Exit 1)
In file included from tensorflow/core/kernels/sparse_reduce_op.cc:25:0:
./tensorflow/core/util/sparse/sparse_tensor.h: In static member function 'static tensorflow::Status tensorflow::sparse::SparseTensor::Create(tensorflow::Tensor, tensorflow::Tensor, tensorflow::sparse::SparseTensor::VarDimArray, tensorflow::sparse::SparseTensor::VarDimArray, tensorflow::sparse::SparseTensor*)':
./tensorflow/core/util/sparse/sparse_tensor.h:68:22: warning: 'dims' may be used uninitialized in this function [-Wmaybe-uninitialized]
     if (order.size() != dims) {
         ~~~~~~~~~~~~~^~~~~~~
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:124:0,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/numeric_types.h:20,
                 from ./tensorflow/core/framework/allocator.h:23,
                 from ./tensorflow/core/framework/op_kernel.h:23,
                 from tensorflow/core/kernels/sparse_reduce_op.cc:20:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h: In static member function 'static void std::_Function_handler<void(_ArgTypes ...), _Functor>::_M_invoke(const std::_Any_data&, _ArgTypes&& ...) [with _Functor = Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable, Tileable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<std::complex<float>, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::DimensionList<long int, 1ul>, const Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 1, 1, long int>, 0, Eigen::MakePointer>, Eigen::MakePointer> >; bool Vectorizable = true; bool Tileable = false]::<lambda(Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<std::complex<float>, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::DimensionList<long int, 1ul>, const Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 1, 1, long int>, 0, Eigen::MakePointer>, Eigen::MakePointer> >, Eigen::ThreadPoolDevice, true, false>::StorageIndex, Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<std::complex<float>, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::DimensionList<long int, 1ul>, const Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 1, 1, long int>, 0, Eigen::MakePointer>, Eigen::MakePointer> >, Eigen::ThreadPoolDevice, true, false>::StorageIndex)>; _ArgTypes = {long int, long int}]':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:801:9: internal compiler error: in emit_move_insn, at expr.c:3547
         values[i] = internal::InnerMostDimReducer<Self, Op>::reduce(*this, firstIndex + i * num_values_to_reduce,
         ^~~~~~
`
"
27832,"Converting unsupported operations -  Enter, Exit, Merge, Switch","**System information**
- OS Platform and Distribution : Ubuntu 16.04.3
- TensorFlow installed from: pip
- TensorFlow version: 2.0 alpha

I am using bazel to convert .pb file to .tflite. This is the command that I used to convert the model. 

abdullah@abdullah-OptiPlex-7060:~/tensorflow/tensorflow/lite/toco$ bazel run . --define=with_select_tf_ops=true -- --output_file=/home/abdullah/Documents/CRNN_MODEL/bazelled.tflite   --input_file=/home/abdullah/Documents/CRNN_MODEL/my_model__.pb   --input_arrays=the_input   --output_arrays=softmax/truediv 

I receive the following output: 
```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FULLY_CONNECTED, LESS, LOGICAL_AND, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, RANGE, REDUCE_MAX, RESHAPE, REVERSE_V2, SHAPE, STRIDED_SLICE, SUB, SUM, TANH, TILE, TRANSPOSE, UNPACK, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: Enter, Exit, LoopCond, Merge, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3
```
When I added the  --allow_custom_ops flag to the same command above, I received the following output:
``` 
E tensorflow/lite/toco/toco_tooling.cc:456] TensorFlow Lite currently doesn't support control flow ops: Enter, Exit, Merge, Switch.
```

It is possible to avoid using those control flow ops (Enter, Exit, Merge, Switch) in my original .pb model? 
Thanks for your help.
"
27831,"Add built-in helper functions for _bytes_feature, _float_feature and _int64_feature from ""Using TFRecords and tf.Example"" page","The TF documentation page ""Using TFRecords and tf.Example"" https://www.tensorflow.org/tutorials/load_data/tf_records lists these helper functions:

```
# The following functions can be used to convert a value to a type compatible
# with tf.Example.

def _bytes_feature(value):
  """"""Returns a bytes_list from a string / byte.""""""
  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

def _float_feature(value):
  """"""Returns a float_list from a float / double.""""""
  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))

def _int64_feature(value):
  """"""Returns an int64_list from a bool / enum / int / uint.""""""
  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))
```

Searching github code shows these have been cut and pasted 3453 times into other projects:
https://github.com/search?q=_bytes_feature+_float_feature&type=Code
and presumably many more times besides.

Could TF include helper functions for these and other common tf.train.Features/Examples helpers?

**System information**
- TensorFlow version: 1.13.1
- Are you willing to contribute it: Yes (at some point)

**Describe the feature and the current behavior/state.**
Examples and Features are recommended as the canonical way to store TF datasets.  However understanding the protobufs is non-trivial: they are multiple layers deep and have a verbose API.

**Will this change the current api? How?**
This will make the API simpler and more pythonic for building usual Features and Examples.

**Who will benefit with this feature?**
All users building datasets.

**Any Other info.**
"
27830,tf-nightly-gpu 1.14.1 failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error,"My Python version is 3.7.1, the operating system is Windows 10, the CUDA version is cuda_10.0.130_411.31_win10, the cudnn version is cudnn-10.0-windows 10-x64-v7.4.2.24, the GPU is Intel (R) UHD Graphics 620, and the tf-nightly-gpu-1.14.1.20190323 is installed.
When I run `import tensorflow as tf`, there is no problem.
When I run:
`m1 = tf.constant([[3, 3]])`
`m2 = tf.constant([[2], [2]])`
`p = tf.matmul(m1, m2)`
`sess =  tf.Session()`
`r = sess.run(p)`
there is no variable ""r"" in my variable explorer.
When I run:
`print(p)`
I get this:
![QQ20190414154532 (2)](https://user-images.githubusercontent.com/38285222/56089888-83422400-5ecc-11e9-9d7a-5ff8bee54cf4.jpg)
What should I do? Thanks!"
27829,Cannot create a stateful RNN with recurrent dropout,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOSX 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
tf.version.VERSION=2.0.0-dev20190413
tf.version.GIT_VERSION=v1.12.0-12481-gc7ce6f4cd9
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
I get an exception when trying to use `recurrent_dropout` in a stateful RNN:

```
.../tensorflow/python/ops/resource_variable_ops.py in __imul__(self, unused_other)
   1449
   1450   def __imul__(self, unused_other):
-> 1451     raise RuntimeError(""Variable *= value not supported. Use ""
   1452                        ""`var.assign(var * value)` to modify the variable or ""
   1453                        ""`var = var * value` to get a new Tensor object."")

RuntimeError: Variable *= value not supported. Use `var.assign(var * value)` to modify the variable or `var = var * value` to get a new Tensor object.
```

The full stacktrace is below.

**Describe the expected behavior**
No exception.

**Code to reproduce the issue**

```python
from tensorflow import keras

model = keras.models.Sequential([
    keras.layers.GRU(128, return_sequences=True, stateful=True,
                     batch_input_shape=[32, None, 5],
                     recurrent_dropout=0.2)
])
```

**Other info / logs**
Complete stacktrace:

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-1-3e98e7412ec2> in <module>
      4     keras.layers.GRU(128, return_sequences=True, stateful=True,
      5                      batch_input_shape=[32, None, 5],
----> 6                      recurrent_dropout=0.2)
      7 ])

.../tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    456     self._self_setattr_tracking = False  # pylint: disable=protected-access
    457     try:
--> 458       result = method(self, *args, **kwargs)
    459     finally:
    460       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

.../tensorflow/python/keras/engine/sequential.py in __init__(self, layers, name)
    106     if layers:
    107       for layer in layers:
--> 108         self.add(layer)
    109
    110   @property

.../tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    456     self._self_setattr_tracking = False  # pylint: disable=protected-access
    457     try:
--> 458       result = method(self, *args, **kwargs)
    459     finally:
    460       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

.../tensorflow/python/keras/engine/sequential.py in add(self, layer)
    167           # and create the node connecting the current layer
    168           # to the input layer we just created.
--> 169           layer(x)
    170           set_inputs = True
    171

.../tensorflow/python/keras/layers/recurrent.py in __call__(self, inputs, initial_state, constants, **kwargs)
    620
    621     if initial_state is None and constants is None:
--> 622       return super(RNN, self).__call__(inputs, **kwargs)
    623
    624     # If any of `initial_state` or `constants` are specified and are Keras

.../tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    631                       base_layer_utils.AutoAddUpdates(self,
    632                                                       inputs)) as auto_updater:
--> 633                 outputs = call_fn(inputs, *args, **kwargs)
    634                 auto_updater.set_outputs(outputs)
    635

.../tensorflow/python/keras/layers/recurrent_v2.py in call(self, inputs, mask, training, initial_state)
    328           input_length=timesteps,
    329           time_major=self.time_major,
--> 330           zero_output_for_mask=self.zero_output_for_mask)
    331       # This is a dummy tensor for testing purpose.
    332       runtime = _runtime('unknown')

.../tensorflow/python/keras/backend.py in rnn(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)
   3558     # the value is discarded.
   3559     output_time_zero, _ = step_function(input_time_zero,
-> 3560                                         initial_states + constants)
   3561     output_ta = tuple(
   3562         tensor_array_ops.TensorArray(

.../tensorflow/python/keras/layers/recurrent_v2.py in step(cell_inputs, cell_states)
    316
    317       def step(cell_inputs, cell_states):
--> 318         return self.cell.call(cell_inputs, cell_states, **kwargs)
    319
    320       last_output, outputs, states = K.rnn(

.../tensorflow/python/keras/layers/recurrent.py in call(self, inputs, states, training)
   1706
   1707       if 0. < self.recurrent_dropout < 1.:
-> 1708         h_tm1 *= rec_dp_mask[0]
   1709
   1710       if self.reset_after:

.../tensorflow/python/ops/resource_variable_ops.py in __imul__(self, unused_other)
   1449
   1450   def __imul__(self, unused_other):
-> 1451     raise RuntimeError(""Variable *= value not supported. Use ""
   1452                        ""`var.assign(var * value)` to modify the variable or ""
   1453                        ""`var = var * value` to get a new Tensor object."")

RuntimeError: Variable *= value not supported. Use `var.assign(var * value)` to modify the variable or `var = var * value` to get a new Tensor object.
```"
27828,Build tensorflow C++ failed,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):nightly
- Python version:
- Bazel version (if compiling from source):0.24.1
- GCC/Compiler version (if compiling from source):6.3.0
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

```
ERROR: /home/fsx950223/tensorflow/tensorflow/core/kernels/BUILD:3370:1: C++ compilation of rule '//tensorflow/core/kernels:batch_matmul_op' failed (Exit 1)
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:296:0,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/kernels/batch_matmul_op_impl.h:25,
                 from tensorflow/core/kernels/batch_matmul_op_real.cc:16:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/products/GeneralBlockPanelKernel.h: In member function 'void Eigen::internal::lhs_process_one_packet<nr, LhsProgress, RhsProgress, LhsScalar, RhsScalar, ResScalar, AccPacket, LhsPacket, RhsPacket, ResPacket, GEBPTraits, LinearMapper, DataMapper>::peeled_kc_onestep(Eigen::Index, const LhsScalar*, const RhsScalar*, GEBPTraits, LhsPacket*, Eigen::internal::lhs_process_one_packet<nr, LhsProgress, RhsProgress, LhsScalar, RhsScalar, ResScalar, AccPacket, LhsPacket, RhsPacket, ResPacket, GEBPTraits, LinearMapper, DataMapper>::RhsPacketx4*, RhsPacket*, AccPacket*, AccPacket*, AccPacket*, AccPacket*) [with int nr = 4; long int LhsProgress = 1l; long int RhsProgress = 1l; LhsScalar = Eigen::half; RhsScalar = Eigen::half; ResScalar = Eigen::half; AccPacket = Eigen::half; LhsPacket = Eigen::half; RhsPacket = Eigen::half; ResPacket = Eigen::half; GEBPTraits = Eigen::internal::gebp_traits<Eigen::half, Eigen::half, false, false, 1, 0>; LinearMapper = Eigen::internal::BlasLinearMapper<Eigen::half, long int, 0>; DataMapper = Eigen::internal::blas_data_mapper<Eigen::half, long int, 0, 0>]':
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/products/GeneralBlockPanelKernel.h:1351:33: internal compiler error: in assign_temp, at function.c:961
     __asm__  ("""" : ""+x,m"" (*A0));
```

**Describe the expected behavior**
Compile success

**Code to reproduce the issue**

Provide a reproducible test case that is the bare minimum necessary to generate the problem.
``` sh
bazel build -c dbg --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/cc/example:example --local_resources 2048,.5,1.0
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27827,Eager: Network with eager execution is not learning and low performance,"**System information**
- OS Platform: Win 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): b'v1.13.1-0-g6612da8951' 1.13.1
- Python version: 3.6.7
- CUDA/cuDNN version: CUDA V10.0.130, cuDNN 7.5
- GPU model and memory: RTX 2070 8GB


**Describe the current behavior**
I've convert my keras network to tensorflow eager and this network dosn't learn anything. The network predicts only the same values. The performance is 2-3 times slower as the keras model.
With tensorflow graph without eager execution everything works fine.

**Describe the expected behavior**
I want to predict actions for various input images in reinforcement learning 

**Code to reproduce the issue**
**Keras model**

```python
from keras.layers import *
from keras.models import Model
from keras.optimizers import Adam
import tensorflow as tf


class KerasTest:
    def __init__(self, state_space, action_space, lr):
        self.state_space = state_space
        self.action_space = action_space
        self.lr = lr

        inputs = Input(shape=(84, 84, 4))
        rewards = Input(shape=(1,))

        x = Conv2D(32, kernel_size=[8, 8], padding='valid', strides=[4, 4], activation=None)(inputs)
        x = BatchNormalization(trainable=True, epsilon=1e-5)(x)
        x = Activation('relu')(x)
        x = Conv2D(64, kernel_size=[4, 4], padding='valid', strides=[2, 2], activation=None)(x)
        x = BatchNormalization(trainable=True, epsilon=1e-5)(x)
        x = Activation('relu')(x)
        x = Conv2D(64, kernel_size=(4, 4), padding='valid', strides=(2, 2), activation=None)(x)
        x = BatchNormalization(trainable=True, epsilon=1e-5)(x)
        x = Activation('relu')(x)
        x = Flatten()(x)
        x = Dense(512, activation='relu')(x)
        x = Dropout(0.5)(x)
        x = Dense(256, activation='relu')(x)
        x = Dropout(0.3)(x)
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.05)(x)
        logits = Dense(self.action_space, activation=None)(x)

        self.model = Model(inputs=[inputs, rewards], outputs=logits)

        def policy_loss(r):
            def loss(labels, logits):
                policy = tf.nn.softmax(logits)
                entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=policy, logits=logits)
                log = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)
                p_loss = log * tf.stop_gradient(r)
                p_loss = p_loss - 0.01 * entropy
                total_loss = tf.reduce_mean(p_loss)
                return total_loss

            return loss

        self.model.compile(optimizer=Adam(lr=lr), loss=policy_loss(rewards))
        self.model.summary()

    def get_probs(self, s):
        s = s[np.newaxis, :]
        probs = self.model.predict([s, np.array([1])])
        probs = probs.squeeze()
        probs = self.softmax(probs)
        return probs

    def softmax(self, x):
        e_x = np.exp(x - np.max(x))
        return e_x / e_x.sum(axis=0)

    def update_policy(self, s, r, a):
        self.model.train_on_batch([s, r], a)
```

**Eager**
```python
import tensorflow as tf
from tensorflow.python.keras.layers import *
import numpy as np

tf.enable_eager_execution()
print(tf.executing_eagerly())


class EagerTest:
    def __init__(self, state_space, action_space, lr):
        self.action_space = action_space
        self.lr = lr

        inputs = Input(shape=(84, 84, 4))
        rewards = Input(shape=(1,))

        x = Conv2D(32, kernel_size=[8, 8], padding='valid', strides=[4, 4], activation=None)(inputs)
        x = BatchNormalization(trainable=True, epsilon=1e-5)(x)
        x = Activation('relu')(x)
        x = Conv2D(64, kernel_size=[4, 4], padding='valid', strides=[2, 2], activation=None)(x)
        x = BatchNormalization(trainable=True, epsilon=1e-5)(x)
        x = Activation('relu')(x)
        x = Conv2D(64, kernel_size=(4, 4), padding='valid', strides=(2, 2), activation=None)(x)
        x = BatchNormalization(trainable=True, epsilon=1e-5)(x)
        x = Activation('relu')(x)
        x = Flatten()(x)
        x = Dense(512, activation='relu')(x)
        x = Dropout(0.5)(x)
        x = Dense(256, activation='relu')(x)
        x = Dropout(0.3)(x)
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.05)(x)
        logits = Dense(self.action_space, activation=None)(x)

        self.model = tf.keras.Model(inputs=[inputs, rewards], outputs=logits)

        def policy_loss(r):
            def loss(labels, logits):
                policy = tf.nn.softmax(logits)
                entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=policy, logits=logits)
                log = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)
                p_loss = log * tf.stop_gradient(r)
                p_loss = p_loss - 0.01 * entropy
                total_loss = tf.reduce_mean(p_loss)
                return total_loss

            return loss

        self.model.compile(optimizer=tf.train.AdamOptimizer(lr), loss=policy_loss(rewards))
        self.model.summary()

    def get_probs(self, s):
        s = s[np.newaxis, :]
        probs = self.model([s, np.array([1])]).numpy()
        probs = probs.squeeze()
        probs = self.softmax(probs)
        return probs

    def softmax(self, x):
        e_x = np.exp(x - np.max(x))
        return e_x / e_x.sum(axis=0)

    def update_policy(self, s, r, a):
        self.model.train_on_batch([s, r], a)

```
**And same issue if I try to optimize manually**
```python
import tensorflow as tf
from tensorflow.python.keras.layers import *

import numpy as np

tf.enable_eager_execution()
print(tf.executing_eagerly())


class EagerSeqTest:
    def __init__(self, state_space, action_space, lr):
        self.state_space = state_space
        self.action_space = action_space

        self.model = tf.keras.Sequential()

        self.model.add(InputLayer(input_shape=(84, 84, 4)))

        # Conv
        self.model.add(Conv2D(filters=32, kernel_size=(8, 8), strides=(4, 4), name='conv1'))
        self.model.add(BatchNormalization(trainable=True, epsilon=1e-5, name='batch_norm1'))
        self.model.add(ReLU(name='conv_1_out'))

        self.model.add(Conv2D(filters=64, kernel_size=(4, 4), strides=(2, 2), name='conv2'))
        self.model.add(BatchNormalization(trainable=True, epsilon=1e-5, name='batch_norm2'))
        self.model.add(ReLU(name='conv_2_out'))

        self.model.add(Conv2D(filters=64, kernel_size=(4, 4), strides=(2, 2), name='conv3'))
        self.model.add(BatchNormalization(trainable=True, epsilon=1e-5, name='batch_norm3'))
        self.model.add(ReLU(name='conv_3_out'))

        self.model.add(Flatten(name='flatten'))

        # Fully connected
        self.model.add(Dense(units=512, activation='relu', name='fc1'))
        self.model.add(Dropout(rate=0.4, name='dr1'))
        self.model.add(Dense(units=256, activation='relu', name='fc2'))
        self.model.add(Dropout(rate=0.3, name='dr2'))
        self.model.add(Dense(units=64, activation='relu', name='fc3'))
        self.model.add(Dropout(rate=0.03, name='dr3'))

        # Logits
        self.model.add(Dense(units=self.action_space, activation='linear', name='logits'))

        self.model.summary()

        # Optimizer
        self.optimizer = tf.train.AdamOptimizer(learning_rate=lr)

    def get_probs(self, s):
        s = s[np.newaxis, :]
        logits = self.model(s)
        probs = tf.nn.softmax(logits).numpy().squeeze()
        return probs

    def update_policy(self, s, r, a):
        with tf.GradientTape() as tape:
            loss = self.calc_loss(s, r, a)
        grads = tape.gradient(loss, self.model.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))

    def calc_loss(self, s, r, a):
        logits = self.model(s)
        policy_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=a, logits=logits)
        policy_loss = tf.reduce_mean(policy_loss * tf.stop_gradient(r))
        return policy_loss
```


**Other info / logs**
**Keras outputs first episode**
```
[0.33398542 0.32970214 0.33631247]
[0.33452868 0.32947394 0.33599734]
[0.33389196 0.32968676 0.3364213 ]
[0.33383334 0.32978088 0.33638576]
[0.33387497 0.3296482  0.33647686]
[0.33390424 0.32952887 0.33656695]
[0.33359385 0.32956737 0.33683875]
[0.33371714 0.32963908 0.3366438 ]
[0.33354157 0.32960638 0.33685204]
[0.33366755 0.32962722 0.33670527]
            ...
```
**Keras outputs after 10 episodes**
```
[0.2601601  0.31925505 0.42058486]
[0.3081141  0.35050547 0.34138042]
[0.30224514 0.41226208 0.28549278]
[0.2596298  0.4011653  0.33920488]
[0.24976756 0.44619036 0.30404213]
[0.22203408 0.50170064 0.2762653 ]
[0.25094017 0.460491   0.28856885]
[0.25819647 0.44147474 0.30032873]
[0.26891825 0.42985788 0.30122384]
[0.24224553 0.46432737 0.2934271 ]
              ...
```
**Eager outputs first episode**
```
[0.32854515 0.33827797 0.33317688]
[0.3286925  0.33824813 0.3330594 ]
[0.3283261  0.33858737 0.33308655]
[0.328541   0.3381491  0.33330992]
[0.328702   0.33778957 0.3335084 ]
[0.32902354 0.3377183  0.33325812]
[0.32862762 0.33784387 0.3335285 ]
[0.3286057  0.3380287  0.33336568]
[0.32879072 0.33825696 0.33295232]
[0.3287569  0.33825108 0.33299205]
              ...
```
**Eager outputs after 10, 20, 100, .... episodes**
```
[0.3276771  0.33870533 0.33361754]
[0.3276953  0.33861196 0.33369276]
[0.3273576  0.33890665 0.33373576]
[0.32751966 0.3387702  0.33371013]
[0.3276457  0.33878204 0.33357224]
[0.32758534 0.3388276  0.33358708]
[0.3277567  0.3381556  0.33408773]
[0.32765684 0.33842626 0.33391687]
[0.32840943 0.3379219  0.33366865]
[0.32794353 0.33860624 0.33345023]
              ...
```


"
27826,C++ compilation of rule '//tensorflow/core/grappler/optimizers:memory_optimizer' failed (Exit 2),"Hi,

When i run: bazel build tensorflow/tools/graph_transforms:transform_graph i get the following issue:
tensorflow/core/grappler/optimizers/BUILD:428:1: C++ compilation of rule '//tensorflow/core/grappler/optimizers:memory_optimizer' failed (Exit 2)

I am running this on Windows 10, has anyone come accross this isse?

Thanks in advance."
27824,Tensorflow gpu import error,"

**System information**
- Windows 10
- Python version: conda python 3.6
- Installed using pip
- CUDA 10.1 /cuDNN 7.5.0 version:
- GPU 1050 Ti


**Describe the problem**
Traceback (most recent call last):
  File ""C:\Users\clint\Anaconda3\envs\myenv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\clint\Anaconda3\envs\myenv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\clint\Anaconda3\envs\myenv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\clint\Anaconda3\envs\myenv\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\clint\Anaconda3\envs\myenv\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\clint\Anaconda3\envs\myenv\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\clint\Anaconda3\envs\myenv\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\clint\Anaconda3\envs\myenv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\clint\Anaconda3\envs\myenv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\clint\Anaconda3\envs\myenv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\clint\Anaconda3\envs\myenv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\clint\Anaconda3\envs\myenv\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\clint\Anaconda3\envs\myenv\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

"
27823,tf.keras.layers.BatchNormalization() causes KeyError when converting model with TFLiteConverter ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **colab**
- TensorFlow installed from (source or binary): **binary** 
- TensorFlow version (or github SHA if from source): **2.0.0-alpha0**


**Provide the text output from tflite_convert**

```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-58-c06b2de5a868> in <module>()
      6 
      7 converter = tf.lite.TFLiteConverter.from_concrete_function(concrete_func)
----> 8 tflite_model = converter.convert()

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)
    244     """"""
    245     frozen_func = _convert_to_constants.convert_variables_to_constants_v2(
--> 246         self._func)
    247     input_tensors = [
    248         tensor for tensor in frozen_func.inputs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2(func)
    171       resource_placeholders[input_name] = {
    172           ""dtype"": node.attr[""dtype""],
--> 173           ""data"": tensor_data[input_name],
    174       }
    175 

KeyError: 'sequential_21/batch_normalization_v2_20/batchnorm/ReadVariableOp/resource'
```

Also, please include a link to a GraphDef or the model if possible.

Colab containing following code:
https://colab.research.google.com/drive/1lT_Hg4sstFCBi_3ksVQYYhhfxLKhDtdt

**Any other info / logs**

The model is a simple MNIST example, with an added Batch Normalization.
```python3
from __future__ import absolute_import, division, print_function, unicode_literals

!pip install tensorflow-gpu==2.0.0-alpha0
import tensorflow as tf

mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train, x_test


model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.BatchNormalization(),  
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax),
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)

run_model = tf.function(lambda x: model(x, training = False))
concrete_func = run_model.get_concrete_function(
  tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype)
)

converter = tf.lite.TFLiteConverter.from_concrete_function(concrete_func)
tflite_model = converter.convert()
```"
27822,tflite support for Softplus (exp and log are already supported),"**System information**
- gLinux
- TensorFlow installed from source
- TensorFlow version head


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, MAX_POOL_2D, MEAN, MIRROR_PAD, MUL, RSQRT, SQUARED_DIFFERENCE, SUB, TRANSPOSE_CONV. Here is a list of operators for which you will need custom implementations: Softplus.
```

We already support log and exp. Softplus seems like a simple addition."
27820,Eager execution Blas SGEMM launch failed and GPU huge consumption.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No. Code based on [https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb).
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): <s>Ubuntu 16.04.5 LTS</s> Red Hat Enterprise Linux 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Tensorflow docker image(tensorflow/tensorflow:latest-gpu-py3)
- TensorFlow version (use command below): 1.14.1-dev20190413
- Python version: 3.5.2(in docker image)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory:  4 GeForce RTX 2080


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When eager execution is enabled, GPU resources are consumed significantly with only single calculation. Following are two simplified versions of the problem, the first one comes from [https://github.com/tensorflow/tensorflow/issues/25403](https://github.com/tensorflow/tensorflow/issues/25403) and the second one is my problem.

1. Huge GPU consumption when running a simple matrix multiplication.

2. Blas SGEMM launch failed Error when running loading the InceptionV3 model and feed with a zero vector(same error when feeding with dataset samples).
There is **NO** Blas SGEMM launch failed Error when Eager Execution is NOT enabled.

**Describe the expected behavior**
Be able to run the scripts.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

First run the docker image then run the code in python:

docker run -it --runtime=nvidia --rm -e NVIDIA_VISIBLE_DEVICE=""0,1,2,3"" -v /usr/scratch:/usr/scratch tensorflow/tensorflow:latest-gpu-py3 python

1. import tensorflow as tf
tf.enable_eager_execution()
print(tf.matmul([[1., 2.],[3., 4.]], [[1., 2.],[3., 4.]]))
nvidia-smi

2. import tensorflow as tf
tf.enable_eager_execution()
image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')
new_input = image_model.input
hidden_layer = image_model.layers[-1].output
image_features_extract_model = tf.keras.Model(new_input, hidden_layer)
x = tf.zeros([1, 299, 299, 3])  # just use a zero tensor instead of dataset tensor
image_features_extract_model(x)  # this will raise the error: ""InternalError: Blas SGEMM launch failed : m=5329, n=80, k=64 [Op:Conv2D]""

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
1. **vnivia-smi output before running code:**
+---------------------------------------------------------------------------------+
| NVIDIA-SMI 418.40.04    Driver Version: 418.40.04    CUDA Version: 10.1     |
|-------------------------------+----------------------+--------------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|   Memory-Usage | GPU-Util  Compute M. |
|===============================+================|
|   0  GeForce RTX 2080    Off  | 00000000:19:00.0 Off |                  N/A |
| 24%   39C    P0    41W / 215W |      0MiB /  7952MiB |      0%      Default |
+-------------------------------+----------------------+------------------------+
|   1  GeForce RTX 2080    Off  | 00000000:1A:00.0 Off |                  N/A |
| 24%   40C    P0    41W / 215W |      0MiB /  7952MiB |      1%      Default |
+-------------------------------+----------------------+------------------------+
|   2  GeForce RTX 2080    Off  | 00000000:67:00.0 Off |                  N/A |
| 22%   43C    P0    51W / 215W |      0MiB /  7952MiB |      0%      Default |
+-------------------------------+----------------------+------------------------+
|   3  GeForce RTX 2080    Off  | 00000000:68:00.0 Off |                  N/A |
| 14%   45C    P0     1W / 215W |      0MiB /  7951MiB |      0%      Default |
+-------------------------------+----------------------+------------------------+
+---------------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
**nvidia-smi after running the code:**
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.40.04    Driver Version: 418.40.04    CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|   Memory-Usage | GPU-Util  Compute M. |
|===============================+=================|
|   0  GeForce RTX 2080    Off  | 00000000:19:00.0 Off |                  N/A |
| 24%   41C    P2    42W / 215W |   7633MiB /  7952MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce RTX 2080    Off  | 00000000:1A:00.0 Off |                  N/A |
| 23%   43C    P2    40W / 215W |    121MiB /  7952MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce RTX 2080    Off  | 00000000:67:00.0 Off |                  N/A |
| 24%   46C    P2    51W / 215W |    121MiB /  7952MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce RTX 2080    Off  | 00000000:68:00.0 Off |                  N/A |
| 22%   48C    P2    51W / 215W |    121MiB /  7951MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=================================================|
|    0     96311      C   python                                      7623MiB |
|    1     96311      C   python                                       111MiB |
|    2     96311      C   python                                       111MiB |
|    3     96311      C   python                                       111MiB |
+-----------------------------------------------------------------------------+
**A simple matrix multiplcation consumes an entire GPU andsome in other three.**

2. **nvidia-smi before running the code:**
same as 1, nothing running GPU.
**nvidia-smi after running the code except for the last line of code of ""image_features_extract_model(x)"":**
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.40.04    Driver Version: 418.40.04    CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|   Memory-Usage | GPU-Util  Compute M. |
|===============================+================|
|   0  GeForce RTX 2080    Off  | 00000000:19:00.0 Off |                  N/A |
| 24%   40C    P2    42W / 215W |   7667MiB /  7952MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce RTX 2080    Off  | 00000000:1A:00.0 Off |                  N/A |
| 23%   41C    P8    13W / 215W |    121MiB /  7952MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce RTX 2080    Off  | 00000000:67:00.0 Off |                  N/A |
| 24%   44C    P8    24W / 215W |    121MiB /  7952MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce RTX 2080    Off  | 00000000:68:00.0 Off |                  N/A |
| 22%   46C    P8    20W / 215W |    121MiB /  7951MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|==============================================|
|    0     96729      C   python                                      7657MiB |
|    1     96729      C   python                                       111MiB |
|    2     96729      C   python                                       111MiB |
|    3     96729      C   python                                       111MiB |
+-----------------------------------------------------------------------------+
**Error when running the last line of code:**
2019-04-13 15:52:15.698521: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
2019-04-13 15:52:15.724436: E tensorflow/stream_executor/cuda/cuda_blas.cc:510] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-04-13 15:52:15.724477: W tensorflow/stream_executor/stream.cc:2130] attempting to perform BLAS operation using StreamExecutor without BLAS support
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 592, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py"", line 815, in call
    mask=masks)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py"", line 1000, in _run_internal_graph
    output_tensors = layer(computed_tensor, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 592, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/convolutional.py"", line 194, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py"", line 966, in __call__
    return self.conv_op(inp, filter)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py"", line 591, in __call__
    return self.call(inp, filter)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py"", line 208, in __call__
    name=self.name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 994, in conv2d
    name=name, ctx=_ctx)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 1072, in conv2d_eager_fallback
    ctx=_ctx, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/execute.py"", line 66, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: Blas SGEMM launch failed : m=5329, n=80, k=64 [Op:Conv2D]"
27819,"Tensorflow 2.0.0 gpu Performance Issue on RTX 2060, win 10 ","Hi I'm not used to write English so Please Understand my situation.
Actually My proplem is not quite sure about performance problem or just myself problem.
But No Solution appear in my circumstance include searching stackover-flow or googling.
And also in my country quite many of them is saying same issue about it.
And Yeah i do almost everything concern this problem searching googling or blogs or sites
So I decide to ask u to help me out.

My problem is I Change my GPU device NVIDIA GTX1050 TI to RTX 2060.
And i ran 1050 ti in tensorflow gpu well enough but when i change my GPU and unistall CUDA 9 before version and reinstall CUDA 10 and reinstall matching cudnn also
Cause As u know CUDA 10 is the only choice of RTX Series.
But After Changing my GPU and CUDA, cudnn, My GPU performance is too low to confuse to using it. Yeah It's working But TOO LOW PERFORMANCE!
Actually I usually jupyter notebook in anaconda env system and my sub env 1 is tensorflow 2.0.0 alpha and sub env 2 is tensorflow 1.13. 

But When i build conv model and train images,  Both tensorflow version of gpu is too low under 15% And CPU usage is almost 100%, RAM 60~70%
Usually GPU use 5 ~ 10%. 
When i use 1050 ti, GPU performance is almost 80 to 90% high

And I know tensorflow recommend docker Linux only but Even though I install All of these gpu concern program and other needed program.

So i sincerely ask tensorflow Is this Right Performance or My OS problem or some other probelm
I want to be clear Answer about it.
And Even if tensorflow 2.0 is not fit on windows 10, Why my another env tensorflow 1.13 version of GPU is also low and CPU is almost 100% right now

and Here is the image that Run tensorflow 2.0 tutorial about GPU 
And My GPU is Fine Device proof png file is upload
Thank you for reading my nonsence skill of eng writing.

![gpu1](https://user-images.githubusercontent.com/43261434/56080846-32391e00-5e41-11e9-95cb-476a0404cf2d.png)

![gpu colab  2](https://user-images.githubusercontent.com/43261434/56080864-572d9100-5e41-11e9-9e8c-271faf53645f.png)

![3d mark2](https://user-images.githubusercontent.com/43261434/56080973-a0321500-5e42-11e9-9176-ae339b5caf26.png)
![3d mark](https://user-images.githubusercontent.com/43261434/56080974-a0321500-5e42-11e9-836c-6b42dfd85cc3.png)

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Anaconda
- TensorFlow version (use command below): Tensorflow 2.0.0
- Python version: 3.6.8
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10, cuDNN 7.5(2.21.2019 for CUDA 10)
- GPU model and memory: RTX 2060 Emtec & 6G RAM(Edited)
- CPU : intel i5-8400
- Computer RAM - 8G

"
27817,Input shape of initial_state of keras.layers.RNN,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOSX
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): r1.13.1

**Describe the current behavior**

```
ValueError: An `initial_state` was passed that is not compatible with `cell.state_size`. Received `state_spec`=[InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2)]; however `cell.state_size` is [[8, 8], [8, 8], [8, 8]]
```

**Describe the expected behavior**

No errors during `model.compile()`, just like when using `keras` instead of `tf.keras`.

**Code to reproduce the issue**

```python
#import keras
import tensorflow.keras as keras

timesteps = 10
input_dim = 8
output_dim = 8

cells = [
    keras.layers.LSTMCell(output_dim),
    keras.layers.LSTMCell(output_dim),
    keras.layers.LSTMCell(output_dim),
]

inputs = keras.Input((timesteps, input_dim))
encoder_output = keras.layers.RNN(cells, return_state=True)(inputs)

states = encoder_output[1:]

decoder_output = keras.layers.RNN(cells)(inputs, initial_state=states)

model = keras.models.Model(inputs, decoder_output)
model.compile(optimizer='rmsprop', loss='mse')
```

**Other info / logs**
```
Traceback (most recent call last):
  File ""rnn_test.py"", line 19, in <module>
    decoder_output = keras.layers.RNN(cells)(inputs, initial_state=states)
  File ""/Users/username/python-env/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 742, in __call__
    output = super(RNN, self).__call__(full_input, **kwargs)
  File ""/Users/username/python-env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 538, in __call__
    self._maybe_build(inputs)
  File ""/Users/username/python-env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1603, in _maybe_build
    self.build(input_shapes)
  File ""/Users/username/python-env/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 629, in build
    self._validate_state_spec(state_size, self.state_spec)
  File ""/Users/username/python-env/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 664, in _validate_state_spec
    raise validation_error
ValueError: An `initial_state` was passed that is not compatible with `cell.state_size`. Received `state_spec`=[InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2)]; however `cell.state_size` is [[8, 8], [8, 8], [8, 8]]
```
"
27816,How to fix 'ImportError: DLL load failed: The specified procedure could not be found.' while importing tensorflow-gpu?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.13.1
- Python version: 3.6.0
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: 10.1/7.5.0
- GPU model and memory: GEFORCE 840M cuda enabled compute capability 5.0



I cannot import tensorflow-gpu==1.13.1 on python 3.6.0.

What i have already done:
1. Nvidia's drivers.
2. I installed Visual Studio 2017 Professional (did not select any specifications in workload).
3. I installed CUDA 10.1 (was having issues with Visual studio integration so i disabled it during the installation).
4. I pasted Cudnn 7.5.0 onto cuda's folders.
5. Restarted.
6. Pip installed tensorflow-gpu==1.13.1 on python 3.6.0.
7. but then i cannot import tensorflow at all it shows the error below.


ERROR:
```
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\BenBouali\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\BenBouali\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\BenBouali\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\BenBouali\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\BenBouali\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\BenBouali\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\BenBouali\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\BenBouali\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\BenBouali\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\BenBouali\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\BenBouali\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\BenBouali\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\BenBouali\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>

```"
27815,Differential (Delta) Model Update,"I'm new to tensorflow. My area of usage would be on device execution for item recognition.
As my use case requires frequent updates on the items to recognize, I was wondering if it is possible to update models in a delta way, e.g. to only transfer the updated data and not the whole model each time."
27813,"after quantization aware training and convert to tflite, when running the converted tflite model, ERROR: tensorflow/lite/kernels/pooling.cc:104 input->params.scale != output->params.scale (0 != 1067861562)","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Android 8.1
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.14
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.0
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
when run tflite model using tflite benchmark tool, following error happened:

ERROR: tensorflow/lite/kernels/pooling.cc:104 input->params.scale != output->params.scale (0 != 1067861562)
ERROR: Node number 92 (AVERAGE_POOL_2D) failed to prepare.
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27811,Can't enable eager execution mode in map function,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0.dev20190412
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

I write some preprocess code which can only be executed eagerly in map function for dataset formating, but I find that the code can't be executed properly. Even if the main function is executed in eager mode, the map function is still executed in graph mode.

**Describe the expected behavior**

I hope the map function can also work in eager mode to ease the coding difficulty.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

assert tf.executing_eagerly can trigger an assertion failure.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27809,Can't import tensorflow 2.0-alpha on win10 python=3.6.8,"**System information**
- OS Platform and Distribution: ```Win10```
- TensorFlow installed from (source or binary): ```pip install tensorflow==2.0.0-alpha0```
- TensorFlow version: ```tensorflow 2.0.0a0```
- Python version: ```Python 3.6.8 :: Anaconda, Inc.```
- Installed using virtualenv? pip? conda?: ```Anaconda pip```

**Describe the problem**
1. Run ```python``` at anaconda prompt, start python runtime.
2. Run ```import tensorflow```, get error as below:
```
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensorflow'
```"
27808,cannot import _mklinit,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Windows 10
- TensorFlow installed from pip install tensorflow-gpu
- TensorFlow version: 1.13.1
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: 9.0
- GPU model and memory: GTX 970M- 3GB



**Describe the problem**
I accidentally deleted Anaconda3 in my home directory so I installed Anaconda again and things started to fall apart. Before that, everything worked perfectly with tensorflow 1.9, pthon 3.6.4, and CUDA 9.0
**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Hiep Nguyen\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 24, in <mo
dule>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Hiep Nguyen\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 47,
 in <module>
    import numpy as np
  File ""C:\Users\Hiep Nguyen\Anaconda3\lib\site-packages\numpy\__init__.py"", line 140, in <module
>
    from . import _distributor_init
  File ""C:\Users\Hiep Nguyen\Anaconda3\lib\site-packages\numpy\_distributor_init.py"", line 34, in
 <module>
    from . import _mklinit
  ImportError: DLL load failed: The specified module could not be found.
```"
27807,NNAPI doesn't support tensors with rank 0 (index 3 name PadV2/constant_values),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution : Windows 10 and Android 9.0
- Mobile device : Xiaomi 8
- TensorFlow version : org.tensorflow:tensorflow-lite:0.0.0-nightly (using 1.11.0 has the same problem)
- Model: resnet18 convert from Pytorch 

**logs from Android 9.0**
E/tflite: NNAPI doesn't support tensors with rank 0 (index 3 name PadV2/constant_values)
E/tflite: Returning error since TFLite returned failure nnapi_delegate.cc:736.
    Failed to build graph for NNAPI

**other**
if I use resnet50 from tensorflow.contrib.slim.python.slim.nets, it also has similar problem like:
E/tflite: NNAPI doesn't support tensors with rank 0 (index 7 name inference_model/Mean/reduction_indices)
    Returning error since TFLite returned failure nnapi_delegate.cc:736.
    Failed to build graph for NNAPI
"
27806,Does the latest version 1.13 support CUDA10.1? I need to install tensorflow with gpu.,"
"
27804,SparkFun Edge Build Failure,"On macOS 10.14.4 I'm following the directions at this location:

https://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/#0

Everything goes fine until step 4 when I try:

make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=sparkfun_edge micro_speech_bin

and get the error:

In file included from tensorflow/lite/experimental/micro/kernels/fully_connected.cc:16:0:
./tensorflow/lite/kernels/internal/reference/fully_connected.h:18:10: fatal error: fixedpoint/fixedpoint.h: No such file or directory
 #include ""fixedpoint/fixedpoint.h""
          ^~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.

Any ideas why fixedpoint/fixedpoint.h is not found?"
27802,Autograph fails after restoring from the checkpoint,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
```bash
conda install tensorflow-gpu==2.0-alpha
```
- Python version:
3.7.1
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
not relevent
- GPU model and memory:
not relevent

**Describe the current behavior**
Running the following code for the first time show no warning. However, running it the second time results in this Warning message:
```
W0412 15:23:38.694277 140471310108480 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7fc1a2ba3908> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.W0412 15:23:38.697970 140471310108480 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7fc1a00bfb38> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/t
ensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.
```
**Describe the expected behavior**
Autograph should works just find even after restoring from a checkpoint.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
import os
import tensorflow as tf
from tensorflow.keras import layers, models

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

network = models.Sequential([layers.Dense(1, activation='relu')])


@tf.function
def forward(batch):
    network(batch)


if __name__ == ""__main__"":
    ckpt = tf.train.Checkpoint(model=network)
    manager = tf.train.CheckpointManager(ckpt, '/tmp/ckpttest', max_to_keep=10)
    if manager.latest_checkpoint:
        print(f'Resume training using {manager.latest_checkpoint}')
        ckpt.restore(manager.latest_checkpoint)

    batch = tf.zeros((1, 1), dtype=tf.float32)
    forward(batch)
    manager.save()
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27796,shuffling of test_dataset is not explicitly done,"**System information**
- TensorFlow version:2.0 
- Doc Link:https://www.tensorflow.org/alpha/tutorials/generative/pix2pix
**Input Pipeline Cell**


**Documentation/Code Issue:**
Cell has the following code:

1. test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')
2. *#shuffling so that for every epoch a different image is generated*
3. *#to predict and display the progress of our model.*
4. train_dataset = train_dataset.shuffle(BUFFER_SIZE)
5. test_dataset = test_dataset.map(load_image_test)
6. test_dataset = test_dataset.batch(1)


From line no 4, it seems we are not shuffling the test_dataset.  Should it have been 
""test_dataset = test_dataset.shuffle(BUFFER_SIZE)"" ?
"
27794,problem with hashes to install tensorflow,"Has anyone had this problem when installing tensorflow? How did they solve it? Thank you


THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.
    tensorflow from https://www.piwheels.org/simple/tensorflow/tensorflow-1.13.1-cp35-none-linux_armv7l.whl#sha256=6c00dd13db0791e83cb08d532f007cc7fd44c8d7b52662a4a0065ac4fe7ca18a:
        Expected sha256 6c00dd13db0791e83cb08d532f007cc7fd44c8d7b52662a4a0065ac4fe7ca18a
             Got        b3233a35689ec7246f10b55ba82e10a951f9e4ee6205cfb77d90a6fc7b959b76
"
27792,Text classification with movie reviews lacks examples of results at end,"The ""[Text classification with movie reviews](https://www.tensorflow.org/tutorials/keras/basic_text_classification)"" :

could really do with some examples of correct / incorrect classification as you work through / at end. At present the dataset feels a bit like a black box, and a user doesn't really get a feel for the claimed 87% accuracy (c.f. the earlier image classification where we see samples of correct/incorrect labelling).

As example the sanity check found at end of this similar example: https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184
with 5 most discriminating words for both positive and negative reviews would give more of a feel to the user on how the model was performing?"
27791,"TFL Detect ""TFLite Demo has stopped."" error using provided pets sample assets","**System information**
- Linux Ubuntu 16.04 and / or ( Mint 18 ):
- TensorFlow installed from binary:
- TensorFlow version 1.12.0 ( Compiled without AVX ):
- Python: 3.5.2 
- Cuda: 9.0
- Cudnn: 7.4
- GPU: 1050ti
- System: Dual  Xeon E5540 with 56GB Memory

I am having an unique issue implementing a custom trained model into the Android Demo located here within the tensorflow source

[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/android/app](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/android/app)

specifically, i followed the tutorial located at

https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193

after Android Studio is installed and configured, I clone a clean version of the tensorflow source, set up the Workspace and then run

`bazel build -c opt --config=android_arm{,64} --cxxopt='--std=c++11' tensorflow/lite/examples/android:tflite_demo`

The build succeeds with a couple warnings, when pushed to my device ( Android Note 8 ) via

`adb install -r tflite_demo.apk`

All three apps work great!.

The problem starts when I try to utilize the sample model and label file linked in the tensorflow tutorial on medium.com

[https://storage.googleapis.com/download.tensorflow.org/models/tflite/pets_ssd_mobilenet_v1_0.75_quant_2018_06_29.zip](https://storage.googleapis.com/download.tensorflow.org/models/tflite/pets_ssd_mobilenet_v1_0.75_quant_2018_06_29.zip)

i place both the detect.tflite and the pets_labels_list.txt in the following location

`//tensorflow/lite/examples/android/app/src/main/assets`

and change the following source files as instructed with the following changes

BUILD file -> //tensorflow/lite/examples/android

`
assets = [        ""//tensorflow/lite/examples/android/app/src/main/assets:labels_mobilenet_quant_v1_224.txt"",
        ""@tflite_mobilenet_quant//:mobilenet_v1_1.0_224_quant.tflite"",
        ""@tflite_conv_actions_frozen//:conv_actions_frozen.tflite"",
        ""//tensorflow/lite/examples/android/app/src/main/assets:conv_actions_labels.txt"",
        ""@tflite_mobilenet_ssd//:mobilenet_ssd.tflite"",
        ""//tensorflow/lite/examples/android/app/src/main/assets:detect.tflite"",
        ""//tensorflow/lite/examples/android/app/src/main/assets:box_priors.txt"",
        ""//tensorflow/lite/examples/android/app/src/main/assets:labels.txt"",
    ],
`

DetectorActivity.java -> //tensorflow/lite/examples/android/app/src/main/java/org/tensorflow/demo

`
  private static final int TF_OD_API_INPUT_SIZE = 300;
  private static final boolean TF_OD_API_IS_QUANTIZED = true;
  private static final String TF_OD_API_MODEL_FILE = ""detect.tflite"";
  private static final String TF_OD_API_LABELS_FILE = ""file:///android_asset/pets_labels_list.txt"";
`

Once updated, everything recompiles with the same output as before and it installs just fin on the device. TFL Classify and TFL Speech work the same as before, but the TFL Detect presents me with a ""TFLite Demo has stopped."" popup when ran. Using logcat, I was able to get the following stacktrace.

`
04-11 15:52:09.383 23876 23876 E tensorflow: DetectorActivity: Exception initializing classifier!

...

04-11 15:52:09.712 23876 23893 E AndroidRuntime: FATAL EXCEPTION: inference
04-11 15:52:09.712 23876 23893 E AndroidRuntime: Process: org.tensorflow.lite.demo, PID: 23876
04-11 15:52:09.712 23876 23893 E AndroidRuntime: java.lang.NullPointerException: Attempt to invoke interface method 'java.util.List org.tensorflow.demo.Classifier.recognizeImage(android.graphics.Bitmap)' on a null object reference
04-11 15:52:09.712 23876 23893 E AndroidRuntime:        at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:247)
04-11 15:52:09.712 23876 23893 E AndroidRuntime:        at android.os.Handler.handleCallback(Handler.java:789)
04-11 15:52:09.712 23876 23893 E AndroidRuntime:        at android.os.Handler.dispatchMessage(Handler.java:98)
04-11 15:52:09.712 23876 23893 E AndroidRuntime:        at android.os.Looper.loop(Looper.java:164)
04-11 15:52:09.712 23876 23893 E AndroidRuntime:        at android.os.HandlerThread.run(HandlerThread.java:65)
`

With the new error, I put the original files back and undid my changes to the source and recompiled a third time with every back to the way it was. I then installed that APK and the Detect demo was working again.

I am at a loss on what to try next as exchanging the model with the other provided sample assets should work? I did try a custom trained model of my own and got the same stacktrace with my own model. I can provide info on those models if it help at all.

Thanks in advance!

"
27790,"google::protobuf::python::oneof_descriptor::GetHasOptions(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs 10.14.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): source
- TensorFlow version: -
- Python version:
```
python --version
Python 3.6.5
```
- Installed using virtualenv? pip? conda?:
```
pip --version
pip 19.0.3 from /usr/local/lib/python3.6/site-packages/pip (python 3.6)
- Bazel version (if compiling from source):
```
- GCC/Compiler version (if compiling from source):
```
gcc --version
Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/c++/4.2.1
Apple LLVM version 10.0.1 (clang-1001.0.46.3)
Target: x86_64-apple-darwin18.2.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
```
- CUDA/cuDNN version: -
- GPU model and memory: -

`time bazel build tensorflow/python/tools:freeze_graph`

on commit 

```
commit d1db9860a24af2ce64626fe4c3bee69f83700afa (HEAD -> master, origin/master, origin/HEAD)
Author: Ayush Dubey <ayushd@google.com>
Date:   Fri Apr 12 07:53:06 2019 -0700
```

End of build log:

```
  ""__Py_NoneStruct"", referenced from:
      google::protobuf::python::message_descriptor::CopyToProto(google::protobuf::python::PyBaseDescriptor*, _object*) in descriptor.o
      google::protobuf::python::message_descriptor::GetContainingType(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o
      google::protobuf::python::field_descriptor::GetDefaultValue(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o
      google::protobuf::python::field_descriptor::GetMessageType(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o
      google::protobuf::python::field_descriptor::GetEnumType(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o
      google::protobuf::python::field_descriptor::GetContainingType(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o
      google::protobuf::python::field_descriptor::GetExtensionScope(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o
      ...
  ""__Py_NotImplementedStruct"", referenced from:
      google::protobuf::python::descriptor::RichCompare(google::protobuf::python::PyContainer*, _object*, int) in descriptor_containers.o
      google::protobuf::python::extension_dict::RichCompare(google::protobuf::python::ExtensionDict*, _object*, int) in extension_dict.o
      google::protobuf::python::cmessage::RichCompare(google::protobuf::python::CMessage*, _object*, int) in message.o
      google::protobuf::python::repeated_composite_container::RichCompare(_object*, _object*, int) in repeated_composite_container.o
      google::protobuf::python::repeated_scalar_container::RichCompare(_object*, _object*, int) in repeated_scalar_container.o
  ""__Py_TrueStruct"", referenced from:
      google::protobuf::python::message_descriptor::IsExtendable(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o
      google::protobuf::python::message_descriptor::GetHasOptions(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o
      google::protobuf::python::field_descriptor::GetHasOptions(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o
      google::protobuf::python::enum_descriptor::GetHasOptions(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o
      google::protobuf::python::enumvalue_descriptor::GetHasOptions(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o
      google::protobuf::python::file_descriptor::GetHasOptions(google::protobuf::python::PyFileDescriptor*, void*) in descriptor.o
      google::protobuf::python::oneof_descriptor::GetHasOptions(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o
      ...
  ""__Py_ZeroStruct"", referenced from:
      google::protobuf::python::message_descriptor::IsExtendable(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o
      google::protobuf::python::message_descriptor::GetHasOptions(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o
      google::protobuf::python::field_descriptor::GetHasOptions(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o
      google::protobuf::python::enum_descriptor::GetHasOptions(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o
      google::protobuf::python::enumvalue_descriptor::GetHasOptions(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o
      google::protobuf::python::file_descriptor::GetHasOptions(google::protobuf::python::PyFileDescriptor*, void*) in descriptor.o
      google::protobuf::python::oneof_descriptor::GetHasOptions(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o
      ...
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target //tensorflow/python/tools:freeze_graph failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 703.629s, Critical Path: 91.08s
INFO: 2309 processes: 2309 local.
FAILED: Build did NOT complete successfully
```"
27789,The text generation tutorial doesn't seem to be passing along hidden state while generating text,"**System information**
- TensorFlow version: `tensorflow-gpu==1.10.1`
- Doc Link: https://www.tensorflow.org/tutorials/sequences/text_generation#the_prediction_loop

**Describe the documentation issue**
I followed the text generation tutorial, and trained my model until it achieved a cross entropy loss of ~0.5 after 30 epochs; however, the generated text was mostly garbage. Reading the code linked in the section above, I saw:

```python
      # We pass the predicted word as the next input to the model
      # along with the previous hidden state
      input_eval = tf.expand_dims([predicted_id], 0)
```

which doesn't make sense to me. The comment says we're passing along the hidden state (which maybe the model is doing for us?) but the only text that's passed along is the last predicted character. After modifying the text generation function to pass to the model last at most `seq_length` generated characters, the output text started to look like actual Shakespeare/English.

My question is: is the model supposed to implicitly propagate hidden state, (and I've done something wrong following the tutorial), or is the tutorial accidentally omitting the part where we pass along the last set of generated text?

> __Note__: Since I'm running tensorflow 1.10, and the tutorial seems to require 1.13 I had to replace the loss function with `tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)`. I don't think that change is related, but is it possible that tf 1.10 doesn't propagate the hidden state in the model?

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**

I'm happy to contribute my code changes to the tutorial, but want to make sure I'm understanding what's going wrong first.
"
27788,Cant import tensorflow,"**System information**
- Windows 10
- tensorflow-1.13.1
- python 3.6.8
- Installed using pip



**Describe the problem**
When i try ""import tensorflow as tf"" i get the error below

installed python 3.6.8 then used pip install tensorflow which said ""Successfully installed numpy-1.16.2 tensorflow-1.13.1""


**Any other info / logs**

Python 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy as np
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Eine DLL-Initialisierungsroutine ist fehlgeschlagen.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Eine DLL-Initialisierungsroutine ist fehlgeschlagen.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
27787,Issue with neural network output,"I'm trying to create a neural network which take as input a table with the format : [[9][5]] and which got 3 outputs but as output i got a table with this format : [[5][3]] and i don'y understand why


**System information**
- tensorflow 1.13


Here my network class :


```
class DQNetwork:
    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        
        with tf.variable_scope(name):
            # We create the placeholders
            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote
            # [None, 84, 84, 4]
            self.inputs_ = tf.placeholder(tf.float32, shape=[state_size[1], state_size[0]], name=""inputs"")
            self.actions_ = tf.placeholder(tf.float32, [None, self.action_size], name=""actions_"")
            
            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')
            self.target_Q = tf.placeholder(tf.float32, [None], name=""target"")
            
			
            self.fc = tf.layers.dense(inputs = self.inputs_,
                                      units = 50,
									  kernel_initializer=tf.contrib.layers.xavier_initializer(),
                                      activation = tf.nn.elu)
			
            
            self.output = tf.layers.dense(inputs = self.fc, 
                                        units = self.action_size,
										kernel_initializer=tf.contrib.layers.xavier_initializer(),
                                        activation=None)
            

  
            # Q is our predicted Q value.
            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_))
            
            # The loss is the difference between our predicted Q_values and the Q_target
            # Sum(Qtarget - Q)^2
            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))
            
            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)
```

Here a set of input i can give to my network :

```
state = []
	state.append([0,0,0,0,0,0,0,0,0])
	state.append([0,1,0,0,0,0,0,0,0])
	state.append([0,0,0,0,1,0,0,0,0])
	state.append([0,0,1,0,0,1,0,0,0])
	state.append([0,0,0,0,0,0,0,0,0])
```

then state is a possible input

As output i expect something like : [[0.125, 0.56, 0.301]]

But i got something like :

```
[ [-0.06873338  0.12694651 -0.30928543]
 [-0.06873338  0.12694651 -0.30928543]
 [-0.06873338  0.12694651 -0.30928543]
 [-0.06873338  0.12694651 -0.30928543]
 [-0.06955577  0.12819782 -0.31083506]]
```"
27786,ImportError: DLL load failed: The specified module could not be found.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home
- TensorFlow installed from (source or binary): 
- TensorFlow version: 2.0.0 alpha0 
- Python version: 3.7.0
- Installed using virtualenv? pip? conda?: pip install tensorflow-gpu==2.0.0-alpha0
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: 9.0/ cuDNN = 7
- GPU model and memory: Nvidia GTX 1050, 4GB



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**: import tensorflow as tf


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Traceback (most recent call last):
  File ""C:\Users\shikh\AppData\Local\conda\conda\envs\pytorch\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\shikh\AppData\Local\conda\conda\envs\pytorch\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\shikh\AppData\Local\conda\conda\envs\pytorch\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\shikh\AppData\Local\conda\conda\envs\pytorch\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\shikh\AppData\Local\conda\conda\envs\pytorch\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\shikh\AppData\Local\conda\conda\envs\pytorch\lib\site-packages\tensorflow\__init__.py"", line 27, in <module>
    from tensorflow._api.v2 import audio
  File ""C:\Users\shikh\AppData\Local\conda\conda\envs\pytorch\lib\site-packages\tensorflow\_api\v2\audio\__init__.py"", line 8, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\shikh\AppData\Local\conda\conda\envs\pytorch\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\shikh\AppData\Local\conda\conda\envs\pytorch\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\shikh\AppData\Local\conda\conda\envs\pytorch\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\shikh\AppData\Local\conda\conda\envs\pytorch\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\shikh\AppData\Local\conda\conda\envs\pytorch\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\shikh\AppData\Local\conda\conda\envs\pytorch\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\shikh\AppData\Local\conda\conda\envs\pytorch\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.


"
27785,How to's fully_connected_reader.py raises an AttributeError,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: Mac OSX
- Mobile device: No
- TensorFlow installed from (source or binary): pip installed
- TensorFlow version: `tensorflow==1.12.0`
- Python version: Python 2.7.16
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: no
- GPU model and memory: no


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

`('v1.12.0-rc2-3-ga6d8ffae09', '1.12.0')`

**Describe the current behavior**

When running this how to script:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py

An `AttributeError` is raised

```
Traceback (most recent call last):
  File ""fully_connected_reader.py"", line 218, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/Users/aaron/Documents/github/tfx/venv/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""fully_connected_reader.py"", line 186, in main
    run_training()
  File ""fully_connected_reader.py"", line 140, in run_training
    train=True, batch_size=FLAGS.batch_size, num_epochs=FLAGS.num_epochs)
  File ""fully_connected_reader.py"", line 129, in inputs
    iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)
AttributeError: 'module' object has no attribute 'v1'
```

**Describe the expected behavior**

Code should run without error in the ""how to""

**Code to reproduce the issue**

Call the Python file with `tf` installed

```
python fully_connected_reader.py
```

**To fix**

To fix this error, I changed `line 129` from:

```
iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)
```

to:

```
iterator = tf.data.Dataset.make_one_shot_iterator(dataset)
```"
27781,Fail to use tf.transpose() after tf.nn.embedding_lookup() and tf.layers.conv1d() while build a tflite file,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.3
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): 1.13.1


**Provide the text output from tflite_convert**
2019-04-12 16:14:22.935998: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 13 operators, 22 arrays (0 quantized)
2019-04-12 16:14:22.936163: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 13 operators, 22 arrays (0 quantized)
2019-04-12 16:14:22.939819: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 6 operators, 14 arrays (0 quantized)
2019-04-12 16:14:22.939890: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 5 operators, 12 arrays (0 quantized)
2019-04-12 16:14:22.939945: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 5 operators, 12 arrays (0 quantized)
2019-04-12 16:14:22.939972: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1605] Check failed: axis < input_shape.dimensions_count() (76044600 vs. 4)
Aborted (core dumped)
```
# Copy and paste here
```

Also, please include a link to a GraphDef or the model if possible.


**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
`ipts = tf.placeholder(dtype=tf.int32, shape=(None, 50), name='x')`
`embedding = tf.get_variable(name='embedding', shape=[3000, 256], initializer=initializer)`
`res = tf.nn.embedding_lookup(params=embedding, ids=ipts)`
`res = tf.layers.conv1d(ipts, 100, 1)`
`res = tf.transpose(res, perm=[0, 2, 1])    # Here cause the BUG`
`sess = tf.Session()`
`sess.run(tf.global_variables_initializer())`
`converter = tf.contrib.lite.TFLiteConverter.from_session(sess, [ipts], [res])`
`tflite_model = converter.convert()`
`open('converted_model.tflite', 'wb').write(tflite_model)`
"
27780,DepthwiseConv mixed precision train super slow(Caused byDepthwiseConv2dNativeBackpropFilter ),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MS Windows10 X64 1809 build 17763
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary(pip)
- TensorFlow version (use command below):1.13.1
- Python version:3.6
- Bazel version (if compiling from source):N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version:10.0/7.5.0.56
- GPU model and memory:NVIDIA Geforce RTX2080TI 11GB

**Describe the current behavior**
I am trying to port mnasnet's TPU implementation to the GPU.It works fine when using FP32.But when using mixed precision training, the speed is very slow.By using the Profiler to track the training loop, I found that DepthwiseConv2dNativeBackpropFilter consumed too much time.I tried adjusting the loss_scale but it didn't work.I want to know if Depthwise convolution does not support backpropagation at half precision, or there are performance issues.
![image](https://user-images.githubusercontent.com/40640909/56020277-59a4c380-5d39-11e9-9811-3166dfec6978.png)

**Describe the expected behavior**

**Code to reproduce the issue**
https://github.com/rootkitchao/mnasnet_temp

python mnasnet_main.py --data_dir=D:\dataset\imagenet_tfrecord\ --model_dir=D:\tf_project\mnasnet\  --model_name=mnasnet-a1 --use_bfloat16=True  --use_keras=False --mode=train --train_batch_size=96 --num_gpus=1 --train_steps=2335456 --steps_per_eval=33363

(Temporary version.The code does not really use bfloat16, use float16 instead.)

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
N/A
"
27778,linux build tensorflow c++ API error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.9
- Python version: python3
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source):0.11.0- (@non-git) 
- GCC/Compiler version (if compiling from source): gcc 4.8
- CUDA/cuDNN version: no
- GPU model and memory: no



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
how to solve the error. thanks. which version is stable
1. tensorflow/contrib/makefile/build_all_linux.sh 
2. ./configure
3. bazel build --config=opt //tensorflow:libtensorflow_cc.so.
show Info:
[root@192 tensorflow]# bazel build --config=opt //tensorflow:libtensorflow_cc.so
WARNING: Config values are not defined in any .rc file: opt
WARNING: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/protobuf_archive/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions
ERROR: /root/tensorflow/tensorflow/core/platform/default/build_config/BUILD:153:1: no such package '@png_archive//': Traceback (most recent call last):
	File ""/root/tensorflow/third_party/repo.bzl"", line 88
		_apply_patch(ctx, ctx.attr.patch_file)
	File ""/root/tensorflow/third_party/repo.bzl"", line 56, in _apply_patch
		fail(""patch command is not found, ple..."")
patch command is not found, please install it and referenced by '//tensorflow/core/platform/default/build_config:platformlib'
ERROR: Analysis of target '//tensorflow:libtensorflow_cc.so' failed; build aborted: no such package '@png_archive//': Traceback (most recent call last):
	File ""/root/tensorflow/third_party/repo.bzl"", line 88
		_apply_patch(ctx, ctx.attr.patch_file)
	File ""/root/tensorflow/third_party/repo.bzl"", line 56, in _apply_patch
		fail(""patch command is not found, ple..."")
patch command is not found, please install it
INFO: Elapsed time: 27.651s
FAILED: Build did NOT complete successfully (40 packages loaded)

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

bazel version
[root@192 bazel-0.11]# bazel version
Build label: 0.11.0- (@non-git)
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Apr 24 19:52:56 +51264 (1555578330776)
Build timestamp: 1555578330776
Build timestamp as int: 1555578330776

"
27777,The unexpected training behavior if read data from tf.record file,"To save time I skip writing data into the tf.record files. Instead data is read from tf.record files created in other training experiments (from the same source raw data). But training step marked as finished much earlier than the requested steps. This problem will not happen if I write the rf.record from scratch.

Noted that the training data are basically the same, it is possibly caused by some inner training mechanism in tensorflow data management that I have not noticed. 

**Code to reproduce the issue**
Here I define the model with 
```python
num_train_steps = 200000
model_fn_builder(
      ...
      num_train_steps=num_train_steps,
)
```
```python
train_file = os.path.join(FLAGS.output_dir, ""train.tf_record"")
train_input_fn = file_based_input_fn_builder(
        input_file=train_file,
       )
```
the training is marked as finished at, say step `1008`. 

If I rewrite the tf.record again, the training step will run as expected.
```python
writer = tf.python_io.TFRecordWriter(output_file)
writer.write(tf_example.SerializeToString())
...

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.11
- Python version: py3.6"
27776,Cannot compute gradients in graphs using RaggedTensors,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.14
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): `pip install tensorflow==1.13.1`
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**

Computing gradients for some graphs involving `RaggedTensor`s causes an error

**Describe the expected behavior**

The gradients should work. This looks similar to https://github.com/tensorflow/tensorflow/issues/26015

**Code to reproduce the issue**

```python
num_buckets = 10
embedding_size = 3

texts = tf.constant([""hello how"", ""are you ?""])
ragged = tf.RaggedTensor.from_sparse(tf.strings.split(texts))
ragged_ids = tf.ragged.map_flat_values(
    tf.string_to_hash_bucket,
    ragged,
    num_buckets
)
embeddings = tf.get_variable(""embeddings"", shape=[num_buckets, embedding_size])
ragged_embeddings = tf.ragged.map_flat_values(
  tf.nn.embedding_lookup, embeddings, ragged_ids
)

variable = tf.get_variable(
    ""variable"",
    [1, 1, embedding_size]
)

ragged_embeddings_1 = tf.RaggedTensor.from_row_splits(
    values=(ragged_embeddings + variable),
    row_splits=ragged_embeddings.row_splits,
)

ragged_embeddings_2 = tf.concat([ragged_embeddings, ragged_embeddings], 1)


sess.run(tf.global_variables_initializer())

def test_grad(ragged_tensor):
  sess.run(ragged_tensor)
  grad, = tf.gradients([ragged_tensor.to_tensor()], [embeddings])
  sess.run(grad)

# No error
test_grad(ragged_embeddings)

# Raises InvalidArgumentError: Operation 'RaggedTile/Tile' has no attr named '_XlaCompile'.
test_grad(ragged_embeddings_1)

# Raises LookupError: gradient registry has no entry for: RaggedGather
test_grad(ragged_embeddings_2)
```

**Other info / logs**

Full error for grad of `ragged_embeddings_1`:

```
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2408       with c_api_util.tf_buffer() as buf:
-> 2409         c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)
   2410         data = c_api.TF_GetBuffer(buf)

InvalidArgumentError: Operation 'RaggedTile/Tile' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)
    414     try:
--> 415       xla_compile = op.get_attr(""_XlaCompile"")
    416       xla_separate_compiled_gradients = op.get_attr(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2412       # Convert to ValueError for backwards compatibility.
-> 2413       raise ValueError(str(e))
   2414     x = attr_value_pb2.AttrValue()

ValueError: Operation 'RaggedTile/Tile' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    454                 preferred_dtype=default_dtype,
--> 455                 as_ref=input_arg.is_ref)
    456             if input_arg.number_attr and len(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_n_to_tensor(values, dtype, name, as_ref, preferred_dtype, ctx)
   1239             preferred_dtype=preferred_dtype,
-> 1240             ctx=ctx))
   1241   return ret

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)
   1174     if ret is None:
-> 1175       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1176 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)
    976         ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r"" %
--> 977         (dtype.name, t.dtype.name, str(t)))
    978   return t

ValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: 'Tensor(""gradients/RaggedTile/Tile_grad/Shape:0"", shape=(2,), dtype=int32)'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-38-299d0ab83362> in <module>()
     38   sess.run(grad)
     39 
---> 40 test_grad(ragged_embeddings_1)

<ipython-input-38-299d0ab83362> in test_grad(ragged_tensor)
     35 def test_grad(ragged_tensor):
     36   sess.run(ragged_tensor)
---> 37   grad, = tf.gradients([ragged_tensor.to_tensor()], [embeddings])
     38   sess.run(grad)
     39 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)
    662     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,
    663                             gate_gradients, aggregation_method, stop_gradients,
--> 664                             unconnected_gradients)
    665 
    666 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    963                 # functions.
    964                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--> 965                                          lambda: grad_fn(op, *out_grads))
    966               else:
    967                 # For function call ops, we add a 'SymbolicGradient'

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)
    418       xla_scope = op.get_attr(""_XlaScope"").decode()
    419     except ValueError:
--> 420       return grad_fn()  # Exit early
    421 
    422   if not xla_compile:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in <lambda>()
    963                 # functions.
    964                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--> 965                                          lambda: grad_fn(op, *out_grads))
    966               else:
    967                 # For function call ops, we add a 'SymbolicGradient'

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py in _TileGrad(op, grad)
    579   #   axes = [0, 2, 4]
    580   split_shape = array_ops.reshape(
--> 581       array_ops.transpose(array_ops.stack([op.inputs[1], input_shape])), [-1])
    582   axes = math_ops.range(0, array_ops.size(split_shape), 2)
    583   # Sum reduces grad along the first dimension for IndexedSlices

/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    178     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    179     try:
--> 180       return target(*args, **kwargs)
    181     except (TypeError, ValueError):
    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in stack(values, axis, name)
   1003                                                       expanded_num_dims))
   1004 
-> 1005   return gen_array_ops.pack(values, axis=axis, name=name)
   1006 
   1007 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in pack(values, axis, name)
   5446   axis = _execute.make_int(axis, ""axis"")
   5447   _, _, _op = _op_def_lib._apply_op_helper(
-> 5448         ""Pack"", values=values, axis=axis, name=name)
   5449   _result = _op.outputs[:]
   5450   _inputs_flat = _op.inputs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    481                                 (prefix, dtype.name))
    482               else:
--> 483                 raise TypeError(""%s that don't all match."" % prefix)
    484             else:
    485               raise TypeError(

TypeError: Tensors in list passed to 'values' of 'Pack' Op have types [int64, int32] that don't all match.
```


Full error for `ragged_embeddings_2`:

```
LookupError                               Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    906           try:
--> 907             grad_fn = ops.get_gradient_function(op)
    908           except LookupError:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_gradient_function(op)
   2545     op_type = op.type
-> 2546   return _gradient_registry.lookup(op_type)
   2547 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/registry.py in lookup(self, name)
     93       raise LookupError(
---> 94           ""%s registry has no entry for: %s"" % (self._name, name))

LookupError: gradient registry has no entry for: RaggedGather

During handling of the above exception, another exception occurred:

LookupError                               Traceback (most recent call last)
<ipython-input-39-8a8bafd157d6> in <module>()
     38   sess.run(grad)
     39 
---> 40 test_grad(ragged_embeddings_2)

<ipython-input-39-8a8bafd157d6> in test_grad(ragged_tensor)
     35 def test_grad(ragged_tensor):
     36   sess.run(ragged_tensor)
---> 37   grad, = tf.gradients([ragged_tensor.to_tensor()], [embeddings])
     38   sess.run(grad)
     39 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)
    662     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,
    663                             gate_gradients, aggregation_method, stop_gradients,
--> 664                             unconnected_gradients)
    665 
    666 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    921               raise LookupError(
    922                   ""No gradient defined for operation '%s' (op type: %s)"" %
--> 923                   (op.name, op.type))
    924         if loop_state:
    925           loop_state.EnterGradWhileContext(op, before=False)

LookupError: No gradient defined for operation 'RaggedConcat/RaggedGather/RaggedGather' (op type: RaggedGather)
```


"
27770,AttributeError: module 'tensorflow._api.v1.nn' has no attribute 'softmax_cross_entryopy_with_logits_v2',"**System info**
```
geoff@ubuntu:~$ uname -a
Linux ubuntu 4.15.0-47-generic #50-Ubuntu SMP Wed Mar 13 10:44:52 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
geoff@ubuntu:~/work/interview/coursera_dl$ python --version
Python 3.6.6 :: Anaconda, Inc.
geoff@ubuntu:~$ python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"" 
b'v1.13.1-0-g6612da8951' 1.13.1
geoff@ubuntu:~/work/interview/coursera_dl$ python -c ""import keras; print(keras.__version__)""
Using TensorFlow backend.
2.2.4
```

**Describe Issue**
No luck getting this to work so far, and I can't find any similar issues.

**Reproduce issue**

[reproduce.txt](https://github.com/tensorflow/tensorflow/files/3071694/reproduce.txt)


[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3071647/tf_env.txt)
"
27769,[TF 2.0 keras] Unable save and load weights for double nested models,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7

**Describe the current behavior**
load_weights throw exception on a doubly nested model

**Describe the expected behavior**
load_weights should work

This problem only happens on two+ layers of nested model with non-trainable weights.
The reason is save_weights and load_weights handles nested model differently
save_weights -> call layer.weights for each layer
load_weights -> recursively call model.weights if layer is a nested Model

**Code to reproduce the issue**
```
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization

shape = (None, None, 3)

def BNModel():
    x = inputs = Input(shape)
    x = Conv2D(3, 1)(x)
    x = BatchNormalization()(x)
    return Model(inputs, x)

x = inner_inputs = Input(shape)
x = BNModel()(x)
x = BNModel()(x)
inner_model = Model(inner_inputs, x)

inputs = Input(shape)
model = Model(inputs, inner_model(inputs))

inner_model.save_weights('test.h5')
inner_model.load_weights('test.h5')  # works fine

model.save_weights('test.h5')
model.load_weights('test.h5')   # Exception: axes don't match array !!!
```

**Other info / logs**
This bug is also reported on upstream keras https://github.com/keras-team/keras/pull/11847
Here is a detailed analysis on why this is happening https://github.com/keras-team/keras/pull/11847#issuecomment-482438283

Full Exception
```
  File ""test.py"", line 27, in <module>
    model.load_weights('test.h5')   # Exception: axes don't match array !!!
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py"", line 1497, in load_weights
    hdf5_format.load_weights_from_hdf5_group(f, self.layers)
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 751, in load_weights_from_hdf5_group
    layer, weight_values, original_keras_version, original_backend)
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 377, in preprocess_weights_for_loading
    weights = convert_nested_model(weights)
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 365, in convert_nested_model
    original_backend=original_backend))
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 377, in preprocess_weights_for_loading
    weights = convert_nested_model(weights)
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 353, in convert_nested_model
    original_backend=original_backend))
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 459, in preprocess_weights_for_loading
    weights[0] = np.transpose(weights[0], (3, 2, 0, 1))
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 598, in transpose
    return _wrapfunc(a, 'transpose', axes)
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 51, in _wrapfunc
    return getattr(obj, method)(*args, **kwds)
ValueError: axes don't match array
```"
27766,which version of bazel should I used to build the tensorflow-2.0.0-alpha0,"**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow version: tensorflow-2.0.0-alpha0
- Python version: python 3.5.2
- Bazel version: 0.22.0
- GCC/Compiler version: 5.4.0
- CUDA/cuDNN version: no 
- GPU model and memory: no 


I want to build tensorflow-2.0.0-alpha0 by source and used the bazel with version 0.22.0, but bazel cannot recognize the BUILD file:

```
root@XX-Latitude-E5470:~/share/project/tensorflow-2.0.0-alpha0# bazel build --config=opt //tensorflow/tools/pip-package:build_pip_package
Starting local Bazel server and connecting to it...
INFO: Repository rule 'build_bazel_rules_swift' returned: {""remote"": ""https://github.com/bazelbuild/rules_swift.git"", ""commit"": ""001736d056d7eae20f1f4da41bc9e6f036857296"", ""shallow_since"": ""2019-01-18"", ""init_submodules"": False, ""verbose"": False, ""strip_prefix"": """", ""patches"": [], ""patch_tool"": ""patch"", ""patch_args"": [""-p0""], ""patch_cmds"": [], ""name"": ""build_bazel_rules_swift""}
DEBUG: /home/alcht0/.cache/bazel/_bazel_root/2c79635619ee2e9671f2cc31f8facdc4/external/build_bazel_rules_apple/apple/repositories.bzl:35:5: 
WARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.

ERROR: Skipping '//tensorflow/tools/pip-package:build_pip_package': no such package 'tensorflow/tools/pip-package': BUILD file not found on package path
WARNING: Target pattern parsing failed.
ERROR: no such package 'tensorflow/tools/pip-package': BUILD file not found on package path
INFO: Elapsed time: 4.753s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
```
it is the same with bazel version 0.19.2.
so I don't know which bazel should I used to build the tensorflow-2.0.0-alpha0.


I installed bazel by 
```
./bazel-0.22.0-installer-linux-x86_64.sh
```
and switch the version also by the `.sh` cmd, I don't know if the issue related to it.

Best regards.
"
27765,Unable to convert fine-tuned model to tflite model,"**System information**
- Linux Ubuntu 18.04:
- pip:
- tensorflow==1.13.1:

**CMD**

```bash
bazel-bin/tensorflow/lite/toco/toco \
--input_file=../output/ner_model.pb \
--output_file=../output/ner_model.tflite \
--input_format=TENSORFLOW_GRAPHDEF \
--output_format=TFLITE \
--inference_type=QUANTIZED_UINT8 \
--input_shapes=1,512:1,512 \
--input_arrays=input_ids,input_mask \
--output_arrays=pred_ids \
--default_ranges_min=0 --default_ranges_max=512 \
--allow_custom_ops
```

**Provide the text output from tflite_convert**

```
Constant array bert/embeddings/word_embeddings lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
Unimplemented: this graph contains an operator of type OneHot for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

```bash
bazel-bin/tensorflow/tools/graph_transforms/summarize_graph \
--in_graph=../output/ner_model.pb
```

**output**

```
Found 2 possible inputs: (name=input_ids, type=int32(3), shape=[?,512]) (name=input_mask, type=int32(3), shape=[?,512]) 
No variables spotted.
Found 1 possible outputs: (name=pred_ids, op=Identity) 
Found 101686282 (101.69M) const parameters, 0 (0) variable parameters, and 18 control_edges
Op types used: 683 Const, 214 Identity, 136 Mul, 106 Add, 74 MatMul, 73 BiasAdd, 57 Reshape, 57 Pack, 52 Transpose, 50 Mean, 38 Sub, 25 StopGradient, 25 SquaredDifference, 25 Rsqrt, 24 BatchMatMul, 23 Enter, 19 ExpandDims, 16 Shape, 14 StridedSlice, 12 Sqrt, 12 Softmax, 12 RealDiv, 12 Erf, 9 Range, 8 Switch, 8 NextIteration, 8 Merge, 7 ConcatV2, 4 TensorArrayV3, 4 Select, 4 Fill, 4 Less, 3 Assert, 3 Exit, 3 Slice, 3 Squeeze, 3 All, 3 Cast, 3 Max, 3 Maximum, 2 TensorArraySizeV3, 2 TensorArrayScatterV3, 2 TensorArrayReadV3, 2 ReverseSequence, 2 TensorArrayGatherV3, 2 TensorArrayWriteV3, 2 ArgMax, 2 Equal, 2 Placeholder, 2 Minimum, 2 LoopCond, 2 LogicalAnd, 2 GreaterEqual, 1 Sign, 1 OneHot, 1 Sum, 1 Abs, 1 Tanh, 1 LessEqual, 1 GatherV2, 1 GatherNd
```
"
27763,"after freezing quantization aware training model , batchnorm create size node, cast node, and it's not supported to convert to tflite by tflite_convert","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):nightly(1.14)
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9.2, cudnn7
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
after freezing quantization aware training model, batchnorm create some new nodes, called size and cast, and they are not supported to convert to tllite by tflite_convert

2019-04-12 08:58:02.410692: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-04-12 08:58:02.430576: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz
2019-04-12 08:58:02.431034: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55678114f760 executing computations on platform Host. Devices:
2019-04-12 08:58:02.431048: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-04-12 08:58:02.648299: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-04-12 08:58:02.648376: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2019-04-12 08:58:02.902493: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:684] Optimization results for grappler item: graph_to_optimize
2019-04-12 08:58:02.902517: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   model_pruner: Graph size after: 1684 nodes (-478), 1984 edges (-478), time = 14.17ms.
2019-04-12 08:58:02.902521: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   implementation_selector: Graph size after: 1684 nodes (0), 1984 edges (0), time = 2.849ms.
2019-04-12 08:58:02.902525: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   function_optimizer: Graph size after: 1684 nodes (0), 1984 edges (0), time = 2.628ms.
2019-04-12 08:58:02.902527: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   constant folding: Graph size after: 1538 nodes (-146), 1834 edges (-150), time = 73.3ms.
2019-04-12 08:58:02.902530: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   shape_optimizer: Graph size after: 1538 nodes (0), 1834 edges (0), time = 3.219ms.
2019-04-12 08:58:02.902533: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   arithmetic_optimizer: Graph size after: 1085 nodes (-453), 1834 edges (0), time = 28.22ms.
2019-04-12 08:58:02.902536: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   loop_optimizer: Graph size after: 1085 nodes (0), 1834 edges (0), time = 4.45ms.
2019-04-12 08:58:02.902539: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   dependency_optimizer: Graph size after: 1081 nodes (-4), 1826 edges (-8), time = 9.244ms.
2019-04-12 08:58:02.902542: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   memory_optimizer: Graph size after: 1081 nodes (0), 1826 edges (0), time = 30.734ms.
2019-04-12 08:58:02.902545: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   model_pruner: Graph size after: 1081 nodes (0), 1826 edges (0), time = 4.841ms.
2019-04-12 08:58:02.902549: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   implementation_selector: Graph size after: 1081 nodes (0), 1826 edges (0), time = 1.598ms.
2019-04-12 08:58:02.902553: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   function_optimizer: Graph size after: 1081 nodes (0), 1826 edges (0), time = 1.4ms.
2019-04-12 08:58:02.902556: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   constant folding: Graph size after: 1081 nodes (0), 1826 edges (0), time = 16.989ms.
2019-04-12 08:58:02.902559: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   shape_optimizer: Graph size after: 1081 nodes (0), 1826 edges (0), time = 2.596ms.
2019-04-12 08:58:02.902562: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   arithmetic_optimizer: Graph size after: 1081 nodes (0), 1826 edges (0), time = 15.685ms.
2019-04-12 08:58:02.902565: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   dependency_optimizer: Graph size after: 1081 nodes (0), 1826 edges (0), time = 9.8ms.
Traceback (most recent call last):
  File ""/home/syshang/anaconda3/envs/tfnightly0410/bin/tflite_convert"", line 10, in <module>
    sys.exit(main())
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 448, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 444, in run_main
    _convert_model(tflite_flags)
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 192, in _convert_model
    output_data = converter.convert()
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 742, in convert
    **converter_kwargs)
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/python/convert.py"", line 410, in toco_convert_impl
    input_data.SerializeToString())
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/python/convert.py"", line 176, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-04-12 08:58:03.681724: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.688089: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688112: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688225: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.688236: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688242: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688274: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.688282: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688287: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688338: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.688345: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688351: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688521: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.688530: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688535: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688587: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.688595: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688600: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688631: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.688638: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688643: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688700: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.688708: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688713: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688745: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.688752: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688758: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688795: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.688802: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688808: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688838: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.688846: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688851: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688900: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.688907: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688913: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688942: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.688950: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688955: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.688992: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.688999: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689005: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689034: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.689042: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689047: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689083: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.689091: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689096: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689126: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.689135: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689141: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689197: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.689209: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689222: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689265: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.689272: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689281: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689334: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.689342: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689347: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689378: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.689386: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689391: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689429: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.689436: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689441: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689472: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.689479: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689485: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689521: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.689529: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689534: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689564: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.689572: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689577: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689611: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.689618: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689624: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689666: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.689674: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689680: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689710: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.689717: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689723: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689759: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.689766: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689772: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689802: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.689809: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689814: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689850: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.689858: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689863: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689893: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.689901: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689906: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689942: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.689949: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.689955: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.690004: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-04-12 08:58:03.690012: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.690018: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-04-12 08:58:03.696569: F tensorflow/lite/toco/tooling_util.cc:1040] Check failed: array->has_shape() 
Fatal Python error: Aborted

Current thread 0x00007f8353181740 (most recent call first):
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33 in execute
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/absl/app.py"", line 251 in _run_main
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/absl/app.py"", line 300 in run
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40 in run
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59 in main
  File ""/home/syshang/anaconda3/envs/tfnightly0410/bin/toco_from_protos"", line 10 in <module>
Aborted (core dumped)


**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27761,"Getting an ""Unimplemented"" error when using tensorflow-gpu built from source on TF Object Detection API model. ","My model is `faster_rcnn_inception_v2_coco` found within the TensorFlow object detection API located [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md).

When I use `tensorflow-gpu` `1.13.1` and simply install it via Docker w/ the instructions found [here](https://www.tensorflow.org/install/docker#gpu_support) everything works perfectly upon inference. 



But when I use the try to run `tensorflow-gpu` built from source (running TF `1.13.1`) built from source (via Docker w/ the instructions [here](https://www.tensorflow.org/install/source#gpu_support_2)) to take advantage of CPU optimizations I get hit with the following error upon attempting to do inference. 


```
[evhttp_server.cc : 237] RAW: Entering the event loop ...
2019-04-11 20:27:59.196073: W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1408] OP_REQUIRES failed at conv_ops_fused_impl.h:648 : Unimplemented: Fusion is not implemented: [BiasAdd,Relu6]
2019-04-11 20:27:59.196141: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:637] Executor failed to create kernel. Unimplemented: Fusion is not implemented: [BiasAdd,Relu6]
	 [[{{node Conv/Relu6}}]]
2019-04-11 20:28:01.013470: W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1408] OP_REQUIRES failed at conv_ops_fused_impl.h:648 : Unimplemented: Fusion is not implemented: [BiasAdd,Relu6]
2019-04-11 20:28:01.013541: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:637] Executor failed to create kernel. Unimplemented: Fusion is not implemented: [BiasAdd,Relu6]
	 [[{{node Conv/Relu6}}]]
2019-04-11 20:28:02.892400: W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1408] OP_REQUIRES failed at conv_ops_fused_impl.h:648 : Unimplemented: Fusion is not implemented: [BiasAdd,Relu6]
2019-04-11 20:28:02.892476: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:637] Executor failed to create kernel. Unimplemented: Fusion is not implemented: [BiasAdd,Relu6]
	 [[{{node Conv/Relu6}}]]
```

This is super odd, because things worked fine with tensorflow-gpu installed normally with Docker when doing inference. But, after building it from source, something breaks!

"
27760,TFLite Toco unable to fully quantize (QUANTIZED_UINT8) graph with unpack/unstack,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  No.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Using Tensorflow in Anaconda
- TensorFlow version (use command below): 1.12.0, 1.13.1, tf-nightly
- Python version: 3.6.xxx

**Describe the current behavior**
I have a quantization-aware trained static rnn LSTM, with unstack. I used TFLiteLSTMCell defined in this file: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/lstm/rnn_cell.py
I manually inserted fake quant nodes in the graph and generated a .pb file.
When I want to convert my .pb file to .tflite model in toco with ""converter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8""
I got this error below:
F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:474] Unimplemented: this graph contains an operator of type Unpack for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\nAborted (core dumped)

**Describe the expected behavior**
Expectation: The graph can be convert to .tflite file. 

**Code to reproduce the issue**
I have my .pb file attached below:
[my_frozen_graph.zip](https://github.com/tensorflow/tensorflow/files/3069858/my_frozen_graph.zip)

My toco code below to regenerate the error:
`graph_def_file = ""my_frozen_graph.pb""     # This is the .pb file.
input_arrays = [""Reshape_1""]        # This is the name of the input node
output_arrays = [""labels_softmax""]  # This is the name of the ouput node

converter = tf.contrib.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)
converter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8
input_arrays = converter.get_input_arrays()
converter.quantized_input_stats = {input_arrays[0] : (227., 0.92)}  # mean, std_dev
tflite_model = converter.convert()
open(""static_rnn_with_unstack.tflite"", ""wb"").write(tflite_model)   # The resulting .tflite file.`

**Other info / logs**
As asked by @jdduke , I filled this issue. Thanks in advance."
27759,Issue in code of image captioning notebook,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: Any
- Doc Link: https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/image_captioning.ipynb


**Describe the documentation issue**
The code in this [cell](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/image_captioning.ipynb#scrollTo=uEWM9xrYcg45) which sets `vocab_size = len(tokenizer.word_index) + 1` is wrong. `vocab_size` is set to `8235+1`, but actually needs to be set to `5000+1`, as `top_k = 5000`

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?** Yes
"
27757,[Build error] TensorFlow Lite GPU delegate,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
    - Debian 4.19.20-1rodete1 (2019-02-12 > 2018) x86_64 GNU/Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
    - Build for Android by adding the following code:
```
android_sdk_repository (
    name = ""androidsdk"",
    api_level = 26,
    build_tools_version = ""26.0.2"",
    path = ""..."",
)

android_ndk_repository(
    name = ""androidndk"",
    path = "".../android-ndk-r19c"",
    api_level = 19,
)
```
- TensorFlow installed from (source or binary): source
- TensorFlow version: master, commit number 7bdb14a0b
- Python version: 2.7.16rc1
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): Android (5058415 based on r339409) clang version 8.0.2
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

Tried [TensorFlow Lite GPU delegate](https://www.tensorflow.org/lite/performance/gpu) and got a build error.

After adding the following code into WORKSPACE under tensorflow source code's root directory,
```
android_sdk_repository (
    name = ""androidsdk"",
    api_level = 26,
    build_tools_version = ""26.0.2"",
    path = ""..."",
)

android_ndk_repository(
    name = ""androidndk"",
    path = "".../android-ndk-r19c"",
    api_level = 19,
)
```

I ran the following command:
```
bazel build --cxxopt=--std=c++11 //tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo
```

it reports:
```
bazel-out/android-armeabi-v7a-opt/bin/external/FP16/_virtual_includes/FP16/fp16.h:5:10: fatal error: 'fp16/fp16.h' file not found
#include <fp16/fp16.h>
         ^~~~~~~~~~~~~
1 error generated.
Target //tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo failed to build
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27754,Building Tensorflow on Windows (CPU only) issue,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10 - c5.nlarge AWS Instance**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **NA**
- TensorFlow installed from (source or binary): **Source**
- TensorFlow version: **r1.13** (Tried with **r.12** as well)
- Python version: **Python 3.6** (Conda)
- Installed using virtualenv? pip? conda?: **NA**
- Bazel version (if compiling from source): **0.19**
- GCC/Compiler version (if compiling from source): **Using Visual Studio 2017**
- CUDA/cuDNN version: **NA**
- GPU model and memory: **NA**

**Describe the problem**
I started off with the [instructions provided on TensorFlow page](https://www.tensorflow.org/install/source_windows), 

1. I installed Python 3.6 in PATH.
2. Installed Bazel 0.19 from Releases page. 
3. Installed Visual Studio 2017 since VS 2015 was not installing properly on Windows 10.
4. Installed the c++ build tools for VS 2017.
5. Installed MSYS2, and installed patch, unzip and git
6. Created the environment variables as specified in [Bazel documentation](https://docs.bazel.build/versions/master/windows.html#build-c)
```
C:\projects\bazel> bazel build //examples/cpp:hello-world

C:\projects\bazel> bazel-bin\examples\cpp\hello-world.exe
```
The above example worked fine. The paths I used were different.
7. Used git to clone tensorflow and then checked out **r1.13**
8. Used `python .\configure.py` [Log file](https://github.com/tensorflow/tensorflow/files/3069060/configure_log.txt)
9. Used `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package` [Log file]
(https://github.com/tensorflow/tensorflow/files/3069130/bazel_log.txt)

I have tried the following things as well but they have resulted in errors everytime:

```bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

bazel build --config=opt --define=no_tensorflow_py_deps=true --local_resources 2048,.5,1.0 //tensorflow/tools/pip_package:build_pip_package```


I would appreciate any help regarding this."
27753,ValueError: Provide an input shape for input array 'Placeholder',"**System information**
- OS Platform and Distribution : Ubuntu 16.04
- TensorFlow installed from : conda install
- TensorFlow version : gpu-1.13.1
------------------------------------------------Error-------------------------------------------------------<
`File ""convertpb2tflite.py"", line 5, in <module>
    tflite_model=convert.convert()
File ""/home/yuchen/anaconda2/envs/Conv-TasNet/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 406, in ### ### convert
    ""'{0}'."".format(_tensor_name(tensor)))
### **ValueError: Provide an input shape for input array 'Placeholder'.**
`

----------------------------------------------------My code----------------------------------------------------------<
I try to convert the model from [https://github.com/chaodengusc/DeWave](url) to tflite format.
And, the inference looks like this:
`p_keep_ff = tf.placeholder(tf.float32, shape=None)
p_keep_rc = tf.placeholder(tf.float32, shape=None)
in_data = tf.placeholder(tf.float32, shape=[batch_size, FRAMES_PER_SAMPLE, NEFF])
BiModel = Model(n_hidden, batch_size, p_keep_ff, p_keep_rc)
embedding = BiModel.inference(in_data)`


Then, I use this code to convert frozen.pb:
`convert= tf.lite.TFLiteConverter.from_frozen_graph(""frozen.pb"",input_arrays=[""Placeholder"",""Placeholder_1"",""Placeholder_2""],output_arrays=[""l2_normalize""],input_shapes={""Placeholder"":None,""Placeholder_1"":None,""Placeholder_2"":[128, 100, 129]})
convert.post_training_quantize=True
tflite_model=convert.convert()
open(""model.tflite"",""wb"").write(tflite_model)`


-----------------------------------------------------MyQuestion--------------------------------------------------<
As above, original shape placeholder is none, how should I set the shape of Placeholder?
Thank you!!

"
27752,KeyError: 'ExperimentalFunctionBufferingResource' in Tf >= 1.13 (mkl),"- OS Platform and Distribution - Linux Ubuntu 16.04)
- TensorFlow installed from Anaconda dist 3.6.
- TensorFlow version 1.13-mkl
- Python version: Python 3.6.6 :: Anaconda, Inc.

**Describe the current behavior**
Models trained using the `Iterator-Gen` for data and saved as `meta` & `ckpt` cannot be imported using the `tf.train.import_meta_graph` function. Gives a `KeyError`. 
**Describe the expected behavior**
Working fine with Tensorflow1.12(mkl). Error found in Tf version >= 1.13

**Other info / logs**
Here is the Traceback
`read=tf.train.import_meta_graph(self.paths[0], clear_devices=True)
  File ""/home/pnayak/anaconda3/envs/coreml/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1435, in import_meta_gra                                                    ph
    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]
  File ""/home/pnayak/anaconda3/envs/coreml/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1457, in _import_meta_gr                                                    aph_with_return_elements
    **kwargs))
  File ""/home/pnayak/anaconda3/envs/coreml/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py"", line 806, in import_scoped_meta_graph_with_return_elements                                         
    return_elements=return_elements)
  File ""/home/pnayak/anaconda3/envs/coreml/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func return func(*args, **kwargs)
  File ""/home/pnayak/anaconda3/envs/coreml/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 399, in import_graph_def
    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)
  File ""/home/pnayak/anaconda3/envs/coreml/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 159, in _RemoveDefaultAttrs
    op_def = op_dict[node.op]
KeyError: 'ExperimentalFunctionBufferingResource'`
"
27750,TF 2 - Method estimator.model_to_estimator( ) fails but model.fit works for tf.keras created model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 / Colab ( Linux )
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): **TF 2.0.0 Alpha**
- Python version: 3.5

**Describe the current behavior**
The below code has been taken from the Tensorflow without PHD series - RNN time series prediction.

```
def compile_keras_sequential_model(list_of_layers, msg):
  
    # a tf.keras.Sequential model is a sequence of layers
    model = tf.keras.Sequential(list_of_layers)
    
    # keras does not have a pre-defined metric for Root Mean Square Error. Let's define one.
    def rmse(y_true, y_pred): # Root Mean Squared Error
      return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))
    
    print('\nModel ', msg)
    
    #Optimizer
    sgd = tf.keras.optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
    
    # to finalize the model, specify the loss, the optimizer and metrics
    model.compile(
       loss = 'mean_squared_error',
       optimizer = sgd,
#         optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9),
       metrics = [rmse])
    
    # this prints a description of the model
    model.summary()
    
    return model
```
```
#Create Keras model
def model_fn_keras():
    
    # RNN model (RMSE: 0.164 after 10 epochs)
    model_layers_RNN = [
        l.Reshape([SEQLEN, 1], input_shape=[SEQLEN,]), # [BATCHSIZE, SEQLEN, 1] is necessary for RNN model
        l.GRU(RNN_CELLSIZE, return_sequences=True),  # output shape [BATCHSIZE, SEQLEN, RNN_CELLSIZE]
        l.GRU(RNN_CELLSIZE), # keep only last output in sequence: output shape [BATCHSIZE, RNN_CELLSIZE]
        l.Dense(1) # output shape [BATCHSIZE, 1]
    ]

    model_RNN = compile_keras_sequential_model(model_layers_RNN, ""RNN"")
    
    return(model_RNN)
```
**While converting the Keras model to estimator version, below line gives error:**

`estimator = tf.keras.estimator.model_to_estimator(keras_model=model_fn_keras())`

```
Model  RNN
Model: ""sequential_27""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
reshape_27 (Reshape)         (None, 16, 1)             0         
_________________________________________________________________
unified_gru_57 (UnifiedGRU)  (None, 16, 32)            3360      
_________________________________________________________________
unified_gru_58 (UnifiedGRU)  (None, 32)                6336      
_________________________________________________________________
dense_27 (Dense)             (None, 1)                 33        
=================================================================
Total params: 9,729
Trainable params: 9,729
Non-trainable params: 0
_________________________________________________________________
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1334     try:
-> 1335       return fn(*args)
   1336     except errors.OpError as e:

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\client\session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1317       # Ensure any changes to the graph are reflected in the runtime.
-> 1318       self._extend_graph()
   1319       return self._call_tf_sessionrun(

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\client\session.py in _extend_graph(self)
   1352     with self._graph._session_run_lock():  # pylint: disable=protected-access
-> 1353       tf_session.ExtendSession(self._session)
   1354 

InvalidArgumentError: Node 'training/SGD/gradients/unified_gru_58/StatefulPartitionedCall_grad/StatefulPartitionedCall': Connecting to invalid output 4 of source node unified_gru_58/StatefulPartitionedCall which has 4 outputs

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-52-05ea50da2f1c> in <module>()
      5 #Convert Keras model to Estimator
      6 # tf.disable_eager_execution()
----> 7 estimator = tf.keras.estimator.model_to_estimator(keras_model=model_fn_keras())
      8 # estimator = model_fn_keras()
      9 

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\keras\estimator\__init__.py in model_to_estimator(keras_model, keras_model_path, custom_objects, model_dir, config)
     71       custom_objects=custom_objects,
     72       model_dir=model_dir,
---> 73       config=config)
     74 
     75 # LINT.ThenChange(//tensorflow_estimator/python/estimator/keras.py)

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow_estimator\python\estimator\keras.py in model_to_estimator(keras_model, keras_model_path, custom_objects, model_dir, config)
    488   if keras_model._is_graph_network:
    489     warm_start_path = _save_first_checkpoint(keras_model, custom_objects,
--> 490                                              config)
    491   elif keras_model.built:
    492     logging.warning('You are creating an Estimator from a Keras model manually '

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow_estimator\python\estimator\keras.py in _save_first_checkpoint(keras_model, custom_objects, config)
    365           # pylint: disable=protected-access
    366           model._make_train_function()
--> 367           K._initialize_variables(sess)
    368           # pylint: enable=protected-access
    369         saver = saver_lib.Saver()

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\keras\backend.py in _initialize_variables(session)
    760     # marked as initialized.
    761     is_initialized = session.run(
--> 762         [variables_module.is_variable_initialized(v) for v in candidate_vars])
    763     uninitialized_vars = []
    764     for flag, v in zip(is_initialized, candidate_vars):

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    928     try:
    929       result = self._run(None, fetches, feed_dict, options_ptr,
--> 930                          run_metadata_ptr)
    931       if run_metadata:
    932         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1151     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1152       results = self._do_run(handle, final_targets, final_fetches,
-> 1153                              feed_dict_tensor, options, run_metadata)
   1154     else:
   1155       results = []

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\client\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1327     if handle is None:
   1328       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1329                            run_metadata)
   1330     else:
   1331       return self._do_call(_prun_fn, handle, feeds, fetches)

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1347           pass
   1348       message = error_interpolation.interpolate(message, self._graph)
-> 1349       raise type(e)(node_def, op, message)
   1350 
   1351   def _extend_graph(self):

InvalidArgumentError: Node 'training/SGD/gradients/unified_gru_58/StatefulPartitionedCall_grad/StatefulPartitionedCall': Connecting to invalid output 4 of source node unified_gru_58/StatefulPartitionedCall which has 4 outputs

```

**Describe the expected behavior**
Should not give any error

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27749,negative pixel values for ssim input?,"ssim, and ms_ssim take images as input and max_val which is defined as the dynamic range, implying that images can be eg in [-1,1] with maxval = 2. however they call convert_image_dtype() which assumes pixels are in [0,?]. so it is not clear whether ssim, ms_ssim can take images with negative pixel values."
27748,Object detection API config file/protobuf errors,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Google Deep Learning VM version m15, based on Debian GNU/Linux 9.8**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **I used one of the binaries pre-built for this particular type of VM, tensorflow_gpu-1.12.0-cp27-cp27mu-linux_x86_64.whl**
- TensorFlow version: **1.12**
- Python version: **2.7**
- Installed using virtualenv? pip? conda?: **Used pre-installed binary, however I am using a virtualenv environment for the project**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: **1 x NVIDIA Tesla K80, 4 vCPUs, 15 GB memory, hosted on Compute Engine**

**Describe the problem**
Issue previously posted on Stack Overflow, as I kept running into exception `TypeError: For training mode, the  train_config must be a train_pb2.TrainConfig.` when attempting to run training using an adapted version of the faster_rcnn_resnet50_coco.config configuration. However, there are no obvious issues with config file itself.
[https://stackoverflow.com/questions/55238397/tensorflow-object-detection-train-config-file-error/55287833?noredirect=1#comment97930542_55287833](url)

**Provide the exact sequence of commands / steps that you executed before running into the problem**
The value of isinstance(train_config, train_pb2.TrainConfig) equated to false, which doesn't make sense as the result of printing ( type(config) ) resulted in <class 'object_detection.protos.train_pb2.TrainConfig'>, which I believe is the correct file type.

I manually rebuilt the protobuf compilers as per the documentation here: [https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md#manual-protobuf-compiler-installation-and-usage](url) however I kept running into the same problem. As we thought there may still be an issue with the protoc library, I manually removed and reinstalled protobuf version 3.0, before attempting training again. This resulted in the following traceback:

`Traceback (most recent call last):
  File ""trainer/task.py"", line 24, in <module>
    import tensorflow as tf
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 59, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 15, in <module>
    from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/node_def_pb2.py"", line 15, in <module>
    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>
    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__ha
ndle__pb2
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 22, in <modu
le>
    serialized_pb=_b('\n/tensorflow/core/framework/resource_handle.proto\x12\ntensorflow\""r\n\x13ResourceHandleProt
o\x12\x0e\n\x06\x64\x65vice\x18\x01 \x01(\t\x12\x11\n\tcontainer\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\
x12\x11\n\thash_code\x18\x04 \x01(\x04\x12\x17\n\x0fmaybe_type_name\x18\x05 \x01(\tBn\n\x18org.tensorflow.framework
B\x0eResourceHandleP\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\xf8\x01\x01\x62\x06proto3')
TypeError: __new__() got an unexpected keyword argument 'serialized_options'
`
This problem still persists despite changing the protobuf library to version 3.4.

**Any other info / logs**

I also had to use the command `pip install scikit-image --force-reinstall` in order to get the correct version of NumPy for this project, as I believe the latest release of NumPy caused some incompatibility issues with python 2.7 and was resulting in the error: AttributeError: 'numpy.ufunc' object has no attribute '__module__'
"
27747,Inception for unity,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):  1.13.1




I wanted to use Tensorflow Inception in Unity with reference to this link.
 https://github.com/chen0040/unity-tensorflow-samples

Below is the code I used, but I do not see any errors, but I can not discriminate the image. In other words, nothing is displayed in the concole window. Do you know the solution?
  
first code, 
using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using TensorFlow;

public class InceptionTest
{

    private TFGraph graph;
    private TFSession session;

    private static string[] labels = new string[] {
            ""Linear type"",
            ""point type"",
            ""sidewalk block"",
            
    };

    public void LoadModel(string your_name_graph)
    {
        if (graph == null)
        {
            Debug.Log(""Loading tensor graph "" + your_name_graph);
            TextAsset graphModel = Resources.Load<TextAsset>(your_name_graph);

            if (graphModel == null)
            {
                Debug.LogError(""Failed to load tensor graph "" + your_name_graph);
            }

            graph = new TFGraph();
            graph.Import(graphModel.bytes);
            session = new TFSession(graph);
        }
    }

    public int PredictClass(Texture2D image)
    {
        Debug.Log(image);


        float[] imageBytes = new float[image.width * image.height * 3];

        int idx = 0;
        for (int i = 0; i < image.width; i++)
        {
            for (int j = 0; j < image.height; j++)
            {
                Color pixel = image.GetPixel(i, j);

                imageBytes[idx++] = pixel.r / 255.0f;
                imageBytes[idx++] = pixel.g / 255.0f;
                imageBytes[idx++] = pixel.b / 255.0f;
            }
        }


        var runner = session.GetRunner();
        runner
            .AddInput(graph[""conv2d_1_input""][0], TFTensor.FromBuffer(new TFShape(new long[] { 1, 32, 32, 3 }), imageBytes, 0, 3072))
            .AddInput(graph[""dropout_1/keras_learning_phase""][0], new TFTensor(TFDataType.Bool, new long[0], 1));

        runner.Fetch(graph[""output_node0""][0]);
        float[,] recurrent_tensor = runner.Run()[0].GetValue() as float[,];

        float maxVal = float.MinValue;
        int bestIdx = -1;
        for (int i = 0; i < recurrent_tensor.GetUpperBound(1); ++i)
        {
            float val = recurrent_tensor[0, i];
            if (val > maxVal)
            {
                maxVal = val;
                bestIdx = i;
            }
        }

        return bestIdx;
    }

    public string PredictLabel(Texture2D image)
    {
        int classId = PredictClass(image);
        if (classId >= 0 && classId < labels.Length)
        {
            return labels[classId];
        }
        return ""unknown"";
    }
}



second, 
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class cshInceptionTest : MonoBehaviour
{
    private InceptionTest classifier = new InceptionTest();
    
    // Start is called before the first frame update
    void Start()
    {
        Debug.Log(TensorFlow.TFCore.Version);
        classifier.LoadModel(""tf_models/output_graph"");

        string[] image_names = new string[9];
        int index = 0;
        for (int i = 1; i <= 3; ++i)
        {
            image_names[index++] = ""Linear type"" + i;
            image_names[index++] = ""Point type"" + i;
            image_names[index++] = ""sidewalk block"" + i;
        }
        foreach (string image_name in image_names)
        {
            Debug.Log(image_name);
            Texture2D img = Resources.Load<Texture2D>(""images/inception/"" + image_name);
            Debug.Log(""Predicted: "" + classifier.PredictLabel(img));
        }

    }

    // Update is called once per frame
    void Update()
    {
        
    }
}



First, I try to determine the image of the folder by inserting the second code into any object.
Please Help me T_T



"
27746,How can I warm up the cache by calling the predict method with an all zero inputs in browsers?,"You can warm up the cache by calling the predict method with an all zero inputs
"
27744,Failed to build a debug version of tensorflow from scratch,"**System information**
- OS Platform and Distribution:  ubuntu 16.04 - NVIDIA Docker 
- TensorFlow installed from (source or binary): source 
- TensorFlow version: branch  r1.13 on TensorFlow repo 
- Python version:  3.6.7
- Bazel version (if compiling from source):  0.22.0
- GCC/Compiler version (if compiling from source):  gcc version 5.4.0 20160609
- CUDA/cuDNN version:  CUDA 10.0 and cuDNN 7.3 

**Describe the problem**

**It's failed to build a debug version of TensorFlow from scratch**

I wanted to use `gdb` and `cuda-gdb` tools to debug the c++ code on TensorFlow.  I followed the instruction on `tensorflow/BUILD` file,  which indicated that `dbg` was used to build with debug symbol for Tensorflow as the following:
```
config_setting(
    name = ""debug"",
    values = {
        ""compilation_mode"": ""dbg"",
    },
    visibility = [""//visibility:public""],
)
```
I built the code on branch `r1.13` with the following command:
```
bazel build  -c dbg  --config cuda //tensorflow/tools/pip_package:build_pip_package
```
and I got an error 
```
1 error detected in the compilation of ""/tmp/tmpxft_00007343_00000000-7_zero_initializer_op_gpu.cu.compute_70.cpp1.ii"".
ERROR: /root/tensorflow/tensorflow/contrib/framework/BUILD:109:1: output 'tensorflow/contrib/framework/_objs/python/ops/_variable_ops_gpu/zero_initializer_op_gpu.cu.pic.o' was not created
ERROR: /root/tensorflow/tensorflow/contrib/framework/BUILD:109:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps
```
I found the location on the `bazel-out/k8-dbg/bin/tensorflow/contrib/framework/_objs/python/ops/_variable_ops_gpu` and couldn't find the file `zero_initializer_op_gpu.cu.pic.o` indeed.  
But the error was not always on the same location. If I ran the command again,  the error was almost the same but with the other files, such as:
```
1 error detected in the compilation of ""/tmp/tmpxft_00007f82_00000000-7_lstm_ops_gpu.cu.compute_70.cpp1.ii"".
ERROR: /root/tensorflow/tensorflow/contrib/rnn/BUILD:215:1: output 'tensorflow/contrib/rnn/_objs/python/ops/_lstm_ops_gpu/lstm_ops_gpu.cu.pic.o' was not created
ERROR: /root/tensorflow/tensorflow/contrib/rnn/BUILD:215:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
``` 

**Another way to build with debug symbol**
I noticed that I could build with `cxxopt` command line option to get debug symbol with the following command
```
bazel build  --cxxopt='-g' --config cuda //tensorflow/tools/pip_package:build_pip_package
```
And I succeeded. I scan the sections on the built shared libraries and they showed the `debug` sections: 
```
readelf -S libtensorflow_framework.so

Section Headers:
  [Nr] Name              Type             Address           Offset
       Size              EntSize          Flags  Link  Info  Align
  [ 0]                   NULL             0000000000000000  00000000
       0000000000000000  0000000000000000           0     0     0
  [ 1] .note.gnu.build-i NOTE             0000000000000200  00000200
       0000000000000020  0000000000000000   A       0     0     4
  [ 2] .gnu.hash         GNU_HASH         0000000000000220  00000220
       0000000000034ce4  0000000000000000   A       3     0     8
  [ 3] .dynsym           DYNSYM           0000000000034f08  00034f08
       00000000000afe30  0000000000000018   A       4     3     8
  [ 4] .dynstr           STRTAB           00000000000e4d38  000e4d38
       0000000000275ace  0000000000000000   A       0     0     1
  [ 5] .gnu.version      VERSYM           000000000035a806  0035a806
       000000000000ea84  0000000000000002   A       3     0     2
  [ 6] .gnu.version_d    VERDEF           0000000000369290  00369290
       0000000000000038  0000000000000000   A       4     2     8
  [ 7] .gnu.version_r    VERNEED          00000000003692c8  003692c8
       0000000000000280  0000000000000000   A       4     9     8
  [ 8] .rela.dyn         RELA             0000000000369548  00369548
       00000000000c2ca0  0000000000000018   A       3     0     8
  [ 9] .init             PROGBITS         000000000042c1e8  0042c1e8
       000000000000001a  0000000000000000  AX       0     0     4
  [10] .plt              PROGBITS         000000000042c210  0042c210
       0000000000000010  0000000000000010  AX       0     0     16
  [11] .plt.got          PROGBITS         000000000042c220  0042c220
       0000000000016078  0000000000000000  AX       0     0     8
  [12] .text             PROGBITS         00000000004422a0  004422a0
       00000000008e3cb7  0000000000000000  AX       0     0     32
  [13] malloc_hook       PROGBITS         0000000000d25f60  00d25f60
       0000000000000272  0000000000000000  AX       0     0     16
  [14] .fini             PROGBITS         0000000000d261d4  00d261d4
       0000000000000009  0000000000000000  AX       0     0     4
  [15] .rodata           PROGBITS         0000000000d261e0  00d261e0
       000000000007e79c  0000000000000000   A       0     0     32
  [16] .eh_frame_hdr     PROGBITS         0000000000da497c  00da497c
       000000000002da34  0000000000000000   A       0     0     4
  [17] .eh_frame         PROGBITS         0000000000dd23b0  00dd23b0
       00000000000f75fc  0000000000000000   A       0     0     8
  [18] .gcc_except_table PROGBITS         0000000000ec99ac  00ec99ac
       000000000002ed0e  0000000000000000   A       0     0     4
  [19] .tdata            PROGBITS         00000000010f90f0  00ef90f0
       0000000000000038  0000000000000000 WAT       0     0     8
  [20] .tbss             NOBITS           00000000010f9128  00ef9128
       0000000000000040  0000000000000000 WAT       0     0     8
  [21] .init_array       INIT_ARRAY       00000000010f9128  00ef9128
       0000000000000ce8  0000000000000000  WA       0     0     8
  [22] .fini_array       FINI_ARRAY       00000000010f9e10  00ef9e10
       0000000000000008  0000000000000000  WA       0     0     8
  [23] .jcr              PROGBITS         00000000010f9e18  00ef9e18
       0000000000000008  0000000000000000  WA       0     0     8
  [24] .data.rel.ro      PROGBITS         00000000010f9e20  00ef9e20
       0000000000025c50  0000000000000000  WA       0     0     32
  [25] .dynamic          DYNAMIC          000000000111fa70  00f1fa70
       0000000000000280  0000000000000010  WA       4     0     8
  [26] .got              PROGBITS         000000000111fcf0  00f1fcf0
       000000000001d310  0000000000000008  WA       0     0     8
  [27] .data             PROGBITS         000000000113d000  00f3d000
       0000000000002afc  0000000000000000  WA       0     0     32
  [28] .bss              NOBITS           000000000113fb00  00f3fafc
       0000000000010170  0000000000000000  WA       0     0     64
  [29] .comment          PROGBITS         0000000000000000  00f3fafc
       0000000000000035  0000000000000001  MS       0     0     1
  [30] .debug_aranges    PROGBITS         0000000000000000  00f3fb31
       0000000000069970  0000000000000000           0     0     1
  [31] .debug_info       PROGBITS         0000000000000000  00fa94a1
       0000000007272cf1  0000000000000000           0     0     1
  [32] .debug_abbrev     PROGBITS         0000000000000000  0821c192
       00000000002437ba  0000000000000000           0     0     1
  [33] .debug_line       PROGBITS         0000000000000000  0845f94c
       000000000057ff0f  0000000000000000           0     0     1
  [34] .debug_str        PROGBITS         0000000000000000  089df85b
       0000000003f8f295  0000000000000001  MS       0     0     1
  [35] .debug_loc        PROGBITS         0000000000000000  0c96eaf0
       00000000027734b9  0000000000000000           0     0     1
  [36] .debug_ranges     PROGBITS         0000000000000000  0f0e1fa9
       000000000095cfc0  0000000000000000           0     0     1
  [37] .shstrtab         STRTAB           0000000000000000  0fe3e9e7
       0000000000000188  0000000000000000           0     0     1
  [38] .symtab           SYMTAB           0000000000000000  0fa3ef70
       00000000000dd040  0000000000000018          39   7705     8
  [39] .strtab           STRTAB           0000000000000000  0fb1bfb0
       0000000000322a37  0000000000000000           0     0     1
```   
```
readelf -S _pywrap_tensorflow_internal.so

Section Headers:
  [Nr] Name              Type             Address           Offset
       Size              EntSize          Flags  Link  Info  Align
  [ 0]                   NULL             0000000000000000  00000000
       0000000000000000  0000000000000000           0     0     0
  [ 1] .note.gnu.build-i NOTE             0000000000000200  00000200
       0000000000000020  0000000000000000   A       0     0     4
  [ 2] .gnu.hash         GNU_HASH         0000000000000220  00000220
       00000000000ab0d0  0000000000000000   A       3     0     8
  [ 3] .dynsym           DYNSYM           00000000000ab2f0  000ab2f0
       00000000002928f0  0000000000000018   A       4     3     8
  [ 4] .dynstr           STRTAB           000000000033dbe0  0033dbe0
       00000000011555b2  0000000000000000   A       0     0     1
  [ 5] .gnu.version      VERSYM           0000000001493192  01493192
       0000000000036e14  0000000000000002   A       3     0     2
  [ 6] .gnu.version_d    VERDEF           00000000014c9fa8  014c9fa8
       0000000000000038  0000000000000000   A       4     2     8
  [ 7] .gnu.version_r    VERNEED          00000000014c9fe0  014c9fe0
       0000000000000350  0000000000000000   A       4    11     8
  [ 8] .rela.dyn         RELA             00000000014ca330  014ca330
       00000000004469e8  0000000000000018   A       3     0     8
  [ 9] .init             PROGBITS         0000000001910d18  01910d18
       000000000000001a  0000000000000000  AX       0     0     4
  [10] .plt              PROGBITS         0000000001910d40  01910d40
       0000000000000010  0000000000000010  AX       0     0     16
  [11] .plt.got          PROGBITS         0000000001910d50  01910d50
       00000000000312c0  0000000000000000  AX       0     0     8
  [12] .text             PROGBITS         0000000001942040  01942040
       0000000004f9b080  0000000000000000  AX       0     0     64
  [13] text_env          PROGBITS         00000000068dd0c0  068dd0c0
       00000000000029b4  0000000000000000  AX       0     0     16
  [14] .fini             PROGBITS         00000000068dfa74  068dfa74
       0000000000000009  0000000000000000  AX       0     0     4
  [15] .rodata           PROGBITS         00000000068e0000  068e0000
       0000000000b22bf0  0000000000000000   A       0     0     4096
  [16] .nv_fatbin        PROGBITS         0000000007402bf0  07402bf0
       000000000dc10ba8  0000000000000000   A       0     0     8
  [17] .eh_frame_hdr     PROGBITS         0000000015013798  15013798
       0000000000128404  0000000000000000   A       0     0     4
  [18] .eh_frame         PROGBITS         000000001513bba0  1513bba0
       0000000000672ddc  0000000000000000   A       0     0     8
  [19] .gcc_except_table PROGBITS         00000000157ae97c  157ae97c
       0000000000069e81  0000000000000000   A       0     0     4
  [20] .tdata            PROGBITS         0000000015a18a68  15818a68
       0000000000000008  0000000000000000 WAT       0     0     8
  [21] .tbss             NOBITS           0000000015a18a70  15818a70
       00000000000000b0  0000000000000000 WAT       0     0     8
  [22] .init_array       INIT_ARRAY       0000000015a18a70  15818a70
       0000000000003680  0000000000000000  WA       0     0     8
  [23] .fini_array       FINI_ARRAY       0000000015a1c0f0  1581c0f0
       0000000000000008  0000000000000000  WA       0     0     8
  [24] .jcr              PROGBITS         0000000015a1c0f8  1581c0f8
       0000000000000008  0000000000000000  WA       0     0     8
  [25] .data.rel.ro      PROGBITS         0000000015a1c100  1581c100
       00000000001228f8  0000000000000000  WA       0     0     32
  [26] .dynamic          DYNAMIC          0000000015b3e9f8  1593e9f8
       00000000000002c0  0000000000000010  WA       4     0     8
  [27] .got              PROGBITS         0000000015b3ecb8  1593ecb8
       0000000000062330  0000000000000008  WA       0     0     8
  [28] .data             PROGBITS         0000000015ba1000  159a1000
       000000000000e1ac  0000000000000000  WA       0     0     32
  [29] .nvFatBinSegment  PROGBITS         0000000015baf1b0  159af1b0
       00000000000011b8  0000000000000000  WA       0     0     8
  [30] .bss              NOBITS           0000000015bb0380  159b0368
       0000000000013808  0000000000000000  WA       0     0     64
  [31] .comment          PROGBITS         0000000000000000  159b0368
       0000000000000035  0000000000000001  MS       0     0     1
  [32] .debug_aranges    PROGBITS         0000000000000000  159b039d
       00000000002afd30  0000000000000000           0     0     1
  [33] .debug_info       PROGBITS         0000000000000000  15c600cd
       000000003bd2d579  0000000000000000           0     0     1
  [34] .debug_abbrev     PROGBITS         0000000000000000  5198d646
       000000000080f1d5  0000000000000000           0     0     1
  [35] .debug_line       PROGBITS         0000000000000000  5219c81b
       000000000259eba1  0000000000000000           0     0     1
  [36] .debug_str        PROGBITS         0000000000000000  5473b3bc
       00000000767919c5  0000000000000001  MS       0     0     1
  [37] .debug_loc        PROGBITS         0000000000000000  caeccd81
       00000000120ceea9  0000000000000000           0     0     1
  [38] .debug_ranges     PROGBITS         0000000000000000  dcf9bc2a
       000000000551db30  0000000000000000           0     0     1
  [39] .shstrtab         STRTAB           0000000000000000  e5913cef
       00000000000001a1  0000000000000000           0     0     1
  [40] .symtab           SYMTAB           0000000000000000  e24b9760
       00000000005f9c28  0000000000000018          41   148688     8
  [41] .strtab           STRTAB           0000000000000000  e2ab3388
       0000000002e60967  0000000000000000           0     0     1
```
But in this way, I couldn't debug cuda-related code with `cuda-gdb` after I dumped the sections information about the cubin files on the `_pywrap_tensorflow_internal.so` with `cuobjdump`

**Question**
* How to build a debug version of TensorFlow with bazel ?"
27743,import tensorflow failed after installing tensorflow CPU-only on Windows,"After installing tensorflow CPU-only on Windows, the import tensorflow command fails as below:

```
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: (DLL)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: (DLL)


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

There are related issues, but it seems there has no resolution for the issue until now."
27742,Invalid value in tensor used for shape: -1879048192,"When I  try to reshape a tensor with high dimension, it causes the error as follows:

Invalid value in tensor used for shape: -1879048192

The code is : tf.reshape(scores, shape=[-1, nb_features*nb_features]), where scores is a matrix whose shape is 45192*45192 and nb_features is 45192.

WHy I get this error? Is the dimension too high?

Thank you."
27741,Implementation of class weights with tf.nn.softmax_cross_entropy_with_logits_v2,"**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): Yes, if my skills are enough.


__Requested Feature__
Currently while implementing weighted loss there is tf.nn.weighted_cross_entropy_with_logits which uses sigmoid so it is restricted to binary classification. For multiclass classification using softmax categorical cross entropy, there isn't any feature.

Also in many forums, while implementing the class weights people often suggest something like this:
```python
weighted_logits = tf.multiply(weights, logits)
loss = tf.nn.softmax_cross_entropy_with_logits_v2
```
Is this the correct method? Because we are messing around with the output of the model, while we should have been messing around with the calculated [batch_size, num_class] shape loss tensor.

I think this is the correct way:
```python
        y_hat_softmax = tf.nn.softmax(logits)
        y_cross = y * tf.log(y_hat_softmax)
        weighted_cross = tf.multiply(class_weight, y_cross)
        weighted_cross_reduced = - tf.reduce_sum(weighted_cross, 1)
        loss = tf.reduce_mean(weighted_cross_reduced)
```
Here we are changing the y_cross which should be actually changed, which then changes the gradient flow. But changing logits (by multiplying with weight) will make the model understand that the correct output is different thus biasing result.

eg. If a model has three classes and the output logit is [1, 4, 2] then it is making the correct decision if the actual class is the second one. But multiplying with weight [1, 1, 4] will make the result [1, 4, 8] thus saying that the third logit should have been high to the loss function, thus completely changing the supervision in supervised learning. But actually, we should have been messing around with the loss by scaling the loss according to the weights. 

**Will this change the current api? How?**
Don't know.

**Who will benefit with this feature?**
Whoever is implementing this incorrectly will benefit from this, democratizing Machine Learning.

"
27739,[Tensorflow 2.0] AttributeError: Tensor.op is meaningless when eager execution is enabled.,"**System information**
- Enviroment : Google Colaboratory
- TensorFlow installed from (source or binary):  !pip install tensorflow-gpu==2.0.0-alpha
- TensorFlow version (use command below): 2.0-alpha

My code is followed,

```shell
import numpy as np
import tensorflow as tf
from tensorflow.keras import initializers
from tensorflow.keras import backend as K
print(tf.__version__)

class CustomLayer(tf.keras.layers.Layer):
  def __init__(self, 
               momentum=0.9, 
               epsilon=1e-4,
               axis = -1,
               moving_mean_initializer='zeros',
               **kwargs):
    
    super(CustomLayer, self).__init__(**kwargs)
    self.momentum = momentum
    self.epsilon = epsilon
    self.axis = axis
    self.moving_mean_initializer = initializers.get(moving_mean_initializer)

  def build(self, input_shape):
    param_shape = input_shape[-1]
    
    self.moving_mean = self.add_weight(shape=param_shape,
                                       name='moving_mean',
                                       initializer=self.moving_mean_initializer,
                                       trainable=False)
    
  def call(self, inputs, training=None):
    input_shape = K.int_shape(inputs)
    boundary_axis = 1
    ndim = len(input_shape)
    
    reduction_axes = list(range(ndim))
    for i in range(1): del reduction_axes[self.axis]
    
    broadcast_shape = [1] * ndim
    broadcast_shape[self.axis] = input_shape[self.axis]
    
    mean = K.reshape(K.mean(inputs, axis=reduction_axes), broadcast_shape)
    
    
    update_list = []
    update_list.append(K.moving_average_update(K.reshape(self.moving_mean, broadcast_shape), mean, self.momentum))
    self.add_update(update_list, inputs)
    
    return K.in_train_phase(1,
                            0,
                            training=training) 

a = tf.constant(np.random.randn(128,224,224,3), dtype=tf.float32)
custom_layer_1 = CustomLayer()(a)

print(custom_layer_1)
```

Then I got this Error,

```shell
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-3-26a5ee86fab6> in <module>()
      4 #inputs_data = tf.stack([a,b], axis=1)
      5 
----> 6 custom_layer_1 = CustomLayer()(a)
      7 
      8 print(custom_layer_1)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    658           with base_layer_utils.autocast_context_manager(
    659               input_list, self._mixed_precision_policy.should_cast_variables):
--> 660             outputs = self.call(inputs, *args, **kwargs)
    661           self._handle_activity_regularization(inputs, outputs)
    662           self._set_mask_metadata(inputs, outputs, previous_mask)

<ipython-input-2-1f5e23b011a1> in call(self, inputs, training)
     42 
     43     update_list = []
---> 44     update_list.append(K.moving_average_update(K.reshape(self.moving_mean, broadcast_shape), mean, self.momentum))
     45     self.add_update(update_list, inputs)
     46 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in moving_average_update(x, value, momentum)
   1399   from tensorflow.python.training import moving_averages  # pylint: disable=g-import-not-at-top
   1400   return moving_averages.assign_moving_average(
-> 1401       x, value, momentum, zero_debias=True)
   1402 
   1403 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/moving_averages.py in assign_moving_average(variable, value, decay, zero_debias, name)
    101         return strategy.extended.update(v, update_fn, args=(value,))
    102 
--> 103       return replica_context.merge_call(merge_fn, args=(variable, value))
    104     else:
    105       strategy = distribution_strategy_context.get_cross_replica_context()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py in merge_call(self, merge_fn, args, kwargs)
   1373     if kwargs is None:
   1374       kwargs = {}
-> 1375     return self._merge_call(merge_fn, args, kwargs)
   1376 
   1377   def _merge_call(self, merge_fn, args, kwargs):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py in _merge_call(self, merge_fn, args, kwargs)
   1380         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access
   1381     try:
-> 1382       return merge_fn(self._strategy, *args, **kwargs)
   1383     finally:
   1384       _pop_per_thread_mode()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/moving_averages.py in merge_fn(strategy, v, value)
     99         value = strategy.extended.reduce_to(
    100             ds_reduce_util.ReduceOp.MEAN, value, v)
--> 101         return strategy.extended.update(v, update_fn, args=(value,))
    102 
    103       return replica_context.merge_call(merge_fn, args=(variable, value))

/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py in update(self, var, fn, args, kwargs, group)
   1173     if kwargs is None:
   1174       kwargs = {}
-> 1175     return self._update(var, fn, args, kwargs, group)
   1176 
   1177   def _update(self, var, fn, args, kwargs, group):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py in _update(self, var, fn, args, kwargs, group)
   1544     # The implementations of _update() and _update_non_slot() are identical
   1545     # except _update() passes `var` as the first argument to `fn()`.
-> 1546     return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)
   1547 
   1548   def _update_non_slot(self, colocate_with, fn, args, kwargs, should_group):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py in _update_non_slot(self, colocate_with, fn, args, kwargs, should_group)
   1550     # once that value is used for something.
   1551     with ops.colocate_with(colocate_with), UpdateContext(colocate_with):
-> 1552       result = fn(*args, **kwargs)
   1553       if should_group:
   1554         return result

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/moving_averages.py in update_fn(v, value, decay)
     85       decay = math_ops.cast(decay, v.dtype.base_dtype)
     86     if zero_debias:
---> 87       update_delta = _zero_debias(v, value, decay)
     88     else:
     89       update_delta = (v - value) * decay

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/moving_averages.py in _zero_debias(unbiased_var, value, decay)
    204   """"""
    205   with variable_scope.variable_scope(
--> 206       unbiased_var.op.name, values=[unbiased_var, value, decay]) as scope:
    207     with ops.colocate_with(unbiased_var):
    208       with ops.init_scope():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in op(self)
    932   def op(self):
    933     raise AttributeError(
--> 934         ""Tensor.op is meaningless when eager execution is enabled."")
    935 
    936   @property

AttributeError: Tensor.op is meaningless when eager execution is enabled.
```

My purpose is to customize the batch normalization layer. And I refer to this document : https://github.com/keras-team/keras/blob/master/keras/layers/normalization.py#L16

I think that this error is caused by K.moving_average_update. Tensorflow 2.0 is not allowed K.moving_average_update api? I found this api in tf 2.0 docs. 

How can I solve this error?"
27732,"When test An exception has occurred, use %tb to see the full traceback.","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I download code from https://github.com/tegg89/SRCNN-Tensorflow
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 64 bits
- TensorFlow installed from (source or binary): anaconda3 64bits  conda install
- TensorFlow version (use command below): tensorflow-gpu 1.13.1

- Python version: 3.6.8  spyder 3.3.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0.130  cudnn 7.3.1 installed via conda with tensorflow together
- GPU model and memory: notebook gtx1060 3GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
![image](https://user-images.githubusercontent.com/41459859/55927092-bd9e8d80-5c4e-11e9-9b88-afa87c4d1717.png)


**Describe the current behavior**

runfile('F:/F_coding/other/SRCNN-Tensorflow-master/main.py', wdir='F:/F_coding/other/SRCNN-Tensorflow-master')
C:\Users\myaccount\AppData\Roaming\Python\Python36\site-packages\h5py\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
{'batch_size': <absl.flags._flag.Flag object at 0x000001A7E0DF4518>,
 'c_dim': <absl.flags._flag.Flag object at 0x000001A7E6D0C390>,
 'checkpoint_dir': <absl.flags._flag.Flag object at 0x000001A7E6D0C588>,
 'epoch': <absl.flags._flag.Flag object at 0x000001A7E0DDEC18>,
 'h': <tensorflow.python.platform.app._HelpFlag object at 0x000001A7E6D0C710>,
 'help': <tensorflow.python.platform.app._HelpFlag object at 0x000001A7E6D0C710>,
 'helpfull': <tensorflow.python.platform.app._HelpfullFlag object at 0x000001A7E6D0C780>,
 'helpshort': <tensorflow.python.platform.app._HelpshortFlag object at 0x000001A7E6D0C7F0>,
 'image_size': <absl.flags._flag.Flag object at 0x000001A7E0DF4908>,
 'is_train': <absl.flags._flag.BooleanFlag object at 0x000001A7E6D0C630>,
 'label_size': <absl.flags._flag.Flag object at 0x000001A7E175F6D8>,
 'learning_rate': <absl.flags._flag.Flag object at 0x000001A7E6AABBE0>,
 'sample_dir': <absl.flags._flag.Flag object at 0x000001A7E6D0C5F8>,
 'scale': <absl.flags._flag.Flag object at 0x000001A7E6D0C438>,
 'stride': <absl.flags._flag.Flag object at 0x000001A7E6D0C4E0>}
WARNING:tensorflow:From D:\scholarship\anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From D:\scholarship\anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\util\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
 [*] Reading checkpoints...
WARNING:tensorflow:From D:\scholarship\anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\training\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
INFO:tensorflow:Restoring parameters from checkpoint\srcnn_21\SRCNN.model-2550000
 [*] Load SUCCESS
Testing...
An exception has occurred, use %tb to see the full traceback.
![image](https://user-images.githubusercontent.com/41459859/55927114-d1e28a80-5c4e-11e9-8537-53bf878702d3.png)


SystemExit

**Describe the expected behavior**
idk
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27730,"freezed pb Convert to tflite, ValueError: NodeDef mentions attr 'half_pixel_centers' not in Op<name=ResizeBilinear ","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.13,1.14
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9.2, cudnn7
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
when convert freezed pb to tflite, the following problem happened:

ValueError: NodeDef mentions attr 'half_pixel_centers' not in Op<name=ResizeBilinear; signature=images:T, size:int32 -> resized_images:float; attr=T:type,allowed=[DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=align_corners:bool,default=false>; NodeDef: {{node ResizeBilinear}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).

Anyone has any ideas to solve it ?

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27728,Decoupling preprocessing and training,"**Describe the feature and the current behavior/state.**

Currently, TensorFlow performs preprocessing and training on the same host. In situations where the data preprocessing is efficient and yet the CPU resources available on the host are not sufficient to keep up with the training workload on the accelerator, the preprocessing becomes a bottleneck. 

The proposed feature is to extend the tf.data / tf.distribute APIs to make it possible to decouple the preprocessing from training.

**Will this change the current api? How?**

Yes. In the least, the users will need to specify the set of ""input"" hosts that should perform the preprocessing and the set of ""training"" hosts to perform the training. 

The preferred solution would allow users to express their input pipeline in tf.data as if it was executing on the same host as training and through the means of tf.data / tf.distribute configuration express how it should be distributed.

**Who will benefit with this feature?**

Users that execute preprocessing intensive training jobs."
27726,Support large embeddings with `MirroredStrategy` and `MultiWorkerMirroredStrategy`,"**System information**
- TensorFlow version (you are using): TF 2.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Currently, `MirroredStrategy` and `MultiWorkerMirroredStrategy` assume a model replica including its embeddings could fit in one GPU or one machine. Allgather is used to aggregated gradients with respect to embeddings.

We would like to understand whether it is necessary and important to support
1. placing embeddings on host with `MirroredStrategy` and why using `ParameterServerStrategy` is not ideal
2. replicating embeddings on host with `MultiWorkerMirroredStrategy` and why in this case using `ParameterServerStrategy` is not ideal
2. sharding embeddings in `MultiWorkerMirroredStrategy` and why `ParameterServerStrategy` is not ideal.

**Will this change the current api? How?**
Yes.

**Who will benefit with this feature?**
Users who use large embeddings in their models.

**Any Other info.**
N/A
"
27725,Support synchronous training with parameter servers using Distribution Strategies,"**System information**
- TensorFlow version (you are using): TF 2.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Right now we have `MirroredStrategy` and `MultiWorkerMirroredStrategy` for synchronous training where variables and ops are all replicated and all-reduce is used for gradient aggregation. We also have `ParameterServerStrategy` where gradient updates from workers are purely asynchronous since `SyncReplicasOptimizer` is buggy and has been deprecated.
 
We would like to first collect use cases where synchronous training with `MirroredStrategy` and `MultiWorkerMirroredStrategy` is not ideal and synchronous training with parameter servers is necessary.

If this feature is necessary and important enough, we will then use this issue to track the progress of the development of this feature.

We have a separate feature request to support large embeddings with `MirroredStrategy` and `MultiWorkerMirroredStrategy`: https://github.com/tensorflow/tensorflow/issues/27726

**Will this change the current api? How?**
Yes.

**Who will benefit with this feature?** 
Those who use distributed training.

**Any Other info.**
N/A
"
27724,Support model parallelism in tf.distribute.Strategy,"Current tf.distribute.Strategy only supports data parallelism. We would like to support the following form of model parallelism: user specifies a number of ""logical devices"" per replica upfront, and then explicitly places ops on a specific logical device. 


"
27720,How i upgrade tensorflow 1.13.1 to 1.4.0 in window(without GPU),"i want to upgrade tensroflow 1.13.1 to 1.4.0 in window 10 without GPU 
i upgrade it but it stop 1.13.1. 
is this possible for me to reach 1.4.0 tensorflow version,
 if it is then hoW?"
27719,tf-nightly-2.0-preview's tf.summary doesn't work with tensorboard,"**System information**
- Have I written custom code: no
- OS: macOS 10.14.4
- TensorFlow installed from: binary
- TensorFlow version: 2.0.0-dev20190410 (latest nightly)
- Python version: 3.6.7 (miniconda)

**Current behavior & code to reproduce the issue**

The following code

```py
import tensorflow as tf

summary_writer = tf.summary.create_file_writer(""tmp"")

with summary_writer.as_default():
    for i in range(100):
        tf.summary.scalar(""index"", i, step=i)
summary_writer.close()
```

outputs an events file to `/tmp` but trying to view it with `tensorboard --logdir ./tmp`  throws
```
Exception in thread Reloader:
AttributeError: module 'tensorflow._api.v2.compat.v1' has no attribute 'pywrap_tensorflow'
```
followed by
```
W0410 17:26:13.712886 123145489154048 core_plugin.py:172] Unable to get first event timestamp for run .: No event timestamp could be found
```
and an empty TB dashboard. I'm running the latest `tb-nightly`. Any ideas what's causing this?

**Perhaps related:**
- #27713"
27716,"Distribution strategies and conditional parameters update fails with ""Operation has been marked as not fetchable""","**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: `2.0.0a0`
- Python version: 3.5.2

**Describe the current behavior**

When using conditional parameters update and a distribution strategy in graph mode, the first `sess.run` fails with the error:

> ""Operation '*' has been marked as not fetchable.""

**Describe the expected behavior**

Distribution strategies should support conditional parameters update.

**Code to reproduce the issue**

I tried to compile a small and representative snippet that is the base logic for gradient accumulation:

```python
import tensorflow as tf

def train_op(accum_steps=None):
    step = tf.Variable(initial_value=0, dtype=tf.int32, trainable=False)
    optimizer = tf.optimizers.SGD(learning_rate=0.1)
    variables = [tf.Variable(tf.zeros([4, 5], dtype=tf.float32))]
    gradients = [tf.Variable(tf.ones([4, 5], dtype=tf.float32), trainable=False)]
    if accum_steps is None:
        return optimizer.apply_gradients(zip(gradients, variables))
    else:
        return tf.cond(
            tf.equal((step + 1) % accum_steps, 0),
            true_fn=lambda: optimizer.apply_gradients(zip(gradients, variables)),
            false_fn=tf.no_op)

devices = [""/gpu:2"", ""/gpu:3""]
strategy = tf.distribute.MirroredStrategy(devices=devices)
#strategy = tf.distribute.OneDeviceStrategy(device=devices[0])
accum_count = 8

with tf.Graph().as_default():
    with strategy.scope():
        train_op = strategy.extended.call_for_each_replica(train_op, args=(accum_count,))

    config = tf.compat.v1.ConfigProto(allow_soft_placement=True)
    with tf.compat.v1.Session(config=config) as sess:
        sess.run(tf.compat.v1.global_variables_initializer())
        _ = sess.run(train_op)
```

The above code does not fail if `OneDeviceStrategy` is used or `accum_count` is set to `None`.

**Other info / logs**

```text
Traceback (most recent call last):
  File ""cond_apply.py"", line 27, in <module>
    sess.run(tf.compat.v1.global_variables_initializer())
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 930, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1138, in _run
    self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 480, in __init__
    self._assert_fetchable(graph, fetch)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 497, in _assert_fetchable
    'Operation %r has been marked as not fetchable.' % op.name)
ValueError: Operation 'init' has been marked as not fetchable.
```

---

For an additional context, I'm porting a code from V1 `tf.estimator` and manual graph replication to V2 `tf.estimator` and distribution strategies. I'm struggling to port the gradient accumulation code that [we have working in V1](https://github.com/OpenNMT/OpenNMT-tf/blob/v1.22.0/opennmt/utils/optim.py#L199-L261). "
27715,Tensorflow Docker: Failed to get a convolutional algorithm. This is probably because cuDNN failed to initialize. ,"Hey all, 

I saw that there are already some issues regarding` Failed to get convolutional algorithm. This is probably because cuDNN failed to initialize, so try look to see if a warning log message was printed above. ` Probably this error is caused by an incompatibility of cuDNN or so. However, I am working within an Docker image. I run `nvidia-docker run -it -v home_Path:container_path --rm tensorflow/tensorflow:latest-gpu-py3 ` and inside this image i run a Faster R-CNN code. The first two/three times everything worked well but afterwards every time this error is printed and the programm stops running. Even if I restart the container (by exiting and executing the same command another time), this error appear.  

I hope you guys can help me. I am struggling since days with this problem and I have really no clue what's the problem or how to solve it. 


Thanks a lot in advance. 
------------------------------------------------------------------------------------------------------
Output: 


`../Lib/faster_rcnn_config.py:313: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  yaml_cfg = edict(yaml.load(f))
Restoring from Cityscapes.yml file
{'seed': 1234, 'restore_slim_file': '/root/OD/tf-Faster-RCNN/Data/ResNet50/resnet_v1_50.ckpt', 'num_epochs': 5, 'file_epoch': 1, 'run_num': 31, 'restore': False, 'restore_num': 1, 'batch_size': 1, 'data_directory': '/root/OD/tf-Faster-RCNN/Data/Cityscapes_for_FasterRCNN_REDUCED_REDUCED_imagesize/', 'save_directory': '../Logs/', 'gpu': 0, 'vis': False, 'display_step': 1000, 'learning_rate': 0.003, 'model_directory': 'Cityscapes/'}
Using GPU 0
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Tensor(""model/resnet_v1_50/block3/unit_6/bottleneck_v1/Relu:0"", shape=(1, ?, ?, 1024), dtype=float32)
conv_1 output: (1, ?, ?, 512)
conv_1 output: (1, ?, ?, 18)
WARNING:tensorflow:From ../Networks/anchor_target_layer.py:40: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, use
    tf.py_function, which takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means 'tf.py_function's can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    
conv_1 output: (1, ?, ?, 36)
WARNING:tensorflow:From ../Lib/roi_pool.py:31: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
flat_1 output: (?, 50176)
WARNING:tensorflow:From ../Lib/TensorBase/tensorbase/base.py:274: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use 'rate' instead of 'keep_prob'. Rate should be set to 'rate = 1 - keep_prob'.
fc_1 output: (?, 4096)
fc_2 output: (?, 4096)
fc_1 output: (?, 4)
fc_1 output: (?, 16)
Tensor(""model_1/resnet_v1_50/block3/unit_6/bottleneck_v1/Relu:0"", shape=(1, ?, ?, 1024), dtype=float32)
conv_1 output: (1, ?, ?, 512)
conv_1 output: (1, ?, ?, 18)
conv_1 output: (1, ?, ?, 36)
flat_1 output: (?, 50176)
fc_1 output: (?, 4096)
fc_2 output: (?, 4096)
fc_1 output: (?, 4)
fc_1 output: (?, 16)
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
2019-04-10 15:29:35.327116: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x7d5c080 executing computations on platform CUDA. Devices:
2019-04-10 15:29:35.327156: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5
2019-04-10 15:29:35.329004: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3700305000 Hz
2019-04-10 15:29:35.329289: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x7dc5f20 executing computations on platform Host. Devices:
2019-04-10 15:29:35.329312: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-04-10 15:29:35.329518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:04:00.0
totalMemory: 7.76GiB freeMemory: 6.97GiB
2019-04-10 15:29:35.329541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-04-10 15:29:35.493916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-04-10 15:29:35.493961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-04-10 15:29:35.493975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-04-10 15:29:35.494236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7871 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:04:00.0, compute capability: 7.5)
2019-04-10 15:29:36.902368: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 7.69G (8253855488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
Restoring TF-Slim Model.
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
INFO:tensorflow:Restoring parameters from /root/OD/tf-Faster-RCNN/Data/ResNet50/resnet_v1_50.ckpt
2019-04-10 15:29:38.656957: W tensorflow/core/framework/allocator.cc:124] Allocation of 822083584 exceeds 10% of system memory.
2019-04-10 15:29:38.879858: W tensorflow/core/framework/allocator.cc:124] Allocation of 822083584 exceeds 10% of system memory.
2019-04-10 15:29:39.126316: W tensorflow/core/framework/allocator.cc:124] Allocation of 822083584 exceeds 10% of system memory.
2019-04-10 15:29:39.343851: W tensorflow/core/framework/allocator.cc:124] Allocation of 822083584 exceeds 10% of system memory.
2019-04-10 15:29:39.831089: W tensorflow/core/framework/allocator.cc:124] Allocation of 822083584 exceeds 10% of system memory.
Learning Rate: 0.003000
Epochs: 5
None
{'seed': 1234, 'restore_slim_file': '/root/OD/tf-Faster-RCNN/Data/ResNet50/resnet_v1_50.ckpt', 'num_epochs': 5, 'file_epoch': 1, 'run_num': 31, 'restore_directory': '../Logs/Cityscapes/Model1/', 'restore': False, 'restore_num': 1, 'batch_size': 1, 'data_directory': '/root/OD/tf-Faster-RCNN/Data/Cityscapes_for_FasterRCNN_REDUCED_REDUCED_imagesize/', 'save_directory': '../Logs/', 'gpu': 0, 'vis': False, 'display_step': 1000, 'logging_directory': '../Logs/Cityscapes/Model31/', 'learning_rate': 0.003, 'model_directory': 'Cityscapes/'}
Training for 5 epochs
epochs:   0%|                                                                                                                                                                                                                                                | 0/5 [00:00<?, ?it/s2019-04-10 15:29:41.114390: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally                                                                                                                     | 0/2096 [00:00<?, ?it/s]
2019-04-10 15:29:41.140134: E tensorflow/stream_executor/cuda/cuda_blas.cc:510] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-04-10 15:29:41.141836: E tensorflow/stream_executor/cuda/cuda_blas.cc:510] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-04-10 15:29:41.143191: E tensorflow/stream_executor/cuda/cuda_blas.cc:510] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-04-10 15:29:41.144476: E tensorflow/stream_executor/cuda/cuda_blas.cc:510] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-04-10 15:29:41.154047: E tensorflow/stream_executor/cuda/cuda_blas.cc:510] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-04-10 15:29:41.155242: E tensorflow/stream_executor/cuda/cuda_blas.cc:510] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-04-10 15:29:41.157620: E tensorflow/stream_executor/cuda/cuda_blas.cc:510] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-04-10 15:29:41.158772: E tensorflow/stream_executor/cuda/cuda_blas.cc:510] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-04-10 15:29:41.161637: E tensorflow/stream_executor/cuda/cuda_blas.cc:510] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-04-10 15:29:41.162776: E tensorflow/stream_executor/cuda/cuda_blas.cc:510] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-04-10 15:29:41.165628: E tensorflow/stream_executor/cuda/cuda_blas.cc:510] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-04-10 15:29:41.166792: E tensorflow/stream_executor/cuda/cuda_blas.cc:510] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-04-10 15:29:41.195178: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-04-10 15:29:41.197679: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR

Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node model/resnet_v1_50/conv1/Conv2D}}]]
	 [[{{node model/roi_proposal/rpn_softmax/transpose_3}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""faster_rcnn_resnet50ish.py"", line 300, in <module>
    main()
  File ""faster_rcnn_resnet50ish.py"", line 293, in main
    model.train()
  File ""faster_rcnn_resnet50ish.py"", line 169, in train
    summary = self._run_train_iter(feed_dict)
  File ""faster_rcnn_resnet50ish.py"", line 137, in _run_train_iter
    summary, _ = self.sess.run([self.merged, self.optimizer], feed_dict=feed_dict)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node model/resnet_v1_50/conv1/Conv2D (defined at ../Networks/resnet50_reduced.py:79) ]]
	 [[node model/roi_proposal/rpn_softmax/transpose_3 (defined at ../Lib/rpn_softmax.py:46) ]]

Caused by op 'model/resnet_v1_50/conv1/Conv2D', defined at:
  File ""faster_rcnn_resnet50ish.py"", line 300, in <module>
    main()
  File ""faster_rcnn_resnet50ish.py"", line 291, in main
    model = FasterRcnnRes50(flags, dictionary)
  File ""faster_rcnn_resnet50ish.py"", line 44, in __init__
    super().__init__(flags_input, flags_input['run_num'], vram=cfg.VRAM, restore=flags_input['restore_num'], restore_slim=flags_input['restore_slim_file'])
  File ""../Lib/TensorBase/tensorbase/base.py"", line 676, in __init__
    self._network()
  File ""faster_rcnn_resnet50ish.py"", line 82, in _network
    self._faster_rcnn(self.x['TRAIN'], self.gt_boxes['TRAIN'], self.im_dims['TRAIN'], 'TRAIN')
  File ""faster_rcnn_resnet50ish.py"", line 93, in _faster_rcnn
    feature_maps = resnet50_reduced(x)
  File ""../Networks/resnet50_reduced.py"", line 79, in resnet50_reduced
    net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope='conv1')
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/slim/python/slim/nets/resnet_utils.py"", line 146, in conv2d_same
    scope=scope)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1155, in convolution2d
    conv_dims=2)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1058, in convolution
    outputs = layer.apply(inputs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 1227, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py"", line 530, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 554, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/convolutional.py"", line 194, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py"", line 966, in __call__
    return self.conv_op(inp, filter)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py"", line 591, in __call__
    return self.call(inp, filter)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py"", line 208, in __call__
    name=self.name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 1026, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

UnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node model/resnet_v1_50/conv1/Conv2D (defined at ../Networks/resnet50_reduced.py:79) ]]
	 [[node model/roi_proposal/rpn_softmax/transpose_3 (defined at ../Lib/rpn_softmax.py:46) ]]
`
"
27714,"Convert freezed pb graph to tensorflow lite failed, problems:""Check failed: dim_x == dim_y (8 vs. 48)Dimensions must match""","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.13
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9.2, cudnn7
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
when convert tofreezed pb to tflite, following problem happened:

ConverterError: TOCO failed. See console for info.
2019-04-10 22:06:36.005315: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 473 operators, 617 arrays (0 quantized)
2019-04-10 22:06:36.021194: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 473 operators, 617 arrays (0 quantized)
2019-04-10 22:06:36.021657: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:118] Check failed: dim_x == dim_y (8 vs. 48)Dimensions must match
Fatal Python error: Aborted

Current thread 0x00007fb89fd9c740 (most recent call first):
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33 in execute
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/absl/app.py"", line 251 in _run_main
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/absl/app.py"", line 300 in run
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40 in run
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59 in main
  File ""/home/syshang/anaconda3/envs/tfnightly0410/bin/toco_from_protos"", line 10 in <module>
Aborted (core dumped)


**Describe the expected behavior**
"
27713,Tensorboard file writer not working any more after nightly build tf-nightly 1.14.1.dev20190410,"I had to restart my script this morning and it automatically installs the latest nightly build of tensorflow (no other changes)

Unfortunately the file write doesn't produce any output anymore.
Yesterday with build 2.0.0-dev20190405 it worked just fine.

**System information**
- Colab with tf-nightly 1.14.1.dev20190410 and w.o. HW acceleration
- installed with !pip install tf-nightly-2.0-preview

**Describe the current behavior**
Tensorboard File Writer does not create any output files. The folder itself is still created.

**Code to reproduce the issue**
```
writer = tf.summary.create_file_writer(TENSORBOARD_LOG)               
with writer.as_default():
    tf.summary.scalar('running_reward', running_reward,current_run)
    for w in self.model.trainable_variables:
        tf.summary.histogram(name=w.name, data=w, step=current_run)

```

"
27711,error save keras model have Conv2D and dilation_rate=2 use tf.saved_model.save(),"install tensorflow2.0: pip install tf-nightly-2.0-preview==2.0.0.dev20190405

keras model have use Conv2D with dilation_rate=2.
tf.saved_model.save(model, OUTPUT_DIR) out error:

```
Traceback (most recent call last):
  File ""convert_h5_to_tflite.py"", line 146, in <module>
    tf.saved_model.save(model, OUTPUT_DIR)
  File ""/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py"", line 798, in save
    meta_graph_def, saveable_view, signatures)
  File ""/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py"", line 529, in _fill_meta_graph_def
    signatures = _generate_signatures(signature_functions, resource_map)
  File ""/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py"", line 407, in _generate_signatures
    function, mapped_inputs, resource_map)
  File ""/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py"", line 358, in _call_function_with_mapped_captures
    function.graph.captures, resource_map)
  File ""/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py"", line 280, in _map_captures_to_created_tensors
    .format(interior))
AssertionError: Tried to export a function which references untracked object Tensor(""StatefulPartitionedCall/forward_6_1b_conv2D/stack:0"", shape=(2, 2), dtype=int32).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.
```
dilation_rate=1 not error"
27709,Link to the definiton tf.image broken,"### **On the page** 
https://www.tensorflow.org/api_docs/python/tf/image

### **The link Under heading**
 `Module
    tf.Image`

https://www.tensorflow.org/code/stable/tensorflow/_api/v1/image/__init__.py
**is broken.**


"
27708,[TF 2.0] Build issue for r2.0 branch,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):
Source
- TensorFlow version:
Branch r2.0
- Python version:
3.5.6 (from Anaconda)
- Installed using virtualenv? pip? conda?:
miniconda
- Bazel version (if compiling from source):
0.24.0
- GCC/Compiler version (if compiling from source):
5.4.0
- CUDA/cuDNN version:
CPU version
- Android SDK build tools version
28.0.3

**Describe the problem**

While building TensorFlow from `r2.0` branch the build fails because of problems with Basel build files (it seems like this).

**Provide the exact sequence of commands / steps that you executed before running into the problem**

`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`

**Any other info / logs**
```
DEBUG: Rule 'build_bazel_rules_swift' indicated that a canonical reproducible form can be obtained by modifying arguments commit = ""001736d056d7eae20f1f4da41bc9e6f036857296"", shallow_since = ""1547844730 -0800"" and dropping [""tag""]
DEBUG: /home/p.vytovtov/.cache/bazel/_bazel_p.vytovtov/bd2de6272d8995218e0b1ae22a8f3506/external/build_bazel_rules_apple/apple/repositories.bzl:35:5: 
WARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.

ERROR: /home/p.vytovtov/.cache/bazel/_bazel_p.vytovtov/bd2de6272d8995218e0b1ae22a8f3506/external/hwloc/BUILD.bazel:189:54: The `+` operator for dicts is deprecated and no longer supported. Please use the `update` method instead. You can temporarily enable the `+` operator by passing the flag --incompatible_disallow_dict_plus=false
ERROR: /home/p.vytovtov/.cache/bazel/_bazel_p.vytovtov/bd2de6272d8995218e0b1ae22a8f3506/external/hwloc/BUILD.bazel:200:9: Traceback (most recent call last):
        File ""/home/p.vytovtov/.cache/bazel/_bazel_p.vytovtov/bd2de6272d8995218e0b1ae22a8f3506/external/hwloc/BUILD.bazel"", line 195
                template_rule(name = ""include_private_hwloc_au..."", <3 more arguments>)
        File ""/home/p.vytovtov/.cache/bazel/_bazel_p.vytovtov/bd2de6272d8995218e0b1ae22a8f3506/external/hwloc/BUILD.bazel"", line 199, in template_rule
                if_cuda(_INCLUDE_PRIVATE_HWLOC_AUTOIGEN_..., ...)
        File ""/home/p.vytovtov/.cache/bazel/_bazel_p.vytovtov/bd2de6272d8995218e0b1ae22a8f3506/external/hwloc/BUILD.bazel"", line 200, in if_cuda
                _INCLUDE_PRIVATE_HWLOC_AUTOIGEN_CONFIG_H_CUDA_SUBS
name '_INCLUDE_PRIVATE_HWLOC_AUTOIGEN_CONFIG_H_CUDA_SUBS' is not defined
ERROR: /home/p.vytovtov/Documents/tensorflow/tensorflow/core/BUILD:2319:1: Target '@hwloc//:hwloc' contains an error and its package is in error and referenced by '//tensorflow/core:lib_internal_impl'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Cannot compute config conditions
INFO: Elapsed time: 0.395s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (1 packages loaded, 2 targets configured)
```"
27707,convert to tflite in tensorflow 2.0 not fold batchnorm,"pip install tf-nightly-2.0-preview==2.0.0.dev20190405, it don't fold BatchNorm therefore it generate error tflife not support BatchNormalization. It must auto fold BatchNorm?
code:
model is tensorflow.keras model have BatchNormalization layer
```
@tf.function(input_signature=[tf.TensorSpec([None,300,300,3], tf.float32, name='fts_input_images'), tf.TensorSpec([], tf.float32, name='fts_input_threshold_confident')])
def func(x, confident_threshold):
	x = model(x)
	x = tf.identity(x, name='fts_output_no_merge')
	x = fn(x, confident_threshold)
	x = tf.identity(x, name='final_fts_output_full')
	return x
concrete_func = func.get_concrete_function()
converter = tf.lite.TFLiteConverter.from_concrete_function(concrete_func)
tflite_model = converter.convert()
```

log error:
```
2019-04-12 16:48:44.545659: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-04-12 16:48:44.545815: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2019-04-12 16:48:44.617968: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:684] Optimization results for grappler item: graph_to_optimize
2019-04-12 16:48:44.618014: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   function_optimizer: Graph size after: 1313 nodes (0), 1509 edges (0), time = 8.142ms.
2019-04-12 16:48:44.618027: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   function_optimizer: Graph size after: 1313 nodes (0), 1509 edges (0), time = 3.993ms.
2019-04-12 16:48:45.135108: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-04-12 16:48:45.135255: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2019-04-12 16:48:45.667690: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:684] Optimization results for grappler item: graph_to_optimize
2019-04-12 16:48:45.667785: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   model_pruner: Graph size after: 1285 nodes (-28), 1493 edges (-16), time = 33.469ms.
2019-04-12 16:48:45.667826: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   implementation_selector: Graph size after: 1285 nodes (0), 1493 edges (0), time = 4.814ms.
2019-04-12 16:48:45.667843: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   function_optimizer: Graph size after: 1285 nodes (0), 1493 edges (0), time = 4.937ms.
2019-04-12 16:48:45.667857: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   constant folding: Graph size after: 1216 nodes (-69), 1426 edges (-67), time = 194.179ms.
2019-04-12 16:48:45.667880: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   shape_optimizer: Graph size after: 1216 nodes (0), 1426 edges (0), time = 7.638ms.
2019-04-12 16:48:45.667897: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   arithmetic_optimizer: Graph size after: 727 nodes (-489), 1397 edges (-29), time = 37.714ms.
2019-04-12 16:48:45.667912: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   loop_optimizer: Graph size after: 727 nodes (0), 1397 edges (0), time = 7.935ms.
2019-04-12 16:48:45.667927: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   dependency_optimizer: Graph size after: 717 nodes (-10), 1318 edges (-79), time = 14.987ms.
2019-04-12 16:48:45.668006: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   memory_optimizer: Graph size after: 717 nodes (0), 1318 edges (0), time = 80.326ms.
2019-04-12 16:48:45.668021: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   model_pruner: Graph size after: 717 nodes (0), 1318 edges (0), time = 6.515ms.
2019-04-12 16:48:45.668032: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   implementation_selector: Graph size after: 717 nodes (0), 1318 edges (0), time = 2.238ms.
2019-04-12 16:48:45.668043: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   function_optimizer: Graph size after: 717 nodes (0), 1318 edges (0), time = 2.07ms.
2019-04-12 16:48:45.668054: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   constant folding: Graph size after: 717 nodes (0), 1318 edges (0), time = 30.632ms.
2019-04-12 16:48:45.668065: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   shape_optimizer: Graph size after: 717 nodes (0), 1318 edges (0), time = 5.239ms.
2019-04-12 16:48:45.668075: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   arithmetic_optimizer: Graph size after: 717 nodes (0), 1318 edges (0), time = 32.872ms.
2019-04-12 16:48:45.668086: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   dependency_optimizer: Graph size after: 717 nodes (0), 1318 edges (0), time = 20.462ms.
Traceback (most recent call last):
  File ""convert_h5_to_tflite.py"", line 192, in <module>
    tflite_model = converter.convert()
  File ""/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 276, in convert
    **converter_kwargs)
  File ""/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 410, in toco_convert_impl
    input_data.SerializeToString())
  File ""/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 176, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-04-12 16:13:57.979333: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 652 operators, 925 arrays (0 quantized)
2019-04-12 16:13:57.999356: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 652 operators, 925 arrays (0 quantized)
2019-04-12 16:13:58.034861: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 486 operators, 761 arrays (0 quantized)
2019-04-12 16:13:58.058513: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 486 operators, 761 arrays (0 quantized)
2019-04-12 16:13:58.074463: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 486 operators, 761 arrays (0 quantized)
2019-04-12 16:13:58.101510: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 32761600 bytes, theoretical optimal value: 28800000 bytes.
2019-04-12 16:13:58.107182: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, ARG_MAX, BATCH_TO_SPACE_ND, CAST, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FLOOR_DIV, FLOOR_MOD, GATHER, GREATER, GREATER_EQUAL, LOGICAL_AND, LOGICAL_NOT, LOGISTIC, MAXIMUM, MEAN, MINIMUM, MUL, PACK, RANGE, REDUCE_ANY, REDUCE_MAX, REDUCE_PROD, RESHAPE, RSQRT, SHAPE, SPACE_TO_BATCH_ND, SQUARED_DIFFERENCE, STRIDED_SLICE, SUB, SUM, TILE, TOPK_V2. Here is a list of operators for which you will need custom implementations: BatchNormalization.
Traceback (most recent call last):
  File ""/docker_environment/home/docker/anaconda3/bin/toco_from_protos"", line 11, in <module>
    sys.exit(main())
  File ""/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/docker/anaconda3/lib/python3.6/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/docker/anaconda3/lib/python3.6/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, ARG_MAX, BATCH_TO_SPACE_ND, CAST, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FLOOR_DIV, FLOOR_MOD, GATHER, GREATER, GREATER_EQUAL, LOGICAL_AND, LOGICAL_NOT, LOGISTIC, MAXIMUM, MEAN, MINIMUM, MUL, PACK, RANGE, REDUCE_ANY, REDUCE_MAX, REDUCE_PROD, RESHAPE, RSQRT, SHAPE, SPACE_TO_BATCH_ND, SQUARED_DIFFERENCE, STRIDED_SLICE, SUB, SUM, TILE, TOPK_V2. Here is a list of operators for which you will need custom implementations: BatchNormalization.
```"
27706,nvcc error   : 'cudafe++' died with status 0xC0000005 (ACCESS_VIOLATION),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro, 1809, 17763.379
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: v1.13.1
- Python version: v3.5.0:374f501f4567
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source): Microsoft (R) C/C++ Optimizing Compiler Version 19.00.24234.1 for x64
- CUDA/cuDNN version: 10.0/7.5
- GPU model and memory: NVIDIA GeForce RTX 2080 Ti



**Describe the problem**
Build of kernels:transpose_functor_gpu  fails with the following error
```
nvcc error   : 'cudafe++' died with status 0xC0000005 (ACCESS_VIOLATION)
```
and this error:
```
ERROR: D:/tensorflow/tensorflow/core/kernels/BUILD:1687:1: C++ compilation of rule '//tensorflow/core/kernels:transpose_functor_gpu' failed (Exit 5): python.exe failed: error executing command
```

The problem seems very similar to [this](https://github.com/tensorflow/tensorflow/issues/27576) issue.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

(tensorflow-v1.13) d:\tensorflow>python ./configure.py
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
nul
WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: e482c527-4d04-440e-b726-4dcee4cabbba
You have bazel 0.20.0 installed.
Please specify the location of python. [Default is C:\Users\admin\tensorflow-v1.13\Scripts\python.exe]:


Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: module 'site' has no attribute 'getsitepackages'
Found possible Python library paths:
  C:\Users\admin\tensorflow-v1.13\Lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\admin\tensorflow-v1.13\Lib\site-packages]

Do you wish to build TensorFlow with XLA JIT support? [y/N]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.0]: 10.0


Please specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7.5


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 7.5


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]: /arch:AVX2


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apacha Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.

(tensorflow-v1.13) d:\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
ERROR: D:/tensorflow/tensorflow/core/kernels/BUILD:1687:1: C++ compilation of rule '//tensorflow/core/kernels:transpose_functor_gpu' failed (Exit 5): python.exe failed: error executing command

nvcc error   : 'cudafe++' died with status 0xC0000005 (ACCESS_VIOLATION)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 542.611s, Critical Path: 293.00s
INFO: 3067 processes: 3067 local.
FAILED: Build did NOT complete successfully"
27705,Keras subclassing and explicit dtype of Input,"**System information**

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
**OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Arch Linux
**Tensorflow Version:** 2.0.0-alpha0
**Description**
When using Keras subclassing there is no apparent way of defining the dtype of the Input node of the network. In some cases, it would be neccecary to use tf.float16 instead of 32 but as of now i cannot find any way to adjust this. Also trying to set the dtype using self.dtype = tf.float16 is not permitted."
27704,XLA: build failed on 32bits platform,"**System information**
- OS Platform and Distribution: Linux
- TensorFlow installed from (source or binary): build tensorflow on arm cortex-a15
- TensorFlow version: 1.13.1
- Bazel version (if compiling from source): binary, 0.19.2
- GCC/Compiler version (if compiling from source):  binary, gcc 5.3

**Describe the problem**
Build failed at tensorflow/compiler/xla/service/llvm_ir/llvm_util.cc:378
```c++
378                 b->CreateIntToPtr(b->getInt64(absl::bit_cast<int64>(&LogS64)),
379                                   log_function_type->getPointerTo()),
```
Looks like XLA doesn't support 32bits platform?

**Any other info / logs**
**external/com_google_absl/absl/base/casts.h:179:3: error: static assertion failed: Source and destination types should have equal sizes.**
```
tensorflow/compiler/xla/service/llvm_ir/llvm_util.cc:378:76:   required from here
...
In instantiation of 'Dest absl::bit_cast(const Source&) [with Dest = long long int; Source = const char*; typename std::enable_if<(! absl::internal_casts::is_bitcastable<Dest, Source>::value), int>::type <anonymous> = 0]':
```
- Note: related code snippet (tensorflow/compiler/xla/service/llvm_ir/llvm_util.cc:378)
```c++
374 void EmitLogging(const char* tag, llvm::Value* value, llvm::IRBuilder<>* b) {
375   llvm::FunctionType* log_function_type = llvm::FunctionType::get(
376       b->getVoidTy(), {b->getInt64Ty(), b->getInt64Ty()}, /*isVarArg=*/false);
377   b->CreateCall(log_function_type,
378                 b->CreateIntToPtr(b->getInt64(absl::bit_cast<int64>(&LogS64)),
379                                   log_function_type->getPointerTo()),
380                 {b->getInt64(absl::bit_cast<int64>(tag)), value});
381 }
```
"
27703,When using long time inference TFLite GPU not working in mobile (demo app),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device= Pixel 2, Samsung Galaxy 6, 7, 8
- TensorFlow Lite version : experimental-0.0.1
- GPU : Adreno 540 

**Describe the current behavior**
Using more than about 10 minutes, keep running Demo App, but not working inference.
Normal inference time: about 50ms (pixel 2)
Abnormal inference time: about 3ms (pixel 2)
The same problem occurs in other phones.(Galaxy series ( >= 6), Xiaomi redmi note series)

**Code to reproduce the issue**
[Just demo code](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo)
"
27701,Bug for tf.data.experimental.TFRecordWriter(filename),"Thanks for your attention! I'm using Windows and my tensorflow version is 1.13.1 installed using pip, and the following issue occurred: when I was testing the API tf.data.experimental.TFRecordWriter with the folloing codes:
```
filename = 'hello'
writer = tf.data.experimental.TFRecordWriter(filename)
```
my interpreter raised the following error information:

```
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\data\experimental\ops\writers.py"", line 35, in __init__
    filename, dtypes.string, name=""filename"")
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1039, in convert_to_tensor
    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1097, in convert_to_tensor_v2
    as_ref=False)
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1175, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 304, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 245, in constant
    allow_broadcast=True)
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 283, in _constant_impl
    allow_broadcast=allow_broadcast))
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\tensor_util.py"", line 501, in make_tensor_proto
    (dtype, nparray.dtype, values))
TypeError: Incompatible types: <dtype: 'string'> vs. object. Value is hello
```
I can't quite understand the difference between the type of 'hello' and ""<dtype:'string'>"". Then I tried the API tf.python_io.TFRecordWriter(filename) with the following codes:
```
tf.python_io.TFRecordWriter(filename)
```
no error information raised. 
So I'm pulling the issue to look for your help. Thanks for your time and attention again."
27697,tensorflow/core/framework/op_def.pb.h:10:40: fatal error: google/protobuf/port_def.inc: No such file or directory,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): use plugin
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): tensorflow/tensorflow:latest-gpu-py3
- TensorFlow version (use command below): 1.14.1-dev20190409
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
As of `1.14.1-dev20190409`, custom plugins fail to build with the following error:

```
  In file included from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_def_builder.h:24:0,
                   from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op.h:23,
                   from horovod/tensorflow/mpi_ops.cc:22:
  /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_def.pb.h:10:40: fatal error: google/protobuf/port_def.inc: No such file or directory
   #include <google/protobuf/port_def.inc>
                                          ^
  compilation terminated.
```

This seems to be related to https://github.com/tensorflow/tensorflow/commit/6168f476b52d6d40eeff1823943ed2c0ea28adde.  It appears that not all the files are placed in the proper locations after the installation.

**Describe the expected behavior**
Plugins should build successfully.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
$ docker run -it --rm tensorflow/tensorflow:latest-gpu-py3
# apt install -y mpich
# HOROVOD_WITH_TENSORFLOW=1 pip install -v horovod
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

cc @martinwicke @gunan "
27696,tf.keras.estimator.model_to_estimator crashing when using tf.distribute.MirroredStrategy() with Only TensorFlow native optimizers are supported with DistributionStrategy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
I am using a keras model with TF 2.0. I am converting the model to estimator:
`tf.keras.estimator.model_to_estimator `

then it is crashing when running `estimator_train_model.train(..)` when using `tf.distribute.MirroredStrategy()` (after migrating the code to TF 2.0 of course). 

It works fine when using `None` as strategy

I tried to follow the instruction: https://www.tensorflow.org/alpha/guide/distribute_strategy

**Describe the expected behavior**
The same was working with TF 1.x

**Code to reproduce the issue**
Work in progress notebook can be found here:
http://localhost:8888/notebooks/proj_DL_models_and_pipelines_with_GCP/notebook/TF_2.0/08-Mnist_keras_estimator.ipynb

I am using a very basic Keras model with 

```
optimiser = tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, epsilon=1e-07)
# Compile model
model.compile(loss='categorical_crossentropy',
                  optimizer=optimiser,
                  metrics=['accuracy'])

strategy=None # working
#strategy = tf.distribute.MirroredStrategy() # crashing with TF 2.0 but working with TF 1.X

# config tf.estimator to use a give strategy
training_config = tf.estimator.RunConfig(train_distribute=strategy,
                                         model_dir=FLAGS.model_dir,
                                         save_summary_steps=1,
                                         save_checkpoints_steps=100,
                                         keep_checkpoint_max=3,
                                         log_step_count_steps=10)

# transfor keras model to estimator model
estimator_train_model = tf.keras.estimator.model_to_estimator(keras_model=model_opt_tf,
                                         config=training_config)

# Fit the model (using estimator.train and data.Dataset)
estimator_train_model.train(input_fn=lambda:mnist_v1.input_mnist_tfrecord_dataset_fn(path_train_tfrecords+'*', FLAGS, mode=tf.estimator.ModeKeys.TRAIN, batch_size=FLAGS.batch_size),
                            steps=1000)
```
**Other info / logs**
I0409 22:06:27.293305 4531660224 model.py:211] input_dataset_fn: TRAIN, train
I0409 22:06:27.469371 123145492971520 estimator.py:1126] Calling model_fn.
I0409 22:06:27.475003 123145492971520 coordinator.py:219] Error reported to Coordinator: Only TensorFlow native optimizers are supported with DistributionStrategy.
Traceback (most recent call last):
  File ""/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 882, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1127, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py"", line 278, in model_fn
    raise ValueError('Only TensorFlow native optimizers are supported with '
ValueError: Only TensorFlow native optimizers are supported with DistributionStrategy.

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<timed eval> in <module>

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    357 
    358       saving_listeners = _check_listeners_type(saving_listeners)
--> 359       loss = self._train_model(input_fn, hooks, saving_listeners)
    360       logging.info('Loss for final step: %s.', loss)
    361       return self

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
   1135   def _train_model(self, input_fn, hooks, saving_listeners):
   1136     if self._train_distribution:
-> 1137       return self._train_model_distributed(input_fn, hooks, saving_listeners)
   1138     else:
   1139       return self._train_model_default(input_fn, hooks, saving_listeners)

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)
   1198       self._config._train_distribute.configure(self._config.session_config)
   1199       return self._actual_train_model_distributed(
-> 1200           self._config._train_distribute, input_fn, hooks, saving_listeners)
   1201     # pylint: enable=protected-access
   1202 

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _actual_train_model_distributed(self, strategy, input_fn, hooks, saving_listeners)
   1267                     labels,  # although this will be None it seems
   1268                     ModeKeys.TRAIN,
-> 1269                     self.config))
   1270           loss = strategy.reduce(reduce_util.ReduceOp.SUM,
   1271                                  grouped_estimator_spec.loss)

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)
   1076     if kwargs is None:
   1077       kwargs = {}
-> 1078     return self._call_for_each_replica(fn, args, kwargs)
   1079 
   1080   def _call_for_each_replica(self, fn, args, kwargs):

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py in _call_for_each_replica(self, fn, args, kwargs)
    663   def _call_for_each_replica(self, fn, args, kwargs):
    664     return _call_for_each_replica(self._container_strategy(), self._device_map,
--> 665                                   fn, args, kwargs)
    666 
    667   def _configure(self,

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py in _call_for_each_replica(distribution, device_map, fn, args, kwargs)
    191     for t in threads:
    192       t.should_run.set()
--> 193     coord.join(threads)
    194 
    195   return values.regroup(device_map, tuple(t.main_result for t in threads))

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py in join(self, threads, stop_grace_period_secs, ignore_live_threads)
    387       self._registered_threads = set()
    388       if self._exc_info_to_raise:
--> 389         six.reraise(*self._exc_info_to_raise)
    390       elif stragglers:
    391         if ignore_live_threads:

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py in stop_on_exception(self)
    295     """"""
    296     try:
--> 297       yield
    298     except:  # pylint: disable=bare-except
    299       self.request_stop(ex=sys.exc_info())

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py in run(self)
    880               self._captured_var_scope, reuse=self.replica_id > 0), \
    881           variable_scope.variable_creator_scope(self.variable_creator_fn):
--> 882         self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
    883         self.done = True
    884     finally:

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)
   1125 
   1126     logging.info('Calling model_fn.')
-> 1127     model_fn_results = self._model_fn(features=features, **kwargs)
   1128     logging.info('Done calling model_fn.')
   1129 

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py in model_fn(features, labels, mode)
    276         not isinstance(keras_model.optimizer,
    277                        (tf_optimizer_module.Optimizer, optimizers.TFOptimizer)):
--> 278       raise ValueError('Only TensorFlow native optimizers are supported with '
    279                        'DistributionStrategy.')
    280 

ValueError: Only TensorFlow native optimizers are supported with DistributionStrategy.


"
27695,"Failed to load the native TensorFlow runtime, Ubuntu 16.04, in virtualenv ","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04
- TensorFlow installed from (source or binary): binary, using  pip3 install --upgrade --ignore-installed tensorflow-gpu==1.5

- TensorFlow version:1.5
- Python version: tried in both Python 2.7 and 3.5
- Installed using virtualenv? pip? conda?: virtualenv
- CUDA/cuDNN version: CUDA 10.0. cuDNN: cuDNN v7.5.0 (Feb 21, 2019), for CUDA 10.0
- GPU model and memory: NVIDIA TITAN Xp



**Describe the problem**

I**nstalled Tensorflow and tried to run it via python, and using ""import tensorflow as tf"". The error message details are shared below.**

Python 3.5.2 (default, Nov 12 2018, 13:43:14) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""/home/rrku16sys3/venv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/rrku16sys3/venv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/rrku16sys3/venv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/rrku16sys3/venv/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/rrku16sys3/venv/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/rrku16sys3/venv/lib/python3.5/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/rrku16sys3/venv/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/rrku16sys3/venv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/rrku16sys3/venv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/rrku16sys3/venv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/rrku16sys3/venv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/rrku16sys3/venv/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/rrku16sys3/venv/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>> 

**Need help.**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27693,import error,"Traceback (most recent call last):
  File ""C:/Users/GAURAV/PycharmProjects/Object_Detection/object"", line 9, in <module>
    from utils import label_map_util
ImportError: cannot import name 'label_map_util'"
27692,Converting Tensor to Numpy array extremely slow in TF 2.0,"**Current Issue / Bug Report**
I tried to convert a list of tensors to a numpy array.
This was no issue in Tensorflow 1.13.x but is now orders of magnitudes slower in Tensorflow 2.0.0-dev20190405

I linked a piece of code to reproduce the issue in Colab (no HW acceleration required) and to also show the difference in execution time between 1.13.x and 2.0.0/nightly

I tested the issue with 
np.array(LIST_OF_TENSORS)
np.shape(LIST_OF_TENSORS)

**System information**
Colab w.o. HW acceleration and TF 2.0.0-dev20190405

**Code to reproduce the issue**
[Code Example](https://github.com/markste-in/colab/blob/master/bug_tensor_numpy.ipynb
)
.
"
27691,Unable to do transpose on a  yolo model ,"Hello,

I am trying to add a transpose layer at the end of Yolo model.

I am using the following script, 

model = load_model(filepath=""yolov2-tiny-voc.h5"")
output = model.output
output_shape = model.output.shape
transpose = Lambda(lambda x: tf.transpose(output, prem=(0,3,1,2),name='transpose'))(output)
final_model = Model(inputs= model.input, outputs= transpose )
save_model(model=final_model, filepath= ""trial_attempt.h5"")


But I am getting the following error:
AttributeError: 'Conv2D' object has no attribute 'outbound_nodes'


Could someone help on this issue?


"
27690,Problem with Keras sparse input,"In keras.backend.GraphExecutionFunction.__call__ function, when this function receives sparse input, it will transform sparse input into coo_matrix, and extract (indices, data, shape) from it. But this function will try to invoke numpy.asarray function on value tuple: (indices, data, shape), and will cause error here.

  def __call__(self, inputs):
    inputs = nest.flatten(inputs)

    session = get_session()
    feed_arrays = []
    array_vals = []
    feed_symbols = []
    symbol_vals = []
    for tensor, value in zip(self.inputs, inputs):
      if value is None:
        continue
      if is_sparse(tensor):
        sparse_coo = value.tocoo()
        indices = np.concatenate((np.expand_dims(sparse_coo.row, 1),
                                  np.expand_dims(sparse_coo.col, 1)), 1)
        value = (indices, sparse_coo.data, sparse_coo.shape)
      if tensor_util.is_tensor(value):
        # Case: feeding symbolic tensor.
        feed_symbols.append(tensor)
        symbol_vals.append(value)
      else:
        # Case: feeding Numpy array.
        feed_arrays.append(tensor)
        # We need to do array conversion and type casting at this level, since
        # `callable_fn` only supports exact matches.
        tensor_type = dtypes_module.as_dtype(tensor.dtype)
        
        # ------------------------------------
        #  SHOULD BE THAT?
        # if is_sparse(tensor):
        #   array_vals.append(value)
        # else:
        # ------------------------------------

        array_vals.append(np.asarray(value,
                                     dtype=tensor_type.as_numpy_dtype))"
27689,ImportError: DLL load failed: Unable to find specific module.,"Hi all,

Im unable to import keras or tensorflow in jupyter:

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Pro
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: HP Omen, i7 7700HQ, 16GB RAM
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu
- **TensorFlow version (use command below)**: 1.13.1
- **Python version**: 3.7.3
- **CUDA/cuDNN version**: 10.1
- **GPU model and memory**: GTX 1060 laptop
- **Exact command to reproduce**: import tensorflow as tf

---TRACE---

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\AppData\Local\Continuum\anaconda3\lib\imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

~\AppData\Local\Continuum\anaconda3\lib\imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: DLL load failed: No se puede encontrar el mdulo especificado.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-6-9ab9087e201e> in <module>
----> 1 import tensorflow
      2 print(tensorflow.__version__)
      3 # Puedes aadir todos los imports adicionales que necesites aqu
      4 import keras
      5 from keras.datasets import fashion_mnist

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\__init__.py in <module>
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 from tensorflow._api.v1 import app

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\dizquierdo\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\dizquierdo\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\dizquierdo\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\dizquierdo\AppData\Local\Continuum\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\dizquierdo\AppData\Local\Continuum\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: No se puede encontrar el mdulo especificado.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Regards"
27688,RuntimeError: Unable to create link (name already exists) during model saving with ModelCheckpoint,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): TF 2.0 downloaded from repo
- TensorFlow version (use command below): 
tf-nightly-gpu-2.0-preview --> 2.0.0.dev20190314 
tensorflow-hub  --> 0.4.0
- Python version: 3.6
- CUDA/cuDNN version: CUDA Version 10.0.130/ cuDNN 7.5.0
- GPU model and memory: Nvidia RTX 2080 Ti 11GB (and GTX 1060 6GB) 

**Describe the current behavior**
I've downloaded an inception model from TF-Hub (specifically this one: https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2), I have added to it two Keras layers (a Dropout layer and a Dense layer) and during the training, I'm trying to save the model using the `ModelCheckpoint` Keras callback. Unfortunately, after one epoch and during the model saving I receive the following error:
``` python
Traceback (most recent call last):
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3291, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-3945e7fb8367>"", line 1, in <module>
    runfile('/run/media/federico/XData/PycharmProjectsXData/ash/ash/prova_gan_plain_test.py', wdir='/run/media/federico/XData/PycharmProjectsXData/ash/ash')
  File ""/opt/pycharm-professional/helpers/pydev/_pydev_bundle/pydev_umd.py"", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""/opt/pycharm-professional/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/run/media/federico/XData/PycharmProjectsXData/ash/ash/prova_gan_plain_test.py"", line 98, in <module>
    main()
  File ""/run/media/federico/XData/PycharmProjectsXData/ash/ash/prova_gan_plain_test.py"", line 90, in main
    logdir,
  File ""/run/media/federico/XData/PycharmProjectsXData/ash/ash/testers/gan_plain.py"", line 85, in __init__
    self._model = self._download_and_train_model()
  File ""/run/media/federico/XData/PycharmProjectsXData/ash/ash/testers/gan_plain.py"", line 255, in _download_and_train_model
    callbacks=[cback_checkpoint],
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1508, in fit_generator
    steps_name='steps_per_epoch')
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py"", line 324, in model_iteration
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 290, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 892, in on_epoch_end
    self.model.save_weights(filepath, overwrite=True)
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1395, in save_weights
    hdf5_format.save_weights_to_hdf5_group(f, self.layers)
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 693, in save_weights_to_hdf5_group
    param_dset = g.create_dataset(name, val.shape, dtype=val.dtype)
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/h5py/_hl/group.py"", line 139, in create_dataset
    self[name] = dset
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/h5py/_hl/group.py"", line 371, in __setitem__
    h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl)
  File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper
  File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper
  File ""h5py/h5o.pyx"", line 202, in h5py.h5o.link
RuntimeError: Unable to create link (name already exists)
```

Highlighting the error: **RuntimeError: Unable to create link (name already exists)**

**Describe the expected behavior**
I'm expecting to be able to use the `ModelCheckpoint` callback to save the (best) model.
Alternatively, I'm also expecting to be able to save the model with the `model.save(filepath)` function.

**Code to reproduce the issue**
You could reproduce the error on the following TF-Hub Colab page: https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_image_retraining.ipynb#scrollTo=CCpdfXPsh47Q 

adding a cell with the `ModelCheckpoint` code

``` python
cback_checkpoint = tf.keras.callbacks.ModelCheckpoint(
            filepath=""best.h5"",
            verbose=1,
            save_best_only=True,
        )  
```

and then adding the callback to the `fit_generator` function of the model:

``` python
steps_per_epoch = train_generator.samples // train_generator.batch_size
validation_steps = valid_generator.samples // valid_generator.batch_size
hist = model.fit_generator(
    train_generator,
    epochs=5, steps_per_epoch=steps_per_epoch,
    validation_data=valid_generator,
    validation_steps=validation_steps,
    callbacks=[cback_checkpoint]).history
```

**Other info / logs**
I've found some issues online regarding a similar problem like [this](https://groups.google.com/forum/#!topic/keras-users/my13TUe2dlU) and issues: [#5280](https://github.com/keras-team/keras/issues/5820), [#6844](https://github.com/keras-team/keras/issues/6844) and the more recent #26811. Many of them speak about some problem regarding the naming of the layers (or weights) or about creating the `ModelCheckpoint` with `save_best_only=True` or `save_weights_only=True`. I have tried all the proposed approaches but without success.
Even with `model.save(filepath)` Keras function I face the same problem.

**EDIT**: Please follow [this Colab link](https://colab.research.google.com/drive/1_sDTb1DLaoBlNGbRfKqMWgWV6B2DaYUv) to have all the code set up to be reproduced.
"
27687,quantized mobilenet_v1_1.0_224 is 4x slower than un-qualtized version (both testing on tflite),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):1.13.0-rc0
- Python version:Python 3.5.5 :: Anaconda, Inc
- Bazel version (if compiling from source):NA
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10/7.4.1
- GPU model and memory:Titan V, 12 GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
quantized mobilenet_v1_1.0_224 is 4x slower than un-qualtized version (both testing on tflite)
**Describe the expected behavior**

**Code to reproduce the issue**
```
# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import numpy as np

from PIL import Image
import tensorflow as tf
import time
import os

def Covert_to_tflite():    
    graph_def_file = os.path.join(os.getcwd(), 'mobilenet_v1_1.0_224/mobilenet_v1_1.0_224_frozen.pb')
    input_arrays = [""input""]
    output_arrays = [""MobilenetV1/Predictions/Softmax""]

    converter = tf.lite.TFLiteConverter.from_frozen_graph(
    graph_def_file, input_arrays, output_arrays)
    tflite_model = converter.convert()
    open(""mobilenet_v1_1.0_224/converted_model.tflite"", ""wb"").write(tflite_model)

    converter.post_training_quantize=True
    tflite_quantized_model=converter.convert()
    open(""mobilenet_v1_1.0_224/quantized_converted_model.tflite"", ""wb"").write(tflite_quantized_model)
def load_labels(filename):
  my_labels = []
  input_file = open(filename, 'r')
  for l in input_file:
    my_labels.append(l.strip())
  return my_labels
if __name__ == '__main__':
    
    # print('### for coverting to tflite - START ###')
    # Covert_to_tflite()
    # print('### for coverting to tflite - END ###')
    
    print('### for testing performance/accuracy - START###')
    file_name = ""./grace_hopper.jpg""
    # model_file = ""mobilenet_v1_1.0_224/mobilenet_v1_1.0_224.tflite""
    # model_file = ""mobilenet_v1_1.0_224/mobilenet_v1_1.0_224_quant.tflite""
    label_file = ""mobilenet_v1_1.0_224/labels.txt""
    input_mean = 127.5
    input_std = 127.5
    floating_model = False

    parser = argparse.ArgumentParser()
    parser.add_argument(""--graph"", help="".tflite model to be executed"")
    args = parser.parse_args()

    if args.graph:
        model_file = args.graph
  
    interpreter = tf.lite.Interpreter(model_path=model_file)
    interpreter.allocate_tensors()

    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # check the type of the input tensor
    if input_details[0]['dtype'] == type(np.float32(1.0)):
        floating_model = True

    # NxHxWxC, H:1, W:2
    height = input_details[0]['shape'][1]
    width = input_details[0]['shape'][2]
    img = Image.open(file_name)
    img = img.resize((width, height))

    # add N dim
    input_data = np.expand_dims(img, axis=0)

    if floating_model:
        input_data = (np.float32(input_data) - input_mean) / input_std

    start = time.time()
    interpreter.set_tensor(input_details[0]['index'], input_data)

    interpreter.invoke()
    end = time.time()

    output_data = interpreter.get_tensor(output_details[0]['index'])
    results = np.squeeze(output_data)

    top_k = results.argsort()[-5:][::-1]
    labels = load_labels(label_file)
    for i in top_k:
        if floating_model:
            print('{0:08.6f}'.format(float(results[i]))+"":"", labels[i])
        else:
            print('{0:08.6f}'.format(float(results[i]/255.0))+"":"", labels[i])
    print(""{}, Inference time: {} sec"".format(model_file, end-start))
    print('### for testing performance/accuracy - END###')
```


Provide a reproducible test case that is the bare minimum necessary to generate the problem.
**Other info / logs**
```
$ python convert_and_test_tflite.py --graph mobilenet_v1_1.0_224/converted_model.tflite
### for testing performance/accuracy - START###
0.445140: 653:military uniform
0.162699: 458:bow tie, bow-tie, bowtie
0.137379: 907:Windsor tie
0.121642: 466:bulletproof vest
0.047150: 668:mortarboard
mobilenet_v1_1.0_224/converted_model.tflite, Inference time: 0.056842803955078125 sec
### for testing performance/accuracy - END###

$ python convert_and_test_tflite.py --graph mobilenet_v1_1.0_224/quantized_converted_model.tflite
### for testing performance/accuracy - START###
0.259418: 653:military uniform
0.239987: 466:bulletproof vest
0.239753: 907:Windsor tie
0.180003: 458:bow tie, bow-tie, bowtie
0.016782: 668:mortarboard
mobilenet_v1_1.0_224/quantized_converted_model.tflite, Inference time: 0.21327567100524902 sec
### for testing performance/accuracy - END###
```

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27686,axis parameter in tf.random_shuffle,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.13
- Are you willing to contribute it (Yes/No): Yes, if you help me to do

**Describe the feature and the current behavior/state.**
No, current behavior will not change

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
now code is 
x = tf.transpose(x, [3, 0, 1, 2])
x = tf.random_shuffle(x)
x = tf.transpose(x, [1, 2, 3, 0]

code will be
x = tf.random_shuffle(x, axis=-1)
**Any Other info.**
"
27685,"tf.layers.batch_normalization missing moving variance and mean, low convolutional model accuracy","**System information**
- I have written custom code
- Linux Ubuntu 16.04
- TensorFlow installed via pip from binary
- TensorFlow version: 1.7.0
- Python version: 3.6.3
- Bazel: 0.14.1
- GCC: 5.4.0
- CUDA 9.1.0, cuDNN: 7.1.2
- GPU model and memory: NVIDIA Quadro P6000/PCIe/SSE2, 24GB memory

**Describe the current behavior**
A custom, deep convolutional model, with batch normalization, does not converge. It is a recreated version of a Keras model that reaches accuracy of > 0.8 on the same data, whereas the tensorflow model built from tf.layers gets stuck under accuracy of < 0.3.

Presumed issue is lack of the moving mean and variance operation in the update ops, but all recommended methods of including them (https://towardsdatascience.com/pitfalls-of-batch-norm-in-tensorflow-and-sanity-checks-for-training-networks-e86c207548c8) have failed.

**Describe the expected behavior**

The model should converge to a much higher accuracy and the moving mean and variance operations should be visible in the update ops.

**Code to reproduce the issue**
I've created a github gist file that can be run on randomly generated data and prints reports about the ops:

https://gist.github.com/mateuszjurewicz/881d13e066f00b349b3eaafe901207f4

Usage:

python train_tensorflow.py --random_data=True

Alternatively the preprocessed data can be downloaded from:
https://archive.org/details/voice_data

You can also easily use it in combination with tensorboard, just ask the command argument parser for some --help.

**Other info / logs**
Larger context about this and the pure keras script that converges can be found here:
https://github.com/mateuszjurewicz/tensorflow_speech_recognition/blob/master/train_keras.py"
27684,nan appearing on False fork of tf.where propgates to e,"https://github.com/tensorflow/tensorflow/issues/20091

Same issue, but that one was closed. The fix is to protect every single argument against the fork.


```python
import tensorflow as tf
import numpy as np
x = tf.convert_to_tensor(np.random.randn(100).astype(np.float32))
alpha = tf.Variable(1, name='asdf', dtype=tf.float32, trainable=True)
with tf.GradientTape(persistent=True) as t:
    safe_x = tf.where(x > 0, x, tf.ones_like(x))
    # x = safe_x # this fixes it
    s = tf.where(x >= 0, alpha * tf.math.log(x), tf.zeros_like(x))
gradients = t.gradient(s, [alpha])


In [326]: reload(d); d.gradients
Out[326]: [<tf.Tensor: id=2832835, shape=(), dtype=float32, numpy=-35.19606>]

In [327]: reload(d); d.gradients
Out[327]: [<tf.Tensor: id=2832875, shape=(), dtype=float32, numpy=nan>]

In [346]: sys.version
Out[346]: '3.7.3 (default, Mar 27 2019, 16:54:48) \n[Clang 4.0.1 (tags/RELEASE_401/final)]'

In [347]: platform.platform()
Out[347]: 'Darwin-17.7.0-x86_64-i386-64bit'

In [348]: tf.__version__
Out[348]: '2.0.0-alpha0'

```

"
27683,KeyError in tensorflow\contrib\layers\python\layers\feature_column.py,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0 cuDNN 7.4.2
- GPU model and memory: GeForce GTX 960M totalMemory: 4.00GiB freeMemory: 3.34GiB

**Describe the current behavior**
Here is my code, I use a random forest estimator 

`feature_col = [real_valued_column(column_name=""x"", dimension=38)]`
`model = tf_random_forest.TensorForestEstimator(
    hparams,
    model_dir=dataset_path,
    feature_columns=feature_col,
    report_feature_importances=True)`

I have 38 features, when I train the model, I got a KeyError.

> Traceback (most recent call last):
  File ""tf_dataset.py"", line 130, in <module>
    model.train(input_fn=train_input_fn, steps=num_steps)
  File ""D:\venv\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py"", line 358, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""D:\venv\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py"", line 1124, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""D:\venv\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py"", line 1154, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""D:\venv\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py"", line 1112, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""D:\venv\project\Malicious_TLS_Detection\machine_learning\tf_random_forest.py"", line 162, in _model_fn
    layers.transform_features(features, feature_columns))
  File ""D:\venv\lib\site-packages\tensorflow\contrib\layers\python\layers\feature_column_ops.py"", line 656, in transform_features
    transformer.transform(column)
  File ""D:\venv\lib\site-packages\tensorflow\contrib\layers\python\layers\feature_column_ops.py"", line 848, in transform
    feature_column.insert_transformed_feature(self._columns_to_tensors)
  File ""D:\venv\lib\site-packages\tensorflow\contrib\layers\python\layers\feature_column.py"", line 1873, in insert_transformed_feature
    input_tensor = self._normalized_input_tensor(columns_to_tensors[self.name])
KeyError: 'x'

I followed the instructions below, but I got `KeyError: ''`
[KeyError_stackoverflow](https://stackoverflow.com/questions/39687554/keyerror-in-tensorflow-when-calling-predict-on-trained-model)

whatever I changed the column name, I always got a KeyError. Can someone explain why?


"
27681,"Hi Everyone. I am working on deploying tensorflow lite model file on Raspberry Pi 3. When I try to import a tflite model into the tflite interpreter, it throws an error. Please help!!!","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): resbian 9.0
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below):1.13.1
- Python version:3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

interpreter=tf.lite.Interpreter(""/home/pi/download/mobilenet_v1_1.0_224_quant.tflite"")

ERROR//

/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412
  return f(*args, **kwds)
1.13.1
Traceback (most recent call last):
  File ""tensorlite.py"", line 5, in <module>
    interpreter = tf.lite.Interpreter(""/home/pi/mobilenet_v1_1.0_224_quant.tflite"")
  File ""/home/pi/.local/lib/python3.5/site-packages/tensorflow/lite/python/interpreter.py"", line 54, in __init__
    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(
  File ""/home/pi/.local/lib/python3.5/site-packages/tensorflow/python/util/lazy_loader.py"", line 61, in __getattr__
    module = self._load()
  File ""/home/pi/.local/lib/python3.5/site-packages/tensorflow/python/util/lazy_loader.py"", line 44, in _load
    module = importlib.import_module(self.__name__)
  File ""/usr/lib/python3.5/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 673, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 673, in exec_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
  File ""/home/pi/.local/lib/python3.5/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 28, in <module>
    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()
  File ""/home/pi/.local/lib/python3.5/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_tensorflow_wrap_interpreter_wrapper', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
  File ""<frozen importlib._bootstrap>"", line 693, in _load
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 914, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: /home/pi/.local/lib/python3.5/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils24NeonVectorScalarMultiplyEPKaifPf

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27680,tf.data.Dataset.shuffle produces the same results at each dataset iteration in tensorflow 2 alpha,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes. Create a simple dataset, shuffle it and iterate through it.  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 
2.0.0-alpha0
- Python version:
Python 3.7.3
- CUDA/cuDNN version:
CUDA10.0
Not relevant.
- GPU model and memory:
Not relevant.

**Describe the current behavior**
when using tf.data.Dataset.shuffle and iterating through the dataset multiple times the shuffled order is always the same.

**Describe the expected behavior**
The order should change for every iteration.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import tensorflow as tf
input_array = tf.range(10)
print(input_array)
dataset = tf.data.Dataset.from_tensor_slices(input_array).shuffle(5)

def print_values():
    for val in dataset:
        print(val.numpy(), end="" "")
    print()

print_values()
print_values()
```

output:
```
tf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32)
2 1 4 6 0 8 9 5 7 3 
2 1 4 6 0 8 9 5 7 3 

```


**Other info / logs**
Behaviour is correct in tensorflow 1.13."
27679,Passing a dictionary of tensors to py_function,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.0
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0 alpha
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

tf.data.Dataset supports dictionary as a valid type of its elements, which is convenient when your data source have multiple tensors and you want to assign a name to each of them. However if I want to manipulate these tensors by mapping the dataset to a tf.py_function, it complains that the dictionary is not compatible with the Tensor, even though its values are Tensors.

```python
import numpy as np
import tensorflow as tf


def make_dictionary(inp):
    out = {}
    out[""image1""] = inp[0]
    out[""image2""] = inp[1]
    out[""image3""] = inp[2]
    return out


def process_data(data):
    print(""image1"", data[""image1""])
    print(""image2"", data[""image2""])
    print(""image3"", data[""image3""])
    return 0.0


dataset = tf.data.Dataset.from_tensor_slices(np.arange(10))

dataset = tf.data.Dataset.zip((dataset, dataset, dataset))
dataset = dataset.map(lambda *stuff: make_dictionary(stuff))
# now each element in dataset is a dictionary
# processing the dictionary however is not supported by py_function:

dataset = dataset.map(
      lambda data: tf.py_function(
          process_data, [data], tf.float32))

for image in dataset.take(4):
    print(image)
```

The error I get is:
```
Traceback (most recent call last):
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py"", line 559, in make_tensor_proto
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py"", line 559, in <listcomp>
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/util/compat.py"", line 61, in as_bytes
    (bytes_or_text,))
TypeError: Expected binary or unicode string, got {'image1': <tf.Tensor 'args_0:0' shape=() dtype=int64>, 'image2': <tf.Tensor 'args_1:0' shape=() dtype=int64>, 'image3': <tf.Tensor 'args_2:0' shape=() dtype=int64>}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 471, in _apply_op_helper
    as_ref=input_arg.is_ref)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1251, in internal_convert_n_to_tensor
    ctx=ctx))
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1186, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 304, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 245, in constant
    allow_broadcast=True)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 283, in _constant_impl
    allow_broadcast=allow_broadcast))
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py"", line 563, in make_tensor_proto
    ""supported type."" % (type(values), values))
TypeError: Failed to convert object of type <class 'dict'> to Tensor. Contents: {'image1': <tf.Tensor 'args_0:0' shape=() dtype=int64>, 'image2': <tf.Tensor 'args_1:0' shape=() dtype=int64>, 'image3': <tf.Tensor 'args_2:0' shape=() dtype=int64>}. Consider casting elements to a supported type.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tmp.py"", line 28, in <module>
    lambda data: tf.py_function(
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1012, in map
    return MapDataset(self, map_func, preserve_cardinality=True)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2986, in __init__
    use_legacy_function=use_legacy_function)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2388, in __init__
    self._function = wrapper_fn._get_concrete_function_internal()
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1319, in _get_concrete_function_internal
    *args, **kwargs)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1313, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1580, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1512, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 694, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2381, in wrapper_fn
    ret = _wrapper_helper(*args)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2326, in _wrapper_helper
    ret = func(*nested_args)
  File ""tmp.py"", line 29, in <lambda>
    process_data, [data], tf.float32))
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 389, in eager_py_func
    return _internal_py_func(func=func, inp=inp, Tout=Tout, eager=True, name=name)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 278, in _internal_py_func
    input=inp, token=token, Tout=Tout, name=name)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/ops/gen_script_ops.py"", line 74, in eager_py_func
    ""EagerPyFunc"", input=input, token=token, Tout=Tout, name=name)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36_tf20/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 502, in _apply_op_helper
    ""%s that are invalid. Tensors: %s"" % (prefix, values))
TypeError: Tensors in list passed to 'input' of 'EagerPyFunc' Op have types [<NOT CONVERTIBLE TO TENSOR>] that are invalid. Tensors: [{'image1': <tf.Tensor 'args_0:0' shape=() dtype=int64>, 'image2': <tf.Tensor 'args_1:0' shape=() dtype=int64>, 'image3': <tf.Tensor 'args_2:0' shape=() dtype=int64>}]
```



"
27674,unrecognized arguments: --output_node_names,"i am following steps mentioned in [this](https://medium.com/testdotai/training-data-for-app-classifier-f217dc005523) article.
at ""tensorflowjs_converter --input_format=tf_frozen_model --output_node_names=final_result  output/saved_model.pb web_model"" this step i am getting below issue;

usage: TensorFlow.js model converters. [-h]
                                       [--input_format {tf_hub,keras,tf_session_bundle,keras_saved_model,tensorflowjs,tfjs_layers_model,tf_saved_model,tf_frozen_model}]
                                       [--output_format {tfjs_layers_model,tensorflowjs,keras,tfjs_graph_model}]
                                       [--signature_name SIGNATURE_NAME]
                                       [--saved_model_tags SAVED_MODEL_TAGS]
                                       [--quantization_bytes {1,2}]
                                       [--split_weights_by_layer] [--version]
                                       [--skip_op_check SKIP_OP_CHECK]
                                       [--strip_debug_ops STRIP_DEBUG_OPS]
                                       [input_path] [output_path]
TensorFlow.js model converters.: error: unrecognized arguments: --output_node_names=final_result



- **OS Platform and Distribution : MacOS 10.14 (MacBook Pro)
- **TensorFlow installed from pip
- **TensorFlow version : 1.13.1
- **Python version : 3.6.5

"
27673,Build from sources fails with undefined symbol _ZN3Aws10FileSystem16GetHomeDirectoryB5cxx11Ev,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Gentoo, Kernel 4.19.32-v7+
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
Raspberry Pi
- TensorFlow installed from (source or binary):
git checkout tags/v1.12.0
- TensorFlow version:
v1.12.0
- Python version:
3.6.5
- Installed using virtualenv? pip? conda?:
n/a
- Bazel version (if compiling from source):
0.20.0
- GCC/Compiler version (if compiling from source):
8.2.0
- CUDA/cuDNN version:
n/a
- GPU model and memory:
n/a


**Describe the problem**
Build fails with the error message ""ImportError: .../_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws10FileSystem16GetHomeDirectoryB5cxx11Ev""

**Provide the exact sequence of commands / steps that you executed before running into the problem**
- H/W Raspberry Pi with USB-SSD, 4GB swap partition
- install bazel by Gentoo ebuild
- clone git repo, checkout v1.12.0
- configure:
I did just correct the Python paths and the optimization flags. I answered all other questions from the configure script with 'n'. Nevertheless I see this content in .tf_configure.bazelrc - I don't want to use AWS or ignite or something of the like... maybe the configure script does not work like expected.
```
build --action_env PYTHON_BIN_PATH=""/usr/bin/python""
build --action_env PYTHON_LIB_PATH=""/usr/lib/python3.6/site-packages""
build --python_path=""/usr/bin/python""
build:ignite --define with_ignite_support=true
build:xla --define with_xla_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_ROCM=""0""
build --action_env TF_NEED_CUDA=""0""
build --action_env TF_DOWNLOAD_CLANG=""0""
build:opt --copt=-march=armv7-a
build:opt --copt=-mfpu=neon-vfpv4
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build:v2 --define=tf_api_version=2
```
- patch platform.h
vim tensorflow/core/platform/platform.h
```
// Require an outside macro to tell us if we're building for Raspberry Pi or
// another ARM device that's not a mobile platform.
//#if !defined(RASPBERRY_PI) && !defined(ARM_NON_MOBILE)
//#define IS_MOBILE_PLATFORM
//#endif  // !defined(RASPBERRY_PI) && !defined(ARM_NON_MOBILE)
#if defined(IS_MOBILE_PLATFORM)
#error Wrong platform, stop building...
#endif
```
- build by:
```
bazel --host_jvm_args=""-Xms512m"" --host_jvm_args=""-Xmx1024m"" \
   build -c opt --copt=""-mfpu=neon-vfpv4"" \
   --copt=""-funsafe-math-optimizations"" \
   --copt=""-ftree-vectorize"" \
   --copt=""-fomit-frame-pointer"" \
   --verbose_failures tensorflow/tools/pip_package:build_pip_package \
   --define=grpc_no_ares=true \
   --incompatible_remove_native_http_archive=false
```
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/reorder_elementwise_unary.cc:
tensorflow/contrib/lite/toco/graph_transformations/reorder_elementwise_unary.cc: In member function 'virtual bool toco::ReorderElementwiseUnary::Run(toco::Model*, std::size_t)':
tensorflow/contrib/lite/toco/graph_transformations/reorder_elementwise_unary.cc:125:23: warning: comparison of integer expressions of different signedness: 'int' and 'std::vector<std::unique_ptr<toco::Operator> >::size_type' {aka 'unsigned int'} [-Wsign-compare]
     for (int i = 0; i < model->operators.size(); i++) {
                     ~~^~~~~~~~~~~~~~~~~~~~~~~~~
tensorflow/contrib/lite/toco/graph_transformations/reorder_elementwise_unary.cc:127:25: warning: comparison of integer expressions of different signedness: 'int' and 'std::vector<std::__cxx11::basic_string<char> >::size_type' {aka 'unsigned int'} [-Wsign-compare]
       for (int j = 0; j < consumer->inputs.size(); j++) {
                       ~~^~~~~~~~~~~~~~~~~~~~~~~~~
Slow read: a 114883940-byte read from /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/_pywrap_tensorflow_internal.so took 22043 ms.
ERROR: /usr/src/tensorflow/tensorflow/BUILD:533:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1) bash failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/execroot/org_tensorflow && \
  exec env - \
    PATH=/opt/vc/bin:/usr/lib/llvm/7/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/bin \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/lib/python3.6/site-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/arm-opt/genfiles/tensorflow_api/v1/ --apiname=tensorflow --apiversion=1 --package=tensorflow.python --output_package=tensorflow._api.v1 bazel-out/arm-opt/genfiles/tensorflow/_api/v1/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/app/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/bitwise/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/compat/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/data/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/data/experimental/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/debugging/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/distributions/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/dtypes/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/errors/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/feature_column/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/gfile/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/graph_util/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/image/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/io/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/initializers/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/activations/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/densenet/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/inception_resnet_v2/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/inception_v3/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/mobilenet/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/mobilenet_v2/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/nasnet/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/resnet50/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/vgg16/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/vgg19/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/xception/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/backend/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/callbacks/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/constraints/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/datasets/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/datasets/boston_housing/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/datasets/cifar10/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/datasets/cifar100/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/datasets/fashion_mnist/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/datasets/imdb/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/datasets/mnist/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/datasets/reuters/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/estimator/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/initializers/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/layers/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/losses/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/metrics/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/models/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/optimizers/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/image/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/sequence/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/text/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/regularizers/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/utils/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/wrappers/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/wrappers/scikit_learn/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/layers/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/linalg/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/logging/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/losses/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/manip/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/math/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/metrics/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/nn/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/nn/rnn_cell/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/profiler/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/python_io/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/quantization/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/random/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/resource_loader/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/strings/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/saved_model/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/saved_model/builder/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/saved_model/constants/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/saved_model/loader/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/saved_model/main_op/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/saved_model/signature_constants/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/saved_model/signature_def_utils/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/saved_model/tag_constants/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/saved_model/utils/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/sets/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/sparse/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/spectral/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/summary/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/sysconfig/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/test/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/train/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/train/queue_runner/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/user_ops/__init__.py')
Execution platform: @bazel_tools//platforms:host_platform

Use --sandbox_debug to see verbose messages from the sandbox: bash failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/execroot/org_tensorflow && \
  exec env - \
    PATH=/opt/vc/bin:/usr/lib/llvm/7/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/bin \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/lib/python3.6/site-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/arm-opt/genfiles/tensorflow_api/v1/ --apiname=tensorflow --apiversion=1 --package=tensorflow.python --output_package=tensorflow._api.v1 bazel-out/arm-opt/genfiles/tensorflow/_api/v1/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/app/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/bitwise/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/compat/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/data/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/data/experimental/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/debugging/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/distributions/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/dtypes/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/errors/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/feature_column/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/gfile/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/graph_util/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/image/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/io/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/initializers/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/activations/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/densenet/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/inception_resnet_v2/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/inception_v3/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/mobilenet/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/mobilenet_v2/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/nasnet/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/resnet50/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/vgg16/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/vgg19/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/applications/xception/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/backend/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/callbacks/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/constraints/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/datasets/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/datasets/boston_housing/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/datasets/cifar10/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/datasets/cifar100/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/datasets/fashion_mnist/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/datasets/imdb/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/datasets/mnist/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/datasets/reuters/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/estimator/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/initializers/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/layers/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/losses/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/metrics/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/models/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/optimizers/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/image/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/sequence/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/text/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/regularizers/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/utils/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/wrappers/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/keras/wrappers/scikit_learn/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/layers/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/linalg/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/logging/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/losses/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/manip/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/math/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/metrics/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/nn/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/nn/rnn_cell/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/profiler/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/python_io/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/quantization/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/random/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/resource_loader/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/strings/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/saved_model/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/saved_model/builder/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/saved_model/constants/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/saved_model/loader/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/saved_model/main_op/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/saved_model/signature_constants/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/saved_model/signature_def_utils/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/saved_model/tag_constants/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/saved_model/utils/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/sets/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/sparse/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/spectral/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/summary/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/sysconfig/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/test/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/train/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/train/queue_runner/__init__.py bazel-out/arm-opt/genfiles/tensorflow/_api/v1/user_ops/__init__.py')
Execution platform: @bazel_tools//platforms:host_platform

Use --sandbox_debug to see verbose messages from the sandbox
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws10FileSystem16GetHomeDirectoryB5cxx11Ev

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws10FileSystem16GetHomeDirectoryB5cxx11Ev


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Slow read: a 117326424-byte read from /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/execroot/org_tensorflow/bazel-out/arm-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so took 23412 ms.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 94586.192s, Critical Path: 40695.34s
INFO: 3233 processes: 3233 linux-sandbox.
FAILED: Build did NOT complete successfully
anduin /usr/src/tensorflow #
  File ""/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws10FileSystem16GetHomeDirectoryB5cxx11Ev

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/sandbox/linux-sandbox/3234/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws10FileSystem16GetHomeDirectoryB5cxx11Ev


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Slow read: a 117326424-byte read from /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/execroot/org_tensorflow/bazel-out/arm-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so took 23412 ms.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 94586.192s, Critical Path: 40695.34s
INFO: 3233 processes: 3233 linux-sandbox.
FAILED: Build did NOT complete successfully
```"
27672,tensorflow2.0.0-alpha no eagarTensor when using dataset.map,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):  binary
- TensorFlow version (use command below): 2.0.0-alpha
- Python version: 3.7
- Bazel version (if compiling from source):  
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:  v9.0.176
- GPU model and memory:  NVIDIA GeForce GTX   42GB 


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
when using dataset.map to parse tfrecord, the tensor is not eagerTensor

**Describe the expected behavior**
hope all the tensor could be eagerTensor in tf2.0

**Code to reproduce the issue**

Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
def parse_tf_record(tf_record):
    features = {'image': tf.io.FixedLenFeature([], tf.string, default_value=""""),
                'height': tf.io.FixedLenFeature([], tf.int64, default_value=0),
                'width': tf.io.FixedLenFeature([], tf.int64, default_value=0),
                'shape': tf.io.VarLenFeature(tf.float32),
                }

    parsed_features = tf.io.parse_single_example(tf_record, features)
    image = tf.io.decode_raw(parsed_features['image'], tf.uint8)
    height = tf.cast(parsed_features['height'], tf.int32)
    width = tf.cast(parsed_features['width'], tf.int32)
    image = tf.reshape(image, [height, width, 3])

    image = tf.image.convert_image_dtype(image, dtype=tf.float32)

    shape = tf.sparse.to_dense(parsed_features['shape'], default_value=0.0)

    shape = tf.reshape(shape, [Config.num_point, 6])

    print(shape.numpy())   ##  <------ when using dataset.map, will fail at this line 

    return image, shape

def run_train(train_tf_record)

    dataset = tf.data.TFRecordDataset([train_tf_record])

    for item in dataset:
        parse_tf_record(item)    ##  <---------- when debug into , the all the tensor in 'parse_tf_record' is eagerTensor 
 

    dataset = dataset.repeat().shuffle(1000).map(parse_tf_record,  
   num_parallel_calls=4).batch(32)

## <---------when debug into, all the tensor in 'parse_tf_record' is tensor

    for item in dataset.take(1):
        print(item)


if __name__ == '__main__':
    train_tf_record = 
    run_train(train_tf_record)

```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27671,Not found: No registered '_FusedMatMul' OpKernel for CPU devices compatible with node,"I changed the title of this post since it occurs new problems. And you can see it at the latest comment.Thx.
_________________________________________________________________________________

<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):linux 3.10.107
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):1.12
- Python version:2.7.5
- Bazel version (if compiling from source):No bazel.I use the build_all_linux.sh.
- GCC/Compiler version (if compiling from source): 4.8.2
- CUDA/cuDNN version: no
- GPU model and memory: no


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I trained a model with python and save it as ckpt files, and then transform it to pb file. I make the tensorflow to a stastic library and then try to load the model on c++. I can create the session and ReadBinaryProto, but when I run this code ,it fails and return theis message:
`tensorflow::Status status_create = m_session->Create(*m_graphdef);`

> Invalid argument: No OpKernel was registered to support Op 'Sin' with these attrs.  Registered devices: [CPU], Registered kernels:
>   <no registered kernels>
> 
>          [[{{node parallel_0/rnnsearch_0/add_timing_signal/Sin}} = Sin[T=DT_FLOAT, _device=""/gpu:0""](parallel_0/rnnsearch_0/add_timing_signal/mul_2)]]
> 

**Describe the expected behavior**
The model will be loaded successfully.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
tensorflow::Status status_create_sess = tensorflow::NewSession(tensorflow::SessionOptions(), &m_sess);
	if(!status_create_sess.ok()){
		cout<<""ERROR: FAIL TO CREATE SESSION""<<endl;
		cout<<status_create_sess.ToString()<<endl;
		m_eris = FAIL_TO_CREATE_SESSION;
		return m_eris;
	}

	m_graphdef = new(std::nothrow) tensorflow::GraphDef;
	if(m_graphdef==NULL){
		cout<<""ERROR: FAIL TO LOAD MODEL""<<endl;
		m_eris = FAIL_TO_LOAD_MODEL;
		return m_eris;
	}

	tensorflow::Status status_load = tensorflow::ReadBinaryProto(tensorflow::Env::Default(), model_path, m_graphdef);
	if(!status_load.ok()){
		cout<<""ERROR: FAIL TO LOAD MODEL""<<endl;
		cout<<status_load.ToString()<<endl;
		m_eris = FAIL_TO_LOAD_MODEL;
		return m_eris;
	}   
	tensorflow::Status status_create = m_sess->Create((*m_graphdef));
	if(!status_create.ok()){
		cout<<""ERROR: FAIL TO CREATE GRAPh IN SESSION""<<endl;
		cout<<status_create.ToString()<<endl;
		m_eris=FAIL_TO_CREATE_GRAPH;
		return m_eris;
	}
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27669,TF 2.0 strange issue with code upgraded from TF 1.x.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Win 10
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.0.0-alpha0
- Python version:3.6

**Describe the current behavior**
The session return an ndarray [[value_estimate]] instead of value_estimate.

**Describe the expected behavior**
The session should return a scalar float. Just value_estimate. It should not be in an ndarray.
Workaround: get the actual scalar float in the ndarray if it is not a float
https://github.com/SetoKaiba/ml-agents/blob/tf2/ml-agents/mlagents/trainers/ppo/policy.py#L199-L200
```python
if not isinstance(value_estimate, float):
            value_estimate = value_estimate[0][0]
```

**Code to reproduce the issue**
https://github.com/SetoKaiba/ml-agents/blob/tf2/ml-agents/mlagents/trainers/ppo/policy.py#L199-L200
The problem can be reproduced without the two lines workaround.

**Other info / logs**
The value_estimate is calculated by the session and model.value. Code is lines below.
https://github.com/SetoKaiba/ml-agents/blob/tf2/ml-agents/mlagents/trainers/ppo/policy.py#L198
```python
value_estimate = self.sess.run(self.model.value, feed_dict)
```
self.model.value is defined like this. The previous layer is dense layer with one units.
And the tf.identity should just return a scalar float.
The code is running correctly in TF 1.x. 
But it's working incorrectly in TF 2.0.
https://github.com/SetoKaiba/ml-agents/blob/tf2/ml-agents/mlagents/trainers/models.py#L298-L299
```python
        value = tf.layers.dense(hidden_value, 1, activation=None)
        self.value = tf.identity(value, name=""value_estimate"")
```"
27668,Automatic mixed precision for tensorflow-gpu,"Recently, I noticed that NVIDIA updated the Deep Learning SDK documentation and added [Tensorflow Automixing Accuracy Training.](https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html#tensorflow-amp)This looks very simple,only need to set:
`os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'`
But I didn't find any relevant information on the tensorflow website and github.Does this really work?Or need additional conditions?"
27667,"Unexpected output size for depthwise convolution with stride=2, dilation=3","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **CentOS**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **Apr5 tf-nightly-gpu**
- TensorFlow version (use command below): **v1.12.0-11808-ga1e3d4490d 1.14.1-dev20190405**
- Python version: **3.6**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: **10.1**
- GPU model and memory: **Tesla**


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
input shape = [?, 128, 128, 8]
output = depthwise_conv2d(input, filters=8, kernel_size=3,
                                 strides=(2, 2),
                                 dilation_rate=(3, 3),
                                 padding='same')

output shape = [?, 65, 65, 8]

**Describe the expected behavior**
Expected output shape:
output = [?, 64, 64, 8]

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27665,Add name=None argument to roll function,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version 2.0:
- Are you willing to contribute it Yes:



**Describe the feature and the current behavior/state.**
`tensorflow.python.ops.manip_ops.roll` does not take name=None as parameter the current API
does not allow that

**Will this change the current api? How?**
Yes, roll function in `tensorflow.python.ops.manip_ops.roll` will take name=None as argument which will return a tensor with `name=name` and not the default one `roll_n`

**File where roll exists**
[manip_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/manip_ops.py)

"
27664,"Using slim to train my own model, ImportError: cannot import name 'slim'","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):window7 Anacoda3 env
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):1.2.1
- Python version:3.5.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
a importerror when i copy nets from slim ,however i have installed a slim in c:\programdata\anacoda3\lib\site-packages


from nets import nets_factory

**Other info / logs**
ImportError                               Traceback (most recent call last)
<ipython-input-5-a29f10245850> in <module>()
      2 import tensorflow as tf
      3 from PIL import Image
----> 4 from nets import nets_factory
      5 import numpy as np

E:\Tensorflow\learning\nets\nets_factory.py in <module>()
     22 import tensorflow as tf
     23 
---> 24 from nets import alexnet
     25 from nets import cifarnet
     26 from nets import i3d

E:\Tensorflow\learning\nets\alexnet.py in <module>()
     39 import tensorflow as tf
     40 
---> 41 slim = tf.contrib.slim
     42 trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)
     43 

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\util\lazy_loader.py in __getattr__(self, item)
     51 
     52   def __getattr__(self, item):
---> 53     module = self._load()
     54     return getattr(module, item)
     55 

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\util\lazy_loader.py in _load(self)
     40   def _load(self):
     41     # Import the target module and insert it into the parent's namespace
---> 42     module = importlib.import_module(self.__name__)
     43     self._parent_module_globals[self._local_name] = module
     44 

C:\ProgramData\Anaconda3\lib\importlib\__init__.py in import_module(name, package)
    124                 break
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 
    128 

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\__init__.py in <module>()
     55 from tensorflow.contrib import saved_model
     56 from tensorflow.contrib import seq2seq
---> 57 from tensorflow.contrib import slim
     58 from tensorflow.contrib import solvers
     59 from tensorflow.contrib import sparsemax

ImportError: cannot import name 'slim'"
27663,,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
27662,saved_model_cli fails when --input_examples option contains string feature,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
```
(venv) []$ python --version
Python 3.7.2

(venv) []$ saved_model_cli --version
0.1.0

tensorflow: 1.13.1
```

**Describe the current behavior**
I have a model that accepts two string features, say 'subject' and 'body', and predict if the string content is SPAM. When trying to test the inference with `saved_model_cli --input_examples`, it produced error message.

Here is the model's signature def info: 
```
(venv) [jwan@jwan-mbp15 conversation_appropriateness]$ saved_model_cli show --dir working_dir/exported_model_dir/1554752450/ --tag_set serve --signature_def classification
The given SavedModel SignatureDef contains the following input(s):
  inputs['inputs'] tensor_info:
      dtype: DT_STRING
      shape: (-1)
      name: input_example_tensor:0
The given SavedModel SignatureDef contains the following output(s):
  outputs['classes'] tensor_info:
      dtype: DT_STRING
      shape: (-1, 2)
      name: linear/head/Tile:0
  outputs['scores'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 2)
      name: linear/head/predictions/probabilities:0
Method name is: tensorflow/serving/classify
```

Here is the cmd line and error:
```
(venv) []$ saved_model_cli run --dir working_dir/exported_model_dir/1554752450/ --tag_set serve --signature_def classification --input_examples 'inputs=[{""subject"":[""love""], ""body"":[""money""]}]'
Traceback (most recent call last):
  File ""/Users/jwan/Documents/source_code/python_snippets/venv/bin/saved_model_cli"", line 10, in <module>
    sys.exit(main())
  File ""/Users/jwan/Documents/source_code/python_snippets/venv/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 911, in main
    args.func(args)
  File ""/Users/jwan/Documents/source_code/python_snippets/venv/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 641, in run
    args.inputs, args.input_exprs, args.input_examples)
  File ""/Users/jwan/Documents/source_code/python_snippets/venv/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 553, in load_inputs_from_input_arg_string
    input_examples = preprocess_input_examples_arg_string(input_examples_str)
  File ""/Users/jwan/Documents/source_code/python_snippets/venv/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 472, in preprocess_input_examples_arg_string
    _create_example_string(example) for example in example_list
  File ""/Users/jwan/Documents/source_code/python_snippets/venv/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 472, in <listcomp>
    _create_example_string(example) for example in example_list
  File ""/Users/jwan/Documents/source_code/python_snippets/venv/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 490, in _create_example_string
    feature_list)
TypeError: 'love' has type str, but expected one of: bytes
```

The problem occurs at the saved_model_cli.py line 487 (https://github.com/tensorflow/tensorflow/blob/a9fe6cb21dd725cdb4d6708c970c2d044c13ce60/tensorflow/python/tools/saved_model_cli.py#L487):

```
    elif isinstance(feature_list[0], str):
      example.features.feature[feature_name].bytes_list.value.extend(
          feature_list)
```

It checks if the list `feature_list` contains string values, and then try to append the list to feature's `bytes_list` which expects the element to be bytes, and not string.

**Describe the expected behavior**
The code should convert the string to the bytes, for example:
```
    elif isinstance(feature_list[0], str):
      feature_list = list(map(lambda x: x.encode(),feature_list))
      example.features.feature[feature_name].bytes_list.value.extend(
          feature_list)
```

**Code to reproduce the issue**

**Other info / logs**
A [stack overflow post](https://stackoverflow.com/questions/55422537/testing-tf-serving-model-fails-with-bytes-as-strings-and-strings-as-bytes-confus) reported the same issue.

PR to fix this issue: #27661

Thanks! "
27660,gradient not computed unless variable is assigned.,"**System information**

On Google CoLab,

```
python --version
> Python 3.6.7
```

TensorFlow Version: `2.0.0-dev20190405`



**Describe the current behavior**

The following code results in the gradient being `None`:

```
def f(x):
  return x ** 2

def grad(x):
  with tf.GradientTape() as t:
    t.watch(x)
    # out = f(x)
  return t.gradient(f(x), x) 

x = tf.convert_to_tensor(9.0)

grad(x).numpy()
```

But if you use the assigned value `out = f(x)`, it works fine:

```
def f(x):
  return x ** 2

def grad(x):
  with tf.GradientTape() as t:
    t.watch(x)
    out = f(x)
  return t.gradient(out, x) 

x = tf.convert_to_tensor(9.0)

grad(x).numpy()
```

**Describe the expected behavior**

I'd expect that assignment to a local variable doesn't change the ability to compute gradients!

**Code to reproduce the issue**

See above.

Thanks for any insight!"
27657,[TF 2.0 API Docs] tf.keras.activations.selu,"**System information**
- TensorFlow version: 2.0 alpha
- Doc Link:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations/selu
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/activations.py

**- Links**
Should format the referenced paper - get rid of ""-"" and add the authors/year of publishing:
`""Self-Normalizing Neural Networks"" (Klambauer et al, 2017)""`

**- Definition**
The current definition does not specify the fixed values for `alpha` and `scale` constants. It also assumes knowledge of the ELU activation function. We should also include a link (i.e. https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations/elu).

Proposed modified definition:

```
The Scaled Exponential Linear Unit (SELU) activation function is:

`scale` * `x` if `x > 0` and `scale * alpha * (exp(x)-1)` if `x < 0`

where `alpha` and `scale` are pre-defined constants (`alpha = 1.6732632423543772848170429916717` and `scale = 1.0507009873554804934193349852946`.
The SELU activation function multiplies  `scale` > 1 with the `[elu](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations/elu)` (Exponential Linear Unit (ELU)) to ensure a slope larger than one for positive net inputs. 
```

Followed by what is already in the docs with the formatted `lecun_normal` initialization bit and a link to it: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/lecun_normal:

```
...The values of alpha and scale are chosen so that the mean and variance of the inputs are preserved between two consecutive layers as long as the weights are initialized correctly (see `[lecun_normal` initialization](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/lecun_normal)) and the number of inputs is ""large enough"" (see references for more information).
```

**- Examples**
Can add a modified example fro the Intro to CNNs tutorials (use `selu` instead of `relu`)

```
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='selu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='selu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='selu'))
```

**- Returns**
Can modify to include the word `function`:
```
The scaled exponential unit activation function`: `scale * elu(x, alpha)`.
``` 
and format markdown for plain text.

**- Raises**
Not defined.

**- Visuals**
Should be added, similar to: https://cdn-images-1.medium.com/max/1600/1*WyQS-lnoemRA3_FpRL7r5w.png

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Yes"
27654,TypeError: __init__() got an unexpected keyword argument 'serialized_options',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.4
- TensorFlow installed from (source or binary): Binary from pip/PyPi
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7
- CUDA/cuDNN version: CPU only
- GPU model and memory: MacPro integrated


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I have previously running code that explodes on import prior to any runtime execution. Reports the error above.

**Describe the expected behavior**
This library needs to run and not choke on its own imports.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Using TensorFlow backend.
Traceback (most recent call last):
  File ""/Users/raymond/Depot/ai-worker/duplicate_ai/test.py"", line 1, in <module>
    from model import SiameseDream
  File ""/Users/raymond/Depot/ai-worker/duplicate_ai/model.py"", line 1, in <module>
    import keras.backend as K
  File ""/Users/raymond/env/ai-worker/lib/python3.7/site-packages/keras/__init__.py"", line 3, in <module>
    from . import utils
  File ""/Users/raymond/env/ai-worker/lib/python3.7/site-packages/keras/utils/__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""/Users/raymond/env/ai-worker/lib/python3.7/site-packages/keras/utils/conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""/Users/raymond/env/ai-worker/lib/python3.7/site-packages/keras/backend/__init__.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""/Users/raymond/env/ai-worker/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""/Users/raymond/env/ai-worker/lib/python3.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/Users/raymond/env/ai-worker/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/Users/raymond/env/ai-worker/lib/python3.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 15, in <module>
    from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2
  File ""/Users/raymond/env/ai-worker/lib/python3.7/site-packages/tensorflow/core/framework/node_def_pb2.py"", line 15, in <module>
    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
  File ""/Users/raymond/env/ai-worker/lib/python3.7/site-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
  File ""/Users/raymond/env/ai-worker/lib/python3.7/site-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>
    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
  File ""/Users/raymond/env/ai-worker/lib/python3.7/site-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 22, in <module>
    serialized_pb=_b('\n/tensorflow/core/framework/resource_handle.proto\x12\ntensorflow\""r\n\x13ResourceHandleProto\x12\x0e\n\x06\x64\x65vice\x18\x01 \x01(\t\x12\x11\n\tcontainer\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x11\n\thash_code\x18\x04 \x01(\x04\x12\x17\n\x0fmaybe_type_name\x18\x05 \x01(\tBn\n\x18org.tensorflow.frameworkB\x0eResourceHandleP\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\xf8\x01\x01\x62\x06proto3')
TypeError: __init__() got an unexpected keyword argument 'serialized_options'

"
27653,Support for Other Types of Tensors in tf.data (For example RaggedTensors / Sparse Tensors),"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**
Currently tf.data only supports Regular Tensors.
**Will this change the current api? How?**
This will not change the API on a large scale. This will just allow tf.data to return RaggedTensor or SparseTensors depending on the Users Need. This will save a lot of memory when it comes to reading Variable Length Sequences.
**Who will benefit with this feature?**
Anyone Who is using tensorflow.
**Any Other info.**
I did some digging in the repository, while trying to manually trace function calls and see which functions and classes are needed to be updated.
1. A similar Overload of GetNextInternal is needed in dataset.h for every new type of vector of Tensor to be Supported.(Sparse and Ragged) https://github.com/tensorflow/tensorflow/blob/222fea0388a9ca5ef5e736156e6604fbe51f07b0/tensorflow/core/framework/dataset.h#L840-L842
2. A new similar overload of GetNext() with new type of vector of the type of Tensor to be supported (Sparse and Ragged) https://github.com/tensorflow/tensorflow/blob/222fea0388a9ca5ef5e736156e6604fbe51f07b0/tensorflow/core/framework/dataset.h#L720-L721
3. Implement this GetNextInternal definition in the data kernel ops. Similar to these
https://github.com/tensorflow/tensorflow/blob/222fea0388a9ca5ef5e736156e6604fbe51f07b0/tensorflow/core/kernels/data/interleave_dataset_op.cc#L162-L164
4. Create modifications of Python Interfaces for these kernels, here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/dataset_ops.py"
27652,[TF 2.0 API Docs] tf.keras.activations.elu,"**System information**
- TensorFlow version: 2.0 alpha
- Doc Link:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations/elu
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/activations.py

**- Links**
Links exist. Should format the link to the original paper (delete ""-""). Also, should add authors and year of publishing 

E.g. 
```
Reference: ""Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)"" (Clevert et al, 2015)
```

**- Definition**
Since it just says: `Exponential linear unit` we can modify it to the following - similar to the [original paper](https://arxiv.org/abs/1511.07289) :
```
The exponential linear unit (ELU) with `alpha` > 0 is:
`x` if `x > 0` and `alpha * (exp(x)-1)` if `x < 0`
The ELU hyperparameter `alpha` () controls the value to which an ELU saturates for negative net inputs.
ELUs diminish the vanishing gradient effect.
```
Followed by word-for-word stuff from the [original paper](https://arxiv.org/abs/1511.07289):
```
ELUs have negative values which pushes the mean of the activations closer to zero. 
Mean activations that are closer to zero enable faster learning as they bring the gradient closer to the natural gradient. 
ELUs saturate to a negative value when the argument gets smaller. 
Saturation means a small derivative which decreases the variation and the information that is propagated to the next layer.""
```

**- Examples**
No examples given. Can add a modified example from the Intro to CNNs tutorial (where `elu` replaces ReLU - `relu`) 

E.g.
```
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='elu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='elu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='elu'))
```

**- Parameters**
Both params defined already but to aid the user it should state that `alpha` () should be set at 1.0  _by default_ and that:
```
`alpha` controls the value to which an ELU saturates for negative net inputs.
```
(source: [paper](https://arxiv.org/abs/1511.07289)) 

... instead of just 
```
`alpha`: A scalar, slope of negative section
```

**- Returns**
Defined but for clarity should say: 
```
The exponential linear unit (ELU) activation function: `x` if `x > 0` and `alpha * (exp(x) - 1)` if `x < 0`
```
 instead of simply `The exponential linear activation:...[equation]`

**- Raises**
Not defined.

**- Visuals**
Should be added to help the user. Example - see p.5 of the original paper: https://arxiv.org/pdf/1511.07289.pdf.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Yes"
27651,Starting a TF session (and nothing else) uses over 350MB of GPU memory,"On TF `1.13.1`, the code below:
```
with tf.Session(config=config) as sess:
    while True:
        print(""Session is open!"")
        time.sleep(1)
Uses 363MiB / 16280MiB according to nvidia-smi.
```

Am I missing something or is this normal? Shouldn't GPU usage be 0MiB at this point? If not, why?"
27649,strided_slice leads to unknown shape for Input layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
```bash
conda install tensorflow-gpu==2.0-alpha
```
- Python version:
3.7.1
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
not relevent
- GPU model and memory:
not relevent


**Describe the current behavior**
```python
import tensorflow as tf
from tensorflow.keras.layers import Input

with tf.device('/cpu:0'):
    t = Input((2, 3, 4), batch_size=1)
    output = tf.strided_slice(t, [0, 0, 0, 0], [-1, -1, -1, 2])
    print(output.shape)
```
Output is `<unknown>`

**Describe the expected behavior**
strided_slice should calculate the shape of the output tensor correctly.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27647,Kubernetes Charts for Tensorflow 2.0,"**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Would be nice to have Kubernetes charts which users can deploy and train or server the tf models. 

**Will this change the current API? How?**
NO, but will help users.

**Who will benefit with this feature?**
Those who are using K8s already...

Would need 2 charts:
1. CPU Version TF-2.0
2. GPU Version TF-2.0"
27646,Training on device with TensorFlow Lite,"This ticket tracks supporting on-device training with TensorFlow Lite. 

A few high-level milestones:

- [ ] Functional prototype for basic training (e.g. convolution / dense network)
- [ ] Functional prototype for training with control flow (e.g. RNN / LSTM networks)
- [ ] Productionize / optimize basic training
- [ ] Productionize / optimize training with control flow

See also #17328"
27644,[Feature Request] Need for manual parallelization of a tensorflow graph (or function),"- TensorFlow version (you are using): 2.0

**Describe the feature and the current behavior/state.**
Today, there's no easy way to parallelize a single tf graph (or function in 2.0) to different inputs. tf.map_fn doesn't parallelize and tf.while_loop doesn't generate error when the graph is run sequentially. It's strange really what the keyword parallel_iterations even does in tf.while_loop

**Will this change the current api? How?**
I request a feature named tf.prange (parallel_range) or something equivalent which can be used inside tf.function, as shown below,

     @tf.function
     def foo():
          for i in tf.prange(100):
                 ...

which will help us to get an error if an op depends on the values of previous flows in a function. Or some other solution to parallelize and raise error in case of no parallelization. As of now, everything happens in the backend and it's difficult to find the problems that lead to such sequential execution of tf graph.

**Who will benefit with this feature?**
Coders wanting to manually parallelize a particular graph to multiple inputs. (Potentially every ML developer with minimal resources)

P.S. This would make things extremely convenient.
https://github.com/tensorflow/tensorflow/issues/24774 and https://github.com/tensorflow/tensorflow/issues/1984 also state how inconvenient manual parallelization is in tensorflow since quite some time.

An exact same issue has been raise before. https://github.com/tensorflow/tensorflow/issues/12492"
27643,Java: add support for Alpine Linux (Docker),"**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
I'm using the TF Java library to serve prediction requests and I don't have any issue there.
However, I'm building a docker image from my server. The size is 343mb from openjdk:8-jdk-alpine, but TF does not work (Error loading shared library ld-linux-x86-64.so.2).
I noticed in the doc that only Ubuntu was supported. Ok.
I created the image from ubuntu:18.04 and installed the jdk over it... no more error as expected, but the size of the image jumps to 500m :(, even after cleaning out everything I could.

Any plans to make it work from the openjdk:8-jdk-alpine image (or provide a minimalist dockerfile which inherits from that one) ?

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
A lot, I can imagine

**Any Other info.**
"
27642,close,
27640,TF Lite conversion of minimal graph with tf.matmul fails on Linux but works on MacOS,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
`16.04.6 LTS (GNU/Linux 4.15.0-47-generic x86_64)` and `macOS Mojave Version 10.14.4`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): `pip install tf-nightly`
- TensorFlow version (use command below): `1.14.1-dev20190408`
- Python version: `2.7.16` (Mac) and `2.7.12` (Linux)
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
The example code below creates a minimal TensorFlow graph that computes a `tf.matmul` between two input matrices and exports the graph to TensorFlow Lite from the current session via the Python API. It invokes the TF Lite Interpreter on example input and compares the output to the result of `session.run`.

The code works on Mac, but fails on Linux during TF Lite conversion (see logs below).

**Describe the expected behavior**
It should work (or at least behave the same) on both operating systems.

I know that the [TF Lite Operator Compatibility](https://www.tensorflow.org/lite/guide/ops_compatibility#compatible_operations) states:

> tf.matmul - as long as the second argument is constant and transposition is not used

This is not the case here, but it still curious that it works on Mac.

**Code to reproduce the issue**
```
import numpy
import tensorflow as tf

def export_tflite_from_session(session, input_nodes, output_nodes, tflite_filename):
    print(""Converting to tflite..."")
    converter = tf.lite.TFLiteConverter.from_session(session, input_nodes, output_nodes)
    tflite_model = converter.convert()
    with open(tflite_filename, ""wb"") as f:
        f.write(tflite_model)
    print(""Converted %s."" % tflite_filename)

def test_tflite_model(tflite_filename, examples):
    print(""Loading TFLite interpreter for %s..."" % tflite_filename)
    interpreter = tf.lite.Interpreter(model_path=tflite_filename)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    print(""input details: %s"" % input_details)
    print(""output details: %s"" % output_details)

    for i, input_tensor in enumerate(input_details):
        interpreter.set_tensor(input_tensor['index'], examples[i])
    interpreter.invoke()
    model_output = []
    for i, output_tensor in enumerate(output_details):
        model_output.append(interpreter.get_tensor(output_tensor['index']))
    return model_output

def main():
    tflite_filename = ""model.tflite""
    shape_a = (2, 3, 4)
    shape_b = (2, 4, 5)

    a = tf.placeholder(dtype=tf.float32, shape=shape_a, name=""A"")
    b = tf.placeholder(dtype=tf.float32, shape=shape_b, name=""B"")
    c = tf.matmul(a, b, name=""output"")

    numpy.random.seed(1234)
    a_ = numpy.random.rand(*shape_a).astype(numpy.float32)
    b_ = numpy.random.rand(*shape_b).astype(numpy.float32)
    with tf.Session() as session:
        session_output = session.run(c, feed_dict={a: a_, b: b_})
        export_tflite_from_session(session, [a, b], [c], tflite_filename)

    tflite_output = test_tflite_model(tflite_filename, [a_, b_])
    tflite_output = tflite_output[0]

    print(""Input example:"")
    print(a_)
    print(a_.shape)
    print(b_)
    print(b_.shape)
    print(""Session output:"")
    print(session_output)
    print(session_output.shape)
    print(""TFLite output:"")
    print(tflite_output)
    print(tflite_output.shape)
    print(numpy.allclose(session_output, tflite_output))

if __name__ == '__main__':
    main()
```

**Other info / logs**
Output on Mac:
```
2019-04-08 14:46:05.835019: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Converting to tflite...
2019-04-08 14:46:05.837757: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-04-08 14:46:05.837803: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2019-04-08 14:46:05.839940: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-04-08 14:46:05.839979: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
Converted model.tflite.
Loading TFLite interpreter for model.tflite...
INFO: Initialized TensorFlow Lite runtime.
input details: [{'index': 0, 'shape': array([2, 3, 4], dtype=int32), 'quantization': (0.0, 0L), 'name': 'A', 'dtype': <type 'numpy.float32'>}, {'index': 1, 'shape': array([2, 4, 5], dtype=int32), 'quantization': (0.0, 0L), 'name': 'B', 'dtype': <type 'numpy.float32'>}]
output details: [{'index': 2, 'shape': array([2, 3, 5], dtype=int32), 'quantization': (0.0, 0L), 'name': 'output', 'dtype': <type 'numpy.float32'>}]
Input example:
[[[0.19151945 0.62210876 0.43772775 0.7853586 ]
  [0.77997583 0.2725926  0.27646425 0.8018722 ]
  [0.95813936 0.87593263 0.35781726 0.5009951 ]]

 [[0.6834629  0.71270204 0.37025076 0.5611962 ]
  [0.50308317 0.01376845 0.7728266  0.8826412 ]
  [0.364886   0.6153962  0.07538124 0.368824  ]]]
(2, 3, 4)
[[[0.9331401  0.65137815 0.39720258 0.78873014 0.31683612]
  [0.56809866 0.8691274  0.4361734  0.8021476  0.14376682]
  [0.70426095 0.7045813  0.21879211 0.92486763 0.44214076]
  [0.90931594 0.05980922 0.18428709 0.04735528 0.6748809 ]]

 [[0.59462476 0.5333102  0.04332406 0.5614331  0.32966843]
  [0.5029668  0.11189432 0.6071937  0.5659447  0.00676406]
  [0.6174417  0.9121229  0.7905241  0.99208146 0.95880175]
  [0.7919641  0.28525096 0.62491673 0.4780938  0.19567518]]]
(2, 4, 5)
Session output:
[[[1.5545473  1.0208298  0.58792216 1.0921113  0.87367964]
  [1.8065444  0.98772776 0.636969   1.1275158  0.94971865]
  [2.0992541  1.6674836  0.93324846 1.812999   0.9258208 ]]

 [[1.437925   0.942041   1.1057518  1.422692   0.69494617]
  [1.4822663  1.226527   1.1926711  1.4789319  1.0796423 ]
  [0.86513305 0.43742114 0.679548   0.8042561  0.26889932]]]
(2, 3, 5)
TFLite output:
[[[1.5545473  1.0208298  0.58792216 1.0921113  0.87367964]
  [1.8065444  0.98772776 0.636969   1.1275158  0.94971865]
  [2.0992541  1.6674836  0.93324846 1.812999   0.9258208 ]]

 [[1.437925   0.942041   1.1057518  1.422692   0.69494617]
  [1.4822663  1.226527   1.1926711  1.4789319  1.0796423 ]
  [0.86513305 0.43742114 0.679548   0.8042561  0.26889932]]]
(2, 3, 5)
True
```

Output on Linux:
```
2019-04-08 14:47:09.730317: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-04-08 14:47:09.734305: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-04-08 14:47:10.718760: E tensorflow/stream_executor/cuda/cuda_driver.cc:320] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2019-04-08 14:47:10.718805: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:166] retrieving CUDA diagnostic information for host: everest6
2019-04-08 14:47:10.718811: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:173] hostname: everest6
2019-04-08 14:47:10.718867: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 410.104.0
2019-04-08 14:47:10.718890: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 410.104.0
2019-04-08 14:47:10.718896: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:308] kernel version seems to match DSO: 410.104.0
2019-04-08 14:47:10.737178: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4200000000 Hz
2019-04-08 14:47:10.737608: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x52d4340 executing computations on platform Host. Devices:
2019-04-08 14:47:10.737622: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-04-08 14:47:10.738962: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1288] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
Converting to tflite...
2019-04-08 14:47:10.739692: I tensorflow/core/grappler/devices.cc:50] Number of eligible GPUs (core count >= 8): 0
2019-04-08 14:47:10.739747: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2019-04-08 14:47:10.741001: I tensorflow/core/grappler/devices.cc:50] Number of eligible GPUs (core count >= 8): 0
2019-04-08 14:47:10.741033: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
Traceback (most recent call last):
  File ""minimal_tflite_test.py"", line 67, in <module>
    main()
  File ""minimal_tflite_test.py"", line 47, in main
    export_tflite_from_session(session, [a, b], [c], tflite_filename)
  File ""minimal_tflite_test.py"", line 9, in export_tflite_from_session
    tflite_model = converter.convert()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/lite.py"", line 742, in convert
    **converter_kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/convert.py"", line 410, in toco_convert_impl
    input_data.SerializeToString())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/convert.py"", line 176, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-04-08 14:47:11.490702: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1 operators, 3 arrays (0 quantized)
2019-04-08 14:47:11.490766: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1 operators, 3 arrays (0 quantized)
2019-04-08 14:47:11.490876: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 11 operators, 25 arrays (0 quantized)
2019-04-08 14:47:11.490918: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1800] Check failed: axis >= 0 (-323499096 vs. 0)
Aborted (core dumped)
```
The error occurs during TF Lite conversion. The erroneous axis value (`-323499096`) is different every time the script is called. If the value is positive, the error is:
```
2019-04-08 15:14:43.877396: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1801] Check failed: axis < input_shape.dimensions_count() (539352088 vs. 3)
```
"
27639,keras `Model.__call__` fails to propagate `Lambda`s with multiple outputs correctly,"
## System Information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes (see below)
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from: pip
- TensorFlow version (use command below): ('v1.13.1-0-g6612da8951', '1.13.1')
- Python version: 2.7.12
- GPU model and memory: quadro 620

## Current Behaviour
Calls to models created using the functional interface fail to appropriately propagate `Lambda` outputs with multiple outputs. Model construction works as expected.

## Expected Behaviour
Outputs of intermediate layers should have the same structure during model construction as during model call.

## Code to Reproduce
```python
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf

x = tf.keras.layers.Input(shape=(), dtype=tf.float32)
y = tf.keras.layers.Input(shape=(), dtype=tf.float32)


def add_and_mul(args):
    x, y = args
    return x + y, x * y


def add_mul_model():
    x = tf.keras.layers.Input(shape=(), dtype=tf.float32)
    y = tf.keras.layers.Input(shape=(), dtype=tf.float32)
    s = tf.keras.layers.Lambda(lambda a: a[0] + a[1])([x, y])
    p = tf.keras.layers.Lambda(lambda a: a[0] * a[1])([x, y])
    return tf.keras.models.Model(inputs=[x, y], outputs=[s, p])


layer = tf.keras.layers.Lambda(add_and_mul)  # bugged
# layer = add_mul_model()                    # not bugged
s, p = layer([x, y])
# the following gives the same output either way
print('s = %s' % s)
print('p = %s' % p)


def double(x):
    print('argument to double: %s' % str(x))
    # using the bugged version, this prints:
    # argument to double:
    #       Tensor(""lambda/add:0"", shape=(?,), dtype=float32)
    # argument to double: (
    #     <tf.Tensor 'model/lambda/add:0' shape=(1,) dtype=float32>,
    #     <tf.Tensor 'model/lambda/mul:0' shape=(1,) dtype=float32>)

    # unbugged version prints:
    # argument to double:
    #       Tensor(""model/lambda/add:0"", shape=(?,), dtype=float32)
    # argument to double:
    #       Tensor(""model_1/model/lambda/add:0"", shape=(1,), dtype=float32)
    return x * 2


s2 = tf.keras.layers.Lambda(double)(s)


model = tf.keras.models.Model(inputs=[x, y], outputs=s2)

x = tf.constant([3.0], dtype=tf.float32)
y = tf.constant([4.0], dtype=tf.float32)

out = model([x, y])
print(out)

with tf.Session() as sess:
    print(sess.run(out))
    # (([7.], [12.]), ([7.], [12.])) for bugged version, 14. for good version
```
"
27638,Typo in SpaceToBatchND and BatchToSpaceND documentation,"**System information**
- TensorFlow version: 1.13
- Doc Link: https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/space-to-batch-n-d, https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/batch-to-space-n-d


**Describe the documentation issue**
Example 2 in https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/space-to-batch-n-d says

> The output tensor has shape `[4, 1, 1, 3]` and value:

>     [[[1, 2, 3]], [[4, 5, 6]], [[7, 8, 9]], [[10, 11, 12]]]

However, that value's shape is `[4, 1, 3]`. It appears the shape is correct, but the value should be

    [[[[1, 2, 3]]], [[[4, 5, 6]]], [[[7, 8, 9]]], [[[10, 11, 12]]]]

The same issue is present in https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/batch-to-space-n-d example 2.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Yes."
27634,Missing documentation for variable ops.,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: v1.13
- Doc Link:


**Describe the documentation issue**
Hi, I am looking for documentation on the ReadVariableOp and VarHandleOp. Specifically as to what are the inputs that they take and what do they produce exactly. I dug around the source code for mentions of these ops and found some calls [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/resource_variable_ops.py#L196) and [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/resource_variable_ops.py#L864). While this hints at the signature of these ops, I would love to look at the implementation, which probably exists in [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/resource_variable_ops.py#L37) module, but I can not find it in this public repo's master branch. Could someone point me to this?

Alternately, is there any documentation on these ops written [this](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/fused-batch-norm) way?

Thanks!
**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
27633,[doc/keras] incorrect comment in `__init__` of `tf.keras.layers.AveragePooling1D`,"
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling1D#arguments


**Describe the documentation issue**
See the comment below

```
pool_size: Integer, size of the **max** pooling windows.
```

It should be **average** instead of **max** since this is average pooling.

Furthermore, the format of the description for `input shape` and `output shape` is broken.
It is not rendered in a list format."
27632,[doc/keras] incorrect comment in the example for `tf.keras.layers.Add`,"
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add#class_add


**Describe the documentation issue**

See the code example (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add#class_add)

```python
    added = keras.layers.Add()([x1, x2])  # equivalent to added =
    keras.layers.add([x1, x2])
```

It should be

```
    added = keras.layers.Add()([x1, x2])  # equivalent to added = keras.layers.add([x1, x2])
```"
27631,[doc/keras] `predict` of `tf.keras.models.Sequential` is rendered incorrectly,"
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential#predict


**Describe the documentation issue**

The argument list is not rendered correctly.

See it below

```
Arguments:
x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset or a dataset iterator. - A generator or keras.utils.Sequence instance. * batch_size: Integer or None. Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size is your data is in the form of symbolic tensors, dataset, dataset iterators, generators, or keras.utils.Sequence instances (since they generate batches). * verbose: Verbosity mode, 0 or 1. * steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None. * max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. * workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. * use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True, use process-based threading. If unspecified, use_multiprocessing will default to False. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes.
```

It should be rendered in a list/item format."
27630,Model Start At Different Loss Level After Resuming The Training,"I'm using `tensorflow.keras` in order to build a simple neural network with 3 dense layers. I was able to successfully train the model for 9000 epochs reaching a Mean of Squared Errors (`MSE`) of **0.0496**. However resuming the model, it starts training at about **57** `MSE`.

This might indicates that the model weights were not loaded successfully but when restarting the training process from the beginning (without loading previous saved weights), `MSE` starts at about +9000.

Here is my code:

```
from __future__ import absolute_import, division, print_function

import pathlib

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import model_from_json
from tensorflow.keras.models import load_model

print(tf.__version__)

dataset_path = 'D:\\FXData\\data.csv'
checkpoint_model_json_path = 'modelBackup/model.json'
checkpoint_weights_h5_path = 'modelBackup/weights00009000.h5'
resume_from_checkpoint = True

print('reading dataset...')
column_names = ['paircode','x1o','x1h','x1l','x1c','x1v','x2o','x2h','x2l','x2c','x2v','x3o','x3h','x3l','x3c','x3v','x4o','x4h','x4l','x4c','x4v','x5o','x5h','x5l','x5c','x5v','x6o','x6h','x6l','x6c','x6v','x7o','x7h','x7l','x7c','x7v','x8o','x8h','x8l','x8c','x8v','x9o','x9h','x9l','x9c','x9v','x10o','x10h','x10l','x10c','x10v','x11o','x11h','x11l','x11c','x11v','x12o','x12h','x12l','x12c','x12v','x13o','x13h','x13l','x13c','x13v','x14o','x14h','x14l','x14c','x14v','x15o','x15h','x15l','x15c','x15v','x16o','x16h','x16l','x16c','x16v','x17o','x17h','x17l','x17c','x17v','x18o','x18h','x18l','x18c','x18v','x19o','x19h','x19l','x19c','x19v','x20o','x20h','x20l','x20c','x20v','x21o','x21h','x21l','x21c','x21v','x22o','x22h','x22l','x22c','x22v','x23o','x23h','x23l','x23c','x23v','x24o','x24h','x24l','x24c','x24v','x25o','x25h','x25l','x25c','x25v','x26o','x26h','x26l','x26c','x26v','x27o','x27h','x27l','x27c','x27v','x28o','x28h','x28l','x28c','x28v','x29o','x29h','x29l','x29c','x29v','x30o','x30h','x30l','x30c','x30v','x31o','x31h','x31l','x31c','x31v','x32o','x32h','x32l','x32c','x32v','x33o','x33h','x33l','x33c','x33v','x34o','x34h','x34l','x34c','x34v','x35o','x35h','x35l','x35c','x35v','x36o','x36h','x36l','x36c','x36v','x37o','x37h','x37l','x37c','x37v','x38o','x38h','x38l','x38c','x38v','x39o','x39h','x39l','x39c','x39v','x40o','x40h','x40l','x40c','x40v','x41o','x41h','x41l','x41c','x41v','x42o','x42h','x42l','x42c','x42v','x43o','x43h','x43l','x43c','x43v','x44o','x44h','x44l','x44c','x44v','x45o','x45h','x45l','x45c','x45v','x46o','x46h','x46l','x46c','x46v','x47o','x47h','x47l','x47c','x47v','x48o','x48h','x48l','x48c','x48v','x49o','x49h','x49l','x49c','x49v','x50o','x50h','x50l','x50c','x50v','nextclose']
dataset = pd.read_csv(dataset_path, names=column_names,
                      na_values = ""?"", comment='\t',
                      sep="","", skipinitialspace=True, skiprows = [0])

print('printing dataset tail...')
print(dataset.tail())

train_dataset = dataset.sample(frac=0.8,random_state=0)
test_dataset = dataset.drop(train_dataset.index)

train_labels = train_dataset.pop('nextclose')
test_labels = test_dataset.pop('nextclose')

def norm(x):
  return x
#  return (x - train_stats['mean']) / train_stats['std']

print('normalizing dataset...')  
normed_train_data = norm(train_dataset)
normed_test_data = norm(test_dataset)

def build_model():
  print('building the model')
  model = keras.Sequential([
    layers.Dense(512, activation=tf.nn.relu, input_shape=[len(train_dataset.keys())]),
    layers.Dense(512, activation=tf.nn.relu), layers.Dense(256, activation=tf.nn.relu),
    layers.Dense(1, activation='linear')
  ])
  
  model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
  return model

def load_model_():
  print('loading the model')
  loaded_model = load_model(checkpoint_weights_h5_path)
  return loaded_model


if resume_from_checkpoint:
  model = load_model_()
else:
  model = build_model()

model.summary()

print('testing 10 widthed batch...')
example_batch = normed_train_data[:10]
example_result = model.predict(example_batch)
print(example_result)

print('fitting the model...')
mc = keras.callbacks.ModelCheckpoint('weights{epoch:08d}.h5', save_weights_only=False, period=100)

history = model.fit(
  normed_train_data, train_labels,
  epochs=100, validation_split = 0.2, verbose=2,
  batch_size=1000000, callbacks=[mc])

print('evaluating the model...')
loss, accuracy = model.evaluate(normed_test_data, test_labels, verbose=0)
print(""Testing set MSE: {:5.2f} nextclose"".format(loss))
print(""Testing set Accuracy: {:5.2f} nextclose"".format(accuracy))
```"
27629,Tensorflow lite undefined reference,"**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: source
- TensorFlow version: 1.12.1
- Bazel version: 0.15.0
- GCC/Compiler version: 7.3.0
(No python related information since I try to compile lite for C/C++ usage)

**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

First of all, I use CMake through a combo  `add_custom_command` and `add_custom_target` to handle the tensorflow build for me. I try to build tensorflow lite for x86 or ARM depending on my needs (I've done it before on the 1.10 tensorflow version).

The commands I execute to build tensorflow are in a script: 
```
bash configure
bazel --output_base=~/.tmp/tensorflow_lite/1.12.1/.Debug_x86 \
         build --jobs=4 \
                 //tensorflow/contrib/lite/experimental/c:libtensorflowlite_c.so
```
The build happens fine, I link `libtensorflowlite_c.so` to the custom CMake target I want to use in my own environment and it shows correctly on my make logs. 

However I have undefined references for: 
```
`tflite::DefaultErrorReporter()'
`tflite::Interpreter::AllocateTensors()'
`tflite::InterpreterBuilder::InterpreterBuilder(tflite::FlatBufferModel const&, tflite::OpResolver const&)'
`vtable for tflite::MutableOpResolver'
`tflite::InterpreterBuilder::operator()(std::unique_ptr<tflite::Interpreter, std::default_delete<tflite::Interpreter> >*)'
`tflite::InterpreterBuilder::~InterpreterBuilder()'
`tflite::Interpreter::Invoke()'
`tflite::Interpreter::~Interpreter()'
`vtable for tflite::ops::builtin::BuiltinOpResolver'
`tflite::Interpreter::SetNumThreads(int)'
`tflite::FlatBufferModel::~FlatBufferModel()'
`tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'
`tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'
```
I quickly checked the bazel workspace and build directives and it seems my issues are related to the `framework` cc_library bazel is creating and that is linked to `libtensorflowlite_c` through the `c_api`.

I don't know bazel a lot, I'm a bit lost with the dependencies. I would say the dependencies should propagate like this `framework -> c_api & c_api_internal -> libtensorflowlite_c.so`. Is it what happen?
Should I add `//tensorflow/contrib/lite:framework` and other bazel target to be sure they are built? 

How bazel handles mutual dependency, is this an acceptable situation? It happens between `context` (`//tensorflow/contrib/lite`) and `c_api_internal` (`//tensorflow/contrib/lite/experimental/c`) and between `frawework` (`//tensorflow/contrib/lite`) and `c_api_internal` (`//tensorflow/contrib/lite/experimental/c`).

```
# context definition, needs c_api_internal
cc_library(
    name = ""context"",
    hdrs = [""context.h""],
    deps = [""//tensorflow/contrib/lite/c:c_api_internal""],
)
# framework definition, needs c_api_internal (only deps are shown)
cc_library(
    name = ""framework"",
    srcs = [...] + select({...}),
    hdrs = [...],
    copts = tflite_copts(),
    defines = select({...}),
    linkopts = [] + select({...}),
    deps = [
        "":arena_planner"",
        "":graph_info"",
        "":memory_planner"",
        "":schema_fbs_version"",
        "":simple_memory_arena"",
        "":string"",
        "":util"",
        ""//tensorflow/contrib/lite/c:c_api_internal"",
        ""//tensorflow/contrib/lite/core/api"",
        ""//tensorflow/contrib/lite/kernels:eigen_support"",
        ""//tensorflow/contrib/lite/kernels:gemm_support"",
        ""//tensorflow/contrib/lite/nnapi:nnapi_lib"",
        ""//tensorflow/contrib/lite/profiling:profiler"",
        ""//tensorflow/contrib/lite/schema:schema_fbs"",
    ] + select({...}),
)
# c_api_internal definition, needs context & framework
cc_library(
    name = ""c_api_internal"",
    srcs = [""c_api.h""],
    hdrs = [""c_api_internal.h""],
    copts = tflite_copts(),
    visibility = [
        ""//tensorflow/contrib/lite/experimental/c:__subpackages__"",
    ],
    deps = [
        ""//tensorflow/contrib/lite:context"",
        ""//tensorflow/contrib/lite:framework"",
    ],
)
# 
```

CLOSED: Still have the issue but it's a CMake linkage issue, not related to tensorflow lite build."
27628,tensorflow wont start windows 10 python 3.7.3,"
  File ""<ipython-input-2-0786882784e8>"", line 1, in <module>
    runfile('C:/Users/Christophe/.spyder-py3/temp.py', wdir='C:/Users/Christophe/.spyder-py3')

  File ""e:\program files\python3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 824, in runfile
    execfile(filename, namespace)

  File ""e:\program files\python3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/Christophe/.spyder-py3/temp.py"", line 7, in <module>
    import tensorflow as tf

  File ""e:\program files\python3\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import

  File ""e:\program files\python3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""e:\program files\python3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""e:\program files\python3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""e:\program files\python3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""e:\program files\python3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""e:\program files\python3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""e:\program files\python3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
"
27627,[doc/keras] `__init__` of `tf.keras.losses.BinaryCrossentropy` is not rendered correctly,"
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy#class_binarycrossentropy


**Describe the documentation issue**

The `__init__` function of `tf.keras.losses.BinaryCrossentropy` is not rendered correctly.

See the link here https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy#methods"
27626,Huawei (android 6.0OpenGL3.1 is Mali) GPU slower than CPU,"I have two mobile phones, one is XiaoMi  (Android 9.0OpenGL3.1 is Adreno), the other is Huawei (Android 6.0, OpenGL3.1 is Mail). When I run this demo app (Tensor Flow Lite GPU delegate), GPU is faster in the XiaoMi mobile phone than CPU, but in Huawei mobile phone, GPU is slower than CPU. What's the reason, the system version problem? Or the OpenGL ES chip problem?"
27623,CUDA 10 / CUDNN 7.5 performance loss (Titan V),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): openSUSE Leap 42.3
- GPU model and memory: Titan V
- GCC/Compiler version (if compiling from source): 4.8
- Nvidia Driver Version: 418.56

**Problem:** I experience consistent performance losses in TF using CUDA 10.0/CUDNN 7.5. 
Here is a minimal example:

```
import tensorflow as tf
import time

matrix_dim = (5120,5120)
noof_matmul = 512
num_gpus = 1
float_type = tf.float32
c = []

print(tf.__file__)
for d in range(num_gpus):
    with tf.device('/gpu:%s' % d):
        a=tf.random_normal(matrix_dim,dtype=float_type)
        for i in range(noof_matmul):
            a=tf.matmul(a,a)
        c.append(a.op)

with tf.Session() as sess:
    sess.run(c)
    start=time.time()
    for i in range(10):
        sess.run(c)

t = (time.time()-start)/10.
tflops = noof_matmul * (2 * matrix_dim[0]**3 - matrix_dim[0]**2) / t / 10**12
print(t, 'sec, ', tflops, 'TFlops')
```

**Results Titan V:**
Tensorflow 1.13.1 built from source, bazel 0.24.0, CUDA 10.0, CUDNN 7.5.0:
```diff
-FP16: 1.6899 sec, 81.3209 TFlops
-FP32: 10.6873 sec, 12.8588 TFlops
```
Tensorflow 1.9 built from source (same configuration besides versions), bazel 0.11.1, CUDA 9.1, CUDNN 7.1.4:
```diff
+FP16: 1.5391 sec, 89.2890 TFlops
+FP32: 10.1137 sec, 13.5881 TFlops
```
Pip wheel tensorflow_gpu-1.13.1-cp27-cp27mu-manylinux1_x86_64.whl, CUDA 10.0, CUDNN 7.5.0:
```diff
-FP16: 1.6902 sec, 81.3057 TFlops
-FP32: 10.5577 sec, 13.01656 TFlops
```
Conda (Python 3.6) linux-64/tensorflow-gpu-1.13.1-h0d30ee6_0.tar.bz2, CUDA 9.2, CUDNN 7.3.1:
```diff
+FP16: 1.5389 sec, 89.2995 TFlops
+FP32: 10.1150 sec, 13.5863 TFlops
```

Have you experienced similar performance losses? Do you know any solutions? If necessary, I can run tests on other GPU models as well.






"
27622,about TensorFlow Lite GPU delegate,"about TensorFlow Lite GPU delegate,Here's a description
If some of the ops are not supported by the GPU delegate, the framework will only run a part of the graph on the GPU and the remaining part on the CPU. Due to the high cost of CPU/GPU synchronization, a split execution mode like this will often result in a performance slower than when the whole network is run on the CPU alone
I have a questionIf some of the ops are not supported by the GPU delegatewhich ops not supported by the GPU delegate  ??????"
27621,Tensorflow speech commands error,"<em>Im getting error when  convert h5 file to tflite file in speech commands example</em>

**System information**
- Ubuntu 18.04

- TensorFlow version:1.10.0
- Python version:3.6.7
- Installed using pip
- CUDA/cuDNN version:
- GPU model and memory:



**
When I try to create tflite file Im getting that
https://github.com/tensorflow/examples/blob/master/lite/examples/speech_commands/android/README.md


My error:
(keras) eco@eco-1Y3-GNB1554A1I3:~/Masast/yz/keras/ml/export$ python convert_keras_lite.py
WARNING:tensorflow:From convert_keras_lite.py:28: TocoConverter.from_keras_model_file (from tensorflow.contrib.lite.python.lite) is deprecated and will be removed in a future version.
Instructions for updating:
Use `lite.TFLiteConverter.from_keras_model_file` instead.
Traceback (most recent call last):
  File ""convert_keras_lite.py"", line 28, in <module>
    output_arrays)
  File ""/home/eco/Masast/yz/keras/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 306, in new_func
    return func(*args, **kwargs)
  File ""/home/eco/Masast/yz/keras/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py"", line 556, in from_keras_model_file
    input_shapes, output_arrays)
  File ""/home/eco/Masast/yz/keras/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py"", line 368, in from_keras_model_file
    keras_model = _keras.models.load_model(model_file)
  File ""/home/eco/Masast/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py"", line 230, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""/home/eco/Masast/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py"", line 310, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""/home/eco/Masast/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py"", line 64, in deserialize
    printable_module_name='layer')
  File ""/home/eco/Masast/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 173, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/home/eco/Masast/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1292, in from_config
    process_layer(layer_data)
  File ""/home/eco/Masast/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1278, in process_layer
    layer = deserialize_layer(layer_data, custom_objects=custom_objects)
  File ""/home/eco/Masast/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py"", line 64, in deserialize
    printable_module_name='layer')
  File ""/home/eco/Masast/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 175, in deserialize_keras_object
    return cls.from_config(config['config'])
  File ""/home/eco/Masast/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1606, in from_config
    return cls(**config)
  File ""/home/eco/Masast/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py"", line 330, in __init__
    self.activation = activations.get(activation)
  File ""/home/eco/Masast/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/activations.py"", line 206, in get
    return deserialize(identifier)
  File ""/home/eco/Masast/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/activations.py"", line 197, in deserialize
    printable_module_name='activation function')
  File ""/home/eco/Masast/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 193, in deserialize_keras_object
    function_name)
ValueError: Unknown activation function:relu6


**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27620,LD_LIRARY_PATH instead of LD_LIBRARY_PATH ,"Hello, 
in tensorflow/stream_executor/platform/default/dso_loader.cc line 50
LD_LIRARY_PATH is called instead of LD_LIBRARY_PATH
Best"
27618,not support MatrixBandPart in tflite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version : 2.0alpha


**Provide the text output from tflite_convert**

```
Here is a list of operators for which you will need custom implementations: MatrixBandPart.
```

"
27617,'flatten_atrous_conv'  misses to flatten some atrous_convs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):  master
- Python version: 2.7
- Bazel version (if compiling from source): 0.22.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
I'm trying to optimize TF Model ' [xception65_coco_voc_trainval](http://download.tensorflow.org/models/deeplabv3_pascal_trainval_2018_01_04.tar.gz)' with graph_transforms tools.

Optimize for inference
`
bazel-bin/tensorflow/python/tools/optimize_for_inference --input=frozen.pb --output=stripped.pb --frozen_graph=True --input_names=""sub_7"" --output_names=""ResizeBilinear_2""
`
Graph Transforms
`bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=""stripped.pb"" --out_graph=""flatten.pb"" --inputs='sub_7' --outputs='ResizeBilinear_2' --transforms='flatten_atrous_conv'`

**Describe the expected behavior**
'flatten_atrous_conv'  misses to flatten some atrous_convs. 
![transform](https://user-images.githubusercontent.com/31765154/55704734-80988800-5a0f-11e9-8985-69f0bcb7b837.jpg)

"
27615, https://www.tensorflow.org/alpha/tutorials/next_steps is 404,"on this page
https://www.tensorflow.org/alpha/tutorials/keras

the next step link
 
 
 learning resources are listed in next steps.
 https://www.tensorflow.org/alpha/tutorials/next_steps
 
 is 404
 "
27614,how to freeze graph in tensorflow 2.0,not found any related document
27613,No improvement in performance of deeplabv3_257_mv_gpu.tflite on TFLite for GPU,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MI 5
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: 

**Describe the current behavior**
I'm trying to run TFLite Model '[deeplabv3_257_mv_gpu.tflite](https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/deeplabv3_257_mv_gpu.tflite)' with GPU delegate on Android  devices.  According to PR for label_image #27464,  I modified the label_image code to support the gpu delegate and removed the code from [205](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/label_image.cc#L205) to [251](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/label_image.cc#L251) lines .  

```
void RunInference(Settings* s) {
  if (!s->model_name.c_str()) {
    LOG(ERROR) << ""no model file name\n"";
    exit(-1);
  }

  std::unique_ptr<tflite::FlatBufferModel> model;
  std::unique_ptr<tflite::Interpreter> interpreter;
  model = tflite::FlatBufferModel::BuildFromFile(s->model_name.c_str());
  if (!model) {
    LOG(FATAL) << ""\nFailed to mmap model "" << s->model_name << ""\n"";
    exit(-1);
  }
  LOG(INFO) << ""Loaded model "" << s->model_name << ""\n"";
  model->error_reporter();
  LOG(INFO) << ""resolved reporter\n"";

  tflite::ops::builtin::BuiltinOpResolver resolver;

  tflite::InterpreterBuilder(*model, resolver)(&interpreter);
  if (!interpreter) {
    LOG(FATAL) << ""Failed to construct interpreter\n"";
    exit(-1);
  }

  interpreter->UseNNAPI(s->accel);
  interpreter->SetAllowFp16PrecisionForFp32(s->allow_fp16);

  if (s->verbose) {
    LOG(INFO) << ""tensors size: "" << interpreter->tensors_size() << ""\n"";
    LOG(INFO) << ""nodes size: "" << interpreter->nodes_size() << ""\n"";
    LOG(INFO) << ""inputs: "" << interpreter->inputs().size() << ""\n"";
    LOG(INFO) << ""input(0) name: "" << interpreter->GetInputName(0) << ""\n"";

    int t_size = interpreter->tensors_size();
    for (int i = 0; i < t_size; i++) {
      if (interpreter->tensor(i)->name)
        LOG(INFO) << i << "": "" << interpreter->tensor(i)->name << "", ""
                  << interpreter->tensor(i)->bytes << "", ""
                  << interpreter->tensor(i)->type << "", ""
                  << interpreter->tensor(i)->params.scale << "", ""
                  << interpreter->tensor(i)->params.zero_point << ""\n"";
    }
  }

  if (s->number_of_threads != -1) {
    interpreter->SetNumThreads(s->number_of_threads);
  }

  int image_width = 224;
  int image_height = 224;
  int image_channels = 3;
  std::vector<uint8_t> in = read_bmp(s->input_bmp_name, &image_width,
                                     &image_height, &image_channels, s);

  int input = interpreter->inputs()[0];
  if (s->verbose) LOG(INFO) << ""input: "" << input << ""\n"";

  const std::vector<int> inputs = interpreter->inputs();
  const std::vector<int> outputs = interpreter->outputs();

  if (s->verbose) {
    LOG(INFO) << ""number of inputs: "" << inputs.size() << ""\n"";
    LOG(INFO) << ""number of outputs: "" << outputs.size() << ""\n"";
  }
#if defined(ANDROID) || defined(__ANDROID__)
  TfLiteGpuDelegateOptions kMyOptions = {
      .metadata = nullptr,
      .compile_options =
          {
              .precision_loss_allowed = 0,
              .preferred_gl_object_type = TFLITE_GL_OBJECT_TYPE_FASTEST,
              .dynamic_batch_enabled = 0,
          },
  };
  if (s->allow_fp16) kMyOptions.compile_options.precision_loss_allowed = 1;

  TfLiteDelegate* delegate;
  if (s->gl_backend) {
    delegate = TfLiteGpuDelegateCreate(&kMyOptions);
    if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) return;
  } else {
#endif
    if (interpreter->AllocateTensors() != kTfLiteOk) {
      LOG(FATAL) << ""Failed to allocate tensors!"";
    }
#if defined(ANDROID) || defined(__ANDROID__)
  }
#endif

  if (s->verbose) PrintInterpreterState(interpreter.get());

  // get input dimension from the input tensor metadata
  // assuming one input only
  TfLiteIntArray* dims = interpreter->tensor(input)->dims;
  int wanted_height = dims->data[1];
  int wanted_width = dims->data[2];
  int wanted_channels = dims->data[3];

  switch (interpreter->tensor(input)->type) {
    case kTfLiteFloat32:
      s->input_floating = true;
      resize<float>(interpreter->typed_tensor<float>(input), in.data(),
                    image_height, image_width, image_channels, wanted_height,
                    wanted_width, wanted_channels, s);
      break;
    case kTfLiteUInt8:
      resize<uint8_t>(interpreter->typed_tensor<uint8_t>(input), in.data(),
                      image_height, image_width, image_channels, wanted_height,
                      wanted_width, wanted_channels, s);
      break;
    default:
      LOG(FATAL) << ""cannot handle input type ""
                 << interpreter->tensor(input)->type << "" yet"";
      exit(-1);
  }

  profiling::Profiler* profiler = new profiling::Profiler();
  interpreter->SetProfiler(profiler);

  if (s->profiling) profiler->StartProfiling();
  if (s->loop_count > 1)
    for (int i = 0; i < s->number_of_warmup_runs; i++) {
      if (interpreter->Invoke() != kTfLiteOk) {
        LOG(FATAL) << ""Failed to invoke tflite!\n"";
      }
    }

  struct timeval start_time, stop_time;
  gettimeofday(&start_time, nullptr);
  for (int i = 0; i < s->loop_count; i++) {
    if (interpreter->Invoke() != kTfLiteOk) {
      LOG(FATAL) << ""Failed to invoke tflite!\n"";
    }
  }
  gettimeofday(&stop_time, nullptr);
  LOG(INFO) << ""invoked \n"";
  LOG(INFO) << ""average time: ""
            << (get_us(stop_time) - get_us(start_time)) / (s->loop_count * 1000)
            << "" ms \n"";
#if 0
  if (s->profiling) {
    profiler->StopProfiling();
    auto profile_events = profiler->GetProfileEvents();
    for (int i = 0; i < profile_events.size(); i++) {
      auto op_index = profile_events[i]->event_metadata;
      const auto node_and_registration =
          interpreter->node_and_registration(op_index);
      const TfLiteRegistration registration = node_and_registration->second;
      PrintProfilingInfo(profile_events[i], op_index, registration);
    }
  }

  const float threshold = 0.001f;

  std::vector<std::pair<float, int>> top_results;

  int output = interpreter->outputs()[0];
  TfLiteIntArray* output_dims = interpreter->tensor(output)->dims;
  // assume output dims to be something like (1, 1, ... ,size)
  auto output_size = output_dims->data[output_dims->size - 1];
  switch (interpreter->tensor(output)->type) {
    case kTfLiteFloat32:
      get_top_n<float>(interpreter->typed_output_tensor<float>(0), output_size,
                       s->number_of_results, threshold, &top_results, true);
      break;
    case kTfLiteUInt8:
      get_top_n<uint8_t>(interpreter->typed_output_tensor<uint8_t>(0),
                         output_size, s->number_of_results, threshold,
                         &top_results, false);
      break;
    default:
      LOG(FATAL) << ""cannot handle output type ""
                 << interpreter->tensor(input)->type << "" yet"";
      exit(-1);
  }

  std::vector<string> labels;
  size_t label_count;

  if (ReadLabelsFile(s->labels_file_name, &labels, &label_count) != kTfLiteOk)
    exit(-1);

  for (const auto& result : top_results) {
    const float confidence = result.first;
    const int index = result.second;
    LOG(INFO) << confidence << "": "" << index << "" "" << labels[index] << ""\n"";
  }
#endif  
}
```

**Describe the expected behavior**

Version with GPUDelegate is a lot slower (228.445 ms per run vs 132.303 ms) than CPU.

**Other info / logs**

GPU:
```
gemini:/data/local/tmp $ ./label_image -g 1 -m deeplabv3_257_mv_gpu.tflite
nnapi error: requires android sdk version to be at least 27
Loaded model deeplabv3_257_mv_gpu.tflite
resolved reporter
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
invoked 
average time: 228.445 ms 
```
CPU:
```
gemini:/data/local/tmp $ ./label_image -m deeplabv3_257_mv_gpu.tflite                                   <
nnapi error: requires android sdk version to be at least 27
Loaded model deeplabv3_257_mv_gpu.tflite
resolved reporter
INFO: Initialized TensorFlow Lite runtime.
invoked 
average time: 132.303 ms
```


"
27612,Incomplete instructions for using images as metadata in https://www.tensorflow.org/guide/embedding,"**System information**
- Doc Link: https://www.tensorflow.org/guide/embedding


**Describe the documentation issue**

The guide says

> To use images as metadata, you must produce a single sprite image, consisting of small thumbnails, one for each vector in the embedding. 

It then goes on to explain what the sprite image should look like (but it doesn't make clear if it needs to be ""square"" where each row contains a number of thumbnails close to the square root of the total number of vectors). Is any standard image format OK? What does one do with the image file? It doesn't work to choose it in the dialog that says ""Step 2 (optional): Load a TSV file of metadata."". If one creates a config.json file how should one refer to the sprite image URL?
"
27611,E1102 non-callable raised by pylint when extending from tf.keras.layers.Layer,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): custom-ops container
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): nightly-2.0-preview
- Python version: the one with custom-ops container
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

As given in this page : https://www.tensorflow.org/tutorials/eager/custom_layers
I am trying to change the code in seq2seq as follows:
in decoder.py : `class BaseDecoder(tf.keras.layers.Layer):`

However, while running the pylint stage of sanity_checks it produces the following error:

`tensorflow_addons/seq2seq/decoder_test.py:63: [E1102(not-callable), DecodeRNNTest._testDecodeRNN] my_decoder is not callable

tensorflow_addons/seq2seq/decoder_test.py:140: [E1102(not-callable), DecodeRNNTest._testDynamicDecodeRNNWithTrainingHelperMatchesDynamicRNN] my_decoder is not callable
`

**Describe the expected behavior**

The E1102 warning should not be raised.

**Code to reproduce the issue**
To reproduce, in this pull request: https://github.com/tensorflow/addons/pull/145
Go to decoder.py and change the signature of class in line 132

The warning disappear for pylint2 in current pull request, as currently it imports layers from tensorflow.python.keras and then extends layers.Layers


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

"
27610,PackageNotFoundError: Package missing in current win-64 channels: - python 3.7*,"**System information**
- Windows 10
- TensorFlow installed from GitHub (devtools::install_github(""rstudio/tensorflow""))
- TensorFlow version: latest per 2019.04.08 (r1.13)
- Python version: 3.7
- Bazle version: 0.24.1 (bazel-0.24.1-windows-x86_64.exe)
- Installed using 'default' for windows, i.e. 'system'. The same with 'conda'

**I followed issue [20517](https://github.com/tensorflow/tensorflow/issues/20517), but I keep getting this exception in RStudio with R/3.5.0:**

```
> install_tensorflow()
Creating r-tensorflow conda environment for TensorFlow installation...
Fetching package metadata ...........

PackageNotFoundError: Package missing in current win-64 channels: 
  - python 3.7*

Error: Error 1 occurred creating conda environment r-tensorflow
```
**I get the same error when I try installing Kras:**

```
library(keras)
install_keras()

Creating r-tensorflow conda environment for TensorFlow installation...
Fetching package metadata ...........

PackageNotFoundError: Package missing in current win-64 channels: 
  - python 3.7*

Error: Error 1 occurred creating conda environment r-tensorflow

```

**I tried to install version 1.13.1, but after installation, I got this error when running 'sess = tf$Session()':**

```
Error: Installing TensorFlow requires a 64-bit version of Python 3.5 or 3.6

Please install 64-bit Python 3.5 or 3.6 to continue, supported versions include:

 - Anaconda Python (Recommended): https://www.anaconda.com/download/#windows
 - Python Software Foundation   : https://www.python.org/downloads/
```"
27609,"IOS tensorflow-lite , error: 'tensorflow/core/framework/resource_handle.h' file not found","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `MacOS High Sierra v10.13.7`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `Simulator iPhone XR`

**Describe the problem**
I am trying to run the sample IOS app, by following this guideline
https://www.tensorflow.org/lite/guide/ios
I did the following steps
- [x] Install `Xcode`
- [x] Install `brew`
- [x] Run
```
brew install automake
brew install libtool
```
- [x] git clone https://github.com/tensorflow/tensorflow
- [x] run `sudo gem install cocoapods`
- [x] run `pod install`
- [x] run `build_all_ios.sh`
- [x] run `tensorflow/lite/examples/ios/download_models.sh`

All seems ok. But as soon as, I build the app in XCode, it throws below error
<img width=""1133"" alt=""Screen Shot 2019-04-08 at 9 42 48 AM"" src=""https://user-images.githubusercontent.com/241914/55693661-bf641900-59e2-11e9-9421-aa3417053f7b.png"">

It looks similar to #9354 but here the missing file is `resource_handle.h` not `resource_handle.pb.h`

Any suggestion? Thanks
"
27608,Multidimensional Keras recurrent layers,"Tensorflow 2.0.0-alpha

currently Keras recurrent layer expect their input to have exactly 3 dimensions: (batch, sequence_length, element_size)

Input of higher dimensions will result in the following error:
> ValueError: Input 0 of layer unified_lstm is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [2, 3, 5, 4]

The wanted behavior is as follows:
let's say that I have in input of shape (2, 3, 5, 4) where every element in my batch have 3 documents: (batch, document_number, sequence_length, element_size).
I would like for the LSTM to consume only the last 2 dimensions (2*3 time)  in the same manner that tf.keras.layers.Dense consumes only the last dimension.

**This will not change the current api**
**I an willing to help (though it will be my first time committing to FT)**
**Anyone that wants to run several RNNs in parallel will benefit from this feature**"
27607,ImportError: DLL load failed: The specified module could not be found.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (Windows 10):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):anaconda
- TensorFlow version:1.13.1
- Python version:3.5
- Installed using pip
- Bazel version (if compiling from source):none
- GCC/Compiler version (if compiling from source):none
- CUDA/cuDNN version:none
- GPU model and memory:none

This is supposed to be the tensorflow cpu version

just trying a simple import script and im recieving this error

**This is my small script**

import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile

from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image

sys.path.append("".."")
from object_detection.utils import ops as utils_ops

from utils import label_map_util

from utils import visualization_utils as vis_util

**Describe the problem**
Traceback (most recent call last):
  File ""D:/tens/test.py"", line 6, in <module>
    import tensorflow as tf
  File ""C:\Users\Name\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Name\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Name\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Name\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Name\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Abdelrahman\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Process finished with exit code 1

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27606,CMake Error at tf_core_ops.cmake,"
**System information**
- OS Platform and Distribution : Windows 10 64bit
- TensorFlow installed from (source or binary): source
- TensorFlow version: v1.12.0
- Python version: python3.6 64bit
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): bazel-0.24.1-windows-x86_64.exe
- GCC/Compiler version (if compiling from source): vs2015
- CUDA/cuDNN version: cpu only



I have been trying to build a tensorflow c++ lib for a long time but never succeed.

This time, I tried to build it with cmake(gui), when I click ""generate"" some error occurred.

> CMake Error at tf_core_ops.cmake:73 (add_library):
>   Cannot find source file:
> 
>     D:/tensorflow/tensorflow/contrib/data/ops/dataset_ops.cc
> 
>   Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm
>   .hpp .hxx .in .txx
> Call Stack (most recent call first):
>   tf_core_ops.cmake:92 (GENERATE_CONTRIB_OP_LIBRARY)
>   CMakeLists.txt:512 (include)
> 
> 
> CMake Error at tf_core_kernels.cmake:221 (add_library):
>   Cannot find source file:
> 
>     D:/tensorflow/tensorflow/contrib/data/kernels/assert_next_dataset_op.cc
> 
>   Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm
>   .hpp .hxx .in .txx
> Call Stack (most recent call first):
>   CMakeLists.txt:514 (include)
> 
> 
> CMake Error at tf_core_ops.cmake:73 (add_library):
>   No SOURCES given to target: tf_contrib_data_dataset_ops
> Call Stack (most recent call first):
>   tf_core_ops.cmake:92 (GENERATE_CONTRIB_OP_LIBRARY)
>   CMakeLists.txt:512 (include)
> 
> 
> CMake Error at tf_core_kernels.cmake:221 (add_library):
>   No SOURCES given to target: tf_core_kernels
> Call Stack (most recent call first):
>   CMakeLists.txt:514 (include)
> 
> "
27604,tensorflow-gpu works with this exact config on Windows 10 (python 3.6.2 ONLY),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10-1703
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12.0
- Python version: 3.6.2
- Installed using virtualenv? pip? conda?: pip3
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA 9.0, cuDNN 7.1
- GPU model and memory: GeForce MX150/920M/740M



**Tensorflow-GPU installs perfectly without DLL Load/Runtime Errors on the exact configuration stated above. Reason of incompatibility with other dependencies (protobuf) unknown. To run tf-gpu, use this configuration: python==3.6.2, tensorflow-gpu==1.12.0, protobuf==3.5 or later, cuda==9.0, cudnn==7.1 or later. I think it has more to do with python 3.6.2. It supports.**



"
27603,GPU Installation Issue,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, i5-750 only SSE4.2, without AVX.
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip install tf-nightly-gpu-2.0-preview
- TensorFlow version: TensorFlow 2.0
- Python version: conda create --name tensorflow-2.0 python=3.6
- Installed using virtualenv? pip? conda?: pip install tf-nightly-gpu-2.0-preview
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda10, cuDNN 7.3.1,
- GPU model and memory: GTX1060 6GB

Do you know how to make tensorflow-2.0 work on nvidia gpu? I did the following without any error:
conda create --name tensorflow-2.0 python=3.6
activate tensorflow-2.0
pip install tf-nightly-gpu-2.0-preview
conda install -c anaconda cudatoolkit
conda install -c anaconda cudnn

But then:
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\Frust\Anaconda3\envs\tensorflow-2.0\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Frust\Anaconda3\envs\tensorflow-2.0\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Frust\Anaconda3\envs\tensorflow-2.0\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Frust\Anaconda3\envs\tensorflow-2.0\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Frust\Anaconda3\envs\tensorflow-2.0\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError:         (DLL).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Frust\Anaconda3\envs\tensorflow-2.0\lib\site-packages\tensorflow\__init__.py"", line 27, in <module>
    from tensorflow._api.v2 import audio
  File ""C:\Users\Frust\Anaconda3\envs\tensorflow-2.0\lib\site-packages\tensorflow\_api\v2\audio\__init__.py"", line 8, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\Frust\Anaconda3\envs\tensorflow-2.0\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Frust\Anaconda3\envs\tensorflow-2.0\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Frust\Anaconda3\envs\tensorflow-2.0\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Frust\Anaconda3\envs\tensorflow-2.0\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Frust\Anaconda3\envs\tensorflow-2.0\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Frust\Anaconda3\envs\tensorflow-2.0\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Frust\Anaconda3\envs\tensorflow-2.0\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError:         (DLL).


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
27602,tf_upgrade_v2 fails on google colab and latest jupyter notebook,"in all fairness tensorflow 2 is in alpha... but the docs indicate the tf1 > tf2 script should work:
https://www.tensorflow.org/alpha/guide/upgrade

**System information**
using the code from https://www.tensorflow.org/alpha/guide/upgrade
I get syntax errors on google colab and on jupyter notebook

- TensorFlow installed from (source or binary):
!pip install tensorflow==2.0.0-alpha0 

- TensorFlow version (use command below):
mesh-tensorflow          0.0.5                
tensorflow               2.0.0a0              
tensorflow-estimator     1.13.0               
tensorflow-hub           0.4.0                
tensorflow-metadata      0.13.0               
tensorflow-probability   0.6.0  

- Python version:
google-api-python-client 1.6.7                
ipython                  5.5.0                
ipython-genutils         0.2.0                
ipython-sql              0.3.9                
opencv-contrib-python    3.4.3.18             
opencv-python            3.4.5.20             
python-apt               1.6.3+ubuntu1        
python-chess             0.23.11              
python-dateutil          2.5.3                
python-louvain           0.13                 
python-rtmidi            1.2.1                
python-slugify           3.0.2                
python-utils             2.3.0 



**Describe the current behavior**
tested on jupyter notebook 
jupyter --version
4.4.0
and google colab


**Describe the expected behavior**
tf_upgrade_v2  should work in in jupyter and google colab
**Code to reproduce the issue**
try the code yourself here:
https://colab.research.google.com/drive/1pVvIgkjGeWNyGWdy4A4gfEFQRdLJO8_G


**Other info / logs**
here is the source code
https://colab.research.google.com/drive/1pVvIgkjGeWNyGWdy4A4gfEFQRdLJO8_G


Here is the video showing the results:
https://youtu.be/u15oD3c_xHk"
27601,tensorflow/contrib/android build feild nsync.a: malformed archive header name at 8,"
"
27598,RaggedTensor casting bug,"version 2.0.0-alpha.
Nested RaggedTensor are cast to int64 without apparent reason:

with regular tensors (ok):
```
>>> tf.constant([[1]], dtype=tf.int8)
<tf.Tensor: id=98, shape=(1, 1), dtype=int8, numpy=array([[1]], dtype=int8)>
```
with nested RaggedTensor (not ok):
```
>>> tf.ragged.constant([[1]], dtype=tf.int8)
tf.RaggedTensor(values=tf.Tensor([1], shape=(1,), dtype=int8), row_splits=tf.Tensor([0 1], shape=(2,), dtype=int64))
```

Also they can not be used to create generators even with dtype=int64. The following code leads to:

> The expected type was int64, but the yielded element was <tf.RaggedTensor [[6]]>.


```

class LineGenerator(object):
  def get_next_line(self):
    while True:
      out = [[6]]
      yield tf.ragged.constant(out, dtype=tf.int64)

class Dataset(object):
  def __init__(self, generator=LineGenerator()):
    self.next_element = self.build_iterator(generator)

  def build_iterator(self, gen: LineGenerator):
    dataset = tf.data.Dataset.from_generator(gen.get_next_line,output_types = tf.int64)
    #some other code...
```"
27597,RuntimeError: Attempted to use a closed Session. with flask,"I'm trying to run a Tacotron-2 training in flask, get parameters in json format. but the following error arises when I execute the training:

```
ARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.
Using TensorFlow backend.
 * Serving Flask app ""server"" (lazy loading)
 * Environment: production
   WARNING: Do not use the development server in a production environment.
   Use a production WSGI server instead.
 * Debug mode: on
 * Running on http://0.0.0.0:8891/ (Press CTRL+C to quit)
 * Restarting with stat
/usr/local/lib/python3.6/dist-packages/numba/errors.py:105: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9
  warnings.warn(msg)
WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.
Using TensorFlow backend.
 * Debugger is active!
 * Debugger PIN: 303-138-053
Checkpoint path: logs-Tacotron-2/taco_pretrained/tacotron_model.ckpt
Loading training data from: tacotron-files/servex-tacotron/training_data/train.txt
Using model: Tacotron
Hyperparameters:
  GL_on_GPU: True
  NN_init: True
  NN_scaler: 0.3
  allow_clipping_in_normalization: True
  attention_dim: 128
  attention_filters: 32
  attention_kernel: (31,)
  attention_win_size: 7
  batch_norm_position: after
  cbhg_conv_channels: 128
  cbhg_highway_units: 128
  cbhg_highwaynet_layers: 4
etc...
Tacotron Parameters       29.039 Million.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
initialisation done /gpu:0
Initialized Tacotron model. Dimensions (? = dynamic shape): 
  Train mode:               False
  Eval mode:                True
  GTA mode:                 False
  Synthesis mode:           False
  Input:                    (?, ?)
  device:                   0
  embedding:                (?, ?, 512)
  enc conv out:             (?, ?, 512)
  encoder out:              (?, ?, 512)
  decoder out:              (?, ?, 80)
  residual out:             (?, ?, 512)
  projected residual out:   (?, ?, 80)
  mel out:                  (?, ?, 80)
  linear out:               (?, ?, 1025)
  <stop_token> out:         (?, ?)
  Tacotron Parameters       29.039 Million.
Tacotron training set to a maximum of 200000 steps
holaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
Loading checkpoint logs-Tacotron-2/taco_pretrained/tacotron_model.ckpt-0
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Exiting due to exception: '<' not supported between instances of 'int' and 'str'
Traceback (most recent call last):
  File ""/home/manuel_garcia02/Tacotron/tacotron/train.py"", line 224, in train
    while not coord.should_stop() and step < args.tacotron_train_steps:
TypeError: '<' not supported between instances of 'int' and 'str'
186.179.100.228 - - [07/Apr/2019 02:52:15] ""POST /tts/train HTTP/1.1"" 200 -
Generated 15 test batches of size 32 in 0.891 sec
Generated 64 train batches of size 32 in 2.169 sec
Exception in thread background:
Traceback (most recent call last):
  File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/manuel_garcia02/Tacotron/tacotron/feeder.py"", line 169, in _enqueue_next_train_group
    self._session.run(self._enqueue_op, feed_dict=feed_dict)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1075, in _run
    raise RuntimeError('Attempted to use a closed Session.')



```
Why can it be produced?
---

### Expected Behavior

I should start training normally
If I execute the training without flask, this is executed normally

### Environment

* Python version: 3.6
* Flask version: 1.0.2
* Tensorflow version: 1.13.1"
27593,ModuleNotFoundError: No module named 'tensorflow.python',"**System information**
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: tried ""conda install tensorflow"", ""pip install tensorflow"", and ""pip install https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.13.1-cp36-cp36m-win_amd64.whl"" 
- **TensorFlow version**: latest
- **Python version**: 3.6
- **Installed using virtualenv? pip? conda?**: tried pip and conda

I am trying to run Tensorflow in an anaconda 3.6 environment. I've tried installing tensorflow using pip, conda, and pip with the link to the wheel. No matter the installation method when I try to import tensorflow to test the installation I receive the error:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\""USERNAME""\AppData\Roaming\Python\Python36\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
ModuleNotFoundError: No module named 'tensorflow.python'

I've looked through the C:\Users\""USERNAME""\AppData\Roaming\Python\Python36\site-packages\tensorflow directory and cannot find any files named ""tensorflow.python"" nor can I find the ""pywrap_tensorflow"" file. 

After searching the issues tab I found thread #22300 with a similar problem. The admins suggested the user was missing a dll that contains the tensorflow.python file, but there was never a resolution. The user in thread #22300 is on linux while I am on windows 10. Is there a way to download the missing dlls or any other debugging steps I can take?"
27584,No simple function optimisation examples for tensorflow 2.0?,"I don't see any simple function optimisation example for tensorflow 2.0.

Something like log(x) ** 2 could be good."
27583,"Doc keras/Model for ""predict"" function the list of argument is not display properly (fullblock instead of list)","
**System information**
- TensorFlow version: 2.0
- Doc Link:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model

**Describe the documentation issue**

In the documentation keras/Model for ""predict"" function the list of argument is not display properly (fullblock instead of list) on both Chrome and Firefox

<img width=""910"" alt=""Screenshot 2019-04-06 at 20 59 00"" src=""https://user-images.githubusercontent.com/12021701/55674016-0cdf6980-58af-11e9-995f-fc569131a741.png"">

"
27582,Error during testing of Object Detection API,"**System information**
- OS Platform and Distribution :Windows 10
- TensorFlow installed from (source or binary):cmd
- TensorFlow version:1.13.1
- Python version:3.7
- Installed using virtualenv? pip? conda?:pip
- CUDA/cuDNN version: don't know
- GPU model and memory: 1.13.1



**Describe the problem**

python object_detection/builders/model_builder_test.py



**Any other info / logs**
Traceback (most recent call last):
  File ""C:\Users\Abhitm\mawa\abc\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Abhitm\mawa\abc\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Abhitm\mawa\abc\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Abhitm\mawa\abc\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Abhitm\mawa\abc\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""object_detection/builders/model_builder_test.py"", line 20, in <module>
    import tensorflow as tf
  File ""C:\Users\Abhitm\mawa\abc\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Abhitm\mawa\abc\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Abhitm\mawa\abc\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Abhitm\mawa\abc\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Abhitm\mawa\abc\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Abhitm\mawa\abc\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Abhitm\mawa\abc\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Abhitm\mawa\abc\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.
"
27581,keras model.predict_on_batch/model.test_on_batch with tf.dataset: dataset.make_initializable_iterator is not supported when eager execution is enabled.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): yes 
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

best_model.test_on_batch(testing_dataset)
and 
best_model.predict_on_batch(training_dataset)

crashing with the following error messages
""RuntimeError: dataset.make_initializable_iterator is not supported when eager execution is enabled.""

but best_model.evaluate(testing_dataset,) is wokring.

**Describe the expected behavior**
Exact same code working with Tensorflow 1.12

**Code to reproduce the issue**
All the code and llogs are here:
https://github.com/tarrade/proj_DL_models_and_pipelines_with_GCP/blob/master/notebook/TF_2.0/05-Mnist_model_validation_and_interpretation.ipynb

**Other info / logs**
Here the full logs:

---------------------------------------------------------------------------
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in make_initializable_iterator(dataset, shared_name)
   1852     # some datasets (e.g. for prefetching) override its behavior.
-> 1853     return dataset._make_initializable_iterator(shared_name)  # pylint: disable=protected-access
   1854   except AttributeError:

AttributeError: 'PrefetchDataset' object has no attribute '_make_initializable_iterator'

During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
<ipython-input-21-3d53d52be0f8> in <module>
      1 #!! Bug TF 2.0
----> 2 score = best_model.test_on_batch(testing_dataset)
      3 
      4 # print test accuracy
      5 print('Loss:')

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in test_on_batch(self, x, y, sample_weight, reset_metrics)
   1310     # Validate and standardize user data.
   1311     x, y, sample_weights = self._standardize_user_data(
-> 1312         x, y, sample_weight=sample_weight, extract_tensors_from_dataset=True)
   1313 
   1314     if self.run_eagerly:

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)
   2439       if extract_tensors_from_dataset:
   2440         # We do this for `train_on_batch`/etc.
-> 2441         x, y, sample_weight = training_utils.extract_tensors_from_dataset(x)
   2442     elif isinstance(x, iterator_ops.Iterator):
   2443       # Graph mode iterator. We extract the symbolic tensors.

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in extract_tensors_from_dataset(dataset)
   1382     Tuple of tensors `x, y, weights`. `y` and `weights` entry may be None.
   1383   """"""
-> 1384   iterator = get_iterator(dataset)
   1385   inputs, targets, sample_weight = unpack_iterator_input(iterator)
   1386   return inputs, targets, sample_weight

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in get_iterator(dataset)
   1362 def get_iterator(dataset):
   1363   """"""Create and initialize an iterator from a dataset.""""""
-> 1364   iterator = dataset_ops.make_initializable_iterator(dataset)
   1365   initialize_iterator(iterator)
   1366   return iterator

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in make_initializable_iterator(dataset, shared_name)
   1853     return dataset._make_initializable_iterator(shared_name)  # pylint: disable=protected-access
   1854   except AttributeError:
-> 1855     return DatasetV1Adapter(dataset)._make_initializable_iterator(shared_name)  # pylint: disable=protected-access
   1856 
   1857 

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in _make_initializable_iterator(self, shared_name)
   1495     if context.executing_eagerly():
   1496       raise RuntimeError(
-> 1497           ""dataset.make_initializable_iterator is not supported when eager ""
   1498           ""execution is enabled."")
   1499     _ensure_same_dataset_graph(self)

RuntimeError: dataset.make_initializable_iterator is not supported when eager execution is enabled."
27577,TensorFlow 2.0 Alpha Tensorboard graph bug,"**System information**
- Have I written custom code: No, I'm using MNIST Tensorboard example provided by Google.
- OS Platform and Distribution: Ubuntu 16.04
- Binary installation (sudo pip3 install tf-nightly-2.0-preview)
- TensorFlow version (use command below): 2.0.0-dev20190405
- Python version: 3.5.2
- CUDA/cuDNN version: 10.0
- GPU model and memory: NVIDIA GTX 1080 Ti 11GB

**Describe the current behavior**
My code is provided below.

Running this code gives a directory called logs. I launch Tensorboard using **tensorboard --logdir=./logs**
I go to localhost:6006, click the Graphs tab in my browser, and I get the message saying:
**Graph: Failed Normalizing names**
Tensorboard console doesn't show any errors.

I'm using the MNIST Tensorboard example provided here: https://www.tensorflow.org/tensorboard/r2/graphs
And it doesn't work in TensorFlow 2.0. Why?

**Describe the expected behavior**
I expect it to show the graph for the model.

**Code to reproduce the issue**
```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from datetime import datetime
#from packaging import version

import tensorflow as tf
from tensorflow import keras

print(""TensorFlow version: "", tf.__version__)

# Define the model.
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy'])
(train_images, train_labels), _ = keras.datasets.fashion_mnist.load_data()
train_images = train_images / 255.0

logdir=""logs/fit/"" + datetime.now().strftime(""%Y%m%d-%H%M%S"")
tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)

# Train the model.
model.fit(
    train_images,
    train_labels, 
    batch_size=64,
    epochs=5, 
    callbacks=[tensorboard_callback])
```

Attached file is zipped logs directory to be viewed with Tensorboard.
[logs.zip](https://github.com/tensorflow/tensorflow/files/3050778/logs.zip)
"
27576,nvcc error   : 'cudafe++' died with status 0xC0000005 (ACCESS_VIOLATION),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 1809 17763.316
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0.0-alpha0
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.23.0
- GCC/Compiler version (if compiling from source): Visual Studio 2015 Build tools update 3
- CUDA/cuDNN version: CUDA 10.0 / cuDNN 7.5
- GPU model and memory: GTX980 4GB



**Describe the problem**
Building from tag 2.0.0-alpha0 results in `nvcc error   : 'cudafe++' died with status 0xC0000005 (ACCESS_VIOLATION)` being thrown on 2 files, repeatably:
 - tensorflow/core/kernels/bincount_op_gpu.cu.cc
 - tensorflow/core/kernels/multinomial_op_gpu.cu.cc
 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
C:\projects\tensorflow [(v2.0.0-alpha0)]> ./configure
WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.23.0 installed.
Please specify the location of python. [Default is C:\Python37\python.exe]:


Found possible Python library paths:
  C:\Python37\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Python37\lib\site-packages]

Do you wish to build TensorFlow with XLA JIT support? [y/N]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.0]: 10.0


Please specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7.5


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 5.2


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]: /arch:AVX2


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y
Eigen strong inline overridden.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apache Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.

C:\projects\tensorflow [(v2.0.0-alpha0)]> bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
ERROR: C:/projects/tensorflow/tensorflow/core/kernels/BUILD:3963:1: C++ compilation of rule '//tensorflow/core/kernels:bincount_op_gpu' failed (Exit 5): python.exe failed: error executing command
  cd C:/users/damlo/_bazel_damlo/fttvoi5m/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\VCPackages;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\WINDOWS\Microsoft.NET\Framework64\;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Python37/python.exe
    SET PYTHON_LIB_PATH=C:/Python37/lib/site-packages
    SET TEMP=C:\Users\damlo\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=5.2
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\damlo\AppData\Local\Temp
  C:/Python37/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/cub_archive /Ibazel-out/x64_windows-opt/genfiles/external/cub_archive /Ibazel-out/x64_windows-opt/bin/external/cub_archive /Ibazel-out/x64_windows-opt/bin/external/cub_archive/_virtual_includes/cub /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include/crt /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN /arch:AVX2 -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/bincount_op_gpu/bincount_op_gpu.cu.o /c tensorflow/core/kernels/bincount_op_gpu.cu.cc
Execution platform: @bazel_tools//platforms:host_platform
external/com_google_absl\absl/strings/string_view.h(496): warning: expression has no effect

c:\users\damlo\_bazel_damlo\fttvoi5m\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/arch/GPU/PacketMathHalf.h(149): warning: missing return statement at end of non-void function ""Eigen::internal::ptrue(const Packet &) [with Packet=half2]""

c:\users\damlo\_bazel_damlo\fttvoi5m\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/products/Parallelizer.h(24): warning: variable ""m_maxThreads"" was set but never used

external/protobuf_archive/src\google/protobuf/arena_impl.h(55): warning: integer conversion resulted in a change of sign

external/protobuf_archive/src\google/protobuf/arena_impl.h(309): warning: integer conversion resulted in a change of sign

external/protobuf_archive/src\google/protobuf/arena_impl.h(310): warning: integer conversion resulted in a change of sign

external/protobuf_archive/src\google/protobuf/map.h(1025): warning: invalid friend declaration

.\tensorflow/core/lib/gtl/flatmap.h(157): warning: invalid friend declaration

external/com_google_absl\absl/strings/string_view.h(496): warning: expression has no effect

external/protobuf_archive/src\google/protobuf/arena_impl.h(55): warning: integer conversion resulted in a change of sign

external/protobuf_archive/src\google/protobuf/arena_impl.h(309): warning: integer conversion resulted in a change of sign

external/protobuf_archive/src\google/protobuf/arena_impl.h(310): warning: integer conversion resulted in a change of sign

external/protobuf_archive/src\google/protobuf/map.h(1025): warning: invalid friend declaration

.\tensorflow/core/lib/gtl/flatmap.h(157): warning: invalid friend declaration

nvcc error   : 'cudafe++' died with status 0xC0000005 (ACCESS_VIOLATION)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 359.982s, Critical Path: 193.33s
INFO: 253 processes: 253 local.
FAILED: Build did NOT complete successfully
```
"
27575,State build requires bazel <=0.23.0 for v2.0.0-alpha0,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: v2.0.0-alpha0
- Doc Link: https://www.tensorflow.org/install/source_windows#install_bazel


**Describe the documentation issue**
I downloaded the latest (at the time of writing, 0.24.1) Bazel binary and when I tried to `./configure.py` I got this: 
```
You have bazel 0.24.1 installed.
Please downgrade your bazel installation to version 0.23.0 or lower to build TensorFlow! To downgrade: download the installer for the old version (from https://github.com/bazelbuild/bazel/releases) then run the installer.
```
I think it would be useful to state explicitly that 0.23.0 is the version required to build Tensorflow to save some time. I even just downloaded 0.23.2 because I didn't see that the message says 0.23.0 and sure enough, I got the same complaint about 0.23.2.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**

Yes. Will do a PR shortly.
"
27568,TF2 Keras rms_optimizer.get_updates error,"Hello, trying to migrate from standalone Keras and TF 1.12 to TF2.Keras. 
I used the tf2 conversion script and, then changed import keras.xx to import **tensorflow**.keras.xx. Yet, still getting error: `TypeError: get_updates() takes 3 positional arguments but 4 were given. `

in the line:
`updates = self.rms_optimizer.get_updates(self.model.trainable_weights, [], loss)`

It works in the standalone Keras and tf 1.12. Any idea what else I need to change. or is it a bug in tf2.
using tf-nightly-2.0-preview           2.0.0.dev20190404 in windows 10/Python 3.6.6"
27565,[TF==2.0.0a0] @tf.function raises ValueError when computing gradients,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow version (use command below): pip install tensorflow(-gpu)==2.0.0a0
- Python version: 3.6

**Describe the current behavior**
The code executes normally, but raise ValueError when computing gradients (`tape.gradient)` if I decorate the training function with `@tf.function`. The traceback is as follows:
<pre>
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/Workspaces/fgenl/run.py in <module>()
     80     for batch_id in range(num_batches_each_epoch):
     81         batch_data = data_generator.get_data() # v2
---> 82         loss, outputs = train_one_step(batch_data) # v2
     83         # _, loss, outputs, inputs = sess.run([opt_op, loss_, outputs_, batch_data])
     84         if loss_metrics is None:

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    424     # This is the first call of __call__, so we have to initialize.
    425     initializer_map = {}
--> 426     self._initialize(args, kwds, add_initializers_to=initializer_map)
    427     if self._created_variables:
    428       try:

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    368     self._concrete_stateful_fn = (
    369         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 370             *args, **kwds))
    371
    372     def invalid_creator_scope(*unused_args, **unused_kwds):

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   1311     if self._input_signature:
   1312       args, kwargs = None, None
-> 1313     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   1314     return graph_function
   1315

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   1578           or call_context_key not in self._function_cache.missed):
   1579         self._function_cache.missed.add(call_context_key)
-> 1580         graph_function = self._create_graph_function(args, kwargs)
   1581         self._function_cache.primary[cache_key] = graph_function
   1582         return graph_function, args, kwargs

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   1510             arg_names=arg_names,
   1511             override_flat_arg_shapes=override_flat_arg_shapes,
-> 1512             capture_by_value=self._capture_by_value),
   1513         self._function_attributes)
   1514

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    692                                           converted_func)
    693
--> 694       func_outputs = python_func(*func_args, **func_kwargs)
    695
    696       # invariant: `func_outputs` contains only Tensors, IndexedSlices,

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    315         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    316         # the function a weak reference to itself to avoid a reference cycle.
--> 317         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    318     weak_wrapped_fn = weakref.ref(wrapped_fn)
    319

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    684                   optional_features=autograph_options,
    685                   force_conversion=True,
--> 686               ), args, kwargs)
    687
    688         # Wrapping around a decorator allows checks like tf_inspect.getargspec

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)
    390     return _call_unconverted(f, args, kwargs)
    391
--> 392   result = converted_f(*effective_args, **kwargs)
    393
    394   # The converted function's closure is simply inserted into the function's

/tmp/tmpx0xgcbu3.py in tf__train_one_step(batch_data)
      6     outputs = ag__.converted_call(model, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (batch_data,), {})
      7     loss, info = ag__.converted_call('calculate_loss', loss_object, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_1, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (outputs, batch_data), {})
----> 8   gradients = ag__.converted_call('gradient', tape, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_2, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (loss, model.trainable_variables), {})
      9   update_list = [(grad, var) for grad, var in ag__.converted_call(zip, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_3, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (gradients, model.trainable_variables), {}) if grad is not None]
     10   ag__.converted_call('apply_gradients', optimizer, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_4, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (update_list,), {})

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)
    265
    266   if not options.force_conversion and conversion.is_whitelisted_for_graph(f):
--> 267     return _call_unconverted(f, args, kwargs)
    268
    269   # internal_convert_user_code is for example turned off when issuing a dynamic

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs)
    186     return f.__self__.call(args, kwargs)
    187
--> 188   return f(*args, **kwargs)
    189
    190

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)
    954         flat_sources,
    955         output_gradients=output_gradients,
--> 956         unconnected_gradients=unconnected_gradients)
    957
    958     if not self._persistent:

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, unconnected_gradients)
     70       sources,
     71       output_gradients,
---> 72       compat.as_str(unconnected_gradients.value))

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in _aggregate_grads(gradients)
    565         indexed_slices = ops.IndexedSlices(
    566             grad,
--> 567             math_ops.range(grad.shape[0]),
    568             constant_op.constant(grad.shape.as_list()))
    569         indexed_slices_list.append(indexed_slices)

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py in range(start, limit, delta, dtype, name)
   1258   with ops.name_scope(name, ""Range"", [start, limit, delta]) as name:
   1259     start = ops.convert_to_tensor(start, dtype=dtype, name=""start"")
-> 1260     limit = ops.convert_to_tensor(limit, dtype=dtype, name=""limit"")
   1261     delta = ops.convert_to_tensor(delta, dtype=dtype, name=""delta"")
   1262

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)
   1048   preferred_dtype = deprecation.deprecated_argument_lookup(
   1049       ""dtype_hint"", dtype_hint, ""preferred_dtype"", preferred_dtype)
-> 1050   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
   1051
   1052

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)
   1106       name=name,
   1107       preferred_dtype=dtype_hint,
-> 1108       as_ref=False)
   1109
   1110

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)
   1184
   1185     if ret is None:
-> 1186       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1187
   1188     if ret is NotImplemented:

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    302                                          as_ref=False):
    303   _ = as_ref
--> 304   return constant(v, dtype=dtype, name=name)
    305
    306

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
    243   """"""
    244   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--> 245                         allow_broadcast=True)
    246
    247

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    281       tensor_util.make_tensor_proto(
    282           value, dtype=dtype, shape=shape, verify_shape=verify_shape,
--> 283           allow_broadcast=allow_broadcast))
    284   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
    285   const_tensor = g.create_op(

~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
    453   else:
    454     if values is None:
--> 455       raise ValueError(""None values not supported."")
    456     # if dtype is provided, forces numpy array to be the type
    457     # provided if possible.

ValueError: None values not supported.
</pre>

**Describe the expected behavior**
The code should also execute normally when using `@tf.function`.

**Code to reproduce the issue**
~~Sorry, I do not have a simple snippet to reproduce this issue. But could you find something in the traceback?~~ See below please.
"
27562,tf.estimator.train_and_evaluate does not show anything when running multi-worker example in TensorFlow 2.0 Alpha,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): TF 2.0 Alpha CPU only 
- Python version: 2.7

I have run this example in TF 2.0, however the estimator train and evaluate does not show anything as provided in this link: https://www.tensorflow.org/alpha/tutorials/distribute/multi_worker

```
from __future__ import absolute_import, division, print_function

import tensorflow_datasets as tfds
import tensorflow as tf

import os, json

BUFFER_SIZE = 10000
BATCH_SIZE = 64

def input_fn(mode, input_context=None):
  datasets, ds_info = tfds.load(name='mnist',
                                with_info=True,
                                as_supervised=True)
  mnist_dataset = (datasets['train'] if mode == tf.estimator.ModeKeys.TRAIN else
                   datasets['test'])

  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label

  if input_context:
    mnist_dataset = mnist_dataset.apply(tf.data.experimental.filter_for_shard(
        input_context.num_input_pipelines, input_context.input_pipeline_id))
  return mnist_dataset.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

NUM_WORKERS = 1
IP_ADDRS = ['localhost']
PORTS = [12345]

os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': ['%s:%d' % (IP_ADDRS[w], PORTS[w]) for w in range(NUM_WORKERS)]
    },
    'task': {'type': 'worker', 'index': 0}
})

LEARNING_RATE = 1e-4
def model_fn(features, labels, mode):
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')
  ])
  logits = model(features, training=False)
  
  if mode == tf.estimator.ModeKeys.PREDICT:
    predictions = {'logits': logits}
    return tf.estimator.EstimatorSpec(labels=labels, predictions=predictions)
  
  optimizer = tf.compat.v1.train.GradientDescentOptimizer(
      learning_rate=LEARNING_RATE)
  loss = tf.keras.losses.SparseCategoricalCrossentropy(
      from_logits=True)(labels, logits)
  if mode == tf.estimator.ModeKeys.EVAL:
    return tf.estimator.EstimatorSpec(mode, loss=loss)
  
  return tf.estimator.EstimatorSpec(
      mode=mode,
      loss=loss,
      train_op=optimizer.minimize(
          loss, tf.compat.v1.train.get_or_create_global_step()))

strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

config = tf.estimator.RunConfig(train_distribute=strategy, eval_distribute=strategy)

classifier = tf.estimator.Estimator(
    model_fn=model_fn, model_dir='/tmp/multiworker', config=config)
tf.estimator.train_and_evaluate(
    classifier,
    train_spec=tf.estimator.TrainSpec(input_fn=input_fn),
    eval_spec=tf.estimator.EvalSpec(input_fn=input_fn)
)
```
The result in the terminal is like this:

```
2019-04-06 19:26:08.807447: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-04-06 19:26:08.825158: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2693670000 Hz
2019-04-06 19:26:08.825963: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x5944140 executing computations on platform Host. Devices:
2019-04-06 19:26:08.825997: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>
WARNING: Logging before flag parsing goes to stderr.
W0406 19:26:08.827748 47224889664000 cross_device_ops.py:1106] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.
2019-04-06 19:26:08.830148: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12345}
2019-04-06 19:26:08.831611: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:359] Started server with target: grpc://localhost:12345
W0406 19:26:08.841363 47224889664000 cross_device_ops.py:1106] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.
W0406 19:26:08.843209 47224889664000 cross_device_ops.py:1106] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.
W0406 19:26:09.397347 47224889664000 monitored_session.py:344] Collective ops may deadlock with `save_checkpoints_secs` please use `save_checkpoint_steps` instead. Clearing `save_checkpoint_secs` and setting `save_checkpoint_steps` to 1000 now.
2019-04-06 19:26:09.526794: I tensorflow/core/distributed_runtime/master_session.cc:1194] Start master session fdddc32e0a2bc48c with config: device_filters: ""/job:worker/task:0"" device_filters: ""/job:worker/task:0"" device_filters: ""/job:worker/task:0"" allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE scoped_allocator_optimization: ON scoped_allocator_opts { enable_op: ""CollectiveReduce"" enable_op: ""CollectiveReduce"" enable_op: ""CollectiveReduce"" } } } experimental { collective_group_leader: ""/job:worker/replica:0/task:0"" }
2019-04-06 19:26:10.319007: W ./tensorflow/core/framework/model.h:202] Encountered a stop event that was not preceded by a start event.
2019-04-06 19:26:23.553348: W tensorflow/core/common_runtime/eager/context.cc:195] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.
```

Is this bug? or how to solve this problem?


"
27560,Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model,"Hi,
I am training a model for image classification in TensorFlow using pre-trained model of MobileNet and embedding the .tflite file in Android app. 

It so happens that if I do `model.add(layers.Flatten())` after adding the MobileNet model, I get the following error, when executing the Android app:
`Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model`

However, if I use `model.add(layers.GlobalAveragePooling2D())` after the MobileNet model, the app works normally. 

Could someone please help me as to why the `layers.Flatten()` is causing trouble?

Complete code is available [here](https://github.com/PikkaPikkachu/train-em-all/). 
Thanks!
"
27559,Poincare hyperbolic embeddings support,"As you might have seen Facebook released hyperbolic embeddings training; 
Is there any plan to support those in Tensorflow?

https://github.com/facebookresearch/poincare-embeddings"
27557,tf.keras lambda layer with sparse tensor caused AttributeError: 'SparseTensor' object has no attribute 'tocoo',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OSX 10.14
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): 1.13
- Python version: 3.6

**Describe the current behavior**
sparse tensor operation inside a custom keras layer should not affect outside behavior if returning the expected type

**Describe the expected behavior**
`AttributeError: 'SparseTensor' object has no attribute 'tocoo'`

**Code to reproduce the issue**
```python

import tensorflow as tf
import scipy.sparse
import numpy as np

def input_fn():

    x = scipy.sparse.random(1, 400)
    y = scipy.random.randint(2, size=(1,1))

    indices = np.mat([x.row, x.col]).transpose()
    sp = tf.sparse.SparseTensor(indices, x.data, x.shape)
    d = tf.data.Dataset.from_tensors((sp,y))
    return d

input_layer = tf.keras.layers.Input(shape=(400, ), sparse=True)
weights = tf.get_variable(name='weights', shape=(400, 1))

weights_mult = lambda x: tf.sparse_tensor_dense_matmul(x, weights)
output_layer=  tf.keras.layers.Lambda(weights_mult)(input_layer)
model = tf.keras.Model([input_layer], output_layer)
model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])
d = input_fn()
model.fit(d.make_one_shot_iterator(), epochs=3, steps_per_epoch=1)
```

**Other info / logs**
```
Epoch 1/3
Traceback (most recent call last):
  File ""sparse.py"", line 24, in <module>
    model.fit(d.make_one_shot_iterator(), epochs=3, steps_per_epoch=1)
  File ""/Users/jz/coding/t1/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 880, in fit
    validation_steps=validation_steps)
  File ""/Users/jz/coding/t1/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 266, in model_iteration
    batch_outs = f(actual_inputs)
  File ""/Users/jz/coding/t1/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 3046, in __call__
    sparse_coo = value.tocoo()
AttributeError: 'SparseTensor' object has no attribute 'tocoo'
```"
27556,Wheel and libtensorflow archives contain duplicated .so files instead of symlinks,"Although https://github.com/tensorflow/tensorflow/pull/27493 resolves our CI failures, there's still a critical issue to address that **must be fixed before April 15**: we couldn't figure out how to get symlinks working properly in Bazel-generated archives, so the extra `tensorflow_framework.so.*.*` files are all duplicates. The pip wheel is now ~120MB:

```
... ... 120M Apr  5 16:05 tensorflow-1.13.1-cp27-cp27mu-linux_x86_64.whl
```
https://github.com/pypa/packaging-problems/issues/101 seems to be the most recent documentation explaining TF's maximum wheel size limit, which is currently 100MB. We won't be able to upload the 1.14 release until this is addressed. TF is [just scraping the limit at 92MB](https://pypi.org/project/tensorflow/#files), so all extra files will have to be symlinks to fix this properly.

The libtensorflow archives are affected by this as well.

FYI @gunan, assigning to myself and @perfinion."
27552,GcsFilesystem not able to set dns cache when using long constructor,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0.0-alpla0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

The GcsFileSystem has a default constructor and a long constructor. While the default constructor initialize variables either to default option or from environment variables, the long constructor initialize a few variables that's passed in from constructor parameters, but left out the others (e.g. dns cache).

**Will this change the current api? How?**

No. We can still initialize the other things from environment variables.

**Who will benefit with this feature?**

anyone who uses GcsFileSystem can have more options configuring the filesystem while using the long constructor.

**Any Other info.**
"
27549,CUBLAS_STATUS_NOT_INITIALIZED,"Hi I am trying to run ResNet50 (v1.5) with data parallelism using horovod. When I run the model on one GPU it works. However, when I try to train the model on multiple GPUs, I get crazy errors, as follows: 

2019-04-05 18:59:27.749865: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 28.82G (30950542336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.754252: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 25.94G (27855489024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.772229: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 23.35G (25069938688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.779624: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 21.01G (22562945024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.783168: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 18.91G (20306649088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.787585: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 17.02G (18275983360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.791698: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 15.32G (16448385024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.795240: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 13.79G (14803546112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.798853: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 12.41G (13323191296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.802389: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 11.17G (11990872064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.805892: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 10.05G (10791784448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.809407: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 9.04G (9712606208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.812910: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 8.14G (8741345280 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.816456: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 7.33G (7867210752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.819975: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 6.59G (7080489472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.823475: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 5.93G (6372440576 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.826972: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 5.34G (5735196160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.830468: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 4.81G (5161676288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.834110: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 4.33G (4645508608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.837658: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 3.89G (4180957696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.841171: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 3.50G (3762861824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.844700: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 3.15G (3386575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.848335: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.84G (3047918080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.851924: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.55G (2743126272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.859055: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.07G (2221932032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory                                                  [257/1934]
2019-04-05 18:59:27.862563: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 1.86G (1999738880 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-04-05 18:59:27.866054: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 1.68G (1799764992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0405 18:59:28.200468 140411113244416 tf_logging.py:115] Running local_init_op.
I0405 18:59:28.241837 140411113244416 tf_logging.py:115] Done running local_init_op.
I0405 18:59:28.467390 139802759370496 tf_logging.py:115] Running local_init_op.
I0405 18:59:28.531050 139802759370496 tf_logging.py:115] Done running local_init_op.
I0405 18:59:30.066596 140411113244416 tf_logging.py:115] Saving checkpoints for 0 into /tmp/cifar10_model/model.ckpt.
I0405 18:59:30.281409 139802759370496 tf_logging.py:115] Saving checkpoints for 0 into /tmp/cifar10_model/model.ckpt.
2019-04-05 18:59:31.456932: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-04-05 18:59:31.464278: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-04-05 18:59:31.469508: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-04-05 18:59:31.695706: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-04-05 18:59:31.708371: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-04-05 18:59:31.723334: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-04-05 18:59:32.145557: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-04-05 18:59:32.152672: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR

For looking online, the CUDA_ERROR_OUT_OF_MEMORY errors don't matter because the program keeps running afterwards. However, I don't know why I am getting the other errors. Why can I train the model on one GPU and not multiple? My packages should be compatible with one another. I am using: 
tensorflow-gpu==1.12.0
cuda-9.0 
cudnn-7
These packages should be compatible with one another, right? 

If anyone has any idea what the issue may be, I am open to any suggestions. 

Thank you 
"
27548,tf.contrib.labeled_tensor incorrectly handling or failing with None dimensions,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution : macOS Sierra
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12
- Python version: 2.7

**labeled_tensor.mul fails with None dimensions when tf.multiply doesn't.**

ipdb> X

<LabeledTensor 'X:0' shape=(1, 1, 2, 12, 1, 300) dtype=float32
 axes=[('a', Dimension(1)),
       ('b', Dimension(1)),
       ('c', Dimension(2)),
       ('d', Dimension(12)),
       ('e', Dimension(1)),
       ('f', Dimension(200))]>

ipdb> Y

<LabeledTensor 'Y:0' shape=(?, 6, 1, 1, 200, 300) dtype=float32
 axes=[('a', Dimension(None)),
       ('b', Dimension(6)),
       ('c', Dimension(1)),
       ('d', Dimension(1)),
       ('e', Dimension(200)),
       ('f', Dimension(300))]>

ipdb> X*Y

*** ValueError: Mismatched 'a' axis on input tensors: Axis('a', Dimension(1)) and Axis('a', Dimension(None))

ipdb> lt.mul(X,Y)

*** ValueError: Mismatched 'a' axis on input tensors: Axis('a', Dimension(1)) and Axis('a', Dimension(None))

ipdb> tf.multiply(X,Y)

<tf.Tensor  'Mul_2:0' shape=(?, 6, 2, 12, 200, 300) dtype=float32>
"
27547,Add Tensorflow.Distributions to 2.0,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): tf_nightly_gpu_2.0_preview-2.0.0.dev20190327-cp36-cp36m-win_amd64
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**

Will tensorflow.distributions be added to 2.0? Currently, TFP doesn't work with 2.0 due to its absence.

Thanks!

**Will this change the current api? How?**

No

**Who will benefit with this feature?**

Users of Tensorflow Probability

**Any Other info.**
"
27545,[TF 2.0]  tf.estimator.ProfilerHook... is not compatible with eager execution,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 8.8
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): via pip
- TensorFlow version (use command below): `tf.__git_version__ : 'v1.12.0-9492-g2c319fb415', version tensorflow==2.0.0-alpha0`
- Python version: 3.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
When constructing a `tf.estimator.ProfilerHook` a error is thrown: `RuntimeError: tf.summary.FileWriter is not compatible with eager execution. Use tf.contrib.summary instead.`

**Describe the expected behavior**

**Code to reproduce the issue**

```
import tensorflow as tf
tf.estimator.ProfilerHook(10)
```


**Other info / logs**

I'm not very familiar with tf 2.0, so maybe this is just user error? Is it possible to use tf2 not in eager mode?

Full traceback: 

```
RuntimeError                              Traceback (most recent call last)
<ipython-input-7-4eefc50dcc45> in <module>
----> 1 tf.estimator.ProfilerHook(10)

~/tf2-venv/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py in __init__(self, save_steps,save_secs, output_dir, show_dataflow, show_memory)
   1013     """"""
   1014     self._output_file = os.path.join(output_dir, ""timeline-{}.json"")
-> 1015     self._file_writer = SummaryWriterCache.get(output_dir)
   1016     self._show_dataflow = show_dataflow
   1017     self._show_memory = show_memory

~/tf2-venv/lib/python3.6/site-packages/tensorflow/python/summary/writer/writer_cache.py in get(logdir)
     61       if logdir not in FileWriterCache._cache:
     62         FileWriterCache._cache[logdir] = FileWriter(
---> 63             logdir, graph=ops.get_default_graph())
     64       return FileWriterCache._cache[logdir]

~/tf2-venv/lib/python3.6/site-packages/tensorflow/python/summary/writer/writer.py in __init__(self, logdir, graph, max_queue, flush_secs, graph_def, filename_suffix, session)
    358     if context.executing_eagerly():
    359       raise RuntimeError(
--> 360           ""tf.summary.FileWriter is not compatible with eager execution. ""
    361           ""Use tf.contrib.summary instead."")
    362     if session is not None:

RuntimeError: tf.summary.FileWriter is not compatible with eager execution. Use tf.contrib.summary instead.
```
"
27544,tf.add_n is inaccurate for float16,"**System information**
- TensorFlow version (you are using): master
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

`tf.add_n` is insufficiently accurate with `float16` precision for large scale training, for two reasons:

1. It sums the numbers serially, rather than in tree order.  As a result, we get stuff like this:

```
>>> f = lambda x: tf.constant(x, dtype=tf.float16)
>>> tf.add_n([f(1)]+[f(2**-11)]*7).numpy()
1.0
>>> tf.add_n([f(2**-11)]*7+[f(1)]).numpy()
1.004
```

2. It uses intermediate `float16` values, even though it would be cheap to use intermediate `float32` values.

I'd like to fix this by (1) using tree order in the Eigen kernels and (2) making the op cast up to `float32`, do the sum in `float32`, then downcast back to `float16`.

**Will this change the current api? How?**

No.

**Who will benefit with this feature?**

People using `float16` training (primarily GPU users).

**Any Other info.**

`tf.add_n` is memory bandwidth limited, so there should be negligible slowdown."
27543,tf2.0 failed to save model if an input is used not used in the model directly,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.0a0
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

I have this use case where the model requires multiple inputs and some inputs go into the model layers and some inputs go into the loss function directly. It works fine in the past until I updated my package unit test to tf2.0 to test drive and find out tf2.0 failed test that run fine with tf1.x

I noticed that its mainly due to the tensor that goes directly to the loss function in this case is not presented in ``self._network_nodes`` in the line ``if node_key not in nn.keras_model._network_nodes:`` in tensorflow keras network.py. 

**Describe the expected behavior**

I expect the model to be saved successfully with tf2.0 just as tf1.x

**Code to reproduce the issue**
```python3
import numpy as np

import tensorflow as tf
import tensorflow.keras as tfk
Sequence = tfk.utils.Sequence

Dense = tfk.layers.Dense
Input = tfk.layers.Input

Model = tfk.models.Model

def special_loss(weights):
    def special_loss_internal(true, pred):
        return (true - pred / weights)
    return special_loss_internal

# Model 1 which does not have Flatten
input_tensor1 = Input(shape=[200], name='input_1')
input_tensor2 = Input(shape=[10], name='input_2')
output_tensor1 = Dense(units=10, name='output_1')(input_tensor1)
output_tensor2 = Dense(units=10, name='output_2')(input_tensor1)

neuralnet = Model(inputs=[input_tensor1, input_tensor2], outputs=[output_tensor1, output_tensor2])
neuralnet.compile(loss=special_loss(input_tensor2), optimizer='adam')

neuralnet.save(""test.h5"")
```

**Other info / logs**
```
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
~\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\util\nest.py in pack_sequence_as(structure, flat_sequence, expand_composites)
    430     final_index, packed = _packed_nest_with_indices(structure, flat_sequence,
--> 431                                                     0, is_seq)
    432     if final_index < len(flat_sequence):

~\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\util\nest.py in _packed_nest_with_indices(structure, flat, index, is_seq)
    380     else:
--> 381       packed.append(flat[index])
    382       index += 1

IndexError: list index out of range

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-1-6f88ca6a69df> in <module>
     26 neuralnet.compile(loss=special_loss(input_tensor2), optimizer='adam')
     27
---> 28 neuralnet.save(""test.h5"")

~\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\keras\engine\network.py in save(self, filepath, overwrite, include_optimizer)
   1312
   1313     from tensorflow.python.keras.models import save_model  # pylint: disable=g-import-not-at-top
-> 1314     save_model(self, filepath, overwrite, include_optimizer)
   1315
   1316   def save_weights(self, filepath, overwrite=True, save_format=None):

~\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\keras\saving\hdf5_format.py in save_model(model, filepath, overwrite, include_optimizer)
     99         {
    100             'class_name': model.__class__.__name__,
--> 101             'config': model.get_config()
    102         },
    103         default=serialization.get_json_type).encode('utf8')

~\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\keras\engine\network.py in get_config(self)
   1107       model_inputs.append(
   1108           tf_utils.ListWrapper([layer.name, new_node_index, tensor_index]))
-> 1109     model_inputs = nest.pack_sequence_as(self._nested_inputs, model_inputs)
   1110     model_inputs = tf_utils.convert_inner_node_data(model_inputs)
   1111     config['input_layers'] = model_inputs

~\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\util\nest.py in pack_sequence_as(structure, flat_sequence, expand_composites)
    438           ""Could not pack sequence. Structure had %d elements, but ""
    439           ""flat_sequence had %d elements.  Structure: %s, flat_sequence: %s."" %
--> 440           (len(flat_structure), len(flat_sequence), structure, flat_sequence))
    441   return _sequence_like(structure, packed)
    442

ValueError: Could not pack sequence. Structure had 2 elements, but flat_sequence had 1 elements.  Structure: [<tf.Tensor 'input_1:0' shape=(None, 200) dtype=float32>, <tf.Tensor 'input_2:0' shape=(None, 10) dtype=float32>], flat_sequence: [<tensorflow.python.keras.utils.tf_utils.ListWrapper object at 0x000001958CF2DDA0>].
```
"
27541,[TF==2.0.0-alpha0] tf.keras.Model reinitializes set weights. ,"### System information
- Have I written custom code: **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**
- TensorFlow installed from (source or binary): **pip install tensorflow==2.0.0-alpha0**
- TensorFlow version (use command below): **2.0.0-alpha0**
- Python version: **3.7.1**
- **Using only CPU**

### Describe the current behavior
When I initialize a convolution layer with some weights, instead of the weights being set, it will be randomly re-initialized. Print output returns at each run a different weight.

**Code to reproduce the issue**
```
import tensorflow.keras
import numpy as np

weight = np.ndarray(
    shape=[1, 1, 1, 1],
    dtype='float32')
weight[0, 0, 0, 0] = -0.8888
bias = np.ndarray(
    shape=[1],
    dtype='float32')
bias[0] = 0

weights = [weight, bias]

inputs = tensorflow.keras.layers.Input(shape=(1,1,1), name='inputs')
outputs = tensorflow.keras.layers.Conv2D(filters=1,
                                 kernel_size=1,
                                 strides=(1,1),
                                 kernel_regularizer=tensorflow.keras.regularizers.l2(0.0005),
                                 weights=weights,
                                 use_bias=True,
                                 activation=None,
                                 padding='same')(inputs)
model = tensorflow.keras.Model(inputs=inputs,
                              outputs=outputs)

for layer in model.layers:
    for weight in layer.get_weights():
        print(weight) # wrong output: [[[[1.6804]]]] [0.]
```

### Describe the expected behavior
with:
- keras version 2.1.5 
- using tensorflow backend version 1.6.0

When I initialize a convolution layer with some weights, the weights will be set and stick. Print output returns as expected: [[[[-0.8888]]]] [0.]

This is an useful feature, especially when one is converting from one model format to another. For example when converting darknet (model.cfg, model.weights) to keras (model.h5). 

**Code to reproduce**
```
import keras
import numpy as np

weight = np.ndarray(
    shape=[1, 1, 1, 1],
    dtype='float32')
weight[0, 0, 0, 0] = -0.8888
bias = np.ndarray(
    shape=[1],
    dtype='float32')
bias[0] = 0
weights = [weight, bias]
inputs = keras.layers.Input(shape=(1,1,1), name='inputs')
outputs = keras.layers.Conv2D(filters=1,
                                 kernel_size=1,
                                 strides=(1,1),
                                 kernel_regularizer=keras.regularizers.l2(0.0005),
                                 weights=weights,
                                 use_bias=True,
                                 activation=None,
                                 padding='same')(inputs)
model = keras.Model(inputs=inputs,
                              outputs=outputs)

for layer in model.layers:
    for weight in layer.get_weights():
        print(weight)  # correct output: [[[[-0.8888]]]] [0.]
```

### Questions:
 Is this intended? Or how can this be disabled? How can I help you solve this issue. Thanks for taking your time looking into this. "
27539,"ValueError: Input 0 of layer dense is incompatible with the layer: its rank is undefined, but the layer requires a defined rank on Colab script downloaded to local machine","As a companion issue to #27538 with tensorflow 2.0.0a. Script fails with

> ValueError: Input 0 of layer dense is incompatible with the layer: its rank is undefined, but the layer requires a defined rank.

in the line

```  layers.Dense(image_data.num_classes, activation='softmax')```
**System information**
- TensorFlow version:
> ('v1.12.0-9492-g2c319fb415', '2.0.0-alpha0')

using an updated script

```python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

which also works for tensorflow 1.
- Doc Link: https://www.tensorflow.org/tutorials/images/hub_with_keras


**Describe the documentation issue**
The downloaded .py script (see #27538) fails to run (it's not clear-cut if this is a documentation issue or simply a bug) at
```
model = tf.keras.Sequential([
  feature_extractor_layer,
  layers.Dense(image_data.num_classes, activation='softmax')
])
```

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
If it's a doc issue, I'd love to. It could go deeper than that. The same issue appears for the MCVE at stackoverflow: https://stackoverflow.com/questions/55490885/error-converting-keras-model-to-tfjs-duplicate-weight-name-variable
"
27538,"AttributeError: 'module' object has no attribute 'KerasLayer' on Colab script when downloaded to local, non-cuda laptop","The Colab for https://www.tensorflow.org/tutorials/images/hub_with_keras fails to run when downloaded to a local machine.
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): ('v1.13.1-0-g6612da8951', '1.13.1')
- Python version: 2.7.15rc1
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: n.a.
- GPU model and memory:  Mesa DRI Intel(R) HD Graphics 520 (Skylake GT2) 
32 GB max as of https://www.intel.com/content/www/us/en/support/products/88355/graphics-drivers/graphics-for-6th-generation-intel-processors/intel-hd-graphics-520.html

**Describe the current behavior**
Fails at line
```
    hub.KerasLayer(classifier_url, input_shape=IMAGE_SHAPE+(3,))
```
with

> AttributeError: 'module' object has no attribute 'KerasLayer'

**Describe the expected behavior**
Script runs to completion

**Code to reproduce the issue**
1. Open in Browser: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/hub_with_keras.ipynb
2. export as .py file (File -> Download .py)
3. remove pip commands from file, run tf-hub install by hand, avoid tf-gpu-nightly as no CUDA gpu available
```pip install tensorflow tensorflow-hub```
4. execute script: ```python hub_with_keras.py ```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

> $ python hub_with_keras.py 
> WARNING: Logging before flag parsing goes to stderr.
> W0405 12:54:22.202450 140138733332288 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14
> Traceback (most recent call last):
>   File ""hub_with_keras.py"", line 28, in <module>
>     hub.KerasLayer(classifier_url, input_shape=IMAGE_SHAPE+(3,))
> AttributeError: 'module' object has no attribute 'KerasLayer'

When upgrading to 2.0.0a (1.14 seems to be unavailable on pip) the script fails with another error, see #27539.

[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3047549/tf_env.txt)
"
27537,Unexpected UnicodeDecodeError: invalid continuation byte when reading lines from a file,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.5.2
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
Unexpected and undocumented runtime exception/error when handling malformed data.

**Describe the expected behavior**
Expected a ""TypeError"" or an empty list as a result.

**Code to reproduce the issue**
```
import csv
import sys
import tensorflow as tf

input_file_name = sys.argv[1]

with tf.gfile.Open(input_file_name, ""r"") as f:
  reader = csv.reader(f, delimiter=""\t"", quotechar=None)
  for line in reader:
    print(line)
```
Run with the path to the attached file as a command line argument.

**Other info / logs**

Traceback (most recent call last):
  File ""tensorflow_bug.py"", line 9, in <module>
    for line in reader:
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py"", line 220, in \_\_next\_\_
    return self.next()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py"", line 214, in next
    retval = self.readline()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py"", line 184, in readline
    return self._prepare_value(self._read_buf.ReadLineAsString())
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py"", line 100, in _prepare_value
    return compat.as_str_any(val)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py"", line 107, in as_str_any
    return as_str(value)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py"", line 80, in as_text
    return bytes_or_text.decode(encoding)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xed in position 0: invalid continuation byte

[corrupted_file1.zip](https://github.com/tensorflow/tensorflow/files/3047460/corrupted_file1.zip)
"
27536,Unexpected UnicodeDecodeError: invalid start byte when reading lines from a file,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.5.2
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
Unexpected and undocumented runtime exception/error when handling malformed data.

**Describe the expected behavior**
Expected a ""TypeError"" or an empty list as a result.

**Code to reproduce the issue**
```
import csv
import sys
import tensorflow as tf

input_file_name = sys.argv[1]

with tf.gfile.Open(input_file_name, ""r"") as f:
  reader = csv.reader(f, delimiter=""\t"", quotechar=None)
  for line in reader:
    print(line)
```
Run with the path to the attached file as a command line argument.

**Other info / logs**

Traceback (most recent call last):
  File ""tensorflow_bug.py"", line 9, in <module>
    for line in reader:
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py"", line 220, in \_\_next\_\_
    return self.next()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py"", line 214, in next
    retval = self.readline()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py"", line 184, in readline
    return self._prepare_value(self._read_buf.ReadLineAsString())
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py"", line 100, in _prepare_value
    return compat.as_str_any(val)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py"", line 107, in as_str_any
    return as_str(value)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py"", line 80, in as_text
    return bytes_or_text.decode(encoding)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbb in position 1: invalid start byte


[corrupted_file0.zip](https://github.com/tensorflow/tensorflow/files/3047441/corrupted_file0.zip)
"
27535,FileSystem destructor is not being called when done,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **CentOS 7**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **v1.13.1**
- Python version: **Python 3.6.3**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

When a FileSystem is registered it is allocated in the heap, but not destroyed on exit when using a Python script.

The FileSystem destructor (~FileSystem()) must be called on exit.

**Code to reproduce the issue**
```
--- /opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorflow/include/tensorflow/core/platform/file_system.h	2019-04-05 10:05:27.253673315 +0200
+++ /test/tensorflow/tensorflow/core/platform/file_system.h	2019-04-05 10:22:15.999269084 +0200
@@ -225,7 +225,7 @@
 
   FileSystem() { printf(""create FileSystem\n""); }
 
-  virtual ~FileSystem();
+  virtual ~FileSystem() { printf(""destroy FileSystem\n""); }
 };
```
Then we run python and load our FileSystem plugin:
```
[alexey@workstation test]$ python -c ""import tensorflow as tf; tf.load_library('build/lib/libfilesystem_tensorflow.so')""
create FileSystem
[alexey@workstation test]$
```
**Other info / logs**
It is necessary for our custom FileSystem plugin to call some functions on destruction. It seems that C-style pointers are used when registering a FileSystem:
https://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/core/platform/env.h#L450-L457

https://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/core/platform/file_system.h#L336

The scheme is actually mapped to a unique pointer to the class:
https://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/core/platform/env.cc#L64

But I suspect that move could be the issue:
https://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/core/platform/env.cc#L114
"
27534,What's the license of pre-trained TensorFlow.js models?,"I want to use tfjs-models in a commercial application. While source code in the [tfjs-models repository](https://github.com/tensorflow/tfjs-models) is licensed under Apache 2.0, I did not find any info about the license of the models hosted on storage.googleapis.com.

For example [body-pix](https://github.com/tensorflow/tfjs-models/tree/master/body-pix) is loading its model from [storage.googleapis.com/tfjs-models](https://storage.googleapis.com/tfjs-models)
e.g.
[posenet_mobilenet_025_partmap/model.json](https://storage.googleapis.com/tfjs-models/savedmodel/posenet_mobilenet_025_partmap/model.json)
[posenet_mobilenet_025_partmap/tensorflowjs_model.p](https://storage.googleapis.com/tfjs-models/savedmodel/posenet_mobilenet_025_partmap/tensorflowjs_model.pb)
[posenet_mobilenet_025_partmap/weights_manifest.json](https://storage.googleapis.com/tfjs-models/savedmodel/posenet_mobilenet_025_partmap/weights_manifest.json)

Are the files on [storage.googleapis.com/tfjs-models](https://storage.googleapis.com/tfjs-models) licensed under Apache 2.0 as well?

If yes, it would be great to document it, and I'm willing to submit a PR.

"
27532,Custom Object Detection crashes when convert_to_grayscale is enabled.,"**System Information:**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.2
* Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy): NA
* TensorFlow installed from (source or binary): binary
* TensorFlow version: 1.12.0
* Python version: 3.6
* Installed using virtualenv? pip? conda?: Conda
* Bazel version (if compiling from source): NA
* GCC/Compiler version (if compiling from source): NA
* CUDA/cuDNN version: NA
* GPU model and memory: NA


**Problem Description:**

Enabling convert_to_grayscale option in image_resizer pipeline.config for FASTER_RCNN Object detection causes failure during export. The training runs without any issue. 

**Error Log:**

2019-04-05 09:39:01.001689: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-04-05 09:39:01.002097: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.
Traceback (most recent call last):
  File ""/Users/madhukandasamy/miniconda3/envs/cs-object-detection/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/Users/madhukandasamy/miniconda3/envs/cs-object-detection/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/Users/madhukandasamy/miniconda3/envs/cs-object-detection/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: **Assign requires shapes of both tensors to match. lhs shape= [7,7,1,8] rhs shape= [7,7,3,8]**
	 [[{{node save/Assign_10}} = Assign[T=DT_FLOAT, _class=[""loc:@FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/depthwise_weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/depthwise_weights, save/RestoreV2:10)]]

**Steps to reproduce:**

Follow the documented steps to do custom object detection with pretained model faster_rcnn_inception_v2_coco[http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz]  as a starting point. 
Use default values wherever applicable in the pipeline config.
Enable convert_to_grayscale option for image_resizer in the pipeline config.
```python   image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 600
        max_dimension: 1024
        convert_to_grayscale: true
      }
```

Run the training(just one or two steps is sufficient) and export the model as per documentation.Notice the failure during export.

**Initial Analysis:**

The convert_to_grayscale() image resizer modifies the input image from (H, W, 3, 8) => (H, W, 1, 8) during pre-process. This works fine during training and models is being checkpointed without any issue. When we try to freeze the model by reading from the checkpoint, its done as two parts. The graph definition is built using _build_detection_graph() and values are read later from the check-point data. The graph definition is built without considering the changes to the number of channels(it should be taking care of width and height) by the image resizer. This keeps the number of channels un-changed(i.e kept as 3) in defintion, where as the actual check-point data is accounted only for one channel."
27531,TF2.0 / `SequenceFeatures` doesn't have a `_is_feature_layer` property.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0-alpha
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I follow the example [Classify structred data](https://www.tensorflow.org/alpha/tutorials/keras/feature_columns) as below:
```
feature_layer = SequenceFeatures(columns, name=INPUT_NAME)

model = keras.Sequential([
    feature_layer,
    keras.layers.LSTM(1, return_sequence=True),
    keras.layers.LSTM(1),
    keras.layers.Dense(1, activation='relu', name=OUTPUT_NAME)
])

model.compile(optimizer='adam', loss='mse')
model.fit(DATASET, epochs=5)
```
and it shows an error:
`ValueError: Passing a dictionary input to a Sequential Model which doesn't have FeatureLayer as the first layer is an error.`

However, I found the reason that `SequenceFeatures` doesn't have a property `is_feature_layer` as below:
```
## tensorflow/python/keras/engine/training_utils.py
# TODO(rohanj): This is a hack to get around not depending on feature_column and
# create a cyclical dependency. Figure out a cleaner solution
def is_feature_layer(layer):
  """"""Returns whether `layer` is a FeatureLayer or not.""""""
  return getattr(layer, '_is_feature_layer', False)

## tensorflow/python/feature_column/feature_column_v2.py
@keras_export('keras.layers.DenseFeatures')
class DenseFeatures(_BaseFeaturesLayer):
  def __init__(self, feature_columns, trainable=True, name=None, **kwargs):

    super(DenseFeatures, self).__init__(
        feature_columns=feature_columns,
        trainable=trainable,
        name=name,
        expected_column_type=DenseColumn,
        **kwargs)

  @property
  def _is_feature_layer(self):
    return True
   
  def _target_shape(self, input_shape, total_elements):
    return (input_shape[0], total_elements)
    ...

## tensorflow/python/feature_column/sequence_feature_column.py
@keras_export('keras.experimental.SequenceFeatures')
class SequenceFeatures(fc._BaseFeaturesLayer):
    def __init__(self, feature_columns, trainable=True, name=None, **kwargs):

    super(SequenceFeatures, self).__init__(
        feature_columns=feature_columns,
        trainable=trainable,
        name=name,
        expected_column_type=fc.SequenceDenseColumn,
        **kwargs)

  def _target_shape(self, input_shape, total_elements):
    return (input_shape[0], input_shape[1], total_elements)
   ...
```
I think it would be better to move the `_is_feature_layer` property to the`_BaseFeaturesLayer` for a while.
 
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27530,"Duplicate layer name when using a + b and Add()([c, d]) in the same Keras model","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOSX 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below):
tf.version.VERSION=2.0.0-dev20190404
tf.version.GIT_VERSION=v1.12.0-11729-g98c3cfbf74
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
When using `a + b` then `keras.layers.Add` in the same Keras model, I get an exception saying the name `""add""` was used twice. The problem goes away if I use `keras.layers.Add` first, or if I use twice the same operation (either `+` twice, or `keras.layers.Add` twice).

**Describe the expected behavior**
I don't expect any name conflicts.

**Code to reproduce the issue**
The following code raises a `ValueError` exception (full stacktrace below).

```python
from tensorflow import keras

inputs = keras.layers.Input(shape=[2])
add1 = inputs + inputs
add2 = keras.layers.Add()([inputs, inputs])
model = keras.models.Model(inputs=[inputs], outputs=[add1, add2])
```

**Other info / logs**
Here is the full stacktrace:

```python
ValueError                                Traceback (most recent call last)
<ipython-input-1-07b5faf0201d> in <module>
      4 add1 = inputs + inputs
      5 add2 = keras.layers.Add()([inputs, inputs])
----> 6 model = keras.models.Model(inputs=[inputs], outputs=[add1, add2])

.../tensorflow/python/keras/engine/training.py in __init__(self, *args, **kwargs)
    121
    122   def __init__(self, *args, **kwargs):
--> 123     super(Model, self).__init__(*args, **kwargs)
    124     # initializing _distribution_strategy here since it is possible to call
    125     # predict on a model without compiling it.

.../tensorflow/python/keras/engine/network.py in __init__(self, *args, **kwargs)
    137         'inputs' in kwargs and 'outputs' in kwargs):
    138       # Graph network
--> 139       self._init_graph_network(*args, **kwargs)
    140     else:
    141       # Subclassed network

.../tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    456     self._setattr_tracking = False  # pylint: disable=protected-access
    457     try:
--> 458       result = method(self, *args, **kwargs)
    459     finally:
    460       self._setattr_tracking = previous_value  # pylint: disable=protected-access

.../tensorflow/python/keras/engine/network.py in _init_graph_network(self, inputs, outputs, name)
    284     # Keep track of the network's nodes and layers.
    285     nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(
--> 286         self.inputs, self.outputs)
    287     self._network_nodes = nodes
    288     self._nodes_by_depth = nodes_by_depth

.../tensorflow/python/keras/engine/network.py in _map_graph_network(inputs, outputs)
   1916     if all_names.count(name) != 1:
   1917       raise ValueError('The name ""' + name + '"" is used ' +
-> 1918                        str(all_names.count(name)) + ' times in the model. '
   1919                        'All layer names should be unique.')
   1920   return network_nodes, nodes_by_depth, layers, layers_by_depth

ValueError: The name ""add"" is used 2 times in the model. All layer names should be unique.
```"
27527,Tensorflow C++ API on Windows/VS2015: unresolved external symbol,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.1
- Python version: 3.5
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.19
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.2
- GPU model and memory: Nvidia GTX 1070

**Describe the problem**

I successfully built Tensorflow using Bazel from source.

I obtained the following files.

- bazel-bin\tensorflow\libtensorflow_cc.dll (renamed from libtensorflow_cc.so)
- bazel-bin\tensorflow\libtensorflow_cc.lib (renamed from liblibtensorflow_cc.so.ifso)
- bazel-bin\tensorflow\libtensorflow_framework.dll (renamed from libtensorflow_framework.so)
- bazel-bin\tensorflow\libtensorflow_framework.lib (renamed from liblibtensorflow_framework.so.ifso)

In Visual Studio 2015

- I included headers from bazel-genfiles (include/, third_pary/, com_google_absl, com_googlesource_code_re2, protobuf_archive\src, etc)
- I also configured Linked to link with .lib files.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

In the code, I simply declare the following param:
auto root = tensorflow::Scope::NewRootScope();

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Severity	Code	Description	Project	File	Line	Source	Suppression State
Error	LNK1120	3 unresolved externals	Program	D:\Project_Tensorflow\x64\Release\Program.exe	1	Build	
Error	LNK2011	precompiled object not linked in; image may not run	Program	D:\Project_Tensorflow\Program\TestTensorflow.obj	1	Build	
Error	LNK2001	unresolved external symbol ""public: static class tensorflow::Scope __cdecl tensorflow::Scope::NewRootScope(void)"" (?NewRootScope@Scope@tensorflow@@SA?AV12@XZ)	Program	D:\Project_Tensorflow\Program\utils.obj	1	Build	
Error	LNK2001	unresolved external symbol ""public: __cdecl tensorflow::Scope::~Scope(void)"" (??1Scope@tensorflow@@QEAA@XZ)	Program	D:\Project_Tensorflow\Program\utils.obj	1	Build	

Please help if you have an idea. Thank you very much.
"
27526,[Deleted],Deleted. I just realized I made a mistake.
27525,tf.function-decorated function ,"**System information**
- I have written several custom layers for this code. However, they are tested and are not the cause of this bug.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Debian Stable (up to date)
- TensorFlow installed from (source or binary): downloaded the latest alpha of tf2 through pip
- TensorFlow version (use command below): That command does not work for tf2
- Python version: 3.6.8



I have an error in my code (located : [here](https://github.com/alexsludds/6888_final_project)). If you look at the main.py file you see that I am trying to change a variable and train a model repeatedly, extracting the testing accuracy each time. If you run this file for a minute, you see that on the second step of this first for loop I get an error:

ValueError: tf.function-decorated function tried to create variables on non-first call.

To me this means that train is trying to create a new tf variable, but I don't understand why, the model should be different and the everything is getting called again. 

Is there anything I can do to get around this by flushing the graph, or is this a bug in the alpha version of tf2?

Thank you"
27523,`tf.train.NanTensorHook` does not prevent `tf.train.NanLossDuringTrainingError` when using custom Estimator.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.14.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): BINARY
- TensorFlow version (use command below): 1.13
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

I am trying to use `tf.train.NanTensorHook` to prevent my training script from halting with an Error. (Specifically this is to do Hyperparameter optimization on cloudml, but I observe this bug on my local machine as well). My instantiation looks like:

```
def model_fn():
  loss = ...
  train_op = ...

  training_hooks=[]
  # Report training failed if loss becomes `Nan`.
  training_hooks.append(tf.train.NanTensorHook(loss, fail_on_nan_loss=False))

  return tf.estimator.EstimatorSpec(
    mode=mode,
    loss=loss,
    train_op=train_op,
    predictions=predict_output,
    eval_metric_ops=eval_metric_ops,
    training_hooks=training_hooks,
  )
```

**Describe the expected behavior**

I expect that this should log a warning and halt training.

**Code to reproduce the issue**
Will update when I have written a minimal example.

**Other info / logs**
Error thrown:

> ERROR:tensorflow:Model diverged with loss = NaN.
> Traceback (most recent call last):
>   File ""/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
>     ""__main__"", mod_spec)
>   File ""/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/runpy.py"", line 85, in _run_code
>     exec(code, run_globals)
>   File ""/Users/noah/Documents/CHU/super_resolution/super_resolution/trainer/train_basic_model.py"", line 140, in <module>
>     main()
>   File ""/Users/noah/Documents/CHU/super_resolution/super_resolution/trainer/train_basic_model.py"", line 132, in main
>     eval_parse_fns=parse_fns,
>   File ""/Users/noah/Documents/CHU/super_resolution/super_resolution/trainer/train.py"", line 166, in run_train_and_evaluate
>     log_step_count=args.log_step_count,
>   File ""/Users/noah/Documents/CHU/super_resolution/super_resolution/trainer/train.py"", line 81, in train_and_evaluate
>     tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
>   File ""/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
>     return executor.run()
>   File ""/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
>     return self.run_local()
>   File ""/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
>     saving_listeners=saving_listeners)
>   File ""/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
>     loss = self._train_model(input_fn, hooks, saving_listeners)
>   File ""/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
>     return self._train_model_default(input_fn, hooks, saving_listeners)
>   File ""/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1241, in _train_model_default
>     saving_listeners)
>   File ""/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1471, in _train_with_estimator_spec
>     _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
>   File ""/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 671, in run
>     run_metadata=run_metadata)
>   File ""/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1156, in run
>     run_metadata=run_metadata)
>   File ""/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1255, in run
>     raise six.reraise(*original_exc_info)
>   File ""/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/six.py"", line 693, in reraise
>     raise value
>   File ""/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1240, in run
>     return self._sess.run(*args, **kwargs)
>   File ""/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1320, in run
>     run_metadata=run_metadata))
>   File ""/Users/noah/Documents/CHU/super_resolution/conda_env/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 753, in after_run
>     raise NanLossDuringTrainingError"
27521,Bug in tf.parallel_stack,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): master branch from 9e0b9b9ff2b01e07c2a71ce0fbbae93a4bba86f9 and onward
- Python version: 2.7
- Bazel version (if compiling from source): 0.23.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
tf.parallel_stack is producing incorrect output.
**Describe the expected behavior**
tf.parallel_stack has to have same output as tf.stack or tf.concat
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

The below reproducer code/test passes with any commit that precedes 9e0b9b9ff2b01e07c2a71ce0fbbae93a4bba86f9
```
import tensorflow as tf
import numpy as np

graph = tf.Graph()
with graph.as_default():
  images = np.random.random_sample((100, 24, 24, 3))
  parallel_stacked_images = tf.parallel_stack(images)
  stacked_images = tf.stack(images)
  concatenated_images = tf.concat(images, 0)

with tf.Session(graph=graph) as sess:
  np_parallel_stacked_images, np_stacked_images, np_concatenated_images = sess.run(
    [parallel_stacked_images, stacked_images, concatenated_images])

if np.array_equal(np_parallel_stacked_images, np_stacked_images):
  print (""Pass: outputs of tf.parallel_stack and tf.stack are equal"")
else:
  print (""Fail: outputs of tf.parallel_stack and tf.stack are not equal"")

if np.array_equal(np_parallel_stacked_images, np_concatenated_images):
  print (""Pass: outputs of tf.parallel_stack and tf.concat are equal"")
else:
  print (""Fail: outputs of tf.parallel_stack and tf.concat are not equal"")
```

**Other info / logs**
We have debugged this issue and found that this commit 9e0b9b9ff2b01e07c2a71ce0fbbae93a4bba86f9 actually causes the bug. That commit changes the set of `Edge`s in the graph from std::set (ordered) to gtl::FlatSet (unordered). TF has a graph optimizer pass for parallel_stack (or parallel_concat) that replaces ParallelConcat node with several _ParallelConcatUpdate nodes. But it seems that it assumes all in_edges are ordered which is not the case with gtl::FlatSet. Therefore, there is a problem in updating the output buffer that is using the incremental index (the `loc` attribute) which does not correspond to the right input_edge. In particular, I am talking about this code: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/parallel_concat_optimizer.cc#L83

"
27520,segmentation fault using tensorflow debugger,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

When i type run in tfdbg i get a segmentation fault.

**System information**
- Have I written custom code?
yes i have.
- OS Platform and Distribution:
kali linux rolling 2019 (see tf_env)
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
not compiled from source
- CUDA/cuDNN version:
i don't know what CUDA is but i don't have Nvidia GPU i think but radeon amd?
i think tensorflow runs on my cpu?
- GPU model and memory:
 lspci -vnn | grep VGA -A 12
01:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Thames [Radeon HD 7500M/7600M Series] [1002:6840] (prog-if 00 [VGA controller])
	Subsystem: Toshiba America Info Systems Radeon HD 7670M [1179:fb22]
	Flags: bus master, fast devsel, latency 0, IRQ 29
	Memory at b0000000 (64-bit, prefetchable) [size=256M]
	Memory at c0000000 (64-bit, non-prefetchable) [size=128K]
	I/O ports at 3000 [size=256]
	Expansion ROM at 000c0000 [disabled] [size=128K]
	Capabilities: [50] Power Management version 3
	Capabilities: [58] Express Legacy Endpoint, MSI 00
	Capabilities: [a0] MSI: Enable+ Count=1/1 Maskable- 64bit+
	Capabilities: [100] Vendor Specific Information: ID=0001 Rev=1 Len=010 <?>
	Capabilities: [150] Advanced Error Reporting
	Kernel driver in use: radeon

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
output of tf_env_collect.sh:
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3045026/tf_env.txt)


You can also obtain the TensorFlow version 
with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
i installed tensorflow in pycharm using pip (my projects uses the system interpreter)
tf.VERSION:
1.13.1
tf.GIT_VERSION:
b'v1.13.0-rc2-5-g6612da8'

**Describe the current behavior**
I first go to a terminal and type the following:
tensorboard --logdir=/root/files/Tensorboard/logs/ --debugger_port=6064
then in another terminal i type:
python3 NeuralNetwork.py --debug
i then see the run-start CLI like this.
![Schermafdruk van 2019-04-04 20-48-05](https://user-images.githubusercontent.com/41961612/55580465-e5f02e80-571a-11e9-9d56-e0ebe3fea6b5.png)

then i type run and i get an segmentation fault.
![Schermafdruk van 2019-04-04 20-50-34](https://user-images.githubusercontent.com/41961612/55580547-13d57300-571b-11e9-9b43-4dc0c2733fc5.png)



**Describe the expected behavior**
the expected behavior i think is that the run command would not crash the debugger.

**Code to reproduce the issue**
[Example.txt](https://github.com/tensorflow/tensorflow/files/3044988/Example.txt)
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27519,TF 2.0 'Tensor' object has no attribute 'numpy' while using .numpy() although eager execution enabled by default,"Although Eager_execution is enabled by default in TF 2.0, I am getting errors while using .numpy()

Please note that i am not using the code in compatibility mode to TF 1.0.

  expt = [[[  0,   0,   0],
            [  4,  71, 141],
            [  0,   0,  0]],

           [[ 83,  25,  85],
            [ 90, 190, 143],
            [  4, 141,  49]],

           [[  0,   0,   0],
            [  4,  71,  49],
            [  0,   0,   0]]]
expt = tf.convert_to_tensor(expt)

expected_values = expt.numpy()


AttributeError: 'Tensor' object has no attribute 'numpy'


CPU TEST VERSION OF TENSORFLOW 2.0. "
27518,STOP USING ELIPSES!,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: All
- Doc Link: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim/python/slim/data


Every time higher level API like Slim says it will present an example for how to load data it starts with this or it's moral equivalent:

```
# Load the data
my_encoded_data = ...
data_decoder = MyDataDecoder()
# Decode the inputs and labels:
decoded_input, decoded_labels = data_decoder.Decode(data, ['input', 'labels'])
# Decode just the inputs:
decoded_input = data_decoder.Decode(data, ['input'])
# Check which items a data decoder knows how to decode:
for item in data_decoder.list_items():
  print(item)
```

I am sorely tempted to lay some biting and just dripping sarcasm down here, but I know all the deeply experienced TF programmers and creators probably honestly think this kind of thing is enlightening. For some reason.

If I knew how to load data I would not be grepping the Internet and finding stuff like this.

What goes in the elipsis?"
27517,ML Kit for Android fails to load valid TFLite model,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: BLU Vivo XI
- TensorFlow installed from (source or binary): Source
- TensorFlow version: ('v1.13.1-0-g6612da8951', '1.13.1') 
- Python version: 2.7
- Bazel version (if compiling from source): 0.24.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: NVIDIA GeForce GTX 1050, ~5 GB

**Describe the current behavior**
I am currently attempting to use Firebase ML Kit to run a Tensorflow-Lite model that was converted to the .tflite format from Keras. I have been following [this](https://firebase.google.com/docs/ml-kit/android/use-custom-models) tutorial to host a custom model locally. 
Every attempt thus far has ended with the same error: ""ByteBuffer is not a valid flatbuffer model"" (full stack trace shown below).
The .tflite model has been created correctly; running the Interpreter (from the Python Tensorflow module) to allocate tensors and get the input and output details returned the proper information without error. 
Furthermore, the iOS version of the app (using the same model, stored locally and loaded with Firebase ML Kit) is working properly. It used a custom build of Tensorflow (following [this](https://firebase.google.com/docs/ml-kit/ios/use-custom-tflite) tutorial) that started by forking  Tensorflow 12.0.0 and then cherry-picked future commits to include the additional ops required. I did not write any custom ops myself; all of the required ops had already been added to Tensorflow at one time or another.
First, I attempted to use the same build by following [this](https://firebase.google.com/docs/ml-kit/android/use-custom-tflite) tutorial with Bazel version 0.18.0, which failed. I also attempted [this](https://heartbeat.fritz.ai/compiling-a-tensorflow-lite-build-with-custom-operations-cf6330ee30e2) tutorial to import TFLite as a module rather than publish it to my local maven repository. This had the same result.
Then, under the assumption that the latest release of Tensorflow would have all the necessary ops anyway, I built Tensorflow 1.13.1 using Bazel version 0.24.0. This failed with the same error.

Here are my questions, as succinctly as possible:
1. Is ML Kit currently unequipped to handle the most recent versions of Tensorflow-Lite? (If so, why would the iOS version still work?)
2. Is Tensorflow-Lite for Android currently not working for Tensorflow 1.13.1? I'm no expert in machine learning, so this may be nonsensical, but are there operations/features created for Tensorflow that Tensorflow-Lite is unable to handle? (And again, if this was the case, why would the iOS version still work?)
3. Are the tutorials for ML Kit missing any essential information? 
4. Has anyone else been able to get a custom model converted to .tflite and have it work with ML Kit?

**Other info / logs**
E/ModelResourceManager: Error preloading model resource
    com.google.firebase.ml.common.FirebaseMLException: Local model load failed: 
        at com.google.android.gms.internal.firebase_ml.zzpe.zza(Unknown Source:129)
        at com.google.android.gms.internal.firebase_ml.zzpe.zzlp(Unknown Source:104)
        at com.google.android.gms.internal.firebase_ml.zznx.zzf(Unknown Source:56)
        at com.google.android.gms.internal.firebase_ml.zznz.zzls(Unknown Source:7)
        at com.google.android.gms.internal.firebase_ml.zznz.call(Unknown Source:24)
        at com.google.android.gms.internal.firebase_ml.zznn.zza(Unknown Source:29)
        at com.google.android.gms.internal.firebase_ml.zzno.run(Unknown Source:2)
        at android.os.Handler.handleCallback(Handler.java:790)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at com.google.android.gms.internal.firebase_ml.zzi.dispatchMessage(Unknown Source:6)
        at android.os.Looper.loop(Looper.java:164)
        at android.os.HandlerThread.run(HandlerThread.java:65)
     Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model
        at org.tensorflow.lite.NativeInterpreterWrapper.createModelWithBuffer(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:69)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:175)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:163)
        at com.google.android.gms.internal.firebase_ml.zzpe.zzc(Unknown Source:224)
        at com.google.android.gms.internal.firebase_ml.zzpf.zzd(Unknown Source:0)
        at com.google.android.gms.internal.firebase_ml.zzpe.zzb(Unknown Source:150)
        at com.google.android.gms.internal.firebase_ml.zzpe.zza(Unknown Source:118)
        at com.google.android.gms.internal.firebase_ml.zzpe.zzlp(Unknown Source:104)
        at com.google.android.gms.internal.firebase_ml.zznx.zzf(Unknown Source:56)
        at com.google.android.gms.internal.firebase_ml.zznz.zzls(Unknown Source:7)
        at com.google.android.gms.internal.firebase_ml.zznz.call(Unknown Source:24)
        at com.google.android.gms.internal.firebase_ml.zznn.zza(Unknown Source:29)
        at com.google.android.gms.internal.firebase_ml.zzno.run(Unknown Source:2)
        at android.os.Handler.handleCallback(Handler.java:790)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at com.google.android.gms.internal.firebase_ml.zzi.dispatchMessage(Unknown Source:6)
        at android.os.Looper.loop(Looper.java:164)
        at android.os.HandlerThread.run(HandlerThread.java:65)
"
27516,Placeholder removed from tf 2.0,tensorflow placeholder is not found on tf 2.0. I need to create a Tensor with shape = None. But As the placeholder is gone . I am not finding ways to do it the 2.0 way. Please help. 
27512,High RAM usage after loading inference model into GPU,"OS - Ubuntu 16.04
CUDA Version 10.0.130
Tensorflow version 1.13.1
YoloV2 - https://pjreddie.com/darknet/yolov2/

Trained a YOLOv2 architecture on the custom images and after freezing the graph, Model weight (.pb) file size is 268MB, Once we load this into gpu for inference it is consuming 7.93GB. I know tensorflow allocates the buffers for the output data at each stage at the beginning. Please, can somebody explain why there is so much memory usage by tensorflow.

Btw, if we run the same model with darknet framework it is taking 2.9GB of RAM.
"
27511,[TF==2.0.0-alpha0] Memory leak with tf.keras.models.load_model,"**System information**
- Have I written custom code? Yes
- OS Platform and Distribution  (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Docker_image tensorflow/tensorflow:2.0.0a0-py3-jupyter
- TensorFlow version : 2.0.0a0-py3-jupyter 
- Python version:  3.5
- Using only CPU


**Describe the current behavior**

I have a memory leak when I load several keras model with `load_model` function previously save with `model.save`.
Here is what I obtain when i load 20 times a model (the same model for the example).
The output is obtain with the memory profiler library. *https://pypi.org/project/memory-profiler/* (see code below)

First Iteration:

```
Line #    Mem usage    Increment   Line Contents
================================================
    14    229.5 MiB    229.5 MiB   @profile
    15                             def load_model_keras(model_dir):
    16    232.5 MiB      2.9 MiB       K.clear_session()
    17    252.9 MiB     20.4 MiB       model = load_model(model_dir)
    18    252.9 MiB      0.0 MiB       return model
```

After 20 iterations:
```
Line #    Mem usage    Increment   Line Contents
================================================
    14    539.9 MiB    539.9 MiB   @profile
    15                             def load_model_keras(model_dir):
    16    539.9 MiB      0.0 MiB       K.clear_session()
    17    566.4 MiB     26.5 MiB       model = load_model(model_dir)
    18    566.4 MiB      0.0 MiB       return model
```
**Describe the expected behavior**

I do not have this problem with tensorflow==1.13.1
Here is what I obtain with this version and the behavior I expect to have.


First Iterations :

```
Line #    Mem usage    Increment   Line Contents
================================================
    14    210.7 MiB    210.7 MiB   @profile
    15                             def load_model_keras(model_dir):
    16    214.2 MiB      3.4 MiB       K.clear_session()
    17    239.8 MiB     25.6 MiB       model = load_model(model_dir)
    18    239.8 MiB      0.0 MiB       return model
```

After 20 iteration :
```
Line #    Mem usage    Increment   Line Contents
================================================
    14    257.9 MiB    257.9 MiB   @profile
    15                             def load_model_keras(model_dir):
    16    257.9 MiB      0.0 MiB       K.clear_session()
    17    259.0 MiB      1.1 MiB       model = load_model(model_dir)
    18    259.0 MiB      0.0 MiB       return model
```


**Code to reproduce the issue**
```
import os
import tensorflow as tf
from tensorflow.keras.models import load_model
import tensorflow.keras.backend as K
from memory_profiler import profile
model_dir = ""MODEL_PATH.h5""

@profile
def load_model_keras(model_dir):
    K.clear_session()
    model = load_model(model_dir)
    return model

for i in range(100):
    print(i)
    load_model_keras(model_dir)

```

**Other info / logs**

I also tried to put within the function : 
* model=None
* del model
* gc.collect() (import gc)

with no effect"
27510,Add HDF5 to tf.data.Dataset,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.13 (Though I would like in on tf 2.0)
- Are you willing to contribute it (Yes/No): No. Sorry, I don't know how this works... I am simply not qualified to contribute.



**Describe the feature and the current behavior/state.**
I am currently using HDF5 files (.h5 or .hdf5) to store my data, which is a data type frequently used in scientific research. (See https://github.com/tensorflow/tensorflow/issues/2089 for a similar but different request which makes the case for a HDF5 interface nicely). It is very convenient and widely used. MATLAB for example uses HDF5 files for large files. Indeed, it is much more convenient to use than Tensorflow's TFRecord format. 

However, in Tensorflow, there is no native support for HDF5 files in the tf.data.Dataset API, which is supposed to be the new API for all data loading. Currently, I am using tf.py_funtion to load my data for the simple reason that tf Dataset is always in graph mode and hence cannot give out the values of the files that I want it to read.

Moreover, I have found that reading an HDF5 file in this way **DRAMATICALLY** slows down data I/O for unknown reasons. When I used the tf.keras.utils.Sequence API to read HDF5 files without the supposed optimizations that tensorflow is making, an operation that previously took hours now took just a few seconds. (However, I suspect that using tf.defun somehow got tangled up with this. I am not sure why but when I removed some lines, the code sped up, but was still much slower than even a single threaded for loop) 

Therefore, I would like to propose creating a new API in tf Dataset for HDF5 files. It could be called HDF5Dataset, similar to TFRecordDataset or CSVDataset. 

Moreover, this would allow Tensorflow to make I/O optimizations for reading using the C++ API for HDF5 instead of h5py, which has many limitations and factors that newbie users might not be familiar with. 

For example, most builds of h5py cannot do multiprocessing. Also, most people do not know how to chunk their data slices, though this can make a 5-fold difference in read/write speed.

I believe that adding this API would make Tensorflow much friendlier to scientific calculation.

**Will this change the current api? How?**
This would add a new Dataset type in tf.data.Dataset, or a new method/function for making a dataset from an HDF5 file of an arbitrary format. This may require some low-level integration with the HDF5 format.

**Who will benefit with this feature?**
People in medical imaging, video datasets, astronomy, or any other type of very large dataset, which is often stored in HDF5. See their [website](https://www.hdfgroup.org/solutions/hdf5/) for information on its utility. Also people who don't want to go through the difficult process of making TFRecord files.

**Any Other info.**
Perhaps integrating aspects of h5py will make the process easier. 
"
27509,*help* changing gradients  value through Masking the gradients with a matrix whose shape same with var but non-compatible,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: tf 1.8
- Doc Link:https://tensorflow.google.cn/versions/r1.8/api_docs/python/tf/train/AdamOptimizer?hl=zh-cn#compute_gradients

> This is the first part of minimize(). It returns a list of (gradient, variable) pairs where ""gradient"" is the gradient for ""variable"". Note that ""gradient"" can be a Tensor, an IndexedSlices, or None if there is no gradient for the given variable.

**Describe the documentation issue**
I have a corresponding mask matrix which is obtained with a trained model, and using the code as following. 
`grad = tf.multiply(grad.values, tf.convert_to_tensor(tf.abs(gradient_mask[name])), name='%s_pruning_mask'%name)`   it is report a error of compatible because this gradient has  a shape(17374, 512) and matrix shape(17353, 512). In Truth, I have to found the gradient shape is correspond other tensors but found nothing . the mask matrix is target embeddings weights shape. So what's wrong with this code or documents, why? please and Thank you.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
27508,Missing device_attributes.pb.h when make tensorflow to .a file,"I want make tensorflow to a static link file so I can use it to run the pretrained model with the cpp API.After I run the **build_all_linux.sh**, I move the **tensorflow** and **third_party** file and the **libtensorflow-core.a** and **libprotobuf.a** and **libnsync.a** to a work path.
And when I compile the cpp code, it returns an error which said 

> fatal error: tensorflow/core/framework/device_attributes.pb.h: No such file or directory

And it occurs in the session.h. I find there is only device_attributes.proto in the path,but I dont know why because I just followed the ways of many blogs online.Is there any thing that I missed?

Any help is appreciate!Thx!"
27507,Weird behavior of tf.Variable.assign in a tf.Dataset.map callback function,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX Mojave
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): pip3.7
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7.2
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: Running the CPU version
- GPU model and memory: NA


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""


I was trying using a Variable to track a seed number in Dataset. Because I want my model to resume training with the same training order as before break if I can checkpoint the variable.


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

A minimal code is like:

```
import tensorflow as tf

seed = tf.Variable(0, dtype=tf.int64, trainable=False,
    use_resource=True)
def eject(x):
    seed.assign(x) 
    return {'seed': seed, 'x': x}

dataset = tf.data.Dataset.range(10000)
dataset = dataset.map(eject, num_parallel_calls=1)

iterator = dataset.make_initializable_iterator()

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(iterator.initializer)
    next_element = iterator.get_next()

    for i in range(10):
        fetches = sess.run(next_element)
        print(fetches)

```

**Describe the expected behavior**

The expected print should be 0-9 for both `x` and `seed`. 

**Describe the current behavior**

But the output of `seed` does not stays the same and changes over different executions. The output is like:

```
{'seed': 0, 'x': 0}
{'seed': 1, 'x': 1}
{'seed': 2, 'x': 2}
{'seed': 2, 'x': 3}
{'seed': 4, 'x': 4}
{'seed': 4, 'x': 5}
{'seed': 6, 'x': 6}
{'seed': 7, 'x': 7}
{'seed': 7, 'x': 8}
{'seed': 8, 'x': 9}
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27505,source build won't accept GPU with compute capability lower than 3.5,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.1
- Python version: 2.7
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: 10.0
- GPU model and memory: GeForce GTX 970

**Describe the problem**

The set of CUDA compute capabilities (CCC from now on) is set as {3.0, 5.2} during configuration. File `.tf_configure.bazelrc` contains `build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""3.0,5.2""` (full file: [tf_configure.bazelrc.txt](https://github.com/tensorflow/tensorflow/files/3043058/tf_configure.bazelrc.txt)). After transfering the resulting pip package to a PC with CCC 3.0, tensorflow will reject a GPU with CCC lower than 3.5.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. `./configure` (resulting in [tf_configure.bazelrc.txt](https://github.com/tensorflow/tensorflow/files/3043058/tf_configure.bazelrc.txt))
2. `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`
3. `./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`
4. move `/tmp/tensorflow_pkg/tensorflow-1.13.1-cp27-cp27mu-linux_x86_64.whl` to other PC with CCC 3.0

"
27503,can't save keras model built by functional api,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): n/a
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6
- Bazel version (if compiling from source): na
- GCC/Compiler version (if compiling from source): na
- CUDA/cuDNN version: na
- GPU model and memory: na


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

> lib/python3.6/site-packages/tensorflow/python/util/serialization.py"", line 69, in get_json_type
>     raise TypeError('Not JSON Serializable:', obj)
> TypeError: ('Not JSON Serializable:', b""\n\x04Mean\x12\x04Mean\x1a'embedding_1/embedding_lookup/Identity_2\x1a\x16Mean/reduction_indices*\x07\n\x01T\x12\x020\x01*\n\n\x04Tidx\x12\x020\x03*\x0f\n\tkeep_dims\x12\x02(\x01"")


**Describe the expected behavior**
model.save(path) works with a keras model

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```

import copy
 import tensorflow as tf
 import pandas as pd
 from tensorflow.keras.layers import Input, Embedding, concatenate, Dense, Flatten
 from tensorflow import feature_column
 from tensorflow.python.keras.engine import training_utils


 def df_to_dataset(dataframe, shuffle=True, batch_size=32):
   dataframe = dataframe.copy()
   labels = dataframe.pop('target')
   d = dict(dataframe)

   ds = tf.data.Dataset.from_tensor_slices((d, labels))
   if shuffle:
     ds = ds.shuffle(buffer_size=len(dataframe))
   ds = ds.batch(batch_size)
   return ds


 def get_model():
     first_input = Input(shape = (1,), name='first_input')
     second_input = Input(shape = (1,), name='second_input' )
     embedding_layer = Embedding(input_dim=10, output_dim=3, input_length=1)

     first_input_encoded = embedding_layer(first_input)
     first_input_encoded = tf.keras.layers.Reshape((3,))(first_input_encoded)


     selected = embedding_layer(second_input)
     item_average = tf.reduce_mean(selected, axis=1, keepdims=True)
     second_input_encoded = tf.keras.layers.Reshape((3,))(item_average)


     o = concatenate([first_input_encoded, second_input_encoded])
     o = Dense(1)(o)

     inputs = [first_input, second_input]
     model = tf.keras.models.Model(inputs=inputs, outputs=o)
     return model


 df = pd.DataFrame(
     [[3,[4,2,3],5, 'boy', 0],
     [2,[6,1,2],7, 'girl', 1]], columns=['first_input', 'second_input', 'child_month_young', 'child_gender_young', 'target'])

 train_ds = df_to_dataset(df)
 val_ds = df_to_dataset(df)

 model = get_model()

 model.compile(optimizer='adam',
               loss='binary_crossentropy',
               metrics=['accuracy'])


 model.fit(
     train_ds,
     validation_data=val_ds,
     epochs=1
 )

 print(model.summary())

 # tf.keras.models.save_model(model, 'test.h5')
 model.save('test.h5')
```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27502,A result calculated inside of the tf.while_loop is wrong and not raising any error/alert when it occurred by GPU OOM.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 with tensorflow on Spyder IDE
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7
- Bazel version (if compiling from source): Not applicable
- GCC/Compiler version (if compiling from source): Not applicable
- CUDA/cuDNN version: CUDA 10.0 / cuDNN 7.5.0
- GPU model and memory: Titan RTX w/ 24.0GB of mem.

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When we are dealing with huge data that are going to be processed by any tf operations, tf.while_loop does not raise any error/flag/alert when it reaches to the GPU memory limit and thus getting the wrong result. I think you should add some assertion so that people can learn what's the problem.

Here's the output
`WARNING:tensorflow:From C:\Users\SHS\Anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\ops\tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.`
Instructions for updating:
Colocations handled automatically by placer.
shapes?  (?, **512**, 16, 16) (?, **512**, 16, 16)
values?  [[0. 0.]
 [0. 0.]] [[0. 0.]
 [0. 0.]] **>> Shouldn't be zeros. Have to be some numbers because I assigned random number and multiplied them.**
iterations?  50
Result? [[ -8.234    0.3125]
 [ -7.92   -10.53  ]]

runfile('C:/Users/SHS/FPGA Project files/Practices/test_for_fl16.py', wdir='C:/Users/SHS/FPGA Project files/Practices')
shapes?  (?, **64**, 16, 16) (?, **64**, 16, 16)
values?  [[-16.64  42.38]
 [ 37.9   34.2 ]] [[  3.514 -10.414]
 [ 20.88   24.62 ]] >> **when I reduce the data size it gives me the right values.**
iterations?  50
Result? [[-3.031   -0.01172]
 [-5.043    2.027  ]]

**Describe the expected behavior**
Output printed should not be zeros, because I assigned some random number.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
import tensorflow as tf
import numpy as np
    
sess = tf.InteractiveSession()

a = tf.random.uniform([512, 512, 16, 16], minval=-3, maxval=3, dtype=tf.float16)
b = tf.random.uniform([512, 512, 16, 16], minval=-3, maxval=3, dtype=tf.float16)
c = tf.random.uniform([50, 512, 16, 16], minval=-3, maxval=3, dtype=tf.float16)
d = tf.random.uniform([50, 512, 16, 16], minval=-3, maxval=3, dtype=tf.float16)

i = 0
real = tf.TensorArray(dtype=tf.float16, size=50, name='REAL')
imag = tf.TensorArray(dtype=tf.float16, size=50, name='IMAG')
loop_cond = lambda i, *_: i < 50
def loop_body(i, r_array, i_array):
  r_array = r_array.write(i, tf.subtract(tf.multiply(a,c[i]), tf.multiply(b,d[i])))
  i_array = i_array.write(i, tf.add(tf.multiply(a,d[i]), tf.multiply(b,c[i])))
  i += 1
  return i, r_array, i_array
i, real, imag = tf.while_loop(loop_cond, loop_body, loop_vars=[i, real, imag])

real = real.stack()
imag = imag.stack()
real = tf.reduce_sum(real, axis=2)
imag = tf.reduce_sum(imag, axis=2)
print('shapes? ', real.shape, imag.shape)
print('values? ', sess.run(real[0,0, :2, :2]), imag.eval()[0,0, :2, :2])
print('iterations? ', i.eval())

i = 0
tt = tf.add(tf.multiply(a,c[i]), tf.multiply(b,d[i]))
print('Result?', tt.eval()[0,0,:2,:2])

```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27500,[Grappler] RemoveIdentityTranspose also removes conjugate,"
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.13
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 10.0
- **GPU model and memory**: N/A
- **Exact command to reproduce**: see below


During graph optimization, `RemoveIdentityTranspose` also removes a transpose if it conjugates the input (see [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc#L1092))

Simple example:
```
import tensorflow as tf
with tf.Graph().as_default():
    sess = tf.Session()
    a = tf.placeholder(tf.complex64)
    data = [[1j], [1j]]
    print(sess.run(tf.transpose(a, (0, 1), conjugate=True), {a: data}))  # not optimized
    print(sess.run(tf.transpose(a, (0, 1), conjugate=True) + 1, {a: data}))  # optimized, no conjugate will be applied
    print(sess.run(tf.conj(a) + 1, {a: data}))
```

Output:
```
[[0.-1.j]
 [0.-1.j]]
[[1.+1.j]
 [1.+1.j]]
[[1.-1.j]
 [1.-1.j]]
```

This can happen when using einsum/tensordot and a probably related issue is [#19771](https://github.com/tensorflow/tensorflow/issues/19771)

A possible fix would be to also check for `IsConjugateTranspose` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc#L1137) "
27499,"optimizer_v2.apply_gradients() - surprising behavior, not explicitly documented","**System information**
- TensorFlow version: 2.0.0-aplha0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/Optimizer#apply_gradients

I was implementing a DD Policy Gradient for reinforcement learning, and had a bug where my agent would minimize the reward, instead of maximizing it. It turned out optimizer.apply_gradients() follows negative of the gradients I give to it. This is a surprising behavior when using this function separately, outside of .minimize() context.

Documentation does not mention anywhere that negative gradient is followed by this method.

had to use a unit test to clarify the behavior:

```
    opt = tf.optimizers.Adam()
    x = tf.Variable([1], dtype=tf.float32)
    dx = tf.ones([1], dtype=tf.float32)

    opt.apply_gradients( [(dx, x)] )
    assert x.numpy()[0] > 1
```
There is also no flag to invert this behavior and follow positive gradient. I have to pass negative gradient of the expected reward to follow it in positive direction.


```
    def train_actor(self, sars):

        obs1, actions, rewards, obs2 = sars
        with tf.GradientTape() as tape:
            would_do_actions = self.actor(obs1)
            score = tf.reduce_mean( self.critic( observations=obs1, actions=would_do_actions ) )
            inverted = - score

        # tf optimizer follows negative of the provided gradients.
        # For this reason we provide negative gradient of the score -
        # it will result in positive gradient being followed.
        grads = tape.gradient( inverted, self.actor.trainable_weights )
        self.optimizer.apply_gradients( zip(grads, self.actor.trainable_weights) )
```"
27498,Where is the BahdanauAttention?,"tf 1.0: tensorflow.contrib.seq2seq.BahdanauAttention
Where is the BahdanauAttention with tf 2.0???"
27497,`tf.reduce_sum` with multiple negative axes and `tf.RaggedTensor` bugged,"## System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below):
- Python version: ('v1.13.1-0-g6612da8951', '1.13.1')
- GPU model and memory: quadro k620 2gb

## Current behaviour
Multiple axes passed to `tf.reduce_sum` with first argument being a ragged tensor results in incorrect behaviour.

```python
import tensorflow as tf

x_values = tf.random.normal(shape=(100, 5, 6))
x_row_lengths = tf.constant([20, 30, 50], dtype=tf.int64)
x_ragged = tf.RaggedTensor.from_row_lengths(x_values, x_row_lengths)
print(x_ragged.shape)
# [3, ?, 5, 6]

# wrong shape
print(tf.reduce_sum(x_ragged, axis=(-2, -3)).shape)
# [50, 6]

# positive axes work
print(tf.reduce_sum(x_ragged, axis=(1, 2)).shape)
# [3, 6]

# separate reductions work
print(tf.reduce_sum(tf.reduce_sum(x_ragged, axis=-3), axis=-2).shape)
# [3, 6]
```

## Expected behaviour
Same result as corresponding positive indices/separate reductions.
"
27496,Can not set 'dynamic' property on custom layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab environment
- TensorFlow version (use command below): 2.0 alpha

I am trying to write a custom layer in TF 2.0 and having trouble making it run eagerly.

For example if I open a documentation page: https://www.tensorflow.org/alpha/guide/eager

And try to use the following piece of code from that page to create custom layer:
```python
class MySimpleLayer(tf.keras.layers.Layer):
  def __init__(self, output_units):
    super(MySimpleLayer, self).__init__()
    self.output_units = output_units
    self.dynamic = True

  def build(self, input_shape):
    # The build method gets called the first time your layer is used.
    # Creating variables on build() allows you to make their shape depend
    # on the input shape and hence removes the need for the user to specify
    # full shapes. It is possible to create variables during __init__() if
    # you already know their full shapes.
    self.kernel = self.add_variable(
      ""kernel"", [input_shape[-1], self.output_units])

  def call(self, input):
    # Override call() instead of __call__ so we can perform some bookkeeping.
    return tf.matmul(input, self.kernel)
```

And then try to create an instance, for example like this:

```python
MySimpleLayer(10)
```
I get the following error message:

```python
AttributeError                            Traceback (most recent call last)
<ipython-input-26-1fe9d7429f3d> in <module>()
----> 1 MySimpleLayer(10)

<ipython-input-25-e48804825d2c> in __init__(self, output_units)
      3     super(MySimpleLayer, self).__init__()
      4     self.output_units = output_units
----> 5     self.dynamic = True
      6 
      7   def build(self, input_shape):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __setattr__(self, name, value)
   1778         # Exclude @property.setters from tracking
   1779         hasattr(self.__class__, name)):
-> 1780       super(Layer, self).__setattr__(name, value)
   1781       return
   1782 

AttributeError: can't set attribute
```
This error appears in Colab as well as on my machine (Windows 8).

Am I doing something wrong here?
"
27495,to build with '-g' option,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2eebcc63a6cd3aad483bf7c1cb25df2b8780ef67
- Python version: Python 3.5.2
- Installed using virtualenv? pip? conda?:  virtualenv
- Bazel version (if compiling from source): bazel-0.24.0-installer-linux-x86_64.sh
- GCC/Compiler version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) 
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the problem**
My purpose is to build tensorflow c library (libtensorflow_framework.so and libtensorflow.so) to be stepped into within gdb via source code.

I tried two methods:
1) add '-g -O0' option during ./configure
![image](https://user-images.githubusercontent.com/10448440/55536831-5bdda100-56ed-11e9-9cb5-7d586981b292.png)

and then run the below commands to build the c libraries:
bazel test -c opt //tensorflow/tools/lib_package:libtensorflow_test
bazel build -c opt //tensorflow/tools/lib_package:libtensorflow

but, gdb shows that it is unable to read the symbols as below:
0x00007fffedea2400  0x00007ffff41f73ef  Yes (*)     /usr/local/lib/libtensorflow.so
0x00007fffe9245bc0  0x00007fffea040ea0  Yes (*)     /usr/local/lib/libtensorflow_framework.so

2) i tried dbg version with:
bazel test -c dbg //tensorflow/tools/lib_package:libtensorflow_test

and it never stops, with:
INFO: Build option --compilation_mode has changed, discarding analysis cache.
INFO: Analysed target //tensorflow/tools/lib_package:libtensorflow_test (0 packages loaded, 15440 targets configured).
INFO: Found 1 test target...
[7,070 / 7,073] Linking tensorflow/libtensorflow.so; 14876s local
^C
Bazel caught interrupt signal; shutting down.

and the build result is weird:
$ file bazel-out/k8-dbg/bin/tensorflow/libtensorflow.so          
bazel-out/k8-dbg/bin/tensorflow/libtensorflow.so: data


btw, my build commands come from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md"
27494,'Sequential' object has no attribute 'total_loss',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution 
Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
Source
- TensorFlow version (use command below):
1.13.1
- Python version:
3.7.1 
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
I try to fit a Sequential model with both a training dataset and a validation dataset with fit_generator function. After of running it shows - 'Sequential' object has no attribute 'total_loss'-

**Describe the expected behavior**
Training should work fine.

**Code to reproduce the issue**
```
import os
import random
import numpy as np
from PIL import Image
import multiprocessing 
import tensorflow as tf
from resizeimage import resizeimage

def images_list(path):
    images_list = []
    XY = []
    with open(path,""r"") as file:
        images_list = file.read().split('\n')
        XY = [row.split("" "") for row in images_list if len(row.split("" "")) > 1]
    return np.asarray(XY)

def load_images(X,Y,i):
    root = 'E:\\images\\rvl-cdip\\rvl-cdip\\images'
    img_matrixes = []
    labels = []
    length = len(X)
    for index in range(len(X)):
        matrix = Image.open(os.path.join(root, X[index].replace('/',""\\"")))
        img_matrixes.append(resize_image_500(matrix))
        labels.append(Y[index])
        
    img_matrixes = np.asarray(img_matrixes)
    labels = np.asarray(labels)
    
    assert len(img_matrixes) == len(labels)
          
    #print(""{}: Loaded {} images"".format(i,length))
          
    return np.reshape(img_matrixes,(img_matrixes.shape[0],500,500,1)),labels

def resize_image(img):
    np_img = np.asarray(img)
    
    if(np_img.shape[1] < 3235):
        missing_width = 3235 - np_img.shape[1]
        white_matrix = np.empty((1000,missing_width),dtype=float)
        white_matrix.fill(255)
        np_img = np.hstack((np_img, white_matrix))
        
    assert np_img.shape[0] == 1000
    assert np_img.shape[1] == 3235
    
    return np_img

def resize_image_500(img):
    resized = resizeimage.resize_cover(img, [500, 500])
    np_img = np.asarray(resized)
    
    assert np_img.shape[0] == 500
    assert np_img.shape[1] == 500
    
    return np_img

def iterate_minibatches(inputs, targets, batchsize):
    assert len(inputs) == len(targets)
    indices = np.arange(len(inputs))
    np.random.shuffle(indices)
    i = 0 
    for start_idx in np.arange(0, len(inputs) - batchsize + 1, batchsize):
        excerpt = indices[start_idx:start_idx + batchsize]
        i+=1
        yield load_images(inputs[excerpt], targets[excerpt],i)

from tensorflow.keras import layers

model = tf.keras.models.Sequential()
#H1
model.add(layers.Conv2D(8, kernel_size=(5, 5), strides=(1, 1),
                 activation='relu'))
model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
#H2
model.add(layers.Conv2D(16, kernel_size=(5, 5), strides=(1, 1),
                 activation='relu'))
model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
#H3
model.add(layers.Conv2D(32, kernel_size=(5, 5), strides=(1, 1),
                 activation='relu'))
model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
#H4
model.add(layers.Conv2D(64, kernel_size=(5, 5), strides=(1, 1),
                 activation='relu'))
model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
#H5
model.add(layers.Flatten())
#Dense
model.add(layers.Dense(1024, activation='relu'))
model.add(layers.Dense(1024, activation='relu'))

model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),
              optimizer='adam',
              metrics=[tf.keras.metrics.Accuracy()])
#Training data
XY_train =  images_list(train_path)
X_train = XY_train[:,0]
Y_train = XY_train[:,1].astype(int)

#Testing data
XY_test =  images_list(test_path)
X_test = XY_test[:,0]
Y_test = XY_test[:,1].astype(int)

#Validation data
XY_val = images_list(valid_path)
X_val = XY_val[:,0]
Y_val = XY_val[:,1].astype(int)

batch_size = 750
history = model.fit_generator(generator=iterate_minibatches(X_train, Y_train,batch_size),
                                  validation_data=iterate_minibatches(X_test, Y_test, batch_size),
                                  # validation_data=None,
                                  steps_per_epoch=len(X_train)//batch_size,
                                  validation_steps=len(X_test)//batch_size,
                                  verbose=1,
                                  epochs=100,
                                  use_multiprocessing=True,
                                  workers=multiprocessing.cpu_count() 
                             )
```

```
batch_size = 750
history = model.fit_generator(generator=iterate_minibatches(X_train, Y_train,batch_size),
                                  validation_data=iterate_minibatches(X_test, Y_test, batch_size),
                                  # validation_data=None,
                                  steps_per_epoch=len(X_train)//batch_size,
                                  validation_steps=len(X_test)//batch_size,
                                  verbose=1,
                                  epochs=100,
                                  use_multiprocessing=True,
                                  workers=multiprocessing.cpu_count() 
                             )

WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-10-367767e34155> in <module>
      8                                   epochs=100,
      9                                   use_multiprocessing=True,
---> 10                                   workers=multiprocessing.cpu_count()
     11                              )

~\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1424         use_multiprocessing=use_multiprocessing,
   1425         shuffle=shuffle,
-> 1426         initial_epoch=initial_epoch)
   1427 
   1428   def evaluate_generator(self,

~\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)
    123 
    124   batch_function = _make_execution_function(
--> 125       model, mode, class_weight=class_weight)
    126 
    127   # Create the queue for the generator.

~\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_generator.py in _make_execution_function(model, mode, class_weight)
    425   if mode == 'train':
    426     if not context.executing_eagerly():
--> 427       model._make_fit_function()
    428     f = functools.partial(model.train_on_batch, class_weight=class_weight)
    429   elif mode == 'test':

~\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in _make_fit_function(self)
   1924     ]
   1925     self._make_train_function_helper(
-> 1926         '_fit_function', [self.total_loss] + metrics_tensors)
   1927 
   1928   def _make_test_function_helper(self, fn_name, outputs, metric_updates=None):

AttributeError: 'Sequential' object has no attribute 'total_loss'
```"
27492,Use of tf.custom_gradient prevents garbage collection of Graph,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs 10.14
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): pip install tensorflow==1.13.1
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.4
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**

Using an op decorated with `tf.custom_gradient` prevents the `tf.Graph` it was created in from being garbage collected later. So if a script creates multiple such graphs in a row, it will use more and more memory unnecessarily.
I believe it's due to the `Graph` being held as an indirect reference to the custom gradient registry.

**Describe the expected behavior**

Ideally such a `Graph` would not be referenced forever, allowing it to be garbage-collected.

**Code to reproduce the issue**

```python
import gc
import objgraph
import tensorflow as tf


@tf.custom_gradient
def op(x):
    def grad(dy):
        return dy * 10.0
    return x, grad


def run(use_custom_gradient):
    with tf.Session(graph=tf.Graph()) as sess:
        x = tf.constant([1, 2, 3])
        if use_custom_gradient:
            print(""(Using custom_gradient)"")
            x = op(x)
        sess.run(x)


def log():
    gc.collect()
    print(
        ""number of uncollected graphs:"",
        len(objgraph.by_type('Graph')))


for use_custom_gradient in [False, True, True, False, False, True]:
    run(use_custom_gradient)
    log()
```

outputs:

```
number of uncollected graphs: 0
(Using custom_gradient)
number of uncollected graphs: 1
(Using custom_gradient)
number of uncollected graphs: 2
number of uncollected graphs: 2
number of uncollected graphs: 2
(Using custom_gradient)
number of uncollected graphs: 3
```

(ideally it should always say `number of uncollected graphs: 0`)

**Other info / logs**
n/a
"
27491,Using .numpy() with the tf.function decorator.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): `2.0.0-alpha0`
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
The .numpy() method doesn't work in a function with @tf.function decorator.

For example, the following code is excuted properly
```python
# Calculate neighbor list using ASE (a third party library).
def neighborlist(r):
    atoms = ase.Atoms(positions=r.numpy(), pbc=True, cell=[10,10,10])
    i, j = ase.neighborlist.neighbor_list(""ij"", atoms, cutoff=3)

    i = tf.constant(i)
    j = tf.constant(j)

    return i, j

# Positions of 4 particles.
positions = tf.Variable(tf.random.uniform(shape=[4,3]))
# Calculate neighbor list.
neighborlist(v)
```
Output:
```
(<tf.Tensor: id=134, shape=(42,), dtype=int64, numpy=
 array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,
        3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6])>,
 <tf.Tensor: id=135, shape=(42,), dtype=int64, numpy=
 array([1, 2, 3, 4, 5, 6, 6, 4, 5, 2, 0, 3, 0, 1, 3, 4, 5, 6, 5, 6, 4, 2,
        1, 0, 0, 1, 2, 3, 5, 6, 6, 4, 3, 1, 0, 2, 4, 0, 1, 2, 3, 5])>)
```

But the following code gives errors.
```python
# Calculate neighbor list using ASE (a third party library).
@tf.function
def neighborlist(r):
    # r automatically casted to np.array.
    atoms = ase.Atoms(positions=r.numpy(), pbc=True, cell=[10,10,10])
    i, j = ase.neighborlist.neighbor_list(""ij"", atoms, cutoff=3)

    #return ri, rj
    i = tf.constant(i)
    j = tf.constant(j)

    return i, j

# Positions of 4 particles.
positions = tf.Variable(tf.random.uniform(shape=[4,3]))
# Calculate neighbor list.
neighborlist(v)
```

Error:
```
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-30-21258fe33b35> in <module>
      2 positions = tf.Variable(tf.random.uniform(shape=[4,3]))
      3 # Calculate neighbor list.
----> 4 neighborlist(v)

~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    424     # This is the first call of __call__, so we have to initialize.
    425     initializer_map = {}
--> 426     self._initialize(args, kwds, add_initializers_to=initializer_map)
    427     if self._created_variables:
    428       try:

~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    368     self._concrete_stateful_fn = (
    369         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 370             *args, **kwds))
    371 
    372     def invalid_creator_scope(*unused_args, **unused_kwds):

~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   1311     if self._input_signature:
   1312       args, kwargs = None, None
-> 1313     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   1314     return graph_function
   1315 

~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   1578           or call_context_key not in self._function_cache.missed):
   1579         self._function_cache.missed.add(call_context_key)
-> 1580         graph_function = self._create_graph_function(args, kwargs)
   1581         self._function_cache.primary[cache_key] = graph_function
   1582         return graph_function, args, kwargs

~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   1510             arg_names=arg_names,
   1511             override_flat_arg_shapes=override_flat_arg_shapes,
-> 1512             capture_by_value=self._capture_by_value),
   1513         self._function_attributes)
   1514 

~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    692                                           converted_func)
    693 
--> 694       func_outputs = python_func(*func_args, **func_kwargs)
    695 
    696       # invariant: `func_outputs` contains only Tensors, IndexedSlices,

~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    315         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    316         # the function a weak reference to itself to avoid a reference cycle.
--> 317         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    318     weak_wrapped_fn = weakref.ref(wrapped_fn)
    319 

~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    684                   optional_features=autograph_options,
    685                   force_conversion=True,
--> 686               ), args, kwargs)
    687 
    688         # Wrapping around a decorator allows checks like tf_inspect.getargspec

~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)
    390     return _call_unconverted(f, args, kwargs)
    391 
--> 392   result = converted_f(*effective_args, **kwargs)
    393 
    394   # The converted function's closure is simply inserted into the function's

/tmp/tmplnfiywp3.py in tf__neighborlist(r)
      3   do_return = False
      4   retval_ = None
----> 5   atoms = ag__.converted_call('Atoms', ase, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (), {'positions': r.numpy(), 'pbc': True, 'cell': [10, 10, 10]})
      6   i, j = ag__.converted_call('neighbor_list', ase.neighborlist, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_1, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), ('ij', atoms), {'cutoff': 3})
      7   i = ag__.converted_call('constant', tf, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_2, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (i,), {})

~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py in numpy(self)
    826       return self.read_value().numpy()
    827     raise NotImplementedError(
--> 828         ""numpy() is only available when eager execution is enabled."")
    829 
    830   @deprecated(None, ""Prefer Dataset.range instead."")

NotImplementedError: numpy() is only available when eager execution is enabled.
```

The main problem of above issue is here. If the function (neighborlist) is inside a model,

```python
class Model(...):
    ....
    @tf.function
    def call(self, ...):
        ....
        i, j = neighborlist(x)
        ....
```

`Model.call()` will give errors.
It should be pointed out that I don't need the gradients of the opreration (neighborlist).
I think it is common case that using operations with the value of tensors in erger mode. So it can be problematic.

I tried to resolve the problem by using `tf.py_function` but I failed. How can I resolve the problem?

**Will this change the current api? How?**
Probably not.
**Who will benefit with this feature?**
Use complex operations inside the tensorflow graph.
**Any Other info.**
"
27490,ImportError: cannot import name model_fn,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Springdale Linux 7.5 (Verona)
- TensorFlow version: 1.13.1
- TensorFlow-GPU version: 1.12
- Python version: 3.4.9
- Installed using virtualenv? pip? conda?: using pip inside a virtualenv
- CUDA/cuDNN version: 7

**Describe the problem**

I'm attempting to use a `layer_norm` with the line:
```
layer_norm = tf.contrib.layers.layer_norm
```
 but am thrown the following stack trace:
```
File ""projectdir/venv/lib64/python3.4/site-packages/tensorflow/python/util/lazy_loader.py"", line 53, in __getattr__
    module = self._load()
  File ""projectdir/venv/lib64/python3.4/site-packages/tensorflow/python/util/lazy_loader.py"", line 42, in _load
    module = importlib.import_module(self.__name__)
  File ""/usr/lib64/python3.4/importlib/__init__.py"", line 109, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 2254, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 2237, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 2226, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1200, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1129, in _exec
  File ""<frozen importlib._bootstrap>"", line 1471, in exec_module
  File ""<frozen importlib._bootstrap>"", line 321, in _call_with_frames_removed
  File ""projectdir/venv/lib64/python3.4/site-packages/tensorflow/contrib/__init__.py"", line 48, in <module>
    from tensorflow.contrib import distribute
  File ""projectdir/venv/lib64/python3.4/site-packages/tensorflow/contrib/distribute/__init__.py"", line 34, in <module>
    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy
  File ""projectdir/venv/lib64/python3.4/site-packages/tensorflow/contrib/distribute/python/tpu_strategy.py"", line 27, in <module>
    from tensorflow.contrib.tpu.python.ops import tpu_ops
  File ""projectdir/venv/lib64/python3.4/site-packages/tensorflow/contrib/tpu/__init__.py"", line 73, in <module>
    from tensorflow.contrib.tpu.python.tpu.keras_support import tpu_model as keras_to_tpu_model
  File ""projectdir/venv/lib64/python3.4/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py"", line 71, in <module>
    from tensorflow.python.estimator import model_fn as model_fn_lib
ImportError: cannot import name 'model_fn'
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I ran the lines:
```
import tensorflow as tf
layer_norm = tf.contrib.layers.layer_norm
```
"
27489,LSTMBlockFusedCell does not support using outputprojectionwrapper,"I want to replace LSTMCell to LSTMBlockFusedCell for better performance.
but I have found that the LSTMBlockFusedCell does not have the project operation of the output.
so I add the outputprojectionwrapper to the LSTMBlockFusedCell by `cell = tf.contrib.rnn.OutputProjectionWrapper( tf.contrib.rnn.LSTMBlockFusedCell(config.hidden_size, forget_bias=0.0, cell_clip=config.cell_clip, use_peephole=config.use_peephole), output_size=n_outputs)`, but I got an error `TypeError: The argument 'cell' (<tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f2c08f1eb90>) is not an RNNCell: 'output_size' property is missing, 'state_size' property is missing, 'zero_state' method is missing.`
how to add projection in LSTMBlockFusedCell ?

**System information**
- TensorFlow version (you are using):conda install tensorflow 1.9.0
- Are you willing to contribute it (Yes/No): Yes"
27487,Non-OK-status for CudaLaunchKernel when torch is also imported,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary (pip installed)
- TensorFlow version (use command below): tensorflow-gpu==2.0.0a0
- Python version: Python 3.6.7
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: CUDA Version: 10.1? (10.0.130), CUDNN 7.4.2
- GPU model and memory:  GeForce RTX 2080, 7949MiB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

If torch is imported before tensorflow, tensorflow is unable to use the GPU. It throws the following error:
```
2019-04-03 19:31:08.167586: F tensorflow/core/kernels/random_op_gpu.cu.cc:64] Non-OK-status: CudaLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: invalid configuration argument
Aborted (core dumped)
```
Which is not very helpful for figuring out what the problem is

**Describe the expected behavior**

Both torch and tensorflow can be imported in any order without GPU issues

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

if the import torch line below is commented out, this code works. If tensorflow is imported before torch, it will work as well.
```
from time import time

import torch
import tensorflow as tf
from tensorflow.keras import layers
from predictor_brain import TFBrain

import numpy as np
data = np.random.random((10000, 32))
labels = np.random.random((10000, 10))

model = tf.keras.Sequential([
# Adds a densely-connected layer with 64 units to the model:
layers.Dense(64, activation='relu', input_shape=(32,)),
# Add another:
layers.Dense(64, activation='relu'),
# Add a softmax layer with 10 output units:
layers.Dense(10, activation='softmax')])

model.compile(optimizer=tf.optimizers.Adam(0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

a = time()
hist = model.fit(data, 
    labels,
    epochs=10, 
    batch_size=32
    )
b = time()
print(f'time {b-a} seconds') 
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

The pytorch version is:  torch==1.0.1.post2

From executing the code above

```
$ python test.py
2019-04-03 19:31:04.290594: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
 FMA                                                                          
2019-04-03 19:31:04.298331: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-04-03 19:31:05.024887: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x4e83c10 executing computations on platform CUDA. Devices:
2019-04-03 19:31:05.024921: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): GeForce RTX 2080, Compute Capability 7.5
2019-04-03 19:31:05.046149: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3493095000 Hz
2019-04-03 19:31:05.047498: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x551b970 executing computations on platform Host. Devices:
2019-04-03 19:31:05.047521: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>
2019-04-03 19:31:05.048183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties:
name: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.71
pciBusID: 0000:42:00.0
totalMemory: 7.76GiB freeMemory: 7.65GiB
2019-04-03 19:31:05.048206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0
2019-04-03 19:31:07.731519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-04-03 19:31:07.731583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0
2019-04-03 19:31:07.731590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N
2019-04-03 19:31:07.732016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6809 MB
 memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:42:00.0, compute capability: 7.5)
2019-04-03 19:31:08.167586: F tensorflow/core/kernels/random_op_gpu.cu.cc:64] Non-OK-status: CudaLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, blo
ck_size, 0, d.stream(), gen, data, size, dist) status: Internal: invalid configuration argument
Aborted (core dumped)
```"
27486,What's happened with tensorflow.org?,What's happened with tensorflow.org? It just show a Service Unavailable hint.
27485,Failed to load the native TensorFlow runtime.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: 1.12.0
- Python version:2.7
- Installed using  conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: 


ImportErrorTraceback (most recent call last)
<ipython-input-1-81ca492b35f0> in <module>()
      7 #os.environ[""THEANO_FLAGS""]  = ""device=gpu%d""%(1)
      8 import numpy as np
----> 9 import tensorflow as tf
     10 #import theano.tensor as T
     11 from keras.utils import np_utils

/opt/anaconda2/envs/tf-gpu/lib/python2.7/site-packages/tensorflow/__init__.py in <module>()
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 try:

/opt/anaconda2/envs/tf-gpu/lib/python2.7/site-packages/tensorflow/python/__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 from tensorflow.python.tools import component_api_helper

/opt/anaconda2/envs/tf-gpu/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""/opt/anaconda2/envs/tf-gpu/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda2/envs/tf-gpu/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda2/envs/tf-gpu/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /opt/anaconda2/envs/tf-gpu/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so: undefined symbol: cuDevicePrimaryCtxGetState


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors"
27484,How to save a model after trained and restore the model in another script?,"### Hello guys I need help. I'm begin my studies in TensorFlow and I qhave some questions. For example, I'm using this example in my first neural network (https://github.com/tensorflow/docs/blob/master/site/en/tutorials/eager/custom_training_walkthrough.ipynb). 
**My question is, using this example from Oficial TensorFlow (https://www.tensorflow.org/tutorials/eager/custom_training_walkthrough), how to save my model and restore the model saved in another script? Thanks a lot
The full code is this:**

`from` __future__ import absolute_import, division, print_function, unicode_literals

import os
import matplotlib.pyplot as plt

import tensorflow as tf

tf.enable_eager_execution()

print(""TensorFlow version: {}"".format(tf.__version__))
print(""Eager execution: {}"".format(tf.executing_eagerly()))

train_dataset_url = ""https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv""

train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),
                                           origin=train_dataset_url)

print(""Local copy of the dataset file: {}"".format(train_dataset_fp))

# column order in CSV file
column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']

feature_names = column_names[:-1]
label_name = column_names[-1]

print(""Features: {}"".format(feature_names))
print(""Label: {}"".format(label_name))

class_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']

batch_size = 32

train_dataset = tf.contrib.data.make_csv_dataset(
    train_dataset_fp,
    batch_size, 
    column_names=column_names,
    label_name=label_name,
    num_epochs=1)

features, labels = next(iter(train_dataset))

features


plt.scatter(features['petal_length'].numpy(),
            features['sepal_length'].numpy(),
            c=labels.numpy(),
            cmap='viridis')

plt.xlabel(""Petal length"")
plt.ylabel(""Sepal length"");


def pack_features_vector(features, labels):
  """"""Pack the features into a single array.""""""
  features = tf.stack(list(features.values()), axis=1)
  return features, labels


train_dataset = train_dataset.map(pack_features_vector)
features, labels = next(iter(train_dataset))

print(features[:5])

model = tf.keras.Sequential([
  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required
  tf.keras.layers.Dense(10, activation=tf.nn.relu),
  tf.keras.layers.Dense(3)
])

predictions = model(features)
predictions[:5]


tf.nn.softmax(predictions[:5])

print(""Prediction: {}"".format(tf.argmax(predictions, axis=1)))
print(""    Labels: {}"".format(labels))


def loss(model, x, y):
  y_ = model(x)
  return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)


l = loss(model, features, labels)
print(""Loss test: {}"".format(l))


def grad(model, inputs, targets):
  with tf.GradientTape() as tape:
    loss_value = loss(model, inputs, targets)
  return loss_value, tape.gradient(loss_value, model.trainable_variables)

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)

global_step = tf.Variable(0)

loss_value, grads = grad(model, features, labels)

print(""Step: {}, Initial Loss: {}"".format(global_step.numpy(),
                                          loss_value.numpy()))

optimizer.apply_gradients(zip(grads, model.trainable_variables), global_step)

print(""Step: {},         Loss: {}"".format(global_step.numpy(),
                                          loss(model, features, labels).numpy()))

## Note: Rerunning this cell uses the same model variables

from tensorflow import contrib
tfe = contrib.eager

# keep results for plotting
train_loss_results = []
train_accuracy_results = []

num_epochs = 201

for epoch in range(num_epochs):
  epoch_loss_avg = tfe.metrics.Mean()
  epoch_accuracy = tfe.metrics.Accuracy()

  # Training loop - using batches of 32
  for x, y in train_dataset:
    # Optimize the model
    loss_value, grads = grad(model, x, y)
    optimizer.apply_gradients(zip(grads, model.trainable_variables),
                              global_step)

    # Track progress
    epoch_loss_avg(loss_value)  # add current batch loss
    # compare predicted label to actual label
    epoch_accuracy(tf.argmax(model(x), axis=1, output_type=tf.int32), y)

  # end epoch
  train_loss_results.append(epoch_loss_avg.result())
  train_accuracy_results.append(epoch_accuracy.result())
  
  if epoch % 50 == 0:
    print(""Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}"".format(epoch,
                                                                epoch_loss_avg.result(),
                                                                epoch_accuracy.result()))

fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))
fig.suptitle('Training Metrics')

axes[0].set_ylabel(""Loss"", fontsize=14)
axes[0].plot(train_loss_results)

axes[1].set_ylabel(""Accuracy"", fontsize=14)
axes[1].set_xlabel(""Epoch"", fontsize=14)
axes[1].plot(train_accuracy_results);

test_url = ""https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv""

test_fp = tf.keras.utils.get_file(fname=os.path.basename(test_url),
                                  origin=test_url)


test_dataset = tf.contrib.data.make_csv_dataset(
    test_fp,
    batch_size, 
    column_names=column_names,
    label_name='species',
    num_epochs=1,
    shuffle=False)

test_dataset = test_dataset.map(pack_features_vector)

test_accuracy = tfe.metrics.Accuracy()

for (x, y) in test_dataset:
  logits = model(x)
  prediction = tf.argmax(logits, axis=1, output_type=tf.int32)
  test_accuracy(prediction, y)

print(""Test set accuracy: {:.3%}"".format(test_accuracy.result()))

tf.stack([y,prediction],axis=1)


predict_dataset = tf.convert_to_tensor([
    [5.1, 3.3, 1.7, 0.5,],
    [5.9, 3.0, 4.2, 1.5,],
    [6.9, 3.1, 5.4, 2.1]
])

predictions = model(predict_dataset)

for i, logits in enumerate(predictions):
  class_idx = tf.argmax(logits).numpy()
  p = tf.nn.softmax(logits)[class_idx]
  name = class_names[class_idx]
  print(""Example {} prediction: {} ({:4.1f}%)"".format(i, name, 100*p))

`



"
27483,.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
27482,Have option to extend tf.image beyond 3-4 channels,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
27478,tf.print the example doesn't work with TF 2.0,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/print


**Describe the documentation issue**
the example doesn't work with TF 2.0
```python
    tf.enable_eager_execution()
    @tf.contrib.eager.defun
    def f():
        tensor = tf.range(10)
        tf.print(tensor, output_stream=sys.stderr)
        return tensor
    range_tensor = f()
```
```python
    sess = tf.Session()
    with sess.as_default():
        tensor = tf.range(10)
        print_op = tf.print(""tensors:"", tensor, {2: tensor * 2},
                            output_stream=sys.stdout)
        with tf.control_dependencies([print_op]):
          tripled_tensor = tensor * 3
        sess.run(tripled_tensor)
```
**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
27477,building pip package causes IndexError: list index out of range,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: master branch
- Python version: 3.6
- Installed using virtualenv? pip? conda?: NA
- Bazel version (if compiling from source): 0.22
- GCC/Compiler version (if compiling from source): VC14
- CUDA/cuDNN version: NA
- GPU model and memory: NA

TensorFlow successfully compiles a windows version, but building a pip package fails

```
(tf_113) C:\Users\pvenkat2\Desktop\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

INFO: Elapsed time: 41316.964s, Critical Path: 35436.10s
INFO: 4534 processes: 4534 local.
INFO: Build completed successfully, 6007 total actions

(tf_113) C:\Users\pvenkat2\Desktop\tensorflow>bazel-bin\tensorflow\tools\pip_package\build_pip_package C:/Users/pvenkat2/Desktop/tensorflow_pkg
Wed Apr 3 09:43:33 PDT 2019 : === Preparing sources in dir: /tmp/tmp.sDKWoKSGMv
Unzipping simple_console_for_windows.zip to create runfiles tree...
Unzip finished.
/c/Users/pvenkat2/Desktop/tensorflow /c/Users/pvenkat2/Desktop/tensorflow
/c/Users/pvenkat2/Desktop/tensorflow
Wed Apr 3 10:43:07 PDT 2019 : === Building wheel
warning: no files found matching '*.pd' under directory '*'
warning: no files found matching '*.dll' under directory '*'
warning: no files found matching '*.h' under directory 'tensorflow\include\tensorflow'
warning: no files found matching '*' under directory 'tensorflow\include\Eigen'
warning: no files found matching '*.h' under directory 'tensorflow\include\google'
warning: no files found matching '*' under directory 'tensorflow\include\third_party'
warning: no files found matching '*' under directory 'tensorflow\include\unsupported'
Traceback (most recent call last):
  File ""setup.py"", line 304, in <module>
    keywords='tensorflow tensor machine learning',
  File ""C:\Users\pvenkat2\AppData\Local\conda\conda\envs\tf_113\lib\distutils\core.py"", line 148, in setup
    dist.run_commands()
  File ""C:\Users\pvenkat2\AppData\Local\conda\conda\envs\tf_113\lib\distutils\dist.py"", line 955, in run_commands
    self.run_command(cmd)
  File ""C:\Users\pvenkat2\AppData\Local\conda\conda\envs\tf_113\lib\distutils\dist.py"", line 974, in run_command
    cmd_obj.run()
  File ""C:\Users\pvenkat2\AppData\Local\conda\conda\envs\tf_113\lib\site-packages\wheel\bdist_wheel.py"", line 179, in run
    self.run_command('build')
  File ""C:\Users\pvenkat2\AppData\Local\conda\conda\envs\tf_113\lib\distutils\cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""C:\Users\pvenkat2\AppData\Local\conda\conda\envs\tf_113\lib\distutils\dist.py"", line 974, in run_command
    cmd_obj.run()
  File ""C:\Users\pvenkat2\AppData\Local\conda\conda\envs\tf_113\lib\distutils\command\build.py"", line 135, in run
    self.run_command(cmd_name)
  File ""C:\Users\pvenkat2\AppData\Local\conda\conda\envs\tf_113\lib\distutils\cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""C:\Users\pvenkat2\AppData\Local\conda\conda\envs\tf_113\lib\distutils\dist.py"", line 974, in run_command
    cmd_obj.run()
  File ""C:\Users\pvenkat2\AppData\Local\conda\conda\envs\tf_113\lib\site-packages\setuptools\command\build_ext.py"", line 75, in run
    _build_ext.run(self)
  File ""C:\Users\pvenkat2\AppData\Local\conda\conda\envs\tf_113\lib\distutils\command\build_ext.py"", line 339, in run
    self.build_extensions()
  File ""C:\Users\pvenkat2\AppData\Local\conda\conda\envs\tf_113\lib\distutils\command\build_ext.py"", line 448, in build_extensions
    self._build_extensions_serial()
  File ""C:\Users\pvenkat2\AppData\Local\conda\conda\envs\tf_113\lib\distutils\command\build_ext.py"", line 473, in _build_extensions_serial
    self.build_extension(ext)
  File ""C:\Users\pvenkat2\AppData\Local\conda\conda\envs\tf_113\lib\site-packages\setuptools\command\build_ext.py"", line 196, in build_extension
    _build_ext.build_extension(self, ext)
  File ""C:\Users\pvenkat2\AppData\Local\conda\conda\envs\tf_113\lib\distutils\command\build_ext.py"", line 558, in build_extension
    target_lang=language)
  File ""C:\Users\pvenkat2\AppData\Local\conda\conda\envs\tf_113\lib\distutils\ccompiler.py"", line 717, in link_shared_object
    extra_preargs, extra_postargs, build_temp, target_lang)
  File ""C:\Users\pvenkat2\AppData\Local\conda\conda\envs\tf_113\lib\distutils\_msvccompiler.py"", line 461, in link
    build_temp = os.path.dirname(objects[0])
IndexError: list index out of range

```
"
27476,TFLiteConverter: converter.convert() does not work,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.5
- Bazel version (if compiling from source): not applicable
- GCC/Compiler version (if compiling from source): not applicable
- CUDA/cuDNN version: not applicable
- GPU model and memory: not applicable


**Describe the current behavior**

I am trying make an new tflite quantized.
The code bellow run normally until the line:
`input_arrays = converter.get_input_arrays()`

Can you help me please?

**_Follows the error:_**

D:\quantization\venv\Scripts\python.exe D:/quantization/venv/quantization.py
TF VERSION:  1.13.1
2019-04-03 15:21:12.605076: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
WARNING:tensorflow:From D:/quantization/venv/quantization.py:25: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.gfile.GFile.
Traceback (most recent call last):
  File ""D:/quantization/venv/quantization.py"", line 30, in <module>
    input_arrays = converter.get_input_arrays()
  File ""D:\quantization\venv\lib\site-packages\tensorflow\lite\python\lite.py"", line 471, in get_input_arrays
    return [_tensor_name(tensor) for tensor in self._input_tensors]
  File ""D:\quantization\venv\lib\site-packages\tensorflow\lite\python\lite.py"", line 471, in <listcomp>
Loading Graph
    return [_tensor_name(tensor) for tensor in self._input_tensors]
  File ""D:\quantization\venv\lib\site-packages\tensorflow\lite\python\convert.py"", line 217, in tensor_name
    return x.name.split("":"")[0]
AttributeError: 'str' object has no attribute 'name'



**Describe the expected behavior**

**Code to reproduce the issue**

```
import numpy as np
import tensorflow as tf
from tensorflow.python.platform import gfile
tf.enable_eager_execution()

import sys
import os
if sys.version_info.major >= 3:
    import pathlib
else:
    import pathlib2 as pathlib

pbfiles_path = ""D:\\Ricardo\\Ambientes Virtuals do Python\\virtualTFr1.13\\Scripts\\pbfiles""
saved_model_dir = str(sorted(pathlib.Path(pbfiles_path).glob(""*.pbtxt"" or ""*.pb""))[-1])

os.environ['CUDA_VISIBLE_DEVICES'] = ""-1""
with tf.Graph().as_default() as graph: 


    with tf.Session() as sess:
        print(""Loading Graph"")

        modelFile = '/pbfiles/saved_model.pb'
        with gfile.FastGFile(modelFile, 'rb') as f:
            converter = tf.lite.TFLiteConverter.from_session(sess,
                                                             input_tensors={""input_1""},
                                                             output_tensors={""conv2d_19/Sigmoid""})
            converter.inference_type = tf.lite.constants.QUANTIZED_UINT8
            input_arrays = converter.get_input_arrays()
            tflite_model = converter.convert()
            open(""quant_converted_model.tflite"", ""wb"").write(tflite_model)
```

"
27474,Shutdown crash while running this script (most times),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MS WINDOWS 10 x64

- TensorFlow installed from (source or binary):
binary

- TensorFlow version (use command below):
2.0.0-a0

- Python version:
Python 3.7.1

- CUDA/cuDNN version:
cuda_10.1.105_418.96_win10.exe
cudnn-10.1-windows10-x64-v7.5.0.56.zip

- GPU model and memory:
Gforce GTX 1050 Ti (DELL laptop)

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

EXECUTION OF LINE:
C:\Users\steph>python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'GIT_VERSION'


**Describe the current behavior**
Durring running of this script, the computer shutdown crashes and reboots.

**Describe the expected behavior**
Finish the 100 Epochs of training and save the model *.h5

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.



[Python Scripts.zip](https://github.com/tensorflow/tensorflow/files/3040051/Python.Scripts.zip)

The data is from Kaggle dogs-vs-cats.zip (file too big to attach: extract all to same subdirectory)

The crash seems to occur most often during the 100 Epochs (2nd) fit.

"
27473,Multiclass AUC,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.13.1.
- Are you willing to contribute it (Yes/No): Yes.



**Describe the feature and the current behavior/state.**
Multi-class AUC is a measure to examine the performance of multi-class classifiers. It is a natural extension of the well known and widely used AUC (Area Under the ROC curve) metric, which is defined for only binary classifiers. This measure was introduced in the paper 'Hand, David J., and Robert J. Till. ""A simple generalization of the area under the ROC curve for multiple class classification problems."" Machine learning 45, no. 2 (2001): 171-186'. It is widely used as the original paper currently has 1435 citation. Multi-class AUC reduces to the standard AUC in case of two classes.

**Will this change the current api? How?**
No. It will be just an additional function which can be used as a metric.

**Who will benefit with this feature?**
Machine learning practitioners and researchers, especially those in the medical field, for whom class separability and false positives are very important.

**Any Other info.**
I wrote the TensorFlow implementation and will be happy to contribute."
27472,10x performance loss when using eager execution with tf.keras.fit(tf.data.Dataset) ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS (Bionic Beaver)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When training `tf.keras.fit(dataset)` with eager execution, where dataset is a `tf.data.Dataset`, I am finding a ~10x performance loss as compared to turning off eager execution. 

**Describe the expected behavior**
In the code attached below, I have tested training with and without eager execution, and with and without tf.data.Dataset.

Here are the training times for one epoch of training on a 4 core CPU:
Eager               + tf.data.Dataset : 219s  - 22ms/step
Without Eager + tf.data.Dataset : 25s  - 3ms/step
Eager               + numpy dataset : 26s  - 259us/sample
Without Eager + numpy dataset : 26s  - 257us/sample

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
import tensorflow as tf
import numpy as np

use_eager     = True
use_TFDataset = True

if not use_eager:
    tf.compat.v1.disable_eager_execution()

# Build dataset
n_data = 10**5
my_data = np.random.random((n_data,10,1))
my_targets = np.random.randint(0,2,(n_data,1))
data = ({'x_input':my_data}, {'target':my_targets})

# Create tf.data.Dataset
BATCH_SIZE = 10
dataset = tf.data.Dataset.from_tensor_slices(data)
dataset = dataset.batch(BATCH_SIZE)
dataset = dataset.prefetch(1)

#Build model
x_input = tf.keras.layers.Input((None,1), name='x_input')
RNN = tf.keras.layers.SimpleRNN(100, name='RNN')(x_input)
hidden = tf.keras.layers.Dense(100, name='hidden')(RNN)
dense = tf.keras.layers.Dense(1, name='target')(hidden)
my_model = tf.keras.models.Model(inputs = [x_input], outputs = [dense])
my_model.compile(optimizer='SGD', loss = 'binary_crossentropy')

# Train model
if use_TFDataset:
    my_model.fit(dataset, epochs = 1, steps_per_epoch=n_data//BATCH_SIZE) # divide by BATCH_SIZE to keep the number of training steps the same
else:
    my_model.fit(x = my_data, y = my_targets, epochs = 1, batch_size= BATCH_SIZE)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27471,Linking of _pywrap_tensorflow_internal.so fails with no obvious cause,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu Linux 18.10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not mobile device, laptop
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.1
- Python version: 3.7.2
- Installed using virtualenv? pip? conda?: source
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 6.5.0
- CUDA/cuDNN version: cuda 10.0 / cudnn 7
- GPU model and memory: GeForce GT 650M

**Describe the problem**

Steps to reproduce:

```
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout v1.13.1
./configure
bazel build --config=opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package
```

Output from building, building fails during linking:

```
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Invocation ID: f796fcbd-0770-4160-9f04-3535b3dcccbd
WARNING: /home/bfh/extra/repos/tensorflow/tensorflow/python/BUILD:2986:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /home/bfh/extra/repos/tensorflow/tensorflow/python/BUILD:77:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/gan/BUILD:136:1: in py_library rule //tensorflow/contrib/gan:losses_impl: target '//tensorflow/contrib/gan:losses_impl' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:76:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:233:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:356:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/bfh/extra/repos/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /home/bfh/extra/repos/tensorflow/tensorflow/python/BUILD:4057:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/bfh/.cache/bazel/_bazel_bfh/530929906cac2e0c8d776b7911283383/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc-7 \
    PATH=/bin:/usr/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/bfh/.pyenv/versions/3.7.2/bin/python \
    PYTHON_LIB_PATH=/home/bfh/.pyenv/versions/3.7.2/lib/python3.7/site-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.0 \
    TF_CUDA_VERSION=10.0 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION='' \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o bazel-out/k8-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so '-Wl,-rpath,$ORIGIN/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' -Lbazel-out/k8-opt/bin/_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow -Lbazel-out/k8-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/k8-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/k8-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Wl,--version-script bazel-out/k8-opt/bin/tensorflow/python/pywrap_tensorflow_internal_versionscript.lds '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..' -Wl,-soname,_pywrap_tensorflow_internal.so -Wl,-z,muldefs -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 -pthread -Wl,-no-as-needed -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -fno-canonical-system-headers -B/usr/bin -Wl,--gc-sections -Wl,@bazel-out/k8-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so-2.params)
Execution platform: @bazel_tools//platforms:host_platform
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 15.002s, Critical Path: 14.56s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
```

So it seems that linking of `_pywrap_tensorflow_internal.so` fails. However I see no way of diagnosing why that linking failed. The setup should be pretty straight-forward, so I am really wondering why it fails.

Any and all help is very appreciated. Thanks in advance."
27470,pip install tf-nightly-2.0-preview No matching distribution found for python 3.7,"Has anything changed with the pip naming of this? I see it was updated 7 hours ago and I think it worked yesterday.

Could also be something to do with the pip chain."
27469,Strange cuda error,"**System information**
-  Ubuntu 16.04
- TensorFlow installed from source
- TensorFlow newest version
- Python version 3.4
- Installed using virtualenv
- Bazel version 1.22.0
- CUDA/cuDNN version 10.0/7
- GPU Tesla K10.G1.8GB

I compiled TensorFlow from source because compute capatibility is only 3.0.
During training I got the strange error below. I guess it is a CUDA problem but I am not sure. Do you have any idea?

**ERROR MESSAGE:**

F tensorflow/stream_executor/cuda/cuda_driver.cc:184] Check failed: is_host_ptr == points_to_host_memory (0 vs. 1)dst pointer is not actually on GPU: 0x4304b80500
Fatal Python error: Aborted

Thread 0x00007f204affd700 (most recent call first):
  File ""/usr/lib/python3.4/threading.py"", line 290 in wait
  File ""/usr/lib/python3.4/queue.py"", line 167 in get
  File ""/opt/lampp/htdocs/venv/lib/python3.4/site-packages/tensorflow/python/summary/writer/event_file_writer.py"", line 159 in run
  File ""/usr/lib/python3.4/threading.py"", line 920 in _bootstrap_inner
  File ""/usr/lib/python3.4/threading.py"", line 888 in _bootstrap

Thread 0x00007f2050d9f700 (most recent call first):
  File ""/usr/lib/python3.4/threading.py"", line 290 in wait
  File ""/usr/lib/python3.4/queue.py"", line 167 in get
  File ""/opt/lampp/htdocs/venv/lib/python3.4/site-packages/tensorflow/python/summary/writer/event_file_writer.py"", line 159 in run
  File ""/usr/lib/python3.4/threading.py"", line 920 in _bootstrap_inner
  File ""/usr/lib/python3.4/threading.py"", line 888 in _bootstrap

Thread 0x00007f2088ac5700 (most recent call first):
  File ""/opt/lampp/htdocs/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1410 in _call_tf_sessionrun
  File ""/opt/lampp/htdocs/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1322 in _run_fn
  File ""/opt/lampp/htdocs/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1337 in _do_call
  File ""/opt/lampp/htdocs/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1331 in _do_run
  File ""/opt/lampp/htdocs/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1155 in _run
  File ""/opt/lampp/htdocs/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 932 in run
  File ""/opt/lampp/htdocs/venv/lib/python3.4/site-packages/tensorflow/python/training/saver.py"", line 1286 in restore
  File ""/opt/l2019-04-03 17:13:00.027397: F tensorflow/stream_executor/cuda/cuda_driver.cc:184] Check failed: is_host_ptr == points_to_host_memory (0 vs. 1)dst pointer is not actually on GPU: 0x4304b81100


"
27468,tf-nightly: cannot find -ltensorflow_framework (new naming convention?),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian Jessie, macOS Mojave
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.14.1-dev20190402
- Python version: 2.7.9
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

When building [Horovod](https://github.com/horovod/horovod), we link against the `libtensorflow_framework.so` using `tf.sysconfig.get_link_flags()`.  

The most recent nightly builds (starting 2019-04-02) have replaced `libtensorflow_framework.so` with two separate .so files: `libtensorflow_framework.so.1` and `libtensorflow_framework.so.1.13.1`.  However, `tf.sysconfig.get_link_flags()` still references [-ltensorflow_framework](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/sysconfig.py#L78), which causes our build process to fail.

We found that replacing this line with `flags.append('-l:libtensorflow_framework.so.1')` fixed the issue with gcc.

My question is: is this intended to be the new library organization for TensorFlow going forward? If we were to submit a PR to update `tf.sysconfig` to use the correct libraries filenames, is that expected to work for the foreseeable future, or are these new artifacts temporary or accidents?  Alternatively, could your build process be updated to generate symlinks so `tf.sysconfig` will continue to work as is?

Thanks.
"
27467,OOM ERROR  when computing_gradients on single gpu(TITAN Xp 12gb),"I was trying to training network with 4k resolution and I'm using adam optimizer  and compute gradient are not fitting in single seperate gpu
**My system config is 4GPU(TITAN Xp 12gb) and 48 cpu processor and 128gb ram**

Can somebody help me to solve how compute gradient for single graph with batch 1 on multi gpu without getting OOM
Below is the stack trace of my error
**2019-04-03 16:43:30.883434: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_2_bfc) ran out of memory trying to allocate 384.00MiB.  Current allocation summary follows.
2019-04-03 16:43:30.883548: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 38, Chunks in use: 37. 9.5KiB allocated for chunks. 9.2KiB in use in bin. 6.3KiB client-reques
ted in use in bin.
2019-04-03 16:43:30.883579: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 24, Chunks in use: 24. 12.0KiB allocated for chunks. 12.0KiB in use in bin. 12.0KiB client-req
uested in use in bin.
2019-04-03 16:43:30.883598: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 50, Chunks in use: 50. 50.2KiB allocated for chunks. 50.2KiB in use in bin. 50.0KiB client-req
uested in use in bin.
2019-04-03 16:43:30.883617: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 34, Chunks in use: 34. 68.0KiB allocated for chunks. 68.0KiB in use in bin. 68.0KiB client-req
uested in use in bin.
2019-04-03 16:43:30.883635: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 23, Chunks in use: 23. 94.8KiB allocated for chunks. 94.8KiB in use in bin. 94.8KiB client-req
uested in use in bin.
2019-04-03 16:43:30.883652: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 9, Chunks in use: 9. 72.0KiB allocated for chunks. 72.0KiB in use in bin. 72.0KiB client-requested in use in bin.
2019-04-03 16:43:30.883669: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks: 2, Chunks in use: 2. 36.0KiB allocated for chunks. 36.0KiB in use in bin. 36.0KiB client-requested in use in bin.
2019-04-03 16:43:30.883685: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-04-03 16:43:30.883703: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks: 6, Chunks in use: 6. 384.0KiB allocated for chunks. 384.0KiB in use in bin. 384.0KiB client-requested in use in bin.
2019-04-03 16:43:30.883721: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks: 5, Chunks in use: 5. 704.0KiB allocated for chunks. 704.0KiB in use in bin. 704.0KiB client-requested in use in bin.
2019-04-03 16:43:30.883737: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144):        Total Chunks: 8, Chunks in use: 8. 2.03MiB allocated for chunks. 2.03MiB in use in bin. 2.03MiB client-requested in use in bin.
2019-04-03 16:43:30.883754: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288):        Total Chunks: 7, Chunks in use: 7. 3.81MiB allocated for chunks. 3.81MiB in use in bin. 3.81MiB client-requested in use in bin.
2019-04-03 16:43:30.883771: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576):       Total Chunks: 11, Chunks in use: 11. 11.00MiB allocated for chunks. 11.00MiB in use in bin. 11.00MiB client-requested in use in bin.
2019-04-03 16:43:30.883795: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152):       Total Chunks: 9, Chunks in use: 9. 19.50MiB allocated for chunks. 19.50MiB in use in bin. 19.50MiB client-requested in use in bin.
2019-04-03 16:43:30.883813: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304):       Total Chunks: 15, Chunks in use: 15. 80.00MiB allocated for chunks. 80.00MiB in use in bin. 80.00MiB client-requested in use in bin.
2019-04-03 16:43:30.883830: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608):       Total Chunks: 27, Chunks in use: 27. 311.00MiB allocated for chunks. 311.00MiB in use in bin. 311.00MiB client-requested in use in bin.
2019-04-03 16:43:30.883848: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216):      Total Chunks: 24, Chunks in use: 23. 568.00MiB allocated for chunks. 544.00MiB in use in bin. 544.00MiB client-requested in use in bin.
2019-04-03 16:43:30.883864: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432):      Total Chunks: 35, Chunks in use: 35. 1.67GiB allocated for chunks. 1.67GiB in use in bin. 1.65GiB client-requested in use in bin.
2019-04-03 16:43:30.883881: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864):      Total Chunks: 16, Chunks in use: 16. 1.46GiB allocated for chunks. 1.46GiB in use in bin. 1.46GiB client-requested in use in bin.
2019-04-03 16:43:30.883897: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728):     Total Chunks: 14, Chunks in use: 14. 2.56GiB allocated for chunks. 2.56GiB in use in bin. 2.56GiB client-requested in use in bin.
2019-04-03 16:43:30.883913: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456):     Total Chunks: 8, Chunks in use: 7. 4.43GiB allocated for chunks. 4.12GiB in use in bin. 4.12GiB client-requested in use in bin.
2019-04-03 16:43:30.883930: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 384.00MiB was 256.00MiB, Chunk State:
2019-04-03 16:43:30.883953: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 312.18MiB | Requested Size: 0B | in_use: 0, prev:   Size: 384.00MiB | Requested Size: 384.00MiB | in_use: 1
2019-04-03 16:43:30.883970: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f3c000000 of size 1280
2019-04-03 16:43:30.883983: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f3c000500 of size 256
2019-04-03 16:43:30.883995: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f3c000600 of size 256
019-04-03 16:43:30.884175: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f84c25000 of size 6912
2019-04-03 16:43:30.884187: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f84c26b00 of size 589824
2019-04-03 16:43:30.884198: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f84cb6b00 of size 294912
2019-04-03 16:43:30.884210: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f84cfeb00 of size 256
2019-04-03 16:43:30.884221: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f84cfec00 of size 4096
2019-04-03 16:43:30.884233: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f84cffc00 of size 8388608
2019-04-03 16:43:30.884246: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f854ffc00 of size 8192
2019-04-03 16:43:30.884259: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f85501c00 of size 8192
2019-04-03 16:43:30.884271: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f85503c00 of size 20480
2019-04-03 16:43:30.884283: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f85508c00 of size 256
2019-04-03 16:43:30.884295: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f85508d00 of size 2097152
2019-04-03 16:43:30.884306: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f85708d00 of size 8192
2019-04-03 16:43:30.884318: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f8570ad00 of size 1024
2019-04-03 16:43:30.884331: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f8570b100 of size 16777216
2019-04-03 16:43:30.884343: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f8670b100 of size 2048
2019-04-03 16:43:30.884355: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f8670b900 of size 67108864
2019-04-03 16:43:30.884367: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f8a70b900 of size 4096
2019-04-03 16:43:30.884379: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f8a70c900 of size 134217728
2019-04-03 16:43:30.884391: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9270c900 of size 4194304
2019-04-03 16:43:30.884402: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f92b0c900 of size 2048
2019-04-03 16:43:30.884414: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f92b0d100 of size 9437184
2019-04-03 16:43:30.884426: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9340d100 of size 2048
2019-04-03 16:43:30.884437: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9340d900 of size 4194304
2019-04-03 16:43:30.884449: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9380d900 of size 4194304
2019-04-03 16:43:30.884460: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f93c0d900 of size 2048
2019-04-03 16:43:30.884471: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f93c0e100 of size 9437184
2019-04-03 16:43:30.884482: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9450e100 of size 2048
2019-04-03 16:43:30.884493: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9450e900 of size 4194304
2019-04-03 16:43:30.884505: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9490e900 of size 2097152
2019-04-03 16:43:30.884516: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f94b0e900 of size 2048
2019-04-03 16:43:30.884527: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f94b0f100 of size 9437184
2019-04-03 16:43:30.884538: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9540f100 of size 2048
2019-04-03 16:43:30.884549: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9540f900 of size 4194304
2019-04-03 16:43:30.884561: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9580f900 of size 1048576
2019-04-03 16:43:30.884572: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9590f900 of size 1024
2019-04-03 16:43:30.884584: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f9590fd00 of size 2359296
2019-04-03 16:43:30.884595: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f95b4fd00 of size 1024
2019-04-03 16:43:30.884607: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f95b50100 of size 1048576
2019-04-03 16:43:30.884618: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f95c50100 of size 1048576
2019-04-03 16:43:30.884629: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f95d50100 of size 1024
2019-04-03 16:43:30.884640: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f95d50500 of size 2359296
2019-04-03 16:43:30.884651: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f95f90500 of size 1024
2019-04-03 16:43:30.884662: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f95f90900 of size 1048576
2019-04-03 16:43:30.884673: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96090900 of size 1048576
2019-04-03 16:43:30.884684: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96190900 of size 1024
2019-04-03 16:43:30.884695: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96190d00 of size 2359296
2019-04-03 16:43:30.884706: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f963d0d00 of size 1024
2019-04-03 16:43:30.884717: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f963d1100 of size 1048576
2019-04-03 16:43:30.884728: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f964d1100 of size 1048576
2019-04-03 16:43:30.884739: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f965d1100 of size 1024
2019-04-03 16:43:30.884750: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f965d1500 of size 2359296
2019-04-03 16:43:30.884761: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96811500 of size 1024
2019-04-03 16:43:30.884772: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96811900 of size 1048576
2019-04-03 16:43:30.884783: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96911900 of size 1048576
2019-04-03 16:43:30.884794: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96a11900 of size 1024
2019-04-03 16:43:30.884805: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96a11d00 of size 2359296
2019-04-03 16:43:30.884817: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96c51d00 of size 1024
2019-04-03 16:43:30.884828: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96c52100 of size 1048576
2019-04-03 16:43:30.884840: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f2f96d52100 of size 524288
2019-04-03 16:43:30.890317: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 10 Chunks of size 6291456 totalling 60.00MiB
2019-04-03 16:43:30.890331: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 8388608 totalling 8.00MiB
2019-04-03 16:43:30.890344: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 9437184 totalling 27.00MiB
2019-04-03 16:43:30.890358: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 23 Chunks of size 12582912 totalling 276.00MiB
2019-04-03 16:43:30.890371: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 16777216 totalling 16.00MiB
2019-04-03 16:43:30.890384: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 22 Chunks of size 25165824 totalling 528.00MiB
2019-04-03 16:43:30.890398: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 50320896 totalling 47.99MiB
2019-04-03 16:43:30.890411: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 31 Chunks of size 50331648 totalling 1.45GiB
2019-04-03 16:43:30.890425: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 62914560 totalling 180.00MiB
2019-04-03 16:43:30.890439: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 67108864 totalling 64.00MiB
2019-04-03 16:43:30.890452: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 88080384 totalling 84.00MiB
2019-04-03 16:43:30.890465: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 14 Chunks of size 100663296 totalling 1.31GiB
2019-04-03 16:43:30.890479: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 134217728 totalling 128.00MiB
2019-04-03 16:43:30.890895: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 13 Chunks of size 201326592 totalling 2.44GiB
2019-04-03 16:43:30.890927: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 402653184 totalling 1.12GiB
2019-04-03 16:43:30.890942: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 805306368 totalling 3.00GiB
2019-04-03 16:43:30.890958: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 10.77GiB
2019-04-03 16:43:30.890975: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:
Limit:                 11916971213
InUse:                 11564461824
MaxInUse:              11564461824
NumAllocs:                     371
MaxAllocSize:            805306368

2019-04-03 16:43:30.891029: W tensorflow/core/common_runtime/bfc_allocator.cc:279] **************************************************************************************************__
Traceback (most recent call last):
  File ""trianpy"", line 216, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""trian.py"", line 172, in main
    results = sess.run(fetches,options=run_options)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.
         [[Node: percept_loss/perceptual_loss/vgg_19/Relu_19/_505 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:2"", send_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device_incarnation=1, tensor_name=""edge_1462_percept_loss/perceptual_loss/vgg_19/Relu_19"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:2""]()]]
         [[Node: club_loss/add_1/_1623 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device_incarnation=1, tensor_name=""edge_6336_club_loss/add_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]**
 "
27465,[DOC 1.x] save_keras_model requires eager mode,"**System information**
- TensorFlow version: 1.13.1
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/contrib/saved_model/save_keras_model?hl=en


**Describe the documentation issue**
I followed [this tutorial](https://www.tensorflow.org/tutorials/keras/save_and_restore_models) and this [API reference page](https://www.tensorflow.org/api_docs/python/tf/contrib/saved_model/save_keras_model?hl=en) to save a trained Keras model as a SavedModel, which in my use case is required to upload it to GCP ML Engine.
But I encounter a non documented `AssertionError` specifying I should use eager mode.
This seems related to [this commit](https://github.com/tensorflow/tensorflow/commit/6603c69fa71d6ebdee717863079ca34308c9ddb1).
Did I understand something wrong or should this compatibility requirements also be documented for `save_keras_model` (at least with `serving_only` set to `True`) ?"
27463,[TF 2.0 API Docs] tf.keras.Sequential,"**System information**
- TensorFlow version: 2.0 alpha
- Doc Link:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Sequential
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/sequential.py

**Describe the documentation issue**

**- Description:**

The order of _Properties_ and _Methods_ is alphabetical, probably by TF doc design. With a large doc for an arguably popular module such as `tf.keras.Sequential` navigating around it may be confusing to the user. Perhaps we should start with the most important ones e.g. `compile` and `fit` under Methods. Since `tf.keras.Sequential`, like many other modules/classes, expands on `keras.Sequential` it would still be logical to list certain Properties, Methods etc in the beginning (at the top of the page) in order of importance for better UX. See: https://keras.io/models/sequential/ as a good example where these are not in alphabetical order.

Also, in the beginning after a short intro (""inherits from `Model`... a linear stack of layers""), a link to ""Build a Simple Model"" (with `tf.keras.Sequential`) in TensorFlow would be cool for those who are new to `(tf.)Keras`/TF - https://www.tensorflow.org/alpha/guide/keras/overview#sequential_model. Also, the official Keras.io docs include a URL in https://keras.io/models/sequential/ to ""Getting started with the Keras Sequential model"": https://keras.io/getting-started/sequential-model-guide.

**- Examples:**

There already is an example right in the beginning. Maybe adding adding a separate link to or copying an example from this Sequential notebook would be cool too - https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/quickstart/beginner.ipynb (source: https://www.tensorflow.org/alpha/guide/keras/overview#sequential_model)

**- Parameters**

Inconsistent. Some reformatting may be needed e.g. input_shape -> `input_shape`. 
Also, both _Arguments_ and _Args_ are used throughout the doc.

**- Returns, Raises:**

Sometimes exist, sometimes not - creates a UX issue because of inconsistency.

**- Visuals:**

A simple one similar to https://www.tensorflow.org/alpha/guide/keras/functional would be appreciated.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
For sure."
27461,[TF 2.0 API Docs] tf.keras.Model,"**System information**
- TensorFlow version: 2.0 alpha
- Doc Link: 
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py

**Describe the documentation issue**

**- Links:**

A link to **_Keras Functional API in TensorFlow_** would be appreciated for those who are new to `(tf.)Keras` and TensorFlow 1.x and 2.0 - https://www.tensorflow.org/alpha/guide/keras/functional. E.g. See this Keras Model class API doc: https://keras.io/models/model/#model-class-api.

**- Description:**

Taking into account this is a 'heavy' module it may not be easy to write good docs for `tf.keras.Model`. Since `tf.keras` is a crucial API to TF 2.0, so let's make the docs a delight to read. The current documentation for `tf.keras.Model` is a bit incomprehensible for a novice or experienced user.

In terms of user experience for someone who is new or not new to TF, maybe the description should be improved and follow the keras.io docs more closely. For instance, arguments under `compile` were apparently copy-pasted from `keras.Model` docs (https://keras.io/models/model/). However, you have to scroll all the way down to see them here: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model#compile (_update: maybe because Methods etc are listed in alphabetical order which is not intuitive_).

The description at keras.io is neater and more organized imo. It starts with a short description and jumps to args from `compile`. I'd suggest we follow the same structure.

Also, as in https://keras.io/models/model/, we should include a link to the guide to **_Keras Functional API in TensorFlow_** - https://www.tensorflow.org/alpha/guide/keras/functional - which is quite well written.

**- Examples:**

Not enough examples - they are mentioned here and there. See UX issues above under Description. Recommend to rewrite it to follow the original `keras.Model` module - https://keras.io/models/model/#model-class-api - along with examples. 

**- Parameters, Returns, Raises:**

UX issues - see Description above.

**- Visuals:**

Recommend to add visuals from https://www.tensorflow.org/alpha/guide/keras/functional

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Yes, let's make `tf.keras` docs awesome."
27460,Reloading trained model gives random predictions every time (tf.keras),"tensorflow-version: 1.13.1
I have trained model using **tf.keras**. It has custom layers and is doing multi tasking. Below is code for same:

```
trainable_model = keras.models.load_model(
            path, custom_objects={'Attention': Attention, 'MultiTaskLoss': MultiTaskLoss}, compile=True) # tried with compile=False
        trainable_model.summary()
        input = trainable_model.inputs[0]
        i_output = trainable_model.get_layer('i_output').output
        t_output = trainable_model.get_layer('t_output').output
        testable_model = keras.models.Model(
            inputs=input, outputs=[i_output, t_output])
       # Tried without below for loop.
        for l in testable_model.layers:
            l_name = l.name
            l.set_weights(trainable_model.get_layer(name=l_name).get_weights())
        testable_model.compile('adam', loss={
                               'i_output': 'categorical_crossentropy', 't_output': 'categorical_crossentropy'})
        return testable_model
```
But when I predict using testable_model, predictions are different and random everytime, as if weights are totally different from trained model.
Model summary is as follows:
**Trainable model**:
![trainable_model](https://user-images.githubusercontent.com/25479695/55475976-f200c100-5632-11e9-9819-2d0bee121744.png)

**Testable_model**:
![testable_model](https://user-images.githubusercontent.com/25479695/55475996-047afa80-5633-11e9-8723-2c03665411b8.png)



"
27459,ImportError: /usr/lib/x86_64-linux-gnu/libstdc++,"During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""asrserver.py"", line 10, in <module>
    import keras
  File ""/usr/local/lib/python3.7/site-packages/keras/__init__.py"", line 3, in <module>
    from . import utils
  File ""/usr/local/lib/python3.7/site-packages/keras/utils/__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""/usr/local/lib/python3.7/site-packages/keras/utils/conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""/usr/local/lib/python3.7/site-packages/keras/backend/__init__.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/local/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/local/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /usr/local/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
27458,Not able to decode,"Hi,
I am trying to fit my own model in this app. When I speak into the app, it extracts & prints the MFCC features, but crashes afterwards giving the following error:

04-03 12:56:16.754 24654-24815/org.tensorflow.demo E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[SeqLen], outputs:[SparseToDense]

--------- beginning of crash
04-03 12:56:16.755 24654-24815/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: Thread-7556
Process: org.tensorflow.demo, PID: 24654
java.lang.IllegalArgumentException: Expects arg[0] to be int32 but float is provided
at org.tensorflow.Session.run(Native Method)
at org.tensorflow.Session.access$100(Session.java:48)
at org.tensorflow.Session$Runner.runHelper(Session.java:314)
at org.tensorflow.Session$Runner.run(Session.java:264)
at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:228)
at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)
at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:187)
at org.tensorflow.demo.SpeechActivity.recognize(SpeechActivity.java:229)
at org.tensorflow.demo.SpeechActivity.access$100(SpeechActivity.java:48)
at org.tensorflow.demo.SpeechActivity$3.run(SpeechActivity.java:193)
at java.lang.Thread.run(Thread.java:818)

Can someone please help me fix this? I'm not able to understand where that arg[0] is pointing too.
If anyone has a solution, please do share. Thanks in advance."
27457,Workaround for self-attention using tf.matmul,"We are currently trying to convert a tensorflow model to a tf-lite graph. In particular we are working on a transformer which uses self-attention. The problem with that operation seems to be that `tf.matmul` receives two non-constant inputs which is, according to [the docs](https://www.tensorflow.org/lite/guide/ops_compatibility), not allowed:

> `tf.matmul` - *as long as the second argument is constant and transposition is not used*

This seems like a major limitation for the self-attention mechanism. Is there any workaround available for this?

(see also [stackoverflow question](https://stackoverflow.com/questions/55491752/workaround-for-using-tf-matmul-with-two-non-constant-inputs))"
27456,LazyAdamOptimizer  is not faster than AdamOptimizer ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux redhat
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
source
- TensorFlow version (use command below):
1.12
- Python version:
Python 3.6.8 |Anaconda,
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
There are 2.7 hundred million variables in my embedding files.And instread of adamOptimer,i used lazyadam.But it cost the same time as adamOptimizer
**Describe the expected behavior**
it should be faster than adamOptimizer when sparse update.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
return tf.estimator.DNNClassifier(
        model_dir=FLAGS.model_dir,
        feature_columns=deep_columns,
        #optimizer=tf.train.AdamOptimizer(learning_rate=0.001),
        optimizer =  tf.contrib.opt.LazyAdamOptimizer(learning_rate = 0.001),
        hidden_units=hidden_units,
        config=my_checkpointing_config
```
and it cost 200s for every 100 steps ,the same time as AdamOptimizer


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27455,TF2.0 gradient problem of using tf.nn.relu in tf.keras.Model.,"**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version: 2.0.0-alpha0
- Python version: 3.6.8

**Describe the current behavior**
I built a keras model with only a `tf.nn.relu`, but the gradient seems to be `None` after being decorated by `@tf.function`

**Code to reproduce the issue**
1. `tf.nn.relu` + `tf.keras.Model` + `@tf.function` (this is the only case that produce `None` gradient)
```python
import tensorflow as tf

z = tf.keras.Input(())
h = tf.nn.relu(z)
m = tf.keras.Model(z, h)

@tf.function
def f(x):  # with @tf.function
    with tf.GradientTape() as t:
        t.watch(x)
        z = m(x ** 2)
    return t.gradient(z, x)

print(f(tf.convert_to_tensor(10.0)))

>>> None
```

1.2 `tf.nn.relu` + `tf.keras.Model` without `@tf.function`
```python
def f(x):  # without @tf.function
    with tf.GradientTape() as t:
        t.watch(x)
        z = m(x ** 2)
    return t.gradient(z, x)

print(f(tf.convert_to_tensor(10.0)))

>>> tf.Tensor(20.0, shape=(), dtype=float32)
```

2. `tf.keras.layers.ReLU()` + `tf.keras.Model` + `@tf.function`
```python
import tensorflow as tf

z = tf.keras.Input(())
h = tf.keras.layers.ReLU()(z)
m = tf.keras.Model(z, h)

@tf.function
def f(x):  # with @tf.function
    with tf.GradientTape() as t:
        t.watch(x)
        z = m(x ** 2)
    return t.gradient(z, x)

print(f(tf.convert_to_tensor(10.0)))

>>> tf.Tensor(20.0, shape=(), dtype=float32)
```

2.2 `tf.keras.layers.ReLU()` + `tf.keras.Model` without `@tf.function`
```python
def f(x):  # without @tf.function
    with tf.GradientTape() as t:
        t.watch(x)
        z = m(x ** 2)
    return t.gradient(z, x)

print(f(tf.convert_to_tensor(10.0)))

>>> tf.Tensor(20.0, shape=(), dtype=float32)
```

3. only `tf.nn.relu`
```python
import tensorflow as tf
m = tf.nn.relu

@tf.function
def f(x):  # with @tf.function
    with tf.GradientTape() as t:
        t.watch(x)
        z = m(x ** 2)
    return t.gradient(z, x)

print(f(tf.convert_to_tensor(10.0)))

>>> tf.Tensor(20.0, shape=(), dtype=float32)
```

So, I think its the problem between `tf.nn.relu` and `tf.keras.Model`? Besides, `tf.nn.tanh` has the same problem."
27453,no module namedtensorflow.python tensorflow is not a package,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
27452,Import tensorflow not working on RHEL6,"**System information**
- Red Hat Enterprise Linux Server release 6.8 (Santiago)
- Install using Virtualenv : https://www.tensorflow.org/install/pip
- TensorFlow version: latest
- Python version: 3.6.4
- Installed using virtualenv
- Bazel version (if compiling from source): Not compiling from source
- GCC/Compiler version (if compiling from source): gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-18)

**Describe the problem**

We have installed Tensorflow using Virtualenv.
Below steps which we followed from our end:

virtualenv --system-site-packages -p python3 venv
[root@abi venv]$ ls
bin  include  lib  lib64  pip-selfcheck.json
source ~/tensorflow/venv/bin/activate
pip3 install --upgrade tensorflow // in venv environment
 
After installation we tried to import ""tensorflow"" in python like ""import tensorflow"".
Facing below issue:

`(venv) [root@abinaya-simics-r6 ~]$ python
Python 3.6.4 (default, Oct  8 2018, 06:53:21)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/root/tensorflow/venv/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/root/tensorflow/venv/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /lib64/libc.so.6: version `GLIBC_2.16' not found (required by /root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/root/tensorflow/venv/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/root/tensorflow/venv/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /lib64/libc.so.6: version `GLIBC_2.16' not found (required by /root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>
`

As per the issue we could understand that ""/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so"" need ""GLIBC 2.14 atleast.

But available GLIBC version in RHEL6 is:

`[root@abinaya-simics-r6 ~]$ strings /usr/lib64/libstdc++.so.6 | grep GLIBC_
GLIBC_2.2.5
GLIBC_2.3
GLIBC_2.4
GLIBC_2.3.2
`
Could you please confirm us whether the tensorflow really need latest version of GLIBC.
Provide some Doc on this case for better understand."
27451,ImportError: DLL load failed: Access is denied.,"

**System information**  

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit  
- TensorFlow installed from (source or binary): pip  
- TensorFlow version: 1.12.0  
- Python version: 3.6.7 64bit  
- Installed using virtualenv? pip? conda?: No virtualenv. Direct install with pip. No conda. Virtualenv tried, but result is the same.  
- CUDA/cuDNN version: cuda_9.0.176_win10, cudnn-9.0-windows10-x64-v7.5.0.56  
- GPU model and memory: RTX2070 8G  
- CPU model: intel i5-9400f (CPU without integrated GPU)  



**Describe the problem**

To be clear, this is NOT a issue about the dll files that can not be found. Files are there, but something is wrong with the previlige.

I recently built a new windows 10 PC and installed tensorflow on it. The installation was successful, but I had to run the code with Administrator. Otherwise a simple import code like ""import tensorflow as tf"" would result in ""ImportError: DLL load failed: Access is denied"" if I try to run it without ""Run as Administrator"". The code did work flawlessly if I choose to run as admin though. But on my old notebook I could execute without admin previlige.

The installations on my new and old computer were the same, so the software version and system environment variables should not be the problem. I am wondering why this is happening and is there any way I can get rid of the ""run as admin"" process which is rather danger and annoying for every time I want to run the code.



**Any other info / logs**

```
3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD64)]
Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Access is denied.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""hello tensorflow.py"", line 12, in <module>
    import tensorflow as tf
  File ""C:\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Access is denied.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```"
27450,[TF 2.0 Build] Bazel version checking bug when compiling from source,"**System information**
- OS: macOSMojave 10.14.4 Beta
- Device: MacBook Pro
- TensorFlow version: 2.0.0 alpha0
- Python version: 3.7.3
- Bazel version (if compiling from source): 0.24.0
- GCC/Compiler version (if compiling from source): Apple LLVM 10.0
- CUDA/cuDNN version: None
- GPU model and memory: Intel HD640


**Problem Description**

**Bazel 0.24.0 is correctly installed in my system but tensorflow failed to recognize it and logged the following when compiling from source:**
`
You have bazel 0.24.0 installed.
Please downgrade your bazel installation to version 0.24 or lower to build TensorFlow! To downgrade: download the installer for the old version (from https://github.com/bazelbuild/bazel/releases) then run the installer.
`

This can be reproduced by running` ./configure` when following the official **'Build From Source'** instructions. I have investigated the issue and realize that it is caused by incorrect conversion of bazel version info to integer in **'configure.py'**.

After adding print statements at line 472, 474, 476 of **'configure.py'**, such bug is easy to see:
```python3
min_version_int = convert_version_to_int(min_version)
print('min version required:', min_version_int)
curr_version_int = convert_version_to_int(curr_version)
print('currently installed version:', curr_version_int)
max_version_int = convert_version_to_int(max_version)
print('maximum version allowed:', max_version_int)
```
**Then  running** `./configure` **again produces:**
```
min version required: 19000
currently installed version: 24000
maximum version allowed: 24
You have bazel 0.24.0 installed.
Please downgrade your bazel installation to version 0.24 or lower to build TensorFlow! To downgrade: download the installer for the old version (from https://github.com/bazelbuild/bazel/releases) then run the installer.
```
The version checking logic in **'configure.py'** relies on such conversion.
Below is the version checking logic from line 475 to 493 in  **'configure.py'**:
```python3
# Check if current bazel version can be detected properly.
  if not curr_version_int:
    print('WARNING: current bazel installation is not a release version.')
    print('Make sure you are running at least bazel %s' % min_version)
    return curr_version

  print('You have bazel %s installed.' % curr_version)

  if curr_version_int < min_version_int:
    print('Please upgrade your bazel installation to version %s or higher to '
          'build TensorFlow!' % min_version)
    sys.exit(1)
  if (curr_version_int > max_version_int and
      'TF_IGNORE_MAX_BAZEL_VERSION' not in os.environ):
    print('Please downgrade your bazel installation to version %s or lower to '
          'build TensorFlow! To downgrade: download the installer for the old '
          'version (from https://github.com/bazelbuild/bazel/releases) then '
          'run the installer.' % max_version)
    sys.exit(1)
```

I am very willing to contribute to a fix to this issue. I will create a pull request soon after I fully tested my solution on this.
Thank you very much.

"
27449,[Enhancement] Automatically chose either memory or storage to cache in tf.data API,"**System information**
- TensorFlow version (you are using): 1.13
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
At the moment, one has to keep in mind what the systems resources will be when using the tf.data pipeline API. Specifically, when using the https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache operation, I have to ask myself whether it's worth caching the data and whether to do so in memory or storage.

As I both switch systems and datasets a lot I often chose wrong and end up with filled swap memory or extremely slow epochs because I have caching disabled.

What I would like to see, although I understand the complexities involved, is some sort of dynamic cache where the systems itself decides what to use (memory, storage or no caching), depending on available resources and the computations to cache.

A intermediate solution might be one where the user can select how much memory (percentage) to use to cache and for the remaining required cache to use storage so we get some hybrid memory/storage solution.

**Will this change the current api? How?**
It will either add to it or change the current cache() parameters. How and what's best here is not up to me.

**Who will benefit with this feature?**
Anyone using the tf.data API looking to improve performance."
27448,How can I use Queue with tf 2.0?,"How can I use Queue with tf 2.0?

~~~
from __future__ import print_function

import tensorflow as tf

import time

_placeholders = [
            tf.keras.backend.placeholder([None, None], dtype=tf.int32, name='inputs'),
            tf.keras.backend.placeholder([None], dtype=tf.int32, name='input_lengths'),
            tf.keras.backend.placeholder([None], dtype=tf.float32, name='loss_coeff'),
            tf.keras.backend.placeholder([None, None,10], dtype=tf.float32, name='mel_targets'),
            tf.keras.backend.placeholder([None, None, 10], dtype=tf.float32, name='linear_targets'),
        ]
dtypes = [tf.int32, tf.int32, tf.float32, tf.float32, tf.float32]

queue = tf.queue.FIFOQueue(capacity=10, dtypes=dtypes)
enque = queue.enqueue(_placeholders)
~~~"
27447,How can I use Queue with tf 2.0?,"How can I use Queue with tf 2.0?

~~~
from __future__ import print_function

import tensorflow as tf

import time

_placeholders = [
            tf.keras.backend.placeholder([None, None], dtype=tf.int32, name='inputs'),
            tf.keras.backend.placeholder([None], dtype=tf.int32, name='input_lengths'),
            tf.keras.backend.placeholder([None], dtype=tf.float32, name='loss_coeff'),
            tf.keras.backend.placeholder([None, None,10], dtype=tf.float32, name='mel_targets'),
            tf.keras.backend.placeholder([None, None, 10], dtype=tf.float32, name='linear_targets'),
        ]
dtypes = [tf.int32, tf.int32, tf.float32, tf.float32, tf.float32]

queue = tf.queue.FIFOQueue(capacity=10, dtypes=dtypes)
enque = queue.enqueue(_placeholders)
~~~"
27445,Custom RNN cell's internal layers are not built properly,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MaxOSX 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below):
`tf.version.VERSION = '2.0.0-dev20190402'`
`tf.version.GIT_VERSION = 'v1.12.0-11462-gf6732b0261'`
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
When creating a custom RNN cell containing a few layers (created in the constructor), these internal layers are not properly built the first time the cell is used.

**Describe the expected behavior**
I expect custom cells to behave like custom layers: the first time the cell is used, all of its internal layers should be automatically built.

**Code to reproduce the issue**
The following code fails because the internal `SimpleRNNCell` is not built before it is used. If you uncomment the two lines that define the `build()` method, then everything works fine. But this should not be required (just like it is not required for custom layers):

```python
import numpy as np
from tensorflow import keras

class MyCell(keras.layers.Layer):
    def __init__(self, units, **kwargs):
        super().__init__(**kwargs)
        self.units = units
        self.state_size = units
        self.simple_rnn_cell = keras.layers.SimpleRNNCell(self.units)
#    def build(self, step_input_shape):
#        self.simple_rnn_cell.build(step_input_shape)
    def call(self, inputs, states):
        outputs, new_states = self.simple_rnn_cell.call(inputs, states)
        return outputs, new_states

model = keras.models.Sequential([
    keras.layers.RNN(MyCell(20), return_sequences=True),
    keras.layers.RNN(MyCell(20), return_sequences=True),
    keras.layers.TimeDistributed(keras.layers.Dense(1))
])

X = np.random.randn(1000, 30, 10)
Y = np.random.randn(1000, 30, 1)
model.compile(loss=""mse"", optimizer=""adam"")
history = model.fit(X, Y) # <= AttributeError: 'SimpleRNNCell' object has no attribute 'kernel'
```

**Other info / logs**
Here is the full stacktrace:

```python
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../tensorflow/python/keras/engine/training.py"", line 807, in fit
    shuffle=shuffle)
  File "".../tensorflow/python/keras/engine/training.py"", line 2417, in _standardize_user_data
    self._set_inputs(cast_inputs)
  File "".../tensorflow/python/keras/engine/training.py"", line 2646, in _set_inputs
    outputs = self(inputs, **kwargs)
  File "".../tensorflow/python/keras/engine/base_layer.py"", line 601, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File "".../tensorflow/python/keras/engine/sequential.py"", line 256, in call
    outputs = layer(inputs, **kwargs)
  File "".../tensorflow/python/keras/layers/recurrent.py"", line 639, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File "".../tensorflow/python/keras/engine/base_layer.py"", line 601, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File "".../tensorflow/python/keras/layers/recurrent.py"", line 764, in call
    zero_output_for_mask=self.zero_output_for_mask)
  File "".../tensorflow/python/keras/backend.py"", line 3527, in rnn
    initial_states + constants)
  File "".../tensorflow/python/keras/layers/recurrent.py"", line 749, in step
    output, new_states = self.cell.call(inputs, states, **kwargs)
  File ""<stdin>"", line 10, in call
  File "".../tensorflow/python/keras/layers/recurrent.py"", line 1241, in call
    h = K.dot(inputs, self.kernel)
AttributeError: 'SimpleRNNCell' object has no attribute 'kernel'
```"
27444,TF 2.0 / `SequenceFeatures` changes order of columns.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 
- Python version: 2.0 alpha
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: None
- GPU model and memory: None


The order of input data is changed.
For example:
```
inputs = {'A': [1,2,3,4], 'B':[7,8,9,10]}
A = fc.sequence_numeric_column('A')
B = fc.sequence_numeric_column('B')
columns = [A,B]
seq_feature_layer = keras.experimental.SequenceFeatures(columns)
seq_input, seq_len = seq_feature_layer(inputs)
```
And It prints like
```
print(seq_input)
> [[7, 1], [8, 2], [9, 3], [10,4]]
```"
27443,Tensorflow lite model inference time increases when adding JNI on Android,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS10.14.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: HW P9, Mi8, Oneplus 5, VIVOX9, HuaWei Honor V9....
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):  1.13.0.dev20190126
- Python version: 2.7/3.7(both tryed)


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

I use tflite model (quantization-aware training and fully quantized with toco) and deploy on Android for segmentation task. I got correct output of the model, and the inference time is 30ms(1 thread), 17ms(2 threads), 15ms(3 threads). However, when i add some post-processing with C++(JNI), the inference time (only the function `Interpreter.run(input, output)`  run time)increases to 47ms(1 thread), 34ms(2 threads) , 30ms(3 threads), respectively, and I got the same inference time even though I didn't use the post-processing (just put the jni code in my project). 
When I use post-processing with java code, the inference time is 30ms(1 thread), 17ms(2 threads), 15ms(3 threads) again, so I guess JNI would influence the inference time.

**Describe the expected behavior**
Get same inference time with JNI on Android.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
just add some jni code to the official demo would got the same issue.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27442,TF 2.0 / `SequenceFeatures` shows an error when I put same sequence length of features.,"`keras.experimental.SequenceFeatures` requires `SparseTensor` but it measures the sequence length without `0` values.

For example, sequence data [[0,0,1,2], [0,1,2,3]] has `4` sequence lengths but when it converted to `SparseTensor`, it has a `2` sequence length and a `3` sequence length.

It shows an error as below:
```
Traceback (most recent call last):
  File ""pipeline2.py"", line 280, in <module>
    seq_input, seq_len = seq_feature_layer(inputs)
  File ""/Users/seungbaeji/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 660, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/Users/seungbaeji/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/feature_column/sequence_feature_column.py"", line 140, in call
    sequence_length = _assert_all_equal_and_return(sequence_lengths)
  File ""/Users/seungbaeji/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/feature_column/sequence_feature_column.py"", line 489, in _assert_all_equal_and_return
    assert_equal_ops.append(check_ops.assert_equal(tensors[0], t))
  File ""/Users/seungbaeji/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 496, in assert_equal
    (message or '', index_and_values_str, summary_msg)))
tensorflow.python.framework.errors_impl.InvalidArgumentError: 
Condition x == y did not hold.
Indices of first 3 different values:
[[0]
 [1]
 [2]]
Corresponding x values:
[95 94 93]
Corresponding y values:
[96 96 96]
First 3 elements of x:
[95 94 93]
First 3 elements of y:
[96 96 96]
```"
27441,Failing on a in tensorflow_cc.so on Windows 7 on Quadro R5000 16Gb with v1.12 and CUDA 10.0.130 and CUDNN 7.4.2.24 OK under Windows 10 Quadro P5000 and GTX 1060 6Gb,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
I have linked against the //tensorflow:libtensorflow_cc.so and //tensorflow:libtensorflow_framework.so targets using other libs, abseil-cpp, libprotobuf etc
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10 (build) and Window 7 (deployment)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.12.0
(tensorflow-cuda10) C:\Users\user\dev\tensorflow-cuda10\tensorflow\tensorflow\core\common_runtime\gpu>python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
b'v1.12.0-0-ga6d8ffae09' 1.12.0
- Python version: 3.6 (N/A)
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source):MSVC 14.0
- CUDA/cuDNN version: 10.0.130, 7.4.2.24
- GPU model and memory: GTX 1060 6Gb and Quadro R5000 16Gb


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

(tensorflow-cuda10) C:\Users\user\dev\tensorflow-cuda10\tensorflow\tensorflow\core\common_runtime\gpu>python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
b'v1.12.0-0-ga6d8ffae09' 1.12.0

**Describe the current behavior**
The application is currently crashed when initialising the session on the Quadro card on the client's computer running Windows 7 with the error messsage:

2019-04-02 11:30:18.871580: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1

here is the code for that file, where LOG FATAL is line 274

```
// This function must be called periodically to check whether pending
// events have recorded, and then retire them.  Initial observations
// suggest that typical behavior in a TensorFlow program is to have
// 0-3 events pending most of the time, but there are occasionally
// spikes of up to several hundred outstanding.
//
// NOTE: If all events are on the same stream, no later event will
// complete before an earlier event, except possibly if the earlier
// event transitions to an error state, so there's no advantage in
// looking past the first kPending event.  However, if we're using
// multiple streams there may be some gain in looking deeper.
// As a compromise, PollEvent() calls that are triggered by the queueing
// of a single event never look past the first kPending event.  Calls
// coming from the dedicated polling thread always sweep the full queue.
//
// Note that allowing the queue to grow very long could cause overall
// GPU memory use to spike needlessly.  An alternative strategy would
// be to throttle new Op execution until the pending event queue
// clears.
void EventMgr::PollEvents(bool is_dedicated_poller,
                          gtl::InlinedVector<InUse, 4>* to_free) {
  VLOG(2) << ""PollEvents  free_events_ "" << free_events_.size()
          << "" used_events_ "" << used_events_.size();
  // Sweep the remaining events in order.  If this is the dedicated
  // polling thread, check the entire set.  Otherwise, just sweep up to
  // the first non-complete record that is still pending.
  for (auto& iu : used_events_) {
    if (iu.event == nullptr) continue;
    se::Event::Status s = iu.event->PollForStatus();
    switch (s) {
      case se::Event::Status::kUnknown:
      case se::Event::Status::kError:
        // We don't expect to see these.  Someday maybe propagate
        // a Status error, but for now fail hard.
        LOG(FATAL) << ""Unexpected Event status: "" << static_cast<int>(s);
        break;
      case se::Event::Status::kPending:
        if (!is_dedicated_poller) return;  // quit processing queue
        break;
      case se::Event::Status::kComplete:
        // Make a copy of the InUse record so we can free it after releasing
        // the lock
        to_free->push_back(iu);
        free_events_.push_back(iu.event);
        // Mark this InUse record as completed.
        iu.event = nullptr;
    }
  }
  // Then clear any completed InUse records from the front of the queue.
  while (!used_events_.empty()) {
    InUse& iu = used_events_.front();
    if (iu.event == nullptr) {
      used_events_.pop_front();
    } else {
      break;
    }
  }
}

}  // namespace tensorflow
```

**Describe the expected behavior**
I would expect the software to load the graph into a fresh session and compute


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
	        tensorflow::SessionOptions options;
	        tensorflow::ConfigProto* config = &options.config;
                options.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(0.9);
		device_count->insert({ ""GPU"",1});
	}
	        device_count->insert({ ""CPU"", 1 });
                //bytes is read from graph_file_name
                graph_def->ParseFromArray(bytes.data(), (int)bytes.size()))
		session>reset(tensorflow::NewSession(options);
		std::cout << ""Rotobot: Swapping to model: "" << graph_file_name << "" using a single model per render is more efficent"" << std::endl;
                //crashes after here
		auto status = (*session)->Create(graph_def);
                auto status2 = (*session)->Run(Input_Tensors);

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

You can download the built software from:
https://kognat.com/product/rotobot-openfx-plugin-windows-64-gpu-v1-2-0-rc2-cuda-10/


You will just need an OpenFX host like Natron
https://natrongithub.github.io/

This tutorial will give you reproduction steps
https://kognat.com/2019/03/28/rotobot-srgb/"
27438,DLL Loading Failed & Using An AMD GPU,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro x64
- TensorFlow installed from (source or binary): Unknown
- TensorFlow version: Unknown
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: None
- GPU model and memory: 4GB ATI Radeon RX 580 (MSI)



**Describe the problem**

Unable to import a DLL & Getting Tensorflow to work on an AMD GPU

**Provide the exact sequence of commands / steps that you executed before running into the problem**
import tensorflow as tf

**Any other info / logs**
`Traceback (most recent call last):
  File ""F:\Python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""F:\Python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""F:\Python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""F:\Python\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""F:\Python\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""F:\Python\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""F:\Python\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""F:\Python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""F:\Python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""F:\Python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""F:\Python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""F:\Python\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""F:\Python\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.`
"
27437,LSTM quantization aware training and fully quanitzation in TfLite,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12.0, 1.13.1, and tf-nightly

- Are you willing to contribute it (Yes/No):
I'm not able to figure it out at this moment...

**Describe the feature and the current behavior/state.**
Currently, I can convert a LSTM model to .tflite file. But, I have no idea how to do fully quantization on it (meaning quantization-aware training + TfLite fully quantization). For me, post training quantization is not very useful, since I'd like to do 8-bit calculations during inference.

**Will this change the current api? How?**
Not sure. Maybe api like tf.contrib.quanitze.create_training_graph() or tf.contrib.quanitze.create_eval_graph() needs to be changed?

**Who will benefit with this feature?**
Devices that need 8-bit calculation can directly benefit from this feature. 

**Any Other info.**
I know there are a few issues related to this problem. But there is a lack of official reply on it. I'm writing to ask if it's still in the recent plan? Thanks in advance."
27435,AttributeError: module 'tensorflow' has no attribute 'enable_eager_execution' in Verify installation,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: not applicable
- TensorFlow installed from (source or binary): binary
- TensorFlow version: tensorflow-gpu 2.0.0a0
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: virtualenv + pip
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): - 
- CUDA/cuDNN version: 10.0/7.5.0.56
- GPU model and memory: NVIDIA GTX 1080 Max-Q



**Describe the problem**
I've just followed the installation guides for tensorflow 2.0 (see above) from:
- https://www.tensorflow.org/install/pip
- https://www.tensorflow.org/install/gpu#software_requirements
The final step is to ""Verify the install"" (see the pip link above) which gives me the following output:
```
$ python -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'
```

Which I did not expect/hope to see.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
- Follow the guides mentioned above.
- Create all the environment variables. 
-- Path within venv then contains: 
```
D:\dev\project\project-venv\
D:\dev\tools\cuda\cudnn-10.0-windows10-x64-v7.5.0.56\cuda\bin;
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin;
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\extras\CUPTI\libx64;
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\include;
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\libnvvp;
C:\ProgramData\DockerDesktop\version-bin;
C:\Program Files\Docker\Docker\Resources\bin;
D:\dev\tools\oraclexe\oraclexe\app\oracle\product\11.2.0\server\bin;
C:\Program Files\Python36\Scripts\;
C:\Program Files\Python36\;
C:\Program Files\Microsoft MPI\Bin\;
C:\Program Files (x86)\Common Files\Oracle\Java\javapath;
C:\ProgramData\Oracle\Java\javapath;
C:\WINDOWS\system32;
C:\WINDOWS;
C:\WINDOWS\System32\Wbem;
C:\WINDOWS\System32\WindowsPowerShell\v1.0\;
C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;
C:\Program Files\dotnet\;
C:\Program Files\Microsoft SQL Server\130\Tools\Binn\;
D:\dev\tools\putty\;
C:\Program Files\TortoiseSVN\bin;
C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;
C:\Program Files\nodejs\;
C:\WINDOWS\system32;
C:\WINDOWS;
C:\WINDOWS\System32\Wbem;
C:\WINDOWS\System32\WindowsPowerShell\v1.0\;
C:\WINDOWS\System32\OpenSSH\;
C:\dev\tools\python\python3.6.7\Scripts\;
C:\dev\tools\python\python3.6.7\;
C:\Program Files\Python36\Scripts\;
C:\Program Files\Python36\;
C:\Users\tve21314\AppData\Local\Microsoft\WindowsApps;
D:\dev\tools\maven\apache-maven-3.5.3\bin;
C:\Program Files\Java\jdk1.8.0_201\bin;
C:\Program Files\Java\jre1.8.0_201\bin;
C:\Program Files\Git\bin;
C:\Users\tve21314\AppData\Local\Programs\Microsoft VS Code\bin;
C:\Users\tve21314\AppData\Roaming\npm;
```
- Finally run from within the venv:
```python -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

- The traceback again:
```
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'
```

- If I use ```python -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random_normal([1000, 1000])))""``` the error becomes:
```
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'random_normal'
```"
27434,tensorflow-gpu installation failing ,"Please find the error below.

>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\priya\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\priya\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\priya\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\priya\Anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\priya\Anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\priya\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\priya\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\priya\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\priya\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\priya\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\priya\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\priya\Anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\priya\Anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.


"
27433,Clearing Tensorflow-Keras GPU memory,"
I'm fitting a model in a for loop,but i'm getting an error that my gpu's memory is full. I'm using Keras in Anaconda spyder ide.My gpu is asus gtx 1060 6gb.

I've also used codes like : K.clear_session() , gc.collect() , tf.reset_default_graph() , del custom_model but none of them worked. Gpu properties say's 98% of memory is full. enter image description here

Nothing flush gpu memory except numba.cuda.close() but won't allow me to use my gpu again. The only way to clear it is restarting kernel and rerun my code.

I'm looking for any script code to add my code allow me to use my code in for loop and clear gpu in every loop.


Part of my code :

image_input = Input(shape=(224, 224, 3))
base_model = Xception(input_tensor=image_input, include_top=False,weights='imagenet')
base_model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])
hist = base_model.fit(X,Y,epochs=2)

**System information**
- Have I written custom code :
- Windows 10 64-bit
- TensorFlow installed from conda install tensorflow-gpu
- TensorFlow version: 1.3
- Python version: 3.6
- CUDA/cuDNN version: 9.2
- GPU model and memory: Asus GTX 1060 6gb

"
27432,Errors with tensorflow-gpu 2.0 alpha0,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: tensorflow-gpu 2.0 alpha0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cudatoolkit 10.0.130, and cudnn 7.5
- GPU model and memory: NVIDIA GTX 1070/6GB



**Describe the problem**
I have installed tensorflow-gpu 2.0alpha0, cudatoolkit 10.0.130, and cudnn 7.5 in a windows 10.

TensorFlow 2.0 works well with CPU, but encounters errors while running with GPU.

Errors messages are as bellow:
```
2019-04-02 23:47:38.646661: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-04-02 23:47:38.666653: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'nvcuda.dll'; dlerror: nvcuda.dll not found
2019-04-02 23:47:38.666842: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)
```

I think the main issue is ""Could not dlopen library 'nvcuda.dll'"".

However, I have installed the latest NVIDIA driver (version 419.67), and 'nvcuda.dll' can be found in C:\Windows\System32\nvcuda.dll.

The issue is also posted in [stackoverflow](https://stackoverflow.com/questions/55478968/errors-with-tensorflow-gpu-2-0alpha0).

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27431,Using layer classes as attribute throw an exception,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: `2.0.0.dev20190402`
- Python version: 2.7.12

**Describe the current behavior**

When a layer class is used as attribute, the code will throw a `TypeError` exception when calling `self._gather_children_attribute`. It appears that the layer class is tracked.

**Describe the expected behavior**

Only layer instances should be tracked, not classes.

**Code to reproduce the issue**

```python
import tensorflow as tf

class Layer(tf.keras.layers.Layer):

    def __init__(self):
        super(Layer, self).__init__()
        self.layer_fn = tf.keras.layers.Dense

layer = Layer()
print(layer.variables)
```

**Other info / logs**

```text
Traceback (most recent call last):
  File ""tf2/class.py"", line 10, in <module>
    print(layer.variables)
  File ""/tmp/tf2/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1330, in variables
    return self.weights
  File ""/tmp/tf2/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 708, in weights
    return self.trainable_weights + self.non_trainable_weights
  File ""/tmp/tf2/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 687, in trainable_weights
    nested = self._gather_children_attribute('trainable_weights')
  File ""/tmp/tf2/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1850, in _gather_children_attribute
    getattr(layer, attribute) for layer in nested_layers))
TypeError: 'property' object is not iterable
```"
27430,[Feature] Store shared library name in tf.sysconfig,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.13.1 or 2.0.0
- Are you willing to contribute it (Yes/No): Probably not the best person


**Describe the feature and the current behavior/state.**
As of #22797 the tensorflow shared library is now versioned. This makes linking to the shared library difficult (at least with my knowledge of bazel). The recommended method in https://github.com/tensorflow/custom-op links a [static file name](https://github.com/tensorflow/custom-op/blob/master/tf/BUILD.tpl#L12)

**Will this change the current api? How?**
Add a new shared library name to tf.sysconfig 

**Who will benefit with this feature?**
The community who is building upon TensorFlow. Given the push for modularity it's an important thing to figure out.


**Any Other info.**
This stems from https://github.com/tensorflow/addons/pull/130
"
27429,ValueError: Invalid tensors 'Mul' were found.,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
Python:2.7
tensorflow:1.9

python script
`input_arrays= [""Mul""]
output_arrays=[""final_result""]
converter = tf.contrib.lite.TocoConverter.from_frozen_graph(graph_def_file,input_arrays,output_arrays)
tflite_model = converter.convert()
open(""model.tflite"", ""wb"").write(tflite_model)`

It was working fine a month before.Now it shows the following error

  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert_saved_model.py"", line 205, in get_tensors_from_tensor_names
    "","".join(invalid_tensors)))
ValueError: Invalid tensors 'Mul' were found.
"
27428,Tensorflow 2.0.0 multiple GPU output zero for non-root GPU,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No.
- TensorFlow installed from (source or binary):
Source.
- TensorFlow version (use command below):
v2.0.0-alpha0-0-g2c319fb415 2.0.0-alpha0
- Python version:
3.7
- Bazel version (if compiling from source):
0.23.0
- GCC/Compiler version (if compiling from source):
GCC 4.8
- CUDA/cuDNN version:
CUDA 10.0, cuDNN 7
- GPU model and memory:
A simple model can reproduce.

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

When I run the `tf_env_collect.sh` script, tensorflow just hangs there forever. I couldn't even kill it. What the hell is going on?

```bash
(base)   ~ ./tfenv.sh 
Collecting system information...
2019-04-02 22:07:13.977020: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-04-02 22:07:15.731652: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x56368643d070 executing computations on platform CUDA. Devices:
2019-04-02 22:07:15.731716: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): GeForce GTX TITAN X, Compute Capability 5.2
2019-04-02 22:07:15.731733: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (1): GeForce GTX TITAN X, Compute Capability 5.2
2019-04-02 22:07:15.731751: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (2): TITAN X (Pascal), Compute Capability 6.1
2019-04-02 22:07:15.731765: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (3): TITAN X (Pascal), Compute Capability 6.1
2019-04-02 22:07:15.731780: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (4): TITAN X (Pascal), Compute Capability 6.1
2019-04-02 22:07:15.731795: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (5): GeForce GTX TITAN X, Compute Capability 5.2
2019-04-02 22:07:15.731810: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (6): GeForce GTX TITAN X, Compute Capability 5.2
2019-04-02 22:07:15.757870: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299805000 Hz
2019-04-02 22:07:15.761353: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x563686573540 executing computations on platform Host. Devices:
2019-04-02 22:07:15.761414: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>
2019-04-02 22:07:15.762076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties: 
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:08:00.0
totalMemory: 11.93GiB freeMemory: 11.81GiB
2019-04-02 22:07:15.762510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 1 with properties: 
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:09:00.0
totalMemory: 11.93GiB freeMemory: 11.81GiB
2019-04-02 22:07:15.762924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 2 with properties: 
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:82:00.0
totalMemory: 11.91GiB freeMemory: 11.76GiB
2019-04-02 22:07:15.763343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 3 with properties: 
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:85:00.0
totalMemory: 11.91GiB freeMemory: 11.76GiB
2019-04-02 22:07:15.763634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 4 with properties: 
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:86:00.0
totalMemory: 11.91GiB freeMemory: 5.41GiB
2019-04-02 22:07:15.764061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 5 with properties: 
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:89:00.0
totalMemory: 11.93GiB freeMemory: 11.81GiB
2019-04-02 22:07:15.764483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 6 with properties: 
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:8a:00.0
totalMemory: 11.93GiB freeMemory: 11.81GiB
2019-04-02 22:07:15.770500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6
2019-04-02 22:07:15.770587: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
```

**Describe the current behavior**
Run the test code below. For GPU 0 the behavior is normal, but for GPU 1 the output becomes zero.
**Describe the expected behavior**
The output of GPU 1 should be the same as GPU 0.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
run `python test.py`.

```python
import tensorflow as tf
import numpy as np
import os

os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1""

def linear(name, input, output_dim, reuse=False):
    with tf.variable_scope(name, reuse=reuse):
        w = tf.get_variable(""weight"", shape=[input.get_shape()[-1], output_dim], initializer=tf.orthogonal_initializer)
        b = tf.get_variable(""bias"", [output_dim], initializer=tf.constant_initializer(.0))

        x = tf.matmul(input, w) + b
        with tf.device(""/device:CPU:0""):
            x = tf.Print(x, [tf.reduce_sum(tf.abs(w)), tf.reduce_sum(tf.abs(b))], name + ""/weight_bias: "")
    
    return x

def tower(x, reuse=False):
    rec_tensor = []
    rec_name = []
    x = linear(""fc1"", x, 10, reuse)
    rec_tensor.append(x); rec_name.append(""fc1"")
    x = linear(""fc2"", x, 1, reuse)
    rec_tensor.append(x); rec_name.append(""fc2"")
    return x, rec_tensor, rec_name

x = tf.placeholder(tf.float32, [None, 3])
x_data = np.random.rand(5, 3)
feed_dict = {x: x_data}
ys = []
rec_xs = []
rec_names = []

for i in range(2):
    with tf.device(tf.DeviceSpec(device_type=""GPU"", device_index=i)):
        y_, v_, n_ = tower(x, i>0)
        ys.append(y_)
        rec_xs.append(v_)
        rec_names.append(n_)

config = tf.ConfigProto()
config.gpu_options.allow_growth = True
config.allow_soft_placement = False
sess = tf.Session(config=config)

sess.run(tf.global_variables_initializer())

print(""=> Check forward"")
for i in range(2):
    print(""=> Check GPU %d"" % i)
    for j in range(len(rec_xs[i])):
        t = sess.run(rec_xs[i][j], feed_dict)[0]
        l1norm = np.sum(np.abs(t))
        print(""=> %s: %.5f"" % (rec_names[i][j], l1norm))
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

My running log of `test.py`:

```
WARNING: Logging before flag parsing goes to stderr.
W0402 21:57:00.129247 139765947877184 deprecation.py:506] From /home/atlantix/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py:883: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0402 21:57:00.150027 139765947877184 deprecation.py:323] From test.py:15: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.
Instructions for updating:
Use tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:
```python
    sess = tf.Session()
    with sess.as_default():
        tensor = tf.range(10)
        print_op = tf.print(tensor)
        with tf.control_dependencies([print_op]):
          out = tf.add(tensor, tensor)
        sess.run(out)
    ```
Additionally, to use tf.print in python 2.7, users must make sure to import
the following:

  `from __future__ import print_function`

2019-04-02 21:57:00.195153: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-04-02 21:57:00.626301: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x55bf73aa0640 executing computations on platform CUDA. Devices:
2019-04-02 21:57:00.626354: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): GeForce GTX TITAN X, Compute Capability 5.2
2019-04-02 21:57:00.626368: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (1): GeForce GTX TITAN X, Compute Capability 5.2
2019-04-02 21:57:00.649925: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299805000 Hz
2019-04-02 21:57:00.653093: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x55bf73a9fad0 executing computations on platform Host. Devices:
2019-04-02 21:57:00.653157: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>
2019-04-02 21:57:00.653809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties: 
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:08:00.0
totalMemory: 11.93GiB freeMemory: 11.81GiB
2019-04-02 21:57:00.654269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 1 with properties: 
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:09:00.0
totalMemory: 11.93GiB freeMemory: 11.81GiB
2019-04-02 21:57:00.654961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0, 1
2019-04-02 21:57:00.655060: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-04-02 21:57:01.477672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-04-02 21:57:01.477727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 1 
2019-04-02 21:57:01.477757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N Y 
2019-04-02 21:57:01.477767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 1:   Y N 
2019-04-02 21:57:01.478354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11422 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:08:00.0, compute capability: 5.2)
2019-04-02 21:57:01.478791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11422 MB memory) -> physical GPU (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0, compute capability: 5.2)
2019-04-02 21:57:01.487682: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
=> Check forward
=> Check GPU 0
2019-04-02 21:57:01.527837: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
fc1/weight_bias: [8.01475811][0]
=> fc1: 1.84453
fc1/weight_bias: [8.01475811][0]
fc2/weight_bias: [2.51356649][0]
=> fc2: 0.19745
=> Check GPU 1
fc1/weight_bias: [8.01475811][0]
=> fc1: 0.00000
fc1/weight_bias: [8.01475811][0]
fc2/weight_bias: [2.51356649][0]
=> fc2: 0.00000
```

As for my GPUs, the driver version is `410.104`, installed according to official instructions.

Hope it helps.
Could anybody help me ASAP? I am almost crazy with TF's multiple GPU. 
Thank you very much!"
27425,Unity3D ml-agents not working properly in TF 2.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Win 10
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.0.0-alpha0
- Python version:3.6

**Describe the current behavior**
The value estimate became an array instead of scalar float after 8000+ steps

**Describe the expected behavior**
The value estimate should be a scalar float

**Code to reproduce the issue**
https://github.com/SetoKaiba/ml-agents/tree/tf2

**Other info / logs**
After some deep dig, I found the problem is here. It uses the tf session to estimate the value. The value became an array instead of scalar float after 8000+ steps
https://github.com/SetoKaiba/ml-agents/blob/tf2/ml-agents/mlagents/trainers/ppo/policy.py#L198
"
27420,Official website is causing high cpu usage in firefox and chrome when scrolled to top,Website is causing high cpu usage in firefox and chrome when scrolled to top
27418,"Minor ""Error"" in Tutorial",
27417,tf.gather shows an error in presence of different dtype tensor under tf.function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): Source (1st April 2019)
- TensorFlow version (use command below): v2.0
- Python version: 3
- Bazel version (if compiling from source): 0.22.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: 10.0, 7.5.0
- GPU model and memory: Zotac GTX 1060, 6GB

**Describe the current behavior**
tf.gather throws error in presence of another independent tensor inside tf.function

**Describe the expected behavior**
Run smoothly

**Code to reproduce the issue**

    import tensorflow as tf

    a = tf.Variable(tf.random.normal([10]))
    b = tf.Variable(tf.random.uniform([10], minval=-1, maxval=1, dtype=tf.dtypes.int32))
    value = tf.Variable(tf.random.normal([10]))

    print(a.numpy)
    print(b.numpy)

    @tf.function
    def run():
        for j in tf.range(10):

            var = tf.gather(a, j)
            tf.print(var)

            var_int = tf.gather(b, j)
            tf.print(var_int)

    run()

**Information**
When you comment out the float variable, it functions correctly with a warning. When both the int32 and float 32 are present, tf.gather shows an error as shown below.

**Other info / logs**
File ""test.py"", line 20, in <module>
    run()
  File ""/home/caissalover/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 437, in __call__
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File ""/home/caissalover/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 559, in _filtered_call
    (t for t in nest.flatten((args, kwargs))
  File ""/home/caissalover/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 633, in _call_flat
    outputs = self._inference_function.call(ctx, args)
  File ""/home/caissalover/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 416, in call
    ctx=ctx)
  File ""/home/caissalover/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 66, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:CPU:0 vs /job:localhost/replica:0/task:0/device:GPU:0. The edge src node is while/exit/_27 , and the dst node is while [Op:__inference_run_98]
"
27416,Unable to use FeatureColumn with Keras Functional API,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below):  2.0.0-alpha0
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
I followed the guide: [Classify structured data](https://www.tensorflow.org/alpha/tutorials/keras/feature_columns). The author used TF Feature Column and TF Data alongside a Sequential model, which worked out just fine. 

Then, I've tried implementing the same using Keras Functional API, but was greeted by the error message below:

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-19-9647c70de900> in <module>
----> 1 inputs = layers.Input(tensor=feature_layer, name='features')
      2 x = layers.Dense(128, activation='relu')(inputs)
      3 x = layers.Dense(64, activation='relu')(x)
      4 
      5 baggage_pred = layers.Dense(1, activation='sigmoid', name='baggage')(x)

~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_layer.py in Input(shape, batch_size, name, dtype, sparse, tensor, **kwargs)
    231       dtype=dtype,
    232       sparse=sparse,
--> 233       input_tensor=tensor)
    234   # Return tensor including `_keras_history`.
    235   # Note that in this case train_output and test_output are the same pointer.

~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_layer.py in __init__(self, input_shape, batch_size, dtype, input_tensor, sparse, name, **kwargs)
     77         dtype = backend.floatx()
     78       else:
---> 79         dtype = backend.dtype(input_tensor)
     80     elif input_tensor is not None and input_tensor.dtype != dtype:
     81       raise ValueError('`input_tensor.dtype` differs from `dtype`: %s vs. %s' %

~/.local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in dtype(x)
   1025   ```
   1026   """"""
-> 1027   return x.dtype.base_dtype.name
   1028 
   1029 

AttributeError: 'str' object has no attribute 'base_dtype'
```

**Describe the expected behavior**
If I understood correctly, TF Keras is supposed to be interoperable with Feature Column. And the way to achieve that is to wrap a list of feature columns with `tf.keras.layers.DenseFeatures()` and parse it to the input layer as a tensor like so:

`feature_layer = tf.keras.layers.DenseFeatures(feature_columns)`
`inputs = layers.Input(tensor=feature_layer, name='features')`

**Code to reproduce the issue**
Here's the code to reproduce the error:
```
from __future__ import absolute_import, division, print_function

import numpy as np
import pandas as pd

#!pip install tensorflow==2.0.0-alpha0
import tensorflow as tf

from tensorflow import feature_column
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

URL = 'https://storage.googleapis.com/applied-dl/heart.csv'
dataframe = pd.read_csv(URL)
dataframe.head()

train, test = train_test_split(dataframe, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
print(len(train), 'train examples')
print(len(val), 'validation examples')
print(len(test), 'test examples')

# A utility method to create a tf.data dataset from a Pandas Dataframe
def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()
  labels = dataframe.pop('target')
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)
  return ds

batch_size = 5 # A small batch sized is used for demonstration purposes
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

age = feature_column.numeric_column(""age"")
age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])
thal = feature_column.categorical_column_with_vocabulary_list(
      'thal', ['fixed', 'normal', 'reversible'])
thal_one_hot = feature_column.indicator_column(thal)
thal_embedding = feature_column.embedding_column(thal, dimension=8)
thal_hashed = feature_column.categorical_column_with_hash_bucket(
      'thal', hash_bucket_size=1000)
crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)

feature_columns = []

# numeric cols
for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:
  feature_columns.append(feature_column.numeric_column(header))

# bucketized cols
age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])
feature_columns.append(age_buckets)

# indicator cols
thal = feature_column.categorical_column_with_vocabulary_list(
      'thal', ['fixed', 'normal', 'reversible'])
thal_one_hot = feature_column.indicator_column(thal)
feature_columns.append(thal_one_hot)

# embedding cols
thal_embedding = feature_column.embedding_column(thal, dimension=8)
feature_columns.append(thal_embedding)

# crossed cols
crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)
crossed_feature = feature_column.indicator_column(crossed_feature)
feature_columns.append(crossed_feature)

batch_size = 32
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

feature_layer = tf.keras.layers.DenseFeatures(feature_columns)

inputs = layers.Input(tensor=feature_layer, name='features')
x = layers.Dense(128, activation='relu')(inputs)
x = layers.Dense(64, activation='relu')(x)

baggage_pred = layers.Dense(1, activation='sigmoid')(x)

model = keras.Model(inputs=inputs,outputs=baggage_pred)

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
```

**Other info / logs**
N/A
"
27415,How to improve efficiency when sess.run,"When I test my model using Tensorflow, I wonder how to imporve the speed (or reduce running time) when I execute  session like:

with tf.Session(graph=graph) as sess:
    model.saver.restore(sess, conf[""init_model""])
    feed = { 
        model.data: mydata,  
        model.label: mylabel
        }
    scores = sess.run(model.logits, feed_dict = feed)
    losses = sess.run(model.loss, feed_dict = feed)

As my model size and number of parameter being fixed (after training), How can I impove the speed when I test model with only little batch (even one batch would be slow) to get scores and losses.
I wonder if taking more GPUs could be help. I am not sure.
Does there any other idea to solve it? Thank you !"
27414,tf.data.Iterator won't release memory when reinitialized,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.10.1
- Python version: 3.6.6
- CUDA/cuDNN version: 9.0
- GPU model and memory: NVIDIA GTX1080Ti, 11GB GPU/ 64 GB RAM

**Describe the current behavior**
When testing my model, I had several batches of images with batch size equals to 10. As the images are large, I loaded them from hard drive every time. My code is structured as <br />
```python
iterator = tf.data.Iterator.from_structure(output_types=tf.float32, output_shapes=[1536, 1536, 2])
output  = model_fn(iterator.get_next())
for images in files:
    dataset = tf.data.Dataset.from_tensor_slices(tf.constant(images))
    sess.run(iterator.make_initializer(dataset))
    output_value = []
    for _ in range(10):
        output_value.append(sess.run(output))
``` 
Here each of my `images` is a numpy array with a size of [10, 1536, 1536, 2].<br />
While the code running, the memory usage is growing fast -- every image set it read in stayed there in the memory.


**Describe the expected behavior**
I'm expecting the iterator release the memory each time it reinitialized.


**Code to reproduce the issue**
For a reproducible test, I also tried this, to see if the memory leakage is due to something else:
```python
import tensorflow as tf
import numpy as np
iterator = tf.data.Iterator.from_structure(output_types=tf.float64, output_shapes=[1536,1536,2])
sess = tf.InteractiveSession()
for i in range(100):
    data = tf.constant(np.random.rand(10,1536,1536,2))
    dataset = tf.data.Dataset.from_tensor_slices(data)
    ite_data = iterator.make_initializer(dataset)
    sess.run(ite_data)
```
I can still see a huge memory usage when executing this. 
"
27409,Invalid argument: Conv2DSlowBackpropInput: input and out_backprop must have the same batch size,"I want to use tensorflow to train my model  
firstly, I use conv2d_transpose in order to realize upsampling, then, I use Batch_normalization after each conv2d_transpose layer, and then I use conv2d and maxpool.  
But when I train my model , I met a problem, such like this:  

Invalid argument: Conv2DSlowBackpropInput: input and out_backprop must have the same batch size  input batch: 64  outbackprop batch: 56 batch_dim: 0  

what makes me more puzzled is the training can run some steps, my batch_size=64, when the steps arrives at 101 steps, the training stopped!!!!  

here is my deconv functions:  
```python
def deconv2d(input_, output_shape,
             k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,
             name=""deconv2d"", with_w=False):
    with tf.variable_scope(name):

        w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],
                            initializer=tf.random_normal_initializer(stddev=stddev))

        deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,
                                        strides=[1, d_h, d_w, 1])

        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))
        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())

        if with_w:
            return deconv, w, biases
        else:
            return deconv
```
and this is how I use this deconv2d fucntion:  
```python
h1 = deconv2d(h0, [batch_size, s_h8, s_w8, f_dim * 4], name=""deconv1"")
        h1 = tf.nn.relu(d_bn_1(h1))
```

and this is my batch_normalization fuction:  
```python
class BatchNorm(object):
    def __init__(self, epsilon=1e-5, momentum=0.9, name=""batch_norm""):
        # with tf.variable_scope(name):
        self.epsilon = epsilon
        self.momentum = momentum
        self.name = name


    def __call__(self, x, train=True):
        return tf.contrib.layers.batch_norm(x,
                                            decay=self.momentum,
                                            updates_collections=None,
                                            epsilon=self.epsilon,
                                            scale=True,
                                            is_training=train,
                                            scope=self.name)
```
I have tried some methods , such as use tf.reshape() right after the conv2d_transpose.  
But it didn't work!

I hope anyone else can help me resolve this problem.  
Thank you very much!!!"
27402,Build TFLite Model Benchmark Tool with GPU delegate?,"**System information**
- TensorFlow version (you are using):nightly-build
- Are you willing to contribute it (Yes/No):Yes



**Describe the feature and the current behavior/state.**
I'm trying to build TFLite Model Benchmark Tool with GPU delegate on Android and iOS devices.
But unfortunately, It seems to be too difficult for me especially on iOS.
Would you like to provide bazel script to build TFLite Model Benchmark Tool with GPU delegate?

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Anyone using the TFLite.

**Any Other info.**
"
27401,longer latency after post-training quantization on NVIDIA Jetson TX2,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
I modified the tutorial below to convert my customized models to print inference latency.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
NVIDIA Jetson TX2
- TensorFlow installed from (source or binary):
source
- TensorFlow version (use command below):
1.12.0
- Python version:
3.5.2
- CUDA/cuDNN version:
V9.0.252
- GPU model and memory:
GPU model: NVIDIA Pascal, 256 CUDA cores
memory: 8 GB 128 bit LPDDR4

**Describe the problem**
I modified the tutorial below to convert three customized models to print inference latency.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_quant.ipynb

In the 3-layer and 9-layer model,, the inference time of tflite model is shorter than the original one.
However, in resnet50 model, the inference time of tflite model is NOT shorter than the original one.
I summarized the time table and graph below.
![](https://user-images.githubusercontent.com/40556694/55369939-36089e80-552b-11e9-925f-67ec4908b4f4.png)

I read the closed issue #23759:longer latency after post-training quantization. In that issue, the explanation of the longer latency is because the speed-up of integer arithmetic require special/optimized instructions/kernels, while such optimizations are not done on desktop CPU.
Is this explanation applicable to NVIDIA Jetson TX2?

**Any other info / logs**
Include any logs or source code
[tflite.zip](https://github.com/tensorflow/tensorflow/files/3031972/tflite.zip)
 that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27400,tensorflow op ExtractImagePatches is not supported,"Hello

I am trying to convert yolov2-voc converted from darknet to TF using dark flow.
I am getting an error: ""ValueError: tensorflow op ExtractImagePatches is not supported""
Currently I am using pre-build 1.9.0. Looking around seems like I need to build it from scratch, but it's not clear what I need to do on windows to get it to work.
It was mentioned to add it to tensorflow/core/kernels/BUILD, but it's not clear where exactly in it, and if I need to modify any other files.
"
27398,Confusion about how bucketized feature columns work,"I had some confusion about how bucketized feature columns represent input to the model. According to the [blog post on feature columns](https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html), when we bucketize a feature like `year` this puts each value in buckets based on the defined boundaries, and creates a binary vector, turning on each bucket based on the input value, but the example in the [documentation](https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column) shows the output as a single integer. I'm confused as to how the input is to the model when using a bucketized column..
**System information**
- TensorFlow version:
1.4.0
- Doc Link:
https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column"
27395,labe_image bazel build failed! ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version: 1.12
- Python version:2.7
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):0.24.0
- GCC/Compiler version (if compiling from source): gcc  5.4.0 20160609
- CUDA/cuDNN version: 9.0
- GPU model and memory:

**Describe the problem**

To build label_image for android ARMv8:

```
> bazel build --config android_arm64 --config monolithic --cxxopt=-std=c++11 \
  //tensorflow/lite/examples/label_image:label_image
```

error infor:
```
ERROR: /home/apuser/deeplearning/tensorflow/tensorflow-master/tensorflow/lite/kernels/internal/BUILD:496:1: no such package '@com_google_absl//absl/base': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/abseil/abseil-cpp/archive/2c8421e1c6cef0da9e8a20b01c15256ec9ec116d.tar.gz, https://github.com/abseil/abseil-cpp/archive/2c8421e1c6cef0da9e8a20b01c15256ec9ec116d.tar.gz] to /home/apuser/.cache/bazel/_bazel_apuser/4972b2c1cfd6d41e3f1f5f386fde1d1a/external/com_google_absl/2c8421e1c6cef0da9e8a20b01c15256ec9ec116d.tar.gz: All mirrors are down: [Connection reset, GET returned 404 Not Found] and referenced by '//tensorflow/lite/kernels/internal:tensor_utils'
ERROR: Analysis of target '//tensorflow/lite/examples/label_image:label_image' failed; build aborted: no such package '@com_google_absl//absl/base': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/abseil/abseil-cpp/archive/2c8421e1c6cef0da9e8a20b01c15256ec9ec116d.tar.gz, https://github.com/abseil/abseil-cpp/archive/2c8421e1c6cef0da9e8a20b01c15256ec9ec116d.tar.gz] to /home/apuser/.cache/bazel/_bazel_apuser/4972b2c1cfd6d41e3f1f5f386fde1d1a/external/com_google_absl/2c8421e1c6cef0da9e8a20b01c15256ec9ec116d.tar.gz: All mirrors are down: [Connection reset, GET returned 404 Not Found]

```"
27392,Runtime error when using ExponentialMovingAverage with MirroredStrategy (TF 1.13.1),"I'm trying to use ExponentialMovingAverage with MirroredStrategy training the model on several GPUs, but getting an error:
```
RuntimeError: Tried to create variable dense/kernel/replica_1/ExponentialMovingAverage/ with mismatching name on device 1
```

This error can also be reproduced on a CPU machine by specifying the number of GPUs in the ""tf.contrib.distribute.MirroredStrategy"" more or equal to 2.

The model works fine if I'm training it on a single GPU (by specifying the number of GPUs to 1).

I'm using TF v1.13.1. Here is a simplified code:
```python
import tensorflow as tf


def input_fn():
    features = tf.data.Dataset.from_tensors([1., 2., 3.])
    labels = tf.data.Dataset.from_tensors(1.)
    dataset = tf.data.Dataset.zip((features, labels)).repeat(10).batch(1)
    return dataset


def model_fn(features, labels, mode, params):
    logits = tf.layers.dense(features, 1, activation=tf.nn.relu)
    logits = tf.reshape(logits, (-1,))
    loss = tf.losses.mean_squared_error(logits, labels)

    ema = tf.train.ExponentialMovingAverage(0.999)

    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0001)
        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())

        # apply moving averages
        with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):
            ema_update_op = ema.apply(tf.trainable_variables())

        with tf.control_dependencies([train_op]):
            train_op = tf.group(ema_update_op)

        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

    raise NotImplementedError


def train(model_dir):
    distribution = tf.contrib.distribute.MirroredStrategy(num_gpus=2)

    estimator = tf.estimator.Estimator(
        model_fn=model_fn,
        config=tf.estimator.RunConfig(
            train_distribute=distribution,
            model_dir=model_dir,
            log_step_count_steps=1,
        ),
    )

    estimator.train(input_fn=input_fn)


if __name__ == '__main__':
    tf.logging.set_verbosity(tf.logging.INFO)
    train('training/test1')
```
"
27390,Reading a tensor from file in python which was saved using C++,"I'm having trouble reading/interpreting a tensor from a file using read_file() operation in Python. The tensor was saved using Save() ops in tensorflow CC API. I can read the underlying byte encoded tensor, but find no methods to convert that back to float(original content of the tensor). I have tried decode_raw() in python but it complaints about of not being of proper multiple of 4. 
The above tensor comprises of a 3 channel nd array with float contents. Would appreciate any help in deciphering the content
More precisely would really appreciate on some details on how Save() ops in tensorflow CC API works?"
27386,Tensorflow 2.0: Optimizer.minimize ('Adam' object has no attribute 'minimize'),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave
- TensorFlow installed from (source or binary): latest 2.0.0-alpha0 via pycharm
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.7


For my Reinforcement Learning application, I need to be able to apply custom gradients / minimize changing loss function. According to documentation [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/Optimizer], it should be possible with Optimizer.minimize() function. However, my pip-installed version appears not to have this feature at all.

https://stackoverflow.com/questions/55459087/tensorflow-2-0-optimizer-minimize-adam-object-has-no-attribute-minimize

My code:

```
from tensorflow.python.keras.optimizers import Adam, SGD
print(tf.version.VERSION)
optim = Adam()
optim.minimize(loss, var_list=network.weights)
```
output:

```
2.0.0-alpha0
Traceback (most recent call last):
  File ""/Users/ikkamens/Library/Preferences/PyCharmCE2018.3/scratches/testo.py"", line 18, in <module>
    optim.minimize(loss, var_list=network.weights)
AttributeError: 'Adam' object has no attribute 'minimize'
```"
27384,How to use estimator with graph creation routines in model_fn using placeholders?,"It seems like tf.estimator.Estimator is worth using even for optimization of custom graphs. I am falling down a pattern of creating graph constructors using placeholders for the input. The particluar graph chunk will appear in multiples places in the large graph with the same (shared) weights but different inputs. 

It is not clear from the estimator documentation what the ""right"" pattern inside the model_fn is. I have a feeling running a session with a feed_dict in there is wrong. 

Are there any more examples that handle something like this? "
27383,transpose() can be very slow on CPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 / WSL
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary (MKL) (pip install intel-tensorflow)
- TensorFlow version (use command below): b'v1.13.1-0-g6612da8' 1.13.1
- Python version: 3.6.8 (Anaconda)

**Describe the current behavior**
On CPU, `tf.transpose()` can be 10 times slower than `np.tranpose(...).copy()`, depending on the dimensions of the tensor. In my example case, the speed can be fixed by sandwiching the `tf.transpose` in reshapes that reduce the dimensionality for the transpose.

**Describe the expected behavior**
`tf.tranpose()` should be fast without the need to resort to reshaping tricks (which will not always apply anyway). Note also that some transposes are implicit (performed, say, by `tf.tensordot()`).

**Code to reproduce the issue**
```python
import time
import numpy as np

import tensorflow as tf
tf.enable_v2_behavior()

d = 2
N = 22
# NOTE: TF starts getting a lot slower than numpy at N=9, it seems. 
# Try d=5, N=9  vs  d=7, N=8.

# Create a N-dimensional ndarray
psi = np.random.randn(*[d for n in range(N)]).astype(np.float64)

psi_np = psi.copy()
for _ in range(5):
    # numpy's transpose is a metadata operation.
    # copy() actually carries out the reordering of the data in memory.
    t0 = time.time()
    psi_np = np.transpose(psi_np, (*list(range(2,N)), 0, 1)).copy()
    print(time.time() - t0)  # around 0.07s on my system

psi_tf = tf.convert_to_tensor(psi)

@tf.contrib.eager.defun  # make this a graph, just in case
def f(p):
    return tf.transpose(p, (*list(range(2,N)), 0, 1))

for _ in range(5):
    t0 = time.time()
    psi_tf = f(psi_tf)
    print(time.time() - t0)  # around 0.5s on my system (almost 10 times slower!)

# check results are the same
print(np.linalg.norm(psi_tf.numpy().ravel() - psi_np.ravel()))  # should print 0.0

# Note that the speed depends on the shape!
# We now do an equivalent tranpose, but sandwiched 
# by reshapes to reduce the number of dims.

psi_tf = tf.convert_to_tensor(psi)

@tf.contrib.eager.defun
def f2(p):
    # Rephrase the high-dimensional transpose as a matrix transpose.
    pr = tf.reshape(p, (d**2, d**(N-2)))
    prt = tf.transpose(pr, (1,0))
    return tf.reshape(prt, p.shape)

for _ in range(5):
    t0 = time.time()
    psi_tf = f2(psi_tf)
    print(time.time() - t0)  # around 0.008s on my system

# check results are the same
print(np.linalg.norm(psi_tf.numpy().ravel() - psi_np.ravel()))  # should print 0.0
```

**Other info / logs**
Compared to #15697, I am actually timing the equivalent op in numpy (transpose then copy).
Also, this happens on MKL builds as well as non-MKL builds of  TF 1.13.1."
27381,Generating fully-quantized models in Pre-trained checkpoints (.chkpt) or GraphDef (.pb)  format,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

**System information**
- TensorFlow version (you are using):1.12.0-rc2
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
By Using the TensorFlow Lite Converter tflite_convert to optimize the TensorFlow graphs and convert them to the TensorFlow Lite format for 8-bit inference. 

Can we have a feature to optimize the TensorFlow graphs and convert them to .chkpt or .pb format?  So, We can Initialize the WB in tensorflow and do inference with 8-bit.

**Will this change the current API? How?**
No

**Who will benefit with this feature?**
The developers who want only Int-8 quantization model(WB) and not depend on TFLite.

**Any Other info.**
No"
27380,[Feature Request] Support Sparse Tensors in tf.linalg operations,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0.0a0
- Are you willing to contribute it (Yes/No): Maybe, if there are any pointers on how to go about implementing something like this.



**Describe the feature and the current behavior/state.**

Currently the `tf.linalg` operations seem to only support dense tensors. I have verified `tf.linalg.solve`. Consider the following variables

```
A = tf.sparse.SparseTensor(
    indices=[[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2], [2, 0], [2, 1], [2, 2]],
    values=[3., 2., -1., 2., -2., 4., -1., 0.5, -1.],
    dense_shape=(3, 3)
)

b = tf.sparse.SparseTensor(
    indices=[[0, 0], [1, 0], [2, 0]],
    values=[-1., 2., 0.],
    dense_shape=(3, 1)
)
```

now if I do

```
tf.linalg.solve(A, b)
```

this errors out with

```
ValueError: Attempt to convert a value (<tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7f28a1e3fc50>) with an unsupported type (<class 'tensorflow.python.framework.sparse_tensor.SparseTensor'>) to a Tensor.
```

However, if I do

```
tf.linalg.solve(tf.sparse.to_dense(A), tf.sparse.to_dense(b))
```

it works, giving

```
<tf.Tensor: id=11, shape=(3, 1), dtype=float32, numpy=
array([[-1.0000006],
       [ 2.0000014],
       [ 2.0000012]], dtype=float32)>
```

which is roughly the correct answer. Example taken from the [scipy sparse matrix factorized](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.factorized.html) documentation.

**Will this change the current api? How?**

Instead of erroring out, the `linalg` operations will work for sparse inputs just as well as for dense inputs.

**Who will benefit with this feature?**

I came across this while implementing [NVIDIA's FastPhotoStyle](https://github.com/NVIDIA/FastPhotoStyle) in TensorFlow 2.0 + Keras.

The algorithm essentially has 2 steps,

- A `PhotoWCT` transformation that is `WCT` but uses maxpooling argvalues as unpooling masks
- A Photorealistic smoothing step that restores geometric artifacts for objects distorted using regular style transform
- An additional post processing step that applies a GPU based smoothing filter for further reducing structural defects

The NVIDIA implementation provides a [`photo_wct` net implementation in PyTorch](https://github.com/NVIDIA/FastPhotoStyle/blob/master/photo_wct.py). However for the second step, [a CPU implementation in scipy is used](https://github.com/NVIDIA/FastPhotoStyle/blob/master/photo_smooth.py). The second step essentially solves a closed form system of equations. 

Consider [line 48](https://github.com/NVIDIA/FastPhotoStyle/blob/master/photo_smooth.py#L48), what it is essentially doing is creating a diagonal matrix of size (width x height) of the image. So for a 512x512 image, in dense form it will have to create a matrix of 2^36 floats.

The matrix is later used to solve a system of equations (https://github.com/NVIDIA/FastPhotoStyle/blob/master/photo_smooth.py#L52).

2^36 tf.float32 values will occupy 2^8 = 256GB of memory, which will definitely overflow. Ideally, I should be able to work with tensorflow's `linalg` module just the same as the scipy implementation. An additional flexibility with TensorFlow is that it will place operations on GPUs automatically. Additionally, I can put it in a keras `Lambda` layer to add it to a model.


**Any Other info.**

No"
27379,Docs load really slow ,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>
System information

    chrome
    Internet speed 120+ Mbps
    TensorFlow version: all
    Doc Link: all pages

It takes a really long time to load the docs. Navigating to another page can take 10+ seconds to load a new page."
27378,tensorflow-gpu(1.13.1) tf.gradients(),"I want to know where I'm wrong.



![](https://user-images.githubusercontent.com/42964873/55328415-e3919880-54be-11e9-99e9-c6d61e445c53.PNG)
![1](https://user-images.githubusercontent.com/42964873/55328494-0d4abf80-54bf-11e9-8beb-2734bc09f706.PNG)

"
27377,Docs load really slow,"**System information**
- chrome
- Internet speed 120+ Mbps
- TensorFlow version: all
- Doc Link: all pages

It takes a really long time to load the docs. Navigating to another page can take 10+ seconds to load a new page.


"
27376,# From tensorflow/models/research/ issue,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27375,ReadBinaryProto in tensorflow/core/platform/env.cc may report success on partial input,"The code is calling proto->ParseFromCodedStream(&coded_stream), without also calling coded_stream.ConsumedEntireMessage(). This can result in ReadBinaryProto reporting success after reading just part of the input stream, returning a partially instantiated proto from data that didn't actually encode the desired proto in binary format."
27374,"""Import"" exported model for training","Is it possible to ""import"" an exported model (i.e. **not** from a checkpoint) to continue training? If not: what are the essential parts?"
27364,Include TF in keras.models.load_model for custom functions,"**System information**
- TensorFlow version (you are using): Latest
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

```
from keras import models, layers
import tensorflow as tf
in_tensor = layers.Input((50, 1), name='InSpeech')
def sum_tensor(x):
  return tf.reduce_sum(x, 1)
sum_out = layers.Lambda(sum_tensor, name='SumTensor')(in_tensor)
k_model = models.Model(inputs=[in_tensor],
            outputs=[sum_out])
k_model.save('sum_model.h5')
```

Loading the model

```
models.load_model('sum_model.h5')
```

or 

```
tf.lite.TFLiteConverter.from_keras_model_file('sum_model.h5')
```

Results in the error message:
```
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py in sum_tensor(x)
      3 in_tensor = layers.Input((50, 1), name='InSpeech')
      4 def sum_tensor(x):
----> 5   return tf.reduce_sum(x, 1)
      6 sum_out = layers.Lambda(sum_tensor, name='SumTensor')(in_tensor)
      7 k_model = models.Model(inputs=[in_tensor],

NameError: name 'tf' is not defined
```

While running `models.load_model('sum_model.h5', custom_objects={'tf': tf})` works fine. Unfortunately with the tflite wrapper function to convert models you cannot specify custom objects.

**Will this change the current api? How?**

No, it would just make the default custom objects include tensorflow.

**Who will benefit with this feature?**

Anyone using simple tensorflow-derived lambda functions inside of the HDF5 models in Keras. In particular people hoping to convert the models to tflite.

"
27361,Sample code is having syntax error in TensorFlow nodejs code,"**System information**
- TensorFlow version:
- Doc Link:


**Syntax error in TensorFlow nodejs example**
[See sample code for Node.js usage](https://www.tensorflow.org/js/tutorials/setup)
Extra semicolon is causing the syntax error.

`onEpochEnd: (epoch, log) => console.log(`Epoch ${epoch}: loss = ${log.loss}`);`
In the above line, semi-colon in the end should be removed.
****
"
27360,TFLite Named Inputs and Outputs,"Using models with multiple inputs and outputs it would be helpful to have the names of the input and output channels available inside the Java and C++ APIs for TFLite.

**System information**
- TensorFlow version (you are using): latest
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

The model inputs and outputs can be listed and queried by size but not by name (no argument to get name of a `Tensor` or get input/output tensor names from an `Interpreter`. It would be very helpful for using models if you could get the names of the inputs and outputs particularly since many models have multiple of each (that are often not discriminated by size).

**Will this change the current api? How?**

It would just at methods `.getInputNames` and `.getOutputNames` to the `org.tensorflow.lite.Interpreter` class or add `.name` to the `org.tensorflow.lite.Tensor` class 

**Who will benefit with this feature?**

It would make it a lot easier to have backward compatible models (newer versions predict more features and code doesn't always have to use all of the outputs a model provides). It would also allow reusing the same model for different applications and have the code select the output with the correct name rather than hard coding an output index to take or checking by output size (my current solutions).

**Any Other info.**
"
27357,-D_GLIBCXX_DEBUG compiler flag causes prediction failure,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**


- OS Platform and Distribution Linux Ubuntu 18.04
- TensorFlow installed from source: v1.13.1
- Bazel version 0.21
- GCC/Compiler version: gcc version 7.3.0 (Ubuntu 7.3.0-27ubuntu1~18.04) 

**Describe the current behavior**

-D_GLIBCXX_DEBUG compiled example code gives:

Invalid argument: Must specify at least one target to fetch or execute.

Compiled without this flag:

Run session successfully
Tensor<type: float shape: [] values: 6>
output value: 6


**Code to reproduce the issue**

#include <tensorflow/core/platform/env.h>
#include <tensorflow/core/public/session.h>
#include <iostream>

int main(int argc, char **argv)
{

    tensorflow::GraphDef GraphDef;
    tensorflow::Session* Session = nullptr;
    tensorflow::Status Status;

    Status = tensorflow::ReadBinaryProto(tensorflow::Env::Default(), argv[1], &GraphDef);
    if (!Status.ok())
    {
      printf(""Error reading graph definition from %s: %s\n"", argv[1], Status.ToString().c_str());
      return false;
    }

    Session = tensorflow::NewSession(tensorflow::SessionOptions());
    if (Session == nullptr)
    {
      printf(""Could not create Tensorflow session.\n"");
      return false;
    }

    Status = Session->Create(GraphDef);
    if (!Status.ok())
    {
      printf(""Error creating graph: %s\n"", Status.ToString().c_str());
      return false;
    }

    //predict

    tensorflow::Tensor a(tensorflow::DT_FLOAT, tensorflow::TensorShape());
    a.scalar<float>()() = 3.0;

    tensorflow::Tensor b(tensorflow::DT_FLOAT, tensorflow::TensorShape());
    b.scalar<float>()() = 2.0;

    std::vector<std::pair<std::string, tensorflow::Tensor>> inputs = {
      { ""a"", a },
      { ""b"", b },
    };

    std::vector<tensorflow::Tensor> outputs;

    auto status = Session->Run(inputs, {""c""}, {}, &outputs);
    if (!status.ok()) {
      std::cerr << status.ToString() << std::endl;
      return 1;
    } else {
      std::cout << ""Run session successfully"" << std::endl;
    }

    auto output_c = outputs[0].scalar<float>();
    std::cout << outputs[0].DebugString() << std::endl;
    std::cout << ""output value: "" << output_c() << std::endl;
    Session->Close();

    return 0;
}

**Other info / logs**

The simple graph file attached - supply its path as a command line argument
[graph.pb.gz](https://github.com/tensorflow/tensorflow/files/3027851/graph.pb.gz)"
27356,tft.sparse_tensor_to_dense_with_shape throwing indices out of bound error on large dataset,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 - Dataflow runtime
- TensorFlow installed from: binary
- TensorFlow version: 1.13.1
- TensorFlow Transform: 0.12.0
- Python version: 2.7.x

**Describe the current behavior**
Tensorflow transform throwing error indices out of bound when processing large number of records (> 1mio) but running well on small records (< 10k).
Note that this tensorflow transform run on Beam pipeline on Google Cloud Dataflow.

**Describe the expected behavior**
Transform should be able to process large number of records.

**Code to reproduce the issue**
Tensorflow preprocessing function to convert sparse to dense tensor.

```
def preprocessing_fn(inputs):
        split = tf.string_split(
            inputs['words'], delimiter="""", skip_empty=True)

        token = tft.compute_and_apply_vocabulary(
            split, vocab_filename='char_token')

        token_pad = tft.sparse_tensor_to_dense_with_shape(
            token, shape=(None, 100))

        return {
            'label': inputs['label'],
            'token': token_pad,
        }
```

**Other info / logs**
Detailed logs: Captured from Stackdriver Dataflow logs.

```
indices[66312] = [4668,100] is out of bounds: need 0 <= index < [7710,100]
	 [[node transform/transform/SparseToDense (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_transform/saved/saved_transform_io.py:227) ]]

Caused by op u'transform/transform/SparseToDense', defined at:
  File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/usr/local/lib/python2.7/dist-packages/dataflow_worker/start.py"", line 86, in <module>
    main()
  File ""/usr/local/lib/python2.7/dist-packages/dataflow_worker/start.py"", line 82, in main
    batchworker.BatchWorker(properties, sdk_pipeline_options).run()
  File ""/usr/local/lib/python2.7/dist-packages/dataflow_worker/batchworker.py"", line 840, in run
    deferred_exception_details=deferred_exception_details)
  File ""/usr/local/lib/python2.7/dist-packages/dataflow_worker/batchworker.py"", line 642, in do_work
    work_executor.execute()
  File ""/usr/local/lib/python2.7/dist-packages/dataflow_worker/executor.py"", line 172, in execute
    op.start()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/impl.py"", line 436, in process
    lambda: self._make_graph_state(saved_model_dir))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/shared.py"", line 221, in acquire
    return _shared_map.acquire(self._key, constructor_fn)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/shared.py"", line 183, in acquire
    result = control_block.acquire(constructor_fn)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/shared.py"", line 85, in acquire
    result = constructor_fn()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/impl.py"", line 436, in <lambda>
    lambda: self._make_graph_state(saved_model_dir))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/impl.py"", line 412, in _make_graph_state
    self._exclude_outputs, tf_config)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/impl.py"", line 316, in __init__
    saved_model_dir, {}))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_transform/saved/saved_transform_io.py"", line 370, in partially_apply_saved_transform_internal
    saved_model_dir, logical_input_map, tensor_replacement_map)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_transform/saved/saved_transform_io.py"", line 227, in _partially_apply_saved_transform_impl
    input_map=input_map)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1435, in import_meta_graph
    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1457, in _import_meta_graph_with_return_elements
    **kwargs))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.py"", line 806, in import_scoped_meta_graph_with_return_elements
    return_elements=return_elements)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py"", line 442, in import_graph_def
    _ProcessNewOps(graph)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py"", line 235, in _ProcessNewOps
    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3433, in _add_new_tf_operations
    for c_op in c_api_util.new_tf_operations(self)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3325, in _create_op_from_tf_operation
    ret = Operation(c_op, self)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): indices[66312] = [4668,100] is out of bounds: need 0 <= index < [7710,100]
	 [[node transform/transform/SparseToDense (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_transform/saved/saved_transform_io.py:227) ]]
 while applying transform function for tensors [u'label', u'token']
```


"
27355,Dataset c++ extend documentation is outdated for tf 2.0 & DatasetV2,"**System information**
- TensorFlow version: 2.0
- Doc Link: https://www.tensorflow.org/guide/extend/formats

**Describe the documentation issue**

The C++ code to extend Dataset (especially since DatasetV2) is outdated.

Here is a working version of files needed in the documentation : https://github.com/vrince/tensorflow_addons/tree/master/tensorflow_addons/dataset

NOTE: the only part I am not really sure about is this one : https://github.com/vrince/tensorflow_addons/blob/master/tensorflow_addons/dataset/cc/my_dataset.cpp#L76 ... basically let it as it was but I don't see the point.

There is also and external test running from python and bazel files.

Not sure where or if I even can do a pull request to change the doc."
27353,tf.keras is ignoring specified step_per_epoch when keras doesn't,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):unknown
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):colab
- TensorFlow version (use command below):1.13
- Python version:python 3
- Bazel version (if compiling from source):unknown
- GCC/Compiler version (if compiling from source):unknown
- CUDA/cuDNN version:unknown
- GPU model and memory:unknown


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
`history = model.fit_generator(
      train_generator,
      steps_per_epoch=8,  
      epochs=15,
      verbose=1)`
If train_generator is a instance of Sequence.( in my case, coming from flow_from_directory), steps_per_epoch is overridden by len(train_generator) in this file [training_generator.py](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/keras/engine/training_generator.py#L371)
This doesn't happen in keras. In keras, steps_per_epoch is kept.

**Describe the expected behavior**
Use specified steps_per_epoch instead of len(data) if specified.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
I find this when doing a coursera online course. The notebook i used can be found at [link](https://github.com/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%208%20-%20Lesson%202%20-%20Notebook.ipynb)

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27352,"[ROCM] Build failed due to libhip_hcc.so, but HIP and HCC are installed","**System information**
- OS Platform and Distribution : Manjaro Linux x86_64
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 2.0.0-alpha0
- Python version: Python 3.7.2
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): Build label: 0.22.0- (@non-git)
- GCC/Compiler version : gcc 8.2.1 ; HCC clang version 9.0.0; clang version 7.0.1,
- CUDA/cuDNN version: NA / ROCm 2.2.0-1 ; ROCm-opencl-runtime 2.2.0-3, HIP version 2.2.0-1
- GPU model and memory: Vega 64, 8GB

**Describe the problem**
 Building with ROCm support fails.

Inside  2.0.0-alpha0 folder: 
- ./configure
- bazel build --config=opt --config=rocm //tensorflow/tools/pip_package:build_pip_package


**Any other info / logs**
```
INFO: Invocation ID: a35bccb9-a4af-40de-bede-9b7df9283be4
DEBUG: Rule 'build_bazel_rules_swift' modified arguments {""commit"": ""001736d056d7eae20f1f4da41bc9e6f036857296"", ""shallow_since"": ""1547844730 -0800""} and dropped [""tag""]
DEBUG: /home/auyer/.cache/bazel/_bazel_auyer/965b880f59234d9db802b221eaf21de8/external/build_bazel_rules_apple/apple/repositories.bzl:35:5:
WARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.

ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': in /home/auyer/tensorflow/tensorflow-2.0.0-alpha0/tensorflow/tensorflow.bzl: Encountered error while reading extension file 'rocm/build_defs.bzl': no such package '@local_config_rocm//rocm': Traceback (most recent call last):
	File ""/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl"", line 755
		_create_local_rocm_repository(repository_ctx)
	File ""/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl"", line 631, in _create_local_rocm_repository
		_find_libs(repository_ctx, rocm_config)
	File ""/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl"", line 376, in _find_libs
		_find_rocm_lib(""hip_hcc"", repository_ctx, cpu_value, ...)
	File ""/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl"", line 361, in _find_rocm_lib
		auto_configure_fail((""Cannot find rocm library %s"" %...))
	File ""/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl"", line 121, in auto_configure_fail
		fail((""\n%sROCm Configuration Error:%...)))

ROCm Configuration Error: Cannot find rocm library libhip_hcc.so
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/tools/pip_package': in /home/auyer/tensorflow/tensorflow-2.0.0-alpha0/tensorflow/tensorflow.bzl: Encountered error while reading extension file 'rocm/build_defs.bzl': no such package '@local_config_rocm//rocm': Traceback (most recent call last):
	File ""/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl"", line 755
		_create_local_rocm_repository(repository_ctx)
	File ""/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl"", line 631, in _create_local_rocm_repository
		_find_libs(repository_ctx, rocm_config)
	File ""/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl"", line 376, in _find_libs
		_find_rocm_lib(""hip_hcc"", repository_ctx, cpu_value, ...)
	File ""/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl"", line 361, in _find_rocm_lib
		auto_configure_fail((""Cannot find rocm library %s"" %...))
	File ""/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl"", line 121, in auto_configure_fail
		fail((""\n%sROCm Configuration Error:%...)))

ROCm Configuration Error: Cannot find rocm library libhip_hcc.so
INFO: Elapsed time: 0.066s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/pip_package
    Fetching @local_config_rocm; fetching
```"
27351,Broken link https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: API r1.13
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing


**Describe the documentation issue**
A _404 - Page not found_ error is displayed when clicking on the link in the paragraph below:

> Additional documentation can be found [on the keras site](https://keras.io/preprocessing/), 

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
27349,Tensorflow 2.0 BatchNorm not working,"

**System information**
- I written custom code
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from: pip
- TensorFlow version: 2.0
- Python version: 3.6
- CUDA/cuDNN version: 10.0/ 7.5
- GPU model and memory: GTX 1070/ 8 gig




 So I wrote a simple model using tf.keras and I was using batchnorm, It was working just fine then sudddenly it stopped working.


**logs**
Traceback (most recent call last):
  File ""/home/kislay/Documents/DeepQ/DQmodel.py"", line 30, in <module>
    print(dqn.callback(np.random.rand(4,210,160,1),random_action(4)))
  File ""/home/kislay/Documents/DeepQ/DQmodel.py"", line 18, in callback
    frame_stack = self.BatchNorm(frame_stack)
  File ""/home/kislay/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 660, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/kislay/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/normalization.py"", line 589, in call
    outputs = self._fused_batch_norm(inputs, training=training)
  File ""/home/kislay/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/normalization.py"", line 465, in _fused_batch_norm
    training, _fused_batch_norm_training, _fused_batch_norm_inference)
  File ""/home/kislay/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py"", line 56, in smart_cond
    pred, true_fn=true_fn, false_fn=false_fn, name=name)
  File ""/home/kislay/.local/lib/python3.6/site-packages/tensorflow/python/framework/smart_cond.py"", line 56, in smart_cond
    return false_fn()
  File ""/home/kislay/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/normalization.py"", line 462, in _fused_batch_norm_inference
    data_format=self._data_format)
  File ""/home/kislay/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py"", line 1206, in fused_batch_norm
    name=name)
  File ""/home/kislay/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 3931, in _fused_batch_norm
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: Could not find valid device for node.
Node: {{node FusedBatchNorm}}
All kernels registered for op FusedBatchNorm :
  device='XLA_CPU'; T in [DT_FLOAT]
  device='XLA_GPU'; T in [DT_FLOAT]
  device='XLA_CPU_JIT'; T in [DT_FLOAT]
  device='CPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_FLOAT]
  device='XLA_GPU_JIT
"
27346,benchmark_model is much slower than expected,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MI 8
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 
- Python version: 2.7
- Bazel version (if compiling from source): 0.22.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


**Describe the current behavior**
I use the benchmark_model to run the inference of models with tflite, but the speed of Mobilenet is rather slow on Snapdragon 845, which is nearly **166 ms**. (According to the [performance site](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/benchmarks.md), I expect that it could run faster than 100 ms.) I wonder whether it is built with NEON or not, and is there anything else I should do to make this problem fixed.

```
adb shell /data/local/tmp/benchmark_model  --graph=/data/local/tmp/tflite_model/mobilenet_v1_1.0_224.tflite   --num_threads=4 --warmup_runs=3 --num_runs=50 --use_nnapi=true
STARTING!
Min num runs: [50]
Min runs duration (seconds): [1]
Inter-run delay (seconds): [-1]
Num threads: [4]
Benchmark name: []
Output prefix: []
Min warmup runs: [3]
Min warmup runs duration (seconds): [0.5]
Graph: [/data/local/tmp/tflite_model/mobilenet_v1_1.0_224.tflite]
Input layers: []
Input shapes: []
Use nnapi : [1]
Allow fp16 : [0]
nnapi error: unable to open function ANeuralNetworksModel_relaxComputationFloat32toFloat16
Loaded model /data/local/tmp/tflite_model/mobilenet_v1_1.0_224.tflite
resolved reporter
INFO: Initialized TensorFlow Lite runtime.
Initialized session in 175.762ms
Running benchmark for at least 3 iterations and at least 0.5 seconds
count=4 first=157773 curr=166340 min=155120 max=170646 avg=162470 std=6283

Running benchmark for at least 50 iterations and at least 1 seconds
count=50 first=173787 curr=167988 min=151156 max=179750 avg=166446 std=6661

Average inference timings in us: Warmup: 162470, Init: 175762, no stats: 166446
```


**Describe the expected behavior**
The speed of Mobilenet's inference on Snapdragon 845 is expected faster than 100 ms. And how could I know whether the benchmark is built with NEON or not?


**Code to reproduce the issue**
1. `git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git`
2. 
```
./configure 
You have bazel 0.22.0 installed.
Please specify the location of python. [Default is /usr/bin/python]: 

Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: -march='armv8-a' -mfpu='neon'

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: y
Searching for NDK and SDK installations.

Please specify the home path of the Android NDK to use. [Default is /home/tfl/Android/Sdk/ndk-bundle]: 

Please specify the home path of the Android SDK to use. [Default is /home/tfl/Android/Sdk]: 

Please specify the Android SDK API level to use. [Available levels: ['26', '27', '28']] [Default is 28]: 27

Please specify an Android build tools version to use. [Available versions: ['26.0.3', '27.0.3', '28.0.3']] [Default is 28.0.3]: 27.0.3

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=gdr         	# Build with GDR support.
	--config=verbs       	# Build with libverbs support.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=noignite    	# Disable Apache Ignite support.
	--config=nokafka     	# Disable Apache Kafka support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
```
3. 
```
bazel build -c opt \
  --config=android_arm64 \
  --cxxopt='--std=c++11' \
  tensorflow/lite/tools/benchmark:benchmark_model
```
4. 
```
adb push bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model /data/local/tmp
adb shell chmod +x /data/local/tmp/benchmark_model
adb shell /data/local/tmp/benchmark_model  --graph=/data/local/tmp/tflite_model/mobilenet_v1_1.0_224.tflite   --num_threads=4 --warmup_runs=3 --num_runs=50 --use_nnapi=true
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27345,[TF2.0] EstimatorV2 uses non existing export_savedmodel method,"Dear tensorflowers,

as per the title, EstimatorV2's exporter.py calls a method that has been removed. The fix is pretty easy, but I'm not sure about the implications of the change. See more below. 

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: custom code, but using estimators
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 4.19.20-1rodete1-amd64 #1 SMP Debian 4.19.20-1rodete1 
- **TensorFlow installed from (source or binary)**: pip install --upgrade tensorflow==2.0.0alpha0
- **TensorFlow version (use command below)**: 2.0.0-alpha0
- **Python version**: 3.7.1

### Describe the problem
using `tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)` results in `AttributeError: 'EstimatorV2' object has no attribute 'export_savedmodel'` being thrown.

2 small modifications to my local `tensorflow_estimator/python/estimator/exporter.py` file fix the issue. However, before submitting a PR i wanted to clarify that I had to:

1. Rename the method call to `estimator.export_saved_model(...)` and this poses no problem.
2. Remove the `strip_default_attrs=self._strip_default_attrs` [keyword argument](https://github.com/tensorflow/estimator/blob/d14b0dce35baea00f27e17d8a44690080abd7bce/tensorflow_estimator/python/estimator/exporter.py#L120), but I have no idea what this entails. I assume that, since you can't specify this argument anymore, it defaults to the default policy in TF 1.X of stripping the GraphDef default attributes. If that's the case, is there any action required?

#### Logs

```
  File ""/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py"", line 473, in train_and_evaluate
    return executor.run()
  File ""/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py"", line 613, in run
    return self.run_local()
  File ""/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py"", line 714, in run_local
    saving_listeners=saving_listeners)
  File ""/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 359, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1139, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1173, in _train_model_default
    saving_listeners)
  File ""/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1452, in _train_with_estimator_spec
    any_step_done = True
  File ""/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 856, in __exit__
    self._close_internal(exception_type)
  File ""/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 889, in _close_internal
    h.end(self._coordinated_creator.tf_sess)
  File ""/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 588, in end
    self._save(session, last_step)
  File ""/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 607, in _save
    if l.after_save(session, step):
  File ""/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py"", line 519, in after_save
    self._evaluate(global_step_value)  # updates self.eval_result
  File ""/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py"", line 539, in _evaluate
    self._evaluator.evaluate_and_export())
  File ""/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py"", line 927, in evaluate_and_export
    is_the_final_export)
  File ""/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py"", line 960, in _export_eval_result
    is_the_final_export=is_the_final_export))
  File ""/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/exporter.py"", line 420, in export
    is_the_final_export)
  File ""/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/exporter.py"", line 120, in export
    export_result = estimator.export_savedmodel(
AttributeError: 'EstimatorV2' object has no attribute 'export_savedmodel'
```
##### Code

I'm trying to adapt the [CMLE template](https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/cloudml-template) to TF 2.0 . I avoided putting the input_fns and model_fns since the model trains and evals correctly. The only problem is during the export.

```python
 train_input_fn = model_input.generate_input_fn(
      file_names_pattern=FLAGS.train_files,
      mode=tf.estimator.ModeKeys.TRAIN,
      num_epochs=FLAGS.num_epochs,
      batch_size=FLAGS.train_batch_size)

  eval_input_fn = model_input.generate_input_fn(
      file_names_pattern=FLAGS.eval_files,
      mode=tf.estimator.ModeKeys.EVAL,
      batch_size=FLAGS.eval_batch_size)



  if metadata.TASK_TYPE == ""classification"":
    estimator = model.create_classifier(config=run_config)
  elif metadata.TASK_TYPE == ""regression"":
    estimator = model.create_regressor(config=run_config)
  else:
    estimator = model.create_estimator(config=run_config)

  train_spec = tf.estimator.TrainSpec(
      train_input_fn,
      max_steps=int(train_steps),
  )

  # Export for the final prediction graph.
  exporter_default = tf.estimator.FinalExporter(
      'estimator',
      model_input.SERVING_FUNCTIONS[FLAGS.export_format](),
      as_text=False )

  eval_spec = tf.estimator.EvalSpec(
      eval_input_fn,
      steps=FLAGS.eval_steps,
      exporters=[exporter_default],  # This tells
      throttle_secs=FLAGS.eval_every_secs,
      start_delay_secs=0)

  # Main train and evaluate loop.
  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)

```


Happy to provide any additional information!"
27344,q,
27343,j,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27342,Building tensorflow wheel met error `clang: error: no input files`,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 Darwin Kernel Version 18.0.0, Mac OS 10.14
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from:
build from source
- TensorFlow version:
Master branch
- Python version:
2.7
- Installed using virtualenv? pip? conda?:
No
- Bazel version (if compiling from source):
0.23.2
- GCC/Compiler version (if compiling from source):
Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/c++/4.2.1
- CUDA/cuDNN version:
without it
- GPU model and memory:
without GPU.


**Describe the problem**
I want to build tensorflow from source, use command:
```
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
```
And here is result:
```
Target //tensorflow/tools/pip_package:build_pip_package up-to-date:
  bazel-bin/tensorflow/tools/pip_package/build_pip_package
INFO: Elapsed time: 5926.109s, Critical Path: 623.43s
INFO: 10670 processes: 10670 local.
INFO: Build completed successfully, 11295 total actions
```
Then I try to build the `wheel`, use command:
```
./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
```
But got error:
```
warning: no files found matching '*.pyd' under directory '*'
warning: no files found matching '*.pd' under directory '*'
warning: no files found matching '*.dll' under directory '*'
warning: no files found matching '*.lib' under directory '*'
warning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'
warning: no files found matching '*' under directory 'tensorflow/include/Eigen'
warning: no files found matching '*.h' under directory 'tensorflow/include/google'
warning: no files found matching '*' under directory 'tensorflow/include/third_party'
warning: no files found matching '*' under directory 'tensorflow/include/unsupported'
clang: error: no input files
error: command 'clang' failed with exit status 1
```
"
27341,evaluate,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
27340,"Add a ""tf.contrib.*"" warning to tf_upgrade_v2 upgrade script","Tensorflow 1.13 and 2.0 come with the support of tf_upgrage_v2 script.
The script ignores imports and usage of parts of tf.contrib.* package that is no longer available in TF2.0. Due to that converted script fails with ""_ModuleNotFoundError: No module named 'tensorflow.contrib'_"". I suggest adding a user warning/error to the script to notify the user about incompatible library being used."
27339,TPU training with Keras API raises error in Tensorflow 2.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): no 
- TensorFlow version (use command below): 2.0.0.dev20190330
- Python version: 3.6
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: Google Colab Colud TPU

**The current behavior**

By following the [official](https://www.tensorflow.org/alpha/guide/distribute_strategy) guide on Tensorflow 2.0 distributed training, the `model.fit()` method raises an error regarding unregistered op type *( Op type not registered 'ExperimentalRebatchDataset' in binary running on n-23c14aca-w-0 )*.

**The expected behavior**

The expected bahaviour is running code without any exception.

**Code to reproduce the issue**

The code in the [google colab](https://gist.github.com/Mrpatekful/60401e8c7b8965bb3624c2c2e9b8df0f).

```python
!pip install tf-nightly-2.0-preview
from __future__ import absolute_import, division, print_function
import tensorflow as tf

tf.compat.v1.disable_eager_execution()

resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.tpu.experimental.initialize_tpu_system(resolver)
tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)

with tpu_strategy.scope():
  model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])
  model.compile(loss='mse', optimizer='sgd')

BATCH_SIZE_PER_REPLICA = 10
batch_size = BATCH_SIZE_PER_REPLICA * tpu_strategy.num_replicas_in_sync
dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(1000).batch(batch_size, drop_remainder=True)
model.fit(dataset, epochs=2)
model.evaluate(dataset)
```
**Other info / logs**

The full exception:

```
WARNING: Logging before flag parsing goes to stderr.
W0331 08:24:17.732600 140554474739584 training_utils.py:1268] Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1336     try:
-> 1337       return fn(*args)
   1338     except errors.OpError as e:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1319       # Ensure any changes to the graph are reflected in the runtime.
-> 1320       self._extend_graph()
   1321       return self._call_tf_sessionrun(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _extend_graph(self)
   1354     with self._graph._session_run_lock():  # pylint: disable=protected-access
-> 1355       tf_session.ExtendSession(self._session)
   1356 

NotFoundError: Op type not registered 'ExperimentalRebatchDataset' in binary running on n-23c14aca-w-0. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
<ipython-input-7-60cafe7e9d96> in <module>()
      2 batch_size = BATCH_SIZE_PER_REPLICA * tpu_strategy.num_replicas_in_sync
      3 dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(1000).batch(batch_size, drop_remainder=True)
----> 4 model.fit(dataset, epochs=2)
      5 model.evaluate(dataset)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    745             steps_per_epoch=steps_per_epoch,
    746             validation_steps=validation_steps,
--> 747             validation_freq=validation_freq)
    748 
    749     batch_size = self._validate_or_infer_batch_size(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py in fit_distributed(model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq)
    117         steps_per_epoch=steps_per_epoch,
    118         validation_steps=validation_steps,
--> 119         validation_freq=validation_freq)
    120   else:
    121     return training_arrays.fit_loop(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py in experimental_tpu_fit_loop(model, dataset, epochs, verbose, callbacks, initial_epoch, steps_per_epoch, val_dataset, validation_steps, validation_freq)
    310   # TODO(fchollet): add support for `steps_per_epoch=None` in TPU loops.
    311   current_strategy = model._distribution_strategy
--> 312   iterator = distributed_training_utils.get_iterator(dataset, current_strategy)
    313   steps_per_epoch = training_utils.infer_steps_for_dataset(
    314       dataset, steps_per_epoch, epochs, steps_name='steps_per_epoch')

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/distributed_training_utils.py in get_iterator(dataset, distribution_strategy)
    519   with distribution_strategy.scope():
    520     iterator = distribution_strategy.make_dataset_iterator(dataset)
--> 521   initialize_iterator(iterator, distribution_strategy)
    522   return iterator
    523 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/distributed_training_utils.py in initialize_iterator(iterator, distribution_strategy)
    527     init_op = control_flow_ops.group(iterator.initialize())
    528     if not context.executing_eagerly():
--> 529       K.get_session((init_op,)).run(init_op)
    530 
    531 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    930     try:
    931       result = self._run(None, fetches, feed_dict, options_ptr,
--> 932                          run_metadata_ptr)
    933       if run_metadata:
    934         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1153     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1154       results = self._do_run(handle, final_targets, final_fetches,
-> 1155                              feed_dict_tensor, options, run_metadata)
   1156     else:
   1157       results = []

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1329     if handle is None:
   1330       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1331                            run_metadata)
   1332     else:
   1333       return self._do_call(_prun_fn, handle, feeds, fetches)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1349           pass
   1350       message = error_interpolation.interpolate(message, self._graph)
-> 1351       raise type(e)(node_def, op, message)
   1352 
   1353   def _extend_graph(self):

NotFoundError: Op type not registered 'ExperimentalRebatchDataset' in binary running on n-23c14aca-w-0. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.
```
"
27338,Sequence generator as validation_data not working if Flatten() involved in model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10
- GPU model and memory: GTX1060 6GB

**Describe the current behavior**

Sequence generator as validation_data not working if Flatten() layer involved in model

**Describe the expected behavior**

I expect it to work as expected

**Code to reproduce the issue**

Define basic stuffs
```python3
import numpy as np

import tensorflow as tf
import tensorflow.keras as tfk
Sequence = tfk.utils.Sequence

Dense = tfk.layers.Dense
Input = tfk.layers.Input
Flatten = tfk.layers.Flatten
Model = tfk.models.Model


class CustomGenerator(Sequence):
    def __init__(self, batch_size, shuffle, steps_per_epoch, data):
        self.inputs = data[0]
        self.labels = data[1]
        self.shuffle = shuffle
        self.batch_size = batch_size
        self.steps_per_epoch = steps_per_epoch

        # initial idx
        self.idx_list = self._get_exploration_order(range(self.inputs.shape[0]))
        self.current_idx = 0

    def __len__(self):
        return self.steps_per_epoch

    def _get_exploration_order(self, idx_list):
        """"""
        :param idx_list:
        :return:
        """"""
        # shuffle (if applicable) and find exploration order
        if self.shuffle is True:
            idx_list = np.copy(idx_list)
            np.random.shuffle(idx_list)

        return idx_list

    def _data_generation(self, inputs, labels, idx_list_temp):
        x = inputs[idx_list_temp]
        y = labels[idx_list_temp]
        return x, y

    def __getitem__(self, index):
        x, y = self._data_generation(self.inputs,
                                     self.labels,
                                     self.idx_list[self.current_idx:self.current_idx + self.batch_size])
        self.current_idx += self.batch_size
        return x, y

    def on_epoch_end(self):
        # shuffle the list when epoch ends for the next epoch
        self.idx_list = self._get_exploration_order(range(self.inputs.shape[0]))
        # reset counter
        self.current_idx = 0

# Model 1 which does not have Flatten
input_tensor = Input(shape=[200], name='input')
output_tensor = Dense(units=10, name='output')(input_tensor)
neuralnet = Model(inputs=input_tensor, outputs=output_tensor)
neuralnet.compile(loss='mse', optimizer='adam')

# Model 2 which has Flatten
input_tensor = Input(shape=[200, 1], name='input')
flat = Flatten()(input_tensor)
output_tensor = Dense(units=10, name='output')(flat)
neuralnet_flat = Model(inputs=input_tensor, outputs=output_tensor)
neuralnet_flat.compile(loss='mse', optimizer='adam')
```

This works as expected
```python3
predgen = CustomGenerator(batch_size=64, shuffle=True, steps_per_epoch=10, data=[np.random.normal(size=(700, 200, 1)), np.random.normal(size=(700, 10))])

neuralnet_flat.fit_generator(generator=predgen, epochs=5)
```

This also works
```python3
predgen = CustomGenerator(batch_size=64, shuffle=True, steps_per_epoch=10, data=[np.random.normal(size=(700, 200)), np.random.normal(size=(700, 10))])
valgen = CustomGenerator(batch_size=64, shuffle=True, steps_per_epoch=1, data=[np.random.normal(size=(64, 200)), np.random.normal(size=(64, 10))])

neuralnet.fit_generator(generator=predgen, validation_data=valgen, epochs=5)
```

But this is not working
```python3
predgen = CustomGenerator(batch_size=64, shuffle=True, steps_per_epoch=10, data=[np.random.normal(size=(700, 200, 1)), np.random.normal(size=(700, 10))])
valgen = CustomGenerator(batch_size=64, shuffle=True, steps_per_epoch=1, data=[np.random.normal(size=(64, 200, 1)), np.random.normal(size=(64, 10))])

# does not work
neuralnet_flat.fit_generator(generator=predgen, validation_data=valgen, epochs=5)
```


**Other info / logs**
```
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-2-ced9d06428c4> in <module>
      3
      4 # does not work
----> 5 neuralnet_flat.fit_generator(generator=predgen, validation_data=valgen, epochs=5)

~\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1424         use_multiprocessing=use_multiprocessing,
   1425         shuffle=shuffle,
-> 1426         initial_epoch=initial_epoch)
   1427
   1428   def evaluate_generator(self,

~\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)
    223           use_multiprocessing=use_multiprocessing,
    224           max_queue_size=max_queue_size,
--> 225           mode='test')
    226
    227       if not isinstance(val_results, list):

~\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)
    189       progbar.on_batch_begin(step, batch_logs)
    190
--> 191       batch_outs = batch_function(*batch_data)
    192       if not isinstance(batch_outs, list):
    193         batch_outs = [batch_outs]

~\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in test_on_batch(self, x, y, sample_weight, reset_metrics)
   1254       else:
   1255         self._make_eval_function()
-> 1256         outputs = self._eval_function(inputs)  # pylint: disable=not-callable
   1257
   1258     if reset_metrics:

~\Anaconda3\lib\site-packages\tensorflow\python\keras\backend.py in __call__(self, inputs)
   3074
   3075     fetched = self._callable_fn(*array_vals,
-> 3076                                 run_metadata=self.run_metadata)
   3077     self._call_fetch_callbacks(fetched[-len(self._fetches):])
   3078     return nest.pack_sequence_as(self._outputs_structure,

~\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in __call__(self, *args, **kwargs)
   1437           ret = tf_session.TF_SessionRunCallable(
   1438               self._session._session, self._handle, args, status,
-> 1439               run_metadata_ptr)
   1440         if run_metadata:
   1441           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    526             None, None,
    527             compat.as_text(c_api.TF_Message(self.status.status)),
--> 528             c_api.TF_GetCode(self.status.status))
    529     # Delete the underlying status object from memory otherwise it stays alive
    530     # as there is a reference to status from this from the traceback due to

InvalidArgumentError: Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero
         [[{{node flatten/Reshape}}]]
         [[{{node ConstantFoldingCtrl/loss_1/output_loss/MeanSquaredError/weighted_loss/broadcast_weights/assert_broadcastable/AssertGuard/Switch_0}}]]
```"
27336,Add examples of loading model that has been saved with save_keras_model,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.13
- Doc Link: https://www.tensorflow.org/tutorials/images/hub_with_keras


**Describe the documentation issue**

The above example provides no example of loading the model after it's saved. I'm used to using the ""old style"" of freezing, optimizing (`optimize_for_inference_lib.optimize_for_inference`), and saving models with:
`tf.train.write_graph(output_graph_def, output_dir, 'model.pb', as_text=False)`

Which gives me a graph: `model.pb`. I run predictions on this graph locally on my mobile applications with full Tensorflow (in Unity, so not using TF-Lite).

In my local prediction script, I load the graph like this:
```python
def load_graph(model_file):
  graph = tf.Graph()
  graph_def = tf.GraphDef()

  with open(model_file, ""rb"") as f:
    graph_def.ParseFromString(f.read())
  with graph.as_default():
    tf.import_graph_def(graph_def)
  return graph
```
Then grab the graph input and output ops and run a prediction session. This works pretty well.

However, this new example uses `tf.contrib.saved_model.save_keras_model` to export the model, which I would love to use because it exports all kinds of useful information (which I'd like to use in conjunction with TF Serving for certain applications), and it looks to be making it into TF 2.0 core.

With `save_keras_model`, I end up with:
`assets  labels.csv  saved_model.pb  variables`

When trying to use my old graph loading script with this newly generated model with `save_keras_model`, I get an error like this:
```bash
...
    graph_def.ParseFromString(f.read())
google.protobuf.message.DecodeError: Error parsing message
```
This had me puzzled, as I expected this `.pb` graph generated with the new API to work the same as the one I generated with `tf.train.write_graph`. Browsing through their contents, they look the same, but the newer API's model is smaller in file size. Which makes me wonder if it's missing something.

I found an alternative method to load it as a graph anyways, but I ran into issues down the road with running it's ops, so I'm not sure I'm approaching this correctly by loading it as a graph, or if there's a ""new, Keras way"" of loading a model and running predictions. Maybe I'm too far in the weeds with trying to load this model as a graph and run it's ops, I'd expect there's a higher level API to do this.


So I have a few questions:
- Is the exported `saved_model.pb` frozen?
- Is it optimized? If not, I can convert the model I have to a graph, then back to a model for something like `save_keras_model`?

**Ultimately**, can an example be added to this tutorial that demonstrates how to load a saved model?


I wrote our initial codebase using TF 1.7 a little while back and I haven't changed much since then, but the API is changing a lot and I want to take full advantage of the advancements. Also I understand if this issue should be moved, I assumed this was appropriate since there was no documentation demonstrating loading a saved model with the new API.

Thank you! 

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Yes
"
27329,Unable to convert frozen graph to usable model on iPhone,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.13.1
- Python version: 2.7.15rc1
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.0
- GPU model and memory: GeForce GTX 1080


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**


I am fine-tuning the [pretrained SSD-MobileNetV1](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz ) model using the `/models/research/object_detection/samples` [config file](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_mobilenet_v1_pets.config) for detecting the bounding boxes of objects in an image. 


```
python model_main.py --logtostderr --model_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config
```



After training, I am freezing the model using 

```
python export_inference_graph.py --input_type=image_tensor --pipeline_config_path=training/ssd_mobilenet_v1_pets.config --output_directory=inference_graph --trained_checkpoint_prefix=training/model.ckpt-3740 
```

After the `frozen_inference_graph.pb`  is being generated, I'd like to convert the model to a model format that is compatible with iPhones. I tried the following approaches: 

### Approach 1 - Convert to mlmodel

```
$ python convert_to_mlmodel.py 
WARNING:root:TensorFlow version 1.13.1 detected. Last version known to be fully compatible is 1.12.0 .

Loading the TF graph...
2019-03-30 18:39:48.956255: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-30 18:39:49.156054: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5592d6f7e880 executing computations on platform CUDA. Devices:
2019-03-30 18:39:49.156083: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2019-03-30 18:39:49.177932: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3398155000 Hz
2019-03-30 18:39:49.178509: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5592d6fdf420 executing computations on platform Host. Devices:
2019-03-30 18:39:49.178548: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-03-30 18:39:49.178847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 10.92GiB freeMemory: 211.44MiB
2019-03-30 18:39:49.178889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-03-30 18:39:49.179849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-30 18:39:49.179872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-03-30 18:39:49.179882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-03-30 18:39:49.180081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 211 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Graph Loaded.
Traceback (most recent call last):
  File ""convert_to_mlmodel.py"", line 4, in <module>
    output_feature_names = ['softmax:0'])
  File ""/home/deepvision/.local/lib/python2.7/site-packages/tfcoreml/_tf_coreml_converter.py"", line 586, in convert
    custom_conversion_functions=custom_conversion_functions)
  File ""/home/deepvision/.local/lib/python2.7/site-packages/tfcoreml/_tf_coreml_converter.py"", line 167, in _convert_pb_to_mlmodel
    OPS = _topological_sort_ops(OPS)
  File ""/home/deepvision/.local/lib/python2.7/site-packages/tfcoreml/_tf_graph_transform.py"", line 194, in _topological_sort_ops
    _push_stack(stack, node, in_stack)
  File ""/home/deepvision/.local/lib/python2.7/site-packages/tfcoreml/_tf_graph_transform.py"", line 38, in _push_stack
    raise ValueError('Graph has cycles.')
ValueError: Graph has cycles.
```

The conversion code looks like:

```
import tfcoreml as tf_converter
tf_converter.convert(tf_model_path = 'frozen_inference_graph.pb',
                     mlmodel_path = 'my_model.mlmodel',
                     output_feature_names = ['softmax:0'])
```

Tried the above with Faster RCNN ResNet101 and SSD MobileNet - both give the same error.

Not sure why the `output_feature_names` has to be a `softmax` as its a detection problem.



### Approach 2 - Convert to tflite 

```
$ tflite_convert --output_file=output.tflite --saved_model_dir=/home/deepvision/code/models/research/object_detection/inference_graph/saved_model/
2019-03-30 19:22:30.354332: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-30 19:22:30.757830: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5592ab466420 executing computations on platform CUDA. Devices:
2019-03-30 19:22:30.757861: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2019-03-30 19:22:30.785890: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3398155000 Hz
2019-03-30 19:22:30.791145: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5592ab498d20 executing computations on platform Host. Devices:
2019-03-30 19:22:30.791161: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-03-30 19:22:30.791279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 10.92GiB freeMemory: 69.38MiB
2019-03-30 19:22:30.791294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-03-30 19:22:30.791745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-30 19:22:30.791755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-03-30 19:22:30.791762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-03-30 19:22:30.791844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 69 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
WARNING:tensorflow:From /home/deepvision/.local/lib/python2.7/site-packages/tensorflow/lite/python/convert_saved_model.py:61: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
2019-03-30 19:22:31.298973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-03-30 19:22:31.299020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-30 19:22:31.299032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-03-30 19:22:31.299040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-03-30 19:22:31.299186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 69 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
WARNING:tensorflow:From /home/deepvision/.local/lib/python2.7/site-packages/tensorflow/lite/python/convert_saved_model.py:275: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.compat.v1.graph_util.convert_variables_to_constants
WARNING:tensorflow:From /home/deepvision/.local/lib/python2.7/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.compat.v1.graph_util.extract_sub_graph
Traceback (most recent call last):
  File ""/home/deepvision/.local/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/home/deepvision/.local/lib/python2.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 442, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/deepvision/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/deepvision/.local/lib/python2.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 438, in run_main
    _convert_model(tflite_flags)
  File ""/home/deepvision/.local/lib/python2.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 191, in _convert_model
    output_data = converter.convert()
  File ""/home/deepvision/.local/lib/python2.7/site-packages/tensorflow/lite/python/lite.py"", line 411, in convert
    ""invalid shape '{1}'."".format(_tensor_name(tensor), shape_list))
ValueError: None is only supported in the 1st dimension. Tensor 'image_tensor' has invalid shape '[None, None, None, 3]'.
```

Also, tried the following:

` tflite_convert --output_file=output.tflite --graph_def_file=frozen_inference_graph.pb --input_arrays=input --output_arrays=`

But, I am not sure what the last layer is, as the model is fairly convoluted. Tried visualizing with Netron but 

**Describe the expected behavior**
Seamless conversion to tflite and mlmodel files.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
NA

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
NA
"
27326,DLL load failure,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip
- TensorFlow version: 1.12
- Python version: 3.6.4
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA v9.0
- GPU model and memory: GTX 1060 6gb



**Describe the problem**
Basically trying to import tensor results in an ""ImportError: DLL load failure."" I already have cudart64_90.dll installed to its destination along with other necessary files. A complete re-install of TensorFlow does not fix this problem. Checked the CUDA paths only to find no issues.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
`import tensorflow`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

> Traceback (most recent call last):
>   File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
>     from tensorflow.python.pywrap_tensorflow_internal import *
>   File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
>     _pywrap_tensorflow_internal = swig_import_helper()
>   File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
>   File ""C:\Python\Python36\lib\imp.py"", line 243, in load_module
>     return load_dynamic(name, filename, file)
>   File ""C:\Python\Python36\lib\imp.py"", line 343, in load_dynamic
>     return _load(spec)
> ImportError: DLL load failed: The specified module could not be found.
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""C:/PycharmProjects/TensPlease/CanITens.py"", line 7, in <module>
>     import tensorflow as tf
>   File ""C:\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
>   File ""C:\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
>     from tensorflow.python import pywrap_tensorflow
>   File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
>     raise ImportError(msg)
> ImportError: Traceback (most recent call last):
>   File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
>     from tensorflow.python.pywrap_tensorflow_internal import *
>   File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
>     _pywrap_tensorflow_internal = swig_import_helper()
>   File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
>   File ""C:\Python\Python36\lib\imp.py"", line 243, in load_module
>     return load_dynamic(name, filename, file)
>   File ""C:\Python\Python36\lib\imp.py"", line 343, in load_dynamic
>     return _load(spec)
> ImportError: DLL load failed: The specified module could not be found.

"
27325,TFLite delegates/gpu/libmetal_delegate.a is missing,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave (10.14.1)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Not installed.
- TensorFlow version: master branch (latest commit 6353d940289a225cfbc104cc647b3c6970077faa)
- Python version: 3.7.2
- Installed using virtualenv? pip? conda?: Not installed, just calling shell scripts from repo
- Bazel version (if compiling from source): 0.18.0
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**
Attempting to build iOS Metal TFLite Delegate with `create_ios_frameworks.sh` yields the following error:

```
$ tensorflow/lite/lib_package/create_ios_frameworks.sh -g
Starting
File /path/to/tensorflow/tensorflow/lite/lib_package/../delegates/gpu/libmetal_delegate.a doesn't exist.
It's requried for building TFLite Framework with GPU. Aborting.
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

This is how I am attempting to create the iOS framework:
```
$ tensorflow/lite/tools/make/download_dependencies.sh
$ tensorflow/lite/tools/make/build_ios_universal_lib.sh
$ tensorflow/lite/lib_package/create_ios_frameworks.sh -g
```

I saw that the `-g` flag got added to `create_ios_frameworks.sh` in 59d535a0df17eaf3033bbff73ef4e1e1988c454e. Without the flag, I am able to successfully build the framework but, as expected, the GPU is not utilized.

I know the GPU delegates only got open-sourced a couple of days ago, and before that I had unsurprisingly been getting the same error but with `metal_delegate.h`, which was added in fb772b781b011471dec443e1f3cd6b664958b767. Is `libmetal_delegate.a` supposed to be present or is it still pending open-sourcing?

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

N/A"
27323,There is no accuracy graph on the Tensorboard,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
27322,Unable to import tensorflow on googlecloud with gpu,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 on gcloud
-
- TensorFlow installed from (source or binary): not sure 
- TensorFlow version: 1.13.
- Python version: 3.5 
- Installed using virtualenv? pip? conda?: pip 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: 10 
- GPU model and memory: Nvidia K-80 15 GB



**Describe the problem**
I created a gpu vm instance on google cloud and followed the steps to install tensorflow-gpu 


**Provide the exact sequence of commands / steps that you executed before running into the problem**

# update apt-get
sudo apt-get update
 
# work as root
sudo su

#!/bin/bash
echo ""Checking for CUDA and installing.""
# Check for CUDA and try to install.
if ! dpkg-query -W cuda; then
    curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.0.176-1_amd64.deb
    sudo dpkg -i cuda-repo-ubuntu1604_9.0.176-1_amd64.deb
    sudo apt-get update
    sudo apt-get install cuda-9-0
    sudo nvidia-smi -pm 1
fi


Test that your GPU is sucessfully installed:
# check that GPU is working
nvidia-smi


Install your Deep Neural Network (cuDNN) binaries that you uploaded earlier (check your version):

sudo dpkg -i libcudnn7_7.4.2.24-1+cuda10.0_amd64.deb

echo 'export CUDA_HOME=/usr/local/cuda' >> ~/.bashrc
echo 'export PATH=$PATH:$CUDA_HOME/bin' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# Install tensorflow
sudo apt-get install python3-dev python3-pip libcupti-dev

# install tensorflow-gpu
sudo pip3 install tensorflow-gpu

# install ipython3
apt install ipython3

**Any other info / logs**

here is my error file when i do python3 import tensorflow as tf 

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <modul
e>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_i
mport_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory
Failed to load the native TensorFlow runtime.
"
27320,Can not build lite/delegate/gpu on Raspberry Pi,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux (Raspbian)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1bf6646b871d0ce601715f8ed2f50430ca504da7
- Python version: -
- Installed using virtualenv? pip? conda?: -
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source):  gcc version 6.3.0 20170516 (Raspbian 6.3.0-18+rpi1+deb9u1)
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the problem**

Probably, version of gcc is older on Raspbian OS. I attached log on this issue (see bottom line of this description).

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Sorry, I didn't use bazel since bazel take huge CPU/Memory resources for Raspberry Pi. I tried to build gpu with following Makefile. I put this Makefile on lite/delegate/gpu. However, this is not a matter for this error may be.

https://gist.github.com/mattn/0dfdee17e025066e4a2386ee3a6af3b0

**Any other info / logs**

<details>
<pre>
g++ -std=c++14 -c -DTF_COMPILE_LIBRARY -DGL_GLEXT_PROTOTYPES -I/mnt/usbdisk2/tensorflow -I/mnt/usbdisk2/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/mnt/usbdisk2/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/mnt/usbdisk2/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I. gl/api.cc -o gl/api.o
In file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/object_manager.h:27:0,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:29,
                 from gl/api.cc:16:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h: In constructor tflite::gpu::gl::gl_buffer_internal::BufferId::BufferId():
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:154:10: error: request for member IgnoreError in tflite::gpu::gl::gl_call_internal::CallAndCheckError<void (*)(int, unsigned int*), int (*)(), {int, unsigned int*}>(std::__cxx11::basic_string<char>(((const char*)""glGenBuffers in /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:153""), std::allocator<char>()), glGenBuffers, tflite::gpu::gl::GetOpenGlErrors, 1, (&((tflite::gpu::gl::gl_buffer_internal::BufferId*)this)->tflite::gpu::gl::gl_buffer_internal::BufferId::id_)), which is of non-class type int
         .IgnoreError();
          ^~~~~~~~~~~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h: In destructor tflite::gpu::gl::gl_buffer_internal::BufferId::~BufferId():
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:162:52: error: request for member IgnoreError in tflite::gpu::gl::gl_call_internal::CallAndCheckError<void (*)(int, const unsigned int*), int (*)(), {int, unsigned int*}>(std::__cxx11::basic_string<char>(((const char*)""glDeleteBuffers in /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:162""), std::allocator<char>()), glDeleteBuffers, tflite::gpu::gl::GetOpenGlErrors, 1, (&((tflite::gpu::gl::gl_buffer_internal::BufferId*)this)->tflite::gpu::gl::gl_buffer_internal::BufferId::id_)), which is of non-class type int
       TFLITE_GPU_CALL_GL(glDeleteBuffers, 1, &id_).IgnoreError();
                                                    ^~~~~~~~~~~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h: In constructor tflite::gpu::gl::gl_buffer_internal::BufferBinder::BufferBinder(GLenum, GLuint):
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:182:51: error: request for member IgnoreError in tflite::gpu::gl::gl_call_internal::CallAndCheckError<void (*)(unsigned int, unsigned int), int (*)(), {const unsigned int&, unsigned int&}>(std::__cxx11::basic_string<char>(((const char*)""glBindBuffer in /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:182""), std::allocator<char>()), glBindBuffer, tflite::gpu::gl::GetOpenGlErrors, ((tflite::gpu::gl::gl_buffer_internal::BufferBinder*)this)->tflite::gpu::gl::gl_buffer_internal::BufferBinder::target_, id), which is of non-class type int
     TFLITE_GPU_CALL_GL(glBindBuffer, target_, id).IgnoreError();
                                                   ^~~~~~~~~~~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h: In destructor tflite::gpu::gl::gl_buffer_internal::BufferBinder::~BufferBinder():
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:186:50: error: request for member IgnoreError in tflite::gpu::gl::gl_call_internal::CallAndCheckError<void (*)(unsigned int, unsigned int), int (*)(), {const unsigned int&, int}>(std::__cxx11::basic_string<char>(((const char*)""glBindBuffer in /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:186""), std::allocator<char>()), glBindBuffer, tflite::gpu::gl::GetOpenGlErrors, ((tflite::gpu::gl::gl_buffer_internal::BufferBinder*)this)->tflite::gpu::gl::gl_buffer_internal::BufferBinder::target_, 0), which is of non-class type int
     TFLITE_GPU_CALL_GL(glBindBuffer, target_, 0).IgnoreError();
                                                  ^~~~~~~~~~~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h: In destructor tflite::gpu::gl::gl_buffer_internal::BufferMapper::~BufferMapper():
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:200:64: error: request for member IgnoreError in tflite::gpu::gl::gl_call_internal::CallAndCheckError<unsigned char (*)(unsigned int), int (*)(), {const unsigned int&}>(std::__cxx11::basic_string<char>(((const char*)""glUnmapBuffer in /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:200""), std::allocator<char>()), glUnmapBuffer, tflite::gpu::gl::GetOpenGlErrors, ((tflite::gpu::gl::gl_buffer_internal::BufferMapper*)this)->tflite::gpu::gl::gl_buffer_internal::BufferMapper::target_), which is of non-class type int
   ~BufferMapper() { TFLITE_GPU_CALL_GL(glUnmapBuffer, target_).IgnoreError(); }
                                                                ^~~~~~~~~~~
In file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/model.h:29:0,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:24,
                 from gl/api.cc:16:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h: In function int tflite::gpu::gl::CreateReadWriteShaderStorageBuffer(uint32_t, tflite::gpu::gl::GlBuffer*):
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member ok in status2, which is of non-class type const int
     if (!status2.ok()) return status2; \
                  ^
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:217:3: note: in expansion of macro RETURN_IF_ERROR
   RETURN_IF_ERROR(TFLITE_GPU_CALL_GL(glBufferData, GL_SHADER_STORAGE_BUFFER,
   ^~~~~~~~~~~~~~~
In file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/object_manager.h:28:0,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:29,
                 from gl/api.cc:16:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h: In constructor tflite::gpu::gl::gl_texture_internal::TextureId::TextureId():
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h:165:10: error: request for member IgnoreError in tflite::gpu::gl::gl_call_internal::CallAndCheckError<void (*)(int, unsigned int*), int (*)(), {int, unsigned int*}>(std::__cxx11::basic_string<char>(((const char*)""glGenTextures in /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h:164""), std::allocator<char>()), glGenTextures, tflite::gpu::gl::GetOpenGlErrors, 1, (&((tflite::gpu::gl::gl_texture_internal::TextureId*)this)->tflite::gpu::gl::gl_texture_internal::TextureId::id_)), which is of non-class type int
         .IgnoreError();
          ^~~~~~~~~~~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h: In destructor tflite::gpu::gl::gl_texture_internal::TextureId::~TextureId():
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h:172:53: error: request for member IgnoreError in tflite::gpu::gl::gl_call_internal::CallAndCheckError<void (*)(int, const unsigned int*), int (*)(), {int, unsigned int*}>(std::__cxx11::basic_string<char>(((const char*)""glDeleteTextures in /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h:172""), std::allocator<char>()), glDeleteTextures, tflite::gpu::gl::GetOpenGlErrors, 1, (&((tflite::gpu::gl::gl_texture_internal::TextureId*)this)->tflite::gpu::gl::gl_texture_internal::TextureId::id_)), which is of non-class type int
       TFLITE_GPU_CALL_GL(glDeleteTextures, 1, &id_).IgnoreError();
                                                     ^~~~~~~~~~~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h: In constructor tflite::gpu::gl::gl_texture_internal::TextureBinder::TextureBinder(GLenum, GLuint):
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h:192:52: error: request for member IgnoreError in tflite::gpu::gl::gl_call_internal::CallAndCheckError<void (*)(unsigned int, unsigned int), int (*)(), {const unsigned int&, unsigned int&}>(std::__cxx11::basic_string<char>(((const char*)""glBindTexture in /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h:192""), std::allocator<char>()), glBindTexture, tflite::gpu::gl::GetOpenGlErrors, ((tflite::gpu::gl::gl_texture_internal::TextureBinder*)this)->tflite::gpu::gl::gl_texture_internal::TextureBinder::target_, id), which is of non-class type int
     TFLITE_GPU_CALL_GL(glBindTexture, target_, id).IgnoreError();
                                                    ^~~~~~~~~~~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h: In destructor tflite::gpu::gl::gl_texture_internal::TextureBinder::~TextureBinder():
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h:196:51: error: request for member IgnoreError in tflite::gpu::gl::gl_call_internal::CallAndCheckError<void (*)(unsigned int, unsigned int), int (*)(), {const unsigned int&, int}>(std::__cxx11::basic_string<char>(((const char*)""glBindTexture in /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_texture.h:196""), std::allocator<char>()), glBindTexture, tflite::gpu::gl::GetOpenGlErrors, ((tflite::gpu::gl::gl_texture_internal::TextureBinder*)this)->tflite::gpu::gl::gl_texture_internal::TextureBinder::target_, 0), which is of non-class type int
     TFLITE_GPU_CALL_GL(glBindTexture, target_, 0).IgnoreError();
                                                   ^~~~~~~~~~~
In file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/model.h:29:0,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:24,
                 from gl/api.cc:16:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/runtime/shared_buffer.h: In member function int tflite::gpu::gl::SharedBufferData::CreateSharedGlBuffer(tflite::gpu::gl::GlBuffer*):
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member ok in status2, which is of non-class type const int
     if (!status2.ok()) return status2; \
                  ^
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/runtime/shared_buffer.h:62:5: note: in expansion of macro RETURN_IF_ERROR
     RETURN_IF_ERROR(TFLITE_GPU_CALL_GL(glBufferData, GL_SHADER_STORAGE_BUFFER,
     ^~~~~~~~~~~~~~~
In file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/runtime.h:30:0,
                 from gl/api.cc:35:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/runtime/shared_buffer.h:67:21: error: cannot convert tflite::gpu::Status to int in return
     return OkStatus();
                     ^
gl/api.cc: In member function virtual int tflite::gpu::gl::{anonymous}::InferenceContextImpl::Execute():
gl/api.cc:61:69: error: cannot convert tflite::gpu::Status to int in return
       return FailedPreconditionError(""InferenceContext is not reset"");
                                                                     ^
gl/api.cc: In member function virtual int tflite::gpu::gl::{anonymous}::InferenceContextImpl::Reset():
gl/api.cc:71:21: error: cannot convert tflite::gpu::Status to int in return
     return OkStatus();
                     ^
gl/api.cc: In member function virtual int tflite::gpu::gl::{anonymous}::InferenceContextWithBatchImpl::Execute():
gl/api.cc:97:69: error: cannot convert tflite::gpu::Status to int in return
       return FailedPreconditionError(""InferenceContext is not reset"");
                                                                     ^
gl/api.cc:113:78: error: cannot convert tflite::gpu::Status to int in return
             ""Object "", id, "" does not match expected byte size: "", byte_size));
                                                                              ^
gl/api.cc:122:35: error: cannot convert tflite::gpu::Status to int in return
               "" vs "", num_batches));
                                   ^
gl/api.cc:137:67: error: cannot convert tflite::gpu::Status to int in return
                 absl::StrCat(""Reference to "", id, "" is not found""));
                                                                   ^
In file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/model.h:29:0,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:24,
                 from gl/api.cc:16:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member ok in status2, which is of non-class type const int
     if (!status2.ok()) return status2; \
                  ^
gl/api.cc:139:11: note: in expansion of macro RETURN_IF_ERROR
           RETURN_IF_ERROR(buffer->MakeView(b * byte_size, byte_size, ref));
           ^~~~~~~~~~~~~~~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member ok in status2, which is of non-class type const int
     if (!status2.ok()) return status2; \
                  ^
gl/api.cc:142:7: note: in expansion of macro RETURN_IF_ERROR
       RETURN_IF_ERROR(runtime_->Execute());
       ^~~~~~~~~~~~~~~
gl/api.cc:144:21: error: cannot convert tflite::gpu::Status to int in return
     return OkStatus();
                     ^
gl/api.cc: In member function virtual int tflite::gpu::gl::{anonymous}::InferenceContextWithBatchImpl::Reset():
gl/api.cc:151:21: error: cannot convert tflite::gpu::Status to int in return
     return OkStatus();
                     ^
In file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/model.h:29:0,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:24,
                 from gl/api.cc:16:
gl/api.cc: In member function int tflite::gpu::gl::{anonymous}::CompiledModelImpl::Add(const tflite::gpu::gl::WorkgroupsCalculator&, tflite::gpu::gl::ShaderCode):
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member ok in status2, which is of non-class type const int
     if (!status2.ok()) return status2; \
                  ^
gl/api.cc:212:5: note: in expansion of macro RETURN_IF_ERROR
     RETURN_IF_ERROR(
     ^~~~~~~~~~~~~~~
gl/api.cc:221:21: error: cannot convert tflite::gpu::Status to int in return
     return OkStatus();
                     ^
In file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/model.h:29:0,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:24,
                 from gl/api.cc:16:
gl/api.cc: In member function int tflite::gpu::gl::{anonymous}::CompiledModelImpl::AddFullShader(const string&, const uint3&, size_t*):
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member ok in status2, which is of non-class type const int
     if (!status2.ok()) return status2; \
                  ^
gl/api.cc:232:7: note: in expansion of macro RETURN_IF_ERROR
       RETURN_IF_ERROR(
       ^~~~~~~~~~~~~~~
gl/api.cc:240:21: error: cannot convert tflite::gpu::Status to int in return
     return OkStatus();
                     ^
In file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/model.h:29:0,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:24,
                 from gl/api.cc:16:
gl/api.cc: In member function virtual int tflite::gpu::gl::{anonymous}::CompiledModelImpl::NewRun(const tflite::gpu::gl::RuntimeOptions&, const tflite::gpu::gl::ObjectManager*, tflite::gpu::gl::CommandQueue*, std::unique_ptr<tflite::gpu::gl::InferenceContext>*) const:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member ok in status2, which is of non-class type const int
     if (!status2.ok()) return status2; \
                  ^
gl/api.cc:256:9: note: in expansion of macro RETURN_IF_ERROR
         RETURN_IF_ERROR(buffer->MakeView(0, s.second, &ref));
         ^~~~~~~~~~~~~~~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member ok in status2, which is of non-class type const int
     if (!status2.ok()) return status2; \
                  ^
gl/api.cc:257:9: note: in expansion of macro RETURN_IF_ERROR
         RETURN_IF_ERROR(refs->RegisterBuffer(s.first, std::move(ref)));
         ^~~~~~~~~~~~~~~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member ok in status2, which is of non-class type const int
     if (!status2.ok()) return status2; \
                  ^
gl/api.cc:263:7: note: in expansion of macro RETURN_IF_ERROR
       RETURN_IF_ERROR(runtime->AddProgram(shaders_[c.shader_idx], c.parameters,
       ^~~~~~~~~~~~~~~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member ok in status2, which is of non-class type const int
     if (!status2.ok()) return status2; \
                  ^
gl/api.cc:266:5: note: in expansion of macro RETURN_IF_ERROR
     RETURN_IF_ERROR(runtime->PrepareForExecution());
     ^~~~~~~~~~~~~~~
gl/api.cc:274:21: error: cannot convert tflite::gpu::Status to int in return
     return OkStatus();
                     ^
In file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/model.h:29:0,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:24,
                 from gl/api.cc:16:
gl/api.cc: In member function virtual int tflite::gpu::gl::{anonymous}::CompiledModelImpl::OnProgram(const std::vector<tflite::gpu::gl::UniformParameter>&, const std::vector<tflite::gpu::gl::Object>&, const uint3&, const uint3&, size_t):
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member ok in status2, which is of non-class type const int
     if (!status2.ok()) return status2; \
                  ^
gl/api.cc:290:5: note: in expansion of macro RETURN_IF_ERROR
     RETURN_IF_ERROR(AddFullShader(partial_shaders_[partial_shader_index],
     ^~~~~~~~~~~~~~~
gl/api.cc:299:21: error: cannot convert tflite::gpu::Status to int in return
     return OkStatus();
                     ^
gl/api.cc: In member function virtual int tflite::gpu::gl::{anonymous}::CompiledModelImpl::Serialize(std::vector<unsigned char>*) const:
gl/api.cc:339:21: error: cannot convert tflite::gpu::Status to int in return
     return OkStatus();
                     ^
gl/api.cc: In member function virtual int tflite::gpu::gl::{anonymous}::CompiledModelImpl::OnShader(absl::Span<const char>):
gl/api.cc:345:21: error: cannot convert tflite::gpu::Status to int in return
     return OkStatus();
                     ^
gl/api.cc: In function int tflite::gpu::gl::Compile(const tflite::gpu::gl::CompilationOptions&, const GraphFloat32&, const tflite::gpu::gl::NodeShader&, const tflite::gpu::gl::WorkgroupsCalculator&, std::unique_ptr<tflite::gpu::gl::CompiledModel>*):
gl/api.cc:389:78: error: cannot convert tflite::gpu::Status to int in return
     return InvalidArgumentError(""Only identical batch dimension is supported"");
                                                                              ^
In file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/model.h:29:0,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:24,
                 from gl/api.cc:16:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member ok in status2, which is of non-class type const int
     if (!status2.ok()) return status2; \
                  ^
gl/api.cc:392:3: note: in expansion of macro RETURN_IF_ERROR
   RETURN_IF_ERROR(RequestGpuInfo(&gpu_info));
   ^~~~~~~~~~~~~~~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member ok in status2, which is of non-class type const int
     if (!status2.ok()) return status2; \
                  ^
gl/api.cc:396:3: note: in expansion of macro RETURN_IF_ERROR
   RETURN_IF_ERROR(compiler->Compile(model, [&](ShaderCode code) -> Status {
   ^~~~~~~~~~~~~~~
gl/api.cc:400:19: error: cannot convert tflite::gpu::Status to int in return
   return OkStatus();
                   ^
In file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/model.h:29:0,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:24,
                 from gl/api.cc:16:
gl/api.cc: In function int tflite::gpu::gl::ReadSerializedModel(const std::vector<unsigned char>&, std::unique_ptr<tflite::gpu::gl::CompiledModel>*):
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member ok in status2, which is of non-class type const int
     if (!status2.ok()) return status2; \
                  ^
gl/api.cc:407:3: note: in expansion of macro RETURN_IF_ERROR
   RETURN_IF_ERROR(RequestGpuInfo(&gpu_info));
   ^~~~~~~~~~~~~~~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/common/status.h:68:18: error: request for member ok in status2, which is of non-class type const int
     if (!status2.ok()) return status2; \
                  ^
gl/api.cc:409:3: note: in expansion of macro RETURN_IF_ERROR
   RETURN_IF_ERROR(DeserializeCompiledModel(
   ^~~~~~~~~~~~~~~
gl/api.cc:412:19: error: cannot convert tflite::gpu::Status to int in return
   return OkStatus();
                   ^
In file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:25:0,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/object_manager.h:27,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:29,
                 from gl/api.cc:16:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h: In instantiation of int tflite::gpu::gl::gl_call_internal::Caller<void>::operator()(const string&, F, ErrorF, Params&& ...) [with F = void (*)(int, unsigned int*); ErrorF = int (*)(); Params = {int, unsigned int*}; std::__cxx11::string = std::__cxx11::basic_string<char>]:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:88:24:   required from int tflite::gpu::gl::gl_call_internal::CallAndCheckError(const string&, F, ErrorF, Params&& ...) [with F = void (*)(int, unsigned int*); ErrorF = int (*)(); Params = {int, unsigned int*}; std::__cxx11::string = std::__cxx11::basic_string<char>]
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:153:5:   required from here
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:16: error: request for member ok in status, which is of non-class type const int
     if (status.ok()) return OkStatus();
         ~~~~~~~^~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:38: error: cannot convert tflite::gpu::Status to int in return
     if (status.ok()) return OkStatus();
                                      ^
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:26: error: request for member code in status, which is of non-class type const int
     return Status(status.code(), status.error_message() + "": "" + context);
                   ~~~~~~~^~~~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:41: error: request for member error_message in status, which is of non-class type const int
     return Status(status.code(), status.error_message() + "": "" + context);
                                  ~~~~~~~^~~~~~~~~~~~~
In file included from /usr/include/EGL/eglplatform.h:119:0,
                 from /usr/include/EGL/egl.h:39,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/portable_gl31.h:21,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_shader.h:23,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_program.h:24,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/command_queue.h:23,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:26,
                 from gl/api.cc:16:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:12: error: expression list treated as compound expression in functional cast [-fpermissive]
     return Status(status.code(), status.error_message() + "": "" + context);
            ^
In file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:25:0,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/object_manager.h:27,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:29,
                 from gl/api.cc:16:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h: In instantiation of int tflite::gpu::gl::gl_call_internal::Caller<void>::operator()(const string&, F, ErrorF, Params&& ...) [with F = void (*)(int, const unsigned int*); ErrorF = int (*)(); Params = {int, unsigned int*}; std::__cxx11::string = std::__cxx11::basic_string<char>]:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:88:24:   required from int tflite::gpu::gl::gl_call_internal::CallAndCheckError(const string&, F, ErrorF, Params&& ...) [with F = void (*)(int, const unsigned int*); ErrorF = int (*)(); Params = {int, unsigned int*}; std::__cxx11::string = std::__cxx11::basic_string<char>]
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:162:7:   required from here
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:16: error: request for member ok in status, which is of non-class type const int
     if (status.ok()) return OkStatus();
         ~~~~~~~^~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:38: error: cannot convert tflite::gpu::Status to int in return
     if (status.ok()) return OkStatus();
                                      ^
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:26: error: request for member code in status, which is of non-class type const int
     return Status(status.code(), status.error_message() + "": "" + context);
                   ~~~~~~~^~~~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:41: error: request for member error_message in status, which is of non-class type const int
     return Status(status.code(), status.error_message() + "": "" + context);
                                  ~~~~~~~^~~~~~~~~~~~~
In file included from /usr/include/EGL/eglplatform.h:119:0,
                 from /usr/include/EGL/egl.h:39,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/portable_gl31.h:21,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_shader.h:23,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_program.h:24,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/command_queue.h:23,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:26,
                 from gl/api.cc:16:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:12: error: expression list treated as compound expression in functional cast [-fpermissive]
     return Status(status.code(), status.error_message() + "": "" + context);
            ^
In file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:25:0,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/object_manager.h:27,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:29,
                 from gl/api.cc:16:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h: In instantiation of int tflite::gpu::gl::gl_call_internal::Caller<void>::operator()(const string&, F, ErrorF, Params&& ...) [with F = void (*)(unsigned int, unsigned int); ErrorF = int (*)(); Params = {const unsigned int&, unsigned int&}; std::__cxx11::string = std::__cxx11::basic_string<char>]:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:88:24:   required from int tflite::gpu::gl::gl_call_internal::CallAndCheckError(const string&, F, ErrorF, Params&& ...) [with F = void (*)(unsigned int, unsigned int); ErrorF = int (*)(); Params = {const unsigned int&, unsigned int&}; std::__cxx11::string = std::__cxx11::basic_string<char>]
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:182:5:   required from here
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:16: error: request for member ok in status, which is of non-class type const int
     if (status.ok()) return OkStatus();
         ~~~~~~~^~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:38: error: cannot convert tflite::gpu::Status to int in return
     if (status.ok()) return OkStatus();
                                      ^
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:26: error: request for member code in status, which is of non-class type const int
     return Status(status.code(), status.error_message() + "": "" + context);
                   ~~~~~~~^~~~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:41: error: request for member error_message in status, which is of non-class type const int
     return Status(status.code(), status.error_message() + "": "" + context);
                                  ~~~~~~~^~~~~~~~~~~~~
In file included from /usr/include/EGL/eglplatform.h:119:0,
                 from /usr/include/EGL/egl.h:39,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/portable_gl31.h:21,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_shader.h:23,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_program.h:24,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/command_queue.h:23,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:26,
                 from gl/api.cc:16:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:12: error: expression list treated as compound expression in functional cast [-fpermissive]
     return Status(status.code(), status.error_message() + "": "" + context);
            ^
In file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:25:0,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/object_manager.h:27,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:29,
                 from gl/api.cc:16:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h: In instantiation of int tflite::gpu::gl::gl_call_internal::Caller<void>::operator()(const string&, F, ErrorF, Params&& ...) [with F = void (*)(unsigned int, unsigned int); ErrorF = int (*)(); Params = {const unsigned int&, int}; std::__cxx11::string = std::__cxx11::basic_string<char>]:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:88:24:   required from int tflite::gpu::gl::gl_call_internal::CallAndCheckError(const string&, F, ErrorF, Params&& ...) [with F = void (*)(unsigned int, unsigned int); ErrorF = int (*)(); Params = {const unsigned int&, int}; std::__cxx11::string = std::__cxx11::basic_string<char>]
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:186:5:   required from here
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:16: error: request for member ok in status, which is of non-class type const int
     if (status.ok()) return OkStatus();
         ~~~~~~~^~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:38: error: cannot convert tflite::gpu::Status to int in return
     if (status.ok()) return OkStatus();
                                      ^
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:26: error: request for member code in status, which is of non-class type const int
     return Status(status.code(), status.error_message() + "": "" + context);
                   ~~~~~~~^~~~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:41: error: request for member error_message in status, which is of non-class type const int
     return Status(status.code(), status.error_message() + "": "" + context);
                                  ~~~~~~~^~~~~~~~~~~~~
In file included from /usr/include/EGL/eglplatform.h:119:0,
                 from /usr/include/EGL/egl.h:39,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/portable_gl31.h:21,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_shader.h:23,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_program.h:24,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/command_queue.h:23,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:26,
                 from gl/api.cc:16:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:12: error: expression list treated as compound expression in functional cast [-fpermissive]
     return Status(status.code(), status.error_message() + "": "" + context);
            ^
In file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:25:0,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/object_manager.h:27,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:29,
                 from gl/api.cc:16:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h: In instantiation of int tflite::gpu::gl::gl_call_internal::Caller<void>::operator()(const string&, F, ErrorF, Params&& ...) [with F = unsigned char (*)(unsigned int); ErrorF = int (*)(); Params = {const unsigned int&}; std::__cxx11::string = std::__cxx11::basic_string<char>]:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:88:24:   required from int tflite::gpu::gl::gl_call_internal::CallAndCheckError(const string&, F, ErrorF, Params&& ...) [with F = unsigned char (*)(unsigned int); ErrorF = int (*)(); Params = {const unsigned int&}; std::__cxx11::string = std::__cxx11::basic_string<char>]
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:200:21:   required from here
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:16: error: request for member ok in status, which is of non-class type const int
     if (status.ok()) return OkStatus();
         ~~~~~~~^~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:38: error: cannot convert tflite::gpu::Status to int in return
     if (status.ok()) return OkStatus();
                                      ^
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:26: error: request for member code in status, which is of non-class type const int
     return Status(status.code(), status.error_message() + "": "" + context);
                   ~~~~~~~^~~~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:41: error: request for member error_message in status, which is of non-class type const int
     return Status(status.code(), status.error_message() + "": "" + context);
                                  ~~~~~~~^~~~~~~~~~~~~
In file included from /usr/include/EGL/eglplatform.h:119:0,
                 from /usr/include/EGL/egl.h:39,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/portable_gl31.h:21,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_shader.h:23,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_program.h:24,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/command_queue.h:23,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:26,
                 from gl/api.cc:16:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:12: error: expression list treated as compound expression in functional cast [-fpermissive]
     return Status(status.code(), status.error_message() + "": "" + context);
            ^
In file included from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_buffer.h:25:0,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/object_manager.h:27,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:29,
                 from gl/api.cc:16:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h: In instantiation of int tflite::gpu::gl::gl_call_internal::Caller<void>::operator()(const string&, F, ErrorF, Params&& ...) [with F = void (*)(unsigned int, long int, const void*, unsigned int); ErrorF = int (*)(); Params = {int, unsigned int, unsigned char*, int}; std::__cxx11::string = std::__cxx11::basic_string<char>]:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:88:24:   required from int tflite::gpu::gl::gl_call_internal::CallAndCheckError(const string&, F, ErrorF, Params&& ...) [with F = void (*)(unsigned int, long int, const void*, unsigned int); ErrorF = int (*)(); Params = {int, unsigned int, unsigned char*, int}; std::__cxx11::string = std::__cxx11::basic_string<char>]
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/runtime/shared_buffer.h:62:5:   required from here
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:16: error: request for member ok in status, which is of non-class type const int
     if (status.ok()) return OkStatus();
         ~~~~~~~^~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:73:38: error: cannot convert tflite::gpu::Status to int in return
     if (status.ok()) return OkStatus();
                                      ^
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:26: error: request for member code in status, which is of non-class type const int
     return Status(status.code(), status.error_message() + "": "" + context);
                   ~~~~~~~^~~~
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:41: error: request for member error_message in status, which is of non-class type const int
     return Status(status.code(), status.error_message() + "": "" + context);
                                  ~~~~~~~^~~~~~~~~~~~~
In file included from /usr/include/EGL/eglplatform.h:119:0,
                 from /usr/include/EGL/egl.h:39,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/portable_gl31.h:21,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_shader.h:23,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_program.h:24,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/command_queue.h:23,
                 from /mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/api.h:26,
                 from gl/api.cc:16:
/mnt/usbdisk2/tensorflow/tensorflow/lite/delegates/gpu/gl/gl_call.h:74:12: error: expression list treated as compound expression in functional cast [-fpermissive]
     return Status(status.code(), status.error_message() + "": "" + context);
            ^
Makefile:120:  'gl/api.o' 
make: *** [gl/api.o]  1
</pre>
</details>"
27319,[MMAP allocation] Error compilation for Window 10 using MVSC 2017,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Windows 10 Pro
- TensorFlow installed from: source
- TensorFlow version: 1.12.0
- Python version: No
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): MVSC 2017
- CUDA/cuDNN version: No
- GPU model and memory: No
- IDE: Qt 4.8,2
- Build system: Cmake 3.14



**Describe the problem**
Trying to compile Tensorflow Lite for Windows I am having the next error in the lines 17 and 20 of the file <em>mmap_allocation.cc</em>: 
- `sys/mman.h no such file or directory`
-` unistd.h file not found `

<img width=""1920"" alt=""error_tflite"" src=""https://user-images.githubusercontent.com/9532966/55277989-1301a300-530f-11e9-9621-86144bd2e34e.png"">
"
27318,Callback object has no attribute 'validation_data' error,"Since version 1.11.0 in tensorflow keras ""training_arrays.py"" module was removed this part of code:

```
for cbk in callbacks:
    cbk.validation_data = val_ins
```
In original keras library this code still presented.

And now we can't access from callback to validation data. In Callback class self.validation_data property exist, but is always None.  This is bug or planned behavior?"
27317,"Please add ""Abandon 996"" badge in this project to support us.","---------------------------
- Add this [badge](https://github.com/996icu/996.ICU/blob/master/externals/instruction.md) - to your project to support 996.ICU.  
- [](https://github.com/996icu/996.ICU/blob/master/externals/instruction.md)996.ICU
![https://996.icu](https://img.shields.io/badge/link-996.icu-red.svg)
--------------------------------------------
Chinese software engineers are protesting against the country  s 996 work schedule.
An unleagal work schedule (9a.m. ~ 9p.m., 6 days a week) .
Search 996.icu to support us !!

 996 
 996  9  9  6 
 996.icu  
---------------------------
# Working Men of All developers, Unite!"
27316,Weights returned from `keras.model.Model.get_weights()` are different from the ones fetched from the graph directly.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v1.13.1-0-g6612da8951  1.13.1
- Python version:  3.6.5 [Conda virtual env]
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a


**Current behavior**
```python
import tf.keras.layers as L
def feed_forward_nn():
    with tf.variable_scope(""keras_""):
        _input_ = L.Input(shape=(3,))
        layer_1 = L.Dense(5, Z.relu)(_input_)
        layer_2 = L.Dense(5, Z.relu)(layer_1)
        output = L.Dense(1, Z.tanh)(layer_2)
    return Model(inputs=_input_, outputs=output)

model = feed_forward_nn()
sess.run(tf.global_variables_initializer())
```
**Getting weights from the graph itself vs using model.get_weights(): **
```bash
In [5]: sess.run(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))                                                                                                       
Out[5]: 
[array([[ 0.06312895,  0.41315466, -0.7228936 ,  0.66872925, -0.29116392],
        [ 0.4684649 ,  0.07688528, -0.6403172 ,  0.22407418,  0.2667293 ],
        [ 0.6107295 , -0.12038571,  0.6050673 , -0.423037  ,  0.13438004]],
       dtype=float32),
 array([0., 0., 0., 0., 0.], dtype=float32),
 array([[ 0.02425474, -0.45923662,  0.57829964, -0.346116  ,  0.1658817 ],
        [ 0.45619428, -0.7142852 ,  0.09248799,  0.10972124, -0.32121253],
        [ 0.4513328 ,  0.7124406 ,  0.620286  ,  0.41065276, -0.26047498],
        [ 0.31083477, -0.46835992, -0.20313907,  0.48168743,  0.4530183 ],
        [-0.4258671 , -0.18113756,  0.18505335, -0.49022397,  0.70809317]],
       dtype=float32),
 array([0., 0., 0., 0., 0.], dtype=float32),
 array([[ 0.14051151],
        [ 0.326164  ],
        [ 0.7357073 ],
        [ 0.82876945],
        [-0.52645826]], dtype=float32),
 array([0.], dtype=float32)]
In [6]: model.get_weights()                                                                                                                                              
Out[6]: 
[array([[-0.7260302 , -0.8412575 ,  0.461235  , -0.18713814,  0.04823786],
        [-0.7028123 , -0.7654265 , -0.4894964 ,  0.14831334,  0.49071664],
        [-0.3141371 ,  0.23999351,  0.00772947,  0.49302799,  0.20739347]],
       dtype=float32),
 array([0., 0., 0., 0., 0.], dtype=float32),
 array([[-0.6223821 , -0.5421302 ,  0.25800073, -0.56548995, -0.31957147],
        [ 0.12645936,  0.16026676, -0.5968463 , -0.4449416 , -0.5069353 ],
        [ 0.7126628 ,  0.36706662,  0.16168451,  0.6257292 ,  0.6289371 ],
        [-0.41813722, -0.46337304,  0.62247336, -0.7190004 , -0.5288886 ],
        [ 0.35295987, -0.68682575,  0.7552402 , -0.728891  , -0.285271  ]],
       dtype=float32),
 array([0., 0., 0., 0., 0.], dtype=float32),
 array([[0.18540406],
        [0.84715295],
        [0.11520958],
        [0.47841287],
        [0.46228695]], dtype=float32),
 array([0.], dtype=float32)]
```
**Using model.trainable_weights property:**
```bash
In [14]: sess.run(model.trainable_weights)                                                                                                                               
Out[14]: 
[array([[ 0.06312895,  0.41315466, -0.7228936 ,  0.66872925, -0.29116392],
        [ 0.4684649 ,  0.07688528, -0.6403172 ,  0.22407418,  0.2667293 ],
        [ 0.6107295 , -0.12038571,  0.6050673 , -0.423037  ,  0.13438004]],
       dtype=float32),
 array([0., 0., 0., 0., 0.], dtype=float32),
 array([[ 0.02425474, -0.45923662,  0.57829964, -0.346116  ,  0.1658817 ],
        [ 0.45619428, -0.7142852 ,  0.09248799,  0.10972124, -0.32121253],
        [ 0.4513328 ,  0.7124406 ,  0.620286  ,  0.41065276, -0.26047498],
        [ 0.31083477, -0.46835992, -0.20313907,  0.48168743,  0.4530183 ],
        [-0.4258671 , -0.18113756,  0.18505335, -0.49022397,  0.70809317]],
       dtype=float32),
 array([0., 0., 0., 0., 0.], dtype=float32),
 array([[ 0.14051151],
        [ 0.326164  ],
        [ 0.7357073 ],
        [ 0.82876945],
        [-0.52645826]], dtype=float32),
 array([0.], dtype=float32)]
```"
27315,this two message one after another ,"installing tensorflow-gpu on python 3.6.5 conda 

> when try install 
> tensorflow 0.9.0 has requirement protobuf==3.0.0b2, but you'll have protobuf 3.7.1 which is incompatible.

after I install protobuf 3.0.0b2

> tensorflow-gpu 1.12.0 has requirement protobuf>=3.6.1, but you'll have protobuf 3.0.0b2 which is incompatible.
> tensorboard 1.12.2 has requirement protobuf>=3.4.0, but you'll have protobuf 3.0.0b2 which is incompatible.
> tb-nightly 1.14.0a20190319 has requirement protobuf>=3.6.0, but you'll have protobuf 3.0.0b2 which is incompatible.

what should i do ?
"
27314,[tflite]:operator of type Floor for which the quantized form is not yet implemented,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):source
- TensorFlow version (or github SHA if from source):1.13
- code on jupyter: 
```
import tensorflow as tf
tf.enable_eager_execution()
model_path='./VESPCN.pb'
input_arrays=['images_next_curr',""images_prev_curr""]
output_arrays=['Tanh_2']
converter = tf.lite.TFLiteConverter.from_frozen_graph(model_path, input_arrays, output_arrays,
                                                      input_shapes={'images_next_curr':[1, 360, 640, 6],'images_prev_curr':[1, 360, 640, 6]})
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
converter.allow_custom_ops=True
converter.inference_type=tf.lite.constants.QUANTIZED_UINT8
converter.default_ranges_stats=(0,6)
converter.quantized_input_stats={'images_next_curr':(0,1),'images_prev_curr':(0,1)}
tflite_model = converter.convert()
```

**Provide the text output from tflite_convert**

```
ConverterError: TOCO failed. See console for info.
2019-03-30 17:14:23.618847: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 739 operators, 1251 arrays (0 quantized)
2019-03-30 17:14:23.638961: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 739 operators, 1251 arrays (0 quantized)
2019-03-30 17:14:23.683782: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 376 operators, 643 arrays (2 quantized)
2019-03-30 17:14:23.718247: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 376 operators, 643 arrays (2 quantized)
2019-03-30 17:14:23.762560: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 300 operators, 459 arrays (2 quantized)
2019-03-30 17:14:23.774999: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 300 operators, 459 arrays (2 quantized)
2019-03-30 17:14:23.789415: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 271 operators, 430 arrays (2 quantized)
2019-03-30 17:14:23.806875: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 271 operators, 430 arrays (2 quantized)
2019-03-30 17:14:23.821737: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before default min-max range propagation graph transformations: 271 operators, 430 arrays (2 quantized)
2019-03-30 17:14:23.836560: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After default min-max range propagation graph transformations pass 1: 271 operators, 430 arrays (2 quantized)
2019-03-30 17:14:23.841059: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 271 operators, 430 arrays (2 quantized)
2019-03-30 17:14:23.842802: F ./tensorflow/lite/toco/toco_tooling.h:38] Check failed: s.ok() Unimplemented: this graph contains an operator of type Floor for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).
Fatal Python error: Aborted

Current thread 0x00007fb07e2fd700 (most recent call first):
  File ""/home/disk1/zww/anaconda2/envs/test/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33 in execute
  File ""/home/disk1/zww/anaconda2/envs/test/lib/python3.6/site-packages/absl/app.py"", line 251 in _run_main
  File ""/home/disk1/zww/anaconda2/envs/test/lib/python3.6/site-packages/absl/app.py"", line 300 in run
  File ""/home/disk1/zww/anaconda2/envs/test/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40 in run
  File ""/home/disk1/zww/anaconda2/envs/test/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59 in main
  File ""/home/disk1/zww/anaconda2/envs/test/bin/toco_from_protos"", line 10 in <module>
Aborted (core dumped)
```

**Describe the problem**
I try to deploy a VESPCN(video SR) model on android, but after I have taken some time to do custom ops , I failed to quantize the model with this problem, as far as I'm concerned, since the
data is uint8 type, there is no need of quantization for floor op. can you tell me how to solve this problem,  thanks

"
27313,"Error if input size (of test set) not divisible by TPU core count. ""InvalidArgumentError: slice index 0 of dimension 0 out of bounds."" or ""AssertionError: batch_size must be divisible by the number of TPU cores in use (7 vs 8)""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- TensorFlow installed from (source or binary):  Google Colab
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- CUDA/cuDNN version: release 10.0, V10.0.130
- GPU model and memory: Google Colab TPU

**Describe the current behavior**

The following error occurs:

```

Train on 10000 samples, validate on 1153031 samples
Epoch 1/25
INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 650), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='activation_7_target_10')]
INFO:tensorflow:Overriding default placeholder.
INFO:tensorflow:Remapping placeholder for input_1
INFO:tensorflow:Started compiling
INFO:tensorflow:Finished compiling. Time elapsed: 7.617780447006226 secs
INFO:tensorflow:Setting weights on TPU model.
 8192/10000 [=======================>......] - ETA: 4s - loss: 1.5425 - mean_squared_error: 1.5425INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(98,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(98, 650), dtype=tf.float32, name='input_10'), TensorSpec(shape=(98, 1), dtype=tf.float32, name='activation_7_target_10')]
INFO:tensorflow:Overriding default placeholder.
INFO:tensorflow:Remapping placeholder for input_1
INFO:tensorflow:Started compiling
INFO:tensorflow:Finished compiling. Time elapsed: 6.491861820220947 secs
 9216/10000 [==========================>...] - ETA: 2s - loss: 1.4734 - mean_squared_error: 1.4734INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 650), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='activation_7_target_10')]
INFO:tensorflow:Overriding default placeholder.
INFO:tensorflow:Remapping placeholder for input_1
INFO:tensorflow:Started compiling
INFO:tensorflow:Finished compiling. Time elapsed: 5.56870174407959 secs
INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(0,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(0, 650), dtype=tf.float32, name='input_10'), TensorSpec(shape=(0, 1), dtype=tf.float32, name='activation_7_target_10')]

---------------------------------------------------------------------------

InvalidArgumentError                      Traceback (most recent call last)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)
   1658   try:
-> 1659     c_op = c_api.TF_FinishOperation(op_desc)
   1660   except errors.InvalidArgumentError as e:

InvalidArgumentError: slice index 0 of dimension 0 out of bounds. for 'strided_slice_3' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.

During handling of the above exception, another exception occurred:
```
So basically the input shapes gets remapped to something nonesensical:  (0,)

**Describe the expected behavior**

Input shapes get remapped to something that makes sense.

**Code to reproduce the issue**

```
def train_on_TPU_regression(model,model_save_loc, X_train, y_train, X_test, y_test, batch_size=1024, epochs=25, save_best_only=True, period=1, train_patience=5, mse=True):
 
  #Identify TPU worker
  TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']
  tf.logging.set_verbosity(tf.logging.INFO)
  
  #Useful to avoid clutter from old models / layers.
  tf.keras.backend.clear_session()

  #Convert model to TPU model
  tpu_model = tf.contrib.tpu.keras_to_tpu_model(model,strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))
  
  print(""\n"")

  if mse:
    #Compile the model
    tpu_model.compile(
      optimizer=tf.train.AdamOptimizer(), 
      loss=tf.keras.losses.mean_squared_error,
      metrics=['mse']
    )
  else:
    #Compile the model
    tpu_model.compile(
      optimizer=tf.train.AdamOptimizer(), 
      loss=tf.keras.losses.mean_absolute_error,
      metrics=['mae']
    )
  
  #Configure how to save model and early stopping
  callbacks_list = [
      tf.keras.callbacks.ModelCheckpoint(
          filepath=model_save_loc,
          save_weights_only=True,
          monitor='val_loss', 
          save_best_only=save_best_only,
          mode='auto',
          period=period),
      tf.keras.callbacks.EarlyStopping(monitor='val_loss', 
                                       patience=train_patience,
                                       mode='auto')
  ]
  
  history = tpu_model.fit(X_train,
                          y_train,
                          validation_data=(X_test,y_test),
                          epochs=epochs,
                          batch_size=batch_size,
                          callbacks=callbacks_list,
                          verbose=1)

  return tpu_model, history
```
```
print(X_train.shape)
print(y_train_scaled.shape)
print(X_test.shape)
print(y_test_scaled.shape)

(6533755, 650)
(6533755,)
(1153031, 650)
(1153031,)
```

```
inputs = tf.keras.layers.Input(shape=(SEQUENCE_LEN,))

x = tf.keras.layers.Embedding(CLASSES, 8, input_length=SEQUENCE_LEN)(inputs) 
x = tf.keras.layers.Conv1D(128, 7)(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Activation(""relu"")(x)
x = tf.keras.layers.Conv1D(128, 3)(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Activation(""relu"")(x)
x = tf.keras.layers.Conv1D(128, 3)(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Activation(""relu"")(x)

x = tf.keras.layers.MaxPooling1D(3)(x)
x = tf.keras.layers.Conv1D(256, 3)(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Activation(""relu"")(x)
x = tf.keras.layers.Conv1D(256, 3)(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Activation(""relu"")(x)
x = tf.keras.layers.Conv1D(256, 3)(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Activation(""relu"")(x)

x = tf.keras.layers.GlobalAveragePooling1D()(x)
x = tf.keras.layers.Dense(256)(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Activation(""relu"")(x)
x = tf.keras.layers.Dense(1)(x)
x = tf.keras.layers.Activation(""linear"")(x)

model = tf.keras.Model(inputs=inputs, outputs=x)
model.summary()
```

```
tpu_model, history = train_on_TPU_regression(model,model_saves_folder_location+""model_#01_08.hdf5"", X_train[:10000], y_train_scaled[:10000], X_test, y_test_scaled, train_patience=10, batch_size=1024)
```

**How it can be prevented apparently**

```
tpu_model, history = train_on_TPU_regression(model,model_saves_folder_location+""model_#01_08.hdf5"", X_train[:10000], y_train_scaled[:10000], X_test[:-7], y_test_scaled[:-7], train_patience=10, batch_size=1024)
```
By changing the size of the training data this error can be prevented? In this case I made the number of training examples divisible by 1024, however, it worked too by just using 10000: `X_test[:10000], y_test_scaled[:10000]`, which is how i noticed this in the first place.

**Question:**
It this really a bug or am I missing something?

**UPDATE**

When trying to predict, I get the following error:

```
TPU worker setup:
INFO:tensorflow:Querying Tensorflow master (grpc://10.14.138.218:8470) for TPU system metadata.
INFO:tensorflow:Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 13438656196918098020)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 12429978056820721148)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 3813600547282296097)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 18120823110831817385)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 429246965946293269)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5942823189813875074)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 1996329778228354684)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 15775279347369926923)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6250325552725876457)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 17570123092856457982)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 3640626136020975642)
WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.


Calculating predictions:
INFO:tensorflow:New input shapes; (re-)compiling: mode=infer (# of cores 8), [TensorSpec(shape=(4, 650), dtype=tf.float32, name='input_10')]
INFO:tensorflow:Overriding default placeholder.
INFO:tensorflow:Remapping placeholder for input_1
INFO:tensorflow:Started compiling
INFO:tensorflow:Finished compiling. Time elapsed: 2.8085508346557617 secs
INFO:tensorflow:Setting weights on TPU model.
1152832/1153031 [============================>.] - ETA: 0s

---------------------------------------------------------------------------

AssertionError                            Traceback (most recent call last)

<ipython-input-36-aa8045e6a687> in <module>()
----> 1 y_preds, rmse_err, mae_err = calc_reg_pred(model, weights_location, X_test, y_test, mse=True, scaler=y_train_scaled)

<ipython-input-35-fc3b6ce0949b> in calc_reg_pred(model, weight_loc, X_test, y_test, batch_size, mse, scaler)
     32 
     33   print(""Calculating predictions:"")
---> 34   y_pred = cpu_model.predict(X_test, verbose=1)
     35 
     36   if not(scaler is None):

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in predict(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)
   1982           max_queue_size=max_queue_size,
   1983           workers=workers,
-> 1984           use_multiprocessing=use_multiprocessing)
   1985 
   1986   @property

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)
   1111     else:
   1112       return training_arrays.predict_loop(
-> 1113           self, x, batch_size=batch_size, verbose=verbose, steps=steps)
   1114 
   1115   def reset_metrics(self):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)
    327 
    328         # Get outputs.
--> 329         batch_outs = f(ins_batch)
    330         if not isinstance(batch_outs, list):
    331           batch_outs = [batch_outs]

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in __call__(self, inputs)
   1254     infeed_manager = self._lookup_infeed_manager(inputs)
   1255     input_tensors, inputs = self._construct_input_tensors_and_inputs(inputs)
-> 1256     infeed_instance = infeed_manager.make_infeed_instance(inputs)
   1257     del inputs  # To avoid accident usage.
   1258     input_specs = infeed_instance.make_input_specs(input_tensors)

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in make_infeed_instance(self, inputs)
    663 
    664   def make_infeed_instance(self, inputs):
--> 665     sharded_inputs = self._split_tensors(inputs)
    666     return self.NumpyInfeedInstance(sharded_inputs)
    667 

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in _split_tensors(self, inputs)
    652     assert batch_size % self._tpu_assignment.num_towers == 0, (
    653         'batch_size must be divisible by the number of TPU cores in use (%s '
--> 654         'vs %s)' % (batch_size, self._tpu_assignment.num_towers))
    655     shard_size = batch_size // self._tpu_assignment.num_towers
    656     input_list = []

AssertionError: batch_size must be divisible by the number of TPU cores in use (7 vs 8)
```

I get the error, even though my batch size was 128 and later 1024. Then I again applied `X_test[:-7], y_test[:-7]` and the error is gone. 
*So I would conclude that the TPU currently cant handle imput sizes not divisible by muber of TPUs for prediction.*"
27312,import results in Recursion-Error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
    `import tensorflow`
    `import kivy`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Windows 10
- TensorFlow installed from (source or binary): pip install (I guess it's binary)
- TensorFlow version (use command below): v1.12.0-9492-g2c319fb415, 2.0.0-alpha0
- Python version: 3.7 (Anaconda Win64)
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a


**Describe the current behavior**
Hundreds of messages ` WARNING: Logging before flag parsing goes to stderr.`, finally `RecursionError: maximum recursion depth exceeded while calling a Python object`. Output attached here: [tst.txt](https://github.com/tensorflow/tensorflow/files/3025303/tst.txt)


**Describe the expected behavior**
- no error by importing both packages
- independent behaviour from the order of imports

**Code to reproduce the issue**

    import tensorflow
    import kivy
order is important. kivy version is 1.10.1 - not sure if installed by pip or conda...

**Other info / logs**

Seems to be an issue with incompatible setup of `logging` - but as the error is raised in Tensorflow code, that's where it needs to be fixed.
"
27311,[TF 2.0] tf.summary.histogram error,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Testing
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): '2.0.0-dev20190327'
- Python version: 3.7


**Describe the current behavior**

I want to log gradients and weights of the model as it is training, using `tf.summary.histogram()`.
It seems that the function is not properly implemented, however, as using it results in an error:
```
  File ""/usr/local/lib/python3.7/dist-packages/tensorboard/plugins/histogram/summary_v2.py"", line 67, in histogram
    tensor = _buckets(data, bucket_count=buckets)
  File ""/usr/local/lib/python3.7/dist-packages/tensorboard/plugins/histogram/summary_v2.py"", line 85, in _buckets
    with tf.name_scope('buckets', values=[data, bucket_count]):
```

**Describe the expected behavior**

`tf.summary.histogram()` should properly log histogram data.

**Code to reproduce the issue**
``` python3
import tensorflow as tf

tf.summary.histogram(
    'test',
    [1.0, 2.0, 3.0],
    step=1,
)
```

**Other info / logs**

```
  File ""dat.py"", line 6, in <module>
    step=1,
  File ""/usr/local/lib/python3.7/dist-packages/tensorboard/plugins/histogram/summary_v2.py"", line 67, in histogram
    tensor = _buckets(data, bucket_count=buckets)
  File ""/usr/local/lib/python3.7/dist-packages/tensorboard/plugins/histogram/summary_v2.py"", line 85, in _buckets
    with tf.name_scope('buckets', values=[data, bucket_count]):
TypeError: __init__() got an unexpected keyword argument 'values'
```
"
27310,ModuleNotFoundError: No module named 'tensorflow.optimizers',"  my tensorflow version is tf-nightly-2.0-preview-2.0.0.dev20190329

but when i try 'import tensorflow.optimizers.Adam',it raise ModuleNotFoundError: No module named 'tensorflow.optimizers'. 

"
27309,BasicLSTMCell performances questions,"I was reading tf code now, but not understand following comments: why ""+"", ""*"" performances are bad? can anyone help to explain? Thanks!



**Note that using `add` and `multiply` instead of `+` and `*` gives a
performance improvement. So using those at the cost of readability.**

    add = math_ops.add
    multiply = math_ops.multiply
    new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))),
                multiply(sigmoid(i), self._activation(j)))
    new_h = multiply(self._activation(new_c), sigmoid(o))"
27307,Python module generated by `tf.load_op_library` contains no custom ops,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):1.13.1
- Python version:3.6
- Bazel version (if compiling from source):0.19.2-dist
- GCC/Compiler version (if compiling from source):5.4.0
- CUDA/cuDNN version:N/A
- GPU model and memory:N/A

**Problem**
I followed the instructions on https://tensorflow.google.cn/guide/extend/op to customize my own OPs step by step. For simple verification, I used the fact OP that comes with tensorflow (I also tried my own OP). I add BUILD file in the same directory with fact.cc 
```
load(""//tensorflow:tensorflow.bzl"", ""tf_custom_op_library"")

tf_custom_op_library(
    name = ""fact.so"",
    srcs = [""fact.cc""],
)
```
and then build 
```
bazel build --config opt //tensorflow/core/user_ops:fact.so
```
But when I use fact.so as the instructions describe:
```
# python
import tensorflow as tf
fact_module = tf.load_op_library('./fact.so')
fact_module.fact
```
I got following errors
```
Traceback (most recent call last):
  File ""/tmp/remote_python/main.py"", line 10, in <module>
    fact_module.fact
AttributeError: module '670cc8cfec5b6d3b8635f39bd583d769' has no attribute 'fact'
```
Is this normal?"
27306,ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory,"I am getting the error mentioned above when trying to use TF GPU.
My setup is as follows:
OS: Ubuntu 16.04
Python 3.5.2
Tensorflow: tensorflow-gpu 1.13.1
Cuda version: 10.0
libcublas.so.10.0 is present in /usr/local/cuda-10.0/lib64
/usr/local/cuda-10.0/lib64 is added to PATH and LD_LIBRARY_PATH.

Other Github issue threads related to similar issues didn't help.

I would be grateful for any help/suggestions, thanks!"
27305,Document stride parameter for XlaBuilder::Slice,"Doc Link: https://www.tensorflow.org/xla/operation_semantics
The documentation for [XlaBuilder::Slice](https://www.tensorflow.org/xla/operation_semantics#slice) does not mention the stride parameter. "
27304,FileNotFoundError while running %tensorboard,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
mac os 10.14
- TensorFlow installed from (source or binary):
`pip install tensorflow==2.0.0-alpha0`
- TensorFlow version:
2.0.0-alpha0
- Python version:
 3.7.2
- Installed using virtualenv? pip? conda?:
conda: tf was installed in a conda env, the jupyter notebook is launched in the base environment. There is a jupyter kernel for the tensorflow env.

**Describe the problem**
After loading the tensorboard extension with `%load_ext tensorboard.notebook`,  the `%tensorboard` magic fails with:
```
---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
<ipython-input-4-d46782f9619b> in <module>
----> 1 get_ipython().run_line_magic('tensorboard', '--logdir logs')

[...]

FileNotFoundError: [Errno 2] No such file or directory: 'tensorboard': 'tensorboard'
```



**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
import tensorflow
%load_ext tensorboard.notebook
%tensorboard --logdir logs
```

**Any other info / logs**
The issue is that the `tensorboard` binary is installed in the environment's bin dir which is added to the path when the env is activated. However, when selecting the kernel in the notebook, the path is not modified. So, while the tensorboard extension loads, the magic `%tensorboard` fails because it cannot find the `tensorboard` executable.

A workaround is to manually add the environment's bin dir to the path from inside the notebook:

```
import os
PATH = os.getenv('PATH')
%env PATH=/Users/anto/miniconda3/envs/tf2_env/bin:$PATH
````

With this the `%tensorboard` magic starts working."
27303,Need tf.signal.rfft op in TFLite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.4
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 1.13.1


**Provide the text output from tflite_convert**
If I pass the SELECT_TF_OPS option then I get:

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, DIV, EXPAND_DIMS, FLOOR_DIV, FULLY_CONNECTED, GATHER, LOG, MAXIMUM, MINIMUM, MUL, PACK, PAD, RANGE, RESHAPE, SHAPE, SPLIT, SPLIT_V, STRIDED_SLICE, SUB, TRANSPOSE. Here is a list of operators for which you will need custom implementations: RFFT.
```
If I don't pass the SELECT_TF_OPS option then I get:
```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, DIV, EXPAND_DIMS, FLOOR_DIV, FULLY_CONNECTED, GATHER, LOG, MAXIMUM, MINIMUM, MUL, PACK, PAD, RANGE, RESHAPE, SHAPE, SPLIT, SPLIT_V, STRIDED_SLICE, SUB, TRANSPOSE. Here is a list of operators for which you will need custom implementations: ComplexAbs, Cos, LinSpace, RFFT.
```

Also, please include a link to a GraphDef or the model if possible.
The code to create the model is pretty short since I hardcoded a bunch of parameters for now:

    with tf.Graph().as_default(), tf.Session() as sess:
        # input sound data as a waveform
        waveform = tf.placeholder(tf.float32, [None])
        # A Tensor of [batch_size, num_samples] mono PCM samples in the range [-1, 1].
        pcm = tf.math.scalar_mul(1/32768.0, waveform)

        # compute Short Time Fourier Transform
        stft = tf.signal.stft(pcm, frame_length=400, frame_step=160, fft_length=512)
        spectrogram = tf.abs(stft)

        # Warp the linear scale spectrograms into the mel-scale.
        num_spectrogram_bins = stfts.shape[-1].value
        lower_edge_hertz, upper_edge_hertz, num_mel_bins = 125.0, 7500.0, 64
        linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(
          num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,
          upper_edge_hertz)
        mel_spectrogram = tf.tensordot(
          spectrogram, linear_to_mel_weight_matrix, 1)
        mel_spectrogram.set_shape(spectrogram.shape[:-1].concatenate(
          linear_to_mel_weight_matrix.shape[-1:]))

        # Compute a stabilized log to get log-magnitude mel-scale spectrograms.
        log_mel_spectrogram = tf.log(mel_spectrogram + 1e-6)
        # with the model loaded and input/output tensors defined, convert to tf.lite
        converter = tf.lite.TFLiteConverter.from_session(sess, [waveform], [log_mel_spectrogram])
        converter.target_ops = [tf.lite.OpsSet.SELECT_TF_OPS, tf.lite.OpsSet.TFLITE_BUILTINS]
        tflite_model = converter.convert()

**Any other info / logs**
Assuming the SELECT_TF_OPS option produces a model that will work on TFLite on iOS, then I guess all I need is RFFT.  Thank you!

"
27298,Tensorflow 2.0 tf.name_scope has no effect on weights created by keras functional api,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.7.1


**Describe the current behavior**

tf.name_scope does not effect the name of weights created by keras.layers

output in tf 2.0.0-alpha0 
```
dense/kernel:0
dense/bias:0
```

**Describe the expected behavior**
tf.name_scope is applied to wights created by keras.layers
output in tf 1.13.1 is the expected behavior
```
block/dense/kernel:0
block/dense/bias:0
```
**Code to reproduce the issue**
```
import tensorflow as tf
print(tf.__version__)

inputs = tf.keras.Input(shape=[2])
with tf.name_scope('block'):
    outputs = tf.keras.layers.Dense(10)(inputs)
model = tf.keras.Model(inputs, outputs)
for w in model.weights:
    print(w.name)
```

**Other info / logs**

I was able to reproduce the output of tf 1.13 in tf 2.0.0 by using the following code
```
from tensorflow.python.keras.backend import get_graph
with get_graph().as_default(), tf.name_scope('block'):
    outputs = tf.keras.layers.Dense(10)(inputs)
```
keras.layers is using the `keras graph` which overrides the name_scope generated by   tf.name_scope in the `default graph`

I think the problem is actually caused by tf.name_scope incorrectly set `_has_symbolic_input_in_eager` to False when we are building graph using the keras functional api. since the input here is keras.Input, the name_scope should be applied to keras graph instead of default graph
"
27297,Training set+Validation set+Test set != data set,"Hello,
My training data set size is 82278 rows. But when I run a train on the csv file, I see below size for my train, validation and test sets. Can you help me run training on the full data set? 

Loading NLP pipeline
Writing dataset
Writing train set metadata with vocabulary
Training set: 21343
Validation set: 2988
Test set: 6041

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27295,tf.keras.layers.AveragePooling1D data_format init argument broken,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu GNU/Linux 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.12
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA 9.2, cuDNN 7.3.1
- GPU model and memory: Nvidia Titan V, P100, K80


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When instantiating a tf.keras.layers.AveragePooling1D, the data_format argument does not properly change which axis over which the pooling is performed - the pooling operation is conducted across the 2nd dimension of the input tensor regardless of the init argument supplied.

To illustrate this, consider 2 layers instantiated as
```
pool_1 = tf.keras.layers.AveragePooling1D(data_format='channels_first', pool_size=64)(prev_layer0) 
pool_2 = tf.keras.layers.AveragePooling1D(data_format='channels_last', pool_size=64)(prev_layer1)
```

- Assuming the output shape of prev_layer0 is (None,32,None), pool_1 outputs (None,0,None) when it should be outputting (None,32,None)
- Assuming the output shape of prev_layer1 is (None,None,32), pool_2 outputs (None,None,32) correctly

I have confirmed this is NOT merely the model summary misreporting the output shape - if you run training or prediction with a model containing an output shape of (None,0,None) generated in this way, it will lead to a 'Floating point exception (core dumped)' error.

**Describe the expected behavior**
When the `data_format='channels_first'` init argument is supplied, AveragePooling1D layers should pool along the last dimension of the 3D input tensors instead of the penultimate dimension (which is the expected behaviour when `data_format='channels_last'` is supplied)

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
#Channels first case
input_0 = Input(shape=(1,None))
conv1d_1 = tf.keras.layers.Conv1D(filters=16, kernel_size=5, padding='same', activation=tf.nn.leaky_relu, data_format = 'channels_first')(input_0)
conv1d_2 = tf.keras.layers.Conv1D(filters=32, kernel_size=5, padding='same', activation=tf.nn.leaky_relu, data_format = 'channels_first')(conv1d_1)
pool_2 = tf.keras.layers.AveragePooling1D(data_format='channels_first', pool_size=64)(conv1d_2)
conv1d_3 = tf.keras.layers.Conv1D(filters=256, kernel_size=1, padding='same', activation=tf.nn.leaky_relu, data_format = 'channels_first')(pool_2)
mod = Model(input_0, conv1d_3)
mod.summary() #Note the output shape of pool_2 - it will be (None,0,None)
#print(mod.predict(tf.random.uniform((1,1,128), dtype = tf.keras.backend.floatx())).shape) #If you run this it will cause a core dump

#Channels last case
input_0 = Input(shape=(None,1)) #implement arbitrary input shapes later
conv1d_1 = tf.keras.layers.Conv1D(filters=16, kernel_size=5, padding='same', activation=tf.nn.leaky_relu, data_format = 'channels_last')(input_0)
conv1d_2 = tf.keras.layers.Conv1D(filters=32, kernel_size=5, padding='same', activation=tf.nn.leaky_relu, data_format = 'channels_last')(conv1d_1)
pool_2 = tf.keras.layers.AveragePooling1D(data_format='channels_last', pool_size=64)(conv1d_2)
conv1d_3 = tf.keras.layers.Conv1D(filters=256, kernel_size=1, padding='same', activation=tf.nn.leaky_relu, data_format = 'channels_last')(pool_2)

mod2 = Model(input_0, conv1d_3)
mod2.summary()

print(mod2.predict(tf.random.uniform((1,128,1), dtype = tf.keras.backend.floatx())).shape) #Outputs a tensor of shape (1,2,256) as expected
```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27292,keras.layers.RNN with contants ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): No
- TensorFlow version (use command below): 1.13 and 1.14
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1 and 10.0
- GPU model and memory: 1080 Ti

**Describe the current behavior**

TypeError: can only concatenate list (not ""tuple"") to list in RNN::build() if a call the RNN with a Tensor as constants.

**Describe the expected behavior**

Basically the build() function of the RNNCellWithConstants should be called, with the input_shape = [(3,3,5), (3,3)]

**Code to reproduce the issue**
```
import tensorflow as tf


class RNNCellWithConstants(tf.keras.layers.Layer):

    def __init__(self, **kwargs):
        self.state_size = 5
        super(RNNCellWithConstants, self).__init__(**kwargs)

    def build(self, input_shape):
        print(input_shape)
        self.built = True

    def call(self, inputs, states, constants):
        print(inputs, states, constants)
        return inputs, [inputs]


# Test basic case.
x = tf.keras.Input((None, 5))
c = tf.keras.Input((3,))
cell = RNNCellWithConstants()
layer = tf.keras.layers.RNN(cell)
y = layer(x, constants=c) # Works as expected.

# Test basic case.
x = tf.zeros([3, 3, 5], dtype=tf.float32)
c = tf.zeros([3, 3], dtype=tf.float32)
cell = RNNCellWithConstants()
layer = tf.keras.layers.RNN(cell)
y = layer(x, constants=c) # Crash with the following error
```

**Other info / logs**

Exception from example:
```
Traceback (most recent call last):
  File ""bug.py"", line 25, in <module>
    y = layer(x, constants=c)
  File ""/home/matthias/.local/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 690, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""/home/matthias/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 585, in __call__
    self._maybe_build(inputs)
  File ""/home/matthias/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1706, in _maybe_build
    self.build(input_shapes)
  File ""/home/matthias/.local/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 555, in build
    self.cell.build([step_input_shape] + constants_shape)
TypeError: can only concatenate list (not ""tuple"") to list
```
If I correct the error temporarily I come to another problem, that the input shapes at build call are not correct any more: [(3, 5), (5,)]

So I think the mistake lies in that distinction:

```
    if is_keras_tensor:
      # Compute the full input spec, including state and constants
      full_input = [inputs] + additional_inputs
      # The original input_spec is None since there could be a nested tensor
      # input. Update the input_spec to match the inputs.
      full_input_spec = [None for _ in range(len(nest.flatten(inputs)))
                        ] + additional_specs
      # Perform the call with temporarily replaced input_spec
      self.input_spec = full_input_spec
      output = super(RNN, self).__call__(full_input, **kwargs)
      # Remove the additional_specs from input spec and keep the rest. It is
      # important to keep since the input spec was populated by build(), and
      # will be reused in the stateful=True.
      self.input_spec = self.input_spec[:-len(additional_specs)]
      return output
    else:
      if initial_state is not None:
        kwargs['initial_state'] = initial_state
      if constants is not None:
        kwargs['constants'] = constants
      return super(RNN, self).__call__(inputs, **kwargs)
```
If I set is_keras_tensor to True, everything will behave as expected."
27290,AttributeError: 'KerasTPUModel' object has no attribute '_distribution_strategy',"I load inceptionV3 with:

```
from keras.applications import InceptionV3

model= InceptionV3(weights='imagenet',
                             include_top=False,
                             pooling='max',
                             input_shape=(229, 229, 3))
```

Then I convert it to TPU model with:

```
TPU_WORKER = 'grpc://10.x.x.x:8470'
model = tf.contrib.tpu.keras_to_tpu_model(
            model,
            strategy=tf.contrib.tpu.TPUDistributionStrategy(
                tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))
```

And I try to extract feature vector of an image with:

`model.predict`

The error I receive is

>  AttributeError: 'KerasTPUModel' object has no attribute '_distribution_strategy'

My code works fine without the TPU part, however its very slow and I want to do the predict part on TPU for better performance."
27289,"Test case reports an exception to stdout, with stack trace, when testing under self.assertRaises()","**System information**
- Have I written custom code: Only for testing
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): From PyPI via pip3 install --upgrade tensorflow==1.11.0
- TensorFlow version: v1.11.0-0-gc19e29306c 1.11.0
- Python version: 3.5.2
- CUDA/cuDNN version: CUDA 9.0, cuDNN 7.0
- GPU model and memory: NVIDIA GeForce GTX 1080, 8GB


**Describe the current behavior**
Exception is reported to stdout, with stack trace, when testing under `self.assertRaises()`:

```
2019-03-29 13:48:50.433118: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
ERROR:tensorflow:assertion failed: [1]
         [[{{node Assert/AssertGuard/Assert}} = Assert[T=[DT_FLOAT], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Assert/AssertGuard/Assert/Switch, Assert/AssertGuard/Assert/Switch_1)]]

Caused by op 'Assert/AssertGuard/Assert', defined at:
  File ""/usr/lib/python3.5/runpy.py"", line 184, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.5/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
...
  File ""/usr/lib/python3.5/unittest/case.py"", line 600, in run
    testMethod()
  File ""/home/pryldm1/work/tmp/mock_test.py"", line 19, in testRaises
    sess.run(foo())
  File ""/home/pryldm1/work/tmp/mock_test.py"", line 7, in foo
    max_assert = tf.Assert(tf.greater(max_val, 1.01), [max_val])
  File ""/home/dl_docker/.local/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py"", line 189, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""/home/dl_docker/.local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 158, in Assert
    guarded_assert = cond(condition, no_op, true_assert, name=""AssertGuard"")
  File ""/home/dl_docker/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/dl_docker/.local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2087, in cond
    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)
  File ""/home/dl_docker/.local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1920, in BuildCondBranch
    original_result = fn()
  File ""/home/dl_docker/.local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 156, in true_assert
    condition, data, summarize, name=""Assert"")
  File ""/home/dl_docker/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_logging_ops.py"", line 52, in _assert
    name=name)
  File ""/home/dl_docker/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/dl_docker/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/dl_docker/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""/home/dl_docker/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): assertion failed: [1]
         [[{{node Assert/AssertGuard/Assert}} = Assert[T=[DT_FLOAT], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Assert/AssertGuard/Assert/Switch, Assert/AssertGuard/Assert/Switch_1)]]

..
----------------------------------------------------------------------
Ran 2 tests in 0.045s

OK

```

**Describe the expected behavior**
The exception must be saliently catched:

```
2019-03-29 13:52:58.147694: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
..
----------------------------------------------------------------------
Ran 2 tests in 0.026s

OK
```

**Code to reproduce the issue**
```
import tensorflow as tf


def foo():                                                                                                                                                                                                                                                            
    result = tf.constant(0)                                                                                                                                                                                                                                           
    max_val = tf.constant(1.0)                                                                                                                                                                                                                                        
    max_assert = tf.Assert(tf.greater(max_val, 1.01), [max_val])                                                                                                                                                                                                      
    with tf.control_dependencies([max_assert]):                                                                                                                                                                                                                       
        result = tf.identity(result)                                                                                                                                                                                                                                  
    return result                                                                                                                                                                                                                                                     


class DummyTestCase(tf.test.TestCase):                                                                                                                                                                                                                                
                                                                                                                                                                                                                                                                      
    def testRaises(self):                                                                                                                                                                                                                                             
        with self.test_session() as sess:                                                                                                                                                                                                                             
            with self.assertRaises(tf.errors.InvalidArgumentError):                                                                                                                                                                                                   
                sess.run(foo())                                                                                                                                                                                                                                       


if __name__ == '__main__':                                                                                                                                                                                                                                            
    tf.test.main()
```
"
27288,"GPU Memory Leak for eager mode, tf.scatter_nd_update.","- Have I written custom code : yes
- OS Platform and Distribution : ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: v1.12.0
- Python version: 3.6.8
- CUDA/cuDNN version: 9.2/7.2.1
- GPU model and memory: Tesla P40, 24GB

when i use `tf.scatter_nd_update`, if ref is tf.int32 Variable, everything is fine. if ref is tf.float32 Variable, then there is a memory leak.

i hope the folloing code will reproduce the bug.

```python
import os
import tensorflow as tf
from tensorflow.contrib.memory_stats import BytesInUse

os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""
os.environ[""CUDA_VISIBLE_DEVICES""] = ""2""
config = tf.ConfigProto(allow_soft_placement=True)
config.gpu_options.allow_growth = True
tf.enable_eager_execution(config=config)

for i in range(10):
    ref = tf.zeros((128, 21, 4), dtype=tf.int32)
    indices = tf.zeros([128, 2], dtype=tf.int32)
    updates = tf.ones([128, 4], dtype=tf.int32)
    update = tf.scatter_nd_update(tf.Variable(ref), indices, updates)
    tf.set_random_seed(1)
    print(BytesInUse().numpy())

print('------------------------------------')

for i in range(10):
    ref = tf.zeros((128, 21, 4), dtype=tf.float32)
    indices = tf.zeros([128, 2], dtype=tf.int32)
    updates = tf.ones([128, 4], dtype=tf.float32)
    update = tf.scatter_nd_update(tf.Variable(ref), indices, updates)
    tf.set_random_seed(1)
    print(BytesInUse().numpy())
```

when i run the code, get the following results

```
1280
1280
1280
1280
1280
1280
1280
1280
1280
1280
------------------------------------
89344
132608
175616
261376
303616
347648
389632
433664
475648
519680
```
"
27285,Depthwise separable conv with floating point 16 and batch norm is too slow.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution: **16.04.5 LTS (Xenial Xerus)**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **n.a.**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version (use command below): **1.12.0**
- Python version: **3.6.7**
- Bazel version (if compiling from source): **0.21.0**
- GCC/Compiler version (if compiling from source): **5.4.0**
- CUDA/cuDNN version: **10.0 / 7.3.0**
- GPU model and memory: **v100 16gb**

**Describe the current behavior**
The depthwise separable convolution, unlike the normal convolution when used with floating point 16 is slower than floating point 32, but more importantly when it is used with batch norm and an optmizer (sgd, adam, etc.), it becomes very much (~40 times) slower than floating point 32. Another unexpected behavior is getting ""core dumped"" with most of the filter sizes, that only happens in case of separable conv with fp16, but not with the other combinations. 
Following is what I am getting from the given code:
```
Experiment {'fp16': False, 'sep_conv': False, 'bn': True, 'kernel_size': 4} started
and took 1945 ms
Experiment {'fp16': True, 'sep_conv': False, 'bn': True, 'kernel_size': 4} started
and took 1425 ms
Experiment {'fp16': False, 'sep_conv': True, 'bn': True, 'kernel_size': 4} started
and took 4430 ms
Experiment {'fp16': True, 'sep_conv': True, 'bn': True, 'kernel_size': 4} started
and took 161081 ms
Experiment {'fp16': True, 'sep_conv': True, 'bn': False, 'kernel_size': 4} started
and took 7191 ms
Experiment {'fp16': True, 'sep_conv': True, 'bn': False, 'kernel_size': 3} started
2019-03-29 13:20:30.522213: F tensorflow/stream_executor/stream_executor_pimpl.cc:712] Check failed: 0 == size % 4 (0 vs. 2)need 32-bit multiple size to fill with 32-bit pattern
Aborted (core dumped)
```

**Describe the expected behavior**
I expect the batch norm not to make it so much slower and not to get core dumped with different kernel sizes.
Also, I expected fp 16, to be faster than fp 32, (as is in normal conv).

**Code to reproduce the issue**
```
import time
import tensorflow as tf
from tensorflow.contrib import layers

depth = 50
width = 100
num_filters = 100
batch_size = 16
repeat = 3


def create_optimize_op(use_fp_16: bool, use_separable_conv: bool, use_batch_norm: bool, kernel_size: int,
                       exp_index: int) -> tf.Tensor:

    label = tf.random_normal(shape=[batch_size, width, width, num_filters],
                             dtype=tf.float16 if use_fp_16 else tf.float32)
    input_image = tf.random_normal(shape=[batch_size, width, width, 3], dtype=tf.float16 if use_fp_16 else tf.float32)
    head = input_image
    for layer_ind in range(depth):
        conv_f = layers.separable_conv2d if use_separable_conv else layers.conv2d
        head = conv_f(head, num_filters, (kernel_size, kernel_size), stride=(1, 1), padding=""SAME"", normalizer_fn=None,
                      reuse=tf.AUTO_REUSE, scope=f""{exp_index}layer{layer_ind}"", data_format=""NHWC"")
        if use_batch_norm:
            head = tf.contrib.layers.batch_norm(head, scale=True, is_training=True)
    reshaped_label = tf.reshape(label, shape=[batch_size, -1])
    reshaped_pred = tf.reshape(head, shape=[batch_size, -1])
    loss = tf.losses.softmax_cross_entropy(reshaped_label, reshaped_pred)
    return tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)


experiments = [{""fp16"": False, ""sep_conv"": False, ""bn"": True, ""kernel_size"": 4},
               {""fp16"": True, ""sep_conv"": False, ""bn"": True, ""kernel_size"": 4},
               {""fp16"": False, ""sep_conv"": True, ""bn"": True, ""kernel_size"": 4},
               {""fp16"": True, ""sep_conv"": True, ""bn"": True, ""kernel_size"": 4},
               {""fp16"": True, ""sep_conv"": True, ""bn"": False, ""kernel_size"": 4},
               {""fp16"": True, ""sep_conv"": True, ""bn"": False, ""kernel_size"": 3}]

with tf.Session() as session:
    for index, experiment in enumerate(experiments):
        step = create_optimize_op(experiment[""fp16""], experiment[""sep_conv""], experiment[""bn""],
                                  experiment[""kernel_size""], index)
        init_op = tf.global_variables_initializer()
        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
        print(f""Experiment {experiment} started"")
        # a first warmup run.
        _ = session.run([init_op])
        _ = session.run([update_ops, step])

        start = time.clock()
        for i in range(repeat):
            _ = session.run([init_op])
            _ = session.run([update_ops, step])
        end = time.clock()
        time_taken = int((end - start) * 1000)
        print(f""and took {time_taken} ms"")
```
"
27283,"InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'Reshape' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NO
- TensorFlow installed from (source or binary): N/A
- TensorFlow version (use command below):1.3.0 GPU
- Python version:2.7.12
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):NO
- CUDA/cuDNN version: 9/6
- GPU model and memory: Tesla K80 11441MiB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
So I've loaded my model via a `.pb` file and it loaded fine, but when I try to use it to predict an image it throws the error below.

```
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'Reshape' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; Tshape in [DT_INT32]
  device='GPU'; T in [DT_COMPLEX128]; Tshape in [DT_INT32]
  device='GPU'; T in [DT_COMPLEX64]; Tshape in [DT_INT32]
  device='GPU'; T in [DT_INT8]; Tshape in [DT_INT32]
  device='GPU'; T in [DT_UINT8]; Tshape in [DT_INT32]
  device='GPU'; T in [DT_INT16]; Tshape in [DT_INT32]
  device='GPU'; T in [DT_UINT16]; Tshape in [DT_INT32]
  device='GPU'; T in [DT_INT64]; Tshape in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tshape in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tshape in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tshape in [DT_INT32]

           [[Node: Reshape_165 = Reshape[T=DT_FLOAT, Tshape=DT_INT64](transpose_145, add_31)]]
```

PS: I loaded this model on TF v1.13 and it worked fine, so maybe the problem is in v1.3. I just want to know why this is happening.
**Describe the expected behavior**
It should produce the predicted arrays. 
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27282,/tensorflow/lite/experimental/c/c_api_types.h is not readable on Windows filesystem.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version: f089b3180ec7ddecd503d6d499cd6977c22b8f11
- Python version: No
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): gcc 8.3
- CUDA/cuDNN version: No
- GPU model and memory: No

**Describe the problem**

`/tensorflow/lite/experimental/c/c_api_types.h` is symbolic link to `/tensorflow/lite/c/c_api_internal.h`.  On DOS compatible file system, it is replaced with following text file.
```
../../c/c_api_internal.h
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Clone repository, and make sure file `/tensorflow/lite/experimental/c/c_api_types.h`  is NOT a symbolic link on Windows.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

No"
27281, Error polling for event status: failed to query event: CUDA_ERROR_UNKNOWN during training process,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.10
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 9.0/7.2.1
- GPU model and memory: 3x NVIDIA RTX 2080 Ti


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Sometimes, during training process , i get this error:  E tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_UNKNOWN: unknown error
2019-03-28 20:00:09.965396: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1

This happens sometimes when i am not connected to that machine. For some reason, when i am working on that machine, i'm not getting that error.

**Describe the expected behavior**

**Small snippet of code
model.compile(loss='binary_crossentropy',
              optimizer='adam', metrics=['accuracy','binary_crossentropy'])
filepath=""weights-{epoch:02d}-{acc:.4f}.h5""
mc = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=50)
baseline = model.fit(X,Y,epochs=200, batch_size=32,validation_data=(Xtest, Ytest), callbacks=[mc])


**Code to reproduce the issue**
It reproduces at an epoch numeber bigger than 150( sometimes is around 200, sometimes 500)

**Other info / logs**.
"
27279,TFLite allocate tensors fails,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below):
2.0.0-dev20190327
- Python version:
3.6.8
No GPU

**Describe the current behavior**
I have been trying to convert Yolov3 from tensorflow to tflite and was facing issues due to the lack of ResizeNearestNeighbor custom operation. However with the recent nightly build I was able to convert the model and save the tflite file. When I attempt to load the tflite model, allocate tensors fails with the following error : 

`~/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py in allocate_tensors(self)
     93   def allocate_tensors(self):
     94     self._ensure_safe()
---> 95     return self._interpreter.AllocateTensors()
     96 
     97   def _safe_to_run(self):

~/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py in AllocateTensors(self)
    104 
    105     def AllocateTensors(self):
--> 106         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
    107 
    108     def Invoke(self):

RuntimeError: tensorflow/lite/kernels/concatenation.cc:57 t0->dims->size <= 4 was not true.Node number 265 (CONCATENATION) failed to prepare.`

Code to convert tensorflow frozen graph to tflite
`import tensorflow as tf
INPUT_ARRAYS = ['Placeholder']
OUTPUT_ARRAYS = ['concat_9','mul_6']
converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(
            'checkpoint/yolov3_cpu_nms.pb', INPUT_ARRAYS, OUTPUT_ARRAYS)
converter.allow_custom_ops=True
tflite_model = converter.convert()`

Code to load tflite model and allocate tensors:
`import tensorflow as tf
interpreter=tf.lite.Interpreter(model_path='test.tflite')

interpreter.allocate_tensors()`

"
27278,Error occur while converting graphdef to .tflite. ConverterError: TOCO failed. See console for info.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version (or github SHA if from source): 1.13

I am try to convert the graphDef model to tflite using tf.lite.TFLiteConverter.from_frozen_graph()
checked every steps as mentioned in the document. And also tried to check with the code available [here.](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python)

```
`ConverterError                            Traceback (most recent call last)
<ipython-input-60-6e749ab72451> in <module>
      9 converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shape)
     10 #converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]
---> 11 tflite_model = converter.convert()
     12 open(""E:/training_models/tensorflow_face_detection/model/detect_face.tflite"", ""wb"").write(tflite_model)

~\AppData\Local\Continuum\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\lite\python\lite.py in convert(self)
    453           input_tensors=self._input_tensors,
    454           output_tensors=self._output_tensors,
--> 455           **converter_kwargs)
    456     else:
    457       result = _toco_convert_graph_def(

~\AppData\Local\Continuum\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\lite\python\convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, *args, **kwargs)
    440   data = toco_convert_protos(model_flags.SerializeToString(),
    441                              toco_flags.SerializeToString(),
--> 442                              input_data.SerializeToString())
    443   return data
    444 

~\AppData\Local\Continuum\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\lite\python\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)
    203       stderr = _try_convert_to_unicode(stderr)
    204       raise ConverterError(
--> 205           ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
    206   finally:
    207     # Must manually cleanup files.`
```

Please check the GraphDef model [here](https://github.com/jyotirmayghosh/tensorflow-face-detection).

Below is the detail ConverterError Log:
`ConverterError: TOCO failed. See console for info.
2019-03-29 15:04:07.619011: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3
2019-03-29 15:04:07.619546: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""WrapDatasetVariant"" device_type: ""CPU""') for unknown op: WrapDatasetVariant
2019-03-29 15:04:07.619822: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""WrapDatasetVariant"" device_type: ""GPU"" host_memory_arg: ""input_handle"" host_memory_arg: ""output_handle""') for unknown op: WrapDatasetVariant
2019-03-29 15:04:07.620164: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""UnwrapDatasetVariant"" device_type: ""CPU""') for unknown op: UnwrapDatasetVariant
2019-03-29 15:04:07.620413: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""UnwrapDatasetVariant"" device_type: ""GPU"" host_memory_arg: ""input_handle"" host_memory_arg: ""output_handle""') for unknown op: UnwrapDatasetVariant
2019-03-29 15:04:07.620818: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayV3
2019-03-29 15:04:07.621030: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayScatterV3
2019-03-29 15:04:07.621242: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayScatterV3
2019-03-29 15:04:07.621439: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3
2019-03-29 15:04:07.621643: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayV3
2019-03-29 15:04:07.621821: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.621980: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.622124: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.622431: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.622693: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.622951: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.623237: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: LoopCond
2019-03-29 15:04:07.623506: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: LoopCond
2019-03-29 15:04:07.623810: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.624086: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.624364: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.624635: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.624921: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayReadV3
2019-03-29 15:04:07.625226: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayReadV3
2019-03-29 15:04:07.625557: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.625834: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.626113: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayWriteV3
2019-03-29 15:04:07.626416: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayWriteV3
2019-03-29 15:04:07.626709: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Exit
2019-03-29 15:04:07.626974: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Exit
2019-03-29 15:04:07.627246: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArraySizeV3
2019-03-29 15:04:07.627534: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArraySizeV3
2019-03-29 15:04:07.627767: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayGatherV3
2019-03-29 15:04:07.628010: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayGatherV3
2019-03-29 15:04:07.655854: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3
2019-03-29 15:04:07.656194: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayV3
2019-03-29 15:04:07.656419: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3
2019-03-29 15:04:07.656584: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayV3
2019-03-29 15:04:07.656792: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3
2019-03-29 15:04:07.657003: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayV3
2019-03-29 15:04:07.657222: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayScatterV3
2019-03-29 15:04:07.657419: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayScatterV3
2019-03-29 15:04:07.657637: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayScatterV3
2019-03-29 15:04:07.657834: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayScatterV3
2019-03-29 15:04:07.658037: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayScatterV3
2019-03-29 15:04:07.658200: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayScatterV3
2019-03-29 15:04:07.658401: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3
2019-03-29 15:04:07.658657: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayV3
2019-03-29 15:04:07.658953: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3
2019-03-29 15:04:07.659242: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayV3
2019-03-29 15:04:07.659429: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3
2019-03-29 15:04:07.659616: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayV3
2019-03-29 15:04:07.659818: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3
2019-03-29 15:04:07.660003: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayV3
2019-03-29 15:04:07.660208: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.660408: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.660574: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.660748: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.660924: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.661116: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.661290: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.661484: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.661666: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.661830: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.662024: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.662207: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.662376: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: LoopCond
2019-03-29 15:04:07.662560: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: LoopCond
2019-03-29 15:04:07.662757: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.662926: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.663089: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.663260: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.663444: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayReadV3
2019-03-29 15:04:07.663614: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayReadV3
2019-03-29 15:04:07.663804: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.664024: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.664298: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.664561: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.664845: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayReadV3
2019-03-29 15:04:07.665148: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayReadV3
2019-03-29 15:04:07.665444: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.665711: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.665988: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.666254: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.666525: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayReadV3
2019-03-29 15:04:07.666812: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayReadV3
2019-03-29 15:04:07.667245: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Where
2019-03-29 15:04:07.667517: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Where
2019-03-29 15:04:07.667823: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.668095: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.668382: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Where
2019-03-29 15:04:07.668655: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Where
2019-03-29 15:04:07.668981: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: NonMaxSuppression
2019-03-29 15:04:07.669255: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: NonMaxSuppression
2019-03-29 15:04:07.669547: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Where
2019-03-29 15:04:07.669712: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Where
2019-03-29 15:04:07.669987: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Where
2019-03-29 15:04:07.670221: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Where
2019-03-29 15:04:07.670478: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: NonMaxSuppression
2019-03-29 15:04:07.670756: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: NonMaxSuppression
2019-03-29 15:04:07.671031: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Size
2019-03-29 15:04:07.671248: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Size
2019-03-29 15:04:07.672109: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.672288: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.672471: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayWriteV3
2019-03-29 15:04:07.672678: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayWriteV3
2019-03-29 15:04:07.672832: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.673133: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.673332: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayWriteV3
2019-03-29 15:04:07.673552: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayWriteV3
2019-03-29 15:04:07.673795: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.673988: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.674160: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayWriteV3
2019-03-29 15:04:07.674376: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayWriteV3
2019-03-29 15:04:07.674649: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2019-03-29 15:04:07.674840: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Enter
2019-03-29 15:04:07.675038: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayWriteV3
2019-03-29 15:04:07.675345: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayWriteV3
2019-03-29 15:04:07.675728: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Exit
2019-03-29 15:04:07.676027: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Exit
2019-03-29 15:04:07.676381: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Exit
2019-03-29 15:04:07.676575: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Exit
2019-03-29 15:04:07.676756: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Exit
2019-03-29 15:04:07.676946: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Exit
2019-03-29 15:04:07.677100: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Exit
2019-03-29 15:04:07.677284: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: Exit
2019-03-29 15:04:07.677453: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArraySizeV3
2019-03-29 15:04:07.677657: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArraySizeV3
2019-03-29 15:04:07.677871: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayGatherV3
2019-03-29 15:04:07.678061: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayGatherV3
2019-03-29 15:04:07.678274: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArraySizeV3
2019-03-29 15:04:07.678468: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArraySizeV3
2019-03-29 15:04:07.678671: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayGatherV3
2019-03-29 15:04:07.678837: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayGatherV3
2019-03-29 15:04:07.679068: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArraySizeV3
2019-03-29 15:04:07.679302: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArraySizeV3
2019-03-29 15:04:07.679614: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayGatherV3
2019-03-29 15:04:07.679907: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayGatherV3
2019-03-29 15:04:07.680209: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArraySizeV3
2019-03-29 15:04:07.680499: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArraySizeV3
2019-03-29 15:04:07.680816: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayGatherV3
2019-03-29 15:04:07.681064: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TensorArrayGatherV3
2019-03-29 15:04:07.757845: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1461 operators, 2604 arrays (0 quantized)
2019-03-29 15:04:07.881035: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 1402 operators, 2490 arrays (0 quantized)
2019-03-29 15:04:08.033914: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1402 operators, 2490 arrays (0 quantized)
2019-03-29 15:04:08.127816: F tensorflow/lite/toco/graph_transformations/resolve_constant_slice.cc:59] Check failed: dim_size >= 1 (0 vs. 1)
`

I tried to check very things but not getting the error. Can anyone please check.
"
27276,Crash when opening .npz file with tf.gfile.GFile on Python 3.7 ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): v1.13.0-rc2-5-g6612da8 / 1.13.1
- Python version: Python 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1 / 7.5.0
- GPU model and memory: GTX 1080 / 8G


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
A code below works well on Python 3.6 with TF 1.12. On Python 3.7 with TF 1.13, however, the code crashes. 


**Describe the expected behavior**

Do not crash.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```Python
import numpy as np
import tensorflow as tf

information = { ""a"": 1, ""b"": 2 }
np.savez_compressed(""a.npz"", **information)

with tf.gfile.Open(""a.npz"", ""rb"") as file_:
  info = np.load(file_)

info_dict = { key: info[key] for key in info.items() }  # crash on Python 3.7 with TF 1.13.1
```

Traceback 
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 1, in <dictcomp>
  File ""/csehome/youhanmir/.virtualenvs/learning-implicit-action/lib/python3.7/_collections_abc.py"", line 744, in __iter__
    yield (key, self._mapping[key])
  File ""/csehome/youhanmir/.virtualenvs/learning-implicit-action/lib/python3.7/site-packages/numpy/lib/npyio.py"", line 251, in __getitem__
    bytes = self.zip.open(key)
  File ""/usr/lib64/python3.7/zipfile.py"", line 1480, in open
    self._fpclose, self._lock, lambda: self._writing)
  File ""/usr/lib64/python3.7/zipfile.py"", line 722, in __init__
    self.seekable = file.seekable
AttributeError: 'GFile' object has no attribute 'seekable'
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27275,The training value of tf.keras.Model.call() becomes None when tf.keras.Model.fit(). (tf2.0.0-alpha0),"I read the [document](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model#__call__) and implemented a model with `tf.keras.Model` as a subclass, but the value of training argument becomes `None`.

Is this a bug? Or is my understanding wrong? This is a basic question, but please let me know.

Here is the simplified code.

```python
import numpy as np
import tensorflow as tf
import tensorflow.keras as tfk


class MyLayer(tfk.layers.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()

    def call(self, inputs, training=None):
        # got: Tensor(""keras_learning_phase:0"", shape=(), dtype=bool)
        print(""layer training arg: "", training)
        
        return inputs


class MyModel(tfk.Model):
    def __init__(self):
        super(MyModel, self).__init__()
        self.l1 = MyLayer()

    def call(self, inputs, training=None):
        # want: Tensor(""keras_learning_phase:0"", shape=(), dtype=bool)  got: None
        print(""model training arg: "", training)
        
        inputs = self.l1(inputs)
        return inputs


if __name__ == '__main__':
    x = np.zeros((1, 2))
    model = MyModel()
    model.compile(loss=""mse"", optimizer=""sgd"")
    model.fit(x, x, epochs=1)

```



**System information**

- macOS Mojave 10.14.3
- TensorFlow installed from : pip
- TensorFlow version: 2.0.0a0
- Python version: 3.5.6 (using pyenv)"
27272,ImportError: Missing module: '_pywrap_tensorflow_internal' [Win10][tf-cpu],"**System information**
- **Top-level directory**: C:\Users\user\Documents\GitHub\pysc2-examples
- **OS Platform and Distribution**: Windows 10
- **TensorFlow installed from**: pip
- **TensorFlow version installed** (as of publising the issue): tensorflow-cpu 1.12.0
- **Python version**: 3.6.3
- **Installed using virtualenv? pip? conda?**: Using pip
- **Bazel version**: Not used (it doesn't seem to be required?)
- **GCC/Compiler version (if compiling from source)**: Not used (it doesn't seem to be required?)
- **CUDA/cuDNN version**: Not used (it doesn't seem to be required?)
- **GPU model and memory**: Intel Graphics HD 4000, 4GB RAM
- **CPU model**: Intel Core i3-3217U

After successfully installing tensorflow and the aforementioned project, after running the aforementioned command, this stacked trace is put out:

```
Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train_mineral_shards.py"", line 5, in <module>
    from baselines import deepq
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\baselines\deepq\__init__.py"", line 1, in <module>
    from baselines.deepq import models  # noqa
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\baselines\deepq\models.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

A similar stack trace appears when Tensorflow is imported on python, even when *the current directory is different from where I installed the project*:

```
C:\> python
Python 3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 17:26:49) [MSC v.1900 32 bit (Intel)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

Similar issues have been published, though there is no simple solution to this problem, and all workarounds seem to be outdated, one way or another.

One of the most notable differences from other similar issues, though,  is that **I don't have a problem with the DLL file**, as I have the latest Microsoft Visual C++ [2015 and 2017] Redistributable [x64 and x86]. I have tried to follow the stack trace to where it triggers, and it's this `Path Error` that is being triggered inside `pywrap_tensorflow_internal.py`. I have tried to inspect the **apparently not missing** `_pywrap_tensorflow_internal.so` to no avail."
27270,[TF 2.0] two issues on v2 ctc_loss,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.5.2
- CUDA/cuDNN version: 10.0
- GPU model and memory: Tesla V100, 32Gb


**Describe the current behavior**
 Hi, I'am working on a project and trying to migrate my code to use tf 2.0 preview, and discovered two issues using `ctc_loss`. One is about a missing keyword argument from v2, and the other is about weird behavior of gradient using v1. 

 Current version of `tf.nn.ctc_loss` raises an exception when it encounters outputs longer than label, saying that `ignore_longer_outputs_than_inputs` flag should be used to turn this exception into a warning. However, I noticed that this keyword argument was deleted from new version.
 
Hence I just decided to go back to v1 but another issue came out, which is model being trained to predict blank tokens only, just after one iteration. I'm aware that this could have happened due to other parts of my code, but I'm reporting this after several tests on that.   

**Describe the expected behavior** 
 The `ignore_longer_outputs_than_inputs` keyword should be added to the new `ctc_loss`, or set True by default. 


**Code to reproduce the issue**
I will provide code later in the comment

**Other info / logs**
```
2019-03-29 13:38:40.841685: W tensorflow/core/framework/op_kernel.cc:1431] OP_REQUIRES failed at ctc_loss_op.cc:168 : Invalid argument: Not enough time for target transition sequence (required: 49, available: 48)32You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs
2019-03-29 13:38:40.841912: W tensorflow/core/common_runtime/base_collective_executor.cc:214] BaseCollectiveExecutor::StartAbort Invalid argument: Not enough time for target transition sequence (required: 49, available: 48)32You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs
         [[{{node replica_2/CTCLoss}}]]
         [[Adam/group_deps/_1107]]
2019-03-29 13:38:40.841981: W tensorflow/core/common_runtime/base_collective_executor.cc:214] BaseCollectiveExecutor::StartAbort Invalid argument: Not enough time for target transition sequence (required: 49, available: 48)32You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs
         [[{{node replica_2/CTCLoss}}]]
         [[Adam/update_2_1/AssignAddVariableOp/_1091]]
2019-03-29 13:38:40.842035: W tensorflow/core/common_runtime/base_collective_executor.cc:214] BaseCollectiveExecutor::StartAbort Invalid argument: Not enough time for target transition sequence (required: 49, available: 48)32You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs
         [[{{node replica_2/CTCLoss}}]]
         [[GroupCrossDeviceControlEdges_0/Adam/update_2_1/Const/_1063]]
2019-03-29 13:38:40.842177: W tensorflow/core/common_runtime/base_collective_executor.cc:214] BaseCollectiveExecutor::StartAbort Invalid argument: Not enough time for target transition sequence (required: 49, available: 48)32You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs
         [[{{node replica_2/CTCLoss}}]]
         [[Adam/group_deps/_1103]]
2019-03-29 13:38:40.848029: E tensorflow/core/common_runtime/process_function_library_runtime.cc:764] Component function execution failed: Invalid argument: Not enough time for target transition sequence (required: 49, available: 48)32You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs
         [[{{node replica_2/CTCLoss}}]]
         [[GroupCrossDeviceControlEdges_0/Adam/update_2_1/Const/_1063]]
2019-03-29 13:38:40.848362: E tensorflow/core/common_runtime/process_function_library_runtime.cc:764] Component function execution failed: Invalid argument: Not enough time for target transition sequence (required: 49, available: 48)32You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs
         [[{{node replica_2/CTCLoss}}]]
         [[Adam/group_deps/_1107]]
2019-03-29 13:38:40.850240: E tensorflow/core/common_runtime/process_function_library_runtime.cc:764] Component function execution failed: Invalid argument: Not enough time for target transition sequence (required: 49, available: 48)32You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs
         [[{{node replica_2/CTCLoss}}]]
         [[Adam/group_deps/_1103]]
2019-03-29 13:38:40.852283: E tensorflow/core/common_runtime/process_function_library_runtime.cc:764] Component function execution failed: Invalid argument: Not enough time for target transition sequence (required: 49, available: 48)32You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs
         [[{{node replica_2/CTCLoss}}]]
         [[Adam/update_2_1/AssignAddVariableOp/_1091]]
2019-03-29 13:38:40.867602: W tensorflow/core/common_runtime/base_collective_executor.cc:214] BaseCollectiveExecutor::StartAbort Invalid argument: Not enough time for target transition sequence (required: 49, available: 48)32You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs
         [[{{node replica_2/CTCLoss}}]]
2019-03-29 13:38:40.881608: E tensorflow/core/common_runtime/process_function_library_runtime.cc:764] Component function execution failed: Invalid argument: Not enough time for target transition sequence (required: 49, available: 48)32You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs
         [[{{node replica_2/CTCLoss}}]]
Traceback (most recent call last):
  File ""train.py"", line 196, in <module>
    exp_fn = main(args, CONFIG)
  File ""train.py"", line 120, in main
    distributed_train()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/def_function.py"", line 414, in __call__
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/function.py"", line 1288, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/function.py"", line 574, in _filtered_call
    (t for t in nest.flatten((args, kwargs))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/function.py"", line 627, in _call_flat
    outputs = self._inference_function.call(ctx, args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/function.py"", line 415, in call
    ctx=ctx)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/execute.py"", line 66, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Not enough time for target transition sequence (required: 49, available: 48)32You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs
         [[{{node replica_2/CTCLoss}}]]
         [[GroupCrossDeviceControlEdges_0/Adam/update_2_1/Const/_1063]] [Op:__inference_distributed_train_49990]
```"
27269,[doc] broken link in tf.logging,"
**System information**
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/logging


**Describe the documentation issue**
> Defined in **tensorflow/_api/v1/logging/ _ _ init _ _.py**.

The link for _ _ init _ _.py is broken: https://www.tensorflow.org/code/stable/tensorflow/_api/v1/logging/__init__.py

https://www.tensorflow.org/api_docs/python/tf/logging"
27268,"if I use feature_column.embedding_column, how to get embedding variables?","```
uid = fc.categorical_column_with_hash_bucket(""uid"", 10000, dtype=tf.int64)
item_id = fc.categorical_column_with_hash_bucket(""item_id"", 10000, dtype=tf.int64)
uid_embed = fc.embedding_column(uid, 64)
item_id_embed = fc.embedding_column(item_id, 64)
```
I want to extract the embedding matrix for other task, how to implement this idea?"
27267,TensorFlow 1.x to TensorFlow 2.x,"tf.contrib.training.HParams is TensorFlow 1.x
How can I use tf.contrib.training.HParams in 2.x?"
27266,Incorrect flops calculation for operations with complex numbers,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 4.8.5-16
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): via Anaconda
- TensorFlow version (use command below): 1.12
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Code to reproduce the issue**
```
tf.reset_default_graph()
x = tf.constant(2 + 3j, dtype='complex64')
y = tf.constant(3 + 2j, dtype='complex64')
z = x * y

session = tf.Session()
session.run(tf.global_variables_initializer())

run_meta = tf.RunMetadata()
opts = tf.profiler.ProfileOptionBuilder.float_operation()    
flops = tf.profiler.profile(run_meta=run_meta, cmd='op', options=opts) 
print(flops.total_float_ops) 
```
The output is:
```
1
```

**Describe the current behavior**
Multiplication of complex numbers typically requires 6 floating point operations (4 real multiplications and 2 real additions) and not just 1 floating point operation. The [functions that calculate the flops statistics](https://github.com/tensorflow/tensorflow/blob/f43d458a318d4d97298710654f1692f6e8364f82/tensorflow/python/profiler/internal/flops_registry.py) do not seem to distinguish between floats and complex numbers.

**Expected behavior**
For complex numbers, addition and multiplication should require 2 and 6 flops (with real numbers) respectively.

"
27265,Download Images fail: Get curl: (23) Failed writing body (0 != 2696),"<em>Tried to follow the tutorial: TensorFlow For Poets, but failed to download images on the second step
</em>


**System information**
- TensorFlow version: Not related
- Doc Link: https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#2


**Describe the documentation issue**
I used the command copied from tensor document: [TensorFlow For Poets](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#2)

The commond is `curl http://download.tensorflow.org/example_images/flower_photos.tgz | tar xz -C tf_files`
Then I got curl: (23) Failed writing body (0 != 2696)
"
27262,How to get Gather instead of GatherV2 in GraphDef?,"Tried to use ""tf.gather"", each time when I export GraphDef, just saw op GatherV2, so how could I get Gather in graphdef?"
27261,OOM error,"```
2019-03-28 20:17:05.067925: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at cast_op.cc:109 : Resource exhausted: OOM when allocating tensor with shape[612,612,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 
by allocator cpu
Traceback (most recent call last):
  File ""/home/models/TF-2/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/home/models/TF-2/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/models/TF-2/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[427,640,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
         [[{{node rgb_to_grayscale/convert_image/Cast}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[{{node superpoint/IteratorGetNext}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[{{node superpoint/vgg/conv1_1/bn/beta/read}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""experiment.py"", line 148, in <module>
    args.func(config, output_dir, args)
  File ""experiment.py"", line 86, in _cli_train
    train(config, config['train_iter'], output_dir)
  File ""experiment.py"", line 27, in train
    keep_checkpoints=config.get('keep_checkpoints', 1))
  File ""/home/models/codes/SuperPoint/superpoint/models/base_model.py"", line 313, in train
    options=options, run_metadata=run_metadata)
  File ""/home/models/TF-2/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/home/models/TF-2/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/models/TF-2/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/home/models/TF-2/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[427,640,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
         [[{{node rgb_to_grayscale/convert_image/Cast}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[node superpoint/IteratorGetNext (defined at /home/models/codes/SuperPoint/superpoint/models/base_model.py:260) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
```

TF version: 1.13.1
cudnn: 7
cuda: 10.0

Question: I do not understand what is meaning of it. Is GPU memory exhausted. Which is api by which I can change policy of cacheing etc."
27260,tensorflow.keras model.fit_generator not working with use multiprocessing,"I am using Ubuntu 18 with conda and python 3.6 with tf 1.20 GPU over a gtx 1070 and 2080ti

When I am using this code 

    model.fit_generator(i, epochs=1, workers=16,
                            use_multiprocessing=False, max_queue_size=16,
                            verbose=1)

It works fine

When I change use_multiprocessing to True, and run, it does nothing, the cpu is not utilized by python at all, and nothing shows on the monitor. It used to work fine in the past. 

What am I missing here? "
27259,r1.13 multi-gpu towering fails due to DeviceSpec parsing during model creation,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): This is a custom written yolov3 model written primarily with the tf.layers api which is soon to deprecated. 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 & Power9
- TensorFlow installed from (source or binary): compiled from r1.13 source branch and Conda tensorflow-gpu package from https://public.dhe.ibm.com/ibmdl/export/pub/software/server/ibm-ai/conda/
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- Bazel version (if compiling from source): 0.18.1
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: Cuda 10.0 CuDNN 7.5 NCCL 2.4
- GPU model and memory: GT 1080 Ti 12GB and V100 16GB

**Describe the current behavior**
Under Tensorflow r1.13 my single node multi-gpu training code produces a 
```
splits = [x.split("":"") for x in spec.split(""/"")]
AttributeError: 'DeviceSpec' object has no attribute 'split'
```

Error when constructing the model, before a single forward pass has happened.

The towering code I am using is based on the cifar10_multi_gpu_train.py example:
https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py

**Describe the expected behavior**
My codebase works correctly on Tensroflow r1.12

Given the error and my reading of the tensorflow source code throwing the error, my best guess is some op is missing/has a malformed device_spec, or a deivce_spec object is being passed into the `DeviceSpec().parse_from_string(spec)` function. 

The layer that seems to be generating this (see trace below) is 
```
tf.layers.conv2d_transpose
``` 

I am using several: `tf.device('/cpu:0')` and `tf.device('/gpu:#')` statements to control the placement of ops within different towers onto specific devices. and the problematic `tf.layers.conv2d_transpose` is happening once per tower under a 
```with tf.device('/gpu:#'):``` However, if this error were tied to my mishandling of those control statements the many layers of the model that get added before `tf.layers.conv2d_transpose` would have caused an error. 

These issues in the past seem somewhat relevant, but they all refer to much older source code for tensorflow, and this is a change since r1.12. So I am not putting much stock in any of the discussion from these issues:
- https://github.com/keras-team/keras/issues/10008
- https://github.com/keras-team/keras/issues/10490

Does anyone have any pointers where I could look for solutions? My entire codebase will be shifting to tf2.0 api once thats out of alpha. However, it is my understanding that model data parallel training is not fully operational at this time for custom models (non tf.keras.models.Sequential models).

**Other info / logs**
```
Creating model
Building model towers
  tower 0
    building feature map
shutdown complete
  File ""train_yolo.py"", line 348, in <module>
    train_model()
  File ""train_yolo.py"", line 169, in train_model
    train_op, all_loss_total_op, all_loss_xy_op, all_loss_wh_op, all_loss_objectness_op, all_loss_class_op, ops_per_gpu, train_init_op, test_init_op, is_training_placeholder = yolov3.build_towered_model(train_reader, test_reader, GPU_IDS, LEARNING_RATE)
  File ""/fs/wrk/mmajursk/yolov3/src/yolov3.py"", line 562, in build_towered_model
    outputs, total_loss_op, loss_xy_op, loss_wh_op, loss_objectness_op, loss_class_op = tower_loss(image_batch, label_batch, is_training_placeholder)
  File ""/fs/wrk/mmajursk/yolov3/src/yolov3.py"", line 464, in tower_loss
    pred_feature_map, outputs = add_inference_ops(images, is_training=is_training)
  File ""/fs/wrk/mmajursk/yolov3/src/yolov3.py"", line 446, in add_inference_ops
    pred_feature_map = build_feature_maps(images, is_training=is_training)
  File ""/fs/wrk/mmajursk/yolov3/src/yolov3.py"", line 211, in build_feature_maps
    inputs = upsample_2x(inputs)
  File ""/fs/wrk/mmajursk/yolov3/src/yolov3.py"", line 168, in upsample_2x
    trainable=False)  
  File ""/home/mmajursk/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""/home/mmajursk/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py"", line 1279, in conv2d_transpose
    return layer.apply(inputs)
  File ""/home/mmajursk/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1227, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/home/mmajursk/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 530, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/home/mmajursk/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 554, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/mmajursk/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 832, in call
    dilation_rate=self.dilation_rate)
  File ""/home/mmajursk/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 4267, in conv2d_transpose
    x, tf_data_format = _preprocess_conv2d_input(x, data_format, force_transpose)
  File ""/home/mmajursk/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 4082, in _preprocess_conv2d_input
    if not _has_nchw_support() or force_transpose:
  File ""/home/mmajursk/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 592, in _has_nchw_support
    explicitly_on_cpu = _is_current_explicit_device('CPU')
  File ""/home/mmajursk/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 561, in _is_current_explicit_device
    device = _get_current_tf_device()
  File ""/home/mmajursk/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 541, in _get_current_tf_device
    graph._apply_device_functions(op)
  File ""/home/mmajursk/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 4246, in _apply_device_functions
    op._set_device(device_spec.function(op))
  File ""/home/mmajursk/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/device.py"", line 314, in _device_function
    current_device = DeviceSpec.from_string(node_def.device or """")
  File ""/home/mmajursk/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/device.py"", line 232, in from_string
    return DeviceSpec().parse_from_string(spec)
  File ""/home/mmajursk/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/device.py"", line 150, in parse_from_string
    splits = [x.split("":"") for x in spec.split(""/"")]
AttributeError: 'DeviceSpec' object has no attribute 'split'
```

Thanks for any help/insights you can provide.
~Michael
"
27255,Issues with @tf.function and abstracted classes,"**System information**
- Have I written custom code: YES
- OS Platform and Distribution: Ubuntu 18.04
- Mobile device: NO
- TensorFlow installed from (source or binary): PyPI
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6.4
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
When using `@tf.function` on a function that involves abstract classes (e.g. for instance layers or other NN abstractions) Tensorflow fails with the error:
```
2019-03-28 19:44:23.869165: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-28 19:44:23.901972: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2693750000 Hz
2019-03-28 19:44:23.902265: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x555bfd2f8e40 executing computations on platform Host. Devices:
2019-03-28 19:44:23.902288: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>
Traceback (most recent call last):
  File ""/home/alex/work/python/simple-kf/tf_git/example.py"", line 87, in <module>
    main()
  File ""/home/alex/work/python/simple-kf/tf_git/example.py"", line 81, in main
    loss = train_one_step(params, data)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 426, in __call__
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 370, in _initialize
    *args, **kwds))
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1313, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1580, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1512, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 694, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 317, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 686, in wrapper
    ), args, kwargs)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 392, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/tmpc8qghhfj.py"", line 6, in tf__train_one_step
    logits, extra = ag__.converted_call(model, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_1, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (variables, batch), {'return_inputs': False, 'return_outputs': False})
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 392, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/tmp2wyz0_pd.py"", line 5, in tf____call__
    result, extra = ag__.converted_call('__call__', super(ParametricFunction, self), ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call, defun_1, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (params, inputs), dict(kwargs, **{'return_inputs': return_inputs, 'return_outputs': return_outputs}))
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 392, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/tmpn16v62zu.py"", line 6, in tf____call__
    retval_ = ag__.converted_call('call', self, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call, defun_1, ag__.convert, ag__.do_not_convert, ag__.converted_call, defun_2, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (params, inputs), dict(kwargs, **{'return_inputs': return_inputs, 'return_outputs': return_outputs}))
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 392, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/tmp43quxi5v.py"", line 3, in tf__call
    raise ag__.converted_call(NotImplementedError, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call, defun_1, ag__.convert, ag__.do_not_convert, ag__.converted_call, defun_2, ag__.convert, ag__.do_not_convert, ag__.converted_call, defun_3, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), ('call not implemented for {}.'.format(self.__dict__),), {})
NotImplementedError: call not implemented for {'_name': None, 'initializers': None, 'out_dim': 784}.
```

**Describe the expected behaviour**
To work as though if it was run without `@tf.function` (e.g. in Eager Mode).

**Code to reproduce the issue**
```
from abc import abstractmethod
import tensorflow as tf
import logging


class Function(object):
    def __init__(self, name=None):
        self._name = name

    def call(self, params, inputs, **kwargs):
        raise NotImplementedError(""call not implemented for {}."".format(self.__dict__))

    def __call__(self, params, inputs, return_inputs=False, return_outputs=False, **kwargs):
        return self.call(params, inputs,
                         return_inputs=return_inputs,
                         return_outputs=return_outputs,
                         **kwargs)


def expand_bias_to(bias, shape):
    return tf.reshape(bias, [1] * (len(shape) - len(bias.shape)) + bias.shape.as_list())


def call_or_get(maybe_callable, *args, **kwargs):
    if callable(maybe_callable):
        return maybe_callable(*args, **kwargs)
    else:
        return maybe_callable


class ParametricFunction(Function):
    def __init__(self, initializers, name=None):
        super(ParametricFunction, self).__init__(name=name)
        self.initializers = initializers

    def __call__(self, params, inputs, return_inputs=False, return_outputs=False, **kwargs):
        result, extra = super(ParametricFunction, self). \
            __call__(params, inputs,
                     return_inputs=return_inputs,
                     return_outputs=return_outputs,
                     **kwargs)
        return result, extra

    @abstractmethod
    def call(self, params, inputs, **kwargs):
        raise NotImplementedError()


class Affine(ParametricFunction):
    def __init__(self, out_dim,
                 name=None):
        super(Affine, self).__init__(initializers=None, name=name)
        self.out_dim = out_dim

    def call(self, params, inputs, **kwargs):
        if len(params) == 2:
            w, b = params
            b = expand_bias_to(b, inputs.shape)
        else:
            w, b = params[0], 0
        return tf.matmul(inputs, w) + b, None


# !!! IF THIS IS COMMENTED OUT IT WORKS !!!
@tf.function
def train_one_step(variables, batch):
    model = Affine(784)
    logits, extra = model(variables, batch, return_inputs=False, return_outputs=False)
    loss_value = tf.reduce_mean(logits ** 2)
    return loss_value


def main():
    batch_size = 100
    tf.get_logger().setLevel(logging.WARNING)
    params = (tf.Variable(initial_value=tf.random.normal([784, 784]), name=""W""),
              tf.Variable(initial_value=tf.zeros([784]), name=""b""))
    data = tf.random.normal([batch_size, 784])

    for epoch in range(10):
        loss = train_one_step(params, data)
        epoch += 1
        print(""Epoch {}, loss: {:0.3f}"".format(epoch, loss))


if __name__ == '__main__':
    main()
```

**Other info / logs**
This seems to stem somehow from the inheritance of abstract methods.
"
27253,[CUDA] tensorflow/core/kernels:cwise_op_gpu is not Clang-compatible,"**System information**
- OS Platform and Distribution: Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): source
- TensorFlow version: f09580c8362385f934e7d8321353de06512d92be in master (Date:  Thu Mar 28 02:02:36 2019 -0700)
- Python version: 3.6
- Installed using virtualenv? pip? conda?: not yet installed
- Bazel version (if compiling from source): 0.24.0
- GCC/Compiler version (if compiling from source): clang r348507-1
- CUDA/cuDNN version: CUDA 10.0, cuDNN 7.5.0
- GPU model and memory: not likely related to the issue



**Describe the problem**
A build process fails every time at the same point '//tensorflow/core/kernels:cwise_op_gpu'.  I suspect the file ""cwise_ops_gpu_common.cu.h"" and/or the related .h file(s) might have an inconsistency.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

## How to reproduce the issue

**First, change all ${TF_*_VERSION}, ${*_PATH} according to your environment.**

```console
# Preparation
sudo apt install -y bazel_0.24.0-linux-x86_64.deb
sudo apt install -y apt-utils wget pkg-config ca-certificates apt-transport-https gnupg2 curl liblapack-dev libeigen3-dev git-core subversion automake zlib1g-dev unzip python zip libflann-dev libvtk6-dev cmake libboost-filesystem-dev libboost-date-time-dev libboost-thread-dev libboost-iostreams-dev libboost-system-dev libproj-dev python3-numpy software-properties-common curl libtool
sudo apt-get install /tmp/nv-tensorrt-repo-ubuntu1604-cuda10.0-trt5.0.2.6-ga-20181009_1-1_amd64.deb && \
sudo apt-key add /var/nv-tensorrt-repo-cuda10.0-trt5.0.2.6-ga-20181009/7fa2af80.pub
sudo apt update && sudo apt install -y tensorrt libnccl2 libnccl-dev

# Fit these envs
  export PYTHON_BIN_PATH=/usr/bin/python3
  export PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages
  export CUDA_TOOLKIT_PATH=/usr/local/cuda
  export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu
  export TENSORRT_INSTALL_PATH=/usr/lib/x86_64-linux-gnu
  export NCCL_INSTALL_PATH=/usr/lib/x86_64-linux-gnu

  export TF_NEED_ROCM=0
  export TF_NEED_CUDA=1
  export TF_CUDA_VERSION=""$($CUDA_TOOLKIT_PATH/bin/nvcc --version | sed -n 's/^.*release \(.*\),.*/\1/p')""
  export TF_CUDA_COMPUTE_CAPABILITIES=6.1
  export TF_CUDNN_VERSION=7.5.0
  export TF_NEED_TENSORRT=1
  export TF_TENSORRT_VERSION=5.0.2
  export TF_NCCL_VERSION=2.4.2

  export TF_CUDA_CLANG=1
  export TF_DOWNLOAD_CLANG=1

  export TF_NEED_HDFS=1
  export TF_NEED_OPENCL=0
  export TF_NEED_JEMALLOC=1
  export TF_ENABLE_XLA=1
  export TF_NEED_VERBS=0
  export TF_NEED_MKL=0
  export TF_DOWNLOAD_MKL=0
  export TF_NEED_AWS=0
  export TF_NEED_MPI=0
  export TF_NEED_GDR=0
  export TF_NEED_S3=0
  export TF_NEED_OPENCL_SYCL=0
  export TF_SET_ANDROID_WORKSPACE=0
  export TF_NEED_COMPUTECPP=0
  export TF_NEED_KAFKA=0

  export CC_OPT_FLAGS=""-O3 -march=native""

  export C_INCLUDE_PATH=/usr/include/python3.6m
  export CPLUS_INCLUDE_PATH=/usr/include/python3.6m

# Clone
  git clone https://github.com/tensorflow/tensorflow.git --depth 1
  cd tensorflow/
  ./configure


  time bazel build -c opt --distinct_host_configuration=false --copt=-march=native --copt=-O3 --verbose_failures //tensorflow/core/kernels:cwise_op_gpu

```

Then, Clang and Bazel will exit with an error log like below.

```
In file included from tensorflow/core/kernels/cwise_op_gpu_xdivy.cu.cc:18:
In file included from ./tensorflow/core/kernels/cwise_ops_gpu_common.cu.h:27:
In file included from ./tensorflow/core/framework/tensor_types.h:19:
In file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14:
In file included from external/eigen_archive/Eigen/Core:150:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:211:51: error: invalid operands to binary expression ('const float4' and 'const float4')
pxor(const Packet& a, const Packet& b) { return a ^ b; }
                                                ~ ^ ~
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:258:33: note: in instantiation of function template specialization 'Eigen::internal::pxor<float4>' requested here
pzero(const Packet& a) { return pxor(a,a); }
                                ^
./tensorflow/core/kernels/cwise_ops.h:624:20: note: in instantiation of function template specialization 'Eigen::internal::pzero<float4>' requested here
    Packet zeros = pzero(x);
                   ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:520:22: note: in instantiation of function template specialization 'Eigen::internal::xdivy_op<float>::packetOp<float4>' requested here
    return m_functor.packetOp(m_leftImpl.template packet<LoadMode>(index), m_rightImpl.template packet<LoadMode>(index));
                     ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:159:75: note: in instantiation of function template specialization 'Eigen::TensorEvaluator<const Eigen::TensorCwiseBinaryOp<Eigen::internal::xdivy_op<float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, int>, 16, MakePointer>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> >, Eigen::GpuDevice>::packet<16>' requested here
    m_leftImpl.template writePacket<LhsStoreMode>(i, m_rightImpl.template packet<RhsLoadMode>(i));
                                                                          ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:350:12: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, int>, 16, MakePointer>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::xdivy_op<float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, int>, 16, MakePointer>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> > >, Eigen::GpuDevice>::evalPacket' requested here
      eval.evalPacket(i);
           ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:367:63: note: in instantiation of member function 'Eigen::internal::EigenMetaKernelEval<Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, int>, 16, MakePointer>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::xdivy_op<float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, int>, 16, MakePointer>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> > >, Eigen::GpuDevice>, long, true>::run' requested here
  EigenMetaKernelEval<Evaluator, StorageIndex, vectorizable>::run(eval, first_index, size, step_size);
                                                              ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:386:10: note: in instantiation of function template specialization 'Eigen::internal::EigenMetaKernel<Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, int>, 16, MakePointer>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::xdivy_op<float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, int>, 16, MakePointer>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> > >, Eigen::GpuDevice>, long>' requested here
        (EigenMetaKernel<TensorEvaluator<Expression, GpuDevice>, StorageIndex>),
         ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35:59: note: in instantiation of member function 'Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, int>, 16, MakePointer>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::xdivy_op<float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, int>, 16, MakePointer>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> > >, Eigen::GpuDevice, true, false>::run' requested here
      internal::TensorExecutor<const Assign, DeviceType>::run(assign, m_device);
                                                          ^
./tensorflow/core/kernels/cwise_ops_gpu_common.cu.h:54:28: note: in instantiation of function template specialization 'Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, int>, 16, MakePointer>, Eigen::GpuDevice>::operator=<Eigen::TensorCwiseBinaryOp<Eigen::internal::xdivy_op<float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, int>, 16, MakePointer>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> > >' requested here
    To32Bit(out).device(d) =
                           ^
```


**Any other info / logs**
- *#26159 may be related to this issue. Please check it out as well.*
- *This is a full version of the error log. Just for your information.*
-- [full-error.log](https://github.com/tensorflow/tensorflow/files/3019127/full-error.log)
- Workaround : ~Now I'm trying nvcc to reproduce the problem.~  With nvcc, build succeeded. "
27252,tensorflow won't compile,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: only import tensorflow
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**: pip 
- **TensorFlow version (use command below)**:
- **Python version**: 3.7.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:  cuda 10.1, cudnn v.7.5
- **GPU model and memory**: gtx 1070
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

## and this is the error


Traceback (most recent call last):
  File ""C:\Users\kevol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\kevol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\kevol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\kevol\Anaconda3\envs\tensorflow\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\kevol\Anaconda3\envs\tensorflow\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\kevol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\kevol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\kevol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\kevol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\kevol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\kevol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\kevol\Anaconda3\envs\tensorflow\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\kevol\Anaconda3\envs\tensorflow\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
27250,helper_functions.inc (Google howto fail),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): VMware Ubuntu 18.04.2
- TensorFlow installed from (source or binary): according to howto (see description of problem)
- TensorFlow version: according to howto (see description of problem)
- Python version: latest
- Installed using virtualenv? pip? conda?: according to howto (see description of problem)
- Bazel version (if compiling from source): not installed
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: NA
- GPU model and memory: VMware


**Describe the problem**

ludwig1@ubuntu:~/dev$ make -f tensorflow/tensorflow/lite/experimental/micro/tools/make/Makefile test
tensorflow/tensorflow/lite/experimental/micro/tools/make/Makefile:5: tensorflow/lite/experimental/micro/tools/make/helper_functions.inc: No such file or directory
/bin/sh: 1: [[: not found
make: *** No rule to make target 'tensorflow/lite/experimental/micro/tools/make/helper_functions.inc'.  Stop.


**Provide the exact sequence of commands / steps that you executed before running into the problem**

Following howto: https://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/#2

Used command: ""make -f tensorflow/lite/experimental/micro/tools/make/Makefile test""

**Any other info / logs**
NA
"
27249,importError: cannot import name 'bitwise',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

import tensorflow as tf

gives me this error:

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-12-fe025adf6030> in <module>()
      1 # Import TensorFlow
----> 2 import tensorflow as tf
      3 
      4 # Define a and b as placeholders
      5 a = tf.placeholder(dtype=tf.int8)

/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in <module>()
     38 
     39 from tensorflow import app
---> 40 from tensorflow import bitwise
     41 from tensorflow import compat
     42 from tensorflow import data

ImportError: cannot import name 'bitwise'




**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27248,How to transfer a ckpt file to the code?,"I have a demand that need to transfer the ckpt.meta file to the code. Don't use the ""import_meta_graph"". I means that if we know the input_tensor name and logit name, how we get the subgraph of this and transfer the graph to ""inference"" code."
27247,Feedback limitation in a documentation web page ,"Hello tf developers team. 

I'm coming to this page: https://www.tensorflow.org/api_docs/python/tf/bitwise, and I see no indication whether this is implemented on GPU or CPU only. Will tf help me for my project or not? I find this information page extremely unhelpful for my purpose. As the overview should state clearly when to use the library and when not (I need only bitwise operations). 

I left a feedback on your page giving two stars - but I can't write a string for you. How would you know what to fix if everyone are just saying the document is bad, and can't explain why and what they're missing? 
"
27246,Build Issue on windows - cd command does not change drive without /d flag,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Windows 10:
- TensorFlow installed from (source or binary): building
- TensorFlow version: Master
- Python version: 3.6
- Bazel version (if compiling from source):23.0
- GCC/Compiler version (if compiling from source):VS2015
- CUDA/cuDNN version:7.1
- GPU model and memory:GTX970



**Describe the problem**

When building with bazel the it fails with the following 

`ERROR: D:/phil/python/tensorflow/tensorflow/core/BUILD:2676:1: Executing genrule //tensorflow/core:version_info_gen failed (Exit 1): bash.exe failed: error executing command
  cd C:/users/username/_bazel_username/frxyltss/execroot/org_tensorflow

  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1

   <more set parameters>

  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/tensorflow/tools/git/gen_git_source.exe --generate external/local_config_git/gen/spec.json external/local_config_git/gen/head external/local_config_git/gen/branch_ref ""bazel-out/x64_windows-opt/genfiles/tensorflow/core/util/version_info.cc"" --git_tag_override=${GIT_TAG_OVERRIDE:-}

Execution platform: @bazel_tools//platforms:host_platform

D:\Phil\Virtual Environments\tensorflow_compilation\Scripts\python.exe: can't find '__main__' module in 'C:\\users\\yvann'
Target //tensorflow/tools/pip_package:build_pip_package failed to build`

The cause of this is that the tensorflow source directory is on the D drive, but the temporary files are created in the windows user directory that is on the C drive. On windows the cd command does not change drive by default. Thus the line 
`cd C:/users/username/_bazel_username/frxyltss/execroot/org_tensorflow`
will not change the directory if the current working directory is on the D drive. This can be fixed by making the above line cd /d <path> which will then change drive. Alternatively you can insert cd D:
This command appears to be autogenerated but i cannot easily see how to change it.
"
27245,tf.data input pipeline got strange bug with tensorflow-gpu,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 16.04
- Mobile device if the issue happens on mobile device: Dell Precision Tower 7910
- TensorFlow installed from (source or binary): tensorflow-gpu 1.12.0
- TensorFlow version (use command below): tensorflow-gpu 1.12.0
- Installed from: conda
- Python version: python 3.6
- GCC/Compiler version (if compiling from source): 6.5.0
- CUDA/cuDNN version: 9.2 but nvcc 7.5
- GPU model and memory:Quadro M4000


**Describe the current behavior**
The following code worked fine with tensorflow-cpu 1.12.0 but not with the gpu version.
I got error:
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: transpose expects a vector of size 5. But input(1) is a vector of size 4
	 [[{{node gradients/Conv1/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](IteratorGetNext/_19, PermConstNHWCToNCHW-LayoutOptimizer)]]
	 [[{{node mean_squared_error/num_present/broadcast_weights/assert_broadcastable/AssertGuard/Assert/Switch_2/_37}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_63_me...t/Switch_2"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
```
Here's the traceback:

> Traceback (most recent call last):
>   File ""/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
>     return fn(*args)
>   File ""/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
>     options, feed_dict, fetch_list, target_list, run_metadata)
>   File ""/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
>     run_metadata)

Another thread on parallel threw:

> Traceback (most recent call last):
>   File ""20190328_.py"", line 52, in <module>    sess.run([train_op])
>   File ""/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
>     run_metadata_ptr)
>   File ""/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
>     feed_dict_tensor, options, run_metadata)
>   File ""/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
>   File ""/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
>     raise type(e)(node_def, op, message)

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import h5py
import threading
import numpy as np
import tensorflow as tf

# make some random img data
for i in range(100):
    with h5py.File('./test_{}.h5'.format(i), 'w') as f:
        f.create_dataset('X', shape=(1000, 100, 100), dtype='float32', data=np.random.rand(10**7).reshape(1000, 100, 100))
        f.create_dataset('y', shape=(1000, 100, 100), dtype='float32', data=np.random.rand(10**7).reshape(1000, 100, 100))
        print(threading.get_ident())

# params
num_cores = 3
shuffle_size = 1
batch_size = 1

# read .h5 file
def parse_file(f):
    print(f.decode('utf-8'))
    with h5py.File(f.decode(""utf-8""), 'r') as fi:
        X = fi['X'][:].reshape(1000, 100, 100, 1)
        y = fi['y'][:].reshape(1000, 100, 100, 1)
        print(threading.get_ident())  # to see the thread id
        return X, y

# py_func wrapper
def parse_file_tf(filename):
    return tf.py_func(parse_file, [filename], [tf.float32, tf.float32])

# input pipeline
files = tf.data.Dataset.list_files('./test_*.h5')
dataset = files.map(parse_file_tf, num_parallel_calls=num_cores)
dataset = dataset.batch(batch_size).shuffle(shuffle_size).prefetch(10)
it = dataset.make_initializable_iterator()
iter_init_op = it.initializer
X_it, y_it = it.get_next()

# simplest model
with tf.name_scope(""Conv1""):
    W = tf.get_variable(""W"", shape=[3, 3, 1, 1],
                         initializer=tf.contrib.layers.xavier_initializer())
    b = tf.get_variable(""b"", shape=[1], initializer=tf.contrib.layers.xavier_initializer())
    layer1 = tf.nn.conv2d(X_it, W, strides=[1, 1, 1, 1], padding='SAME') + b
    out = tf.nn.relu(layer1)

loss = tf.reduce_mean(tf.losses.mean_squared_error(labels=y_it, predictions=out))
train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)

# session
sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess.run(iter_init_op)
sess.run([train_op])
sess.close()
```
Thanks for your help
"
27244,TensorFlow estimator train_and_evaluate loss is None model does not train when reading data from s3 ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
Anaconda
- TensorFlow version (use command below):
1.12
- Python version:
Python 3.6.8 :: Anaconda, Inc.

- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
i have build tensorflow distribution code using estimator api.And it worked well when reading data from local using train_and_evaluate.But if data is located in s3,it just checkpoints at step 0, the loss being None and moves to the evaluate phase. I'm not sure why this is happening, and any suggestions about what to look for would be great 
**Describe the expected behavior**
it should training normally when reading data from s3
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
train_input = lambda: fetch_train_data(FLAGS.train_data, FLAGS.train_epoch,batch_size=FLAGS.train_batch_size,is_eval=False,shuffle=True)
    eval_input  = lambda: fetch_eval_data (FLAGS.eval_data,  FLAGS.eval_epoch, batch_size=FLAGS.eval_batch_size, is_eval=True, shuffle=True)

    train_spec = tf.estimator.TrainSpec(train_input,max_steps=9223372036854775807)
    eval_spec = tf.estimator.EvalSpec(eval_input,steps=1)
    tf.estimator.train_and_evaluate(model, train_spec, eval_spec)
```
```
def fetch_train_data(filename, epochs=1, batch_size=256,is_eval=False,shuffle=False):
    print (""fetch_train_data"");
    blocks = get_blocks()
    #files = __list_files(filename)

    def chunks(arr, m):
      n = int(math.floor(len(arr) / float(m)))
      return [arr[i:i + n] for i in range(0, len(arr), n)]
    print (""is_eval is %d""%(is_eval))
    data_file = tf_record_pattern(filename, is_eval)
    #random.shuffle(data_file)
    num_shards = FLAGS.gpu_nums
    if FLAGS.gpu_nums!=0 and not is_eval:
        data_file = chunks(data_file, num_shards)

    for file in data_file:
        print (file)

    sys.stdout.flush()
    ds = [[] for i in range(num_shards)]
    feature_shards = [[] for i in range(num_shards)]
    label_shards = [[] for i in range(num_shards)]
    for i in range(num_shards):
      if not is_eval:
        ds[i] = tf.data.TFRecordDataset.list_files(data_file[i])
        ds[i] = ds[i].interleave(lambda filename: tf.data.TFRecordDataset(filename, buffer_size = 20*1024*1024), cycle_length = 20)
      else:
        ds[i] = tf.data.TFRecordDataset(data_file[i], buffer_size = 20*1024*1024)
      ds[i] = ds[i].prefetch(buffer_size=batch_size)
      if shuffle:
          ds[i] = ds[i].shuffle(buffer_size = 10000)
      if FLAGS.sync == 0:
        ds[i] = ds[i].repeat(epochs)
      else:
        ds[i] = ds[i].repeat()

      ds[i] = ds[i].batch(batch_size).map(__parse_ins(blocks), num_parallel_calls = 20)
      ds[i] = ds[i].prefetch(FLAGS.gpu_nums)
      feature_shards[i], label_shards[i] = ds[i].make_one_shot_iterator().get_next()
    return feature_shards, label_shards

```

"
27242,:gather_string failed (Exit 127),"Hello all, 

I'm having issues installing tensorflow on my windows system using bazel. If there is any other alternative or fix please let me know

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Build 17134
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Yes
- **TensorFlow version (use command below)**: Latest version (I'm building it off current master, can't see which version it is)
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: 0.23.2
- **GCC/Compiler version (if compiling from source)**: Using VS2017 with v140 toolkit
- **CUDA/cuDNN version**: CUDA; 10.1.105 cudnn:7.5.0
- **GPU model and memory**: GTX 1060 6GB
- **Exact command to reproduce**:
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
python ./configure.py
bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings //tensorflow/tools/pip_package:build_pip_package


### Describe the problem
HItting this error
ERROR: C:/users/darshan/tensorflow/tensorflow/lite/python/testdata/BUILD:35:1: Executing genrule //tensorflow/lite/python/testdata:gather_string failed (Exit 127): bash.exe failed: error executing command
### Source code / logs
INFO: From Linking tensorflow/lite/toco/toco:
LINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/lib64'; ignored
LINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64'; ignored
LINK : warning LNK4044: unrecognized option '/ldl'; ignored
LINK : warning LNK4044: unrecognized option '/ldl'; ignored
LINK : warning LNK4044: unrecognized option '/ldl'; ignored
   Creating library bazel-out/x64_windows-opt/bin/tensorflow/lite/toco/toco.lib and object bazel-out/x64_windows-opt/bin/tensorflow/lite/toco/toco.exp
libcudnn_plugin.lo(cuda_dnn.o) : warning LNK4217: locally defined symbol ?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11MAEBV?$DeviceMemory@M@2@H2HMPEAV52@H@Z (public: class stream_executor::Stream & __cdecl stream_executor::Stream::ThenBlasGemm(enum stream_executor::blas::Transpose,enum stream_executor::blas::Transpose,unsigned __int64,unsigned __int64,unsigned __int64,float,class stream_executor::DeviceMemory<float> const &,int,class stream_executor::DeviceMemory<float> const &,int,float,class stream_executor::DeviceMemory<float> *,int)) imported in function ""public: virtual bool __cdecl stream_executor::gpu::CudnnSupport::DoMatMul(class stream_executor::Stream *,class stream_executor::DeviceMemory<float> const &,class stream_executor::DeviceMemory<float> const &,class stream_executor::dnn::BatchDescriptor const &,class stream_executor::dnn::BatchDescriptor const &,class stream_executor::DeviceMemory<float> *)"" (?DoMatMul@CudnnSupport@gpu@stream_executor@@UEAA_NPEAVStream@3@AEBV?$DeviceMemory@M@3@1AEBVBatchDescriptor@dnn@3@2PEAV53@@Z)
ERROR: C:/users/darshan/tensorflow/tensorflow/lite/python/testdata/BUILD:35:1: Executing genrule //tensorflow/lite/python/testdata:gather_string failed (Exit 127): bash.exe failed: error executing command
  cd C:/users/darshan/_bazel_darshan/jojkqojs/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET CUDNN_INSTALL_PATH=C:/Program Files/cuda
    SET PATH=D:\Programs\msys2\usr\bin;D:\Programs\msys2\bin;D:\Programs\Python Installed\Scripts\;D:\Programs\Python Installed\;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\libnvvp;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Strawberry\c\bin;C:\Strawberry\perl\site\bin;C:\Strawberry\perl\bin;C:\Program Files (x86)\GnuWin32\bin;C:\Program Files\Git\cmd;D:\Software\MPC;C:\Program Files\dotnet\;C:\Program Files\Microsoft SQL Server\130\Tools\Binn\;C:\Program Files\Java\jre1.8.0_181\bin;C:\Program Files\PuTTY\;C:\Program Files (x86)\Microsoft SQL Server\Client SDK\ODBC\130\Tools\Binn\;C:\Program Files (x86)\Microsoft SQL Server\140\Tools\Binn\;C:\Program Files (x86)\Microsoft SQL Server\140\DTS\Binn\;C:\Program Files (x86)\Microsoft SQL Server\140\Tools\Binn\ManagementStudio\;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;C:\Program Files\NVIDIA Corporation\Nsight Compute 2019.1\;C:\Users\Darshan\AppData\Local\Microsoft\WindowsApps;C:\Users\Darshan\AppData\Local\Programs\Microsoft VS Code\bin;C:\Users\Darshan\AppData\Roaming\Dashlane\6.5.0.12978\bin\Firefox_Extension\{442718d9-475e-452a-b3e1-fb1ee16b8e9f}\components;C:\Users\Darshan\AppData\Roaming\Dashlane\6.5.0.12978\ucrt;C:\Users\Darshan\AppData\Roaming\Dashlane\6.5.0.12978\bin\Qt;C:\Users\Darshan\AppData\Roaming\Dashlane\6.5.0.12978\ucrt;C:\Users\Darshan\AppData\Roaming\Dashlane\6.5.0.12978\bin\Ssl;C:\Users\Darshan\AppData\Local\GitHubDesktop\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\extras\CUPTI\lib64;D:\Programs\Python Installed;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\tools;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\lib\x64;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\include;D:\Programs;C:\Program Files\Java\jdk1.8.0_201;D:\Programs\msys2\usr\bin;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC;
    SET PYTHON_BIN_PATH=D:/Programs/Python Installed/python.exe
    SET PYTHON_LIB_PATH=D:/Programs/Python Installed/lib/site-packages
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_VERSION=10.1
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
  D:/Programs/msys2/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/tensorflow/lite/toco/toco --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_file=tensorflow/lite/python/testdata/gather.pbtxt --output_file=bazel-out/x64_windows-opt/genfiles/tensorflow/lite/python/testdata/gather_string.tflite --input_arrays=input,indices --output_arrays=output
Execution platform: @bazel_tools//platforms:host_platform
C:/users/darshan/_bazel_darshan/jojkqojs/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/lite/toco/toco: error while loading shared libraries: cudnn64_7.dll: cannot open shared object file: No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 126.000s, Critical Path: 24.69s
INFO: 32 processes: 32 local.
FAILED: Build did NOT complete successfully


Previously I tried another combination of tensorflow-1.12 and bazel-0.15, everything else was same. But I kept running into another which I looked around a lot and there were threads about it but no solution. So I started all over with latest version. (My protobuf version is 5.6.0, saw in another thread where they downgraded it to 5.2.1. I don't know if it will really help me here since I'm configuring the latest version to work on my system)

Any and all help is really appreciated!!



--
Thanks
Best Regards"
27241,"Exception: TensorFlow Flex runtime does not support some operators in the model: Merge, Switch`","Dear friends, I recently tried to convert MobileFaceNet_TF to tensorflow lite format, but the error is displayed as ""Exception: TensorFlow Flex runtime does not support some operators in the model: Merge, Switch`, this version of MobileFaceNet_TF is implemented by tf.slim There are a lot of Merge and Switch. So what can I do to fix this error?

**System information**
- OS Platform and Distribution (MacOS 10.13.6):
- TensorFlow installed from (pip install):
- TensorFlow version (tensorflow 1.14.1-dev20190326):


**Provide the text output from tflite_convert**
 
First try with `--target_ops=TFLITE_BUILTINS,SELECT_TF_OPS`
```
(python3.6) ROYALLI-MB1:SFBox_FR royalli$ tflite_convert --output_file=converted_model.tflite  --input_arrays=""input"" --output_arrays=""embeddings"" --graph_def_file=""/Users/royalli/Progrem/gitlab/FR/SFBox_FR/arch/pretrained_model/MobileFaceNet_9925_9680.pb"" --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS
2019-03-28 18:56:43.571468: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-28 18:56:44.120211: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-03-28 18:56:44.120352: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2019-03-28 18:56:44.997732: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:657] Optimization results for grappler item: graph_to_optimize
2019-03-28 18:56:44.997778: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:659]   model_pruner: Graph size after: 2524 nodes (-397), 4369 edges (-397), time = 41.658ms.
2019-03-28 18:56:44.997785: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:659]   implementation_selector: Graph size after: 2524 nodes (0), 4369 edges (0), time = 7.838ms.
2019-03-28 18:56:44.997790: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:659]   function_optimizer: Graph size after: 2524 nodes (0), 4369 edges (0), time = 7.122ms.
2019-03-28 18:56:44.997795: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:659]   constant folding: Graph size after: 2458 nodes (-66), 4303 edges (-66), time = 218.802ms.
2019-03-28 18:56:44.997799: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:659]   shape_optimizer: Graph size after: 2458 nodes (0), 4303 edges (0), time = 17.782ms.
2019-03-28 18:56:44.997884: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:659]   arithmetic_optimizer: Graph size after: 1836 nodes (-622), 3162 edges (-1141), time = 109.467ms.
2019-03-28 18:56:44.997895: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:659]   loop_optimizer: Graph size after: 1836 nodes (0), 3162 edges (0), time = 16.807ms.
2019-03-28 18:56:44.997901: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:659]   dependency_optimizer: Graph size after: 1628 nodes (-208), 2798 edges (-364), time = 42.48ms.
2019-03-28 18:56:44.997905: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:659]   memory_optimizer: Graph size after: 1628 nodes (0), 2798 edges (0), time = 161.64ms.
2019-03-28 18:56:44.997910: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:659]   model_pruner: Graph size after: 1628 nodes (0), 2798 edges (0), time = 16.13ms.
2019-03-28 18:56:44.997915: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:659]   implementation_selector: Graph size after: 1628 nodes (0), 2798 edges (0), time = 6.881ms.
2019-03-28 18:56:44.997919: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:659]   function_optimizer: Graph size after: 1628 nodes (0), 2798 edges (0), time = 5.458ms.
2019-03-28 18:56:44.997924: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:659]   constant folding: Graph size after: 1628 nodes (0), 2798 edges (0), time = 56.981ms.
2019-03-28 18:56:44.997928: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:659]   shape_optimizer: Graph size after: 1628 nodes (0), 2798 edges (0), time = 8.543ms.
2019-03-28 18:56:44.997932: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:659]   arithmetic_optimizer: Graph size after: 1628 nodes (0), 2798 edges (0), time = 50.383ms.
2019-03-28 18:56:44.997937: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:659]   dependency_optimizer: Graph size after: 1628 nodes (0), 2798 edges (0), time = 29.484ms.
Traceback (most recent call last):
  File ""/Users/royalli/software/anaconda3/envs/python3.6/bin/tflite_convert"", line 10, in <module>
    sys.exit(main())
  File ""/Users/royalli/software/anaconda3/envs/python3.6/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py"", line 448, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/Users/royalli/software/anaconda3/envs/python3.6/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/Users/royalli/software/anaconda3/envs/python3.6/lib/python3.6/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/Users/royalli/software/anaconda3/envs/python3.6/lib/python3.6/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/Users/royalli/software/anaconda3/envs/python3.6/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py"", line 444, in run_main
    _convert_model(tflite_flags)
  File ""/Users/royalli/software/anaconda3/envs/python3.6/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py"", line 192, in _convert_model
    output_data = converter.convert()
  File ""/Users/royalli/software/anaconda3/envs/python3.6/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 739, in convert
    **converter_kwargs)
  File ""/Users/royalli/software/anaconda3/envs/python3.6/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 410, in toco_convert_impl
    input_data.SerializeToString())
  File ""/Users/royalli/software/anaconda3/envs/python3.6/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 176, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-03-28 18:56:46.569963: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1591 operators, 2721 arrays (0 quantized)
2019-03-28 18:56:46.636818: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 1172 operators, 2197 arrays (0 quantized)
2019-03-28 18:56:46.702441: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1172 operators, 2197 arrays (0 quantized)
2019-03-28 18:56:46.703022: F tensorflow/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:98] Check failed: other_op->type == OperatorType::kMerge Found Mul as non-selected output from Switch, but only Merge supported.
Fatal Python error: Aborted

Current thread 0x00007fff9cadd380 (most recent call first):
  File ""/Users/royalli/software/anaconda3/envs/python3.6/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33 in execute
  File ""/Users/royalli/software/anaconda3/envs/python3.6/lib/python3.6/site-packages/absl/app.py"", line 251 in _run_main
  File ""/Users/royalli/software/anaconda3/envs/python3.6/lib/python3.6/site-packages/absl/app.py"", line 300 in run
  File ""/Users/royalli/software/anaconda3/envs/python3.6/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40 in run
  File ""/Users/royalli/software/anaconda3/envs/python3.6/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59 in main
  File ""/Users/royalli/software/anaconda3/envs/python3.6/bin/toco_from_protos"", line 10 in <module>
```
Second try with `--target_ops=SELECT_TF_OPS`

```
019-03-27 18:16:41.138180: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.138191: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.138202: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Relu
2019-03-27 18:16:41.138212: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Abs
2019-03-27 18:16:41.138225: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Sub
2019-03-27 18:16:41.138235: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Mul
2019-03-27 18:16:41.138245: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.138258: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Conv2D
2019-03-27 18:16:41.138275: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.138288: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.138305: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.138323: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.138335: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.138347: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.138358: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.138369: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.138382: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.138393: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.138403: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.138417: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.138434: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.138446: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.138459: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Conv2D
2019-03-27 18:16:41.138474: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.138490: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.138508: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.138526: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.138538: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.138550: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.138561: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.138572: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.138583: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.138597: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.138607: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.138619: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.138633: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.138645: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Relu
2019-03-27 18:16:41.138655: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Abs
2019-03-27 18:16:41.138665: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Sub
2019-03-27 18:16:41.138676: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Mul
2019-03-27 18:16:41.138686: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.138701: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: DepthwiseConv2dNative
2019-03-27 18:16:41.138715: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.138728: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.138748: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.138763: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.138775: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.138787: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.138801: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.138812: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.138823: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.138834: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.138846: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.138858: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.138870: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.138881: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Relu
2019-03-27 18:16:41.138891: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Abs
2019-03-27 18:16:41.138904: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Sub
2019-03-27 18:16:41.138915: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Mul
2019-03-27 18:16:41.138925: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.138938: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Conv2D
2019-03-27 18:16:41.138955: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.138968: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.138984: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.139002: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.139014: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.139026: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.139037: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.139051: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.139062: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.139072: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.139082: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.139097: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.139109: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.139121: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.139133: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Conv2D
2019-03-27 18:16:41.139150: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.139164: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.139181: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.139199: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.139211: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.139222: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.139233: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.139245: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.139258: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.139269: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.139279: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.139291: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.139304: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.139316: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Relu
2019-03-27 18:16:41.139326: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Abs
2019-03-27 18:16:41.139336: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Sub
2019-03-27 18:16:41.139347: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Mul
2019-03-27 18:16:41.139358: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.139373: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: DepthwiseConv2dNative
2019-03-27 18:16:41.139387: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.139400: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.139419: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.139434: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.139446: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.139461: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.139472: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.139483: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.139494: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.139505: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.139517: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.139529: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.139540: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.139552: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Relu
2019-03-27 18:16:41.139562: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Abs
2019-03-27 18:16:41.139575: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Sub
2019-03-27 18:16:41.139586: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Mul
2019-03-27 18:16:41.139598: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.139614: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Conv2D
2019-03-27 18:16:41.139636: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.139653: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.139674: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.139696: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.139708: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.139720: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.139732: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.139745: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.139757: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.139767: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.139777: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.139792: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.139804: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.139816: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.139829: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Conv2D
2019-03-27 18:16:41.139846: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.139860: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.139877: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.139895: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.139907: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.139919: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.139931: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.139941: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.139955: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.139965: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.139976: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.139987: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.140001: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.140013: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Relu
2019-03-27 18:16:41.140023: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Abs
2019-03-27 18:16:41.140035: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Sub
2019-03-27 18:16:41.140047: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Mul
2019-03-27 18:16:41.140058: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.140075: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: DepthwiseConv2dNative
2019-03-27 18:16:41.140091: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.140107: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.140133: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.140361: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.140430: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.140489: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.140510: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.140526: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.140541: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.140555: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.140568: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.140582: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.140597: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.140613: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Relu
2019-03-27 18:16:41.140626: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Abs
2019-03-27 18:16:41.140639: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Sub
2019-03-27 18:16:41.140652: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Mul
2019-03-27 18:16:41.140663: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.140680: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Conv2D
2019-03-27 18:16:41.140701: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.140720: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.140746: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.140769: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.140786: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.140802: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.140817: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.140831: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.140846: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.140862: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.140875: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.140889: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.140903: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.140917: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.140934: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Conv2D
2019-03-27 18:16:41.140952: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.140964: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.140980: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.140993: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.141004: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.141013: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.141023: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.141031: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.141040: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.141049: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.141057: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.141066: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.141076: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.141085: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Relu
2019-03-27 18:16:41.141094: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Abs
2019-03-27 18:16:41.141102: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Sub
2019-03-27 18:16:41.141113: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Mul
2019-03-27 18:16:41.141126: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.141141: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: DepthwiseConv2dNative
2019-03-27 18:16:41.141153: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.141167: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.141209: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.141227: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.141239: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.141251: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.141263: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.141274: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.141285: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.141296: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.141306: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.141318: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.141330: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.141341: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Relu
2019-03-27 18:16:41.141352: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Abs
2019-03-27 18:16:41.141362: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Sub
2019-03-27 18:16:41.141373: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Mul
2019-03-27 18:16:41.141384: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.141398: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Conv2D
2019-03-27 18:16:41.141413: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.141426: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.141443: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.141458: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.141471: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.141484: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.141496: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.141506: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.141517: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.141528: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.141549: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.141568: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.141582: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.141613: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.141629: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Conv2D
2019-03-27 18:16:41.141682: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.141704: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.141723: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.141742: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.141754: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.141766: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.141777: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.141788: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.141800: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.141817: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.141829: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.141842: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.141860: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.141875: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Relu
2019-03-27 18:16:41.141887: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Abs
2019-03-27 18:16:41.141900: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Sub
2019-03-27 18:16:41.141914: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Mul
2019-03-27 18:16:41.141926: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.141942: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: DepthwiseConv2dNative
2019-03-27 18:16:41.141957: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.141970: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.142033: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.142049: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.142059: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.142068: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.142077: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.142085: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.142094: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.142102: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.142110: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.142119: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.142128: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.142136: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Relu
2019-03-27 18:16:41.142143: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Abs
2019-03-27 18:16:41.142151: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Sub
2019-03-27 18:16:41.142160: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Mul
2019-03-27 18:16:41.142168: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.142179: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Conv2D
2019-03-27 18:16:41.142191: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.142201: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.142215: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.142261: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.142292: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.142312: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.142323: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.142332: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.142357: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.142367: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.142375: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.142384: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.142393: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.142405: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Conv2D
2019-03-27 18:16:41.142418: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.142429: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.142445: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.142457: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.142466: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.142476: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.142487: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.142496: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.142518: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.142541: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.142564: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.142579: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.142592: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.142604: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Relu
2019-03-27 18:16:41.142614: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Abs
2019-03-27 18:16:41.142625: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Sub
2019-03-27 18:16:41.142636: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Mul
2019-03-27 18:16:41.142647: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.142660: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: DepthwiseConv2dNative
2019-03-27 18:16:41.142685: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.142709: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.142740: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.142756: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.142767: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.142777: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.142785: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.142794: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.142802: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.142810: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.142817: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.142826: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.142835: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.142844: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Relu
2019-03-27 18:16:41.142851: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Abs
2019-03-27 18:16:41.142859: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Sub
2019-03-27 18:16:41.142868: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Mul
2019-03-27 18:16:41.142876: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.142887: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Conv2D
2019-03-27 18:16:41.142899: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.142910: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.142924: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.142937: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.142947: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.142956: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.142965: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.142973: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.142981: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.142989: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.142996: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.143006: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.143015: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143024: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.143035: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Conv2D
2019-03-27 18:16:41.143046: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143056: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.143071: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.143084: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143093: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143102: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143110: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143117: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143126: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143134: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.143141: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.143150: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.143158: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143167: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Relu
2019-03-27 18:16:41.143174: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Abs
2019-03-27 18:16:41.143181: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Sub
2019-03-27 18:16:41.143190: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Mul
2019-03-27 18:16:41.143198: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.143208: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: DepthwiseConv2dNative
2019-03-27 18:16:41.143219: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143229: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.143243: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.143264: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143274: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143283: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143292: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143300: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143308: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143316: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.143323: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.143331: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.143340: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143349: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Relu
2019-03-27 18:16:41.143356: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Abs
2019-03-27 18:16:41.143363: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Sub
2019-03-27 18:16:41.143371: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Mul
2019-03-27 18:16:41.143379: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.143390: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Conv2D
2019-03-27 18:16:41.143403: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143414: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.143430: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.143449: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143463: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143477: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143487: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143496: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143505: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143516: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.143524: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.143534: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.143544: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143554: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.143565: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Conv2D
2019-03-27 18:16:41.143578: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143589: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.143604: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.143617: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143628: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143638: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143647: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143656: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143665: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143674: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.143682: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.143691: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.143701: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143710: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Relu
2019-03-27 18:16:41.143719: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Abs
2019-03-27 18:16:41.143727: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Sub
2019-03-27 18:16:41.143736: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Mul
2019-03-27 18:16:41.143745: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Add
2019-03-27 18:16:41.143755: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: DepthwiseConv2dNative
2019-03-27 18:16:41.143766: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143777: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.143793: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.143806: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143816: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143826: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143835: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143844: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143853: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143861: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.143869: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.143879: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.143888: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143901: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Conv2D
2019-03-27 18:16:41.143913: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143924: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.143939: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.143953: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143963: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143973: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.143982: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.143991: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.144000: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.144009: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.144017: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.144027: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.144037: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.144049: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Conv2D
2019-03-27 18:16:41.144061: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.144072: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.144087: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNorm
2019-03-27 18:16:41.144100: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.144110: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.144120: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.144129: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.144138: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.144147: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Switch
2019-03-27 18:16:41.144156: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.144164: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.144173: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Identity
2019-03-27 18:16:41.144183: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Merge
2019-03-27 18:16:41.144193: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Squeeze
2019-03-27 18:16:41.144202: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Square
2019-03-27 18:16:41.144211: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Sum
2019-03-27 18:16:41.144221: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Maximum
2019-03-27 18:16:41.144230: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Rsqrt
2019-03-27 18:16:41.144238: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Mul
2019-03-27 18:16:41.191937: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1331 operators, 2669 arrays (0 quantized)
2019-03-27 18:16:41.249912: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 912 operators, 2041 arrays (0 quantized)
2019-03-27 18:16:41.307836: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 912 operators, 2041 arrays (0 quantized)
2019-03-27 18:16:41.356943: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 912 operators, 2041 arrays (0 quantized)
2019-03-27 18:16:41.417318: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 912 operators, 2041 arrays (0 quantized)
2019-03-27 18:16:41.467803: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 912 operators, 2041 arrays (0 quantized)
2019-03-27 18:16:41.527702: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.
2019-03-27 18:16:41.537430: I tensorflow/lite/toco/toco_tooling.cc:433] Estimated count of arithmetic ops: 0 billion (note that a multiply-add is counted as 2 ops).
2019-03-27 18:16:41.543176: E tensorflow/lite/toco/toco_tooling.cc:456] Some of the operators in the model are not supported by TensorFlow Flex runtime: Merge, Switch.
Traceback (most recent call last):
  File ""/Users/royalli/software/anaconda3/envs/python3.6/bin/toco_from_protos"", line 10, in <module>
    sys.exit(main())
  File ""/Users/royalli/software/anaconda3/envs/python3.6/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/Users/royalli/software/anaconda3/envs/python3.6/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/Users/royalli/software/anaconda3/envs/python3.6/lib/python3.6/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/Users/royalli/software/anaconda3/envs/python3.6/lib/python3.6/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/Users/royalli/software/anaconda3/envs/python3.6/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)
Exception: Some of the operators in the model are not supported by TensorFlow Flex runtime: Merge, Switch.
```

## pretrain model link and script link
Also, please include a link to a GraphDef or the model if possible.
link of pretrain model   [here](
https://github.com/sirius-ai/MobileFaceNet_TF/blob/master/arch/pretrained_model/MobileFaceNet_9925_9680.pb)

the model train.py is   [here](https://github.com/sirius-ai/MobileFaceNet_TF/blob/master/train_nets.py)

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27240,TFLite GPU delegate returns only zeros for part of the outputs,"I'm testing the GPU delegate with a pose detection model (https://github.com/ildoonet/tf-pose-estimation modified to run heatmap maxpooling as part of the graph). This model works with no problems when using the CPU. When I enable the GPU delegate, the inference time improves by about 4x, but one of the outputs contains only zeros, whereas the other outputs seem to contain reasonable values (`heatmap` is zeros, `part affinity map` and `maxpooled heatmap` are OK). I also get zeros when I initialize the output array using other values, indicating that the output is actually written too, just with the wrong values.

The zeroed output (`heatmap`) is a direct predecessor of another (correctly produced) output (`maxpooled heatmap`).

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
_Yes, app using TFLite_

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
_Tensorflow Lite on Android_

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
_several, e.g. Sony Xperia XZ1 compact_ 

- TensorFlow installed from (source or binary):
_Binary_ 

- TensorFlow version (use command below):
````
org.tensorflow:tensorflow-android:1.12.0
org.tensorflow:tensorflow-lite:0.0.1-gpu-experimental
````

**Describe the current behavior**
Output has correct values without GPU delegate, but contains only zeros (0.0f) when using GPU delegate

**Describe the expected behavior**
Similar outputs in both cases

**Code to reproduce the issue**
See repository https://github.com/gerhardgossen/GpuDelegateTest (the TFLite part is in Application/src/main/java/com/example/android/camera2basic/TfliteInferer.java). When running the app, it prints the ratio of zero values in each output to the Android log. This value is != 1.0 for all outputs without GPU delegate, but == 1.0 with GPU delegate.


**Other info / logs**

Model description as produced by tensorflow/lite/tools/visualize.py: [pose.zip](https://github.com/tensorflow/tensorflow/files/3017522/pose.zip)"
27237,Tutorial Setup Code Example error,"
**System information**
- TensorFlow version: latest
- Doc Link: https://www.tensorflow.org/js/tutorials/setup

https://www.tensorflow.org/js/tutorials/setup
**Describe the documentation issue**

 ```javascript
const tf = require('@tensorflow/tfjs');

// Optional Load the binding:
// Use '@tensorflow/tfjs-node-gpu' if running with GPU.
require('@tensorflow/tfjs-node');

// Train a simple model:
const model = tf.sequential();
model.add(tf.layers.dense({units: 100, activation: 'relu', inputShape: [10]}));
model.add(tf.layers.dense({units: 1, activation: 'linear'}));
model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});

const xs = tf.randomNormal([100, 10]);
const ys = tf.randomNormal([100, 1]);

model.fit(xs, ys, {
  epochs: 100,
  callbacks: {
    onEpochEnd: (epoch, log) => console.log(`Epoch ${epoch}: loss = ${log.loss}`);   <--- wrong semicolumn
  }
});
  ```
"
27234,It doesn't workout~,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
It couldn't work.
the error information is below:
C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.




---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\Anaconda3\envs\tensorflow\lib\imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

~\Anaconda3\envs\tensorflow\lib\imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: DLL load failed:  (DLL) 

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-8-31fa606241a9> in <module>()
----> 1 from keras.models import Sequential
      2 from keras.layers import Dense,Dropout

~\Anaconda3\envs\tensorflow\lib\site-packages\keras\__init__.py in <module>()
      1 from __future__ import absolute_import
      2 
----> 3 from . import utils
      4 from . import activations
      5 from . import applications

~\Anaconda3\envs\tensorflow\lib\site-packages\keras\utils\__init__.py in <module>()
      4 from . import data_utils
      5 from . import io_utils
----> 6 from . import conv_utils
      7 
      8 # Globally-importable utils.

~\Anaconda3\envs\tensorflow\lib\site-packages\keras\utils\conv_utils.py in <module>()
      7 from six.moves import range
      8 import numpy as np
----> 9 from .. import backend as K
     10 
     11 

~\Anaconda3\envs\tensorflow\lib\site-packages\keras\backend\__init__.py in <module>()
     87 elif _BACKEND == 'tensorflow':
     88     sys.stderr.write('Using TensorFlow backend.\n')
---> 89     from .tensorflow_backend import *
     90 else:
     91     # Try and load external backend.

~\Anaconda3\envs\tensorflow\lib\site-packages\keras\backend\tensorflow_backend.py in <module>()
      3 from __future__ import print_function
      4 
----> 5 import tensorflow as tf
      6 from tensorflow.python.framework import ops as tf_ops
      7 from tensorflow.python.training import moving_averages

~\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 from tensorflow._api.v1 import app

~\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

~\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed:  (DLL) 


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

**Describe the expected behavior**
Sorry I'm not good neither in technology nor in English. All I want is the code in jupyter notebook can work. Please help me~~~thank you~~~ 
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27231,"tf1.13 can be installed with cuda8.0, cudnn5.1 but tf1.6 cannot","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27229,Convert Unsupported Operations,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS High Sierra 10.13.6
- TensorFlow installed from (source or binary): via pip
- TensorFlow version (or github SHA if from source): tensorflow==1.13.1


**Provide the text output from tflite_convert**

```
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-03-27 17:40:24.648521: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: IteratorV2
2019-03-27 17:40:24.656267: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-03-27 17:40:24.656283: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: IteratorGetNext
2019-03-27 17:40:24.656301: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-03-27 17:40:24.656376: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-03-27 17:40:24.656460: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: SoftmaxCrossEntropyWithLogits
2019-03-27 17:40:24.656709: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 21 operators, 35 arrays (0 quantized)
2019-03-27 17:40:24.656884: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 21 operators, 35 arrays (0 quantized)
2019-03-27 17:40:24.657083: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 13 operators, 24 arrays (0 quantized)
2019-03-27 17:40:24.657094: F tensorflow/lite/toco/tooling_util.cc:897] Check failed: GetOpWithInput(model, input_array.name()) Specified input array ""weights"" is not consumed by any op in this graph. Is it a typo? To silence this message, pass this flag:  allow_nonexistent_arrays
```

Also, please include a link to a GraphDef or the model if possible.

[model.zip](https://github.com/tensorflow/tensorflow/files/3016307/model.zip)

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
train_data = tf.data.Dataset.from_tensor_slices(train)
train_data = train_data.shuffle(10000)
train_data = train_data.batch(batch_size)

iterator = tf.data.Iterator.from_structure(train_data.output_types,
                                           train_data.output_shapes)
img, label = iterator.get_next()
train_init = iterator.make_initializer(train_data) # initializer for train_data
```

"
27225,Tensorflow Installation for C does not work for Ubuntu 18.04 CUDA 10,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12
- Python version: N/A. Downloading C library from [Install TensorFlow for C](https://www.tensorflow.org/install/lang_c)
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: RTX 2080 Ti


**Problem Description**
Trying to use the pre-built C library for my project. Followed through the step in [Install Tensorflow for C](https://www.tensorflow.org/install/lang_c). However, when I tried to compile the hello_tf.c, it prompted a bunch of errors such as 
```
/opt/tensorflow/lib/libtensorflow.so: undefined reference to tensorflow::Status tensorflow::FunctionLibraryDefinition::GetAttr<bool>(tensorflow::Node const&, std::string const&, bool*) const'.
```
At the very start it gave a warning stating that
```
/usr/bin/ld: warning: libcublas.so.9.0, needed by /opt/tensorflow/lib/libtensorflow.so, not found (try using -rpath or -rpath-link)
``` 
so I believe it is because this pre-built is only for CUDA 9.0 but I have 10.0 installed on my machine. I cannot find any officially provided 10.0 build, but here is a workaround. 

**Workaround**
Basically after you built TF from source, there is a pre-built binary for C API in bazel-bin directory (inspired by [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md)). **DO NOT** follow the steps in the previous link to test and build it again. I have done that and it broke my old build so my Tensorflow Go didn't work anymore and I have to build again. I guess the reason was that it had a default configuration which didn't work with my system, while the formal build from the source steps let you configure your CUDA version, etc. Anyway, you can do the following:
 
1. Build the Tensorflow from source( follow the instructions [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/README.md))
2. The C API that is suitable to your machine is generated in `bazel-bin/tensorflow/tools/lib_package/libtensorflow.tar.gz`. Use this instead of downloading from TF.
3. Follow the remaining steps in [Install Tensorflow for C](https://www.tensorflow.org/install/lang_c)

**Proposal to fix**
While this is not a problem for me anymore, I just want to leave it here so someone won't have to go through this as I did. Also, if CUDA version is the actually reason that failed my installation, I hope someone in charge of this adding the CUDA version into the filename to notify people.
"
27223,No way to use string type with Lite C APIs.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10, Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: a26413ef0a179dd93b5228b031de83dce97a8684
- Python version: 3.7 but I'm writing TfLite with C
- Installed using virtualenv? pip? conda?: see above
- Bazel version (if compiling from source): see above
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the problem**

I'm trying to implement smartreply using go-tflite. Most of features can be implemented with C APIs. But no way to manipulate dynamic-buffer. Do you have plan to add APIs for dynamic-buffers?

**Provide the exact sequence of commands / steps that you executed before running into the problem**

No way to explain to reproduce.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


"
27222,"ragged batch_gather fails for ragged indices, non-ragged params","# System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes (see below)
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from: pip
- TensorFlow version: ('v1.13.1-0-g6612da8951', '1.13.1')
- Python version: 2.7.12
- CUDA/cuDNN version: 10.0/7
- GPU model and memory: quadro

# Current behaviour
Using `tf.batch_gather(params, indices)` with a `tf.Tensor` params and `tf.RaggedTensor` indices throws an uneccessary error.

```python
import tensorflow as tf
if not tf.executing_eagerly():
    try:
        tf.compat.v1.enable_eager_execution()
    except Exception:
        tf.enable_eager_execution()

params = tf.range(10, dtype=tf.int64)
indices = tf.RaggedTensor.from_row_lengths(
    tf.constant([2, 3, 1, 5, 0], dtype=tf.int64),
    tf.constant([3, 2], dtype=tf.int64))

out = tf.gather(params, indices)
print(out)
# <tf.RaggedTensor [[2, 3, 1], [5, 0]]>
# so far so good

batched_params = tf.expand_dims(params, axis=0)
batched_indices = tf.expand_dims(indices, axis=0)
batched_out = tf.batch_gather(batched_params, batched_indices)
# ValueError: batch shape from indices does not match params shape
```

# Expected Behaviour
`batch_gather` behaviour to correspond to `gather` being `map`ped across `params` and `indices` (see work-around below). If the use-case is sufficiently small that this isn't prioritised, an accurate error message would be preferable.

# Workaround
```python
def batch_gather_workaround(params, indices):
    if (
            isinstance(indices, tf.RaggedTensor) and
            not isinstance(params, tf.RaggedTensor)):
        shape = tf.shape(params, out_type=tf.int64)
        dims = tf.unstack(shape)
        batch_size = dims[0]
        stride = dims[1]
        offset = tf.range(batch_size, dtype=tf.int64)*stride
        offset = tf.reshape(offset, [batch_size] + [1]*(indices.shape.ndims-1))
        flat_params = tf.reshape(params, [batch_size*stride] + dims[2:])
        return tf.gather(flat_params, indices + offset)
    else:
        return tf.batch_gather(params, indices)


batched_out = batch_gather_workaround(batched_params, batched_indices)
# <tf.RaggedTensor [[[2, 3, 1], [5, 0]]]>
```"
27217,[TF 2.0 alpha] Keras Callbacks,"
**System information**
- TensorFlow version: 2.0.0-alpha0
- Doc Link:


**When creating a callback , if we need an accuracy threshold for training , previous TF versions have logs.get('acc') but in this version we need to use logs.get('accuracy') for it to work. There wasn't any documentation regarding this change**

"
27216,raspberry pi 3B+ BERT,"how to use BERT on tensorflow for raspberry pi?
"
27215,Commit to setup.py missing an import,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.13.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: building pip wheel
- Bazel version (if compiling from source): 0.24.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version:
- GPU model and memory:




When attempting to build pip the process fails with the following:

prompt:~/_path_/tensorflow$ ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
Wed 27 Mar 20:48:26 GMT 2019 : === Preparing sources in dir: /tmp/tmp.RsnPbFm3Zx
~/_path_/tensorflow ~/path/tensorflow
~/_path_/tensorflow
Wed 27 Mar 20:48:30 GMT 2019 : === Building wheel
Traceback (most recent call last):
  File ""setup.py"", line 276, in <module>
    Extension(
NameError: name 'Extension' is not defined


I ran:
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

then attempted to run:
./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
and got the above error.

Commit e5778e4d763493b0b3ec05a9f5658a04aed47a3f added
# Make setup aware this is an extension that cannot go into purelib.
ext_modules=[
        Extension(
            name='tensorflow',
            sources=[],
        )
    ],

but missed the import to ensure Extension was available.

I added from setuptools import Extension and it appears to be working (I haven't proceeded past that step yet though).

I'll update once I know if my build was actually successful (which may be tomorrow morning, GMT)"
27214,[TF2.0] Syntax highlighting in PyCharm,"Has anyone here had any success using tensorflow==2.0.0-alpha in pycharm? The new module organization is making it hard for PyCharm to infer namespaces correctly. 

For example, if I import as `tf.python.keras` syntax highlighting is fine and run time fails but if I import as `tf.keras` syntax highlight fails and run time works."
27209,TensorFlowLiteSwift performance problem with bitcode enabled on iOS,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, I converted the [SpeechCommands example](https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands/ios/SpeechCommands) to use the TensorFlowLiteSwift API instead of the C wrapper in the original example.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPad Mini 2 real device and iPhone 6s simulator
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): latest commit is 95e72f827f8aa0c352d6e1f47ffeec58fe4c9ec9 from March 22.
- Python version: N/A for this problem
- Bazel version (if compiling from source): 0.23.2
- GCC/Compiler version (if compiling from source):
$ gcc --version
Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/c++/4.2.1
Apple LLVM version 10.0.0 (clang-1000.11.45.5)
Target: x86_64-apple-darwin18.5.0

- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Compiling the TensorFlowLiteC_framework with bitcode support enabled slows down model inference on the sample model from ~72 ms (without bitcode support) to ~1200 ms (with bitcode support).

**Describe the expected behavior**
Would be nice if performance was approximately equivalent.

**Code to reproduce the issue**
Compile the TensorFlowLiteC_framework without bitcode support:
bazel build tensorflow/lite/experimental/c:TensorFlowLiteC_framework -c opt --ios_multi_cpus=x86_64,armv7,arm64
Then for comparison compile it with bitcode support:
bazel build tensorflow/lite/experimental/c:TensorFlowLiteC_framework -c fastbuild --ios_multi_cpus=x86_64,armv7,arm64 --apple_bitcode=embedded --copt=-fembed-bitcode

Use these with the experimental TensorFlowLiteSwift CocoaPod (not yet publicly available, see #25800 for reference) to perform a model inference on a real device or a simulator.  Regardless of debug/release build, if you use the framework with bitcode support in it then performance suffers greatly.

**Other info / logs**

If it would help to provide my Xcode project, I'm happy to provide it.  The Podfile would need to be modified to locate your own build CocoaPods because the TensorFlowLiteC pod uses the framework in question and it's not yet publicly available."
27207,TF2.0 distribution strategy hanging on keras,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.5.1804 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source 
 from [tf2.0 branch](https://github.com/tensorflow/tensorflow/tree/v2.0.0-alpha0)
- TensorFlow version (use command below): 2.0.0a0
- Python version: 3.5.2
- Bazel version (if compiling from source): 19.2
- GCC/Compiler version (if compiling from source): 6.3
- CUDA/cuDNN version: NA
- GPU model and memory: NA


**Describe the current behavior**
Trying to run simple example on cpu in distributed mode according to the documentation.  [tf2.0 documentation](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/guide/distribute_strategy.ipynb) 

If I run the code using only python py_file.py then it is running fine. 
If I run it by setting TF_CONFIG environment variable with only 2 different node or even a single node then it is hanging. 


It hangs after generating this information: 
2019-03-27 11:44:19.963538: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-03-27 11:44:19.976632: I tensorflow/core/common_runtime/process_util.cc:92] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
WARNING: Logging before flag parsing goes to stderr.
W0327 11:44:19.979773 47985184916416 deprecation.py:506] From /panfs/users/mdkamruz/anaconda3/envs/py352_otf2.0_source/lib/python3.5/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0327 11:44:20.015144 47985184916416 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an ""evaluator"" task exists in the cluster.
W0327 11:44:20.015259 47985184916416 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
2019-03-27 11:44:20.021537: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:3002}
2019-03-27 11:44:20.027697: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:359] Started server with target: grpc://localhost:3002
2019-03-27 11:44:20.027718: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:363] Server already started (target: grpc://localhost:3002)
W0327 11:44:20.095507 47985184916416 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an ""evaluator"" task exists in the cluster.
W0327 11:44:20.095634 47985184916416 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
W0327 11:44:20.097250 47985184916416 distributed_training_utils.py:933] ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.

**Describe the expected behavior**
It should run smoothly as described in the documentation. 

**Code to reproduce the issue**
Python code: 

```
dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100).shuffle(buffer_size=10000).batch(10)
# single node
ds_strategy=tf.distribute.MirroredStrategy()
# cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()
# multinode
# ds_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
with ds_strategy.scope():
    model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])
    model.compile(loss='mse', optimizer='sgd')
    model.fit(dataset,epochs=200)

```
TF_CONFIG environment variable: 
```
export TF_CONFIG='{
    ""cluster"": {
          ""worker"": [""IP_ADDR:3001""]
          },
    ""task"": {""type"": ""worker"", ""index"": 0}
}'
# for single node multi worker
export TF_CONFIG='{
    ""cluster"": {
          ""worker"": [""IP_ADDR:3001"",""IP_ADDR:3002""]
          },
    ""task"": {""type"": ""worker"", ""index"": 0}
}'
# for multinode 
# node 1 and similar for node 2
export TF_CONFIG='{
    ""cluster"": {
          ""worker"": [""IP_ADDR_1:3001"",""IP_ADDR_2:3002""]
          },
    ""task"": {""type"": ""worker"", ""index"": 0}
}'
```

**Other info / logs**
Running on cpu, machine don't have any gpu
"
27204,[TF2.0] How to build with Api v.2,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Buster
- TensorFlow version: v2.0.0-alpha0
- Python version: 3.7
- Bazel version (if compiling from source): 0.23.0
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: 10 / 7



**Describe the problem**

I have tried compiling TF 2.0 from source, because my platform has a libc which is too old for the normal distribution.
I have followed the instructions from here: https://www.tensorflow.org/install/source.

It compiles successfully, however it seems that the created package still uses the old v1 api.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```bash
git checkout v2.0.0-alpha0

bazel build --local_resources 4096,25,1 --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

./bazel-bin/tensorflow/tools/pip_package/build_pip_package --nightly_flag /tmp/tensorflow_pkg

pip3 install tf_nightly-2.0.0a0-cp37-cp37m-linux_x86_64.whl --user
```

I then get the error

```
AttributeError: module 'tensorflow._api.v1.summary' has no attribute 'create_file_writer'
```

"
27203,install page wrong for version 2.0 alpha,"on page https://www.tensorflow.org/install/pip

change ""tensorflow==2.0.0-alpha0"" to ""tensorflow==2.0.0-a0""    (two instances)
change ""tensorflow-gpu==2.0.0-alpha0"" to ""tensorflow-gpu==2.0.0-a0""
change ""tensorflow==2.0.0-alpha0-gpu"" to ""tensorflow-gpu==2.0.0-a0""

on page https://www.tensorflow.org/install/gpu

change ""tensorflow==2.0.0-alpha0"" to ""tensorflow==2.0.0-a0"" 

i had to make these changes on my ubuntu-18-04 system to get it to install."
27202,Tflite JNI wraps seems failing to release int array.,"Hi, it seems that current impl of tflite jni overlooked ref release for array, And the current tflite really could make JNI reference table overflow some phones with Android 4.4.4 (API 19).

How: 
Just invoke `resizeInput` every time you run interpreter, even put the same int array to it. You can see the reference table is booming.

version:
I've tried 1.13.1 and 0.0.0-nightly, it's the same.

the relevant code is [here](https://github.com/tensorflow/tensorflow/blob/c18034bc927f733e5e5a43d0c421775f969e0d04/tensorflow/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc#L104)

The strange thing is the code above, althrough didn't deal with release in some situation, should work fine with same int array. "
27201,Error when restoring model using an LSTM layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 1803
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
Reincarnation of bug #25327 for different layer type (LSTM rather than Embedding). After applying fix [cebce4a](https://github.com/tensorflow/tensorflow/commit/cebce4a2b5e33a06a1c92772008082895568f10a) for bug #25327, the code provided below results in `ValueError: Input 0 of node lstm/while/ReadVariableOp/Enter_1 was passed float from lstm/kernel_1:0 incompatible with expected resource.`.

**Describe the expected behavior**
Should restore model rather than raise error.

**Code to reproduce the issue**
```python
import tensorflow as tf
from tensorflow import keras
# from tensorflow.python.framework import graph_util
import graph_util_fixed as graph_util  # using file with fix cebce4a
from tensorflow.python.framework import graph_io
from tensorflow.python.platform import gfile

model = keras.models.Sequential()
model.add(keras.layers.Embedding(0x1000, output_dim=128))
model.add(keras.layers.LSTM(512))
model.add(keras.layers.Dense(1, activation=""sigmoid""))

sess = keras.backend.get_session()
output_node_names = [node.op.name for node in model.outputs]
constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), output_node_names)
graph_io.write_graph(constant_graph, ""."", ""test.pb"", as_text=False)

with tf.Session() as sess:
    with gfile.FastGFile(""test.pb"", ""rb"") as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
        sess.graph.as_default()
        tf.import_graph_def(graph_def, name="""")
```
"
27200,[tflite] multithreaded DepthwiseConv returns wrong results on aarch64 platform,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): nope
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android Pie (Pixel 2) and Debian (Coral EdgeTPU Dev Board)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 2
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): recent master 8d1c139712a65a25419e951eca737c38f917262b
- Python version: N/A
- Bazel version (if compiling from source): 0.24.0
- GCC/Compiler version (if compiling from source): gcc 6 on Coral Dev Board, clang in Android NDK r17b
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


**Describe the current behavior**
```
> adb shell
walleye:/ $ cd /data/local/tmp
walleye:/data/local/tmp $ ./label_image_new_aarch64  -m mobilenet_v1_1.0_224_quant.tflite  -t 1                                                              
Loaded model mobilenet_v1_1.0_224_quant.tflite
resolved reporter
INFO: Initialized TensorFlow Lite runtime.
invoked 
average time: 66.784 ms 
0.419608: 907 Windsor tie
0.356863: 653 military uniform
0.0352941: 458 bow tie
0.027451: 668 mortarboard
0.0235294: 543 drumstick
walleye:/data/local/tmp $./label_image_new_aarch64  -m mobilenet_v1_1.0_224_quant.tflite  -t 2                                                              
Loaded model mobilenet_v1_1.0_224_quant.tflite
resolved reporter
INFO: Initialized TensorFlow Lite runtime.
invoked 
average time: 36.938 ms 
0.215686: 543 drumstick
0.129412: 594 harmonica
0.0470588: 700 panpipe
0.0470588: 559 flute
0.0235294: 773 safety pin
walleye:/data/local/tmp $./label_image_new_aarch64  -m mobilenet_v1_1.0_224_quant.tflite  -t 3                                                              
Loaded model mobilenet_v1_1.0_224_quant.tflite
resolved reporter
INFO: Initialized TensorFlow Lite runtime.
invoked 
average time: 32.779 ms 
0.592157: 797 ski mask
0.258824: 434 bathing cap
0.0156863: 728 planetarium
0.0117647: 736 poncho
0.00784314: 773 safety pin
walleye:/data/local/tmp $ ./label_image_new_aarch64  -m mobilenet_v1_1.0_224_quant.tflite  -t 4                                                              <
Loaded model mobilenet_v1_1.0_224_quant.tflite
resolved reporter
INFO: Initialized TensorFlow Lite runtime.
invoked 
average time: 25.737 ms 
0.823529: 728 planetarium
0.0352941: 669 mosque
0.0156863: 434 bathing cap
0.0156863: 418 balloon
0.0117647: 611 jersey
walleye:/data/local/tmp $ 
```
**Describe the expected behavior**
2, 3, 4 threads (`-t 2, -t 3, -t 4`) are expected to return the same results as single-thread case.
**Code to reproduce the issue**
TFLite `label_image_new_aarch64`: 

```
bazel build --config android_arm --cxxopt=-std=c++11  tensorflow/lite/examples/label_image:label_image  --config monolithic
```

label_image needs input image and labels file. I use
input_image:  [grace_hoper.bmp](https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp)
labels.txt: from [mobilenet_v1_1.0_224_frozen.tgz](https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_1.0_224_frozen.tgz 
)
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
27199,Building Tensorflow Examples via Bazel on macOS doesn't work. Building for Object Tracking Support.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: macOS 10.13.6
- Mobile device: Not applicable
- TensorFlow installed from (source or binary): Source
- TensorFlow version: The latest one as of writing
- Python version: 2.7
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 0.23.2
- GCC/Compiler version (if compiling from source): 10.0.0
- CUDA/cuDNN version: none
- GPU model and memory: none



**Describe the problem**

I'm trying to build the Tensorflow Examples via Bazel so that I can have the Object Tracking Support for Object Detection in TF-Lite.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

[I used this guide](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/android/app#building-from-source-with-bazel), supplemented with [this guide](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android#bazel)

And here are the steps I took.

> cd ~/my/android/directory/
>git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git

I checked for my bazel version:
> bazel version
> WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
> Build label: 0.23.2
> Build target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
> Build time: Mon Mar 11 16:50:03 2019 (1552323003)
> Build timestamp: 1552323003
> Build timestamp as int: 1552323003

> cd tensorflow
> ./configure

Here were the output, I just hit the defaults for all of these:

> WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
> You have bazel 0.23.2 installed.
> Please specify the location of python. [Default is /usr/local/opt/python@2/bin/python2.7]: 

> Found possible Python library paths:
>   /usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages
> Please input the desired Python library path to use.  Default is [/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages]

> Do you wish to build TensorFlow with XLA JIT support? [y/N]: 
> No XLA JIT support will be enabled for TensorFlow.

> Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
> No OpenCL SYCL support will be enabled for TensorFlow.

> Do you wish to build TensorFlow with ROCm support? [y/N]: 
> No ROCm support will be enabled for TensorFlow.

> Do you wish to build TensorFlow with CUDA support? [y/N]: 
> No CUDA support will be enabled for TensorFlow.

> Do you wish to download a fresh release of clang? (Experimental) [y/N]: 
> Clang will not be downloaded.

> Do you wish to build TensorFlow with MPI support? [y/N]: 
> No MPI support will be enabled for TensorFlow.

> Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 

> Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
> Not configuring the WORKSPACE for Android builds.

> Would you like to configure TensorFlow for iOS builds? [y/N]: 
> Not configuring TensorFlow for iOS builds.

> Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
> 	--config=mkl         	# Build with MKL support.
> 	--config=monolithic  	# Config for mostly static monolithic build.
> 	--config=gdr         	# Build with GDR support.
> 	--config=verbs       	# Build with libverbs support.
> 	--config=ngraph      	# Build with Intel nGraph support.
> 	--config=numa        	# Build with NUMA support.
> 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
> Preconfigured Bazel build configs to DISABLE default on features:
> 	--config=noaws       	# Disable AWS S3 filesystem support.
> 	--config=nogcp       	# Disable GCP support.
> 	--config=nohdfs      	# Disable HDFS support.
> 	--config=noignite    	# Disable Apache Ignite support.
> 	--config=nokafka     	# Disable Apache Kafka support.
> 	--config=nonccl      	# Disable NVIDIA NCCL support.
> Configuration finished

The guide in the 2nd link says:

> ""The Android entries in <workspace_root>/WORKSPACE must be uncommented with the paths filled in appropriately depending on where you installed the NDK and SDK. Otherwise an error such as: ""The external label '//external:android/sdk' is not bound to anything"" will be reported.""

But I don't see anything on that directory aside from the tensorflow folder that was cloned?

And in the next paragraph, it says:
> ""Also edit the API levels for the SDK in WORKSPACE to the highest level you have installed in your SDK. This must be >= 23 (this is completely independent of the API level of the demo, which is defined in AndroidManifest.xml). The NDK API level may remain at 14.""

But when I checked my WORKSPACE file through nano and cat, I only see this:


> workspace(name = ""org_tensorflow"")
> 
> load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"", ""http_file"")
> 
> http_archive(
>     name = ""io_bazel_rules_closure"",
>     sha256 = ""ddce3b3a3909f99b28b25071c40b7fec7e2e1d1d1a4b2e933f3082aa99517105"",
>     strip_prefix = ""rules_closure-316e6133888bfc39fb860a4f1a31cfcbae485aef"",
>     urls = [
>         ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/316e6133888bfc39fb860a4f1a31cfcbae485aef.tar.gz"",
>         ""https://github.com/bazelbuild/rules_closure/archive/316e6133888bfc39fb860a4f1a31cfcbae485aef.tar.gz"",  # 2019-03-21
>     ],
> )
> 
> load(""@io_bazel_rules_closure//closure:defs.bzl"", ""closure_repositories"")
> 
> closure_repositories()
> 
> load(""//third_party/toolchains/preconfig/generate:archives.bzl"",
>      ""bazel_toolchains_archive"")
> 
> bazel_toolchains_archive()
> 
> load(
>     ""@bazel_toolchains//repositories:repositories.bzl"",
>     bazel_toolchains_repositories = ""repositories"",
> )
> 
> bazel_toolchains_repositories()
> 
> load(
>     ""@io_bazel_rules_docker//repositories:repositories.bzl"",
>     container_repositories = ""repositories"",
> )
> 
> container_repositories()
> 
> load(""//third_party/toolchains/preconfig/generate:workspace.bzl"",
>      ""remote_config_workspace"")
> 
> remote_config_workspace()
> 
> # Apple and Swift rules.
> http_archive(
>     name = ""build_bazel_rules_apple"",
>     sha256 = ""4b90786009fa8df25230442244bad2832ba8d6bc4987f68150a7de59c8827e90"",
>     strip_prefix = ""rules_apple-0.14.0"",
>     urls = [""https://github.com/bazelbuild/rules_apple/archive/0.14.0.tar.gz""],
> )
> http_file(
>    name = ""xctestrunner"",
>     executable = 1,
>     urls = [""https://github.com/google/xctestrunner/releases/download/0.2.6/ios_test_runner.par""],
> )
> 
> http_archive(
>     name = ""bazel_skylib"",
>     sha256 = ""2c62d8cd4ab1e65c08647eb4afe38f51591f43f7f0885e7769832fa137633dcb"",
>     strip_prefix = ""bazel-skylib-0.7.0"",
>     urls = [""https://github.com/bazelbuild/bazel-skylib/archive/0.7.0.tar.gz""],
> )
> 
> http_archive(
>     name = ""build_bazel_apple_support"",
>     sha256 = ""835663c4bb02f4bf01dce8a2a176df7fa682dbb867d3698ae12258c1628bb8f0"",
>     strip_prefix = ""apple_support-0.5.0"",
>     urls = [""https://github.com/bazelbuild/apple_support/archive/0.5.0.tar.gz""],
> )
> 
> http_archive(
>     name = ""build_bazel_rules_swift"",
>     sha256 = ""32d124878cd49775d84f59ba90440c8b23b7c775aec8fec1978f751c76ddee8a"",
>     strip_prefix = ""rules_swift-0.7.0"",
>     urls = [""https://github.com/bazelbuild/rules_swift/archive/0.7.0.tar.gz""],
> )
> 
> http_archive(
>     name = ""com_github_apple_swift_swift_protobuf"",
>     type = ""zip"",
>     strip_prefix = ""swift-protobuf-1.2.0/"",
>     urls = [""https://github.com/apple/swift-protobuf/archive/1.2.0.zip""],
> )
> 
> # Use swift_rules_dependencies to fetch the tolchains.
> # Since we defined all the ""git_repository"" rules above, the following call will
> # skip redefining them.
> load(""@build_bazel_rules_swift//swift:repositories.bzl"", ""swift_rules_dependencies"")
> swift_rules_dependencies()
> 
> # We must check the bazel version before trying to parse any other BUILD
> # files, in case the parsing of those build files depends on the bazel
> # version we require here.
> load(""//tensorflow:version_check.bzl"", ""check_bazel_version_at_least"")
> check_bazel_version_at_least(""0.19.0"")
> 
> load(""//tensorflow:workspace.bzl"", ""tf_workspace"")
> 
> load(""//third_party/android:android_configure.bzl"", ""android_configure"")
> android_configure(name=""local_config_android"")
> load(""@local_config_android//:android.bzl"", ""android_workspace"")
> android_workspace()
> 
> # Please add all new TensorFlow dependencies in workspace.bzl.
> tf_workspace()
> 
> http_archive(
>     name = ""inception_v1"",
>     build_file = ""//:models.BUILD"",
>     sha256 = ""7efe12a8363f09bc24d7b7a450304a15655a57a7751929b2c1593a71183bb105"",
>     urls = [
>         ""http://storage.googleapis.com/download.tensorflow.org/models/inception_v1.zip"",
>         ""http://download.tensorflow.org/models/inception_v1.zip"",
>     ],
> )
> 
> http_archive(
>     name = ""mobile_ssd"",
>     build_file = ""//:models.BUILD"",
>     sha256 = ""bddd81ea5c80a97adfac1c9f770e6f55cbafd7cce4d3bbe15fbeb041e6b8f3e8"",
>     urls = [
>         ""http://storage.googleapis.com/download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_android_export.zip"",
>         ""http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_android_export.zip"",
>     ],
> )
> 
> http_archive(
>    name = ""mobile_multibox"",
>     build_file = ""//:models.BUILD"",
>     sha256 = ""859edcddf84dddb974c36c36cfc1f74555148e9c9213dedacf1d6b613ad52b96"",
>     urls = [
>         ""http://storage.googleapis.com/download.tensorflow.org/models/mobile_multibox_v1a.zip"",
>         ""http://download.tensorflow.org/models/mobile_multibox_v1a.zip"",
>     ],
> )
> 
> http_archive(
>     name = ""stylize"",
>     build_file = ""//:models.BUILD"",
>     sha256 = ""3d374a730aef330424a356a8d4f04d8a54277c425e274ecb7d9c83aa912c6bfa"",
>     urls = [
>         ""http://storage.googleapis.com/download.tensorflow.org/models/stylize_v1.zip"",
>         ""http://download.tensorflow.org/models/stylize_v1.zip"",
>     ],
> )
> 
> http_archive(
>     name = ""speech_commands"",
>     build_file = ""//:models.BUILD"",
>     sha256 = ""c3ec4fea3158eb111f1d932336351edfe8bd515bb6e87aad4f25dbad0a600d0c"",
>     urls = [
>         ""http://storage.googleapis.com/download.tensorflow.org/models/speech_commands_v0.01.zip"",
>         ""http://download.tensorflow.org/models/speech_commands_v0.01.zip"",
>     ],
> )

There wasn't anything that resembled `api_levels` or anything similar. 

And when I finally hit:
> bazel build -c opt --cxxopt='--std=c++11' --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \
>  //tensorflow/lite/examples/android:tflite_demo

I got the following errors:

> ERROR: /private/var/tmp/_bazel_angelopvillasanta/70db096b7d05a90f0f37f9f041e057b0/external/local_config_cc/BUILD:45:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'
> ERROR: Analysis of target '//tensorflow/lite/examples/android:tflite_demo' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed; build aborted
> INFO: Elapsed time: 14.026s
> INFO: 0 processes.
> FAILED: Build did NOT complete successfully (24 packages loaded, 215 targets configured)
>     Fetching @local_jdk; fetching

Now I'm not sure what I should do next. I'm not sure how to install C++11 on the macOS, as that seems to be the only dependency I'm not sure I have covered. 

Basically my objective is to build the Tensorflow example in a macOS, because I need to be able to have support of the Object Tracking feature in Object Detection for Tensorflow-Lite.

Is this possible? Did I miss any steps?




**Any other info / logs**
The logs are all above. 
"
27198,"Error importing tensorflow, tensorflow library was compiled to use AVX instructions, but these aren;t in your machine","

**System information**
-  Linux Ubuntu 16.04
- 
- TensorFlow installed from binary (pip install)
- TensorFlow version:
- Python version: 3.5
- Installed using virtualenv? pip? conda?: pip and virtualenv
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Problem described** 

i was following the tutorial for using intel neural stick 2 for object detection https://towardsdatascience.com/speed-up-predictions-on-low-power-devices-using-neural-compute-stick-and-openvino-98f3ae9dcf41
in the  example i install the prerequisites using the command 
`sudo ./opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/install_prerequisites/install_prerequisites.sh`

tensorflow was installed with the prerequisites , i also installed tensorflow using pip install , but when i run the next command 

`mo_tf.py \
    --input_model ~/Downloads/ssd_mobilenet_v1_coco_2018_01_28/frozen_inference_graph.pb \
    --tensorflow_use_custom_operations_config     /opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/extensions/front/tf/ssd_support.json \
    --tensorflow_object_detection_api_pipeline_config ~/Downloads/ssd_mobilenet_v1_coco_2018_01_28/pipeline.config \
    --data_type FP16`


i get the following error 
`F tensorflow/core/platform/cpu_feature_guard.cc:37] 
The tensorflow library was compiled to use AVX instructions, but these aren't available in your machine 
Aborted (core dumped) ` 

i am getting the same error when try and **import tensorflow** 

what should i do to solve this error ? 



"
27197,pr_curve gives error regardless of label type,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- I'm Trying to use pr_curve with tensorboard, and in one case I get an error telling me by labels need to be of type bool.  If I change it to a bool, I get an error from deep within the tf code that the type need to be float.  Everything else is working great.
- Linux Ubuntu 16.04
- Source compiled for CUDA 10 and CDNN 7.4.2
- TensorFlow version: 1.13.0-dev20190218
- Python version: 3.6
- Bazel version (if compiling from source): 0.18.1
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: 10/7.4.2
- GPU model and memory: 1070ti with 4GB


I'm trying to use pr_curve with tensorboard, and when I issue the pr_curve command I get an error telling me the labels (parameter 3) are supposed to by of type bool.  If I set it to a bool, the command runs, but later from deep within tf code errors with parameter is to be of type float, which is was originally.

If I remove the pr_curve command, I no longer get any errors.

**Code to reproduce the issue**  Problem is line 109 or 110.

    from __future__ import absolute_import
    from __future__ import division
    from __future__ import print_function
    import pdb 
    import numpy as np
    import math, time
    import tensorflow as tf
    from tensorboard import summary as summ_lib
    import flags as f
    import input, lenet5, evaluate
    
    tf.logging.set_verbosity(tf.logging.INFO)
    similar results every run)
    tf.set_random_seed(1)
    np.random.seed(10)
    
    summaries = {'train': [], 'validate': [], 'test': []}
    
    def main(unused_argv):
         train_dataset, validate_dataset, test_dataset = 
    input.input(shuffle_files=False)
         info = tf.constant(
                [""Batch size = %s"" % f.FLAGS.batch_size,
                 ""Epochs = %s"" % f.FLAGS.num_epochs,
                 ""Learning rate = %s"" % f.FLAGS.learning_rate,
                 ""Batch normalization = No"",
                 ""Window size = %s"" % f.FLAGS.window_size,
                 ""Shuffle Files = No"",
                 ""CNN model = %s"" % f.FLAGS.cnn_model,
                 ""Shuffle Samples = YES""]
          )
          with tf.name_scope('input'):
                x = tf.placeholder(tf.float32, [None, input.SAMPLE_DEPTH, 
    input.SAMPLE_HEIGHT, input.SAMPLE_WIDTH])
                y_ = tf.placeholder(tf.float32, [None, 2])
                dropout_rate = tf.placeholder(tf.float32)
                is_training = tf.placeholder(tf.bool)
    
          with tf.name_scope('logits'):
                if f.FLAGS.cnn_model == ""lenet5"":
                      logits = lenet5.model_fn(sample_input = x, 
    is_training=is_training, summaries=summaries)
    
          with tf.name_scope('loss'):
                cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=logits)
                mean_cross_entropy_loss = tf.reduce_mean(cross_entropy)
    
                loss_summ = tf.summary.scalar('Mean_cross_entropy_loss', mean_cross_entropy_loss)
                summaries['train'].append(loss_summ)
    
          with tf.name_scope('adam_optimizer'):
                optimizer = tf.train.AdamOptimizer(f.FLAGS.learning_rate).minimize(mean_cross_entropy_loss)
    
          with tf.name_scope('accuracy'):
                preds = tf.argmax(logits, 1)
                correct_preds = tf.argmax(y_, 1)
                equal = tf.equal(preds, correct_preds)
                training_accuracy_op = tf.reduce_mean(tf.cast(equal, tf.float32))
    
    summaries['train'].append(tf.summary.scalar('Training_Accuracy', training_accuracy_op))
    
    
          with tf.name_scope('Evaluation_Metrics'):
                tp_op = evaluate.tp(logits=logits, labels=y_)
                fp_op = evaluate.fp(logits=logits, labels=y_)
                tn_op = evaluate.tn(logits=logits, labels=y_)
                fn_op = evaluate.fn(logits=logits, labels=y_)
    
                tp_sum = tf.placeholder(tf.float32)
                tn_sum = tf.placeholder(tf.float32)
                fp_sum = tf.placeholder(tf.float32)
                fn_sum = tf.placeholder(tf.float32)
    
                precision_op = evaluate.precision(tp=tp_sum, fp=fp_sum, tn=tn_sum, fn=fn_sum)
                accuracy_op = evaluate.accuracy(tp=tp_sum, fp=fp_sum, tn=tn_sum, fn=fn_sum)
                recall_op = evaluate.recall(tp=tp_sum, fp=fp_sum, tn=tn_sum, fn=fn_sum)
                fscore_op = evaluate.fscore(tp=tp_sum, fp=fp_sum, tn=tn_sum, fn=fn_sum)
    
                precision_summ = tf.summary.scalar('Precision', precision_op)
                accuracy_summ = tf.summary.scalar('Accuracy', accuracy_op)
                recall_summ = tf.summary.scalar('Recall', recall_op)
                fscore_summ = tf.summary.scalar('Fscore', fscore_op)
    
                summaries['validate'].append(accuracy_summ)
                summaries['validate'].append(precision_summ)
                summaries['validate'].append(recall_summ)
                summaries['validate'].append(fscore_summ)
    
                summaries['test'].append(accuracy_summ)
                summaries['test'].append(precision_summ)
                summaries['test'].append(recall_summ)
                summaries['test'].append(fscore_summ)
    
          print (""Saving graph to %s"" % f.FLAGS.log_dir)
          mAP_summ = summ_lib.pr_curve(name='mAP',predictions=logits,labels=y_,num_thresholds=11)
          # OR THIS ONE mAP_summ = summ_lib.pr_curve(name='mAP',predictions=logits,labels=tf.cast(y_,tf.bool),num_thresholds=11)
    
          train_writer = tf.summary.FileWriter(f.FLAGS.log_dir + ""/train"")
          validate_writer = tf.summary.FileWriter(f.FLAGS.log_dir + ""/validate"")
          test_writer = tf.summary.FileWriter(f.FLAGS.log_dir + ""/test"")
          train_writer.add_graph(tf.get_default_graph())
    
          train_summaries = tf.summary.merge(summaries['train'])
          validate_summaries = tf.summary.merge(summaries['validate'])
          test_summaries = tf.summary.merge(summaries['test'])
    
          with tf.Session() as sess:
                train_writer.add_summary(sess.run(tf.summary.text(""Information"", info)))
                train_iter = train_dataset.make_initializable_iterator()
                train_next_elem = train_iter.get_next()
                sess.run(tf.global_variables_initializer())
                global_step = 0
                display_freq = 10
                validate_freq = 50
                test_freq = 50
                for epoch in range(1, f.FLAGS.num_epochs+1):
                      sess.run(train_iter.initializer)
                      step_time = 0.0
                      fetch_time = 0.0
                      while True:
                            try:
                                  a = time.time()
                                  global_step += 1
                                  #print(make_bread())
                                  sample, label = sess.run(train_next_elem)
                                  fetch_time += time.time() - a
                                  a = time.time()
                                  _, summ = sess.run([optimizer, train_summaries], feed_dict={x: sample, y_: label, dropout_rate: 0.5, is_training: True})
                                  train_writer.add_summary(summ, global_step)
                                  step_time += time.time() - a
                            except tf.errors.OutOfRangeError:
                                  break
    
                            if global_step % display_freq == 0:
                                  batch_loss, batch_accuracy = sess.run([mean_cross_entropy_loss, training_accuracy_op],
                                                                        feed_dict={x: sample, y_: label, dropout_rate: 1.0, is_training: False})
                                  print (""Epoch {:3}\t Step {:5}:\t Loss={:.3f}, \tTraining Accuracy={:.5f} \tStep Time {:4.2f}m, Fetch Time {:4.2f}m"".
                                         format(epoch, global_step, batch_loss, batch_accuracy, step_time/60, fetch_time/60))
                                  step_time = 0.0
                                  fetch_time = 0.0
    
    
                      #Validate and test after each epoch
                      val_it = validate_dataset.make_one_shot_iterator()
                      val_next_elem = val_it.get_next()
                      tot_tp, tot_tn, tot_fp, tot_fn = 0, 0, 0, 0
                      while True:
                            try:
                                  sample, label = sess.run(val_next_elem)
                                  tp, fp, tn, fn = sess.run([tp_op, fp_op, tn_op, fn_op],
                                                            feed_dict={x: sample, y_: label, dropout_rate: 1.0, is_training: False})
                            except tf.errors.OutOfRangeError:
                                  break
                            tot_tp += tp
                            tot_fp += fp
                            tot_fn += fn
                            tot_tn += tn
                      precision, recall, accuracy, fscore, summ = sess.run([precision_op, recall_op, accuracy_op, fscore_op, validate_summaries],
                                                                           feed_dict={tp_sum: tot_tp, tn_sum: tot_tn, fp_sum: tot_fp, fn_sum: tot_fn})
                      validate_writer.add_summary(summ, global_step)
                      print (""Epoch %d, Step %d"" % (epoch, global_step))
                      print (""=""*10, ""Validating Results"", ""=""*10)
                      print (""TP: %g\nTN: %g\nFP: %g\nFN: %g"" % (tot_tp, tot_tn, tot_fp, tot_fn))
                      print (""\tPrecision: %g\n\tRecall: %g\n\tF1_score: %g\n\tAccuracy: %g"" % (precision, recall, fscore, accuracy))
    
    
                      test_it = test_dataset.make_one_shot_iterator()
                      test_next_elem = test_it.get_next()
                      tot_tp, tot_tn, tot_fp, tot_tn = 0, 0, 0, 0
                      while True:
                            try:
                                  sample, label = sess.run(test_next_elem)
                                  tp, fp, tn, fn, pred, lab = sess.run([tp_op, fp_op, tn_op, fn_op, logits, y_],
                                                            feed_dict={x: sample, y_: label, dropout_rate: 1.0, is_training: False})
                            except tf.errors.OutOfRangeError:
                                  break
                            tot_tp += tp
                            tot_fp += fp
                            tot_fn += fn
                            tot_tn += tn
                      precision, recall, accuracy, fscore, summ = sess.run([precision_op, recall_op, accuracy_op, fscore_op, test_summaries],
                                                                           feed_dict={tp_sum: tot_tp, tn_sum: tot_tn, fp_sum: tot_fp, fn_sum: tot_fn})
    
                      test_writer.add_summary(summ, global_step)
    
                      print (""=""*10, ""Testing Results"", ""=""*10)
                      print (""TP: %g\nTN: %g\nFP: %g\nFN: %g"" % (tot_tp, tot_tn, tot_fp, tot_fn))
                      print (""\tPrecision: %g\n\tRecall: %g\n\tF1_score: %g\n\tAccuracy: %g"" % (precision, recall, fscore, accuracy))
                      print (""=""*10, ""==============="", ""=""*10)
                map_writer = tf.summary.FileWriter(f.FLAGS.log_dir + ""/mAP"")
                map_writer.add_summary(sess.run(mAP_summ),global_step=0)
                map_writer.close()
    
    if __name__ == ""__main__"":
          tf.app.run()


**Other info / logs**  Here are the errors depending on the pr_curve command used around line 110.


    10:06:00:26.03.2019:/media/randy/Data1/Python/Machine_Learning/3d_cnn ./run2.sh
    Number of training files:  68
    Number of validation files:  23
    Number of testing files:  23
    W0326 22:06:28.115116 139978789848896 deprecation.py:506] From /home/randy/.local/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py:187: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
    Instructions for updating:
    Call initializer instance with the dtype argument instead of passing it to the constructor
    Saving graph to /media/randy/Data1/log/proc_order/ABS-Correlated
    Traceback (most recent call last):
      File ""cnn_model.py"", line 208, in <module>
        tf.app.run()
      File ""/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40, in run
        _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
      File ""/home/randy/.local/lib/python3.6/site-packages/absl/app.py"", line 300, in run
        _run_main(main, args)
      File ""/home/randy/.local/lib/python3.6/site-packages/absl/app.py"", line 251, in _run_main
        sys.exit(main(argv))
      File ""cnn_model.py"", line 108, in main
        mAP_summ = summ_lib.pr_curve(name='mAP',predictions=logits,labels=y_,num_thresholds=11)
      File ""/home/randy/.local/lib/python3.6/site-packages/tensorboard/plugins/pr_curve/summary.py"", line 94, in op
        tf.compat.v1.assert_type(labels, tf.bool)
      File ""/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 1502, in assert_type
        tf_type))
    TypeError:   input/Placeholder_1:0 must be of type <dtype: 'bool'>
    
    
    
    
    
    10:06:28:26.03.2019:/media/randy/Data1/Python/Machine_Learning/3d_cnn ./run2.sh
    Number of training files:  68
    Number of validation files:  23
    Number of testing files:  23
    W0326 22:07:04.819849 140201659422528 deprecation.py:506] From /home/randy/.local/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py:187: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
    Instructions for updating:
    Call initializer instance with the dtype argument instead of passing it to the constructor
    Saving graph to /media/randy/Data1/log/proc_order/ABS-Correlated
    2019-03-26 22:07:04.963555: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
    2019-03-26 22:07:04.966910: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened CUDA library libcuda.so.1
    2019-03-26 22:07:05.054544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1010] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
    2019-03-26 22:07:05.054971: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x4dd1b80 executing computations on platform CUDA. Devices:
    2019-03-26 22:07:05.054985: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): GeForce GTX 1070 Ti, Compute Capability 6.1
    2019-03-26 22:07:05.072833: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192000000 Hz
    2019-03-26 22:07:05.073556: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x4d5bc70 executing computations on platform Host. Devices:
    2019-03-26 22:07:05.073585: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>
    2019-03-26 22:07:05.073788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1464] Found device 0 with properties:
    name: GeForce GTX 1070 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
    pciBusID: 0000:01:00.0
    totalMemory: 7.93GiB freeMemory: 7.53GiB
    2019-03-26 22:07:05.073797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1543] Adding visible gpu devices: 0
    2019-03-26 22:07:05.073837: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened CUDA library libcudart.so.10.0
    2019-03-26 22:07:05.074339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:
    2019-03-26 22:07:05.074346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0
    2019-03-26 22:07:05.074349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N
    2019-03-26 22:07:05.074521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1146] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7324 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
    W0326 22:07:05.090326 140201659422528 deprecation.py:323] From cnn_model.py:122: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
    2019-03-26 22:07:06.920871: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened CUDA library libcublas.so.10.0
    2019-03-26 22:07:07.021886: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened CUDA library libcudnn.so.7
    Epoch   1        Step    10:     Loss=0.723,    Training Accuracy=0.75977       Step Time 0.57m, Fetch Time 0.01m
    Epoch   1        Step    20:     Loss=0.528,    Training Accuracy=0.80859       Step Time 0.55m, Fetch Time 0.00m
    Epoch   1        Step    30:     Loss=0.399,    Training Accuracy=0.86523       Step Time 0.55m, Fetch Time 0.00m
    W0326 22:09:01.058223 140201659422528 deprecation.py:323] From cnn_model.py:157: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
    Epoch 1, Step 35
    ========== Validating Results ==========
    TP: 980
    TN: 3093
    FP: 302
    FN: 1257
            Precision: 0.764431
            Recall: 0.438087
            F1_score: 0.556976
            Accuracy: 0.723189
    ========== Testing Results ==========
    TP: 1124
    TN: 3500
    FP: 272
    FN: 2505
            Precision: 0.805158
            Recall: 0.309727
            F1_score: 0.447363
            Accuracy: 0.62478
    ========== =============== ==========
    Traceback (most recent call last):
      File ""/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
        return fn(*args)
      File ""/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1320, in _run_fn
        options, feed_dict, fetch_list, target_list, run_metadata)
      File ""/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1408, in _call_tf_sessionrun
        run_metadata)
    tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input/Placeholder_1' with dtype float and shape [?,2]
             [[{{node input/Placeholder_1}}]]
             [[mAP/stack/_199]]
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""cnn_model.py"", line 208, in <module>
        tf.app.run()
      File ""/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40, in run
        _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
      File ""/home/randy/.local/lib/python3.6/site-packages/absl/app.py"", line 300, in run
        _run_main(main, args)
      File ""/home/randy/.local/lib/python3.6/site-packages/absl/app.py"", line 251, in _run_main
        sys.exit(main(argv))
      File ""cnn_model.py"", line 204, in main
        map_writer.add_summary(sess.run(mAP_summ),global_step=0)
      File ""/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 930, in run
        run_metadata_ptr)
      File ""/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1153, in _run
        feed_dict_tensor, options, run_metadata)
      File ""/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1329, in _do_run
        run_metadata)
      File ""/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1349, in _do_call
        raise type(e)(node_def, op, message)
    tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input/Placeholder_1' with dtype float and shape [?,2]
             [[node input/Placeholder_1 (defined at cnn_model.py:50) ]]
             [[mAP/stack/_199]]
    
    Original stack trace for 'input/Placeholder_1':
      File ""cnn_model.py"", line 208, in <module>
        tf.app.run()
      File ""/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40, in run
        _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
      File ""/home/randy/.local/lib/python3.6/site-packages/absl/app.py"", line 300, in run
        _run_main(main, args)
      File ""/home/randy/.local/lib/python3.6/site-packages/absl/app.py"", line 251, in _run_main
        sys.exit(main(argv))
      File ""cnn_model.py"", line 50, in main
        y_ = tf.placeholder(tf.float32, [None, 2])
      File ""/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 2084, in placeholder
        return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)
      File ""/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 6098, in placeholder
        ""Placeholder"", dtype=dtype, shape=shape, name=name)
      File ""/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 800, in _apply_op_helper
        op_def=op_def)
      File ""/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
        return func(*args, **kwargs)
      File ""/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3473, in create_op
        op_def=op_def)
      File ""/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1961, in __init__
        self._traceback = tf_stack.extract_stack()
    
    
    
    
    
    
    
    
    
    
"
27196,Failed to load the native TensorFlow runtime and ImportError: No module named '_pywrap_tensorflow_internal',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12.0
- Python version: 3.7.2
- Installed using virtualenv? pip? conda?: python -m pip install --upgrade https://storage.googleapis2.0-py3-none-any.whl
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

Recently Installed TensorFlow using  >>>python -m pip install --upgrade https://storage.goo2.0-py3-none-any.whl

Error:
Failed to load the native TensorFlow runtime.


**Provide the exact sequence of commands / steps that you executed before running into the problem**

>>> import numpy
>>> import tensorflow as tf

Errors:


Traceback (most recent call last):
  File ""C:\Users\Ashwani\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\Ashwani\AppData\Local\Programs\Python\Python37-32\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Ashwani\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Ashwani\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Ashwani\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Ashwani\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Ashwani\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Ashwani\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Ashwani\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\Ashwani\AppData\Local\Programs\Python\Python37-32\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Ashwani\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Ashwani\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Ashwani\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Installation Logs:

Collecting tensorflow==1.12.0 from https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none
  Downloading https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl (62.0MB)
    100% || 62.0MB 168kB/s
Collecting tensorboard<1.13.0,>=1.12.0 (from tensorflow==1.12.0)
  Downloading https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b (3.0MB)
    100% || 3.1MB 1.6MB/s
Collecting protobuf>=3.6.1 (from tensorflow==1.12.0)
  Downloading https://files.pythonhosted.org/packages/8c/bf/e7f80b9d8e45d90ba411d5851c1154ff25b35ebb4c5728eb340 (865kB)
    100% || 870kB 1.0MB/s
Requirement already satisfied, skipping upgrade: six>=1.10.0 in c:\users\ashwani\appdata\local\programs\pythonow==1.12.0) (1.12.0)
Collecting keras-applications>=1.0.6 (from tensorflow==1.12.0)
  Downloading https://files.pythonhosted.org/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710ne-any.whl (51kB)
    100% || 61kB 1.9MB/s
Collecting absl-py>=0.1.6 (from tensorflow==1.12.0)
  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc86947
    100% || 102kB 2.5MB/s
Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in c:\users\ashwani\appdata\local\programs\pythflow==1.12.0) (1.16.2)
Collecting termcolor>=1.1.0 (from tensorflow==1.12.0)
  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab4617
Collecting wheel>=0.26 (from tensorflow==1.12.0)
  Downloading https://files.pythonhosted.org/packages/96/ba/a4702cbb6a3a485239fbe9525443446203f00771af9ac000fa30771af9ac000fa3ef2788201/wheel-0.33.1-py2.py3-none-any.whl
Collecting grpcio>=1.8.6 (from tensorflow==1.12.0)                                              88e8f7b4b033953
  Downloading https://files.pythonhosted.org/packages/4f/51/5e14559ceae1826a33158c48cec067b43dfe88e8f7b4b033953a004230b9/grpcio-1.19.0-cp37-cp37m-win32.whl (1.3MB)
    100% || 1.3MB 1.3MB/s
Collecting keras-preprocessing>=1.0.5 (from tensorflow==1.12.0)                                 73f2c31ac719ab2  Downloading https://files.pythonhosted.org/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl (59kB)
    100% || 61kB 5.1MB/s
Collecting astor>=0.6.0 (from tensorflow==1.12.0)                                               242d0b9f59534ef
  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl                                       c62b9de38ac4a4a
Collecting gast>=0.2.0 (from tensorflow==1.12.0)
  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a21/gast-0.2.2.tar.gz
Collecting werkzeug>=0.11.10 (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0)
  Downloading https://files.pythonhosted.org/packages/24/4d/2fc4e872fbaaf44cc3fd5a9cd42fda7e57c031f08e28c9f356898/Werkzeug-0.15.1-py2.py3-none-any.whl (328kB)    100% || 337kB 2.2MB/s
Collecting markdown>=2.6.8 (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0)
  Downloading https://files.pythonhosted.org/packages/f5/e4/d8c18f2555add57ff21bf25af36d827145896a07607486cc79aaf/Markdown-3.1-py2.py3-none-any.whl (87kB)
    100% || 92kB 5.4MB/s
Requirement already satisfied, skipping upgrade: setuptools in c:\users\ashwani\appdata\local\programs\python\7-32\lib\site-packages (from protobuf>=3.6.1->tensorflow==1.12.0) (40.6.2)Collecting h5py (from keras-applications>=1.0.6->tensorflow==1.12.0)
  Downloading https://files.pythonhosted.org/packages/e2/3e/76c3d0b25fee11ab73049475ca1833229fbf6e6a6bb56a8bab8f2/h5py-2.9.0-cp37-cp37m-win32.whl (2.1MB)    100% || 2.1MB 2.3MB/s
Installing collected packages: protobuf, werkzeug, grpcio, markdown, wheel, tensorboard, h5py, keras-applicatioications, absl-py, termcolor, keras-preprocessing, astor, gast, tensorflow  

The script markdown_py.exe is installed in 'C:\Users\Ashwani\AppData\Local\Programs\Python\Python37-32\Scrip\Scripts' which is not on PATH.                                                                          -locat  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.n.  

The script wheel.exe is installed in 'C:\Users\Ashwani\AppData\Local\Programs\Python\Python37-32\Scripts' whichh is not on PATH.  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.n.  

The script tensorboard.exe is installed in 'C:\Users\Ashwani\AppData\Local\Programs\Python\Python37-32\Scripts'' which is not on PATH.  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.n.  

Running setup.py install for absl-py ... done  
Running setup.py install for termcolor ... done
Running setup.py install for gast ... done

  The scripts freeze_graph.exe, saved_model_cli.exe, tensorboard.exe, tflite_convert.exe, toco.exe and toco_from_prprotos.exe are installed in 'C:\Users\Ashwani\AppData\Local\Programs\Python\Python37-32\Scripts' which is not onAT PATH.  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.n.

Successfully installed absl-py-0.7.1 astor-0.7.1 gast-0.2.2 grpcio-1.19.0 h5py-2.9.0 keras-applications-1.0.7 kerasas-preprocessing-1.0.9 markdown-3.1 protobuf-3.7.1 tensorboard-1.12.2 tensorflow-1.12.0 termcolor-1.1.0 werkzeug-150.15.1 wheel-0.33.1



"
27195,[TF2.0alpha] Eagerness/Distribute Strategies performance issues,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04/18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-dev20190327
- Python version: 3.6
- CUDA/cuDNN version: 10.0
- GPU model and memory: TITAN X 12 GB

**Describe the current behavior**

I have some code, where to the best of my knowledge not everything can be wrapped with the tf.function. Thus, in the function calling strategy.experimental_run I removed the `@tf.function` wrapper. However, this tremendously decreases the overall performance and falls way behind the 'normal' eager execution. 

For the toy example in this issue, these are the performance differences:

```
Computing for 1000-steps, no strategy: Time: 7.228003740310669 sec
Computing for 1000-steps, strategy with tf-function: Time: 2.6209354400634766
Computing for 1000-steps, strategy with no tf-function: Time: 63.60745906829834
```

**Describe the expected behavior**
I would expect that without tf.function at least the usual performance of eagerness is achieved.

**Code to reproduce the issue**

```python
import time

import tensorflow as tf


class DebugModel(tf.keras.Model):
  def __init__(self):
    super(DebugModel, self).__init__()
    self.dense = tf.keras.layers.Dense(20, activation='relu', input_shape=(5,))

  def __call__(self, input, *args, **kwargs):
    x = self.dense(input)
    return x


def train_step(input):
  with tf.GradientTape() as tape:
    out = my_model(input)
    my_loss = -tf.reduce_mean(out)
  gradients = tape.gradient(my_loss, my_model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, my_model.trainable_variables))
  return my_loss


def distributed_train():
  return strategy.experimental_run(train_step, train_iterator)

@tf.function
def distributed_train_tf_function():
  return strategy.experimental_run(train_step, train_iterator)


if __name__ == '__main__':
  dataset = tf.data.Dataset.from_tensor_slices(
    (tf.random.uniform([3000, 16, 5])))
  N_steps = 1000

  # Plain computation
  my_model = DebugModel()
  optimizer = tf.keras.optimizers.Adam(1e-2)
  _s = time.time()
  i = 0
  for data in dataset:
    if i >= N_steps:
      break
    train_step(data)
    i += 1
  print('Computing for {}-steps, no strategy: Time: {} sec'.format(
      N_steps, time.time() - _s))


  # Mirrored strategy with @tf.function
  dataset = tf.data.Dataset.from_tensor_slices(
    (tf.random.uniform([3000, 16, 5])))
  strategy = tf.distribute.MirroredStrategy()

  with strategy.scope():
    train_iterator = strategy.make_dataset_iterator(dataset)
    train_iterator.initialize()
    optimizer = tf.keras.optimizers.Adam(1e-2)
    my_model = DebugModel()

    _s = time.time()
    for s in range(N_steps):
      distributed_train_tf_function()
    print('Computing for {}-steps, strategy with tf-function: Time: {}'.format(
      N_steps, time.time() - _s))


  # Mirrored strategy without @tf.function decorator
  dataset = tf.data.Dataset.from_tensor_slices(
    (tf.random.uniform([3000, 16, 5])))
  strategy = tf.distribute.MirroredStrategy()
  with strategy.scope():
    train_iterator = strategy.make_dataset_iterator(dataset)
    train_iterator.initialize()
    optimizer = tf.keras.optimizers.Adam(1e-2)
    my_model = DebugModel()

    _s = time.time()
    for s in range(N_steps):
      distributed_train()
    print('Computing for {}-steps, strategy with no tf-function: Time: {}'.format(
      N_steps,
      time.time() - _s))
```
"
27193,Non-uniform FFT layer,"**System information**
- TensorFlow version (you are using): 1.13
- Are you willing to contribute it (Yes/No): Yes (but I only know how to code in Python)

**Describe the feature and the current behavior/state.**
Have a layer similar to the fft layer that will implement the non-uniform FFT.

**Will this change the current api? How?** This will allow to have one more option when using the signal module, `nfft`.

**Who will benefit with this feature?** People working with neural networks involving Non-uniform FFT. The main application I know of is MRI, but according to the [Wikipedia page](https://en.wikipedia.org/wiki/Non-uniform_discrete_Fourier_transform) there may be more applications. In particular, I think computed tomography can make use of it to accelerate acquisition times.

**Any Other info.** Since the NuFFT is not based on the FFT it would require a custom kernel and is therefore not trivial to implement. I haven't looked much into it but it could potentially make use of the [Flatiron Institute library](https://finufft.readthedocs.io/en/latest/).
"
27192,myWork,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
27191,AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'decode',"i ran on Google Colab and saw error at the start of training model. 
----> 8     for (batch, (img_tensor, target)) in enumerate(dataset):
--->545   def __next__(self):  # For Python 3 compatibility
--> 546     return self.next()
--> 575       return self._next_internal()
--> 567             output_shapes=self._flat_output_shapes)
-> 1848       _six.raise_from(_core._status_to_exception(e.code, message), None)
UnknownError: AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'decode'
Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py"", line 205, in __call__
    return func(device, token, args)

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py"", line 107, in __call__
    ret = self._func(*args)

  File ""<ipython-input-45-2085415d85d1>"", line 2, in map_func
    img_tensor = np.load(img_name.decode('utf-8')+'.npy')"
27190,Losses with Reduction.NONE do not keep the input shape (the result is averaged along the last axis),"**System information**
- OS Platform and Distribution: **Linux Manjaro**
- TensorFlow installed from: **binary**
- TensorFlow version (use command below): **2.0.0-dev20190326** (nightly build)
- Python version: **3.7.2**

**Describe the current behavior**
Currently, when a loss object is created with reduction=tf.losses.Reduction.NONE, the result is averaged along the last axis. This happens with all the losses in the tf.losses module.

**Describe the expected behavior**
When a loss object is created with reduction=tf.losses.Reduction.NONE, the output shape should match the input shape (y_true and y_pred shape).
This is also reported in the doc: [tf.losses.Reduction](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/losses/Reduction)

Moreover, when a  _sample_weight_ parameter ([BinaryCrossentropy](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/losses/BinaryCrossentropy)) is provided with the same shape of y_pred, it throws an exception. 

**Code to reproduce the issue**

Loss function with Reduction.NONE 
```
bce = tf.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.NONE)
y_true = [1., 1., 1., 1.]
y_pred = [1., 1., 0.5, 0.5]
loss = bce(y_true, y_pred)
print('Loss: ', loss.numpy())
```
CURRENT RESULT: 0.34657347
EXPECTED RESULT: [0., 0., 0.69314694, 0.69314694]

Loss function with Reduction.NONE and sample_weight
```
bce = tf.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.NONE)
y_true = [1., 1., 1., 1.]
y_pred = [1., 1., 0.5, 0.5]
sample_weight = [1, 1, 2, 1]
loss = bce(y_true, y_pred, sample_weight=sample_weight)
print('Loss: ', loss.numpy())
```
CURRENT RESULT: Exception
EXPECTED RESULT: [-0., -0., 1.3862939, 0.69314694]


**Other info / logs**
Currently, to achieve the expected result, an additional ""fake"" dimension must be added to y_true and y_pred, as in the following example.
```
bce = tf.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.NONE)
y_true = [1., 1., 1., 1.]
y_pred = [1., 1., 0.5, 0.5]
sample_weight = [1, 1, 2, 1]
loss = bce(tf.expand_dims(y_true, -1), tf.expand_dims(y_pred, -1), sample_weight=sample_weight)
print('Loss: ', loss.numpy())
```"
27189,bazel error: name 'http_archive' is not defined,"

I want to build libtensorflowLite.so for Android
```
git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git -b r1.12
cd tensorflow
vim WORKSPACE 
```
# 1. modify WORKSPACE:
```
android_sdk_repository(
   name = ""androidsdk"",
   api_level = 29,
   build_tools_version = ""29.0.0"",
   path = ""/Users/gongjia/Library/Android/sdk"",
)

# Android NDK r12b is recommended (higher may cause issues with Bazel)
android_ndk_repository(
   name=""androidndk"",
   # path=""/Users/gongjia/Library/Android/sdk/ndk-bundle"",
   path=""/Users/gongjia/Library/Android/sdk/android-ndk-r14b"",
   api_level=21
)
```
# 2. add in tensorflow/contrib/lite/BUILD
```
cc_binary(
    name = ""libtensorflowLite.so"",
    linkopts = [""-shared"", ""-Wl,-soname=libtensorflowLite.so""],
    visibility = [""//visibility:public""],
    linkshared = 1,
    copts = tflite_copts(),
    deps = [
        "":framework"",
        ""//tensorflow/contrib/lite/kernels:builtin_ops"",
    ],
)
```
# 3. exec: 
```
bazel build -c opt //tensorflow/contrib/lite:libtensorflowLite.so   --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=""-std=c++11"" --verbose_failures
```

# 4. result:
```
ERROR: /Users/gongjia/Documents/github/tensorflow/WORKSPACE:3:1: name 'http_archive' is not defined
ERROR: Error evaluating WORKSPACE file
ERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error loading package 'external': Could not load //external package
ERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error loading package 'external': Could not load //external package
INFO: Elapsed time: 0.082s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
```

# 5. Info:
tensorflow: r1.12
configure: default
macOS Mojave 10.14.3
python:2.7
bazel: 0.24.0"
27187,tflite InterpreterBuilder error and crash,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution : MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
OPPO A57,  M6
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.5
- Bazel version (if compiling from source): 0.20.0-homebrew
- GCC/Compiler version (if compiling from source): 8.2.0 (Homebrew GCC 8.2.0)
- CUDA/cuDNN version: no
- GPU model and memory: no
- ndk version: 16

**Describe the current behavior**
I build jni so file to use on android, but there will be a unscheduled SIGSEGV bug when I loaded the tflite model to build the interpreter. And the app will crash.

**Other info / logs**

    #00 pc 00103e5c libtensorflowlite.so tflite::GetRegistrationFromOpCode(tflite::OperatorCode const*, tflite::OpResolver const&, tflite::ErrorReporter*, _TfLiteRegistration const**) [armeabi-v7a]
    #01 pc 000f9251 libtensorflowlite.so tflite::InterpreterBuilder::operator()(std::__ndk1::unique_ptr<tflite::Interpreter, std::__ndk1::default_delete<tflite::Interpreter> >*, int) [armeabi-v7a]
    #02 pc 0001f44d /data/app/com.ziipin.softkeyboard-2/lib/arm/libsmallime.so (tensorflow::tflite_inference::Inference::LoadLSTMModel(char const*)+92) [armeabi-v7a]


"
27186,Wrong function referenced in doc,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.13
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn


**Describe the documentation issue**
On the warning about this function being deprecated, it is suggested to use instead use keras.layers.RNN. However, the function is called tf.keras.layers.RNN (https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN) 

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
27182,"[TF 2.0 alpha] Tutorial ""Loading Data"" wrong link path ","<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: Tensorflow 2.0 alpha
- Doc Link: 
[Load images with tf.data](https://www.tensorflow.org/alpha/tutorials/load_data/images)
[Using TFRecords and tf.Example](https://www.tensorflow.org/alpha/tutorials/load_data/tf_records)

**Describe the documentation issue**
[Load images with tf.data](https://www.tensorflow.org/alpha/tutorials/load_data/images) and [Using TFRecords and tf.Example](https://www.tensorflow.org/alpha/tutorials/load_data/tf_records) have wrong links to Colab and github.

They should link to 
https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/load_data/images.ipynb
https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/load_data/tf_records.ipynb

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
