Issue Number,Issue Title,Issue Body
8134,Download of test data not working,"The get started part has a section MNIST for ML Beginners. In this tutorial you should download the test data via:

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)

Unfortunately that's not working because of an connection error. "
8133,"About using tf.contrib.layers.optimize_loss with optimizer=""Momentum"" as arg ","I'm trying to implement a train op using tf.contrib.layers.optimize_loss. I would like to use the optimizer train.MomentumOptimizer . One way to do it is to pass the arg ""Momentum"" to the parameter optimizer like this:

```python
    train_op = tf.contrib.layers.optimize_loss(
        loss=loss,
        global_step=tf.contrib.framework.get_global_step(),
        learning_rate=0.001,
        optimizer=""Momentum"")
```
It turns out I get an error:
```
__init__() missing 1 required positional argument: 'momentum'
```
I think the reason is that when the function optimize_loss wants to instanciate a new optimizer by looking into ```OPTIMIZER_CLS_NAMES``` it calls  tensorflow\contrib\layers\python\layers\optimizers.py"", line 195, in optimize_loss : 
```python
opt = OPTIMIZER_CLS_NAMES[optimizer](learning_rate=lr)
```
However, among the whole list of optimizers in ```OPTIMIZER_CLS_NAMES```, train.MomentumOptimizer is the only one that needs a second mandatory parameter in its ```__init__``` method:
```python
class MomentumOptimizer(optimizer.Optimizer):
  """"""Optimizer that implements the Momentum algorithm.

  @@__init__
  """"""

  def __init__(self, learning_rate, momentum,
               use_locking=False, name=""Momentum"", use_nesterov=False):
```
... whereas the other ones don't. Am I missing something?
Thank you in advance"
8132,One shot learning example,It might be a good thing to give an example of one shot learning :)
8131,Better document when to use tf.sparse_tensor_dense_matmul vs embedding lookup,"`tf.sparse_tensor_dense_matmul` has a regular (non-sparse) tensor gradient ([source](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/sparse_grad.py#L153)):

```python
  # gradient w.r.t. dense
  b_grad = sparse_ops.sparse_tensor_dense_matmul(sp_t, grad,
                                                 adjoint_a=not adj_a)
```

where `sparse_ops.sparse_tensor_dense_matmul` returns a regular tensor.

The [documentation](https://www.tensorflow.org/api_docs/python/tf/sparse_tensor_dense_matmul) does not note this and it may be confusing for people coming from e.g. NLP where you frequently work with dense-sparse multiplications with sparse gradients.  

In many cases where you might naively think you want to use `tf.sparse_tensor_dense_matmul` you actually want to use [`tf.nn.embedding_lookup_sparse`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup_sparse), even if your task has nothing to do with embeddings.  It would be helpful if there were a cross-reference in the documentation to guide users in the right direction.
"
8130,TensorFlow 1.0 error messages point to broken link,"When `import tensorflow` fails in TF 1.0, we print an error message containing the following text:

```
Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error

for some common reasons and solutions. Include the entire stack trace above this error message when asking for help.
```

Unfortunately, https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error is a broken link. I realize this is probably because we've moved all the docs around, but is there some way we could leave a breadcrumb there for our unfortunate users?

The proper link should include the `r1.0` branch: https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/g3doc/get_started/os_setup.md#import_error
"
8129,No event files found within logdir in tensorboard,"Dear Sir/mam

I confuse how to display using tensorboard. here is my program
graph = ""d://""
import tensorflow as tf

# Build our graph nodes, starting from the inputs
a = tf.constant(5, name=""input_a"")
b = tf.constant(3, name=""input_b"")
c = tf.multiply(a,b, name=""mul_c"")
d = tf.add(a,b, name=""add_d"")
e = tf.add(c,d, name=""add_e"")

# Open up a TensorFlow Session
sess = tf.Session()

# Execute our output node, using our Session
output = sess.run(e)

# Open a TensorFlow SummaryWriter to write our graph to disk
writer = tf.summary.FileWriter(graph, sess.graph)

# Close our SummaryWriter and Session objects
writer.close()
sess.close()

the events file has been created in drive D (i'm using windows 8.1, and tensorflow 1.0). and i found it when using explorer

When I type 
tensorboard --inspect --logdir =""d:\\""
tensorboard --inspect --logdir =""d:\""
tensorboard --inspect --logdir =""d://""
tensorboard --inspect --logdir =""d:/""

tensorboard --inspect --logdir ='d:\\'
tensorboard --inspect --logdir ='d:\'
tensorboard --inspect --logdir ='d://'
tensorboard --inspect --logdir ='d:/'

the result are : No event files found within logdir, 

pls tell me why this happened.

Thx
"
8127,AttributeError: module 'tensorflow' has no attribute 'streaming_accuracy',"```
accuracy = tf.streaming_accuracy (y_pred,y_true,name='acc')
recall = tf.streaming_recall (y_pred,y_true,name='acc')
precision = tf.streaming_precision(y_pred,y_true,name='acc')
confusion = tf.confuson_matrix(Labels, y_pred,num_classes=10,dtype=tf.float32,name='conf')
```
for the above code, I have received the same error in past few days. 
Is the syntax same as its in the API documentation?"
8126,yann.lecun.com/exdb/mnist/ down,"The website of Yann LeCun is down at the moment. 
Therefore the MNIST-Script for downloading the files doesn't work either. (-> learn/python/learn/datasets/mnist.py won't work)
Are there any mirrors?"
8124,"Build failure, XLA, OS/X","I've written OS/X, but I'm pretty sure this is going to fail on all platforms:

```
tensorflow/compiler/tf2xla/xla_compiler.cc:228:44: error: no type named 'size_typ' in 'std::__1::vector<tensorflow::XlaCompiler::Argument, std::__1::allocator<tensorflow::XlaCompiler::Argument> >'; did you mean 'size_type'?
  for (std::vector<XlaCompiler::Argument>::size_typ i = 0;
       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~
                                           size_type
/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/vector:470:54: note: 'size_type' declared here
    typedef typename __base::size_type               size_type;
                                                     ^
tensorflow/compiler/tf2xla/xla_compiler.cc:480:33: error: no member named 'VariableWrite' in 'tensorflow::XlaCompiler'
  for (std::vector<XlaCompiler::VariableWrite>::size_type  i = 0;
                   ~~~~~~~~~~~~~^
tensorflow/compiler/tf2xla/xla_compiler.cc:480:47: error: no template named 'size_type' in the global namespace; did you mean 'std::__size_type'?
  for (std::vector<XlaCompiler::VariableWrite>::size_type  i = 0;
                                              ^~~~~~~~~~~
                                              std::__size_type
/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/memory:1090:8: note: 'std::__size_type' declared here
struct __size_type
       ^
tensorflow/compiler/tf2xla/xla_compiler.cc:480:49: error: use of class template '::__size_type' requires template arguments
  for (std::vector<XlaCompiler::VariableWrite>::size_type  i = 0;
                                                ^
/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/memory:1090:8: note: template is declared here
struct __size_type
       ^
4 errors generated.

```
"
8123,"Build failure, XLA, OS/X","Maybe OS/X only.   I have Apple clang 8.0.0 (clang-800.0.42.1)

```
tensorflow/compiler/xla/service/allocation_tracker.cc:178:54: error: non-constant-expression cannot be narrowed from type 'std::vector<se::DeviceMemoryBase>::size_type' (aka 'unsigned long') to 'long long' in initializer list [-Wc++11-narrowing]
        ShapeUtil::GetSubshape(allocation->shape(), {i}),
                                                     ^
tensorflow/compiler/xla/service/allocation_tracker.cc:178:54: note: insert an explicit cast to silence this issue
        ShapeUtil::GetSubshape(allocation->shape(), {i}),
                                                     ^
                                                     static_cast<long long>( )

```

"
8122,A Bug Report about TensorBoard,"# Bug Description:
On win10 platform, you use command __tensorboard --logdir=path/to/logdir__ where located to __another driver__ which don't include the logdir. __TensorBoard will think the logdir in that driver__.
# Example:
In VS Prompt:

1.  **F**:\Program Files (x86)\Microsoft Visual Studio 14.0>tensorboard --logdir=**G:\machine_learning\models\Alexnet_tf\log_** --debug

__cmd output__:
INFO:tensorflow:TensorBoard is in debug mode.
INFO:tensorflow:Starting TensorBoard in directory F:\Program Files (x86)\Microsoft Visual Studio 14.0
INFO:tensorflow:**TensorBoard path_to_run is: {'F:\\machine_learning\\models\\Alexnet_tf\\log_': 'G'}**
INFO:tensorflow:Event Multiplexer initializing.
INFO:tensorflow:Event Multiplexer done initializing
INFO:tensorflow:TensorBoard reload process beginning
INFO:tensorflow:Starting AddRunsFromDirectory: F:\machine_learning\models\Alexnet_tf\log_
INFO:tensorflow:Done with AddRunsFromDirectory: F:\machine_learning\models\Alexnet_tf\log_
INFO:tensorflow:TensorBoard reload process: Reload the whole Multiplexer
INFO:tensorflow:Beginning EventMultiplexer.Reload()
INFO:tensorflow:Finished with EventMultiplexer.Reload()
INFO:tensorflow:TensorBoard done reloading. **Load took 0.000 secs**
INFO:tensorflow:TensorBoard is tag: b'41'
Starting TensorBoard b'41' on port 6006
(You can navigate to http://192.168.1.202:6006)

__Nothing you can see in Chrome, a blank page.__

2.  **G**:\>tensorboard --logdir=**G:\machine_learning\models\Alexnet_tf\log_** --debug

__cmd output__:
INFO:tensorflow:TensorBoard is in debug mode.
INFO:tensorflow:Starting TensorBoard in directory G:\
INFO:tensorflow:**TensorBoard path_to_run is: {'G:\\machine_learning\\models\\Alexnet_tf\\log_': 'G'}**
INFO:tensorflow:Event Multiplexer initializing.
INFO:tensorflow:Event Multiplexer done initializing
INFO:tensorflow:TensorBoard reload process beginning
INFO:tensorflow:Starting AddRunsFromDirectory: G:\machine_learning\models\Alexnet_tf\log_
INFO:tensorflow:Adding events from directory G:\machine_learning\models\Alexnet_tf\log_
INFO:tensorflow:Constructing EventAccumulator for G:\machine_learning\models\Alexnet_tf\log_
INFO:tensorflow:Done with AddRunsFromDirectory: G:\machine_learning\models\Alexnet_tf\log_
INFO:tensorflow:TensorBoard reload process: Reload the whole Multiplexer
INFO:tensorflow:Beginning EventMultiplexer.Reload()
DEBUG:tensorflow:Opening a record reader pointing at G:\machine_learning\models\Alexnet_tf\log_\events.out.tfevents.1488779830.ubuntu-Default-string
INFO:tensorflow:TensorBoard is tag: b'41'
Starting TensorBoard b'41' on port 6006
(You can navigate to http://192.168.1.202:6006)
DEBUG:tensorflow:No more events in G:\machine_learning\models\Alexnet_tf\log_\events.out.tfevents.1488779830.ubuntu-Default-string
INFO:tensorflow:No path found after G:\machine_learning\models\Alexnet_tf\log_\events.out.tfevents.1488779830.ubuntu-Default-string
INFO:tensorflow:Finished with EventMultiplexer.Reload()
INFO:tensorflow:TensorBoard done reloading. **Load took 4.234 secs**

__Now Scalars page has loss graph defined.__

# Question:
Why TensorBoard reads event-file depend on the command where inputed(As shown abone, logdir in G: but go to F: ? It shouldn't be."
8121,tensorflow.contrib.distributions.python.ops.distribution_util' has no attribute 'get_logits_and_probs',"windows 10
upgraded with pip install to tensorflow 1.0

I got this error after trying to use Binomial.py

 self._logits, self._probs = distribution_util.get_logits_and_probs(
AttributeError: module 'tensorflow.contrib.distributions.python.ops.distribution_util' has no attribute 'get_logits_and_probs'

"
8120,Inconsistency in Variable Creation Methods,"Currently it is not possible to scope all variables cleanly when using RNN cells and Optimizers. RNN cells create variables with tf.get_variable and optimizers create variables with tf.Variable. These two methods don't get along, creating messy variable names when variable scopes are handled differently by the two methods.

Script for testing different variable creation and scoping combinations.
```python
# -*- coding: utf-8 -*-

import tensorflow as tf


def build_loss(use_get_variable=True):
    y = tf.placeholder('int32', [None], name='y')
    x = tf.placeholder('float32', shape=[None, None, 10], name='x')
    cell = tf.contrib.rnn.GRUCell(128)  # RNN cell
    if use_get_variable:
        # Create variable with tf.get_variable
        w = tf.get_variable('w', dtype='float32', initializer=tf.random_normal([128, 10]))
    else:
        # Create variable with tf.Variable
        w = tf.Variable(initial_value=tf.random_normal([128, 10]), dtype='float32', name='w')
    rnn_out, _ = tf.nn.dynamic_rnn(cell, x, dtype='float32', time_major=False)  # RNN
    rnn_out = rnn_out[:, -1, :]  # Last timestep
    rnn_out = tf.matmul(rnn_out, w)  # Output layer projection
    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=rnn_out))  # Cross entropy
    return loss_op


def inspect(use_get_variable=True, scope_optimizer=True, re_enter_scope=None, optimizer=tf.train.AdamOptimizer):
    scope = 'scope'
    if re_enter_scope:
        with tf.variable_scope(scope) as variable_scope:  # Capture variable scope
            if re_enter_scope == 'object':
                # Save captured object for re-entering scope with captured variable scope object
                scope = variable_scope
            elif re_enter_scope == 'original_name_scope':
                # Save original name scope of captured variable scope for re-entering the scope with original name scope
                scope = variable_scope.original_name_scope
    with tf.variable_scope(scope):  # (Re-)enter scope
        loss_op = build_loss(use_get_variable=use_get_variable)
        if scope_optimizer:
            # Create optimizer in the variable scope
            train_op = optimizer(0.1).minimize(loss_op)
    if not scope_optimizer:
        # Create optimizer outside of variable scope
        train_op = optimizer(0.1).minimize(loss_op)
    with tf.Session() as sess:
        # Initialize variables
        sess.run(tf.global_variables_initializer())

    description = 'optimizer ' + ('NOT' if not scope_optimizer else 'IS') + ' scoped, '
    if not re_enter_scope:
        description += 'using ORIGINAL scope, '
    elif re_enter_scope == 'object':
        description += 're-entering scope with OBJECT, '
    elif re_enter_scope == 'original_name_scope':
        description += 're-entering scope with ORIGINAL_NAME_SCOPE, '
    description += 'variables created with ' + ('tf.get_variable' if use_get_variable else 'tf.Variable')
    print(description)

    print('\n'.join(['  '+var.name for var in tf.global_variables()]))
    print()

    tf.reset_default_graph()


def main():
    # Ideally scope everything with re-entered scope, leads to double scoping of gradient variables and having duplicate
    # scope (with unique name) for Adam optimizer beta power variables
    inspect(use_get_variable=True, scope_optimizer=True, re_enter_scope='object')
    # Not scoping optimizer fixes double scoping of RNN variables but leads to having AdamOptimizer beta_powers not
    # scoped
    inspect(use_get_variable=True, scope_optimizer=False, re_enter_scope='object')
    # Using tf.Variable for variable creation when re-entering scope with scope object leads to creation of duplicate
    # scope (with unique name) for non-RNN variables
    inspect(use_get_variable=False, scope_optimizer=False, re_enter_scope='object')
    # Changing scope re-entering method to variable_scope.original_name_scope fixes the problem of duplicate scope
    # when using tf.Variable for variable creation, but creates additional problem of having double slashes in
    # scope hierarchy path with RNN variables.
    inspect(use_get_variable=False, scope_optimizer=False, re_enter_scope='original_name_scope')
    # Similar problem is observed when using tf.get_variable for variable creation and re-entering scope with
    # original_name_scope but this time double slashes are observed also with non-RNN variables.
    inspect(use_get_variable=True, scope_optimizer=False, re_enter_scope='original_name_scope')
    # Scoping optimizer without re-entering the scope fixes scoping of Adam optimizer beta power variables but does not
    # fix the problem of having double scopes for RNN variables. However re-entering scopes is desired for OOP.
    inspect(use_get_variable=False, scope_optimizer=True, re_enter_scope=False)
    # Using tf.get_variable for variable creation won't help since Optimizer creates variables with tf.Variable and RNN
    # cells create variables with tf.get_variable, these two methods don't get along
    inspect(use_get_variable=True, scope_optimizer=True, re_enter_scope=False)
    # Swapping AdamOptimizer for RMSPropOptimizer fixes the problem with AdamOptimizer's beta power variables
    inspect(use_get_variable=True, scope_optimizer=False, re_enter_scope='object', optimizer=tf.train.RMSPropOptimizer)


if __name__ == '__main__':
    main()
```
outputs
```
optimizer IS scoped, re-entering scope with OBJECT, variables created with tf.get_variable
  scope/w:0
  scope/rnn/gru_cell/gates/weights:0
  scope/rnn/gru_cell/gates/biases:0
  scope/rnn/gru_cell/candidate/weights:0
  scope/rnn/gru_cell/candidate/biases:0
  scope_1/beta1_power:0
  scope_1/beta2_power:0
  scope/scope/w/Adam:0
  scope/scope/w/Adam_1:0
  scope/scope/rnn/gru_cell/gates/weights/Adam:0
  scope/scope/rnn/gru_cell/gates/weights/Adam_1:0
  scope/scope/rnn/gru_cell/gates/biases/Adam:0
  scope/scope/rnn/gru_cell/gates/biases/Adam_1:0
  scope/scope/rnn/gru_cell/candidate/weights/Adam:0
  scope/scope/rnn/gru_cell/candidate/weights/Adam_1:0
  scope/scope/rnn/gru_cell/candidate/biases/Adam:0
  scope/scope/rnn/gru_cell/candidate/biases/Adam_1:0

optimizer NOT scoped, re-entering scope with OBJECT, variables created with tf.get_variable
  scope/w:0
  scope/rnn/gru_cell/gates/weights:0
  scope/rnn/gru_cell/gates/biases:0
  scope/rnn/gru_cell/candidate/weights:0
  scope/rnn/gru_cell/candidate/biases:0
  beta1_power:0
  beta2_power:0
  scope/w/Adam:0
  scope/w/Adam_1:0
  scope/rnn/gru_cell/gates/weights/Adam:0
  scope/rnn/gru_cell/gates/weights/Adam_1:0
  scope/rnn/gru_cell/gates/biases/Adam:0
  scope/rnn/gru_cell/gates/biases/Adam_1:0
  scope/rnn/gru_cell/candidate/weights/Adam:0
  scope/rnn/gru_cell/candidate/weights/Adam_1:0
  scope/rnn/gru_cell/candidate/biases/Adam:0
  scope/rnn/gru_cell/candidate/biases/Adam_1:0

optimizer NOT scoped, re-entering scope with OBJECT, variables created with tf.Variable
  scope_1/w:0
  scope/rnn/gru_cell/gates/weights:0
  scope/rnn/gru_cell/gates/biases:0
  scope/rnn/gru_cell/candidate/weights:0
  scope/rnn/gru_cell/candidate/biases:0
  beta1_power:0
  beta2_power:0
  scope_1/w/Adam:0
  scope_1/w/Adam_1:0
  scope/rnn/gru_cell/gates/weights/Adam:0
  scope/rnn/gru_cell/gates/weights/Adam_1:0
  scope/rnn/gru_cell/gates/biases/Adam:0
  scope/rnn/gru_cell/gates/biases/Adam_1:0
  scope/rnn/gru_cell/candidate/weights/Adam:0
  scope/rnn/gru_cell/candidate/weights/Adam_1:0
  scope/rnn/gru_cell/candidate/biases/Adam:0
  scope/rnn/gru_cell/candidate/biases/Adam_1:0

optimizer NOT scoped, re-entering scope with ORIGINAL_NAME_SCOPE, variables created with tf.Variable
  scope/w:0
  scope//rnn/gru_cell/gates/weights:0
  scope//rnn/gru_cell/gates/biases:0
  scope//rnn/gru_cell/candidate/weights:0
  scope//rnn/gru_cell/candidate/biases:0
  beta1_power:0
  beta2_power:0
  scope/w/Adam:0
  scope/w/Adam_1:0
  scope//rnn/gru_cell/gates/weights/Adam:0
  scope//rnn/gru_cell/gates/weights/Adam_1:0
  scope//rnn/gru_cell/gates/biases/Adam:0
  scope//rnn/gru_cell/gates/biases/Adam_1:0
  scope//rnn/gru_cell/candidate/weights/Adam:0
  scope//rnn/gru_cell/candidate/weights/Adam_1:0
  scope//rnn/gru_cell/candidate/biases/Adam:0
  scope//rnn/gru_cell/candidate/biases/Adam_1:0

optimizer NOT scoped, re-entering scope with ORIGINAL_NAME_SCOPE, variables created with tf.get_variable
  scope//w:0
  scope//rnn/gru_cell/gates/weights:0
  scope//rnn/gru_cell/gates/biases:0
  scope//rnn/gru_cell/candidate/weights:0
  scope//rnn/gru_cell/candidate/biases:0
  beta1_power:0
  beta2_power:0
  scope//w/Adam:0
  scope//w/Adam_1:0
  scope//rnn/gru_cell/gates/weights/Adam:0
  scope//rnn/gru_cell/gates/weights/Adam_1:0
  scope//rnn/gru_cell/gates/biases/Adam:0
  scope//rnn/gru_cell/gates/biases/Adam_1:0
  scope//rnn/gru_cell/candidate/weights/Adam:0
  scope//rnn/gru_cell/candidate/weights/Adam_1:0
  scope//rnn/gru_cell/candidate/biases/Adam:0
  scope//rnn/gru_cell/candidate/biases/Adam_1:0

optimizer IS scoped, using ORIGINAL scope, variables created with tf.Variable
  scope/w:0
  scope/rnn/gru_cell/gates/weights:0
  scope/rnn/gru_cell/gates/biases:0
  scope/rnn/gru_cell/candidate/weights:0
  scope/rnn/gru_cell/candidate/biases:0
  scope/beta1_power:0
  scope/beta2_power:0
  scope/scope/w/Adam:0
  scope/scope/w/Adam_1:0
  scope/scope/rnn/gru_cell/gates/weights/Adam:0
  scope/scope/rnn/gru_cell/gates/weights/Adam_1:0
  scope/scope/rnn/gru_cell/gates/biases/Adam:0
  scope/scope/rnn/gru_cell/gates/biases/Adam_1:0
  scope/scope/rnn/gru_cell/candidate/weights/Adam:0
  scope/scope/rnn/gru_cell/candidate/weights/Adam_1:0
  scope/scope/rnn/gru_cell/candidate/biases/Adam:0
  scope/scope/rnn/gru_cell/candidate/biases/Adam_1:0

optimizer IS scoped, using ORIGINAL scope, variables created with tf.get_variable
  scope/w:0
  scope/rnn/gru_cell/gates/weights:0
  scope/rnn/gru_cell/gates/biases:0
  scope/rnn/gru_cell/candidate/weights:0
  scope/rnn/gru_cell/candidate/biases:0
  scope/beta1_power:0
  scope/beta2_power:0
  scope/scope/w/Adam:0
  scope/scope/w/Adam_1:0
  scope/scope/rnn/gru_cell/gates/weights/Adam:0
  scope/scope/rnn/gru_cell/gates/weights/Adam_1:0
  scope/scope/rnn/gru_cell/gates/biases/Adam:0
  scope/scope/rnn/gru_cell/gates/biases/Adam_1:0
  scope/scope/rnn/gru_cell/candidate/weights/Adam:0
  scope/scope/rnn/gru_cell/candidate/weights/Adam_1:0
  scope/scope/rnn/gru_cell/candidate/biases/Adam:0
  scope/scope/rnn/gru_cell/candidate/biases/Adam_1:0

optimizer NOT scoped, re-entering scope with OBJECT, variables created with tf.get_variable
  scope/w:0
  scope/rnn/gru_cell/gates/weights:0
  scope/rnn/gru_cell/gates/biases:0
  scope/rnn/gru_cell/candidate/weights:0
  scope/rnn/gru_cell/candidate/biases:0
  scope/w/RMSProp:0
  scope/w/RMSProp_1:0
  scope/rnn/gru_cell/gates/weights/RMSProp:0
  scope/rnn/gru_cell/gates/weights/RMSProp_1:0
  scope/rnn/gru_cell/gates/biases/RMSProp:0
  scope/rnn/gru_cell/gates/biases/RMSProp_1:0
  scope/rnn/gru_cell/candidate/weights/RMSProp:0
  scope/rnn/gru_cell/candidate/weights/RMSProp_1:0
  scope/rnn/gru_cell/candidate/biases/RMSProp:0
  scope/rnn/gru_cell/candidate/biases/RMSProp_1:0
```

Workaround for clean scoping is to create optimizer op outside of the variable scope and using eg. `RMSPropOptimizer`. However this is confusing behaviour, ideally everything should be able to be scoped. Unfortunately there is nothing to get variable names clean when using `AdamOptimizer`. `AdamOptimizer` creates variables for beta powers with `tf.Variable`.

All of the mentioned problems would probably be fixed by using `tf.get_variable` everywhere. Until (and if) this change happens users should be instructed to use `tf.get_variable` only, creating optimizer outside of variable scope and avoid using `AdamOptimizer` if clean scoping is desired. Double scoping of RNN gradient variables would be tolerable but having several nested classes implementing different models with their own variable scopes lead to extremely messy variable names making debugging more difficult.

Tested on Ubuntu 16.04 with Tensorflow 1.0.0 installed with pip.

Related issues:
#5786 
#6189 
#6007 
"
8119,.,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8118,Setup instructions link broken,"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md

The above link is broken. Loading leads to a 404 error."
8117,could not find cuDevicePrimaryCtxSetFlags,"when i execute sess = tf.Session() or sess = tf.InteractiveSession(), it shows problems below: 
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
F tensorflow/stream_executor/cuda/cuda_driver.cc:94] Check failed: s.ok() could not find cuDevicePrimaryCtxSetFlags in libcuda DSO; dlerror: /usr/lib/x86_64-linux-gnu/libcuda.so.1: undefined symbol: cuDevicePrimaryCtxSetFlags
Aborted (core dumped)
how can i fix this , thank you !"
8116,Errno 110 occured when running all python files in /examples/tutorials/mnist/ directory.,"davidtest@CaffeVM:~/tensorflow/tensorflow/tensorflow/examples/tutorials/mnist$ python fully_connected_feed.py 
Traceback (most recent call last):
  File ""fully_connected_feed.py"", line 277, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""fully_connected_feed.py"", line 222, in main
    run_training()
  File ""fully_connected_feed.py"", line 120, in run_training
    data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 211, in read_data_sets
    SOURCE_URL + TRAIN_IMAGES)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py"", line 208, in maybe_download
    temp_file_name, _ = urlretrieve_with_retry(source_url)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py"", line 165, in wrapped_fn
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py"", line 190, in urlretrieve_with_retry
    return urllib.request.urlretrieve(url, filename)
  File ""/usr/lib/python2.7/urllib.py"", line 94, in urlretrieve
    return _urlopener.retrieve(url, filename, reporthook, data)
  File ""/usr/lib/python2.7/urllib.py"", line 240, in retrieve
    fp = self.open(url, data)
  File ""/usr/lib/python2.7/urllib.py"", line 208, in open
    return getattr(self, name)(url)
  File ""/usr/lib/python2.7/urllib.py"", line 345, in open_http
    h.endheaders(data)
  File ""/usr/lib/python2.7/httplib.py"", line 1013, in endheaders
    self._send_output(message_body)
  File ""/usr/lib/python2.7/httplib.py"", line 864, in _send_output
    self.send(msg)
  File ""/usr/lib/python2.7/httplib.py"", line 826, in send
    self.connect()
  File ""/usr/lib/python2.7/httplib.py"", line 807, in connect
    self.timeout, self.source_address)
  File ""/usr/lib/python2.7/socket.py"", line 571, in create_connection
    raise err
"
8114,image_retrainning can not run,"my tensorflow  version is 1.0.0
I follow the image_retraining.md
1.    curl -O http://download.tensorflow.org/example_images/flower_photos.tgz
tar xzf flower_photos.tgz
2.   bazel build --config opt tensorflow/examples/image_retraining:retrain

but when I run
 bazel-bin/tensorflow/examples/image_retraining/retrain
there is a errorLast login: Mon Mar  6 14:46:11 on ttys000
wangjingsideMacBook-Pro:~ ginthva$ cd tensorflow
wangjingsideMacBook-Pro:tensorflow ginthva$ bazel build --config opt tensorflow/examples/image_retraining:retrain
INFO: Found 1 target...
Target //tensorflow/examples/image_retraining:retrain up-to-date:
  bazel-bin/tensorflow/examples/image_retraining/retrain
INFO: Elapsed time: 0.374s, Critical Path: 0.00s
wangjingsideMacBook-Pro:tensorflow ginthva$ bazel-bin/tensorflow/examples/image_retraining/retrain 
Traceback (most recent call last):
  File ""/Users/ginthva/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 79, in <module>
    import tensorflow as tf
  File ""/Users/ginthva/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/Users/ginthva/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 124, in <module>
    from tensorflow.python.platform import test
  File ""/Users/ginthva/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 50, in <module>
    import mock                # pylint: disable=g-import-not-at-top,unused-import
ImportError: No module named mock


I also uninstall mock and reinstall, but it doesn't work. what should I  do?"
8113,I can not run my code Tensorflow,"I working on `Ubuntu 14.04` ,i wrote a code for Recognition of letters whith `Tensorflow V 0.11` ,
i'm creat a code source for uses the model `LeNet5` 
my code source : 

`

> 
>     import PIL
>     
>     import numpy
>     import tensorflow as tf
>     # from tensorflow.examples.tutorials.mnist import input_data
>     import Input as input_data
>     from tensorflow.python.framework.importer import import_graph_def
>     
>     from Resize import Resize_img
>     
>     # these functions to optimize the accurancy of the mnist training
>     #from imp_image import imp_img
>     import scipy.misc
>     
>     
>     def weight_variable(shape):
>         initial = tf.truncated_normal(shape, stddev=0.1)
>         return tf.Variable(initial)
>     
>     
>     def bias_variable(shape):
>         initial = tf.constant(0.1, shape=shape)
>         return tf.Variable(initial)
>     
>     
>     def conv2d(x, W):
>         return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')
>     
>     
>     def max_pool_2x2(x):
>         return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
>     
>     
>     # ============================================================ End Functions part
>     
>     # mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)
>     
>     class MNIST:
>     
>         def __init__(self):
>     
>             # Open the compuation session
>             self.sess = tf.InteractiveSession()
>             # Load the network
>             self.Deep_Network()
>     
>         def Deep_Network(self):
>     
>             # nodes for the input images and target output classes.
>             # supervised classifier
>             self.x = tf.placeholder(tf.float32, shape=[None, 784])
>             self.y_ = tf.placeholder(tf.float32, shape=[None, 10])
>     
>             # First convolutionanal Layer =====================================
>             # It will consist of convolution, followed by max pooling
>             # The convolutional will compute 32 features for each 5x5 patch.
>             self.W_conv1 = weight_variable([5, 5, 1, 32])
>             self.b_conv1 = bias_variable([32])
>     
>             # To apply the layer, we first reshape x to a 4d tensor,
>             #  with the second and third dimensions corresponding to image width and height,
>             #  and the final dimension corresponding to the number of color channels.
>             self.x_image = tf.reshape(self.x, [-1, 28, 28, 1])
>     
>             # We then convolve x_image with the weight tensor, add the bias, apply the ReLU function, and finally max pool.
>             self.h_conv1 = tf.nn.relu(conv2d(self.x_image, self.W_conv1) + self.b_conv1)
>             self.h_pool1 = max_pool_2x2(self.h_conv1)
>     
>             # Second Convolutional Layer =====================================
>     
>             # In order to build a deep network, we stack several layers of this type.
>             # The second layer will have 64 features for each 5x5 patch.
>     
>             self.W_conv2 = weight_variable([5, 5, 32, 64])
>             self.b_conv2 = bias_variable([64])
>     
>             self.h_conv2 = tf.nn.relu(conv2d(self.h_pool1, self.W_conv2) + self.b_conv2)
>             self.h_pool2 = max_pool_2x2(self.h_conv2)
>     
>             # Densely Connected Layer
>     
>             # Now that the image size has been reduced to 7x7, we add a fully-connected layer with 1024 neurons
>             # to allow processing on the entire image. We reshape the tensor from the pooling layer into
>             # a batch of vectors, multiply by a weight matrix, add a bias, and apply a ReLU.
>     
>             self.W_fc1 = weight_variable([7 * 7 * 64, 1024])
>             self.b_fc1 = bias_variable([1024])
>     
>             self.h_pool2_flat = tf.reshape(self.h_pool2, [-1, 7 * 7 * 64])
>             self.h_fc1 = tf.nn.relu(
>                 tf.matmul(self.h_pool2_flat, self.W_fc1) + self.b_fc1)  # ReLu Computes rectified linear: max(features, 0).
>     
>             # Dropout
>     
>             self.keep_prob = tf.placeholder(tf.float32)
>             self.h_fc1_drop = tf.nn.dropout(self.h_fc1, self.keep_prob)
>     
>             # Readout Layer ========================================
>             # Finally, we add a softmax layer, just like for the one layer softmax regression above.
>     
>             self.W_fc2 = weight_variable([1024, 10])
>             self.b_fc2 = bias_variable([10])
>     
>             self.y_conv = tf.nn.softmax(tf.matmul(self.h_fc1_drop, self.W_fc2) + self.b_fc2)
>             self.cross_entropy = -tf.reduce_sum(self.y_ * tf.log(self.y_conv))
>             self.correct_prediction = tf.equal(tf.argmax(self.y_conv, 1), tf.argmax(self.y_, 1))
>             self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))
>     
>         def Prediction(self, imageName):
>     
>             # Load the trained model
>             ' Restore the model '
>             'here i should create the model saver'
>             Saved_model_dir = '/home/brm17/Desktop/PFE/'
>             saver = tf.train.Saver()
>             ckpt = tf.train.get_checkpoint_state(Saved_model_dir)
>     
>             'verifie if the saved model exists or not!'
>             if ckpt and ckpt.model_checkpoint_path:
>                 saver.restore(self.sess, ckpt.model_checkpoint_path)
>             else:
>                 print '# No saved model found!'
>                 exit()  # exit the prgm
>     
>             # image_test = 'number-3.jpg'
>             ResizedImage = Resize_img(imageName)
>     
>             ImageInput = ResizedImage.mnist_image_input.reshape(1, -1)
>     
>             print 'Predection > ', tf.argmax(self.y_conv, 1).eval(feed_dict={self.x: ImageInput, self.keep_prob: 1.0})
>     
>         # print(""test accuracy %g""%accuracy.eval(feed_dict={x: myTestImg, y_: myLabel, keep_prob: 1.0}))
>     
>     
>     def main():
>         image = '/home/brm17/Desktop/PFE/n2.jpeg'
>         model = MNIST()
>         model.Prediction(image)
>     
>     if __name__ == ""__main__"":
>         main()
>     

    `
    
if i run this code , he print the error :

>     brm17@Brahim:~/Desktop/PFE$ python LeNet5.py 
>     Traceback (most recent call last):
>       File ""LeNet5.py"", line 137, in <module>
>         model.Prediction(image)
>       File ""LeNet5.py"", line 120, in Prediction
>         saver.restore(self.sess, ckpt.model_checkpoint_path)
>       File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1129, in restore
>         {self.saver_def.filename_tensor_name: save_path})
>       File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 710, in run
>         run_metadata_ptr)
>       File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 908, in _run
>         feed_dict_string, options, run_metadata)
>       File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 958, in _do_run
>         target_list, options, run_metadata)
>       File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 978, in _do_call
>         raise type(e)(node_def, op, message)
>     tensorflow.python.framework.errors.NotFoundError: Tensor name ""Variable_1"" not found in checkpoint files /home/brm17/Desktop/PFE/MNISTmodel-20000
>     	 [[Node: save/restore_slice_1 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/restore_slice_1/tensor_name, save/restore_slice_1/shape_and_slice)]]
>     Caused by op u'save/restore_slice_1', defined at:
>       File ""LeNet5.py"", line 137, in <module>
>         model.Prediction(image)
>       File ""LeNet5.py"", line 115, in Prediction
>         saver = tf.train.Saver()
>       File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 861, in __init__
>         restore_sequentially=restore_sequentially)
>       File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 519, in build
>         filename_tensor, vars_to_save, restore_sequentially, reshape)
>       File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 272, in _AddRestoreOps
>         values = self.restore_op(filename_tensor, vs, preferred_shard)
>       File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 187, in restore_op
>         preferred_shard=preferred_shard)
>       File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/io_ops.py"", line 203, in _restore_slice
>         preferred_shard, name=name)
>       File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 359, in _restore_slice
>         preferred_shard=preferred_shard, name=name)
>       File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 703, in apply_op
>         op_def=op_def)
>       File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2317, in create_op
>         original_op=self._default_original_op, op_def=op_def)
>       File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1239, in __init__
>         self._traceback = _extract_stack()
>     
>      
> 

what is the problem and how resolved this ? "
8112,tf_upgrade doesn't update RNN cells,"Migration script `tf_upgrade.py` does not update RNN cell locations from `tf.nn.rnn_cell` to `tf.contrib.rnn` leading to error: `AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell'`

Upgrading
```python
import tensorflow as tf

rnn = tf.nn.rnn_cell.GRUCell(128)
tf.initialize_all_variables()  # Checking if tf_upgrade works at all
```
becomes
```python
import tensorflow as tf

rnn = tf.nn.rnn_cell.GRUCell(128)
tf.global_variables_initializer()  # Checking if tf_upgrade works at all
```
when it should be
```python
import tensorflow as tf

rnn = tf.contrib.rnn.GRUCell(128)
tf.global_variables_initializer()  # Checking if tf_upgrade works at all
```"
8110,"How to Implement a Secure, Multi-Tenant tensorflow?","I am using tensorflor as miner/ML platform, and now customers are required to support multi-tenant capabilities;
Between the tenants to achieve resource isolation (CPU, MEM, disk, etc.), security isolation.
How do i achieve it?
Has the new version already planned for these features?"
8109,libxsmm build errors with Python 3,"It seems like some dependencies are not Python 3 compatible.

Building with `--define tensorflow_xsmm=1 --define tensorflow_xsmm_backward=1` gives following 

cc @benoitsteiner 

```
ERROR: /local_home/yaroslav/bazel/external/libxsmm_archive/BUILD.bazel:112:1: Co
uldn't build file python3/external/libxsmm_archive/scripts/libxsmm_utilities.py:
 Converting to Python 3: external/libxsmm_archive/scripts/libxsmm_utilities.py f
ailed: 2to3 failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/host
/genfiles/python3/external/libxsmm_archive/scripts --write-unchanged-files ... (
remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitSta
tusException: Process exited with status 1.                                     ERROR: /local_home/yaroslav/bazel/external/libxsmm_archive/BUILD.bazel:130:1: Co
uldn't build file python3/external/libxsmm_archive/scripts/libxsmm_dispatch.py:
Converting to Python 3: external/libxsmm_archive/scripts/libxsmm_dispatch.py fai
led: 2to3 failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/host/g
enfiles/python3/external/libxsmm_archive/scripts --write-unchanged-files ... (re
maining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatu
sException: Process exited with status 1.                                       ERROR: /local_home/yaroslav/bazel/external/libxsmm_archive/BUILD.bazel:112:1: Co
uldn't build file python3/external/libxsmm_archive/scripts/libxsmm_specialized.p
y: Converting to Python 3: external/libxsmm_archive/scripts/libxsmm_specialized.
py failed: 2to3 failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/
host/genfiles/python3/external/libxsmm_archive/scripts --write-unchanged-files .
.. (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExi
tStatusException: Process exited with status 1.                                 ERROR: /local_home/yaroslav/bazel/external/libxsmm_archive/BUILD.bazel:124:1: Co
uldn't build file python3/external/libxsmm_archive/scripts/libxsmm_config.py: Co
nverting to Python 3: external/libxsmm_archive/scripts/libxsmm_config.py failed:
 2to3 failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/host/genfi
les/python3/external/libxsmm_archive/scripts --write-unchanged-files ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
ERROR: /local_home/yaroslav/bazel/external/libxsmm_archive/BUILD.bazel:118:1: Couldn't build file python3/external/libxsmm_archive/scripts/libxsmm_interface.py:
 Converting to Python 3: external/libxsmm_archive/scripts/libxsmm_interface.py f
ailed: 2to3 failed: error executing command bazel-out/host/bin/external/bazel_to
ols/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/host/genfiles/python3/external/libxsmm_archive/scripts --write-unchanged-files ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitSta
tusException: Process exited with status 1.
```"
8108,unable to install tensor flow using pip.,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8107,"import tf fails in ""imp.find_module('_pywrap_tensorflow_internal',""","I get error below trying to import TensorFlow built at commit 7e9f2dc ([head](https://github.com/yaroslavvb/tensorflow/tree/dev) as of Sat 4).

I haven't made any changes to my [build procedure](https://github.com/yaroslavvb/tensorflow-community-wheels) in last month, and installing similarly built wheel from Mar 1 works.

Looking at relevant differences between Mar 1 and Mar 4 I see that @jhseu 's  718812c refactored Python module loading in https://github.com/tensorflow/tensorflow/blob/b10f50ff15944badb7262a207f6628dfa52d6a9d/tensorflow/python/pywrap_tensorflow.py

Stack trace
```
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 51, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```"
8105,ValueError: '-_Loss/Const' is not a valid node name,"I am trying to build a network using tflearn library and tensorflow v.1, and I got this error 

**error**
Scipy not supported!
[+] Building CNN
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 491, in apply_op
    preferred_dtype=default_dtype)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 716, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 176, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 169, in constant
    attrs={""value"": tensor_value, ""dtype"": dtype_value}, name=name).outputs[0]
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 1220, in __init__
    raise ValueError(""'%s' is not a valid node name"" % node_def.name)
ValueError: '-_Loss/tags' is not a valid node name

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""emotion_recognition.py"", line 101, in <module>
    import poc
  File ""C:\Users\Mohamed\Desktop\Capstone Project\emotion-recognition-neural-networks-master\poc.py"", line 46, in <module>
    network.build_network()
  File ""C:\Users\Mohamed\Desktop\Capstone Project\emotion-recognition-neural-networks-master\emotion_recognition.py"", line 42, in build_network
    tensorboard_verbose = 2
  File ""C:\Program Files\Python35\lib\site-packages\tflearn\models\dnn.py"", line 57, in __init__
    session=session)
  File ""C:\Program Files\Python35\lib\site-packages\tflearn\helpers\trainer.py"", line 111, in __init__
    clip_gradients)
  File ""C:\Program Files\Python35\lib\site-packages\tflearn\helpers\trainer.py"", line 561, in initialize_training_ops
    ema_num_updates=self.training_steps)
  File ""C:\Program Files\Python35\lib\site-packages\tflearn\summaries.py"", line 243, in add_loss_summaries
    summaries_collection_key)
  File ""C:\Program Files\Python35\lib\site-packages\tflearn\summaries.py"", line 46, in get_summary
    summ = tf.summary.scalar(tag, value)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\summary\summary.py"", line 120, in scalar
    tags=scope.rstrip('/'), values=tensor, name=scope)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\ops\gen_logging_ops.py"", line 281, in _scalar_summary
    name=name)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 504, in apply_op
    values, as_ref=input_arg.is_ref).dtype.name
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 716, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 176, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 169, in constant
    attrs={""value"": tensor_value, ""dtype"": dtype_value}, name=name).outputs[0]
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 1220, in __init__
    raise ValueError(""'%s' is not a valid node name"" % node_def.name)
ValueError: '-_Loss/Const' is not a valid node name"
8103,bmp format,If I use opencv to read a bmp image into a mat. Can I feed it to a tensor? Or I have to comvert it to jpg or png?
8102,Scatter_nd doc not clear about concurrent updates,"The doc on the scatter_nd does not specify the consequence of multiple updates that reference the same location. 

I've tested this using the following code:

```python
indices = tf.constant([[4], [3], [1], [1]])
updates = tf.constant([9, 10, 11, 12])
shape = tf.constant([8])
scatter = tf.scatter_nd(indices, updates, shape)
with tf.Session() as sess:
  print sess.run(scatter)
```
The resulting tensor is:
```python
[0, 23, 0, 10, 9, 0, 0, 0]
```
So it seems the updates are added. Is this the intended usage? If so, it would be great to clarify this in the docs. 

Thanks!"
8101,tf.nn.moments produces NaNs with axes=[-1],"Hi,

tf.nn.moments does not work with negative axes. Is this intentional? Will this be possible in future versions? Thanks.

```
>>> x = tf.placeholder(dtype=tf.float32)
>>> mean, var = tf.nn.moments(x, axes=[1])
>>> sess.run([mean, var], {x:np.arange(12).reshape((2,6)).astype(np.float32)})
[array([ 2.5,  8.5], dtype=float32), array([ 2.91666675,  2.91666675], dtype=float32)]
>>> mean, var = tf.nn.moments(x, axes=[-1])
>>> sess.run([mean, var], {x:np.arange(12).reshape((2,6)).astype(np.float32)})
[array([ nan,  nan], dtype=float32), array([ nan,  nan], dtype=float32)]
```
"
8100,XLA Design :: Incorporate Polyhedral Compilation (through LLVM Polly).,"Polyhedral Compilation is a method of modeling iterations of loop nests into points on a multidimensional space (Determined by the loop nest depth). A detailed description can be found on the website as given [here](http://polyhedral.info/).

Is it possible to pass the deeply nested kernels (for-loops) modeled in TensorFlow via XLA-JIT (or independently) through [Polly](http://polly.llvm.org/) that will extract dependence information and perform scheduling using Polyhedral compilation? Tensorflow Ops can be reproduced and modeled to basic math operators and can be applied to points on the Integer Polyhedra. 

This can lead to significant speedup in program execution. However, since Polyhedral compilation can be expensive, it can lead to increase in compile time. (Trade-off in compile time)

I am a student working on Polyhedral Compilation and would love to contribute to this if it spans out!"
8098,ValueError: setting an array element with a sequence. (tf 1.0.0's invalid use of np),"Using `tf 1.0.0` I get the following error.
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-153-fcd2318ab5dc> in <module>()
     56         batch_ys = labels[randidx, :]
     57         # Fit training using batch data
---> 58         sess.run(optm, feed_dict={x: batch_xs, y: batch_ys})
     59         # Compute average loss
     60         avg_cost += sess.run(cost, 

/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    765     try:
    766       result = self._run(None, fetches, feed_dict, options_ptr,
--> 767                          run_metadata_ptr)
    768       if run_metadata:
    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
    936                 ' to a larger type (e.g. int64).')
    937 
--> 938           np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)
    939 
    940           if not subfeed_t.get_shape().is_compatible_with(np_val.shape):

/opt/conda/lib/python3.5/site-packages/numpy/core/numeric.py in asarray(a, dtype, order)
    529 
    530     """"""
--> 531     return array(a, dtype, copy=False, order=order)
    532 
    533 

ValueError: setting an array element with a sequence.
```

This is a minimal example that should replicate this. I came across this problem also using several higher level abstractions to tf but not until I got to barebones `tf` I believed that this is a bug and not just an error on my part.

```
from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils
labels = ['Male', 'Female'] * 5 
encoder = LabelEncoder()
encoder.fit(labels)
labels = encoder.transform(labels)
labels = np_utils.to_categorical(labels)
labels.shape

results = np.ndarray(shape=(10,2623))

tf.set_random_seed(0)
# Parameters
learning_rate   = 0.001
training_epochs = 200
batch_size      = 5
display_step    = 5
ntrain =  10

# Network Parameters
n_hidden_1 = 50 # 1st layer num features
n_input    = 2623 # data input 
n_classes  = 2 # total classes (0-9 digits)

# tf Graph input
x = tf.placeholder(""float"", [None, n_input])
y = tf.placeholder(""float"", [None, n_classes])

# Create model
def multilayer_perceptron(_X, _weights, _biases):
    layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1'])) 
    return tf.matmul(layer_1, _weights['out']) + _biases['out']
    
# Store layers weight & bias
stddev = 0.1 # <== This greatly affects accuracy!! 
weights = {
    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=stddev)),
    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes], stddev=stddev))
}
biases = {
    'b1': tf.Variable(tf.random_normal([n_hidden_1])),
    'out': tf.Variable(tf.random_normal([n_classes]))
}
print (""Network Ready to Go!"")

pred = multilayer_perceptron(x, weights, biases)

# Define loss and optimizer
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)) 
optm = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)
corr = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))    
accr = tf.reduce_mean(tf.cast(corr, ""float""))

# Initializing the variables
init = tf.initialize_all_variables()
print (""Functions ready"")
sess = tf.Session()
sess.run(init)
# Training cycle
for epoch in range(training_epochs):
    avg_cost = 0.
    total_batch = int(ntrain/batch_size)
    # Loop over all batches
    for i in range(total_batch):
        randidx = np.random.randint(ntrain, size=batch_size)
        batch_xs = results[randidx, :]
        batch_ys = labels[randidx, :]   
        # Fit training using batch data
        sess.run(optm, feed_dict={x: batch_xs, y: batch_ys})
        # Compute average loss
        avg_cost += sess.run(cost, 
                feed_dict={x: batch_xs, y: batch_ys})/total_batch
        # Display logs per epoch step
    if epoch % display_step == 0:
        print (""Epoch: %03d/%03d cost: %.9f"" % 
               (epoch, training_epochs, avg_cost))
        train_acc = sess.run(accr, feed_dict={x: batch_xs, y: batch_ys})
        print ("" Training accuracy: %.3f"" % (train_acc))

        print (""Optimization Finished!"")
```"
8096,Error while install tensorflow via recommended method on Getting Started guide,"I'm having trouble install the Tensorflow engine via  `conda` and `pip3`. I'm pasting the information regarding the python toolchain and the system I'm trying to install on.


```sh
 conda install -c conda-forge tensorflow
Fetching package metadata .........
Solving package specifications: ..........

Package plan for installation in environment /Users/eklavya/anaconda:

The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    tensorflow-1.0.0           |           py35_0        32.7 MB  conda-forge

The following NEW packages will be INSTALLED:

    mock:       2.0.0-py35_0  conda-forge
    pbr:        1.10.0-py35_0 conda-forge
    protobuf:   3.2.0-py35_0  conda-forge
    tensorflow: 1.0.0-py35_0  conda-forge

The following packages will be SUPERCEDED by a higher-priority channel:

    conda:      4.2.13-py35_0             --> 4.2.13-py35_0 conda-forge
    conda-env:  2.6.0-0                   --> 2.6.0-0       conda-forge

Proceed ([y]/n)? y

Fetching packages ...
An unexpected error has occurred.#####################################################                 | ETA:  0:02:21  50.21 kB/s
Please consider posting the following information to the
conda GitHub issue tracker at:

    https://github.com/conda/conda/issues



Current conda install:

               platform : osx-64
          conda version : 4.2.13
       conda is private : False
      conda-env version : 4.2.13
    conda-build version : 1.21.3
         python version : 3.5.2.final.0
       requests version : 2.10.0
       root environment : /Users/eklavya/anaconda  (writable)
    default environment : /Users/eklavya/anaconda
       envs directories : /Users/eklavya/anaconda/envs
          package cache : /Users/eklavya/anaconda/pkgs
           channel URLs : https://repo.continuum.io/pkgs/free/osx-64
                          https://repo.continuum.io/pkgs/free/noarch
                          https://repo.continuum.io/pkgs/pro/osx-64
                          https://repo.continuum.io/pkgs/pro/noarch
            config file : None
           offline mode : False



`$ /Users/eklavya/anaconda/bin/conda install -c conda-forge tensorflow`




    Traceback (most recent call last):
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/requests/packages/urllib3/response.py"", line 228, in _error_catcher
        yield
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/requests/packages/urllib3/response.py"", line 310, in read
        data = self._fp.read(amt)
      File ""/Users/eklavya/anaconda/lib/python3.5/http/client.py"", line 448, in read
        n = self.readinto(b)
      File ""/Users/eklavya/anaconda/lib/python3.5/http/client.py"", line 488, in readinto
        n = self.fp.readinto(b)
      File ""/Users/eklavya/anaconda/lib/python3.5/socket.py"", line 575, in readinto
        return self._sock.recv_into(b)
      File ""/Users/eklavya/anaconda/lib/python3.5/ssl.py"", line 929, in recv_into
        return self.read(nbytes, buffer)
      File ""/Users/eklavya/anaconda/lib/python3.5/ssl.py"", line 791, in read
        return self._sslobj.read(len, buffer)
      File ""/Users/eklavya/anaconda/lib/python3.5/ssl.py"", line 575, in read
        v = self._sslobj.read(len, buffer)
    socket.timeout: The read operation timed out
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/requests/models.py"", line 664, in generate
        for chunk in self.raw.stream(chunk_size, decode_content=True):
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/requests/packages/urllib3/response.py"", line 353, in stream
        data = self.read(amt=amt, decode_content=decode_content)
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/requests/packages/urllib3/response.py"", line 320, in read
        flush_decoder = True
      File ""/Users/eklavya/anaconda/lib/python3.5/contextlib.py"", line 77, in __exit__
        self.gen.throw(type, value, traceback)
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/requests/packages/urllib3/response.py"", line 233, in _error_catcher
        raise ReadTimeoutError(self._pool, None, 'Read timed out.')
    requests.packages.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='binstar-cio-packages-prod.s3.amazonaws.com', port=443): Read timed out.
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/fetch.py"", line 421, in download
        for chunk in resp.iter_content(2**14):
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/requests/models.py"", line 671, in generate
        raise ConnectionError(e)
    requests.exceptions.ConnectionError: HTTPSConnectionPool(host='binstar-cio-packages-prod.s3.amazonaws.com', port=443): Read timed out.
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/cli/install.py"", line 405, in install
        execute_actions(actions, index, verbose=not context.quiet)
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/plan.py"", line 643, in execute_actions
        inst.execute_instructions(plan, index, verbose)
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/instructions.py"", line 135, in execute_instructions
        cmd(state, arg)
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/instructions.py"", line 47, in FETCH_CMD
        fetch_pkg(state['index'][arg + '.tar.bz2'])
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/fetch.py"", line 353, in fetch_pkg
        download(url, path, session=session, md5=info['md5'], urlstxt=True)
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/fetch.py"", line 440, in download
        raise CondaRuntimeError(""Could not open %r for writing (%s)."" % (pp, e))
    conda.exceptions.CondaRuntimeError: Runtime error: Could not open '/Users/eklavya/anaconda/pkgs/tensorflow-1.0.0-py35_0.tar.bz2.part' for writing (HTTPSConnectionPool(host='binstar-cio-packages-prod.s3.amazonaws.com', port=443): Read timed out.).
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/exceptions.py"", line 479, in conda_exception_handler
        return_value = func(*args, **kwargs)
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/cli/main.py"", line 145, in _main
        exit_code = args.func(args, p)
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/cli/main_install.py"", line 80, in execute
        install(args, parser, 'install')
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/cli/install.py"", line 422, in install
        raise CondaSystemExit('Exiting', e)
      File ""/Users/eklavya/anaconda/lib/python3.5/contextlib.py"", line 77, in __exit__
        self.gen.throw(type, value, traceback)
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/cli/common.py"", line 573, in json_progress_bars
        yield
      File ""/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/cli/install.py"", line 420, in install
        raise CondaRuntimeError('RuntimeError: %s' % e)
    conda.exceptions.CondaRuntimeError: Runtime error: RuntimeError: Runtime error: Could not open '/Users/eklavya/anaconda/pkgs/tensorflow-1.0.0-py35_0.tar.bz2.part' for writing (HTTPSConnectionPool(host='binstar-cio-packages-prod.s3.amazonaws.com', port=443): Read timed out.).

```
### Environment info
Operating System:

```sh
 uname -a
Darwin abhinavs-MBP.Home 14.5.0 Darwin Kernel Version 14.5.0: Wed Jul 29 02:26:53 PDT 2015; root:xnu-2782.40.9~1/RELEASE_X86_64 x86_64
                                                                                                                                  
```
### Other attempts

I've also tried installing via the pip3 package
```sh
 pip3 install tensorflow    
Collecting tensorflow
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow
                                                                                                                                  
~   
 pip3 install tensorflow-gpu
Collecting tensorflow-gpu
  Could not find a version that satisfies the requirement tensorflow-gpu (from versions: )
No matching distribution found for tensorflow-gpu
                                                                                                                                  
```
"
8095,when using convert_to_tensor the variable must have same value format,"such as:
a=[np.array]
will get error
a=np.array([list])
will work fine.
hope to metion in api document."
8093,CentOS - failed call to cuInit: CUresult(-1),"So, again me - I'm on centOS on a cluster. Have used [this](http://stackoverflow.com/questions/33655731/error-while-importing-tensorflow-in-python2-7-in-ubuntu-12-04-glibc-2-17-not-f) hack for bypassing and using a different GLIBC. Now, however, there seems to be some weird issue with Tensorflow not wanting to run my GPU by failing something on the CUDA side. 
Some general info:
```
LSB Version:    :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch
Distributor ID: CentOS
Description:    CentOS release 6.5 (Final)
Release:        6.5
Codename:       Final
CUDA version: 8.0
CUDNN version: 5.1
Python version: 3.6.0
LD_LIBRARY_PATH=/share/apps/barber/system/lib/:/share/apps/barber/system/lib64/:/opt/gridengine/lib/linux-x64:/opt/openmpi/lib/:/share/apps/barber/cuda/lib64/:/share/apps/barber/cuda/nvvm/lib64/:/share/apps/barber/cuda/extras/CUPTI/lib64:/share/apps/barber/cudnn/lib64/:/share/apps/barber/arrayfire-3/lib/:/share/apps/python-3.6.0-shared/lib/
```
The actual error I get from my code is (first 2 lines are printed by me):
```
Running tool: tensorflow
/gpu:0
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could spe
ed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could s
peed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could s
peed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could spee
d up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could spe
ed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could spee
d up CPU computations.
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUresult(-1)
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: tesla2.local
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: tesla2.local
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.48  Sat Sep  3 18:21:08 PD
T 2016
GCC version:  gcc version 4.4.7 20120313 (Red Hat 4.4.7-16) (GCC)
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.48.0
Traceback (most recent call last):
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1022, in _do_call
    return fn(*args)
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1000, in _run_fn
    self._extend_graph()
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1049, in _extend_graph
    self._session, graph_def.SerializeToString(), status)
  File ""/share/apps/python-3.6.0-shared/lib/python3.6/contextlib.py"", line 89, in __exit__
    next(self.gen)
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'gradients/ff_1/add_grad/BroadcastGradientArgs': Could not satisfy explicit d
evice specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:
0
         [[Node: gradients/ff_1/add_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/device:GPU:0""](gradients/ff_1/add_grad/Shape, gradients/ff_1/
add_grad/Shape_1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""bench.py"", line 23, in <module>
    execute_all(**vars(parser.parse_args()))
  File ""/home/abotev/work/python/deep-bench/deep_bench/executor.py"", line 40, in execute_all
    wide=w, depth=d, batch=b)
  File ""/home/abotev/work/python/deep-bench/deep_bench/executor.py"", line 123, in single_experiment
    f(model, device, temp_folder, **kwargs)
  File ""/home/abotev/work/python/deep-bench/deep_bench/frameworks/tensorflow/executor.py"", line 80, in execute_experiment
    session.run(init)
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'gradients/ff_1/add_grad/BroadcastGradientArgs': Could not satisfy explicit d
evice specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:
0
         [[Node: gradients/ff_1/add_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/device:GPU:0""](gradients/ff_1/add_grad/Shape, gradients/ff_1/
add_grad/Shape_1)]]

Caused by op 'gradients/ff_1/add_grad/BroadcastGradientArgs', defined at:
  File ""bench.py"", line 23, in <module>
    execute_all(**vars(parser.parse_args()))
  File ""/home/abotev/work/python/deep-bench/deep_bench/executor.py"", line 40, in execute_all
    wide=w, depth=d, batch=b)
  File ""/home/abotev/work/python/deep-bench/deep_bench/executor.py"", line 123, in single_experiment
    f(model, device, temp_folder, **kwargs)
  File ""/home/abotev/work/python/deep-bench/deep_bench/frameworks/tensorflow/executor.py"", line 75, in execute_experiment
    x_in, y_in, inference, train, cost = build_model(batch)
  File ""/home/abotev/work/python/deep-bench/deep_bench/frameworks/tensorflow/ff.py"", line 21, in build_ff_net
    train = optimizer.minimize(cost)
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 288, in minimize
    grad_loss=grad_loss)
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 354, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 482, in gradients
    in_grads = grad_fn(op, *out_grads)
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py"", line 586, in _AddGrad
    rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 411, in _broadcast_gradient_args
    name=name)
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

...which was originally created as op 'ff_1/add', defined at:
  File ""bench.py"", line 23, in <module>
    execute_all(**vars(parser.parse_args()))
[elided 2 identical lines from previous traceback]
  File ""/home/abotev/work/python/deep-bench/deep_bench/frameworks/tensorflow/executor.py"", line 75, in execute_experiment
    x_in, y_in, inference, train, cost = build_model(batch)
  File ""/home/abotev/work/python/deep-bench/deep_bench/frameworks/tensorflow/ff.py"", line 14, in build_ff_net
    h = net.dense_layer(h, shape=(arch_specs[i-1], arch_specs[i]), nonlinearity=""tanh"")
  File ""/home/abotev/work/python/deep-bench/deep_bench/frameworks/tensorflow/net.py"", line 40, in dense_layer
    return f(tf.matmul(x_in, w) + tf.reshape(b, [1, -1]))
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 884, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 73, in add
    result = _op_def_lib.apply_op(""Add"", x=x, y=y, name=name)
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Cannot assign a device to node 'gradients/ff_1/add_grad/BroadcastGradientArgs': Could not satisfy explicit device specific
ation '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0
         [[Node: gradients/ff_1/add_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/device:GPU:0""](gradients/ff_1/add_grad/Shape, gradients/ff_1/
add_grad/Shape_1)]]
```
One thing slightly suspicious is that it is picking up the default GCC (4.4) rather than the local user one (4.9), however I don't see how this relates to CUDA. 

However, the second error about `libcuda` version not found has been reported before in #4267, while a similar issue seems to be #2882. At this stage, there is not much I can really say why this is happening.

Also for reference, both Theano and Pytorch worked out of the box, no errors, no complaints directly linking to both CUDA and CuDNN, so this is not a CUDA installation issue. Potentially the fact I use a separate GLIBC might be an issue, but again I don't see why myself. 

PS: This potentially could be some problem of the alternative GLIBC not detecting the GPUs, which to be more general than tensorflow, but I will need to talk with the system admin.
"
8092, fail to build tensorflow from source::unexpected pipe read status,"Hello,everyone!
I am new about tensorflow,and I am just trying to build tensorflow from source according to the steps described step by step, but I get the following error, Is there anyone who can help on it? That would be very kind of you!
Thank you very much!

cynthia@cynthia-pc:~/tensorflow$ ./configure

..........continue........
then the result came with the following......
I guess that maybe something wrong with my bazel ,but I'm new about this,and I'm really no idea how to solve it.

unexpected pipe read status: No such file or directory
Server presumed dead. Now printing '/home/cynthia/.cache/bazel/_bazel_cynthia/581b5660c7c191b39a76e8a607014a30/server/jvm.out':
Java HotSpot(TM) Server VM warning: You have loaded library /home/cynthia/.cache/bazel/_bazel_cynthia/install/6a615f09339464d6ea7e8fe00f96c132/_embedded_binaries/libunix.so which might have disabled stack guard. The VM will try to fix the stack guard now.
It's highly recommended that you fix the library with 'execstack -c ', or link it with '-z noexecstack'.
JNI initialization failed: /home/cynthia/.cache/bazel/_bazel_cynthia/install/6a615f09339464d6ea7e8fe00f96c132/_embedded_binaries/libunix.so: /home/cynthia/.cache/bazel/_bazel_cynthia/install/6a615f09339464d6ea7e8fe00f96c132/_embedded_binaries/libunix.so: wrong ELF class: ELFCLASS64 (Possible cause: architecture word width mismatch). Possibly your installation has been corrupted; if this problem persists, try 'rm -fr /home/cynthia/.cache/bazel/_bazel_cynthia/install/6a615f09339464d6ea7e8fe00f96c132'.
java.lang.UnsatisfiedLinkError: /home/cynthia/.cache/bazel/_bazel_cynthia/install/6a615f09339464d6ea7e8fe00f96c132/_embedded_binaries/libunix.so: /home/cynthia/.cache/bazel/_bazel_cynthia/install/6a615f09339464d6ea7e8fe00f96c132/_embedded_binaries/libunix.so: wrong ELF class: ELFCLASS64 (Possible cause: architecture word width mismatch)
at java.lang.ClassLoader$NativeLibrary.load(Native Method)
at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941)
at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1857)
at java.lang.Runtime.loadLibrary0(Runtime.java:870)
at java.lang.System.loadLibrary(System.java:1122)
at com.google.devtools.build.lib.UnixJniLoader.loadJni(UnixJniLoader.java:28)
at com.google.devtools.build.lib.unix.ProcessUtils.(ProcessUtils.java:28)
at com.google.devtools.build.lib.util.ProcessUtils.getpid(ProcessUtils.java:47)
at com.google.devtools.build.lib.util.OsUtils.forceJNI(OsUtils.java:55)
at com.google.devtools.build.lib.util.OsUtils.maybeForceJNI(OsUtils.java:43)
at com.google.devtools.build.lib.runtime.BlazeRuntime.newRuntime(BlazeRuntime.java:934)
at com.google.devtools.build.lib.runtime.BlazeRuntime.createBlazeRPCServer(BlazeRuntime.java:838)
at com.google.devtools.build.lib.runtime.BlazeRuntime.serverMain(BlazeRuntime.java:777)
at com.google.devtools.build.lib.runtime.BlazeRuntime.main(BlazeRuntime.java:565)
at com.google.devtools.build.lib.bazel.BazelMain.main(BazelMain.java:57)"
8090,fused_batch_norm doesn't work with epsilon < 1e-5,"[This line in nn_impl.py](https://github.com/tensorflow/tensorflow/blob/067cba5e4b873829f6cdfa61256079d2cfc45d02/tensorflow/python/ops/nn_impl.py#L806) has the following comment: ""Add 1e-12 to epsilon when epsilon <= 1e-5 to prevent CUDNN exception."" However, I still get the cuDNN exception when using values of epsilon less than 1e-5.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Nothing related.

### Environment info

- Operating System: CentOS release 6.3 (Final)
- Installed version of CUDA: 8.0.44
- Installed version of cuDNN: 8.0v5.1
- Installed `tensorflow-gpu` from pip-3.5

```
$ python3.5 -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.0
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```
import tensorflow as tf
from itertools import chain

with tf.device('/gpu:0'):
        x = tf.zeros([4, 4, 4, 4])
        scale, offset = tf.ones(4), tf.zeros(4)

        y, _, _ = tf.nn.fused_batch_norm(x, scale, offset, mean=None, variance=None, epsilon=1e-8,
                data_format='NCHW', is_training=True)

        init_op = tf.group(*(v.initializer for v in chain(tf.global_variables(),
                tf.local_variables())))

        sess = tf.Session()
        sess.run(init_op)
        sess.run(y)
```

### Logs or other output that would be helpful

```
$ python3.5 tests/fused_batch_norm_crash.py
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:84:00.0
Total memory: 7.92GiB
Free memory: 7.81GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:84:00.0)
E tensorflow/stream_executor/cuda/cuda_dnn.cc:2177] failed to enqueue forward batch normalization on stream: CUDNN_STATUS_BAD_PARAM
Traceback (most recent call last):
  File ""/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1022, in _do_call
    return fn(*args)
  File ""/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1004, in _run_fn
    status, run_metadata)
  File ""/share/apps/python3/3.5.3/intel/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InternalError: cuDNN launch failure : input shape ([4,4,4,4])
	 [[Node: FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=""NCHW"", epsilon=1.0001e-08, is_training=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](random_normal, ones, zeros, Const, Const_1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tests/fused_batch_norm_crash.py"", line 16, in <module>
    sess.run(y)
  File ""/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: cuDNN launch failure : input shape ([4,4,4,4])
	 [[Node: FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=""NCHW"", epsilon=1.0001e-08, is_training=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](random_normal, ones, zeros, Const, Const_1)]]

Caused by op 'FusedBatchNorm', defined at:
  File ""tests/fused_batch_norm_crash.py"", line 9, in <module>
    data_format='NCHW', is_training=True)
  File ""/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py"", line 818, in fused_batch_norm
    name=name)
  File ""/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 1257, in _fused_batch_norm
    is_training=is_training, name=name)
  File ""/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

InternalError (see above for traceback): cuDNN launch failure : input shape ([4,4,4,4])
	 [[Node: FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=""NCHW"", epsilon=1.0001e-08, is_training=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](random_normal, ones, zeros, Const, Const_1)]]
```
"
8089,Feature requests: erfcinv,"I find there is no erfcinv op in TensorFlow. Since erfcinv is provided by CUDA, it should be straightforward to add such an op. 
The gradient is also very simple:
-0.5*exp(erfcinv(x)**2)*sqrt(pi)

Is it possible to add it in the near future?

Thanks!"
8087,SystemError: <built-in function TF_Run> returned a result with an error set using tf.contrib.layers,"(Using `tf 1.0.0`)
I tried to follow the following example (https://www.tensorflow.org/get_started/tflearn) from the TF website and I get the following error:

```
INFO:tensorflow:Using default config.
INFO:tensorflow:Using config: {'_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9cfb283e80>, '_task_id': 0, '_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 5, '_save_checkpoints_steps': None, '_task_type': None, '_environment': 'local', '_tf_config': gpu_options {
  per_process_gpu_memory_fraction: 1.0
}
, '_master': '', '_save_checkpoints_secs': 600, '_is_chief': True, '_tf_random_seed': None, '_num_ps_replicas': 0, '_save_summary_steps': 100, '_evaluation_master': ''}
WARNING:tensorflow:From <ipython-input-131-b49d002a31c2>:14: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))
WARNING:tensorflow:From <ipython-input-131-b49d002a31c2>:14: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))
WARNING:tensorflow:From /opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.
/opt/conda/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py:247: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.
  equality = a == b
INFO:tensorflow:Create CheckpointSaverHook.
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
TypeError: expected bytes, tuple found

During handling of the above exception, another exception occurred:

SystemError                               Traceback (most recent call last)
<ipython-input-131-b49d002a31c2> in <module>()
     12 classifier.fit(x=results,
     13                y=labels,
---> 14                steps=2000)
     15 
     16 # Evaluate accuracy.

/opt/conda/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    278             _call_location(), decorator_utils.get_qualified_name(func),
    279             func.__module__, arg_name, date, instructions)
--> 280       return func(*args, **kwargs)
    281     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(
    282         func.__doc__, date, instructions)

/opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)
    408     _verify_input_args(x, y, input_fn, None, batch_size)
    409     if x is not None:
--> 410       SKCompat(self).fit(x, y, batch_size, steps, max_steps, monitors)
    411       return self
    412 

/opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in fit(self, x, y, batch_size, steps, max_steps, monitors)
   1351                         steps=steps,
   1352                         max_steps=max_steps,
-> 1353                         monitors=all_monitors)
   1354     return self
   1355 

/opt/conda/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    278             _call_location(), decorator_utils.get_qualified_name(func),
    279             func.__module__, arg_name, date, instructions)
--> 280       return func(*args, **kwargs)
    281     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(
    282         func.__doc__, date, instructions)

/opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)
    424       hooks.append(basic_session_run_hooks.StopAtStepHook(steps, max_steps))
    425 
--> 426     loss = self._train_model(input_fn=input_fn, hooks=hooks)
    427     logging.info('Loss for final step: %s.', loss)
    428     return self

/opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in _train_model(self, input_fn, hooks)
    982         loss = None
    983         while not mon_sess.should_stop():
--> 984           _, loss = mon_sess.run([model_fn_ops.train_op, model_fn_ops.loss])
    985       summary_io.SummaryWriterCache.clear()
    986       return loss

/opt/conda/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)
    460                           feed_dict=feed_dict,
    461                           options=options,
--> 462                           run_metadata=run_metadata)
    463 
    464   def should_stop(self):

/opt/conda/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)
    784                               feed_dict=feed_dict,
    785                               options=options,
--> 786                               run_metadata=run_metadata)
    787       except errors.AbortedError:
    788         logging.info('An AbortedError was raised. Closing the current session. '

/opt/conda/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)
    742 
    743   def run(self, *args, **kwargs):
--> 744     return self._sess.run(*args, **kwargs)
    745 
    746 

/opt/conda/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)
    889                                   feed_dict=feed_dict,
    890                                   options=options,
--> 891                                   run_metadata=run_metadata)
    892 
    893     for hook in self._hooks:

/opt/conda/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)
    742 
    743   def run(self, *args, **kwargs):
--> 744     return self._sess.run(*args, **kwargs)
    745 
    746 

/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    765     try:
    766       result = self._run(None, fetches, feed_dict, options_ptr,
--> 767                          run_metadata_ptr)
    768       if run_metadata:
    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
    963     if final_fetches or final_targets:
    964       results = self._do_run(handle, final_targets, final_fetches,
--> 965                              feed_dict_string, options, run_metadata)
    966     else:
    967       results = []

/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1013     if handle is None:
   1014       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
-> 1015                            target_list, options, run_metadata)
   1016     else:
   1017       return self._do_call(_prun_fn, self._session, handle, feed_dict,

/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1020   def _do_call(self, fn, *args):
   1021     try:
-> 1022       return fn(*args)
   1023     except errors.OpError as e:
   1024       message = compat.as_text(e.message)

/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1002         return tf_session.TF_Run(session, options,
   1003                                  feed_dict, fetch_list, target_list,
-> 1004                                  status, run_metadata)
   1005 
   1006     def _prun_fn(session, handle, feed_dict, fetch_list):

SystemError: <built-in function TF_Run> returned a result with an error set
```

To replicate:
(For better accuracy use Docker image `jupyter/datascience-notebook` and install tf)
```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import tensorflow as tf

results = np.ndarray(10,2632)
labels = [0,1] * 5

feature_columns = [tf.contrib.layers.real_valued_column("""", dimension=4)]


classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
                                            hidden_units=[2623,2],
                                            n_classes=2,
                                            model_dir='./model')

# Fit model.
classifier.fit(x=results,
               y=labels,
               steps=2000)

# Evaluate accuracy.
accuracy_score = classifier.evaluate(x=test_set.data,
                                     y=test_set.target)[""accuracy""]
print('Accuracy: {0:f}'.format(accuracy_score))
```

I think this might be a problem with tensorflow badly interacting with the underlying file system? Not sure."
8085,Running Multiple instances.,"Operating System: Ubuntu 16.10
Installed version of CUDA and cuDNN: 8.0 and cuDNN 5.2
Tensorflow Version : 1.0.0-rc2


I have a website that will take an image of a number plate, the website passes the numberplate to tensor for to be detected, sensor flow then returns the detected text back to the website to be inserted into the database. 

I found this issue when using a dropzone to detect many number plates at once when I upload an image one by one, the system is quite reliable. When using the dropzone the system is very unreliable. 

I tested this by opening 6 terminal sessions and running the following command in each instance:

```
/usr/bin/python3 /home/chad/Documents/anpr-python/detect.py /var/www/Honours/public/number_plate/fec5e719-ee8f-4b7c-a259-0cf8ddc5a28f.jpeg /home/chad/Documents/anpr-python/weights.npz out.jpg
```

I need to be able to fix this as the website needs to allow for mass upload of images to be detected.

Could you help me figure out the best way to fix this?


```chad@chad-GA-990XA-UD3:/var/www/Honours/public/number_plate$ /usr/bin/python3 /home/chad/Documents/anpr-python/detect.py /var/www/Honours/public/number_plate/fec5e719-ee8f-4b7c-a259-0cf8ddc5a28f.jpeg /home/chad/Documents/anpr-python/weights.npz out.jpg
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.683
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 126.19MiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 126.19M (132317184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
E tensorflow/stream_executor/cuda/cuda_dnn.cc:364] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
F tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) 
Aborted (core dumped)
chad@chad-GA-990XA-UD3:/var/www/Honours/public/number_plate$ 

```


```
chad@chad-GA-990XA-UD3:/var/www/Honours/public/number_plate$ /usr/bin/python3 /home/chad/Documents/anpr-python/detect.py /var/www/Honours/public/number_plate/fec5e719-ee8f-4b7c-a259-0cf8ddc5a28f.jpeg /home/chad/Documents/anpr-python/weights.npz out.jpg
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.683
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 82.19MiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 82.19M (86179840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 15.62M (16374016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 15.62M (16374016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256):       Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512):       Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728):         Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456):         Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 256.00MiB was 256.00MiB, Chunk State: 
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ec00000 of size 1280
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ec00500 of size 4864
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ec01800 of size 1039104
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ecff300 of size 307200
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ed4a300 of size 1024
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ed4a700 of size 256
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ed4a800 of size 256
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ed4a900 of size 819200
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ee12900 of size 512
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ee12b00 of size 8192
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ee14b00 of size 2072576
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030f00eb00 of size 65551360
I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size: 
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 256 totalling 512B
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 512 totalling 512B
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1024 totalling 1.0KiB
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280 totalling 1.2KiB
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 4864 totalling 4.8KiB
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 8192 totalling 8.0KiB
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 307200 totalling 300.0KiB
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 819200 totalling 800.0KiB
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1039104 totalling 1014.8KiB
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2072576 totalling 1.98MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 65551360 totalling 62.51MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 66.57MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats: 
Limit:                    86179840
InUse:                    69805824
MaxInUse:                 69805824
NumAllocs:                      12
MaxAllocSize:             65551360

W tensorflow/core/common_runtime/bfc_allocator.cc:274] ******************************************************************************xxxxxxxxxxxxxxxxxxxxxx
W tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 256.00MiB.  See logs for memory state.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256):       Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512):       Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608):   Total Chunks: 1, Chunks in use: 0 15.61MiB allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728):         Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456):         Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 47.56MiB was 32.00MiB, Chunk State: 
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ec00000 of size 1280
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ec00500 of size 4864
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ec01800 of size 1039104
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ecff300 of size 307200
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ed4a300 of size 1024
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ed4a700 of size 256
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ed4a800 of size 256
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ed4a900 of size 819200
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ee12900 of size 512
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ee12b00 of size 8192
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ee14b00 of size 2072576
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030f00eb00 of size 65551360
I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10313800000 of size 4864
I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x10313801300 of size 16369152
I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size: 
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 256 totalling 512B
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 512 totalling 512B
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1024 totalling 1.0KiB
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280 totalling 1.2KiB
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 4864 totalling 9.5KiB
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 8192 totalling 8.0KiB
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 307200 totalling 300.0KiB
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 819200 totalling 800.0KiB
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1039104 totalling 1014.8KiB
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2072576 totalling 1.98MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 65551360 totalling 62.51MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 66.58MiB
```
"
8084,Difference of allocate_persistent() and allocate_temp(),"This is a question on the usage of the two functions which are used in Compute() method for defining new Op.

My understanding is that allocate_temp() are used **only and once in constructing Op** and cannot be used during traning or testing steps later on; whereas allocate_persistent() can be used in **both Op construction and traning or testing steps later on**. Am I correct?

When I was defining my own new Op, I need to allocate a temporay GPU memory buffer in my Compute() method. The buffer will be used every time I call Compute(), however, I don't need to store the value in the temporary buffer for future reuse. In this case, which function should I use?

When calling the two functions, do we need to call corresponding functions like free_persistent() or free_temp()?

"
8082,Slicing a tensor by an index tensor in Tensorflow,"Hi the tensorflow team in Github,

I am not sure if it is appropriate to ask questions in Issue. If questions should not be here, please let me know so that I can close it (I already asked this question on [Stackoverflow](http://stackoverflow.com/questions/42597520/slicing-a-tensor-by-an-index-tensor-in-tensorflow) but I think It would be better to ask this right here in Tensorflow?)

I have two following tensors (note that they are both Tensorflow tensors which means they are still virtually symbolic at the time I construct the following slicing op before I launch a tf.Session()):

`params`: has shape (64,784, 256)
`indices`: has shape (64, 784)
and I want to construct an op that returns the following tensor:

`output`: has shape (64,784) where
`output[i,j] = params_tensor[i,j, indices[i,j] ]`

What is the most efficient way in Tensorflow to do so?

Many thanks.
-Bests"
8078,error: can't copy 'tensorflow/python/pywrap_tensorflow.py': doesn't exist or not a regular file,"Hello, now my cuda is 7.5 , so I install tf from the source, but when I run the command 'bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg', it shows error:
`Sun Mar 5 02:09:40 CST 2017 : === Using tmpdir: /tmp/tmp.4Bm642hjyI
~/ztgong/dl-tools/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/ztgong/dl-tools/tensorflow
~/ztgong/dl-tools/tensorflow
/tmp/tmp.4Bm642hjyI ~/ztgong/dl-tools/tensorflow
Sun Mar 5 02:09:41 CST 2017 : === Building wheel
error: can't copy 'tensorflow/python/pywrap_tensorflow.py': doesn't exist or not a regular file
`

I still not fix this error, how to solve it?
thank you in advance!!"
8076,New Build causes Illegal instruction on Import in Python 2.7 and 3.4,"I built tensorflow with gpu support for python 2.7 and 3.4 using CUDA 8.0 and CUDNN 5.1, running oo Linux.

On import of tensorflow installed with the wheels I built I get an 'Illegal Instruction' error and python quits.

Can someone confirm if this is an error on my part or if is an issue with the current build?"
8073,"Invalid value with ""sigmoid_cross_entropy_with_logits""","Hello, I started learning tensorflow and trying to solve a lot of sample program and lessons, 
Then I faced the problem with ""sigmoid_cross_entropy_with_logits"" methods,
I attached the error & code which I used.
Please help me to figure out this,,, Thank you very much in advance.
--------
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

#placeholders of input & output
x = tf.placeholder(tf.float32, shape=(None,2), name=""x"")
y_ = tf.placeholder(tf.float32, shape=(None,1), name=""y"")

#model parameter
a = tf.Variable(-10 * tf.ones((2,1)), name=""a"")
b = tf.Variable(200., name=""b"")

#model function
u = tf.matmul(x, a) + b
y = tf.sigmoid(u)

loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(u, y_))

train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)

train_x = np.array([[2.,3.], [0., 16.], [3., 1.][2., 8.]])
train_y = np.array([1.,1.,0.,0.]).reshape(4,1)
print(""x="", train_x)
print(""y="", train_y)

sess = tf.Session()
init = tf.global_variables_initializer()
sess.run(init)

for i in range(1000):
    _, l, a_, b_ = sess.run([train_step, loss, a, b], feed_dict={x:train_x, y:train_y})
    if (i+1) % 100 == 0:
        print(""Step=%3d, a1=%6.2f, a2=%6.2f, b=%6.2f, loss=%.2f"" % (i+1, a_[0], a_[1], b_, l))

        
est_a, est_b = sess.run([a, b], feed_dict={x: train_x, y_: train_y})
print(""Estimated: a1=%6.2f, a2=%6.2f, b=%6.2f"" % (est_a[0], est_a[1], est_b))
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-4-2a36f4933eac> in <module>()
     15 y = tf.sigmoid(u)
     16 
---> 17 loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(u, y_))
     18 
     19 train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)

/Users/GenkiA/.pyenv/versions/3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py in sigmoid_cross_entropy_with_logits(_sentinel, labels, logits, name)
    144   # pylint: disable=protected-access
    145   nn_ops._ensure_xent_args(""sigmoid_cross_entropy_with_logits"",
--> 146                            _sentinel, labels, logits)
    147   # pylint: enable=protected-access
    148 

/Users/GenkiA/.pyenv/versions/3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py in _ensure_xent_args(name, sentinel, labels, logits)
   1531   if sentinel is not None:
   1532     raise ValueError(""Only call `%s` with ""
-> 1533                      ""named arguments (labels=..., logits=..., ...)"" % name)
   1534   if labels is None or logits is None:
   1535     raise ValueError(""Both labels and logits must be provided."")

ValueError: Only call `sigmoid_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)"
8071,Downloads and Setups in Tutorial appears to be 404 error,"Just as the headline, I can not open this page, who can fix this porblem ?"
8064,Missing images in docs,"The page https://www.tensorflow.org/api_docs/python/tf/gather has a broken link to an image: https://www.tensorflow.org/api_docs/images/Gather.png

https://www.tensorflow.org/api_docs/python/tf/scatter_add also has a broken link https://www.tensorflow.org/api_docs/images/ScatterAdd.png

In addition, the images seem to be missing from the tensorflow repo.  Could those be put in there? It is hard to build the docs locally for offline use without the images."
8063," convert_graphdef_memmapped_format does not work on graph quantized with mode=""weights""","Using the `quantize_graph` tool and choosing `mode='weights'` produces a graph that when run through `convert_graphdef_memmapped_format` reports:
```
tensorflow/contrib/util/convert_graphdef_memmapped_format_lib.cc:168] Converted 0 nodes
```
Graphs quantized with other modes work (for example `weights_rounded`):
```
tensorflow/contrib/util/convert_graphdef_memmapped_format_lib.cc:168] Converted 10 nodes
```"
8061,Multi-GPU Inference using FIFO Queues issue in TF v1.0.0,"Here is an example multi-gpu inference code which produces incorrect Output in TF v1.0.0 but produces correct output in v0.11.0 (single GPU produces correct output in both versions). Please refer to the attached code for more details.

[InferenceTest.txt](https://github.com/tensorflow/tensorflow/files/817729/InferenceTest.txt)
[Inference.txt](https://github.com/tensorflow/tensorflow/files/817730/Inference.txt)


**Correct Output (TF v0.11) -- 2 GPUs** 
python InferenceTest.py 2
-------------------------
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
**************** Using 2 GPUs *****************
TensorFlow version:  0.11.0
GPU ID List:  [0, 1]
Device:  /gpu:0
Device:  /gpu:1
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: 
name: Tesla M40
major: 5 minor: 2 memoryClockRate (GHz) 1.112
pciBusID 0000:00:05.0
Total memory: 11.21GiB
Free memory: 11.09GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x37b9dc0
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties: 
name: Tesla M40
major: 5 minor: 2 memoryClockRate (GHz) 1.112
pciBusID 0000:00:09.0
Total memory: 11.21GiB
Free memory: 11.09GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 0 to device ordinal 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 1 to device ordinal 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla M40, pci bus id: 0000:00:05.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla M40, pci bus id: 0000:00:09.0)
Input Value 0
Input Value 1
Input Value 2
Input Value 3
Input Value 4
Input Value 5
Input Value 6
Input Value 7
Input Value 8
Input Value 9
Input Queue Size=  10
Results Queue Size=  0
Processing: Input Queue Size=  8
Processing: Results Queue Size=  2
Processing: Input Queue Size=  6
Processing: Results Queue Size=  4
Processing: Input Queue Size=  4
Processing: Results Queue Size=  6
Processing: Input Queue Size=  2
Processing: Results Queue Size=  8
Processing: Input Queue Size=  0
Processing: Results Queue Size=  10
************* Dequeue Results ********************
Results Queue Size=  10
Dequeue Results: Results Output  1:
Dequeue Results: Results Queue Size=  9
Dequeue Results: Results Output  2:
Dequeue Results: Results Queue Size=  8
Dequeue Results: Results Output  3:
Dequeue Results: Results Queue Size=  7
Dequeue Results: Results Output  4:
Dequeue Results: Results Queue Size=  6
Dequeue Results: Results Output  6:
Dequeue Results: Results Queue Size=  5
Dequeue Results: Results Output  5:
Dequeue Results: Results Queue Size=  4
Dequeue Results: Results Output  7:
Dequeue Results: Results Queue Size=  3
Dequeue Results: Results Output  8:
Dequeue Results: Results Queue Size=  2
Dequeue Results: Results Output  9:
Dequeue Results: Results Queue Size=  1
Dequeue Results: Results Output  10:
Dequeue Results: Results Queue Size=  0
Final Input Queue Size=  0
Final Results Queue Size=  0

***************************************************************************************************
**Wrong Output (TF v1.0.0) -- 2 GPUs** 
python InferenceTest.py 2
-------------------------
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
**************** Using 2 GPUs *****************
TensorFlow version:  1.0.0
GPU ID List:  [0, 1]
Device:  /gpu:0
Device:  /gpu:1
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: Tesla M40
major: 5 minor: 2 memoryClockRate (GHz) 1.112
pciBusID 0000:00:05.0
Total memory: 11.21GiB
Free memory: 11.09GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x1999d60
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Tesla M40
major: 5 minor: 2 memoryClockRate (GHz) 1.112
pciBusID 0000:00:09.0
Total memory: 11.21GiB
Free memory: 11.09GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla M40, pci bus id: 0000:00:05.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla M40, pci bus id: 0000:00:09.0)
I tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:1 for node 'Tower_1/Dequeue_Input_Data/fifo_queue_Dequeue' because the input edge from 'Input_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0
I tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:1 for node 'Tower_1/Results_Enqueue/fifo_queue_enqueue' because the input edge from 'Results_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0
I tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:0 for node 'Tower_0/Dequeue_Input_Data/fifo_queue_Dequeue' because the input edge from 'Input_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0
I tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:0 for node 'Tower_0/Results_Enqueue/fifo_queue_enqueue' because the input edge from 'Results_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0
Input Value 0
Input Value 1
Input Value 2
Input Value 3
Input Value 4
Input Value 5
Input Value 6
Input Value 7
Input Value 8
Input Value 9
Input Queue Size=  10
Results Queue Size=  0
Processing: Input Queue Size=  9
Processing: Results Queue Size=  2
Processing: Input Queue Size=  8
Processing: Results Queue Size=  4
Processing: Input Queue Size=  7
Processing: Results Queue Size=  6
Processing: Input Queue Size=  6
Processing: Results Queue Size=  8
Processing: Input Queue Size=  5
Processing: Results Queue Size=  10
Processing: Input Queue Size=  4
Processing: Results Queue Size=  12
Processing: Input Queue Size=  3
Processing: Results Queue Size=  14
Processing: Input Queue Size=  2
Processing: Results Queue Size=  16
Processing: Input Queue Size=  1
Processing: Results Queue Size=  18
Processing: Input Queue Size=  0
Processing: Results Queue Size=  20
************* Dequeue Results ********************
Results Queue Size=  20
Dequeue Results: Results Output  1:
Dequeue Results: Results Queue Size=  19
Dequeue Results: Results Output  1:
Dequeue Results: Results Queue Size=  18
Dequeue Results: Results Output  2:
Dequeue Results: Results Queue Size=  17
Dequeue Results: Results Output  2:
Dequeue Results: Results Queue Size=  16
Dequeue Results: Results Output  3:
Dequeue Results: Results Queue Size=  15
Dequeue Results: Results Output  3:
Dequeue Results: Results Queue Size=  14
Dequeue Results: Results Output  4:
Dequeue Results: Results Queue Size=  13
Dequeue Results: Results Output  4:
Dequeue Results: Results Queue Size=  12
Dequeue Results: Results Output  5:
Dequeue Results: Results Queue Size=  11
Dequeue Results: Results Output  5:
Dequeue Results: Results Queue Size=  10
Dequeue Results: Results Output  6:
Dequeue Results: Results Queue Size=  9
Dequeue Results: Results Output  6:
Dequeue Results: Results Queue Size=  8
Dequeue Results: Results Output  7:
Dequeue Results: Results Queue Size=  7
Dequeue Results: Results Output  7:
Dequeue Results: Results Queue Size=  6
Dequeue Results: Results Output  8:
Dequeue Results: Results Queue Size=  5
Dequeue Results: Results Output  8:
Dequeue Results: Results Queue Size=  4
Dequeue Results: Results Output  9:
Dequeue Results: Results Queue Size=  3
Dequeue Results: Results Output  9:
Dequeue Results: Results Queue Size=  2
Dequeue Results: Results Output  10:
Dequeue Results: Results Queue Size=  1
Dequeue Results: Results Output  10:
Dequeue Results: Results Queue Size=  0
Final Input Queue Size=  0
Final Results Queue Size=  0

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
None

### Environment info
Operating System: CentOS 7.2.1511

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
![cudaversion](https://cloud.githubusercontent.com/assets/21690396/23567127/a771fd10-0009-11e7-88bb-daa27a1b28c2.PNG)

If installed from binary pip package, provide:

1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp35-cp35m-linux_x86_64.whl 
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

See the output above for 2 different tensor flow versions

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8060,"Hi, I am using TF 1.0.0 on ubuntu 16.04 64bit os , I also meets this error, can you help to resolve it, thank you.","```
bazel-bin/inception/imagenet_train --num_gpus=2 --batch_size=32 --train_dir=/data/imagenet-train --data_dir=/data/imagenet-data                                                              
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally             
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally                
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally              
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally                 
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally             
Traceback (most recent call last):                                                                                    
  File ""/root/tensorflow/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/imagenet_train.py"", line 41, in <module>                                                                                          
    tf.app.run()                                                                                                      
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run                    
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))                                                                
  File ""/root/tensorflow/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/imagenet_train.py"", line 37, in main                                                                                              
    inception_train.train(dataset)                                                                                    
  File ""/root/tensorflow/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/inception_train.py"", line 217, in train                                                                                           
    num_preprocess_threads=num_preprocess_threads)                                                                    
  File ""/root/tensorflow/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/image_processing.py"", line 136, in distorted_inputs                                                                               
    num_readers=FLAGS.num_readers)                                                                                    
  File ""/root/tensorflow/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/image_processing.py"", line 490, in batch_inputs
    example_serialized)
  File ""/root/tensorflow/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/image_processing.py"", line 397, in parse_example_proto
    bbox = tf.concat(0, [ymin, xmin, ymax, xmax])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 1029, in concat
    dtype=dtypes.int32).get_shape(
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 637, in convert_to_tensor
    as_ref=False)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 702, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 110, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 99, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 367, in make_tensor_proto
    _AssertCompatible(values, dtype)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 302, in _AssertCompatible
    (dtype.name, repr(mismatch), type(mismatch).__name__))
TypeError: Expected int32, got list containing Tensors of type '_Message' instead.
```"
8059,Unable to run tensorflow on Android through android studio,"The build is successful but the app crashes immediately when it's brought up on any device.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
https://github.com/tensorflow/tensorflow/issues/7256
https://github.com/tensorflow/tensorflow/issues/3444 (assembleDebug error - resolved)

### Environment info
Operating System:
Running Android Studio 2.2.3 on Mac with Sierra 10.12.3
Trying to run on a virtual Nexus 7 with Marshmallow x86

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I followed the instructions in the tensorflow GitHub readme [here]([https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android]).

### What other attempted solutions have you tried?
I am able to compile using bazel build but I cannot get it running through Android Studio. I tried using a Pixel C and a Visual Land Prestige.

### Logs or other output that would be helpful
03/03 10:27:22: Launching android
$ adb shell am start -n ""org.tensorflow.demo/org.tensorflow.demo.ClassifierActivity"" -a android.intent.action.MAIN -c android.intent.category.LAUNCHER
Connected to process 4001 on device emulator-5554
I/InstantRun: Instant Run Runtime started. Android package is org.tensorflow.demo, real application class is null.

              [ 03-03 10:27:22.381  1648: 1669 D/         ]
              HostConnection::get() New Host Connection established 0x9d43f2c0, tid 1669
D/tensorflow: CameraActivity: onCreate org.tensorflow.demo.ClassifierActivity@9ad6111
D/tensorflow: CameraActivity: onStart org.tensorflow.demo.ClassifierActivity@9ad6111
D/tensorflow: CameraActivity: onResume org.tensorflow.demo.ClassifierActivity@9ad6111
D/OpenGLRenderer: Use EGL_SWAP_BEHAVIOR_PRESERVED: true

                  [ 03-03 10:27:22.710  4001: 4001 D/         ]
                  HostConnection::get() New Host Connection established 0xaa7bfb80, tid 4001


                  [ 03-03 10:27:22.840  4001: 4017 D/         ]
                  HostConnection::get() New Host Connection established 0xb2d1a000, tid 4017
I/OpenGLRenderer: Initialized EGL, version 1.4
W/OpenGLRenderer: Failed to choose config with EGL_SWAP_BEHAVIOR_PRESERVED, retrying without...
I/CameraManagerGlobal: Connecting to camera service
I/tensorflow: CameraConnectionFragment: Adding size: 640x480
I/tensorflow: CameraConnectionFragment: Not adding size: 352x288
I/tensorflow: CameraConnectionFragment: Not adding size: 320x240
I/tensorflow: CameraConnectionFragment: Not adding size: 176x144
I/tensorflow: CameraConnectionFragment: Chosen size: 640x480
I/TensorFlowImageClassifier: Reading labels from: imagenet_comp_graph_label_strings.txt
E/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)
I/TensorFlowInferenceInterface: Loading tensorflow_inference.
D/AndroidRuntime: Shutting down VM
E/AndroidRuntime: FATAL EXCEPTION: main
                  Process: org.tensorflow.demo, PID: 4001
                  java.lang.RuntimeException: Error initializing TensorFlow!
                      at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:131)
                      at org.tensorflow.demo.CameraActivity$1.onPreviewSizeChosen(CameraActivity.java:158)
                      at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:394)
                      at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:411)
                      at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:63)
                      at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:94)
                      at android.view.TextureView.getHardwareLayer(TextureView.java:368)
                      at android.view.View.updateDisplayListIfDirty(View.java:15151)
                      at android.view.View.draw(View.java:15948)
                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)
                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)
                      at android.view.View.updateDisplayListIfDirty(View.java:15169)
                      at android.view.View.draw(View.java:15948)
                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)
                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)
                      at android.view.View.draw(View.java:16181)
                      at android.view.View.updateDisplayListIfDirty(View.java:15174)
                      at android.view.View.draw(View.java:15948)
                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)
                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)
                      at android.view.View.updateDisplayListIfDirty(View.java:15169)
                      at android.view.View.draw(View.java:15948)
                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)
                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)
                      at android.view.View.updateDisplayListIfDirty(View.java:15169)
                      at android.view.View.draw(View.java:15948)
                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)
                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)
                      at android.view.View.draw(View.java:16181)
                      at com.android.internal.policy.PhoneWindow$DecorView.draw(PhoneWindow.java:2690)
                      at android.view.View.updateDisplayListIfDirty(View.java:15174)
                      at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:281)
                      at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:287)
                      at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:322)
                      at android.view.ViewRootImpl.draw(ViewRootImpl.java:2615)
                      at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2434)
                      at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2067)
                      at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1107)
                      at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6013)
                      at android.view.Choreographer$CallbackRecord.run(Choreographer.java:858)
                      at android.view.Choreographer.doCallbacks(Choreographer.java:670)
                      at android.view.Choreographer.doFrame(Choreographer.java:606)
                      at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:844)
                      at android.os.Handler.handleCallback(Handler.java:739)
                      at android.os.Handler.dispatchMessage(Handler.java:95)
                      at android.os.Looper.loop(Looper.java:148)
                      at android.app.ActivityThread.main(ActivityThread.java:5417)
                      at java.lang.reflect.Method.invoke(Native Method)
                      at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:726)
                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:616)
                   Caused by: java.lang.RuntimeException: Native TF methods not found; check that the correct native libraries are present and loaded.
                      at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:57)
                      at org.tensorflow.demo.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:101)
                      at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:121)
                      at org.tensorflow.demo.CameraActivity$1.onPreviewSizeChosen(CameraActivity.java:158)
                      at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:394)
                      at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:411)
                      at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:63)
                      at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:94)
                      at android.view.TextureView.getHardwareLayer(TextureView.java:368)
                      at android.view.View.updateDisplayListIfDirty(View.java:15151)
                      at android.view.View.draw(View.java:15948)
                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)
                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)
                      at android.view.View.updateDisplayListIfDirty(View.java:15169)
                      at android.view.View.draw(View.java:15948)
                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)
                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)
                      at android.view.View.draw(View.java:16181)
                      at android.view.View.updateDisplayListIfDirty(View.java:15174)
                      at android.view.View.draw(View.java:15948)
                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)
                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)
                      at android.view.View.updateDisplayListIfDirty(View.java:15169)
                      at android.view.View.draw(View.java:15948)
                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)
                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)
                      at android.view.View.updateDisplayListIfDirty(View.java:15169)
                      at android.view.View.draw(View.java:15948)
                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)
                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)
                      at android.view.View.draw(View.java:16181)
                      at com.android.internal.policy.PhoneWindow$DecorView.draw(PhoneWindow.java:2690)
                      at android.view.View.updateDisplayListIfDirty(View.java:15174)
                      at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:281)
                      at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:287)
                      at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:322)
                      at android.view.ViewRootImpl.draw(ViewRootImpl.java:2615)
                      at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2434)
                      at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2067)
                      at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1107)
                      at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6013)
                      at android.view.Choreographer$CallbackRecord.run(Choreographer.java:858)
                      at android.view.Choreographer.doCallbacks(Choreographer.java:670)
                      at android.view.Choreographer.doFrame(Choreographer.java:606)
                      at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:844)
                      at android.os.Handler.handleCallback(Handler.java:739)
                      at android.os.Handler.dispatchMessage(Handler.java:95)
                      at android.os.Looper.loop(Looper.java:148)
                      at android.app.ActivityThread.main(ActivityThread.java:5417)
                      at java.lang.reflect.Method.invoke(Native Method)
                      at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:726)
                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:616)
Application terminated.
"
8057,"AdamOptimizer's slots ""beta1_power"" and ""beta2_power"" are not available via ""get_slot()"" and ""get_slot_names()""","**Preamble.** 
I want to explicitly pass list of variables to `tf.variables_initializer()`. I do something like this:
``` python
model_variables = my_model.get_variables_list()
optimizer_slots = [
    optimizer.get_slot(var, name)
    for name in optimizer.get_slot_names()
    for var in model_variables
]
all_variables = [
    *model_variables,
    *optimizer_slots,
    global_step,
]
init_op = tf.variables_initializer(all_variables)
```

When I used the `AdamOptimizer`, I got such exception:
```
FailedPreconditionError (see above for traceback): Attempting to use uninitialized value beta2_power
```

**The problem.**
After digging into the TensorFlow sources, I found that AdamOptimizer overrides its `_create_slots()` in that way:
``` python
  def _create_slots(self, var_list):
    # Create the beta1 and beta2 accumulators on the same device as the first
    # variable.
    if (self._beta1_power is None or
        self._beta1_power.graph is not var_list[0].graph):
      with ops.colocate_with(var_list[0]):
        self._beta1_power = variables.Variable(self._beta1,
                                               name=""beta1_power"",
                                               trainable=False)
        self._beta2_power = variables.Variable(self._beta2,
                                               name=""beta2_power"",
                                               trainable=False)
    # Create slots for the first and second moments.
    for v in var_list:
      self._zeros_slot(v, ""m"", self._name)
      self._zeros_slot(v, ""v"", self._name)
```
It creates two `Variables` and does not store them into `self._slots`, therefore I can not access them using public interface.

This problem refers to library's API design.

I see that *beta1_power* and *beta2_power* slots are not subject to any variable while `self._slots` implies that each slot have both name and related variable, so there is no obvious solution... It may be reasonable to extend public API to cover such cases."
8056,error installing tenserflow,"Hello Gents,

I am trying to install the TenserFlow  but when i reach and type this command:

""bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package""

after following all instruction in the following link:
""https://alliseesolutions.wordpress.com/2016/07/05/how-to-install-gpu-tensorflow-0-9-from-sources-ubuntu-14-04/"" 

i get the following error (I managed to reduced errors from 5 to only this one):

NOTE: i am a beginner in Linux  (Ubuntu 14.04) and thus i require step by step instructions if possible.



ERROR: /home/maher/tensorflow/tensorflow/compiler/tf2xla/BUILD:23:1: C++ compilation of rule '//tensorflow/compiler/tf2xla:xla_compiler' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 180 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
tensorflow/compiler/tf2xla/xla_compiler.cc: In function 'tensorflow::Status tensorflow::{anonymous}::CheckSignature(const DataTypeVector&, const std::vector<tensorflow::XlaCompiler::Argument>&)':
tensorflow/compiler/tf2xla/xla_compiler.cc:48:34: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < types.size(); ++i) {
                                  ^
tensorflow/compiler/tf2xla/xla_compiler.cc: In member function 'tensorflow::Status tensorflow::XlaCompiler::BuildExecutable(const tensorflow::XlaCompiler::CompilationResult&, std::unique_ptr<xla::LocalExecutable>*)':
tensorflow/compiler/tf2xla/xla_compiler.cc:139:52: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < result.xla_input_shapes.size(); ++i) {
                                                    ^
tensorflow/compiler/tf2xla/xla_compiler.cc: In function 'tensorflow::Status tensorflow::{anonymous}::BuildArguments(const std::vector<tensorflow::XlaCompiler::Argument>&, bool, xla::ComputationBuilder*, std::vector<tensorflow::XlaContext::Argument>*, std::vector<int>*, std::vector<xla::Shape, std::allocator<xla::Shape> >*)':
tensorflow/compiler/tf2xla/xla_compiler.cc:228:8: error: 'size_typ' is not a member of 'std::vector<tensorflow::XlaCompiler::Argument>'
   for (std::vector<XlaCompiler::Argument>::size_typ i = 0;
        ^
tensorflow/compiler/tf2xla/xla_compiler.cc:228:53: error: expected ';' before 'i'
   for (std::vector<XlaCompiler::Argument>::size_typ i = 0;
                                                     ^
tensorflow/compiler/tf2xla/xla_compiler.cc:229:8: error: 'i' was not declared in this scope
        i < args.size(); ++i) {
        ^
tensorflow/compiler/tf2xla/xla_compiler.cc: In member function 'tensorflow::Status tensorflow::XlaCompiler::CompileGraph(const string&, std::unique_ptr<tensorflow::Graph>, tensorflow::FunctionLibraryRuntime*, const std::vector<tensorflow::XlaCompiler::Argument>&, tensorflow::XlaCompiler::CompilationResult*)':
tensorflow/compiler/tf2xla/xla_compiler.cc:480:20: error: 'VariableWrite' is not a member of 'tensorflow::XlaCompiler'
   for (std::vector<XlaCompiler::VariableWrite>::size_type  i = 0;
                    ^
tensorflow/compiler/tf2xla/xla_compiler.cc:480:20: error: 'VariableWrite' is not a member of 'tensorflow::XlaCompiler'
tensorflow/compiler/tf2xla/xla_compiler.cc:480:46: error: template argument 1 is invalid
   for (std::vector<XlaCompiler::VariableWrite>::size_type  i = 0;
                                              ^
tensorflow/compiler/tf2xla/xla_compiler.cc:480:46: error: template argument 2 is invalid
tensorflow/compiler/tf2xla/xla_compiler.cc:480:60: error: invalid type in declaration before 'i'
   for (std::vector<XlaCompiler::VariableWrite>::size_type  i = 0;
                                                            ^
tensorflow/compiler/tf2xla/xla_compiler.cc:480:60: error: expected ';' before 'i'
tensorflow/compiler/tf2xla/xla_compiler.cc:480:60: error: 'i' was not declared in this scope
tensorflow/compiler/tf2xla/xla_compiler.cc:481:43: error: expected ')' before ';' token
        i < result->variable_updates.size(); ++i) {
                                           ^
tensorflow/compiler/tf2xla/xla_compiler.cc:480:49: warning: unused variable 'size_type' [-Wunused-variable]
   for (std::vector<XlaCompiler::VariableWrite>::size_type  i = 0;
                                                 ^
tensorflow/compiler/tf2xla/xla_compiler.cc:481:47: error: 'i' was not declared in this scope
        i < result->variable_updates.size(); ++i) {
                                               ^
tensorflow/compiler/tf2xla/xla_compiler.cc:481:48: error: expected ';' before ')' token
        i < result->variable_updates.size(); ++i) {
                                                ^
tensorflow/compiler/tf2xla/xla_compiler.cc:508:1: error: expected '}' at end of input
 }  // namespace tensorflow
 ^
tensorflow/compiler/tf2xla/xla_compiler.cc: At global scope:
tensorflow/compiler/tf2xla/xla_compiler.cc:508:1: error: expected '}' at end of input
tensorflow/compiler/tf2xla/xla_compiler.cc: In member function 'tensorflow::Status tensorflow::XlaCompiler::CompileGraph(const string&, std::unique_ptr<tensorflow::Graph>, tensorflow::FunctionLibraryRuntime*, const std::vector<tensorflow::XlaCompiler::Argument>&, tensorflow::XlaCompiler::CompilationResult*)':
tensorflow/compiler/tf2xla/xla_compiler.cc:508:1: warning: control reaches end of non-void function [-Wreturn-type]
 }  // namespace tensorflow
 ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps."
8055,"TensorBoard: Can't load >5 Audio summaries, Broken pipe error","*Possibly related:* https://github.com/tensorflow/tensorflow/issues/4207
*Setup:* OSX 10.10, TF nightly build of 2017-03-02 without GPU, Python 2.7.13. Affects both Chrome 56.0 and Safari 10.0. I think the official 1.0.0 release also had this issue.

My model logs relatively long audio summaries (~3 minutes long at 16 kHz), and created around 60 of them overnight. The issue also appears with fewer summaries created, at least on Chrome.

When using Chrome, they only load upon playback. Only the first 5 or so that I play will work; after that (even if I pause them) I can't get the others to play unless I restart TensorBoard. When using Safari, all of them try to load at once and nothing works.

On the TensorBoard side, I see a bunch of `error: [Errno 32] Broken pipe`, which could mean the browser tries to manage the incoming streams and TensorBoard doesn't know how to defer the transfers (see [this post](http://stackoverflow.com/questions/14207708/ioerror-errno-32-broken-pipe-python)).

For Chrome, there is a [limit of 6 concurrent audio downloads](https://bugs.chromium.org/p/chromium/issues/detail?id=162627). I think in my case the audio keeps the connection open even after I pause it, and instead of downloading the whole file it streams it, so it never closes and I can't play other files. I may need to restart TensorBoard to clear the browser cache, or maybe TensorBoard itself has a limit on the concurrent streams and they don't close even after a page reload. For Safari, I think it doesn't wait until playback to download the files, so all 60 players start the download concurrently and only the first ones survive.

If it's not a bug on the TensorBoard server and the summaries can't be prevented from keeping the connection open on pause or stop, I guess a possible solution could be to have a global player and load the audio files to it on-demand when requesting a summary, rather than each summary having its own player."
8054,summarize_graph doesn't recognize VariableV2 ,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
None
### Environment info
Operating System:
Windows 10

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
None

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
8cac382a5425d64f3083cb5adec525baa163e18e
2. The output of `bazel version`
compiled by cmake

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?

```
--- a/tensorflow/tools/graph_transforms/summarize_graph_main.cc
+++ b/tensorflow/tools/graph_transforms/summarize_graph_main.cc
@@ -109,7 +109,7 @@ Status SummarizeGraph(const GraphDef& graph, const string& graph_path) {
     if (node.op() == ""Placeholder"") {
       placeholders.push_back(&node);
     }
-    if (node.op() == ""Variable"") {
+    if (node.op() == ""Variable"" || node.op() == ""VariableV2"") {
       variables.push_back(&node);
     }
   }
@@ -168,7 +168,7 @@ Status SummarizeGraph(const GraphDef& graph, const string& graph_path) {
     if (node.device() != """") {
       ++device_counts[node.device()];
     }
-    if ((node.op() == ""Const"") || (node.op() == ""Variable"")) {
+    if ((node.op() == ""Const"") || (node.op() == ""Variable"") || (node.op() == ""VariableV2"")) {
       Tensor tensor;
       if (node.attr().count(""value"") &&
           tensor.FromProto(node.attr().at(""value"").tensor())) {
```"
8053,per_process_gpu_memory_fraction causes memory error,"I wrote an LSTM model in tensorflow and use a `Saver` to create a checkpoint after each epoch. So far so good. However, when I add the option `per_process_gpu_memory_fraction=0.9` to the session, it runs out of memory when I save the model right after the first epoch. I have a GeForce Titan X with 12GB of memory; tf uses a bit more than half of it (see below).

Interestingly, if I do not save the model, it does not crash (at least not after the first epoch). Also,

| per_process_gpu_memory_fraction | memory used | crashes |
| ------------------------------------------- | ----------------- | --------- |
| 0.9 | 7377MiB | yes |
| 0.8 | 7288MiB | yes |
| 0.5 | 6272MiB | no |
| not set | 6955MiB | no |

I forgot with what parameters, but I also saw the program survive the first epoch, only to crash after the fifth.

I have modified the [ptf_word_lm.py](https://github.com/DavidNemeskey/models/blob/saver_memory_error/tutorials/rnn/ptb/ptb_word_lm.py) in tensorflow/models, so that it can be used to reproduce the error. It does not crash with `per_process_gpu_memory_fraction=0.8`, as my model is a bit different, but it does at `0.9`. Please call the script with `--model medium`.

On the surface, I only see a `Dst tensor is not initialized` error, but as described in #7025, it is a sign of a memory error. Also, in tf 0.11, it returned with a proper out of memory error, but I have updated to 1.0 since, as I hoped this bug had been fixed there. Apparently not.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
I couldn't find anything similar.

### Environment info
[environment.txt](https://github.com/tensorflow/tensorflow/files/816786/environment.txt)

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
See `ptb_word_lm.py` in [this branch](https://github.com/DavidNemeskey/models/tree/saver_memory_error)

### What other attempted solutions have you tried?
--

### Logs or other output that would be helpful
[log_err.txt](https://github.com/tensorflow/tensorflow/files/816796/log_err.txt)
"
8051,"Make tf.Tensor's magic functions (e.g., `__add__()`) play nice with unknown types","`tf.Tensor`'s magic functions should return `NotImplemented` rather than raising an exception when they are given an object of an unsupported type (i.e., an object that TensorFlow cannot `convert_to_tensor()`). This would give that object a chance to perform the operation with its reverse magic function (e.g., `__radd__()`). See the code example below. Note that this is what NumPy arrays do.

### Environment info
Operating System: **Ubuntu 16.10**
Installed version of CUDA and cuDNN:  **N/A**
Installed pip package:
https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp35-cp35m-linux_x86_64.whl
TensorFlow version: **1.0.0**

### Reproducible example

```
>>> class AmazingMatrix(object):
...   def __add__(self, b):
...     print(""I know how to handle TensorFlow Tensors!"")
...   def __radd__(self, a):
...     print(""I know how to handle TensorFlow Tensors!"")
... 
>>> am = AmazingMatrix()
>>> t = tf.constant([[0.]])
>>> am + t
I know how to handle TensorFlow Tensors!
>>> t + am
Traceback (most recent call last):
[...]/python/ops/math_ops.py"", line 799, in binary_op_wrapper
    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=""y"")
[...]
TypeError: Expected float32, got <__main__.AmazingMatrix object at 0x7f5999a73518> of type 'AmazingMatrix' instead.
```

### Related GitHub issues or StackOverflow threads
http://stackoverflow.com/questions/38599637/overloading-tensorflow-tensor-magic-method-priority-similar-to-numpys-array

### What other attempted solutions have you tried?
None. I guess one could create Tensor wrappers, with the appropriate behavior, but it would be quite cumbersome to use.

### Logs or other output that would be helpful
None
"
8049,AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell',"on `win7 x64`

python version is `3.5`
tensorflow's version is `1.0.0`, and it is installed by `pip install tensorflow-gpu`

and, it seems I can replace `tf.nn.rnn_cell with tf.contrib.rnn`,..."
8047,Error in tensorflow debugger (tfdbg) while executing session run call in child thread,"I ran tensorflow debugger using the command ""python -m <PythonModule> --debug"" but got the following error (i,e Signal only works in main thread):

![screenshot3png](https://cloud.githubusercontent.com/assets/21690396/23540767/24df69c6-ff98-11e6-96bf-5ee98a439f77.PNG)

The error is thrown when a session run call is executed in a child thread (spawned from main thread). Is tensorflow debugger only supported for single threaded applications?

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
None

### Environment info
Operating System: CentOS 7.2.1511

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
![screenshot](https://cloud.githubusercontent.com/assets/21690396/23540624/38732a46-ff97-11e6-9749-a43b8af6c5aa.PNG)


If installed from binary pip package, provide:

1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp35-cp35m-linux_x86_64.whl

2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
![screenshot2](https://cloud.githubusercontent.com/assets/21690396/23540694/b42648f8-ff97-11e6-93ff-4cabc2d2d199.PNG)

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8044,Broken link in Adding a New Op to zero_out_op_1.py,The link in [Adding a New Op](https://github.com/tensorflow/tensorflow/blob/b10f50ff15944badb7262a207f6628dfa52d6a9d/tensorflow/docs_src/extend/adding_an_op.md#L244) to `zero_out_op_1.py` is broken. It points to: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/g3doc/how_tos/adding_an_op/zero_out_op_1.py.
8043,DNNLinearCombinedClassifier weight_column_name doesn't work in v1.0,"After migrating to tensorflow v1.0 of my sources, weight_column_name in estimators (tested in tf.contrib.learn.DNNLinearCombinedClassifier) seems to not working now.
Scripts now outputs this error:
`ERROR:tensorflow:Could not create metric ops for MetricSpec(metric_fn=_predictions_streaming_mean, prediction_key=logistic, label_key=None, weight_key=class_weights)`

### Environment info
Operating System: 
CentOS Linux release 7.3.1611 (Core)

Docker image within TF package:
1. Docker latest image: tensorflow/tensorflow
Created container:
`docker run -it -p 8888:8888 -v ..... --memory=12g tensorflow/tensorflow`
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`:
`1.0.0`

### Reproducing

1. Download wide_n_deep_tutorial.py: [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py](url)
(This solely works fine)
2. Then add weightings (insert lines at):
line 26 `import numpy as np`
line 130 `weight_column_name='class_weights',`
line 153 `feature_cols['class_weights'] = tf.constant(np.ones(np.shape(df[LABEL_COLUMN].values)))`
3. Code executes with error:
`ERROR:tensorflow:Could not create metric ops for MetricSpec(metric_fn=_predictions_streaming_mean, prediction_key=logistic, label_key=None, weight_key=class_weights)`
Full trace of error is below.

If we repeat this procedures with version 0.10, code works well.

I tried to reshape weight feature col to [2, n], to [n, 2] - also, doesn't help.

### Trace
```
ERROR:tensorflow:Could not create metric ops for MetricSpec(metric_fn=_predictions_streaming_mean, prediction_key=logistic, label_key=None, weight_key=class_weights).
Traceback (most recent call last):
  File ""wide_n_deep_tutorial.py"", line 236, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""wide_n_deep_tutorial.py"", line 199, in main
    FLAGS.train_data, FLAGS.test_data)
  File ""wide_n_deep_tutorial.py"", line 188, in train_and_eval
    m.fit(input_fn=lambda: input_fn(df_train), steps=train_steps)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 280, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 426, in fit
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 934, in _train_model
    model_fn_ops = self._call_legacy_get_train_ops(features, labels)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1003, in _call_legacy_get_train_ops
    train_ops = self._get_train_ops(features, labels)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1162, in _get_train_ops
    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1133, in _call_model_fn
    model_fn_results = self._model_fn(features, labels, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py"", line 312, in _dnn_linear_combined_model_fn
    features, labels, mode, _make_training_op, logits=logits)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py"", line 531, in head_ops
    labels, predictions)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py"", line 1390, in _eval_metric_ops
    return estimator._make_metrics_ops(metrics, features, labels, predictions)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 258, in _make_metrics_ops
    result[name] = metric.create_metric_ops(features, labels, predictions)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/metric_spec.py"", line 220, in create_metric_ops
    weights=inputs[self.weight_key])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py"", line 1440, in _predictions_streaming_mean
    return metrics_lib.streaming_mean(predictions, weights=weights)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py"", line 392, in streaming_mean
    updates_collections=updates_collections, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/metrics_impl.py"", line 262, in mean
    math_ops.to_float(weights), values)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/weights_broadcast_ops.py"", line 166, in broadcast_weights
    with ops.control_dependencies((assert_broadcastable(weights, values),)):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/weights_broadcast_ops.py"", line 102, in assert_broadcastable
    weights_rank_static))
```
### Update 3/10/2017
Somehow in process of editing/formatting this post, forgot one line at the end of traceback:
`ValueError: weights can not be broadcast to values. values.rank=2. weights.rank=1.`"
8042,Estimator Tensor not from same graph for predict(),"I run Tensorflow 1.0.
I use Mac OSX Sierra.
I use the `Estimator` and `Layers` api.

This is the first part of my `model_fn` which I pass to `.Estimator(model_fn=...)`.
```
class Model(object):

  def __init__(self):
    ...
    self._estimator = learn.Estimator(
        model_fn=self.model_fn,
        model_dir=modeldir,
        params={
            'width': width,
            'height': height,
            'classes': classes
        },
        config=RunConfig(save_checkpoints_secs=30, gpu_memory_fraction=0.75)
    )

  def model_fn(self, features, labels, mode, params):
    input_layer = tf.reshape(features, [-1, width, height, 1], name='fitinput')

    conv1 = tf.layers.conv2d(
        inputs=input_layer,
        filters=32,
        kernel_size=[5, 5],
        padding=""same"",
        activation=tf.nn.relu,
        name=""conv1""
    )
...
```
I only use `input_fn` to pass data to the individual steps (images, labels).
I train with `estimator.fit(...)` and evaluate with `estimator.evaluate(...)`. Both work fine and create summaries and checkpoints in my model directory.
After that I run `estimator.predict(...)` with the same data that I trained on (just to troubleshoot this problem). For `predict` I only pass images.
Calling `predict` results in the following error:
```
...
    estimator.predict(input_fn=predict_data_fn)
  File ""/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 280, in new_func
    return func(*args, **kwargs)
  File ""/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 561, in predict
    as_iterable=as_iterable)
  File ""/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 865, in _infer_model
    infer_ops = self._call_legacy_get_predict_ops(features)
  File ""/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 996, in _call_legacy_get_predict_ops
    infer_ops = self._get_predict_ops(features)
  File ""/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1217, in _get_predict_ops
    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.INFER)
  File ""/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1133, in _call_model_fn
    model_fn_results = self._model_fn(features, labels, **kwargs)
  File ""ocrcnn.py"", line 197, in cnn_model_fn
    name=""conv1""
  File ""/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/python/layers/convolutional.py"", line 509, in conv2d
    return layer.apply(inputs)
  File ""/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/python/layers/base.py"", line 303, in apply
    return self.__call__(inputs, **kwargs)
  File ""/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/python/layers/base.py"", line 273, in __call__
    outputs = self.call(inputs, **kwargs)
  File ""/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/python/layers/convolutional.py"", line 156, in call
    data_format=utils.convert_data_format(self.data_format, self.rank + 2))
  File ""/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py"", line 586, in convolution
    with ops.name_scope(name, ""convolution"", [input, filter]) as name:
  File ""/users/anaconda/envs/ocnncr/lib/python3.5/contextlib.py"", line 59, in __enter__
    return next(self.gen)
  File ""/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 4219, in name_scope
    g = _get_graph_from_inputs(values)
  File ""/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3968, in _get_graph_from_inputs
    _assert_same_graph(original_graph_element, graph_element)
  File ""/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3907, in _assert_same_graph
    ""%s must be from the same graph as %s."" % (item, original_item))
ValueError: Tensor(""conv1/kernel:0"", shape=(5, 5, 1, 32), dtype=float32_ref) must be from the same graph as Tensor(""fitinput:0"", shape=(50, 28, 28, 1), dtype=float32).
```

I have no idea if this is a bug or I did something wrong. So I am sorry if this should have been a Stackoverflow issue instead."
8041,broken link for flower photos in the official tutorial about transfer learning,"In official tutorial ""How to Retrain Inception's Final Layer for New Categories"", link for downloading flower photos is broken:
`curl -O http://download.tensorflow.org/example_../images/flower_photos.tgz`
"
8040,Is it able to set `allow_growth=True` to environment variables?,"Hi all, we a device with 3 gpus, and there are several people who are learning deep-learning.
However, while using tensorflow, all gpus will be used, which causes others not able to calculate.

We know setting `config.gpu_options.allow_growth = True` in python code can avoid this, but there are always some people who forget to do this.

So I wonder if we can set `config.gpu_options.allow_growth = True` to default (such as environment variables)? Thanks!"
8037,"How to compile tensorflow using SSE4.1, SSE4.2, and AVX. ","Just got tensorflow running. Now running into this error. 

Currently using Mac Yosemite, downloaded tensorflow using pip3 through anaconda, using python 3.5. 

`W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.`

`W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.`

`W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.`

So since anaconda has a special set of commands, how do you get tensorflow to run on SSE4.1, SSE4.2, and AVX via the anaconda command system ?  I am really confused how to go about this. 
"
8036,Could LSTM model be used to predict from multivariate variables,"I am doing a project on water content predictions. The input variables on time series include temperature, rainfall and so on. I do not know whether LSTM can predict from multiple variables or not. And I do not know how to do it? Could anybody please help me and provide such a related example? Thank you very much."
8035,obsfucate_names spelled wrong,"I assume this isn't a pun...

https://github.com/tensorflow/tensorflow/search?utf8=%E2%9C%93&q=obsfucate_names"
8034,SparseTensor constructor change not noted as a breaking change in 1.0,"The `shape` keyword argument of the `SparseTensor` constructor changes its name to `dense_shape` between Tensorflow 0.12 ([relevant API document](https://www.tensorflow.org/versions/r0.12/api_docs/python/sparse_ops/sparse_tensor_representation#SparseTensor)) and Tensorflow 1.0 ([relevant API document](https://www.tensorflow.org/versions/r0.11/api_docs/python/sparse_ops/sparse_tensor_representation#SparseTensor)).  However, this is neither documented on [the migration page](https://www.tensorflow.org/install/migration) nor handled by the migration script.

"
8033,Direct access to Tensor Buffers in C++ interface,"I would like to request direct access to the tensorflow buffers through the C++ interface.  I have commented previously about this on [StackOverflow](http://stackoverflow.com/questions/39379747/import-opencv-mat-into-c-tensorflow-without-copying), and have gotten it to work by exposing the `TensorCApi` class.  This required some fiddling around with the source code.

My specific use case is using tensorflow from a [ROS](http://www.ros.org/) (robot operating system) node.  ROS has its own build system, so I have to link to the tensorflow library (libtensorflow.so) externally.  The C++ interface is much more convenient than the C interface, as I only load and do forward inference on static graphs.

Because I get an OpenCV array as input, my only other option is to iterate the entire array and copy it to a newly allocated tensor buffer as suggested in [this post](http://stackoverflow.com/questions/36044197/how-do-i-pass-an-opencv-mat-into-a-c-tensorflow-graph).  In my case, copying the buffer by iterating it can take 25-75ms, whereas simply pointing to the memory already allocated by OpenCV incurs almost no overhead.  I am doing all of this in real time as part of a tight control loop, so this extra time is absolutely critical.

I realize this patch is probably not the right way to expose the interface, and my example code provides no memory checking or safeguards, but it is an example (diff against the r1.0 tensorflow tag):

```
diff --git a/tensorflow/c/c_api.cc b/tensorflow/c/c_api.cc
index 83ce3e2..34ae2b4 100644
--- a/tensorflow/c/c_api.cc
+++ b/tensorflow/c/c_api.cc
@@ -464,14 +464,11 @@ TF_Tensor* TF_Tensor_EncodeStrings(const Tensor& src) {
                       dimvec.size(), base, size, DeleteArray, base);
 }
 
-class TensorCApi {
- public:
-  static TensorBuffer* Buffer(const Tensor& tensor) { return tensor.buf_; }
-  static Tensor MakeTensor(TF_DataType type, const TensorShape& shape,
-                           TensorBuffer* buf) {
-    return Tensor(static_cast<DataType>(type), shape, buf);
-  }
-};
+TensorBuffer* TensorCApi::Buffer(const Tensor& tensor) { return tensor.buf_; }
+Tensor TensorCApi::MakeTensor(TF_DataType type, const TensorShape& shape,
+                         TensorBuffer* buf) {
+  return Tensor(static_cast<DataType>(type), shape, buf);
+}
 
 // Create an empty tensor of type 'dtype'. 'shape' can be arbitrary, but has to
 // result in a zero-sized tensor.
diff --git a/tensorflow/c/c_api.h b/tensorflow/c/c_api.h
index e625d65..7479a1f 100644
--- a/tensorflow/c/c_api.h
+++ b/tensorflow/c/c_api.h
@@ -18,6 +18,7 @@ limitations under the License.
 
 #include <stddef.h>
 #include <stdint.h>
+#include ""tensorflow/core/framework/tensor.h""
 
 // --------------------------------------------------------------------------
 // C API for TensorFlow.
@@ -64,6 +65,10 @@ limitations under the License.
 //   and the API just provides high level controls over the number of
 //   devices of each type.
 
+using tensorflow::Tensor;
+using tensorflow::TensorBuffer;
+using tensorflow::TensorShape;
+
 #ifdef __cplusplus
 extern ""C"" {
 #endif
@@ -1030,6 +1035,15 @@ extern void TF_DeleteLibraryHandle(TF_Library* lib_handle);
 // in this address space.
 extern TF_Buffer* TF_GetAllOpList();
 
+namespace tensorflow{
+class TensorCApi {
+public:
+ static TensorBuffer* Buffer(const Tensor& tensor);
+ static Tensor MakeTensor(TF_DataType type, const TensorShape& shape,
+                          TensorBuffer* buf);
+};
+};
+
 #ifdef __cplusplus
 } /* end extern ""C"" */
 #endif
diff --git a/tensorflow/core/framework/tensor.h b/tensorflow/core/framework/tensor.h
index c2a1c3d..14e9881 100644
--- a/tensorflow/core/framework/tensor.h
+++ b/tensorflow/core/framework/tensor.h
@@ -36,7 +36,7 @@ limitations under the License.
 namespace tensorflow {
 
 class TensorBuffer;  // Forward declaration.
-class TensorCApi;
+// class TensorCApi;
 
 /// @ingroup core
 /// Represents an n-dimensional array of values.
```

and a small snippet of code to use it:
```
// Put an image in the cameraImg mat
cv::resize(image->image, cameraImg, cv::Size(inputwidth, inputheight), 0, 0, cv::INTER_AREA);
// Create a new tensor pointing to that memory:
const int64_t tensorDims[4] = {1,inputheight,inputwidth,3};
int *imNumPt = new int(1);
TF_Tensor* tftensor = TF_NewTensor(TF_DataType::TF_UINT8, tensorDims, 4,
                           cameraImg.data, inputheight * inputwidth * 3,
                           NULL, imNumPt);
Tensor inputImg = tensorflow::TensorCApi::MakeTensor(tftensor->dtype, tftensor->shape, tftensor->buffer);
```"
8031,Can't Build XLA compiler for Tensorflow,"I am trying to compiler tensorflow using bazel following the steps in https://www.tensorflow.org/install/install_sources#ConfigureInstallation.

My system is Ubuntu 16.04
gcc -v 5.4
I want to use XLA compiler so set it in the ./configuration file.
I am using CPU only for now.
I tried with pip and pip3

I get the following Error:

```
ERROR: /home/achang/tensorflow/tensorflow/compiler/tf2xla/BUILD:23:1: C++ compilation of rule '//tensorflow/compiler/tf2xla:xla_compiler' failed: gcc failed: error executing command 
  (cd /home/achang/.cache/bazel/_bazel_root/6c98a6c54aeed71fa731445a9f51836b/execroot/tensorflow && \
  exec env - \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++0x' '-march=native' -MD -MF bazel-out/local-opt/bin/tensorflow/compiler/tf2xla/_objs/xla_compiler/tensorflow/compiler/tf2xla/xla_compiler.pic.d '-frandom-seed=bazel-out/local-opt/bin/tensorflow/compiler/tf2xla/_objs/xla_compiler/tensorflow/compiler/tf2xla/xla_compiler.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D_DEBUG -DLLVM_BUILD_GLOBAL_ISEL -iquote . -iquote bazel-out/local-opt/genfiles -iquote external/jemalloc -iquote bazel-out/local-opt/genfiles/external/jemalloc -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -iquote external/protobuf -iquote bazel-out/local-opt/genfiles/external/protobuf -iquote external/eigen_archive -iquote bazel-out/local-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local-opt/genfiles/external/farmhash_archive -iquote external/highwayhash -iquote bazel-out/local-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/local-opt/genfiles/external/local_config_cuda -iquote external/llvm -iquote bazel-out/local-opt/genfiles/external/llvm -isystem external/jemalloc/include -isystem bazel-out/local-opt/genfiles/external/jemalloc/include -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/protobuf/src -isystem bazel-out/local-opt/genfiles/external/protobuf/src -isystem external/eigen_archive -isystem bazel-out/local-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/local-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/include -isystem bazel-out/local-opt/genfiles/external/local_config_cuda/cuda/include -isystem external/llvm/include -isystem bazel-out/local-opt/genfiles/external/llvm/include -isystem external/llvm/lib/IR -isystem bazel-out/local-opt/genfiles/external/llvm/lib/IR -isystem external/llvm/include/llvm/IR -isystem bazel-out/local-opt/genfiles/external/llvm/include/llvm/IR -isystem external/llvm/lib/Target/PowerPC -isystem bazel-out/local-opt/genfiles/external/llvm/lib/Target/PowerPC -isystem external/llvm/lib/Target/X86 -isystem bazel-out/local-opt/genfiles/external/llvm/lib/Target/X86 -isystem external/llvm/lib/Target/AArch64 -isystem bazel-out/local-opt/genfiles/external/llvm/lib/Target/AArch64 -isystem external/llvm/lib/Target/ARM -isystem bazel-out/local-opt/genfiles/external/llvm/lib/Target/ARM -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/compiler/tf2xla/xla_compiler.cc -o bazel-out/local-opt/bin/tensorflow/compiler/tf2xla/_objs/xla_compiler/tensorflow/compiler/tf2xla/xla_compiler.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
tensorflow/compiler/tf2xla/xla_compiler.cc: In function 'tensorflow::Status tensorflow::{anonymous}::CheckSignature(const DataTypeVector&, const std::vector<tensorflow::XlaCompiler::Argument>&)':
tensorflow/compiler/tf2xla/xla_compiler.cc:48:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < types.size(); ++i) {
                     ^
tensorflow/compiler/tf2xla/xla_compiler.cc: In member function 'tensorflow::Status tensorflow::XlaCompiler::BuildExecutable(const tensorflow::XlaCompiler::CompilationResult&, std::unique_ptr<xla::LocalExecutable>*)':
tensorflow/compiler/tf2xla/xla_compiler.cc:139:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < result.xla_input_shapes.size(); ++i) {
                     ^
tensorflow/compiler/tf2xla/xla_compiler.cc: In function 'tensorflow::Status tensorflow::{anonymous}::BuildArguments(const std::vector<tensorflow::XlaCompiler::Argument>&, bool, xla::ComputationBuilder*, std::vector<tensorflow::XlaContext::Argument>*, std::vector<int>*, std::vector<xla::Shape, std::allocator<xla::Shape> >*)':
tensorflow/compiler/tf2xla/xla_compiler.cc:228:8: error: 'size_typ' is not a member of 'std::vector<tensorflow::XlaCompiler::Argument>'
   for (std::vector<XlaCompiler::Argument>::size_typ i = 0;
        ^
tensorflow/compiler/tf2xla/xla_compiler.cc:229:8: error: 'i' was not declared in this scope
        i < args.size(); ++i) {
        ^
tensorflow/compiler/tf2xla/xla_compiler.cc: In member function 'tensorflow::Status tensorflow::XlaCompiler::CompileGraph(const string&, std::unique_ptr<tensorflow::Graph>, tensorflow::FunctionLibraryRuntime*, const std::vector<tensorflow::XlaCompiler::Argument>&, tensorflow::XlaCompiler::CompilationResult*)':
tensorflow/compiler/tf2xla/xla_compiler.cc:480:20: error: 'VariableWrite' is not a member of 'tensorflow::XlaCompiler'
   for (std::vector<XlaCompiler::VariableWrite>::size_type  i = 0;
                    ^
tensorflow/compiler/tf2xla/xla_compiler.cc:480:20: error: 'VariableWrite' is not a member of 'tensorflow::XlaCompiler'
tensorflow/compiler/tf2xla/xla_compiler.cc:480:46: error: template argument 1 is invalid
   for (std::vector<XlaCompiler::VariableWrite>::size_type  i = 0;
                                              ^
tensorflow/compiler/tf2xla/xla_compiler.cc:480:46: error: template argument 2 is invalid
tensorflow/compiler/tf2xla/xla_compiler.cc:480:60: error: expected ';' before 'i'
   for (std::vector<XlaCompiler::VariableWrite>::size_type  i = 0;
                                                            ^
tensorflow/compiler/tf2xla/xla_compiler.cc:480:60: error: 'i' was not declared in this scope
tensorflow/compiler/tf2xla/xla_compiler.cc:481:43: error: expected ')' before ';' token
        i < result->variable_updates.size(); ++i) {
                                           ^
tensorflow/compiler/tf2xla/xla_compiler.cc:480:49: warning: unused variable 'size_type' [-Wunused-variable]
   for (std::vector<XlaCompiler::VariableWrite>::size_type  i = 0;
                                                 ^
tensorflow/compiler/tf2xla/xla_compiler.cc:481:47: error: 'i' was not declared in this scope
        i < result->variable_updates.size(); ++i) {
                                               ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1162.629s, Critical Path: 913.22s
```
"
8030,"TensorBoard not finding files to display, suggested solution?","It seems that the default location for the log files, for most tutorials is in the system root tmp directory.. eg /tmp. This directory cannot normally be found by starting the tensorboard program according to instructions, nor do the debug instructions on the readme page for Tensorboard help at all.. Since they: Grep/ starting in debug Mode etc.. also depend on seeing directories within the user space.

I'd suggest simply modifying the tutorials to add the ./tmp so that the files end up in user space and also that the tensorBoard start up default directory be configured to match, only requiring configuration when needed for larger and more complex installs.

### Environment info
Operating System:
Ubuntu 16.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8029,Multiple broken g3doc links,"There are, by my count, at least 20 broken g3doc links scattered among documentation, code, and build scripts caused by [this commit](https://github.com/tensorflow/tensorflow/commit/1c707ac780313f48a6733dc3beedf4b8a2b3df77) migrating g3docs to the docs_src + source code documentation generating system.  So far, in relation to this, the following issues have been opened: #8014, #7999, #7989, and the following pull requests have been made: #8018, #7990, #7965.  Two of the pull requests, #8018 and #7990, take care of the two broken links on the core TensorFlow README and are waiting on the signing of CLAs.

Due to the restructuring that took place in the transition from g3doc to docs_src, a simple find-replace won't work.  For example, in reference to [this line in fully-connected-reader.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py#L20), `tensorflow/g3doc/how_tos/reading_data.md#reading-from-files` would have to be changed to `tensorflow/docs_src/programmers_guide/reading_data.md#reading-from-files`.  In fact, according to the new reference syntax, it should be changed to `@{$reading_data#reading-from-files}`.

Moreover, some g3doc files that are still linked to by existing documentation no longer exist in this repository.  E.g., the [adding an op](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/extend/adding_an_op.md) extension guide links to many of the Python files contained in [this g3doc folder](https://github.com/tensorflow/tensorflow/tree/8120e2a270c28e0a62b9f522164b196a90f113b7/tensorflow/g3doc/how_tos/adding_an_op) (which has now been deleted.)  Although these Python files should probably be reinstated somewhere, this appears to be the only collection of Python files in g3doc that were deleted and are still needed.

So, how best to go about fixing these issues?  Manually fix each broken link in a single pull request?  And where should the [Python files](https://github.com/tensorflow/tensorflow/tree/8120e2a270c28e0a62b9f522164b196a90f113b7/tensorflow/g3doc/how_tos/adding_an_op) pertaining to the [adding an op](https://www.tensorflow.org/extend/adding_an_op) extension guide be placed?"
8027,"Unable to load ""_lstm_ops.so"" when using rnn.LSTMBlockCell on Windows 10","### Environment info
Operating System:
Windows 10, 64 bit

Installed version of CUDA and cuDNN: 
CUDA 8.0
cuDNN 5.1

Intalled TF by the following:
pip3 install --upgrade tensorflow-gpu

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
```python
import tensorflow as tf
from tensorflow.contrib import rnn
seq_max_len = 6
num_hidden = 5
sample_size = 4

#print(seq_len_arr)
x = tf.placeholder(tf.float32, [None, seq_max_len, sample_size], name='x')

seq_len = tf.placeholder(tf.int32, [None])

lstm_cell = rnn.LSTMBlockCell(num_hidden)
lstmLayers = rnn.MultiRNNCell([lstm_cell]*1)
outputs, states = tf.nn.dynamic_rnn(lstmLayers, x, sequence_length=seq_len, dtype=np.float32)
init_op = tf.global_variables_initializer()
```

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
Traceback (most recent call last):
  File ""debug_lstm_zeros2.py"", line 33, in <module>
    outputs, states = tf.nn.dynamic_rnn(lstmLayers, x, sequence_length=seq_len, dtype=np.float32)
  File ""C:\Users\jun\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\ops\rnn.py"", line 545, in dynamic_rnn
    dtype=dtype)
  File ""C:\Users\jun\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\ops\rnn.py"", line 712, in _dynamic_rnn_loop
    swap_memory=swap_memory)
  File ""C:\Users\jun\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2626, in while_loop
    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""C:\Users\jun\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2459, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""C:\Users\jun\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2409, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""C:\Users\jun\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\ops\rnn.py"", line 695, in _time_step
    skip_conditionals=True)
  File ""C:\Users\jun\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\ops\rnn.py"", line 177, in _rnn_step
    new_output, new_state = call_cell()
  File ""C:\Users\jun\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\ops\rnn.py"", line 683, in <lambda>
    call_cell = lambda: cell(input_t, state)
  File ""C:\Users\jun\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\contrib\rnn\python\ops\core_rnn_cell_impl.py"", line 655, in __call__
    cur_inp, new_state = cell(cur_inp, cur_state)
  File ""C:\Users\jun\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\contrib\rnn\python\ops\lstm_ops.py"", line 402, in __call__
    use_peephole=self._use_peephole)
  File ""C:\Users\jun\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\contrib\rnn\python\ops\lstm_ops.py"", line 122, in _lstm_block_cell
    return _lstm_ops_so.lstm_block_cell(
AttributeError: 'NoneType' object has no attribute 'lstm_block_cell'

Thanks a lot for your assistance!

Jun"
8026,(Withdrawn),Withdrawn
8025,quantize_graph round and quantize modes are broken,"Looking at code in master. /CC @petewarden 

A `KeyError` is always produced.

[Consider the `round` mode](already_visited):
```python
    if self.mode == ""round"":
      self.already_visited = {}
      for output_node in output_nodes:
        self.round_nodes_recursively(output_node)
```
which [will fails for all calls](https://github.com/tensorflow/tensorflow/blob/fa4ba830f437fdb9dc1085b4d68a3bab41a16e20/tensorflow/tools/quantization/quantize_graph.py#L402-L404):
```
  def round_nodes_recursively(self, current_node):
    """"""The entry point for simple rounding quantization.""""""
    if self.already_visited[current_node.name]:
```
with a key error since the `already_visited` dict will be empty. "
8024,Building with XLA throws 'error: non-constant-expression cannot be narrowed from type 'std::vector<se::DeviceMemoryBase>::size_type' (aka 'unsigned long') to 'long long' in initializer list',"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
https://github.com/tensorflow/tensorflow/issues/5143
https://github.com/bazelbuild/bazel/issues/596
https://github.com/tensorflow/tensorflow/issues/4103
http://stackoverflow.com/questions/39157631/tensorflow-build-error-with-bazel
http://stackoverflow.com/questions/37313212/tensorflow-bazel-build-failing
http://stackoverflow.com/questions/34941620/unable-to-build-tensorflow-from-source-with-bazel-22nd-january-2016
http://stackoverflow.com/questions/38603017/tensorflow-install-bazel-build-error-failed-package-loads

Unfortunately none of these posts report similar errors.


### Environment info
Operating System:
Mac OSX Sierra

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
Neither are installed on my system:
```
~/tensorflow master*
 ls -l /path/to/cuda/lib/libcud*
ls: /path/to/cuda/lib/libcud*: No such file or directory
```

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
```
~/tensorflow master*
 git rev-parse HEAD
8cac382a5425d64f3083cb5adec525baa163e18e
```
2. The output of `bazel version`
```
~/tensorflow master*
 bazel version
Warning: ignoring LD_PRELOAD in environment.
............
Build label: 0.4.4-homebrew
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Feb 2 01:05:15 2017 (1485997515)
Build timestamp: 1485997515
Build timestamp as int: 1485997515
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I simply followed [these instructions](https://www.tensorflow.org/install/install_sources). This was my interaction with the `configure` script:
```
~/tensorflow master* 4m 13s
 ./configure
Please specify the location of python. [Default is /usr/local/bin/python]: 
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] n
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] n
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y
XLA JIT support will be enabled for TensorFlow
Found possible Python library paths:
  /usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages
Please input the desired Python library path to use.  Default is [/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages]

Using python library path: /usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages
Do you wish to build TensorFlow with OpenCL support? [y/N] n
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] n
No CUDA support will be enabled for TensorFlow
Configuration finished
Warning: ignoring LD_PRELOAD in environment.
Extracting Bazel installation...
..........................................................
INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
Warning: ignoring LD_PRELOAD in environment.
..........................................................
INFO: All external dependencies fetched successfully.
```

I then ran
```
~/tensorflow master* 1m 35s
 bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
```
and got the following error

```
ERROR: /Users/ethan/tensorflow/tensorflow/compiler/xla/service/BUILD:406:1: C++ compilation of rule '//tensorflow/compiler/xla/service:allocation_tracker' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 105 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
tensorflow/compiler/xla/service/allocation_tracker.cc:178:54: error: non-constant-expression cannot be narrowed from type 'std::vector<se::DeviceMemoryBase>::size_type' (aka 'unsigned long') to 'long long' in initializer list [-Wc++11-narrowing]
        ShapeUtil::GetSubshape(allocation->shape(), {i}),
                                                     ^
tensorflow/compiler/xla/service/allocation_tracker.cc:178:54: note: insert an explicit cast to silence this issue
        ShapeUtil::GetSubshape(allocation->shape(), {i}),
                                                     ^
                                                     static_cast<long long>( )
1 error generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```

### What other attempted solutions have you tried?

Because XLA seemed to be causing the error, I tried building without it and the build was successful.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

- It does look like bazel is recording a log but I couldn't find it.

- I posted the full output of `bazel build` in [this gist](https://gist.github.com/lobachevzky/3bcd214e747613d4d0aaaafda2f48f4c).

"
8023,logger dublication,"When basicConfig() from logging (python 2.7.11) is used, tensorflow duplicates output of logger (load logger twice).

Example code and output:
```
from logging import basicConfig
import tensorflow as tf
from tensorflow.contrib.learn import Estimator

basicConfig()
tf.logging.set_verbosity(tf.logging.INFO)

def cnn_model_fn(features,labels,mode):
    pass

if __name__ == '__main__':
    classifier = Estimator(
        model_fn=cnn_model_fn)



```

output: 
WARNING:tensorflow:Using temporary folder as model directory: /var/folders/qn/z_b3zdss1g730kn_mlrwcwnc0000gn/T/tmpEdygBY
WARNING:tensorflow:Using temporary folder as model directory: /var/folders/qn/z_b3zdss1g730kn_mlrwcwnc0000gn/T/tmpEdygBY
INFO:tensorflow:Using default config.
INFO:tensorflow:Using default config.
"
8022,ADD to CONTRIB: single_image_random_do_stereograms for data analysis,"I have written a TensorFlow OP ""single_image_random_dot_stereograms"" which takes a 2 D array with Z values and outputs a Stereogram image for use in ""encode_png/jpg"".  

it is based upon this paper:
http://www.learningace.com/doc/4331582/b6ab058d1e206d68ab60e4e1ead2fe6e/sirds-paper

I am sure this can be used to output data to TensorBoard as well but haven't tried that yet.

I would like to offer to add this as a contrib for TensorFlow and would like some advice if there is interest on getting the process started.  Right now I compile it into the ""user_ops"" section, but think it needs to move for contrib.

Basically, this helps display 3D data without hidden lines like a ""waterfall"" plot.

Here is the basic code as a demo:

    img=[[1,2,3,3,2,1],
         [1,2,3,4,5,2],
         [1,2,3,4,5,3],
         [1,2,3,4,5,4],
         [6,5,4,4,5,5]]

    session = tf.InteractiveSession()

    sirds = single_image_random_dot_stereograms(img,convergence_dots_size=8,number_colors=256,normalize=True)

    out = sirds.eval()

    png = tf.image.encode_png(out).eval()

    with open('picture_out.png', 'wb') as f:
        f.write(png)

The output image then looks like the attached image.  If you can view this image outside of github it will look better.  Try to look beyond the picture so the 2 dots at the bottom appear to be 3 dots.  The image should then converge and see the shape as described in ""img"" above.
![picture_out](https://cloud.githubusercontent.com/assets/18412448/23521590/a2deb0fc-ff4d-11e6-9dfc-b00759f921c8.png)

If there is interest, I can breakup the code to better match one of the contrib subdirectories and start a pull request.

Here is a different image, inverted cone, windowed in full color.
![picture_out_color_cone](https://cloud.githubusercontent.com/assets/18412448/23529957/a4737112-ff6d-11e6-8d35-dbdd29049edf.png)

"
8021,pywrap_tensorflow.list_devices() allocates all available memory (on all GPU devices),"### Environment info
Operating System:
Ubuntu 14.04.5 LTS
Python 3.4.3

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
libcudadevrt.a  libcudart.so.8.0     libcudart_static.a  libcudnn.so.5      libcudnn_static.a
libcudart.so    libcudart.so.8.0.61  libcudnn.so         libcudnn.so.5.1.5

If installed from binary pip package, provide:

1. A link to the pip package you installed:
`pip install tensorflow-gpu`
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
```
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.0
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
```
from tensorflow.python import pywrap_tensorflow
pywrap_tensorflow.list_devices()
```

I have tried this locally on a system with two Quadro K620 and another system with one TITAN X."
8020,Make quantize_graph importable,"Currently `quantize_graph` is a runnable script but does not seem to be importable.
This import fails:
```
from tensorflow.tools.quantization import quantize_graph
```"
8019,[Windows - Bazel] ERROR: No toolchain found for cpu 'x64_windows',"It is more probable to be something related to Bazel but I am posting here as well just in case.  

Windows 10 x64
Msys2 v20160205
Bazel 0.4.3 (tested with 0.4.4 as well)
Visual C++ 2015

I get this error when building with Bazel after successfully configuring TensorFlow 
```
$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
ERROR: No toolchain found for cpu 'x64_windows'. Valid cpus are: [
  k8,
  piii,
  arm,
  darwin,
  ppc,
].
INFO: Elapsed time: 24.952s
```

#### Steps to reproduce:
- Compile Bazel or use a pre-built binary (both yielded same result)
- Clone TensorFlow repository
- Configure TensorFlow
- Build it with bazel

#### I tried so far as suggested:

- Configuring again and build ([entire log](https://github.com/bazelbuild/bazel/issues/2594#issuecomment-283073027)) using:
```
export BUILD_OPTS='--cpu=x64_windows_msvc --host_cpu=x64_windows_msvc --copt=/w --verbose_failures --experimental_ui'
bazel build -c opt $BUILD_OPTS tensorflow/tools/pip_package:build_pip_package
```

- Using `-c opt` instead of `--config=cuda`.

- Running without `--config=cuda`, which generates the error:
 `AssertionError: Could not find python binary: python3.exe` perhaps related to [Bazel issue 2457](https://github.com/bazelbuild/bazel/issues/2457).

[Issue on Bazel](https://github.com/bazelbuild/bazel/issues/2594).
"
8017,AttributeError: module 'tensorflow' has no attribute 'streaming_accuracy',"```
#Accuracy calculation
accuracy = tf.streaming_accuracy(y_p, mnist.train.lables, weights=None,
                       metrics_collections=None, updates_collections=None,
                       name=""Accuracy"")
acc = sess.run(accuracy)
print (acc)
```
while calculating Accuracy in my model I encountered this error!
Don't have any clue why?"
8016,C++ demo failed with lib in the makefile project,"I can not build and run the demo on the C++ API page:

https://www.tensorflow.org/api_guides/cc/guide

When link to the lib built in the /contrib/makefile project, does that mean that the cc/ops are totally not supported or I missed some steps or information?

Current errors are as the following:

```
../../../../Git/tensorflow/bazel-out/local-fastbuild/genfiles/tensorflow/cc/ops/array_ops.h:24:57: error: no type named 'Input' in namespace 'tensorflow::ops'; did you mean simply 'Input'?
  BatchMatrixBandPart(const ::tensorflow::Scope& scope, ::tensorflow::ops::Input
                                                        ^~~~~~~~~~~~~~~~~~~~~~~~
                                                        Input
```"
8014,404 in contrib.layers documentation,"In the official doc for version 1.0, [this link](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/linear) to **tf.contrib.layers.linear** in [here](https://www.tensorflow.org/api_guides/python/contrib.layers) leads to a 404.

"
8012,Not support i686?,"Install from source.

 git rev-parse HEAD
`8cac382a5425d64f3083cb5adec525baa163e18e`

 bazel version
```
Build label: 0.4.4- (@non-git)
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Mar 2 08:11:08 2017 (1488442268)
Build timestamp: 1488442268
Build timestamp as int: 1488442268
```

./configure
Please specify the location of python. [Default is /env/tensorflow/bin/python]:
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]:
Do you wish to use jemalloc as the malloc implementation? [Y/n]
jemalloc enabled
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N]
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N]
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]
No XLA support will be enabled for TensorFlow
Found possible Python library paths:
  /env/tensorflow/lib/python3.5/site-packages
Please input the desired Python library path to use.  Default is [/env/tensorflow/lib/python3.5/site-packages]

Using python library path: /env/tensorflow/lib/python3.5/site-packages
Do you wish to build TensorFlow with OpenCL support? [y/N]
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N]
No CUDA support will be enabled for TensorFlow
Configuration finished


.
INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
..................................
ERROR: /prj/tmp/tensorflow/tensorflow/core/distributed_runtime/rpc/BUILD:321:1: no such package '@grpc//': Error downloading [http://bazel-mirror.storage.googleapis.com/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz, https://github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz] to /root/.cache/bazel/_bazel_root/3dd36c466b8b5ba9ee6025155ebdf4cb/external/grpc/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: Connection reset and referenced by '//tensorflow/core/distributed_runtime/rpc:grpc_testlib_server'.
ERROR: /prj/tmp/tensorflow/third_party/eigen3/BUILD:20:1: no such package '@eigen_archive//': Error downloading [http://bazel-mirror.storage.googleapis.com/bitbucket.org/eigen/eigen/get/290bfb42684a.tar.gz, https://bitbucket.org/eigen/eigen/get/290bfb42684a.tar.gz] to /root/.cache/bazel/_bazel_root/3dd36c466b8b5ba9ee6025155ebdf4cb/external/eigen_archive/290bfb42684a.tar.gz: Tried to reconnect at offset 498,992 but server didn't support it and referenced by '//third_party/eigen3:eigen3'.
ERROR: Evaluation of query ""deps(((//tensorflow/... - //tensorflow/contrib/nccl/...) - //tensorflow/examples/android/...))"" failed: errors were encountered while computing transitive closure.

"
8011,TypeError: Fetch argument None has invalid type <class 'NoneType'>,"Feature request for a better error description OR for better summary handling:

The following code works fine if some summaries where defined before:
```
ops=[] 
ops += [tf.summary.merge_all()]
session.run(ops)
```
However if there were no summaries we get:
TypeError: Fetch argument None has invalid type <class 'NoneType'>

Which is really saying:  ""One of the session.run ops where empty, which is forbidden.""
Alternatively let merge_all return a NoOp if there are no summaries."
8010,tf.identity() not copying varying inputs,"When using `tf.identity()` it seems like a proper implementation to pass through the input tensor without copying. However, when the input is the value of a variable it might change later, leading to surprising behavior:

```python
var = tf.Variable(1)
old = tf.identity(var.value())
with tf.control_dependencies([old]):
  with tf.control_dependencies([var.assign_add(1)]):
    new = tf.identity(var.value())

sess = tf.Session()
sess.run(tf.global_variables_initializer())
print(sess.run([old, new]))  # Unexpected: [2, 2]
```

I would think this code should be equivalent to the following workaround that uses a second variable to remember the previous value:

```python
var = tf.Variable(1)
old = tf.Variable(var.initialized_value())
with tf.control_dependencies([old.assign(var.value())]):
  with tf.control_dependencies([var.assign_add(1)]):
    new = tf.identity(var.value())

sess = tf.Session()
sess.run(tf.global_variables_initializer())
print(sess.run([old, new]))  # Expected: [1, 2]
```

Can we make `tf.identity()` aware of whether its input is static or varying, to always return the value from the time it's executed?"
8009,Connection timed out when loading MNIST,"Hi,

I got **Connection timed out** error when I was trying to load MNIST data using tensorflow script.
```
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
```

Error:
```
Traceback (most recent call last):
  File ""create_mnistm.py"", line 9, in <module>
    mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
  File ""/home/user/miniconda2/envs/domain_adapt/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 211, in read_data_sets
    SOURCE_URL + TRAIN_IMAGES)
  File ""/home/user/miniconda2/envs/domain_adapt/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py"", line 208, in maybe_download
    temp_file_name, _ = urlretrieve_with_retry(source_url)
  File ""/home/user/miniconda2/envs/domain_adapt/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py"", line 165, in wrapped_fn
    return fn(*args, **kwargs)
  File ""/home/user/miniconda2/envs/domain_adapt/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py"", line 190, in urlretrieve_with_retry
    return urllib.request.urlretrieve(url, filename)
  File ""/home/user/miniconda2/envs/domain_adapt/lib/python2.7/urllib.py"", line 98, in urlretrieve
    return opener.retrieve(url, filename, reporthook, data)
  File ""/home/user/miniconda2/envs/domain_adapt/lib/python2.7/urllib.py"", line 245, in retrieve
    fp = self.open(url, data)
  File ""/home/user/miniconda2/envs/domain_adapt/lib/python2.7/urllib.py"", line 213, in open
    return getattr(self, name)(url)
  File ""/home/user/miniconda2/envs/domain_adapt/lib/python2.7/urllib.py"", line 350, in open_http
    h.endheaders(data)
  File ""/home/user/miniconda2/envs/domain_adapt/lib/python2.7/httplib.py"", line 1038, in endheaders
    self._send_output(message_body)
  File ""/home/user/miniconda2/envs/domain_adapt/lib/python2.7/httplib.py"", line 882, in _send_output
    self.send(msg)
  File ""/home/user/miniconda2/envs/domain_adapt/lib/python2.7/httplib.py"", line 844, in send
    self.connect()
  File ""/home/user/miniconda2/envs/domain_adapt/lib/python2.7/httplib.py"", line 821, in connect
    self.timeout, self.source_address)
  File ""/home/user/miniconda2/envs/domain_adapt/lib/python2.7/socket.py"", line 575, in create_connection
    raise err
IOError: [Errno socket error] [Errno 110] Connection timed out
```

### Environment info
Operating System: Ubuntu 16.04

Installed version of CUDA and cuDNN: 
```
-rw-r--r-- 1 root root   558720 Jan 19 11:54 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Jan 19 11:54 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Jan 19 11:54 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rwxr-xr-x 1 root root   415432 Jan 19 11:54 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 Jan 19 11:54 /usr/local/cuda/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 79337624 Jan 19 12:13 /usr/local/cuda/lib64/libcudnn.so
-rwxr-xr-x 1 root root 79337624 Jan 19 12:13 /usr/local/cuda/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 79337624 Jan 19 12:13 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Jan 19 12:13 /usr/local/cuda/lib64/libcudnn_static.a
```


The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`:
```
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
0.12.1
```
"
8006,tensorflow-gpu fail for windows 7 T_T,"My computer installed tensorflow-gpu for windows 7 (FX 2800M)

but GPU not understanding my counter...T_T

always tensorflow cpu understandig...

CUDA  v8.0 and v6.5 Cudnn v5.1 and v 2.1

Fail log is 
F c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_driver.cc:94] Check failed: s.ok() could not find cuDevice

plz.. help me!!
"
8005,tf.contrib.rnn module object is not callable. ,"I am trying to run a tutorial (http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and have some issue with modules that have moved. 

I researched a bit and found that some classes have moved and fixed: 
tf.nn.rnn_cell.BasicRNNCell(state_size)
to 
tf.contrib.rnn.BasicRNNCell(state_size)

This works now.

However, I have this function I can't seem to fix: 

rnn_outputs, final_state = tf.nn.rnn(cell, rnn_inputs, initial_state=init_state)
--> as expected, says: module 'tensorflow.python.ops.nn' has no attribute 'rnn'

So I tried: 
rnn_outputs, final_state = tf.contrib.rnn(cell, rnn_inputs, initial_state=init_state)

But that says: 'module' object is not callable

I am using python3 (tried 2 as well), with a fresh installed TensorFlow 1.0.0 through pip3. 

I tried looking at the API, but it's not making much sense to me, as I am still going through the tutorial to try to understand what is happening. I would think maybe the arguments have changed, or maybe this is in a sub-function now? https://www.tensorflow.org/api_docs/python/tf/contrib/rnn
"
8004,tf.contrib.slim API not support tensorflow newest API,"I like using slim write cnn net structures, but there is something emergency to catch up tensorflow newest API, 
this simple code got deprecate warning follow!
```
predictions = inference.lenet(images, num_classes=FLAGS.num_classes+1, activation_fn=None)
slim.losses.softmax_cross_entropy(predictions, labels)
```

```
softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.
Instructions for updating:
Use tf.losses.softmax_cross_entropy instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:394: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.
Instructions for updating:
Use tf.losses.compute_weighted_loss instead.
```
I think this is essue about slim.losses, however I tried using `tf.losses.softmax_cross_entropy`instead, got this error:
```
Traceback (most recent call last):
  File ""train_tiny5_tensorflow.py"", line 154, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""train_tiny5_tensorflow.py"", line 150, in main
    run_training()
  File ""train_tiny5_tensorflow.py"", line 128, in run_training
    tf.losses.softmax_cross_entropy(predictions, labels)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py"", line 529, in softmax_cross_entropy
    name=""xentropy"")
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1617, in softmax_cross_entropy_with_logits
    precise_logits, labels, name=name)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 2265, in _softmax_cross_entropy_with_logits
    features=features, labels=labels, name=name)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 585, in apply_op
    param_name=input_name)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 61, in _SatisfiesTypeConstraint
    "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
TypeError: Value passed to parameter 'features' has DataType int32 not in list of allowed values: float16, float32, float64
```"
8003,A small problem about tf.case function and Tensor's shape,"I write the following code on jupyter notebook:
a = tf.placeholder(shape=(5, ),  dtype=tf.float32)
b = tf.placeholder(shape=(5, ),  dtype=tf.float32)
x = tf.constant(0)
y = tf.constant(0)
f1 = lambda: a
f2 = lambda: b
res_tensor = tf.case([ (tf.not_equal(x, y), f1) ], default=f2)

But I found that the res_tensor's shape is unknown rather than (5, )."
8002, load model package in a linux tensorflow project,"
![model](https://cloud.githubusercontent.com/assets/17719977/23498012/a391d9e4-ff60-11e6-895b-5aeb167d612d.png)
Hello,master:
      How do I load models package in a linux tensorflow project that has already been installed?thanks"
8000,Android Camera Detection with Yolo model not working,"I trained and generated my yolo model accodring to the repository https://github.com/thtrieu/darkflow. While testing on few images locally, the model detects objects.

Once I build and test it on the phone (adding .pb to assets and setting flag to YOLO), TFDetect runs but I obtain no detections at all.

What could be the cause of that and how could I actually start debugging?

(trying to run the default graph-tiny-yolo-voc.pb specified in the example demo, succeeded)"
7999,Broken link,"The ""See Download and Setup"" link in the readme is broken."
7998,Strange messages shown up when define a tensorflow variable,"When I try to define a variable, the following messages show. 
However, these messages will only show when I define the first variable.  


Python 3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> bias = tf.Variable(tf.random_normal([1]))
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
"
7997,I can successfully open cupti64_80.dll library. But It seems the program run all the time and will never stop.,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System: Windows 10

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

`C:\Users\Jun Xiao>nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Sat_Sep__3_19:05:48_CDT_2016
Cuda compilation tools, release 8.0, V8.0.44`

1. A link to the pip package you installed:
`C:\Users\Jun Xiao>pip3.5 install --upgrade tensorflow-gpu
Requirement already up-to-date: tensorflow-gpu in c:\users\jun xiao\appdata\local\programs\python\python35\lib\site-packages
Requirement already up-to-date: protobuf>=3.1.0 in c:\users\jun xiao\appdata\local\programs\python\python35\lib\site-packages (from tensorflow-gpu)
Requirement already up-to-date: six>=1.10.0 in c:\users\jun xiao\appdata\local\programs\python\python35\lib\site-packages (from tensorflow-gpu)
Requirement already up-to-date: numpy>=1.11.0 in c:\users\jun xiao\appdata\local\programs\python\python35\lib\site-packages (from tensorflow-gpu)
Requirement already up-to-date: wheel>=0.26 in c:\users\jun xiao\appdata\local\programs\python\python35\lib\site-packages (from tensorflow-gpu)
Requirement already up-to-date: setuptools in c:\users\jun xiao\appdata\local\programs\python\python35\lib\site-packages (from protobuf>=3.1.0->tensorflow-gpu)
Requirement already up-to-date: appdirs>=1.4.0 in c:\users\jun xiao\appdata\local\programs\python\python35\lib\site-packages (from setuptools->protobuf>=3.1.0->tensorflow-gpu)
Requirement already up-to-date: packaging>=16.8 in c:\users\jun xiao\appdata\local\programs\python\python35\lib\site-packages (from setuptools->protobuf>=3.1.0->tensorflow-gpu)
Requirement already up-to-date: pyparsing in c:\users\jun xiao\appdata\local\programs\python\python35\lib\site-packages (from packaging>=16.8->setuptools->protobuf>=3.1.0->tensorflow-gpu)`
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
`>>> import tensorflow
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally
>>> print(tensorflow.__version__)
1.0.0`

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
`I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:841] OptimizeLoss/gradients/generator/fully_connected/BatchNorm/moments/normalize/Square_grad/mul/x: (Const)/job:localhost/replica:0/task:0/gpu:0
OptimizeLoss/gradients/generator/fully_connected/BatchNorm/moments/sufficient_statistics/SquaredDifference_grad/scalar:
(Const): /job:localhost/replica:0/task:0/gpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:841] OptimizeLoss/gradients/generator/fully_connected/BatchNorm/moments/sufficient_statistics/SquaredDifference_grad/scalar: (Const)/job:localhost/replica:0/task:0/gpu:0
OptimizeLoss/train/value: (Const): /job:localhost/replica:0/task:0/cpu:0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:841] OptimizeLoss/train/value: (Const)/job:localhost/replica:0/task:0/cpu:0
[I 13:13:00.712 NotebookApp] Saving file at /WGAN-tensorflow-master/WGAN.ipynb
successfully opened CUDA library cupti64_80.dll locally`

`KeyboardInterrupt                         Traceback (most recent call last)
<ipython-input-10-58ca95c5b364> in <module>()
----> 1 main()

<ipython-input-9-221574c19227> in main()
     32                 citers = Citers
     33             for j in range(citers):
---> 34                 feed_dict = next_feed_dict()
     35                 if i % 100 == 99 and j == 0:
     36                     run_options = tf.RunOptions(

<ipython-input-9-221574c19227> in next_feed_dict()
     20                                mode='constant', constant_values=-1)
     21             train_img = np.expand_dims(train_img, -1)
---> 22         batch_z = np.random.normal(0, 1, [batch_size, z_dim])             .astype(np.float32)
     23         feed_dict = {real_data: train_img, z: batch_z}
     24         return feed_dic`


### What other attempted solutions have you tried?
 I google for the problem. At first, I can't open ""not find cuptiActivityRegisterCallbacksin libcupti DSO"", and I adopt the idea from the [""menggangmark"".](https://github.com/tensorflow/tensorflow/issues/6235)
 and it can successfully open the cupti64_80 library. 
### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

"
7996,Cant  implement Miking Human,"I want to implement a neural architecture from  A Neural Architecture Mimicking Humans
End-to-End for Natural Language Inference . In keras , I can get accuracy  86% . But the same  architecture in tensorflow , just get 76% . Here is the main code 
`class miki(BaseModel):

    def build(self):
        params = self.params
        batch_size = params.batch_size
        max_length = params.max_length
        nr_hidden = params.nr_hidden
        keep_dr=params.dr


        ids1 = tf.placeholder(tf.int32, shape=[batch_size, max_length], name='premise')  # none l1
        ids2 = tf.placeholder(tf.int32, shape=[batch_size, max_length], name='hypoyhesis')
        lable = tf.placeholder(tf.float32, shape=[batch_size, 3], name='lable')
        train_dr = tf.placeholder(tf.bool)


        oov_W = np.load(open(os.path.join('glove/', 'oov_W.weights'), 'rb'))
        oov_W = oov_W.astype('float32')
        unchanged_W = np.load(open(os.path.join('glove/', 'unchanged_W.weights'), 'rb'))
        unchanged_W = unchanged_W.astype('float32')
        embedding = np.concatenate((oov_W, unchanged_W), axis=0)
        W = tf.get_variable(name=""W"", shape=embedding.shape, initializer=tf.constant_initializer(embedding),
                            trainable=False)

        def he_nomal(fan_in):
            s = np.sqrt(2. / fan_in)
            seed = np.random.randint(10e8)
            return tf.random_normal_initializer(0.0, s, dtype=tf.float32, seed=seed)

        with tf.variable_scope('embedding1'):
            embed1 = tf.nn.embedding_lookup(W, ids1)  # none l1  h
            em_reshape1=tf.reshape(embed1,[-1,nr_hidden])
            network_en1 = tl.layers.InputLayer(inputs=em_reshape1, name='em_layer1-1')
            network_en1 = tl.layers.DropoutLayer(network_en1, is_fix=True, keep=keep_dr, name='emdrop1-1',is_train=train_dr)
            network_en1 = tl.layers.DenseLayer(network_en1, n_units=nr_hidden,
                                           act=tf.nn.relu, name='emrelu2-1')
            embed1_out=network_en1.outputs
            embed1_re=tf.reshape(embed1_out,[-1,max_length,nr_hidden])


        with tf.variable_scope('embedding2'):
            embed2 = tf.nn.embedding_lookup(W, ids1)  # none l1  h
            em_reshape2=tf.reshape(embed2,[-1,nr_hidden])
            network_en2 = tl.layers.InputLayer(inputs=em_reshape2, name='em_layer2-1')
            network_en2 = tl.layers.DropoutLayer(network_en2, is_fix=True, keep=keep_dr, name='emdrop2-1',is_train=train_dr)
            network_en2 = tl.layers.DenseLayer(network_en2, n_units=nr_hidden,
                                           act=tf.nn.relu, name='emrelu1-1')
            embed2_out=network_en2.outputs
            embed2_re=tf.reshape(embed2_out,[-1,max_length,nr_hidden])


        _seq_len = tf.fill(tf.expand_dims(batch_size, 0),
                           tf.constant(max_length, dtype=tf.int32))

        with tf.variable_scope('ecode1'):
            fwd_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)
            back_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)
            h, _ = tf.nn.bidirectional_dynamic_rnn(
                cell_fw=fwd_cell, cell_bw=back_cell, inputs=embed1_re, sequence_length=(_seq_len), dtype=tf.float32)
            encode1 = tf.concat(2, h)  # none l 2h

        with tf.variable_scope('ecode2'):
            fwd_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)
            back_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)
            h, _ = tf.nn.bidirectional_dynamic_rnn(
                cell_fw=fwd_cell, cell_bw=back_cell, inputs=embed2_re, sequence_length=(_seq_len), dtype=tf.float32)
            encode2 = tf.concat(2, h)  # none l 2h

        with tf.variable_scope('atte_layer'):

            encode2_tr = tf.transpose(encode2, perm=[0, 2, 1])  # none 2h l2
            attention = tf.batch_matmul(encode1, encode2_tr)  # none l1 l2
            e = tf.exp(attention - tf.reduce_max(attention, 2, keep_dims=True))
            s = tf.reduce_sum(e, 2, keep_dims=True)  #none l1 1
            am_att = e / s  #none l1 l2
            aligh_attention = tf.batch_matmul(am_att, encode2)  # none l1 2h
            concat = tf.concat(2, [aligh_attention, encode1])  # none l1 4h
            concat_reshape=tf.reshape(concat,[-1,4*nr_hidden])    #none*l1 4h


        with tf.variable_scope('task1_operator'):
            network_task1=tl.layers.InputLayer(concat_reshape, name='task_layer1-1')
            network_task1 = tl.layers.DropoutLayer(network_task1, keep=keep_dr, name='drop1-1',is_train=train_dr,is_fix=True)
            network_task1 = tl.layers.DenseLayer(network_task1, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu1-1')
            network_task1 = tl.layers.DropoutLayer(network_task1, keep=keep_dr, name='drop1-2',is_train=train_dr,is_fix=True)
            network_task1 = tl.layers.DenseLayer(network_task1, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu1-2')
            task1 = network_task1.outputs  # none*l 2h
            task1_re = tf.reshape(task1, [-1, max_length, 2 * nr_hidden])  #none l h

        with tf.variable_scope('task2_operator'):
            network_task2=tl.layers.InputLayer(concat_reshape, name='task_layer2-1')
            network_task2 = tl.layers.DropoutLayer(network_task2, keep=keep_dr, name='drop2-1',is_train=train_dr,is_fix=True)
            network_task2 = tl.layers.DenseLayer(network_task2, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu2-1')
            network_task2 = tl.layers.DropoutLayer(network_task2, keep=keep_dr, name='drop2-2',is_train=train_dr,is_fix=True)
            network_task2 = tl.layers.DenseLayer(network_task2, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu2-2')
            task2 = network_task2.outputs  # none*l 2h
            task2_re = tf.reshape(task2, [-1, max_length, 2 * nr_hidden])  #none l h

        with tf.variable_scope('task3_operator'):
            network_task3=tl.layers.InputLayer(concat_reshape, name='task_layer3-1')
            network_task3 = tl.layers.DropoutLayer(network_task3, keep=keep_dr, name='drop3-1',is_train=train_dr,is_fix=True)
            network_task3 = tl.layers.DenseLayer(network_task3, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu3-1')
            network_task3 = tl.layers.DropoutLayer(network_task3, keep=keep_dr, name='drop3-2',is_train=train_dr,is_fix=True)
            network_task3 = tl.layers.DenseLayer(network_task3, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu3-2')
            task3 = network_task3.outputs  # none*l 2h
            task3_re = tf.reshape(task3, [-1, max_length, 2 * nr_hidden])  #none l h

        with tf.variable_scope('task4_operator'):
            network_task4=tl.layers.InputLayer(concat_reshape, name='task_layer4-1')
            network_task4 = tl.layers.DropoutLayer(network_task4, keep=keep_dr, name='drop4-1',is_train=train_dr,is_fix=True)
            network_task4 = tl.layers.DenseLayer(network_task4, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu4-1')
            network_task4 = tl.layers.DropoutLayer(network_task4, keep=keep_dr, name='drop4-2',is_train=train_dr,is_fix=True)
            network_task4 = tl.layers.DenseLayer(network_task4, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu4-2')
            task4 = network_task4.outputs  # none*l 2h
            task4_re = tf.reshape(task4, [-1, max_length, 2 * nr_hidden])  #none l h


        with tf.variable_scope('gate_operator'):
            network_gate= tl.layers.InputLayer(concat_reshape, name='gate_layer')
            network_gate = tl.layers.DropoutLayer(network_gate, is_fix=True, keep=keep_dr, name='gate_drop1',is_train=train_dr)
            network_gate = tl.layers.DenseLayer(network_gate, n_units=2 * nr_hidden,
                                                act=tf.nn.relu, name='relu_gate1')
            network_gate = tl.layers.DropoutLayer(network_gate, is_fix=True, keep=keep_dr, name='gate_drop2',is_train=train_dr)
            network_gate = tl.layers.DenseLayer(network_gate, n_units=4,
                                                act=tf.nn.softmax, name='relu_gate2')
            gate = network_gate.outputs
            gate_re = tf.reshape(gate, [-1, max_length, 4])  # none l 2

        def repeat(gate_i):

            gate_a = gate_i  # none l
            gate_b = tf.tile(gate_a, [1, 2 * nr_hidden])
            gate_c = tf.reshape(gate_b, [-1, 2 * nr_hidden, max_length])
            gate_d = tf.transpose(gate_c, perm=[0, 2, 1])
            return gate_d  # none l 2h

        def Out(t1,t2,t3,t4,gate):
            gate_shuffle = tf.transpose(gate, perm=[0, 2, 1])  # none 4 l

            g1 = repeat(gate_shuffle[:, 0, :])  # none l
            g2 = repeat(gate_shuffle[:, 1, :])
            g3 = repeat(gate_shuffle[:, 2, :])  # none l
            g4 = repeat(gate_shuffle[:, 3, :])

            O1=g1*t1
            O2 = g2 * t2
            O3=g3*t3
            O4 = g4 * t4

            out = O1 + O2 +O3+O4
            return out

        task_output = Out(task1_re,task2_re,task3_re,task4_re,gate_re)  # none l1 2h


        with tf.variable_scope('aggregate'):
            fwd_lstm = tf.nn.rnn_cell.BasicLSTMCell(2 * nr_hidden)
            x_output, x_state = tf.nn.dynamic_rnn(cell=fwd_lstm, inputs=task_output, dtype=tf.float32,sequence_length=(_seq_len))

            composable = x_output[:,-1, :]  # none  2h

        with tf.variable_scope('Entaiment'):
            network3 = tl.layers.InputLayer(composable, name='layer_entalment1')
            network3 = tl.layers.DropoutLayer(network3, keep=keep_dr, name='entaidrop3',is_train=train_dr,is_fix=True)
            network3 = tl.layers.DenseLayer(network3, n_units=2 * nr_hidden,
                                            act=tf.nn.tanh, name='layer_entaiment2')
            network3 = tl.layers.DenseLayer(network3, n_units=3,
                                            act=tf.nn.softmax, name='entai_relu3.2')
            entaiment = network3.outputs
with tf.name_scope('Loss'):
            cross_entropy = tf.reduce_mean(-tf.reduce_sum(lable * tf.log(entaiment), reduction_indices=[1]))

            loss = cross_entropy

        with tf.variable_scope('Accuracy'):
            predicts = tf.cast(tf.argmax(entaiment, 1), 'int32')   #entaiment none 3
            lable_one = tf.cast(tf.argmax(lable, 1), 'int32')
            corrects = tf.equal(predicts, lable_one)
            num_corrects = tf.reduce_sum(tf.cast(corrects, tf.float32))
            accuracy = tf.reduce_mean(tf.cast(corrects, tf.float32))

        optimizer = tf.train.AdamOptimizer(params.learning_rate)
        opt_op = optimizer.minimize(loss, global_step=self.global_step)
`"
7995,ImportError: DLL load failed: The specified module could not be found.,"Traceback (most recent call last):
  File ""d:\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""d:\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""d:\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""d:\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 21, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""d:\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow')
  File ""d:\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""d:\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""d:\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""d:\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""d:\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""d:\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""d:\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 21, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""d:\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow')
  File ""d:\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

====
Finally,I find the **solution**:
Install [Microsoft Visual C++ 2015 Redistributable Update 3 x64.](https://www.microsoft.com/en-us/download/details.aspx?id=53587)
"
7994,Loading and re-training the model,"Hi, I am confused, if it is possible to load the last checkpoint for the model and now use another data to train the same model, so widen it's knowledge ?

Usecase: chatbot
data1: cornell-movie data
data2: stack exchange dataset

Firstlt, I had trained a seq2seq model on the cornell data, with some hyperparameters and other important variables, now i want my same model to train on other datasets also. Is this possible ?"
7993,tf.contrib.slim Out of Date API,"I am using tf.contrib.slim API to build nets, it's simple and convenient, however, I think it's some kind of confusion me, I occurs a lot of out-of-date api which I think should be update or fix (except I got wrong idea to using it).
Basiclly, I got images and labels from tfrecord file, run this:

```
def run_training():

    train_log_dir = './train_log'
    if not tf.gfile.Exists(train_log_dir):
        tf.gfile.MakeDirs(train_log_dir)

    images, labels = inputs(train=True, batch_size=FLAGS.batch_size,
                            num_epochs=FLAGS.num_epochs, one_hot_labels=True)
    predictions = vgg.vgg_16(images, is_training=True)

    slim.losses.softmax_cross_entropy(predictions, labels)
    total_loss = slim.losses.get_total_loss()
    tf.summary.scalar('loss', total_loss)

    optimizer = tf.train.RMSPropOptimizer(0.001, 0.9)
    train_op = slim.learning.create_train_op(total_loss, optimizer, summarize_gradients=True)

    slim.learning.train(train_op, train_log_dir, save_summaries_secs=20)
```
all imports from `tf.contirb.slim`. 
And I got these error.
```
File ""train_slim_vgg16.py"", line 157, in run_training2
    slim.losses.softmax_cross_entropy(predictions, labels)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 117, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py"", line 380, in softmax_cross_entropy
    logits.get_shape().assert_is_compatible_with(onehot_labels.get_shape())
AttributeError: 'tuple' object has no attribute 'get_shape'
```
I read and load images labels like this:
```
def read_and_decode(filename_queue):
    reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)
    features = tf.parse_single_example(
        serialized=serialized_example,
        features={
            'image/height': tf.FixedLenFeature([], tf.int64),
            'image/width': tf.FixedLenFeature([], tf.int64),
            'image/channels': tf.FixedLenFeature([], tf.int64),
            'image/encoded': tf.FixedLenFeature([], tf.string),
            'image/class/label': tf.FixedLenFeature([], tf.int64),
        })
    height = tf.cast(features['image/height'], dtype=tf.int32)
    width = tf.cast(features['image/width'], dtype=tf.int32)
    channels = tf.cast(features['image/channels'], dtype=tf.int32)
    label = tf.cast(features['image/class/label'], dtype=tf.int32)

    image = tf.image.decode_jpeg(features['image/encoded'], channels=3)
    image = tf.image.resize_image_with_crop_or_pad(
        image=image,
        target_height=FLAGS.target_image_height,
        target_width=FLAGS.target_image_width,
    )
    image = tf.cast(image, tf.float32) * (1. / 255) - 0.5
    return image, label


def inputs(train, batch_size, num_epochs, one_hot_labels):
    if not num_epochs:
        num_epochs = None
    with tf.name_scope('input'):
        filename_queue = tf.train.string_input_producer(
            tf_records_walker(tf_records_dir=FLAGS.tf_record_dir),
            num_epochs=num_epochs,
            shuffle=True)
        image, label = read_and_decode(filename_queue)

        if one_hot_labels:
            label = tf.one_hot(indices=label, depth=FLAGS.num_classes+1, dtype=tf.int32)
        images, sparse_labels = tf.train.shuffle_batch(
            [image, label],
            batch_size=batch_size,
            num_threads=2,
            capacity=10 + 3 * batch_size,
            min_after_dequeue=10)

        return images, sparse_labels
```
Obviously, the way used to write slim nets and train graph not effect anymore, I suggest develope team  update this contrib docs or update some APIs. (If I did something wrong, sincerely help you guys help me out, I just can't get my network run.)"
7991,"{Base,LocalCLI}DebugWrapperSession and SessionInterface don't have 'as_default'.","I tried to use tfdbg to hunt those Infs, and it complained `LocalCLIDebugWrapperSession`  was lacking `as_default`, which I use to set the default session for each thread. Neither its parent class `BaseDebugWrapperSession` or the interface `SessionInterface` it implements seem to have it. I thought it was supposed to be a drop-in replacement, is this part of the design?"
7989,"""Download and Setup"" link broken","In README file, the link of ""Download and Setup"" is pointing to _https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md_
However, it looks like that is broken now, please help fix, thanks~

""
Installation
See Download and Setup for instructions on how to install our release binaries or how to build from source.
"""
7988,Scope issue with Adam after upgrade from 0.12 to 1.0.0.,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

None directly related, but indirectly this one is related: https://github.com/tensorflow/tensorflow/issues/7462 (but seems to be code error).

THE ISSUE:

My code works with r0.12:
git@github.com:pseudotensor/temporal_autoencoder.git (branch timeconv, commit 3cfc56566931ad592eef7a9b7ddbb32f3346819c, i.e. just below HEAD)

Bit Fails with r1.0.0 (after using upgrade script):
git@github.com:pseudotensor/temporal_autoencoder.git (branch timeconv, commit f7ee1e5325797173cc26e132f152f969839f9975, i.e. HEAD)

Error:

Traceback (most recent call last):
  File ""main.py"", line 476, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""main.py"", line 473, in main
    autoencode(continuetrain=continuetrain,modeltype=modeltype,num_balls=num_balls)
  File ""main.py"", line 273, in autoencode
    train_operation = tf.train.AdamOptimizer(FLAGS.adamvar).minimize(loss)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 289, in minimize
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 403, in apply_gradients
    self._create_slots(var_list)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/adam.py"", line 117, in _create_slots
    self._zeros_slot(v, ""m"", self._name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 647, in _zeros_slot
    named_slots[var] = slot_creator.create_zeros_slot(var, op_name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/slot_creator.py"", line 123, in create_zeros_slot
    colocate_with_primary=colocate_with_primary)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/slot_creator.py"", line 101, in create_slot
    return _create_slot_var(primary, val, '')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/slot_creator.py"", line 55, in _create_slot_var
    slot = variable_scope.get_variable(scope, initializer=val, trainable=False)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 988, in get_variable
    custom_getter=custom_getter)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 890, in get_variable
    custom_getter=custom_getter)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 348, in get_variable
    validate_shape=validate_shape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 333, in _true_getter
    caching_device=caching_device, validate_shape=validate_shape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 657, in _get_single_variable
    ""VarScope?"" % name)
ValueError: Variable cnn_1_cnn/weights/Adam/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?

Hypothesis1:  Something changed in how tensorflow1.0.0 handles scope so that Adam for some reason thinks it should be under cnn_1_cnn_weights scope, when should be in base scope
Hypothesis2: Code was ""wrong"" to begin with, even though worked perfectly with r0.12, but now stricter tensorflow 1.0.0 checks led to this message.
In either case, I'm not sure what to do, but I'll continue checking that what I'm doing is correct as per examples at: https://www.tensorflow.org/programmers_guide/variable_scope

### Environment info
Operating System:

Ubuntu 16.04.2 LTS (latest updates/upgrades)

Installed version of CUDA and cuDNN: 

ls -l $CUDA_HOME/lib64/libcud*
-rwxr-xr-x 1 root root    558720 Jan 29 16:23 /usr/local/cuda/lib64/libcudadevrt.a*
lrwxrwxrwx 1 root root        16 Jan 29 16:23 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0*
lrwxrwxrwx 1 root root        19 Jan 29 16:23 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44*
-rwxr-xr-x 1 root root    415432 Jan 29 16:23 /usr/local/cuda/lib64/libcudart.so.8.0.44*
-rwxr-xr-x 1 root root    775162 Jan 29 16:23 /usr/local/cuda/lib64/libcudart_static.a*
lrwxrwxrwx 1 jon  users       13 Jul 26  2016 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5*
lrwxrwxrwx 1 jon  users       17 Jul 26  2016 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5*
-rwxrwxr-x 1 jon  users 79337624 Jul 26  2016 /usr/local/cuda/lib64/libcudnn.so.5.1.5*
-rwxrwxr-x 1 jon  users 69756172 Jul 26  2016 /usr/local/cuda/lib64/libcudnn_static.a*


2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

jon@pseudotensor:~/tensorflow$ cd ..
jon@pseudotensor:~$ python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.0

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)

jon@pseudotensor:~/tensorflow$ git rev-parse HEAD
29a6b4661258ef99842904d7c54993c963a8c2c0

2. The output of `bazel version`

jon@pseudotensor:~/tensorflow$ bazel version
.....................
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261


I also tried the latest nightly build: https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.0.0-cp27-none-linux_x86_64.whl  and this fails in the same way as my compiled-from-source version.

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Step 1) git@github.com:pseudotensor/temporal_autoencoder.git
Step 2) cd temporal_autoencoder
Step 3) git checkout timecov
Step 4) python main.py

### What other attempted solutions have you tried?

Checking scope makes sense, googling, looking at existing issues.  No positive result.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7986,tf.contrib.metrics.accuracy does not accept name,"The `tf.contrib.metrics.accuracy` operation does not take in a name and instead is hardcoded to be `accuracy`:
https://github.com/tensorflow/tensorflow/blob/a230417c58c258b2417225c739a1e5f0890491e6/tensorflow/contrib/metrics/python/metrics/classification.py#L55

signature should be:
```
def accuracy(predictions, labels, weights=None, name=None):
```
and then:
```
with ops.name_scope(name, 'accuracy', values=[predictions, labels]):
```"
7983,failed initializing StreamExecutor for CUDA device ordinal 0,"I encountered a strange issue that tensorflow asks for extreme amount of memory and report an error, which I cannot find any solution over the internet. I have two Titan X, the gpu0 is running traning process which occupies the whole memory of gpu0 and 100m of gpu1. I was trying to run inference python program on gpu1 and got that error. 

System settings: Ubuntu 16.04, Cuda 8.0, CUDNN 5.1 for 8.0, Nvidia 367.57 driver, tensorflow 1.0.0. The rest you can see in the log. 

```
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
E tensorflow/core/common_runtime/direct_session.cc:137] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 18446744073709551615
Traceback (most recent call last):
  File ""inference.py"", line 143, in <module>
    main()
  File ""inference.py"", line 102, in main
    sess = tf.Session(config=config)
  File ""/home/zhouzh/virtualenvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1187, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/home/zhouzh/virtualenvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 552, in __init__
    self._session = tf_session.TF_NewDeprecatedSession(opts, status)
  File ""/usr/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/home/zhouzh/virtualenvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
```

"
7980,TensorFlow upgrade to 1.0.0 breaking import,"On the Mac OS X El Capitan V 10.11.6 using python 2.7.11 with anaconda

when I run this code:

pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.9.0-py2-none-any.whl

I get a working version of tensorflow 0.9.0. import tensorflow in python throws no errors.

I type in the command:

pip install --upgrade tensorflow

I get a successful install 

```
Collecting tensorflow
  Using cached tensorflow-1.0.0-cp27-cp27m-macosx_10_11_x86_64.whl
Requirement already up-to-date: mock>=2.0.0 in ./anaconda/lib/python2.7/site-packages (from tensorflow)
Requirement already up-to-date: six>=1.10.0 in ./anaconda/lib/python2.7/site-packages (from tensorflow)
Requirement already up-to-date: numpy>=1.11.0 in ./anaconda/lib/python2.7/site-packages (from tensorflow)
Collecting protobuf>=3.1.0 (from tensorflow)
  Using cached protobuf-3.2.0-py2.py3-none-any.whl
Requirement already up-to-date: wheel in ./anaconda/lib/python2.7/site-packages (from tensorflow)
Requirement already up-to-date: funcsigs>=1; python_version < ""3.3"" in ./anaconda/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow)
Requirement already up-to-date: pbr>=0.11 in ./anaconda/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow)
Requirement already up-to-date: setuptools in ./anaconda/lib/python2.7/site-packages (from protobuf>=3.1.0->tensorflow)
Requirement already up-to-date: appdirs>=1.4.0 in ./anaconda/lib/python2.7/site-packages (from setuptools->protobuf>=3.1.0->tensorflow)
Requirement already up-to-date: packaging>=16.8 in ./anaconda/lib/python2.7/site-packages (from setuptools->protobuf>=3.1.0->tensorflow)
Requirement already up-to-date: pyparsing in ./anaconda/lib/python2.7/site-packages (from packaging>=16.8->setuptools->protobuf>=3.1.0->tensorflow)
Installing collected packages: protobuf, tensorflow
  Found existing installation: protobuf 3.0.0b2
    Uninstalling protobuf-3.0.0b2:
      Successfully uninstalled protobuf-3.0.0b2
  Found existing installation: tensorflow 0.9.0
    Uninstalling tensorflow-0.9.0:
      Successfully uninstalled tensorflow-0.9.0
Successfully installed protobuf-3.2.0 tensorflow-1.0.0

```

and I get a broken import statement in python (.. modify dir names)

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""../anaconda/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""../anaconda/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 124, in <module>
    from tensorflow.python.platform import test
  File ""../anaconda/lib/python2.7/site-packages/tensorflow/python/platform/test.py"", line 83, in <module>
    import mock                # pylint: disable=g-import-not-at-top,unused-import
  File ""../anaconda/lib/python2.7/site-packages/mock/__init__.py"", line 2, in <module>
    import mock.mock as _mock
  File ""../anaconda/lib/python2.7/site-packages/mock/mock.py"", line 71, in <module>
    _v = VersionInfo('mock').semantic_version()
  File ""../anaconda/lib/python2.7/site-packages/pbr/version.py"", line 460, in semantic_version
    self._semantic = self._get_version_from_pkg_resources()
  File ""../anaconda/lib/python2.7/site-packages/pbr/version.py"", line 447, in _get_version_from_pkg_resources
    result_string = packaging.get_version(self.package)
  File ""../anaconda/lib/python2.7/site-packages/pbr/packaging.py"", line 750, in get_version
    name=package_name))
Exception: Versioning for this project requires either an sdist tarball, or access to an upstream git repository. It's also possible that there is a mismatch between the package name in setup.cfg and the argument given to pbr.version.VersionInfo. Project name mock was given, but was not able to be found.
```

when I import again in the same interface, I get a different error which repeats if I do it anymore

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""../anaconda/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""../anaconda/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""../anaconda/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
ImportError: cannot import name pywrap_tensorflow


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```"
7979,.configure error Unrecognized option: --action_env=PATH,"OS: Ubuntu 16.04
gcc 5.4.0
bazel info >> bazel is already the newest version (0.4.4)
Cuda compilation tools, release 8.0, V8.0.61

Tried with tensorflow source https://github.com/tensorflow/tensorflow, commit 27a9808

gopi@gp:~/tensorflow$ ./configure
Please specify the location of python. [Default is /usr/bin/python]:
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]:
Do you wish to use jemalloc as the malloc implementation? [Y/n] Y
jemalloc enabled
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] N
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] N
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] N
No XLA JIT support will be enabled for TensorFlow
Found possible Python library paths:
/usr/local/lib/python2.7/dist-packages
/usr/lib/python2.7/dist-packages
Please input the desired Python library path to use. Default is [/usr/local/lib/python2.7/dist-packages]

Using python library path: /usr/local/lib/python2.7/dist-packages
Do you wish to build TensorFlow with OpenCL support? [y/N] N
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] y
CUDA support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]:
Please specify the location where CUDA toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 5.1.10
Please specify the location where cuDNN 5.1.10 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/lib/x86_64-linux-gnu
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 6.1
.
INFO: Options provided by the client:
Inherited 'common' options: --isatty=1 --terminal_columns=97
INFO: Reading options for 'clean' from /home/gopi/tensorflow/tools/bazel.rc:
Inherited 'build' options: --force_python=py2 --host_force_python=py2 --python2_path=/usr/bin/python --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --define PYTHON_BIN_PATH=/usr/bin/python --spawn_strategy=standalone --genrule_strategy=standalone -c opt
INFO: Reading options for 'clean' from /etc/bazel.bazelrc:
Inherited 'build' options: --action_env=PATH --action_env=LD_LIBRARY_PATH --action_env=TMPDIR --test_env=PATH --test_env=LD_LIBRARY_PATH
Unrecognized option: --action_env=PATH"
7976,distributed tensorflow data Data in parallelps server can only use cpu? use gpu is faster?,"
When I use kears and tensorflow to do the parallel,use sgd sync optimizer I have 10 machines,
 each machine has a gpu, so I do



1.Each gpu corresponds to a worker hostsuse cuda_visible_devices 
2.every ps server only use cpu
3.use Gigabit Ethernet 
when i run it,i find it is so lower,,

who can tell me ?what is wrong??






flags.DEFINE_string(""train_desc_file"",'train.json','train config')
flags.DEFINE_string(""val_desc_file"",'test.json','dev config')
flags.DEFINE_string(""save_dir"",'model','save model dir')
flags.DEFINE_integer('epochs',1000,'epoch number')
flags.DEFINE_integer('batch_size',64,'batch_size')
flags.DEFINE_string(""ps_hosts"",""172.16.186.86:2221"",
                    ""Comma-separated list of hostname:port pairs"")
flags.DEFINE_string(""worker_hosts"", ""172.16.186.86:2222,172.16.186.86:2223"",""Comma-separated list of hostname:port pairs"")
flags.DEFINE_string(""job_name"", None,""job name: worker or ps"")
flags.DEFINE_integer(""task_index"", None,
                     ""Worker task index, should be >= 0. task_index=0 is ""
                     ""the master worker task the performs the variable ""
                     ""initialization "")


if FLAGS.job_name is None or FLAGS.job_name == """":
        raise ValueError(""Must specify an explicit `job_name`"")
    if FLAGS.task_index is None or FLAGS.task_index == """":
        raise ValueError(""Must specify an explicit `task_index`"")
    print(""job name = %s"" % FLAGS.job_name)
    print(""task index = %d"" % FLAGS.task_index)
    ps_spec = FLAGS.ps_hosts.split("","")
    worker_spec = FLAGS.worker_hosts.split("","")
    num_workers = len(worker_spec)

    cluster = tf.train.ClusterSpec({
        ""ps"": ps_spec,
        ""worker"": worker_spec})
    server = tf.train.Server(
        cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)
    if FLAGS.job_name == ""ps"":
        server.join()

    with tf.device(tf.train.replica_device_setter(
            worker_device=""/job:worker/task:%d"" % FLAGS.task_index,
            cluster=cluster)):
        is_chief = (FLAGS.task_index == 0)
        model=....
        acoustic_input = model.inputs[0]
        network_output = model.outputs[0]
        network_output = tf.transpose(network_output, [1, 0, 2])
        ctc_cost = warpctc_tensorflow.ctc(network_output, sess_label, sess_label_lens, sess_output_lens)
        ctc_cost = tf.reduce_mean(ctc_cost)
        trainable_vars = model.trainable_weights
        learning_rate = 2e-4

        global_step = tf.contrib.framework.get_or_create_global_step()
        # optimizer = SGD(lr=learning_rate, momentum=0.9, nesterov=True,clipnorm=100)
        optimizer=tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.99,use_nesterov=True)
        optimizer = tf.train.SyncReplicasOptimizer(
            optimizer,
            replicas_to_aggregate=num_workers,
            total_num_replicas=num_workers,
            use_locking=True,
            name=""sync_replicas"")

        grads = optimizer.compute_gradients(ctc_cost, trainable_vars)
        grads, _ = tf.clip_by_global_norm(tf.gradients(ctc_cost, trainable_vars), 100)
        train_op = optimizer.apply_gradients(zip(grads, trainable_vars), global_step=global_step)
        time_begin = time.time()

        chief_queue_runner = optimizer.get_chief_queue_runner()
        sync_init_op = optimizer.get_init_tokens_op()
        ready_for_local_init_op = optimizer.ready_for_local_init_op
        K.manual_variable_initialization(True)
        local_init_op = optimizer.local_step_init_op
        if is_chief:
            local_init_op = optimizer.chief_init_op
        sv = tf.train.Supervisor(
            is_chief=is_chief,
            logdir='train_log',
            local_init_op=local_init_op,
            ready_for_local_init_op=ready_for_local_init_op,
            recovery_wait_secs=1,
            global_step=global_step)

        sess = sv.prepare_or_wait_for_session(server.target)
        K.set_session(sess)
        K.get_session().run(sync_init_op)
        sv.start_queue_runners(sess, [chief_queue_runner])
        print(""Training begins @ %f"" % time_begin)
        main(FLAGS.train_desc_file, FLAGS.val_desc_file, FLAGS.epochs, FLAGS.save_dir,
             FLAGS.sortgrad, model=model, run_require_op=[ctc_cost, train_op], val_fn=val_fn)




but when i run it,"
7975,Uable to load cuDNN DSO.,"My server is CentOS 6.7
when i do ""import tensorflow as tf""
show   I tensorflow/stream_executor/dso_loader.cc:77] LD_LIBRARY_PATH: :/usr/local/cuda/lib64
           I tensorflow/stream_executor/cuda/cuda_dnn.cc:1062]unable to load cuDNN DSO.

How can i solve it? Thans!"
7974,InvalidArgumentError for tf.gather_nd() function,"When I use `tf.gather_nd(im, idx)`, I encountered the following error:
```
  File ""model.py"", line 86, in <module>
    num=len(up_outputs) - 1, pixels_count=pixels_count)(up_outputs)
  File ""/home/dl/gds/dl_modules/keras/keras/engine/topology.py"", line 572, in __call__
    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)
  File ""/home/dl/gds/dl_modules/keras/keras/engine/topology.py"", line 635, in add_inbound_node
    Node.create_node(self, inbound_layers, node_indices, tensor_indices)
  File ""/home/dl/gds/dl_modules/keras/keras/engine/topology.py"", line 172, in create_node
    output_tensors = to_list(outbound_layer.call(input_tensors, mask=input_masks))
  File ""/home/dl/gds/important/key_frame/frame_propagation/lib/layer_utils_tf.py"", line 101, in call
    results.append(_interpolate(vects[i], ori_loc, stride=self.strides[i], ori_shp=self.ori_shp))
  File ""/home/dl/gds/important/key_frame/frame_propagation/lib/layer_utils_tf.py"", line 65, in _interpolate
    Id = tf.gather_nd(im, idx_d)
  File ""/home/dl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1473, in gather_nd
    name=name)
  File ""/home/dl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/home/dl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/dl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Must have updates.shape = indices.shape[:IXDIM] + params_shape[IXDIM:], got updates.shape [1,200,256], indices.shape [1,200,3], params_shape [1,128,128,256]
	 [[Node: gradients/GatherNd_3_grad/ScatterNd = ScatterNd[T=DT_FLOAT, Tindices=DT_INT32, _class=[""loc:@GatherNd_3""], _device=""/job:localhost/replica:0/task:0/cpu:0""](stack_3/_1441, gradients/mul_10_grad/Reshape_1/_1525, gradients/GatherNd_3_grad/Shape)]]
```
The shape of tensor `im` is `(batch, row, col, channel) (1, 128, 128, 256)`, and the shape of indices tensor `ind` is `(batch, 200, 3)`, of which the 3 values in the third dimension represent `(batch, row, col)`.
Thus we get a tensor of shape `(1, 200, 256)`, however, why it said that:
```
Must have updates.shape = indices.shape[:IXDIM] + params_shape[IXDIM:], got updates.shape [1,200,256], indices.shape [1,200,3], params_shape [1,128,128,256]
```
It's weird, what does `IXDIM` represents here ? Anyone knows ?"
7973,im2txt: W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for,"When I run training script provided by im2txt show and tell model, when I run this script --input_file_pattern=""${MSCOCO_DIR}/train-?????-of-00256""   --inception_checkpoint_file=""${INCEPTION_CHECKPOINT}""   --train_dir=""${MODEL_DIR}/train""   --train_inception=false   --number_of_steps=1000000


I get following error
can anyone help me with this please.....


INFO:tensorflow:Prefetching values from 256 files matching im2txt/data/new/train-?????-of-00256
WARNING:tensorflow:From /home/hajira/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py:344 in __init__.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.
INFO:tensorflow:Restoring Inception variables from checkpoint file 
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for 
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for 
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for 
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for 
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for 
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for 
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for 
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for 
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for 


"
7972,batch_matmul,How can I use batch_matmul in tensorflow 1.0
7971,During initial training when I write this script ,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7970,SyncReplicasOptimizer race condition strange behavior?,"It seems there is a strange race condition in SyncReplicasOptimizer leading to strange behaviour. I include below an example code to reproduce what seems to be a bug (hopefully in my code) as well as the commands to reproduce it (pretty much the same code as in mnist_replica.py).


I am trying to implement  synchronized SGD using SyncReplicasOptimizer, I also used the queue trick to make the parameter server stop gracefully when all workers are done. I have 4 workers and 1 parameter server. Worker 0 is the chief worker.

Please bear with me for the long explanation of the different issues (they depend on the order in which processes are launched)

**** First kind of issue ****

launch the processes in this order 
    
    python test.py --job_name ps
    python test.py --job_name worker --taks_index 0
    python test.py --job_name worker --taks_index 1
    python test.py --job_name worker --taks_index 2
    python test.py --job_name worker --taks_index 3

The last worker throws the following error :

    I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Unavailable: {""created"":""@1488366991.043859719"",""description"":""OS Error"",""errno"":104,""file"":""external/grpc/src/core/lib/iomgr/tcp_posix.c"",""file_line"":229,""grpc_status"":14,""os_error"":""Connection reset by peer"",""syscall"":""recvmsg""}

and quits, and it happens also that it hangs (not realising that the variable epoch is greater than 4, triggering the break from the training loop, and the enqueue operation to let the ps stop gracefully).

It also happen that all is fine, and the execution terminates without any errors.


**** Second kind of issue ****

launch the processes in this order 
    
    python test.py --job_name ps
    python test.py --job_name worker --taks_index 3
    python test.py --job_name worker --taks_index 2
    python test.py --job_name worker --taks_index 1
    python test.py --job_name worker --taks_index 0


The chief here being launched at last.

Strangely, the chief completes the loop and quits ( I thought with SyncReplicasOptimizer it had to wait for the other workers to complete each step).

As for the other workers, I had all sort of results when doing the same experiment many times 

1) Some workers simply hang and do not execute a single step in the `while true` training loop

2) Some execute some steps, then simply hang, apparently they lose contact with the chief, and do not realise that the variable `epoch` is greater than 4, triggering the `break from the training loop.

Thank you for help with this issue.

Below is the code of test.py

    import os
    import shutil
    import tempfile
    import numpy as np
    import pandas as pd
    import argparse
    
    from keras.models import Sequential
    from keras.layers.core import Dense
    from keras.regularizers import l2
    import tensorflow as tf
    import keras
    
    nb_samples = 50
    nb_features = 5
    X_train = np.random.randn(nb_samples * nb_features).reshape((nb_samples, nb_features))
    Y_train = np.random.randn(nb_samples).reshape((nb_samples, 1))
    
    def build_keras_model(input_dim):
      hidden_dim = 10
    
      model = Sequential()
      model.add(Dense(input_dim = input_dim,
                      output_dim=hidden_dim,
                      activation='tanh'
                      ))
    
      model.add(Dense(output_dim=1, activation='linear'))
    
      model.compile(loss='mse', optimizer='adam')
      
      return model
    
    
    
    
    ################################################
    # DISTRIBUTE
    ################################################
    
    parser = argparse.ArgumentParser(description='tensorflow')
    parser.add_argument('--job_name', dest='job_name')
    parser.add_argument('--task_index', dest='task_index', default=0)
    args = parser.parse_args()
    
    
    ps_hosts = ['localhost:2222']
    worker_hosts = ['localhost:2223', 'localhost:2224', 'localhost:2225', 'localhost:2226']
    job_name = args.job_name
    task_index = int(args.task_index)
    
    # Create a cluster from the parameter server and worker hosts.
    cluster = tf.train.ClusterSpec({""ps"": ps_hosts, ""worker"": worker_hosts})
      
    server = tf.train.Server(cluster,
                             job_name=job_name,
                             task_index=task_index,
                             config=tf.ConfigProto(log_device_placement=True,
                                                   inter_op_parallelism_threads=1,
                                                   intra_op_parallelism_threads=1))
    
    
    if job_name =='ps':
      with tf.device(""/job:ps/task:0""):
        queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=""done_queue"")
      sess = tf.Session(server.target)
      # wait until all workers are done
      for i in range(len(worker_hosts)):
        sess.run(queue.dequeue())
    else:
      with tf.device(tf.train.replica_device_setter(
                                  worker_device=""/job:worker/task:%d"" % task_index,
                                  cluster=cluster)):
    
        keras.backend.set_learning_phase(1)
        keras.backend.manual_variable_initialization(True)
    
        model = build_keras_model(nb_features)
        preds = model.output
        targets = tf.placeholder(tf.float32, [None, 1])
        total_loss = tf.reduce_mean(
                            keras.objectives.mean_squared_error(targets, preds))
    
        global_step = tf.Variable(0, name=""global_step"", trainable=False)
        # For early stopping management
        epoch = tf.Variable(0, name=""epoch"", trainable=False)
        inc_epoch_op = tf.assign_add(epoch, 1)
    
        is_chief=(task_index == 0)
    
        opt = tf.train.AdamOptimizer()
        num_workers = len(worker_hosts)
        replicas_to_aggregate = num_workers
        opt = tf.train.SyncReplicasOptimizer(
                                             opt,
                                             replicas_to_aggregate=replicas_to_aggregate,
                                             total_num_replicas=num_workers,
                                             name=""sync_replicas"")
    
        train_op = opt.minimize(total_loss, global_step=global_step)
        local_init_op = opt.local_step_init_op
        if is_chief:
          local_init_op = opt.chief_init_op
        ready_for_local_init_op = opt.ready_for_local_init_op
    
        # Initial token and chief queue runners required by the sync_replicas mode
        chief_queue_runner = opt.get_chief_queue_runner()
        sync_init_op = opt.get_init_tokens_op()
    
        init_op = tf.global_variables_initializer()
        with tf.device(""/job:ps/task:0""):
          queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=""done_queue"")
          enqueue_op = queue.enqueue(1)
     
        train_dir = tempfile.mkdtemp(prefix = 'worker_%d' % task_index)
        sv = tf.train.Supervisor(
                                 is_chief=is_chief,
                                 logdir=train_dir,
                                 init_op=init_op,
                                 local_init_op=local_init_op,
                                 ready_for_local_init_op=ready_for_local_init_op,
                                 recovery_wait_secs=1,
                                 global_step=global_step)
        
        print '######################################### ALL CREATED'
        sess = sv.prepare_or_wait_for_session(server.target)
        keras.backend.set_session(sess)
        print '#######  SESSION OK ********'
        if is_chief:
          sess.run(sync_init_op)
          sv.start_queue_runners(sess, [chief_queue_runner])
        local_step = 0
        while True:
          train_feed = {model.input: X_train, targets: Y_train}
    
          _, step = sess.run([train_op, global_step], feed_dict=train_feed)
          loss = sess.run(total_loss, feed_dict = train_feed)
          if is_chief:
            sess.run(inc_epoch_op)
          local_step += 1
          print '## epoch ', epoch.eval(sess)
          if epoch.eval(sess) > 4:
            print '######################  TRYING TO LEAVE'
            break
    
        shutil.rmtree(train_dir)
        print '######################  WHILE LOOP LEFT'
        sess.run(enqueue_op)
        print '## ENQUEUE OP DONE'
    
"
7969,Bottom-right precedence of padding?,"Why has Tensorflow chosen to prefer padding on the bottom right?
With 'SAME' padding, to me it would feel logical to start the kernel's center anchor at the first real pixel. Due to the use of asymmetric padding, this results in a discrepancy with some other frameworks. I do understand that asymmetric padding in principle is good because otherwise one would be left with an unused padding row/column.

![image](https://cloud.githubusercontent.com/assets/7721540/23457266/e6b57d8e-fe7f-11e6-9b88-dc793ce10c73.png)
"
7968,TypeError: __init__() got an unexpected keyword argument 'state_is_tuple' on GRU or RNN,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Compile error on RNN or GRU models.

### Environment info
Operating System:
> # lsb_release -a
> No LSB modules are available.
> Distributor ID:	Ubuntu
> Description:	Ubuntu 16.10
> Release:	16.10
> Codename:	yakkety

Installed version of CUDA and cuDNN: 
> # ls -l /opt/cuda/lib64/libcud*
> lrwxrwxrwx 1 ubuntu users       13 Jul 27  2016 /opt/cuda/lib64/libcudnn.so -> libcudnn.so.5
> lrwxrwxrwx 1 ubuntu users       17 Jul 27  2016 /opt/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
> -rwxrwxr-x 1 ubuntu users 79337624 Jul 27  2016 /opt/cuda/lib64/libcudnn.so.5.1.5
> -rw-rw-r-- 1 ubuntu users 69756172 Jul 27  2016 /opt/cuda/lib64/libcudnn_static.a

If installed from binary pip package, provide:
> # python3.5 -c ""import tensorflow; print(tensorflow.__version__)""
> I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
> I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
> I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
> I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
> I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
> 1.0.0
> 

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
```
        if args.model == 'rnn':
            cell_fn = rnn.BasicRNNCell
        elif args.model == 'gru':
            cell_fn = rnn.GRUCell
        elif args.model == 'lstm':
            cell_fn = rnn.BasicLSTMCell
        else:
            raise Exception(""model type not supported: {}"".format(args.model))

        cell = cell_fn(args.rnn_size, state_is_tuple=True)
```
using lstm is fine but any other than that 'rnn' or 'gru' causes the failure."
7967,"""data_format == FORMAT_NHWC"" error discrepancy","`tf.nn.conv2d()`'s behaviour on throwing the ""data_format == FORMAT_NHWC"" assertion seems odd. I cooked up a minimal example:

```py
input_np = np.zeros([1,1,4,4], dtype=np.float32)
x = tf.constant(input_np, dtype=tf.float32)
filter = tf.ones([1,1,1,1], dtype=tf.float32)
out = tf.nn.conv2d(x, filter, [1,1,1,1], padding='SAME', data_format='NCHW')
sess= tf.Session()
sess.run(tf.global_variables_initializer())
sess.run(out)
```
Throws:
```
F tensorflow/core/kernels/conv_ops.cc:62] Check failed: data_format == FORMAT_NHWC
Generic conv implementation only supports NHWC tensor format for now.
Aborted (core dumped)
```

While this works fine, and it uses exactly the same variables, but now I am feeding a placeholder.
```py
input_np = np.zeros([1,1,4,4], dtype=np.float32)
x = tf.placeholder(""float"", [None, 1,4,4])
filter = tf.ones([1,1,1,1], dtype=tf.float32)
out = tf.nn.conv2d(x, filter, [1,1,1,1], padding='SAME', data_format='NCHW')
sess= tf.Session()
sess.run(tf.global_variables_initializer())
sess.run(out, feed_dict={x:input_np})
```

Using 0.12.1, error is consistent across: OS-X as Ubuntu 14.04, regardless of CPU or GPU device placement."
7966,Compile TensorFlow 1.0 on Windows 10 for Python 3.6 (Anaconda) cause compile issues,"As per agreement in #6999 [specific comment](https://github.com/tensorflow/tensorflow/issues/6999#issuecomment-280749568), I'm creating a new issue for compilation errors for TF 1.0 under Windows for Python 3.6

**Pre-requisites:**
Windows 10 v.14986
Python 3.6 64 bit (Anaconda 4.3)
git version 2.11.0.windows.1
Visual Studio Build Tools (called ""Build Tools for Visual Studio 2017 RC"" in the [download](https://www.visualstudio.com/downloads/#build-tools-for-visual-studio-2017-rc) section)
cmake-3.8.0-rc1 and swigwin-3.0.12 (just downloaded latest version at that moment)
TensorFlow 1.0.0 from source (master branch from github)

**Reproduction**. I followed [instruction](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md) for CMake just running two commands - CMake first and MSBuild second.

To compile, I made two changes in sources. I found them based on compilation errors.
One was adding `#include <intrin.h>` to `tensorflow\core\platform\windows\cpu_info.h` inside of `#ifndef` block because of the following error:
> error C3861: '__cpuidex': identifier not found 

I found [commit](https://github.com/tensorflow/tensorflow/commit/a672cb166dae93ae955c1d38f3de8903dd242373) introduced usage of `__cpuidex` function. Based on [this](https://msdn.microsoft.com/en-us/library/hskdteyh.aspx) MSDN link, I discovered that Header file `<intrin.h>` has to be included.  That's why I put it inside `cpu_info.h` (not sure if it's exactly perfect place).

**Second issue** was commenting out procedures `_mm256_extract_epi32` and `_mm256_insert_epi32` in `tensorflow\core\platform\windows\intrinsics_port.h` due to error

> error C2169: '_mm256_extract_epi32': intrinsic function, cannot be defined 

The second issue with `_mm256_extract_epi32` may be connected with the same reason, I don't know. 
In the meantime, `platform/windows/intrinsics_port.h` [references](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/windows/intrinsics_port.h#L22) `immintrin.h`, but I don't see it's actually included anywhere. I assume it may correlate with `instrih.h` inclusion (based on [this](https://msdn.microsoft.com/en-us/library/26td21ds.aspx) link). Meanwhile, `_mm256_extract_epi32 `can be found in `avxintrin.h` which may be is included by `immintrin.h`.

> // the following avx intrinsics are not defined on windows
> // in immintrin.h so we define them here.

After fixing those two issues, that, I was able to successfully run
> MSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj

Which gave me `tensorflow-1.0.0-cp36-cp36m-win_amd64.whl`, that I successfully installed into Python using `pip`. It seems to be working - I was able to run couple of jupyter notebooks, including udacity assignments.

BTW, compilation took about 2-3 hours on my week laptop (i5-4200U)."
7964,How to specify the original name of an image using summary.image? ,"I want to visualize detection results on images using `tf.summary.image` and record the original name of this image at the same time.
In the old version `tf.image_summary()`, I can pass a placeholder for an image name and feed the name in `sess.run`.  Like this,
```
log_image_data = tf.placeholder(tf.uint8, [None, None, 3])
log_image_name = tf.placeholder(tf.string)
log_image = tf.image_summary(log_image_name, tf.expand_dims(log_image_data, 0), max_images=1)
```
But the new version of api only allows to speficy a fixed `name` in string type instead of a placeholder when building the summry graph. 
 ```
tf.summary.image(name, tensor, max_outputs=3, collections=None) 
# Args: 
# name: A name for the generated node, which cannot be a placeholder
```

How can I name the visualized image on the fly? "
7962,Clarify Migration From session_bundle.exporter to saved_model.builder.SavedModelBuilder,"In looking at the docs for [Serving a TensorFlow Model](https://tensorflow.github.io/serving/serving_basic.html), and [comparing to wayback machine](https://web.archive.org/web/20160415224950/https://tensorflow.github.io/serving/serving_basic.html), it seems the serving guide went from using `session_bundle.exporter` to `saved_model.builder.SavedModelBuilder` with no documentation as to this change or the migration path.

Both tools wrap `Savers`. What is the difference between these? I believe `session_bundle` is deprecated and `SavedModelBuilder` is supposed to be a replacement however I have not seen this point made explicitly in the docs. All I could find is a comment in the `loader`:
```
Load legacy TF Exporter/SessionBundle checkpoint.
```

[Report from SO](http://stackoverflow.com/questions/42499228/tensorflow-session-bundle-vs-saved-model)."
7961,PermissionError: [Errno 13] Permission denied: '/usr/local/lib/python3.4/dist-packages/google/protobuf/__init__.py,"I am trying to install tensorflow within virtual environment. After creating virtual env , I run following command : 

`pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp34-cp34m-linux_x86_64.whl`

And the output is : 

Collecting tensorflow==0.9.0 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp34-cp34m-linux_x86_64.whl
  Using cached https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp34-cp34m-linux_x86_64.whl
Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.4/dist-packages (from tensorflow==0.9.0)
Collecting protobuf==3.0.0b2 (from tensorflow==0.9.0)
  Using cached protobuf-3.0.0b2-py2.py3-none-any.whl
Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.4/dist-packages (from tensorflow==0.9.0)
Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.4/dist-packages (from tensorflow==0.9.0)
Requirement already satisfied: setuptools in /usr/local/lib/python3.4/dist-packages (from protobuf==3.0.0b2->tensorflow==0.9.0)
Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.4/dist-packages (from setuptools->protobuf==3.0.0b2->tensorflow==0.9.0)
Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.4/dist-packages (from setuptools->protobuf==3.0.0b2->tensorflow==0.9.0)
Requirement already satisfied: pyparsing in /usr/local/lib/python3.4/dist-packages (from packaging>=16.8->setuptools->protobuf==3.0.0b2->tensorflow==0.9.0)
Installing collected packages: protobuf, tensorflow
  Found existing installation: protobuf 3.0.0b1.post2
    Uninstalling protobuf-3.0.0b1.post2:
Exception:
Traceback (most recent call last):
  File ""/usr/lib/python3.4/shutil.py"", line 523, in move
    os.rename(src, real_dst)
PermissionError: [Errno 13] Permission denied: '/usr/local/lib/python3.4/dist-packages/google/protobuf/__init__.py' -> '/tmp/pip-frvrcwtb-uninstall/usr/local/lib/python3.4/dist-packages/google/protobuf/__init__.py'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.4/dist-packages/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""/usr/local/lib/python3.4/dist-packages/pip/commands/install.py"", line 342, in run
    prefix=options.prefix_path,
  File ""/usr/local/lib/python3.4/dist-packages/pip/req/req_set.py"", line 778, in install
    requirement.uninstall(auto_confirm=True)
  File ""/usr/local/lib/python3.4/dist-packages/pip/req/req_install.py"", line 754, in uninstall
    paths_to_remove.remove(auto_confirm)
  File ""/usr/local/lib/python3.4/dist-packages/pip/req/req_uninstall.py"", line 115, in remove
    renames(path, new_path)
  File ""/usr/local/lib/python3.4/dist-packages/pip/utils/__init__.py"", line 267, in renames
    shutil.move(old, new)
  File ""/usr/lib/python3.4/shutil.py"", line 536, in move
    os.unlink(src)
PermissionError: [Errno 13] Permission denied: '/usr/local/lib/python3.4/dist-packages/google/protobuf/__init__.py'"
7960,double free or corruption (!prev) in Docker image tensorflow/tensorflow:1.0.0,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

https://github.com/tensorflow/tensorflow/issues/6968
https://github.com/tensorflow/tensorflow/issues/7839

### Environment info

I run tensorflow in Docker image [tensorflow/tensorflow:1.0.0](https://hub.docker.com/r/tensorflow/tensorflow/).

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I used tensorflow/tensorflow:1.0.0 to run [examples/tutorials/mnist/mnist_softmax.py](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/examples/tutorials/mnist/mnist_softmax.py). My docker run command is:

```shell
$ docker run -ti \
    -v /Users/lienhua34/Programs/python/tensorflow/tensorflow/examples/tutorials/mnist:/mnist \
   tensorflow/tensorflow:1.0.0 /bin/bash
```

Then cd to /mnist, and run mnist_softmax.py:

```shell
root@5a17d6873afc:/notebooks# cd /mnist/
root@5a17d6873afc:/mnist# ls -l
total 52
-rw-r--r-- 1 root root 2581 Feb 16 15:42 BUILD
-rw-r--r-- 1 root root  979 Feb 16 15:42 __init__.py
drwxr-xr-x 6 root root  204 Mar  1 03:41 data
-rw-r--r-- 1 root root 9515 Feb 16 15:42 fully_connected_feed.py
-rw-r--r-- 1 root root 1107 Feb 16 15:42 input_data.py
-rw-r--r-- 1 root root 5292 Feb 16 15:42 mnist.py
-rw-r--r-- 1 root root 2772 Feb 17 09:01 mnist_softmax.py
-rw-r--r-- 1 root root 3684 Feb 16 15:42 mnist_softmax_xla.py
-rw-r--r-- 1 root root 8391 Feb 16 15:42 mnist_with_summaries.py
root@5a17d6873afc:/mnist# python mnist_softmax.py --data_dir=data
Loading data....
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
0.919
*** Error in `python': double free or corruption (!prev): 0x000000000179b6c0 ***
Aborted
```

### What other attempted solutions have you tried?

I followed the issue https://github.com/tensorflow/tensorflow/issues/6968 to install libtcmalloc-minimal4,

```shell
sudo apt-get install libtcmalloc-minimal4
export LD_PRELOAD=""/usr/lib/libtcmalloc_minimal.so.4""
```

After that, it will run fine.

```
root@5b7be5bcf590:/mnist# python mnist_softmax.py --data_dir=data
Loading data....
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
0.9212
```

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7959,ImportError: cannot import name model_fn,"I tried run cnn_mnist.py and I got the following error.

Traceback (most recent call last):
  File "" cnn_mnist.py"", line 13, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib
ImportError: cannot import name model_fn

cuda veriosn
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Sun_Sep__4_22:14:01_CDT_2016
Cuda compilation tools, release 8.0, V8.0.44

tensorflow version
tensorflow (0.10.0)

python version
Python 2.7.12
"
7958,Support MPSCNN (MetalPerformanceShaders) on iOS,"Related to: https://github.com/tensorflow/tensorflow/issues/3001

Take advantage of the [MPSCNN (Metal Performance Shaders) framework from Apple](https://developer.apple.com/reference/metalperformanceshaders).

See [blog post](http://machinethink.net/blog/apple-deep-learning-bnns-versus-metal-cnn/) for a comparison of BNSS to MPSCNN (and [associated code](https://github.com/hollance/BNNS-vs-MPSCNN)).

**TL; DR** BNNS is faster for smaller networks but slower for bigger networks.

Related: https://github.com/tensorflow/tensorflow/issues/4846"
7957,tf.contrib.layers.sparse_column_with_hash_bucket() lead to 'out of memory error' with large hash_bucket_size?,"  I'm using the wide_n_deep model on my own data, which has feature dimension about _100million_. When I use the _sparse_column_with_hash_bucket()_ and set the _hash_bucket_size_ to _10million_ (because some categorical features have that much different values), the program always failed with the out of memory error.

  So I wonder if tensorflow's wide_n_deep model could handles data with high dimension like the case above? "
7956,Method log_prob_with_logits() for Dirichlet,"It would be useful to have a log_prob_with_logits() method for the Dirichlet distribution.

The reason being is that it is often useful to model the discrete posterior distribution in log space, which doesn't let all probabilities to go exactly to zero. Then the Dirichlet prior can be applied to the data in log space. Note, that if the posterior is converted to the normalised distribution, then some of the discrete probabilities may actually go to zero due to the rounding errors. Then the Dirichlet prior cannot be applied to this distribution because it doesn't allow zero probabilities.

The log_prob_with_logits() calculation would be very simple to implement. The log(x_i) would need to be replaced with just x_i, and an additional term with LogSumExp(x) added.

This would actually enable the possibility to use the Dirichlet prior on the discrete viariables in log space (can also be part of a neural network)."
7954,Feature Request - Session Cancellation Function,"It'd be great to have a between node cancellation function for the tensorflow python session. In particular, if you call `sess.cancel_graph()` on a separate thread, then the tensorflow session on the main thread would be cancelled (raising an exception like the timeout exception). It would also be great for these cancellation signals to be caught between every tf operation, rather than only after particular operations like dequeue, etc.



"
7952,[Java] Loading frozen graph causes a segfault,"I froze then saved a tensorflow graph in python with [freeze_graph.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) producing a single model.pb file.

However, loading the graph in Java (where modelStream is an InputStream from reading the .pb file):
```
Graph g = new Graph();
InputStream modelStream = new FileInputStream(""/path/to/model.pb"");
g.importGraphDef(IOUtils.toByteArray(modelStream);
```
causes a segfault:
```
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x0000000125526b1c, pid=6482, tid=7171
#
# JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops)
# Problematic frame:
# C  [libtensorflow_jni.dylib+0x1bfdb1c]  tensorflow::shape_inference::InferenceContext::WithRank(tensorflow::shape_inference::ShapeHandle, int, tensorflow::shape_inference::ShapeHandle*)+0x1c
#
```
The same model.pb file loads and runs properly in Python.

### Environment info
Operating System: MacOS Sierra 10.12.3
Native library for OSX downloaded from [here](https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-cpu-darwin-x86_64-1.0.0-PREVIEW1.tar.gz) by following the [readme on the Java Tensorflow repo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/README.md)"
7951,[Enhancement] Redesigning TensorFlow's input pipelines,"[**TL;DR:** We're designing a new input pipeline API for TensorFlow, and we'd like to collect your feature requests on this issue.]

We've noticed that one of the biggest challenges in getting started with TensorFlow is how to load your own data into your programs. While TensorFlow has several methods that can be used to build complex input pipelines (such as [`tf.train.string_input_producer()`](https://www.tensorflow.org/api_docs/python/tf/train/string_input_producer), [`tf.train.batch()`](https://www.tensorflow.org/api_docs/python/tf/train/batch), etc.), they were designed for a particular use case (processing a static set of files repeatedly), and the average user experience with these methods is not great. For example:

* Once you reach the end of a pipeline, it becomes closed and you can never use it again in the same session. This requires users to use unnatural workarounds&mdash;with control flow or multiple sessions&mdash;to get a signal after processing an entire epoch, or switch between processing two datasets (e.g. training and validation data) in the same program.
  * See #2514 and #4535 for feature requests about handling multiple epochs.
  * See #7902 and numerous Stack Overflow questions for examples of processing different datasets in the same program.
* The current pipelines use TensorFlow queues and multiple Python threads, which can lead to poor performance (lock contention in the queues and the Python GIL) and hard-to-understand exceptions (`tf.errors.OutOfRangeError`).
  * See #6845 for a discussion of input pipeline performance.
  * See #7525 and [many more Stack Overflow questions](http://stackoverflow.com/search?q=%5Btensorflow%5D+outofrangeerror) for an example of the confusing error.
* The pipelines behave poorly if you forget to call `tf.train.start_queue_runners(sess)`: in fact, they hang indefinitely and deadlock the user program.
  * See #7945 and [many Stack Overflow questions](http://stackoverflow.com/search?q=%5Btensorflow%5D+hang) for some examples of users who have been bitten by this problem.

We're decided to start from a clean slate and redesign the input pipeline API. The existing methods will remain until TF 2.0 (at least), but we are planning to add a new set of methods for loading and manipulating datasets. We're still preparing a detailed design, which we plan to share soon, but we anticipate that there will be two new APIs:

* A `Dataset` represents a collection of data elements. Each element  can be a tuple of one or more tensors (e.g. an image and its label). We will provide methods for creating datasets from tensors, and deriving them from another dataset (e.g. by slicing its elements, repeating its elements, shuffling its elements, batching its elements, mapping a function over its elements, etc.). 
* An `Iterator` can be created from a `Dataset`. An iterator represents the current position within a dataset, and exposes an operation (like `tf.QueueBase.dequeue()`) that can be run to get the next element. There will be explicit operations for initializing an iterator, so that it can be reused after you have processed all of the elements in a dataset.

A similar pattern turns up in many different settings, including [Java's Stream API](https://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html), [Scala's collections](http://docs.scala-lang.org/overviews/collections/overview.html) (and hence Spark's RDDs), and [.NET's Language Integrated Query](https://msdn.microsoft.com/en-us/library/bb308959.aspx).

We're announcing this plan early because we want to collect feedback on what features you&mdash;as TensorFlow users&mdash;would like to see in an input pipeline API. What other pain points have we missed? What features do you miss from other systems? What other suggestions do you have?

We look forward to hearing from you!"
7949,"Can't do ""weights"" quantization on an LSTM RNN","On TensorFlow 1.0, trying to do quantization on the following graph:

```python
cell = tf.contrib.rnn.BasicLSTMCell(num_units=64)

outputs, _ = tf.nn.dynamic_rnn(
    cell=cell,
    dtype=tf.float32,
    sequence_length=tf.constant([3, 2]),
    inputs=tf.constant([[[1.,1.,1.]], [[1.,1.,0.]]]))

outputs = tf.identity(outputs, name=""y"")
```

Fails with:

```
Traceback (most recent call last):
  File ""import.py"", line 18, in <module>
    result = sess.run(outputs)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'rnn/while/Select_1' has inputs from different frames. The input 'rnn/while/basic_lstm_cell/add_1' is in frame ''. The input 'rnn/while/GreaterEqual_1' is in frame 'rnn/while/rnn/while/'.
```

Instructions to reproduce here: https://github.com/reuben/tf-export-test (just run reproduce.sh)

It seems like the quantization code is unaware of control flow contexts, so it creates a broken graph when adding dequantize nodes. Interestingly, using a BasicRNNCell instead of a BasicLSTMCell works. Maybe because BasicRNNCell only does a single matmul + bias_add, and those have quantized kernel implementations and don't need dequantize nodes? It's not clear to me."
7946,(Saver.max_to_keep) should keep every checkpoint by default,"I was wondering why my checkpoints were being deleted and it's only after looking up the behavior of `Saver.max_to_keep` in the docs that I understood what was happening. I think a better default setting for this is to keep all checkpoints, at least that's what I would have expected."
7945,Input producer freezes if queue runner not started,"Hi,

my entire program freezes when I pop an image from an input queue before starting the queue runner. The terminal window becomes unresponsive (including CTRL+C or CTRL+D signals) and the only way to kill the process is to close the terminal window. In rare cases zombie processes remain even after closing the terminal. 
I understand that I am [not supposed to do that](https://github.com/tensorflow/tensorflow/issues/7034), but I think that Tensorflow absolutely HAS to check whether the queue runner is already initialized and throw an exception otherwise. Several reported issues might be related to that: [7503](https://github.com/tensorflow/tensorflow/issues/7573), [7573](https://github.com/tensorflow/tensorflow/issues/7573) etc.

For reproducibility I provide a minimum (non-)working example. It requires a subfolder `images` with a handful of .jpg images:
```
import tensorflow as tf

# Read images from queue
filename_queue = tf.train.string_input_producer(tf.train.match_filenames_once(""./images/train/*.jpg""), shuffle=False)
image_reader = tf.WholeFileReader()
_, image_file = image_reader.read(filename_queue)
image_query = tf.image.decode_jpeg(image_file)

# Decode the image as a JPEG file, resize and assign to a variable
image_query = tf.image.resize_images(image_query, (256, 256))
image_batch = tf.Variable(tf.zeros((256, 256, 3)), trainable=False, name=""image_batch"")
next_image = tf.assign(image_batch, image_query)
loss = tf.reduce_mean(image_batch)

# Start a new session to show example output.
with tf.Session() as sess:
    # Fix randomness, init variables and run threads
    tf.set_random_seed(42)
    tf.global_variables_initializer().run()

    # Go to the next image
    sess.run([next_image])

    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)

    for i in xrange(0, 5):

        # Get an image tensor and print its value.
        image_tensor = sess.run([image_batch])
        print(image_tensor[0][0, 0, 0])

    # Finish off the filename queue coordinator.
    coord.request_stop()
    coord.join(threads)
```

and here is the working code (the line `sess.run([next_image])` was moved):
```
import tensorflow as tf

# Read images from queue
filename_queue = tf.train.string_input_producer(tf.train.match_filenames_once(""./images/train/*.jpg""), shuffle=False)
image_reader = tf.WholeFileReader()
_, image_file = image_reader.read(filename_queue)
image_query = tf.image.decode_jpeg(image_file)

# Decode the image as a JPEG file, resize and assign to a variable
image_query = tf.image.resize_images(image_query, (256, 256))
image_batch = tf.Variable(tf.zeros((256, 256, 3)), trainable=False, name=""image_batch"")
next_image = tf.assign(image_batch, image_query)
loss = tf.reduce_mean(image_batch)

# Start a new session to show example output.
with tf.Session() as sess:
    # Fix randomness, init variables and run threads
    tf.set_random_seed(42)
    tf.global_variables_initializer().run()

    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)

    for i in xrange(0, 5):
        # Go to the next image
        sess.run([next_image])

        # Get an image tensor and print its value.
        image_tensor = sess.run([image_batch])
        print(image_tensor[0][0, 0, 0])

    # Finish off the filename queue coordinator.
    coord.request_stop()
    coord.join(threads)
```"
7944,can not save the .ckpt file ,"


I run such piece of codes on my lab server (Tensor flow 1.0 + cuda7.5)
`	with tf.Session() as session:
		with tf.device(""/cpu:1""):
			session.run(init)
			coord = tf.train.Coordinator()
			threads = tf.train.start_queue_runners(coord=coord)
			max_iter = 9000
			iter= 0
			if os.path.exists(os.path.join(""model"", 'model.ckpt')) is True:
				tf.train.Saver(max_to_keep=None).restore(session, os.path.join(""model"", 'model.ckpt'))
			while iter < max_iter:
				loss_np, _, label_np, image_np, inf_np = session.run([loss, opti, batch_image, batch_label, inf])

				if iter % 50 == 0:
					print 'trainloss:', loss_np
				#if iter % 1000== 0:
					#tf.train.Saver(max_to_keep=None).save(session, os.path.join('model','model.ckpt'))
				iter = iter+1

			# if iter%500==0: 
			# accuracy_np=session.run([accuracy]) 
			# print '***************test accruacy:',accuracy_np,'*******************' 
			# tf.train.Saver(max_to_keep=None).save(session, os.path.join('model','model.ckpt')) 
				#iter += 1	
			coord.request_stop()# queue
			coord.join(threads)
			tf.train.Saver(max_to_keep=None).save(session, os.path.join('model','model.ckpt'))`

I get this error:(last line of my codes)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'save/restore_all': Could not satisfy explicit device specification '/device:CPU:1' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0, /job:localhost/replica:0/task:0/gpu:0, /job:localhost/replica:0/task:0/gpu:1, /job:localhost/replica:0/task:0/gpu:2, /job:localhost/replica:0/task:0/gpu:3, /job:localhost/replica:0/task:0/gpu:4, /job:localhost/replica:0/task:0/gpu:5, /job:localhost/replica:0/task:0/gpu:6, /job:localhost/replica:0/task:0/gpu:7

I am new to tensorflow, Hope for help!


NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7943,The TensorFlow library wasn't compiled to use ... instructions,"Being flooded with
""The TensorFlow library wasn't compiled to use AVX(etc) instructions, but these are available on your machine and could speed up CPU computations.""

If they are available on our machine,  is it possible to download a pip archive with these features enabled? If not, it is a strong Feature request.
"
7941,XLA standalone,I don't know all the internals but what do you think to standalone XLA? I think that targetting HLO IR could be of general use also for other opensource projects. Expecially if the streamexecutor CUDA and OpenCL component it is mainlined in LLVM parallel-lib repository.
7940,wrong commands for installing protobuf at tutorial,"The commands for installing protobuf at tutorial is wrong:

![bug](https://cloud.githubusercontent.com/assets/1631405/23393228/69fcc090-fdbc-11e6-96e3-413608bfee4a.png)

They are used for installing tensorflow via pip, but obviously what should be given are ways to install google protobuf via pip.

Please fix it.

Here's the url: https://www.tensorflow.org/install/install_linux#installing_with_native_pip"
7939,tf.losses.mean_squared_error does not support name parameter,"Some times MSE is not the only loss in the model, and there maybe multiple MSE losses and adds up to the total loss, a custom name of mse loss layer without using name_scope can be helpful."
7938,Add Python Graph Transform examples,"The docs for the [Graph Transform Tool](https://github.com/tensorflow/tensorflow/blob/d699a66e940b26e991b29b27f4e3ad2e8e3282d2/tensorflow/tools/graph_transforms/README.md#writing-your-own-transforms) talk about writing your own transforms but way to do so is in C++. It would be cool if there was an example of / c++ hook writing transforms in Python as well. For example, here is one Python transform (by @mrry): http://stackoverflow.com/a/40852855/2638485 (https://github.com/tensorflow/tensorflow/issues/5918)."
7936,mnist_softmax_xla.py script does not show the correct timeline with xla flag turned on,"I ran mnist_softmax_xla.py with XLA turned on but did not see the ""_XLALaunch"" in the timeline. It still shows the same timeline (i.e same as without XLA turned on). It looks like that XLA is not getting launched. Any help would be greatly appreciated?

### Environment info
Operating System: SUSE Linux 12

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

-rw-r--r-- 1 root root   558720 Jan 10 14:52 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Jan 10 14:52 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Jan 10 14:52 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rwxr-xr-x 1 root root   415432 Jan 10 14:52 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 Jan 10 14:52 /usr/local/cuda/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 79337624 Nov 22 05:34 /usr/local/cuda/lib64/libcudnn.so
-rwxr-xr-x 1 root root 79337624 Nov 22 05:34 /usr/local/cuda/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 79337624 Nov 22 05:34 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Nov 22 05:34 /usr/local/cuda/lib64/libcudnn_static.a


![timeline_xla](https://cloud.githubusercontent.com/assets/21690396/23388520/8535e576-fd17-11e6-9d1e-92ec009aeab1.png)

1. A link to the pip package you installed: 
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp35-cp35m-linux_x86_64.whl

2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.0


"
7935,RuntimeError: No C++ shape function registered for standard op: NearestNeighbors,"For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Github issue :- [7524](https://github.com/tensorflow/tensorflow/issues/7524)
Stackoverflow question:- [here](http://stackoverflow.com/questions/42250340/runtimeerror-no-c-shape-function-registered-for-standard-op-nearestneighbors)

### Environment info
Operating System: Ubuntu 16.04 / Windows

Installed version of CUDA and cuDNN: NO

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. 1.0

If installed from source, provide NO

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```
from tensorflow.contrib.learn.python.learn.estimators import kmeans as kmeans_lib
from tensorflow.contrib.factorization.python.ops import clustering_ops
import random
import numpy as np
import tensorflow as tf


def input_fn(x):
""""""Returns an input_fn""""""
def _fn():
       return tf.constant(x), None
return _fn

x = np.array([[random.random() for i in range(198)] for j in range(2384)], dtype=np.float32)
km = kmeans_lib.KMeansClustering(num_clusters=200, initial_clusters=clustering_ops.KMEANS_PLUS_PLUS_INIT)
km.fit(input_fn=input_fn(x), max_steps=300)
```

### What other attempted solutions have you tried?
Hi, I found following 2 lines missing in gen_clustering_ops.py. I believe they should there in the file. If not then please raise a bug.  I couldn't find the correct place where the change is to be made
![image](https://cloud.githubusercontent.com/assets/25343161/23388256/21b477fe-fd87-11e6-856c-c07dc3de4fa8.png)


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

```
File ""C:\Users#####\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1568, in call_with_requiring
return getattr(x, f)
File ""C:\Users#####\Anaconda3\lib\site-packages\tensorflow\python\framework\common_shapes.py"", line 610, in call_cpp_shape_fn
debug_python_shape_fn, require_shape_fn)
File ""C:\Users#####\Anaconda3\lib\site-packages\tensorflow\python\framework\common_shapes.py"", line 680, in _call_cpp_shape_fn_impl
""No C++ shape function registered for standard op: %s"" % op.type)
RuntimeError: No C++ shape function registered for standard op: NearestNeighbors
```"
7934,Feature Request: Tranponse of separable_conv2d?,"I am trying to implement transposed version of separable_conv2d probably name it separable_conv2d_transpose

The existing conv operations in TF for convolution are:
```
convolution
conv2d
depthwise_conv2d
depthwise_conv2d_native
separable_conv2d
atrous_conv2d
atrous_conv2d_transpose
conv2d_transpose
conv1d
conv3d
conv3d_transpose
conv2d_backprop_filter
conv2d_backprop_input
conv3d_backprop_filter_v2
depthwise_conv2d_native_backprop_filter
depthwise_conv2d_native_backprop_input
```

Does anyone know how to approach an implementation of the op, or better combine existing backprop filters to achieve the result?
"
7933,pip (python3) does not install *most* example code,"When installing on my Debian (stretch) using the ""native pip"" method via: 

# pip3 install -U tensorflow-gpu

I get tensorflow to install, but the examples directory does not contain everything I believe it should based on the source tree in github: This is what is installed in the examples directory:

/usr/local/lib/python3.5/dist-packages/tensorflow/examples$ tree
.
 __init__.py
 __pycache__
  __init__.cpython-35.pyc
 tutorials
     __init__.py
     mnist
      __init__.py
      input_data.py
      mnist.py
      __pycache__
          __init__.cpython-35.pyc
          input_data.cpython-35.pyc
          mnist.cpython-35.pyc
     __pycache__
         __init__.cpython-35.pyc

5 directories, 10 files

PS. I wonder if this installation bug is related to issue: https://github.com/tensorflow/tensorflow/issues/590
since other issues report having similar problems there even after issue 590 was closed. "
7932,get_shape() does not work for output of tf.image.resize_nearest_neighbor(),"During debugging, I was trying to get the shape of output tensor from tf.image.resize_nearest_neighbor(). It seems get_shape() does not work for it.


"
7930,Error calc. gradient? ValueError: None values not supported.,"```
Traceback (most recent call last):
  File ""cnn_mnist.py"", line 140, in <module>
    adv_x = fgsm(batch_xs[0], y_conv, 0.3)
  File ""cnn_mnist.py"", line 43, in fgsm
    signed_grad = tf.sign(grad)
  File ""C:\Users\soone\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 452, in sign
    return gen_math_ops.sign(x, name=name)
  File ""C:\Users\soone\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 2447, in sign
    result = _op_def_lib.apply_op(""Sign"", x=x, name=name)
  File ""C:\Users\soone\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 504, in apply_op
    values, as_ref=input_arg.is_ref).dtype.name
  File ""C:\Users\soone\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 716, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Users\soone\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 176, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""C:\Users\soone\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 165, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""C:\Users\soone\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\tensor_util.py"", line 360, in make_tensor_proto
    raise ValueError(""None values not supported."")
ValueError: None values not supported.
```

Hi guys. I'm new to tensorflow and is trying to work on a project for creating adversarial samples for fooling an MNIST classifier. 

I found there was a fast gradient sign method and I tried to implement this in my code. However whenever I try to calculate the gradient I would always get ""None values"" even though I'm sure all the weights are loaded properly. 

Any help is appreciated. Thank you!"
7929,anyone else receive the following warning ,"Hey guys, I was wondering if there's anyone else receiving the following warnings. I've updated `tf` through pip and since then I get the following warnings:

```
Using TensorFlow backend.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
```
Any idea what's going on? Prior to the update to the latest version never had this warning before."
7927,tf.nn.rnn_cell.DropoutWrapper() drops out across time-steps," at `class DropoutWrapper(RNNCell):` it seems dropout is implemented across all inputs and outputs without any implementation options. 

Would like to have the option for rnn dropout where one 'dropout mask' is generated and is then applied to each time step. On the next batch, a new mask is generated. This method is described in: 

(https://arxiv.org/pdf/1512.05287.pdf)

 There a few other dropout methods that have been published recently including one from google.

(https://www.aclweb.org/anthology/C/C16/C16-1165.pdf)

And also 'zoneout' which is mentioned in Issue #2789.

Are there any plans to incorporate these advances?

-Brad

"
7926,Freeze graph erroring out claiming there are uninitialized values in embeddings,"This is similar to #7172 but with my own model.

I'm training a CNN with word embeddings and for some reason I'm getting `FailedPreconditionError` exception whenever I try to save a frozen version of the model for later use.

This is despite the fact that I call `sess.run(tf.global_variables_initializer())` just before training and I have no problem monitoring the training in tensorboard and checkpointing the model at regular intervals.

The problem occurs when I try to load a model from a checkpoint and save a frozen model. The function I'm using is as follows:

```python
def freeze_model(checkpoint_path, model_save_path, output_node_names):
    checkpoint = tf.train.get_checkpoint_state(checkpoint_path)
    input_checkpoint = checkpoint.model_checkpoint_path

    saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=True)
    graph = tf.get_default_graph()
    input_graph_def = graph.as_graph_def()
    with tf.Session() as sess:
        saver.restore(sess, input_checkpoint)

        output_graph_def = graph_util.convert_variables_to_constants(
            sess,
            input_graph_def,
            output_node_names
        )

        with tf.gfile.GFile(model_save_path, ""wb"") as f:
            f.write(output_graph_def.SerializeToString())
```

The error that I'm getting is as follows. It looks like the problem is with two embeddings coefficients. If I set `clear_devices=False`, I get the same error but for only one embedding coefficient:

```python
Traceback (most recent call last):
  File ""myproject/train.py"", line 522, in <module>
    tf.app.run()
  File ""/home/foo/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""myproject/train.py"", line 518, in main
    trainer.save_model(preprocessor)
  File ""myproject/train.py"", line 312, in save_model
    ut.freeze_model(self.checkpoint_dir, model_save_path, C.OUTPUT_NODE_NAMES)
  File ""/home/foo/anaconda2/lib/python2.7/site-packages/myproject/utils.py"", line 224, in freeze_model
    output_node_names
  File ""/home/foo/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/graph_util_impl.py"", line 218, in convert_variables_to_constants
    returned_variables = sess.run(variable_names)
  File ""/home/foo/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/home/foo/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/foo/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/home/foo/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
FailedPreconditionError: Attempting to use uninitialized value embeddings/W
	 [[Node: embeddings/W/_20 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_30_embeddings/W"", _device=""/job:localhost/replica:0/task:0/gpu:0""](embeddings/W)]]
	 [[Node: embeddings/W/_21 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_30_embeddings/W"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
```

### Environment info
tensorflow-gpu: 1.0.0
Ubuntu 14.0.4.5
CUDA Version 8.0.44


### What other attempted solutions have you tried?
I am assigning the embeddings pretrained word2vec values after initializing. I have tried running the model without assigning these values and I get the same error.
I have tried running the training for 70K batches.
I have tried running `tf.report_uninitialized_variables` after calling `tf.global_variables_initializer()` and it has come up empty.
"
7925,"Hi, I think I have a similar problem. How can I run python from a directory other than tensorflow ? Thanks !","NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7924,tf.train.import_meta_graph('model.meta') cannot handle seq2seq models with attention?,"Environment: 
Ubuntu 16.04
TensorFlow v1.0.0

When attempting to import a saved graph using ""tf.train.import_meta_graph('model.meta'),"" I get the following error:

    Traceback (most recent call last):
      File ""test_load.py"", line 19, in <module>
        new_saver = tf.train.import_meta_graph('model.meta')
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1577, in import_meta_graph **kwargs)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.py"", line 498, in import_scoped_meta_graph producer_op_list=producer_op_list)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py"", line 259, in import_graph_def raise ValueError('No op named %s in defined operations.' % node.op)
    ValueError: No op named attn_add_fun_f32f32f32 in defined operations.

This error isn't thrown when I retrain my model **without attention** and import the graph with the same line of code.
Is loading a model trained with attention not currently supported? 

Thanks!"
7922,Unable to specify loss function in tf.contrib.learn.DNNLinearCombinedRegressor,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Have been unable to find how to specify loss function for both linear and deep components of the DNNLinearCombinedRegressor initializer. Looked in many places including tf.train (initialize optimizer with specific loss function?) and in learn.estimators. Also could not find a solution on stackoverflow or anywhere else through google searches.

### Environment info
Operating System: mac OS 10.12.3

Installed version of CUDA and cuDNN: None.
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed: 
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. Version 1.0.0

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
Have not been able to specify loss function. 


### What other attempted solutions have you tried?
Tried to create custom estimator using tf.contrib.losses to specify desired loss but unable to combine both deep and wide this way. Would prefer to use the pre-defined linearCombinedRegressor with a loss function specified for _each_ of the linear and deep optimizers. How?

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7921,Error with bazel build when retraining inception final layer for new categories,"I'm doing the TensorFlow tutorial on retraining the inception's final layer for new categories: https://www.tensorflow.org/tutorials/image_retraining.

Then, when I did `bazel build tensorflow/examples/image_retraining:retrain`, it gave me the following errors:

```
ERROR: /home/darth/tensorflow/WORKSPACE:3:1: //external:io_bazel_rules_closure: no such attribute 'urls' in 'http_archive' rule.
ERROR: /home/darth/tensorflow/WORKSPACE:3:1: //external:io_bazel_rules_closure: missing value for mandatory attribute 'url' in 'http_archive' rule.
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error loading package 'external': Could not load //external package.
INFO: Elapsed time: 0.153s
```

Thank you in advance for the help."
7920,Compilation failure on OS/X w/ XLA,"OS/X compilation fails with XLA.

Error is:
```
RROR: /Users/davidn/github/tensorflow/tensorflow/compiler/xla/service/BUILD:108:1: C++ compilation of rule '//tensorflow/compiler/xla/service:versioned_computation_handle' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 93 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
In file included from tensorflow/compiler/xla/service/versioned_computation_handle.cc:16:
./tensorflow/compiler/xla/service/versioned_computation_handle.h:33:19: error: unknown type name 'int64'; did you mean 'tensorflow::int64'?
  using Version = int64;
                  ^~~~~
                  tensorflow::int64
./tensorflow/core/platform/default/integral_types.h:27:19: note: 'tensorflow::int64' declared here
typedef long long int64;
                  ^
In file included from tensorflow/compiler/xla/service/versioned_computation_handle.cc:16:
./tensorflow/compiler/xla/service/versioned_computation_handle.h:38:3: error: unknown type name 'string'; did you mean 'std::string'?
  string ToString() const;
  ^~~~~~
  std::string
/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/iosfwd:194:65: note: 'std::string' declared here
typedef basic_string<char, char_traits<char>, allocator<char> > string;
                                                                ^
tensorflow/compiler/xla/service/versioned_computation_handle.cc:22:1: error: unknown type name 'string'; did you mean 'std::string'?
string VersionedComputationHandle::ToString() const {
^~~~~~
std::string
/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/iosfwd:194:65: note: 'std::string' declared here
typedef basic_string<char, char_traits<char>, allocator<char> > string;
                                                                ^
```

issue was probably introduced in 7817ac8055ca328e5bf902677f335502eb0da926.

It is trivial to fix."
7919,Warning during compilation on OS/X,"... and maybe other systems too.

the warning is

```
./tensorflow/compiler/xla/service/backend.h:37:1: warning: class 'ThreadPoolDevice' was previously declared as a struct [-Wmismatched-tags]
```
It appears on almost every file in the XLA compiler directory, making the spotting of real problems harder.

"
7918,Add support for loading specific frame from SequenceExample,"Hi, 

I have converted my video dataset into TFRecords format by storing each video as a SequenceExample. The problem is I don't neccesarily need to load all frames of a video instead of 2 random frames. Is there any plan to add support for loading specific frame from a SequenceExample to save loading time?  Or is there any method I can use to speed up loading?
"
7917,Unreachable statement,"tensorflow\compiler\xla\service\algebraic_simplifier.cc line 675

additional: return Status::OK();"
7916,tf.TFRecordReader returns multiples copies of the same data with only 1 epoch,"I am using tf.TFRecordReader and tf.train.batch to evaluate one epoch of my dataset (it has only one file). In order to get a better performance I tried to use enqueue_many passing a list of reader calls, because supposedly it launches several reader threads. However its behaviour has not been as I expected. It returns each item duplicated, although I thought it would return the same list of items, but faster. I took the code from @Yaroslavvb github: https://github.com/yaroslavvb/stuff/blob/master/ericyue-slowreader/benchmark.py

Is this behaviour right? It seems really strange to me and I don't know if it is a bug or a feature.

Here you can see a snippet of the code:

    reader = tf.TFRecordReader()
    queue_batch = []
    for i in range(enqueue_many_size):
        _, serialized_example = reader.read(filename_queue)
        queue_batch.append(serialized_example)
    batch_serialized_example = tf.train.batch(
        [queue_batch],
        batch_size=batch_size,
        num_threads=thread_number,
        capacity=capacity,
        enqueue_many=True)

### Environment info
Operating System: Ubuntu 16.04
Nvidia Geforce 1080
Python 3.5
Tensorflow version: 1.0.0

Installed version of CUDA and cuDNN: 
/usr/local/cuda-8.0/lib64/libcudadevrt.a       /usr/local/cuda-8.0/lib64/libcudnn.so
/usr/local/cuda-8.0/lib64/libcudart.so         /usr/local/cuda-8.0/lib64/libcudnn.so.5
/usr/local/cuda-8.0/lib64/libcudart.so.8.0     /usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44  /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5
/usr/local/cuda-8.0/lib64/libcudart_static.a   /usr/local/cuda-8.0/lib64/libcudnn_static.a

The output for batch size of 4 records, enqueue_many_size=2, and record size of 16 floats, is:

    [array([[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
         11.,  12.,  13.,  14.,  15.],
       [  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
         11.,  12.,  13.,  14.,  15.],
       [ 16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,
         27.,  28.,  29.,  30.,  31.],
       [ 16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,
         27.,  28.,  29.,  30.,  31.]], dtype=float32)]
"
7915,url not present for python3.6,"I noticed that the URL for python3.6 is not included on this page,
https://www.tensorflow.org/install/install_linux#TF_PYTHON_URL."
7914,Fails when atrous rate is higher than ~280.,"When `tf.nn.atrous_conv2d` is called with a rate higher than ~280, tensorflow fails with the following message: `F tensorflow/stream_executor/cuda/cuda_dnn.cc:2742] failed to enqueue convolution on stream: CUDNN_STATUS_NOT_SUPPORTED`  
Further invistigation leads to when [tensorflow calls cudnn convolution backward filters operation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2681).

Tested with multiple machines with TITAN X's and all pip packages and CUDA/CUDNN upgraded to latest."
7913,can there any examples about multi GPU or multi machine training,"the news said 64 GPU can acheive 56 speed,how can we build for training test.  
can there any examples?"
7912,Validating Tensorflow On OS X (Sierra 10.12.3),"Using python 2.7. Created virtualenv
Installed tensorflow on virtualenv as per instruction
Went to validation steps and below is the output:

>>> import tensorflow as tf
>>> hello = tf.constant('hello, tensorflow!')
>>> sess = tf.Session()
**_W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations._**
>>> print (sess.run(hello))
hello, tensorflow!

Any idea what I have done wrong? The message does not appear to be a show stopper rather an unhelpful warning message. Will crack on with my R+D but would appreciate knowing what I can do to fix this!

Cheers,

G"
7911,TensorBoard Image download fails if I expand some node before download,"TensorBoard seems unable to download flow diagram picture if before download picture I expand some node. In the save picture window the name of picture isn't generated, appears a generic ""download"" filename, and if I click ""save"", download starts but fails instantly. 
![sample_pic](https://cloud.githubusercontent.com/assets/8108287/23348702/ee3061fc-fcac-11e6-9c79-73df698cca0f.JPG)


Tensorflow v 1.0.0
Windows 10 x64"
7910,How tensorflow handles complex gradient ?,"Let **z** is a complex variable, **C(z)** is its conjugation.
In complex analysis theory, the derivative of **C(z)** w.r.t **z** don't exist. But in tesnsorflow, we can calculate **dC(z)/dz** and the result is just **1**.
Here is an example:
>x = tf.placeholder('complex64',(2,2))
y = tf.reduce_sum(tf.conj(x))
z = tf.gradients(y,x)
sess = tf.Session()
X = np.random.rand(2,2)+1.j*np.random.rand(2,2)
X = X.astype('complex64')
Z = sess.run(z,{x:X})[0]

The input **X** is
>[[ 0.17014372+0.71475762j  0.57455420+0.00144318j]
    [0.57871044+0.61303568j  0.48074263+0.7623235j ]]

and the result **Z** is
>[[ 1.-0.j  1.-0.j]
    [1.-0.j  1.-0.j]]
       
I don't understand why the gradient is set to be **1**?
And I want to know **how tensorflow handles the complex gradients in general**."
7909,is it tf.train.MomentumOptimizer is SGD+monment  Optimizer???,is it tf.train.MomentumOptimizer is SGD+monment  Optimizer???
7908,Changing computer make the pretrained model fail,"I switched to another computer to keep training my model .But I found the loss is as high as random init after loading the pretrained model. I trained overagain on this new computer and the restore seems work.What's wrong? Hope for help.

using TF:1.0 "
7907,Importing tensorflow in newly created virtualenv fails due to setuptools ContextualZipFile,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
https://github.com/pypa/setuptools/issues/252
### Environment info
Operating System: MasOS Sierra 10.12.3

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): None

If installed from binary pip package, provide:

1. A link to the pip package you installed: 
```
$ pip --version
pip 9.0.1 from /Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages (python 2.7)
```
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
```
1.0.0
```
If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
```
from __future__ import print_function
import numpy as np
import tensorflow as tf
from six.moves import cPickle as pickle
from six.moves import range
```

### What other attempted solutions have you tried?
None

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
```
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-1-4a2138a67abe> in <module>()
      3 from __future__ import print_function
      4 import numpy as np
----> 5 import tensorflow as tf
      6 from six.moves import cPickle as pickle
      7 from six.moves import range

/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py in <module>()
     22 
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26 

/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py in <module>()
    122 from tensorflow.python.platform import resource_loader
    123 from tensorflow.python.platform import sysconfig
--> 124 from tensorflow.python.platform import test
    125 
    126 from tensorflow.python.util.all_util import remove_undocumented

/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/test.py in <module>()
     81 import sys
     82 if sys.version_info.major == 2:
---> 83   import mock                # pylint: disable=g-import-not-at-top,unused-import
     84 else:
     85   from unittest import mock  # pylint: disable=g-import-not-at-top

/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/mock/__init__.py in <module>()
      1 from __future__ import absolute_import
----> 2 import mock.mock as _mock
      3 from mock.mock import *
      4 __all__ = _mock.__all__
      5 #import mock.mock as _mock

/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/mock/mock.py in <module>()
     69 from pbr.version import VersionInfo
     70 
---> 71 _v = VersionInfo('mock').semantic_version()
     72 __version__ = _v.release_string()
     73 version_info = _v.version_tuple()

/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/pbr/version.pyc in semantic_version(self)
    458         """"""Return the SemanticVersion object for this version.""""""
    459         if self._semantic is None:
--> 460             self._semantic = self._get_version_from_pkg_resources()
    461         return self._semantic
    462 

/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/pbr/version.pyc in _get_version_from_pkg_resources(self)
    444             # produced from a tarball where the package itself has not been
    445             # installed into anything. Revert to setup-time logic.
--> 446             from pbr import packaging
    447             result_string = packaging.get_version(self.package)
    448         return SemanticVersion.from_pip_string(result_string)

/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/pbr/packaging.py in <module>()
     31 import pkg_resources
     32 import setuptools
---> 33 from setuptools.command import develop
     34 from setuptools.command import easy_install
     35 from setuptools.command import egg_info

/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/setuptools/command/develop.py in <module>()
      9 
     10 from pkg_resources import Distribution, PathMetadata, normalize_path
---> 11 from setuptools.command.easy_install import easy_install
     12 from setuptools import namespaces
     13 import setuptools

/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/setuptools/command/easy_install.py in <module>()
     49 from setuptools.py27compat import rmtree_safe
     50 from setuptools.command import setopt
---> 51 from setuptools.archive_util import unpack_archive
     52 from setuptools.package_index import (
     53     PackageIndex, parse_requirement_arg, URL_SCHEME,

/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/setuptools/archive_util.py in <module>()
      9 from distutils.errors import DistutilsError
     10 
---> 11 from pkg_resources import ensure_directory, ContextualZipFile
     12 
     13 __all__ = [

ImportError: cannot import name ContextualZipFile
```"
7906,Erroneous number of channels in the Guide to TF Layers tutorial.,"The tutorial _[A Guide to TF Layers: Building a Convolutional Neural Network](https://www.tensorflow.org/tutorials/layers)_ seems to have the wrong number of channels in the tensor dimensions given at the end of the paragraph _Convolutional Layer #1_:

> Our output tensor produced by conv2d() has a shape of [batch_size, 28, 28, 1]: the same width and height dimensions as the input, but now with 32 channels holding the output from each of the filters.

To be consistent with the rest of the tutorial, it should probably say _""shape of [batch_size, 28, 28, 32]""_, since the rightmost dimension denotes the number of channels."
7905,pi_examples: Building label_image shows missing graph.pb.h then graph.pb.h version error,"When building the label_image example in pi_examples:

./tensorflow/core/framework/graph.pb.h:12:2: error: #error This file was generated by a newer version of protoc which is
./tensorflow/core/framework/graph.pb.h:13:2: error: #error incompatible with your Protocol Buffer headers. Please update

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

None, the mentions of graph.pb.h were all for that file simply being missing. 

In my case, moving graph.pb.h to the location the compiler expects resulted in the version error message, above.

Also, the issue reports seemed to be pre- TF v.1.0.0, and likely not relevant.

### Environment info
Operating System:
Raspbian Jessie, version ""Jan 2017"", release date ""2017-01-11""
https://www.raspberrypi.org/downloads/raspbian/

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
N/A

If installed from binary pip package, provide:
N/A

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
321d38ee0ff76d749a8578f37128b02bf033ce76

2. The output of `bazel version`
Build label: 0.4.3- (@non-git)
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Feb 24 03:14:34 2017 (1487906074)
Build timestamp: 1487906074
Build timestamp as int: 1487906074

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Build tensorflow from source, exactly as at:
https://github.com/samjabrahams/tensorflow-on-raspberry-pi/blob/master/GUIDE.md

Attempt tp build label_image example, exactly as at:
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/pi_examples/

Corrected error caused by missing graph.pb.h by copying graph.pb.h from:
/home/pi/tensorflow/bazel-tensorflow/tensorflow/core/framework/graph.pb.h
to
/home/pi/tensorflow/tensorflow/core/framework/node_def.pb.h

and ran the label_image make again. Error shown below.

### What other attempted solutions have you tried?
None, not very familiar w/Bazel and TF building yet... after all, the pi_examples are for newbies to get started with and learn TF, right? 

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

pi@raspberrypi:~/tensorflow $ make -f tensorflow/contrib/pi_examples/label_image/Makefile
gcc --std=c++11 -O0 -I/usr/local/include -I. -I/home/pi/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/downloads -I/home/pi/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/downloads/eigen/ -I/home/pi/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/gen/proto/ -I/home/pi/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/gen/proto_text/ -c tensorflow/contrib/pi_examples/label_image/label_image.cc -o /home/pi/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o
In file included from tensorflow/contrib/pi_examples/label_image/label_image.cc:32:0:
./tensorflow/core/framework/graph.pb.h:12:2: error: #error This file was generated by a newer version of protoc which is
 #error This file was generated by a newer version of protoc which is
  ^
./tensorflow/core/framework/graph.pb.h:13:2: error: #error incompatible with your Protocol Buffer headers. Please update
 #error incompatible with your Protocol Buffer headers.  Please update
  ^
./tensorflow/core/framework/graph.pb.h:14:2: error: #error your headers.
 #error your headers.
  ^
In file included from tensorflow/contrib/pi_examples/label_image/label_image.cc:32:0:
./tensorflow/core/framework/graph.pb.h:31:51: fatal error: tensorflow/core/framework/node_def.pb.h: No such file or directory
 #include ""tensorflow/core/framework/node_def.pb.h""
                                                   ^
compilation terminated.
tensorflow/contrib/pi_examples/label_image/Makefile:79: recipe for target '/home/pi/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o' failed
make: *** [/home/pi/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o] Error 1"
7904,add_summary won't accept the tensor returned by tensor_summary ,"### Related Problem
https://github.com/tensorflow/tensorflow/issues/6778

### Environment Info
Operating System: Kubuntu 16.04 LTS

Installed TensorFlow version:
No CUDA acceleration. Compiled from source.

commit hash: 
`4ac9c09d5ca57a03b8daa5fb9e295947b1619854`

bazel version:
```
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261
```

### Example

I restored a graph from a checkpoint and get the variables. Then I tried to write the summary to the log but got an error.

```
sess = tf.Session()
saver = tf.train.import_meta_graph('my-model.meta')
saver.restore(sess, tf.train.latest_checkpoint('./'))
all_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
writer = tf.summary.FileWriter('./many/log',sess.graph)
writer.add_summary(tf.summary.tensor_summary(""conv"", all_vars[0]))
```

```
>>> writer.add_summary(tf.summary.tensor_summary('conv',all_vars[0]))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/summary/writer/writer.py"", line 107, in add_summary
    event = event_pb2.Event(summary=summary)
TypeError: Parameter to MergeFrom() must be instance of same class: expected tensorflow.Summary got Tensor.
```
### Official API Description

According to the official API (https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter), 

`tf.summary.tensor_summary` 

> Returns:
A scalar Tensor of type string. The serialized Summary protocol buffer.

 `add_summary(summary, global_step=None)`

> You can pass the result of evaluating any summary op, using tf.Session.run or tf.Tensor.eval, to this function. Alternatively, you can pass a tf.Summary protocol buffer that you populate with your own data. The latter is commonly done to report evaluation results in event files.

So I believe this is a bug."
7902,Periodically evaluating on a validation set,"A common training pattern is to run an epoch, or a few epochs, of training and then evaluate on a development set. As far as I can tell, there does not seem to be an easy way to do this while also using the TFRecords format. While it is possible to run through a data split by setting the num_epochs parameter in the functions to read in single examples, doing so would require the computation graph to be rebuilt before every validation phase. Instead, it would be preferable to have a symbolic example that could be reset to the beginning of the dataset using a related operation.

I feel that the above pattern is sufficiently widely used to make a new feature worthwhile. Further, having control over the number of epochs also becomes important when trying to replicate the performance numbers of other implementations.

####
The same training-validation pattern using feed dicts:
for epoch in num_epochs:
  for batch in get_batches(train, epoch):
    sess.run(train_op, feed_dict={""batch"": batch)
  num_correct, total = 0, 0
  for batch in get_batches(val, epoch):
    acc = sess.run(acc_op, feed_dict={""batch"", batch)
    total += batch.shape[0]
    num_correct += batch.shape[0] * acc
  print ""Dev accuracy after %s epochs: %s"" % (epoch + 1, num_correct / total)"
7900,Segmenttation fault tensorflow 1.0 osx,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
https://github.com/tensorflow/tensorflow/issues/3263
https://github.com/tensorflow/tensorflow/issues/2278

Note, I already set the link to 
`sudo ln -s /usr/local/cuda/lib/libcuda.dylib /usr/local/cuda/lib/libcuda.1.dylib`

### Environment info
Operating System:
osx 10.12.3

Installed version of CUDA and cuDNN:  8.0.63 / 5.1
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
```
lrwxr-xr-x  1 root  wheel     33 26 Feb 15:35 /usr/local/cuda/lib/libcuda.1.dylib -> /usr/local/cuda/lib/libcuda.dylib
-rwxr-xr-x  1 root  wheel  13504 24 Jan 20:58 /usr/local/cuda/lib/libcuda.dylib
lrwxr-xr-x  1 root  wheel     45 12 Jan 02:33 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudadevrt.a
lrwxr-xr-x  1 root  wheel     50 12 Jan 02:33 /usr/local/cuda/lib/libcudart.8.0.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib
lrwxr-xr-x  1 root  wheel     46 12 Jan 02:33 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.dylib
lrwxr-xr-x  1 root  wheel     49 12 Jan 02:33 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart_static
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
start python shell
import tensorflow

output is
```
Python 3.6.0 |Anaconda custom (x86_64)| (default, Dec 23 2016, 13:19:00)
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.8.0.dylib locally
Segmentation fault: 11
```

### What other attempted solutions have you tried?
setting the link as outlined above in the linked issues

Checking the CUDA sample projects works fine so the library seems to be installed correctly."
7898,"when i use tf.train.SyncReplicasOptimizer ,find worker1 run step 150,but worker0 run step 120,is right??","when i use tf.train.SyncReplicasOptimizer ,find worker1 run step 150,but worker0 run step 120,is right??

opt = tf.SyncReplicasOptimizer(opt, replicas_to_aggregate=number_workers,
                               total_num_replicas=number_workers)

is it right??,find worker1 run step 150,but worker0 run step 120,is right??
"
7897,Missing input file zipaling or aapt while building android example,"### Environment info
Operating System: Windows 10 Pro

If installed from source, provide 
The output of `bazel version`:
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:18 2017 (1485975258)
Build timestamp: 1485975258
Build timestamp as int: 1485975258


### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

It throw 'missing input file zipaling' or 'missing input file aapt'.

`ERROR: missing input file '@androidsdk//:build-tools/25.0.2/zipalign'.
ERROR: C:/tools/msys64/tmp/_bazel_Kornel/i4o1VGAg/external/androidsdk/BUILD.bazel:5:1: declared output 'external/androidsdk/zipalign_runner.sh' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely).
ERROR: C:/tools/msys64/tmp/_bazel_Kornel/i4o1VGAg/external/androidsdk/BUILD.bazel:5:1: not all outputs were created or valid.
ERROR: C:/tools/msys64/tmp/_bazel_Kornel/i4o1VGAg/external/androidsdk/BUILD.bazel:5:1: @androidsdk//:zipalign_binary: missing input file '@androidsdk//:build-tools/25.0.2/zipalign'.
Target //tensorflow/examples/android:tensorflow_demo failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: C:/tools/msys64/tmp/_bazel_Kornel/i4o1VGAg/external/androidsdk/BUILD.bazel:5:1 1 input file(s) do not exist.
`

`ERROR: missing input file '@androidsdk//:build-tools/25.0.2/aapt'.
ERROR: C:/tools/msys64/tmp/_bazel_Kornel/i4o1VGAg/external/androidsdk/BUILD.bazel:5:1: @androidsdk//:aapt_binary: missing input file '@androidsdk//:build-tools/25.0.2/aapt'.
Target //tensorflow/examples/android:tensorflow_demo failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: C:/tools/msys64/tmp/_bazel_Kornel/i4o1VGAg/external/androidsdk/BUILD.bazel:5:1 1 input file(s) do not exist.
`

### What other attempted solutions have you tried?
I tried chaning sdk versions to lower.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
[http://pastebin.com/6HQ4cVMS](url) - Pastebin zipaling log
[http://pastebin.com/VJrmxLSv](url) - Pastebin aapt log

[http://pastebin.com/mqCyPBhp](url) - Pastebin WORKSPACE file"
7896,Issue with while_loop in tf.scan,"Hello,

I have recently updated to the newest version of tensorflow (v1.0) and am suddenly having some trouble when I use tf.scan.

I have already tried setting shapes for all of the variables. Which didn't work! Don't really know what else i could do since that is the solution that is recommended for the error in tf.while_loop...

Here is the code snippet:

`
h, z = tf.scan(
                    self.sample,
                    x,
                    initializer = [ h0, tf.expand_dims(z0, 1 )]
                    )
`

Where h0 is [256, 90], z0 is expanded to [256, 1] and x is [None, 256, 400]. It worked in the previous version so I am assuming the trouble is related to the tf.scan code.


ValueError: The shape for scan/while/Merge_2:0 is not an invariant for the loop. It enters the loop with shape (256, 1), but has shape (256,) after one iteration. Provide shape invariants using either the `shape_invariants` argument of tf.while_loop or set_shape() on the loop variables.
"
7895,Running on Microsoft Server 2012 r2,"Hello Everyone,

Just tried installing on server 2012 and I get the following notifications:


E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""')
 for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for
unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""')
for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""'
) for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') fo
r unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""
') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots


I assume this is due to the server using 2 processors. 
Btw i did uninstall and reinstalled the nightly build."
7894,Unable to use Tensorflow on Android Studio,"I have opened the Android part into android studio from examples/contrib/Android and the project opened.
After that I added the pre compiled jar file libandroid_tensorflow_inference_java from the nightly mode added it as module then added module dependency from the project structure .
After that I get error on bazel location. I have also build bazel-0.40 at the following path def bazel_location = 'C:\\Users\\ANMOL\\Downloads\\bazel-0.4.0\\bazel-0.4.0\\output\\bazel.exe' and when I copy this path into Android Studio it doesnt seem to work .
Any Ideas how to make it work
![android studio bazel error](https://cloud.githubusercontent.com/assets/23556208/23337762/f6ce0df8-fc1d-11e6-89e4-291a5625ec23.jpg)
"
7892,how to aggregate gradients by weight,"`tf.gradients` has a parameter` aggregation_method`. As default, it will sum up all the gradients. What should I do if I want to sum up all the gradients by weight? To be more specific, I want to apply a weight to each gradient, then sum them up. This weight may also comes from a tensorflow variable. I asked a question about this in stackoverflow and attached a minimal code snippet.
http://stackoverflow.com/questions/42464742/how-to-weight-gradient-in-different-examples"
7890,I can successfully open CUDA library cupti64_80.dll but it stop on this step and can't forward. what should I do?  what's the problem? help,"
![image](https://cloud.githubusercontent.com/assets/16426261/23336757/11a08ce6-fc14-11e6-82da-97d77e20da5c.png)
NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7889,"Bug in code for new TF Layer tutorial ""A Guide to TF Layers: Building a Convolutional Neural Network""","The code provided for the new TF Layer API tutorial (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py) does not work if any filter size other than 5x5 is specified, or if color images are used. No matter what other filter size is specified, an error is generated indicating a shape mismatch between the specified filter (3x3 in the error message shown below) and another seemingly hard-coded 5x5 filter buried somewhere in the estimator. The tensor referred to below as ""rhs"" ALWAYS reports as [5,5,1,32] no matter what the actual filter size is set to within the python code OR if a color/3 channel image is used (e.g., lhs reports as 5x5x3x32 while rhs is 5x5x1x32).

Here's a sample code snippet wherein the filter size is set to 3x3 instead of 5x5 for the first convolution layer:

 conv1 = tf.layers.conv2d(
      inputs=input_layer,
      filters=32,
      kernel_size=[3, 3],
      padding=""same"",
      activation=tf.nn.relu)


and here's the error message:

InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [3,3,1,32] rhs shape= [5,5,1,32]
	 [[Node: save/Assign_2 = Assign[T=DT_FLOAT, _class=[""loc:@conv2d/kernel""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](conv2d/kernel, save/RestoreV2_2)]]





### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

None found - this seems to be a new problem related to the new TF Layer API

### Environment info
Operating System: Windows 10, Anaconda3, Python 3.5.2, TensorFlow 1.0

"
7887,Anaconda's release is now 3.6 and it breaks the install,"I am trying to install tensorflow on windows 10 Enterprise Edition.
I followed the instructions to download Anaconda, but the web-site has been upgraded to Python 3.6. 
I tried to download a new whl from the nightly build but it does not work with the latest Anaconda release.  
I get:

C:\>pip install --upgrade C:\Temp\tensorflow-1.0.0rc2-cp35-cp35m-win_amd64.whl
tensorflow-1.0.0rc2-cp35-cp35m-win_amd64.whl is not a supported wheel on this platform.

Any plans to upgrade tensorflow to the latest version of Anaconda Python?

Charles

"
7886,How could I get the weights from the checkpoint file saved using tensorflow-slim?,"I know that if I knew the variable names or graph I could restore the graph or the variables in the checkpoint files.
But if I don't know the actual variable names  like w1,w2,b1,b2...in conv1,conv2,.... 
How could I get the variable values?
For example, I get the checkpoint file of alexnet in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/alexnet.py
How could I get the variable values in these checkpoint files?
"
7885,Dockerfile version error,"Version of TensorFlow was incorrectly updated in the Dockerfile
tensorflow/blob/master/tensorflow/tools/docker/Dockerfile 

` http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.0.0-cp27-none-linux_x86_64.whl`

It should probably be the 1.0.0 release not the 0.0.0 release
` http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp27-none-linux_x86_64.whl`"
7880,Peek operation for queues,"Peeking queues would be a useful addition for multi threaded TensorFlow applications. Right now there is no way to look at the first element without affecting the queue. We could empty a `tf.FIFOQueue`, look at the first element, and enqueue all elements in order. But that is not safe when other threads operate on the queue at the same time (see locking mechanisms #6360).

Example use case is an environment class in reinforcement learning, that buffers the next few time steps in a queue. The agent should be able to look at the current observation and reward at any time. Executing a step operation should remove the current time step so the next one becomes visible to the agent."
7879,Errory in summary writer tensorflow v1.0,"I have the below error when trying to run my code

**Error**
Traceback (most recent call last):
  File ""fer2013_train.py"", line 102, in <module>
    tf.app.run()
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\platform\app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""fer2013_train.py"", line 98, in main
    train()
  File ""fer2013_train.py"", line 64, in train
    summary_writer = tf.train.SummaryWriter(FLAGS.train_dir,
AttributeError: module 'tensorflow.python.training.training' has no attribute 'SummaryWriter'

**Code:**

    # Build the summary operation based on the TF collection of Summaries.
    summary_op = tf.summary.merge_all()

    # Build an initialization operation to run below.
    init = tf.global_variables_initializer()

    # Start running operations on the Graph.
    sess = tf.Session(config=tf.ConfigProto(
        log_device_placement=FLAGS.log_device_placement))
    sess.run(init)

    # Start the queue runners.
    tf.train.start_queue_runners(sess=sess)

    summary_writer = tf.train.SummaryWriter(FLAGS.train_dir,
                                            graph_def=sess.graph_def)

**tensorflow version**
>>> tf.__version__
'1.0.0'
"
7878,GAN,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7877,Native Windows libraries for Java?,"Looking at the java directory, the README only provides links for MacOS and Linux. Do we have to build from source for Windows or are those libraries provided somewhere else? Am I missing something?"
7876,Import meta graph followed by save overwrites the previous checkpoints,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System: Ubuntu 14.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): using CPU version of tensorflow 1.0.0

If installed from binary pip package, provide:

1. A link to the pip package you installed: pip install tensorflow
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. 1.0.0

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
```python
import tensorflow as tf

graph = tf.Graph()
with graph.as_default():
    initializer = tf.random_uniform_initializer(minval=-0.5, maxval=0.5, seed=42, dtype=tf.float32)
    var1 = tf.get_variable('var1', shape=(1,), dtype=tf.float32, initializer=initializer)
    saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=20)
    init_op = tf.global_variables_initializer()
    graph.finalize()

with tf.Session(graph=graph) as sess:
    sess.run(init_op)
    saver.save(sess, 'sample_graph', global_step=0)

graph = tf.Graph()
with tf.Session(graph=graph) as sess:
    saver = tf.train.import_meta_graph('sample_graph-0.meta')
    saver.restore(sess, './sample_graph-0')
    saver.save(sess, 'sample_graph', global_step=1)
    print(saver.last_checkpoints) # lists only ['sample_graph-1'] does not preserve the previous checkpoint sample_graph-0
```
Essentially I am checkpointing a graph and then importing it. On trying to save the next checkpoint, the saver overwrites the previous checkpoint in the checkpoint file (the actual meta, index and data files are not overwritten) and only the last saved checkpoint is present in the `checkpoint` file. Is this the intended behavior? Is there any way to preserve the checkpoints across multiple saves of the graph.

### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7874,strip_unused tool can't read all graph protobufs...,"Hi, 

When trying to strip unused nodes from my graph, the tool fails in cases where the protobuf encodes a continuation byte, but fails to continue. For example, running 

`./bazel-bin/tensorflow/python/tools/strip_unused --input_graph=tensorflow/contrib/some/path/mnist.pb --output_graph=tensorflow/contrib/some/path/mnist.inference.pb --input_node_names=Reshape,dropout --output_node_names=prediction_onehot`

on the attached graph causes a: 

`UnicodeDecodeError: 'utf8' codec can't decode byte 0xc5 in position 45: invalid continuation byte`

It seems decoding the file as a UTF8 string is causing issues...

[mnist.pb.zip](https://github.com/tensorflow/tensorflow/files/800990/mnist.pb.zip)

"
7873,The installing instructions for windows anaconda seems wrong,"The URL in the command line is http://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.0-cp35-cp35m-win_x86_64.whl
I can't install from the URL
however, I replaced x84_64 with amd64 and it works"
7871, building target with GPU support ,"bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer

ERROR: /home/molys/.cache/bazel/_bazel_root/f7299e5dc785b6a7f5b5291a103be54f/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):
	File ""/home/molys/.cache/bazel/_bazel_root/f7299e5dc785b6a7f5b5291a103be54f/external/local_config_cuda/crosstool/BUILD"", line 4
		error_gpu_disabled()
	File ""/home/molys/.cache/bazel/_bazel_root/f7299e5dc785b6a7f5b5291a103be54f/external/local_config_cuda/crosstool/error_gpu_disabled.bzl"", line 3, in error_gpu_disabled
		fail(""ERROR: Building with --config=c..."")
ERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.
ERROR: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/molys/.cache/bazel/_bazel_root/f7299e5dc785b6a7f5b5291a103be54f/external/local_config_cuda/crosstool/BUILD.
INFO: Elapsed time: 0.464s

Thonk you!
"
7869,Test issue,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7868,Hyper-parameter search in TensorFlow,"Hi all,

Thanks for the great work on TensorFlow.

Is anybody aware of any available TF feature for automatic hyper-parameter search? E.g., using Grid Search, or Random Search or Bayesian Optimization?

Thanks!
Hamid"
7866,"Build errors in versioned_computation_handle.{h,cc}","### Issue summary

I'm running on Ubuntu 16.04, working from Tensorflow commit 904510eeaa40b0c8f982fbb679d827688cb35b01. I ./configure'd it to use XLA, CUDA 8.0, cuDNN 5.1, and compute capability 6.1 for a Titan X (Pascal). NVIDIA drivers and CUDA are installed from the NVIDIA repos, while the cuDNN 5.1 deb files are downloaded from NVIDIA's website and installed using dpkg. I'm trying to build tensorflow via

`bazel build --config opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package`

Everything goes fine until it gets to compiling `./tensorflow/compiler/xla/service/versioned_computation_handle.h`
and the associated cc file. The error seems to be that certain ""#includes"" aren't made in this file:

```
In file included from tensorflow/compiler/xla/service/versioned_computation_handle.cc:16:0:
./tensorflow/compiler/xla/service/versioned_computation_handle.h:33:19: error: 'int64' does not name a type
   using Version = int64;
                   ^
./tensorflow/compiler/xla/service/versioned_computation_handle.h:36:3: error: 'Version' does not name a type
   Version version;
   ^
./tensorflow/compiler/xla/service/versioned_computation_handle.h:38:3: error: 'string' does not name a type
   string ToString() const;
   ^
./tensorflow/compiler/xla/service/versioned_computation_handle.h: In member function 'bool xla::VersionedComputationHandle::operator==(const xla::VersionedComputationHandle&) const':
./tensorflow/compiler/xla/service/versioned_computation_handle.h:41:13: error: 'version' was not declared in this scope
            (version == other.version);
             ^
./tensorflow/compiler/xla/service/versioned_computation_handle.h:41:30: error: 'const struct xla::VersionedComputationHandle' has no member named 'version'
            (version == other.version);
                              ^
./tensorflow/compiler/xla/service/versioned_computation_handle.h: In member function 'bool xla::VersionedComputationHandle::operator<(const xla::VersionedComputationHandle&) const':
./tensorflow/compiler/xla/service/versioned_computation_handle.h:46:15: error: 'version' was not declared in this scope
              (version < other.version)));
               ^
./tensorflow/compiler/xla/service/versioned_computation_handle.h:46:31: error: 'const struct xla::VersionedComputationHandle' has no member named 'version'
              (version < other.version)));
                               ^
tensorflow/compiler/xla/service/versioned_computation_handle.cc: At global scope:
tensorflow/compiler/xla/service/versioned_computation_handle.cc:22:1: error: 'string' does not name a type
 string VersionedComputationHandle::ToString() const {
 ^
tensorflow/compiler/xla/service/versioned_computation_handle.cc: In function 'std::ostream& xla::operator<<(std::ostream&, const xla::VersionedComputationHandle&)':
tensorflow/compiler/xla/service/versioned_computation_handle.cc:28:27: error: 'const struct xla::VersionedComputationHandle' has no member named 'ToString'
   out << versioned_handle.ToString();
                           ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```

### Related GitHub issues

Seems to be related to the following, except I'm not on MacOS:
https://github.com/tensorflow/tensorflow/issues/7819

### Environment info
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609 (from gcc --version)
Cuda compilation tools, release 8.0, V8.0.61 (from nvcc --version)

CUDA 8.0 from NVIDIA repos (Linux, x86_64, Ubuntu, 16.04)
cuDNN from nvidia downloads page (https://developer.nvidia.com/rdp/cudnn-download), runtime and developer debs for Ubuntu 14.04. Even though I'm running on 16.04 and the debs are for 14.04, this has worked for me in the past and seems unrelated to my issue. The cuDNN debs were installed with dpkg -i

CUDA libs folder

```
user@SYS-E300-8D-1:/usr/local/cuda-8.0/lib64$ ls -l libcud*
-rw-r--r-- 1 root root 556000 Jan 26 18:48 libcudadevrt.a
lrwxrwxrwx 1 root root     16 Jan 26 18:51 libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root     19 Jan 26 18:51 libcudart.so.8.0 -> libcudart.so.8.0.61
-rw-r--r-- 1 root root 415432 Jan 26 18:48 libcudart.so.8.0.61
-rw-r--r-- 1 root root 775162 Jan 26 18:48 libcudart_static.a
```


```
user@SYS-E300-8D-1:~/tensorflow$ bazel version
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261
```

### What I tried

I'm playing around with including inttypes.h in the header in question, or possibly using an old school typedef instead of that fancy C++11 ""using fancyName = literalType"" syntax. Regardless, I wanted to file the bug before I get too deep into fixing it since it takes a while to recompile, even if I just change one line and run Bazel again."
7862,"RealDiv, VariableV2 ops still in system after uninstalling tf-gpu","Hi, thanks for making tf available! I'm having an issue reverting to tf-cpu. I'm using riga/tfdeploy to distribute trained models, which is not compatible with tf-gpu. I installed tf-gpu for an experiment, then reverted to cpu, all using pip install/uninstall. Now I'm getting error messages about unknown ops like RealDiv and VariableV2. 

My question is: How can I remove any trace of tf-gpu from my system and get back to tf-cpu?

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

#7322 and #7285
and:
http://stackoverflow.com/questions/36902457/how-to-uninstall-tensorflow-completely

### Environment info
Operating System: Win 10 64 bit, Python 3.5.3

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
None

If installed from binary pip package, provide:

1. A link to the pip package you installed: 
Just used ""pip install tensorflow"" (uses tensorflow-1.0.0-cp35-cp35m-win_amd64.whl)
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
1.0.0

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

1. pip install tensorflow
2. run a tf.Session() - no ops like RealDiv etc., tfdeploy runs fine.
3. pip install tensorflow-gpu
4. pip uninstall tensorflow-gpu
5. Some trace of these ops remains registered, causing warnings like `OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits` while runninng tf, and also causing frameworks like riga/tfdeploy to crash 

### What other attempted solutions have you tried?
I have tried completely reinstalling Python 3, deleting all site-packages etc. I've installed and uninstalled CUDA. The ops seem to be registered somewhere, leading to different behavior of tf-cpu after installing and uninstalling tf-gpu.

### Logs or other output that would be helpful
The best clue is maybe errors like `unknown op: BestSplits`, which did not occur in tf-cpu before I installed and uninstalled tf-gpu.

Also, tfdeploy gives error messages like `unknown operation: RealDiv`, which did not happen prior to installing and uninstalling tf-gpu.

Any pointers on getting rid of/unregistering these ops is appreciated!

"
7861,Internal compiler error using 1.0.0-devel docker image ,"First pull the Docker image:

```
docker pull tensorflow/tensorflow:1.0.0-devel
docker run -it tensorflow/tensorflow:1.0.0-devel /bin/bash
```

Then in the docker session,

```
cd /tensorflow
bazel build -c opt //tensorflow:libtensorflow_c.so
```

This succeeds for a while, but eventually outputs

```
ERROR: /tensorflow/tensorflow/core/kernels/BUILD:1921:1: C++ compilation of rule '//tensorflow/core/kernels:svd_op' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-canonical-system-headers ... (remaining 114 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.
gcc: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-4.8/README.Bugs> for instructions.
Target //tensorflow:libtensorflow_c.so failed to build
```

This is Docker 1.13.0 on OS X 10.12.3, if that makes a difference."
7859,Error using TF in Python,"I have Python 3.5.2 and Anaconda 4.1.1 (64bit) and when i tried installing TF i got the same error earlier today.
After following uninstall and then reinstall TF commands i was able to start Python app using TF; but now i am getting OpKernel error for unknown op: UpdateFertileSlots.

![image](https://cloud.githubusercontent.com/assets/11993393/23317661/58b8c90e-fa9d-11e6-8b95-975e2075e61d.png)

Any idea how to fix this?

N.B: i did searched online and could not find any answer in SO or any other resource to fix this issue to start using TF."
7857,"""Expected bool, got 1 of type 'int' instead"" when taking gradient through 'greater'","While I didn't expect this to work, a better error message might be in order:

```python
import tensorflow as tf
x=tf.constant(1)
y=tf.constant(2)
z=tf.greater(x,y)
tf.gradients(z,x)
```

```
TypeError                                 Traceback (most recent call last)
<ipython-input-8-4df05786a146> in <module>()
----> 1 tf.gradients(z,x)

/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.pyc in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)
    377     ys = ops.convert_n_to_tensor_or_indexed_slices(ys, name=""y"")
    378     xs = ops.convert_n_to_tensor_or_indexed_slices(xs, name=""x"")
--> 379     grad_ys = _DefaultGradYs(grad_ys, ys, colocate_gradients_with_ops)
    380
    381     # The approach we take here is as follows: Create a list of all ops in the

/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.pyc in _DefaultGradYs(grad_ys, ys, colocate_gradients_with_ops)
    225         grad_ys[i] = array_ops.fill(
    226             array_ops.shape(y), constant_op.constant(
--> 227                 1, dtype=y.dtype))
    228     else:
    229       if grad_y.dtype != y.dtype:

/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc in constant(value, dtype, shape, name, verify_shape)
    163   tensor_value = attr_value_pb2.AttrValue()
    164   tensor_value.tensor.CopyFrom(
--> 165       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
    166   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
    167   const_tensor = g.create_op(

/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.pyc in make_tensor_proto(values, dtype, shape, verify_shape)
    365       nparray = np.empty(shape, dtype=np_dt)
    366     else:
--> 367       _AssertCompatible(values, dtype)
    368       nparray = np.array(values, dtype=np_dt)
    369       # check to them.

/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.pyc in _AssertCompatible(values, dtype)
    300     else:
    301       raise TypeError(""Expected %s, got %s of type '%s' instead."" %
--> 302                       (dtype.name, repr(mismatch), type(mismatch).__name__))
    303
    304

TypeError: Expected bool, got 1 of type 'int' instead.
```"
7856,[Windows] tensorboard - needs to be started from same drive as logdir,"OS: Windows 10 (64 bit)
TensorFlow Version: 1.0.0
CUDA: 8.0
GPU: yes

Problem: 
**C**:>tensorboard --logdir=**E**:\tmp\tensorflow\mnist\logs
=> tensorboard starts without loading data (not working and difficult to detect reason)

**E**:>tensorboard --logdir=**E**:\tmp\tensorflow\mnist\logs
=> tensorboard starts with loading data (works perfectly) 


"
7855,Layer for 3D convolution transpose,"I am wondering if someone could add a conv3d_transpose layer, similar to `tf.layers.conv2d_transpose`. A `tf.nn.conv3d_transpose` op already exists. Thanks!"
7854,mnist.input_data.read_data_sets() cause Segmentation Fault upon exiting,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your 
While I was trying the [MNIST tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py), I found my tensorflow output Segmentation Fault (Core Dumped). 

However, my mnist_softmax.py can still operates normally, outputing correct output on the terminal. It prints out the core dumped when the program exits.

After some try and error, I think it's this line 
`mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)` 
causing program to Core dumped upon exiting. Maybe this is a r1.0 bug?

### Environment info
Operating System: Ubuntu 14.04.5 LTS
CUDA 8.0
CuDNN 5.1.5

Tensorflow r1.0 installed from source:
1. The commit hash: 29a6b4661258ef99842904d7c54993c963a8c2c0
2. The output of `bazel version`
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261

### If possible, provide a minimal reproducible example
The same code as [MNIST tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py)

### What other attempted solutions have you tried?
I've tried out the [Getting Started tutorial](https://www.tensorflow.org/get_started/get_started#complete_program), and it worked perfectly. 
I've try to comment each line to figure out which line cause the error. The result is 
`mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)` seems to cause the problem.

### Logs or other output that would be helpful
```
$ python example.py
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 770
major: 3 minor: 0 memoryClockRate (GHz) 1.1105
pciBusID 0000:01:00.0
Total memory: 1.95GiB
Free memory: 1.66GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 770, pci bus id: 0000:01:00.0)
0.917
Segmentation fault (core dumped)
```
"
7853,TypeError: concat() got an unexpected keyword argument 'axis',"Traceback (most recent call last):
  File ""train_image_classifier.py"", line 585, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""train_image_classifier.py"", line 482, in main
    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])
  File ""/home/cloud/hoa/workspace/tensorflows/models/slim/deployment/model_deploy.py"", line 195, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""train_image_classifier.py"", line 466, in clone_fn
    logits, end_points = network_fn(images)
  File ""/home/cloud/hoa/workspace/tensorflows/models/slim/nets/nets_factory.py"", line 105, in network_fn
    return func(images, num_classes, is_training=is_training)
  File ""/home/cloud/hoa/workspace/tensorflows/models/slim/nets/inception_resnet_v2.py"", line 169, in inception_resnet_v2
    tower_conv2_2, tower_pool_1])
TypeError: concat() got an unexpected keyword argument 'axis'


When i run restnet or inception_resnet of slim model, this error was raised. Can you hep me?
Thanks."
7851,Retraining the inception model to remove objects,"Hello I have been trying for a while now to find an answer to this question
is there is anyway to retrain the inception model in a way that I remove objects from it (without doing the training from the scratch) and to be possible to integrate in the android demo"
7850,tf.variable_scope argument with some keywords makes TensorBoard work incorrectly,"**Problem:**
Step 1:
`with tf.variable_scope('length'):`

Step2:
Run Tensorboard

Step3:
Switch to Graphs tab, but no graph will show.
"
7849,Cannot build on a non-internet connected machine,"I'm trying to build tensorflow on a machine that has no internet connection.  I've downloaded the
latest bazel (.0.4.4)  and installed and then run the configure script.  I downloaded the git repo with
`git clone --recurse-submodues https://github.com/tensorflow/tensorflow`
The configure then fails with:

This then fails with:
INFO: Unknown host: bazel-mirror.storage.googleapis.com

`ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': Error downloading [http://bazel-mirror.storage.googleapis.com/github.com/bazelbuild/rules_closure/archive/5ca1dab6df9ad02050f7ba4e816407f88690cf7d.tar.gz, https://github.com/bazelbuild/rules_closure/archive/5ca1dab6df9ad02050f7ba4e816407f88690cf7d.tar.gz] to /tmp/steven/.cache/bazel/_bazel_kelso/03aa5bcd96ce8a24592d7d0f1cdaf43b/external/io_bazel_rules_closure/5ca1dab6df9ad02050f7ba4e816407f88690cf7d.tar.gz: All mirrors are down: [Unknown host: bazel-mirror.storage.googleapis.com, Unknown host: github.com].`

This is to be expected as the machine is on a private network.

However, I can manually install tarbalss of packages etc on this box, but I can't work out where they should be installed within the source tree.

Is this possible, or can tensorflow only be built with an active connection to the internet?

Many thanks,
S.

"
7848,[Feature Request] Predict posterior probability of data per each component in GMM ,"It would be very helpful  if  method is provided to predict posterior probability of data per each component in GMM.

A stackoverflow [question ](http://stackoverflow.com/questions/42357268/looking-for-a-method-to-replace-sklearn-mixture-gaussianmixture-predict-probax) has already been raised by longwoo.

"
7846,No 'T' attr on Logical op in GraphDef,"Most graph nodes have a 'T' attr in their node definition.
'LogicalAnd', 'LogicalOr', and others do not in TF 1.0. Is this because these nodes only operate on bools?
Adding a 'T' attr would help tool builders have a uniform interface to ops and would allow easy future extension to logical operations on integer types.

Example:
```python
from __future__ import print_function
import tensorflow as tf

x = tf.constant(1., name='x')
y = tf.constant(2., name='y')
ge = tf.greater(x, y, name='ge')
q = tf.logical_and(ge, ge, name='q')
z = tf.logical_not(q, name='z')

print(tf.get_default_graph().as_graph_def())
```

Output:
```
node {
  name: ""x""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 1.0
      }
    }
  }
}
node {
  name: ""y""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 2.0
      }
    }
  }
}
node {
  name: ""ge""
  op: ""Greater""
  input: ""x""
  input: ""y""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""q""
  op: ""LogicalAnd""
  input: ""ge""
  input: ""ge""
}
node {
  name: ""z""
  op: ""LogicalNot""
  input: ""q""
}
versions {
  producer: 21
}
```"
7845,Getting more done in GitHub with ZenHub,"Hola! @watermelonboss has created a [ZenHub](http://www.zenhub.com) account for the **tensorflow** organization. ZenHub is the only project management tool integrated natively in GitHub  created specifically for fast-moving, software-driven teams.

----

#### How do I use ZenHub?

To get set up with ZenHub, all you have to do is **[download the browser extension](https://www.zenhub.com?utm_source=ZHOnboarding)** and log in with your GitHub account. Once you do, youll get access to ZenHubs complete feature-set immediately.

#### What can ZenHub do?

ZenHub adds a series of enhancements directly inside the GitHub UI:

- Real-time, customizable task boards for GitHub issues;
- Multi-Repository burndown charts, estimates, and velocity tracking based on GitHub Milestones;
- Personal to-do lists and task prioritization;
- Time-saving shortcuts  like a quick repo switcher, a Move issue button, and much more.

### [Add ZenHub to GitHub](https://www.zenhub.com?utm_source=ZHOnboarding)

_Still curious? See [more ZenHub features](https://www.zenhub.com/features?utm_source=ZHOnboarding) or read [user reviews](https://chrome.google.com/webstore/detail/zenhub-for-github/ogcgkffhplmphkaahpmffcafajaocjbd/reviews). This issue was written by your friendly ZenHub bot, posted by request from @watermelonboss._

![ZenHub Board](https://cloud.githubusercontent.com/assets/8771909/11153956/233ac4a8-89f1-11e5-94b1-1569d3f38b4d.png)
"
7844,word2vec at tensorflow 1.0,"hello everyone
I have a error 
Traceback (most recent call last):
  File ""D:/software install/Pycharm/pycharm workplace/Word2vecTest/word2vec.py"", line 45, in <module>
    word2vec = tf.load_op_library(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'word2vec_ops.so'))
  File ""C:\Users\yuquanle\Anaconda3\lib\site-packages\tensorflow\python\framework\load_library.py"", line 63, in load_op_library
    raise errors_impl._make_specific_exception(None, None, error_msg, error_code)
tensorflow.python.framework.errors_impl.NotFoundError: D:\software install\Pycharm\pycharm workplace\Word2vecTest\word2vec_ops.so not found

But that file(word2vec_ops.so) I have used ubuntu to generate the document."
7843,./configure should be made script friendly,"Building tensorflow artefacts today is made difficult by the fact that the ./configure script requires interactive inputs. For default automated builds, one solution is: `echo ""\n\n\n\n\n\n\n\n\n"" | ./configure` (or `yes ''` and catching the error) but this is far from satisfactory, and tweaking the parameter is even more difficult.

See https://github.com/Homebrew/homebrew-core/pull/10273#discussion_r102895387 for instance in homebrew, but other packager will presumably hit the same issue."
7842,Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms),"I got the following error when I run `python cifar10_train.py`.

```
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_dnn.cc:390] Loaded runtime CuDNN library: 5005 (compatibility version 5000) but source was compiled with 5105 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.
F c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\kernels\conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)
```

Operating System: Windows 10
CUDA: `Cuda compilation tools, release 8.0, V8.0.44`
cuDNN: 5.1
tensorflow: 1.0.0

The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`

```
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally
1.0.0
```

I have upgrade cudnn from 5.0 to 5.1. But it didn't work.
"
7841,TensorFlow's introductory examples errors,"I am following along from this point: https://www.tensorflow.org/get_started/get_started#basic_usage
It is a linear regression example.

I have approached this execution from two angles:

Angle 1: Running python and copying one line at a time.
When run this line:  estimator = `tf.contrib.learn.LinearRegressor(feature_columns=features)`

I get this issue: `WARNING:tensorflow:Using temporary folder as model directory: C:\Users\elavi\AppData\Local\Temp\tmprirdvmnn
INFO:tensorflow:Using default config.
INFO:tensorflow:Using config: {'_tf_config': gpu_options {
  per_process_gpu_memory_fraction: 1
}
, '_task_type': None, '_save_checkpoints_steps': None, '_master': '', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000021D2049C390>, '_task_id': 0, '_save_checkpoints_secs': 600, '_save_summary_steps': 100, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_tf_random_seed': None, '_environment': 'local', '_num_ps_replicas': 0}`



Angle 2: It may be more helpful to see how I run it as a script as a whole. The output is:
`WARNING:tensorflow:Using temporary folder as model directory: C:\Users\elavi\AppData\Local\Temp\tmp41huenz5
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WARNING:tensorflow:From C:\Users\elavi\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\head.py:521: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.
2017-02-24 00:02:36.386704: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2017-02-24 00:02:36.388071: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2017-02-24 00:02:36.389235: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2017-02-24 00:02:36.389290: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-02-24 00:02:36.390010: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-02-24 00:02:36.390894: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-02-24 00:02:36.391659: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-02-24 00:02:36.392248: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WARNING:tensorflow:From C:\Users\elavi\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\head.py:521: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
`

I am interested in what is going on but I really need things simplified. A list of steps to fix this would be great. Also, if there are any better ways I could pose my problems in the future, please let me know.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Many. The issue is that I am very new. My ability to understand solutions is very small. I am not well versed with the required terminology. So I have seen this problem posed and closed but I just can't follow along.

### Environment info
Operating System: Windows 10

Installed version of CUDA and cuDNN: I have the latest version but I probably shouldn't because I do not have a dedicated graphics card. (I am doing initial development on Microsoft Surface Pro 4) 

TF Version
1.0.0-rc2"
7840,fused batch norm delay_updates confusion,"In https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/layers/python/layers/layers.py around line 347:

From its definition, `delay_updates` should delay updates to moving mean and moving average if we are training. 

The way it is defined now, when `is_training` evaluates to Trure, `smart_cond` will pick the `_delay_updates` function which actually adds the update ops to the graph. Is that intended ?"
7839, double free or corruption (!prev) and tensorboard show nothing,"


I'm new to use tensorflow.
And I want to  learn how to use tensorboard. 
So I run the demo code mnist_with_summaries.py, 
But in the end it shows  
 
*** Error in `/home/wxy/anaconda2/envs/tensorflow/bin/python': double free or corruption (!prev): 0x0000000002166320 ***

And I input tensorboard --logdir=/tmp/mnist/logs and open  http://0.0.0.0:6006/ it shows nothing.
so I follow the guide to input 

/tmp/mnist/logs | grep tfevents

then

/tmp/mnist/logs/test/events.out.tfevents.1487915616.wxy
/tmp/mnist/logs/train/events.out.tfevents.1487915615.wxy

input

tensorboard --inspect --logdir=/tmp/mnist/logs

then 

Processing event files... (this can take a few minutes)
======================================================================

Found event files in:
/tmp/mnist/logs/test
/tmp/mnist/logs/train

These tags are in /tmp/mnist/logs/test:
audio -
histograms
   layer1/Wx_plus_b/pre_activations
   layer1/activations
   layer1/biases/summaries/histogram
   layer1/weights/summaries/histogram
   layer2/Wx_plus_b/pre_activations
   layer2/activations
   layer2/biases/summaries/histogram
   layer2/weights/summaries/histogram
images
   input_reshape/input/image/0
   input_reshape/input/image/1
   input_reshape/input/image/2
   input_reshape/input/image/3
   input_reshape/input/image/4
   input_reshape/input/image/5
   input_reshape/input/image/6
   input_reshape/input/image/7
   input_reshape/input/image/8
   input_reshape/input/image/9
scalars
   accuracy_1
   cross_entropy_1
   dropout/dropout_keep_probability
   layer1/biases/summaries/max
   layer1/biases/summaries/mean
   layer1/biases/summaries/min
   layer1/biases/summaries/stddev_1
   layer1/weights/summaries/max
   layer1/weights/summaries/mean
   layer1/weights/summaries/min
   layer1/weights/summaries/stddev_1
   layer2/biases/summaries/max
   layer2/biases/summaries/mean
   layer2/biases/summaries/min
   layer2/biases/summaries/stddev_1
   layer2/weights/summaries/max
   layer2/weights/summaries/mean
   layer2/weights/summaries/min
   layer2/weights/summaries/stddev_1
======================================================================

Event statistics for /tmp/mnist/logs/test:
audio -
graph -
histograms
   first_step           0
   last_step            990
   max_step             990
   min_step             0
   num_steps            100
   outoforder_steps     []
images
   first_step           0
   last_step            990
   max_step             990
   min_step             0
   num_steps            100
   outoforder_steps     []
scalars
   first_step           0
   last_step            990
   max_step             990
   min_step             0
   num_steps            100
   outoforder_steps     []
sessionlog:checkpoint -
sessionlog:start -
sessionlog:stop -
======================================================================

These tags are in /tmp/mnist/logs/train:
audio -
histograms
   layer1/Wx_plus_b/pre_activations
   layer1/activations
   layer1/biases/summaries/histogram
   layer1/weights/summaries/histogram
   layer2/Wx_plus_b/pre_activations
   layer2/activations
   layer2/biases/summaries/histogram
   layer2/weights/summaries/histogram
images
   input_reshape/input/image/0
   input_reshape/input/image/1
   input_reshape/input/image/2
   input_reshape/input/image/3
   input_reshape/input/image/4
   input_reshape/input/image/5
   input_reshape/input/image/6
   input_reshape/input/image/7
   input_reshape/input/image/8
   input_reshape/input/image/9
scalars
   accuracy_1
   cross_entropy_1
   dropout/dropout_keep_probability
   layer1/biases/summaries/max
   layer1/biases/summaries/mean
   layer1/biases/summaries/min
   layer1/biases/summaries/stddev_1
   layer1/weights/summaries/max
   layer1/weights/summaries/mean
   layer1/weights/summaries/min
   layer1/weights/summaries/stddev_1
   layer2/biases/summaries/max
   layer2/biases/summaries/mean
   layer2/biases/summaries/min
   layer2/biases/summaries/stddev_1
   layer2/weights/summaries/max
   layer2/weights/summaries/mean
   layer2/weights/summaries/min
   layer2/weights/summaries/stddev_1
======================================================================

Event statistics for /tmp/mnist/logs/train:
audio -
graph
   first_step           0
   last_step            0
   max_step             0
   min_step             0
   num_steps            1
   outoforder_steps     []
histograms
   first_step           1
   last_step            999
   max_step             999
   min_step             1
   num_steps            900
   outoforder_steps     []
images
   first_step           1
   last_step            999
   max_step             999

Can you tell me what's the problem? 
How could I deal with it?
So I want some people to help me.
Thank you so much! "
7838,InvalidArgumentError Assign requires shapes of both tensors to match. lhs shape= [5] rhs shape= [1001],"Caused by op u'save/Assign', defined at:
  File ""train_image_classifier.py"", line 585, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""train_image_classifier.py"", line 575, in main
    init_fn=_get_init_fn(),
  File ""train_image_classifier.py"", line 370, in _get_init_fn
    ignore_missing_vars=FLAGS.ignore_missing_vars)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/variables.py"", line 579, in assign_from_checkpoint_fn
    saver = tf_saver.Saver(var_list, reshape=reshape_variables)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 986, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1015, in build
    restore_sequentially=self._restore_sequentially)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 620, in build
    restore_sequentially, reshape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 369, in _AddRestoreOps
    assign_ops.append(saveable.restore(tensors, shapes))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 212, in restore
    self.op.get_shape().is_fully_defined())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py"", line 45, in assign
    use_locking=use_locking, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 749, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2380, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1298, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [5] rhs shape= [1001]
         [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV4/AuxLogits/Aux_logits/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/
cpu:0""](InceptionV4/AuxLogits/Aux_logits/biases, save/restore_slice)]]


This error was caused when i pre-train fine-tuning inception-v4. I'm opening this issue.Can you help me?
 Thank you so much !"
7837,Hands-on TensorBoard example issue on TF 1.0.0 Win GPU,"### Environment info
Operating System: Windows 10.0.14393, latest patches

Installed version of CUDA and cuDNN: 
nvcc --version says 8.0.44; CUDA is 8.0, cuDNN is 5.1
05.09.2016  15:51            87.834 cuda.lib
05.09.2016  15:51           681.064 cudadevrt.lib
05.09.2016  15:51            64.550 cudart.lib
05.09.2016  15:51         2.318.456 cudart_static.lib
27.07.2016  09:35            37.452 cudnn.lib

Anaconda 4.2.0
pip install --ignore-installed --upgrade tensorflow
tensorflow.__version__ 1.0.0

I ran the 
[example from Dandelion Mane's Hands-on TensorBoard talk at TF Dev Summit 2017](https://gist.github.com/dandelionmane/4f02ab8f1451e276fea1f165a20336f1#file-mnist-py)

### What other attempted solutions have you tried?
It works with TF 1.0.0 on Ubuntu 16.04, both CPU and GPU versions.

### Logs or other output that would be helpful
On TF Windows GPU (versions as noted above):
```
(tensorflow) C:\dl\tf\mnist_tutorial>python mnist_tutorial.py
```
```
Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
Extracting log/data\train-images-idx3-ubyte.gz
...
Extracting log/data\t10k-labels-idx1-ubyte.gz
Starting run for lr_1E-04,conv=2,fc=2
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
...
```

Thanks
G."
7836,Breaking changes to API,"`tf.nn.sampled_softmax_loss` and `tf.nn.nce_loss` have both changed their API such that you need to switch the `inputs, labels` to `labels, inputs` parameters. You might want to put a note here (under breaking changes): https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md

Incase you want to check here is the API of the two versions of tf:
https://www.tensorflow.org/versions/r0.12/api_docs/python/
https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss

Although I do find it odd that tf would simply change the order of these inputs."
7835,TypeError: zeros_initializer() takes at least 1 argument,"Tensorflow 0.12.1 (4d924e7)
I cloned the up-to-dated tensorflow-models today. 

I got the following error msg:
Traceback (most recent call last):
  File ""/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/flowers_train.py"", line 41, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/flowers_train.py"", line 37, in main
    inception_train.train(dataset)
  File ""/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/inception_train.py"", line 241, in train
    scope, reuse_variables)
  File ""/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/inception_train.py"", line 109, in _tower_loss
    scope=scope)
  File ""/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/inception_model.py"", line 87, in inference
    scope=scope)
  File ""/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/slim/inception_model.py"", line 87, in inception_v3
    scope='conv0')
  File ""/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/slim/scopes.py"", line 155, in func_with_args
    return func(*args, **current_args)
  File ""/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/slim/ops.py"", line 234, in conv2d
    outputs = batch_norm(conv, **batch_norm_params)
  File ""/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/slim/scopes.py"", line 155, in func_with_args
    return func(*args, **current_args)
  File ""/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/slim/ops.py"", line 88, in batch_norm
    initializer=tf.zeros_initializer(),
TypeError: zeros_initializer() takes at least 1 argument (0 given)

I read through previous issues on zeors_initializer() & ones_initializer(). And I replaced zeros_initializer() w/ constant_initializer(0.0), but got the new error:
Traceback (most recent call last):
  File ""/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/flowers_train.py"", line 41, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/flowers_train.py"", line 37, in main
    inception_train.train(dataset)
  File ""/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/inception_train.py"", line 241, in train
    scope, reuse_variables)
  File ""/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/inception_train.py"", line 109, in _tower_loss
    scope=scope)
  File ""/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/inception_model.py"", line 87, in inference
    scope=scope)
  File ""/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/slim/inception_model.py"", line 125, in inception_v3
    net = tf.concat([branch1x1, branch5x5, branch3x3dbl, branch_pool], 3)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 1075, in concat
    dtype=dtypes.int32).get_shape(
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 669, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 176, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 165, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 367, in make_tensor_proto
    _AssertCompatible(values, dtype)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 302, in _AssertCompatible
    (dtype.name, repr(mismatch), type(mismatch).__name__))
TypeError: Expected int32, got list containing Tensors of type '_Message' instead.

Anybody can help? Thanks a lot!"
7830,Error while trying out mnist_softmax,"Hello everyone,

I am using python3.4.2 and tensorflow-1.0.0

I am getting started with tensorflow and so was trying out MNIST For ML Beginners (https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist_softmax.py)

I am getting the following output:

$ python3 test2.py --data_dir=./MNIST_data
Extracting ./MNIST_data/train-images-idx3-ubyte.gz
Extracting ./MNIST_data/train-labels-idx1-ubyte.gz
Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz
Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
0.9187
*** Error in `python3': free(): invalid next size (normal): 0x00000000025985f0 ***
[1]    25610 abort      python3 test2.py --data_dir=./MNIST_data

If someone could help me fix the Error in 'python3': free() problem.

Thank You"
7826,bazel error,"Hello everyone,
![bazelerror](https://cloud.githubusercontent.com/assets/20130992/23277161/ec375632-f9da-11e6-8117-d834540c3d4e.png)

As I am having an error ""AttributeError: 'module' object has no attribute 'GRUCell'"" when tried to run transate.py i removed tensorflow from my system and started installing it again from sources. I installed bazel and configured the installation. After ""Build the pip package"", I am getting the following error.

System specifications:
I installed Ubuntu 16.04 on vmware with 2 GB RAM and 100 GB disk space 

Can anyone please help me in resolving these errors

Thanks in advance "
7825,tf version not working on older linux version,"Hi, may I ask if `tf` is built on `ubuntu`? The many linux wheel from pip or even anaconda causes errors on systems such as `CentOS 6.6`. Particularly it is looking for a `GLIBC-2.16.so` but can't find it. On my system there exists only `GLIBC-2.14.so`. Does anyone have any solution for this?
"
7824,Primary install page returning 404 https://www.tensorflow.org/get_started/os_setup ,"https://www.tensorflow.org/get_started/os_setup 

404. That's an error.

The requested URL was not found on this server.
That's all we know."
7823,Gradient Boosting Decision Trees at TensorFlow,"At TensorFlow Dev Summit 2017 Mr. Ashish Agarwal gave a presentation for ML Toolkit and said that Gradient Boosting Decision Trees [are coming soon](https://youtu.be/Tuv5QYKU-MM?t=271).  I would like to ask two questions:

 * Is there an approximate timeline for when this feature might be released? 
 * Is there any information if we will be able to leverage tensorflow's local (GPU computing) and distributed capabilities (for example as [presented by Mr. Jonathan Hseu](https://www.youtube.com/watch?v=yALzr4A2AzY)) for this particular algorithm?
"
7822,Returning argmax with tf.nn.pool,"Feature request for returning argmax for N-D pooling with `tf.nn.pool`, as in `tf.nn.max_pool_with_argmax`."
7821,Cannot upgrade to CUDNN 5.1 (How to compile using CUDNN 5 and CUDA 7.5),"I am using a University Cluster with Tesla K80 with CUDA 7.5 and CUDnn 5.0.5. I don't have root access and cannot upgrade either CUDA or CUDnn. I have tried compiling TF 1.0 from sources using bazel by configuring CUDnn to 5.0.5. I am still experiencing this error while running deep mnist.

2017-02-23 12:29:43.883764: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Loaded runtime CuDNN library: 5110 (compatibility version 5100) but source was compiled with 5005 (compatibility version 5000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.
2017-02-23 12:29:43.883914: F tensorflow/core/kernels/conv_ops.cc:654] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)
Aborted

However, the mnist with simple linear model worked. Please let me know a way around this and also if you need anything else.

Thanks"
7819,XLA build error on macOS 10.12.4 Beta,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Nothing seems to be related

### Environment info
Operating System:

macOS 10.12.4 Beta

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

None

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`): 4ac9c09d5ca57a03b8daa5fb9e295947b1619854
2. The output of `bazel version`: 
```
Build label: 0.4.4-homebrew
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Feb 2 01:05:15 2017 (1485997515)
Build timestamp: 1485997515
Build timestamp as int: 1485997515
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
Enable XLA at configuration and build with `bazel build --config=opt`

### What other attempted solutions have you tried?
Building without XLA is working smoothly.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
```bash
ERROR: /Users/inflation/workspace/tensorflow/tensorflow/compiler/xla/service/BUILD:108:1: C++ compilation of rule '//tensorflow/compiler/xla/service:versioned_computation_handle' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 93 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
In file included from tensorflow/compiler/xla/service/versioned_computation_handle.cc:16:
./tensorflow/compiler/xla/service/versioned_computation_handle.h:33:19: error: unknown type name 'int64'; did you mean 'tensorflow::int64'?
  using Version = int64;
                  ^~~~~
                  tensorflow::int64
./tensorflow/core/platform/default/integral_types.h:27:19: note: 'tensorflow::int64' declared here
typedef long long int64;
                  ^
In file included from tensorflow/compiler/xla/service/versioned_computation_handle.cc:16:
./tensorflow/compiler/xla/service/versioned_computation_handle.h:38:3: error: unknown type name 'string'; did you mean 'std::string'?
  string ToString() const;
  ^~~~~~
  std::string
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/iosfwd:194:65: note: 'std::string' declared here
typedef basic_string<char, char_traits<char>, allocator<char> > string;
                                                                ^
tensorflow/compiler/xla/service/versioned_computation_handle.cc:22:1: error: unknown type name 'string'; did you mean 'std::string'?
string VersionedComputationHandle::ToString() const {
^~~~~~
std::string
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/iosfwd:194:65: note: 'std::string' declared here
typedef basic_string<char, char_traits<char>, allocator<char> > string;
```
"
7817,Reading data with queue is even slower than using feed_dict,"It seems reading data with feed_dict is inefficient in tensorflow (I found it 3-4 times slower than a theano-based implementation with a totally same network structure), I turn to a queue-based method as official recommendation. However, my experiment result give a even worse performance. Is there anything wrong ?

### Related issue
[#3377 Moving data from CPU to GPU is slow](https://github.com/tensorflow/tensorflow/issues/3377)

### Environment info
Operating System: Ubuntu 16.04
CPU: Intel i4790-k
GPU: Nvidia GTX 1070 (8 GB)
Memory: 16 GB
Tensorflow version: 1.0
CUDA version: 8.0
cuDNN version: 5.0

### Implementation Example
1) Using feed_dict
```python
x, y = np.array(...)
in_x, in_y = tf.placeholder(...)
"""""" f() is some computation in the network, including embedding_lookup, 
bidirectional_dynamic_rnn, dense """"""
train_op = f(in_x, in_y) 
sess = tf.Session()
for _ in range(num_epochs): # num_epochs is 100 here
    sess.run(train_op, {in_x: x, in_y: y})
```
2) Using queue
```python
x, y = np.array(...)
x, y = tf.convert_to_tensor(...)
in_x, in_y = tf.train.slice_input_producer([x, y], num_epochs=100)
in_x, in_y = tf.train.batch([in_x, in_y], batch_size=32)
train_op = f(in_x, in_y)
sess = tf.Session()
coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess, coord)
try:
    while not coord.should_stop():
        sess.run(train_op)
    except Exception as e:
        coord.request_stop(e)
    finally:
        coord.request_stop()
coord.join(threads)
```"
7816,tensorflow debugger assertion fails,"When using tensorflow debugger, the program crashes with the following message : 

```
F tensorflow/core/framework/tensor_util.cc:37] Check failed: DT_STRING == other.dtype() (7 vs. 20)
Aborted (core dumped)

```

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Nothing

### Environment info
Operating System: 

Linux ip-172-31-46-19 4.4.0-57-generic #78-Ubuntu SMP Fri Dec 9 23:50:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

Installed version of CUDA and cuDNN: 8.0 / 5.1
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```

-rw-r--r-- 1 root root   558720 sept. 14 23:02 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudadevrt.a
lrwxrwxrwx 1 root root       16 sept. 14 23:05 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 sept. 14 23:05 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0 -> libcudart.so.8.0.44
-rw-r--r-- 1 root root   415432 sept. 14 23:02 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 sept. 14 23:02 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
lrwxrwxrwx 1 root root       13 dc.   7 11:05 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 dc.   7 11:05 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxr-xr-x 1 root root 79337624 dc.   7 11:05 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 dc.   7 11:05 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a
```

If installed from binary pip package, provide:

1. pip package you installed:

```
$  pip show tensorflow-gpu
Name: tensorflow-gpu
Version: 1.0.0
Summary: TensorFlow helps the tensors flow
Home-page: http://tensorflow.org/
Author: Google Inc.
Author-email: opensource@google.com
License: Apache 2.0
Location: /home/ubuntu/anaconda3/envs/dl/lib/python3.5/site-packages
Requires: wheel, protobuf, numpy, six
```

2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
```

python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.0
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Currently cannot find what operation makes the debugger crash

### What other attempted solutions have you tried?

Nothing

### Logs or other output that would be helpful
```

(gdb) bt
#0  0x00007ffff6a1e428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54
#1  0x00007ffff6a2002a in __GI_abort () at abort.c:89
#2  0x00007fffdf6ce0a4 in tensorflow::internal::LogMessageFatal::~LogMessageFatal() () from /home/ubuntu/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
#3  0x00007fffdf61747f in tensorflow::tensor::DeepCopy(tensorflow::Tensor const&) () from /home/ubuntu/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
#4  0x00007fffdda07082 in tensorflow::CopyOp::Compute(tensorflow::OpKernelContext*) () from /home/ubuntu/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
#5  0x00007fffdf362183 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()
   from /home/ubuntu/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
#6  0x00007fffdf351e20 in std::_Function_handler<void (), std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> (tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> >::_M_invoke(std::_Any_data const&) ()
   from /home/ubuntu/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
#7  0x00007fffdf6a6960 in std::_Function_handler<void (), Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int, tensorflow::thread::EigenEnvironment)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /home/ubuntu/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
#8  0x00007fffdf6a5c10 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /home/ubuntu/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
#9  0x00007fffdc573c80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#10 0x00007ffff76d16ba in start_thread (arg=0x7fff83fff700) at pthread_create.c:333
#11 0x00007ffff6aef82d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
```"
7815,tf r1.0 strided_slice documentation,"In the [docs](https://www.tensorflow.org/api_docs/python/tf/strided_slice), the following sentences seem incomplete/incoherent:

- If the ith bit of ellipsis_mask `[is-set-to-something]`, as many unspecified dimensions as needed will be inserted between other dimensions. Only one non-zero bit is allowed in ellipsis_mask.

- If the ith bit of new_axis_mask is one, then `a?` begin, end, and stride are ignored and a new length 1 dimension is added at this point in the output tensor"
7814,"tensorflow 1.0 error: ValueError: Only call `sigmoid_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)","### Environment info
Operating System:
Ubuntu 14.0.4
tensorflow 1.0
cuda 8.0 + cudnn 5.1

### Project
DCGAN: the code is downloaded here
https://github.com/carpedm20/DCGAN-tensorflow

### Logs or other output that would be helpful
![image](https://cloud.githubusercontent.com/assets/25976861/23252160/bd6c2570-f9ea-11e6-92fd-523a2acc8461.png)
### Codes that may have problems
![image](https://cloud.githubusercontent.com/assets/25976861/23252201/e972a39c-f9ea-11e6-8938-a0d4dcc29c9f.png)

I`m new in tf and I don`t know how to solve the problem.
I`ll appreciate any suggestions. Thank you!
"
7813,many functions not found in libtensorflow-core.a,"I am trying to compile Image Recognition with makefile (which is compiled with bazel originally). However, it raise error that:

image_ops.h: No such file..

Following the instructions in [#3017](https://github.com/tensorflow/tensorflow/issues/3017), I add **tensorflow/bazel-genfiles** to my header search path.

It does work, but another error comes out as fllows:

11 warnings generated.
Undefined symbols for architecture x86_64:
  ""tensorflow::ops::DecodeJpeg::DecodeJpeg(tensorflow::Scope const&, tensorflow::Input, tensorflow::ops::DecodeJpeg::Attrs const&)"", referenced from:
      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o
  ""tensorflow::ops::ExpandDims::ExpandDims(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)"", referenced from:
      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o
  ""tensorflow::ops::ResizeBilinear::ResizeBilinear(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)"", referenced from:
      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o
  ""tensorflow::ops::Div::Div(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)"", referenced from:
      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o
  ""tensorflow::ops::Sub::Sub(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)"", referenced from:
      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o
  ""tensorflow::ops::Cast::Cast(tensorflow::Scope const&, tensorflow::Input, tensorflow::DataType)"", referenced from:
      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o
  ""tensorflow::ops::Const(tensorflow::Scope const&, tensorflow::Input::Initializer const&)"", referenced from:
      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o
  ""tensorflow::ops::TopKV2::TopKV2(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)"", referenced from:
      GetTopLabels(std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> > const&, int, tensorflow::Tensor*, tensorflow::Tensor*) in main-b19045.o
  ""tensorflow::ops::ReadFile::ReadFile(tensorflow::Scope const&, tensorflow::Input)"", referenced from:
      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o
  ""tensorflow::ops::DecodeGif::DecodeGif(tensorflow::Scope const&, tensorflow::Input)"", referenced from:
      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o
  ""tensorflow::ops::DecodePng::DecodePng(tensorflow::Scope const&, tensorflow::Input, tensorflow::ops::DecodePng::Attrs const&)"", referenced from:
      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o
  ""tensorflow::Input::Initializer::Initializer(std::initializer_list<tensorflow::Input::Initializer> const&)"", referenced from:
      tensorflow::Input::Input(std::initializer_list<tensorflow::Input::Initializer> const&) in main-b19045.o
  ""tensorflow::Scope::NewRootScope()"", referenced from:
      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o
      GetTopLabels(std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> > const&, int, tensorflow::Tensor*, tensorflow::Tensor*) in main-b19045.o
  ""tensorflow::Operation::Operation(tensorflow::Node*)"", referenced from:
      tensorflow::Output::Output() in main-b19045.o
      tensorflow::Input::Input(std::initializer_list<tensorflow::Input::Initializer> const&) in main-b19045.o
      tensorflow::Input::Input(tensorflow::Tensor const&) in main-b19045.o
      tensorflow::Input::Input(tensorflow::Input::Initializer const&) in main-b19045.o
  ""tensorflow::Scope::ToGraphDef(tensorflow::GraphDef*) const"", referenced from:
      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o
      GetTopLabels(std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> > const&, int, tensorflow::Tensor*, tensorflow::Tensor*) in main-b19045.o
  ""tensorflow::Scope::WithOpName(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const"", referenced from:
      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o
      GetTopLabels(std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> > const&, int, tensorflow::Tensor*, tensorflow::Tensor*) in main-b19045.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

I think these ops are not included in libtensor-core.a. 
My question is: how to link these ops using makefile? "
7812,Build issues,"Building from source shows a lot of warnings,mostly due to either:

- Comparison between signed and unsigned integers
- Initialized variables
- Code reaching ""non-return"" path for functions that return a non-void.

I have submitted a patch that fixes a bunch of these (https://github.com/tensorflow/tensorflow/pull/7752)

`Operating System`:
Linux (ubuntu 14.04) 

`Commit hash` : 4ac9c09d5ca57a03b8daa5fb9e295947b1619854

`gcc version`: 4.8.4

`bazel version`:

bazel version
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261
"
7811,tensorflow1.0   ArgumentError Running CIFAR-10 models,"I am trying to run the CIFAR-10 example of Tensorflow-r1.0. However when executing python cifar10_multi_gpu_train.py I am getting the error attached below.
~/models/tutorials/image/cifar10# python cifar10_multi_gpu_train.py 
Traceback (most recent call last):
  File ""cifar10_multi_gpu_train.py"", line 271, in <module>
    tf.app.run()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""cifar10_multi_gpu_train.py"", line 263, in main
    cifar10.maybe_download_and_extract()
  File ""/home/molys/workspace/tensorflow-r1.0/models/tutorials/image/cifar10/cifar10.py"", line 398, in maybe_download_and_extract
    tarfile.open(filepath, 'r:gz').extractall(dest_directory)
  File ""/usr/lib/python2.7/tarfile.py"", line 2081, in extractall
    self.extract(tarinfo, path)
  File ""/usr/lib/python2.7/tarfile.py"", line 2118, in extract
    self._extract_member(tarinfo, os.path.join(path, tarinfo.name))
  File ""/usr/lib/python2.7/tarfile.py"", line 2194, in _extract_member
    self.makefile(tarinfo, targetpath)
  File ""/usr/lib/python2.7/tarfile.py"", line 2235, in makefile
    copyfileobj(source, target)
  File ""/usr/lib/python2.7/tarfile.py"", line 266, in copyfileobj
    shutil.copyfileobj(src, dst)
  File ""/usr/lib/python2.7/shutil.py"", line 49, in copyfileobj
    buf = fsrc.read(length)
  File ""/usr/lib/python2.7/tarfile.py"", line 831, in read
    buf += self.fileobj.read(size - len(buf))
  File ""/usr/lib/python2.7/tarfile.py"", line 743, in read
    return self.readnormal(size)
  File ""/usr/lib/python2.7/tarfile.py"", line 758, in readnormal
    return self.__read(size)
  File ""/usr/lib/python2.7/tarfile.py"", line 748, in __read
    buf = self.fileobj.read(size)
  File ""/usr/lib/python2.7/gzip.py"", line 268, in read
    self._read(readsize)
  File ""/usr/lib/python2.7/gzip.py"", line 315, in _read
    self._read_eof()
  File ""/usr/lib/python2.7/gzip.py"", line 354, in _read_eof
    hex(self.crc)))
IOError: CRC check failed 0x3c81b31b != 0x6d214c80L

Tank you !
"
7810,How to disable peer to peer gpu memory accessing?,"Our system with 2GPUs always reboot if run training with 2GPUs, but 1 GPU is OK. And the cuda testing case of 'simpleP2P' also make the system reboot.

How could I disable the peer to peer feature?"
7809,Getting the following the error while importing,"python bin/run_analysis.py
Traceback (most recent call last):
  File ""bin/run_analysis.py"", line 7, in <module>
    from src.run_analysis import analyze, render_results_as_images
  File ""/DeepOSM/src/run_analysis.py"", line 6, in <module>
    import label_chunks_cnn_cifar
  File ""/DeepOSM/src/label_chunks_cnn_cifar.py"", line 11, in <module>
    import tflearn
  File ""/usr/local/lib/python2.7/dist-packages/tflearn/__init__.py"", line 21, in <module>
    from .layers import normalization
  File ""/usr/local/lib/python2.7/dist-packages/tflearn/layers/__init__.py"", line 10, in <module>
    from .recurrent import lstm, gru, simple_rnn, bidirectional_rnn, \
  File ""/usr/local/lib/python2.7/dist-packages/tflearn/layers/recurrent.py"", line 8, in <module>
    from tensorflow.contrib.rnn.python.ops.core_rnn import static_rnn as _rnn, \
ImportError: No module named core_rnn

while running the github project [https://github.com/zilongzhong/DeepOSM](url)
In this one I am able to create training data successfully but not able to run_analysis.py and while runnin I'm getting this error and "
7808,TensorFlow builds LLVM even if ./configured without XLA,"At HEAD (4ac9c09d5ca57a03b8daa5fb9e295947b1619854), if I configure TensorFlow without XLA support and run

    blaze build //tensorflow/...

it still builds both XLA and LLVM (or at least parts, since I haven't waited for it to finish).  Needless to say, it also downloads LLVM, which ideally would not be necessary without XLA support.

Is it possible to skip building a compiler if I don't want XLA?"
7807,How to extract the native library,I am new to the tensorflow for java. I want to ask how to extract the native library? Is there some guidance?
7805,Is it a error?,"### Environment info
Operating System: CentOS 7 
tensorflow version: 0.12.1

### Installed version of CUDA and cuDNN: 

/usr/local/cuda-8.0/lib64/libcudadevrt.a
/usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
/usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5
/usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.3
/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.3
/usr/local/cuda-8.0/lib64/libcudnn_static.a

### Question:
When I run a program. It print the below info. (Then the machine shutdown.)
![_20170223100205](https://cloud.githubusercontent.com/assets/4702353/23242344/321555be-f9b3-11e6-8115-63fc129f3922.png)

I remember when it run successfully on two GPU , it print:
```
Y N
N Y
```
not
```
Y Y
Y Y
```"
7804,how copy a tensor to another tensor??,how copy a tensor to another tensor??
7800,Feature Request: access gradients for a specific layer,"Currently tensorflow will provide the gradients for the entire network, i.e. getting grads_and_vars will return all of the gradients but there isn't any nice way to specifically get the gradients of a specific layer.

An example use case:  For the paper Neural Networks are easily fooled, they took the gradients from the last layer and added them to the original image.

There is no current way to do this in tensorflow as far as I know.
"
7796,Problems in Defining New Op ,"I'm trying to define a new Op (a simple matmul) by myself. I defined the both CPU and GPU Opkernel and ran REGISTER_KERNEL_BUILDER with DEVICE_CPU and DEVICE_GPU respectively to register them. When I was testing it on my server installed with GPU, and used
```
with tf.Graph().as_default():
    with tf.device('/gpu:0'):
    # some code for graph
    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))
    sess.run(tf.initialize_all_variables())
    print sess.run(out)
```
to define variable and Ops, I found only the code of CPU Opkernel is used.

Weird thing is that if I delete the CPU Opkernel code and CPU Opkernel registration, keep only GPU Opkernel and register only the GPU Opkernel, GPU Opkernel code will be used now and everything works and with the following output:

```
E tensorflow/core/common_runtime/executor.cc:390] Executor failed to create kernel. Not found: No registered 'MyMatMul' OpKernel for CPU devices compatible with node MyMatMul = MyMatMul[_device=""/job:localhost/replica:0/task:0/gpu:0""](Const, Const_1)
        .  Registered:  device='GPU'

         [[Node: MyMatMul = MyMatMul[_device=""/job:localhost/replica:0/task:0/gpu:0""](Const, Const_1)]]
```

Anyone has idea for it?"
7795,"Typo in ""Validate Your Installation"" document.","https://www.tensorflow.org/install/install_linux#ValidateYourInstallation

Partway down:

> If you are new to TensorFlow, see [Getting Started with TensorFlow]Getting Started With TensorFlow.

Note the extraneous square brackets; looks like a markdown typo. Interestingly, the ""Getting Started With"" link still works (which did not survive my copy-paste into the issue tracker)."
7794,Cannot specify XLA config for learn.estimators ?,"Seems that the MonitoredSession is being allocated directly:

https://github.com/tensorflow/tensorflow/blob/f821ce046df71f5784ed4ce7fb6b87f77d96b031/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L962

and there is no way to pass or specify different `graph_options` to the session to enable the XLA jit configuration. Is that the case or I am missing something? "
7793,pip3 No matching distribution found on OSX 10.9.5 py3.5,"I am trying to install tensorflow in a virtualenv environment on OSX. 

I think I have a good Python version (3.5). This is what it says:
```
$ python
Python 3.5.2 (default, Oct  1 2016, 01:07:00) 
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin
```

that happens when I give the pip install command:

```
$ pip3 install -vvv --upgrade tensorflow
Collecting tensorflow
  1 location(s) to search for versions of tensorflow:
  * https://pypi.python.org/simple/tensorflow/
  Getting page https://pypi.python.org/simple/tensorflow/
  Looking up ""https://pypi.python.org/simple/tensorflow/"" in the cache
  Current age based on date: 471
  Freshness lifetime from max-age: 600
  Freshness lifetime from request max-age: 600
  The response is ""fresh"", returning cached response
  600 > 471
  Analyzing links from page https://pypi.python.org/simple/tensorflow/
    Skipping link https://pypi.python.org/packages/00/16/c8ba385fc6511ca362f32326cd1d6a99bbbabbc8341607ff70c290e0be7b/tensorflow-0.12.1-cp34-cp34m-manylinux1_x86_64.whl#md5=981c0a406eb9865423b11c03b489040d (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/01/c5/adefd2d5c83e6d8b4a8efa5dd00e44dc05de317b744fb58aef6d8366ce2b/tensorflow-0.12.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=ebcd1b32ccf2279bfa688542cbdad5fb (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/03/51/b68d9d6481d16fd709290030ae8a8a13212587bc87fae718e521bdafa723/tensorflow-0.12.0rc1-cp27-cp27mu-manylinux1_x86_64.whl#md5=9329984aa4f6388f5ec2a170e4ae968e (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/0c/7c/29ac00dd86a419a6ecba16d2c319598adb92b261e7984666305f411bbd7d/tensorflow-1.0.0-cp35-cp35m-macosx_10_11_x86_64.whl#md5=51b1aba26e837825a4f1eafd2eb81fe7 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/0d/d7/b49a6ceb055f392f91bce25eb6e1665f9b2f0a4628f7acdbccf1cd1d0ee6/tensorflow-1.0.0-cp27-cp27m-macosx_10_11_x86_64.whl#md5=51235fbe34e705bcdb1b5bbd6a482c57 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/11/c9/2ec86336a8a401a57fabed5b10ee7f0493fc334b33e9672766b1149dd923/tensorflow-0.12.0-cp35-cp35m-win_amd64.whl#md5=d515d2ae08ae25fc8bf81e3374448146 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/16/6e/fa1f09a32d3ba5bf6a2b79c6ec16a43c91e3a79e0035ab3eb52ecad22e0d/tensorflow-0.12.1-cp35-cp35m-macosx_10_11_x86_64.whl#md5=faf5d055cdaecd75e5f0b37a04676102 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/25/23/24681ce011cd33ea5b22b8efd0f28a3f1294085c6ca6f70cc3abd62bf75d/tensorflow-0.12.0rc1-cp35-cp35m-macosx_10_11_x86_64.whl#md5=6c964e9575ed88892b94195a05aa8e65 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/25/c4/162ea5fa9e012f5a5e125105f6b290e56f9fd617a7aadd57d9a26bb386ca/tensorflow-0.12.0rc1-cp34-cp34m-manylinux1_x86_64.whl#md5=4e77b31ae9fc0e36489875ce0f72b267 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/33/4b/20e517870effa573405d30dafc22f330f24c0a8928659b4ad5a44b9a9af2/tensorflow-0.12.0rc0-cp35-cp35m-macosx_10_11_x86_64.whl#md5=8d1376a68f768efa57b1344fc2df0472 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/33/ab/3a62133d6c9f6281959f3ca96ad2a796fb4fed8d642c4f33d9fd97d8bf6f/tensorflow-0.12.0rc1-cp27-cp27m-macosx_10_11_x86_64.whl#md5=491802c10e992905d7e80108b9b16920 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/34/53/0e10581ad88bca25e5005874e46130b12efbd5eb1bda493dbf9b1648cbe2/tensorflow-0.12.1-cp27-cp27m-macosx_10_11_x86_64.whl#md5=d3f44a0596a623458c42c2a694abb0af (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/3f/8a/e80e2fef57bb1ffcff65c597adcddd5a0de17d7a7da5a6187503a28e9d66/tensorflow-1.0.0-cp33-cp33m-manylinux1_x86_64.whl#md5=ed46aa42e9716649572e0f861210b8f5 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/48/58/b71480f9ec9d08d581d672a81b15ab5fec36a5fcda2093558a23614d8468/tensorflow-1.0.0-cp36-cp36m-manylinux1_x86_64.whl#md5=2742cfc22068004af8a1f31071d96546 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/4a/c3/1ad85e5c4fde90b2e9a5101283d97dd41ba6c24f44a1c3a8495ef7098bb5/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl#md5=dad56a38acf7ca501b1bfeeef53f3710 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/64/9c/72aff7713c507f7e6c15df011e0ed18ac85e5bfa16c3763d8cba44585d79/tensorflow-0.12.0rc0-cp34-cp34m-manylinux1_x86_64.whl#md5=93431c5cd8a019d08a72e6c0a75eaaf4 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/64/a3/0054a3329579de44d557f491adbcaf8127809a7992bc46af80f0a589e29b/tensorflow-0.12.1-cp35-cp35m-win_amd64.whl#md5=d657836c76a5cd3c6d5560034bd7ade6 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/65/a3/40bbfb783d955c1a959c8fa939f63b00d4fac9d0c3777e0607d39e31ba0a/tensorflow-1.0.0-cp36-cp36m-macosx_10_11_x86_64.whl#md5=5926a0360b2979cb969e8c38a18f9c0a (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/66/47/d6bb91a11684733ad565b891d561097ef10f8cb7bae87e5ad692207024e3/tensorflow-0.12.0rc1-cp35-cp35m-manylinux1_x86_64.whl#md5=2351fe799a0869487699537ab0e33019 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/67/06/15153c48b2281bc59f8a70f2ae681723ece29ebc0015883117fb28abaf68/tensorflow-0.12.0rc0-cp35-cp35m-manylinux1_x86_64.whl#md5=ebc1ee880633a60e5256e745a27d4058 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/7b/c5/a97ed48fcc878e36bb05a3ea700c077360853c0994473a8f6b0ab4c2ddd2/tensorflow-1.0.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=a7483a4da4d70cc628e9e207238f77c0 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/7e/c6/837f4e249aae5c86a632eaaa9779e601eca1487772f8ad75c347bf9e813f/tensorflow-0.12.1-cp27-cp27mu-manylinux1_x86_64.whl#md5=c98fd26b79a97cc490c942bbafed5462 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/7e/ea/e42e47ddb39d2043a06dc93b5aec042bae8571be52ea664aa03246a83c32/tensorflow-0.12.0-cp35-cp35m-macosx_10_11_x86_64.whl#md5=f8b165ae638eb169220ccbf02658475b (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/90/cf/1d1e12f9f39b6a0ed1c49792ef5ce7615dddc2ce7287fc83ede0dddb9b3c/tensorflow-0.12.0rc0-cp27-cp27m-macosx_10_11_x86_64.whl#md5=7be7b7488a6be83546758a40d5442901 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/91/7c/98b25b74241194d4312a7e230c85a77b254224191dbf17b484811f8a9f61/tensorflow-0.12.1-cp36-cp36m-macosx_10_11_x86_64.whl#md5=3b2f5f08fe61471ea973a4bdbdfbae43 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/96/d6/096f6624a8a48d41801eb23863d35a37532baf32f6d3aa2e03c96f66e6ab/tensorflow-0.12.0-cp34-cp34m-manylinux1_x86_64.whl#md5=a6a0af9cf2a5fc33053f638b05cb3940 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/9e/7c/f50ca985e43cff46a6d92abb088e8328b9be13335dc44af291ac1ed11862/tensorflow-0.12.0-cp35-cp35m-manylinux1_x86_64.whl#md5=2c07109df1a9865a7a44b6fe08756e8d (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/a7/86/cb47c1213779939583091fc97e0950c323d655186824cdbc0f657f42930c/tensorflow-0.12.0-cp27-cp27m-macosx_10_11_x86_64.whl#md5=fc429a5b5b74517e08216ee8b1d04e59 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/b0/a6/a8b857c8b383c4a2a3dece8a3c0f86e9273b8fd7ce31528f8ac809fc5910/tensorflow-1.0.0-cp35-cp35m-manylinux1_x86_64.whl#md5=88f8b298a2efb3f2d236019c5dcc1220 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/b2/da/9132af1e2ef3771b63619072a6d82800bd60acbf2c8bea8e4f26514c768a/tensorflow-0.12.0rc1-cp35-cp35m-win_amd64.whl#md5=69b6c2e9440fd2d7a2d86249933fcde0 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/c0/9c/f094a4a79a5d45612394d97c27d03837a8f012f5f7ad085f2705b4ca0510/tensorflow-1.0.0-cp34-cp34m-manylinux1_x86_64.whl#md5=1e137fc0cb113d544758ba4a3334b3d6 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/ce/2c/6a1cf90746879c2d05df04efc86a8b1edd79d7b06323a5c8fa63f5520824/tensorflow-1.0.0-cp35-cp35m-win_amd64.whl#md5=7e73a31ef4eb5914991cb2ad401fd21d (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/e5/5c/56e6522fdcd6f8739dcbc4de62e8b0040e141785bb42e5b53a83b0ba3e58/tensorflow-0.12.1-cp35-cp35m-manylinux1_x86_64.whl#md5=c6e3ba8579754f37d37be26e863f9d95 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/e7/73/85ff235957f2652f78a2cd7f0a045d3f983302991cb7af2fddedf27d56fe/tensorflow-0.12.1-cp33-cp33m-manylinux1_x86_64.whl#md5=d77e48ef3b45e543ca165db93f27e0ff (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/e9/a5/45b172f20e2fabd19c7f18a44570fc82acc4c628ec9bc4b313de39c4fe37/tensorflow-0.12.1-cp36-cp36m-manylinux1_x86_64.whl#md5=def6ab7dc3930f34f3000caef34f4333 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
    Skipping link https://pypi.python.org/packages/f6/2a/e5bb6320a3fc2886f2677ffa0d4396eefb5914cfc19db94e672c650f0700/tensorflow-0.12.0rc0-cp27-cp27mu-manylinux1_x86_64.whl#md5=00a6b89122d86fd527558a2740fd6f8f (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python
  Could not find a version that satisfies the requirement tensorflow (from versions: )
Cleaning up...
No matching distribution found for tensorflow
Exception information:
Traceback (most recent call last):
  File ""/Users/myname/Envs/tensorflow/lib/python3.5/site-packages/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""/Users/myname/Envs/tensorflow/lib/python3.5/site-packages/pip/commands/install.py"", line 335, in run
    wb.build(autobuilding=True)
  File ""/Users/myname/Envs/tensorflow/lib/python3.5/site-packages/pip/wheel.py"", line 749, in build
    self.requirement_set.prepare_files(self.finder)
  File ""/Users/myname/Envs/tensorflow/lib/python3.5/site-packages/pip/req/req_set.py"", line 380, in prepare_files
    ignore_dependencies=self.ignore_dependencies))
  File ""/Users/myname/Envs/tensorflow/lib/python3.5/site-packages/pip/req/req_set.py"", line 554, in _prepare_file
    require_hashes
  File ""/Users/myname/Envs/tensorflow/lib/python3.5/site-packages/pip/req/req_install.py"", line 278, in populate_link
    self.link = finder.find_requirement(self, upgrade)
  File ""/Users/myname/Envs/tensorflow/lib/python3.5/site-packages/pip/index.py"", line 514, in find_requirement
    'No matching distribution found for %s' % req
pip.exceptions.DistributionNotFound: No matching distribution found for tensorflow
```

These are my current packages in this environment:
```
$ pip freeze
appdirs==1.4.0
nose==1.3.7
packaging==16.8
pbr==1.10.0
pyparsing==2.1.10
QScintilla==2.9.2
sip==4.18
six==1.10.0
stevedore==1.20.0
virtualenv==15.1.0
virtualenv-clone==0.2.6
virtualenvwrapper==4.7.2

```

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

StackOverflow has questions that deal with the period before the tensorflow wheel was on PyPi, or when the Python is not 64-bit or earlier than 3.5.

### Environment info
Operating System:
OSX 10.9.5
Installed version of CUDA and cuDNN: I don't have CUDA I believe.

"
7791,glorot_uniform_initializer and others not exported,"Many initializers defined in `init_ops` are not exported in the global TensorFlow package. These include:

- `glorot_uniform_initializer` (this is even explicitly mentioned in the docs as the default initializer for variables)
- `glorot_normal_initializer`
- `variance_scaling_initializer`

It seems like this might be an oversight  `glorot_uniform_initializer` is mentioned in existing documentation, and `glorot_normal_initializer` has no other hits in the code base, which makes it seem like it's for user consumption.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Searched 

### Environment info
v1.0.0 docker image

### What other attempted solutions have you tried?
Similar initializers are available in `tf.contrib.layers`, but it seems preferable to use the ones that already exist in core."
7789,Generalize TensorFlowTestCase to handle arbitrary devices,"Currently, TensorFlowTestCase can only handle CPU and GPU devices.

```python
class TensorFlowTestCase(googletest.TestCase):
  def test_session(self,
                   graph=None,
                   config=None,
                   use_gpu=False,
                   force_gpu=False)
```

We are currently extending TensorFlow to run on another device and would like to update the signature to handle arbitrary devices. This could be done by changing the signature as follows, for example:

```python
class TensorFlowTestCase(googletest.TestCase):
  def test_session(self,
                   graph=None,
                   config=None,
                   use_device=None,
                   force_device=False)
```

This would certainly have some significant ripples in the test cases, so I would like like to get some feedback before I start anything.

Thoughts?
"
7786,Feature Request: Embedding Lookup Gradient,"Hi,

Since `embedding_lookup` is just a fast way of doing a matrix multiplication between the embedding weights and a tensor of one-hot vectors, it would be nice to have an option to get the gradient of the `embedding_lookup` as if it were such a matrix multiplication.  There are situations where this is actually useful - for example, the one-hots could be sampled from a high-dimensional multinomial distribution computed by a first neural network and passed as input to a second neural network, where using `embedding_lookup` would be much faster because of the high dimensionality.  There are some recent papers with various methods for back-propagating through the sampling procedure that I would like to implement, but I don't see any way to do it efficiently for high-dimensional distributions without a feature like this. 

Thanks,
Shawn
"
7785,Distributed Tensorflow fails : no device,"I am following the example in https://www.tensorflow.org/versions/r0.10/how_tos/distributed/  to create a distributed tensorflow model with a parameter server and n workers. I do not have any GPU, all work is distributed on CPU

In the chief worker, I want to save my variables every some steps, but invoking the saver results in the following exception :

    Cannot assign a device to node 'save_1/RestoreV2_21': 
    Could not satisfy explicit device specification 
    '/job:ps/task:0/device:CPU:0' because no devices matching that 
    specification are registered in this process; available devices: 
    /job:localhost/replica:0/task:0/cpu:0

    [[Node: save_1/RestoreV2_21 = RestoreV2[dtypes=[DT_INT32],
    _device=""/job:ps/task:0/device:CPU:0""](save_1/Const, 
    save_1/RestoreV2_21/tensor_names, save_1/RestoreV2_21/shape_and_slices)]]

I tried :

    server = tf.train.Server(cluster,
                             job_name=self.calib.params['job_name'],
                             task_index=self.calib.params['task_index'],
                             config=tf.ConfigProto(allow_soft_placement=True)

I am using a supervisor as in the example in the documentation:

    sv = tf.train.Supervisor(
                             is_chief=is_chief,
                           ...)

and creating my session as follows :

    sess = sv.prepare_or_wait_for_session(server.target) as sess:

but I am still having the exact same error.

When I print server.target I obtain 

    grpc://localhost:xxxx

where xxxx is 2200 for my first worker, 2201 for my second worker and so on"
7784,ValueError while implementing GMM using skflow,"Hi I am trying to implement GMM using tensorflow. But I am getting the following error :- ValueError: Features are incompatible with given information. Given features: Tensor(""input:0"", shape=(?, 198), dtype=float32), required signatures: TensorSignature(dtype=tf.float64, shape=TensorShape([Dimension(None), Dimension(198)]), is_sparse=False)

Operating System: Ubuntu 16.04
tensorflow version 1.0.0


`from tensorflow.contrib.factorization.python.ops import gmm as gmm_lib
gmm = gmm_lib.GMM(num_clusters, batch_size=1)
gmm.fit(x.astype('float32'),steps=300)
yy = gmm.predict(x,y=None)`

x is a numpy array of shape (2384, 198)

Please find the stack trace in stackoverflow [question](http://stackoverflow.com/questions/42299378/valueerror-features-are-incompatible-with-given-information-given-features-te)

If I pass a `float64` data then I get the following error:-
`ValueError: Tensor conversion requested dtype float64 for Tensor with dtype float32: 'Tensor(""sub_1:0"", shape=(), dtype=float32)'`

Please find the stack trace in stackoverflow [question](http://stackoverflow.com/questions/42299378/valueerror-features-are-incompatible-with-given-information-given-features-te)

I even tried to implement the gmm_test from [here](https://github.com/tensorflow/tensorflow/blob/32bd3d024f33e920a67a1081bc0ae0048350fdee/tensorflow/contrib/factorization/python/ops/gmm_test.py)

But I got a new error :- 
```
Traceback (most recent call last):
  File ""/home/xxxxx/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-61-c8c19df9240a>"", line 1, in <module>
    gmm.fit(input_fn=input_fn(), steps=0)
TypeError: fit() got an unexpected keyword argument 'input_fn'
```

If it is a bug please fix it, otherwise provide an alternative



"
7783,Segfault in runtime executor due to variable overflow,"I'm running Tensorflow 1.0, and I'm encountering a segfault in tensorflow, with the following output:

     F tensorflow/core/common_runtime/executor.cc:484] Check failed: e->src_output() < 32768 (38774 vs. 32768)
     Aborted (core dumped)


My program loads a fairly large sparse matrix (1879549 samples, 556926 features, 0.000038 of the entries in the matrix are nonzero) into memory. I then create a `tf.SparseTensor` out of it:

    # x_indices and x.data are the data for the sparse matrix in the correct format
    x_ind = tf.Variable(initial_value=x_indices.astype(np.int64), trainable=False)
    x_val = tf.Variable(initial_value=x.data, dtype=tf.float32, trainable=False)
    return tf.SparseTensor(x_ind, x_val, dense_shape=x_sparse.shape)

I then split this SparseTensor into minibatch-sized splits. I have several queue-runners feed that feed a Queue by taking a minibatch, transforming it to dense and putting it into the queue.

In code, the process looks like this (where `self._input_x_sp` is the SparseTensor):

        self.x_shape = self._input_x_sp.get_shape().as_list()
        self.y_shape = self._input_y.get_shape().as_list()
        n_batches = self.x_shape[0] // self.batch_size
        self._x_split = tf.sparse_split(sp_input=self._input_x_sp, num_split=n_batches, axis=0)
        self._y_split = tf.split(self._input_y, num_or_size_splits=n_batches, axis=0, name=""y_batch"")

        #   ...  creating a Queue

        # build a list of all possible enqueue OPs.
        # We need to do this here while we're still single-threaded, as the
        # Graph creation is not thread-safe
        # Later in the different threads we can just run the OPs
        self._op_list = []
        for i in range(n_batches):
            x_batch = tf.sparse_tensor_to_dense(self._x_split[i], name=""x_batch"")
            y_batch = self._y_split[i]
            self._op_list.append(self._queue.enqueue_many([x_batch, y_batch]))

The queue-runners randomly pick an operation from `self._op_list` and execute it, in a loop. 


 I am confident that my code is not to blame, as the program runs just fine on smaller input sizes (a sparse matrix of 206208 samples and 133515 features of which 0.000135 nonzero entries), but encounters the segfault on the larger matrix.

Looking at the TF code where the error is generated ([here](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/core/common_runtime/executor.cc#L484)) it seems like the cause is a variable in tensorflow that is an unsigned short when it probably should be something larger than that."
7782,control_dependencies and assign new shape not working (using validate_shape=False),"### Environment info
Operating System: OSX on CPU
Tensorflow 1.0.0

### Problem
Hello, i've been trying to use `tf.assign` with a `tf.control_dependencies` scheme when changing the shape on the fly.
```python
import tensorflow as tf

# I define a ""shape-able"" Variable
x = tf.Variable(
    [], 
    dtype=tf.int32,
    validate_shape=False,
    trainable=False
)
# I build a new shape and assign it to x
concat = tf.concat([x, [0]], 0)
assign_op = tf.assign(x, concat, validate_shape=False)

with tf.control_dependencies([assign_op]):
    # I print x after the assignment
    # Note that the Print call is on ""x"" and NOT ""assign_op""
    print_op_dep = tf.Print(x, data=[x], message=""print_op_dep:"")
    # The assign_op is called, but it seems that print statement happens
    # before the assignment

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(3):
        sess.run(print_op_dep)
```

**Outputs**:
```bash
I tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[]
I tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0]
I tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0 0]
```

**I would expect**:
```
I tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0]
I tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0 0]
I tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0 0 0]
```

Is this a bug ?"
7781,Distribution of sum of several discrete variables,"I've looked for this feature, and I don't think it exists, but it would be really useful for modelling the latent variable models which result in the observation being a **sum** of several discrete variables. This would be especially useful in signal processing, where different signals add up. Additionally, the idea I am presenting below involves clipping, which is also a feature of the signal recording equipment.

Let's say I have a discrete set of signal values that I am tracking, like so:
```
x = tf.constant(
    [-2.0, -1.0, 0.0, 1.0, 2.0],
    name='x')
```
It is important for the below that the values in x are sequential and equally spaced. In fact, I am only interested in integer values centered around zero.

Let's say that some latent variable model produces the probabilities of 3 signals taking on these specific values like so (I am writing as a constant, but it will be in fact a variable):
```
p = tf.constant(
    [[0.1, 0.1, 0.2, 0.4,  0.2],
     [0.5, 0.3, 0.1, 0.05, 0.05],
     [0.1, 0.2, 0.4, 0.2,  0.1]],
    name='p')
```
Assuming these three variables are independent, I would now like a function res = f(x, p) that would compute res[i] = p(sum(signal1 + signal2 + signal3) = x[i]) for 0 < i < len(x) -1. 

Moreover, for at the edges if should produce the output like so (clipping): res[0] = p(sum(signal1 + signal2 + signal3) <= x[0]) and res[-1] = p(sum(signal1 + signal2 + signal3) >= x[-1])

The output of this operation would be of the same shape as x. The result of the operation would be equivalent to a series of convolutions with clipping: conv(p[-1,:], conv(... conv(p[0, :], p[1, :]))). Here we assume the normal convolution function (not cross-correlation).

It might be useful to return the results in the log-space (and be able to accept the inputs as logits too).
"
7779,tensorflow custom op : user_ops,"I want to add my custom op.
so I read https://www.tensorflow.org/extend/adding_an_op.
And then I tried to wirte down follow code with your tutorial.
file name is zero_out.cc and path is ~/tensorflow/tensorflow/core/user_ops.
```
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/framework/op_kernel.h""

using namespace tensorflow;

REGISTER_OP(""ZeroOut"")
    .Input(""to_zero: int32"")
    .Output(""zeroed: int32"")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      c->set_output(0, c->input(0));
      return Status::OK();
    });

class ZeroOutOp : public OpKernel {
 public:
  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}
  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor& input_tensor = context->input(0);
    auto input = input_tensor.flat<int32>();

    // Create an output tensor
    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
                                                     &output_tensor));
    auto output = output_tensor->flat<int32>();

    // Set all but the first element of the output tensor to 0.
    const int N = input.size();
    for (int i = 1; i < N; i++) {
      output(i) = 0;
    }

    // Preserve the first input value if possible.
    if (N > 0) output(0) = input(0);
  }
};

REGISTER_KERNEL_BUILDER(Name(""ZeroOut"").Device(DEVICE_CPU), ZeroOutOp);
```

And I made BUILD file in same path.
```
load(""//tensorflow:tensorflow.bzl"", ""tf_custom_op_library"")

tf_custom_op_library(
    name = ""zero_out.so"",
    srcs = [""zero_out.cc""],
)
```


And then, follow your tutorial, I build with bazel.
I wrote this command on ~/tensorflow.
`bazel build --config opt //tensorflow/core/user_ops:zero_out.so`

complete massage is here.
`Target //tensorflow/core/user_ops:zero_out.so up-to-date:
  bazel-bin/tensorflow/core/user_ops/zero_out.so`

and I put the exactly same tutorial code but it doesn't work.
```
import tensorflow as tf
zero_out_module = tf.load_op_library('zero_out.so')
with tf.Session(''):
  zero_out_module.zero_out([[1, 2], [3, 4]]).eval()
```


error massage is this.
`Traceback (most recent call last):
  File ""/Users/buginus/tf_installwithpip3/Study_TensorFlow/01 - LinearRegression/hi.py"", line 2, in <module>
    zero_out_module = tf.load_op_library('zero_out.so')
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py"", line 64, in load_op_library
    None, None, error_msg, error_code)
tensorflow.python.framework.errors_impl.NotFoundError: dlopen(zero_out.so, 6): image not found`"
7778,"""The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations"" in ""Hello, TensorFlow!"" program","Opening this with reference to https://github.com/tensorflow/tensorflow/issues/7500. 

Installed TensorFlow 1.0 with reference to https://www.tensorflow.org/install/install_windows on Windows 10 and hit the same issue discussed in https://github.com/tensorflow/tensorflow/issues/7500.  With applying the solution suggested in that thread, the original issue disappeared but got the new warnings: 

C:\Users\geldqb>python
Python 3.5.3 (v3.5.3:1880cb95a742, Jan 16 2017, 16:02:32) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> hello = tf.constant('Hello, TensorFlow!')
>>> sess = tf.Session()
2017-02-22 22:28:20.696929: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2017-02-22 22:28:20.698285: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2017-02-22 22:28:20.700143: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2017-02-22 22:28:20.700853: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-02-22 22:28:20.701498: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-02-22 22:28:20.702190: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-02-22 22:28:20.702837: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-02-22 22:28:20.703460: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
>>> print(sess.run(hello))
b'Hello, TensorFlow!'
"
7776,Update Performance guide compiler recommendation for macOS users,"Hi!  

As per the [Performance guide](https://www.tensorflow.org/performance/performance_guide) I tried to build TF from source using gcc version 4.8.3+ on a MacBook.  I tried gcc 4.9 and 6.3 (Homebrew versions).  I got the following errors (same errors for 6.3):

- gcc-4.9: error: unrecognized command line option '-fcolor-diagnostics'
- gcc-4.9: error: unrecognized command line option '-Wthread-safety'
- gcc-4.9: error: unrecognized command line option '-Wself-assign'

According to [one](https://github.com/tensorflow/tensorflow/issues/1192) of the last years issues with the same errors, this is due to the fact that 

> We generally use the default of clang for OS X compilation, which explains your error. When I run gcc --version I get Apple LLVM version 7.0.2 (clang-700.1.81). You should try switching to clang instead of gcc, since that's the supported approach.

If this is still the case and all macOS users should stick to clang, could you please update the performance guide accordingly?


Best,
Andrei

"
7775,"TF-SLIM : Tensor name ""InceptionV3/*/*/*/*/ not found in checkpoint files","Similar problems are in stackoverlow, however they are using flowers example with flowers_train. Using
https://github.com/tensorflow/models/tree/master/slim Fine-tuning a model from an existing checkpoint. The  moving_variance and moving_mean have both had the NotFoundError.


The following train_iamge_classifier

train_image_classifier_nx.py \
--checkpoint_path=data/model/inception-v3/model.ckpt-157585 \
--checkpoint_exclude_scopes=InceptionV3/Logits,InceptionV3/AuxLogits/Logits \
--trainable_scopes=InceptionV3/Logits,InceptionV3/AuxLogits/Logits \
--model_name=inception_v3 \
--train_dir=data/test/train \
--dataset_dir=data/test/traindata


GPU initializes and immediately produces the traceback as follows:

NotFoundError (see above for traceback): Tensor name
""InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance"" not
found in checkpoint files
data/model/inception-v3/model.ckpt-157585
         [[Node: save/RestoreV2_304 = RestoreV2[dtypes=[DT_FLOAT],
         _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0,
         save/RestoreV2_304/tensor_names,
         save/RestoreV2_304/shape_and_slices)]]
                  [[Node: save/RestoreV2_70/_653 =
                  _Recv[client_terminated=false,
                  recv_device=""/job:localhost/replica:0/task:0/gpu:0"",
                  send_device=""/job:localhost/replica:0/task:0/cpu:0"",
                  send_device_incarnation=1,
                  tensor_name=""edge_1621_save/RestoreV2_70"",
                  tensor_type=DT_FLOAT,
                  _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]


"
7774,"after type 'run' in tensorflow debugger, the terminal reappears and stucks","**My program can run and show some traing results, but for a classification question, the accuracies of training set and validation set both keep at around 0.5 for 40 epoches.
So I want to use tensorflow debugger to watch what the variables are.
When I type 'run' and push enter, it jump out of the debugger and the terminal reshows, then the terminal screen stuck at below.** 

name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.835
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 7.28GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)


**the nvidia-smi results is below:**
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1080    Off  | 0000:01:00.0      On |                  N/A |
|  0%   43C    P2    53W / 200W |   1857MiB /  8105MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1011    G   /usr/lib/xorg/Xorg                             300MiB |
|    0      1802    G   compiz                                         133MiB |
|    0      3362    G   .../Enabled/MarkNonSecureAs/show-non-secure-    99MiB |
|    0     14422    C   python                                        1321MiB |
+-----------------------------------------------------------------------------+

-------------------------------------------------------------------------------------------------------------------------------------------
**any suggestion for solve the problem?**"
7772,where is the gradient computation of a variable in cpu,"There are some variables of a neural network. Some are in GPU, and some are in CPU. For the variables in CPU, where are their gradients computed? CPU or GPU?
For example, where is the gradient of `c` computed?
```python
with tf.device('/gpu:0'):
  a = tf.get_variable(""a"", [100, 3])
  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])
  with tf.device('/cpu:0'):
    c = tf.matmul(a, b)
  d = tf.matmul(b,c)
```
Thanks very much."
7770,"""GraphDef cannot be larger than 2GB."" error when saving model after assigning variables","I want to use a pretrained model to warmly start another model with a little difference. Simply, I create a new model, and assign the variables with same name with pretrained model weights. But, when saving the model, error occurred.
```
Traceback (most recent call last):
  File ""tf_test.py"", line 23, in <module>
    save_path = saver.save(sess, ""./model.ckpt"")
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1308, in save
    self.export_meta_graph(meta_graph_filename)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1331, in export_meta_graph
    graph_def=ops.get_default_graph().as_graph_def(add_shapes=True),
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2268, in as_graph_def
    result, _ = self._as_graph_def(from_version, add_shapes)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2231, in _as_graph_def
    raise ValueError(""GraphDef cannot be larger than 2GB."")
ValueError: GraphDef cannot be larger than 2GB.
```

The example code is as follow:
```python
import tensorflow as tf
import numpy as np

# Create some variables.
v1 = tf.get_variable(""L_enc"", [400000, 1024])
v2 = tf.get_variable(""L_dec"", [400000, 1024])

# Add an op to initialize the variables.
init_op = tf.initialize_all_variables()

# Add ops to save and restore all the variables.
saver = tf.train.Saver(tf.all_variables())

with tf.Session() as sess:
  sess.run(init_op)
  for v in tf.trainable_variables():
    embedding = np.random.uniform(-1, 1, (400000, 1024))
    sess.run(v.assign(embedding))
  # Save the variables to disk.
  save_path = saver.save(sess, ""./model.ckpt"")
  print(""Model saved in file: %s"" % save_path)
```
"
7769,How to implement a custom Estimator with multiple model_fns?,"How to implement a estimator with multiple model_fn so that we can train GAN-like models like following:
```
gan = my_estimator(model_fn=multi_model_fn, params=params)
for i in range(nb_train):
  gan.fit(x, y, model_id='gen', ...)
  gan.fit(x, y, model_id='cri', `...)
```"
7767,ParameterServer restart crashes distributed training process,"I've tried running distributed TensorFlow with Supervisor and new MonitoredTrainingSession and I observe following behavior if Parameter Server is restarted.  Code is available at http://pastebin.com/HBUicRp2.  I'm using TensorFlow freshly built from r1.0.

At first, this error happens which is OK:
```
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.UnavailableError'>, {""created"":""@1487751897.191020037"",""description"":""EOF"",""file"":""ex
ternal/grpc/src/core/lib/iomgr/tcp_posix.c"",""file_line"":235,""grpc_status"":14}
W tensorflow/core/framework/op_kernel.cc:993] Unavailable: {""created"":""@1487751897.191020037"",""description"":""EOF"",""file"":""external/grpc/src/core/lib/iomgr/tcp_posix.c"",""file_line"":
235,""grpc_status"":14}
         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:worker/replica:0/task:0/cpu:0"", send_device=""/job:ps/replica:0/task:0/cpu:0"", send_
device_incarnation=8036561443230364107, tensor_name=""edge_30_Assign_1"", tensor_type=DT_INT64, _device=""/job:worker/replica:0/task:0/cpu:0""]()]]
W tensorflow/core/framework/op_kernel.cc:993] Unavailable: {""created"":""@1487751897.191020037"",""description"":""EOF"",""file"":""external/grpc/src/core/lib/iomgr/tcp_posix.c"",""file_line"":
235,""grpc_status"":14}
         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:worker/replica:0/task:0/cpu:0"", send_device=""/job:ps/replica:0/task:0/cpu:0"", send_
device_incarnation=8036561443230364107, tensor_name=""edge_30_Assign_1"", tensor_type=DT_INT64, _device=""/job:worker/replica:0/task:0/cpu:0""]()]]
W tensorflow/core/framework/op_kernel.cc:993] Unavailable: {""created"":""@1487751897.191020037"",""description"":""EOF"",""file"":""external/grpc/src/core/lib/iomgr/tcp_posix.c"",""file_line"":
235,""grpc_status"":14}
         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:worker/replica:0/task:0/cpu:0"", send_device=""/job:ps/replica:0/task:0/cpu:0"", send_
device_incarnation=8036561443230364107, tensor_name=""edge_30_Assign_1"", tensor_type=DT_INT64, _device=""/job:worker/replica:0/task:0/cpu:0""]()]]
```

However, after Parameter Server is restarted, I see this error in logs and training worker crashes:
```
I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000729. Possibly, this worker just restarted.
I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000727. Possibly, this worker just restarted.
I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000731. Possibly, this worker just restarted.
I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000723. Possibly, this worker just restarted.
I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 000000000000072b. Possibly, this worker just restarted.
Traceback (most recent call last):
  File ""tf_dist_mnist.py"", line 140, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""tf_dist_mnist.py"", line 107, in main
    mon_sess.run(train_op, feed_dict={image: image_, label: label_})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 478, in __exit__
    self._close_internal(exception_type)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 511, in _close_internal
    self._sess.close()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 739, in close
    self._sess.close()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 827, in close
    self._coord.join()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 386, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py"", line 234, in _run
    sess.run(enqueue_op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnavailableError: {""created"":""@1487751897.191020037"",""description"":""EOF"",""file"":""external/grpc/src/core/lib/iomgr/tcp_posix.c"",""file_line"":235,""grpc_status"":14}
```

I expect it to recover from latest checkpoint instead."
7766,Buzel build error.," Hello,
  I am using TensorFlow Android Camera Demo. I follow step given in README file.

  1) Import andorid project
  2) Install bazel using chocolaty
  3) Edit in workspace file.
 
 While I run bazel build command :
 bazel build -c opt //tensorflow/examples/android:tensorflow_demo

 I getting error that **Couldn't find java at '$(ls -d C:/Program\ Files/Java/jdk* | sort | tail -n 1)/bin/java.exe**'.

My JDK is installed at C:\Program Files (x86)\Java\jdk1.8.0_66\bin . I also set environment variable    JAVA_HOME=""$(ls -d C:/Program\ Files/Java/jdk* | sort | tail -n 1)""
BAZEL_SH=c:/tools/msys64/usr/bin/bash.exe
  "
7765,ImportError: No module named '_pywrap_tensorflow',"Hi, i installed tensorflow in windows 7 SP1. without GPU. (My GPU is AMD so i just tried CPU version.)

I follow the guidelines,
install python 3.5 and set the path automatically.

then
pip3 install --upgrade tensorflow

import tensorflow as tf

and I obtain the following error:
>>> import tensorflow as tf
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\dso_loader.cc:126] Couldn't open CUDA library cublas64_80.dll
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\cuda\cuda_blas.cc:2294] Unable to load cuBLAS DSO.
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\dso_loader.cc:126] Couldn't open CUDA library cudnn64_5.dll
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\cuda\cuda_dnn.cc:3517] Unable to load cuDNN DSO
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\dso_loader.cc:126] Couldn't open CUDA library cufft64_80.dll
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\cuda\cuda_fft.cc:344] Unable to load cuFFT DSO.
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\dso_loader.cc:126] Couldn't open CUDA library nvcuda.dll
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\cuda\cuda_diagnostics.cc:165] hostname: x
Traceback (most recent call last):
  File ""C:\Users\LAB2\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\LAB2\AppData\Local\Programs\Python\Python35\lib\importlib\__ini
t__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Invalid access to memory location.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\LAB2\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\LAB2\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\python\pywrap_tensorflow.py"", line 21, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\LAB2\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow')
  File ""C:\Users\LAB2\AppData\Local\Programs\Python\Python35\lib\importlib\__ini
t__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\LAB2\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\LAB2\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\python\__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\LAB2\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\LAB2\AppData\Local\Programs\Python\Python35\lib\importlib\__ini
t__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Invalid access to memory location.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\LAB2\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\LAB2\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\python\pywrap_tensorflow.py"", line 21, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\LAB2\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow')
  File ""C:\Users\LAB2\AppData\Local\Programs\Python\Python35\lib\importlib\__ini
t__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_st
arted/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.




Note:
I have seen [https://github.com/tensorflow/tensorflow/issues/7705] and [https://github.com/tensorflow/tensorflow/issues/7529]
i also have installed Visual C++ Redistributable 2015 x64.


Thank you for any help




"
7763,TensorFlow for Poets: Section 5: label_image.py on site is different than curl version,"Link: https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/index.html?index=..%2F..%2Findex#4

On the site, the `label_image.py` script uses relative paths for files while the version the tutorial asks users to curl uses absolute paths for files.

If the user is knowledgeable enough to prefer copying the script on the website vs. curl'ing it from https://goo.gl/tx3dqg, then the user might run into an issue where they first need to run `cd /` before being able to run the script as-is.

![screen shot 2017-02-21 at 10 47 37 pm](https://cloud.githubusercontent.com/assets/1815591/23200175/c6512762-f887-11e6-9d7f-f031bd839ecd.png)
![screen shot 2017-02-21 at 10 47 27 pm](https://cloud.githubusercontent.com/assets/1815591/23200176/c6682af2-f887-11e6-82a2-75a0641a127d.png)
"
7762,TensorFlow for Poets: Section 5: Missing a verb,"In https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/index.html?index=..%2F..%2Findex#4, ""You might warnings"" is missing a verb. Maybe ""You might get warnings"" or ""You might see warnings""?"
7760,The GPU docker image doesn't work in CentOS 7,"### Environment info
Operating System: CentOS 7.0

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```
# ls /usr/local/cuda/lib64/libcuda*
/usr/local/cuda/lib64/libcudadevrt.a    /usr/local/cuda/lib64/libcudart.so.8.0
/usr/local/cuda/lib64/libcudart.so      /usr/local/cuda/lib64/libcudart.so.8.0.27
/usr/local/cuda/lib64/libcudart.so.7.5  /usr/local/cuda/lib64/libcudart_static.a
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Refer to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/README.md , we change the path of cuda files for CentOS and run with the following commands.

```
export CUDA_SO=$(\ls /usr/local/cuda/lib64/libcuda* | xargs -I{} echo '-v {}:{}')
export DEVICES=$(\ls /dev/nvidia* | xargs -I{} echo '--device {}:{}')
docker run -it -p 8888:8888 $CUDA_SO $DEVICES gcr.io/tensorflow/tensorflow:latest-gpu bash
```

Now it can find the GPU devices but fail to init cuda library.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

```
root@f8e47e56d59d:/notebooks# python test_tensorflow.py
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:119] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: f8e47e56d59d
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  361.77  Sun Jul 17 21:18:18 PDT 2016
GCC version:  gcc version 4.8.2 20140120 (Red Hat 4.8.2-16) (GCC)
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 361.77.0
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1092] LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1093] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: f8e47e56d59d
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: f8e47e56d59d
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  361.77  Sun Jul 17 21:18:18 PDT 2016
GCC version:  gcc version 4.8.2 20140120 (Red Hat 4.8.2-16) (GCC)
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 361.77.0
WARNING:tensorflow:From a.py:20 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Epoch: 1, w: -0.844658732414, b: 9.68447113037
Epoch: 2, w: 0.321342378855, b: 10.4512624741
Epoch: 3, w: 1.12122178078, b: 10.3018264771
Epoch: 4, w: 1.55480694771, b: 10.155872345
Epoch: 5, w: 1.77876627445, b: 10.0725793839
Epoch: 6, w: 1.89310014248, b: 10.0290489197
Epoch: 7, w: 1.95129656792, b: 10.0067615509
Epoch: 8, w: 1.98089802265, b: 9.99540710449
Epoch: 9, w: 1.99595046043, b: 9.98963069916
Epoch: 10, w: 2.00360536575, b: 9.98669433594
```"
7757,Broken link to doc in extras.,"In the [doc on `nec_loss`](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss), the target doc of the ""Candidate Sampling Algorithms Reference "", of which the url is https://www.tensorflow.org/api_docs/python/extras/candidate_sampling.pdf, is missing and gives just a 404 error.

"
7755,sess.run(tf.global_variables_initializer()) is slow after installing 1.0.0.,"### The issue

`sess.run(tf.global_variables_initializer())` intermittently takes a long time since installing 1.0.0. I believe this also happened on my Python 2 0.12 install, but only after installing 1.0.0 for Python 3. This particular run was in Python 2, and the initializer took 32 seconds - see bottom of this post (and the attached cprofile output) for more detail.

Strangely, it seems like the more I run the same file, the faster it runs.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

None.

### Environment info
Operating System: Ubuntu 16.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```
ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root   560184 Oct 27 16:26 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Oct 27 16:26 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Oct 27 16:26 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 root root   394472 Oct 27 16:26 /usr/local/cuda/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 root root   737516 Oct 27 16:26 /usr/local/cuda/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 60696704 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn.so
-rwxr-xr-x 1 root root 60696704 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 60696704 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn.so.5.1.3
-rw-r--r-- 1 root root 59715990 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn_static.a
```

If installed from binary pip package, provide:

1. A link to the pip package you installed: `sudo pip install tensorflow-gpu` on 2/21/17
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. 1.0.0

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```python
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import time
from collections import deque

import numpy as np
print('Importing tensorflow')
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

xavier = tf.contrib.layers.xavier_initializer
batch_norm = tf.contrib.layers.batch_norm

tf.set_random_seed(1)

flags = tf.app.flags
FLAGS = flags.FLAGS
flags.DEFINE_string('data_dir', 'data/', 'Directory for storing data')
print('Reading data.')
mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)
initialization_variance = 0.1

print('Creating a session.')
sess = tf.Session()

def fully_connected_layer(inputs, output_size, scope, nonlinearity=True):
  with tf.variable_scope(scope):
    input_size = inputs.get_shape()[1].value

    weights = tf.get_variable('w', 
                              [input_size, output_size],
                              initializer=tf.constant_initializer(0.0))

    bias = tf.get_variable('b', 
                           [output_size],
                           initializer=tf.constant_initializer(0.0))

    multiplied = tf.matmul(inputs, weights)

    return tf.nn.elu(multiplied + bias) if nonlinearity else multiplied + bias

print('Setting up the model.')
input_images = tf.placeholder(tf.float32, [None, 784])
labels = tf.placeholder(tf.float32, [None, 10]) # labels
learning_rate_placeholder = tf.placeholder(tf.float32, name='learning-rate')

layer_a = fully_connected_layer((input_images), 50, 'a')

for i in range(5):
  layer_a = batch_norm(fully_connected_layer(layer_a, 50, str(i)))

layer_b = fully_connected_layer(layer_a, 10, 'b', nonlinearity=False)

predictions = tf.nn.softmax(layer_b) 
 
cross_entropy = tf.reduce_mean(-tf.reduce_sum(labels * tf.log(predictions), reduction_indices=[1]))
train_step = tf.train.AdamOptimizer(learning_rate_placeholder).minimize(cross_entropy)

print('Initializing variables.')
sess.run(tf.global_variables_initializer())
```
### What other attempted solutions have you tried?

None.

### Logs or other output that would be helpful

Some output from `python2 -m cProfile -o out.profile 1-mnist-simple.py`. This is a call to `sess.run(tf.global_variables_initializer())`.
![image](https://cloud.githubusercontent.com/assets/5461398/23191984/132ca252-f85e-11e6-8c55-9b48adc3d40b.png)


[cprofile-output.profile.zip](https://github.com/tensorflow/tensorflow/files/791923/cprofile-output.profile.zip)


"
7754,"tf.split(num_or_size_splits=x,...) fails for x=Dimension(128)","Seen while upgrading our code to TF 1.0, somehow this used to work in 12.1, but in TF 1.0 the following fails with `IndexError: list index out of range` inside of `array_ops.py`

`tf.split(num_or_size_splits=X.get_shape()[1], ...)
`

Looking at array_ops.py, the relevant logic

```
  if isinstance(num_or_size_splits, six.integer_types):
    return gen_array_ops._split(
        split_dim=axis, num_split=num_or_size_splits, value=value, name=name)
  else:
    size_splits = ops.convert_to_tensor(num_or_size_splits)

```

So `Dimension(128)` is treated as ""Tensor"" and code fails with `IndexError: list index out of range` inside `array_ops.py`. I think it would make more sense if the Tensor path checked if `num_or_size_splits` was `Tensor` or convertible to non-scalar Tensor, and then have a catch-all `else` for all other cases. Or perhaps documentation could be updated to say that this path is taken if `num_split` is not a scalar, which is what's happening now, despite the documentation implying that this is only for `Tensor` arguments.

I would fix this myself, but I already have an outstanding PR, and the current system makes it too painful to switch branches -- https://github.com/tensorflow/tensorflow/issues/6911
"
7751,tf.contrib.crf doesn't support sequences of length 1,"[`tf.contrib.crf`](https://www.tensorflow.org/versions/master/api_docs/python/contrib.crf/) doesn't support sequences of length 1.

For example, if I run the example on https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/crf and replace `num_words = 20` by `num_words = 1`, I get the error:

```
Traceback (most recent call last):
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\client\session.py"", line 1022, in _do_call
    return fn(*args)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\client\session.py"", line 1004, in _run_fn
    status, run_metadata)
  File ""C:\Anaconda\envs\py35\lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.UnimplementedError: TensorArray has size zero, but element shape <unknown> is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.
	 [[Node: gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3 = TensorArrayGatherV3[_class=[""loc:@rnn/TensorArray_1""], dtype=DT_FLOAT, element_shape=<unknown>, _device=""/job:localhost/replica:0/task:0/cpu:0""](gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/TensorArrayGradV3, rnn/TensorArrayUnstack/range, gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/gradient_flow)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Francky\Documents\GitHub\nlp\neurodeid\test\CRF_v2.py"", line 47, in <module>
    [unary_scores, transition_params, train_op])
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\client\session.py"", line 767, in run
    run_metadata_ptr)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\client\session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\client\session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\client\session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnimplementedError: TensorArray has size zero, but element shape <unknown> is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.
	 [[Node: gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3 = TensorArrayGatherV3[_class=[""loc:@rnn/TensorArray_1""], dtype=DT_FLOAT, element_shape=<unknown>, _device=""/job:localhost/replica:0/task:0/cpu:0""](gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/TensorArrayGradV3, rnn/TensorArrayUnstack/range, gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/gradient_flow)]]

Caused by op 'gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3', defined at:
  File ""C:\Users\Francky\Documents\GitHub\nlp\neurodeid\test\CRF_v2.py"", line 41, in <module>
    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\training\optimizer.py"", line 288, in minimize
    grad_loss=grad_loss)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\training\optimizer.py"", line 354, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 482, in gradients
    in_grads = grad_fn(op, *out_grads)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\ops\tensor_array_grad.py"", line 186, in _TensorArrayScatterGrad
    grad = g.gather(indices)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\ops\tensor_array_ops.py"", line 348, in gather
    element_shape=element_shape)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\ops\gen_data_flow_ops.py"", line 2226, in _tensor_array_gather_v3
    element_shape=element_shape, name=name)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\framework\ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\framework\ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

...which was originally created as op 'rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3', defined at:
  File ""C:\Users\Francky\Documents\GitHub\nlp\neurodeid\test\CRF_v2.py"", line 37, in <module>
    unary_scores, y_t, sequence_lengths_t)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\contrib\crf\python\ops\crf.py"", line 156, in crf_log_likelihood
    log_norm = crf_log_norm(inputs, sequence_lengths, transition_params)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\contrib\crf\python\ops\crf.py"", line 123, in crf_log_norm
    dtype=dtypes.float32)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\ops\rnn.py"", line 545, in dynamic_rnn
    dtype=dtype)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\ops\rnn.py"", line 663, in _dynamic_rnn_loop
    for ta, input_ in zip(input_ta, flat_input))
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\ops\rnn.py"", line 663, in <genexpr>
    for ta, input_ in zip(input_ta, flat_input))
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\ops\tensor_array_ops.py"", line 400, in unstack
    indices=math_ops.range(0, num_elements), value=value, name=name)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\ops\tensor_array_ops.py"", line 428, in scatter
    name=name)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\ops\gen_data_flow_ops.py"", line 2492, in _tensor_array_scatter_v3
    name=name)
  File ""C:\Anaconda\envs\py35\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 763, in apply_op
    op_def=op_def)

UnimplementedError (see above for traceback): TensorArray has size zero, but element shape <unknown> is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.
	 [[Node: gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3 = TensorArrayGatherV3[_class=[""loc:@rnn/TensorArray_1""], dtype=DT_FLOAT, element_shape=<unknown>, _device=""/job:localhost/replica:0/task:0/cpu:0""](gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/TensorArrayGradV3, rnn/TensorArrayUnstack/range, gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/gradient_flow)]]
```

Tested with TensorFlow 1.0.0 on Windows 7 SP1 x64 Ultimate and TensorFlow-GPU 1.0.0 on Ubuntu 14.04.4 LTS x64."
7750,Making the URLs in the documentation clickable,"It would be nice if the URLs in the documentation were clickable.

[Example](https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/conectionist_temporal_classification__ctc_):


![image](https://cloud.githubusercontent.com/assets/15331/23184189/d337d5b0-f84c-11e6-9c7d-821563b30356.png)

"
7747,Why restore checkpoint will effect the tfrecord reader path? ,"I'm training my model with training dataset in standard format
```
features = tf.parse_example(
    batch_serialized_example,
    features={
        ""label"": tf.FixedLenFeature([], tf.float32),
        ""ids"": tf.VarLenFeature(tf.int64),
        ""values"": tf.VarLenFeature(tf.float32),
    })
batch_labels = features[""label""]
batch_ids = features[""ids""]
batch_values = features[""values""]
```
then I  save the checkpoint and want to do some predicting on my other dataset, **but add two more fields: uid and item_id (just for statistics)**

```
predict_features = tf.parse_example(
    predict_batch_serialized_example,
    features={
        ""label"": tf.FixedLenFeature([], tf.float32),
        ""ids"": tf.VarLenFeature(tf.int64),
        ""values"": tf.VarLenFeature(tf.float32),
        ""uid"": tf.FixedLenFeature([], tf.int64),
        ""item_id"": tf.FixedLenFeature([], tf.int64)
    })
predict_batch_labels = predict_features[""label""]
predict_batch_ids = predict_features[""ids""]
predict_batch_values = predict_features[""values""]
predict_batch_uids = predict_features[""uid""]
predict_batch_item_ids = predict_features[""item_id""]

```

When I restore the checkpoint , the simply read-data  faild `InvalidArgumentError (see above for traceback): Name: <unknown>, Feature: item_id (data type: int64) is required but could not be found.
`
When I remove the restore function , it's seems right.

Why the checkpoint do effect the reader? it's seems that after restore the checkponit ,the reader no longer read from the new path ?

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

https://gist.github.com/ericyue/e694a90338b9fadf9996025719005c9d

mininal code (just read data)

```
      if not restore_session_from_checkpoint(sess, saver,cf):
        logging.error(""No checkpoint found, exit now"")
        exit(1)
      try:
          while not coord.should_stop():
            uids, item_ids = sess.run( [predict_batch_uids, predict_batch_item_ids])
      except tf.errors.OutOfRangeError:
          logging.info(""Done predicting after reading all data"")
      finally:
          coord.request_stop()
          logging.info(""coord stopped"")
```

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
```
  1 Caused by op u'ParseExample_2/ParseExample', defined at:
  2   File ""deepcake-tf.py"", line 139, in <module>
  3     ""item_id"": tf.FixedLenFeature([], tf.int64)
  4   File ""/home/moon/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/parsing_ops.py"", line 445, in parse_example
  5     dense_types, dense_defaults, dense_shapes, name)
  6   File ""/home/moon/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/parsing_ops.py"", line 544, in _parse_example_raw
  7     name=name)
  8   File ""/home/moon/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_parsing_ops.py"", line 167, in _parse_example
  9     dense_shapes=dense_shapes, name=name)
 10   File ""/home/moon/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
 11     op_def=op_def)
 12   File ""/home/moon/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
 13     original_op=self._default_original_op, op_def=op_def)
 14   File ""/home/moon/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
 15     self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Name: <unknown>, Feature: item_id (data type: int64) is required but could not be found.
	 [[Node: ParseExample_2/ParseExample = ParseExample[Ndense=3, Nsparse=2, Tdense=[DT_INT64, DT_FLOAT, DT_INT64], dense_shapes=[[], [], []], sparse_types=[DT_INT64, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch_2, ParseExample_2/ParseExample/names, ParseExample_2/ParseExample/sparse_keys_0, ParseExample_2/ParseExample/sparse_keys_1, ParseExample_2/ParseExample/dense_keys_0, ParseExample_2/ParseExample/dense_keys_1, ParseExample_2/ParseExample/dense_keys_2, ParseExample_2/Const, ParseExample_2/Const_1, ParseExample_2/Const_2)]]
```"
7746,Feature request: Unique tensorboard URLs for different frontend states,"It would be nice if eg clicking on the ""GRAPHS"" tab in tensorboard would rewrite the URL to something like ""localhost:6006/graphs"". And likewise, navigating to ""localhost:6006/graphs"" would immediately load the GRAPHS tab. In general, encoding the tensorboard state in the URL would be useful.

That way, you could refresh the page and go back to the tab you were on, share your tensorboard state just by copying the URL, programmatically load a specific tensorboard state from visualization routines in your client language, etc.

"
7744,tensorflow sequence to sequence models,"Hello everyone,
I executed the code translate.py from sequence to sequence models tutorial of tensorflow for 20 hours and did not get output.
Can anyone tell me how much time will it take to display the output
System specifications:
I installed Ubuntu 16.04 on VMware with a disk space of 100GB
![seqgoo](https://cloud.githubusercontent.com/assets/20130992/23177248/2078472e-f834-11e6-80ec-c264984d51a7.png)
"
7743,[feature request] tf.matrix_triangular_solve misses unit_diagonal= argument,"Hello,

It would be useful to allow ```tf.matrix_triangular_solve``` have ```unit_diagonal=``` kw-argument and treat ```matrix``` argument as having unit diagonal.
Sure, the same may be obtained by composition ```tf.matrix_set_diag``` and ```tf.matrix_triangular_solve```. However, additional assignment here seems to be inefficient. Solving triangular matrix involves quite simple formulae where diagonal elements of ```matrix``` are presented explicitly.
"
7742,Improve retrained inception v3 in Android demo,"I have retrained the Inception-V3 final layer with my own 20 categories. When I am using retrained model in android demo app it takes 6 to 8 seconds to predict.

Running (using `armeabi-v7a`) on 

- LG G4 Stylus -> 6-8 sec
- S6, -> 3-4.5 sec

I have done `optimize_for_inference` it takes 6-9 sec and `quantize_graph` it takes 7-11 sec. Is there any way to improve it?

[Output](https://i.stack.imgur.com/lKryY.png) on LG G4 Stylus.

"
7741,The function 'layers/convolutional.py` does not follow `tf.GraphKeys` convention,"In [convolutional.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py) bias and weight variables are created and named `kernel` and `bias`. However according to the conventions defined in [tf.GraphKeys.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard8/tf.GraphKeys.md) those variables should be named  `weights` and `biases` and assigned the corresponding collections. There is some (third party) functionality which utilizes this convention, including `tf.contrib.layers.summarize_weights`. 

In addition, the names to be used are also stored in the constants `tf.GraphKeys.WEIGHTS` and `tf.GraphKeys.BIASES`. So instead of writing the strings hard in the code, I would suggest utilizing those constants. Do you agree with that? I am happy to submit a pull request solving this issue, if desired. 


"
7740,GradientDescentOptimizer got wrong result,"**I want use gradient descent to solve equation set, but I got wrong result everytime, so I check my code and written a numpy edition, in this edition I provide explicit loss gradient and I can get currect result.**

So I don't understand why GradientDescentOptimizer can not work.

**here is my code without tf:**

```python
import numpy as np


class SolveEquation:
    def __init__(self, rate: float, loss_threshold: float=0.0001, max_epochs: int=1000):
        self.__rate = rate
        self.__loss_threshold = loss_threshold
        self.__max_epochs = max_epochs
        self.__x = None

    def solve(self, coefficients, b):
        _a = np.array(coefficients)
        _b = np.array(b).reshape([len(b), 1])
        _x = np.zeros([_a.shape[1], 1])
        for epoch in range(self.__max_epochs):
            grad_loss = np.matmul(np.transpose(_a), np.matmul(_a, _x) - _b)
            _x -= self.__rate * grad_loss
            if epoch % 10 == 0:
                loss = np.mean(np.square(np.subtract(np.matmul(_a, _x), _b)))
                print('loss = {:.8f}'.format(loss))
                if loss < self.__loss_threshold:
                    break
        return _x

s = SolveEquation(0.1, max_epochs=1)
print(s.solve([[1, 2], [1, 3]], [3, 4]))
```

**And here is my code with tf:**

```python
import tensorflow as tf
import numpy as np


class TFSolveEquation:
    def __init__(self, rate: float, loss_threshold: float=0.0001, max_epochs: int=1000):
        self.__rate = rate
        self.__loss_threshold = tf.constant(loss_threshold)
        self.__max_epochs = max_epochs
        self.__session = tf.Session()
        self.__x = None

    def __del__(self):
        try:
            self.__session.close()
        finally:
            pass

    def solve(self, coefficients, b):
        coefficients_data = np.array(coefficients)
        b_data = np.array(b)
        _a = tf.placeholder(tf.float32)
        _b = tf.placeholder(tf.float32)
        _x = tf.Variable(tf.zeros([coefficients_data.shape[1], 1]))
        loss = tf.reduce_mean(tf.square(tf.matmul(_a, _x) - _b))
        optimizer = tf.train.GradientDescentOptimizer(self.__rate)
        model = optimizer.minimize(loss)
        self.__session.run(tf.global_variables_initializer())
        for epoch in range(self.__max_epochs):
            self.__session.run(model, {_a: coefficients_data, _b: b_data})
            if epoch % 10 == 0:
                if self.__session.run(loss < self.__loss_threshold, {_a: coefficients_data, _b: b_data}):
                    break
        return self.__session.run(_x)

s = TFSolveEquation(0.1, max_epochs=1)
print(s.solve([[1, 2], [1, 3]], [3, 4]))
```

I test these 2 codes with very simple equation set:

x_1 + 2 * x_2 = 3
x_1 + 3 * x_3 = 4

loss = 1/2 * || Ax - b ||^2

Init x_1 = 0, x_2 = 0, rate = 0.1

**Use gradient descent**
**So at 1st compute, the delta x = (0.7, 1.8)**

But unfortunately my code with tf give the 
delta x = 
[[ 0.69999999]
 [ 1.75      ]]

And my code without tf give the
delta x = 
[[ 0.7]
 [ 1.8]]

**Absolutely code without tf is right, but why tf comput gradient may less 0.05 then currect result?**
**I think this is the reason my code without tf can solve the equation set, but tf edition can not solve equation set currently.**

**Can someone tell me why tf give a incurrent gradiant? Thanks**

My platform is Win10 + tensorflow-gpu v1.0
"
7739,tensorflow/example/mutilbox_detector bazel build error,"I think there are lots of out of date docs in this repo, for example:
```
tensorflow/example/mutilbox_detector
```
this example show build mutilbox_detector using bazel and generate an binary executable file, however I just got bunch of errors:
```
WARNING: Config values are not defined in any .rc file: opt
ERROR: /Users/jintian/tensorflow/tensorflow/core/BUILD:1279:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /Users/jintian/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: /Users/jintian/tensorflow/tensorflow/core/BUILD:1279:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /Users/jintian/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: /Users/jintian/tensorflow/tensorflow/core/BUILD:1279:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /Users/jintian/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: Analysis of target '//tensorflow/examples/multibox_detector:detect_objects' failed; build aborted.
INFO: Elapsed time: 26.050s
```
Suggest develop team upgrade useless docs and fix those tutorial that makes us confuse and even lead us to the wrong road....."
7738,slim doesn't play well with the new tf.layers v 1.0,"We use tf.contrib.slim to build NN architecture. After upgrade to tensorflow 1.0 the are some trouble when building models with slim. For example `slim.arg_scope` doesn't work with the tf.layers components. And when we keep using slim.* layers, we got a runtime error:

~~~
tensorflow.python.framework.errors_impl.InvalidArgumentError: Session was not created with a graph before Run()!
~~~

If we use all the layers from tf.layers this runtime error goes away.

Any ideas ?
"
7737,AttributeError: module 'tensorflow' has no attribute 'merge_all_summaries',"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem? No

### Environment info
Operating System: Win10

Installed version of CUDA and cuDNN: No 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide: yes

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. 1.0.0

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
```
Traceback (most recent call last):
  File ""G:/codes/tensorflow2/Autopilot-TensorFlow/train.py"", line 22, in <module>
    merged_summary_op = tf.merge_all_summaries()
AttributeError: module 'tensorflow' has no attribute 'tf.merge_all_summaries'

```
"
7735,API Documentation with Links to Source Code,"I think it would be a really great improvement if the API documentation https://www.tensorflow.org/api_docs/ would contain links to the corresponding source code file on GitHub, similar to python package documentations on readthedocs.org, e.g. https://lasagne.readthedocs.io/en/latest/modules/layers/base.html.

This would also make it much faster to create pull requests that fix errors in the documentation."
7734,Error setting read_batch_size in tf.contrib.learn.read_batch_examples?,"I am using the API _tf.contrib.learn.read_batch_examples()_. When I set the param _read_batch_size_, it always lead to the error ""**ValueError: All shapes must be fully defined: [TensorShape([]), TensorShape([Dimension(None)])]**"".  Use the default value 1 of _read_batch_size_ is OK. I wonder what's happening behind?

The full error stack is below:

```
Traceback (most recent call last):
  File ""wide_n_deep_new.py"", line 258, in <module>
    tf.app.run()
  File ""/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""wide_n_deep_new.py"", line 249, in main
    train(estimator)
  File ""wide_n_deep_new.py"", line 200, in train
    monitors=[eval_monitor])
  File ""/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py"", line 765, in fit
    max_steps=max_steps)
  File ""/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 280, in new_func
    return func(*args, **kwargs)
  File ""/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1302, in fit
    loss = self._train_model_v2(input_fn=input_fn, hooks=hooks)
  File ""/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1312, in _train_model_v2
    features, labels = input_fn()
  File ""wide_n_deep_new.py"", line 198, in <lambda>
    num_epochs=None), 
  File ""wide_n_deep_new.py"", line 156, in input_fn
    examples_dict = read_csv_examples(mode, file_names, batch_size, num_epochs)
  File ""wide_n_deep_new.py"", line 129, in read_csv_examples
    name='read_batch_examples_{}'.format(mode))
  File ""/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/graph_io.py"", line 101, in read_batch_examples
    name=name)
  File ""/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/graph_io.py"", line 166, in read_keyed_batch_examples
    name=name)
  File ""/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/graph_io.py"", line 399, in _read_keyed_batch_examples_helper
    allow_smaller_final_batch=allow_smaller_final_batch)
  File ""/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 1017, in batch_join
    name=name)
  File ""/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 693, in _batch_join
    capacity=capacity, dtypes=types, shapes=shapes, shared_name=shared_name)
  File ""/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py"", line 666, in __init__
    shapes = _as_shape_list(shapes, dtypes)
  File ""/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py"", line 75, in _as_shape_list
    raise ValueError(""All shapes must be fully defined: %s"" % shapes)
ValueError: All shapes must be fully defined: [TensorShape([]), TensorShape([Dimension(None)])]
```"
7731,Provide a way to clear variable and name scope,"This is a follow-up of https://github.com/tensorflow/tensorflow/issues/6189

It would be nice to be able to create a variable or name scope that does not obey all currently open scopes.

As an example:
```python
with tf.variable_scope('foo'):
    with tf.variable_scope('bar'):
        x = tf.constant(1.0, name='x')     # foo/bar/x

with tf.variable_scope('foo'):
    with tf.variable_scope('bar', fresh=True):
        x = tf.constant(1.0, name='x')    # bar/x
```

This would be useful for using scopes with object oriented TensorFlow graphs.

Consider a slightly modified version of the example from #6189 . Assume each method pushes a scope for the instance (with the class name) and for the method.
```python
class Foo(object):
    def __init__(self):
        self.x = tf.get_variable('x', [], initializer=tf.constant(0.0))

    def meth(self, foo):
        z = tf.multiply(foo, 3.14, name='z')
        return self.x.assign(z)

    def boom(self, foo):
        return tf.multiple(2.0, self.meth(foo), name='baz')

f0 = Foo()      # Foo/__init__/x
f1 = Foo()      # Foo_1/__init__/x
f0.meth(1.0)    # Foo/meth/*
f0.boom(2.0)  # Foo/boom/baz, Foo/boom/Foo/meth/z
```

I'd like the variables in ``boom``'s call to ``meth`` to be prefixed with only ``Foo/meth`` rather than ``Foo/boom/Foo/meth``. This requires either releasing the variable scope within boom before calling out (hard) or providing a way for ``meth`` to clear the existing variable scope. "
7729,How to delete existed tensorflow variable?,"Hello, I am using windows GPU tensorflow 1.0.
 I use ipython notebook to build training model because I can reuse script easily. Sometimes I just want to modify one tensorflow variable, such as W1(shape=[3,3],name=""W1"") from a existed model. So I just modify the variable and run the model building script again. However, tensorflow add W1(shape=[5,5],name=""W1_1"")  variable to replace old variable W1. It makes some bugs when I continue the next work with W1. Could you tell me how to delete old variable ""W1"" and add new different variable named ""W1""?"
7728,compile_ios_tensorflow.sh  failed with following error," /Users/xietian/Documents/tensorflow/tensorflow/contrib/makefile/gen/lib/ios_ARMV7/libtensorflow-core-armv7.a -arch armv7 -fembed-bitcode -miphoneos-version-min=8.2 -framework Accelerate -Xlinker -S -Xlinker -x -Xlinker -dead_strip -all_load -L/Users/xietian/Documents/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib -lz -lstdc++ -lprotobuf -lz -lm
Undefined symbols for architecture armv7:
  ""google::protobuf::internal::WireFormatLite::WriteDoubleArray(double const*, int, google::protobuf::io::CodedOutputStream*)"", referenced from:
      tensorflow::TensorProto::SerializeWithCachedSizes(google::protobuf::io::CodedOutputStream*) const in libtensorflow-core-armv7.a(tensor.pb.o)
      tensorflow::HistogramProto::SerializeWithCachedSizes(google::protobuf::io::CodedOutputStream*) const in libtensorflow-core-armv7.a(summary.pb.o)
  ""google::protobuf::Any::MergeFrom(google::protobuf::Any const&)"", referenced from:
      tensorflow::MetaGraphDef_MetaInfoDef::MergeFrom(tensorflow::MetaGraphDef_MetaInfoDef const&) in libtensorflow-core-armv7.a(meta_graph.pb.o)
      google::protobuf::internal::GenericTypeHandler<google::protobuf::Any>::Merge(google::protobuf::Any const&, google::protobuf::Any*) in libtensorflow-core-armv7.a(meta_graph.pb.o)"
7727,when will tf.contrib.distributions move into tf core? Is this planned?,"Hi,

I'm working with distributions a lot in my own library built upon tensorflow. However, I found the api provided in tf.contrib.distributions are subject to change very often, which makes it quite difficult to keep up and maintain a stable codebase.
So I'd like to ask whether tf.contrib.distributions is going to the tensorflow core library and how is this planned. Will you recommend to keep using it or just build my own distributions?

Best,
Jiaxin"
7726,Feature request: Allow import of incomplete graphs,"It seems like there is no reason in principle not to allow this:

```python
import tensorflow as tf

g=tf.Graph()
with g.as_default():
    x = tf.constant(1.0, name=""x"")
    y = tf.exp(x)
g_def=tf.GraphDef()
g_def.node.extend([y.op.node_def])

g2 = tf.Graph()
with g2.as_default():
    x = tf.constant(2.0, name=""x"")
    tf.import_graph_def(g_def)

```

as, after `y` is imported, the `x:0` input it refers to is valid. 

As it stands however, this throws `ValueError: graph_def is invalid at node u'Exp': Input tensor 'x:0' not found in graph_def..`. 

"
7725,BAZEL_SH environment variable is not defined...,"C:\Users\nmehandru>bazel build tensorflow/tensorflow/examples/image_retraining:retrain
Error: BAZEL_SH environment variable is not defined, cannot convert MSYS paths to Windows paths: Invalid or incomplete multibyte or wide character
Error: AsWindowsPathWithUncPrefix(/etc/bazel.bazelrc): AsWindowsPath failed, err=203
: Invalid or incomplete multibyte or wide character
Error: AsWindowsPathWithUncPrefix(/home/nmehandru/.bazelrc): AsWindowsPath failed, err=203
: Invalid or incomplete multibyte or wide character
ERROR: 'null' is not recognized as an internal or external command,
operable program or batch file.
 (exit code: 1)
The 'build' command is only supported from within a workspace.


This is my printout when trying to run a script within the Tensorflow directory I cloned, in cmd in Windows 10. Not sure where to go from here. "
7723,arm-linux-androideabi-gcc: internal compiler error: Killed (program cc1plus),"I'm trying to create a Docker image which has all the dependencies required to build the TensorFlow Android demo app from source (including the Android SDK, NDK and build tools).  

I'm simply starting with `gcr.io/tensorflow/tensorflow:latest-devel` and then adding the Android dependencies on top.  This lists all the steps I've done so far:
https://medium.com/@daj/docker-image-for-building-tensorflow-android-demo-app-97c98ce37d9e#.y0kwcx63j

Unfortunately I keep hitting this error:
```
ERROR: /tensorflow/tensorflow/core/kernels/BUILD:3749:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed: arm-linux-androideabi-gcc failed: error executing command external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables ... (remaining 71 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.
arm-linux-androideabi-gcc: internal compiler error: Killed (program cc1plus)
```

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

These GitHub issues had similar titles, but the details looked a bit different:
https://github.com/tensorflow/tensorflow/issues?q=is%3Aissue+BadExitStatusException+Process+exited+with+status+4++cc1plus+is%3Aopen

### Environment info
Operating System: Docker version 1.13.1, build 092cba3 (running inside Mac OS X, I tried on both El Capitan and Sierra with the same results)

```
# git rev-parse HEAD
07bb8ea2379bd459832b23951fb20ec47f3fdbd4

# bazel version
INFO: Reading 'startup' options from /root/.bazelrc: --batch
Build label: 0.4.2
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Dec 7 18:47:11 2016 (1481136431)
Build timestamp: 1481136431
Build timestamp as int: 1481136431
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I've pushed my Docker image (after hitting the build error) to a public location, so you should be able to pull it and then run the build:
```
docker pull danjarvis/tensorflow-android:1.0.0
docker run -it danjarvis/tensorflow-android:1.0.0
cd /tensorflow
bazel build -c opt //tensorflow/examples/android:tensorflow_demo
```

### What other attempted solutions have you tried?

 - I tried on two different laptops.  
 - I built my Docker image again following [my own instructions](https://medium.com/@daj/docker-image-for-building-tensorflow-android-demo-app-97c98ce37d9e#.niaa80w9p) and saw the same error.

### Logs or other output that would be helpful

Normal build (without the `--verbose_failures` option - scroll down for the `--verbose_failures`, I've included that too):
```
# bazel build -c opt //tensorflow/examples/android:tensorflow_demo
INFO: Reading 'startup' options from /root/.bazelrc: --batch
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_patch_3d.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there.
INFO: Found 1 target...
INFO: From Compiling external/protobuf/src/google/protobuf/io/coded_stream.cc:
external/protobuf/src/google/protobuf/io/coded_stream.cc: In member function 'google::protobuf::int64 google::protobuf::io::CodedInputStream::ReadVarint32Fallback(google::protobuf::uint32)':
external/protobuf/src/google/protobuf/io/coded_stream.cc:445:12: warning: 'temp' may be used uninitialized in this function [-Wmaybe-uninitialized]
     return temp;
            ^
INFO: From Processing Android resources for //tensorflow/examples/android:tensorflow_demo:
Feb 21, 2017 2:50:45 AM com.google.devtools.build.android.AndroidDataMerger doMerge
WARNING: 
CONFLICT: asset:WORKSPACE is provided with ambiguous priority from:
	external/mobile_multibox/WORKSPACE
	external/inception5h/WORKSPACE
CONFLICT: asset:WORKSPACE is provided with ambiguous priority from:
	external/stylize/WORKSPACE
	external/mobile_multibox/WORKSPACE
INFO: From ProtoCompile tensorflow/examples/android/proto/box_coder.pb.cc:
bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/protobuf/src: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/example/example.pb.cc:
bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/protobuf/src: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/example/example.pb.cc [for host]:
bazel-out/host/genfiles/external/protobuf/src: warning: directory does not exist.
ERROR: /tensorflow/tensorflow/core/kernels/BUILD:3749:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed: arm-linux-androideabi-gcc failed: error executing command external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables ... (remaining 71 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.
arm-linux-androideabi-gcc: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <http://source.android.com/source/report-bugs.html> for instructions.
Target //tensorflow/examples/android:tensorflow_demo failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1527.289s, Critical Path: 1501.49s
```

Here's the result with the `--verbose_failures` option set:
```
# bazel --verbose_failures build -c opt //tensorflow/examples/android:tensorflow_demo
INFO: Reading 'startup' options from /root/.bazelrc: --batch
Unknown Bazel startup option: '--verbose_failures'.
  For more info, run 'Bazel help startup_options'.
root@a80d6a5002cd:/tensorflow# bazel build -c opt --verbose_failures //tensorflow/examples/android:tensorflow_demo
INFO: Reading 'startup' options from /root/.bazelrc: --batch
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_patch_3d.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there.
INFO: Found 1 target...
INFO: From Compiling tensorflow/core/kernels/split_op.cc:
tensorflow/core/kernels/split_op.cc: In instantiation of 'void tensorflow::SplitOpCPU<T>::Compute(tensorflow::OpKernelContext*) [with T = Eigen::QUInt8]':
tensorflow/core/kernels/split_op.cc:357:1:   required from here
tensorflow/core/kernels/split_op.cc:159:64: warning: narrowing conversion of '(tensorflow::int64)split_dim_output_size' from 'tensorflow::int64 {aka long long int}' to 'int' inside { } [-Wnarrowing]
         prefix_dim_size, split_dim_output_size, suffix_dim_size};
                                                                ^
tensorflow/core/kernels/split_op.cc: In instantiation of 'void tensorflow::SplitOpCPU<T>::Compute(tensorflow::OpKernelContext*) [with T = float]':
tensorflow/core/kernels/split_op.cc:357:1:   required from here
tensorflow/core/kernels/split_op.cc:159:64: warning: narrowing conversion of '(tensorflow::int64)split_dim_output_size' from 'tensorflow::int64 {aka long long int}' to 'int' inside { } [-Wnarrowing]
tensorflow/core/kernels/split_op.cc: In instantiation of 'void tensorflow::SplitOpCPU<T>::Compute(tensorflow::OpKernelContext*) [with T = int]':
tensorflow/core/kernels/split_op.cc:357:1:   required from here
tensorflow/core/kernels/split_op.cc:159:64: warning: narrowing conversion of '(tensorflow::int64)split_dim_output_size' from 'tensorflow::int64 {aka long long int}' to 'int' inside { } [-Wnarrowing]
ERROR: /tensorflow/tensorflow/core/kernels/BUILD:3749:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed: arm-linux-androideabi-gcc failed: error executing command 
  (cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow && \
  exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables -no-canonical-prefixes -fno-canonical-system-headers '-march=armv7-a' '-mfpu=vfpv3-d16' '-mfloat-abi=softfp' -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_mul_2.d '-frandom-seed=bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_mul_2.o' -DEIGEN_MPL2_ONLY -iquote . -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles -iquote external/protobuf -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/local_config_sycl -iquote external/gemmlowp -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/gemmlowp -isystem external/protobuf/src -isystem bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/eigen_archive -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-mfpu=neon' '-std=c++11' -DTF_LEAN_BINARY -O2 '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm' -isystem external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/include -isystem external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/include-fixed -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c tensorflow/core/kernels/cwise_op_mul_2.cc -o bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_mul_2.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.
arm-linux-androideabi-gcc: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <http://source.android.com/source/report-bugs.html> for instructions.
Target //tensorflow/examples/android:tensorflow_demo failed to build
INFO: Elapsed time: 160.782s, Critical Path: 138.06s
```"
7720,Feature: Hide Metrics for Which Data Exists for Zero Runs,"The list of metrics for for plotting on the scalars tab is cluttered with a number of metrics that don't exist for the selected runs. Why not side these rather than taking up real estate:
![image](https://cloud.githubusercontent.com/assets/51059/23149169/2c980ac0-f7b8-11e6-8493-b4085e089be1.png)
"
7719,Feature request: convenience functions for getting variables from scopes,"There does not currently seem to be a good way to get a collection of variables that belong to a given scope. I've seen issue #7295, but the answer given by Yaroslav (at http://stackoverflow.com/questions/42073239/tf-get-collection-to-extract-variables-of-one-scope) does not nest:

```python
import tensorflow as tf

def bar():
  with tf.variable_scope(""bar"") as scope:
    x = tf.Variable(0, name=""x"")
    print list(map(str, tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)))
    print list(map(str, tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name + ""/"")))
  print list(map(str, tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""bar"")))

def foo():
  with tf.variable_scope(""foo"") as scope:
    with tf.variable_scope(""bar_"") as scope:
      y = tf.Variable(0, name=""y"")
    bar()

with tf.Graph().as_default():
  bar()
print
with tf.Graph().as_default():
  foo()
```

The snippet above outputs (on TF 1.0):
```
['Tensor(""bar/x/read:0"", shape=(), dtype=int32)']
['Tensor(""bar/x/read:0"", shape=(), dtype=int32)']
['Tensor(""bar/x/read:0"", shape=(), dtype=int32)']

['Tensor(""foo/bar_/y/read:0"", shape=(), dtype=int32)', 'Tensor(""foo/bar/x/read:0"", shape=(), dtype=int32)']
['Tensor(""foo/bar/x/read:0"", shape=(), dtype=int32)']
[]
```

The output shows that wrapping the scope ""bar"" in another scope (""foo"") breaks Yaroslav's proposal of using `tf.get_collection(..., ""bar"")`. It also breaks `tf.get_collection(..., scope.name)` by including variables from outside the scope. The only version that works regardless of context is `tf.get_collection(..., scope.name + ""/"")`, which is ugly and too informed.

It would be great to have `scope.trainable_variables()`, or `scope.get_collection(...)`, or even `tf.get_collection(..., scope)`."
7718,Uncaught TypeError: Cannot read property 'url' of undefined,"Shown in the web console for a tensorboard instance:
```
Uncaught TypeError: Cannot read property 'url' of undefined
    at HTMLElement.redraw (tf-tensorboard.html:5739)
    at HTMLElement._toggleExpanded (tf-tensorboard.html:1995)
    at handler (polymer.html:561)
    at HTMLElement.decorated (polymer.html:4462)
    at HTMLElement.fire (polymer.html:1327)
    at Object.fire (polymer.html:899)
    at Object.forward (polymer.html:1196)
    at Object.click (polymer.html:1181)
    at HTMLElement.handleNative (polymer.html:789)
```
I am using the 1.0.0 Docker image."
7717,Segmentation fault with TensorFlow 1.0,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
* No

### Environment info
Operating System:
* Ubuntu 14.04/3.13.0-100-generic
* CentOS 7/3.10.0-123.el7.x86_64

Python Version:
* 2.7.5
* 3.5

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
 * Both CPU/GPU version crash, not related to CUDA

If installed from binary pip package, provide:

1. A link to the pip package you installed:
  * https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp27-none-linux_x86_64.whl
  * https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp27-none-linux_x86_64.whl
  * https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp35-cp35m-linux_x86_64.whl
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
  * 1.0.0

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I can make repro with ubuntu 14.04 + python 2.7, 3.5 and centos 7 + python 2.7, but not with ubuntu 16.04 + python 2.7:
https://gist.github.com/llhe/6d95d2e31ad3c5a886dc8c3bd6ace95b

### What other attempted solutions have you tried?

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
Typical outputs:
```
Extracting /tmp/data/train-images-idx3-ubyte.gz
Extracting /tmp/data/train-labels-idx1-ubyte.gz
Extracting /tmp/data/t10k-images-idx3-ubyte.gz
Extracting /tmp/data/t10k-labels-idx1-ubyte.gz
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Traceback (most recent call last):
  File ""coredump.py"", line 18, in <module>
    train()
  File ""coredump.py"", line 15, in train
    tf.image_summary('input', x, 10)
AttributeError: 'module' object has no attribute 'image_summary'
Segmentation fault (core dumped)
```
Stack trace:
```
(gdb) bt
#0  visit_decref (op=<unknown at remote 0x-1>, data=0x0) at /usr/src/debug/Python-2.7.5/Modules/gcmodule.c:429
#1  0x00007f610674d8de in dict_traverse (op=
    {0x0: <unknown at remote 0x-1>, '__ne__': <wrapper_descriptor at remote 0x1f7d730>, '__ror__': <wrapper_descriptor at remote 0x1f7da00>, '__nonzero__': <wrapper_descriptor at remote 0x1f7d820>, '__new__': <built-in method __new__ of type object at remote 0x7f60fde34f60>, '__rand__': <wrapper_descriptor at remote 0x1f7d8c0>, '__doc__': None, '__xor__': <wrapper_descriptor at remote 0x1f7d910>, '__and__': <wrapper_descriptor at remote 0x1f7d870>, '__le__': <wrapper_descriptor at remote 0x1f7d690>, '__or__': <wrapper_descriptor at remote 0x1f7d9b0>, '__gt__': <wrapper_descriptor at remote 0x1f7d780>, '__hash__': <wrapper_descriptor at remote 0x1f7d5f0>, '__index__': <wrapper_descriptor at remote 0x1f7da50>, '__lt__': <wrapper_descriptor at remote 0x1f7d640>, '__eq__': <wrapper_descriptor at remote 0x1f7d6e0>, '__rxor__': <wrapper_descriptor at remote 0x1f7d960>, '__ge__': <wrapper_descriptor at remote 0x1f7d7d0>}, visit=0x7f61067db1f0 <visit_decref>, arg=0x0)
    at /usr/src/debug/Python-2.7.5/Objects/dictobject.c:2123
#2  0x00007f61067db5b7 in subtract_refs (containers=<optimized out>) at /usr/src/debug/Python-2.7.5/Modules/gcmodule.c:456
#3  collect (generation=generation@entry=2) at /usr/src/debug/Python-2.7.5/Modules/gcmodule.c:999
#4  0x00007f61067dc078 in PyGC_Collect () at /usr/src/debug/Python-2.7.5/Modules/gcmodule.c:1514
#5  0x00007f61067c9339 in Py_Finalize () at /usr/src/debug/Python-2.7.5/Python/pythonrun.c:444
#6  0x00007f61067da545 in Py_Main (argc=<optimized out>, argv=<optimized out>) at /usr/src/debug/Python-2.7.5/Modules/main.c:665
#7  0x00007f6105a07af5 in __libc_start_main () from /usr/lib64/libc.so.6
#8  0x0000000000400721 in _start ()
```"
7716,Upgrade Tensorflow-1.0.0rc2-cp35-cp35m-win_amd64.whl on Windows and get warning,"I upgrade Tensorflow-1.0.0rc2-cp35-cp35m-win_amd64.whl, CPU version on Windows server 2012 R2

pip install --upgrade   
http://ci.tensorflow.org/view/Nightly/job/nightly-win/85/DEVICE=cpu,OS=windows/artifact
/cmake_build/tf_python/dist/tensorflow-1.0.0rc2-cp35-cp35m-win_amd64.whl

It shows 
""Successfully installed tensorflow-1.0.0rc2 werkzeug-0.11.15""

But I get the following warning when I run the hello example, please advise what's the problem.

>>> import tensorflow as tf
>>> hello = tf.constant('Hello, TensorFlow')
>>> sess = tf.Session()
2017-02-20 16:53:15.825086: W c:\tf_jenkins\home\workspace\nightly-win\device\cp
u\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow li
brary wasn't compiled to use SSE instructions, but these are available on your m
achine and could speed up CPU computations.
2017-02-20 16:53:15.826037: W c:\tf_jenkins\home\workspace\nightly-win\device\cp
u\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow li
brary wasn't compiled to use SSE2 instructions, but these are available on your
machine and could speed up CPU computations.
2017-02-20 16:53:15.827289: W c:\tf_jenkins\home\workspace\nightly-win\device\cp
u\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow li
brary wasn't compiled to use SSE3 instructions, but these are available on your
machine and could speed up CPU computations.
2017-02-20 16:53:15.828478: W c:\tf_jenkins\home\workspace\nightly-win\device\cp
u\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow li
brary wasn't compiled to use SSE4.1 instructions, but these are available on you
r machine and could speed up CPU computations.
2017-02-20 16:53:15.829644: W c:\tf_jenkins\home\workspace\nightly-win\device\cp
u\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow li
brary wasn't compiled to use SSE4.2 instructions, but these are available on you
r machine and could speed up CPU computations.
2017-02-20 16:53:15.830997: W c:\tf_jenkins\home\workspace\nightly-win\device\cp
u\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow li
brary wasn't compiled to use AVX instructions, but these are available on your m
achine and could speed up CPU computations.
2017-02-20 16:53:15.832161: W c:\tf_jenkins\home\workspace\nightly-win\device\cp
u\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow li
brary wasn't compiled to use AVX2 instructions, but these are available on your
machine and could speed up CPU computations.
2017-02-20 16:53:15.833336: W c:\tf_jenkins\home\workspace\nightly-win\device\cp
u\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow li
brary wasn't compiled to use FMA instructions, but these are available on your m
achine and could speed up CPU computations.
>>> print(sess.run(hello))
b'Hello, TensorFlow'"
7715,Feature: nadam optimizer ,"Adam with nesterov momentum. 

Keras has it: https://keras.io/optimizers/#nadam
TF implementation (old and doesn't quite work) : https://github.com/tdozat/Optimization/blob/master/tensorflow/nadam.py"
7714,[Distributed Training] Model parallelism support,"Hi,
Looking in the [TF distributed training tutorial](https://www.tensorflow.org/versions/r0.10/how_tos/distributed/) it provides distributed training via data parallelism example only. Does TensorFlow currently support model parallelism? If yes where are docs/examples for that?

Thanks,
Ogail"
7713,"Attempt to ""import tensorflow as tf"" on MacOS 10.10.5, Python 2.7.10, numpy 1.12.0"," - Installed tensorflow using pip on MacOS Yosemite 10.10.5, running Python 2.7.10, with current numpy 1.12.0.  Python runs ok, but attempt to ""import tensorflow as tf"", as per the documentation for TensorFlow install on Mac OS, generates exactly the same error sequence as posted already on github by another user.  The other users posting was closed without comment.  The specific error message is:
      AttributeError: type object 'NewBase' has no attribute 'is_abstract'
The url of the other user, which documented the traceback (which is essentially same as I am getting) is:
     https://github.com/tensorflow/tensorflow/issues/5707

If I can resolve or workaround this, I will post the fix here.   "
7708,Visualize experiment arguments in Tensorboard,"**Feature request**

I am wondering if it's possible to store the arguments that we pass to the training script to be stored as part of other summaries, and have a way to see those arguments/parameters as notes in a separate tab in Tensorboard.
Reason: It's often hard to track individual training runs and relate them to the training/network config if you change training and network parameters frequently which always happens during hyperparameter search. We can do it manually too but if included in Tensorboard, it would make it one go-to visualizer and comparison tool for everything.
One method of doing this that comes to mind is using the `tf.app.flags.FLAGS` arguments. This would keep everything standardized. Or we could also support `argparse` directly.
Is this something in line with Tensorboard's philosophy or is it too straightforward to be a special feature?"
7707,estimator with batch size and data feeding,"Could you provide an example of using the high-level API Estimators with placeholders and feeding batches  like for a basic use:
for step in xrange(max_steps):
   batch_of_inputs,batch_of_targets= get_batch_from_disk(step) # e.g. batches are stored as list where step is and index of the list
    feed_dict = {x:batch_of_inputs,y:batch_of_targets}
    _, loss_value = sess.run([train_op, loss],
                             feed_dict=feed_dict)
How to do the same with Estimator API?
Estimator takes batch_size, steps, input_fuc or feed_fun as an argument and but it is not clear for me how to implement a function which will load data of batch size  e.g. in every iteration from disk?"
7706,Trouble installing python wheel for tensorflow on Windows 7,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

My problem:
http://stackoverflow.com/questions/42280894/tensorflow-wheel-install-not-supported?noredirect=1#comment71724208_42280894

Following the instructions:
https://www.tensorflow.org/install/install_windows

I had Python 3.5.2 installed, but following the instructions to install tensorflow according to the website automatically upgrades python to 3.6. The wheel is not supported.


### Environment info
Operating System:

Windows 7 

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

Not applicable yet, as I am still trying to install the software, prior to running the scripts.

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.


pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.1-cp35-cp35m-win_amd64.whl


If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7705,ImportError: No module named '_pywrap_tensorflow' (MSVCP140.dll is present),"I installed the nightly build windows 64bit release of tensorflow from http://ci.tensorflow.org/view/Nightly/job/nightly-win/85/DEVICE=gpu,OS=windows/
using pip install in the Anaconda distribution of Python 3.5 (v4.1.1.0)

When I try to import tensorflow, I get the following error:

``` Traceback (most recent call last):
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 21, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow')
  File ""C:\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 21, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow')
  File ""C:\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'

Failed to load the native TensorFlow runtime. 
```

I checked the msvcp140.dll and it seems to be present in multiple locations of my %PATH% (in the anaconda folder, in system32, sysWOW64 and some other locations).

I also have environment variables setup for the CUDA path.

The issue filed here: https://github.com/tensorflow/tensorflow/issues/7529 is essentially the same as mine but the user resolved it by shifting development to a VPS running Ubuntu. It still doesn't solve the problem though.

Any help would be appreciated! :)
"
7704,Op type not registered 'TensorArrayV3',"I've been building graph in python and deploy to c++ since r0.10. After upgrading to r1.0, I've retrained the model, re-created the graph, and re-link the library all in r1.0. However, I got this error when try to use the graph in the c++:
 `Not found: Op type not registered 'TensorArrayV3'` 
Here is how I created the tensorflow shared lib:
```
cc_binary(
    name = ""libtensorflow_all.so"",
    linkshared = 1,
    deps = [
        ""//tensorflow/cc:cc_ops"",
        ""//tensorflow/core:framework_internal"",
        ""//tensorflow/core:tensorflow"",
    ],
)
```
I don't find `TensorArrayV3` symbol in the official lib package either ( e.g: `https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-darwin-x86_64-1.0.0.tar.gz`)

Can you please advise how to fix it?"
7703,fail building tensorflow. Error is error trying to exec 'cc1plus',"
I try to build tensorflow from the source code, as https://www.tensorflow.org/install/install_sources shows. But it fails at basel building like following.

commands: bazel build --config=opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
INFO: Found 1 target...
ERROR: /local/xian_titan/.cache/bazel/bazel_xian_titan/d96b63d3ff03346cfbf37aac2a75fe2c/external/protobuf/BUILD:230:1: C++ compilation of rule '@protobuf//:js_embed' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command
(cd /local/xian_titan/.cache/bazel/bazel_xian_titan/d96b63d3ff03346cfbf37aac2a75fe2c/execroot/tensorflow && 
exec env - 
LD_LIBRARY_PATH=/usr/local/lib/:/usr/local/cuda-8.0/lib64 
PATH=/usr/local/cuda-8.0/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/local/xian_titan/bin 
external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -MD -MF bazel-out/host/bin/external/protobuf/objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.d '-frandom-seed=bazel-out/host/bin/external/protobuf/objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.o' -iquote external/protobuf -iquote bazel-out/host/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE=""redacted""' '-D__TIMESTAMP=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c external/protobuf/src/google/protobuf/compiler/js/embed.cc -o bazel-out/host/bin/external/protobuf/_objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
gcc: error trying to exec 'cc1plus': execvp: No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 2.283s, Critical Path: 1.68s

I found some other people also had similar problems before. But I cannot successfully address the problem from previous threads.

I tried bazel 0.4.4 and 0.4.2 releases, they have the same issues.

My system is Red Hat Enterprise Linux Workstation release 6.7 (Santiago),
java version ""1.8.0_121"",
gcc (GCC) 4.8.2 20140120 (Red Hat 4.8.2-15)
g++ (GCC) 4.8.2 20140120 (Red Hat 4.8.2-15)

Hope someone can figure out where's wrong"
7702,"dynamic_rnn_decoder returns shape [?, batch_size, cell.output_size]","According to [the docs](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder),  the dynamic_rnn_decoder returns a tuple contaning `outputs`, which is a Tensor of shape `[max_time, batch_size, cell.output_size]`(provided `time_major==True`).

In my case, however, the first dimension of that Tensor is returned as underspecified (Dimension `?`), and in fact depending on the provided batch when running the RNN. 

If this is the intended behaviour, it should probably be highlighted in the documentation that `max_time` is variable.

Reproduce with:

```import tensorflow as tf
import numpy as np

# toy data, timesteps between 1 and 10
timesteps = np.random.randint(1, 11, [10])
X=np.random.randint(0, 20, [10,10,1])

batch_size = 2
max_ts = 10
inputs = tf.placeholder(tf.float32, 
                        (max_ts, batch_size, 1), name=""X_in"")

cell_fw = tf.contrib.rnn.LSTMCell(50)
cell_bw = tf.contrib.rnn.LSTMCell(50)
cell_dec = tf.contrib.rnn.LSTMCell(50)

seq_lens = tf.placeholder(tf.int32, batch_size, name=""seq_lens"")

enc_outputs, states = tf.nn.bidirectional_dynamic_rnn(
    cell_fw, cell_bw, inputs, time_major=True, sequence_length=seq_lens, dtype=tf.float32)

decoder_inp = tf.concat(enc_outputs, axis=2) 

attention_states = tf.zeros([batch_size, 1, cell_dec.output_size],
                                    name=""attention_states"")

att_keys, att_vals, att_score_fn, att_construct_fn = \
            tf.contrib.seq2seq.prepare_attention(attention_states,
                                                 attention_option=""luong"",
                                                 num_units=50)

dynamic_fn_train = tf.contrib.seq2seq.attention_decoder_fn_train(
            states[0], att_keys, att_vals, att_score_fn, att_construct_fn)

outputs, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(
            cell_dec, dynamic_fn_train, decoder_inp, time_major=True,
            sequence_length=seq_lens)

with tf.Session() as sess:
    feed_dict = {inputs: X[:,:2,:], seq_lens: ts[:2]}
    sess.run(tf.global_variables_initializer())
    out = sess.run(outputs, feed_dict=feed_dict)
    print(out.shape[0])
```

The very last print statement will show that the first output dimension is not max_ts, but the max timestep of the batch (<= 10)"
7701,"ERROR: Evaluation of query ""deps((//tensorflow/... - //tensorflow/examples/android/...))"" failed","root@cjliux-comp:/usr/local/lib/python2.7/dist-packages/tensorflow-1.0.0# ./configure 
Please specify the location of python. [Default is /usr/bin/python]: 
Please specify optimization flags to use during compilation [Default is -march=native]: 
Do you wish to use jemalloc as the malloc implementation? (Linux only) [Y/n] y
jemalloc enabled on Linux
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] n
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] n
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] n
No XLA JIT support will be enabled for TensorFlow
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
  /usr/local/caffe/python
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]

Using python library path: /usr/local/lib/python2.7/dist-packages
Do you wish to build TensorFlow with OpenCL support? [y/N] n
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] y
CUDA support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 5
Please specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 6.1
INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
.......
ERROR: infinite symlink expansion detected
[start of symlink chain]
/usr/local/lib/python2.7/dist-packages
/usr/local/lib/python2.7/dist-packages/tensorflow-1.0.0/util/python/python_lib
[end of symlink chain]
.
ERROR: Evaluation of query ""deps((//tensorflow/... - //tensorflow/examples/android/...))"" failed: errors were encountered while computing transitive closure.

Do any one know what that mean and how to fix it?"
7700,Can't enforce shape invariants with TensorArrays in while_loop,"I can't enforce shape invariants in a while_loop if one of the inputs is a TensorArray. Here's a minimal example: 
```
import tensorflow as tf

def body(i,ta):
    ta = ta.write(i,1.0)
    return (i+1,ta)

arr_size = 10
ta = tf.TensorArray(tf.float32, size=arr_size)

i = tf.constant(0,tf.int32)
input = (i,ta)
cond = lambda i,_ : i < arr_size
output = tf.while_loop(cond, body,input,shape_invariants=(i.get_shape(),tf.TensorShape(arr_size)))

#works fine without shape_invariants:
#output = tf.while_loop(cond, body,input)

mat = output[1].stack()
sess = tf.InteractiveSession()
print(mat.eval())
```

The code above works fine if the while_loop is not fed shape_invariants. Using shape_invariants though, I get the following error: 
```
ValueError: The shape invariant specified for TensorArray:1 is not compatible with the initial shape of the loop variable. It enters the loop with shape <unknown>, but the specified shape invariant is (10,).
```

Am I doing something wrong or is this a bug?

Thanks! "
7699,Error building from source,"Hi,

I'm trying to build from master branch and getting this error during the ./configure step

`Please specify the location of python. [Default is /Users/lucasliu/anaconda/bin/python]: 
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] N
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] N
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] N
No XLA JIT support will be enabled for TensorFlow
Found possible Python library paths:
  /Users/lucasliu/anaconda/lib/python2.7/site-packages
Please input the desired Python library path to use.  Default is [/Users/lucasliu/anaconda/lib/python2.7/site-packages]

Using python library path: /Users/lucasliu/anaconda/lib/python2.7/site-packages
Do you wish to build TensorFlow with OpenCL support? [y/N] N
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] N
No CUDA support will be enabled for TensorFlow
Configuration finished

ERROR: /Users/lucasliu/tensorflow/tensorflow/models/syntaxnet/syntaxnet/BUILD:174:1: no such package 'util/utf8': BUILD file not found on package path and referenced by '//tensorflow/models/syntaxnet/syntaxnet:segmenter_utils'.
ERROR: /Users/lucasliu/tensorflow/tensorflow/models/syntaxnet/syntaxnet/BUILD:95:1: no such package 'util/utf8': BUILD file not found on package path and referenced by '//tensorflow/models/syntaxnet/syntaxnet:utils'.
ERROR: /Users/lucasliu/tensorflow/tensorflow/models/syntaxnet/syntaxnet/BUILD:162:1: no such package 'util/utf8': BUILD file not found on package path and referenced by '//tensorflow/models/syntaxnet/syntaxnet:char_properties'.
ERROR: Evaluation of query ""deps(((//tensorflow/... - //tensorflow/contrib/nccl/...) - //tensorflow/examples/android/...))"" failed: errors were encountered while computing transitive closure.`

Operating System: Mac OS Sierra Version 10.12.3
Bazel: 0.4.4-homebrew
Python: 2.7 

tf/master
No CUDA
No cuDNN

I suspect this is due to my network timeout to bazel mirror site. Could you please advise?"
7698,Quantize nodes via Transform Graph tool cause error,"I am trying to quantize my model by following this [README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md)

I have faced with issue **No attr named 'T' in NodeDef:** when I run **transform_graph** tool with **quantize_nodes** option both for my model and example model (Inception V3).

```
path/to/transform_graph --in_graph=model.pb --out_graph=optimized.pb --inputs='input:0' --outputs='regression:0,classification:0' --transforms='quantize_weights quantize_nodes'
```

Here is relevant part of output:

```
2017-02-20 17:14:19.944778: I tensorflow/tools/graph_transforms/transform_graph.cc:257] Applying quantize_nodes
2017-02-20 17:14:20.599637: E tensorflow/tools/graph_transforms/transform_graph.cc:203] No attr named 'T' in NodeDef:
	 [[Node: pool = MaxPool[ksize=[1, 3, 3, 1], padding=""VALID"", strides=[1, 2, 2, 1]](pool/control_dependency)]]
```

### Environment info

Doesn't seems too important. Tested on two machines with different configurations (referred as M for Mac and U for Ubuntu below).
 
Operating System:
M: macOS Sierra 10.12.3
U: Ubuntu 16.04.2 LTS x86_64

TF has been installed from source. Commit hash:
M: b6f16b8166e3a7761f607be66d46acbd37dfaf43
U: c56c873fbaf976d26d487ad57c8efbc87f05331c

Bazel version / build label
M: 0.4.4-homebrew
U: 0.4.4

### Minimal reproducible example
Download & unpack inception V3. Run the tool (modify path to tool and model if necessary).


```
curl http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz -o /tmp/inceptionv3.tgz
tar xzf /tmp/inceptionv3.tgz -C /tmp/
bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=/tmp/classify_image_graph_def.pb --out_graph=optimized.pb --inputs='input:0' --outputs='regression:0,classification:0' --transforms='quantize_weights quantize_nodes'
```

### What other attempted solutions have you tried?
I have tried to run quantisation on different OS (Mac and Ubuntu) and slightly different revisions of master. Also, initially, I have tried to quantize nodes of my own model, not Inception. In all cases result is the same: error."
7697,How to upgrade Tensorflow from V0.10.0 to V1.0 ?,"Hi ,dear
    I installed tensorflow V0.10.0 by compiling source code in 2016.   
    Now Is there an simplest way for me to upgrade tensorflow from V0.10.0 to V1.0?
    Thank you & Best Regards,

### Environment info
Operating System: Ubuntu16.04

syj@syj-dl:~/tensorflow$ cat /etc/issue
Ubuntu 16.04 LTS \n \l

syj@syj-dl:~/tensorflow$ which python
/home/syj/anaconda2/bin/python

syj@syj-dl:~/tensorflow$ python --version
Python 2.7.12 :: Anaconda custom (64-bit)

Installed version of CUDA and cuDNN: CUDA8.0 CUDNN5.0
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

syj@syj-dl:/usr/local/cuda-8.0/lib64$ ls libcud*
libcudadevrt.a    libcudart.so.8.0.27  libcudnn.so.5
libcudart.so      libcudart_static.a   libcudnn.so.5.1.5
libcudart.so.8.0  libcudnn.so          libcudnn_static.a


syj@syj-dl:~/tensorflow/tensorflow/models/rnn$ python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally
0.10.0

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`)
syj@syj-dl:~/tensorflow$ git rev-parse HEAD
69d67717f7b3da135b8904822838823658da183f

2. The output of `bazel version`
syj@syj-dl:~$ bazel version
Build label: 0.3.1
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jul 29 09:09:52 2016 (1469783392)
Build timestamp: 1469783392
Build timestamp as int: 1469783392

"
7696,"freeze_graph inspect_checkpoint,  Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator","### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Nothing directly applicable to the observed error.

### Environment info
TF 0.12.1, Windows 10/64, Python 3.5/64

Installed version of CUDA and cuDNN: 
8.0, 5.1


### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

**inspect_checkpoint**

lib\python\python lib\tensorflow_cpu\tensorflow\python\tools\inspect_checkpoint.py --file_name model.ckpt-250514.data-00000-of-00001 --tensor_name-''

2017-02-20 08:23:19.291800: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\util\tensor_slice_reader.cc:95] Could not open model.ckpt-250514.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
Unable to open table file model.ckpt-250514.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?



**freeze_graph**

Similar error
tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file model.ckpt-250514.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
         [[Node: save/RestoreV2_445 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_445/tensor_names, save/RestoreV2_445/shape_and_slices)]]

DataLossError (see above for traceback): Unable to open table file C:\g\vx\data\jhole\model\model.ckpt-250514.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
         [[Node: save/RestoreV2_780 = RestoreV2[dtypes=[DT_INT64], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_780/tensor_names, save/RestoreV2_780/shape_and_slices)]]

### What other attempted solutions have you tried?
1. A few different checkpoint files.
2. Tried freeze_graph using TF 1.0, same result

### Logs or other output that would be helpful
Messages similar ""Similar error"" above is repeated many times.
"
7695,How to use the Distributed Tensorflow,"I have three node, which two use the CPU-mode tensorflow, and the left use the GPU-mode tensorflow.
Some website may just use the official example, but I don't know how to execute the python code on the terminal, some use themselves example. but I cannot run it successful. Can you tell me how to write the python code and how to execute it on my terminal. Should I use the docker?
Anyone can teach me?"
7694,TypeError: run() got an unexpected keyword argument 'feedis_dict',"I get this error when trying to add the feedis_dict to a self.sess.run command:

`Traceback (most recent call last):
  File ""spritegen.py"", line 71, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""spritegen.py"", line 54, in main
    dcgan.train(FLAGS)
  File ""/home/lewis/Documents/Sprite Generator/Sprite-Generator/dcgan.py"", line 211, in train
    summary_str = self.sess.run([dis_optim, self.dis_sum],feedis_dict = {self.inputs: batch_images, self.z: batch_z})
TypeError: run() got an unexpected keyword argument 'feedis_dict'
`
Any suggestions as to why feedis_dict is unexpected ? I believe im using tensorflow gpu 1.0.0
"
7693,How to build tensorflow to use SSE3 instructions?,"When I am running tensorflow code, it prompts that `The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.`. I have installed tensorflow using pip and don't know how to deal with that. Any one can help with me?"
7690,Docs about installing Tensorflow for Ubuntu don't always distinguish between python and python3,"In this page [Installing TensorFlow on Ubuntu ](https://www.tensorflow.org/install/install_linux), there are some commands to install Python in Ubuntu. Tough Ubuntu distinguishes between python (2.7) and python3 (3.n) packages. So I think that the page should be updated to distinguish the two cases, as using the current procedure may result in the creation of a python (2.7) environment when a python3 was needed or preferred.

## Update proposal
### [virtualenv](https://www.tensorflow.org/install/install_linux#InstallingVirtualenv)

At the point 1:
```
sudo apt-get install python-pip python-dev python-virtualenv # for Python 2.7
sudo apt-get install python3-pip python3-dev python3-virtualenv # for Python 3.n
```
At the point 2:
```
python -m virtualenv --system-site-packages -p python targetDirectory # for Python 2.7
python3 -m virtualenv --system-site-packages -p python3 targetDirectory # for Python 3.n
```

### [""native"" pip](https://www.tensorflow.org/install/install_linux#InstallingNativePip)

Prerequisites:
```
sudo apt-get install python-pip python-dev # for Python 2.7
sudo apt-get install python3-pip python3-dev # for Python 3.n
```
"
7689,"tf.nn.softmax errors out, when dim=<dim_size - 1> instead of -1","### Environment info
Operating System:
MacOS 10.12.1

Installed version of CUDA and cuDNN: 
lrwxr-xr-x  1 root  wheel     50 Sep 26 15:00 /usr/local/cuda/lib/libcudart.8.0.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib
lrwxr-xr-x  1 root  wheel     47 Oct 24 21:11 /usr/local/cuda/lib/libcudnn.5.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.5.dylib

Tensorflow version 0.12.1

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```
def test_softmax():
  with tf.variable_scope(""test_coattention_layer""):
    doc1_placeholder = tf.placeholder(tf.float32, shape=(None, 4, 3))

  init = tf.global_variables_initializer()

  with tf.Session() as session:
    session.run(init)
    input = np.array(range(2 * 4 * 3), dtype=np.float32).reshape((2, 4, 3))

    softmax1 = tf.nn.softmax(doc1_placeholder, dim=-1) # <======== THIS WORKS
    softmax2 = tf.nn.softmax(doc1_placeholder, dim=2) # <======== THIS BREAKS

    print(""doc1 = "" + str(input))
    softmax1_out = session.run(softmax1, feed_dict={doc1_placeholder: input})
    softmax2_out = session.run(softmax2, feed_dict={doc1_placeholder: input})
    print(""softmax1_out = "" + str(softmax1_out))
    print(""softmax2_out = "" + str(softmax2_out))

```

Error message:

InvalidArgumentError (see above for traceback): Requires start <= limit when delta > 0: 3/2
	 [[Node: range_1 = Range[Tidx=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](range_1/start, Sub, range_1/delta)]]
"
7687,tensorflow support sgd with monment Optimizer??,"tensorflow support sgd with monment???

is it tf.train.MomentumOptimizer"
7686,Looking for a function to replace sklearn.mixture.GaussianMixture.predict_proba(X),"In sklearn, I can use `predict_proba(X)` if I want to Predict posterior probability of data per each component.

However, I cannot find a similar function in 'tensorflow gmm_ops'

Did anyone find that before?
"
7685,"Image problems in Tensorflow documentation at tensorflow.org, Error 404","I'm not sure I should put the issue here. But when I clicked ""Issue tracker"" on tensorflow.org, it redirects me here.

I realized that the directory structure for TF documentation has changed a lot for TF 1.0 on http://www.tensorflow.org. Now there are problems for web pages with images, like https://www.tensorflow.org/api_docs/python/tf/segment_max?hl=bn. A 404 error is reported while loading the images."
7684,embedding variable in ptb_word_lm.py,"In ptb_word_lm.py I see that for word2vec vectors we are doing:

embedding = tf.get_variable(
          ""embedding"", [vocab_size, size], dtype=data_type())
      inputs = tf.nn.embedding_lookup(embedding, input_.input_data)

but where is the variable embedding created? Is it random or is it pretrained?"
7683,Package not Reslove.,"Hello,

I am using TensorFlow Android Camera Demo. In TensorFlowInferenceInterface class there are some package not reslove like:

import org.tensorflow.DataType;
import org.tensorflow.Graph;
import org.tensorflow.Session;
import org.tensorflow.Tensor;
import org.tensorflow.TensorFlow;

Can any one help to find out where some thing is missing."
7682,tensorflow support SGD with nesterov momentum in tensorflow??,tensorflow support SGD with nesterov momentum in tensorflow??
7681,DOCS for offsets in extract_glimpse don't match Implementation,"Docs state:
>    offsets: A `Tensor` of type `float32`.
      A 2-D integer tensor of shape `[batch_size, 2]` containing
      the x, y locations of the center of each window.

whereas [the implementation states](https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/core/kernels/attention_ops.cc#L92):
>      // calling TensorFlow operates with (y,x) as indices.

Notice x,y vs y,x

Here is a rough demo of the issue:
```python
arr = np.zeros((1,5,5,1), dtype=np.float32)
arr[0, 3,2] = 1
arr = tf.constant(arr)
glim = tf.image.extract_glimpse(arr, (1,1), offsets=tf.constant([(2,3)], dtype=tf.float32)[:,::-1] + 1, normalized=False, centered=False)
sess.run(glim)
```
```
> array([[[[ 1.]]]], dtype=float32)
```

It appears that the docs are wrong and that the comment is right."
7680,CUDA_ERROR_OUT_OF_MEMORY with tf.contrib.learn (basic linear regression model),"### Environment info
Operating System: Ubuntu 16.04,  GeForce GTX TITAN X

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
ls /usr/local/cuda-8.0/lib64/libcud*
/usr/local/cuda-8.0/lib64/libcudadevrt.a
/usr/local/cuda-8.0/lib64/libcudart.so
/usr/local/cuda-8.0/lib64/libcudart.so.8.0
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudnn.so
/usr/local/cuda-8.0/lib64/libcudnn.so.5
/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10
/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5
/usr/local/cuda-8.0/lib64/libcudnn_static.a

### If installed from binary pip package, provide:

1) Installed with pip tensorflow-gpu
2) ~ python -c ""import tensorflow; print(tensorflow.__version__)
1.0.0

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
Using the provided tf.contrib.learn model in get_started/get_started documentation
```
import tensorflow as tf
import numpy as np

features = [tf.contrib.layers.real_valued_column(""x"", dimension=1)]
estimator = tf.contrib.learn.LinearRegressor(feature_columns=features)
x = np.array([1., 2., 3., 4.])
y = np.array([0., -1., -2., -3.])
input_fn = tf.contrib.learn.io.numpy_input_fn({""x"":x}, y, batch_size=4, num_epochs=1000)
estimator.fit(input_fn=input_fn, steps=10)

```

The Core model works fine.


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

```
python test-oom.py 
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpew3U6z
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WARNING:tensorflow:From /home/gajop/projekti/ubuntu-ranking-dataset-creator/env/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:01:00.0
Total memory: 11.92GiB
Free memory: 11.52GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0)
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 11.92G (12799180800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
```"
7679,How to reproduce the 58x performance improving claimed by TF r1.0?,TF r1.0 claims that it can achieve 58x performance improving by 64 GPUs for Inception v3. Are there any guidelines or sample code to help us to reproduce the results?
7677,"""Getting started"" first tf.contrib.learn sample fails","The first tf.contrib.learn sample at
https://www.tensorflow.org/get_started/get_started
fails to run.

The  code is right under
""Basic usage
Notice how much simpler the linear regression program becomes with tf.contrib.learn:""

Output:

WARNING:tensorflow:Using temporary folder as model directory: C:\Users\John\AppData\Local\Temp\tmpahtnt89p
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WARNING:tensorflow:From C:\Users\John\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\head.py:521: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.
2017-02-19 11:53:57.997760: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2017-02-19 11:53:58.007657: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2017-02-19 11:53:58.008177: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2017-02-19 11:53:58.008726: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-02-19 11:53:58.009199: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-02-19 11:53:58.009591: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-02-19 11:53:58.010795: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-02-19 11:53:58.011384: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WARNING:tensorflow:From C:\Users\John\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\head.py:521: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.


------------------
(program exited with code: 0)

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
None

### Environment info
Operating System: Windows 10
Python 3.5.3 (v3.5.3:1880cb95a742, Jan 16 2017, 16:02:32) [MSC v.1900 64 bit (AMD64)] on win32

Installed version of CUDA and cuDNN: 
???

If installed from binary pip package, provide:

1. A link to the pip package you installed:
pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.0rc2-cp35-cp35m-win_amd64.whl (to fix the BestSplits"" error)

2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
1.0.0-rc2


"
7675,Error message could be improved,"If your try and initialize a `dynamic_rnn` without specifying an `initial_state` **OR** `dtype`, you get this enigmatic error message:

`If no initial_state is provided, dtype must be`

I don't think that message is very clear. Something better would be along the lines of, 

`If no initial_state is provided, a dtype must be specified`

This error is thrown at 
tensorflow/tensorflow/python/ops/rnn.py, line 518"
7669,"The tutorial ""Logging and Monitoring Basics with tf.contrib.learn"" has error.","When I used the code snippet in the section ""Customizing the Evaluation Metrics with MetricSpec"" of the tutorial [Logging and Monitoring Basics with tf.contrib.learn](https://www.tensorflow.org/get_started/monitors). the code snippet is 

```python
validation_metrics = {
    ""accuracy"":
        tf.contrib.learn.metric_spec.MetricSpec(
            metric_fn=tf.contrib.metrics.streaming_accuracy,
            prediction_key=tf.contrib.learn.prediction_key.PredictionKey.
            CLASSES),
    ""precision"":
        tf.contrib.learn.metric_spec.MetricSpec(
            metric_fn=tf.contrib.metrics.streaming_precision,
            prediction_key=tf.contrib.learn.prediction_key.PredictionKey.
            CLASSES),
    ""recall"":
        tf.contrib.learn.metric_spec.MetricSpec(
            metric_fn=tf.contrib.metrics.streaming_recall,
            prediction_key=tf.contrib.learn.prediction_key.PredictionKey.
            CLASSES)
}
```

My tensorflow version is r1.0 . When I run my program, it print the following error:

```shell
$ python iris.py 
Traceback (most recent call last):
  File ""iris.py"", line 72, in <module>
    tf.app.run()
  File ""/Library/Python/2.7/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""iris.py"", line 24, in main
    ""accuracy"": tf.contrib.learn.metric_spec.MetricSpec(
AttributeError: 'module' object has no attribute 'metric_spec'
```

I found that the class `tf.contrib.learn.metric_spec.MetricSpec` has been renamed to [`tf.contrib.learn.MetricSpec`](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/MetricSpec). 

The class `tf.contrib.learn.prediction_key.PredictionKey` also has been renamed to [`tf.contrib.learn.PredictionKey`](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/PredictionKey)."
7668,Building from source - PYTHONPATH not respected,"So I'm trying to build tensorflow from source, main reason is that I do not have root access and the `GLIBC` version was incompatible with the binaries. Additionally, I can not install packages on the python3.

Steps so far:
1. Build `gcc-4.9.1` from source - SUCCESS
2. Build `bazel-0.4.4` from source - SUCCESS
3. Install all CUDA stuff - SUCCESS
4. Install extra packages in a separate directory (protobuf, nose, argparse, numpy, six etc..) - SUCCESS
5. Build `tensorflow` with the `bazel` binary - FAIL

OS:
```
LSB Version:	:base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch
Distributor ID:	CentOS
Description:	CentOS release 6.5 (Final)
Release:	6.5
Codename:	Final
```
The new `gcc` is installed in `/share/apps/barber/system/` together with the other library dependencies `gcc` needs (gmp, mpfr, mpc, elf). 

The `bazel` binary is also copied into `/share/apps/barber/system/bin`

CUDA is installed under `/share/apps/barber/cuda` and CuDNN under `/share/apps/barber/cudnn`. 

The python I'm using is not the default one and lives in `/share/apps/python-3.6.0-shared/bin/python3`. The alternative directory for my own packages is `/share/apps/barber/system/lib/python3.6/site-packages/` (it contains protobuf, argparse, nose etc...).

Given all this my environment has the following modified deifnitions:
```
export BARBER_PATH=/share/apps/barber
export LD_LIBRARY_PATH=${BARBER_PATH}/system/lib/:${BARBER_PATH}/system/lib64/:/opt/gridengine/lib/linux-x64:/opt/openmpi/lib/
export PATH=${BARBER_PATH}/system/bin/:$PATH
export CC=${BARBER_PATH}/system/bin/gcc
export CXX=${BARBER_PATH}/system/bin/g++
export CUDA_ROOT=${BARBER_PATH}/cuda
export CUDA_HOME=${CUDA_ROOT}
export CUDNN_PATH=${BARBER_PATH}/cudnn
export CPATH=${CUDNN_PATH}/include:$CPATH
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${CUDA_ROOT}/lib64/:${CUDA_ROOT}/nvvm/lib64/:${CUDA_ROOT}/extras/CUPTI/lib64:${CUDNN_PATH}/lib64/
export PYTHONPATH=${BARBER_PATH}/system/lib/python3.6/site-packages/
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/share/apps/python-3.6.0-shared/lib/
alias python=/share/apps/python-3.6.0-shared/bin/python3
alias pip=/share/apps/python-3.6.0-shared/bin/pip
```

Getting back to the tensorflow build, I'm selecting corretly the python to use and the gcc to use when using CUDA. The `./configure` completes fine and works (I think I only had to change something to `_async`). However, when I try to run 
```
 bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures
```
I get the following error:
```
WARNING: Output base '/home/abotev/.cache/bazel/_bazel_abotev/9abd1d3abe11b8f0417e465a29633fc7' is on NFS. This may lead to surprising failures and undetermined behavior.
INFO: Found 1 target...
ERROR: /home/abotev/.cache/bazel/_bazel_abotev/9abd1d3abe11b8f0417e465a29633fc7/external/farmhash_archive/BUILD.bazel:12:1: C++ compilation of rule '@farmhash_archive//:farmhash' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/abotev/.cache/bazel/_bazel_abotev/9abd1d3abe11b8f0417e465a29633fc7/execroot/tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/share/apps/barber/system/lib/:/share/apps/barber/system/lib64/:/opt/gridengine/lib/linux-x64:/opt/openmpi/lib/:/share/apps/barber/cuda//lib64/:/share/apps/barber/cuda//nvvm/lib64/:/share/apps/barber/cuda//extras/CUPTI/lib64:/share/apps/barber/cudnn_5_1/lib64/:/share/apps/barber/arrayfire-3/lib/:/share/apps/python-3.6.0-shared/lib/ \
    PATH=/share/apps/java/bin/:/share/apps/barber/system/bin/:/opt/openmpi/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/bio/ncbi/bin:/opt/bio/mpiblast/bin:/opt/bio/EMBOSS/bin:/opt/bio/clustalw/bin:/opt/bio/tcoffee/bin:/opt/bio/hmmer/bin:/opt/bio/phylip/exe:/opt/bio/mrbayes:/opt/bio/fasta:/opt/bio/glimmer/bin:/opt/bio/glimmer/scripts:/opt/bio/gromacs/bin:/opt/bio/gmap/bin:/opt/bio/tigr/bin:/opt/bio/autodocksuite/bin:/opt/bio/wgs/bin:/opt/eclipse:/opt/ganglia/bin:/opt/ganglia/sbin:/usr/java/latest/bin:/opt/rocks/bin:/opt/rocks/sbin:/opt/gridengine/bin/linux-x64:/home/abotev/bin \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -MD -MF bazel-out/host/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.d '-frandom-seed=bazel-out/host/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.o' -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c external/farmhash_archive/src/farmhash.cc -o bazel-out/host/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
Traceback (most recent call last):
  File ""external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc"", line 41, in <module>
    from argparse import ArgumentParser
ImportError: No module named argparse
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 16.723s, Critical Path: 0.88s
```
Thi suggests that the `bazel` build for some reason is ignoring my `$PYTHONPATH` and can not find argparse. If I run my python argparse is imported with no problems. 

Now, I really am not that much concerned with why this is happening, but more of how could I can bypass it to build `tensorflow`? 

Related issues: 
https://github.com/tensorflow/tensorflow/issues/190 - most related. However, the solution there is only for the case of uncompatible gcc, no resolution for the import error. 
http://stackoverflow.com/questions/15093444/importerror-no-module-named-argparse - I can't install system packages
https://github.com/rg3/youtube-dl/issues/4483 - does not help me for tensorflow
https://github.com/tensorflow/tensorflow/issues/2860 - does not resolve the issue, but seems pretty similar 
https://github.com/tensorflow/tensorflow/issues/2021 - this shows this might be a `bazel` issue"
7665,Dockerfile manual build central repo URL is Incorrect,"Dockerfile and Dockerfile.gpu have URLs that do not point to valid repo

Current URL in Dockerfile and Dockerfile.gpu

http://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.0.0-cp27-none-linux_x86_64.whl

http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.0.0-cp27-none-linux_x86_64.whl

Changing the version number to 1.0.0 fixes the problem. 

Working URL: http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp27-none-linux_x86_64.whl

Not sure if the intent was to leave it at 0.0.0. If so, comments instructing the user to edit the URL to the current version would be helpful."
7664,tf.nn has no attribute rnn_cell in version 1.0.1,"
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System: Ubuntu 16.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1.0.0

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

`<ipython-input-66-e322aca5289e> in makeGRUCells()
      7 
      8         def makeGRUCells():
----> 9             base_cell = tf.nn.rnn_cell.GRUCell(num_units=RNN_HIDDEN_SIZE,)
     10             layered_cell = tf.nn.rnn_cell.MultiRNNCell([base_cell] * NUM_LAYERS,state_is_tuple=False)
     11             attn_cell =tf.contrib.rnn.AttentionCellWrapper(cell=layered_cell,attn_length=ATTN_LENGTH,state_is_tuple=False)

AttributeError: 'module' object has no attribute 'rnn_cell'`

### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7663,Issue with Ubuntu 16.04 GPU install: Bus error (core dumped),"Operating System: Ubuntu 16.04

Installed version of CUDA and cuDNN: CUDA 8.0, cuDNN 5.1

jiexun@jiexun-XPS-15-9560:~$ ls -l /usr/local/cuda-8.0/lib64/libcud*
-rw-r--r-- 1 root root   556000 Feb 19 15:48 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Feb 19 15:48 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Feb 19 15:48 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
-rwxr-xr-x 1 root root   415432 Feb 19 15:48 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
-rw-r--r-- 1 root root   775162 Feb 19 15:48 /usr/local/cuda-8.0/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 Feb 19 16:10 /usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       18 Feb 19 16:10 /usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.10
-rwxr-xr-x 1 root root 42762752 Feb 19 16:10 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10

Installed GPU enabled TensorFlow with:
pip install tensorflow-gpu

Error message importing tensorflow:
Python 3.6.0 |Anaconda custom (64-bit)| (default, Dec 23 2016, 12:22:00) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
Bus error (core dumped)

#### Any idea why the above 'Bus error' is happening? Thanks a lot!


----
I also tried installing from source, but got this error when creating the pip package:
, after running `bazel build --config opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`:
At global scope:
cc1plus: warning: unrecognized command line option '-Wno-self-assign'
ERROR: /home/jiexun/tensorflow/tensorflow/python/BUILD:2279:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow.so' failed: link_dynamic_library.sh failed: error executing command external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o ... (remaining 27 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
bazel-out/local_linux-py3-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Slib/libcudnn.so.5: file not recognized: File truncated
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1332.429s, Critical Path: 853.44s
"
7662,Add a dynamic_partial_sum operator to tensorflow?,"Hi,

In my application I need to do operations that dynamically sum some rows of a matrix to get a new matrix.  There will be an input tensor named ""index"" that guides which part of the tensor to be summed. 

An example is, if the input matrix is
```python
[ [   1,   1,   1],
  [  10,  10,  10],
  [ 100, 100, 100] ]
```
And the index is
```python
[ [ 0, 2 ],
  [ 2, 3 ]]
```
which simply says the output tensor will have two rows (because the ""index"" have two rows), the first row is the sum of rows with row number i that satisfies `0 <= i < 2` in the input, and the second row is the sum of rows with row number 2. So the result should be
```python
[ [  11,  11,  11],
  [ 100, 100, 100] ]
```

I don't find any existing operation that does this job, so I implement it (support only 2d matrix and GPU) by myself.  Since I already have an implementation, I'm not requesting a new feature here. But I do want to know that if the tensorflow team is interested in adding this operation as part of tensorflow. If the answer is yes, I will add CPU support (maybe also xla? I have no idea on how to add xla support yet), and then create a pull request for that."
7661,any pretrained models for tensorflow 1.0?,"Now that TF has been updated to 1.0 with all the high-level apis, are there any supported pretrained models that are compatible with the new interfaces?"
7660,Failed to build with optimization flag AVX2,"**OS : Ubuntu 16.041
CPU : Intel i7 6700k**

Which supports avx2.0.

I am trying to install Tensorflow from sources. 
What I am doing
`./configure
Please specify the location of python. [Default is /usr/bin/python]: 
Please specify optimization flags to use during compilation [Default is -march=native]: AVX2
Do you wish to use jemalloc as the malloc implementation? (Linux only) [Y/n] y
jemalloc enabled on Linux
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] n
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] n
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] n
No XLA JIT support will be enabled for TensorFlow
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]

Using python library path: /usr/local/lib/python2.7/dist-packages
Do you wish to build TensorFlow with OpenCL support? [y/N] n
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] y
CUDA support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 5.1.5
Please specify the location where cuDNN 5.1.5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 6.1
Extracting Bazel installation...
.....
INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
.....
INFO: All external dependencies fetched successfully.
Configuration finished
`

Now when I try to build got error,

`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package 
INFO: Found 1 target...
ERROR: /home/hannan/.cache/bazel/_bazel_hannan/45070a52d8b4aeac18b16b18e9aeca76/external/nanopb_git/BUILD.bazel:8:1: C++ compilation of rule '@nanopb_git//:nanopb' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 36 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
gcc: error: AVX2: No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 5.002s, Critical Path: 2.46s
`

If I go with out providing optimization flag its work fine. 


"
7659,'NoneType' object has no attribute 'keys' in version 1.0,"I was trying to run tensorflow in docker. The command was this:

```
python tensorflow/examples/image_retraining/retrain.py \
--bottleneck_dir=/tf_files/bottlenecks \
--how_many_training_steps 500 \
--model_dir=/tf_files/inception \
--output_graph=/tf_files/retrained_graph.pb \
--output_labels=/tf_files/retrained_labels.txt \
--image_dir /tf_files/flower_photos
```

I put the error I got in the log  

### Environment info
Operating System: OS X

2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. 
version 1.0

### Logs or other output that would be helpful
```
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Image directory '/tf_files/flower_photos' not found.
Traceback (most recent call last):
  File ""tensorflow/examples/image_retraining/retrain.py"", line 1052, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""tensorflow/examples/image_retraining/retrain.py"", line 775, in main
    class_count = len(image_lists.keys())
AttributeError: 'NoneType' object has no attribute 'keys'
```
"
7658,Optimizer var_list does not have effect on the excluded variable!,"I have written piece of TensorFlow code which has two optimizers and I would like to exclude specific variables from being updated while calling ""run"" on any of these optimizers. As suggested by the TensorFlow documentation, I have specifically generated a list of variables to be updated for each optimizer, like below:

    ```
mentor_training_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, ""mentor"")
    train_op_mentor = mnist.training(loss_mentor, FLAGS.learning_rate, mentor_training_vars)
    mentee_training_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, ""mentee"")
    train_op_mentee = mnist.training(loss_mentee, FLAGS.learning_rate, mentee_training_vars)
    mentee_indep_training_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, ""mentee_indep"")
    train_op_mentee_indep = mnist.training(loss_mentee_indep, FLAGS.learning_rate, mentee_indep_training_vars)
```

The training functions in the mnist object is defined as:L

def training(loss, learning_rate, var_list):
  # Add a scalar summary for the snapshot loss.
  tf.summary.scalar('loss', loss)
  # Create the gradient descent optimizer with the given learning rate.
  optimizer = tf.train.GradientDescentOptimizer(learning_rate)
  # Create a variable to track the global step.
  global_step = tf.Variable(0, name='global_step', trainable=False)
  # Use the optimizer to apply the gradients that minimize the loss
  # (and also increment the global step counter) as a single training step.
  train_op = optimizer.minimize(loss, global_step=global_step, var_list=var_list)
  return train_op


```

As it's clear in the above code, I have three namescopes, where each has their own variables.

Now, let's say I only want to train the mentor variables. When I put a breakpoint after running session on the mentor optimizer, I can see that the mentee variables content is being changed after each run. Now I'm wondering whether I'm using this feature correctly, or there is something wrong with this API?"
7657,is_jpeg function only detects JFIF and not EXIF jpeg images in decode_image(),"I am using tf.image.decode_image() function to dynamically decode jpeg, png or gif on the fly. However inside decode_image(), it checks if passed tensor if jpeg image using this condition:

`is_jpeg = math_ops.equal(substr, b'\xff\xd8\xff\xe0', name='is_jpeg')`

EXIF files have a marker of 0xff*e1*, JFIF files have a marker of 0xff*e0*. So all code that relies on 0xffe0 to detect a JPEG file will miss all EXIF files.

When I patched it to only match first 3 bytes, its working fine."
7656,tf.sparse_placeholder does not accept fully specified shapes,"When I try to construct `tf.sparse_placeholder` with fully specified shape, like this
```
tf.sparse_placeholder(tf.float32, shape=(10000, 10000))
```
I get

```
ValueError                                Traceback (most recent call last)
<ipython-input-15-977303dad90f> in <module>()
----> 1 tf.sparse_placeholder(tf.float32, shape=(10000, 10000))

/Users/astepanov/.virtualenvs/py_asr/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in sparse_placeholder(dtype, shape, name)
   1584           dtypes.int64, shape=[None, None],
   1585           name=(name + ""/indices"") if name is not None else None),
-> 1586       dense_shape=shape)
   1587 # pylint: enable=redefined-outer-name
   1588 

/Users/astepanov/.virtualenvs/py_asr/lib/python3.6/site-packages/tensorflow/python/framework/sparse_tensor.py in __init__(self, indices, values, dense_shape)
    134           values, name=""values"", as_ref=True)
    135       dense_shape = ops.convert_to_tensor(
--> 136           dense_shape, name=""dense_shape"", dtype=dtypes.int64)
    137     self._indices = indices
    138     self._values = values

/Users/astepanov/.virtualenvs/py_asr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)
    649       name=name,
    650       preferred_dtype=preferred_dtype,
--> 651       as_ref=False)
    652 
    653 

/Users/astepanov/.virtualenvs/py_asr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)
    714 
    715         if ret is None:
--> 716           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    717 
    718         if ret is NotImplemented:

/Users/astepanov/.virtualenvs/py_asr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)
    587     raise ValueError(
    588         ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r""
--> 589         % (dtype.name, t.dtype.name, str(t)))
    590   return t
    591 

ValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: 'Tensor(""Const_5:0"", shape=(2,), dtype=int32)'
```

But when the shape is not fully specified, it is doing OK

```
In [16]: tf.sparse_placeholder(tf.float32, shape=(10000, None))
Out[16]: <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x112a7ff60>
```

I don't think that's the desired behaviour here. As a workaround for now I dropped the shape altogether in my code, but this way I cannot ensure runtime correctness of values being fed in feed_dict.

### Environment info
Operating System: MacOS Sierra

Installed version of CUDA and cuDNN: none

If installed from binary pip package, provide:

1. A link to the pip package you installed: `pip install tensorflow==1.0.0`
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`: 1.0.0

"
7654,Feature: Tensorboard Starts with all Runs Toggled Off Initially,"Currently the initial load /  full page refresh of tensorboard starts on the scalars tab with all runs toggled on for rendering. If there are a lot of runs in the logdir, the browser hangs for a noticeable period of time while all of the charts for the runs are rendered. I suggest making the initial load have runs off and letting the user opt in / turn on the desired set of runs."
7653,failed call to cuInit: CUDA_ERROR_UNKNOWN,"when i run tensorflow gpu my gpy is gt 620m 
linux mint 18.1
```
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_UNKNOWN
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: essam-goda
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: essam-goda
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 340.101.0
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  340.101  Thu Dec  1 15:52:31 PST 2016
GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) 
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 340.101.0
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 340.101.0
```
"
7652,Provide a working way to shut up warnings,"These messages appear whenever I run my program.

W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.

Now, I don't care that Tensorflow wasn't compiled to use SSE3 instructions, or AVX, or whatever. I don't want to see these messages. I also don't want to recompile from source to remove the warnings, install something else, play games with stderr, or whatever. I just want a clean way to shut up the warnings.

Here's my initialization code.

import tensorflow as tf
from tensorflow.python.framework import ops
tf.logging.set_verbosity(tf.logging.ERROR)
sess = tf.Session(config=tf.ConfigProto(device_count = {'GPU': 0}))

See? I asked for no warnings. Yet I still get them. That's a bug. Even if it's the intended behavior, please fix this.

Version is Tensorflow 1.0 on Linux.

Thank you!
"
7651,AttributeError: 'module' object has no attribute 'Default' (revisited),"### Issue

```
$ python
Python 2.7.6 (default, Oct 26 2016, 20:30:19) 
[GCC 4.8.4] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/schidester/usr/tf/local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/schidester/usr/tf/local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 75, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/home/schidester/usr/tf/local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 9, in <module>
    from google.protobuf import symbol_database as _symbol_database
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/symbol_database.py"", line 165, in <module>
    _DEFAULT = SymbolDatabase(pool=descriptor_pool.Default())
AttributeError: 'module' object has no attribute 'Default'
```

### Existing issues
Already found issues: #5344 and #5319 but neither provides a solution to the problem - that I could see (perhaps I just missed it somewhere)

### Environment info
Operating System: Linux Mint 17.3 Rosa

### Reproduction details
First I built tensorflow from git source, branch r1.0.

```
$ git remote -v
origin	https://github.com/tensorflow/tensorflow (fetch)
origin	https://github.com/tensorflow/tensorflow (push)
$ git branch
  master
* r1.0
$ bazel version
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261
```

No apparent problems encountered during the build.  This resulted in a .whl file in the /tmp directory and I installed it with pip:

```
$ sudo pip install /tmp/tensorflow_pkg/tensorflow-1.0.0-cp27-none-linux_x86_64.whl
```

At this point I get the import error shown at the top of this post whenever `import tensorflow` is used in a python shell.

So I removed the locally built tensorflow with `sudo pip uninstall tensorflow` and re-installed with the pre-built .whl file:

```
$ sudo pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp27-none-linux_x86_64.whl
```

Same import error.  Uninstalled again and tried using a virtualenv.  But I'm still getting the import errors:

```
$ virtualenv --system-site-packages ~/usr/tf
$ source ~/usr/tf/bin/activate
(tf)$ pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp27-none-linux_x86_64.whl
Downloading/unpacking https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp27-none-linux_x86_64.whl
  Downloading tensorflow-1.0.0-cp27-none-linux_x86_64.whl (44.1MB): 44.1MB downloaded
Requirement already satisfied (use --upgrade to upgrade): protobuf>=3.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.0.0)
Requirement already satisfied (use --upgrade to upgrade): wheel in /usr/lib/python2.7/dist-packages (from tensorflow==1.0.0)
Requirement already satisfied (use --upgrade to upgrade): mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.0.0)
Requirement already satisfied (use --upgrade to upgrade): numpy>=1.11.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.0.0)
Requirement already satisfied (use --upgrade to upgrade): six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.0.0)
Installing collected packages: tensorflow
Successfully installed tensorflow
Cleaning up...
(tf)$ python
Python 2.7.6 (default, Oct 26 2016, 20:30:19) 
[GCC 4.8.4] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/schidester/usr/tf/local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/schidester/usr/tf/local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 75, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/home/schidester/usr/tf/local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 9, in <module>
    from google.protobuf import symbol_database as _symbol_database
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/symbol_database.py"", line 165, in <module>
    _DEFAULT = SymbolDatabase(pool=descriptor_pool.Default())
AttributeError: 'module' object has no attribute 'Default'
```

I've tried the following, but neither had any effect:

```
 pip install --upgrade protobuf
 pip install --upgrade tensorflow
```

My best guess is that the python protobuf lib version available for the older version of Linux Mint I'm using is the problem.  But I don't know how to verify that or fix it - if that is indeed the problem."
7649,cuDevicePrimaryCtxSetFlags not found,"Hi all,
i tried running [Darkflow ](https://github.com/thtrieu/darkflow)with following argument
`python flow --model cfg/yolo.cfg --load bin/yolo.weights --demo camera --gpu 1.0`

got below output 

```
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally
Parsing ./cfg/yolo.cfg
Parsing cfg/yolo.cfg
Loading bin/yolo.weights ...
Successfully identified 269862452 bytes
Finished in 0.020000934600830078s

Building net ...
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: Bes
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for
elyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op:
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowT
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for u
tringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: S
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op:
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: Top
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: Top
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown

Source | Train? | Layer description                | Output size
-------+--------+----------------------------------+---------------
       |        | input                            | (?, 416, 416, 3)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 416, 416, 32)
 Load  |  Yep!  | maxp 2x2p0_2                     | (?, 208, 208, 32)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 208, 208, 64)
 Load  |  Yep!  | maxp 2x2p0_2                     | (?, 104, 104, 64)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 104, 104, 128)
 Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 104, 104, 64)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 104, 104, 128)
 Load  |  Yep!  | maxp 2x2p0_2                     | (?, 52, 52, 128)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 52, 52, 256)
 Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 52, 52, 128)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 52, 52, 256)
 Load  |  Yep!  | maxp 2x2p0_2                     | (?, 26, 26, 256)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 26, 26, 512)
 Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 26, 26, 256)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 26, 26, 512)
 Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 26, 26, 256)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 26, 26, 512)
 Load  |  Yep!  | maxp 2x2p0_2                     | (?, 13, 13, 512)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)
 Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 13, 13, 512)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)
 Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 13, 13, 512)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)

F c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_driver.cc:94] Check failed: s.ok() could not find cuDevicePrimaryCtxSetFlags in libcuda DSO; dlerr
or: cuDevicePrimaryCtxSetFlags not found
```

i'm using python 3.5, windows 7(64bit), tensorflow (v1.0 installed via pip) with gpu support, cuda v8, cudnn v5
i'm having NVidia GeForce 210 video card, have installed latest graphic driver 

note: 
i also tried cuda v5 toolkit, but somehow tensorflow is still looking for v8 cuda binaries ..
i installed tensorflow cpu only version, it works. but 0.4fps

Thanks

"
7648,"GPU: no known devices, despite cuda's deviceQuery returning a ""PASS"" result","Hi guys,

Sorry, I know there's already been tons of GPU related issues, but I could not find any that seems to be related to my problem.

The main symptom: when running tensorflow, my gpu is not detected ([the code being run](https://gist.github.com/oelmekki/cafda411bf5c2ea695d984fa98e0995b), and [its output](https://gist.github.com/oelmekki/77235c6b0dde99b3438f190eb557f40f)).

What differs from usual issues is that cuda seems properly installed and running `./deviceQuery` from cuda samples is successful ([output](https://gist.github.com/oelmekki/fe65a15daec45aa90ec33b10b51d3aae)).

I have two graphical cards:

* an old GTX 650 used for my monitors (I don't want to use that one with tensorflow)
* a GTX 1060 that I want to dedicate to tensorflow

I use:

* [tensorflow-1.0.0](https://pypi.python.org/pypi/tensorflow)
* cuda-8.0 ([ls -l /usr/local/cuda/lib64/libcud*](https://gist.github.com/oelmekki/6e5e9d7d1ea871e1d73efae307efe9ce))
* cudnn-5.1.10
* python-2.7.12
* nvidia-drivers-375.26 (this was installed by cuda and replaced my distro driver package)


I've tried:

* adding `/usr/local/cuda/bin/` to `$PATH`
* forcing gpu placement in tensorflow script using `with tf.device('/gpu:1'):` (and `with tf.device('/gpu:0'):` when it failed, for good measure)
* whitelisting the gpu I wanted to use with `CUDA_VISIBLE_DEVICES`, in case the presence of my old unsupported card did cause problems
* running the script with sudo (because why not)

Here are the outputs of [nvidia-smi](https://gist.github.com/oelmekki/7bdcb5cc2f791cea561a60f8b21e87b5) and [nvidia-debugdump -l](https://gist.github.com/oelmekki/b83a5a0a72e8924aeb44b70b3598f9b4), in case it's useful.

At this point, I feel like I have followed all the breadcrumbs and have no idea what I could try else. I'm not even sure if I'm contemplating a bug or a configuration problem. Any advice about how to debug this would be greatly appreciated. Thanks!
"
7647,Error when running image retraining example (retrain.py) with --print_misclassified_test_images flag,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

- http://stackoverflow.com/questions/17322668/typeerror-dict-keys-object-does-not-support-indexing
- http://stackoverflow.com/questions/18552001/accessing-dict-keys-element-by-index-in-python3
- http://stackoverflow.com/questions/26693055/dict-key-object-does-not-support-indexing-python-3

### Environment info
Operating System: macOS 10.12.3
Python version: 3.5.2

Installed version of CUDA and cuDNN: Not using CUDA

1. The commit hash (`git rev-parse HEAD`): 89059e6f0b2788b624e744afc21ba2472523a250
2. The output of `bazel version`: 
```
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:56:35 2017 (1485975395)
Build timestamp: 1485975395
Build timestamp as int: 1485975395
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
Run this command:
`bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos/ --print_misclassified_test_images`

### What other attempted solutions have you tried?
Reviewed source code for retrain.py, both locally and latest version on Github. See lines 892-896 (inside main function). The code looks correct. I suspect it is an issue specific to Python 3. 

I have not tried to run this in Python 2 yet.

### Logs or other output that would be helpful
This is what is output:
```
=== MISCLASSIFIED TEST IMAGES ===
Traceback (most recent call last):
  File ""/Users/jsawruk/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 1061, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/Users/jsawruk/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/Users/jsawruk/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 896, in main
    print('%70s  %s' % (test_filename, image_lists.keys()[predictions[i]]))
TypeError: 'dict_keys' object does not support indexing
```

It appears the solution is to rewrite line 896 from:
`print('%70s  %s' % (test_filename, image_lists.keys()[predictions[i]]))`

to:

`print('%70s  %s' % (test_filename, list(image_lists.keys())[predictions[i]]))`

When I edit the code locally to the line above, I no longer get the error when I run the example."
7646,kernel size in contrib.layers.conv2d,"```conv = tf.contrib.layers.convolution2d(x, num_filters, kernel_size=[height, width])```

Resulting in this error:

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/usr/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/utils.py in n_positive_integers(n, value)
    309   try:
--> 310     value = int(value)
    311   except (TypeError, ValueError):

TypeError: int() argument must be a string, a bytes-like object or a number, not 'tuple'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-89-04da00716174> in <module>()
      9 for height in convolved_words:
     10     conv = tf.contrib.layers.convolution2d(
---> 11         embed, num_filters, kernel_size=(height, embedding_size))
     12     pool = tf.nn.top_k(conv, k=k_max, sorted=False).value
     13     max_pooled.append(pool)

/usr/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py in func_with_args(*args, **kwargs)
    175       current_args = current_scope[key_func].copy()
    176       current_args.update(kwargs)
--> 177     return func(*args, **current_args)
    178   _add_op(func)
    179   setattr(func_with_args, '_key_op', _key_op(func))

/usr/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py in convolution(inputs, num_outputs, kernel_size, stride, padding, data_format, rate, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope)
    814                        input_rank)
    815     conv_dims = input_rank - 2
--> 816     kernel_size = utils.n_positive_integers(conv_dims, kernel_size)
    817     stride = utils.n_positive_integers(conv_dims, stride)
    818     rate = utils.n_positive_integers(conv_dims, rate)

/usr/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/utils.py in n_positive_integers(n, value)
    314       raise ValueError(
    315           'Expected sequence of %d positive integers, but received %r' %
--> 316           (n, value))
    317     try:
    318       values = tuple(int(x) for x in value)

ValueError: Expected sequence of 1 positive integers, but received (3, 128)
```"
7645,CCI Best Practices,"Google is a member of CCI. What do you think to add [CCI badge](https://bestpractices.coreinfrastructure.org) to TF repository?
"
7644,Feature request: Save the logging to a file,"According to [this SO question](http://stackoverflow.com/questions/40559667/how-to-redirect-tensorflow-logging-to-a-file), saving the logger output to a file is not supported.

Would be great if that would be possible.

Thank you


"
7643,Interaction between tf.map_fn and tf.gradients,"Hi,

I am using Tensorflow v0.11 and I have tried on Mac OS X and Centos 6

I am running into an error when running the following code:

```
W = tf.get_variable('W', (5, 3))

x = tf.placeholder(tf.float32, shape=(None, 5))

h = tf.matmul(x, W)

grads = tf.map_fn(lambda x: tf.gradients(x, W)[0], h)
```

I basically want to have the following but without a fixed batch size:
`grads = [tf.gradients(h[t], W)[0] for t in range(batch_size)]`

My error is:

```
Invalid argument: TensorArray map/TensorArray_1@map/while/gradients: Could not write to TensorArray index 3 because it has already been read.
[...]
tensorflow.python.framework.errors.InvalidArgumentError: TensorArray map/TensorArray_1@map/while/gradients: Could not write to TensorArray index 3 because it has already been read.
	 [[Node: map/while/gradients/map/while/TensorArrayRead_grad/TensorArrayWrite = TensorArrayWrite[T=DT_FLOAT, _class=[""loc:@map/TensorArray""], _device=""/job:localhost/replica:0/task:0/cpu:0""](map/while/gradients/map/while/TensorArrayRead_grad/TensorArrayGrad/TensorArrayGrad, map/while/Identity, map/while/gradients/Fill, map/while/gradients/map/while/TensorArrayRead_grad/TensorArrayGrad/gradient_flow)]]
```

I have tried the following workaround using `scan` instead of `map_fn` with a zero initializer but to no avail:
```
initializer = np.zeros((5, 3)).astype('float32')
grads = tf.scan(
	lambda a, x: tf.gradients(x, W)[0],
	h,
	initializer)
```

Is this a know issue? "
7642,v1.0.0 on Centos 7 complie error whith cuda 8.0,"build with --config=cuda get error
[rnd@localhost tensorflow]$ bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package --verbose_failures
INFO: $TEST_TMPDIR defined: output root default is '/home/rnd/tmp'.
......
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
WARNING: /home/rnd/gits/tensorflow/tensorflow/workspace.bzl:27:5: 
Current Bazel is not a release version, cannot check for compatibility.
WARNING: /home/rnd/gits/tensorflow/tensorflow/workspace.bzl:28:5: Make sure that you are running at least Bazel 0.4.2.
.
INFO: Found 1 target...
ERROR: /home/rnd/tmp/_bazel_rnd/707043e71401a80a1e11714c15a7b311/external/pcre/BUILD:5:1: undeclared inclusion(s) in rule '@pcre//:pcre':
this rule is missing dependency declarations for the following files included by 'external/pcre/pcre_globals.c':
  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/limits.h'
  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/syslimits.h'
  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/stddef.h'
  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdarg.h'
  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdint.h'.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 6.763s, Critical Path: 0.73s

### Environment info
Operating System:
CentOS Linux release 7.2.1511 (Core)
Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
[rnd@localhost ~]$ ls /usr/local/cuda/lib64/libcud*
/usr/local/cuda/lib64/libcudadevrt.a
/usr/local/cuda/lib64/libcudart.so
/usr/local/cuda/lib64/libcudart.so.8.0
/usr/local/cuda/lib64/libcudart.so.8.0.44
/usr/local/cuda/lib64/libcudart_static.a
/usr/local/cuda/lib64/libcudnn.so
/usr/local/cuda/lib64/libcudnn.so.5
/usr/local/cuda/lib64/libcudnn.so.5.1.5
/usr/local/cuda/lib64/libcudnn_static.a

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
[rnd@localhost tensorflow]$ git rev-parse HEAD
07bb8ea2379bd459832b23951fb20ec47f3fdbd4

2. The output of `bazel version`
[rnd@localhost bazel]$ git checkout 0.4.2
HEAD is now at ba94a7b... Release 0.4.2 (2016-12-02)
[rnd@localhost bazel]$ git rev-parse HEAD
ba94a7b93e1c95bca1928d8c51c6adc62ed864ab
[rnd@localhost bazel]$ bazel version
INFO: $TEST_TMPDIR defined: output root default is '/home/rnd/tmp'.
......
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Jan 01 00:00:00 1970 (0)
Build timestamp: Thu Jan 01 00:00:00 1970 (0)
Build timestamp as int: 0

### What other attempted solutions have you tried?
delete ""--config=cuda"" will be OK, but I need cuda 

[rnd@localhost tensorflow]$ bazel build -c opt  tensorflow/tools/pip_package:build_pip_package --verbose_failures


"
7641,'LookupError: No gradient defined for operation '...' (op type: ResizeBicubic)' is raised,"I tried to use [tf.image.resize_images](https://www.tensorflow.org/api_docs/python/tf/image/resize_images) with ```method=ResizeMethod.BICUBIC```.

The shorten version of my code is the following:
```python
# module import is omitted
# config setting is omitted
# global variable declaration is omitted

def train():
    # file queuing and reading are omitted.
    magnification = tf.random_uniform([1], minval = 0, maxval = 4, dtype = tf.float32)

    image_patches_blurred = tf.image.resize_images(image_patches, [tf.to_int32(64/magnification[0]), tf.to_int32(64/magnification[0])], \
                                                    method = tf.image.ResizeMethod.BICUBIC)
    image_patches_blurred = tf.image.resize_images(image_patches_blurred, [64, 64], method = tf.image.ResizeMethod.BICUBIC)

    with tf.variable_scope('generator'):
        G = Stage_Unet_model.generator(image_patches_blurred, batch_size = 16, image_size = 64, input_channels = 3, gf_dim = 32)

    # Model Loss definition is omitted

    # Summary Operation is omitted

    # tf variables loading
    all_vars = tf.global_variables()
    model_generator_vars = [k for k in all_vars if k.name.startswith(""generator"")]

    # Gradient Clipping
    generator_grads = tf.gradients(G_loss, model_generator_vars)
    generator_grads, _ = tf.clip_by_global_norm(generator_grads, clip_norm = 1.0)
    generator_var_pairs = zip(generator_grads, model_generator_vars)

    # tf.train.Saver and global_step declaration is omitted

    # tf.train.AdamOptimizer
    Adam = tf.train.AdamOptimizer(config.learning_rate)
    G_optim = Adam.apply_gradients(generator_var_pairs, global_step = G_global_step)

    # merge summary operation is omitted

    with tf.Session() as sess:
        #...
        for epoch in range(config.epoch):
            for idx in range(config.train_data_number):
                # Note that graph operations related with model D are omitted
                train_D_l, train_G_l, train_D_t_s, train_G_t_s, D_step, G_step, _, __ = sess.run([D_loss, G_loss, \
                                                                D_tot_summary, G_tot_summary, \
                                                                D_global_step, G_global_step, D_optim, G_optim])
        #...

if __name__ == '__main__':
    if not tf.gfile.Exists(log_dir):
        tf.gfile.MakeDirs(log_dir)
    train()
```

Running the above code will result in the following error:
```
$ python train_stage1.py 
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
Traceback (most recent call last):
  File ""train_stage1.py"", line 216, in <module>
    train()
  File ""train_stage1.py"", line 150, in train
    generator_grads = tf.gradients(G_loss, model_generator_vars)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 459, in gradients
    (op.name, op.type))
LookupError: No gradient defined for operation 'ResizeBicubic_3' (op type: ResizeBicubic)
```

Thank you for any help.

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
```
$ ls -l /usr/local/cuda-8.0/lib64/libcud*
-rw-r--r-- 1 root root   558720 11  4 05:18 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 11  4 05:18 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 11  4 05:18 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rwxr-xr-x 1 root root   415432 11  4 05:18 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 11  4 05:18 /usr/local/cuda-8.0/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 79337624 11  4 05:20 /usr/local/cuda-8.0/lib64/libcudnn.so
-rwxr-xr-x 1 root root 79337624 11  4 05:20 /usr/local/cuda-8.0/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 79337624 11  4 05:20 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 11  4 05:20 /usr/local/cuda-8.0/lib64/libcudnn_static.a
```

If installed from binary pip package, provide:
```
$ python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.0
```"
7640,ImportError: No module named 'tensorflow.contrib.distributions.python.ops.bijectors',"I want to run tensorflow 1.0 RC2 on windows 10, but encounter issues #7540, #7500, then follow the suggestion to install Nightly Build version (http://ci.tensorflow.org/view/Nightly/job/nightly-win/DEVICE=cpu,OS=windows/87/artifact/cmake_build/tf_python/dist/tensorflow-1.0.0-cp35-cp35m-win_amd64.whl),  but still encounter the following issue:
=================================
(tensorflow_py35) C:\Work\tensorflow\tensorflow\examples\tutorials\mnist>python mnist_softmax.py
Traceback (most recent call last):
  File ""mnist_softmax.py"", line 28, in <module>
    from tensorflow.examples.tutorials.mnist import input_data
  File ""C:\Miniconda3\envs\tensorflow_py35\lib\site-packages\tensorflow\examples\tutorials\mnist\__init__.py"", line 21, in <module>
    from tensorflow.examples.tutorials.mnist import input_data
  File ""C:\Miniconda3\envs\tensorflow_py35\lib\site-packages\tensorflow\examples\tutorials\mnist\input_data.py"", line 29, in <module>
    from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets
  File ""C:\Miniconda3\envs\tensorflow_py35\lib\site-packages\tensorflow\contrib\__init__.py"", line 22, in <module>
    from tensorflow.contrib import bayesflow
  File ""C:\Miniconda3\envs\tensorflow_py35\lib\site-packages\tensorflow\contrib\bayesflow\__init__.py"", line 24, in <module>
    from tensorflow.contrib.bayesflow.python.ops import entropy
  File ""C:\Miniconda3\envs\tensorflow_py35\lib\site-packages\tensorflow\contrib\bayesflow\python\ops\entropy.py"", line 23, in <module>
    from tensorflow.contrib.bayesflow.python.ops.entropy_impl import *
  File ""C:\Miniconda3\envs\tensorflow_py35\lib\site-packages\tensorflow\contrib\bayesflow\python\ops\entropy_impl.py"", line 30, in <module>
    from tensorflow.contrib.bayesflow.python.ops import variational_inference
  File ""C:\Miniconda3\envs\tensorflow_py35\lib\site-packages\tensorflow\contrib\bayesflow\python\ops\variational_inference.py"", line 26, in <module>
    from tensorflow.contrib.bayesflow.python.ops.variational_inference_impl import *
  File ""C:\Miniconda3\envs\tensorflow_py35\lib\site-packages\tensorflow\contrib\bayesflow\python\ops\variational_inference_impl.py"", line 29, in <module>
    from tensorflow.contrib.bayesflow.python.ops import stochastic_graph as sg
  File ""C:\Miniconda3\envs\tensorflow_py35\lib\site-packages\tensorflow\contrib\bayesflow\python\ops\stochastic_graph.py"", line 28, in <module>
    from tensorflow.contrib.bayesflow.python.ops import stochastic_tensor
  File ""C:\Miniconda3\envs\tensorflow_py35\lib\site-packages\tensorflow\contrib\bayesflow\python\ops\stochastic_tensor.py"", line 39, in <module>
    from tensorflow.contrib.distributions.python.ops import distribution
  File ""C:\Miniconda3\envs\tensorflow_py35\lib\site-packages\tensorflow\contrib\distributions\__init__.py"", line 91, in <module>
    from tensorflow.contrib.distributions.python.ops.conditional_transformed_distribution import *
  File ""C:\Miniconda3\envs\tensorflow_py35\lib\site-packages\tensorflow\contrib\distributions\python\ops\conditional_transformed_distribution.py"", line 22, in <module>
    from tensorflow.contrib.distributions.python.ops import transformed_distribution
  File ""C:\Miniconda3\envs\tensorflow_py35\lib\site-packages\tensorflow\contrib\distributions\python\ops\transformed_distribution.py"", line 24, in <module>
    from tensorflow.contrib.distributions.python.ops.bijectors import identity as identity_lib
ImportError: No module named 'tensorflow.contrib.distributions.python.ops.bijectors'


"
7639,nnbnrnhnvjb,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7638,Links to Android nightly builds on README.md are broken,"Links to Android nightly builds on https://github.com/tensorflow/tensorflow/blob/master/README.md are broken.

![image](https://cloud.githubusercontent.com/assets/15331/23086006/c80cc702-f538-11e6-927d-d44b1f5f80e2.png)

-->
![image](https://cloud.githubusercontent.com/assets/15331/23086019/d91aa082-f538-11e6-9d76-7e56cd879900.png)
"
7637,iris_monitors.py broken in release r1.0 due to inacurate MetricSpec namespace,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

`tf.contrib.learn.metric_spec.MetricSpec` should be changed to `MetricSpec` in iris_monitors.py in release 1.0.0

No need to import MetricSpec in iris_monitors.py it's already imported on line 24 in commit [fa4ba830f437fdb9dc1085b4d68a3bab41a16e20](https://github.com/tensorflow/tensorflow/blob/fa4ba830f437fdb9dc1085b4d68a3bab41a16e20/tensorflow/examples/tutorials/monitors/iris_monitors.py):
`tensorflow.contrib.learn.python.learn.metric_spec import MetricSpec`

### Environment info
Operating System:
>tensorflow/tensorflow/examples/tutorials/monitors$ lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 16.04.1 LTS
Release:	16.04
Codename:	xenial
>tensorflow/tensorflow/examples/tutorials/monitors$ uname -a
Linux panchito 4.4.0-62-generic-tuxonice #83~ppa1-Ubuntu SMP Thu Feb 2 23:17:45 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
```
repositories/tensorflow/tensorflow/examples/tutorials/monitors$ python -c ""import tensorflow; print(tensorflow.__version__)""
1.0.0
```
If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

repositories/tensorflow/tensorflow/examples/tutorials/monitors$ python iris_monitors.py
Traceback (most recent call last):
  File ""iris_monitors.py"", line 116, in <module>
    tf.app.run()
  File ""/home/chidochipotle/anaconda3/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""iris_monitors.py"", line 42, in main
    tf.contrib.learn.metric_spec.MetricSpec(
AttributeError: module 'tensorflow.contrib.learn' has no attribute 'metric_spec'

### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7636,Snap package for tensorflow,"It would be nice to have a snap package for tensorflow. https://snapcraft.io/
The advantage is that it is easy to install and supports multiple distributions. "
7635,Inconsistent in r.12 doc,"Hi, in the doc of r.12, the api for tf.split is 
`tf.split(split_dim, num_split, value, name='split')`

yet, the real api is changed to
`tf.split(value, num_split, ... )`


"
7634,Setting import_scope on import_meta_graph causes error for attached metagraph file,"Using the attached meta file:
```
model_fn = './my-model.meta'

graph = tf.Graph()
sess = tf.InteractiveSession(graph=graph)

t_input = tf.placeholder(np.float32, name='images') # define the input tensor
t_preprocessed = tf.expand_dims(t_input, 0)

new_saver = tf.train.import_meta_graph(model_fn, input_map={'images': t_input}, import_scope='import')
new_saver.restore(sess, './')
```
results in:
```
KeyError: ""The name 'gradients/discriminator/minibatch/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/RefEnter:0' refers to a Tensor which does not exist. The operation, 'gradients/discriminator/minibatch/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/RefEnter', does not exist in the graph.""
```
I'm trying to remap the input so I can do image space optimization with a library that assumes the network input is (width, height, channels). Loading doesn't error if I load without the input_map and import_scope keyword arguments; however, this causes problems for the library I'm interacting with. Setting import_scope alone does cause an error.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Asked on Stack Overflow in case this isn't a bug: http://bit.ly/2lW7lRA

### Environment info
Operating System:

Mac

1. A link to the pip package you installed:
https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py3-none-any.whl

2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
1.0.0

[my-model.meta.zip](https://github.com/tensorflow/tensorflow/files/784246/my-model.meta.zip)
"
7633,file not found: -lcublas.8.0,"I am trying to install tensorflow w/ GPU from Source on Mac OS X El Capitan. (CUDA 8.0 & cuDNN 5.1)

After wrestling with LYLD_LIBRARY_PATH [#6729](https://github.com/tensorflow/tensorflow/issues/6729), I ran
`bazel build --config=opt --config=cuda --action_env PATH --action_env LD_LIBRARY_PATH --action_env DYLD_LIBRARY_PATH  //tensorflow/tools/pip_package:build_pip_package`

But, it generates an error that cannot find cublas.8.0 which I have in my directory. If I try with --verbose_failures, it generate following description.

Linking of rule '//tensorflow/python:_pywrap_tensorflow.so' failed: link_dynamic_library.sh failed: error executing command


What am I missing? it should be something with path configuration. :-( "
7632,max_pool3d does not support float16(half),"I'm using version 0.12.1 on Ubuntu 14.04 LTS 64 bit. Here is the minimal example I run to find the error:

```
    with tf.Graph().as_default():
        a = tf.constant(1, shape = [1, 4, 4, 4, 1], dtype = tf.float16)
        b = tf.nn.max_pool3d(a, ksize = [1, 2, 2, 2, 1], strides = [1, 2, 2, 2, 1], padding = 'SAME')

        with tf.Session() as sess:
            print(sess.run([a, b]))
```

The I got error:

```
...
InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'MaxPool3D' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:
  device='CPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_FLOAT]
```

If I use ```dtype = tf.float32```, there is no error. ```max_pool3d``` python docstring says it supports half precision, but why ```dtype = tf.float16``` does not work?


"
7631,FEATURE: Tensorboard Scalars: Simple Way of Setting y axis to start at 0,"I have a number of scalars that I plot in tensorboard that are non negative values (ie accuracy, etc). It would be cool if there was a quick way to have the y axis start at 0 rather than allocating unused space in the negative y direction.
![image](https://cloud.githubusercontent.com/assets/51059/23080071/7b1aa63c-f51d-11e6-8ae0-82bbf299a01f.png)
"
7630,Poor Constant Propagation,"Running on the 1.0.0 Docker image.

Compare this:
```
tensor_util.constant_value_as_shape(ops.convert_to_tensor((tf.constant(15),16)))
> TensorShape([Dimension(15), Dimension(16)])
```
to:
```
tensor_util.constant_value_as_shape(ops.convert_to_tensor((tf.constant(15) * 1,16)))
> TensorShape([Dimension(None), Dimension(16)])
```
It seems to me that constant propagation should happen in the latter. This leads to size information being lost in certain cases such as image upsampling / downsampling where constant size is multiplied and then fed to something like `resize_images`."
7625,tf.train.import_meta_graph should be parallelized,"Hey guys, I've been wondering whether it's possible the `tf.train.import_meta_graph` to be parallelized across multiple cores. When I load a complex model it takes too long and I only see one of my 24 cores being utilized to 100%.

### Environment info
Operating System: CentOS

Installed version of CUDA and cuDNN: 
Not applicable

Minimal example:
```
sess = tf.Session()
new_saver = tf.train.import_meta_graph('my-model.meta')
new_saver.restore(sess, tf.train.latest_checkpoint('./'))
all_vars = tf.get_collection('vars')
for v in all_vars:
    v_ = sess.run(v)
    print(v_)
```

Any plans on changing that in the near future?"
7624,tensorflow codes totally different result with PyTorch's,"Hi, I'm new to tensorflow and trying to refactor one of my project originally in (Py)Torch. However, though the two codes has the same network, the same loss function, the same optimizer parameters, the result is totally different. Is any one have any idea about such issue?

Thanks."
7623,Error in pywrap_tensorflow.py after installing succesful tensorflow,"Hello,

trying to install tensorflow in Windows7 without GPU, I follow the guidelines:

I install python 3.5, then:

pip3 install --upgrade tensorflow

import tensorflow as tf

and I obtain the following error:

Traceback (most recent call last):
  File ""C:\Users\Utente\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Utente\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Impossibile trovare il modulo specificato.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Utente\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Utente\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 21, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Utente\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow')
  File ""C:\Users\Utente\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Utente\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Utente\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Utente\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Utente\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Impossibile trovare il modulo specificato.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Utente\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Utente\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 21, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Utente\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow')
  File ""C:\Users\Utente\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'

thank you for any help"
7622,tf.pad does not support SparseTensor in r1.0,"This works 
```
import tensorflow as tf
session = tf.InteractiveSession()
pad = tf.pad([[1,2]], [[0,0],[0,150]], mode='CONSTANT')
pad.eval()
```

But this doesn't work

```
import tensorflow as tf
session = tf.InteractiveSession()
tensor = tf.SparseTensor(indices=[[0, 0], [0, 1]], values=[1, 2], dense_shape=[1, 2])
pad = tf.pad(tensor, [[0,0],[0,150]], mode='CONSTANT')
pad.eval()
```

The following error is thrown
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/sromano/Dropbox/uba/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 1646, in pad
    return gen_array_ops._pad(tensor, paddings, name=name)
  File ""/Users/sromano/Dropbox/uba/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 2083, in _pad
    name=name)
  File ""/Users/sromano/Dropbox/uba/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 494, in apply_op
    raise err
TypeError: Expected binary or unicode string, got <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x116288450>
```

My use case is that I'm reading a csv file where each row has a different amount of elements and I want to turn it into a matrix of a fixed length. I use string split in each row and I get a SparseTensor, so then I need a different padding in each row to complete the expected width.

I'm using Python 2.7 and Tensorflow r1.0"
7621,After installation error,"Im getting this output 
>>> import tensorflow as tf
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
print(sess.run(hello))
 
 

E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') fo
r unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_t
ype: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""')
 for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for
unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_ty
pe: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""')
for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""'
) for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') fo
r unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') fo
r unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""
') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""C
PU""') for unknown op: UpdateFertileSlots
b'Hello, Tensorflow'


But when i run
>>> import tensorflow as tf
>>>
>>>
>>>#4 line gap
>>>
>>>hello = tf.constant('Hello, TensorFlow!')
>>>sess = tf.Session()
>>>print(sess.run(hello))

then the output is 
>>>b'Hello, TensorFlow!'


so i think its a bug in Tensorflow"
7620,Tensorboard Embedding with t-SNE Crash,"When I run t-SNE on my datasets (on smallest or bigger one) with tensorboard embedding , the app crash all the times (after thereabout 500 iterations). I can't re-run or stop the operation to change the parameters, the iteration number freeze and the other functionalities on embedding part crash also.

Anyone have the same problem ? Other curiosity is if I run on [online tensorboard](http://projector.tensorflow.org/) embedding I have the same problem.
"
7619,Are there any restrictions for TF r1.0 to support C++?,"The previous versions need generate graph by python firstly. For 1.0, based on the doc, we can construct the graph by c++.

Is the C++ supporting totally the same as python now?"
7618,"Windows 7 build problem. ""Release"" folder is missing from protoc.exe path","I'm trying to build tensorflow 1.0 on windows 7, using cmake.

I have generated the project, but when I try to build it I get this error: 

> Error	MSB6006	""cmd.exe"" exited with code 9009.	tf_protos_cc	C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets	171	

The output logs says:

> 1>------ Build started: Project: zlib, Configuration: Release x64 ------
> 1>  Performing update step for 'zlib'
> 2>------ Build started: Project: protobuf, Configuration: Release x64 ------
> 2>  Performing update step for 'protobuf'
> 3>------ Build started: Project: tf_protos_cc, Configuration: Release x64 ------
> 3>  Building Custom Rule D:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
> 3>  CMake does not need to re-run because D:/tensorflow/cmake_build/CMakeFiles/generate.stamp is up-to-date.
> 3>  Running C++ protocol buffer compiler on tensorflow/core/debug/debug_service.proto
> 3>  'protobuf\src\protobuf\\protoc.exe' is not recognized as an internal or external command,
> 3>  operable program or batch file.
> 3>C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(171,5): error MSB6006: ""cmd.exe"" exited with code 9009.
> ========== Build: 2 succeeded, 1 failed, 1 up-to-date, 0 skipped ==========

What I noticed is that it's looking for protoc.exe at the location: `protobuf\src\protobuf\\protoc.exe` and I can confirm that the real location is: `protobuf\src\protobuf\Release\protoc.exe`

The double \\ makes it seem like the path was suppose to include ""Release"" but didn't for some reason.

If you need any additional info please let me know.
"
7616,Change device Placement of existing variable,"How do I change the device placement of a tf.Variable() ?
I tried two methods 
```
a = tf.Variable(1,name = 'a')  # a's device is not set
with tf.device('/gpu:0'):
       a = tf.get_variable('a',1)   
# this creates a new variable on the gpu and doesn't change device assignment for a
```
I tried enforcing variable reuse by using tf.get_variable_scope().reuse_variables()
```
a = tf.Variable(1,name = 'a')  # a's device is not set
tf.get_variable_scope().reuse_variables()
with tf.device('/gpu:0'):
       a = tf.get_variable('a',1)   
# this creates a new variable on the gpu and doesn't change device assignment for a
```
This time, I get an error saying the variable 'a' did not exist in the gpu.

Any help on changing-device-placement or lazy-device-assignment would be appreciated. Thanks



"
7615,tfdbg Dump root directory does not exist,"I run 
python -m tensorflow.python.debug.examples.debug_mnist --debug
on my laptop.which has windows 10 and gtx1070.but after I type first run,it return an exception like this:
Traceback (most recent call last):
  File ""d:\Anaconda3\lib\runpy.py"", line 184, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""d:\Anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""d:\Anaconda3\lib\site-packages\tensorflow\python\debug\examples\debug_mnist.py"", line 138, in <module>
    tf.app.run()
  File ""d:\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""d:\Anaconda3\lib\site-packages\tensorflow\python\debug\examples\debug_mnist.py"", line 131, in main
    acc = sess.run(accuracy, feed_dict=feed_dict(False))
  File ""d:\Anaconda3\lib\site-packages\tensorflow\python\debug\wrappers\framework.py"", line 419, in run
    run_end_resp = self.on_run_end(run_end_req)
  File ""d:\Anaconda3\lib\site-packages\tensorflow\python\debug\wrappers\local_cli_wrapper.py"", line 262, in on_run_end
    self._dump_root, partition_graphs=partition_graphs)
  File ""d:\Anaconda3\lib\site-packages\tensorflow\python\debug\debug_data.py"", line 328, in __init__
    raise IOError(""Dump root directory %s does not exist"" % dump_root)
OSError: Dump root directory C:\Users\tony8\AppData\Local\Temp\tfdbg_cvtcdnhl does not exist


how can I fix it?Thank you"
7613,build failed,"Build fails with
```
bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```
but succeeds with
```
bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
```

I have used the following configurations:
```
Please specify the location of python: /home/mark/.pyenv/shims/python
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified: -march=native
Do you wish to use jemalloc as the malloc implementation? y
Do you wish to build TensorFlow with Google Cloud Platform support? n
Do you wish to build TensorFlow with Hadoop File System support? y
Do you wish to build TensorFlow with CUDA support? y
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? n
Please input the desired Python library path to use: /home/mark/.pyenv/versions/3.6.0/lib/python3.6/site-packages
Do you wish to build TensorFlow with OpenCL support? n
Do you wish to build TensorFlow with CUDA support? y
Please specify which gcc should be used by nvcc as the host compiler: /usr/bin/gcc-5
Please specify the CUDA SDK version you want to use: 8.0
Please specify the location where CUDA 8.0 toolkit is installed: /usr/local/cuda
Please specify the Cudnn version you want to use: 5.1.10
Please specify the location where cuDNN 5.1.10 library is installed: /usr/local/cuda
Please specify a list of comma-separated Cuda compute capabilities you want to build with: 6.1
```

on the following system:

- **Operating System**: Debian Sid
- **Installed version of CUDA and cuDNN**: 
```
ls -l /usr/local/cuda/lib64/libcud*

-rw-r--r-- 1 root staff   556000 Feb 16 23:37 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root staff       16 Feb 16 23:37 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root staff       19 Feb 16 23:37 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
-rwxr-xr-x 1 root staff   415432 Feb 16 23:37 /usr/local/cuda/lib64/libcudart.so.8.0.61
-rw-r--r-- 1 root staff   775162 Feb 16 23:37 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 mark mark        13 Nov  7 02:00 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 mark mark        18 Nov  7 02:00 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.10
-rwxr-xr-x 1 mark mark  84163560 Nov  7 01:47 /usr/local/cuda/lib64/libcudnn.so.5.1.10
-rw-r--r-- 1 mark mark  70364814 Nov  7 01:47 /usr/local/cuda/lib64/libcudnn_static.a
```
- **The commit hash (`git rev-parse HEAD`)**: b6f16b8166e3a7761f607be66d46acbd37dfaf43
- **The output of `bazel version`**: 

the build command
```
bazel build -c opt --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package
```
fails with the following error message:
```
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
INFO: Found 1 target...
ERROR: /home/mark/.cache/bazel/_bazel_mark/b8826e9bec4f8f69428cde0a6fac46ac/external/com_googlesource_code_re2/BUILD:11:1: C++ compilation of rule '@com_googlesource_code_re2//:re2' failed: crosstool_wrapper_driver_is_not_gcc failed:error executing command
  (cd /home/mark/.cache/bazel/_bazel_mark/b8826e9bec4f8f69428cde0a6fac46ac/execroot/tensorflow && \
  exec env - \
    PATH=/home/mark/bin:/home/mark/.pyenv/shims:/home/mark/.pyenv/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/mark/bin:/usr/local/scala/bin:/usr/local/go/bin:/srv/hadoop/bin \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -MD -MF bazel-out/host/bin/external/com_googlesource_code_re2/_objs/re2/external/com_googlesource_code_re2/re2/prefilter_tree.d '-frandom-seed=bazel-out/host/bin/external/com_googlesource_code_re2/_objs/re2/external/com_googlesource_code_re2/re2/prefilter_tree.o' -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -pthread -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c external/com_googlesource_code_re2/re2/prefilter_tree.cc -o bazel-out/host/bin/external/com_googlesource_code_re2/_objs/re2/external/com_googlesource_code_re2/re2/prefilter_tree.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
gcc-5: error trying to exec 'cc1plus': execvp: No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```

How should I proceed from here? Any tip would be appreciated."
7611,How to associate more than one labels with an image in TFRecord file for training purpose?,
7610,3D convolutions unnaturally slow on CPU,"3D convolutions on CPU seem unnaturally slow. 
I can't use GPU due to memory limit, so I'm looking at CPU execution. 

2D convolutions in TensorFlow seem to be well-optimized, all CPU cores are used, performance is just few times below GPU.

With 3D convolutions - the difference is orders of magnitude. 

Also, I've compared with Theano. Theano 3D convolutions run on single core but still are 10 times faster.

Strangely, TF uses all cores on CPU with 3Dconv,  so there must be some bug or extreme inefficiency in implementation.

With the same small test model (just couple of 3d conv layers) I get 1 second epoch time on GPU (both TF and Theano, theano just a bit faster), 5 seconds on CPU Theano single threaded, and 50 seconds with (seemingly) multi-threded TF

### Environment info
I've tried different TF versions from 0.12 to 1.00, installed from pip. Latest one from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp35-cp35m-linux_x86_64.whl

The OS is CentOS Linux release 7.2.1511 (Core) , kernel 3.10.0-327.18.2.el7.x86_64

I'm using Keras back-end, so I can't be 100% sure if the problem is not in the way Keras translates Convolution3D call into TF primitives, but it does not seem likely. 

cheers
Alex"
7609,Maybe  a bug: Different eval step between CPU and GPU,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
ubuntu 14.04
Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

`-rw-r--r-- 1 root root   322936 Aug 16  2015 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root       19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18
-rw-r--r-- 1 root root   720192 Aug 16  2015 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 Oct 20 14:34 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 Oct 20 14:33 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.3
-rwxr-xr-x 1 root root 60696704 Oct 20 14:28 /usr/local/cuda/lib64/libcudnn.so.5.1.3
-rw-r--r-- 1 root root 59715990 Oct 20 14:28 /usr/local/cuda/lib64/libcudnn_static.a`

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`): 16485a3fb5ffcbaa244e55c388e43279d2770982
2. The output of `bazel version`: 
.........
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I'm sorry that I can't reproduce it in minimal (mnist) example(it seems to be all right), but in my code, the global_step would pulse agin in the eval stage:

```
import tensorflow as tf
import os
import random
import tensorflow.contrib.slim as slim
import time
import logging
import numpy as np
import pickle
from PIL import Image


logger = logging.getLogger('Training a chinese write char recognition')
logger.setLevel(logging.INFO)
# formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
ch = logging.StreamHandler()
ch.setLevel(logging.INFO)
logger.addHandler(ch)


tf.app.flags.DEFINE_boolean('random_flip_up_down', False, ""Whether to random flip up down"")
tf.app.flags.DEFINE_boolean('random_brightness', True, ""whether to adjust brightness"")
tf.app.flags.DEFINE_boolean('random_contrast', True, ""whether to random constrast"")

tf.app.flags.DEFINE_integer('charset_size', 120, ""Choose the first `charset_size` character to conduct our experiment."")
tf.app.flags.DEFINE_integer('image_size', 64, ""Needs to provide same value as in training."")
tf.app.flags.DEFINE_boolean('gray', True, ""whether to change the rbg to gray"")
tf.app.flags.DEFINE_integer('max_steps', 12002, 'the max training steps ')
tf.app.flags.DEFINE_integer('eval_steps', 50, ""the step num to eval"")
tf.app.flags.DEFINE_integer('save_steps', 50, ""the steps to save"")

tf.app.flags.DEFINE_string('checkpoint_dir', './checkpoint/', 'the checkpoint dir')
tf.app.flags.DEFINE_string('train_data_dir', '../data/train/', 'the train dataset dir')
tf.app.flags.DEFINE_string('test_data_dir', '../data/test/', 'the test dataset dir')
tf.app.flags.DEFINE_string('log_dir', './log', 'the logging dir')

tf.app.flags.DEFINE_boolean('restore', False, 'whether to restore from checkpoint')
tf.app.flags.DEFINE_boolean('epoch', 1, 'Number of epoches')
tf.app.flags.DEFINE_boolean('batch_size', 128, 'Validation batch size')
tf.app.flags.DEFINE_string('mode', 'train', 'Running mode. One of {""train"", ""valid"", ""test""}')
FLAGS = tf.app.flags.FLAGS


class DataIterator:
    def __init__(self, data_dir):
        # Set FLAGS.charset_size to a small value if available computation power is limited.
        truncate_path = data_dir + ('%05d' % FLAGS.charset_size)
        print(truncate_path)
        self.image_names = []
        for root, sub_folder, file_list in os.walk(data_dir):
            if root < truncate_path:
                self.image_names += [os.path.join(root, file_path) for file_path in file_list]
        random.shuffle(self.image_names)
        self.labels = [int(file_name[len(data_dir):].split(os.sep)[0]) for file_name in self.image_names]

    @property
    def size(self):
        return len(self.labels)

    @staticmethod
    def data_augmentation(images):
        if FLAGS.random_flip_up_down:
            images = tf.image.random_flip_up_down(images)
        if FLAGS.random_brightness:
            images = tf.image.random_brightness(images, max_delta=0.3)
        if FLAGS.random_contrast:
            images = tf.image.random_contrast(images, 0.8, 1.2)
        return images

    def input_pipeline(self, batch_size, num_epochs=None, aug=False):
        images_tensor = tf.convert_to_tensor(self.image_names, dtype=tf.string)
        labels_tensor = tf.convert_to_tensor(self.labels, dtype=tf.int64)
        input_queue = tf.train.slice_input_producer([images_tensor, labels_tensor], num_epochs=num_epochs)

        labels = input_queue[1]
        images_content = tf.read_file(input_queue[0])
        images = tf.image.convert_image_dtype(tf.image.decode_png(images_content, channels=1), tf.float32)
        if aug:
            images = self.data_augmentation(images)
        new_size = tf.constant([FLAGS.image_size, FLAGS.image_size], dtype=tf.int32)
        images = tf.image.resize_images(images, new_size)
        image_batch, label_batch = tf.train.shuffle_batch([images, labels], batch_size=batch_size, capacity=50000,
                                                          min_after_dequeue=10000)
        # print 'image_batch', image_batch.get_shape()
        return image_batch, label_batch


def build_graph(top_k):
    keep_prob = tf.placeholder(dtype=tf.float32, shape=[], name='keep_prob')
    images = tf.placeholder(dtype=tf.float32, shape=[None, 64, 64, 1], name='image_batch')
    labels = tf.placeholder(dtype=tf.int64, shape=[None], name='label_batch')

    conv3_1 = slim.conv2d(images, 64, [3, 3], 1, padding='SAME', scope='conv3_1')
    max_pool_1 = slim.max_pool2d(conv3_1, [2, 2], [2, 2], padding='SAME')
    conv3_2 = slim.conv2d(max_pool_1, 128, [3, 3], padding='SAME', scope='conv3_2')
    max_pool_2 = slim.max_pool2d(conv3_2, [2, 2], [2, 2], padding='SAME')
    conv3_3 = slim.conv2d(max_pool_2, 256, [3, 3], padding='SAME', scope='conv3_3')
    max_pool_3 = slim.max_pool2d(conv3_3, [2, 2], [2, 2], padding='SAME')
    # conv3_4 = slim.conv2d(max_pool_3, 512, [3, 3], padding='SAME', scope='conv3_4')
    # max_pool_4 = slim.max_pool2d(conv3_4, [2, 2], [2, 2], padding='SAME')

    flatten = slim.flatten(max_pool_3)
    fc1 = slim.fully_connected(slim.dropout(flatten, keep_prob), 1024,
    activation_fn=tf.nn.tanh, scope='fc1')
    logits = slim.fully_connected(slim.dropout(fc1, keep_prob), FLAGS.charset_size,activation_fn=None, scope='fc2')
    # logits = slim.fully_connected(flatten, FLAGS.charset_size, activation_fn=None, reuse=reuse, scope='fc')
    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))
    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), labels), tf.float32))

    global_step = tf.get_variable(""step"", [], initializer=tf.constant_initializer(0.0), trainable=False)
    rate = tf.train.exponential_decay(2e-4, global_step, decay_steps=2000, decay_rate=0.8, staircase=True)
    train_op = tf.train.AdamOptimizer(learning_rate=rate).minimize(loss, global_step=global_step)
    probabilities = tf.nn.softmax(logits)

    tf.summary.scalar('loss', loss)
    tf.summary.scalar('accuracy', accuracy)
    merged_summary_op = tf.summary.merge_all()
    predicted_val_top_k, predicted_index_top_k = tf.nn.top_k(probabilities, k=top_k)
    accuracy_in_top_k = tf.reduce_mean(tf.cast(tf.nn.in_top_k(probabilities, labels, top_k), tf.float32))

    return {'images': images,
            'labels': labels,
            'keep_prob': keep_prob,
            'top_k': top_k,
            'global_step': global_step,
            'train_op': train_op,
            'loss': loss,
            'accuracy': accuracy,
            'accuracy_top_k': accuracy_in_top_k,
            'merged_summary_op': merged_summary_op,
            'predicted_distribution': probabilities,
            'predicted_index_top_k': predicted_index_top_k,
            'predicted_val_top_k': predicted_val_top_k}


def train():
    print('Begin training')
    train_feeder = DataIterator(data_dir='../data/train/')
    test_feeder = DataIterator(data_dir='../data/test/')
    with tf.Session() as sess:
        train_images, train_labels = train_feeder.input_pipeline(batch_size=FLAGS.batch_size, aug=True)
        test_images, test_labels = test_feeder.input_pipeline(batch_size=FLAGS.batch_size)
        graph = build_graph(top_k=1)
        sess.run(tf.global_variables_initializer())
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess=sess, coord=coord)
        saver = tf.train.Saver()

        train_writer = tf.summary.FileWriter(FLAGS.log_dir + '/train', sess.graph)
        test_writer = tf.summary.FileWriter(FLAGS.log_dir + '/val')
        start_step = 0
        if FLAGS.restore:
            ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)
            if ckpt:
                saver.restore(sess, ckpt)
                print(""restore from the checkpoint {0}"".format(ckpt))
                start_step += int(ckpt.split('-')[-1])

        logger.info(':::Training Start:::')
        try:
            i = 0
            while not coord.should_stop():
                i += 1
                start_time = time.time()
                train_images_batch, train_labels_batch = sess.run([train_images, train_labels])
                feed_dict = {graph['images']: train_images_batch,
                             graph['labels']: train_labels_batch,
                             graph['keep_prob']: 0.8}
                _, loss_val, train_summary, step = sess.run(
                    [graph['train_op'], graph['loss'], graph['merged_summary_op'], graph['global_step']],
                    feed_dict=feed_dict)
                train_writer.add_summary(train_summary, step)
                end_time = time.time()
                logger.info(""the step {0} takes {1} loss {2}"".format(step, end_time - start_time, loss_val))
                if step > FLAGS.max_steps:
                    break
                if step % FLAGS.eval_steps == 1:
                    test_images_batch, test_labels_batch = sess.run([test_images, test_labels])
                    feed_dict = {graph['images']: test_images_batch,
                                 graph['labels']: test_labels_batch,
                                 graph['keep_prob']: 1.0}
                    accuracy_test, test_summary, step = sess.run(
                        [graph['accuracy'], graph['merged_summary_op'], graph['global_step']],
                        feed_dict=feed_dict)
                    test_writer.add_summary(test_summary, step)
                    logger.info('===============Eval a batch=======================')
                    logger.info('the step {0} test accuracy: {1}'
                                .format(step, accuracy_test))
                    logger.info('===============Eval a batch=======================')

                    for tt in range(10):
                        _, _, step = sess.run(
                            [graph['accuracy'], graph['merged_summary_op'], graph['global_step']],
                            feed_dict=feed_dict)
                        logger.info('eval step again: step = {}'.format(step))


                if step % FLAGS.save_steps == 1:
                    logger.info('Save the ckpt of {0}'.format(step))
                    saver.save(sess, os.path.join(FLAGS.checkpoint_dir, 'my-model'),
                               global_step=graph['global_step'])
        except tf.errors.OutOfRangeError:
            logger.info('==================Train Finished================')
            saver.save(sess, os.path.join(FLAGS.checkpoint_dir, 'my-model'), global_step=graph['global_step'])
        finally:
            coord.request_stop()
        coord.join(threads)



def main(_):
    print(FLAGS.mode)
    if FLAGS.mode == ""train"":
        train()
if __name__ == ""__main__"":
    tf.app.run()

```
Just focus on the block after `if step % FLAGS.eval_steps == 1:`  It is all rightthe step shouldn't  add one to run on CPU(add line `with tf.device(""/cpu:0"")`  after`with tf.Session() as sess:` ) , But as show the below image, it will add one when we use GPU:
On CPU:
![image](https://cloud.githubusercontent.com/assets/3112825/23053809/dcc5d930-f516-11e6-8797-6701370c4da3.png)

On GPU:
![image](https://cloud.githubusercontent.com/assets/3112825/23053823/ee505298-f516-11e6-8b92-954b3da4d34f.png)

It's very strange for me! And I try to reproduce it on MNIST but i seem to be all right both On CPU and GPU.

Anyone could help me with  it ?
If you want to run the code just download the dataset from the url [data download](https://pan.baidu.com/s/1o84jIrg#list/path=%2F) 
"
7604,the use of MultiRNNCell?,"doc is in https://www.tensorflow.org/tutorials/recurrent

I found the line
stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm] * number_of_layers,

I need to use MultiRNNCell

but,I write those lines

a = [tf.nn.rnn_cell.BasicLSTMCell(10)]*3

print id(a[0]), id(a[1])

Its output is 4648063696 4648063696

can MultiRNNCell use the same object BasicLSTMCell as a list for parameter?
"
7602,Error importing TensorFlow in Jupyter Notebook,"I am unable to import TensorFlow in Jupyter Notebook. I have attached the error log at the end of the post. I am able to import the CPU Version of the TensorFlow with no issue. I am also able to import TensorFlow in Python Console launched from Terminal. The path to python executable is same in both Console and Jupyter Notebook (`sys.executable` gave the output as `/Users/Aakaash/miniconda3/bin/python` for both).

## EDIT:
I installed [JupyterLab](https://github.com/jupyterlab/jupyterlab) and to my surprise, TensorFlow imports with no errors when I open the ipynb file in JupyterLab. The problem arises only when I open the ipynb file with Jupyter Notebook.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
I posted a question on StackOverflow and I was told to file an issue on GitHub.

http://stackoverflow.com/questions/42273323/error-importing-tensorflow-gpu-in-jupyter-notebook?noredirect=1#comment71720944_42273323

### Environment info
Operating System: MacOS Sierra 10.12.3

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
````
lrwxr-xr-x  1 root     wheel        33 Feb 16 15:34 /usr/local/cuda/lib/libcuda.1.dylib -> /usr/local/cuda/lib/libcuda.dylib
-rwxr-xr-x  1 root     wheel     13504 Jan 25 01:28 /usr/local/cuda/lib/libcuda.dylib
lrwxr-xr-x@ 1 root     wheel        45 Nov  4 00:10 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudadevrt.a
lrwxr-xr-x@ 1 root     wheel        50 Nov  4 00:10 /usr/local/cuda/lib/libcudart.8.0.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib
lrwxr-xr-x@ 1 root     wheel        46 Nov  4 00:10 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.dylib
lrwxr-xr-x@ 1 root     wheel        49 Nov  4 00:10 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart_static.a
-rwxr-xr-x@ 1 Aakaash  staff  82210088 Nov  7 13:28 /usr/local/cuda/lib/libcudnn.5.dylib
lrwxr-xr-x@ 1 Aakaash  staff        16 Nov  7 13:49 /usr/local/cuda/lib/libcudnn.dylib -> libcudnn.5.dylib
-rw-r--r--@ 1 Aakaash  staff  66197088 Nov  7 13:28 /usr/local/cuda/lib/libcudnn_static.a
````
If installed from binary pip package, provide:

1. A link to the pip package you installed:
I used `pip install tensorflow-gpu`
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`:
````
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.8.0.dylib locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.5.dylib locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.8.0.dylib locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.1.dylib locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.8.0.dylib locally
1.0.0
````

### Logs or other output that would be helpful
````
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
/Users/Aakaash/miniconda3/lib/python3.5/site-packages/tensorflow/python/__init__.py in <module>()
     60     sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)
---> 61     from tensorflow.python import pywrap_tensorflow
     62     sys.setdlopenflags(_default_dlopen_flags)

/Users/Aakaash/miniconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow = swig_import_helper()
     29     del swig_import_helper

/Users/Aakaash/miniconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
     25             finally:

/Users/Aakaash/miniconda3/lib/python3.5/imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

/Users/Aakaash/miniconda3/lib/python3.5/imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: dlopen(/Users/Aakaash/miniconda3/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib
  Referenced from: /Users/Aakaash/miniconda3/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
  Reason: image not found

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-a649b509054f> in <module>()
----> 1 import tensorflow

/Users/Aakaash/miniconda3/lib/python3.5/site-packages/tensorflow/__init__.py in <module>()
     22 
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26 

/Users/Aakaash/miniconda3/lib/python3.5/site-packages/tensorflow/python/__init__.py in <module>()
     70 for some common reasons and solutions.  Include the entire stack trace
     71 above this error message when asking for help."""""" % traceback.format_exc()
---> 72   raise ImportError(msg)
     73 
     74 # Protocol buffers

ImportError: Traceback (most recent call last):
  File ""/Users/Aakaash/miniconda3/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Users/Aakaash/miniconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/Users/Aakaash/miniconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
  File ""/Users/Aakaash/miniconda3/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/Users/Aakaash/miniconda3/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Users/Aakaash/miniconda3/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib
  Referenced from: /Users/Aakaash/miniconda3/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
  Reason: image not found


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
````"
7600,[Discussion] model intellectual property protection,"Usually for mobile devices (or any devices which are publicly accessible) model deployment, there is a requirement for IP protection. Obviously, the current protobuf format is not desirable.

XLA can be a great help which will result in binary/so which is difficult for reverse engineering, but the current emphasis is mostly on performance improvement. For example, XLA does not cover the case when there are some customized op/kernels (executed either on standard CPU/GPU or customized accelerators) which could be common in mobile/IoT area."
7599,cuda::Diagnostician::FindKernelDriverVersion tries to access /proc/driver/nvidia/version on Windows,"Windows 10 64 bit
CUDA 8.0
cuDNN 5.1
tensorflow_gpu 1.0.0

### problem is here:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_diagnostics.cc#L345

### the other place which opens the file is disabled on Windows:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_diagnostics.cc#L143

### Logs or other output that would be helpful

    E ...\stream_executor\cuda\cuda_driver.cc:1002] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
    E ...\stream_executor\cuda\cuda_driver.cc:1002] failed to allocate 1.80G (1932735232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
    E ...\stream_executor\cuda\cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
    >>>    E ...\stream_executor\cuda\cuda_dnn.cc:404] error retrieving driver version: Permission denied: could not open driver version path for reading: /proc/driver/nvidia/version
    E ...\stream_executor\cuda\cuda_dnn.cc:364] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
"
7595,Shape inference issues after running dynamic_rnn,"I am trying to use dynamic_rnn in the encoder part of seq2seq library. I am using a placeholder for encoder input with dimensions [None, None] to reflect dynamic batch size and time steps. 

The problem is that the output that I get after the dynamic_rnn run has unknown shape along time dimension which causes issues when I try to use it along with attention. I have been able to overcome it by using the bucket length to set_shape but now that I am trying to ditch bucketing, it seems a little non-trivial. I was trying to use the maximum of sequence lengths but I have not been able to successfully use the value obtained from tf.reduce_max(sequence_length). Any help/suggestions would be much appreciated. Also, I am using version 0.11."
7591,Virtualenv setup returns ImportError,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
http://stackoverflow.com/questions/42274018/virtualenv-setup-returns-importerror?noredirect=1#comment71721085_42274018

### Environment info
Operating System: OS X El Capitan 10.11.5

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I am trying to install tensorflow with virtualenv, but I get the following error when I run ""virtualenv --system-site-packages tensorflow"". I am currently working this on macOS with python3.5.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

Traceback (most recent call last):
  File ""<stdin>"", line 4, in <module>
  File ""~/tensorflow/lib/python3.5/tempfile.py"", line 45, in <module>
    from random import Random as _Random
ImportError: cannot import name 'Random'
...Installing setuptools, pip, wheel...done.
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.5/bin/virtualenv"", line 11, in <module>
sys.exit(main())
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/virtualenv.py"", line 713, in main
symlink=options.symlink)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/virtualenv.py"", line 945, in create_environment
download=download,
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/virtualenv.py"", line 901, in install_wheel
call_subprocess(cmd, show_stdout=False, extra_env=env, stdin=SCRIPT)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/virtualenv.py"", line 797, in call_subprocess
% (cmd_desc, proc.returncode))
OSError: Command ~/tensorflow/bin/python3.5 - setuptools pip wheel failed with error code 1"
7590,"Tutorials on Estimators, Canned Estimators","Hi, 
I checked out the tensorflow dev-summit yesterday and came to know about Estimators and Canned Estimators. Estimators and Canned Estimators are intended to bring simple model-data wrapping capability to tensorflow, I believe. However, I am not able to find a lot of tutorials/docs on the interface for an estimator or documentation on how to write Canned Estimators/Estimators. tf.contrib.learn has estimator modules only for simple ML methods like mlps, linear regressors. I would like to know if there are gists/github code for estimators/canned estimators for cnns/auto-encoders/LSTMS. "
7585,Invalid argument error from tf.gather_nd after upgrade to r1.0,"I just upgraded to r1.0 and ran into an issue which I find hard to dissect further. The issue did not occur before when I was using version r0.12. The error message is the following:

```
InvalidArgumentError (see above for traceback): Must have updates.shape = indices.shape[:IXDIM] + params_shape[IXDIM:], got updates.shape [1,4,1700], indices.shape [1,4,2], params_shape [1,73,1700]
	 [[Node: gradients/GatherNd_5_grad/ScatterNd = ScatterNd[T=DT_FLOAT, Tindices=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](GatherNd_5/indices/_77, gradients/AddN_9/_79, gradients/GatherNd_5_grad/Shape)]]
```

and the stack trace tells me it stems from the last of the following lines:

```
def positional(visible):
    """"""
    :param visible: a tensor of size (batch_size, input_dim, sequence_length) representing the sequence to be optimized
    """"""
    FEET = np.array([4, 5, 8, 9])
    feet_idx = np.array([range(i, i+3) for i in FEET*3])
    batch_size = visible.get_shape()[0].value
    dim = visible.get_shape()[1].value
    seq_length = visible.get_shape()[2].value
    idx_x, idx_y, idx_z = feet_idx[:, 0], feet_idx[:, 1], feet_idx[:, 2]
    idx_x_t = [[[j, i] for i in idx_x] for j in range(batch_size)]
    idx_y_t = [[[j, i] for i in idx_y] for j in range(batch_size)]
    v_feet_x = tf.gather_nd(visible, idx_x_t)
    v_feet_y = tf.gather_nd(visible, idx_y_t) #  stack trace points here
    ...
```
This function is called to calculate a cost function which is used during an optimization procedure. See below for a minimum working example.  The error, as far as I can tell from the stack trace, does not happen when the graph is built, but when the computation is executed, which is why I'm not sure how I can further narrow down the problem.

### Environment info
Operating System: Ubuntu 14.04, x64

Installed version of CUDA and cuDNN: 8.0 and cuDNN 5.1
```
-rw-r--r-- 1 root root   556000 Jan 27 00:48 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Jan 27 00:51 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Jan 27 00:51 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
-rw-r--r-- 1 root root   415432 Jan 27 00:48 /usr/local/cuda/lib64/libcudart.so.8.0.61
-rw-r--r-- 1 root root   775162 Jan 27 00:48 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 Nov 30 11:39 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 Nov 30 11:39 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxr-xr-x 1 root root 79337624 Nov 30 11:39 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Nov 30 11:39 /usr/local/cuda/lib64/libcudnn_static.a
```
Installed from source from revision `16485a3fb5ffcbaa244e55c388e43279d2770982` using bazel 0.4.4

```
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261
```

### Minimum working example

```
import tensorflow as tf
import numpy as np

def get_costs(velos, seq_length):
    velo_diff = tf.subtract(tf.slice(velos, [0, 0, 1], [-1, -1, seq_length-1]),
                            tf.slice(velos, [0, 0, 0], [-1, -1, seq_length-1]))
    velo_diff_sq = tf.multiply(velo_diff, velo_diff)
    return tf.reduce_mean(velo_diff_sq)


def positional(visible):
    """"""
    :param visible: a tensor of size (batch_size, input_dim, sequence_length) representing the sequence to be optimized
    """"""
    FEET = np.array([4, 5, 8, 9])
    feet_idx = np.array([range(i, i+3) for i in FEET*3])
    batch_size = visible.get_shape()[0].value
    dim = visible.get_shape()[1].value
    seq_length = visible.get_shape()[2].value
    idx_x, idx_y, idx_z = feet_idx[:, 0], feet_idx[:, 1], feet_idx[:, 2]
    idx_x_t = [[[j, i] for i in idx_x] for j in range(batch_size)]
    idx_y_t = [[[j, i] for i in idx_y] for j in range(batch_size)]
    v_feet_x = tf.gather_nd(visible, idx_x_t)
    v_feet_y = tf.gather_nd(visible, idx_y_t)
    return get_costs(v_feet_y, seq_length)


visible = tf.Variable(np.reshape(np.arange(365), [1, 73, 5]), dtype=tf.float32)
cost_op = positional(visible)
train_op = tf.train.AdamOptimizer(0.01).minimize(cost_op)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run([train_op])
```
"
7583,installed r1.0 -models directory is missing .,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7582,Delete some files on Windows10 occured error!,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem? No

### Environment info
Operating System: Win10

Installed version of CUDA and cuDNN: No
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide: Yes

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. 1.0.0

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried? No


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
In pycharm, the error log is :
```
""D:\Program Files (x86)\Miniconda3\python.exe"" G:/codes/tensorflow/models-master/autoencoder/AdditiveGaussianNoiseAutoencoderRunner.py
Extracting MNIST_data\train-images-idx3-ubyte.gz
Extracting MNIST_data\train-labels-idx1-ubyte.gz
Extracting MNIST_data\t10k-images-idx3-ubyte.gz
Extracting MNIST_data\t10k-labels-idx1-ubyte.gz
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
Traceback (most recent call last):
  File ""G:/codes/tensorflow/models-master/autoencoder/AdditiveGaussianNoiseAutoencoderRunner.py"", line 32, in <module>
    scale = 0.01)
  File ""G:\codes\tensorflow\models-master\autoencoder\autoencoder_models\DenoisingAutoencoder.py"", line 25, in __init__
    self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.sub(self.reconstruction, self.x), 2.0))
AttributeError: module 'tensorflow' has no attribute 'sub'

```
I clean some files on **c:**,   then i run the model zoo's example `models-master/autoencoder/AdditiveGaussianNoiseAutoencoderRunner.py` , unfortunately, it occurs the above error, what's happen?"
7581,Status of Allocator API,"Making this issue to track status of allocator C API, based on discussion at tfdev conference: cc @josh11b @vrv @skye @keveman @zheng-xq @keveman @yuanbyu 

Currently allocator API is not public. This API, in `tensorflow/core/framework/allocator.h`, and more specifically, `bytes_in_use`, is currently the only practical way to implement user ops that

1. Decide what to do based on available memory (like `foldr`, `map_fn`, with `swap_memory=True` option)
2. Report available memory to user (https://github.com/tensorflow/tensorflow/issues/7537)

The three options are:

1. Make this API public
2. Keep this API non-public, but make it possible to create a user-op that uses this API.
3. Keep this API non-public and do not provide a way to use it from user-op.

Currently it's somewhere between 2 and 3. API is not public and you can build user-op like [memory_probe_op](https://github.com/yaroslavvb/memory_probe_ops) with gcc, but not with Bazel. From an op-creator standpoint, 1 is preferable to 2 and 2 is preferable to 3. 

Similar issue was https://github.com/tensorflow/tensorflow/issues/1419 where people tried to make a custom reader user op that uses methods from `reader_base.h`. The final solution was the opposite -- it was possible to make work with bazel (by commenting out `disallowed_deps` line), but not with `gcc`."
7579,Broken class hierarchy in sample code in getting started tutorial.,"The last example in https://www.tensorflow.org/get_started/get_started does not work. When run it gives the following error message:

```
<ipython-input-65-6c74aa534fa7> in model(features, labels, mode, params)
     18     # ModelFnOps connects subgraphs we built to the
     19     # appropriate functionality.
---> 20     return tf.contrib.learn.estimators.model_fn.ModelFnOps(
     21         mode=mode, predictions=y,
     22         loss= loss,

AttributeError: module 'tensorflow.contrib.learn' has no attribute 'estimators'
```


This comes from this line:

```
return tf.contrib.learn.estimators.model_fn.ModelFnOps(
        mode=mode, predictions=y,
        loss= loss,
        train_op=train)
```

There is a ModelFnOps in tf.contrib.learn and when using that the code at least runs. The result, though, seems to differ from what is mentioned in the doc.

tf.VERSION is 1.0.0."
7578,"cifar10_multi_gpu_train.py is mentioned in the tutorial, but doesn't exist in the source tree","cifar10_multi_gpu_train.py was removed in the 1.0, but still exists in the docs:
https://www.tensorflow.org/tutorials/deep_cnn

Where can users find examples for multi-GPU/distributed training compatible with v1.0?"
7577,A suggestion for controlled dependency in batch normalization layer,"In documents of `tf.contrib.layers.batch_norm`, it is suggested to bind update op of moving_mean and moving_variance with loss function, as follows:
```
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) 
if update_ops: 
    updates = tf.group(*update_ops)
    total_loss = control_flow_ops.with_dependencies([updates], total_loss)
```
Is it better to bind `update_op` with `train_op`? Say, maybe one want to evaluate loss but do not update parameters?"
7576,GrpcRemoteMaster was chosen instead of LocalMaster,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
none.

### Environment info
Operating System:
Windows 10

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
none

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
 9570f0f2804e857bc5593bed526eda7c1c915ed9
2. The output of `bazel version`
(compiled by cmake)
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
```
import tensorflow as tf
from tensorflow.python.client import session

def main(_):
  # Create a cluster from the parameter server and worker hosts.
  cluster = tf.train.ClusterSpec({
    ""worker"": [
        ""127.0.0.1:10000""        
    ],
    ""ps"": [
        ""127.0.0.1:10001""
    ]})

  # Create and start a server for the local task.
  server = tf.train.Server(cluster,
                           job_name=""worker"",
                           task_index=0)
  with tf.device(""/job:ps/task:0""):
    weights = tf.Variable(tf.random_normal([10], stddev=0.35), name=""weights"")  
  init_op = tf.global_variables_initializer()
  with tf.Session(""grpc://127.0.0.1:10000"") as sess:
    sess.run(init_op)
    w  = sess.run(weights)
    print(w)

if __name__ == ""__main__"":
  tf.app.run()
```
### What other attempted solutions have you tried?
change 
```
  with tf.Session(""grpc://127.0.0.1:10000"") as sess:
```
to
```
  with tf.Session(""grpc://localhost:10000"") as sess:
```

solved it.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
The reason is:
In tensorflow\core\distributed_runtime\rpc\grpc_server_lib.cc: GrpcServer::target() function, the host part of target is hardcoded to ""localhost""
```
const string GrpcServer::target() const {
  return strings::StrCat(""grpc://localhost:"", bound_port_);
}
```
So, if the session was created with another address string, e.g. 127.0.0.1, (localip), etc. GrpcRemoteMaster  will be used instead of LocalMaster. I suggest LocalMaster class should have a isLocalIP() function to deal with this.


"
7573,Tensorflow freezes during execution of session.run(),"We currently experience an issue with an implementation. During the execution of session.run, the process suddenly freezes. It does not crash but is irresponsive to ctrl+c. It isn't consuming any CPU anymore (and is not progressing either). This occurs on CPU, we haven't tested GPU. The issue seems to be highly related to #2788.

We ran the script on a machine running an up-to-date Ubuntu 16.04 with 128gb of ram, and 2 x Xeon CPU E5-2640 v4 processors. The issue occured with tensorflow 0.12.1 installed through anaconda. Then we reproduced the issue using a build of the master branch, without any CUDA support, using the system libraries rather than those shipped with anaconda.

The build of the master branch shows:
print(tensorflow.__version__)
1.0.0-rc2

$ git rev-parse HEAD
1a0742f6a7a06ff54481385b5c51094b0fef8cf3

$ bazel version
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261

Attached is the output of running  thread apply all bt 10 in gdb. All threads appear to be waiting. 
We are running a model locally implemented using GPflow (not one of the default models of that library though), unfortunately this occurs on a confidential data set. If required we can look into providing a MWE but I can not guarantee we can reproduce this easily under different circumstances. 
[gdb.txt](https://github.com/tensorflow/tensorflow/files/780004/gdb.txt)
"
7572,Bazel version required for TensorFlow 1.0 ?,"Hi all,

Neither the README.md nor https://www.tensorflow.org/install/install_sources mentions the minimal bazel version that is required for building tensorflow 1.0 from source.

I have successfully built versions 0.9 and 0.10 from source using bazel 0.3.0 (first I had installed version 0.2.4, then while compiling tensorflow I got errors, opened an issue on github and was told to update to 0.3.0).

Now I tried building tensorflow 1.0 and the same story repeats. I get error messages, google for them. Google points me to a github issue (in this case #6436) and there people are told to upgrade bazel to a newer version (0.4.2).

Please start specifying somewhere a minimal requirement for the bazel version used to build a particular tensorflow version.

And if one needs to install a new bazel version each time one wants to install a newer tensorflow version, then please make bazel a part of tensorflow.

I am building a lot of software from source code. If you for instance look at other build systems as for example CMake. Most software that I build nowadays with CMake still builds fine with version 2.8.12, which was released in 2013. For CMake requiring a newer version is the exception, whereas for bazel it seems to be the rule.

Best regards

Sam"
7570,What can replace tf.RegisterShape in tensorflow 1.0.0?,"```
Traceback (most recent call last):
  File ""tools/train_net.py"", line 18, in <module>
    from networks.factory import get_network
  File ""./ImageToolkit0/face_detection/lib/networks/__init__.py"", line 8, in <module>
    from .VGGnet_train import VGGnet_train
  File ""./ImageToolkit0/face_detection/lib/networks/VGGnet_train.py"", line 2, in <module>
    from networks.network import Network
  File ""./ImageToolkit0/face_detection/lib/networks/network.py"", line 4, in <module>
    import roi_pooling_layer.roi_pooling_op_grad
  File ""./ImageToolkit0/face_detection/lib/roi_pooling_layer/roi_pooling_op_grad.py"", line 7, in <module>
    @tf.RegisterShape(""RoiPool"")
AttributeError: 'module' object has no attribute 'RegisterShape'
```

So what can replace `tf.RegisterShape` in tensorflow 1.0.0?"
7569,AttributeError: module 'tensorflow.contrib.rnn' has no attribute 'stack_bidirectional_dynamic_rnn',"This issue arises both on cpu and gpu version of tensorflow 1.0, installed with pip for python3.
I've tested it with Ubuntu 16.04 (cuda 8.0 and cudnn 5.1) for gpu, and with Archlinux for cpu.

Strangely, the function is present in `/path/to/python/packages/tensorflow/contrib/rnn/python/ops/rnn.py` and seems to be imported in `/path/to/python/packages/tensorflow/contrib/rnn/__init__.py`.

```
In [1]: import tensorflow as tf
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally

In [2]: tf.__version__
Out[2]: '1.0.0'

In [3]: print(dir(tf.contrib.rnn))
['AttentionCellWrapper', 'BasicLSTMCell', 'BasicRNNCell', 'CoupledInputForgetGateLSTMCell', 'DropoutWrapper', 'EmbeddingWrapper', 'FusedRNNCell', 'FusedRNNCellAdaptor', 'GRUBlockCell', 'GRUCell', 'GridLSTMCell', 'InputProjectionWrapper', 'LSTMBlockCell', 'LSTMBlockFusedCell', 'LSTMBlockWrapper', 'LSTMCell', 'LSTMStateTuple', 'LayerNormBasicLSTMCell', 'MultiRNNCell', 'OutputProjectionWrapper', 'RNNCell', 'TimeFreqLSTMCell', 'TimeReversedFusedRNN', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'core_rnn_cell', 'static_bidirectional_rnn', 'static_rnn', 'static_state_saving_rnn']

```"
7568,gen_nn_ops not found,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

In tensorflow/python/ops directory the file gen_nn_ops is missing.
Can you tell how it is being imported?
"
7565,Problems on using static library of tensorflow,"I working on a big c++ project, which is big and doesn't use bazel as its build tool. So I am trying to build a static library on tensorflow. By following the tutorial at [tensorflow/contrib/makefile](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile) , I successfully build libtensorflow-core.a. But when I try to include ""tensorflow/core/public/session.h"" in my main.cc and run ""gcc -c main.cc"". It shows an error: 'tensorflow/core/framework/graph.pb.h' file not found. That file is automatically generated by bazel. However I couldn't use bazel. 
Does anyone know about this problem? Thanks a lot."
7562,Standalone Tensorflow Projector,"Sorry to double open this issue but did not get any answer from StackExchange forum : 
http://stackoverflow.com/questions/41643365/standalone-tensorflow-projector

Tensorflow Projector is a great way to visualize a mutli dimensional dataset and I was wondering if you are planning to open source the version running on the web (ie outside tensorboard), so we would be able to visualize any dataset from a simple TSV file (and not only word embedding or other learnt representation from TF)

Thanks"
7561,Different buckets give different outputs at first time step during decoding,"Hi,

**Background**
I'm trying to build a chat bot and have a basic understanding of tensorflow Sequence2Sequence API  by reading from here: 
https://www.tensorflow.org/tutorials/seq2seq/

I've also read related papers for Neural translation, attention mechanism during encoding/decoding etc.

RNN translate code is taken from here:
https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/seq2seq_model.py

Bucketing/Attention mechanism code is picked from here:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py

During my training(attention mechanism, GRU cell), I gave my buckets as [(2,2),(4,4),(6,6),(,8,8),(10,10)].

**Issue**

During decoding, if I force a bucket_index on the same input I'm getting totally different outputs at the first time step. For example:

1) Input : How're you ?
    Bucket index : 1

    Output : NAME UNK NAME

   Input : How're you ?
    Bucket index : 2

    Output : Hi NAME to the

**Observations**

1)  I can fix the bad outputs at each time step using beam-search. However, why I'm getting different word as output at the first time step during decoding ? Shouldn't the smaller bucket output be a subset of larger bucket output ? 

2) I've tried searching online and everywhere it's mentioned that parameters are shared across various buckets.  Buckets are used for training efficiency and not for model tuning. I've also verified that my trainable parameters are common across all the buckets.

3) The biggest difference I can see between encoder inputs for various buckets is the extra padding at the very start. We are doing attention mechanism on encoded_states, and softmax weights are already learned for those encoded states during training. Therefore,  Can those softmax weights for those extra padded inputs cause sufficient difference leading to different output during first time step of decoding ?

Did any one else also encounter the above issue?  Any help for fixing the above error would be greatly appreciated.

"
7560,"tar: Unrecognized archive format error when trying to unpack flower_photos.tgz, TF tutorials on OSX","This is relating to the 
[How to Retrain Inception's Final Layer for New Categories](https://www.tensorflow.org/tutorials/image_retraining) Tutorial

I'm trying to unpack the flower_photos.tgz after curling it using 

    curl -O http://download.tensorflow.org/example_../images/flower_photos.tgz
    tar xzf flower_photos.tgz

This is from the image retraining tutorial for TensorFlow

Results from curling

```
    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
    100   127  100   127    0     0    255      0 --:--:-- --:--:-- --:--:--   255
```

Then when I try to unpack


```
tar xzf flower_photos.tgz
tar: Unrecognized archive format
tar: Error exit delayed from previous errors.
```"
7559,TypeError: zeros_initializer() missing 1 required positional argument: 'shape',
7558,tensorflow_gpu-1.0.0-cp35-cp35m-win_x86_64.whl is not a supported wheel on this platform.,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7556,issues install on windows conda env,"I have tried to install the tensorflow non-gpu version in the conda env using pip but I got following error:

    ""AttributeError: 'NoneType' object has no attribute 'splitlines'"" 

But when I install the gpu version it seems fine. Any ideas?"
7554,TFLearn Estimator Summary Writer Fails.,"### Issue:
TFLearn estimator summary writer fails because _write_dict_to_summary() in [/tensorflow/contrib/learn/python/learn/estimators/estimator.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L324) dose not allow int values, which naturally stops the np.int64 valued 'global_step' parameter from being written, thus causing the summary writer to fail.

### How to reproduce:
Try and run [wide_n_deep_tutorial.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py), and you will get:

`INFO:tensorflow:Saving dict for global step 202: accuracy = 0.818254, accuracy/baseline_label_mean = 0.236226, accuracy/threshold_0.500000_mean = 0.818254, auc = 0.742517, global_step = 202, labels/actual_label_mean = 0.236226, labels/prediction_mean = 0.182356, loss = 0.680463, precision/positive_threshold_0.500000_mean = 0.764145, recall/positive_threshold_0.500000_mean = 0.333593
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.`

### Pull Request:
#7555"
7553,"The updated documentation site does not contain ""Setting up TensorFlow for Development""","Recently the TensorFlow web site is updated, and some contents are re-organized.
Since I always forget how to install TF from sources, such as ""bazel build ..."" then ""bazel-bin/..."", I frequently refer the official web site.
However, after the update, ""Setting up TensorFlow for Development"" section has gone away (which were originally located at https://www.tensorflow.org/versions/r0.12/get_started/os_setup).
I think the contents should be included in ""Installing TensorFlow from sources"" section, if there was no change in the way for setting for development."
7552,tensorflow_gpu-1.0.0-cp35-cp35m-win_x86_64.whl is not a supported wheel on this platform.,"I created an environment with following command on Windows:
```
conda create --name tensorflow python=3.5 anaconda
```
and try to install tensorflow 1.0
```
pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensor flow_gpu-1.0.0-cp35-cp35m-win_x86_64.whl
```
but it reported an error as following:
```
tensorflow_gpu-1.0.0-cp35-cp35m-win_x86_64.whl is not a supported wheel on this platform.
```
my machine is 64bit win10, can anyone tell me  what's the problem?"
7551,Addition is much slower on non-last axis (non-fused batch norm with NCHW),"I noticed this from observing my models training many times slower when using non-fused batch norm and the NCHW data format. When looking at the timeline, it's dominated by addition (and multiplication) operations.

I can mostly work around this by using the fused batch norm, but DenseNet models in principle (and in practice when using NHWC here) benefit from splitting up the learned beta/gamma and the normalization steps from batch normalization, and using a straightforward implementation (included below) makes my model run significantly slower when using NCHW.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

I did a quick search on the issue tracker and SO, and couldn't find anything similar reported.

### Environment info

gcr.io/tensorflow/tensorflow:1.0.0-devel-gpu on AWS p2.xlarge

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Compare:

```python
%%time

with tf.Graph().as_default(), tf.Session() as sess:
    zeros = tf.zeros((64, 32, 32, 256))
    beta = tf.get_variable(
        'beta',
        (256,),
        initializer=tf.ones_initializer(),
        trainable=True,
    )

    loss = tf.reduce_mean((zeros + beta) ** 2)
    optimizer = tf.train.MomentumOptimizer(0.1, 0.9)
    train_op = optimizer.minimize(loss)

    sess.run(tf.global_variables_initializer())

    for i in range(500):
        sess.run(train_op)
```

```
CPU times: user 7.8 s, sys: 868 ms, total: 8.67 s
Wall time: 9.69 s
```

v.

```python
%%time

with tf.Graph().as_default(), tf.Session() as sess:
    zeros = tf.zeros((64, 256, 32, 32))
    beta = tf.get_variable(
        'beta',
        (256,),
        initializer=tf.ones_initializer(),
        trainable=True,
    )
    beta = tf.reshape(beta, (256, 1, 1))

    loss = tf.reduce_mean((zeros + beta) ** 2)
    optimizer = tf.train.MomentumOptimizer(0.1, 0.9)
    train_op = optimizer.minimize(loss)

    sess.run(tf.global_variables_initializer())

    for i in range(500):
        sess.run(train_op)
```

```
CPU times: user 14.6 s, sys: 2.81 s, total: 17.4 s
Wall time: 18.9 s
```

This is the scale/bias transform that triggers the problem for me in practice:

```python
def affine_transformation(
    inputs,
    axis=-1,
    beta_initializer=tf.zeros_initializer(),
    gamma_initializer=tf.ones_initializer(),
    beta_regularizer=None,
    gamma_regularizer=None,
):
    inputs_shape = inputs.get_shape()
    params_dim = inputs_shape[axis]

    beta = tf.get_variable(
        'beta',
        (params_dim,),
        initializer=beta_initializer,
        regularizer=beta_regularizer,
        trainable=True,
    )
    gamma = tf.get_variable(
        'gamma',
        (params_dim,),
        initializer=gamma_initializer,
        regularizer=gamma_regularizer,
        trainable=True,
    )

    if axis != -1:
        params_shape = [1] * len(inputs_shape)
        params_shape[axis] = params_dim.value

        beta = tf.reshape(beta, params_shape)
        gamma = tf.reshape(gamma, params_shape)

    return gamma * inputs + beta
```"
7550,AttributeError: 'module' object has no attribute 'pack',"Training images using keras with tf backend.The error shows that tf (version==0.12.head) has no attribute ""pack"".
```python
Traceback (most recent call last):
  File ""binary_convnets.py"", line 30, in <module>
    model.add(Flatten())
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 324, in add
    output_tensor = layer(self.outputs[0])
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 517, in __call__
    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 571, in add_inbound_node
    Node.create_node(self, inbound_layers, node_indices, tensor_indices)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 155, in create_node
    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))
  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 440, in call
    return K.batch_flatten(x)
  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 862, in batch_flatten
    x = tf.reshape(x, tf.pack([-1, prod(shape(x)[1:])]))
AttributeError: 'module' object has no attribute 'pack'
```
"
7549,tf.layers.batch_normalization does not support fused,"`tf.layers.batch_normalization` does not accept a `fused` argument, and appears to always use `tf.nn.batch_normalization` instead of `tf.nn.fused_batch_norm`. As such, it does not appear to be possible to follow performance best-practices when using `tf.layers.batch_normalization`.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Searched issues for ""fused"" and ""batch_normalization""

### Environment info
Operating System: gcr.io/tensorflow/tensorflow:1.0.0-devel-gpu

### What other attempted solutions have you tried?
Currently shimming `tf.contrib.layers.batch_norm`
"
7547,Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912,"I am following this tutorial http://www.bitfusion.io/2016/08/31/training-a-bird-classifier-with-tensorflow-and-tflearn/ I assume that training was done but the system was restarted so I can't verify if the 100 epochs were done. Can you please suggest fixes? Is this a tflearn and tensorflow version mismatch? What can be done? 

```
mona@pascal:~/computer_vision/python_playground$ python infer.py test_images/
bird_african_fish_eagle.jpg          bird_mount_bluebird.jpg              not_a_bird_creativecommons_logo.jpg  
bird_bullocks_oriole.jpg             not_a_bird_airplane.jpg              not_a_bird_stop_sign.jpg             
mona@pascal:~/computer_vision/python_playground$ python infer.py test_images/not_a_bird_stop_sign.jpg 
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:03:00.0
Total memory: 11.92GiB
Free memory: 11.85GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3771170
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:83:00.0
Total memory: 11.92GiB
Free memory: 11.85GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40c, pci bus id: 0000:83:00.0)
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tflearn/summaries.py:46 in get_summary.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tflearn/summaries.py:46 in get_summary.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.py:766 in create_summaries.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.merge.
WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.py:130 in __init__.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40c, pci bus id: 0000:83:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40c, pci bus id: 0000:83:00.0)
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.py:378 in restore.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
Traceback (most recent call last):
  File ""infer.py"", line 44, in <module>
    model.load(""bird-classifier.tfl.ckpt-50912"")
  File ""/usr/local/lib/python2.7/dist-packages/tflearn/models/dnn.py"", line 227, in load
    self.trainer.restore(model_file)
  File ""/usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.py"", line 379, in restore
    self.restorer.restore(self.session, model_file)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1388, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 964, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
     [[Node: save_1/RestoreV2_14 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save_1/Const_0, save_1/RestoreV2_14/tensor_names, save_1/RestoreV2_14/shape_and_slices)]]
     [[Node: save_1/RestoreV2_21/_17 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_158_save_1/RestoreV2_21"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

Caused by op u'save_1/RestoreV2_14', defined at:
  File ""infer.py"", line 43, in <module>
    model = tflearn.DNN(network, tensorboard_verbose=0, checkpoint_path='bird-classifier.tfl.ckpt')
  File ""/usr/local/lib/python2.7/dist-packages/tflearn/models/dnn.py"", line 57, in __init__
    session=session)
  File ""/usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.py"", line 125, in __init__
    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1000, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1030, in build
    restore_sequentially=self._restore_sequentially)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 624, in build
    restore_sequentially, reshape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 361, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 200, in restore_op
    [spec.tensor.dtype])[0])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 441, in restore_v2
    dtypes=dtypes, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__
    self._traceback = _extract_stack()

NotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bird-classifier.tfl.ckpt-50912
     [[Node: save_1/RestoreV2_14 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save_1/Const_0, save_1/RestoreV2_14/tensor_names, save_1/RestoreV2_14/shape_and_slices)]]
     [[Node: save_1/RestoreV2_21/_17 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_158_save_1/RestoreV2_21"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

[1]+  Killed                  python2 infer.py ${f} 2> /dev/null
mona@pascal:~/computer_vision/python_playground$ ls *50912(
-bash: syntax error near unexpected token `('
mona@pascal:~/computer_vision/python_playground$ ls *50912*
bird-classifier.tfl.ckpt-50912.data-00000-of-00001  bird-classifier.tfl.ckpt-50912.index  bird-classifier.tfl.ckpt-50912.meta
```
Here are the dumped training files I have http://pastebin.com/9RF58yBB


### Environment info
Operating System:
```
$ uname -a ; lsb_release -a 
Linux pascal 3.13.0-62-generic #102-Ubuntu SMP Tue Aug 11 14:29:36 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 14.04.5 LTS
Release:	14.04
Codename:	trusty
```

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```
$ ls /usr/local/cuda-8.0/lib64/
libcublas_device.a   libcudnn.so.5.1.10   libcuinj64.so.8.0.44   libcusparse.so.8.0.44  libnppicom.so         libnppim.so          libnppisu.so.8.0.44  libnvgraph.so.8.0            libOpenCL.so
libcublas.so         libcudnn_static.a    libculibos.a           libcusparse_static.a   libnppicom.so.8.0     libnppim.so.8.0      libnppitc.so         libnvgraph.so.8.0.44         libOpenCL.so.1
libcublas.so.8.0     libcufft.so          libcurand.so           libnppc.so             libnppicom.so.8.0.44  libnppim.so.8.0.44   libnppitc.so.8.0     libnvgraph_static.a          libOpenCL.so.1.0
libcublas.so.8.0.45  libcufft.so.8.0      libcurand.so.8.0       libnppc.so.8.0         libnppidei.so         libnppi.so           libnppitc.so.8.0.44  libnvrtc-builtins.so         libOpenCL.so.1.0.0
libcublas_static.a   libcufft.so.8.0.44   libcurand.so.8.0.44    libnppc.so.8.0.44      libnppidei.so.8.0     libnppi.so.8.0       libnpps.so           libnvrtc-builtins.so.8.0     stubs
libcudadevrt.a       libcufft_static.a    libcurand_static.a     libnppc_static.a       libnppidei.so.8.0.44  libnppi.so.8.0.44    libnpps.so.8.0       libnvrtc-builtins.so.8.0.44
libcudart.so         libcufftw.so         libcusolver.so         libnppial.so           libnppif.so           libnppi_static.a     libnpps.so.8.0.44    libnvrtc.so
libcudart.so.8.0     libcufftw.so.8.0     libcusolver.so.8.0     libnppial.so.8.0       libnppif.so.8.0       libnppist.so         libnpps_static.a     libnvrtc.so.8.0
libcudart.so.8.0.44  libcufftw.so.8.0.44  libcusolver.so.8.0.44  libnppial.so.8.0.44    libnppif.so.8.0.44    libnppist.so.8.0     libnvblas.so         libnvrtc.so.8.0.44
libcudart_static.a   libcufftw_static.a   libcusolver_static.a   libnppicc.so           libnppig.so           libnppist.so.8.0.44  libnvblas.so.8.0     libnvToolsExt.so
libcudnn.so          libcuinj64.so        libcusparse.so         libnppicc.so.8.0       libnppig.so.8.0       libnppisu.so         libnvblas.so.8.0.44  libnvToolsExt.so.1
libcudnn.so.5        libcuinj64.so.8.0    libcusparse.so.8.0     libnppicc.so.8.0.44    libnppig.so.8.0.44    libnppisu.so.8.0     libnvgraph.so        libnvToolsExt.so.1.0.0
```

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
```
$ python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
0.12.1

```
If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
```
mona@pascal:~/tf/tensorflow$ git rev-parse HEAD
156da397dc2e354baeac10804c5e9c1b3af8b7eb
```

2. The output of `bazel version`
```
$ bazel version
...........................
Build label: 0.4.3
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Dec 22 12:31:25 2016 (1482409885)
Build timestamp: 1482409885
Build timestamp as int: 1482409885
```

"
7544,Mistake in 'a custom model' tutorial code,"The code in [this example](https://www.tensorflow.org/get_started/get_started#a_custom_model) doesn't work as is.

I found changing line 20 from:
```
return tf.contrib.learn.estimators.model_fn.ModelFnOps(
```
to
```
return tf.contrib.learn.ModelFnOps(
```
makes it work.

Thanks for the great code!"
7543,"Error on building with non-system GCC: ""gcc: error trying to exec 'as': execvp: No such file or directory""","# Description
Hi,

I am trying to compile TensorFlow with non-default GCC. 5.4.0, and get follwing error:
```
gcc: error trying to exec 'as': execvp: No such file or directory
ERROR: /local/cvsupport/_bazel_cvsupport/c61d2ac558d6d30ef2694b9af72e4144/external/nccl_archive/BUILD.bazel:33:1: output 'external/nccl_archive/_objs/nccl/external/nccl_archive/src/libwrap.cu.pic.o' was not created.
ERROR: /local/cvsupport/_bazel_cvsupport/c61d2ac558d6d30ef2694b9af72e4144/external/nccl_archive/BUILD.bazel:33:1: not all outputs were created or valid.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
# Environment
```
$ cat /etc/redhat-release 
CentOS Linux release 7.3.1611 (AltArch)

$ uname -a
Linux power004.cluster 3.10.0-514.6.1.el7.ppc64le #1 SMP Thu Jan 19 14:34:54 GMT 2017 ppc64le ppc64le ppc64le GNU/Linux

$ ls -l /trinity/shared/apps/cv-ppc64le/nvidia/cuda/8.0/lib64/libcud*
-rw-r--r-- 1 root root 559800 Oct 29 10:22 /trinity/shared/apps/cv-ppc64le/nvidia/cuda/8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Feb 14 23:26 /trinity/shared/apps/cv-ppc64le/nvidia/cuda/8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root     19 Feb 14 23:26 /trinity/shared/apps/cv-ppc64le/nvidia/cuda/8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.54
-rwxr-xr-x 1 root root 476024 Oct 29 10:22 /trinity/shared/apps/cv-ppc64le/nvidia/cuda/8.0/lib64/libcudart.so.8.0.54
-rw-r--r-- 1 root root 966166 Oct 29 10:22 /trinity/shared/apps/cv-ppc64le/nvidia/cuda/8.0/lib64/libcudart_static.a

$ git rev-parse HEAD
16485a3fb5ffcbaa244e55c388e43279d2770982

$ bazel version
INFO: $TEST_TMPDIR defined: output root default is '/local/cvsupport'.
................
Build label: 0.4.4- (@non-git)
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Feb 9 23:48:24 2017 (1486684104)
Build timestamp: 1486684104
Build timestamp as int: 1486684104
```
# Steps to reproduce
```
$ module display gcc/5.4.0
-------------------------------------------------------------------
/trinity/shared/modulefiles/cv-ppc64le/gcc/5.4.0:

module-whatis    loads the gcc/5.4.0 environment 
prepend-path     PATH /trinity/shared/apps/cv-ppc64le/gcc/5.4.0/bin 
prepend-path     LD_LIBRARY_PATH /trinity/shared/apps/cv-ppc64le/gcc/5.4.0/lib 
prepend-path     LD_LIBRARY_PATH /trinity/shared/apps/cv-ppc64le/gcc/5.4.0/lib64 
prepend-path     LIBRARY_PATH /trinity/shared/apps/cv-ppc64le/gcc/5.4.0/lib 
prepend-path     LIBRARY_PATH /trinity/shared/apps/cv-ppc64le/gcc/5.4.0/lib64 
prepend-path     C_INCLUDE_PATH /trinity/shared/apps/cv-ppc64le/gcc/5.4.0/include 
prepend-path     INCLUDE /trinity/shared/apps/cv-ppc64le/gcc/5.4.0/include 
prepend-path     CPATH /trinity/shared/apps/cv-ppc64le/gcc/5.4.0/include 
prepend-path     MANPATH /trinity/shared/apps/cv-ppc64le/gcc/5.4.0/share/man 

$ cat ../build_vars.sh
export TEST_TMPDIR=/local/cvsupport
export PYTHON_BIN_PATH=/usr/bin/python3.4
export PYTHON_LIB_PATH=/usr/lib64/python3.4/site-packages
export TF_NEED_JEMALLOC=1
export TF_NEED_GCP=0
export TF_NEED_HDFS=0
export TF_NEED_OPENCL=0
export TF_NEED_CUDA=1
export TF_ENABLE_XLA=0
export CC_OPT_FLAGS=""-march=native""
export TF_CUDA_VERSION=8.0
export TF_CUDNN_VERSION=5.1.10
export TF_CUDA_COMPUTE_CAPABILITIES=6.0
export GCC_HOST_COMPILER_PATH=/trinity/shared/apps/cv-ppc64le/gcc/5.4.0/bin/gcc
export CUDA_TOOLKIT_PATH=/trinity/shared/apps/cv-ppc64le/nvidia/cuda/${TF_CUDA_VERSION}
export CUDNN_INSTALL_PATH=/trinity/shared/apps/cv-ppc64le/nvidia/cudnn/${TF_CUDA_VERSION}

$ module load nvidia/cuda/8.0 nvidia/cudnn/8.0 gcc/5.4.0 bazel/0.4.4
$ source ../build_vars.sh
$ ./configure
$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```

# Investigation
`as` is installed on my system, but wasn't built by gcc-5.4, I am using system-default one:
```
$ rpm -qf /usr/bin/as
binutils-2.25.1-22.base.el7.ppc64le
```
Digging into stace output, It seems like builder is trying to find `as` using some incorrect assumptions, and failed:
```
134767 execve(""external/local_config_cuda/crosstool/clang/bin/../../../cuda/bin/../open64/bin/as"", [""as"", ""-I"", ""."", ""-I"", ""external/nccl_archive/src"", ""-I"", ""external/local_config_cuda/cross""..., ""-a64"", ""-mppc64"", ""-many"", ""-mlittle"", ""-o"", ""bazel-out/local_linux-py3-opt/bi""..., ""/tmp/cc7ZvHce.s""], [/* 19 vars */] <unfinished ...>
134767 <... execve resumed> )           = -1 ENOENT (No such file or directory)
134767 execve(""external/local_config_cuda/crosstool/clang/bin/../../../cuda/bin/../nvvm/bin/as"", [""as"", ""-I"", ""."", ""-I"", ""external/nccl_archive/src"", ""-I"", ""external/local_config_cuda/cross""..., ""-a64"", ""-mppc64"", ""-many"", ""-mlittle"", ""-o"", ""bazel-out/local_linux-py3-opt/bi""..., ""/tmp/cc7ZvHce.s""], [/* 19 vars */] <unfinished ...>
134767 <... execve resumed> )           = -1 ENOENT (No such file or directory)
134767 execve(""external/local_config_cuda/crosstool/clang/bin/../../../cuda/bin/as"", [""as"", ""-I"", ""."", ""-I"", ""external/nccl_archive/src"", ""-I"", ""external/local_config_cuda/cross""..., ""-a64"", ""-mppc64"", ""-many"", ""-mlittle"", ""-o"", ""bazel-out/local_linux-py3-opt/bi""..., ""/tmp/cc7ZvHce.s""], [/* 19 vars */] <unfinished ...>
134767 <... execve resumed> )           = -1 ENOENT (No such file or directory)
134767 execve(""/trinity/shared/apps/cv-ppc64le/gcc/5.4.0/bin/as"", [""as"", ""-I"", ""."", ""-I"", ""external/nccl_archive/src"", ""-I"", ""external/local_config_cuda/cross""..., ""-a64"", ""-mppc64"", ""-many"", ""-mlittle"", ""-o"", ""bazel-out/local_linux-py3-opt/bi""..., ""/tmp/cc7ZvHce.s""], [/* 19 vars */] <unfinished ...>
134767 <... execve resumed> )           = -1 ENOENT (No such file or directory)
```"
7542,./configure in interactive mode does not create working config for build,"# Description
Hi,
I encounter following issue. If user runs ./configure script interactively filling blanks it is not possible to build TF with CUDA support. BUT if user exports corresponding env variables before of after ./configure, bazel can happily build binaries.

# Environment
```
$ cat /etc/redhat-release 
CentOS Linux release 7.3.1611 (AltArch)

$ uname -a
Linux power004.cluster 3.10.0-514.6.1.el7.ppc64le #1 SMP Thu Jan 19 14:34:54 GMT 2017 ppc64le ppc64le ppc64le GNU/Linux

$ ls -l /trinity/shared/apps/cv-ppc64le/nvidia/cuda/8.0/lib64/libcud*
-rw-r--r-- 1 root root 559800 Oct 29 10:22 /trinity/shared/apps/cv-ppc64le/nvidia/cuda/8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Feb 14 23:26 /trinity/shared/apps/cv-ppc64le/nvidia/cuda/8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root     19 Feb 14 23:26 /trinity/shared/apps/cv-ppc64le/nvidia/cuda/8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.54
-rwxr-xr-x 1 root root 476024 Oct 29 10:22 /trinity/shared/apps/cv-ppc64le/nvidia/cuda/8.0/lib64/libcudart.so.8.0.54
-rw-r--r-- 1 root root 966166 Oct 29 10:22 /trinity/shared/apps/cv-ppc64le/nvidia/cuda/8.0/lib64/libcudart_static.a

$ git rev-parse HEAD
16485a3fb5ffcbaa244e55c388e43279d2770982

$ bazel version
INFO: $TEST_TMPDIR defined: output root default is '/local/cvsupport'.
................
Build label: 0.4.4- (@non-git)
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Feb 9 23:48:24 2017 (1486684104)
Build timestamp: 1486684104
Build timestamp as int: 1486684104
```

# Steps to reproduce
```
$ git clone https://github.com/tensorflow/tensorflow.git
$ cd tensorflow
$ git checkout r1.0
$ export TEST_TMPDIR=/local/cvsupport
$ ./configure
```
Fill blanks
```
Please specify the location of python. [Default is /bin/python]: /usr/bin/python3.4
Please specify optimization flags to use during compilation [Default is -march=native]: -march=native
Do you wish to use jemalloc as the malloc implementation? (Linux only) [Y/n] 
jemalloc enabled on Linux
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] 
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] 
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] 
No XLA support will be enabled for TensorFlow
Found possible Python library paths:
  /usr/lib/python3.4/site-packages
  /usr/lib64/python3.4/site-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3.4/site-packages]
/usr/lib64/python3.4/site-packages
Do you wish to build TensorFlow with OpenCL support? [y/N] 
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] y
CUDA support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /bin/gcc]: /usr/bin/gcc
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /trinity/shared/apps/cv-ppc64le/nvidia/cuda/8.0
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 5.1.10
Please specify the location where cuDNN 5.1.10 library is installed. Refer to README.md for more details. [Default is /trinity/shared/apps/cv-ppc64le/nvidia/cuda/8.0]: /trinity/shared/apps/cv-ppc64le/nvidia/cudnn/8.0
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 6.0
INFO: $TEST_TMPDIR defined: output root default is '/local/cvsupport'.
................
INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
INFO: $TEST_TMPDIR defined: output root default is '/local/cvsupport'.
..............
INFO: All external dependencies fetched successfully.
Configuration finished
```
Attempt to build:
```
$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

INFO: $TEST_TMPDIR defined: output root default is '/local/cvsupport'.
................
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
ERROR: /local/cvsupport/_bazel_cvsupport/c61d2ac558d6d30ef2694b9af72e4144/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):
        File ""/local/cvsupport/_bazel_cvsupport/c61d2ac558d6d30ef2694b9af72e4144/external/local_config_cuda/crosstool/BUILD"", line 4
                error_gpu_disabled()
        File ""/local/cvsupport/_bazel_cvsupport/c61d2ac558d6d30ef2694b9af72e4144/external/local_config_cuda/crosstool/error_gpu_disabled.bzl"", line 3, in error_gpu_disabled
                fail(""ERROR: Building with --config=c..."")
ERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.
ERROR: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /local/cvsupport/_bazel_cvsupport/c61d2ac558d6d30ef2694b9af72e4144/external/local_config_cuda/crosstool/BUILD.
INFO: Elapsed time: 1.572s
```

# Working method
```
$ cat ../build_vars.sh 
export TEST_TMPDIR=/local/cvsupport
export PYTHON_BIN_PATH=/usr/bin/python3.4
export PYTHON_LIB_PATH=/usr/lib64/python3.4/site-packages
export TF_NEED_JEMALLOC=1
export TF_NEED_GCP=0
export TF_NEED_HDFS=0
export TF_NEED_OPENCL=0
export TF_NEED_CUDA=1
export TF_ENABLE_XLA=0
export CC_OPT_FLAGS=""-march=native""
export TF_CUDA_VERSION=8.0
export TF_CUDNN_VERSION=5.1.10
export TF_CUDA_COMPUTE_CAPABILITIES=6.0
export GCC_HOST_COMPILER_PATH=/usr/bin/gcc
export CUDA_TOOLKIT_PATH=/trinity/shared/apps/cv-ppc64le/nvidia/cuda/${TF_CUDA_VERSION}
export CUDNN_INSTALL_PATH=/trinity/shared/apps/cv-ppc64le/nvidia/cudnn/${TF_CUDA_VERSION}

$ source ../build_vars.sh
$ ./configure
$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```

Experimenting with omitting different variables seems like only following vars are sufficient.
`TF_NEED_CUDA`, `TF_CUDA_VERSION`, `CUDA_TOOLKIT_PATH`, `CUDNN_INSTALL_PATH`, `GCC_HOST_COMPILER_PATH`"
7541,Fatal messages mixing C libtensorflow with python tensorflow,"I'm trying to write mixed C tensorflow code with python tensorflow code by
embedding the CPython interpreter in my application.

I'm mainly doing this because defining the model is only really possible in
Python at the moment due to the lack of gradients (#6268), and I want to define
new models from the C side at speed without needing to invoke or
communicate to an external python process to get a new model.

To reproduce the problem is quite straightforward, simply `import tensorflow`
in python after the libtensorflow library has already been dynamically linked.
Here is a quick reproducer in pure python which will not run:

```python
import ctypes

tf_dll = ctypes.CDLL(""/usr/local/lib/libtensorflow.so"")

import tensorflow
```

libtensorflow can be obtained like so:

```
TF_TYPE=cpu # Set to gpu for GPU support
TF_OS=linux
curl -L \
  ""https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-${TF_TYPE}-${TF_OS}-x86_64-1.0.0.tar.gz"" |
sudo tar -C /usr/local -xz
```

Here are two fatal messages I have encountered (the first from the Python reproducer above, the second from a C program):

```
F tensorflow/stream_executor/cuda/cuda_platform.cc:180] Check failed: ::perftools::gputools::port::Status::OK() == (MultiPlatformManager::RegisterPlatform(std::move(platform))) (OK vs. Internal: platform is already registered with name: ""CUDA"")
```

```
F tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/cc/saved_model/load_attempt_count
```

I assume the problem is that the `_pywrap_tensorflow.so` has tensorflow
statically linked into them, so they don't use libtensorflow. Then you have
two shared libraries conflicting with one another.

Is there a way to avoid this conflict?"
7540,Installing and running Tensorflow 1.0,"I have just installed tensorflow 1 and when I try to run a program I get following errors. 
""C:\Users\CRCV>python a.py
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
b'hello tf'""
Any help?
NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7538,Small mistake in tensorflow document on the website,"I don't know whether it's right to report documents mistakes in this place,i don't find other place to report mistakes...

On tensorflow's website, in the Custom Model section of Getting started with tensorflow page it's mentioned as - 
`tf.contrib.learn.LinearRegressor` is actually a sub-class of `tf.contrib.learn.LinearRegressor`. 

Isn't it supposed to be `tf.contrib.learn.LinearRegressor` is actually a sub-class of `tf.contrib.learn.Estimator` ? "
7537,Feature request: get available GPU memory,"Please add an API to get available memory from the GPU. 

When tf.Session() starts, it shows the free memory. 
![image](https://cloud.githubusercontent.com/assets/7299296/22991586/a298c2fa-f371-11e6-92a3-c514cb148bf3.png)
"
7533,Feature Request: Loading images from a URL,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
I couldn't find a solution for this, so I made a stackoverflow answer:
https://stackoverflow.com/questions/42218771/how-do-i-load-images-from-urls-into-tensorflow?noredirect=1#comment71642082_42218771

### Environment info
Operating System:
macOS Sierra 10.12

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
Here is a gist of me loading images via URL, saving to disk, and processing it. To reproduce, skip the saving to disk part and send the jpeg image directly into tensorflow's `sess.run`. It will fail since it is not the type it is expecting.

### What other attempted solutions have you tried?
I've tried downloading image, saving to disk, and using that. This however is very slow.


Is there a way to do something like this? If not, I would like this to be a feature request.
"
7531,Hyperbolic functions,"I'd like to request support for all hyperbolic ops and their inverse:
sinh, cosh, asinh, acosh, atanh

Especially asinh is interesting, as it can be used as an activation function.
This would also allow to address problems with their calculation centrally, e.g.:
`asinh(x) = log(x + sqrt(x + 1))` will yield -/NaN (and break gradients) for small `x`. This is caused by the fact that the `log` argument `x + sqrt(x + 1)` will get rounded to 0 due to machine precision."
7530,AVX512 support is not functional (does not compile etc),"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

there is an existing issue but it's very vague and at the same time too specific

### Environment info
Operating System:

Linux 

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
no cuda




Current AVX512 support does not function, for example with
 bazel build -c opt  --copt=-mavx2 --copt=-mavx512f --copt=-O3 --copt=-march=skylake --copt=-mfma --copt=-g1  //tensorflow/tools/pip_package:build_pip_package

there are various compile errors. I'm filing this ticket more as a grab bag than it being about a specific compile error; as work is done we can add specific failures to this bug.

If nobody else is working on this I can try to assign someone at work to this, but if there's already code floating around to fix this then I'm more than happy to defer to that."
7529,ImportError: No module named '_pywrap_tensorflow',"When I try to run a simple test to see if Tensorflow works I get the following error. I already tried to upgrade tensorflow with: `$pip tensorflow -- upgrade` and with: `pip install -- upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.0.0rc2-cp35m-win-amd64.whl`

But I  keep on getting the response in my Command Line that everything is up to date, yet this is my error:

  ```
File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 903, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Kan opgegeven module niet vinden.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 21, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow')
  File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Gebruiker\Desktop\Sentiment.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 903, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Kan opgegeven module niet vinden.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 21, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow')
  File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'
````"
7526,Wrong docs for tf.nn.max_pool_with_argmax,"I submitted a PR for a bug in docs and it got closed probably because it lacked test example: https://github.com/tensorflow/tensorflow/pull/7161
Meanwhile I've added test example here: https://github.com/tensorflow/tensorflow/pull/7161#issuecomment-276650432.
@rmlarsen @benoitsteiner can you review this again?"
7525,"OutOfRangeError: RandomShuffleQueue '_22_shuffle_batch_3/random_shuffle_queue' is closed and has insufficient elements (requested 32, current size 0)","I m trying to read images using tensorflow tf.train function but have been getting the following issue. please help me 


    import tensorflow as tf
    import glob
    filelist = glob.glob(""../train/*.png"")

    def read_my_file_format(filename_queue):
        reader = tf.WholeFileReader()
        key, record_string = reader.read(filename_queue)
        example = tf.image.decode_png(record_string)
        label = tf.reshape(key, [1], name=None)
        return example, label

    def input_pipeline(filenames, batch_size, num_epochs = None):
        filename_queue = tf.train.string_input_producer(
        filenames, num_epochs= num_epochs, shuffle = True)
        example, label = read_my_file_format(filename_queue)
        min_after_dequeue = 10000
        capacity = min_after_dequeue+3*batch_size
       example.set_shape([28, 28, 3])
       label.set_shape([1,])
       example_batch, label_batch = tf.train.shuffle_batch(
       [example, label], batch_size = batch_size, capacity = capacity,
       min_after_dequeue = min_after_dequeue)
       return example_batch, label_batch

    example_batch, label_batch = input_pipeline(filelist, 32)


    with tf.Session() as sess:
       # Required to get the filename matching to run.
       tf.initialize_all_variables().run()

      # Coordinate the loading of image files.
      coord = tf.train.Coordinator()
      threads = tf.train.start_queue_runners(coord=coord)

      # Get an image tensor and print its value.
      x = sess.run(example_batch)
      print (x.shape)

     # Finish off the filename queue coordinator.
      coord.request_stop()
      coord.join(threads)


    OutOfRangeError: RandomShuffleQueue '_22_shuffle_batch_3/random_shuffle_queue' is closed and has insufficient elements (requested 32, current size 0)
	 [[Node: shuffle_batch_3 = QueueDequeueMany[_class=[""loc:@shuffle_batch_3/random_shuffle_queue""], component_types=[DT_UINT8, DT_STRING], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch_3/random_shuffle_queue, shuffle_batch_3/n)]]

    OutOfRangeError (see above for traceback): RandomShuffleQueue '_22_shuffle_batch_3/random_shuffle_queue' is closed and has insufficient elements (requested 32, current size 0)
	 [[Node: shuffle_batch_3 = QueueDequeueMany[_class=[""loc:@shuffle_batch_3/random_shuffle_queue""], component_types=[DT_UINT8, DT_STRING], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch_3/random_shuffle_queue, shuffle_batch_3/n)]]


"
7524,[Bug] TypeError: Input 'points' of 'NearestNeighbors' Op has type float64 that does not match expected type of float32.,"Hi, I am trying to implement KMeansClustering using tensorflow.contrib.learn.python.learn.estimators.kmeans

But I am getting the following error while using the code :-
def cluster_data(X, num_clusters) :
    kmeans = KMeansClustering(num_clusters=num_clusters)
    kmeans.fit(X)
    y = kmeans.predict(X)

During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""C:\Users\#####\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-57-5df4c7e34540>"", line 1, in <module>
    kmeans.fit(X)
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\python\util\deprecation.py"", line 191, in new_func
    arg_spec: Output from inspect.getargspec on the called function.
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 355, in fit
    """"""Initializes a BaseEstimator instance.
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 699, in _train_model
    '2016-09-23',
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 1052, in _get_train_ops
    training hooks.
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 1021, in _call_model_fn
    def __init__(self,
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\kmeans.py"", line 201, in _model_fn
    kmeans_plus_plus_num_retries=self.
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\factorization\python\ops\clustering_ops.py"", line 295, in training_graph
    # Implementation of kmeans.
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\factorization\python\ops\clustering_ops.py"", line 195, in _infer_graph
    # nearest_neighbors op.
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\factorization\python\ops\gen_clustering_ops.py"", line 90, in nearest_neighbors
    centers=centers, k=k, name=name)
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 508, in apply_op
    raise TypeError(""%s expected type of %s."" %
TypeError: Input 'points' of 'NearestNeighbors' Op has type float64 that does not match expected type of float32.
"
7523,Tensorflow graph transform quantize_weights Compression method creates corrupted graph.,"Hi,
   
 I have retrained inception v3 model using retrain.py on classes (People, Animal, Plants, Buildings, Birds) and the size is **87.5 MB**. At this stage, the model works fine when I try classification using label_image example.

But, model compressed using quantization methods is producing false or incorrect results. As mentioned below, I have tried two ways of model compression using quantized methods, however both of these are producing a corrupted compressed model that gives incorrect classification results.

**Method 1:** Shrinking file size method in graph_transform. GitHub link: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#shrinking-file-size

**Step 1:** Tried compression with **round_weights** method - size of model was still same **(87.5 MB)** and classification on images works perfectly.

**Step 2:** Compression using **quantize_weights** method - (as suggested by Pete Warden, I ran the command after removing "" and newlines ) quantized model got created with size **(22 MB)** but it is producing false or incorrect results, and giving the same class irrespective of which image is given as input (plant, bird, etc.). Below are sample results for three images none of which were animal images:

Image 1 (plant)
- Animal (1): 0.999713
- Plants (5): 0.00028519
- Buildings (3): 1.18105e-06
- People (2): 1.77942e-07
- Birds (4): 5.95436e-08

Image 2 (bird)
- Animal (1): 0.981817
- Plants (5): 0.0170753
- Buildings (3): 0.000743494
- People (2): 0.000190974
- Birds (4): 0.000148184

Image 3 (building)
- Animal (1): 0.991817
- Plants (5): 0.0070753
- Buildings (3): 0.00143494
- People (2): 0.00090974
- Birds (4): 0.000148184

**Method 2:** Quantization method link : https://www.tensorflow.org/how_tos/quantization/

<code> bazel build tensorflow/contrib/quantization/tools:quantize_graph </code>
ERROR: no such package 'tensorflow/contrib/quantization/tools': BUILD       file not found on package path.

Used <code> bazel build tensorflow/tools/quantization:quantize_graph </code> command for running build.

 On running following command after adding dependencies _""//tensorflow/contrib/quantization:cc_ops"",
""//tensorflow/contrib/quantization/kernels:quantized_ops"",_ to BUILD file of label_image.

<code>
bazel-bin/tensorflow/tools/quantization/quantize_graph \
--input=Trained_Model/CompressedJunkRetrained_graph.pb \
--output_node_names=""final_result"" --output=/Trained_Model/quantized_graph.pb \
--mode=eightbit
</code>

**Note:** Used _final_result_ in _output_node_names_ instead of _softmax_.

This is giving the following error:

ERROR: /tensorflow-master/tensorflow/examples/label_image/BUILD:10:1: no such package 'tensorflow/tools/quantization/kernels': BUILD file not found on package path and referenced by '//tensorflow/examples/label_image:label_image'.
ERROR: /tensorflow-master/tensorflow/examples/label_image/BUILD:10:1: no such target '//tensorflow/tools/quantization:cc_ops': target 'cc_ops' not declared in package 'tensorflow/tools/quantization' defined by /tensorflow-master/tensorflow/tools/quantization/BUILD and referenced by '//tensorflow/examples/label_image:label_image'.
ERROR: Analysis of target '//tensorflow/examples/label_image:label_image' failed; build aborted.


If I run the same command without adding dependencies, it is compressing the model successfully.
But again in this quantized model images are getting wrongly classified. 

"
7522,error when import tensorflow:ImportError: libcudnn.so.5: cannot open shared object file: No such file or directory,"jcc@jcc:~$ pip show tensorflow
Name: tensorflow
Version: 1.0.0rc2
Summary: TensorFlow helps the tensors flow
Home-page: http://tensorflow.org/
Author: Google Inc.
Author-email: opensource@google.com
License: Apache 2.0
Location: /home/jcc/anaconda2/lib/python2.7/site-packages
Requires: werkzeug, six, wheel, mock, numpy, protobuf


I think the lines above has shown that I have installed the tensorflow using anaconda2(I installed tensorflow by building from source). I have not created a conda virtual environment.

Any help?Why this error? I make sure that in bashrc file I have this line:
export LD_LIBRARY_PATH=/home/jcc/cudnn/lib64:$LD_LIBRARY_PATH 
and in the ./configure  process of tensorflow I typed in ""/home/jcc/cudnn"" for cudnn_path
My server is Ubuntu 14.04, titan x gpu, cuda7.5 ,cudnn5

The last command line of my build history is like:
pip install /tmp/tensorflow_pkg/tensorflow-1.0.0rc2-cp27-cp27mu-linux_x86_64.whl

Processing /tmp/tensorflow_pkg/tensorflow-1.0.0rc2-cp27-cp27mu-linux_x86_64.whl
Requirement already satisfied: werkzeug>=0.11.10 in ./anaconda2/lib/python2.7/site-packages (from tensorflow==1.0.0rc2)
Requirement already satisfied: six>=1.10.0 in ./anaconda2/lib/python2.7/site-packages (from tensorflow==1.0.0rc2)
Requirement already satisfied: wheel in ./anaconda2/lib/python2.7/site-packages (from tensorflow==1.0.0rc2)
Requirement already satisfied: mock>=2.0.0 in ./anaconda2/lib/python2.7/site-packages (from tensorflow==1.0.0rc2)
Requirement already satisfied: numpy>=1.11.0 in ./anaconda2/lib/python2.7/site-packages (from tensorflow==1.0.0rc2)
Requirement already satisfied: protobuf>=3.2.0 in ./anaconda2/lib/python2.7/site-packages (from tensorflow==1.0.0rc2)
Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in ./anaconda2/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==1.0.0rc2)
Requirement already satisfied: pbr>=0.11 in ./anaconda2/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==1.0.0rc2)
Requirement already satisfied: setuptools in ./anaconda2/lib/python2.7/site-packages/setuptools-23.0.0-py2.7.egg (from protobuf>=3.2.0->tensorflow==1.0.0rc2)
Installing collected packages: tensorflow
Successfully installed tensorflow-1.0.0rc2

But when import tensorflow, the error reads like \ :
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/jcc/anaconda2/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/jcc/anaconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/jcc/anaconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/jcc/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/home/jcc/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
ImportError: libcudnn.so.5: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.


"
7521,Tensorflow takes all my CPUs,"I am having issue with CPU usage. I am running my code on a node with 4 GPUs and 12 CPUs.
I am using TFRecords for reading my data and it works well. However when I run my model Tensorflow uses all the CPUs available on my node (_even when I specify that I want to use only one thread_):
<img width=""587"" alt=""capture d ecran 2017-02-15 a 10 26 58"" src=""https://cloud.githubusercontent.com/assets/11565723/22968352/5b682558-f36a-11e6-9bef-97b1e8e4caff.png"">

I wanted to check what are the devices founded by Tensorflow by running this:
    **from tensorflow.python.client import device_lib
    print(device_lib.list_local_devices())**

Log:
[name: ""/cpu:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 5188106746934734819
, name: ""/gpu:0""
device_type: ""GPU""
memory_limit: 12051264308
locality {
  bus_id: 1
}
incarnation: 5913495995344792631
physical_device_desc: ""device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0""
]

Given the output it seems Tensorflow finds and uses only one CPU. But maybe this CPU founded is a kind of ""concatenation"" of the 12 CPUs available on my node..

**How to specify Tensorflow to use only one CPU?
How to make Tensorflow understand that I have several CPUs on my node? ( I can't find an config option or something else in the tf.Session()**

Thanks for your help.

I installed Tensorflow using ""pip install tensorflow"" and below is the output of python -c ""import tensorflow; print(tensorflow.__version__)"":
**0.12.0-rc1**

An when I run this, it tells me that I have my 12 cores:
import multiprocessing
multiprocessing.cpu_count()
**12**
"
7520,Can I use tensorflow to implement the red module function of VIDI?,"I not idea to implement  the red module function of VIDI.

it can only use train OK image and inspection NG image.

it base on cnn??or rcnn?"
7519,Can Tensorflow build complex probablistic graph model ?,"Hello. I am surprised by the optimization ability of tensorflow, and want tensorflow to solve more math models. However, I find few articles about how to build  complex probablistic graph model fast and simple. Are there any ideas about introducing tensorflow into complex general probablistic graph model field?"
7518,How to build static libraries and link them to a c++ project in linux platform?,"I am working on a text-classification project, which is big and doesn't use bazel as its build tool. I want to integrate tensorflow into my project, but I find it is hard to change my build tool to bazel. So I wish to  build static libraries on tensorflow and link them into my project. 
Does anyone know how to build standalone static libraries and link them in the existing c++ project? Thanks a lot."
7515,[feature requests] DecodeCSVOP to parse only the first len(record_defaults) columns of a csv,"Suppose that my data.csv is :
_1,2,3_
_2,4,6_

and for some other purpose we add a new column to a new csv data2.csv
_1,2,3,comment_

the code below fails if the input is data2.csv:
`col1,col2,col3 = tf.decode_csv(line,record_defaults=[[1],[1],[1]]) `

Hope that tf.decode_csv() only decodes the first len(record_defaults) columns only, so that it works for both data1.csv as well as data2.csv.
What's more, we can add  ""a column index list parameter"" to indicate which columns to be decoded.
"
7514, Tensorflow support rnn with batch normalzition cell? Http://arxiv.org/abs/1510.01378,"
Tensorflow support rnn with batch normalzition cell? Http://arxiv.org/abs/1510.01378"
7511,non_max_suppression is very slow and doesn't appear to have a cuda or multi-threaded implementation,"
It appears that tf.image.non_max_suppression currently takes about 200ms for about 8000 boxes, runs on a single CPU thread and doesn't have a GPU implementation.

### Environment info
Operating System:
Ubuntu 16.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
8.0, 5.1.5

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
0.12.0-rc1

"
7510,Test issue,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7508,import_graph_def's input_map doesn't remap control inputs,"If a graph `g` has a node `y` with a control input like `^x`, I would have thought that `tf.import_graph_def(g, input_map={""x"": z})` would result in a graph having a node `y` with a control input`^z`. 

Instead I get an error: `ValueError: Attempted to map inputs that were not found in graph_def: [x:0]`.  This is on master.

Complete example: 

```python
import tensorflow as tf
g1=tf.Graph()
with g1.as_default():
    x=tf.constant(0.0, name=""x"")
    with tf.control_dependencies([x]):
        y=tf.constant(1, name=""y"")
g2=tf.Graph()
with g2.as_default():
    z=tf.constant(1.0, name=""z"")
    tf.import_graph_def(g1.as_graph_def(), input_map={""x"": z})

```"
7503,How change the package name of tensorflow android camera demo,I have built two Android demo with two different custom models. However in order to install both apps on my device I need to change the name of one of these. Is there an easy way to change the name of the apk file?
7500,TensorFlow version 1.0.0-rc2 on Windows: OpKernel ('op: BestSplits device_type: CPU') for unknown op: BestSplits with test code,"I installed TensorFlow version 1.0.0-rc2 on Windows 7 SP1 x64 Ultimate (Python 3.5.2 |Anaconda custom (64-bit)) using:

    pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.0rc2-cp35-cp35m-win_amd64.whl

When I try running the test script from https://web.archive.org/web/20170214034751/https://www.tensorflow.org/get_started/os_setup#test_the_tensorflow_installation in Eclipse 4.5 or in the console:

    import tensorflow as tf
    print('TensorFlow version: {0}'.format(tf.__version__))
    hello = tf.constant('Hello, TensorFlow!')
    sess = tf.Session()
    print(sess.run(hello))

I obtain some error message:

    TensorFlow version: 1.0.0-rc2
    'Hello, TensorFlow!'
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflob
    w\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots

Why?

I didn't have such issues with TensorFlow 0.12.1 (installed with `pip install tensorflow==0.12.1`):

    TensorFlow version: 0.12.1
    b'Hello, TensorFlow!'

Stack Exchange thread: [TensorFlow version 1.0.0-rc2 on Windows: ""OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits"" with test code](http://stackoverflow.com/q/42217532/395857)

@drpngx "
7499,error when running conda create -n tensorflow python=2.7,"the error reads like:
jcc@jcc:~$ conda create -n tensorflow python=2.7
Fetching package metadata .......
Solving package specifications: ..........

Package plan for installation in environment /home/jcc/anaconda2/envs/tensorflow:

The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    openssl-1.0.2k             |                0         3.2 MB
    python-2.7.13              |                0        11.5 MB
    setuptools-27.2.0          |           py27_0         521 KB
    pip-9.0.1                  |           py27_1         1.6 MB
    ------------------------------------------------------------
                                           Total:        16.7 MB

The following NEW packages will be INSTALLED:

    openssl:    1.0.2k-0     
    pip:        9.0.1-py27_1 
    python:     2.7.13-0     
    readline:   6.2-2        
    setuptools: 27.2.0-py27_0
    sqlite:     3.13.0-0     
    tk:         8.5.18-0     
    wheel:      0.29.0-py27_0
    zlib:       1.2.8-3      

Proceed ([y]/n)? y

Fetching packages ...
Error: Could not open u'/home/jcc/anaconda2/pkgs/openssl-1.0.2k-0.tar.bz2.part' for writing (seek).
Any help?

"
7498,Import Tensorflow lib with Qt for Android,"I tried to import tensorflow lib into my Qt project, the build procedure was followed as described in contrib/makefile

I somehow managed with Desktop and iOS platforms, but failed with Android with linking problem:
```
/Users/chen/Documents/Git/tensorflow/tensorflow/contrib/makefile/gen_android/protobuf/lib/libprotobuf.a(strutil.o):strutil.cc:function google::protobuf::safe_strtof(char const*, float*): error: undefined reference to 'strtof'
collect2: error: ld returned 1 exit status
make: *** [libCrossDemo.so] Error 1
20:22:40: The process ""/usr/bin/make"" exited with code 2.
Error while building/deploying project CrossDemo (kit: Android for armeabi-v7a (GCC 4.9, Qt 5.8.0))
```
I think this problem is because I did not reference to the std lib correctly. Could someone help me with this problem? thank you. 

====
OS: macOS 10.12.3
SDK: android-21
NDK: r13b"
7497,Bazel fetch issue while compiling Tensorflow,"I'm compiling Tensorflow from source. After the configure step I'm facing the following error:

> ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': Error downloading [http://bazel-mirror.storage.googleapis.com/github.com/bazelbuild/rules_closure/archive/5ca1dab6df9ad02050f7ba4e816407f88690cf7d.tar.gz, https://github.com/bazelbuild/rules_closure/archive/5ca1dab6df9ad02050f7ba4e816407f88690cf7d.tar.gz] to /home/xyzuser/.cache/bazel/_bazel_xyzuser/cb1e63cb5e61cab49a9fd2f5ba92d003/external/io_bazel_rules_closure/5ca1dab6df9ad02050f7ba4e816407f88690cf7d.tar.gz: All mirrors are down: [java.lang.NullPointerException].

This stems from the bazel_clean_and_fetch() in configure file
`bazel fetch ""//tensorflow/... -//tensorflow/contrib/nccl/...
      -//tensorflow/examples/android/...""
`
Same error replicates if I do `bazel fetch //...`

My system is behind a proxy and I've set HTTP_PROXY and HTTPS_PROXY in the env.

**git rev-parse HEAD:**
e946a6b63979a63f9e5a1d1603f6cc21d8aad1cf

**bazel version:**
Build label: 0.4.4- (@non-git)
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar

**Operating System:**
Ubuntu 14.04.5 LTS

Any insights would be helpful as I'm new to Bazel and Tensorflow.
@jart @kchodorow "
7496,TypeError: zeros_initializer() got multiple values for keyword argument 'dtype',"I'm trying to do distributed learning from tutorial [Inception in TensorFlow](https://github.com/tensorflow/models/tree/master/inception)

**Environment**
_Parameter Server_ 
Operating system : Ubuntu 16.04 LTS
Tensorflow : r 0.12
python : 2.7.12

_Worker_
Operating system : CentOS 7
Tensorflow : r 0.12
python : 2.7.5
GPU card : 2 * GTX NVIDIA 1080

but i got this error : 

```
Traceback (most recent call last): 
File ""/home/paslab/tensorflow-models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/imagenet_distributed_train.py"", line 66, in <module> tf.app.run()

File ""/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 44, in run _sys.exit(main(_sys.argv[:1] + flags_passthrough)) File ""/home/paslab/tensorflow-models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/imagenet_distributed_train.py"", line 62, in main inception_distributed_train.train(server.target, dataset, cluster_spec)

File ""/home/paslab/tensorflow-models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/inception_distributed_train.py"", line 120, in train global_step = slim.variables.global_step() 

File ""/home/paslab/tensorflow-models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/slim/scopes.py"", line 155, in func_with_args return func(*args, **current_args) 

File ""/home/paslab/tensorflow-models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/slim/variables.py"", line 244, in global_step trainable=False, collections=collections) 

File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 987, in get_variable custom_getter=custom_getter) 

File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 889, in get_variable custom_getter=custom_getter) 

File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 347, in get_variable validate_shape=validate_shape) 

File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 332, in _true_getter caching_device=caching_device, validate_shape=validate_shape) 

File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 683, in _get_single_variable validate_shape=validate_shape) 

File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 226, in __init__ expected_shape=expected_shape) 

File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 303, in _init_from_args initial_value(), name=""initial_value"", dtype=dtype) 

File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 672, in <lambda> shape.as_list(), dtype=dtype, partition_info=partition_info) 

TypeError: zeros_initializer() got multiple values for keyword argument 'dtype'
```
I checked the following issue before

([TypeError: ones_initializer() got multiple values for keyword argument 'dtype' when execute the inception_train #5742](https://github.com/tensorflow/tensorflow/issues/5742)) 

but it seems not working very well

Any idea?   Thx!
"
7494,TensorBoard Projector does not work on Mac,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

<img width=""1440"" alt=""screen shot 2017-02-14 at 12 49 12 pm"" src=""https://cloud.githubusercontent.com/assets/25763469/22919144/130df352-f2b4-11e6-8eb5-86d8b886e64f.png"">

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7493,"Build issue with bazel ""bazel_bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg""","```
INFO: All external dependencies fetched successfully.
Configuration finished
ankur@node2:~/tensorflow$ bazel build -c opt --config=cuda
WARNING: Output base '/home/ankur/.cache/bazel/_bazel_ankur/5381c626cc1aae82581869f5d7a028ec' is on NFS. This may lead to surprising failures and undetermined behavior.
INFO: Found 0 targets...
INFO: Elapsed time: 4.005s, Critical Path: 0.02s
ankur@node2:~/tensorflow$ bazel-bin/tensorflow/tools/pip_package/build_pip_package/tmp/tensorflow_pkg
-bash: bazel-bin/tensorflow/tools/pip_package/build_pip_package/tmp/tensorflow_pkg: No such file or directory
ankur@node2:~/tensorflow$ bazel-bin/tensorflow/tools/pip_package /tmp/tensorflow_pkg
-bash: bazel-bin/tensorflow/tools/pip_package: No such file or directory
```"
7491,BatchNormalization error with keras,"Hello,
I use keras and tensorflow backend and ran into error with the follow code:
seq.add(LSTM(256,
input_shape=(length, dim),
activation='tanh',
return_sequences=True))
seq.add(BatchNormalization(
mode=0,
axis=1))
the eroor message is:
File ""D:\SciSoft\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 165, in cons
tant
tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
File ""D:\SciSoft\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\tensor_util.py"", line 441, in make
_tensor_proto
tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])
File ""D:\SciSoft\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\tensor_util.py"", line 441, in
tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])
File ""D:\SciSoft\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\compat.py"", line 65, in as_bytes
(bytes_or_text,))
TypeError: Expected binary or unicode string, got 1
 
if I set mode = 2, then everything is fine. If remove seq.add(BatchNormalization)), it also works smoothly.
no idea the problem is with tensorflow or keras.
I tried keras from 1.1.2  to 1.2.2, same error happens.
I am using tensorflow 1.0.0rc2 and Anaconda 4.3.0 on Windows 10
anyone can help me? 
thanks a lot
"
7490,Can't restore previous models,"I installed Tensoflow from source. I can't restore previous models saved with precompiled Tensorflow (installed via pip). I get lots of these:

```
Caused by op u'save/RestoreV2_56', defined at:
  File ""grid-blstm.py"", line 144, in <module>
    loadModel(session, './'+saveDir+'/')
  File ""/home/user/Testing/DeepLearning/debug/grid-blstm/utils.py"", line 267, in loadModel
    saver = tf.train.Saver()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1067, in __init__
    self.build()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1097, in build
    restore_sequentially=self._restore_sequentially)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 691, in build
    restore_sequentially, reshape)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 407, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 247, in restore_op
    [spec.tensor.dtype])[0])
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 669, in restore_v2
    dtypes=dtypes, name=name)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 768, in apply_op
    op_def=op_def)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2402, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

NotFoundError (see above for traceback): Key bidirectional_rnn/fw/multi_rnn_cell/cell_0/GridRNNCell/recurrent/cell_0/lstm_cell/w_i_diag not found in checkpoint. Found: 
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/project_c_0 (DT_DOUBLE) [20,100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/project_c_0/Adam (DT_DOUBLE) [20,100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/project_c_0/Adam_1 (DT_DOUBLE) [20,100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/project_m_0 (DT_DOUBLE) [20,100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/project_m_0/Adam (DT_DOUBLE) [20,100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/project_m_0/Adam_1 (DT_DOUBLE) [20,100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/B (DT_DOUBLE) [400]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/B/Adam (DT_DOUBLE) [400]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/B/Adam_1 (DT_DOUBLE) [400]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_0 (DT_DOUBLE) [200,400]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_0/Adam (DT_DOUBLE) [200,400]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_0/Adam_1 (DT_DOUBLE) [200,400]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_F_diag (DT_DOUBLE) [100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_F_diag/Adam (DT_DOUBLE) [100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_F_diag/Adam_1 (DT_DOUBLE) [100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_I_diag (DT_DOUBLE) [100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_I_diag/Adam (DT_DOUBLE) [100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_I_diag/Adam_1 (DT_DOUBLE) [100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_O_diag (DT_DOUBLE) [100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_O_diag/Adam (DT_DOUBLE) [100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_O_diag/Adam_1 (DT_DOUBLE) [100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/B (DT_DOUBLE) [400]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/B/Adam (DT_DOUBLE) [400]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/B/Adam_1 (DT_DOUBLE) [400]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_0 (DT_DOUBLE) [200,400]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_0/Adam (DT_DOUBLE) [200,400]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_0/Adam_1 (DT_DOUBLE) [200,400]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_F_diag (DT_DOUBLE) [100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_F_diag/Adam (DT_DOUBLE) [100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_F_diag/Adam_1 (DT_DOUBLE) [100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_I_diag (DT_DOUBLE) [100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_I_diag/Adam (DT_DOUBLE) [100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_I_diag/Adam_1 (DT_DOUBLE) [100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_O_diag (DT_DOUBLE) [100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_O_diag/Adam (DT_DOUBLE) [100]
BiRNN/BW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_O_diag/Adam_1 (DT_DOUBLE) [100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/project_c_0 (DT_DOUBLE) [20,100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/project_c_0/Adam (DT_DOUBLE) [20,100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/project_c_0/Adam_1 (DT_DOUBLE) [20,100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/project_m_0 (DT_DOUBLE) [20,100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/project_m_0/Adam (DT_DOUBLE) [20,100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/project_m_0/Adam_1 (DT_DOUBLE) [20,100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/B (DT_DOUBLE) [400]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/B/Adam (DT_DOUBLE) [400]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/B/Adam_1 (DT_DOUBLE) [400]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_0 (DT_DOUBLE) [200,400]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_0/Adam (DT_DOUBLE) [200,400]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_0/Adam_1 (DT_DOUBLE) [200,400]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_F_diag (DT_DOUBLE) [100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_F_diag/Adam (DT_DOUBLE) [100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_F_diag/Adam_1 (DT_DOUBLE) [100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_I_diag (DT_DOUBLE) [100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_I_diag/Adam (DT_DOUBLE) [100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_I_diag/Adam_1 (DT_DOUBLE) [100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_O_diag (DT_DOUBLE) [100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_O_diag/Adam (DT_DOUBLE) [100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_0/LSTMCell/W_O_diag/Adam_1 (DT_DOUBLE) [100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/B (DT_DOUBLE) [400]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/B/Adam (DT_DOUBLE) [400]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/B/Adam_1 (DT_DOUBLE) [400]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_0 (DT_DOUBLE) [200,400]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_0/Adam (DT_DOUBLE) [200,400]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_0/Adam_1 (DT_DOUBLE) [200,400]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_F_diag (DT_DOUBLE) [100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_F_diag/Adam (DT_DOUBLE) [100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_F_diag/Adam_1 (DT_DOUBLE) [100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_I_diag (DT_DOUBLE) [100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_I_diag/Adam (DT_DOUBLE) [100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_I_diag/Adam_1 (DT_DOUBLE) [100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_O_diag (DT_DOUBLE) [100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_O_diag/Adam (DT_DOUBLE) [100]
BiRNN/FW/MultiRNNCell/Cell0/GridRNNCell/recurrent/cell_1/LSTMCell/W_O_diag/Adam_1 (DT_DOUBLE) [100]
Variable (DT_DOUBLE) [100,90]
Variable/Adam (DT_DOUBLE) [100,90]
Variable/Adam_1 (DT_DOUBLE) [100,90]
Variable_1 (DT_DOUBLE) [100,90]
Variable_1/Adam (DT_DOUBLE) [100,90]
Variable_1/Adam_1 (DT_DOUBLE) [100,90]
beta1_power (DT_FLOAT) []
beta2_power (DT_FLOAT) []

	 [[Node: save/RestoreV2_56 = RestoreV2[dtypes=[DT_DOUBLE], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_56/tensor_names, save/RestoreV2_56/shape_and_slices)]]
	 [[Node: save/RestoreV2_66/_91 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_172_save/RestoreV2_66"", tensor_type=DT_DOUBLE, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
```

Any idea?"
7489,how to use baidu wrap-ctc on tensorflow?,"when i use tensorflow ctc, i find it is very slow,,,
so i want use baidu wrap-ctc on tensorflow,

but how to write code?

when i use tensorflow ctc, i write it:
    print logits.get_shape()
    logits = tf.reshape(logits, [batch_size, -1, nout])
    logits = tf.transpose(logits, (1, 0, 2))
    print 'logits shape is '
    print logits.get_shape()
    labels = tf.SparseTensor(indices=label_idx, values=label_vals, shape=label_shape)
    print 'init label'
    loss = ctc_ops.ctc_loss(inputs=logits, labels=labels, sequence_length=seq_len)
    print 'init loss'
    cost = tf.reduce_mean(loss)
    print 'init cost'
    decoded, log_prob = ctc_ops.ctc_greedy_decoder(logits, seq_len)
    print 'init decoded'
    ler = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32),
                                          labels))
    print 'init ler'"
7488,AttributeError: 'module' object has no attribute 'histogram' when using tf-faster-rcnn,"Please have a look at this issue and let me know if you know how to fix it?
```
mona@pascal:~/computer_vision/tf-faster-rcnn$ ./experiments/scripts/test_vgg16.sh $GPU_ID pascal_voc
+ set -e
+ export PYTHONUNBUFFERED=True
+ PYTHONUNBUFFERED=True
+ GPU_ID=0
+ DATASET=pascal_voc
+ array=($@)
+ len=2
+ EXTRA_ARGS=
+ EXTRA_ARGS_SLUG=
+ case ${DATASET} in
+ TRAIN_IMDB=voc_2007_trainval
+ TEST_IMDB=voc_2007_test
+ ITERS=70000
++ date +%Y-%m-%d_%H-%M-%S
+ LOG=experiments/logs/test_vgg16_voc_2007_trainval_.txt.2017-02-13_21-29-30
+ exec
++ tee -a experiments/logs/test_vgg16_voc_2007_trainval_.txt.2017-02-13_21-29-30
tee: experiments/logs/test_vgg16_voc_2007_trainval_.txt.2017-02-13_21-29-30: No such file or directory
+ echo Logging output to experiments/logs/test_vgg16_voc_2007_trainval_.txt.2017-02-13_21-29-30
Logging output to experiments/logs/test_vgg16_voc_2007_trainval_.txt.2017-02-13_21-29-30
+ set +x
+ [[ ! -z '' ]]
+ CUDA_VISIBLE_DEVICES=0
+ time python ./tools/test_vgg16_net.py --imdb voc_2007_test --weight data/imagenet_weights/vgg16.weights --model output/vgg16/voc_2007_trainval/default/vgg16_faster_rcnn_iter_70000.ckpt --cfg experiments/cfgs/vgg16.yml --set
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5.0 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally
Called with args:
Namespace(cfg_file='experiments/cfgs/vgg16.yml', comp_mode=False, imdb_name='voc_2007_test', max_per_image=100, model='output/vgg16/voc_2007_trainval/default/vgg16_faster_rcnn_iter_70000.ckpt', set_cfgs=[], tag='', weight='data/imagenet_weights/vgg16.weights')
Using config:
{'DATA_DIR': '/home/mona/computer_vision/tf-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'vgg16',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'POOLING_MODE': 'crop',
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/mona/computer_vision/tf-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'MODE': 'nms',
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'RPN_TOP_N': 5000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': False,
           'BATCH_SIZE': 256,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': True,
           'BBOX_REG': True,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'BIAS_DECAY': False,
           'DISPLAY': 20,
           'DOUBLE_BIAS': True,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'GAMMA': 0.1,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'LEARNING_RATE': 0.001,
           'MAX_SIZE': 1000,
           'MOMENTUM': 0.9,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_ITERS': 5000,
           'SNAPSHOT_KEPT': 3,
           'SNAPSHOT_PREFIX': 'vgg16_faster_rcnn',
           'STEPSIZE': 30000,
           'SUMMARY_INTERVAL': 180,
           'TRUNCATED': False,
           'USE_FLIPPED': True,
           'USE_GT': False,
           'WEIGHT_DECAY': 0.0005},
 'USE_GPU_NMS': True}
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.8755
pciBusID 0000:03:00.0
Total memory: 11.92GiB
Free memory: 11.85GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:03:00.0)
Loading caffe weights...
Done!
Traceback (most recent call last):
  File ""./tools/test_vgg16_net.py"", line 89, in <module>
    tag='default', anchor_scales=anchors)
  File ""/home/mona/computer_vision/tf-faster-rcnn/tools/../lib/nets/vgg16.py"", line 503, in create_architecture
    self._add_score_summary(key, var)
  File ""/home/mona/computer_vision/tf-faster-rcnn/tools/../lib/nets/vgg16.py"", line 45, in _add_score_summary
    tf.summary.histogram('SCORE/' + tensor.op.name + '/' + key + '/scores', tensor)
AttributeError: 'module' object has no attribute 'histogram'
Command exited with non-zero status 1
5.56user 4.11system 0:07.12elapsed 135%CPU (0avgtext+0avgdata 2082508maxresident)k
0inputs+32outputs (0major+212277minor)pagefaults 0swaps
```



### Environment info
Operating System:
```
$ uname -a; lsb_release -a
Linux pascal 3.13.0-62-generic #102-Ubuntu SMP Tue Aug 11 14:29:36 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 14.04.5 LTS
Release:	14.04
Codename:	trusty
```


Version of TF:

```
mona@pascal:~/computer_vision/tf-faster-rcnn$ python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5.0 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally
0.10.0
```



The output of `bazel version`
```
$ bazel version
Extracting Bazel installation...
Build label: 0.4.3
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Dec 22 12:31:25 2016 (1482409885)
Build timestamp: 1482409885
Build timestamp as int: 1482409885
```
CUDA8"
7486,Contrib breaks rendering with gym,"Using; pyenv, python 3.5.2, tensorflow 0.12, gym 0.7.3, ubuntu 14.04.5 lts, pyglet 1.2.4.

(1) Runs perfectly fine. I see a little game of pong...
```
import tensorflow as tf
import gym
env = gym.make(""Pong-v0"")
env.reset()
for _ in range(1000):
    env.render()
    env.step(env.action_space.sample()) # take a random action
``` 

***
(2) _Raises_: `ImportError` from pyglet.gl due to `'NoneType' object has no attribute 'decode'` in pyglet/compat.py.
```
import tensorflow as tf
import tensorflow.contrib.slim as slim
import gym
env = gym.make(""Pong-v0"")
env.reset()
for _ in range(1000):
    env.render()
    env.step(env.action_space.sample()) # take a random action
``` 

I doubt the problem is tf.slim. But one of the contrib modules seems to break `pyglet.gl`."
7484,zlib-1.2.8.tar.gz 404 NOT FOUND,"Am I missing something, but I am being sent to http://zlib.net/zlib-1.2.8.tar.gz, but this is 404.  I think they are forcing installing 1.2.11 because of a bug?"
7483,sparse_placeholder failed when specifying shape,"When trying to create a sparse placeholder, exception raised if specifying the `shape` parameter:
```
features_placeholder = tf.sparse_placeholder(tf.float32, shape=(16, 20424)) # failed
features_placeholder = tf.sparse_placeholder(tf.float32)  # pass
```
Exception reads like:
```
  File ""tf_learn.py"", line 92, in run_training
    features_placeholder = tf.sparse_placeholder(tf.float32, shape=(FLAGS.batch_size, etc.FEATURE_SIZE))
  File ""C:\local\Anaconda3-4.1.1-Windows-x86_64\envs\tf-py35\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 1653, in sparse_placeholder
    shape=shape
  File ""C:\local\Anaconda3-4.1.1-Windows-x86_64\envs\tf-py35\lib\site-packages\tensorflow\python\framework\sparse_tensor.py"", line 135, in __init__
    shape = ops.convert_to_tensor(shape, name=""shape"", dtype=dtypes.int64)
  File ""C:\local\Anaconda3-4.1.1-Windows-x86_64\envs\tf-py35\lib\site-packages\tensorflow\python\framework\ops.py"", line 669, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\local\Anaconda3-4.1.1-Windows-x86_64\envs\tf-py35\lib\site-packages\tensorflow\python\framework\ops.py"", line 583, in _TensorTensorConversionFunction
    % (dtype.name, t.dtype.name, str(t)))
ValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: 'Tensor(""Const:0"", shape=(2,), dtype=int32)'

```

Is this a bug?"
7482,Feature request: enable modification of data in tfrecord files,"It would be nice to be able to append or remove samples to or from an existing set of samples saved in a tfrecords file. Otherwise the file has to be duplicated, which is unpractical for datasets >> 1TB.
Formulated in a more generalized way, this can also be seen as splitting and concatenating tfrecord files."
7480,preprocessor definition clash with glog,"CHECK macros from `platform/logging.h` leak out into `core/public` headers which clash with users of glog.

One path is through `core/platform/allocator.h`:
```
In file included from external/org_tensorflow/tensorflow/core/platform/logging.h:25:0,
                 from external/org_tensorflow/tensorflow/core/framework/allocator.h:26,
                 from external/org_tensorflow/tensorflow/core/framework/tensor.h:21,
                 from external/org_tensorflow/tensorflow/core/public/session.h:23,
(snip)
external/org_tensorflow/tensorflow/core/platform/default/logging.h:224:0: note: this is the location of the previous definition
 #define CHECK_OP_LOG(name, op, val1, val2)                            \
 ^
```
This one is easy to fix by moving method implementation to allocator.cc.

Another is through `core/lib/core/status.h`.
```
In file included from external/org_tensorflow/tensorflow/core/platform/logging.h:25:0,
                 from external/org_tensorflow/tensorflow/core/lib/core/status.h:24,
                 from external/org_tensorflow/tensorflow/core/lib/core/errors.h:19,
                 from external/org_tensorflow/tensorflow/core/framework/tensor_shape.h:24,
                 from external/org_tensorflow/tensorflow/core/framework/tensor.h:24,
                 from external/org_tensorflow/tensorflow/core/public/session.h:23,
```

This one is more work to fix because `TF_CHECK_OK` is used all over the code, but it does not seem to be necessary for `core/public`.
"
7477,Add support for custom queue runners,"I have a custom implementation of `QueueRunner` but right now TF complains when serializing:

> WARNING - tensorflow - Error encountered when serializing queue_runners.
> Type is unsupported, or the types of the items don't match field type in CollectionDef.
> unbound method to_proto() must be called with QueueRunner instance as first argument (got IteratorRunner instance instead)

The use case for the implementation is to take advantage of automatic thread starting from `Experiment`, `Supervisor`, `managed_session`, etc.

I could inherit from `QueueRunner` but a `QueueRunnerBase` would be ideal as much of the existing implementation assumes the use of `enqueue_ops`.

As an example, I have [`GeneratorRunner`](https://gist.github.com/jimfleming/d1118cc630f5c883223a4b4645cc2e7b) which runs a generator in a thread and enqueues the outputs. This is more flexible than the existing `QueueRunner` since it  supports placeholders with feed dictionaries."
7476,Implement Batch Renormalization in TF,"See paper: https://arxiv.org/pdf/1702.03275.pdf
Keras implementation: https://github.com/ajbrock/Neural-Photo-Editor/blob/master/layers.py#L35"
7474,Gradient of gamma log pdf is broken,"```
In [1]: import tensorflow as tf
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
d
In [2]: dist = tf.contrib.distributions

In [3]: mu = tf.get_variable('mean_arg', [], 'float32')

In [4]: m = tf.nn.softplus(mu)

In [5]: q = dist.Gamma(0.01, 0.01/mu)

In [6]: sess = tf.InteractiveSession()
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 780 Ti
major: 3 minor: 5 memoryClockRate (GHz) 0.928
pciBusID 0000:02:00.0
Total memory: 2.98GiB
Free memory: 2.90GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 780 Ti, pci bus id: 0000:02:00.0)

In [7]: sess.run(tf.global_variables_initializer())

In [8]: sess.run(tf.gradients(q.log_pdf(q.sample()), mu))
Out[8]: [0.74267536]

In [9]: sess.run(tf.gradients(q.log_pdf(q.sample()), mu))
Out[9]: [nan]

In [10]: sess.run(tf.gradients(q.log_pdf(q.sample()), mu))
Out[10]: [nan]

In [20]: tf.__version__
Out[20]: '0.12.0-rc1'
```
It looks like this happens because Gamma samples can be negative:
```
In [80]: z = q.sample()

In [81]: sess.run([z] + tf.gradients(q.log_pdf(z), mu))
Out[81]: [-0.0050091296, 0.74267536]

In [82]: sess.run([z] + tf.gradients(q.log_pdf(z), mu))
Out[82]: [-0.0, nan]

In [83]: sess.run([z] + tf.gradients(q.log_pdf(z), mu))
Out[83]: [-0.0, nan]

In [84]: sess.run([z] + tf.gradients(q.log_pdf(z), mu))
Out[84]: [-0.0, nan]

In [85]: sess.run([z] + tf.gradients(q.log_pdf(z), mu))
Out[85]: [-0.01258331, 0.74267519]

In [86]: sess.run([z] + tf.gradients(q.log_pdf(z), mu))
Out[86]: [-1.0715475e-21, 0.74267536]
```
Happy to run more tests to figure out this issue.

Update: quick fix thanks to @ebrevdo - 
```z = z + np.finfo(z.dtype.as_numpy_dtype).tiny```"
7473,Jupyter notebook: Kernel dies when running timeline trace [Nvidia-docker],"I am trying to use timeline to profile GPU memory usage on an EC2 instance using Tensorflow-Gpu Nvidia-Docker. When I add a few lines to my notebook which runs a convNet, it keeps restarting with a ""Kernel died"" message.

Some of the changes I made:
```
from tensorflow.python.client import timeline
```

```
run_metadata = tf.RunMetadata()

with tf.Session(graph=graph, config=tf.ConfigProto(log_device_placement=True)) as session:
    tf.global_variables_initializer().run()
    for step in range(num_steps):
        batch_data, batch_labels = generate_batch(
          batch_size, num_skips, skip_window)
        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}
        if step == 1000:
            _, l = session.run([optimizer, loss], feed_dict=feed_dict, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata)
            trace = timeline.Timeline(step_stats=run_metadata.step_stats)
            with open('timeline.ctf.json', 'w') as trace_file:
                trace_file.write(trace.generate_chrome_trace_format())
        else:
            _, l = session.run([optimizer, loss], feed_dict=feed_dict)
        ...
```"
7470,Android: Multiple dex files define Lorg/tensorflow/contrib/android/TensorFlowInferenceInterface,"I am getting the following error regarding the TensorFlowInferenceInterface only when I try to build an APK in Android Studio (on MacOS).  But when I instead just ""Run app"", I do not get the error and the app compiles, installs and runs just fine.  (There is also an unrelated warning shown for a CircularQueue that I am working to fix; I include it for completeness)
```
Information:0 warnings
Error:associated EnclosingMethod attribute. This class was probably produced by a
Error:indicate that it is *not* an inner class.
Information:See complete output in console
Error:Execution failed for task ':app:transformClassesWithDexForFastBuildDebug'.
> com.android.build.api.transform.TransformException: com.android.ide.common.process.ProcessException: java.util.concurrent.ExecutionException: java.lang.UnsupportedOperationException
Error:(com.bea.xml.stream.util.CircularQueue$1) that doesn't come with an
Error:warning: Ignoring InnerClasses attribute for an anonymous inner class
Error:Error converting bytecode to dex:
Cause: com.android.dex.DexException: Multiple dex files define Lorg/tensorflow/contrib/android/TensorFlowInferenceInterface;
Error:this warning is that reflective operations on this class will incorrectly
Error:and without specifying any ""-target"" type options. The consequence of ignoring
Information:Gradle tasks [:ai2_common:assembleDebug, :app:assembleFastBuildDebug, :common:assembleDebug]
Information:10 errors
Information:BUILD FAILED
Information:Total time: 54.509 secs
Error:solution is to recompile the class from source, using an up-to-date compiler
Error:compiler that did not target the modern .class file format. The recommended
```

I am using the following TensorFlowInferenceInterface.java file:
```
//
// Source code recreated from a .class file by IntelliJ IDEA
// (powered by Fernflower decompiler)
//

package org.tensorflow.contrib.android;

import android.content.res.AssetManager;
import android.util.Log;
import java.util.Random;

public class TensorFlowInferenceInterface {
    private static final String TAG = ""TensorFlowInferenceInterface"";
    private final long id = (new Random()).nextLong();

    public TensorFlowInferenceInterface() {
        try {
            this.testLoaded();
            Log.i(""TensorFlowInferenceInterface"", ""Native methods already loaded."");
        } catch (UnsatisfiedLinkError var4) {
            Log.i(""TensorFlowInferenceInterface"", ""Loading tensorflow_inference."");

            try {
                System.loadLibrary(""tensorflow_inference"");
            } catch (UnsatisfiedLinkError var3) {
                throw new RuntimeException(""Native TF methods not found; check that the correct native libraries are present and loaded."");
            }
        }

    }

    public native int initializeTensorFlow(AssetManager var1, String var2);
    public native int runInference(String[] var1);
    public native void enableStatLogging(boolean var1);
    public native String getStatString();
    public native void close();
    public native void fillNodeFloat(String var1, int[] var2, float[] var3);
    public native void fillNodeInt(String var1, int[] var2, int[] var3);
    public native void fillNodeDouble(String var1, int[] var2, double[] var3);
    public native void fillNodeByte(String var1, int[] var2, byte[] var3);
    public native void readNodeFloat(String var1, float[] var2);
    public native void readNodeInt(String var1, int[] var2);
    public native void readNodeDouble(String var1, double[] var2);
    public native void readNodeByte(String var1, byte[] var2);
    private native void testLoaded();
}
```
When I search for all references to `TensorFlowInferenceInterface` in the path, I only get these:
```
import org.tensorflow.contrib.android.TensorFlowInferenceInterface;
```
and
```
        public TensorFlowInferenceInterface inferenceInterface;
```
and
```
        inferenceInterface = new TensorFlowInferenceInterface();
```
No other place is it even mentioned.
Has anyone seen this before?"
7469,docs: batch normalization usage in slim,"How to use batch normalization in the testing phase?

I tried to use batch normalization to train a model like this:
```
bn = lambda x: slim.batch_norm(x, is_training=is_training)
conv = slim.conv2d(images, 64, [3, 3], 1, normalizer_fn=bn, padding='SAME', scope='conv')
```
But when I finished training and restored my model from checkpoint files, the model's performance on the testing set was poor, just like random guessing.

If these parameters are not dumped as model variables, is it possible to make an example to illustrate how to use batch normalization in slim, esp. for inference?"
7467,Feature request: Add early stopping mechanism to slim.evaluation_loop,"Would it be possible to add early stopping mechanism to slim.evaluation_loop?

"
7465,Library not loaded: @rpath/libcudart.8.0.dylib,"Hi! Tensorflow is not properlly installed. 

Here's the output of: python -c ""import tensorflow; print(tensorflow.__version__)""

dyld: warning, LC_RPATH $ORIGIN/../../_solib_darwin/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Slib in /Library/Python/2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so being ignored in restricted program because it is a relative path
dyld: warning, LC_RPATH ../local_config_cuda/cuda/lib in /Library/Python/2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so being ignored in restricted program because it is a relative path
dyld: warning, LC_RPATH ../local_config_cuda/cuda/extras/CUPTI/lib in /Library/Python/2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so being ignored in restricted program because it is a relative path
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/Library/Python/2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/Library/Python/2.7/site-packages/tensorflow/python/__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/Library/Python/2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Library/Python/2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/Library/Python/2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
ImportError: dlopen(/Library/Python/2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib
  Referenced from: /Library/Python/2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
  Reason: image not found


OS: OSX 10.11.6 
Tensorflow pip version: tensorflow-gpu==0.12.1

My ""cuda"" libs under /usr/local/cuda/lib/* 

/usr/local/cuda/lib/libcublas.8.0.dylib		/usr/local/cuda/lib/libcusparse.8.0.dylib	/usr/local/cuda/lib/libnppim.8.0.dylib
/usr/local/cuda/lib/libcublas.dylib		/usr/local/cuda/lib/libcusparse.dylib		/usr/local/cuda/lib/libnppim.dylib
/usr/local/cuda/lib/libcublas_device.a		/usr/local/cuda/lib/libcusparse_static.a	/usr/local/cuda/lib/libnppim_static.a
/usr/local/cuda/lib/libcublas_static.a		/usr/local/cuda/lib/libnppc.8.0.dylib		/usr/local/cuda/lib/libnppist.8.0.dylib
/usr/local/cuda/lib/libcuda.dylib		/usr/local/cuda/lib/libnppc.dylib		/usr/local/cuda/lib/libnppist.dylib
/usr/local/cuda/lib/libcudadevrt.a		/usr/local/cuda/lib/libnppc_static.a		/usr/local/cuda/lib/libnppist_static.a
/usr/local/cuda/lib/libcudart.8.0.dylib		/usr/local/cuda/lib/libnppi.8.0.dylib		/usr/local/cuda/lib/libnppisu.8.0.dylib
/usr/local/cuda/lib/libcudart.dylib		/usr/local/cuda/lib/libnppi.dylib		/usr/local/cuda/lib/libnppisu.dylib
/usr/local/cuda/lib/libcudart_static.a		/usr/local/cuda/lib/libnppi_static.a		/usr/local/cuda/lib/libnppisu_static.a
/usr/local/cuda/lib/libcudnn.5.dylib		/usr/local/cuda/lib/libnppial.8.0.dylib		/usr/local/cuda/lib/libnppitc.8.0.dylib
/usr/local/cuda/lib/libcudnn.dylib		/usr/local/cuda/lib/libnppial.dylib		/usr/local/cuda/lib/libnppitc.dylib
/usr/local/cuda/lib/libcudnn_static.a		/usr/local/cuda/lib/libnppial_static.a		/usr/local/cuda/lib/libnppitc_static.a
/usr/local/cuda/lib/libcufft.8.0.dylib		/usr/local/cuda/lib/libnppicc.8.0.dylib		/usr/local/cuda/lib/libnpps.8.0.dylib
/usr/local/cuda/lib/libcufft.dylib		/usr/local/cuda/lib/libnppicc.dylib		/usr/local/cuda/lib/libnpps.dylib
/usr/local/cuda/lib/libcufft_static.a		/usr/local/cuda/lib/libnppicc_static.a		/usr/local/cuda/lib/libnpps_static.a
/usr/local/cuda/lib/libcufftw.8.0.dylib		/usr/local/cuda/lib/libnppicom.8.0.dylib	/usr/local/cuda/lib/libnvToolsExt.1.dylib
/usr/local/cuda/lib/libcufftw.dylib		/usr/local/cuda/lib/libnppicom.dylib		/usr/local/cuda/lib/libnvToolsExt.dylib
/usr/local/cuda/lib/libcufftw_static.a		/usr/local/cuda/lib/libnppicom_static.a		/usr/local/cuda/lib/libnvblas.8.0.dylib
/usr/local/cuda/lib/libcuinj.8.0.dylib		/usr/local/cuda/lib/libnppidei.8.0.dylib	/usr/local/cuda/lib/libnvblas.dylib
/usr/local/cuda/lib/libcuinj.dylib		/usr/local/cuda/lib/libnppidei.dylib		/usr/local/cuda/lib/libnvgraph.8.0.dylib
/usr/local/cuda/lib/libculibos.a		/usr/local/cuda/lib/libnppidei_static.a		/usr/local/cuda/lib/libnvgraph.dylib
/usr/local/cuda/lib/libcurand.8.0.dylib		/usr/local/cuda/lib/libnppif.8.0.dylib		/usr/local/cuda/lib/libnvgraph_static.a
/usr/local/cuda/lib/libcurand.dylib		/usr/local/cuda/lib/libnppif.dylib		/usr/local/cuda/lib/libnvrtc-builtins.8.0.dylib
/usr/local/cuda/lib/libcurand_static.a		/usr/local/cuda/lib/libnppif_static.a		/usr/local/cuda/lib/libnvrtc-builtins.dylib
/usr/local/cuda/lib/libcusolver.8.0.dylib	/usr/local/cuda/lib/libnppig.8.0.dylib		/usr/local/cuda/lib/libnvrtc.8.0.dylib
/usr/local/cuda/lib/libcusolver.dylib		/usr/local/cuda/lib/libnppig.dylib		/usr/local/cuda/lib/libnvrtc.dylib
/usr/local/cuda/lib/libcusolver_static.a	/usr/local/cuda/lib/libnppig_static.a
"
7464,Android Camera Demo sometimes doesn't works,"I have built Tensorflow Android Camera Demo with a custom model.
Sometimes the app works correctly on my phone, but at other times it gives error and is terminated.
These are my adb logcat:
```
02-13 14:53:44.751 22219 22234 F libc    : Fatal signal 11 (SIGSEGV), code 2, fault addr 0xee9c2000 in tid 22234 (ImageListener)
02-13 14:53:44.801  3038  3038 F DEBUG   : *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
02-13 14:53:44.801  3038  3038 F DEBUG   : Build fingerprint: 'samsung/zeroltexx/zerolte:6.0.1/MMB29K/G925FXXU4DPGW:user/release-keys'
02-13 14:53:44.801  3038  3038 F DEBUG   : Revision: '10'
02-13 14:53:44.801  3038  3038 F DEBUG   : ABI: 'arm'
02-13 14:53:44.811  3038  3038 F DEBUG   : pid: 22219, tid: 22234, name: ImageListener  >>> org.tensorflow.demo <<<
02-13 14:53:44.811  3038  3038 F DEBUG   : signal 11 (SIGSEGV), code 2 (SEGV_ACCERR), fault addr 0xee9c2000
02-13 14:53:44.811  3038  3038 F DEBUG   :     r0 000004a8  r1 eec1000c  r2 ee9b800c  r3 00009ff4
02-13 14:53:44.811  3038  3038 F DEBUG   :     r4 00000000  r5 ffffff80  r6 00000662  r7 00000c80
02-13 14:53:44.811  3038  3038 F DEBUG   :     r8 fffffcbf  r9 de0605d8  sl 00009ec0  fp 000001e0
02-13 14:53:44.811  3038  3038 F DEBUG   :     ip 0003ffff  sp f3722408  lr 00000135  pc decf5ee6  cpsr 800e0030
02-13 14:53:44.821  3038  3038 F DEBUG   :
02-13 14:53:44.821  3038  3038 F DEBUG   : backtrace:
02-13 14:53:44.821  3038  3038 F DEBUG   :     #00 pc 00090ee6  /data/app/org.tensorflow.demo-1/lib/arm/libtensorflow_demo.so
02-13 14:53:44.821  3038  3038 F DEBUG   :     #01 pc 00086767  /data/app/org.tensorflow.demo-1/lib/arm/libtensorflow_demo.so (Java_org_tensorflow_demo_env_ImageUtils_convertYUV420ToARGB8888+142)
02-13 14:53:44.821  3038  3038 F DEBUG   :     #02 pc 000523e7  /data/app/org.tensorflow.demo-1/oat/arm/base.odex (offset 0x2b000) (void org.tensorflow.demo.env.ImageUtils.convertYUV420ToARGB8888(byte[], byte[], byte[], int[], int, int, int, int, int, boolean)+218)
02-13 14:53:44.821  3038  3038 F DEBUG   :     #03 pc 0003fdad  /data/app/org.tensorflow.demo-1/oat/arm/base.odex (offset 0x2b000) (void org.tensorflow.demo.ClassifierActivity.onImageAvailable(android.media.ImageReader)+1064)
02-13 14:53:44.821  3038  3038 F DEBUG   :     #04 pc 03c37c61  /system/framework/arm/boot.oat (offset 0x2f37000)
02-13 14:53:45.011  3038  3038 F DEBUG   :
02-13 14:53:45.011  3038  3038 F DEBUG   : Tombstone written to: /data/tombstones/tombstone_07
```
How can I solve this problem?"
7463,Initialize error in 0.12.1,"I run my code well under Tensorflow 0.10, but after I update the version to 0.12.1, all the variables throws a `FailedPreconditionError (see above for traceback): Attempting to use uninitialized value W` error. And my code is  unchanged except I use` tf.global_variables_initializer()` instead of `tf.initialize_all_variables()`. I tried to add `tf.GraphKeys.VARIABLES = tf.GraphKeys.GLOBAL_VARIABLES` under `import tensorflow as tf` but it didn't work. So I opened this issue because I don't know where the bug is.

My code is as below. Omitted some details to be more readable.
And the error is different every time. All tf.Variable defined variables are mentioned wrong.

```
class model(object):
    def __init__(self, paras):
        self.D = tf.constant(D, dtype = tf.float32)
        self.Q = tf.constant(Q, dtype = tf.float32)

        if self.USE_FEATURE:
            self.CF = tf.Variable((np.random.rand(self.rank, d_F) - 0.5) / self.rank, \
                                  dtype = tf.float32, name = 'CF')  #error here
        self.W = tf.Variable((np.random.rand(self.rank, sample_num) - 0.5) / self.rank / 200, \
                                  dtype = tf.float32, name = 'W')   #error here
        self.C = tf.Variable((np.random.rand(context_num, self.rank) - 0.5) / self.rank, \
                                   dtype = tf.float32, name = 'C')  #error here
        
        ED = tf.transpose(self.Q) * (1.0 / (1.0 + tf.exp(- tf.matmul(self.C, self.W))))
        recons = self.D - ED
        W_grad = tf.matmul(tf.transpose(self.C), recons)
        self.W_grad = tf.Variable(W_grad, dtype = tf.float32)  #error here
        
        self._build_update_W_grad()
        if not self.USE_FEATURE:
            self._build_alter_W()
        else:
            self._build_W_with_F()
        self._build_alter_C()

    def _run(self, sess):
        tf.initialize_all_variables().run()  #where the error throws

        for i in xrange(self.max_iter):
            if (i + 1) % self.prun_step == 0:
                self.mu = self.mu * self.prun_rate
            if (i + 1) % 2 == 1:
                for j in xrange(self.inner_maxiter):
                    if not self.USE_FEATURE:
                        self.up_W_grad.run()
                        self.up_W.run()
                    else:
                        self.up_W_grad.run()
                        for k in xrange(self.sgd_batch):
                            self.up_W_CF.run()
                            #raise NotImplementedError
            else:
                for j in xrange(self.inner_maxiter):
                    self.up_C.run()
        
        W = self.W.eval()
        C = self.C.eval()
        print 'end training. save W and C'
        return W, C

    def _build_alter_W(self):
        #codes
        self.up_W = tf.group(updata_W)
        
    def _build_W_with_F(self):    
        #codes
        self.up_W_CF = tf.group(updata_W, updata_CF)
        #raise NotImplementedError
   
    def _build_alter_C(self):
        #codes
        self.up_C = tf.group(updata_C)
   
    def _build_update_W_grad(self):
        #codes
        self.up_W_grad = tf.group(update_W_grad)

#main program
train_epoch = model(paras)
with tf.Session(config = config) as sess:
    W, C = train_epoch._run(sess)
```

Can anybody help? The program worked well in 0.10 but crashed after I updated to 0.12.1. I changed nothing but `tf.initialize_all_variables().run()` to `tf.global_variables_initializer().run()`. "
7462,ValueError: Variable d_bn1/d_bn1_2/d_bn1_2/moments/moments_1/mean/ExponentialMovingAverage/biased does not exist,"I am trying to run the model from here, http://bamos.github.io/2016/08/09/deep-completion/ , but i am facing this issue. Kindly suggest some approach.
Traceback (most recent call last):
  File ""train-dcgan.py"", line 39, in <module>
    is_crop=False, checkpoint_dir=FLAGS.checkpoint_dir)
  File ""/Users/asuma2/Code/GAN/ImageCompletion/dcgan-completion.tensorflow/model.py"", line 65, in __init__
    self.build_model()
  File ""/Users/asuma2/Code/GAN/ImageCompletion/dcgan-completion.tensorflow/model.py"", line 81, in build_model
    self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True)
  File ""/Users/asuma2/Code/GAN/ImageCompletion/dcgan-completion.tensorflow/model.py"", line 312, in discriminator
    h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name='d_h1_conv')))
  File ""/Users/asuma2/Code/GAN/ImageCompletion/dcgan-completion.tensorflow/ops.py"", line 34, in __call__
    ema_apply_op = self.ema.apply([batch_mean, batch_var])
  File ""/Users/asuma2/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/moving_averages.py"", line 391, in apply
    self._averages[var], var, decay, zero_debias=zero_debias))
  File ""/Users/asuma2/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/moving_averages.py"", line 70, in assign_moving_average
    update_delta = _zero_debias(variable, value, decay)
  File ""/Users/asuma2/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/moving_averages.py"", line 177, in _zero_debias
    trainable=False)
  File ""/Users/asuma2/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1024, in get_variable
    custom_getter=custom_getter)
  File ""/Users/asuma2/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 850, in get_variable
    custom_getter=custom_getter)
  File ""/Users/asuma2/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 346, in get_variable
    validate_shape=validate_shape)
  File ""/Users/asuma2/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 331, in _true_getter
    caching_device=caching_device, validate_shape=validate_shape)
  File ""/Users/asuma2/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 650, in _get_single_variable
    ""VarScope?"" % name)
ValueError: Variable d_bn1/d_bn1_2/d_bn1_2/moments/moments_1/mean/ExponentialMovingAverage/biased does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?
"
7461,Add aliases to make layer names more consistent?,"In the new [layers API](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/layers.py) the convolutional layers have short names (conv1d, conv3d, etc.) while the pooling layers have full names (max_pooling1d, average_pooling3d, etc.) which seems inconsistent. tf.contrib.layers had aliases. Could something like that be added in to the core layers too?

E.g.
```
max_pool1d = max_pooling1d
convolution1d = conv1d
```
and so on."
7460,Check failed GetConvolveAlgorithms() on Windows,"I'm following through the set of Udacity Tensorflow examples. 

I can run the first three without any problems, but when I use the code found in the [fourth example,](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/4_convolutions.ipynb) Python crashes and I receive a track trace that leads to 
```
c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\kernels\conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)
```

I'm using tensorflow-gpu which I've installed via: `pip install tensorflow-gpu`

```
import tensorflow as tf
print(tf.VERSION)
```

Reports the version as 0.12.1

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

I see there's issue #6509 which mentions `tf.one_hot()` and #6822 which mentions `MatrixDiag` and `OneHot` but I am not using either of these in my code so I'm not sure if they're directly related.

### Environment info
- OS: Windows 10 Pro 64-bit (10.0, Build 14393) 
- CUDA 8
- TensorFlow 0.12.1
- Nvidia GeForce 860M GPU

### If possible, provide a minimal reproducible example 

1. Run the source provided on [tensorflow/examples/udacity/1_notmnist.ipynb](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb)
2. Run the source provided on [tensorflow/examples/udacity/4_convolutions.ipynb](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/4_convolutions.ipynb)

### Logs or other output that would be helpful
```
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally
Training set (200000, 28, 28) (200000,)
Validation set (10000, 28, 28) (10000,)
Test set (10000, 28, 28) (10000,)
Training set (200000, 28, 28, 1) (200000, 10)
Validation set (10000, 28, 28, 1) (10000, 10)
Test set (10000, 28, 28, 1) (10000, 10)
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:885] Found device 0 with properties:
name: GeForce GPU
major: 5 minor: 0 memoryClockRate (GHz) 0.993
pciBusID 0000:01:00.0
Total memory: 1.00GiB
Free memory: 830.97MiB
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:906] DMA: 0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:916] 0:   Y
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GPU, pci bus id: 0000:01:00.0)
Initialized
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_dnn.cc:378] Loaded runtime CuDNN library: 5005 (compatibility version 5000) but source was compiled with 5105 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.
F c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\kernels\conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)
```
"
7459,Decode csv failed to decode csv file with commas in cell properly.,"### Environment info
Operating System: Ubuntu 16.04

Installed CUDA and cuDNN: 

Tensorflow is build from source.

Tensorflow version: 1.0.0-rc2

### Problem
```python
import tensorflow as tf
import os
filename_queue = tf.train.string_input_producer([os.path.join(os.getcwd(),""train.csv"")])
reader = tf.TextLineReader(skip_header_lines=1)
key, value = reader.read(filename_queue)
decoded = tf.decode_csv(value, record_defaults = [[0.0], [0.0], [0], [""""],[""""], [0.0], [0.0], [0.0], [""""], [0.0], [""""], [""""]])
passenger_id, survived, pclass, name, sex, age, sibsp, parch, ticket, fare, cabin, embarked = tf.train.shuffle_batch(decoded, batch_size=9, capacity=450, min_after_dequeue=9)""
```

The data to import:
```
PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked
1,0,3,""Braund, Mr. Owen Harris"",male,22,1,0,A/5 21171,7.25,,S
2,1,1,""Cumings, Mrs. John Bradley (Florence Briggs Thayer)"",female,38,1,0,PC 17599,71.2833,C85,C
3,1,3,""Heikkinen, Miss. Laina"",female,26,0,0,STON/O2. 3101282,7.925,,S
4,1,1,""Futrelle, Mrs. Jacques Heath (Lily May Peel)"",female,35,1,0,113803,53.1,C123,S
5,0,3,""Allen, Mr. William Henry"",male,35,0,0,373450,8.05,,S
6,0,3,""Moran, Mr. James"",male,,0,0,330877,8.4583,,Q
```

The column ""name"" contains comma. The tenforflow reports invalid arguments.
"
7458,Feature Request: Default project,"New default project, for anyone new to these things.
# watcher# 
>ip camera+tensorflow in order to watch your yard.
>requirements: camera+RPi+internet(pc?) 
>default project that is somewhat 'advanced' for 'I know how to apt-** people'
>install, teach, and it works"
7457,UnboundLocalError: local variable 'status' referenced before assignment,"The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`:

 python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
0.12.1

**Here's the error**:

Epoch 2/2
19125/19125 [==============================] - 78s - loss: 0.4568 - acc: 0.8681 - val_loss: 2.1682 - val_acc: 0.4104
Exception ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7f747161e160>>
Traceback (most recent call last):
  File ""/home/p3/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 581, in __del__
UnboundLocalError: local variable 'status' referenced before assignment
"
7456,Training using multiple GPUs returns Inf values for loss and Nan for grads. ,"I have two Tesla K80 cards (2 GPUs per card) and I spent few days testing a MNIST classification model using multiple GPUs. What I found is that the training process would always diverge (got Nan for grads and Inf for loss) when I use two GPUs which are in the same card, however when I allocated two GPUs to my training operation from different cards, it would lead to convergence. By the way, everything worked well on a single GPU. 

I am not sure about how GPUs compute those networks and it is really weird two GPUs from the same card make my model diverge and from different cards can make it converge.

The output for divergence is like the below:
```man
2017-02-13 12:30:11.255323: step 10, loss = 5799703749333771039308345507840.00 (980.7 examples/sec; 0.102 sec/batch)
2017-02-13 12:30:14.131089: step 20, loss = 2102245862526597403246592.00 (793.5 examples/sec; 0.126 sec/batch)
2017-02-13 12:30:16.995940: step 30, loss = 2.30 (787.6 examples/sec; 0.127 sec/batch)
W tensorflow/core/framework/op_kernel.cc:975] Invalid argument: Nan in summary histogram for: layer2/weights_1
	 [[Node: layer2/weights_1 = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](layer2/weights_1/tag, layer2/weights/read/_117)]]
```
I used cpu to preprocess the input data read from tfreords. My code for computing the average grads:
```man
def average_gradients(tower_grads):
    average_grads = []
    for grad_and_vars in zip(*tower_grads):
        grads = []
        for g, _ in grad_and_vars:
            expanded_g = tf.expand_dims(g, 0)
            grads.append(expanded_g)
        grad = tf.concat(0, grads)
        grad = tf.reduce_mean(grad, 0)
        v = grad_and_vars[0][1]
        grad_and_var = (grad, v)
        average_grads.append(grad_and_var)
    return average_grads 
```

The training_op:
```man
def main(argv=None): 
    with tf.Graph().as_default(), tf.device('/cpu:0'):
        x, y_ = get_input()
        regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)
        
        global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)
        learning_rate = tf.train.exponential_decay(
            LEARNING_RATE_BASE, global_step, 60000 / BATCH_SIZE, LEARNING_RATE_DECAY)       
        
        opt = tf.train.GradientDescentOptimizer(learning_rate)
        
        tower_grads = []
        for i in range(N_GPU):
            with tf.device('/gpu:%d' % i):
                with tf.name_scope('GPU_%d' % i) as scope:
                    cur_loss = get_loss(x, y_, regularizer, scope)
                    tf.get_variable_scope().reuse_variables()
                    grads = opt.compute_gradients(cur_loss)
                    tower_grads.append(grads)
        
        grads = average_gradients(tower_grads)
        for grad, var in grads:
            if grad is not None:
            	tf.summary.histogram('gradients_on_average/%s' % var.op.name, grad)

        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)
        for var in tf.trainable_variables():
            tf.summary.histogram(var.op.name, var)

        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)
        variables_averages_op = variable_averages.apply(tf.trainable_variables())
        train_op = tf.group(apply_gradient_op, variables_averages_op)
```
"
7455,Tensorflow value error when setting up training_data,"Hey, I'm trying to feed a csv to tf.contrib.learn and running into a ValueError over and over again. Here's my code for the training_set variable, plus the ensuing error:

training_set = tf.contrib.learn.datasets.base.load_csv_with_header(
	filename = MET_TRAINING,
	target_dtype = np.float,
	features_dtype = np.str)
Traceback (most recent call last):
  File ""<pyshell#15>"", line 4, in <module>
    features_dtype = np.str)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py"", line 48, in load_csv_with_header
    n_samples = int(header[0])
ValueError: invalid literal for int() with base 10: 'conflict'

The .csv file contains mostly strings, but post training I'd like tf to give me either a '0' or '1' depending on the category an text example ought to fall in. Suggestions on what to do from here?"
7454,(iOS Camera Example from GitHub) 'unsupported/Eigen/CXX11/Tensor' file not found,"
<p>I have just downloaded the tensorflow iOS Camera Example from GitHub and when I run the project it crashes with the error ""'unsupported/Eigen/CXX11/Tensor' file not found"".  I have added an example and would really like some help as soon as possible.</p> <br>

<p>All the best, James</p>

<br>

<img width=""877"" alt=""screen shot 2017-02-12 at 20 11 26"" src=""https://cloud.githubusercontent.com/assets/18594256/22865597/7abd2032-f15f-11e6-801d-7f7909952041.png"">
"
7453,error while building on windows with cuda 7.5,"I have spent days to build tensorflow on windows with cuda 7.5 but met a lot of error. I first followed https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake/readme.md. I modified CmakeList.txt from cuda 8 to cuda 7.5. But nvcc version 7.5 do not support visual studio 2015. When I change to visual studio 2013. It failed with constexpr because visual studio 2013 do not support it. When I remove constexpr it got a lot of error of noexcept, uint16 and so on. So is it possible to build tensorflow with cuda 7.5? In a lot of cases, it's not possible to use cuda 8 . So can it be seen as a feature request to support cuda 7.5 on windows? Thanks."
7452,Bugs in TensorFlowYoloDetector,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
None

### Environment info
Operating System: Windows 10

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
099ef38a99d069b9a1f6d09a289b2df69eaee276
2. The output of `bazel version`
Build label: 0.4.2
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Dec 7 18:47:13 2016 (1481136433)
Build timestamp: 1481136433
Build timestamp as int: 1481136433

----
It looks like there are bugs in [TensorfFlowYoloDetector](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java).

1. **The confidence calculation is broken**
In line 218: 

` final float confidence = output[offset + 4];`

Should be 

` final float confidence = expit( output[offset + 4] );`

You can see line 42 at [DarkFlow file](https://github.com/thtrieu/darkflow/blob/master/net/yolov2/test.py)

2. **The RGB values might be read as BGR in line 152:**
```
    for (int i = 0; i < intValues.length; ++i) {
      floatValues[i * 3 + 0] = (intValues[i] & 0xFF) / 255.0f;
      floatValues[i * 3 + 1] = ((intValues[i] >> 8) & 0xFF) / 255.0f;
      floatValues[i * 3 + 2] = ((intValues[i] >> 16) & 0xFF) / 255.0f;
    }
```

Unless there is a bug in DarkFlow, in order to have the same results, one must change to:
```
    for (int i = 0; i < intValues.length; ++i) {
      floatValues[i * 3 + 2] = (intValues[i] & 0xFF) / 255.0f;
      floatValues[i * 3 + 1] = ((intValues[i] >> 8) & 0xFF) / 255.0f;
      floatValues[i * 3 + 0] = ((intValues[i] >> 16) & 0xFF) / 255.0f;
    }
```

3. **The NMS calculation is completely different from DarkFlow and DarkNet**
See line 242, it just return top K results, without any non-maximum suppression

Here is the code from the file:
```
    final ArrayList<Recognition> recognitions = new ArrayList<Recognition>();
    for (int i = 0; i < Math.min(pq.size(), MAX_RESULTS); ++i) {
      recognitions.add(pq.poll());
    }
```


Here is the correct code from [DarkFlow, line 52](https://github.com/thtrieu/darkflow/blob/master/net/yolov2/test.py):
```
	# non max suppress boxes
	for c in range(C):
		for i in range(len(boxes)):
			boxes[i].class_num = c
		boxes = sorted(boxes, key = prob_compare)
		for i in range(len(boxes)):
			boxi = boxes[i]
			if boxi.probs[c] == 0: continue
			for j in range(i + 1, len(boxes)):
				boxj = boxes[j]
				if box_iou(boxi, boxj) >= .4:
					boxes[j].probs[c] = 0.
```"
7451,"Getting the following error :/usr/prachi/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 944, in _run     % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape()))) ValueError: Cannot feed value of shape (0,) for Tensor u'input/BottleneckInputPlaceholder:0', which has shape '(?, 2048)'","NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
7450,android demo app: tf_detect using  yolo instead of multibox_detect,"Does anyone know how to build android demo app which uses yolo detector instead of multibox detector ?

Thanks,,
"
7449,bazel build --copt=-march=native not using available CPU instructions,"**Update 2017-12-06:**
The current version of my [tensorflow.sh install script](https://github.com/ahundt/robotics_setup/blob/master/tensorflow.sh) has been working well for me, and the update from tf 1.3 to tf 1.4 required only a one character change!

**Original Post:**
Here are the key lines in my install script with a quote from the tensorflow docs:
```
# To be compatible with as wide a range of machines as possible, TensorFlow defaults to only using SSE4.1 SIMD instructions on x86 machines. Most modern PCs and Macs support more advanced instructions, so if you're building a binary that you'll only be running on your own machine, you can enable these by using --copt=-march=native in your bazel build command.

bazel build --copt=-march=native -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```
### Even with `--copt=-march=native` I get the following warnings about the CPU instruction set, contradicting the above statement:

```

W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
```

Here is the exact script I used to build tensorflow:
https://github.com/ahundt/robotics_setup/blob/b5ee71f262ec36f8dbc8374ed2503c0812fb0f47/tensorflow.sh


### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

http://stackoverflow.com/a/41520266/99379

Operating System:
Ubuntu 16.04

2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

```
python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.0
```

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
07bb8ea2379bd459832b23951fb20ec47f3fdbd4

2. The output of `bazel version`

```
 bazel version
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261
```


### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
```

python -c 'import tensorflow as tf; print(tf.__version__); sess = tf.InteractiveSession(); sess.close();'
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.0
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:02:00.0
Total memory: 7.92GiB
Free memory: 7.81GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0)
```

### What other attempted solutions have you tried?
This person tried some other things: http://stackoverflow.com/a/41520266/99379

"
7448,tf.summary.FileWriter crashes with AlreadyExistsError,"We have a cluster with ~20 GPUs that we often use to train multiple networks in parallel, and we use `tf.summary.Filewriter` to keep track of the networks' progress. However, some jobs are crashing when they attempt to create their `FileWriter`s with the following stack trace:

```
[...]
  File ""/ubc/cs/research/tracking-raid/julm/eyescream/tensorflow/pose_estimation/linear_model.py"", line 141, in __init__
    self.train_writer = tf.summary.FileWriter( os.path.join(summaries_dir, 'train' ))
  File ""/ubc/cs/research/tracking-raid/julm/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/summary/writer/writer.py"", line 308, in __init__
    event_writer = EventFileWriter(logdir, max_queue, flush_secs)
  File ""/ubc/cs/research/tracking-raid/julm/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py"", line 69, in __init__
    gfile.MakeDirs(self._logdir)
  File ""/ubc/cs/research/tracking-raid/julm/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 299, in recursive_create_dir
    pywrap_tensorflow.RecursivelyCreateDir(compat.as_bytes(dirname), status)
  File ""/ubc/cs/research/tracking-raid/julm/anaconda/envs/tensorflow/lib/python2.7/contextlib.py"", line 24, in __exit__
    self.gen.next()
  File ""/ubc/cs/research/tracking-raid/julm/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.AlreadyExistsError: /global/scratch/julm/3d_experiments
```

I am writing the `Filewriter`s' progress under `/global/scratch/julm/3d_experiments/`, and creating multiple subdirectories depending on the hyperparameters that each network is using.

The error seems to suggest that the `FileWriter` is trying to create the directory `/global/scratch/julm/3d_experiments/` and crashing because the directory already exists. Moreover, only around 1 in 5 jobs crashes with this error.

Do you know if I could somehow ignore this error? I don't think the fact that the directory exists should trigger an error for the user.

Our cluster runs under OpenSUSE 42.2."
7446,Android Example TF Detect crashes when built with files in source directory,"Android TF Detect crashes when I make the apk with inception_5 and mobile_multibox files in the assets directory and removed in BUILD as per the instructions at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android

It works when built with automatic download. 

I commented out lines 66 & 67 in BUILD.
 ""@inception5h//:model_files"",
 ""@mobile_multibox//:model_files"",

The file size of the apk built with the files added to assets manually is 4195 bytes smaller than the apk built with automatically downloaded files.  "
7445,memory overflow when processing Variable.eval(),"I'm using Tensorflow to process a simple matrix factorization algorithm. Every step went correct but at the last step, where I want to `eval()` a Tensor to store it, the program didn't work and only occupied more and more memory. I tried to `eval()` the initial parameters before the algorithm, it correctly processed. Then I'm confused where the bug is.
Core code is as follows.
```

class model(object):
    def __init__(self, others):
        self.D = tf.constant(D, dtype = tf.float32)
        self.Q = tf.constant(Q, dtype = tf.float32)
        self.W = tf.Variable((np.random.rand(self.rank, sample_num)), dtype = tf.float32, name = 'W')
        self.C = tf.Variable((np.random.rand(context_num, self.rank)), dtype = tf.float32, name = 'C')

    def _run(self, sess):
        Q = self.Q
        D = self.D
        W = self.W
        print W.eval()     #correct results
        C = self.C
        #the optimization step, you can jump this because I think it's not the point
        for i in xrange(self.max_iter):
            if (i + 1) % 2 == 1:
                for j in xrange(self.inner_maxiter):
                    ED = tf.transpose(Q) * (1.0 / (1.0 + tf.exp(- tf.matmul(C, W))))
                    recons = D - ED
                    W_grad = tf.matmul(tf.transpose(C), recons)
                    W = W + self.stepsize * W_grad
            else:
                for j in xrange(self.inner_maxiter):
                    ED = tf.transpose(Q) * (1.0 / (1.0 + tf.exp(- tf.matmul(C, W))))
                    recons = D - ED
                    C_grad = tf.matmul(recons, tf.transpose(W))
                    C = C + self.stepsize * C_grad
            print 'epoch: %d' % i
        
        print W.eval()  #program stopped and occupying memory 
        print C.eval()

train_epoch = model(D, Q, others)
with tf.Session(config = config) as sess:
    tf.initialize_all_variables().run()
    train_epoch._run(sess)
```

It's fairly strange because eval() just work well before the optimization step, but crashed after it. Is this a bug in eval()?"
7444,tfprof model analyzer ignores scalar parameters,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Searched issues for ""tfprof""

### Environment info
gcr.io/tensorflow/tensorflow:1.0.0-rc2-devel-gpu

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
```python
import tensorflow as tf

sess = tf.InteractiveSession()
v = tf.Variable(0., dtype=tf.float32, trainable=True)

tf.contrib.tfprof.model_analyzer.print_model_analysis(tf.get_default_graph())
```

This will show `total_parameters` of `0`, despite there being a parameter.

### What other attempted solutions have you tried?
N/A

### Logs or other output that would be helpful
```
name: ""_TFProfRoot""
exec_micros: 0
requested_bytes: 0
total_exec_micros: 0
total_requested_bytes: 0
total_parameters: 0
children {
  name: ""Variable""
  exec_micros: 0
  requested_bytes: 0
  total_exec_micros: 0
  total_requested_bytes: 0
  total_parameters: 0
  float_ops: 0
  total_float_ops: 0
}
float_ops: 0
total_float_ops: 0
```

I believe this is coming from the logic here: https://github.com/tensorflow/tensorflow/blob/v1.0.0-rc2/tensorflow/tools/tfprof/internal/tfprof_show.cc#L37 which skips nodes with empty shapes, which includes scalars."
7443,Add Continuous Integration + Review Requirements to CONTRIBUTING.md,"Could some basic instructions and suggestions be added to [CONTRIBUTING.md](
https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md)?

Example notes to add:
 - correctly installing and configuring a compliant python/C++ linter
 - links and explanation of other guidelines that must be followed
 - how to run tests on your own machine
"
7440,can't install TensorFlow properly (to use KeraS),"I can't install TensorFlow properly (to use it with Keras)  
I did install it manually with this command:
pip install tensorflow-1.0.0rc2-py2-none-any.whl
My operating system: OSX: 10.12.3

here's the error I get, any suggestions?

Using TensorFlow backend.

Traceback (most recent call last):
  File ""<pyshell#1>"", line 1, in <module>
    import keras
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/__init__.py"", line 2, in <module>
    from . import backend
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/backend/__init__.py"", line 67, in <module>
    from .tensorflow_backend import *
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py"", line 1, in <module>
    import tensorflow as tf
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
ImportError: dlopen(/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): no suitable image found.  Did find:
	/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: mach-o, but wrong architecture


Failed to load the native TensorFlow runtime."
7439,Batch Normalization for Multi-GPU / Data Parallelism,"Where is the batch normalization implementation for Multi-GPU scenarios? How does one keep track of `mean`, `variance`, `offset` and `scale` in the context of the Multi-GPU example as given in the [CIFAR-10 tutorial](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py)?

Why is the question on [StackOverflow](http://stackoverflow.com/questions/41819080/how-do-i-use-batch-normalization-in-a-multi-gpu-setting-in-tensorflow) left unanswered for so long?

For all the beauty that it brings with Tensorboard etc.. , it's kinda appalling to see Tensorflow so far behind Torch in terms of its modeling capability. I'd be really glad if someone takes up responsibility and comes up with a decent Batch Normalization implementation for all cases. Even if it is already there, could anyone care enough to make a **good documentation** out of it?

There are so many issues pertaining to batch normalization with Tensorflow. It's important that you guys straighten this out as batch normalization enables super-fast convergence for very deep networks and it is **REALLY** important for modern day deep learning research.

PS: Please spare my outburst. I've been a Torch user for more than a year and I had very high hopes on Tensorflow."
7434,"InvalidArgumentError Invalid JPEG data, size 4096 for retain.py(image_retrain), Not JPEG file 0x00 0x05","Problem: when i run the retrain script with my sub_folder(my photos) shown in the Image retrain tutorial,
https://www.tensorflow.org/how_tos/image_retraining/
 It gives that error, InvalidArgumentError Invalid JPEG data. I took a look at this thread https://github.com/tensorflow/tensorflow/issues/4009

And changed the tf.image.decode_jpeg to tf.image.decode_image, but no luck(i also made sure all the photos are jpeg). I am wondering the retrain script for the tutorial is there a Max-size set for the images? Because i couldnt find if there is a max-size, or if its still some problem with format of the photos i put in. 

I have ran the flower_photos example with no problem, running on Ubuntu 16.04, CUDA 8.0. 


Edit: I am using the flower_photos dataset from the tensorflow website, i used a USB stick to transfer the file to the window machine running virtual box Ubuntu 16.04, i deleted 4 sub_folders(only training daisy and dandelion), then the bottleneck process went through about 912 photos and suddenly a daisy photo had the same Invalid JPEG error. I am wondering if its because of the photos transfered through a USB from a Mac to Window then onto a virtual box causes JPEG to corrupt? "
7433,"Failed to build from source for r0.12 about :_rules_closure: no such attribute 'urls""","Hi, I'm trying to install tensorflow r0.12 on a cluster with CentOS. i came across with following errors:
```
ERROR: /data0/title/tensorflow/WORKSPACE:3:1: //external:io_bazel_rules_closure: no such attribute 'urls' in 'http_archive' rule.
ERROR: /data0/title/tensorflow/WORKSPACE:3:1: //external:io_bazel_rules_closure: missing value for mandatory attribute 'url' in 'http_archive' rule.
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error loading package 'external': Could not load //external package.

```

bazel 0.4.0
cuda 7.5
cudnn 5.1
small help will welcome, tks."
7432,Batch normalization for  Bidirectional RNN ?tensorflow support it?,Batch normalization for  Bidirectional RNN ?tensorflow support it?
7422,Feature: Add reduce_average (weighted reduce_mean),[Numpy has a function `average`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.average.html) which peforms a weighted mean. I suggest adding this function to tensorflow or adding an optional `weights` argument to `reduce_mean`.
7420,TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.,"I tried to start these tutorial https://github.com/nlintz/TensorFlow-Tutorials/blob/master/08_word2vec.py and get these error
>D:\Programms\Python35>python 08_word2vec.py
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA libr
ary cublas64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA libr
ary cudnn64_5.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA libr
ary cufft64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA libr
ary nvcuda.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA libr
ary curand64_80.dll locally
Word count [('cats', 10), ('dogs', 6), ('and', 5), ('are', 4), ('love', 3)]
Sample data [7, 12, 24, 11, 13, 19, 7, 15, 17, 27] ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', 'I']
Context pairs [[[7, 24], 12], [[12, 11], 24], [[24, 13], 11], [[11, 19], 13], [[13, 7], 19], [[19, 15], 7], [[7, 17], 15], [[15, 27], 17], [
[17, 4], 27], [[27, 0], 4]]
skip-gram pairs [[12, 7], [12, 24], [24, 12], [24, 11], [11, 24]]
Batches (x, y) ([17, 20, 0], [[15], [14], [25]])
Traceback (most recent call last):
File ""D:\Programms\Python35\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 490, in apply_op
preferred_dtype=default_dtype)
File ""D:\Programms\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 669, in convert_to_tensor
ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
File ""D:\Programms\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 583, in _TensorTensorConversionFunction
% (dtype.name, t.dtype.name, str(t)))
ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(""nce_loss/Reshape_1:0"", shape=(?, 1, ?), dtype=fl
oat32)'

> During handling of the above exception, another exception occurred:

>Traceback (most recent call last):
File ""08_word2vec.py"", line 92, in 
loss = tf.reduce_mean(tf.nn.nce_loss(nce_weights, nce_biases, train_labels, embed, num_sampled, voc_size))
File ""D:\Programms\Python35\lib\site-packages\tensorflow\python\ops\nn.py"", line 1336, in nce_loss
name=name)
File ""D:\Programms\Python35\lib\site-packages\tensorflow\python\ops\nn.py"", line 1198, in _compute_sampled_logits
array_ops.reshape(true_w, new_true_w_shape))
File ""D:\Programms\Python35\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 1613, in mul
result = _op_def_lib.apply_op(""Mul"", x=x, y=y, name=name)
File ""D:\Programms\Python35\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 521, in apply_op
inferred_from[input_arg.type_attr]))
TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'."
7419,Build android demo using a custom classifier,"I want build TensorFlow Android Camera Demo using a custom classifier following [this](https://www.oreilly.com/learning/tensorflow-on-android) tutorial.
When I build the app using `bazel build //tensorflow/examples/android:tensorflow_demo` I get:
```
CONFLICT: asset:WORKSPACE is provided with ambiguous priority from:
        external/mobile_multibox/WORKSPACE
        external/inception5h/WORKSPACE
CONFLICT: asset:WORKSPACE is provided with ambiguous priority from:
        external/stylize/WORKSPACE
        external/mobile_multibox/WORKSPACE
```
Thanks in advance!
"
7417,Crashes in tf.sparse_tensor_dense_matmul,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Possibly related to #4282?

Also, [SO seems to suggest](https://stackoverflow.com/questions/29401116/abort-trap-6-in-c-program) that ""abort trap 6"" is related to accessing memory that TF doesn't own.

Both problems boil down to bounds errors crashing the Python session instead of throwing an error.

### Environment info
Operating System: macOS Sierra 10.12.3

TF version 0.12.1. Pretty sure I'm using a CPU-only build, and my graphics card is not CUDA-compatible.

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I've managed to crash Python with sparse_tensor_dense_matmul in two different ways.  I've included a reproducible example for each.  Let me know if you'd prefer them split into two separate issues.

***Crash 1***

An off-by-one error crashes my Python session.

```
import numpy as np
import tensorflow as tf
sess = tf.InteractiveSession()

nrow = 3
ncol = 10

X = tf.sparse_placeholder(tf.float32, shape=[None, ncol])
W = tf.ones([ncol,1])

# Note: largest allowable column index is ncol-1 because of zero-indexing
col_indices = [1, 2, ncol] 
indices = np.transpose(np.array([range(nrow),col_indices]))

sess.run(tf.sparse_tensor_dense_matmul(X,W), feed_dict={X:(indices, [1] * nrow, [nrow, ncol])})
```

This kills my Python session, and I get the following at my bash prompt:

```
F tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:242] Check failed: k < lhs_right (10 vs. 10)
Abort trap: 6
```

***Crash 2***

```
import numpy as np
import tensorflow as tf
sess = tf.InteractiveSession()

n = 3
m = 3324
p = 49

a_sparse = tf.sparse_placeholder(tf.float32, shape=[None, m])
b = tf.Variable(tf.random_normal([m, p]))


indices = [[0,0],[1, 2]]
values = [1, 1]
shape = [n,m]

feed={a_sparse:(np.array([[3828,  135],
        [ 320,   11]]), [1, 1], [2, 3324])}


init = tf.global_variables_initializer()
sess.run(init)
sess.run(tf.sparse_tensor_dense_matmul(a_sparse, b), feed_dict=feed)
```

```
F tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:273] Check failed: m < out.dimension(0) (3828 vs. 2)
Abort trap: 6
```

It looks like I'm exceeding the number of rows this time, and getting a crash instead of an error.

### What other attempted solutions have you tried?

N/A"
7416,Bazel build fails on SUSE Enterprise due to too many open files,"This could also be a bazel issue, I don't know.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
None

### Environment info
Operating System: SUSE Linux Enterprise Server 12 SP1

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```
$ ls /usr/local/cuda/lib/libcuda*
/usr/local/cuda/lib/libcudadevrt.a  /usr/local/cuda/lib/libcudart.so.7.0     /usr/local/cuda/lib/libcudart_static.a
/usr/local/cuda/lib/libcudart.so    /usr/local/cuda/lib/libcudart.so.7.0.28
$ ls ~/.local/lib64/libcudnn*
/home/student/r/rdiederichse/.local/lib64/libcudnn.so    /home/student/r/rdiederichse/.local/lib64/libcudnn.so.4.0.7
/home/student/r/rdiederichse/.local/lib64/libcudnn.so.4  /home/student/r/rdiederichse/.local/lib64/libcudnn_static.a
```
If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`): `d86cadcdf03ae6fef63f34c90150a4518a121de8`
2. The output of `bazel version`
```
Warning: ignoring http_proxy in environment.
Build label: 0.4.4- (@non-git)
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Feb 10 14:39:20 2017 (1486737560)
Build timestamp: 1486737560
Build timestamp as int: 1486737560
```

The error I get when  running `bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` is (after some time)
```
ERROR: /home/student/r/rdiederichse/.cache/bazel/_bazel_rdiederichse/e5cce820cc082410b4fcc604db349066/external/jpeg/BUILD:37:1: C++ compilation of rule '@jpeg//:jpeg' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 33 argument(s) skipped): java.io.IOException: Cannot run program ""/home/student/r/rdiederichse/.cache/bazel/_bazel_rdiederichse/e5cce820cc082410b4fcc604db349066/execroot/tensorflow/_bin/process-wrapper"" (in directory ""/home/student/r/rdiederichse/.cache/bazel/_bazel_rdiederichse/e5cce820cc082410b4fcc604db349066/execroot/tensorflow""): error=24, Too many open files.
```"
7415,Scale-out performance limitation for distributed session,"
![image](https://cloud.githubusercontent.com/assets/433170/22829003/8b585f4e-efa0-11e6-9ddc-34a6b7e65cfb.png)

Scale-out performance for scatter-gather dataflow pipelines is limited.

I use Tensorflow to build custom pipelines (i.e. I write my own OpKernels) of ""embarrassingly parallel"" problems (no coordination between ""local"" pipelines required). Typically these involve the _local_ pipeline being replicated across all the machines in my cluster, and having a source and sink queue to feed input and receive output, respectively.

The issue: the performance degrades linearly with the number of nodes in the cluster, compared to a perfect scale-out performance.

I am not sure if this is a fundamental limitation of the distributed session, or if there is a way within Tensorflow to build such graphs better (with less coordination needed between independent _local_ pipelines).

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Not much guidance. When speaking with other folks at conferences, it seems common to sidestep the distributed session in Tensorflow for custom, non-TF solutions (e.g. MPI). I personally use ZeroMQ to facilitate these sort of scatter-gather patterns.

### Environment info

* Operating System: Linux (Ubuntu 16.04, 4.4.0-22)
* installed from `pip` following the default [download and setup instructions](https://www.tensorflow.org/get_started/os_setup)
* output of `python -c ""import tensorflow; print(tensorflow.__version__)""` -> `0.12.1`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

* [here is a gist with vanilla tensorflow](https://gist.github.com/samwhitlock/3b099f8489909a3293b3e4239a7188d0), with the versions I specified above. My local pipelines that I stamp out across all nodes in my cluster has custom ops, which I replicated in standard Tensorflow with a while loop to imitate the delay of my custom pipeline. I'm not sure if there is a better way to simulate the delay.
* [here is the data I collected on my cluster of 10 machines (only up to 9 to keep the source/sink queue machine unloaded)](https://docs.google.com/spreadsheets/d/1Ht8O9OT4csfYVOxSDybojXnZ-_uV2A60CF9Jt0jhps0/edit). Note that the workload scales up linearly with _only_ changes with the number of nodes; if the scale-out was perfect, the time should be the same regardless of the number of nodes.

### What other attempted solutions have you tried?

* I get around this by limitation by using ZeroMQ as a higher-performance substitute for the source/sink queues (the equivalent of `""source_queue""` and `""sink_queue""` in the example script)"
7414,extract_image_patches gradient only works with float32,"`tf.extract_image_patches()` works well with `tf.float64`, however its gradient requires that `tf.float32`s are passed. Otherwise `tf.gradients()` raises:
`TypeError: Input 'b' of 'SparseTensorDenseMatMul' Op has type float64 that does not match type float32 of argument 'a_values'.`.

Here is an example of the offending code. If I take the gradient with respect to `X` without casting, I get the error above.
```
castX = tf.cast(X, tf.float32, name=""castX"")  # This is needed to get it working
patches = tf.extract_image_patches(tf.reshape(castX, [-1, self.img_size[0], self.img_size[1], 1], name=""rX""),
                                   [1, self.patch_size[0], self.patch_size[1], 1],
                                   [1, 1, 1, 1],
                                   [1, 1, 1, 1], ""VALID"")
shp = tf.shape(patches)
return tf.cast(tf.reshape(patches, [shp[0], shp[1] * shp[2], shp[3]]), float_type)
```"
7412,AttributeError: module 'tensorflow.contrib.tfprof' has no attribute 'model_analyzer',"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem? No

### Environment info
Operating System: Win10

The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. 0.12.1

### What other attempted solutions have you tried?
I installed tensorflow on Win10 platform. The version is 0.12.1. I tried to run the model zoo's example: resnet:
`python ./resnet/resnet_main.py`
It occured the error:

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
```
""D:\Program Files (x86)\Miniconda3\python.exe"" G:/codes/tensorflow/models-master/resnet/resnet_main.py
Traceback (most recent call last):
  File ""G:/codes/tensorflow/models-master/resnet/resnet_main.py"", line 210, in <module>
    tf.app.run()
  File ""D:\Program Files (x86)\Miniconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""G:/codes/tensorflow/models-master/resnet/resnet_main.py"", line 204, in main
    train(hps)
  File ""G:/codes/tensorflow/models-master/resnet/resnet_main.py"", line 57, in train
    param_stats = tf.contrib.tfprof.model_analyzer.print_model_analysis(
AttributeError: module 'tensorflow.contrib.tfprof' has no attribute 'model_analyzer'
```"
7411,bazel build tensorflow/python/tools:optimize_for_inference doesn't work,"I want to build the Android Camera Demo using a custom classifier.
However when I run 

`bazel build tensorflow/python/tools:optimize_for_inference`

in order to optimize my graph I get these errors:

```
ERROR: /home/davide/android/tensorflow/tensorflow/core/BUILD:1259:1: no such target '//tensorflow/tools/
git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /home/david
e_biraghi/android/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: /home/davide/android/tensorflow/tensorflow/core/BUILD:1259:1: no such target '//tensorflow/tools/
git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /home/davide_biraghi/
android/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: /home/davide/android/tensorflow/tensorflow/core/BUILD:1259:1: no such target '//tensorflow/tools/
git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /home/dav
ide_biraghi/android/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'
.
ERROR: Analysis of target '//tensorflow/python/tools:optimize_for_inference' failed; build aborted.
```

Suggestion?"
7410,How to update model parameters with accumulated gradients?,"Hi,

Due to some reason, my model has limited batch size, then this limited batch-size will make the model has a high variance.

So, I want to use some trick to make the batch size larger. My idea is to store the gradients of each mini-batch, for example 64 mini-batches, and then sum the gradients together, use the mean gradients of this 64 mini batches of training data to update the model's parameters. 

This means that for the first 63 mini-batches, do not update the parameters, and after the 64 mini batch, update the model's parameters only once.

But as TensorFlow is graph based, do anyone know how to implement this wanted feature?

Thanks very much."
7408,Cannot achieve the reported accuracy 0.844 with wide_n_deep_tutorial,"After running the code in ""wide_n_deep_tutorial.py"", I achieved accuracy 0.834 for wide-only model, but the achieved accuracy for wide-and-deep model was only about 0.82.

I didn't change anything in the source code and just couldn't achieve the accuracy 0.844 as reported on the website. Can anyone help me to tell where could sth be wrong?"
7407,Estimator does not catch OutOfRange error,"I created a [4, 1, 5] tensor and use 

```
feature_a = tf.train.limit_epochs(testing_data_a, num_epochs=1, name=""feature_a_limit"")
tf.train.batch([feature_a], batch_size=2, enqueue_many=True,
                                                                allow_smaller_final_batch=True, name=""feature_a_batch"")
```

to create a batch queue that produces the tensor only once, and I then use this in the input_fn function of `model.evaluate` with steps=2. Because the batch size is 2 and the steps is 2, I expect this could use up the batch queue without an OutOfRange error (or the Estimator will catch this error and use it as an indicator to stop the evaluation), however the Estimator does not catch it and the program ends.

Is this an expected behavior? If I remember correctly Estimator used to catch such error. Another question is, why the FIFO queue still throw the OutOfRange even if I just request the exact number of elements in the queue?

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

http://stackoverflow.com/questions/42127447/tensorflows-estimator-can-only-get-n-1-batches-from-tf-train-limit-epochs

### Environment info
Operating System:

```
Linux DOMAIN 3.13.0-107-generic #154-Ubuntu SMP Tue Dec 20 09:57:27 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
```

1. The commit hash (`git rev-parse HEAD`)
```
1536a84f32f1fe77efd3fee6e5933a1dfe4e10bb
```
2. The output of `bazel version`

```
Build label: 0.4.3
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Dec 22 12:31:25 2016 (1482409885)
Build timestamp: 1482409885
Build timestamp as int: 1482409885
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```python
import tensorflow as tf
from tensorflow.contrib.layers.python.layers.optimizers import optimize_loss
from tensorflow.contrib.learn.python.learn.estimators import model_fn
from tensorflow.contrib.learn.python.learn.estimators.estimator import Estimator
from tensorflow.python import debug as tf_debug
from tensorflow.python.framework import ops


def main(_):
    hooks = [tf_debug.LocalCLIDebugHook()]

    def func(features, targets, mode, params):
        idx = tf.concat([features['a'], features['b']], axis=1)

        embedding = tf.get_variable(""embed"", [10, 20], dtype=tf.float32)

        pred = tf.reduce_sum(tf.nn.embedding_lookup(embedding, idx))

        train_op = optimize_loss(loss=pred,
                                 global_step=tf.train.get_global_step(),
                                 learning_rate=0.001,
                                 optimizer='Adam',
                                 variables=tf.trainable_variables(),
                                 name=""training_loss_optimizer"")

        eval_metric_dict = dict()
        eval_metric_dict['metric'] = pred

        return model_fn.ModelFnOps(mode=mode,
                                   predictions=pred,
                                   loss=pred,
                                   train_op=train_op,
                                   eval_metric_ops=eval_metric_dict)

    model = Estimator(func, params={})

    model.fit(
        input_fn=lambda: (
            {'a': ops.convert_to_tensor([[1, 2, 3, 4, 5]]), 'b': ops.convert_to_tensor([[2, 3, 4, 3, 5]])},
            None), max_steps=10)

    testing_data_a = [[1, 2, 3, 4, 5], [1, 2, 3, 4, 5] , [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]
    testing_data_b = [[2, 3, 4, 3, 5], [2, 3, 4, 3, 5] , [2, 3, 4, 3, 5], [2, 3, 4, 3, 5]]

    def test_input_fn():
        print(""test_input_fn entered"")
        feature_a = tf.train.limit_epochs(testing_data_a, num_epochs=1, name=""feature_a_limit"")
        feature_b = tf.train.limit_epochs(testing_data_b, num_epochs=1, name=""feature_b_limit"")

        feature_a_producer, feature_b_producer = tf.train.batch([feature_a, feature_b], batch_size=2, enqueue_many=True,
                                                                allow_smaller_final_batch=True, name=""feature_a_batch"")

        print(""test_input_fn exit"")
        return {'a': feature_a_producer, 'b': feature_b_producer}, None

    for i in range(10):
        print(model.evaluate(input_fn=test_input_fn, steps=2))
        print(""one iteration done"")


if __name__ == ""__main__"":
    tf.app.run()
```

### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
```
ssh://bshi@dsg1.crc.nd.edu:22/data/bshi/py3env/bin/python -u /data/bshi/ProjC/estimator_test.py --debug
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpxtkow_oq
test_input_fn entered
test_input_fn exit
W tensorflow/core/framework/op_kernel.cc:993] Out of range: FIFOQueue '_0_feature_a_batch/fifo_queue' is closed and has insufficient elements (requested 2, current size 0)
	 [[Node: feature_a_batch = QueueDequeueUpToV2[component_types=[DT_INT32, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](feature_a_batch/fifo_queue, feature_a_batch/n)]]
Traceback (most recent call last):
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1022, in _do_call
    return fn(*args)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1004, in _run_fn
    status, run_metadata)
  File ""/usr/lib/python3.4/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/framework/errors_impl.py"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_0_feature_a_batch/fifo_queue' is closed and has insufficient elements (requested 2, current size 0)
	 [[Node: feature_a_batch = QueueDequeueUpToV2[component_types=[DT_INT32, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](feature_a_batch/fifo_queue, feature_a_batch/n)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/bshi/ProjC/estimator_test.py"", line 62, in <module>
    tf.app.run()
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/data/bshi/ProjC/estimator_test.py"", line 57, in main
    print(model.evaluate(input_fn=test_input_fn, steps=2))
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/util/deprecation.py"", line 280, in new_func
    return func(*args, **kwargs)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 514, in evaluate
    log_progress=log_progress)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 836, in _evaluate_model
    hooks=hooks)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/contrib/training/python/training/evaluation.py"", line 430, in evaluate_once
    session.run(eval_ops, feed_dict)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py"", line 478, in __exit__
    self._close_internal(exception_type)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py"", line 508, in _close_internal
    h.end(self._coordinated_creator.tf_sess)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 647, in end
    feed_dict=self._final_ops_feed_dict)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_0_feature_a_batch/fifo_queue' is closed and has insufficient elements (requested 2, current size 0)
	 [[Node: feature_a_batch = QueueDequeueUpToV2[component_types=[DT_INT32, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](feature_a_batch/fifo_queue, feature_a_batch/n)]]

Caused by op 'feature_a_batch', defined at:
  File ""/data/bshi/ProjC/estimator_test.py"", line 62, in <module>
    tf.app.run()
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/data/bshi/ProjC/estimator_test.py"", line 57, in main
    print(model.evaluate(input_fn=test_input_fn, steps=2))
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/util/deprecation.py"", line 280, in new_func
    return func(*args, **kwargs)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 514, in evaluate
    log_progress=log_progress)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 800, in _evaluate_model
    features, labels = input_fn()
  File ""/data/bshi/ProjC/estimator_test.py"", line 51, in test_input_fn
    allow_smaller_final_batch=True, name=""feature_a_batch"")
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/input.py"", line 872, in batch
    name=name)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/input.py"", line 665, in _batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/ops/data_flow_ops.py"", line 510, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 1402, in _queue_dequeue_up_to_v2
    timeout_ms=timeout_ms, name=name)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

OutOfRangeError (see above for traceback): FIFOQueue '_0_feature_a_batch/fifo_queue' is closed and has insufficient elements (requested 2, current size 0)
	 [[Node: feature_a_batch = QueueDequeueUpToV2[component_types=[DT_INT32, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](feature_a_batch/fifo_queue, feature_a_batch/n)]]


Process finished with exit code 1
```
"
7406,Should check whether n_class is zero before calling sample_n() in mixture.py,"### Problem Description
Mixture model first use categorical to sample how much samples it need for each mixture components (this is variable `n_class` at [line 308](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distributions/python/ops/mixture.py#L308), but it actually means **number of samples per component**), and then it pass `n_class` to `sample_n()`.

The problem is `n_class` could be 0 and you can't pass `shape=0` to `tf.random_gamma(shape, alpha, ...)`, which is used in Beta distribution. (see [line 310](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distributions/python/ops/mixture.py#L310) in mixture.py)

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
It's easy to reproduce: just create a mixture of Beta + Uniform with 50/50 probability. Half of the time it'll sample from uniform, and half of the time it'll sample from Beta.
```python
#!/usr/bin/python
import tensorflow as tf
ds = tf.contrib.distributions

# Create mixture distribution of Beta + Uniform
components = [ds.Beta(2., 2.), ds.Uniform(a=0., b=1.)]
cat = ds.Categorical(p=[0.5, 0.5])
mix = ds.Mixture(cat=cat, components=components)

# get ONLY 1 sample
x = mix.sample_n(1)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())    
    # repeats until crash
    for i in range(1000):
        print sess.run(x)
```
### What other attempted solutions have you tried?

Two possible solutions:

1. Add a conditional branch in **mixture.py** like this (tested with the above script):
    ```python
    # INSTEAD OF DOING
    # samples_class_c = self.components[c].sample_n(n_class, seed=seed)
    # DO THIS
    samples_class_c = control_flow_ops.cond(
        math_ops.equal(n_class, 0),
        lambda: array_ops.zeros(0, self.components[c].dtype),
        lambda: self.components[c].sample_n(n_class, seed=seed)
    )
    ```
    Just create a zero tensor with shape 0 when `n_class` is **0** and let the reshape operator at [line 330](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distributions/python/ops/mixture.py#L330) to worry about the shape.

2. Support `shape=0` in `random_gamma(shape, alpha, ...)`. Personally I think it's a bad idea. It already caused `InvalidArgumentError` exception, which means the one who implemented this might already considered this problem before.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
```bash
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:02:00.0
Total memory: 11.90GiB
Free memory: 337.50MiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x4190110
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: GeForce GTX 980 Ti
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:01:00.0
Total memory: 5.93GiB
Free memory: 5.08GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)
[ 0.32805401]
[ 0.2802822]
W tensorflow/core/framework/op_kernel.cc:975] Invalid argument: Input shape should have non-zero element count, got: 0
	 [[Node: Beta/sample_n/random_gamma_1/RandomGamma = RandomGamma[S=DT_INT32, T=DT_FLOAT, seed=0, seed2=0, _device=""/job:localhost/replica:0/task:0/cpu:0""](Beta/sample_n/random_gamma_1/shape/_25, Beta/sample_n/random_gamma_1/add/_27)]]
Traceback (most recent call last):
  File ""bug.py"", line 21, in <module>
    print sess.run(x)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 964, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input shape should have non-zero element count, got: 0
	 [[Node: Beta/sample_n/random_gamma_1/RandomGamma = RandomGamma[S=DT_INT32, T=DT_FLOAT, seed=0, seed2=0, _device=""/job:localhost/replica:0/task:0/cpu:0""](Beta/sample_n/random_gamma_1/shape/_25, Beta/sample_n/random_gamma_1/add/_27)]]

Caused by op u'Beta/sample_n/random_gamma_1/RandomGamma', defined at:
  File ""bug.py"", line 15, in <module>
    x = mix.sample_n(1)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/distribution.py"", line 574, in sample_n
    x = self._sample_n(n, seed, **condition_kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/mixture.py"", line 313, in _sample_n
    samples_class_c = self.components[c].sample_n(n_class, seed=seed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/distribution.py"", line 574, in sample_n
    x = self._sample_n(n, seed, **condition_kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/beta.py"", line 205, in _sample_n
    seed=distribution_util.gen_new_seed(seed, ""beta""))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/random_ops.py"", line 437, in random_gamma
    seed2=seed2) / beta
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_random_ops.py"", line 122, in _random_gamma
    seed=seed, seed2=seed2, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Input shape should have non-zero element count, got: 0
	 [[Node: Beta/sample_n/random_gamma_1/RandomGamma = RandomGamma[S=DT_INT32, T=DT_FLOAT, seed=0, seed2=0, _device=""/job:localhost/replica:0/task:0/cpu:0""](Beta/sample_n/random_gamma_1/shape/_25, Beta/sample_n/random_gamma_1/add/_27)]]
```

P.S. The variable name `n_class` confused me for a while."
7405,tf.complex_abs not supported in r1.0.0 ,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.
HI,
I installed tf from [this link](https://www.tensorflow.org/versions/r1.0/get_started/os_setup) on Mac OS Sierra 10.12. However, tf.complex_abs() module is returning an error ""'module' object has no attribute 'complex_abs'"".

 The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. :: 1.0.0-rc2"
7404,No attribute 'outer_context' when calculating gradient from imported graph,"It seems when you import a graph with a ""while"" loop, you can't calculate gradients as you could on the original graph. e.g.

```python
import tensorflow as tf
i=tf.constant(0, name=""input"")
out=tf.while_loop(lambda i: tf.less(i,5), lambda i: [tf.add(i,1)], [i], name=""output"")
graph_def = tf.get_default_graph().as_graph_def()

g = tf.Graph()
with g.as_default():
    tf.import_graph_def(graph_def)
s = tf.Session(graph=g)
i_imported = g.get_tensor_by_name(""import/input:0"")
out_imported = g.get_tensor_by_name(""import/output/Exit:0"")
tf.gradients(out_imported, i_imported)
```

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-12-e7e2b78684d3> in <module>()
----> 1 tf.gradients(out_imported, i_imported)

/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.pyc in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)
    439     pending_count, loop_state = _PendingCount(ops.get_default_graph(), to_ops,
    440                                               from_ops,
--> 441                                               colocate_gradients_with_ops)
    442 
    443     # Iterate over the collected ops.


/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.pyc in _PendingCount(graph, to_ops, from_ops, colocate_gradients_with_ops)
    184   # 'loop_state' is None if there are no while loops.
    185   loop_state = control_flow_ops.MaybeCreateControlFlowState(
--> 186       between_op_list, between_ops, colocate_gradients_with_ops)
    187 
    188   # Initialize pending count for between ops.

/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in MaybeCreateControlFlowState(between_op_list, between_ops, colocate_gradients_with_ops)
   1293           loop_state.AddWhileContext(op, between_op_list, between_ops)
   1294       else:
-> 1295         loop_state.AddWhileContext(op, between_op_list, between_ops)
   1296   return loop_state
   1297 

/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in AddWhileContext(self, op, between_op_list, between_ops)
   1102     if grad_state is None:
   1103       # This is a new while loop so create a grad state for it.
-> 1104       outer_forward_ctxt = forward_ctxt.outer_context
   1105       if outer_forward_ctxt:
   1106         outer_forward_ctxt = outer_forward_ctxt.GetWhileContext()

AttributeError: 'NoneType' object has no attribute 'outer_context'
```"
7403,Incorrect second derivative of softmax cross entropy loss,"### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```python
import tensorflow as tf

logits = tf.Variable([0.5, 0.5])
y = tf.constant([1,0], dtype=tf.float32)

# direct computation
loss1 = -tf.reduce_sum(y * tf.log(tf.nn.softmax(logits)))

# optimized version
loss2 = tf.nn.softmax_cross_entropy_with_logits(logits, y)

feed = {logits: [0.5, 0.5]}

with tf.Session() as sess:
    for i, loss in enumerate([loss1, loss2]):
        g = tf.gradients(loss, [logits])[0]
        
        h0 = tf.gradients(g[0], [logits])[0]
        h1 = tf.gradients(g[1], [logits])[0]
        h = tf.pack([h0, h1])
        
        print 'loss%d:' % (i+1)
        print 'gradient:'
        print g.eval(feed_dict=feed)
        print 'hessian:'
        print h.eval(feed_dict=feed)
        print 
```

produces the output

```
loss1:
gradient:
[-0.5  0.5]
hessian:
[[ 0.25 -0.25]
 [-0.25  0.25]]

loss2:
gradient:
[-0.5  0.5]
hessian:
[[-0.  0.]
 [-0.  0.]]
```

Although the gradient is correct in both cases, the hessian computed using ```loss1``` is correct while the one computed from ```softmax_cross_entropy_with_logits``` is not. 
I have not figured out where the issue arises from."
