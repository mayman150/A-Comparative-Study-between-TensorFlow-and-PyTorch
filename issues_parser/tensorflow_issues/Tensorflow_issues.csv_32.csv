Issue Number,Issue Title,Issue Body
35489,`files_io.get_matching_files` fails for valid filenames that contain globs,"the function `files_io.get_matching_files` says it takes a ""filepath"", but actually it takes a glob

this means that if you save your checkpoints to a folder like `x=[abc]`, then you can't load the previous checkpoint using something like:

```
def load_checkpoint(sess, checkpoint_path):
  saver = tf.train.Saver(tf.global_variables())
  ckpt = tf.train.get_checkpoint_state(checkpoint_path)
  tf.logging.info('Loading model %s.', ckpt.model_checkpoint_path)
  saver.restore(sess, ckpt.model_checkpoint_path)
```

where `checkpoint_path=""./logs/x=[abc]""`."
35488,`files,"the function `files_io.get_matching_files` says it takes a ""filepath"", but actually it takes a glob

this means that if you save your checkpoints to a folder like `x=[abc]`, then you can't load the previous checkpoint using something like:

```
def load_checkpoint(sess, checkpoint_path):
  saver = tf.train.Saver(tf.global_variables())
  ckpt = tf.train.get_checkpoint_state(checkpoint_path)
  tf.logging.info('Loading model %s.', ckpt.model_checkpoint_path)
  saver.restore(sess, ckpt.model_checkpoint_path)
```

where `checkpoint_path=""./logs/x=[abc]""`."
35487,How to Write Input_Signature's for function's and model's names when using tf.fucntion?,"Hi,

I want to decorate tf.function unto my training function
```
@tf.function
def trainOneSample(data, model, optimizer, lossFunc, accFunc):
...
```
where data are a pair of 5D arrays with float32 and uint8 types (for input and label), model is the name of my NN model, optimiser is the name of an optimisation object, lossFunc and accFunc are names of custom functions. How can I decorate them in tf.function by Input_Signature? I cannot find out any solution in tf's tutorial or guide."
35485,apply_gradients Error after do weight modification,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): 1.15.0
- Python version: 3.7.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I want to do some modification of gradient, weight separately. When I do modification of gradient, it works, but when I use similar code to do modification of weight, it does not work.

```python
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))

train_op = tf.train.GradientDescentOptimizer(0.01)
w = tf.trainable_variables()[0]
grads_vars = train_op.compute_gradients(cross_entropy, var_list=[w])
residual = tf.zeros([784, 10])

for i, (g, v) in enumerate(grads_vars):
    import pdb; pdb.set_trace()
    # if len(g.shape) == 2:
    #     g_size = 1
    #     g_shape = g.get_shape().as_list()
    #     for j in g_shape:
    #         g_size *= j
    #     top_k_size = int(g_size * 0.1)
    #     g = tf.add(g, residual)
    #     g_top_k = get_top_k(g, top_k_size)
    #     residual = tf.subtract(g, g_top_k)
    #     grads_vars[i] = (g_top_k, v)
    if len(v.shape) == 2:
        v_size = 1
        v_shape = v.get_shape().as_list()
        for j in v_shape:
            v_size *= j
        top_k_size = int(v_size * 0.1)
        grads_vars[i] = (g, get_top_k(v, top_k_size))

opt = train_op.apply_gradients(grads_vars)
```

```python
Traceback (most recent call last):
  File ""/home/usu_hu_lab/Documents/tfv1/mnist_train.py"", line 52, in <module>
    if len(v.shape) == 2:
  File ""/usr/local/anaconda3/envs/tfv1/lib/python3.7/site-packages/tensorflow_core/python/training/optimizer.py"", line 614, in apply_gradients
    update_ops.append(processor.update_op(self, grad))
  File ""/usr/local/anaconda3/envs/tfv1/lib/python3.7/site-packages/tensorflow_core/python/training/optimizer.py"", line 194, in update_op
    raise NotImplementedError(""Trying to update a Tensor "", self._v)
NotImplementedError: ('Trying to update a Tensor ', <tf.Tensor 'Select:0' shape=(784, 10) dtype=float32>)
```
**Describe the expected behavior**
Actually, when I use pdb to debug the v, it is a variable, it should be updated.
```python
(Pdb) p v
<tf.Variable 'Variable:0' shape=(784, 10) dtype=float32_ref>
(Pdb) p type(v)
<class 'tensorflow.python.ops.variables.RefVariable'>
```

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35484,variational autoencoder code sample error in TF2.0,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example

## Description of issue (what needs changing):

The code given in the guide does not run if the latent dimension is set 1, it runs fine for every latent dimension >1!

### Clear description

when the model is built and trained with code:
```
vae = VariationalAutoEncoder(784, 64, 1)

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())
vae.fit(x_train, x_train, epochs=3, batch_size=64)
```
it throws the error:

**ValueError: The last dimension of the inputs to Dense should be defined. Found None**

### Correct links

n/a

### Parameters defined

latent_dim = 1
(vae = VariationalAutoEncoder(784, 64, 1) )

### Returns defined

n/a

### Raises listed and defined

**ValueError: The last dimension of the inputs to Dense should be defined. Found None**

### Usage example

code in the guide:

```
class Sampling(layers.Layer):
  """"""Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.""""""

  def call(self, inputs):
    z_mean, z_log_var = inputs
    batch = tf.shape(z_mean)[0]
    dim = tf.shape(z_mean)[1]
    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
    return z_mean + tf.exp(0.5 * z_log_var) * epsilon


class Encoder(layers.Layer):
  """"""Maps MNIST digits to a triplet (z_mean, z_log_var, z).""""""

  def __init__(self,
               latent_dim=32,
               intermediate_dim=64,
               name='encoder',
               **kwargs):
    super(Encoder, self).__init__(name=name, **kwargs)
    self.dense_proj = layers.Dense(intermediate_dim, activation='relu')
    self.dense_mean = layers.Dense(latent_dim)
    self.dense_log_var = layers.Dense(latent_dim)
    self.sampling = Sampling()

  def call(self, inputs):
    x = self.dense_proj(inputs)
    z_mean = self.dense_mean(x)
    z_log_var = self.dense_log_var(x)
    z = self.sampling((z_mean, z_log_var))
    return z_mean, z_log_var, z


class Decoder(layers.Layer):
  """"""Converts z, the encoded digit vector, back into a readable digit.""""""

  def __init__(self,
               original_dim,
               intermediate_dim=64,
               name='decoder',
               **kwargs):
    super(Decoder, self).__init__(name=name, **kwargs)
    self.dense_proj = layers.Dense(intermediate_dim, activation='relu')
    self.dense_output = layers.Dense(original_dim, activation='sigmoid')

  def call(self, inputs):
    x = self.dense_proj(inputs)
    return self.dense_output(x)


class VariationalAutoEncoder(tf.keras.Model):
  """"""Combines the encoder and decoder into an end-to-end model for training.""""""

  def __init__(self,
               original_dim,
               intermediate_dim=64,
               latent_dim=32,
               name='autoencoder',
               **kwargs):
    super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)
    self.original_dim = original_dim
    self.encoder = Encoder(latent_dim=latent_dim,
                           intermediate_dim=intermediate_dim)
    self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)

  def call(self, inputs):
    z_mean, z_log_var, z = self.encoder(inputs)
    reconstructed = self.decoder(z)
    # Add KL divergence regularization loss.
    kl_loss = - 0.5 * tf.reduce_mean(
        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)
    self.add_loss(kl_loss)
    return reconstructed



vae = VariationalAutoEncoder(784, 64, 1)

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())
vae.fit(x_train, x_train, epochs=3, batch_size=64)
```
### Request visuals, if applicable

n/a

### Submit a pull request?

n/a
"
35483,High percentage of CPU usage in ssdlite_mobilenet model ,"Hi all,
I trained a model for license plate detection with ssdlite_mobilenet_v2_coco (training images:20000,steps:200000) and this model is going to be used in a traffic control app.

The accuracy of this model on frames of video is very good but the percentage of CPU usage on my system is between 70-80% (CPU info : Intel Core i7-6500 @ 2.50GHz.) and given that traffic control app should be constantly running on client system, this percentage is quite high.

So what are the best ways to optimize CPU usage to analyze frames of video and detect license plate? (reduce to ~30-40%)
Do I need to make any changes to the config file or use another pre-trained model?
Please guide me :)"
35482,non_max_suppression is full of bugs!,"**System information**
The bugs are not related to my system informations. They are caused by a bad coding style in the https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/non_max_suppression_op.cu.cc and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/non_max_suppression_op.cc. Just to be sure those bugs could be easily reproduced on google colab. 

**Describe the current behavior**
I ll mention two major bugs.
The first one is a garbage output when the boxes coordinates are not logical. In fact given a box(y1,x1,y2,x2)  coordinates with y1 = y2 or x1 = x2 (the coordinates of a line) or even worst the coordinates of a point ( x1 = x2 = y1 = y2) the algorithme will only output the line when reaching it. 
**Code to reproduce the issue**
Here is the code to reproduce the behaviour on google colab: 

```
import numpy as np
%tensorflow_version 2.x
import tensorflow as tf
tf.__version__
boxes = np.array([[0.1,0.1,0.2,0.2], [0.3,0.3,0.3,0.4], [0.5,0.5,0.6,0.6], [0.7,0.7,0.8,0.8]], dtype= np.float32)
scores = np.array([0.9,0.8,0.7,0.6], dtype = np.float32)
(tf.image.non_max_suppression(boxes, scores, 8))
```

output:
```
'2.1.0-rc1'
<tf.Tensor: shape=(8,), dtype=int32, numpy=array([0, 1, 1, 1, 1, 1, 1, 1], dtype=int32)>
```

Now I ll explain brievely how the algorithm is coded in tensorflow. Given a list of candidate boxes containing in the beginning all the user boxes and a list of chosen boxes empty, if the box is chosen it will not immediately delete it from the candidate box. In fact, this box will be again processed as a candidate box in the next iteration. But because it is already in the chosen boxes it wont be chosen again . The reason for that is that the IOU of a box with himself is 1 which is always above the threshold. Unfortunately, the IOU of a line or a point with any box is 0. This is applied even when the IOU is calculated of the line with iteself. This will result in adding the line to the chosen boxes again and again. This behaviour is mentioned in this issue but wasn't clearly explained. https://github.com/tensorflow/tensorflow/issues/29628

**Describe the expected behavior**
The expected behaviour must be decided by the tensorflow programer. He can chose between putting it only once in the result : 
-<tf.Tensor: shape=(8,), dtype=int32, numpy=array([0, 1, 2, 3], dtype=int32)> 
or deleting the line box
 -<tf.Tensor: shape=(8,), dtype=int32, numpy=array([0, 2, 3], dtype=int32)>

- The second bug is really inexplainable. Why there is only the gpu specialisation of non_max_suppression_v2 ? Did the developer forgot about it? This was mentioned in several issues under the name : non max suppression work only on cpu. This is completely understandable because the default version of non_max_suppression is v3 which dosent have a gpu specialisation. 

**Code to reproduce the issue**
on colab u can just copy this code:

```
import numpy as np
%tensorflow_version 2.x
import tensorflow as tf
tf.debugging.set_log_device_placement(True)
boxes = np.array([[0.1,0.1,0.2,0.2], [0.3,0.3,0.3,0.4], [0.5,0.5,0.6,0.6], [0.7,0.7,0.8,0.8]], dtype= np.float32)
scores = np.array([0.9,0.8,0.7,0.6], dtype = np.float32)
with tf.device('/GPU:0'):
  print(tf.image.non_max_suppression(boxes, scores, 8))
```

output:

> Executing op NonMaxSuppressionV3 in device /job:localhost/replica:0/task:0/device:CPU:0
> tf.Tensor([0 1 1 1 1 1 1 1], shape=(8,), dtype=int32)

**Describe the expected behavior**

> Executing op NonMaxSuppressionV3 in device /job:localhost/replica:0/task:0/device:GPU:0

Correcting this issue is quite simple and straightforward. Gpu specialisation must be made for non_max_suppression_v3 at least."
35477,SequenceEnqueuer doesn't work,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
`SequenceEnqueuer` object takes a `Sequence` object which then can be used with multi-processing for fast data pipeline. But as soon as you start an `enqueuer`, it throws `NotImplemented` error.

**Describe the expected behavior**
It should produce batches of data using multiprocessing.

**Code to reproduce the issue**
```python
!pip install tensorflow

import numpy as np
import tensorflow as tf
from tensorflow.keras.utils import Sequence, to_categorical, SequenceEnqueuer

class DataGenerator(Sequence):
    def __init__(self, batch_size=32):
        self.batch_size = batch_size
        self.indices = np.arange(1024)

    def __len__(self):
        return 1024
    
    def __getitem__(self, idx):
        x = np.random.rand(self.batch_size, 32, 32, 3).astype(np.float32)
        y = np.random.randint(10, size=(self.batch_size))
        return x, y

ds = DataGenerator(batch_size=32)
enqueuer = SequenceEnqueuer(ds, use_multiprocessing=True)
enqueuer.start(workers=2)
```

**Other info / logs**
```
---------------------------------------------------------------------------

NotImplementedError                       Traceback (most recent call last)

<ipython-input-13-fd99ac8a9d30> in <module>()
      1 enqueuer = SequenceEnqueuer(ds, use_multiprocessing=True)
----> 2 enqueuer.start(workers=2)

1 frames

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/data_utils.py in start(self, workers, max_queue_size)
    727     """"""
    728     if self.use_multiprocessing:
--> 729       self.executor_fn = self._get_executor_init(workers)
    730     else:
    731       # We do not need the init since it's threads.

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/data_utils.py in _get_executor_init(self, workers)
    778         Function, a Function to initialize the pool
    779     """"""
--> 780     raise NotImplementedError
    781 
    782   @abstractmethod

NotImplementedError: 
```"
35473,"linux aarch64 nested, exception is java.lang.UnsatisfiedLinkError","Handler dispatch failed; nested exception is java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: linux, architecture: aarch64."
35472,DCGANs Tutorial's BatchNormalization is maybe something wrong,"## URL(s) with the issue:
https://www.tensorflow.org/tutorials/generative/dcgan

## Description of issue (what needs changing):

https://colab.research.google.com/gist/MokkeMeguru/614e16d83d16f1eb70b5f3b73c7d070b/batchnormalization_debug.ipynb

In you tutorial, BatchNormalization will be Actnormalization in Glow(https://arxiv.org/abs/1807.03039)

### Clear description

We need Correct BathNormalization

### Usage example

We should input the shape when   BatchNormalization is initialized"
35471,"Using Conda, The only packageURL can not work","Hi, I am using macOS and Conda to create a virtual environment, but when I install TensorFlow pip package, I found the **only packageURL**(https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-2.0.0-py2-none-any.whl) **for macOS can not work or even download**. Would you mind telling me a solution if anyone solve this problem. Thank you som much!
"
35466,convolutional_recurrent.py:get_initial_state type mismatch,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary (pip, tf-nightly-gpu)
- TensorFlow version (use command below): 2.1.0-dev20191228
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: Tesla M60 7618MiB

Training a model with a ConvLSTM2D layer and float16 (K.set_floatx('float16')) results in a type mismatch. convolutional_recurrent.py:get_initial_state initializes zeros without specifying a type, defaulting to float32.

For me, specifying the dtype as that of the function inputs resolved the issue:
```
    initial_state = self.cell.input_conv(initial_state,
                                         array_ops.zeros(tuple(shape)),
                                         padding=self.cell.padding)
```
becomes
```
    initial_state = self.cell.input_conv(initial_state,
                                         array_ops.zeros(tuple(shape),dtype=inputs.dtype),
                                         padding=self.cell.padding)```
"
35464,"Sample VAE code throws error: ""ValueError: The last dimension of the inputs to Dense should be defined. Found None""","I am very new to TF2 and tried to customize the example code on the tensorflow guide documentation:

https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example

The code given in the guide does not run if the latent dimension is set 1, it runs fine for every latent dimension >1!

For training I tried to use the code given in the example but set the latent dim to 1:
```
vae = VariationalAutoEncoder(784, 64, 1)

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())
vae.fit(x_train, x_train, epochs=3, batch_size=64)
```

The error when trying to train is:

**ValueError: The last dimension of the inputs to Dense should be defined. Found None** and is thrown upon return from the Sample function where I think:
```
epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
```
can not handle shape=(?,1).

Can someone help I am trying to use the code as a template but I need latent dimension to be 1!

Thanks"
35463,tf.profiler calculating incorrect flops in mobilenetssd_v2,"Below Script is giving correct result for mobilenetssd_v1 Flops but failing in calculating mobilentssd_V2 flops. Mobilenetssd_v2 flops coming out to be 185 Millions only
```
sess = tf.Session()
import numpy as np
import os
import time
import cv2
graph = tf.get_default_graph()
image_np=cv2.imread(""41566-large.jpg"")
image=cv2.resize(image_np,(300, 300), interpolation = cv2.INTER_CUBIC)
image = np.expand_dims(image, axis=0)
#print(image)
for i in range(1):
    tf.reset_default_graph()
    st='/device:cpu:'+str(i)
    #with tf.device(st):
    with graph.as_default():
        with sess.as_default():

            #restoring the model
            run_metadata = tf.RunMetadata()
            saver = tf.train.import_meta_graph('ssd_mobilenet_v2_coco_2018_03_29/model.ckpt.meta')
            saver.restore(sess,tf.train.latest_checkpoint('ssd_mobilenet_v2_coco_2018_03_29'))
            ops = tf.get_default_graph().get_operations()
            all_tensor_names = {output.name for op in ops for output in op.outputs}
            tensor_dict = {}
            for key in ['num_detections', 'detection_boxes', 'detection_scores','detection_classes', 'detection_masks']:
                tensor_name = key + ':0'
                if tensor_name in all_tensor_names:
                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)
            if 'detection_masks' in tensor_dict:
                detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
                detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
                real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)
                detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])
                detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])
                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
                    detection_masks, detection_boxes, image.shape[1], image.shape[2])
                detection_masks_reframed = tf.cast(
                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)
                tensor_dict['detection_masks'] = tf.expand_dims(
                    detection_masks_reframed, 0)
            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')
#                 # Run inference
            opts = tf.profiler.ProfileOptionBuilder.trainable_variables_parameter()
            for i in range(1):
                print(""Inference Flag"")
                start=time.time()
                output_dict = sess.run(tensor_dict,options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),
                                        feed_dict={image_tensor: image},run_metadata=run_metadata)
                end=time.time()
#                 print(run_metadata)
                print(end-start)
            opts = tf.profiler.ProfileOptionBuilder.trainable_variables_parameter()
            flops = tf.profiler.profile(sess.graph,run_meta=run_metadata, options = tf.profiler.ProfileOptionBuilder.float_operation())
            params = tf.profiler.profile(sess.graph, run_meta=run_metadata, cmd='op', options=opts)
            print('FLOP before freezing', flops.total_float_ops)
            print(""parametrs............."",params.total_parameters)
            output_dict['num_detections'] = int(output_dict['num_detections'][0])
            output_dict['detection_classes'] = output_dict[
                'detection_classes'][0].astype(np.int64)
            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]
            output_dict['detection_scores'] = output_dict['detection_scores'][0]
            if 'detection_masks' in output_dict:
                output_dict['detection_masks'] = output_dict['detection_masks'][0]
```
Model is taken from tensorflow API Github Repo http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz this is the link
"
35462,tensorflow installation in raspberry pi : HadoopFileSystem load error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspberry pi 4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary in pip
- TensorFlow version: 1.14
- Python version: 3.7
- Installed using  pip
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: No
- GPU model and memory: No



**Describe the problem**
****
pi@raspberrypi:~/examples/lite/examples/object_detection/raspberry_pi $ python3 detect_picamera.py   --model /tmp/detect.tflite   --labels /tmp/coco_labels.txt
2019-12-28 17:43:12.464130: E tensorflow/core/platform/hadoop/hadoop_file_system.cc:132] HadoopFileSystem load error: libhdfs.so: cannot open shared object file: No such file or directory
Traceback (most recent call last):
  File ""detect_picamera.py"", line 34, in <module>
    from tf.lite.interpreter import Interpreter
ModuleNotFoundError: No module named 'tf'
***

**Provide the exact sequence of commands / steps that you executed before running into the problem**
***
 Tensorflow model running issue replaced the code with in original source code

> from_ tflite_runtime.interpreter import Interpreter

***
**WIth**
***
> from tf.lite import Interpreter
***


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
35460,integrate.odeint in tensorflow 2.0,"Can't find odeint or integrate...
can't import contrib from tf.compat.v1 
where is odeint "
35459,EMNIST Link Broken ,"The link to the [EMNIST page](https://www.tensorflow.org/datasets/catalog/emnist) on Tensorflow's docs is broken.

The existing link is -
https://www.nist.gov/node/1298471/emnist-dataset

It should be replaced by -
http://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip"
35456,tf.print can't print chinese characters,"**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
win10 1909
- TensorFlow installed from (source or binary):
pip install tensorflow
- TensorFlow version (use command below):
2.0.0
- Python version:
3.7.4

**Describe the current behavior**
```
import tensorflow as tf 
tf.print(""中文测试"")
tf.print(""Chinese test"")
print(""中文测试"")
print(""Chinese test"")
```
current output is
涓枃娴嬭瘯
Chinese test
中文测试
Chinese test

expected output is
中文测试
Chinese test
中文测试
Chinese test


"
35454,"HashTableV2, LookupTableFindV2 are not supported","**System information**
- Linux CentOs 7
- TensorFlow installed from pip:
- TensorFlow version 1.15.0:


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, FULLY_CONNECTED, LOGISTIC, MUL, ONE_HOT, PACK, RESHAPE, SHAPE, SPARSE_TO_DENSE, STRIDED_SLICE, SUB, SUM. Here is a list of operators for which you will need custom implementations: HashTableV2, LookupTableFindV2.```


**Any other info / logs**

looks like the above two operators are not supported. We are using Estimator APIs and I think the above ops are used in categorical_column where we need to lookup the feature from a vocabulary list.
"
35453,Metrics with Eager Execution,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): conda package
- TensorFlow version (use command below): TF2.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0/7.6.4
- GPU model and memory: Tesla V100

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

tf.compat.v1.metrics does not support eager execution, i.e. there is no way to log metrics with TF2.0 at this point. Is there an alternate I should be looking at or is the feature underway?"
35452,which is the correct type for debugging.assert_shape?,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/debugging/assert_shapes?version=stable

## Description of issue (what needs changing):

Update Documentation or implementation of `tf.debugging.assert_shapes`.     
`Shapes` in its argument requires LIST OF TUPLE, not DICTIONARIES.

### Clear description

Your Documentation (Python 4's syntax?)
```python
x = tf.random.normal([128, 32, 32, 1])
tf.debugging.assert_shapes(
    [(x: (128, 32, 32, 1))]
)
# => 
#     [(x: (128, 32, 32, 1))]
#       ^
# SyntaxError: invalid syntax
```

With dictionary (Python 3's syntax)
```python
tf.debugging.assert_shapes(
    {x: (128, 32, 32, 1)}
)
# =>
# ... ... Traceback (most recent call last):
#   File ""<stdin>"", line 2, in <module>
#  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py"", line 713, in __hash__
#     raise TypeError(""Tensor is unhashable if Tensor equality is enabled. ""
# TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.
```
With list of tuple (python 3's syntax)(NOT DICTIONARIES)
```python
tf.debugging.assert_shapes([
    (x, (128, 32, 32, 1))
]
)
# => nothing (correct)
```
"
35451,Error while trying to serilize Image Captioning keras model '_UserObject' object is not callable',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
example from https://www.tensorflow.org/tutorials/text/image_captioning

- OS Platform and Distribution:Linux Ubuntu 18
- TensorFlow installed from (source or binary): bin
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0 
- Python version: 3.6.8

**Describe the current behavior**
> tf.saved_model.save(encoder, ""./models/1/encoder"")

model get serialized with errors 

**Describe the expected behavior**
encoder can be loaded without errors

**Code to reproduce the issue**
- train the model from https://www.tensorflow.org/tutorials/text/image_captioning example 

```
tf.saved_model.save(encoder, ""./models/1/encoder"")
encoder = tf.saved_model.load(""./models/1/encoder"")
encoder(img_tensor_val)

```
raises the error > 
'_UserObject' object is not callable


`encoder.fc(img_tensor_val) 
`
ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (1 total):
    * Tensor(""inputs:0"", shape=(1, 120, 64), dtype=float32)
  Keyword arguments: {}

Expected these arguments to match one of the following 0 option(s):

**Other info / logs**
W1227 00:41:32.831883 139767989909312 save_impl.py:77] Skipping full serialization of Keras model <__main__.CNN_Encoder object at 0x7f1daf5354e0>, because its inputs are not defined.
"
35450,All values come out as 0 while converting int32 to float16 using `tf.image.convert_image_dtype` ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Version 1903
- TensorFlow installed from (source or binary): Source (pip install is source I guess?)
- TensorFlow version (use command below): v1.12.1-21327-gd3457b26a0 2.1.0-dev20191226
- Python version: 3.7.3
- GPU model and memory: GeForce 1050Ti / 16GB of RAM

**Describe the current behavior**
When I convert `int32` type of data to `float16`, all values come out as 0.

**Describe the expected behavior**
The values should be like ""1.0"" I believe.

**Code to reproduce the issue**
```
>>> x = tf.constant([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]], dtype=tf.int32)
>>> x
<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=
array([[[ 1,  2,  3],
        [ 4,  5,  6]],

       [[ 7,  8,  9],
        [10, 11, 12]]])>
>>> tf.image.convert_image_dtype(x, dtype=tf.float16, saturate=False) 
<tf.Tensor: shape=(2, 2, 3), dtype=float16, numpy=
array([[[0., 0., 0.],
        [0., 0., 0.]],

       [[0., 0., 0.],
        [0., 0., 0.]]], dtype=float16)>
```
"
35449,Error converting my model trained with TF 2.1rc1 to TFLite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary):  conda
- TensorFlow version (or github SHA if from source): 2.1.0-rc1


**Provide the text output from tflite_convert**

```
(dl4cv) D:\development\tensorflow\ANPR\output\rmsprop>tflite_convert --keras_model_file=clpr-model.h5 --enable_select_tf_ops --output_file=model.tflite
2019-12-27 14:09:27.630637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.summary API due to missing TensorBoard installation.
2019-12-27 14:09:29.765186: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-12-27 14:09:29.788663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5
coreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2019-12-27 14:09:29.797947: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2019-12-27 14:09:29.807131: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2019-12-27 14:09:29.815002: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2019-12-27 14:09:29.823010: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2019-12-27 14:09:29.830529: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2019-12-27 14:09:29.838455: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2019-12-27 14:09:29.850408: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-12-27 14:09:29.855640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2019-12-27 14:09:29.859642: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-12-27 14:09:29.865715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5
coreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2019-12-27 14:09:29.874362: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2019-12-27 14:09:29.878508: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2019-12-27 14:09:29.882420: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2019-12-27 14:09:29.886489: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2019-12-27 14:09:29.890738: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2019-12-27 14:09:29.894738: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2019-12-27 14:09:29.898962: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-12-27 14:09:29.903241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2019-12-27 14:09:30.584878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-27 14:09:30.589782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0
2019-12-27 14:09:30.592385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N
2019-12-27 14:09:30.595864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6271 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.
W1227 14:09:33.236038 28348 hdf5_format.py:177] No training configuration found in save file: the model was *not* compiled. Compile it manually.
2019-12-27 14:09:33.443668: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2019-12-27 14:09:33.450829: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-12-27 14:09:33.467462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5
coreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2019-12-27 14:09:33.478633: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2019-12-27 14:09:33.483411: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2019-12-27 14:09:33.487932: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2019-12-27 14:09:33.492763: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2019-12-27 14:09:33.498057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2019-12-27 14:09:33.502602: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2019-12-27 14:09:33.508437: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-12-27 14:09:33.513114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2019-12-27 14:09:33.516838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-27 14:09:33.521102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0
2019-12-27 14:09:33.524027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N
2019-12-27 14:09:33.529383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6271 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
2019-12-27 14:09:33.597411: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize
2019-12-27 14:09:33.603519: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 380 nodes (0), 484 edges (0), time = 17.869ms.
2019-12-27 14:09:33.610268: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 380 nodes (0), 484 edges (0), time = 4.152ms.
2019-12-27 14:09:33.615867: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_1_backward_gru_1_while_cond_2105
2019-12-27 14:09:33.621215: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2019-12-27 14:09:33.626682: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2019-12-27 14:09:33.631716: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_forward_gru_while_body_1602
2019-12-27 14:09:33.637805: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2019-12-27 14:09:33.643095: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2019-12-27 14:09:33.647759: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_1_forward_gru_1_while_cond_1941
2019-12-27 14:09:33.653473: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2019-12-27 14:09:33.658111: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2019-12-27 14:09:33.663563: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_1_backward_gru_1_while_body_2106
2019-12-27 14:09:33.668843: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2019-12-27 14:09:33.674032: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2019-12-27 14:09:33.679432: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_backward_gru_while_body_1766
2019-12-27 14:09:33.685071: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2019-12-27 14:09:33.690795: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2019-12-27 14:09:33.695366: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_forward_gru_while_cond_1601
2019-12-27 14:09:33.701130: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2019-12-27 14:09:33.705783: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2019-12-27 14:09:33.710792: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_1_forward_gru_1_while_body_1942
2019-12-27 14:09:33.717048: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2019-12-27 14:09:33.721844: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2019-12-27 14:09:33.726940: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_backward_gru_while_cond_1765
2019-12-27 14:09:33.732340: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2019-12-27 14:09:33.737188: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2019-12-27 14:09:34.924139: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2019-12-27 14:09:34.931432: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-12-27 14:09:34.941771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5
coreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2019-12-27 14:09:34.952985: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2019-12-27 14:09:34.957078: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2019-12-27 14:09:34.962751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2019-12-27 14:09:34.967762: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2019-12-27 14:09:34.973494: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2019-12-27 14:09:34.977944: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2019-12-27 14:09:34.983289: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-12-27 14:09:34.987946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2019-12-27 14:09:34.992126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-27 14:09:34.996303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0
2019-12-27 14:09:34.999665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N
2019-12-27 14:09:35.004366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6271 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
2019-12-27 14:09:35.521865: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize
2019-12-27 14:09:35.526582: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 226 nodes (-16), 276 edges (-28), time = 227.782ms.
2019-12-27 14:09:35.532999: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 226 nodes (0), 276 edges (0), time = 111.153ms.
2019-12-27 14:09:35.538097: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_1_backward_gru_1_while_body_2106_frozen
2019-12-27 14:09:35.545175: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 60 nodes (-1), 79 edges (0), time = 2.335ms.
2019-12-27 14:09:35.550989: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 60 nodes (0), 79 edges (0), time = 0.737ms.
2019-12-27 14:09:35.556165: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_1_backward_gru_1_while_cond_2105_frozen
2019-12-27 14:09:35.562428: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.306ms.
2019-12-27 14:09:35.567480: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.176ms.
2019-12-27 14:09:35.573582: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_1_forward_gru_1_while_cond_1941_frozen
2019-12-27 14:09:35.579766: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.262ms.
2019-12-27 14:09:35.584544: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.175ms.
2019-12-27 14:09:35.590845: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_1_forward_gru_1_while_body_1942_frozen
2019-12-27 14:09:35.597525: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 60 nodes (-1), 79 edges (0), time = 1.396ms.
2019-12-27 14:09:35.602989: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 60 nodes (0), 79 edges (0), time = 0.698ms.
2019-12-27 14:09:35.609327: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_backward_gru_while_cond_1765_frozen
2019-12-27 14:09:35.614684: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.274ms.
2019-12-27 14:09:35.621170: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.216ms.
2019-12-27 14:09:35.626793: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_backward_gru_while_body_1766_frozen
2019-12-27 14:09:35.632917: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 60 nodes (-1), 79 edges (0), time = 1.417ms.
2019-12-27 14:09:35.639119: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 60 nodes (0), 79 edges (0), time = 0.711ms.
2019-12-27 14:09:35.645066: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_forward_gru_while_body_1602_frozen
2019-12-27 14:09:35.652559: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 60 nodes (-1), 79 edges (0), time = 1.349ms.
2019-12-27 14:09:35.658901: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 60 nodes (0), 79 edges (0), time = 0.749ms.
2019-12-27 14:09:35.665589: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_forward_gru_while_cond_1601_frozen
2019-12-27 14:09:35.673058: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.295ms.
2019-12-27 14:09:35.680300: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.138ms.
Traceback (most recent call last):
  File ""c:\users\andreas\anaconda3\envs\dl4cv\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\andreas\anaconda3\envs\dl4cv\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Andreas\Anaconda3\envs\dl4cv\Scripts\tflite_convert.exe\__main__.py"", line 7, in <module>
  File ""c:\users\andreas\anaconda3\envs\dl4cv\lib\site-packages\tensorflow_core\lite\python\tflite_convert.py"", line 594, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""c:\users\andreas\anaconda3\envs\dl4cv\lib\site-packages\tensorflow_core\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""c:\users\andreas\anaconda3\envs\dl4cv\lib\site-packages\absl\app.py"", line 299, in run
    _run_main(main, args)
  File ""c:\users\andreas\anaconda3\envs\dl4cv\lib\site-packages\absl\app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""c:\users\andreas\anaconda3\envs\dl4cv\lib\site-packages\tensorflow_core\lite\python\tflite_convert.py"", line 577, in run_main
    _convert_tf2_model(tflite_flags)
  File ""c:\users\andreas\anaconda3\envs\dl4cv\lib\site-packages\tensorflow_core\lite\python\tflite_convert.py"", line 235, in _convert_tf2_model
    tflite_model = converter.convert()
  File ""c:\users\andreas\anaconda3\envs\dl4cv\lib\site-packages\tensorflow_core\lite\python\lite.py"", line 464, in convert
    **converter_kwargs)
  File ""c:\users\andreas\anaconda3\envs\dl4cv\lib\site-packages\tensorflow_core\lite\python\convert.py"", line 457, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""c:\users\andreas\anaconda3\envs\dl4cv\lib\site-packages\tensorflow_core\lite\python\convert.py"", line 203, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2019-12-27 14:09:36.318110: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.summary API due to missing TensorBoard installation.
2019-12-27 14:09:39.097723: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-12-27 14:09:39.107524: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-12-27 14:09:39.163607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5
coreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2019-12-27 14:09:39.164452: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2019-12-27 14:09:39.206090: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2019-12-27 14:09:39.249003: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2019-12-27 14:09:39.256358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2019-12-27 14:09:39.307626: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2019-12-27 14:09:39.326748: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2019-12-27 14:09:39.414022: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-12-27 14:09:39.414730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2019-12-27 14:09:42.459316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-27 14:09:42.459514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0
2019-12-27 14:09:42.459628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N
2019-12-27 14:09:42.460800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2903 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
2019-12-27 14:09:42.536530: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor
2019-12-27 14:09:42.536727: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-12-27 14:09:42.536928: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor
2019-12-27 14:09:42.537124: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-12-27 14:09:42.537289: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve
2019-12-27 14:09:42.537455: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-12-27 14:09:42.537616: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve
2019-12-27 14:09:42.537804: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-12-27 14:09:42.537974: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While
2019-12-27 14:09:42.538136: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-12-27 14:09:42.538282: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-12-27 14:09:42.538438: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While
2019-12-27 14:09:42.538586: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-12-27 14:09:42.538734: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-12-27 14:09:42.538884: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack
2019-12-27 14:09:42.539041: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack
2019-12-27 14:09:42.539238: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor
2019-12-27 14:09:42.539398: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-12-27 14:09:42.539579: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor
2019-12-27 14:09:42.539737: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-12-27 14:09:42.539879: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve
2019-12-27 14:09:42.540034: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-12-27 14:09:42.540195: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve
2019-12-27 14:09:42.540357: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-12-27 14:09:42.540515: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While
2019-12-27 14:09:42.540692: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-12-27 14:09:42.540861: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-12-27 14:09:42.541036: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While
2019-12-27 14:09:42.541193: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-12-27 14:09:42.541338: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-12-27 14:09:42.541488: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack
2019-12-27 14:09:42.541642: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack
2019-12-27 14:09:42.550428: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 98 operators, 264 arrays (0 quantized)
2019-12-27 14:09:42.555345: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 98 operators, 264 arrays (0 quantized)
2019-12-27 14:09:42.562862: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 62 operators, 194 arrays (0 quantized)
2019-12-27 14:09:42.564023: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 60 operators, 192 arrays (0 quantized)
2019-12-27 14:09:42.565092: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 58 operators, 188 arrays (0 quantized)
2019-12-27 14:09:42.566092: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 58 operators, 188 arrays (0 quantized)
2019-12-27 14:09:42.566900: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 58 operators, 188 arrays (0 quantized)
2019-12-27 14:09:42.567646: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Identify nearest upsample.: 58 operators, 188 arrays (0 quantized)
2019-12-27 14:09:42.569453: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 655360 bytes, theoretical optimal value: 655360 bytes.
2019-12-27 14:09:42.569852: I tensorflow/lite/toco/toco_tooling.cc:471] Number of parameters: 4121949
2019-12-27 14:09:42.575404: E tensorflow/lite/toco/toco_tooling.cc:498] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, FILL, FULLY_CONNECTED, MAX_POOL_2D, PACK, RESHAPE, REVERSE_V2, SHAPE, SOFTMAX, STRIDED_SLICE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.
Traceback (most recent call last):
  File ""c:\users\andreas\anaconda3\envs\dl4cv\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\andreas\anaconda3\envs\dl4cv\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Andreas\Anaconda3\envs\dl4cv\Scripts\toco_from_protos.exe\__main__.py"", line 7, in <module>
  File ""c:\users\andreas\anaconda3\envs\dl4cv\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 93, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""c:\users\andreas\anaconda3\envs\dl4cv\lib\site-packages\tensorflow_core\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""c:\users\andreas\anaconda3\envs\dl4cv\lib\site-packages\absl\app.py"", line 299, in run
    _run_main(main, args)
  File ""c:\users\andreas\anaconda3\envs\dl4cv\lib\site-packages\absl\app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""c:\users\andreas\anaconda3\envs\dl4cv\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 56, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, FILL, FULLY_CONNECTED, MAX_POOL_2D, PACK, RESHAPE, REVERSE_V2, SHAPE, SOFTMAX, STRIDED_SLICE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.

```

Also, please include a link to a GraphDef or the model if possible.

[download model](https://drive.google.com/file/d/18kaawo6lfOd98lH76U47H0jkYap3WjWU/view?usp=sharing)

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[GitHub project](https://github.com/aboerzel/ALPR-keras)

Model architecture:
```
class OCR:
    @staticmethod
    def build(input_size, pool_size, output_size):
        conv_filters = 16
        kernel_size = (3, 3)
        time_dense_size = 32
        rnn_size = 512

        input_data = Input(name=""input"", shape=input_size)

        cnn = Conv2D(conv_filters, kernel_size, padding='same', kernel_initializer='he_normal')(input_data)
        cnn = BatchNormalization()(cnn)
        cnn = Activation('relu')(cnn)
        cnn = MaxPooling2D(pool_size=(pool_size, pool_size))(cnn)

        cnn = Conv2D(conv_filters, kernel_size, padding='same', kernel_initializer='he_normal')(cnn)
        cnn = BatchNormalization()(cnn)
        cnn = Activation('relu')(cnn)
        cnn = MaxPooling2D(pool_size=(pool_size, pool_size))(cnn)

        # CNN to RNN
        shape = cnn.get_shape()
        bgru = Reshape((shape[1], shape[2] * shape[3]))(cnn)

        bgru = Bidirectional(GRU(units=rnn_size, return_sequences=True, dropout=0.5))(bgru)
        bgru = TimeDistributed(Dense(units=time_dense_size))(bgru)

        bgru = Bidirectional(GRU(units=rnn_size, return_sequences=True, dropout=0.5))(bgru)
        output_data = TimeDistributed(Dense(units=output_size, activation=""softmax""))(bgru)

        return input_data, output_data
```


"
35448,tf.keras.layers.Reshape not working as expected,"**System information**
- Ubuntu 18.043 LTS
- TensorFlow installed using pip
- TensorFlow version '2.0.0' (CPU version)
- Python version 3.6.9

**Describe the current behavior**

The sample code show below give following (partial) output and exception:

```
(400, 100)
(2, 200, 100)
Lambda reshape worked!
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-54-e2fee5681321> in <module>
     12 print(out_1.shape)
     13 print('Lambda reshape worked!')
---> 14 reshape_layer(inp)

~/code/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    889           with base_layer_utils.autocast_context_manager(
    890               self._compute_dtype):
--> 891             outputs = self.call(cast_inputs, *args, **kwargs)
    892           self._handle_activity_regularization(inputs, outputs)
    893           self._set_mask_metadata(inputs, outputs, input_masks)

~/code/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/core.py in call(self, inputs)
    469   def call(self, inputs):
    470     return array_ops.reshape(inputs,
--> 471                              (array_ops.shape(inputs)[0],) + self.target_shape)
    472 
    473   def get_config(self):

~/code/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py in reshape(tensor, shape, name)
    129     A `Tensor`. Has the same type as `tensor`.
    130   """"""
--> 131   result = gen_array_ops.reshape(tensor, shape, name)
    132   tensor_util.maybe_set_static_shape(result, shape)
    133   return result

~/code/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py in reshape(tensor, shape, name)
   8104       try:
   8105         return reshape_eager_fallback(
-> 8106             tensor, shape, name=name, ctx=_ctx)
   8107       except _core._SymbolicException:
   8108         pass  # Add nodes to the TensorFlow graph.

~/code/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py in reshape_eager_fallback(tensor, shape, name, ctx)
   8142   _attrs =
 (""T"", _attr_T, ""Tshape"", _attr_Tshape)
   8143   _result = _execute.execute(b""Reshape"", 1, inputs=_inputs_flat, attrs=_attrs,
-> 8144                              ctx=_ctx, name=name)
   8145   _execute.record_gradient(
   8146       ""Reshape"", _inputs_flat, _attrs, _result, name)

~/code/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

~/code/venv/lib/python3.6/site-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: Input to reshape is a tensor with 40000 values, but the requested shape has 8000000 [Op:Reshape]
```

**Describe the expected behavior**

Defined Reshape layer should reshape the input of shape (400, 100) to a tensor of shape (2, 200, 100)
Wrapping tf.reshape in a Lambda layer is working as an alternative solution

**Code to reproduce the issue**
```python

from tensorflow.keras.layers import Reshape, Lambda
import tensorflow as tf

max_len = 200
char_hidden_dim = 50
reshape_layer = Reshape((max_len, 2 * char_hidden_dim,))
lambda_reshape_layer = Lambda(lambda x: tf.reshape(x, shape=[-1, max_len, 2 * char_hidden_dim]))

n_samples = 400
dim = 100
inp = np.zeros((n_samples, dim), dtype=np.float32)
print(inp.shape)

out_1 = lambda_reshape_layer(inp)
print(out_1.shape)
print('Lambda reshape worked!')
reshape_layer(inp)
```

Do I have a flawed understanding of the Reshape layer or can someone confirm that it is indeed a bug? 
Please help :)"
35447,Chain multiple estimators to create a single SavedModel ,"Chain multiple estimators to create a single SavedModel with a single serving file/output.

Suppose I have 3 estimators with me, first is BoostedTrees, who's output I want to use as input into DNNClassifier, and the output of which I want to use in my custom Estimator.

Is there a way to chain output/input of each other to create a **mega** estimator of sorts.

Please help me out here."
35446,Failed to load model with scalar inputs,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
> No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
> Both Win10 and Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
> from binary using pip
- TensorFlow version (use command below):
> 2.0.0
- Python version:
> 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
> cudnn-10.2-windows10-x64-v7.6.5.32
- GPU model and memory:
> 24G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**


I defined a model with two inputs: one of shape `(seq_len, fea_size)`, and the other of shape `()`.
It is fine while training and saving the model, but it gets failed when loading the model from the export directory, with error show like this:

```
Traceback (most recent call last):
  File ""D:/Work/Python/track_by_classification/src/test/test_model.py"", line 101, in <module>
    test_model_with_scalar_input()
  File ""D:/Work/Python/track_by_classification/src/test/test_model.py"", line 90, in test_model_with_scalar_input
    new_model = tf.keras.models.load_model(model_home)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\keras\saving\save.py"", line 150, in load_model
    return saved_model_load.load(filepath, compile)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\keras\saving\saved_model\load.py"", line 86, in load
    model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\saved_model\load.py"", line 541, in load_internal
    export_dir)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\keras\saving\saved_model\load.py"", line 103, in __init__
    self._finalize()
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\keras\saving\saved_model\load.py"", line 132, in _finalize
    node._set_inputs(inputs)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2709, in _set_inputs
    outputs = self(inputs, **kwargs)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 842, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\keras\saving\saved_model\utils.py"", line 57, in return_outputs_and_add_losses
    outputs, losses = fn(inputs, *args, **kwargs)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\keras\saving\saved_model\utils.py"", line 111, in wrap_with_training_arg
    lambda: replace_training_and_call(False))
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\keras\utils\tf_utils.py"", line 59, in smart_cond
    pred, true_fn=true_fn, false_fn=false_fn, name=name)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\framework\smart_cond.py"", line 59, in smart_cond
    name=name)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\ops\control_flow_ops.py"", line 1174, in cond
    return cond_v2.cond_v2(pred, true_fn, false_fn, name)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\ops\cond_v2.py"", line 84, in cond_v2
    op_return_value=pred)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\framework\func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\keras\saving\saved_model\utils.py"", line 110, in <lambda>
    lambda: replace_training_and_call(True),
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\keras\saving\saved_model\utils.py"", line 106, in replace_training_and_call
    return wrapped_call(*args, **kwargs)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 503, in _call
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 408, in _initialize
    *args, **kwds))
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1848, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2150, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2041, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\framework\func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 358, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""D:\WorkPrograms\Anaconda3\envs\tf2.0\lib\site-packages\tensorflow_core\python\saved_model\function_deserialization.py"", line 262, in restored_function_body
    ""\n\n"".join(signature_descriptions)))
ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (3 total):
    * [<tf.Tensor 'inputs:0' shape=(None, 10, 128) dtype=float32>, <tf.Tensor 'inputs_1:0' shape=(None, 1) dtype=float32>]
    * True
    * None
  Keyword arguments: {}

Expected these arguments to match one of the following 4 option(s):

Option 1:
  Positional arguments (3 total):
    * [TensorSpec(shape=(None, 10, 128), dtype=tf.float32, name='inputs/0'), TensorSpec(shape=(None,), dtype=tf.float32, name='inputs/1')]
    * False
    * None
  Keyword arguments: {}

Option 2:
  Positional arguments (3 total):
    * [TensorSpec(shape=(None, 10, 128), dtype=tf.float32, name='input_1'), TensorSpec(shape=(None,), dtype=tf.float32, name='input_2')]
    * False
    * None
  Keyword arguments: {}

Option 3:
  Positional arguments (3 total):
    * [TensorSpec(shape=(None, 10, 128), dtype=tf.float32, name='inputs/0'), TensorSpec(shape=(None,), dtype=tf.float32, name='inputs/1')]
    * True
    * None
  Keyword arguments: {}

Option 4:
  Positional arguments (3 total):
    * [TensorSpec(shape=(None, 10, 128), dtype=tf.float32, name='input_1'), TensorSpec(shape=(None,), dtype=tf.float32, name='input_2')]
    * True
    * None
  Keyword arguments: {}

```

And as I have hacked the tensorflow source code, I found the bug comes from here [training_utils.ModelInputs.get_symbolic_inputs](https://github.com/tensorflow/tensorflow/blob/03a2b3f1fb35373c415444f974e4d627cadb1d33/tensorflow/python/keras/engine/training_utils.py#L1852)

```python
  def get_symbolic_inputs(self, return_single_as_list=False):
    """"""Returns inputs to be set as self.inputs for a model.""""""
    # TODO(karmel): There is a side-effect here where what you get
    # with as_list and as_dict depends on whether you have called this
    # method first, since it modifies in place.
    for i, (k, v) in enumerate(zip(self._input_names, self._flattened_inputs)):
      if isinstance(v, (list, float, int)):
        v = np.asarray(v)
        if v.ndim == 1:
          v = np.expand_dims(v, 1)

      if isinstance(v, (np.ndarray, ops.EagerTensor)):
        # We fix the placeholder shape except the batch size.
        # This is suboptimal, but it is the best we can do with the info
        # we have. The user should call `model._set_inputs(placeholders)`
        # to specify custom placeholders if the need arises.
        shape = (None,) + tuple(v.shape[1:])
        if shape == (None,):
          shape = (None, 1)
        dtype = dtypes.as_dtype(v.dtype)
        if dtype.is_floating:
          dtype = K.floatx()
        v = K.placeholder(shape=shape, name=k, dtype=dtype)
      elif isinstance(v, tensor_spec.TensorSpec):
        shape = (None,) + tuple(v.shape.as_list()[1:])
        if shape == (None,):
        >>>>>  shape = (None, 1) <<<< here is where the errors comes from
        v = K.placeholder(shape=shape, name=k, dtype=v.dtype)

      self._flattened_inputs[i] = v

    if self._is_dict:
      return dict(zip(self._input_names, self._flattened_inputs))
    if self._is_single_input and not return_single_as_list:
      return self._flattened_inputs[0]
    return self._flattened_inputs
```

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
def test_model_with_scalar_input():
    model_home = ""models""
    input_shape = 5, 10, 128
    n_class = 5

    batch_size, seq_len, fea_size = input_shape

    input_fea = tf.keras.layers.Input(shape=input_shape[1:])
    input_seq = tf.keras.layers.Input(shape=())

    name = ""test-model""
    forward_layer = tf.keras.layers.LSTM(units=fea_size, return_sequences=True, name=name + ""-forward"")

    backward_layer = tf.keras.layers.LSTM(units=fea_size, return_sequences=True, go_backwards=True,
                                          name=name + ""-backward"")

    bi_lstm = tf.keras.layers.Bidirectional(forward_layer, backward_layer=backward_layer, name=name + ""-bi-lstm"")

    fc = tf.keras.layers.Dense(units=n_class, activation=""softmax"", name=""fc-pred"")

    mask = tf.sequence_mask(input_seq, seq_len)

    y = bi_lstm(input_fea, mask=mask)
    y = fc(y)

    model = tf.keras.models.Model(inputs=[input_fea, input_seq], outputs=y, name=name)

    features = np.random.normal(size=[batch_size * 100, seq_len, fea_size])
    labels = np.random.randint(0, n_class, size=[batch_size * 100, seq_len])
    # seq = np.random.randint(0, seq_len, size=[batch_size * 100])
    seq = [seq_len] * (batch_size * 100)

    dataset = tf.data.Dataset.from_tensor_slices(((features, seq), labels))
    dataset = dataset.batch(batch_size)

    model.compile(optimizer=""adam"", loss=""sparse_categorical_crossentropy"")

    saver = tf.keras.callbacks.ModelCheckpoint(model_home, monitor=""loss"", verbose=1, save_best_only=True)
    model.fit(dataset, epochs=5, callbacks=[saver], steps_per_epoch=10)

    new_model = tf.keras.models.load_model(model_home)

    # print(new_model.layers)

    pred = new_model.predict(dataset)
    pred = np.argmax(pred, axis=-1)
    print(pred)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[The code showed above performs as expected on 2.1.0-rc1](https://colab.research.google.com/drive/1gQqC-vZPFSKwIof4LapCocs98PezC8GL#scrollTo=a2b8z-1QZRX0)"
35445,TfLiteGpuDelegate Init: FULLY_CONNECTED: Amount of input data should match weights width,"**System information**
- Have I written custom code :
```java
private List<Delegate> mDelegates = new ArrayList<>();
mDelegates.add(new GpuDelegate());
```

```java
            Interpreter.Options options = null;
            if (interpreterOptions != null) {
                options = new Interpreter.Options().setNumThreads(interpreterOptions.getNumThreads());
            }
            if (!mDelegates.isEmpty()) {
                Iterator<Delegate> it = mDelegates.iterator();
                while (it.hasNext()) {
                    Delegate delegate = it.next();
                    mLogger.debug(""addDelegate: "" + delegate);
                    options.addDelegate(delegate);
                }
            }
            mInterpreter = new Interpreter(bb, options);
```

- Mobile device :
MI 8 / MIUI 11.0.4

- TensorFlow installed from (source or binary):
implementation 'org.tensorflow:tensorflow-lite:2.0.0'
implementation 'org.tensorflow:tensorflow-lite-gpu:2.0.0'

**Describe the current behavior**
crashed when load the attached file with 'GpuDelegate'

**Describe the expected behavior**
load the attached file correct

**Code to reproduce the issue**
N/A

**Other info / logs**
```text
    java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: TfLiteGpuDelegate Init: FULLY_CONNECTED: Amount of input data should match weights width
    TfLiteGpuDelegate Prepare: delegate is not initialized
    Node number 3 (TfLiteGpuDelegateV2) failed to prepare.
    
    Restored previous execution plan after delegate application failure.
        at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:162)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)
        at com.didi.aoe.runtime.tensorflow.lite.TensorFlowMultipleInputsOutputsInterpreter.run(TensorFlowMultipleInputsOutputsInterpreter.java:159)
        at com.didi.aoe.library.core.NativeProcessorWrapper.run(NativeProcessorWrapper.java:40)
```

[mnist_cnn_keras.tflite.zip](https://github.com/tensorflow/tensorflow/files/4004963/mnist_cnn_keras.tflite.zip)
"
35444,Bug with Adam keras optimizer,"I wanted to report a potential bug of Adam at line: https://github.com/tensorflow/tensorflow/blob/9bfa68666e16d6327cb5cd5e04d25f94248629b2/tensorflow/python/keras/optimizer_v2/adam.py#L164
```
def _prepare_local(self, var_device, var_dtype, apply_state):
    local_step = math_ops.cast(self.iterations + 1, var_dtype)
```

In the beginning, self.iterations would return 1 instead of 0. With this, local_step has a value of 2, and not 1. From [Adam](https://arxiv.org/pdf/1412.6980.pdf) paper, I think time step t (which is local_step in our case) should start with 1, and definitly not 2.

Please correct me if I was wrong."
35443,"ValueError: Column dtype and SparseTensors dtype must be compatible. key: adt_sev_flag, column dtype: <dtype: 'int64'>, tensor dtype: <dtype: 'float32'>","I use TF1.14 ane keras 2.2.4 , code and error is :

from future import absolute_import, division, print_function

import numpy as np
import pandas as pd
from tensorflow import feature_column
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

URL = 'https://storage.googleapis.com/applied-dl/heart.csv'
dataframe = pd.read_csv(URL)
dataframe.head()

train, test = train_test_split(dataframe, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
print(len(train), 'train examples')
print(len(val), 'validation examples')
print(len(test), 'test examples')

def df_to_dataset(dataframe, shuffle=True, batch_size=32):
dataframe = dataframe.copy()
labels = dataframe.pop('target')
ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
if shuffle:
ds = ds.shuffle(buffer_size=len(dataframe))
ds = ds.batch(batch_size)
return ds

batch_size = 5 # A small batch sized is used for demonstration purposes
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

age = feature_column.numeric_column(""age"")
feature_columns = []
feature_layer_inputs = {}

for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:
feature_columns.append(feature_column.numeric_column(header))
feature_layer_inputs[header] = tf.keras.Input(shape=(1,), name=header)

age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])
feature_columns.append(age_buckets)

thal = feature_column.categorical_column_with_vocabulary_list(
'thal', ['fixed', 'normal', 'reversible'])
thal_one_hot = feature_column.indicator_column(thal)
feature_columns.append(thal_one_hot)
feature_layer_inputs['thal'] = tf.keras.Input(shape=(1,), name='thal', dtype=tf.string)

batch_size = 32
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
batch_size = 100 # 小批量大小用于演示
epoch = 10
steps_per_epoch =  len(train)//(batch_size*epoch)
val_steps = len(val)//batch_size
model.fit(train_ds, epochs = epoch, steps_per_epoch=steps_per_epoch, validation_data=val_ds,validation_steps=10)

output the error:

----> 2 model.fit(train_ds, epochs = epoch, steps_per_epoch=steps_per_epoch, validation_data=val_ds,validation_steps=10)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    707         steps=steps_per_epoch,
    708         validation_split=validation_split,
--> 709         shuffle=shuffle)
    710 
    711     # Prepare validation data.

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)
   2556       else:
   2557         cast_inputs = x_input
-> 2558       self._set_inputs(cast_inputs)
   2559     else:
   2560       y_input = y

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _set_inputs(self, inputs, outputs, training)
   2774       kwargs = {'training': training} if self._expects_training_arg else {}
   2775       try:
-> 2776         outputs = self(inputs, **kwargs)
   2777       except NotImplementedError:
   2778         # This Model or a submodel is dynamic and hasn't overridden

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    632                     outputs = base_layer_utils.mark_as_return(outputs, acd)
    633                 else:
--> 634                   outputs = call_fn(inputs, *args, **kwargs)
    635 
    636             except TypeError as e:

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py in call(self, inputs, training, mask)
    259         kwargs['training'] = training
    260 
--> 261       outputs = layer(inputs, **kwargs)
    262 
    263       # `outputs` will be the inputs to the next layer.

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    632                     outputs = base_layer_utils.mark_as_return(outputs, acd)
    633                 else:
--> 634                   outputs = call_fn(inputs, *args, **kwargs)
    635 
    636             except TypeError as e:

/opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    147       except Exception as e:  # pylint:disable=broad-except
    148         if hasattr(e, 'ag_error_metadata'):
--> 149           raise e.ag_error_metadata.to_exception(type(e))
    150         else:
    151           raise

ValueError: in converted code:
    relative to /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column:

    feature_column_v2.py:473 call *
        tensor = column.get_dense_tensor(transformation_cache,
    feature_column_v2.py:4291 get_dense_tensor
        return transformation_cache.get(self, state_manager)
    feature_column_v2.py:2562 get
        transformed = column.transform_feature(self, state_manager)
    feature_column_v2.py:4230 transform_feature
        transformation_cache, state_manager)
    feature_column_v2.py:3710 get_sparse_tensors
        transformation_cache.get(self, state_manager), None)
    feature_column_v2.py:2562 get
        transformed = column.transform_feature(self, state_manager)
    feature_column_v2.py:3688 transform_feature
        return self._transform_input_tensor(input_tensor)
    feature_column_v2.py:3664 _transform_input_tensor
        self.key, self.dtype, input_tensor.dtype))

    ValueError: Column dtype and SparseTensors dtype must be compatible. key: adt_sev_flag, column dtype: <dtype: 'int64'>, tensor dtype: <dtype: 'float32'>


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem."
35442,"[TF 2.0]ParameterServerStrategy and CentralStorageStrategy doesn't work with Keras when using GPU, even though it works well with CPU.","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I used code in [MultiWorkerMirroredStrategy tutorial](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras). And i only changed MultiWorkerMirroredStrategy to ParameterServerStrategy and turned off the eager mode.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS
- TensorFlow installed from (source or binary): pip3
- TensorFlow version (use command below): tensorflow 2.0.0 / tensorflow-gpu 2.0.0
- Python version: python 3.6.8
- CUDA/cuDNN version: CUDA 10.0
- GPU model and memory: TITAN Xp

**Code to reproduce the issue**
```
from __future__ import absolute_import, division, print_function, unicode_literals
import tensorflow_datasets as tfds
import tensorflow as tf

tf.compat.v1.disable_eager_execution()
strategy = tf.distribute.experimental.ParameterServerStrategy()

BUFFER_SIZE = 10000
BATCH_SIZE = 64
NUM_WORKERS = 2

GLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS

def scale(image, label):
  image = tf.cast(image, tf.float32)
  image /= 255
  return image, label

datasets, info = tfds.load(name='mnist',
                           with_info=True,
                           as_supervised=True)

train_datasets_unbatched = datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)
train_datasets = train_datasets_unbatched.batch(GLOBAL_BATCH_SIZE).repeat()

def build_and_compile_cnn_model():
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')
  ])
  model.compile(
      loss=tf.keras.losses.sparse_categorical_crossentropy,
      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
      metrics=['accuracy'])
  return model

with strategy.scope():
  multi_worker_model = build_and_compile_cnn_model()
multi_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=938)
```
**Describe the current behavior**
Above code works well with CPU. But when using GPU it produces errors like below. 
My TF_CONFIG variable was like this ( TF_CONFIG='{""cluster"": {""worker"": [""localhost:7779""], ""ps"": [""localhost:7777""]}, ""task"": {""index"": 0, ""type"": ""ps""}}' ).
And it also produces same errors when I tried to apply CentralStorageStrategy.
```
Traceback (most recent call last):
  File ""temp.py"", line 48, in <module>
    multi_worker_model = build_and_compile_cnn_model()
  File ""temp.py"", line 35, in build_and_compile_cnn_model
    tf.keras.layers.Dense(10, activation='softmax')
  File ""/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/sequential.py"", line 114, in __init__
    self.add(layer)
  File ""/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/sequential.py"", line 178, in add
    layer(x)
  File ""/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 817, in __call__
    self._maybe_build(inputs)
  File ""/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 2141, in _maybe_build
    self.build(input_shapes)
  File ""/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/convolutional.py"", line 165, in build
    dtype=self.dtype)
  File ""/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 2311, in __setattr__
    if val.trainable:
  File ""/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py"", line 477, in trainable
    raise NotImplementedError
NotImplementedError
```

**Describe the expected behavior**
I should work well like when using CPU.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35441,kears.layers.concatenate Does Not Work when Saving a Model,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): A custom model
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): 2.0.0
- Python version:  3.75
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0.1
- GPU model and memory: Quadro K620M



**Describe the current behavior**

I built a custom model as follows

```
	class C3BR(tf.keras.Model):
		''' 3D Convolution + Batch Normalisation + Relu '''
		def __init__(self, filterNum, kSize, strSize, padMode):
			super(C3BR, self).__init__()
			self.conv = layers.Conv3D(filters=filterNum, kernel_size=kSize, strides=strSize, padding=padMode, data_format='channels_first')
			self.BN = layers.BatchNormalization(axis=1)
		
		def call(self, inputs, ifTrain=True):
			x = self.conv(inputs)
			if ifTrain == True:
				x= self.BN(x)
			return activations.relu(x)

		def build_model(self, input_shape):
			''' A work-around to define dimensions of signals through the NN'''
			self.build(input_shape)
			inputs = tf.keras.Input(shape=input_shape[1:])
			_ = self.call(inputs) 

	class SimpleUNet1(tf.keras.Model):
		""""""
		Serialise basic units so as to build up a double-layered encoder-decoder U-Net
		Input:
			inDim: (for initialisation) [modaility/channel, tensor dimensions]
			classNum: background included
			name: name for the net
			inputs: 5D tf tensor of [mbSize, modaility/channel, tensor dimensions]. Inputs must be organised into channel first order
			input_shape: a 1X5 tuple [mbSize, modaility/channel, tensor dimensions]
			ifTrain: True for training, and False for validation and testing
		Returns:
			outputs: 5D tf tensor of [mbSize, classNum, tensor dimensions]
		""""""
		def __init__(self, inDim, classNum, name='SimpleUNet', **kwarg):
			super(SimpleUNet1, self).__init__(name=name, **kwarg)
			self.inDim = inDim
			self.classNum = classNum
			dimEnSt1End = np.array(inDim)[2:]-2-2
			dimEnSt2Ed = dimEnSt1End/2-2-2
			dimBridgeEnd = (dimEnSt2Ed/2-2-2)*2
			dimDEStd1End = (dimBridgeEnd-2-2)*2
			self.outDim = dimDEStd1End-2-2-2
			temp = ((dimEnSt2Ed - dimBridgeEnd)/2).astype('int32')
			crop3d1 = tuple(np.tile(temp, (2, 1)).T)
			temp = ((dimEnSt1End - dimDEStd1End)/2).astype('int32')
			crop3d2 = tuple(np.tile(temp, (2, 1)).T)

			self.en_st1_cbr1 = C3BR(32, 3, 1, 'valid')
			self.en_st1_cbr2 = C3BR(64, 3, 1, 'valid')
			self.en_st2_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')
			self.en_st2_cbr1 = C3BR(64, 3, 1, 'valid')
			self.en_st2_cbr2 = C3BR(128, 3, 1, 'valid')
			self.bridge_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')
			self.bridge_cbr1 = C3BR(128, 3, 1, 'valid')
			self.bridge_cbr2 = C3BR(256, 3, 1, 'valid')    
			self.bridge_tconv1 = layers.Conv3DTranspose(256, 2, strides=2, padding='valid', data_format='channels_first')
			self.de_3dcrop1 = layers.Cropping3D(crop3d1, data_format='channels_first')
			self.de_st1_concat = layers.Concatenate(axis=1)
			self.de_st1_cbr1 = C3BR(256, 3, 1, 'valid')
			self.de_st1_cbr2 = C3BR(128, 3, 1, 'valid')    
			self.de_st1_tconv1 = layers.Conv3DTranspose(128, 2, strides=2, padding='valid', data_format='channels_first')
			self.de_3dcrop2 = layers.Cropping3D(crop3d2, data_format='channels_first')
			self.de_st2_concat = layers.Concatenate(axis=1)
			self.de_st2_cbr1 = C3BR(64, 3, 1, 'valid')
			self.de_st2_cbr2 = C3BR(64, 3, 1, 'valid') 
			self.final_conv3D = layers.Conv3D(filters=self.classNum, kernel_size=3, strides=1, padding='valid', data_format='channels_first')
					
		#@tf.function
		def call(self, inputs, ifTrain=True):
			x0 = self.en_st1_cbr1(inputs, ifTrain)
			xEnSt1End = self.en_st1_cbr2(x0, ifTrain)
			x1 = self.en_st2_mp(xEnSt1End)
			x2 = self.en_st2_cbr1(x1, ifTrain)
			xEnSt2Ed = self.en_st2_cbr2(x2, ifTrain)
			x3 = self.bridge_mp(xEnSt2Ed)  
			x4 = self.bridge_cbr1(x3, ifTrain)
			x5 = self.bridge_cbr2(x4, ifTrain)    
			xBridgeEnd = self.bridge_tconv1(x5)
			xCrop1 = self.de_3dcrop1(xEnSt2Ed)
			print(xBridgeEnd.shape)
			print(xCrop1.shape)
			x6 = self.de_st1_concat([xBridgeEnd, xCrop1])
			print(x6.shape)
			x7 = self.de_st1_cbr1(x6, ifTrain)
			x8 = self.de_st1_cbr2(x7, ifTrain)
			xDeSt1End = self.de_st1_tconv1(x8)
			xCrop2 = self.de_3dcrop2(xEnSt1End)
			x9 = self.de_st2_concat([xDeSt1End, xCrop2])
			x10 = self.de_st2_cbr1(x9, ifTrain)
			x11 = self.de_st2_cbr2(x10, ifTrain)
			x12 = self.final_conv3D(x11)
			outputs = activations.softmax(x12, axis=1)
			
			return outputs
			
		def build_model(self, input_shape):
			''' A work-around to define dimensions of signals through the NN'''
			self.build(input_shape)
			inputs = tf.keras.Input(shape=input_shape[1:])

			_ = self.call(inputs)
			
		def compute_output_shape(self):
			# Override this function if one expects to use the subclassed model in Kera's fit() method; Otherwise, it is optional.
			return tf.TensorShape(np.append(self.classNum, self.outDim))    

```

Please pay attention to the following two definitions. If I use concatenate layer in this way (the C is a capitalised one), when I try and save the model, it works
```
	self.de_st1_concat = layers.Concatenate(axis=1)
	self.de_st2_concat = layers.Concatenate(axis=1)
```
For instance,
```
	modelInDim = (4, 64, 64, 64)
	classNum = 2
	mbSize = 2
	TUNet = SimpleUNet1(modelInDim, classNum)
	TUNet.build_model(input_shape=(mbSize,)+modelInDim)
	x=tf.random.uniform((mbSize, 4, 64, 64, 64))
	y=TUNet(x)
	TUNet.summary()
	TUNet._set_inputs(x)
	TUNet.save(r'...\TTweight', save_format='tf') 
```

But if, as per [this page](https://www.tensorflow.org/api_docs/python/tf/keras/layers/concatenate?hl=en&version=stable) I use layers.concatenate (small 'c') to generate signals x6 and x9, respectively, that is
```
	x6 = layers.concatenate([xBridgeEnd, xCrop1], axis=1)
	x9 = layers.concatenate([xDeSt1End, xCrop2], axis=1)
```
Then save the model in the same way above, it raised an error
```
	  ValueError: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 256, None, None, None), (None, 128, 18, 18, 18)]
```
The detailed log is [here](https://github.com/tensorflow/tensorflow/issues/19241)

It takes me almost an entire day to figure out the root cause. In conclusion, I think the second one may have to be deprecated; otherwise users like me will be confused and misguided."
35440,could not find 'TrtGraphConverter," i want to create a TensorRT inference graph directly from my SavedModel. However i met a problem
""from tensorflow.python.compiler.tensorrt import trt_convert as trt
ModuleNotFoundError: No module named 'tensorflow.python.compiler""
when i change it into ""from tensorflow.contrib.tensorrt.python import trt_convert as trt"" i met another problem that ""AttributeError: module 'tensorflow.contrib.tensorrt.python.trt_convert' has no attribute 'TrtGraphConverter'""
my tensorflow is 1.13.1"
35439,tf.metrics.Mean* metrics miscalculated,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip3
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
tf.metrics.MeanAbsoluteError and others compute the mean of means.

**Describe the expected behavior**
They should compute the mean of all the data to support iterating over evaluation datasets.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

import tensorflow as tf
m=tf.metrics.MeanAbsoluteError()
m.update_state(y_true=[0,0], y_pred=[1,1])
m.update_state(y_true=[0], y_pred=[2])
assert m.result() == 2


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

The fix is simple, instead of the mean_squared_error function use a (new) sum_squared_error function in the metric and pass that to MeanMetricWrapper."
35438,Add wheel for python3.8 on pypi,"Currently, there is no wheel on pypi for python 3.8

I strongly hope that there will be support for latest python version."
35435,"tf.math.tanh can produce values outside of `[-1, 1]`","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  https://colab.research.google.com/
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0-rc2
- Python version: 3.6.9

**Describe the current behavior**

```
import numpy as np
import tensorflow as tf

x = np.float32(-8.51089)
print(tf.math.tanh(x))  # => <tf.Tensor: shape=(), dtype=float32, numpy=-1.0000001>
print(tf.math.tanh(-x))  # => <tf.Tensor: shape=(), dtype=float32, numpy=1.0000001>
```

**Describe the expected behavior**

The output of `tanh(x)` should always be `>= -1` and `<= 1`.

**Code to reproduce the issue**

The above code is a minimal repro.

You can also reproduce the issue using the following colab: https://colab.research.google.com/drive/1BHGZoBgMzOlG0e3cYhGYEnLPp1_uSpIY .


**Other info / logs**

This issue was originally filed as https://github.com/tensorflow/tensorflow/issues/34773 against TF 1.15.0 in Colab, but it is not clear if the issue could be reproduced in TF 1.15.0.

I haven't done an exhaustive search, but it looks like this issue exists in TF 2.1.0-dev20191203 and TF 2.1.0-dev20191218, as well, and was fixed in TF 2.1.0-dev20191219.

 - TF 2.1.0-dev20191218 repro: https://colab.research.google.com/drive/1GfA45tP2Q1ZQDmRIusfz4mmfTA5TW1ne

 - Correct output in TF 2.1.0-dev20191219: https://colab.research.google.com/drive/104qs8RAzWmsHATfCZ-01NbnBDnhqRzv0


If the issue has been fixed in TF 2.1.0-dev20191219,  does this mean the issue will be fixed in TF 2.1.0?

"
35433,Flag --incompatible_no_implicit_file_export will break TensorFlow in a future Bazel release,"Incompatible flag --incompatible_no_implicit_file_export will be enabled by default in a future Bazel release [1], thus breaking TensorFlow.

The flag is documented here: https://github.com/bazelbuild/bazel/issues/10225

Please check the following CI builds for build and test results:

* <a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/361#3e0dba20-d84a-4394-bbe8-4155855f8aa5"" target=""_blank""><img src=""https://raw.githubusercontent.com/buildkite/emojis/master/img-buildkite-64/mac.png"" height=""16""/>macOS, OpenJDK 8</a>
* <a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/361#8067be5e-7d21-4333-a4e1-6e2817d456d4"" target=""_blank""><img src=""https://raw.githubusercontent.com/buildkite/emojis/master/img-buildkite-64/ubuntu.png"" height=""16""/>Ubuntu 18.04, OpenJDK 11</a>
* <a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/361#5664296d-5125-45bc-a16b-80fb8b5b94f1"" target=""_blank""><img src=""https://raw.githubusercontent.com/buildkite/emojis/master/img-buildkite-64/windows.png"" height=""16""/>Windows, OpenJDK 8</a>

Never heard of incompatible flags before? We have [documentation](https://docs.bazel.build/versions/master/backward-compatibility.html) that explains everything.

If you don't want to receive any future issues for TensorFlow or if you have any questions,
please file an issue in https://github.com/bazelbuild/continuous-integration

**Important**: Please do NOT modify the issue title since that might break our tools.

[1] The target release hasn't been determined yet. Our tool will update the issue title once the flag flip has been scheduled.
"
35432,"TypeError: Expected Operation, Variable, or Tensor, got None while saving tensorflow model","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- TensorFlow installed from (source or binary): Source
- TensorFlow version: '2.0.0'
- Python version: Python 3.7.5 /Conda 
- CUDA/cuDNN version: cuda10.0_0/cudnn-7.6.5
- GPU model and memory: Tesla V100-PCIE / 32 GB memory



**Describe the current behavior**
**I am getting TypeError: Expected Operation, Variable, or Tensor, got None while saving the model using model.save('../output/my_model')**

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-49-5ab71d0ebc23> in <module>
----> 1 model.save('../output/my_model')

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)
    973     """"""
    974     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,
--> 975                       signatures, options)
    976 
    977   def save_weights(self, filepath, overwrite=True, save_format=None):

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)
    113   else:
    114     saved_model_save.save(model, filepath, overwrite, include_optimizer,
--> 115                           signatures, options)
    116 
    117 

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer, signatures, options)
     72   # default learning phase placeholder.
     73   with K.learning_phase_scope(0):
---> 74     save_lib.save(model, filepath, signatures, options)
     75 
     76   if not include_optimizer:

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py in save(obj, export_dir, signatures, options)
    868   if signatures is None:
    869     signatures = signature_serialization.find_function_to_export(
--> 870         checkpoint_graph_view)
    871 
    872   signatures = signature_serialization.canonicalize_signatures(signatures)

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/saved_model/signature_serialization.py in find_function_to_export(saveable_view)
     62   # If the user did not specify signatures, check the root object for a function
     63   # that can be made into a signature.
---> 64   functions = saveable_view.list_functions(saveable_view.root)
     65   signature = functions.get(DEFAULT_SIGNATURE_ATTR, None)
     66   if signature is not None:

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py in list_functions(self, obj)
    139     if obj_functions is None:
    140       obj_functions = obj._list_functions_for_serialization(  # pylint: disable=protected-access
--> 141           self._serialization_cache)
    142       self._functions[obj] = obj_functions
    143     return obj_functions

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in _list_functions_for_serialization(self, serialization_cache)
   2420   def _list_functions_for_serialization(self, serialization_cache):
   2421     return (self._trackable_saved_model_saver
-> 2422             .list_functions_for_serialization(serialization_cache))
   2423 
   2424 

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/base_serialization.py in list_functions_for_serialization(self, serialization_cache)
     89         `ConcreteFunction`.
     90     """"""
---> 91     fns = self.functions_to_serialize(serialization_cache)
     92 
     93     # The parent AutoTrackable class saves all user-defined tf.functions, and

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py in functions_to_serialize(self, serialization_cache)
     77   def functions_to_serialize(self, serialization_cache):
     78     return (self._get_serialized_attributes(
---> 79         serialization_cache).functions_to_serialize)
     80 
     81   def _get_serialized_attributes(self, serialization_cache):

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py in _get_serialized_attributes(self, serialization_cache)
     92 
     93     object_dict, function_dict = self._get_serialized_attributes_internal(
---> 94         serialization_cache)
     95 
     96     serialized_attr.set_and_validate_objects(object_dict)

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/model_serialization.py in _get_serialized_attributes_internal(self, serialization_cache)
     45     # cache (i.e. this is the root level object).
     46     if len(serialization_cache[constants.KERAS_CACHE_KEY]) == 1:
---> 47       default_signature = save_impl.default_save_signature(self.obj)
     48 
     49     # Other than the default signature function, all other attributes match with

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in default_save_signature(layer)
    204   original_losses = _reset_layer_losses(layer)
    205   fn = saving_utils.trace_model_call(layer)
--> 206   fn.get_concrete_function()
    207   _restore_layer_losses(original_losses)
    208   return fn

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in get_concrete_function(self, *args, **kwargs)
    774       if self._stateful_fn is None:
    775         initializer_map = object_identity.ObjectIdentityDictionary()
--> 776         self._initialize(args, kwargs, add_initializers_to=initializer_map)
    777         self._initialize_uninitialized_variables(initializer_map)
    778 

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    406     self._concrete_stateful_fn = (
    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 408             *args, **kwds))
    409 
    410     def invalid_creator_scope(*unused_args, **unused_kwds):

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   1846     if self.input_signature:
   1847       args, kwargs = None, None
-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   1849     return graph_function
   1850 

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2148         graph_function = self._function_cache.primary.get(cache_key, None)
   2149         if graph_function is None:
-> 2150           graph_function = self._create_graph_function(args, kwargs)
   2151           self._function_cache.primary[cache_key] = graph_function
   2152         return graph_function, args, kwargs

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2039             arg_names=arg_names,
   2040             override_flat_arg_shapes=override_flat_arg_shapes,
-> 2041             capture_by_value=self._capture_by_value),
   2042         self._function_attributes,
   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    913                                           converted_func)
    914 
--> 915       func_outputs = python_func(*func_args, **func_kwargs)
    916 
    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    357         # the function a weak reference to itself to avoid a reference cycle.
--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    359     weak_wrapped_fn = weakref.ref(wrapped_fn)
    360 

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saving_utils.py in _wrapped_model(*args)
    141     with base_layer_utils.call_context().enter(
    142         model, inputs=inputs, build_graph=False, training=False, saving=True):
--> 143       outputs_list = nest.flatten(model(inputs=inputs, training=False))
    144 
    145     try:

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    845                     outputs = base_layer_utils.mark_as_return(outputs, acd)
    846                 else:
--> 847                   outputs = call_fn(cast_inputs, *args, **kwargs)
    848 
    849             except errors.OperatorNotAllowedInGraphError as e:

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    290   def wrapper(*args, **kwargs):
    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
--> 292       return func(*args, **kwargs)
    293 
    294   if inspect.isfunction(func) or inspect.ismethod(func):

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/transformers/modeling_tf_albert.py in call(self, inputs, **kwargs)
    783 
    784     def call(self, inputs, **kwargs):
--> 785         outputs = self.albert(inputs, **kwargs)
    786 
    787         pooled_output = outputs[1]

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    845                     outputs = base_layer_utils.mark_as_return(outputs, acd)
    846                 else:
--> 847                   outputs = call_fn(cast_inputs, *args, **kwargs)
    848 
    849             except errors.OperatorNotAllowedInGraphError as e:

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    290   def wrapper(*args, **kwargs):
    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
--> 292       return func(*args, **kwargs)
    293 
    294   if inspect.isfunction(func) or inspect.ismethod(func):

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/transformers/modeling_tf_albert.py in call(self, inputs, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, training)
    680 
    681         embedding_output = self.embeddings(
--> 682             [input_ids, position_ids, token_type_ids, inputs_embeds], training=training)
    683         encoder_outputs = self.encoder(
    684             [embedding_output, extended_attention_mask, head_mask], training=training)

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    889           with base_layer_utils.autocast_context_manager(
    890               self._compute_dtype):
--> 891             outputs = self.call(cast_inputs, *args, **kwargs)
    892           self._handle_activity_regularization(inputs, outputs)
    893           self._set_mask_metadata(inputs, outputs, input_masks)

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in return_outputs_and_add_losses(*args, **kwargs)
     55     inputs = args[inputs_arg_index]
     56     args = args[inputs_arg_index + 1:]
---> 57     outputs, losses = fn(inputs, *args, **kwargs)
     58     layer.add_loss(losses, inputs)
     59     return outputs

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in wrap_with_training_arg(*args, **kwargs)
    109         training,
    110         lambda: replace_training_and_call(True),
--> 111         lambda: replace_training_and_call(False))
    112 
    113   # Create arg spec for decorated function. If 'training' is not defined in the

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py in smart_cond(pred, true_fn, false_fn, name)
     57         pred, true_fn=true_fn, false_fn=false_fn, name=name)
     58   return smart_module.smart_cond(
---> 59       pred, true_fn=true_fn, false_fn=false_fn, name=name)
     60 
     61 

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)
     54       return true_fn()
     55     else:
---> 56       return false_fn()
     57   else:
     58     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in <lambda>()
    109         training,
    110         lambda: replace_training_and_call(True),
--> 111         lambda: replace_training_and_call(False))
    112 
    113   # Create arg spec for decorated function. If 'training' is not defined in the

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in replace_training_and_call(training)
    104     def replace_training_and_call(training):
    105       set_training_arg(training, training_arg_index, args, kwargs)
--> 106       return wrapped_call(*args, **kwargs)
    107 
    108     return tf_utils.smart_cond(

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in __call__(self, *args, **kwargs)
    531     if not self.call_collection.tracing:
    532       self.call_collection.add_trace(*args, **kwargs)
--> 533     return super(LayerCall, self).__call__(*args, **kwargs)
    534 
    535   def get_concrete_function(self, *args, **kwargs):

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--> 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    492       # In this case we have not created variables on the first call. So we can
    493       # run the first trace but we should fail if variables are created.
--> 494       results = self._stateful_fn(*args, **kwds)
    495       if self._created_variables:
    496         raise ValueError(""Creating variables on a non-first call to a function""

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
   1820   def __call__(self, *args, **kwargs):
   1821     """"""Calls a graph function specialized to the inputs.""""""
-> 1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
   1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1824 

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2148         graph_function = self._function_cache.primary.get(cache_key, None)
   2149         if graph_function is None:
-> 2150           graph_function = self._create_graph_function(args, kwargs)
   2151           self._function_cache.primary[cache_key] = graph_function
   2152         return graph_function, args, kwargs

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2039             arg_names=arg_names,
   2040             override_flat_arg_shapes=override_flat_arg_shapes,
-> 2041             capture_by_value=self._capture_by_value),
   2042         self._function_attributes,
   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    913                                           converted_func)
    914 
--> 915       func_outputs = python_func(*func_args, **func_kwargs)
    916 
    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    357         # the function a weak reference to itself to avoid a reference cycle.
--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    359     weak_wrapped_fn = weakref.ref(wrapped_fn)
    360 

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in wrapper(*args, **kwargs)
    513         layer, inputs=inputs, build_graph=False, training=training,
    514         saving=True):
--> 515       ret = method(*args, **kwargs)
    516     _restore_layer_losses(original_losses)
    517     return ret

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in wrap_with_training_arg(*args, **kwargs)
    109         training,
    110         lambda: replace_training_and_call(True),
--> 111         lambda: replace_training_and_call(False))
    112 
    113   # Create arg spec for decorated function. If 'training' is not defined in the

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py in smart_cond(pred, true_fn, false_fn, name)
     57         pred, true_fn=true_fn, false_fn=false_fn, name=name)
     58   return smart_module.smart_cond(
---> 59       pred, true_fn=true_fn, false_fn=false_fn, name=name)
     60 
     61 

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)
     54       return true_fn()
     55     else:
---> 56       return false_fn()
     57   else:
     58     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in <lambda>()
    109         training,
    110         lambda: replace_training_and_call(True),
--> 111         lambda: replace_training_and_call(False))
    112 
    113   # Create arg spec for decorated function. If 'training' is not defined in the

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in replace_training_and_call(training)
    104     def replace_training_and_call(training):
    105       set_training_arg(training, training_arg_index, args, kwargs)
--> 106       return wrapped_call(*args, **kwargs)
    107 
    108     return tf_utils.smart_cond(

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in call_and_return_conditional_losses(inputs, *args, **kwargs)
    555   layer_call = _get_layer_call_method(layer)
    556   def call_and_return_conditional_losses(inputs, *args, **kwargs):
--> 557     return layer_call(inputs, *args, **kwargs), layer.get_losses_for(inputs)
    558   return _create_call_fn_decorator(layer, call_and_return_conditional_losses)
    559 

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in get_losses_for(self, inputs)
   1382     losses = [l for l in self.losses if not l._unconditional_loss]
   1383     inputs = nest.flatten(inputs)
-> 1384     reachable = tf_utils.get_reachable_from_inputs(inputs, losses)
   1385     return [l for l in losses if l in reachable]
   1386 

/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py in get_reachable_from_inputs(inputs, targets)
    132       outputs = x.consumers()
    133     else:
--> 134       raise TypeError('Expected Operation, Variable, or Tensor, got ' + str(x))
    135 
    136     for y in outputs:

TypeError: Expected Operation, Variable, or Tensor, got None
**Describe the expected behavior**

**Code to reproduce the issue**

`import tensorflow as tf`
`import pandas as pd`

`from sklearn.model_selection import train_test_split`

`import transformers`
`from transformers import AlbertConfig, AlbertTokenizer, TFAlbertForSequenceClassification,glue_convert_examples_to_features `
`!pip install transformers`

`data_df = pd.read_excel(""../input/test.xlsx"")`
`model_dir = '../input/albert_xxlarge_v2/'`
`EPOCHS = 3`
`MAX_SEQ_LENGTH = 256`
`label_list = [0,1]`

`config = AlbertConfig.from_pretrained('albert-xxlarge-v2')`
`tokenizer = AlbertTokenizer.from_pretrained('albert-xxlarge-v2', cache_dir=model_dir)`
`model = TFAlbertForSequenceClassification.from_pretrained('albert-xxlarge-v2', ``cache_dir=model_dir, config=config)`

`train_df, test_df = train_test_split(data_df[['id','text1', 'text2', 'LABEL']], 
                                                    random_state=42, shuffle=True, 
                                                    test_size=0.20, stratify=data_df['LABEL'])`

`train_InputExamples = train_df.apply(lambda x: InputExample(guid=x['id'],
                                                                           text_a=x['text1'],
                                                                           text_b=x['text2'],
                                                                           label=x['LABEL']), axis=1)`

`train_dataset = glue_convert_examples_to_features(examples=train_InputExamples, tokenizer=tokenizer,
                                                  max_length=MAX_SEQ_LENGTH,
                                                  label_list = label_list, output_mode=""classification"")`

`optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)`
`loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)`
`metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')`

`input_ids_train = []`
`attention_mask_train = []`
`token_type_ids_train = []`
`output_label_train = []`
`for f in train_dataset:`
    `input_ids_train.append(f.input_ids)`
    `attention_mask_train.append(f.attention_mask)`
    `token_type_ids_train.append(f.token_type_ids)`
    `output_label_train.append(f.label)`
`model.compile(optimizer=optimizer, loss=loss, metrics=[metric])`

`input_ids_train = np.array(input_ids_train)`
`attention_mask_train = np.array(attention_mask_train)`
`token_type_ids_train = np.array(token_type_ids_train)`
`output_label_train = np.array(output_label_train)`

`model.fit([input_ids_train,attention_mask_train, token_type_ids_train], y=output_label_train,
          epochs = EPOCHS, batch_size=4)`

`model.save('../output/my_model')`"
35431,Unable to build,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 19.0
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 2.0
- Python version: 3.7.4
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 1.1.0
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
After about 20+ minutes of watching the code build the build stopped abruptly

**Any other info / logs**
Here is a link to an enviorment where you can run my exact dev-env in the cloud
https://gitpod.io/#https://github.com/JesterOrNot/tensorflow/tree/JesterOrNot/gitpod-setup

```
./tensorflow/python/lib/core/pybind11_proto.h:40:44: warning: 'pybind11::str pybind11::detail::object_api<Derived>::str() const [wi
th Derived = pybind11::handle]' is deprecated: Use py::str(obj) instead [-Wdeprecated-declarations]
       std::string(py_object.get_type().str()), "" is not a valid proto.""));
                                            ^
In file included from external/pybind11/include/pybind11/cast.h:13,
                 from external/pybind11/include/pybind11/attr.h:13,
                 from external/pybind11/include/pybind11/pybind11.h:49,
                 from tensorflow/python/client/device_lib_wrapper.cc:18:
external/pybind11/include/pybind11/pytypes.h:147:19: note: declared here
     pybind11::str str() const;
                   ^~~
INFO: From Compiling tensorflow/stream_executor/stream_executor_pimpl.cc [for host]:
tensorflow/stream_executor/stream_executor_pimpl.cc: In member function 'stream_executor::DeviceMemoryBase stream_executor::StreamE
xecutor::Allocate(tensorflow::uint64, tensorflow::int64)':
tensorflow/stream_executor/stream_executor_pimpl.cc:462:31: warning: comparison of integer expressions of different signedness: 'lo
ng long unsigned int' and 'tensorflow::int64' {aka 'long long int'} [-Wsign-compare]
       mem_alloc_bytes_ + size > memory_limit_bytes_) {
       ~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
In file included from ./tensorflow/core/platform/default/logging.h:29,
                 from ./tensorflow/core/platform/logging.h:27,
                 from ./tensorflow/core/platform/status.h:24,
                 from ./tensorflow/core/platform/errors.h:22,
                 from ./tensorflow/core/lib/core/errors.h:19,
                 from ./tensorflow/stream_executor/device_memory_allocator.h:23,
                 from ./tensorflow/stream_executor/stream_executor_pimpl.h:28,
                 from tensorflow/stream_executor/stream_executor_pimpl.cc:20:
./tensorflow/core/platform/default/logging.h: In instantiation of 'std::__cxx11::string* tensorflow::internal::Check_EQImpl(const T1&, const T2&, const char*) [with T1 = int; T2 = long long unsigned int; std::__cxx11::string = std::__cxx11::basic_string<char>]':
tensorflow/stream_executor/stream_executor_pimpl.cc:700:3:   required from here
./tensorflow/core/platform/default/logging.h:386:25: warning: comparison of integer expressions of different signedness: 'const int' and 'const long long unsigned int' [-Wsign-compare]
                         ==)  // Compilation error with CHECK_EQ(NULL, x)?
./tensorflow/core/platform/macros.h:88:49: note: in definition of macro 'TF_PREDICT_TRUE'
 #define TF_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))
                                                 ^
./tensorflow/core/platform/default/logging.h:385:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'
 TF_DEFINE_CHECK_OP_IMPL(Check_EQ,
 ^~~~~~~~~~~~~~~~~~~~~~~
INFO: From Compiling tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc [for host]:
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc: In function 'tensorflow::Status tensorflow::{anonymous}::GetT
PUDevices(tensorflow::Devices, llvm::ArrayRef<tensorflow::DeviceNameUtils::ParsedName>, llvm::SmallVectorImpl<llvm::SmallVector<ten
sorflow::DeviceNameUtils::ParsedName, 8> >*)':
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc:129:27: warning: comparison of integer expressions of differen
t signedness: 'int' and 'size_t' {aka 'long unsigned int'} [-Wsign-compare]
     if (num_tpus_per_host != host_tpu_devices.size())
         ~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~
INFO: From Compiling tensorflow/core/kernels/quantization_utils.cc [for host]:
In file included from external/gemmlowp/public/../internal/dispatch_gemm_shape.h:23,
                 from external/gemmlowp/public/gemmlowp.h:19,
                 from ./tensorflow/core/kernels/quantization_utils.h:37,
                 from tensorflow/core/kernels/quantization_utils.cc:16:
external/gemmlowp/public/../internal/multi_thread_gemm.h: In member function 'void gemmlowp::WorkersPool::LegacyExecuteAndDestroyTa
sks(const std::vector<gemmlowp::Task*>&)':
external/gemmlowp/public/../internal/multi_thread_gemm.h:405:23: warning: comparison of integer expressions of different signedness
: 'int' and 'std::size_t' {aka 'long unsigned int'} [-Wsign-compare]
     for (int i = 0; i < tasks_count - 1; i++) {
                     ~~^~~~~~~~~~~~~~~~~
In file included from tensorflow/core/kernels/quantization_utils.cc:16:
./tensorflow/core/kernels/quantization_utils.h: In function 'void tensorflow::RequantizeManyInNewRangeReference(const qint32*, tensorflow::int64, float, float, float, float, tensorflow::quint8*)':
./tensorflow/core/kernels/quantization_utils.h:271:32: warning: comparison of integer expressions of different signedness: 'size_t' {aka 'long unsigned int'} and 'tensorflow::int64' {aka 'long long int'} [-Wsign-compare]
   for (size_t index = 0; index < count; ++index) {
                          ~~~~~~^~~~~~~
[6,434 / 12,032] 16 actions running
    Compiling tensorflow/python/tfe_wrapper.cc [for host]; 77s local
    Compiling tensorflow/core/kernels/rnn/lstm_ops.cc [for host]; 44s local
    Compiling tensorflow/core/kernels/rnn/gru_ops.cc [for host]; 43s local
    Compiling tensorflow/stream_executor/stream.cc [for host]; 41s local
    Compiling tensorflow/core/kernels/split_lib_cpu.cc [for host]; 28s local
    Compiling .../core/kernels/serialize_sparse_op.cc [for host]; 24s local
    //tensorflow/core/kernels:deserialize_sparse_string_op; 22s local
    Compiling .../core/kernels/sparse_reorder_op.cc [for host]; 22s local ...

Server terminated abruptly (error code: 14, error message: 'Socket closed', log file: '/home/gitpod/.cache/bazel/_bazel_gitpod/2c92b5569ddded7b3a6bd5e139451b60/server/jvm.out'
```"
35430,ReLU layer doesn't handle integer dtype,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10 (buster)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.1.0-rc0-47-g064e153 2.1.0-rc1
- Python version: 3.7.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: None
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
An exception during `tf.keras.layers.ReLU` construction with integer dtype and `max_value`

**Describe the expected behavior**
A layer is properly constructed and functional

**Code to reproduce the issue**
```
import tensorflow as tf
input = tf.keras.layers.Input(shape=(), name='x', dtype='int64')
y = tf.keras.layers.ReLU(max_value=100, dtype='int64')(input)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in _AssertCompatible(values, dtype)
    323   try:
--> 324     fn(values)
    325   except ValueError as e:

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in inner(values)
    262   def inner(values):
--> 263     _ = [_check_failed(v) for v in nest.flatten(values)
    264          if not isinstance(v, expected_types)]

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in <listcomp>(.0)
    263     _ = [_check_failed(v) for v in nest.flatten(values)
--> 264          if not isinstance(v, expected_types)]
    265   return inner

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in _check_failed(v)
    247   # it is safe to use here.
--> 248   raise ValueError(v)
    249 

ValueError: 0.0

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-16-2baabe6a9f04> in <module>
      1 input = tf.keras.layers.Input(shape=(), name='x', dtype='int64')
----> 2 y = tf.keras.layers.ReLU(max_value=100, dtype='int64')(input)

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    771                     not base_layer_utils.is_in_eager_or_tf_function()):
    772                   with auto_control_deps.AutomaticControlDependencies() as acd:
--> 773                     outputs = call_fn(cast_inputs, *args, **kwargs)
    774                     # Wrap Tensors in `outputs` in `tf.identity` to avoid
    775                     # circular dependencies.

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/advanced_activations.py in call(self, inputs)
    317                   alpha=self.negative_slope,
    318                   max_value=self.max_value,
--> 319                   threshold=self.threshold)
    320 
    321   def get_config(self):

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py in relu(x, alpha, max_value, threshold)
   4373   if clip_max:
   4374     max_value = _constant_to_tensor(max_value, x.dtype.base_dtype)
-> 4375     zero = _constant_to_tensor(0., x.dtype.base_dtype)
   4376     x = clip_ops.clip_by_value(x, zero, max_value)
   4377 

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py in _constant_to_tensor(x, dtype)
    676       A tensor.
    677   """"""
--> 678   return constant_op.constant(x, dtype=dtype)
    679 
    680 

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in constant(value, dtype, shape, name)
    256   """"""
    257   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--> 258                         allow_broadcast=True)
    259 
    260 

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    294       tensor_util.make_tensor_proto(
    295           value, dtype=dtype, shape=shape, verify_shape=verify_shape,
--> 296           allow_broadcast=allow_broadcast))
    297   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
    298   const_tensor = g._create_op_internal(  # pylint: disable=protected-access

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
    449       nparray = np.empty(shape, dtype=np_dt)
    450     else:
--> 451       _AssertCompatible(values, dtype)
    452       nparray = np.array(values, dtype=np_dt)
    453       # check to them.

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in _AssertCompatible(values, dtype)
    329     else:
    330       raise TypeError(""Expected %s, got %s of type '%s' instead."" %
--> 331                       (dtype.name, repr(mismatch), type(mismatch).__name__))
    332 
    333 

TypeError: Expected int64, got 0.0 of type 'float' instead.
```


```
tf.constant(0., dtype='int64')
```
fails as well but with different backtrace.

tf_nightly-2.1.0.dev20191226 is affected too."
35426,"Can't install tensorflow even with no cache dir ""pip3 --no-cache-dir install tensorflow"" ","ERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.
    grpcio>=1.8.6 from https://www.piwheels.org/simple/grpcio/grpcio-1.26.0-cp37-cp37m-linux_armv7l.whl#sha256=e1478c82e0848fbdd790389c23fb94290da96447cc576679e9fdc16b7d4bc7e8 (from tensorflow):
        Expected sha256 e1478c82e0848fbdd790389c23fb94290da96447cc576679e9fdc16b7d4bc7e8
             Got        ec6268c7c1aab71ff12c2c97f87ba0bf148229d6d519d57db198c469c033b8b9

"
35423,"TF 2.0 XLA JIT reporting error: ""./bin/ptxas not found""","**System information**
- OS Platform and Distribution: Ubuntu 16.04.6 LTS
- TensorFlow installed from (source or binary): pip3 install tensorflow-gpu
- TensorFlow version (use command below): 2.0.0
- Python version: 3.5.2
- CUDA/cuDNN version: 10.0
- GPU model and memory: TITAN Xp

**Describe the current behavior**

The test code is running with error as bellow:

```
2019-12-26 22:02:59.166382: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-12-26 22:02:59.166422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-12-26 22:02:59.166453: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-12-26 22:02:59.166482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-12-26 22:02:59.166512: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-12-26 22:02:59.166541: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-12-26 22:02:59.166573: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-12-26 22:02:59.171144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-26 22:02:59.171311: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-12-26 22:02:59.174312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-26 22:02:59.174418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2019-12-26 22:02:59.174508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2019-12-26 22:02:59.179990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11439 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:06:00.0, compute capability: 6.1)
sleep
2019-12-26 22:02:59.923393: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-12-26 22:03:00.348503: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
2019-12-26 22:03:00.355159: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at xla_compile_on_demand_op.cc:218 : Not found: ./bin/ptxas not found
Traceback (most recent call last):
  File ""tf.py"", line 8, in <module>
    c = tf.linalg.matmul(a, b)
  File ""/home/thincal/.local/lib/python3.5/site-packages/tensorflow_core/python/util/dispatch.py"", line 180, in wrapper
    return target(*args, **kwargs)
  File ""/home/thincal/.local/lib/python3.5/site-packages/tensorflow_core/python/ops/math_ops.py"", line 2765, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""/home/thincal/.local/lib/python3.5/site-packages/tensorflow_core/python/ops/gen_math_ops.py"", line 6126, in mat_mul
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 2, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: ./bin/ptxas not found [Op:MatMul] name: MatMul/
```

**Describe the expected behavior**

The test code is running successfully.

**Code to reproduce the issue**

```
import tensorflow as tf
try:
  with tf.device('device:XLA_GPU:0'):
    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
    c = tf.linalg.matmul(a, b)
    print(c)
except RuntimeError as e:
  print(e)
```"
35422,x/Unknown while training the first epoch.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Code is ran on Google Colab (However, my machine is Windows 10 Version 1903 if that matters)
- TensorFlow installed from (source or binary): No knowledge, using the already installed version on Google Colab
- TensorFlow version (use command below): v2.1.0-rc1-0-g064e1535a7 2.1.0-rc1
- Python version: 3.6.9
- GCC/Compiler version (if compiling from source): 8.3.0
- GPU model and memory: No knowledge, using the given Google Colab hardware.

**Describe the current behavior**
When fitting the model, on the first epoch, it counts steps like ""337/Unknown"", the Unknown stays there until the first epoch fitting is over(at step 582) but that issue gets fixed after the first epoch is over. The text turns to ""582/582"" and it shows like ""x/582"" on the next epochs.

**Describe the expected behavior**
It should say like ""x/582"" at the first epoch too.

**Code to reproduce the issue**
https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb
Try doing the ""Train the model"" part. You should encounter this problem when you start fitting the model.
"
35421,Android Tensorflow Lite Crash: When using 'options.setUseNNAPI(true); ' with TFLite in SAMSUNG S10+,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:Samsung Galaxy S10+
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0

**Describe the current behavior**
My app has crashed...
```log
2019-12-26 20:32:48.140 16058-16801/? A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x64393 in tid 16801 (neuralnetworks@), pid 16058 (neuralnetworks@)
2019-12-26 20:32:48.183 16807-16807/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
2019-12-26 20:32:48.183 16807-16807/? A/DEBUG: Build fingerprint: 'samsung/beyond2qlteue/beyond2q:9/PPR1.180610.011/G975U1UES2BSKA:user/release-keys'
2019-12-26 20:32:48.184 16807-16807/? A/DEBUG: Revision: '17'
2019-12-26 20:32:48.184 16807-16807/? A/DEBUG: ABI: 'arm64'
2019-12-26 20:32:48.184 16807-16807/? A/DEBUG: pid: 16058, tid: 16801, name: neuralnetworks@  >>> /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti <<<
2019-12-26 20:32:48.184 16807-16807/? A/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x64393
2019-12-26 20:32:48.184 16807-16807/? A/DEBUG:     x0  ffffffffffffffff  x1  0000000000064393  x2  0000000000000008  x3  0000000000000000
2019-12-26 20:32:48.184 16807-16807/? A/DEBUG:     x4  0000007cd58059b8  x5  0000000000000007  x6  0000000000000003  x7  0000000000000003
2019-12-26 20:32:48.184 16807-16807/? A/DEBUG:     x8  0000000000000000  x9  0000000000000010  x10 0000000000000000  x11 0000000000000000
2019-12-26 20:32:48.184 16807-16807/? A/DEBUG:     x12 0000000000000003  x13 0000000000000000  x14 0000000000000003  x15 aaaaaaaaaaaaaaab
2019-12-26 20:32:48.184 16807-16807/? A/DEBUG:     x16 0000007cd67921c0  x17 0000007cd6715a98  x18 0000000000000010  x19 0000007cd5300f28
2019-12-26 20:32:48.184 16807-16807/? A/DEBUG:     x20 0000000000000001  x21 0000007cd5835e18  x22 00000000000001b0  x23 0000007cd5849578
2019-12-26 20:32:48.184 16807-16807/? A/DEBUG:     x24 0000007cd5301588  x25 0000000000000001  x26 0000000000000000  x27 0000000000000003
2019-12-26 20:32:48.184 16807-16807/? A/DEBUG:     x28 0000000000000007  x29 0000007cd5300e20
2019-12-26 20:32:48.184 16807-16807/? A/DEBUG:     sp  0000007cd5300d90  lr  0000005ffb94585c  pc  0000005ffb9458a8
2019-12-26 20:32:48.208 16807-16807/? A/DEBUG: backtrace:
2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #00 pc 00000000000ac8a8  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti (qti::nnhal::gpu::OpTranslation::getTensorParam(int, bool, qti::nnhal::gpu::Permute)+748)
2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #01 pc 00000000000a3fbc  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti (qti::nnhal::gpu::GraphTranslation::conv(qti::nnhal::gpu::OpTranslation&, DlSystem::NetworkDescriptor&, bool)+184)
2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #02 pc 00000000000a8aec  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti (qti::nnhal::gpu::GraphTranslation::translateOp(qti::nnhal::IOperation*, qti::nnhal::IGraph*, DlSystem::NetworkDescriptor&)+420)
2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #03 pc 00000000000a8c88  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti (qti::nnhal::gpu::GraphTranslation::translate(qti::nnhal::IGraph*, DlSystem::NetworkDescriptor&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>>>&)+148)
2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #04 pc 00000000000a0e38  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti (qti::nnhal::gpu::AdrenoModelExecutor::create(qti::nnhal::IGraph*, qti::nnhal::IExecutorContext*)+464)
2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #05 pc 00000000000a03e8  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti (qti::nnhal::gpu::AdrenoAccelerator::createGraphExecutor(qti::nnhal::IGraph*, qti::nnhal::IExecutorContext*)+16)
2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #06 pc 000000000008ea0c  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti (android::hardware::neuralnetworks::V1_1::implementation::NnHalExecution::asyncCreateExecutors(android::sp<android::hardware::neuralnetworks::V1_0::IPreparedModelCallback> const&, android::sp<android::hardware::neuralnetworks::V1_0::IPreparedModel>&)+1288)
2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #07 pc 000000000009196c  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti (android::hardware::neuralnetworks::V1_1::implementation::PreparedModelInitWork::work()+28)
2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #08 pc 000000000008a3f4  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti (android::hardware::neuralnetworks::V1_1::implementation::NnHalWorkerThread::Inner::threadFunc()+252)
2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #09 pc 000000000008bdcc  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti
2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #10 pc 00000000000919c0  /system/lib64/libc.so (__pthread_start(void*)+36)
2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #11 pc 0000000000023fb0  /system/lib64/libc.so (__start_thread+68)
```

**Code to reproduce the issue**
```options.setUseNNAPI(true);```

"
35419,Cannot use fine-tuned ssd_mobilenet_v2 model with TF Lite on Android,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 2 Simulator in Android Studio with Android 7.1.1
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.2
- Python version: 3.6.9
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0/7.5
- GPU model and memory: 1070 max-q 8 GB


**Describe the current behavior**
I fine-tuned the ssd_mobilenet_v2 pretrained model from Tensorflow model zoo to detect two classes. Training was done with TF OD API. After converting the model to detect.tflite and put it in the ""assets"" folder of the [official Android demo](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android) and modified labelmap.txt, upon running I got the following error:
```java
E/AndroidRuntime: FATAL EXCEPTION: inference
    Process: org.tensorflow.lite.examples.detection, PID: 4609
    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/detection_postprocess.cc:404 ValidateBoxes(decoded_boxes, num_boxes) was not true.
    Node number 102 (TFLite_Detection_PostProcess) failed to invoke.
    
        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:152)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:296)
        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:193)
        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:183)
        at android.os.Handler.handleCallback(Handler.java:751)
        at android.os.Handler.dispatchMessage(Handler.java:95)
        at android.os.Looper.loop(Looper.java:154)
        at android.os.HandlerThread.run(HandlerThread.java:61)
```
However, if I use model.ckpt-0 which is the initial checkpoint, the converted .tflite works fine. So I assume that the convertion is OK. Is it due to the training process?
Please help me figure this out.
"
35417,AttributeError: module 'tensorflow' has no attribute 'reset_default_graph',"
- Have I written custom code

import tensorflow as tf

from tensorflow.python.framework import ops
ops.reset_default_graph()

tf.reset_default_graph()
session = tf.InteractiveSession()

i also try :
pip uninstall tensorflow
and again
pip install tensorflow

bt still it's showing me :
AttributeError: module 'tensorflow' has no attribute 'reset_default_graph'





what should I do now
please help me out
"
35416,Bijectors crash tf2.1 autograph in distributed mirrored multi-gpu mode,"**System information**
* Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
* TensorFlow installed from (source or binary): binary
* TensorFlow version (use command below): v2.1.0-rc1-58-g9837ece 2.1.0-rc2 (python3 -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"")
* Python version: Python 3.6.8
* CUDA/cuDNN version: Driver Version: 440.33.01, CUDA Version: 10.2, cuDNN 7.6.2
* GPU model and memory: Tesla V100-SXM2-16GB

**Describe the current behavior**
I'm using Bijectors as a flexible prior for a VAE. 

In tf2.1 autograph distributed mirrored mode 
https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L1152
I am getting
```
google.protobuf.message.DecodeError: Error parsing message
```
when running with multiple GPUs (but not when running with single GPU):
```
bijectors = []
for i in range(16):
    bijectors.append(tfb.MaskedAutoregressiveFlow(
      shift_and_log_scale_fn=tfb.masked_autoregressive_default_template(
          code, hidden_layers=[1024, 1024], name=scope + ""/maf_"" + str(i))))

    bijectors.append(tfb.BatchNormalization(
        batchnorm_layer=tf.layers.BatchNormalization(
                            name=scope + '/batch_norm_' + str(i)),
        name=scope + '/batch_norm_bijector' + str(i)))

    permutation=tf.get_variable('permutation_'+str(i), initializer=np.random.permutation(out_channel).astype(""int32""), trainable=False)
    bijectors.append(tfb.Permute(permutation))
    
flow_bijector = tfb.Chain(list(reversed(bijectors[:-1])))
```
https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L190

**Describe the expected behavior**
Should not crash

**Code to reproduce the issue**
TF2.x code:
https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L190

**Other info / logs**
```
Traceback (most recent call last):
  File ""main.py"", line 134, in <module>
    main()
  File ""main.py"", line 121, in main
    gan.train()
  File ""/app/home/ubuntu/SPADE-Tensorflow.tf2/SPADE.py"", line 1379, in train
    train_loop()
  File ""/app/home/ubuntu/SPADE-Tensorflow.tf2/SPADE.py"", line 1336, in train_loop
    counter, result_inputs, result_losses_det, result_outputs_det, result_outputs_resample_det, result_outputs_random_det, result_outputs_random_gen_det = train_det_grad_both(global_step, self.train_main, *inputs)
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 568, in __call__
    result = self._call(*args, **kwds)
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 615, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 497, in _initialize
    *args, **kwds))
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2389, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2703, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2599, in _create_graph_function
    shared_func_graph=False)
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1511, in __init__
    func_graph, self._attrs, self._garbage_collector)
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 601, in __init__
    self._func_graph.inputs, self._func_graph.outputs, attrs)
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 466, in __init__
    function_def.ParseFromString(compat.as_bytes(proto_data))
google.protobuf.message.DecodeError: Error parsing message
```
[train.CelebAMask-HQ.tf21.4xgpu.maf.log](https://github.com/tensorflow/tensorflow/files/4001259/train.CelebAMask-HQ.tf21.4xgpu.maf.log)
"
35415,Bijectors are orders of magnitude slower in tf2.1 autograph distributed mirrored single-gpu mode,"**System information**
* Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
* TensorFlow installed from (source or binary): binary
* TensorFlow version (use command below): v2.1.0-rc1-58-g9837ece 2.1.0-rc2 (python3 -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"")
* Python version: Python 3.6.8
* CUDA/cuDNN version: Driver Version: 440.33.01, CUDA Version: 10.2, cuDNN 7.6.2
 *GPU model and memory: Tesla V100-SXM2-16GB

**Describe the current behavior**
I'm using Bijectors as a flexible prior for a VAE. 

This code has negligible overhead in tf1.x (for input batch size 18x256x256x3). In tf2.1 autograph distributed mirrored mode 
https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L1152
with single GPU it increases training step duration 1 second (tf1.x) -> 1.9 seconds (tf2.1):
```
bijectors = []
for i in range(16):
    bijectors.append(tfb.MaskedAutoregressiveFlow(
      shift_and_log_scale_fn=tfb.masked_autoregressive_default_template(
          code, hidden_layers=[1024, 1024], name=scope + ""/maf_"" + str(i))))

    bijectors.append(tfb.BatchNormalization(
        batchnorm_layer=tf.layers.BatchNormalization(
                            name=scope + '/batch_norm_' + str(i)),
        name=scope + '/batch_norm_bijector' + str(i)))

    permutation=tf.get_variable('permutation_'+str(i), initializer=np.random.permutation(out_channel).astype(""int32""), trainable=False)
    bijectors.append(tfb.Permute(permutation))
    
flow_bijector = tfb.Chain(list(reversed(bijectors[:-1])))
```
https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L190

I'm using custom masked autoregressive template
https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/masked_autoregressive.py
but it is as slow with the default one:
https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/masked_autoregressive_default_template

Possible suspects:
```
tfb.masked_dense
```
https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/masked_autoregressive.py#L44

```
tf1.make_template()
```
https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/masked_autoregressive.py#L115

**Describe the expected behavior**
Performance in tf2.1 and tf1.x should be comparable.

**Code to reproduce the issue**
TF2.x code:
https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L190

TF1.x code:
https://github.com/olegmyrk/SPADE-Tensorflow/blob/develop/SPADE.py#L190

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35414,no such package '@com_google_protobuf error while running bazel build,"**System information**
- OS Platform and Distribution: Windows10 x64
- TensorFlow installed from: binary
- TensorFlow version: 1.15.0
- Python version: 3.7
- Installed using: conda
- Bazel version (if compiling from source): 1.1/2.0
- CUDA/cuDNN version: 10.0

I was trying to inspect a model following the guide [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#inspecting-graphs)
When running the command
```bash
bazel build tensorflow/tools/graph_transforms:summarize_graph
```
it failed with these logs
```
INFO: Writing tracer profile to 'C:/users/yy/_bazel_yy/zxtlmlwl/command.profile.gz'
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/YY/Anaconda3/python.exe
INFO: Reading rc options for 'build' from e:\tensorflow\tensorflow\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --config=v2
INFO: Found applicable config definition build:v2 in file e:\tensorflow\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:windows in file e:\tensorflow\tensorflow\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --incompatible_windows_native_test_wrapper --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file e:\tensorflow\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Call stack for the definition of repository 'com_google_protobuf' which is a tf_http_archive (rule definition at E:/tensorflow/tensorflow/third_party/repo.bzl:121:19):
 - E:/tensorflow/tensorflow/tensorflow/workspace.bzl:457:5
 - E:/tensorflow/tensorflow/WORKSPACE:19:1
INFO: Repository 'com_google_protobuf' used the following cache hits instead of downloading the corresponding file.
 * Hash 'b9e92f9af8819bbbc514e2902aec860415b70209f31dfc8c4fa72515a5df9d59' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz
If the definition of 'com_google_protobuf' was updated, verify that the hashes were also updated.
ERROR: An error occurred during the fetch of repository 'com_google_protobuf':
   Traceback (most recent call last):
        File ""E:/tensorflow/tensorflow/third_party/repo.bzl"", line 101
                _apply_patch(ctx, <1 more arguments>)
        File ""E:/tensorflow/tensorflow/third_party/repo.bzl"", line 68, in _apply_patch
                _execute_and_check_ret_code(ctx, <1 more arguments>)
        File ""E:/tensorflow/tensorflow/third_party/repo.bzl"", line 52, in _execute_and_check_ret_code
                fail(<1 more arguments>)
Non-zero return code(2) when executing 'C:\Windows\system32\bash.exe -l -c ""patch"" ""-p1"" ""-d"" ""C:/users/yy/_bazel_yy/zxtlmlwl/external/com_google_protobuf"" ""-i"" ""E:/tensorflow/tensorflow/third_party/protobuf/protobuf.patch""':
Stdout:
Stderr: patch: **** Can't change to directory C:/users/yy/_bazel_yy/zxtlmlwl/external/com_google_protobuf : No such file or directory
ERROR: Analysis of target '//tensorflow/tools/graph_transforms:summarize_graph' failed; build aborted: no such package '@com_google_protobuf//': Traceback (most recent call last):
        File ""E:/tensorflow/tensorflow/third_party/repo.bzl"", line 101
                _apply_patch(ctx, <1 more arguments>)
        File ""E:/tensorflow/tensorflow/third_party/repo.bzl"", line 68, in _apply_patch
                _execute_and_check_ret_code(ctx, <1 more arguments>)
        File ""E:/tensorflow/tensorflow/third_party/repo.bzl"", line 52, in _execute_and_check_ret_code
                fail(<1 more arguments>)
Non-zero return code(2) when executing 'C:\Windows\system32\bash.exe -l -c ""patch"" ""-p1"" ""-d"" ""C:/users/yy/_bazel_yy/zxtlmlwl/external/com_google_protobuf"" ""-i"" ""E:/tensorflow/tensorflow/third_party/protobuf/protobuf.patch""':
Stdout:
Stderr: patch: **** Can't change to directory C:/users/yy/_bazel_yy/zxtlmlwl/external/com_google_protobuf : No such file or directory
INFO: Elapsed time: 3.975s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)
    currently loading: tensorflow
```

I've tried diferent versions of bazel, 0.20, 1.1.0, and 2.0.0，and ```bazel clean```, the error is still there.
What makes me confused is that the "" Can't change to directory C:/users/yy/_bazel_yy/zxtlmlwl/external/com_google_protobuf : No such file or directory"", which in fact I can find the dir at that path

I've checked other similar issue, but none can fix this error
"
35413,vgg19.preprocess_input doesn't work in TF2.1 autograph mode,"**System information**

* Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
* TensorFlow installed from (source or binary): binary
* TensorFlow version (use command below): v2.1.0-rc1-58-g9837ece 2.1.0-rc2 (python3 -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"")
* Python version: Python 3.6.8
* CUDA/cuDNN version: Driver Version: 440.33.01, CUDA Version: 10.2, cuDNN 7.6.2
* GPU model and memory: Tesla V100-SXM2-16GB

**Describe the current behavior**
Running 
```
tensorflow.keras.applications.vgg19.preprocess_input
```
inside @tf.function results in exception:
```
TypeError: An op outside of the function building code is being passed
a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: VGGLoss/Const:0
```

**Describe the expected behavior**
Should work the same as in tf1.x
https://github.com/olegmyrk/SPADE-Tensorflow/blob/develop/vgg19_keras.py#L15

**Code to reproduce the issue**
```self.vgg_loss = VGGLoss() ```
https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L631

```g_nondet_vgg_loss = self.vgg_loss(fake_nondet_x_output, real_x)```
https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L776

```g_det_vgg_loss = self.vgg_loss(real_x, fake_det_x_stats[0][0])```
https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L784

This is the line that fails:
```
x_vgg, y_vgg = self.vgg(preprocess_input(x)), self.vgg(preprocess_input(y))
```
https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/vgg19_keras.py#L15

I also tried to patch the code for preprocess_input myself
https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/vgg19_keras.py#L62
It somewhat works but judging by the scale of the VGG loss some input normalization is off.

**Other info / logs**
```
Traceback (most recent call last):
  File ""main.py"", line 134, in <module>
    main()
  File ""main.py"", line 121, in main
    gan.train()
  File ""/app/home/ubuntu/SPADE-Tensorflow.tf2/SPADE.py"", line 1180, in train
    build()
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 568, in __call__
    result = self._call(*args, **kwds)
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 632, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2363, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1611, in _filtered_call
    self.captured_inputs)
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1692, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 545, in call
    ctx=ctx)
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py"", line 76, in quick_execute
    raise e
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py"", line 61, in quick_execute
    num_outputs)
TypeError: An op outside of the function building code is being passed
a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: VGGLoss/Const:0
```"
35412,some problem about pb model,"when i want to load my pb modekl, i met a probelm that ""raise ValueError(""callback %s is not found"" % token) ValueError: callback pyfunc_31 is not found""

what is the pyfunc_31, why i can't load it, and how can i to slove it?"
35411,My GPU doesn't support CUDA,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 8.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): (I've use this) pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl
- TensorFlow version: 1.12.0
- Python version: 3.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: GeForce GTX 745


When I want to run test code I get error ""ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'""

I know that it is caused by lack of CUDA, but my GPU doesn't support it. What can I do with it?

```
Traceback (most recent call last):
  File ""C:\Users\Ja\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\Ja\AppData\Local\Programs\Python\Python38-32\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Ja\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Ja\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Ja\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/Ja/PycharmProjects/tf/tf.py"", line 10, in <module>
    import tensorflow as tf
  File ""C:\Users\Ja\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Ja\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Ja\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Ja\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\Ja\AppData\Local\Programs\Python\Python38-32\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Ja\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Ja\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Ja\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors
```


"
35409,ImportError: DLL load failed with error code -1073741795,"Traceback (most recent call last):
  File ""C:\Users\Hieu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Hieu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Hieu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Hieu\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""fas.py"", line 3, in <module>
    import tensorflow as tf
  File ""C:\Users\Hieu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Hieu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Hieu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Hieu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Hieu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Hieu\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Hieu\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: DLL load failed with error code -1073741795"
35408,Cannot build gl_delegate of android,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu18.04.3 LTS, 64bits
- TensorFlow installed from: https://github.com/tensorflow/tensorflow, 
- TensorFlow version: 8e8fabfee3
- Python version: Python 3.6.9 :: Anaconda, Inc.
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 1.1.0
- GCC/Compiler version (if compiling from source): Not compiled from source
- CUDA/cuDNN version: None
- GPU model and memory: None



**Describe the problem**

I can build the libtensorflow-lite.a. by 

`./tensorflow/lite/tools/make/build_aarch64_lib.sh`

But cannot build the gl_delegate by

`bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:gl_delegate`

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Install tensorflow and build the libtensorflow-lite.a by the instructions from [here](https://www.tensorflow.org/lite/guide/build_arm64)
2. Install bazel by the instructions from [here](https://docs.bazel.build/versions/master/install-ubuntu.html) 
3. Install anaconda, create environment
`conta create --name tensorflow python=3.6`
4. Install tensorflow and others tools by conda
5. Build gl_delegate by `bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:gl_delegate`

**Any other info / logs**

> NFO: Writing tracer profile to '/home/yyyy/.cache/bazel/_bazel_yyyy/9f4180a9f81bdaca56a8020b37cc35bc/command.profile.gz'
> INFO: Options provided by the client:
>   Inherited 'common' options: --isatty=1 --terminal_columns=111
> INFO: Reading rc options for 'build' from /home/yyyy/Qt/3rdLibs/tensorflow/.bazelrc:
>   'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --config=v2
> INFO: Found applicable config definition build:v2 in file /home/yyyy/Qt/3rdLibs/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
> INFO: Found applicable config definition build:android_arm64 in file /home/yyyy/Qt/3rdLibs/tensorflow/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a
> INFO: Found applicable config definition build:android in file /home/yyyy/Qt/3rdLibs/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain
> INFO: Found applicable config definition build:linux in file /home/yyyy/Qt/3rdLibs/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
> INFO: Found applicable config definition build:dynamic_kernels in file /home/yyyy/Qt/3rdLibs/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
> DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
> DEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at /home/yyyy/.cache/bazel/_bazel_yyyy/9f4180a9f81bdaca56a8020b37cc35bc/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):
>  - /home/yyyy/.cache/bazel/_bazel_yyyy/9f4180a9f81bdaca56a8020b37cc35bc/external/bazel_toolchains/repositories/repositories.bzl:37:9
>  - /home/yyyy/Qt/3rdLibs/tensorflow/WORKSPACE:37:1
> ERROR: /home/yyyy/.cache/bazel/_bazel_yyyy/9f4180a9f81bdaca56a8020b37cc35bc/external/local_config_cc/BUILD:47:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'
> ERROR: Analysis of target '//tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_gl.so' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed; build aborted
> INFO: Elapsed time: 0.450s
> INFO: 0 processes.
> FAILED: Build did NOT complete successfully (1 packages loaded, 1 target configured)"
35407,At Runtime: Error while reading resource variable,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow version (use command below): 2.0
- Python version: 3.7
- CUDA/cuDNN version: 10.2/7
- GPU model and memory: RTX 2060 

**Describe the current behavior**
I am getting this error when trying to modify https://github.com/eriklindernoren/Keras-GAN/blob/master/wgan_gp/wgan_gp.py:

```
Error while reading resource variable _AnonymousVar35 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar35/class tensorflow::Var does not exist.
	 [[node mul_1/ReadVariableOp (defined at C:\Users\Harry\Anaconda3\lib\site-packages\tensorflow_core\python\framework\ops.py:1751) ]] [Op:__inference_keras_scratch_graph_8343]

Function call stack:
keras_scratch_graph
```

**Code to reproduce the issue**
```python
from keras.models import Model
from keras.layers import Lambda, Dense, LSTM, Activation, Input, Bidirectional, Dropout, Reshape, Conv2DTranspose, TimeDistributed, Conv1D
from keras.layers.advanced_activations import LeakyReLU
from keras.optimizers import Adam
import keras.backend as K
from keras.layers.merge import _Merge
import librosa
import tensorflow as tf
from functools import partial
import sys
import os
import numpy as np

gpus = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(gpus[0], True)

class RandomWeightedAverage(_Merge):
    """"""Provides a (random) weighted average between real and generated image samples""""""
    def _merge_function(self, inputs):
        alpha = K.random_uniform((32, 1, 1, 1))
        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])

class WGANGP():
    def __init__(self):
        self.latent_dim = 100
        self.d = 64
        self.c = 16
        self.a = 1
        self.Fs = 44100

        # Following parameter and optimizer set as recommended in paper
        self.n_critic = 5
        optimizer = Adam(lr=1e-4, beta_1=0.5, beta_2=0.9)

        # Build the generator and critic
        self.generator = self.build_generator()
        self.critic = self.build_critic()

        #-------------------------------
        # Construct Computational Graph
        #       for the Critic
        #-------------------------------

        # Freeze generator's layers while training critic
        self.generator.trainable = False

        # Image input (real sample)
        real_audio = Input(shape=(self.a*256*self.d, 1))

        # Noise input
        z_disc = Input(shape=(self.a, self.latent_dim))
        # Generate image based of noise (fake sample)
        fake_audio = self.generator(z_disc)

        # Discriminator determines validity of the real and fake images
        fake = self.critic(fake_audio)
        valid = self.critic(real_audio)

        # Construct weighted average between real and fake images
        interpolated_audio = RandomWeightedAverage()([real_audio, fake_audio])
        # Determine validity of weighted sample
        validity_interpolated = self.critic(interpolated_audio)

        # Use Python partial to provide loss function with additional
        # 'averaged_samples' argument
        partial_gp_loss = partial(self.gradient_penalty_loss,
                          averaged_samples=interpolated_audio)
        partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names

        self.critic_model = Model(inputs=[real_audio, z_disc],
                            outputs=[valid, fake, validity_interpolated])
        self.critic_model.compile(loss=[self.wasserstein_loss,
                                              self.wasserstein_loss,
                                              partial_gp_loss],
                                        optimizer=optimizer,
                                        loss_weights=[1, 1, 10])
        #-------------------------------
        # Construct Computational Graph
        #         for Generator
        #-------------------------------

        # For the generator we freeze the critic's layers
        self.critic.trainable = False
        self.generator.trainable = True

        # Sampled noise for input to generator
        z_gen = Input(shape=(self.a, self.latent_dim))
        # Generate images based of noise
        audio = self.generator(z_gen)
        # Discriminator determines validity
        valid = self.critic(audio)
        # Defines generator model
        self.generator_model = Model(z_gen, valid)
        self.generator_model.compile(loss=self.wasserstein_loss, optimizer=optimizer)


    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):
        """"""
        Computes gradient penalty based on prediction and weighted real / fake samples
        """"""
        gradients = K.gradients(y_pred, averaged_samples)[0]
        # compute the euclidean norm by squaring ...
        gradients_sqr = K.square(gradients)
        #   ... summing over the rows ...
        gradients_sqr_sum = K.sum(gradients_sqr,
                                  axis=np.arange(1, len(gradients_sqr.shape)))
        #   ... and sqrt
        gradient_l2_norm = K.sqrt(gradients_sqr_sum)
        # compute lambda * (1 - ||grad||)^2 still for each single sample
        gradient_penalty = K.square(1 - gradient_l2_norm)
        # return the mean as loss over all the batch samples
        return K.mean(gradient_penalty)


    def wasserstein_loss(self, y_true, y_pred):
        return K.mean(y_true * y_pred)

    def apply_phaseshuffle(self, x, rad=2, pad_type='reflect'):
        b, x_len, nch = x.get_shape().as_list()

        phase = tf.random.uniform([], minval=-rad, maxval=rad + 1, dtype=tf.int32)
        pad_l = tf.maximum(phase, 0)
        pad_r = tf.maximum(-phase, 0)
        phase_start = pad_r
        x = tf.pad(x, [[0, 0], [pad_l, pad_r], [0, 0]], mode=pad_type)

        x = x[:, phase_start:phase_start+x_len]
        x.set_shape([b, x_len, nch])

        return x

    def build_generator(self):
        d=self.d
        c=self.c
        a=self.a

        # Prelim layers
        input_layer = Input(shape=(a, 100))

        dense_layer0 = TimeDistributed(Dense(256*d, input_shape=(100,)))(input_layer)#
        reshape_layer0 = TimeDistributed(Reshape((c, c*d)))(dense_layer0)#
        relu_layer0 = TimeDistributed(Activation('relu'))(reshape_layer0)#

        # WaveCNN layers
        c //= 2
        expanded_layer0 = TimeDistributed(Lambda(lambda x: K.expand_dims(x, axis=1)))(relu_layer0)#relu_layer1
        conv1d_t_layer0 = TimeDistributed(Conv2DTranspose(c*d, (1, 25), strides=(1, 4), padding='same'))(expanded_layer0)
        slice_layer0 = Lambda(lambda x: x[:, :, 0])(conv1d_t_layer0)
        relu_layer2 = TimeDistributed(Activation('relu'))(slice_layer0)

        c //= 2
        expanded_layer1 = TimeDistributed(Lambda(lambda x: K.expand_dims(x, axis=1)))(relu_layer2)
        conv1d_t_layer1 = TimeDistributed(Conv2DTranspose(c*d, (1, 25), strides=(1, 4), padding='same'))(expanded_layer1)
        slice_layer1 = Lambda(lambda x: x[:, :, 0])(conv1d_t_layer1)
        relu_layer3 = TimeDistributed(Activation('relu'))(slice_layer1)

        c //= 2
        expanded_layer2 = TimeDistributed(Lambda(lambda x: K.expand_dims(x, axis=1)))(relu_layer3)
        conv1d_t_layer2 = TimeDistributed(Conv2DTranspose(c*d, (1, 25), strides=(1, 4), padding='same'))(expanded_layer2)
        slice_layer2 = Lambda(lambda x: x[:, :, 0])(conv1d_t_layer2)
        relu_layer4 = TimeDistributed(Activation('relu'))(slice_layer2)

        c //= 2
        expanded_layer3 = TimeDistributed(Lambda(lambda x: K.expand_dims(x, axis=1)))(relu_layer4)
        conv1d_t_layer3 = TimeDistributed(Conv2DTranspose(c*d, (1, 25), strides=(1, 4), padding='same'))(expanded_layer3)
        slice_layer3 = Lambda(lambda x: x[:, :, 0])(conv1d_t_layer3)
        relu_layer5 = TimeDistributed(Activation('relu'))(slice_layer3)

        expanded_layer4 = TimeDistributed(Lambda(lambda x: K.expand_dims(x, axis=1)))(relu_layer5)
        conv1d_t_layer4 = TimeDistributed(Conv2DTranspose(1, (1, 25), strides=(1, 4), padding='same'))(expanded_layer4)#strides=(1,1)
        slice_layer4 = Lambda(lambda x: x[:, :, 0])(conv1d_t_layer4)
        tanh_layer0 = TimeDistributed(Activation('tanh'))(slice_layer4)

        reshape_layer1 = Reshape((a*256*d, 1))(tanh_layer0)

        model = Model(inputs=input_layer, outputs=reshape_layer1)

        print(model.summary())

        return model

    def build_critic(self):
        d=self.d
        c=self.c
        a=self.a

        input_layer = Input(shape=(a*256*d, 1))#d*d
        reshape_layer0 = Reshape((a, 256*d, 1))(input_layer)

        conv1d_layer0 = TimeDistributed(Conv1D(d, 25, strides=4, padding='same'))(reshape_layer0)#//2
        LReLU_layer0 = TimeDistributed(LeakyReLU(alpha=0.2))(conv1d_layer0)
        phaseshuffle_layer0 = TimeDistributed(Lambda(lambda x: self.apply_phaseshuffle(x)))(LReLU_layer0)

        conv1d_layer1 = TimeDistributed(Conv1D(2*d, 25, strides=4, padding='same'))(phaseshuffle_layer0)#d
        LReLU_layer1 = TimeDistributed(LeakyReLU(alpha=0.2))(conv1d_layer1)
        phaseshuffle_layer1 = TimeDistributed(Lambda(lambda x: self.apply_phaseshuffle(x)))(LReLU_layer1)

        conv1d_layer2 = TimeDistributed(Conv1D(4*d, 25, strides=4, padding='same'))(phaseshuffle_layer1)#2*d
        LReLU_layer2 = TimeDistributed(LeakyReLU(alpha=0.2))(conv1d_layer2)
        phaseshuffle_layer2 = TimeDistributed(Lambda(lambda x: self.apply_phaseshuffle(x)))(LReLU_layer2)

        conv1d_layer3 = TimeDistributed(Conv1D(8*d, 25, strides=4, padding='same'))(phaseshuffle_layer2)#4*d
        LReLU_layer3 = TimeDistributed(LeakyReLU(alpha=0.2))(conv1d_layer3)
        phaseshuffle_layer3 = TimeDistributed(Lambda(lambda x: self.apply_phaseshuffle(x)))(LReLU_layer3)

        conv1d_layer4 = TimeDistributed(Conv1D(16*d, 25, strides=4, padding='same'))(phaseshuffle_layer3)#8*d,strides=4
        LReLU_layer4 = TimeDistributed(LeakyReLU(alpha=0.2))(conv1d_layer4)
        phaseshuffle_layer4 = TimeDistributed(Lambda(lambda x: self.apply_phaseshuffle(x)))(LReLU_layer4)
    
        reshape_layer1 = Reshape((a, 256*d))(phaseshuffle_layer4)
        slice_layer0 = Lambda(lambda x: x[:, 0])(reshape_layer1)#

        dense_layer1 = Dense(1, input_shape=(a, 256*d))(slice_layer0)#dropout_layer1

        model = Model(inputs=input_layer, outputs=dense_layer1)

        print(model.summary())

        return model

    def train(self, epochs, batch_size, sample_interval=50):

        X_train = []

        for file in os.listdir(r""C:\Users\Harry\source\repos\tfworldhackathon\Data""):
            with open(r""C:\Users\Harry\source\repos\tfworldhackathon\Data"" + fr""\{file}"", ""rb"") as f:
                samples, _ = librosa.load(f, sr=self.Fs)
                X_train.append(np.array([np.array([sample]) for sample in samples[:self.a*256*self.d]]))
                if ""17"" in file:
                    break

        X_train = np.array(X_train)

        print(X_train.shape)

        # Adversarial ground truths
        valid = -np.ones((batch_size, 1))
        fake =  np.ones((batch_size, 1))
        dummy = np.zeros((batch_size, 1)) # Dummy gt for gradient penalty
        for epoch in range(epochs):

            for _ in range(self.n_critic):

                # ---------------------
                #  Train Discriminator
                # ---------------------

                # Select a random batch of images
                idx = np.random.randint(0, X_train.shape[0], batch_size)
                audios = X_train[idx]
                # Sample generator input
                noise = np.random.normal(0, 1, (batch_size, self.a, self.latent_dim))
                # Train the critic
                d_loss = self.critic_model.train_on_batch([audios, noise],
                                                                [valid, fake, dummy])

            # ---------------------
            #  Train Generator
            # ---------------------

            g_loss = self.generator_model.train_on_batch(noise, valid)

            # Plot the progress
            print (""%d [D loss: %f] [G loss: %f]"" % (epoch, d_loss[0], g_loss))

            # If at save interval => save generated image samples
            if epoch % sample_interval == 0:
                self.sample_audio(epoch)

    def sample_audio(self, epoch):
        print(f""Checkpoint {epoch}"")
        noise = np.random.normal(0, 1, (5, self.a, self.latent_dim))
        gen_audios = self.generator.predict(noise)
        for i in range(len(gen_audios)):
            audio = gen_audios[i]
            audio.flatten()
            librosa.output.write_wav(f""output/{epoch}-{i}.wav"", audio, sr=self.Fs)


if __name__ == '__main__':
    wgan = WGANGP()
    wgan.train(epochs=30000, batch_size=8, sample_interval=100)
```

**Other info / logs**
I have not seen any solutions to this issue for Tensorflow 2. 
This error does not throw if I do not train the generator. This leads me to believe there is an issue when oscillating between models getting trained, not the actual training of a given model.

To restate, the error occurs on the line `d_loss = self.critic_model.train_on_batch([audios, noise], [valid, fake, dummy]) ` on the second epoch. So it seems like the following happens:

- First epoch
   - Train discriminator successfully 
   - Train generator successfully 
- Second epoch
   - Train discriminator fails
"
35406,"import frozen graph with error ""Input 0 of node X was passed float from Y:0 incompatible with expected float_ref","It's the same problem with [this early issue](https://github.com/onnx/tensorflow-onnx/issues/77)

However I can not solve this problem like it.

Bacause there is no ""Ref"" node and ""AssignSub"" node and ""AssignAdd"" node in my pb file.


```bash
Input 0 of node MelGAN/Generator/Generator_1thconv1d/weight_norm/Assign was passed float from MelGAN/Generator/Generator_1thconv1d/group__conv1d/g:0  incompatible with expected float_ref
```

My pb file is in [my repository](https://github.com/MachineJeff/BUG)

And the ""print node"" code is also in there.

Who can help me ?"
35405,tensorflowlib for 2.0,"I can see many users over the last 2 weeks struggling with getting the build to work with Windows 10. I also see a few **recent commits** to improve the build in response to the above feedback.

As [stated here](https://www.tensorflow.org/install/source_windows) that the Windows tensorflow 2.0 builds with following configurations are successful.

![image](https://user-images.githubusercontent.com/59223977/71443261-34f74d00-270a-11ea-8cc5-ee2902f336b8.png)
![image](https://user-images.githubusercontent.com/59223977/71443269-44769600-270a-11ea-8857-750f7f3598ee.png)

Please **update** [the links](https://www.tensorflow.org/install/lang_c) here with the successful windows tensorflow 2.0 builds provided above 

For example: we need
https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.0.0.zip
https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-2.0.0.zip

_Although these tentative builds are old_, but it will provide a starting point **to start TESTING the Tensorflow 2.0 features through bindings.**"
35404,ValueError: Flattening a PerReplica to components is not supported in replica context.,"**System information**
- windows
- TensorFlow installed from conda
- TensorFlow version:2.0
- Python version:3.7.4
- CUDA/cuDNN version: 10.2/7.6
- GPU model and memory: 2 nvidia rtx 2070s 8GB

**Describe the current behavior**
i follow the distributed training tutorials to change my code(custom model) for cumtom training loop. but when i run the script, it shows mistake ""ValueError: Flattening a PerReplica to components is not supported in replica context."". i don't understande why it is happens.  it can run in the train step,  but it can't run in the test step
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
# i use this strategy before,but it make the same mistake
# strategy = tf.distribute.MirroredStrategy(
#     cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())

strategy = tf.distribute.MirroredStrategy(
        cross_device_ops=tf.distribute.ReductionToOneDevice(""/device:CPU:0""))

def train_step_fn(rgb, spec):
            with tf.GradientTape() as tape:
                fake_spec = model(rgb, training=True)
                loss = compute_loss(spec, fake_spec)

            gradients = tape.gradient(loss, model.trainable_variables)
            opt.apply_gradients(zip(gradients, model.trainable_variables))
            # update metrics
            rmse1.update_state(spec, fake_spec)
            rmse2.update_state(spec, fake_spec)
            rrmse1.update_state(spec, fake_spec)
            rrmse2.update_state(spec, fake_spec)
            sam.update_state(spec, fake_spec)
            return loss

def test_step_fn(rgb, sepc):
            fake_spec = model(rgb, training=False)
            loss = loss_object(spec, fake_spec)
            # update metrics
            rmse1.update_state(spec, fake_spec)
            rmse2.update_state(spec, fake_spec)
            rrmse1.update_state(spec, fake_spec)
            rrmse2.update_state(spec, fake_spec)
            sam.update_state(spec, fake_spec)
            return loss

@tf.function
def distributed_train_step(rgb, spec):
            per_replica_losses = strategy.experimental_run_v2(train_step_fn,
                                                              args=(rgb, spec))
            return strategy.reduce(tf.distribute.ReduceOp.SUM,
                                   per_replica_losses,
                                   axis=None)

@tf.function
def distributed_test_step(rgb, spec):
            return strategy.experimental_run_v2(test_step_fn, args=(rgb, spec))

for epoch in range(parser.epochs):
            # train
            for step, (rgb, spec) in enumerate(datas[0]):
                train_mean_loss = distributed_train_step(rgb, spec)
                steps += 1
                ckpt.steps.assign(steps)
                if step == 50:
                    break
            # val
             rmse1.reset_state()
             rmse2.reset_state()
             rrmse1.reset_state()
             rrmse2.reset_state()
             sam.reset_state()
            for step, (rgb, spec) in enumerate(datas[1]):

                test_mean_loss = distributed_test_step(rgb, spec)
```
**Other info / logs**
```
ValueError: in converted code:
    mytrainT.py:163 distributed_test_step  *
        return strategy.experimental_run_v2(test_step_fn, args=(rgb, spec))
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py:760 experimental_run_v2
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    mytrainT.py:144 test_step_fn  *
        loss = loss_object(spec, fake_spec)
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\keras\losses.py:125 __call__
        with K.name_scope(scope_name or self.__class__.__name__), graph_ctx:
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\contextlib.py:112 __enter__
        return next(self.gen)
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\keras\utils\tf_utils.py:435 graph_context_for_symbolic_tensors
        if any(is_symbolic_tensor(v) for v in list(args) + list(kwargs.values())):
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\keras\utils\tf_utils.py:435 <genexpr>
        if any(is_symbolic_tensor(v) for v in list(args) + list(kwargs.values())):
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\keras\utils\tf_utils.py:345 is_symbolic_tensor
        return tensor._is_graph_tensor  # pylint: disable=protected-access
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\framework\composite_tensor.py:119 _is_graph_tensor
        components = self._type_spec._to_components(self)  # pylint: disable=protected-access
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\distribute\values.py:500 _to_components
        ""Flattening a PerReplica to components is not supported in replica ""

    ValueError: Flattening a PerReplica to components is not supported in replica context.
```"
35403,using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.,"something is wrong when I use model.get_layer(),
I was confused what's wrong with my code, and I have never used a `tf.Tensor` as a Python `bool` in my code

Here are my code:
------------------------------------
```python
import tensorflow as tf
from tensorflow.keras import layers

def seq(k):
    result = tf.keras.Sequential(name='seq'+str(k))
    result.add(tf.keras.layers.Conv2D(64, (3, 3), strides=2, padding='same', use_bias=False))
    result.add(tf.keras.layers.BatchNormalization(name='bn_'+str(k)))
    result.add(tf.keras.layers.GaussianNoise(stddev=1, name='noise_'+str(k)))
    result.add(tf.keras.layers.ReLU())

    return result


def testModel():
    input_layer = layers.Input(shape=(256, 256, 3))
    l0 = seq(0)(input_layer)
    l1 = seq(1)(l0)
    l2 = seq(2)(l1)
    model = tf.keras.Model(input_layer, l2)
    return model


if __name__ == '__main__':
    model = testModel()
    model.summary()
    seq0 = model.get_layer('seq_0')
    bn0 = seq0.get_layer('bn_0').output
    noise0 = seq0.get_layer('noise_0').output

    sub0 = tf.keras.layers.Subtract()([noise0, bn0])

    new_model = tf.keras.Model(model.input, [sub0, model.output])

    test_tensor = tf.ones(shape=(1, 256, 256, 3))

    (out, z) = new_model(test_tensor)
```
----------------------------------------------------------------
2019-12-25 15:15:54.403715: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Traceback (most recent call last):
  File ""/home/chenhao-gpu/PycharmProjects/GAN/test.py"", line 36, in <module>
    (out, z) = new_model(test_tensor)
  File ""/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 891, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py"", line 708, in call
    convert_kwargs_to_constants=base_layer_utils.call_context().saving)
  File ""/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py"", line 860, in _run_internal_graph
    output_tensors = layer(computed_tensors, **kwargs)
  File ""/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 891, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/normalization.py"", line 659, in call
    outputs = self._fused_batch_norm(inputs, training=training)
  File ""/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/normalization.py"", line 517, in _fused_batch_norm
    training, _fused_batch_norm_training, _fused_batch_norm_inference)
  File ""/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/tf_utils.py"", line 59, in smart_cond
    pred, true_fn=true_fn, false_fn=false_fn, name=name)
  File ""/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/framework/smart_cond.py"", line 59, in smart_cond
    name=name)
  File ""/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 1201, in cond
    if pred:
  File ""/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 765, in __bool__
    self._disallow_bool_casting()
  File ""/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 534, in _disallow_bool_casting
    self._disallow_in_graph_mode(""using a `tf.Tensor` as a Python `bool`"")
  File ""/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 523, in _disallow_in_graph_mode
    "" this function with @tf.function."".format(task))
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function."
35402,TF2.0 hub Universal Sentence Encoder Multilingual Sentenepieceop not registered problem,"
**System information**
- Have I written custom code : No
- OS Platform and Distribution : Windows 10 / Google Colab
- TensorFlow version (use command below):tensorflow==2.0.0
- Python version:Python 3.6.9

Here is my code.

```

import tensorflow as tf
import tensorflow_hub as hub
import tf_sentencepiece
embedding_layer = hub.load(""https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3"")

```
Problematic output

```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in _get_op_def(self, type)
   3819     try:
-> 3820       return self._op_def_cache[type]
   3821     except KeyError:

KeyError: 'SentencepieceOp'

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
8 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in _get_op_def(self, type)
   3822       with c_api_util.tf_buffer() as buf:
   3823         # pylint: disable=protected-access
-> 3824         c_api.TF_GraphGetOpDef(self._c_graph, compat.as_bytes(type), buf)
   3825         # pylint: enable=protected-access
   3826         data = c_api.TF_GetBuffer(buf)

NotFoundError: Op type not registered 'SentencepieceOp' in binary running on e2f7765a82a9. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.

```


I want to get multilingual sentence embeddings using pretrained Google Universal sentence encoder.  But I could not able to get it from Windows 10 and Google Colab even if the internet shows diffrent. I was able to get sentence vectors using diffrent hub link like https://tfhub.dev/google/universal-sentence-encoder/4. That is not dependent to sentencepiece."
35397,no such target '//tensorflow:windows': target 'windows' not declared in package,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from source:
- TensorFlow version 1.13.1:
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?:anaconda3
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source):4.8
- CUDA/cuDNN version: CUDA10, cuDNN7.4
- GPU model and memory: GPU memory 22G, model bert-base-chinese



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build :loader --config=monolithic

**Any other info / logs**
![image](https://user-images.githubusercontent.com/11387828/71431384-6bf93e80-26c9-11ea-9b98-f915b0e8d17b.png)
i don‘t know what happen, i build a tensor add operator demo and it success, but when i load pb model it fails and i rowback to the demo it never success again and report such error, how should i do?"
35393,TF 2.0 Custom Metric 'Tensor' object has no attribute 'numpy' ,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Windows 10
- TensorFlow installed from: pip
- TensorFlow version: tensorflow-gpu 2.0.0
- Python version: 3.7.3

**Code**

model.py
```
from metrics import r2_score


def create_model(input_shape, output_shape):
    model = models.Sequential([
        layers.LSTM(64, return_sequences=True, input_shape=input_shape),
        layers.LSTM(16, activation='relu'),
        layers.Dense(output_shape)
    ])

    model.compile(optimizer='adam', loss='mse', metrics=[r2_score])

    return model
```

metrics.py
```
from sklearn.metrics import r2_score as skl_r2_score


def r2_score(y_true, y_pred):
    return skl_r2_score(y_true.numpy(), y_pred.numpy())

```
**Current behavior**

```
Traceback (most recent call last):
  File ""train.py"", line 38, in <module>
    model = load_model(args.model_path, (history_length, len(series_features)), forecast_length)
  File ""C:\Users\haaka\ML\goog\model.py"", line 24, in load_model
    model = create_model(input_shape, output_shape)
  File ""C:\Users\haaka\ML\goog\model.py"", line 15, in create_model
    model.compile(optimizer='adam', loss='mse', metrics=[r2_score])
  File ""C:\Users\haaka\ML\venv\lib\site-packages\tensorflow_core\python\training\tracking\base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""C:\Users\haaka\ML\venv\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 366, in compile
    masks=self._prepare_output_masks())
  File ""C:\Users\haaka\ML\venv\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2063, in _handle_metrics
    target, output, output_mask))
  File ""C:\Users\haaka\ML\venv\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2014, in _handle_per_output_metrics
    metric_fn, y_true, y_pred, weights=weights, mask=mask)
  File ""C:\Users\haaka\ML\venv\lib\site-packages\tensorflow_core\python\keras\engine\training_utils.py"", line 1067, in call_metric_function
    return metric_fn(y_true, y_pred, sample_weight=weights)
  File ""C:\Users\haaka\ML\venv\lib\site-packages\tensorflow_core\python\keras\metrics.py"", line 193, in __call__
    replica_local_fn, *args, **kwargs)
  File ""C:\Users\haaka\ML\venv\lib\site-packages\tensorflow_core\python\keras\distribute\distributed_training_utils.py"", line 1135, in call_replica_local_fn
    return fn(*args, **kwargs)
  File ""C:\Users\haaka\ML\venv\lib\site-packages\tensorflow_core\python\keras\metrics.py"", line 176, in replica_local_fn
    update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable
  File ""C:\Users\haaka\ML\venv\lib\site-packages\tensorflow_core\python\keras\utils\metrics_utils.py"", line 75, in decorated
    update_op = update_state_fn(*args, **kwargs)
  File ""C:\Users\haaka\ML\venv\lib\site-packages\tensorflow_core\python\keras\metrics.py"", line 581, in update_state
    matches = self._fn(y_true, y_pred, **self._fn_kwargs)
  File ""C:\Users\haaka\ML\goog\metrics.py"", line 7, in r2_score
    return skl_r2_score(y_true.numpy(), y_pred.numpy())
AttributeError: 'Tensor' object has no attribute 'numpy'
```


In the issue [#27519](https://github.com/tensorflow/tensorflow/issues/27519) (which has been closed and does not offer a solution to my problem) others described a problem similar to mine where they were trying to implement a custom metric and encountered the error `'Tensor' object has no attribute 'numpy'`. I believe this is a bug because in TF 2.0 eager execution is enabled by default so I should not be receiving this error."
35389,ERROR: Next operations are not supported by GPU delegate.,"INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
ERROR: Next operations are not supported by GPU delegate:
CONCATENATION: 
CONV_2D: 
CUSTOM TFLite_Detection_PostProcess: Operation is not supported.
DEPTHWISE_CONV_2D: 
LOGISTIC: 
RESHAPE: 
First 0 operations will run on the GPU, and the remaining 64 on the CPU.


I've converted model to tflite and i'm getting this error. Can anybody help me why i'm getting this and/or what could be the possible reason?

Here's the code:

bool ObjectDetector::init(const std::string &model_file, bool is_quantized,
                          const std::string &labels_file)
{
    // Load model.
    model_ = tflite::FlatBufferModel::BuildFromFile(model_file.c_str());
    if (!model_)
    {
        LOG(ERROR) << ""Failed to load model: "" << model_file;
        return false;
    }

    // Create interpreter.
    tflite::ops::builtin::BuiltinOpResolver resolver;
    tflite::InterpreterBuilder(*model_, resolver)(&interpreter_);
    if (!interpreter_)
    {
        LOG(ERROR) << ""Failed to create interpreter!"";
        return false;
    }
    /*if (interpreter_->AllocateTensors() != kTfLiteOk)
    {
        LOG(ERROR) << ""Failed to allocate tensors!"";
        return false;
    }
    const TfLiteGpuDelegateOptions options = {
    .metadata = NULL,
    .compile_options = {
        .precision_loss_allowed = 1,  // FP16
        .preferred_gl_object_type = TFLITE_GL_OBJECT_TYPE_FASTEST,
        .dynamic_batch_enabled = 0,   // Not fully functional yet
    },
    };*/
    auto* delegate = TfLiteGpuDelegateCreate(/*default options=*/nullptr);
    if (interpreter_->ModifyGraphWithDelegate(delegate) != kTfLiteOk) return false;
    //LOG(ERROR) << ""DetectEmptyParkingSlots:: TfLiteGpuDelegateCreate Passed!!! Hurray!....."" << std::endl;
    
    //interpreter_->SetNumThreads(1);
    interpreter_->UseNNAPI(1);



    // Find input tensors.
    if (interpreter_->inputs().size() != 1)
    {
        LOG(ERROR) << ""Graph needs to have 1 and only 1 input!"";
        return false;
    }
    input_tensor_ = interpreter_->tensor(interpreter_->inputs()[0]);
    if (is_quantized)
    {
        if (input_tensor_->type != kTfLiteUInt8)
        {
            LOG(ERROR) << ""Quantized graph's input should be kTfLiteUInt8!"";
            return false;
        }
    }
    else
    {
        if (input_tensor_->type != kTfLiteFloat32)
            LOG(ERROR) << ""Quantized graph's input should be kTfLiteFloat32!"";
        {
            return false;
        }
    }

    // Find output tensors.
    if (interpreter_->outputs().size() != 4)
    {
        LOG(ERROR) << ""Graph needs to have 4 and only 4 outputs!"";
        return false;
    }
    output_locations_ = interpreter_->tensor(interpreter_->outputs()[0]);
    output_classes_ = interpreter_->tensor(interpreter_->outputs()[1]);
    output_scores_ = interpreter_->tensor(interpreter_->outputs()[2]);
    num_detections_ = interpreter_->tensor(interpreter_->outputs()[3]);

    std::vector<std::string> labels;
    ReadLines(labels_file, &labels);
    labels_ = labels;
    return true;
}
"
35387,TF1.14.0 C++  CPU single thread ,"I compiling tensorflow library (libtensorflow_cc.so libtensorflow_framework.so) by bazel，and used it in CPU mode. I want to run the program linked by the built tf library in one thread. The configuration bellow is set.
      
     

- SessionOptions opts;

- opts.config.set_inter_op_parallelism_threads(1)

- opts.config.set_intra_op_parallelism_threads(1)


But the number of the threads  is 4. except the main thread, there are 3 threads, which be created by the bellow function:

- tensorflow::thread::EigenEnvironment::CreateThread()

- Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)

how to close these three threads?
"
35386,Use tf-lite C++ API for both Android and iOS,"**System information**
- OS Platform and Distribution: macOS 10.14.5
- TensorFlow installed from (source or binary): source
- TensorFlow version: v.2.0
- Python version: 3.7
- Installed using virtualenv? pip? conda?: source
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): clang version 7.0.1

I am a mobile developer, using C++ to build portable parts of my apps that work on both Android and iOS.
Here is my workflow:
- For portable parts, I code a cross-platform library on macOS, build/unit-test it on macOS
- then, build/unit-test it on Linux
- create android and ios builds of the library using the respective toolchains, and integrate it into the mobile apps

While exploring tflite, I can see multiple APIs: C, C++, Obj-C, Swift, Java.
To write portable code, Obj-C/Swift/Java APIs are out of picture.
Is there any common API (C or C++) which can be used to evaluate tf-lite models on macOS, Linux, Android and iOS? If yes, what are the bazel targets to build?"
35385,"Interesting bug, in keras, tf.shape not compatible with tensor.get_shape()","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04
- TensorFlow installed from (source or binary):conda binary
- TensorFlow version (use command below):2.0
- Python version:3.7
- CUDA/cuDNN version:430
- GPU model and memory:2 * 24GB Titan RTX

I found this bug is very interesting.

For example:

> a = tf.ones([1, 512, 512, 3])
> b = tf.ones([1, 256, 256, 3])
> c = tf.ones([1, 1, 1, 3])
> 
> a_size = tf.shape(a)[1:3]
> b = tf.resize(b, size = a_size)
> 
> b_size = b.get_shape()[1:3]
> c = tf.resize(c, size = b_size) # Bug line

In the last line (bug line). if the model is run eagerly, it is okay. However, if run with keras.fit (Graph mode I guess), it will give the error ""**raise ValueError('\'size\' must be a 1-D int32 Tensor')**""



"
35384,TF1.15 Distribute Mode Error ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos7.6
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): TF1.15
- Python version: python3.6
- CUDA/cuDNN version: 10.0/7.6.2

**Code**

 mirror_strategy = tf.distribute.MirroredStrategy()
 with mirror_strategy.scope():
    model = test_net()
    sgd = tf.keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-4)
    model.compile(
        optimizer=sgd, loss=""sparse_categorical_crossentropy"", metrics=[""accuracy""]
    )

    model.fit(
        x_train, 
        y_train,
        batch_size=256,
        epochs=10,
        validation_data=(x_test, y_test),
        verbose=2,
    )

**The Error Information**

No registered 'MultiDeviceIteratorGetNextFromShard' OpKernel for GPU devices compatible with node {{node MultiDeviceIteratorGetNextFromShard}}
        .  Registered:  device='CPU'
"
35383,Enable SO_REUSEPORT option in tensorflow training server,"**System information**
- TensorFlow version (you are using): 1.15 and >=2.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Add SO_REUSEPORT option when starting tensorflow training server. It will enable to scan ports to build TF_CONFIG env variable. It is necessary to use distributed tensorflow with resource managers that do not reserve ports (such as Yarn).

It already has been discussed in ticket https://github.com/tensorflow/tensorflow/issues/21492. It is unclear why the option has been disabled in https://github.com/tensorflow/tensorflow/commit/8cf38e81e638db173238a8f95d6ea613c24d3d9f

**Will this change the current api? How?**
No

**Who will benefit with this feature?**

Projects like https://github.com/criteo/tf-yarn (tensorflow on yarn) will use it to implement the recommended way to create the cluster configuration. (from https://www.tensorflow.org/guide/distributed_training): The procedure will be: 
* Launch on all executors a process that will scan ports and reserve a free one
* A master gathers ports numbers. 
* Master creates configuration and broadcasts TF_CONFIG variable to all executors
* Launch tensorflow servers

**Any Other info.**
"
35382,Save model to pb and pbtxt,"I would like to use a model trained on python on C++

tried by tflite but cmake needed don't work
by the past, I had to use pb file and pbtxt file that works.

In TF 2.0 how could I save a Model to pb and pbtxt file to use it on C++
Or is there a better way that doesn't involve Cmake ?

Thanks a lot,"
35381,pip install tensorflow-gpu==2.1.0rc2 failed!,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.1.0rc2
- Python version: 3.7.4
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: Nvidia MX150



**Describe the problem**
The tensorflow-gpu-estimator error message occurs regardless of whether I install tensorflow-estimator first before tensorflow-gpu-2.1.0rc2 or with no installation of tensorflow-estimator.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
C:\>pip install tensorflow-estimator
Collecting tensorflow-estimator
  Using cached https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl
Installing collected packages: tensorflow-estimator
Successfully installed tensorflow-estimator-2.1.0

C:\>pip install tensorflow-gpu==2.1.0rc2
Collecting tensorflow-gpu==2.1.0rc2
  Using cached https://files.pythonhosted.org/packages/c5/5d/4d1cb80c38b987a8d0cace1d22a366c843af8966352bcc178450530e0a4f/tensorflow_gpu-2.1.0rc2-cp37-cp37m-win_amd64.whl
Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\python37-64\lib\site-packages (from tensorflow-gpu==2.1.0rc2) (1.1.0)
Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\python37-64\lib\site-packages (from tensorflow-gpu==2.1.0rc2) (1.17.2+mkl)
Requirement already satisfied: protobuf>=3.8.0 in c:\python37-64\lib\site-packages (from tensorflow-gpu==2.1.0rc2) (3.10.0)
ERROR: Could not find a version that satisfies the requirement tensorflow-gpu-estimator<2.2.0,>=2.1.0rc0 (from tensorflow-gpu==2.1.0rc2) (from versions: none)
ERROR: No matching distribution found for tensorflow-gpu-estimator<2.2.0,>=2.1.0rc0 (from tensorflow-gpu==2.1.0rc2)
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Following is my pip list output before I start installing tensorflow-estimator and tensorflow-gpu-2.1.0rc2.
```
C:\>pip list
Package                 Version
-----------              -----------
absl-py                 0.8.1
appdirs                 1.4.3
astor                   0.8.0
autopep8                1.4.4
cachetools              3.1.1
cairocffi               1.1.0
certifi                 2019.9.11
cffi                    1.13.1
chardet                 3.0.4
Click                   7.0
cycler                  0.10.0
decorator               4.4.1
dlib                    19.17.0
editdistance            0.5.3
entrypoints             0.3
essential-generators    0.9.2
face-recognition        1.2.3
face-recognition-models 0.3.0
flake8                  3.7.9
gast                    0.2.2
google-auth             1.7.0
google-auth-oauthlib    0.4.1
google-pasta            0.1.8
graphviz                0.13
grpcio                  1.24.3
h5py                    2.10.0
idna                    2.8
imageio                 2.6.1
imgaug                  0.3.0
imutils                 0.5.3
kaldiio                 2.14.1
Keras                   2.3.1
Keras-Applications      1.0.8
keras-ocr               0.2
Keras-Preprocessing     1.1.0
kiwisolver              1.1.0
llvmlite                0.30.0
Mako                    1.1.0
Markdown                3.1.1
MarkupSafe              1.1.1
matplotlib              3.1.1
mccabe                  0.6.1
mnist                   0.2.2
mock                    3.0.5
networkx                2.4
numba                   0.46.0
numpy                   1.17.2+mkl
oauthlib                3.1.0
opencv-contrib-python   4.1.1.26
opencv-python-headless  4.1.2.30
opt-einsum              3.1.0
Pillow                  6.1.0
pip                     19.3.1
protobuf                3.10.0
pyasn1                  0.4.7
pyasn1-modules          0.2.7
pycodestyle             2.5.0
pycparser               2.19
pycuda                  2019.1.2
pydot                   1.4.1
pyflakes                2.1.1
pyparsing               2.4.2
pypiwin32               223
pytesseract             0.3.0
python-dateutil         2.8.0
pytools                 2019.1.1
pyttsx3                 2.71
PyWavelets              1.1.1
pywin32                 224
PyYAML                  5.1.2
requests                2.22.0
requests-oauthlib       1.3.0
rsa                     4.0
scikit-cuda             0.5.3
scikit-image            0.16.2
scipy                   1.4.1
setuptools              41.4.0
Shapely                 1.6.4.post2
six                     1.12.0
termcolor               1.1.0
urllib3                 1.25.6
Werkzeug                0.16.0
wheel                   0.33.6
wrapt                   1.11.2
```"
35380,tensorflow lite performance slower than regular tensorflow model,i converted object detection model into tensorflow lite and i am trying to run it in raspberry pi but the performance of tensorflow lite is slower in desktop. How can i increase the performance speed of the tensorflow lite? 
35379,Cannot export keras model to SavedModel if mixed-precision policy is enabled,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 9
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): starting from 2.0, nightly version tested
- Python version: 3.5

**Describe the current behavior**

When keras mixed-precision policy ""mixed_float16"" is in use, we can't save the keras model in SavedModel format with method `keras.models.Model.save` without a specific `signatures`. It seems like a mismatch between input signature inferred by the model itself and the auto-casted inputs:

```
ValueError: Python inputs incompatible with input_signature: inputs ((<tf.Tensor 'conv1_pad/Cast:0' shape=(None, 224, 224, 3) dtype=float16>,)), input_signature ((TensorSpec(shape=(None, None, None, None), dtype=tf.float32, name=None),))
```

Although we can use `graph_rewrite` as mixed-precision training method to bypass this autocasting issue, but `graph_rewrite` is not working in some cases (e.g. train a subclassed model with `tf.GradientTape`) thus it is not recommended by tensorflow official guide. For flexibility we do hope to use mixed-precision policy in mixed-precision training, and directly exporting mixed-precision trained model to SavedModel for deployment is straightforward in production pipeline.


**Code to reproduce the issue**

We can reproduce this bug by using the official image classification training example from https://github.com/tensorflow/models/tree/master/official/vision/image_classification

```
""""""
Test mixed-precision policy model saving
""""""
import logging
import os

from absl import app as absl_app
import tensorflow as tf

from official.vision.image_classification.resnet_model import resnet50


def main(argv):
  tf.compat.v1.enable_eager_execution()
  
  # setup mixed-precision policy
  # the policy enables the autocasting behavior in keras layers
  policy = tf.keras.mixed_precision.experimental.Policy(
        'mixed_float16', loss_scale=128)
  tf.keras.mixed_precision.experimental.set_policy(policy)

  model = resnet50(1000)
  model_dir = 'temp/saved_model_test'

  if not os.path.isdir(model_dir):
    os.makedirs(model_dir)
  model.save(model_dir,
             save_format='tf')
  logging.info('Exported trained model to directory {}'.format(
      model_dir))


if __name__ == '__main__':
  absl_app.run(main)
```
"
35378,TensorflowLite gives wrong results when use GPU delegate,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (Android7.1 Snapdragon 625):
- Mobile device (HUAWEI Nova CAZ-AL 10) if the issue happens on mobile device:
- TensorFlow installed from (source):
- TensorFlow version (org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly):


**Describe the current behavior**
I use **yolov3** model on android platform to do object detection. When I did object detection on CPU the results are right but when I add GPU module the results are totally different.

**Code to reproduce the issue**


/**
 * Wrapper for frozen detection models trained using the Tensorflow Object Detection API:
 * github.com/tensorflow/models/tree/master/research/object_detection
 */
public class TFLitePanoObjectDetectionAPIModel implements Classifier {
  private static final Logger LOGGER = new Logger();

  private static final int NUM_of_classes = 19;
  private static final int NUM_THREADS = 4;
  private boolean isModelQuantized;
  private int inputSize;
  private Vector<String> labels = new Vector<String>();
  private int[] intValues;
  private float[][][][] output_1;
  private float[][][][] output_2;
  private float[][][][] output_3;
  private ByteBuffer imgData;
  private Interpreter tflite;
  private MappedByteBuffer tfliteModel;
  private final Interpreter.Options tfliteOptions = new Interpreter.Options();
  private GpuDelegate gpuDelegate = null;
  private NnApiDelegate nnapiDelegate = null;
  private int gridNum;

  private int BoxNum_each_gird=3;
  private float[][][][] floatValues;


  private TFLitePanoObjectDetectionAPIModel() {}
  private float scoreThreshold = 0.3f;
  private int blockSize=32;

  public static Classifier create(
      final AssetManager assetManager,
      final String modelFilename,
      final String labelFilename,
      final int inputSize,
      final boolean isQuantized)
      throws IOException {
    final TFLitePanoObjectDetectionAPIModel d = new TFLitePanoObjectDetectionAPIModel();
    try {

      d.tfliteModel=loadModelFile(assetManager, modelFilename);
      if (d.gpuDelegate == null)
      {
        d.gpuDelegate = new GpuDelegate();
        d.tfliteOptions.addDelegate(d.gpuDelegate);
      }
      d.tflite = new Interpreter(d.tfliteModel,d.tfliteOptions);
    } catch (Exception e) {
      throw new RuntimeException(e);
    }

    d.isModelQuantized = isQuantized;
    // Pre-allocate buffers.
    int numBytesPerChannel;
    if (isQuantized) {
      numBytesPerChannel = 1; // Quantized
    } else {
      numBytesPerChannel = 4; // Floating point
    }
    d.imgData = ByteBuffer.allocateDirect(1 * d.inputSize * 2*d.inputSize * 3 * numBytesPerChannel);//
    d.imgData.order(ByteOrder.nativeOrder());
    d.intValues = new int[d.inputSize * 2*d.inputSize];

    d.gridNum=d.inputSize/32;
    d.floatValues=new float[1][d.inputSize][2 * d.inputSize ][3];
    d.output_1 = new float[1][][][];
    d.output_2 = new float[1][][][];
    d.output_3 = new float[1][][][];


    return d;
  }
  }

  @Override
  public List<Recognition> recognizeImage(final Bitmap bitmap) {

    bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());
    imgData.rewind();

    for (int i = 0; i < inputSize; ++i)
    {
      for (int j = 0; j < 2*inputSize; ++j)
      {
        int pixelValue = intValues[i * 2*inputSize + j];
          // Float model
          floatValues[0][i][j][0]=((pixelValue >> 16) & 0xFF)/255.0f ;
          floatValues[0][i][j][1]=((pixelValue >> 8) & 0xFF)/255.0f ;
          floatValues[0][i][j][2]=(pixelValue& 0xFF) /255.0f;
      }
    }

     int channelNum=BoxNum_each_gird*(NUM_of_classes+5);

    output_1 = new float[1][gridNum][2*gridNum][channelNum];
    output_2 = new float[1][2*gridNum][4*gridNum][channelNum];
    output_3 = new float[1][4*gridNum][8*gridNum][channelNum];

    Object[] inputArray = {floatValues};
    Map<Integer, Object> outputMap = new HashMap<>();
    outputMap.put(0, output_1);
    outputMap.put(1, output_2);
    outputMap.put(2, output_3);
    Trace.endSection();

    // Run the inference call.
    Trace.beginSection(""run"");
    long startTime = SystemClock.uptimeMillis();
    tflite.runForMultipleInputsOutputs(inputArray, outputMap);
    long lastingTime=SystemClock.uptimeMillis()-startTime;

    LOGGER.i(""runForMultipleInputsOutputs time of each image: "" + lastingTime + ""ms"");
    Trace.endSection(); 
    }

}`
**Describe the expected behavior**

Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35377,Get error when build tensorflow-lite-hexagon.aar,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version:2.0
- Python version:2.7.15
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):1.1.0
- GCC/Compiler version (if compiling from source):8.3.0
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

I try to build a tensorflow-lite-hexagon.aar with the instructions as follows:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/hexagon_delegate.md
It used the command :
bazel build -c opt --config=android_arm64 tensorflow/lite/experimental/delegates/hexagon/java:tensorflow-lite-hexagon
to build.But I get the error like this:
ERROR: /prj/F3060803/Project/Tensorflow/tensorflow-master/tensorflow/lite/c/BUILD:130:1: C++ compilation of rule '//tensorflow/lite/c:common' failed (Exit 1)
Target //tensorflow/lite/experimental/delegates/hexagon/java:tensorflow-lite-hexagon failed to build
I wonder how should I do?Thanks

"
35376,7.4.1 but source was compiled with: 7.6.0,"Successfully opened dynamic library libcudnn.so.7
2019-12-24 05:34:15.862520: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.4.1 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.



**System information**
- Ubuntu 16.04
- TensorFlow version: tensorflow-gpu==1.15.0
- Python version:3.5
- Installed using virtualenv? pip? conda?:-virtualenv
- CUDA 10
cuDNN version:
cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2
#define CUDNN_MAJOR 7
#define CUDNN_MINOR 4
#define CUDNN_PATCHLEVEL 1
--
#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)
- GPU model and memory:





"
35372,Reading a Tensorflow 2.0's tfrecords File?,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>



I have a .tfrecords file that stores my serialised data. How can I re-load it next time? It is impractical to generate it fro raw data each time when it is needed. Note that I tried TFRecordReader from [here](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/t-f-record-reader) but it does not work at all

tf prompts

`
		AttributeError: module 'tensorflow' has no attribute 'TFRecordReader'
`"
35371,How to concatenate two models in tensorflow for End2End system?,"Hi 

I have trained two models **separately**.
**Model A** use `x` to predict `y`;
**Model B** use `y` to predict `z`.

Now I want to splice ​​them into one **Model C** which can use `x` to directly predict `z`.

In other words, I want to merge two graphs into one graph, or I want to freeze two checkpoints into one pb file.

Do you have any good idea to achieve this ? Thanks!
"
35369,Tensorflow Python modules import the same module under multiple different names,"I am using tensorflow 2.1.0rc1-2 on Arch Linux.

A Python project I work with contains some machinery to detect if the same .py file is imported under multiple different names, resulting in different module objects. This [can lead to subtle breakage](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-double-import-trap) as classes defined in the two copies of the module are officially different classes, so instances of one of them return False when using `isinstance()` with the class from the other copy of the module. Basically, it's a bad idea, so the project I work with raises a big error to tell us when we've accidentally done it.

Tensorflow has a large number of modules imported twice under different names, which causes my project to raise an error about it. Of course I can just whitelist tensorflow in my project, but I think this is a bad idea for tensorflow to do. If it is going to have the same module under multiple names, the import machinery hacks in tensorflow should ensure the multiple names at least point to the same module object, rather than there being multiple module objects in the interpreter object corresponding to the same .py file.

The issue can be demonstrated with the following:

```python
import sys
import tensorflow

for name1, module1 in sys.modules.copy().items():
    for name2, module2 in sys.modules.copy().items():
        file1 = getattr(module1, '__file__', None)
        file2 = getattr(module2, '__file__', None)
        if file1 is not None and file2 is not None:
            if file1 == file2 and name1 != name2 and module1 is not module2:
                print(name1, name2)
```
This prints
```
tensorflow._api tensorflow_core._api
tensorflow.python tensorflow_core.python
tensorflow.tools tensorflow_core.tools
tensorflow.core tensorflow_core.core
tensorflow.compiler tensorflow_core.compiler
tensorflow.lite tensorflow_core.lite
tensorflow.keras tensorflow.python.keras.api._v2.keras
tensorflow.keras tensorflow_core.python.keras.api._v2.keras
tensorflow.keras tensorflow_core.keras
tensorflow.compat tensorflow._api.v2.compat
tensorflow.compat tensorflow_core._api.v2.compat

<snip>
```

Basically there are copies of many modules imported both as `tensorflow.<blah>` and `tensorflow_core.<blah>`.

Tensorflow should consider changing its import machinery to avoid this, for the reasons outlined in the above linked post on the double import gotcha in Python."
35365,Cannot save Keras model to pb using model.save(),"Hello!
As been said in https://www.tensorflow.org/guide/keras/save_and_serialize#export_to_savedmodel , to save the Keras model to the format that could be used by TensorFlow, I need to use model.save() and provide save_format='tf', but what it throws me an exception.

**System information**
tf_env_collect.sh has been used to get:

== check python ===================================================
python version: 3.6.10
python branch: 
python build version: ('default', 'Dec 23 2019 13:58:32')
python compiler version: GCC 7.4.0
python implementation: CPython


== check os platform ===============================================
os: Linux
os kernel version: #81-Ubuntu SMP Tue Nov 26 12:20:02 UTC 2019
os release version: 4.15.0-72-generic
os platform: Linux-4.15.0-72-generic-x86_64-with-debian-buster-sid
linux distribution: ('debian', 'buster/sid', '')
linux os distribution: ('debian', 'buster/sid', '')
mac version: ('', ('', '', ''), '')
uname: uname_result(system='Linux', node='hellfire', release='4.15.0-72-generic', version='#81-Ubuntu SMP Tue Nov 26 12:20:02 UTC 2019', machine='x86_64', processor='x86_64')
architecture: ('64bit', '')
machine: x86_64


== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== check pips ===================================================
numpy                1.18.0 
protobuf             3.11.2 
tensorflow           1.14.0 
tensorflow-estimator 1.14.0 

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 1.14.0
tf.version.GIT_VERSION = v1.14.0-rc1-22-gaf24dc91b5
tf.version.COMPILER_VERSION = 4.8.5
== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
/tmp/tf_env_collect.sh: line 147: nvidia-smi: command not found

== cuda libs  ===================================================

== tensorflow installed from info ==================
Name: tensorflow
Version: 1.14.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /home/noxx/.pyenv/versions/3.6.10/envs/dmitriystf2/lib/python3.6/site-packages
Required-by: 

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 6, 10, 'final', 0)

== bazel version  ===============================================


**Describe the current behavior**

Exception:
```
Traceback (most recent call last):
  File ""load_file2.py"", line 14, in <module>
    classifier.save('/tmp/keras-model.pb', save_format='tf')
```

**Describe the expected behavior**

Model is saved as described in https://www.tensorflow.org/guide/keras/save_and_serialize#export_to_savedmodel

**Code to reproduce the issue**

```
import pandas as pd
import tensorflow as tf;
import keras;
from keras import Sequential
from keras.layers import Dense
import json;
import numpy as np;

classifier = Sequential()
classifier.add(Dense(4, activation='relu', kernel_initializer='random_normal', input_dim=4))
classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))
classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics = ['accuracy'])

classifier.save('/tmp/keras-model.pb', save_format='tf')
```

**Other info / logs**
I have the same behavior on tensorflow2

Thanks."
35364,no 2.1.0-rc1 or 2.1.0-rc2 in dockerhub only 2.0.0,"
**System information**
- TensorFlow version: 2.1.0

**Describe the problem**
Tensorflow/tensorflow does not list 2.1.0-rc1 or rc2 in the list of available tags.
These are available on colab but somehow did not make it to dockerhub.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
https://hub.docker.com/r/tensorflow/tensorflow/tags/?page=1&name=2.1.0
observe no tags

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35363,DCGAN Tutorial in TF Learn results different as in the tutoroial,"Hallo. I am a new learner for deep learning. I tried this DCGAN tutorial from Tensor Flow but the training result is different compared to what is shown in the tutorial website. 

This is my result:

![image](https://user-images.githubusercontent.com/36468837/71376946-83e69a80-25c3-11ea-9f77-d9f492a328f0.png)

Based on the tutorial, it supposed to have a result like this:
![image](https://user-images.githubusercontent.com/36468837/71377054-e5a70480-25c3-11ea-8b08-05226e41465a.png)

I already tried to run the train several times after it finishes the 50th epoch. But still it didnt show the same result as the tutorial. I appreciate the help to solve this. Thank you
"
35362,TensorFlow building error: C++ compilation of rule '//tensorflow/compiler/mlir/tensorflow:convert_graphdef' failed,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 16.04.6`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `N/A`
- TensorFlow installed from (source or binary): `source`
- TensorFlow version: `2.0.0 (master)`
- Python version: `Python 3.5.2`
- Installed using virtualenv? pip? conda?: `apt`
- Bazel version (if compiling from source): `1.1.0`
- GCC/Compiler version (if compiling from source): `gcc 5.4.0`
- CUDA/cuDNN version: `CUDA 10.2 / cuDNN 7.6.5.32-1+cuda10.2`
- GPU model and memory: `NVIDIA Tesla V100`



**Describe the problem**
Observed after a25c899dcc938c36b2ca8b77393001cd59fd9b97 commit.
TensorFlow building fails with the error:
```
[2019-12-23T02:02:25.156Z] ERROR: /scrap/jenkins/workspace/_ML_DevOps_team/ml-tensorflow-ci-pipeline/tensorflow/tensorflow/compiler/mlir/tensorflow/BUILD:330:1: C++ compilation of rule '//tensorflow/compiler/mlir/tensorflow:convert_graphdef' failed (Exit 1)
[2019-12-23T02:02:25.156Z] tensorflow/compiler/mlir/tensorflow/translate/import_model.cc: In member function 'tensorflow::GraphImportConfig::InputArrays tensorflow::{anonymous}::SavedModelV1Importer::ParseInputArrays(const google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::TensorInfo>&)':
[2019-12-23T02:02:25.156Z] tensorflow/compiler/mlir/tensorflow/translate/import_model.cc:3015:75: error: no matching function for call to 'std::pair<std::__cxx11::basic_string<char>, tensorflow::ArrayInfo>::pair(<brace-enclosed initializer list>)'
[2019-12-23T02:02:25.156Z]                                                       std::move(array_info)});
[2019-12-23T02:02:25.156Z]                                                                            ^
[2019-12-23T02:02:25.156Z] In file included from /usr/include/c++/5/bits/stl_algobase.h:64:0,
[2019-12-23T02:02:25.156Z]                  from /usr/include/c++/5/bits/char_traits.h:39,
[2019-12-23T02:02:25.156Z]                  from /usr/include/c++/5/string:40,
[2019-12-23T02:02:25.156Z]                  from ./tensorflow/compiler/mlir/tensorflow/translate/import_model.h:19,
[2019-12-23T02:02:25.156Z]                  from tensorflow/compiler/mlir/tensorflow/translate/import_model.cc:16:
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:206:9: note: candidate: template<class ... _Args1, long unsigned int ..._Indexes1, class ... _Args2, long unsigned int ..._Indexes2> std::pair<_T1, _T2>::pair(std::tuple<_Args1 ...>&, std::tuple<_Args2 ...>&, std::_Index_tuple<_Indexes1 ...>, std::_Index_tuple<_Indexes2 ...>)
[2019-12-23T02:02:25.156Z]          pair(tuple<_Args1...>&, tuple<_Args2...>&,
[2019-12-23T02:02:25.156Z]          ^
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:206:9: note:   template argument deduction/substitution failed:
[2019-12-23T02:02:25.156Z] tensorflow/compiler/mlir/tensorflow/translate/import_model.cc:3015:75: note:   '__gnu_cxx::__alloc_traits<std::allocator<absl::string_view> >::value_type {aka absl::string_view}' is not derived from 'std::tuple<_Elements ...>'
[2019-12-23T02:02:25.156Z]                                                       std::move(array_info)});
[2019-12-23T02:02:25.156Z]                                                                            ^
[2019-12-23T02:02:25.156Z] In file included from /usr/include/c++/5/bits/stl_algobase.h:64:0,
[2019-12-23T02:02:25.156Z]                  from /usr/include/c++/5/bits/char_traits.h:39,
[2019-12-23T02:02:25.156Z]                  from /usr/include/c++/5/string:40,
[2019-12-23T02:02:25.156Z]                  from ./tensorflow/compiler/mlir/tensorflow/translate/import_model.h:19,
[2019-12-23T02:02:25.156Z]                  from tensorflow/compiler/mlir/tensorflow/translate/import_model.cc:16:
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:155:9: note: candidate: template<class ... _Args1, class ... _Args2> std::pair<_T1, _T2>::pair(std::piecewise_construct_t, std::tuple<_Args1 ...>, std::tuple<_Args2 ...>)
[2019-12-23T02:02:25.156Z]          pair(piecewise_construct_t, tuple<_Args1...>, tuple<_Args2...>);
[2019-12-23T02:02:25.156Z]          ^
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:155:9: note:   template argument deduction/substitution failed:
[2019-12-23T02:02:25.156Z] tensorflow/compiler/mlir/tensorflow/translate/import_model.cc:3014:67: note:   cannot convert 'node_names.std::vector<_Tp, _Alloc>::at<absl::string_view, std::allocator<absl::string_view> >(0ul)' (type '__gnu_cxx::__alloc_traits<std::allocator<absl::string_view> >::value_type {aka absl::string_view}') to type 'std::piecewise_construct_t'
[2019-12-23T02:02:25.156Z]      results.insert(std::pair<std::string, ArrayInfo>{node_names.at(0),
[2019-12-23T02:02:25.156Z]                                                                    ^
[2019-12-23T02:02:25.156Z] In file included from /usr/include/c++/5/bits/stl_algobase.h:64:0,
[2019-12-23T02:02:25.156Z]                  from /usr/include/c++/5/bits/char_traits.h:39,
[2019-12-23T02:02:25.156Z]                  from /usr/include/c++/5/string:40,
[2019-12-23T02:02:25.156Z]                  from ./tensorflow/compiler/mlir/tensorflow/translate/import_model.h:19,
[2019-12-23T02:02:25.156Z]                  from tensorflow/compiler/mlir/tensorflow/translate/import_model.cc:16:
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:150:12: note: candidate: template<class _U1, class _U2, class> constexpr std::pair<_T1, _T2>::pair(std::pair<_U1, _U2>&&)
[2019-12-23T02:02:25.156Z]   constexpr pair(pair<_U1, _U2>&& __p)
[2019-12-23T02:02:25.156Z]             ^
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:150:12: note:   template argument deduction/substitution failed:
[2019-12-23T02:02:25.156Z] tensorflow/compiler/mlir/tensorflow/translate/import_model.cc:3015:75: note:   '__gnu_cxx::__alloc_traits<std::allocator<absl::string_view> >::value_type {aka absl::string_view}' is not derived from 'std::pair<_T1, _T2>'
[2019-12-23T02:02:25.156Z]                                                       std::move(array_info)});
[2019-12-23T02:02:25.156Z]                                                                            ^
[2019-12-23T02:02:25.156Z] In file included from /usr/include/c++/5/bits/stl_algobase.h:64:0,
[2019-12-23T02:02:25.156Z]                  from /usr/include/c++/5/bits/char_traits.h:39,
[2019-12-23T02:02:25.156Z]                  from /usr/include/c++/5/string:40,
[2019-12-23T02:02:25.156Z]                  from ./tensorflow/compiler/mlir/tensorflow/translate/import_model.h:19,
[2019-12-23T02:02:25.156Z]                  from tensorflow/compiler/mlir/tensorflow/translate/import_model.cc:16:
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:144:12: note: candidate: template<class _U1, class _U2, class> constexpr std::pair<_T1, _T2>::pair(_U1&&, _U2&&)
[2019-12-23T02:02:25.156Z]   constexpr pair(_U1&& __x, _U2&& __y)
[2019-12-23T02:02:25.156Z]             ^
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:144:12: note:   template argument deduction/substitution failed:
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:141:38: error: no type named 'type' in 'struct std::enable_if<false, void>'
[2019-12-23T02:02:25.156Z]        template<class _U1, class _U2, class = typename
[2019-12-23T02:02:25.156Z]                                       ^
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:138:12: note: candidate: template<class _U2, class> constexpr std::pair<_T1, _T2>::pair(const _T1&, _U2&&)
[2019-12-23T02:02:25.156Z]   constexpr pair(const _T1& __x, _U2&& __y)
[2019-12-23T02:02:25.156Z]             ^
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:138:12: note:   template argument deduction/substitution failed:
[2019-12-23T02:02:25.156Z] tensorflow/compiler/mlir/tensorflow/translate/import_model.cc:3014:67: note:   cannot convert 'node_names.std::vector<_Tp, _Alloc>::at<absl::string_view, std::allocator<absl::string_view> >(0ul)' (type '__gnu_cxx::__alloc_traits<std::allocator<absl::string_view> >::value_type {aka absl::string_view}') to type 'const std::__cxx11::basic_string<char>&'
[2019-12-23T02:02:25.156Z]      results.insert(std::pair<std::string, ArrayInfo>{node_names.at(0),
[2019-12-23T02:02:25.156Z]                                                                    ^
[2019-12-23T02:02:25.156Z] In file included from /usr/include/c++/5/bits/stl_algobase.h:64:0,
[2019-12-23T02:02:25.156Z]                  from /usr/include/c++/5/bits/char_traits.h:39,
[2019-12-23T02:02:25.156Z]                  from /usr/include/c++/5/string:40,
[2019-12-23T02:02:25.156Z]                  from ./tensorflow/compiler/mlir/tensorflow/translate/import_model.h:19,
[2019-12-23T02:02:25.156Z]                  from tensorflow/compiler/mlir/tensorflow/translate/import_model.cc:16:
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:133:12: note: candidate: template<class _U1, class> constexpr std::pair<_T1, _T2>::pair(_U1&&, const _T2&)
[2019-12-23T02:02:25.156Z]   constexpr pair(_U1&& __x, const _T2& __y)
[2019-12-23T02:02:25.156Z]             ^
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:133:12: note:   template argument deduction/substitution failed:
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:131:27: error: no type named 'type' in 'struct std::enable_if<false, void>'
[2019-12-23T02:02:25.156Z]        template<class _U1, class = typename
[2019-12-23T02:02:25.156Z]                            ^
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:128:17: note: candidate: constexpr std::pair<_T1, _T2>::pair(std::pair<_T1, _T2>&&) [with _T1 = std::__cxx11::basic_string<char>; _T2 = tensorflow::ArrayInfo]
[2019-12-23T02:02:25.156Z]        constexpr pair(pair&&) = default;
[2019-12-23T02:02:25.156Z]                  ^
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:128:17: note:   candidate expects 1 argument, 2 provided
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:127:17: note: candidate: constexpr std::pair<_T1, _T2>::pair(const std::pair<_T1, _T2>&) [with _T1 = std::__cxx11::basic_string<char>; _T2 = tensorflow::ArrayInfo]
[2019-12-23T02:02:25.156Z]        constexpr pair(const pair&) = default;
[2019-12-23T02:02:25.156Z]                  ^
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:127:17: note:   candidate expects 1 argument, 2 provided
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:124:12: note: candidate: template<class _U1, class _U2, class> constexpr std::pair<_T1, _T2>::pair(const std::pair<_U1, _U2>&)
[2019-12-23T02:02:25.156Z]   constexpr pair(const pair<_U1, _U2>& __p)
[2019-12-23T02:02:25.156Z]             ^
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:124:12: note:   template argument deduction/substitution failed:
[2019-12-23T02:02:25.156Z] tensorflow/compiler/mlir/tensorflow/translate/import_model.cc:3015:75: note:   '__gnu_cxx::__alloc_traits<std::allocator<absl::string_view> >::value_type {aka absl::string_view}' is not derived from 'const std::pair<_T1, _T2>'
[2019-12-23T02:02:25.156Z]                                                       std::move(array_info)});
[2019-12-23T02:02:25.156Z]                                                                            ^
[2019-12-23T02:02:25.156Z] In file included from /usr/include/c++/5/bits/stl_algobase.h:64:0,
[2019-12-23T02:02:25.156Z]                  from /usr/include/c++/5/bits/char_traits.h:39,
[2019-12-23T02:02:25.156Z]                  from /usr/include/c++/5/string:40,
[2019-12-23T02:02:25.156Z]                  from ./tensorflow/compiler/mlir/tensorflow/translate/import_model.h:19,
[2019-12-23T02:02:25.156Z]                  from tensorflow/compiler/mlir/tensorflow/translate/import_model.cc:16:
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:112:26: note: candidate: constexpr std::pair<_T1, _T2>::pair(const _T1&, const _T2&) [with _T1 = std::__cxx11::basic_string<char>; _T2 = tensorflow::ArrayInfo]
[2019-12-23T02:02:25.156Z]        _GLIBCXX_CONSTEXPR pair(const _T1& __a, const _T2& __b)
[2019-12-23T02:02:25.156Z]                           ^
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:112:26: note:   no known conversion for argument 1 from '__gnu_cxx::__alloc_traits<std::allocator<absl::string_view> >::value_type {aka absl::string_view}' to 'const std::__cxx11::basic_string<char>&'
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:108:26: note: candidate: constexpr std::pair<_T1, _T2>::pair() [with _T1 = std::__cxx11::basic_string<char>; _T2 = tensorflow::ArrayInfo]
[2019-12-23T02:02:25.156Z]        _GLIBCXX_CONSTEXPR pair()
[2019-12-23T02:02:25.156Z]                           ^
[2019-12-23T02:02:25.156Z] /usr/include/c++/5/bits/stl_pair.h:108:26: note:   candidate expects 0 arguments, 2 provided
[2019-12-23T02:02:25.156Z] Target //tensorflow/tools/pip_package:build_pip_package failed to build
[2019-12-23T02:02:25.156Z] Use --verbose_failures to see the command lines of failed build steps.
[2019-12-23T02:02:25.156Z] ERROR: /scrap/jenkins/workspace/_ML_DevOps_team/ml-tensorflow-ci-pipeline/tensorflow/tensorflow/lite/toco/python/BUILD:77:1 C++ compilation of rule '//tensorflow/compiler/mlir/tensorflow:convert_graphdef' failed (Exit 1)
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
.tf_configure.bazelrc:
```
build --action_env PYTHON_BIN_PATH=""/usr/bin/python3""
build --action_env PYTHON_LIB_PATH=""/usr/local/lib/python3.5/dist-packages""
build --python_path=""/usr/bin/python3""
build:xla --define with_xla_support=true
build --action_env TF_CUDA_VERSION=""10.2""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env TF_NCCL_VERSION=""2.6.0""
build --action_env TF_CUDA_PATHS=""/hpc/local/oss/cuda10.2/cuda-toolkit,/usr,/usr/local/cuda""
build --action_env CUDA_TOOLKIT_PATH=""/hpc/local/oss/cuda10.2/cuda-toolkit""
build --action_env CUDNN_INSTALL_PATH=""/usr""
build --action_env NCCL_INSTALL_PATH=""<cut>/nccl/stable""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""7.0""
build --action_env LD_LIBRARY_PATH=""<cut>/nccl/stable/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/nccl_rdma_sharp_plugin/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/ucx/lib/ucx:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/ucx/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/sharp/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/hcoll/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/ompi/lib:/hpc/local/oss/cuda10.2/cuda-toolkit/lib64:/hpc/local/oss/cuda10.2/cuda-toolkit/lib64/stubs:/usr/local/nvidia/lib:/usr/local/nvidia/lib64""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc-5""
build --config=cuda
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test --test_env=LD_LIBRARY_PATH
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only
build --action_env TF_CONFIGURE_IOS=""0""
```"
35361,geting error in tensorflow for Keras ,"**System information**
- OS Platform and Distribution (e.g., Windows 8.1):
- TensorFlow installed from (pip install tensor flow):
- TensorFlow version ():
- Python version:3.8.1
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:


**Describe the current behavior**
im using keras for deep learning which use tensorflow as a backend but I try to import the models in keras it gives this error:

File ""E:\Deep\LSTM_model.py"", line 8, in <module>
    import keras.models
  File ""C:\Users\Bilal\AppData\Local\Programs\Python\Python38-32\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\Bilal\AppData\Local\Programs\Python\Python38-32\lib\site-packages\keras\utils\__init__.py"", line 26, in <module>
    from .vis_utils import model_to_dot
  File ""C:\Users\Bilal\AppData\Local\Programs\Python\Python38-32\lib\site-packages\keras\utils\vis_utils.py"", line 7, in <module>
    from ..models import Model
  File ""C:\Users\Bilal\AppData\Local\Programs\Python\Python38-32\lib\site-packages\keras\models.py"", line 10, in <module>
    from .engine.input_layer import Input
  File ""C:\Users\Bilal\AppData\Local\Programs\Python\Python38-32\lib\site-packages\keras\engine\__init__.py"", line 7, in <module>
    from .network import get_source_inputs
  File ""C:\Users\Bilal\AppData\Local\Programs\Python\Python38-32\lib\site-packages\keras\engine\network.py"", line 15, in <module>
    from . import saving
  File ""C:\Users\Bilal\AppData\Local\Programs\Python\Python38-32\lib\site-packages\keras\engine\saving.py"", line 35, in <module>
    from tensorflow.python.lib.io import file_io as tf_file_io
  File ""C:\Users\Bilal\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Bilal\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Bilal\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Bilal\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 114
    def TFE_ContextOptionsSetAsync(arg1, async):
                                         ^
SyntaxError: invalid syntax

**Describe the expected behavior**

**Code to reproduce the issue**
import xlrd
import xlwt
import numpy as np
from numpy import *
from sklearn.externals import joblib
import warnings
**from keras.models import * #here it gives error**

"
35358,Read data from TFRecordDataset throws TensorShape error,"**Describe the current behavior**
I'm trying to save/load a numpy dataset into a TFRecordDataset in TF2.0 for training on TPU. Saving succeeds but when reading the file I and I pass the data through a model I get an error about the shape of the tensors.

I compared the tensors resulted from reading the TFRecordDataset and they are equal. The dataset I get read back from the file reader is a `MapDataset` instead of a `DatasetV1Adapter`. 
I have also opened an issue on stack overflow here: https://stackoverflow.com/questions/59314315/read-data-from-tfrecorddataset-throws-tensorshape-error .

**Describe the expected behaviour**  
Reading the file should result in a dataset identical with the one that was written. Running the dataset through a model should produce similar effects.

**Code to reproduce the issue** . 
A minimum reproducible example is available as a Python notebook here https://gist.github.com/vicpara/3b4ea00553a1990620a2df77d8b6aa1f  .

**System information**
`tf_env_collect.sh` output:   
```
== check python ===================================================
python version: 3.7.4
python branch: 
python build version: ('default', 'Sep 29 2019 19:47:40')
python compiler version: Clang 10.0.1 (clang-1001.0.46.4)
python implementation: CPython


== check os platform ===============================================
os: Darwin
os kernel version: Darwin Kernel Version 18.7.0: Tue Aug 20 16:57:14 PDT 2019; root:xnu-4903.271.2~2/RELEASE_X86_64
os release version: 18.7.0
os platform: Darwin-18.7.0-x86_64-i386-64bit
linux distribution: ('', '', '')
linux os distribution: ('', '', '')
mac version: ('10.14.6', ('', '', ''), 'x86_64')
uname: uname_result(system='Darwin', node='Viktors-MacBook-Pro.local', release='18.7.0', version='Darwin Kernel Version 18.7.0: Tue Aug 20 16:57:14 PDT 2019; root:xnu-4903.271.2~2/RELEASE_X86_64', machine='x86_64', processor='i386')
architecture: ('64bit', '')
machine: x86_64


== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 10.0.1 (clang-1001.0.46.4)
Target: x86_64-apple-darwin18.7.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

== check pips ===================================================
numpy                    1.17.2              
protobuf                 3.9.2               
tensorflow               2.0.0               
tensorflow-datasets      1.2.0               
tensorflow-estimator     2.0.0               
tensorflow-metadata      0.14.0              

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 2.0.0
tf.version.GIT_VERSION = v2.0.0-rc2-26-g64c3d382ca
tf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.11.45.5)
/Users/victor/.pyenv/versions/3.7.4/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.
  warnings.warn(msg)
/Users/victor/.pyenv/versions/3.7.4/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.
  warnings.warn(msg)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_collect.sh: line 147: nvidia-smi: command not found

== cuda libs  ===================================================

== tensorflow installed from info ==================
Name: tensorflow
Version: 2.0.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /Users/victor/.pyenv/versions/3.7.4/lib/python3.7/site-packages
Required-by: 

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 7, 4, 'final', 0)

== bazel version  ===============================================
```"
35357,tf-lite C++ API giving same inference output for every input,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave(v.10.14.5)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n.a.
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): git tag: v1.14.0
- Python version: n.a.
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): Apple clang version 11.0.0 (clang-1100.0.33.12)
- CUDA/cuDNN version: n.a.
- GPU model and memory: n.a.

**Describe the current behavior**
Model giving same output for every input.

**Describe the expected behavior**
Model should give different output for different inputs, and outputs of python API and C++ API should match.

**Code to reproduce the issue**
I want to evaluate tf model in a C++ project.

I built tf-lite dynamic library using following command
`bazel build -c opt //tensorflow/contrib/lite:libtensorflowLite.so --cxxopt=""-std=c++11"" --verbose_failures`.

In my project's cmake file, I set target_link_library property to `tensorflow/bazel-bin/tensorflow/lite/libtensorflowlite.so`, and I set appropriate header search paths so that my code compiles properly.

I converted my model to .tflite format via this snippet
```import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_keras_model_file('model.h5') 
tfmodel = converter.convert() 
open(""model.tflite"", ""wb"").write(tfmodel)
```

I wrote this code for my model evaluation
```// Load the model
std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(""model.tflite"");

// Build the interpreter
tflite::ops::builtin::BuiltinOpResolver resolver;
std::unique_ptr<tflite::Interpreter> interpreter;

tflite::InterpreterBuilder builder(*model, resolver);
builder(&interpreter);
interpreter->AllocateTensors();

// Check interpreter state
tflite::PrintInterpreterState(interpreter.get());

std::vector<std::vector<double>> tensor;     // I filled this vector, (dims are 2050, 6)

int input = interpreter->inputs()[0];      // input dims are (1, 2050, 6)
float* input_data_ptr = interpreter->typed_input_tensor<float>(input);
for (int i = 0; i < 2050; ++i) {
    for (int j = 0; j < 6; j++) {
        *(input_data_ptr) = (float)tensor[i][j];
        input_data_ptr++;
    }
}

interpreter->Invoke();
int output_idx = interpreter->outputs()[0];
float* output = interpreter->typed_output_tensor<float>(output_idx);
std::cout << ""OUTPUT: "" << *output << std::endl;
```

```INFO: Initialized TensorFlow Lite runtime.
Interpreter has 96 tensors and 42 nodes
Inputs: 10
Outputs: 8

         < ........................ other layers>
Tensor   8 dense_2/Sigmoid      kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1
         < ........................ other layers>
Tensor  10 input_1              kTfLiteFloat32  kTfLiteArenaRw      49200 bytes ( 0.0 MB)  1 2050 6
         < ........................ other layers>

OUTPUT: -3.74034
```
Issue is that I get this same output for all the inputs, and its also wrong as sigmoid should be between 0-1

Trying evaluation from python yields expected results. I used this snippet to run in python
```import numpy as np
import tensorflow as tf

# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""model.tflite"")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
print(""INPUT DETAILS: "", input_details)
print(""OUTPUT DETAILS: "", output_details)

# Evaluate
dataset = h5py.File('dataset.h5', 'r')
input_data1 = np.array(dataset[""test""][:])    #shape is (1, 2050, 6)
interpreter.set_tensor(input_details[0]['index'], input_data1)
interpreter.invoke()

output_data = interpreter.get_tensor(output_details[0]['index'])
print(""OUTPUT_DATA: "", output_data)
```
This gives correct output 
```
INPUT DETAULS:  [{'name': 'input_1', 'index': 10, 'shape': array([   1, 2050,    6], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]
OUTPUT DETAILS:  [{'name': 'dense_2/Sigmoid', 'index': 8, 'shape': array([1, 1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]

OUTPUT_DATA [[0.02964767]]
```"
35356,[TF2.0] String dtype will cause Dateset from MirroredStrategy.experimental_distribute_dataset raise RuntimeError when using GPU.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tf-nightly-gpu
- TensorFlow version (use command below): v1.12.1-20829-ga3bf777 2.1.0-dev20191218
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cudatoolkit=10.0.130 cudnn=7.6.4
- GPU model and memory:

**Describe the current behavior**
When iterating a dataset which is returned by ```MirroredStrategy.experimental_distribute_dataset``` and contains ```tf.dtypes.string``` elements with GPU, a ```RuntimeError``` will be raised after the last iteration, and it says ```Can't copy Tensor with type string to device /job:localhost/replica:0/task:0/device:GPU:0.```.

When change to ```OneDeviceStrategy```, everything is fine.


**Describe the expected behavior**
Iteration over the dataset should end successfully no matter which kind of distribute strategy is used and no matter what kind of dtype elements it contains.


**Code to reproduce the issue**
```python
import tensorflow as tf

distribute_strategy = tf.distribute.MirroredStrategy([""/gpu:0""])
## OneDeviceStrategy is fine
#distribute_strategy = tf.distribute.OneDeviceStrategy(""/gpu:0"")

ds = tf.data.Dataset.from_tensor_slices([[""A"", ""C"", ""F""], ["""", ""D"", ""A""], [""B"", ""A"", """"]])
ds = ds.batch(1)
ds = distribute_strategy.experimental_distribute_dataset(ds)

for i, inputs in enumerate(ds):
    print(""step[{}] inputs={}"".format(i, inputs))
```


**Other info / logs**
```
WARNING:tensorflow:Falling back to tensorflow client, its recommended to install the cloud tpu client directly with pip install cloud-tpu-client .
2019-12-23 19:13:59.324449: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-12-23 19:14:04.348064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:83:00.0 name: TITAN X (Pascal) computeCapability: 6.1
coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s
2019-12-23 19:14:04.354811: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2019-12-23 19:14:04.357770: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2019-12-23 19:14:04.368166: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2019-12-23 19:14:04.374694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2019-12-23 19:14:04.395497: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2019-12-23 19:14:04.401722: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2019-12-23 19:14:04.421294: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-12-23 19:14:09.099740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2019-12-23 19:14:09.101649: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-12-23 19:14:09.120678: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 2097350000 Hz
2019-12-23 19:14:09.177180: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55733782fe30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-12-23 19:14:09.177244: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2019-12-23 19:14:09.311769: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5573378979c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2019-12-23 19:14:09.311822: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN X (Pascal), Compute Capability 6.1
2019-12-23 19:14:09.313461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:83:00.0 name: TITAN X (Pascal) computeCapability: 6.1
coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s
2019-12-23 19:14:09.313511: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2019-12-23 19:14:09.313527: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2019-12-23 19:14:09.313552: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2019-12-23 19:14:09.313565: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2019-12-23 19:14:09.313579: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2019-12-23 19:14:09.313592: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2019-12-23 19:14:09.313606: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-12-23 19:14:09.316271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2019-12-23 19:14:09.316310: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2019-12-23 19:14:09.318078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-23 19:14:09.318110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2019-12-23 19:14:09.318141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2019-12-23 19:14:09.320811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11448 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:83:00.0, compute capability: 6.1)
step[0] inputs=[[b'A' b'C' b'F']]
step[1] inputs=[[b'' b'D' b'A']]
step[2] inputs=[[b'B' b'A' b'']]
Traceback (most recent call last):
  File ""test.py"", line 46, in <module>
    for i, inputs in enumerate(ds):
  File ""/home/anaconda3/envs/tf21_nt/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 249, in __next__
    return self.get_next()
  File ""/home/anaconda3/envs/tf21_nt/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 281, in get_next
    global_has_value, replicas = _get_next_as_optional(self, self._strategy)
  File ""/home/anaconda3/envs/tf21_nt/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 177, in _get_next_as_optional
    iterator._iterators[i].get_next_as_list(new_name))  # pylint: disable=protected-access
  File ""/home/anaconda3/envs/tf21_nt/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 905, in get_next_as_list
    strict=True,
  File ""/home/anaconda3/envs/tf21_nt/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/anaconda3/envs/tf21_nt/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 1207, in cond
    result = false_fn()
  File ""/home/anaconda3/envs/tf21_nt/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 904, in <lambda>
    lambda: _dummy_tensor_fn(data.value_structure),
  File ""/home/anaconda3/envs/tf21_nt/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 818, in _dummy_tensor_fn
    return nest.map_structure(create_dummy_tensor, value_structure)
  File ""/home/anaconda3/envs/tf21_nt/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py"", line 568, in map_structure
    structure[0], [func(*x) for x in entries],
  File ""/home/anaconda3/envs/tf21_nt/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py"", line 568, in <listcomp>
    structure[0], [func(*x) for x in entries],
  File ""/home/anaconda3/envs/tf21_nt/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 808, in create_dummy_tensor
    dummy_tensor = array_ops.zeros(tensor_shape.TensorShape(dims), feature_type)
  File ""/home/anaconda3/envs/tf21_nt/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py"", line 2716, in zeros
    output = fill(shape, constant(zero, dtype=dtype), name=name)
  File ""/home/anaconda3/envs/tf21_nt/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py"", line 258, in constant
    allow_broadcast=True)
  File ""/home/anaconda3/envs/tf21_nt/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py"", line 266, in _constant_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""/home/anaconda3/envs/tf21_nt/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py"", line 96, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
RuntimeError: Can't copy Tensor with type string to device /job:localhost/replica:0/task:0/device:GPU:0.
```
"
35355,Load Image Not RUN,"Yout Tutorial Link: https://www.tensorflow.org/tutorials/load_data/images?hl=ko

In Load using tf.data Section Code Error occur....

my Tensorflow Version: '2.0.0-beta1'

Code Here:
```python
def get_label(file_path):
    parts = tf.strings.split(file_path, os.path.sep)
    tf.print(parts, output_stream=sys.stderr)
    tf.print(parts[-2], output_stream=sys.stderr)
    
    return CLASS_NAMES == parts[-2]

def decode_img(img):
    # convert the compressed string to a 3D uint8 tensor
    img = tf.image.decode_jpeg(img, channels=3)
    # Use `convert_image_dtype` to convert to floats in the [0,1] range.
    img = tf.image.convert_image_dtype(img, tf.float32)
    # resize the image to the desired size.
    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])

def process_path(file_path):
    label = get_label(file_path)
    # load the raw data from the file as a string
    img = tf.io.read_file(file_path)
    img = decode_img(img)
    return img, label

#AUTOTUNE = tf.data.experimental.AUTOTUNE

# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.
labeled_ds = list_ds.map(process_path)

for image, label in labeled_ds.take(3):
    print(""Image shape: "", image.numpy().shape)
    print(""Label: "", label.numpy())
```

The result is
```code
["""" ""root"" "".keras"" ... ""flower_photos"" ""dandelion"" ""17457028309_95514c8d02_n.jpg""]
dandelion
Image shape:  (224, 224, 3)
Label:  False
["""" ""root"" "".keras"" ... ""flower_photos"" ""roses"" ""5537794501_a0767743fd_n.jpg""]
roses
Image shape:  (224, 224, 3)
Label:  False
["""" ""root"" "".keras"" ... ""flower_photos"" ""dandelion"" ""4632235020_d00ce1e497.jpg""]
dandelion
Image shape:  (224, 224, 3)
Label:  False
```

**It's Version Mismatch or something wrong**"
35354,Contributing a multistep optimizer for training big NNs/batches,"I noticed a high demand in training big NNs/batches is to create a multistep optimizer. This means the optimizer accumulates gradients from batches until it reaches a desire batch and update model parameters. 

This is similar to [this](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/multistep_optimizer.py), but for file [keras.optimizer_v2](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizer_v2/optimizer_v2.py) instead.

I can contribute to this. However, I noticed by doing this, the code optimizer_v2.py will need to change a lot. I can create a new similar python file (say, multistep_optimizer_v2.py) to avoid changing in optimizer_v2.py, but the optimizer_v2.py code it self is indeed very long.

I thus create a discussion here to see whether you (Tensorflow Team and community) feel it is worthy to do so, as well as how to do that, or any better solution for doing that, if there is.

Best,"
35353,"You tried to call `count_params` on z_input, but the layer isn't built. You can build it manually via: `z_input.build(batch_input_shape","when saved the model as tf by api  **tf.keras.models.save_model(testmodel, ""./1/"",save_format='tf')**
then,I load it by tf.keras.models.load_model('1'),however，I got the issue as title.
It is worth mentioning that it succed if I saved as .h5
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):centos7.4
- TensorFlow version (use command below):tensorflow 2.0
- Python version:3.7
-NVIDIA-SMI 418.88
-Driver Version: 418.88
-CUDA Version: 10.1 
- GPU model and memory:12

"
35352,Go TensorFlow 1.15.0: DataType 20 is not supported,"**System information**

- OS Platform and Distribution : mcOS HighSierra Version 10.13.2
- TensorFlow version: 1.15.0

Tried to load model from python (TF 2) on my go (TF 1.15.0) and i got error when accessing the value of the result. The error message I got was:

`panic: BUG: Please report at https://github.com/tensorflow/tensorflow/issues with the note: Go TensorFlow 1.15.0: DataType 20 is not supported (see https://www.tensorflow.org/code/tensorflow/core/framework/types.proto)`

is there any solution for this?"
35349,Return predictions with .evaluate(),"I work with several custom metrics I compute from model's inference-mode predictions; problem is, this requires me to hand-code loss computation to avoid running `.evaluate()` along `.predict()` (doubling validation time), which may include weight and activity regularizers. 

It'd be quite helpful to have an option for `model.evaluate()` to return predictions along computed metrics (e.g. `evaluate(return_predictions=True)`).

*NOTE*: this is a **feature request**, concerning the [`evaluate`](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/engine/training.py#L730) method of `keras.Model` or `tf.keras.Model`."
35347,Unnecessary synchronization of deterministic variable in Mirrored Distributed mode,"**System information**
* Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
* TensorFlow installed from (source or binary): binary
* TensorFlow version (use command below): v2.1.0-rc0-47-g064e153 2.1.0-rc1 (python3 -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"")
* Python version: Python 3.6.8
* CUDA/cuDNN version: Driver Version: 440.33.01, CUDA Version: 10.2, cuDNN 7.6.2
GPU model and memory: Tesla V100-SXM2-16GB

**Describe the current behavior**
I have a custom implementation of spectral normalization:
https://github.com/olegmyrk/SPADE-Tensorflow/blob/4203f30b6253a9d4743962087896fab26381c67b/ops.py#L579

It defines a non-trainable variable
```
u = tf.compat.v1.get_variable(""u"", [1, w_shape[-1]], initializer=tf.compat.v1.random_normal_initializer(), trainable=False, aggregation=tf.compat.v2.VariableAggregation.ONLY_FIRST_REPLICA)
```
that is used for power iteration
https://en.wikipedia.org/wiki/Power_iteration
It is updated using deterministic computation (ie it depends only on weight matrix, but not batch inputs) at each gradient step:
https://github.com/olegmyrk/SPADE-Tensorflow/blob/4203f30b6253a9d4743962087896fab26381c67b/ops.py#L606

Currently in mirrored distributed mode this adds a very expensive synchronization step for this variable.

I could set 
```
aggregation=tf.compat.v2.VariableAggregation.NONE
```
but that doesn't work in distributed mode, according to manual:
https://www.tensorflow.org/api_docs/python/tf/VariableAggregation
> NONE: This is the default, giving an error if you use a variable-update operation with multiple replicas.

Additionally it takes ~40 minutes to build the graph with spectral normalization in distributed mirrored mode (with multiple GPUs) as opposed to 20 minutes without spectral normalization enabled.

Although this might be a generic issue related to slow AutoGraph in Mirrored Distributed mode:
https://github.com/tensorflow/tensorflow/issues/35346

**Describe the expected behavior**
I need to specify that there is no need to synchronize this variable as it has the same deterministic value on all replicas.

**Code to reproduce the issue**
https://github.com/olegmyrk/SPADE-Tensorflow/blob/4203f30b6253a9d4743962087896fab26381c67b/ops.py#L606

**Other info / logs**
Using `NCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=ALL`:

*Without spectral norm*
INFO:tensorflow:batch_all_reduce: 219 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce: 1058 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce: 2150 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce: 4402 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10

INFO:tensorflow:batch_all_reduce: 219 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce: 1058 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce: 2150 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce: 4402 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10

*With spectral norm*
INFO:tensorflow:batch_all_reduce: 459 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce: 1666 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce: 2662 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce: 6610 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10

INFO:tensorflow:batch_all_reduce: 459 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce: 1666 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce: 2662 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce: 6610 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10"
35346,AutoGraph is compiled 5x slower in TF2.x Multi-GPU Distributed Mirrored Strategy,"**System information**
* Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
* TensorFlow installed from (source or binary): binary
* TensorFlow version (use command below): v2.1.0-rc0-47-g064e153 2.1.0-rc1 (`python3 -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`)
* Python version: Python 3.6.8
* CUDA/cuDNN version: Driver Version: 440.33.01, CUDA Version: 10.2, cuDNN 7.6.2
* GPU model and memory: Tesla V100-SXM2-16GB

**Describe the current behavior**
Compiling autograph function is 4-5x slower in Distributed Mirrored Strategy:
* Single-GPU Distributed Mirrored Strategy: under 5 minutes
* Multi-GPU Distributed Mirrored Strategy: about 30 minutes
The tensorflow code is identical in both setups.

**Describe the expected behavior**
Autograph compilation should take roughly the same time in single and multi-GPU Distributed Mode with Mirrored Strategy.

**Code to reproduce the issue**
Training loop (hierarchical VAE in the current configuration):
https://github.com/olegmyrk/SPADE-Tensorflow/blob/4681ac60f2078144d55178f23fa885d3ed191450/SPADE.py#L1151

The code is adapted from TF1.x repository:
https://github.com/olegmyrk/SPADE-Tensorflow/blob/8866a0b1457cbd4be5d6f549f9bf4075d49b2486/SPADE.py#L1045
and is compiled using TF2.x @tf.function annotation.

It uses a dry-run of the model to pre-create variables using tf.compat.v1.variable_scope(scope, reuse=tf.compat.v1.AUTO_REUSE):
https://github.com/olegmyrk/SPADE-Tensorflow/blob/4681ac60f2078144d55178f23fa885d3ed191450/SPADE.py#L1174
(it takes about 10 minutes to compile and run it in multi-gpu mode)

Then it runs the actual training step(s)
https://github.com/olegmyrk/SPADE-Tensorflow/blob/4681ac60f2078144d55178f23fa885d3ed191450/SPADE.py#L1257
(it takes about 20 minutes to compile and run it for the first time in multi-gpu mode)

It looks that just disabling lines with 'optim.apply_gradients'
https://github.com/olegmyrk/SPADE-Tensorflow/blob/4681ac60f2078144d55178f23fa885d3ed191450/SPADE.py#L1229
https://github.com/olegmyrk/SPADE-Tensorflow/blob/4681ac60f2078144d55178f23fa885d3ed191450/SPADE.py#L1231
https://github.com/olegmyrk/SPADE-Tensorflow/blob/4681ac60f2078144d55178f23fa885d3ed191450/SPADE.py#L1233
slashes about 10 out 20 minutes needed for initial run in multi-gpu mode. Which is essentially compiling back-propagation?

After the first training step that takes 20 minutes the following training steps run at normal speed.

The total number of mirrored parameters is around 500MB.

With 4 V100 GPUs training step is around 5x slower than with single V100 GPU.

Command:
NCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=ALL nohup python3 main.py --dataset CelebAMask-HQ --img_height 256 --img_width 256 --ch 16 --img_ch 3 --phase train --save_freq 10000 --batch_size 12 --gan_type hinge --code_gan_type gan --n_critic 1 --code_num_layers=4 --code_dist_num_layers=0 --augment_flag false --sn=False --train_main=true --train_nondet=true --lr 0.0002 --epoch=50 --decay_epoch=25 --print_freq 100 &> train.CelebAMask-HQ.log&

**Other info / logs**
[train.CelebAMask-HQ.slow_compile.log](https://github.com/tensorflow/tensorflow/files/3993026/train.CelebAMask-HQ.slow_compile.log)
"
35343,Is it a bug with tf.cond?,"Greetings,

I have a code that uses tf.conds as follows:

import tensorflow as tf

```
def func():
	aa = 0
	def abc():
		aa = [0]
		return tf.convert_to_tensor(True, dtype=bool)
	def ghi():
		b = aa[0]
		return tf.convert_to_tensor(False, dtype=bool)
	return a = tf.cond(tf.convert_to_tensor(True, dtype=bool), abc(), ghi())
a = func()
```

I suppose function ghi() should not be run (as it should runs func abc()). But somehow the code there is still running, resulting in an error of 'int' object is not subscriptable (from b = aa[0]).

Is it a bug? If not, could you explain more about this?

Thx."
35342,Support other types of Tensors in tf.data.Dataset.from_generator,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.15.2 (19C57)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:v2.0.0-rc2-26-g64c3d382ca 2.0.0
- **Python version**:3.7.5 (default, Oct 25 2019, 18:18:54) \n[Clang 11.0.0 (clang-1100.0.33.8)]

### Describe the problem

`tf.data.Dataset` supports RaggedTensor and SparseTensor but `tf.data.Dataset.from_generator` is limited to Tensors only. Please support other types of Tensors. 

### Source code / logs

```
    def data_get():
        for i in range(5):
            yield tf.ragged.constant([[i, i], [i]])

    ds = tf.data.Dataset.from_generator(data_get, tf.int32)

    for sample in ds:
        print(sample)
```

produces:

Traceback (most recent call last):

```
  File ""/Users/peak/IdeaProjects/TFmodels/venv-tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/script_ops.py"", line 221, in __call__
    ret = func(*args)

  File ""/Users/peak/IdeaProjects/TFmodels/venv-tf2/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 606, in generator_py_func
    ""element was %s."" % (dtype.name, ret))

TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was <tf.RaggedTensor [[0, 0], [0]]>.
```


"
35341,'ImportError' object has no attribute '_render_traceback_',"使用 jupyter 时候报错，

import tensorflow as tf

ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 3319, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-8eea31c9b10b>"", line 2, in <module>
    tf.keras.datasets.mnist.load_data(path='mnist.npz')
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""D:\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""D:\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2034, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\ultratb.py"", line 1101, in get_records
    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\ultratb.py"", line 319, in wrapped
    return f(*args, **kwargs)
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\ultratb.py"", line 353, in _fixed_getinnerframes
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
  File ""D:\Anaconda3\envs\tensorflow\lib\inspect.py"", line 1495, in getinnerframes
    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
  File ""D:\Anaconda3\envs\tensorflow\lib\inspect.py"", line 1453, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File ""D:\Anaconda3\envs\tensorflow\lib\inspect.py"", line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File ""D:\Anaconda3\envs\tensorflow\lib\inspect.py"", line 733, in getmodule
    if ismodule(module) and hasattr(module, '__file__'):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""D:\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""D:\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 3319, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-8eea31c9b10b>"", line 2, in <module>
    tf.keras.datasets.mnist.load_data(path='mnist.npz')
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""D:\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""D:\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2034, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 3319, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-8eea31c9b10b>"", line 2, in <module>
    tf.keras.datasets.mnist.load_data(path='mnist.npz')
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""D:\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""D:\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2034, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 3242, in run_ast_nodes
    if (await self.run_code(code, result,  async_=asy)):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 3336, in run_code
    self.showtraceback(running_compiled_code=True)
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2037, in showtraceback
    value, tb, tb_offset=tb_offset)
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\ultratb.py"", line 1385, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context)
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\ultratb.py"", line 1288, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\ultratb.py"", line 1150, in structured_traceback
    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)
TypeError: can only concatenate str (not ""list"") to str

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2034, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'TypeError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\ultratb.py"", line 1101, in get_records
    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\ultratb.py"", line 319, in wrapped
    return f(*args, **kwargs)
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\ultratb.py"", line 353, in _fixed_getinnerframes
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
  File ""D:\Anaconda3\envs\tensorflow\lib\inspect.py"", line 1495, in getinnerframes
    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
  File ""D:\Anaconda3\envs\tensorflow\lib\inspect.py"", line 1453, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File ""D:\Anaconda3\envs\tensorflow\lib\inspect.py"", line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File ""D:\Anaconda3\envs\tensorflow\lib\inspect.py"", line 733, in getmodule
    if ismodule(module) and hasattr(module, '__file__'):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""D:\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""D:\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 3319, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-8eea31c9b10b>"", line 2, in <module>
    tf.keras.datasets.mnist.load_data(path='mnist.npz')
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""D:\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""D:\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2034, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 3242, in run_ast_nodes
    if (await self.run_code(code, result,  async_=asy)):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 3336, in run_code
    self.showtraceback(running_compiled_code=True)
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2037, in showtraceback
    value, tb, tb_offset=tb_offset)
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\ultratb.py"", line 1385, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context)
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\ultratb.py"", line 1288, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\ultratb.py"", line 1150, in structured_traceback
    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)
TypeError: can only concatenate str (not ""list"") to str

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2034, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'TypeError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

D:\Anaconda3\envs\tensorflow\lib\imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

D:\Anaconda3\envs\tensorflow\lib\imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: DLL load failed: 找不到指定的模块。

During handling of the above exception, another exception occurred:


During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py in showtraceback(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)
   2033                         # in the engines. This should return a list of strings.
-> 2034                         stb = value._render_traceback_()
   2035                     except Exception:

AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py in run_code(self, code_obj, result, async_)
   3334             if result is not None:
   3335                 result.error_in_exec = sys.exc_info()[1]
-> 3336             self.showtraceback(running_compiled_code=True)
   3337         else:
   3338             outflag = False

D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py in showtraceback(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)
   2035                     except Exception:
   2036                         stb = self.InteractiveTB.structured_traceback(etype,
-> 2037                                             value, tb, tb_offset=tb_offset)
   2038 
   2039                     self._showtraceback(etype, value, stb)

D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\ultratb.py in structured_traceback(self, etype, value, tb, tb_offset, number_of_lines_of_context)
   1383         self.tb = tb
   1384         return FormattedTB.structured_traceback(
-> 1385             self, etype, value, tb, tb_offset, number_of_lines_of_context)
   1386 
   1387 

D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\ultratb.py in structured_traceback(self, etype, value, tb, tb_offset, number_of_lines_of_context)
   1286             # Verbose modes need a full traceback
   1287             return VerboseTB.structured_traceback(
-> 1288                 self, etype, value, tb, tb_offset, number_of_lines_of_context
   1289             )
   1290         elif mode == 'Minimal':

D:\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\ultratb.py in structured_traceback(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)
   1148         exception = self.get_parts_of_chained_exception(evalue)
   1149         if exception:
-> 1150             formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)
   1151             etype, evalue, etb = exception
   1152         else:

TypeError: can only concatenate str (not ""list"") to str
"
35339,Tensorflow-lite gpu output is corrupted when using opencl backend,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android version:9, Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: OnePlus3, GPU:Adreno 530
- TensorFlow installed from (source or binary): tensorflow-lite-gpu 1.15, tensorflow-lite-gpu:0.0.0-nightly from 'https://mvnrepository.com'
- TensorFlow version (use command below): 1.14, 1.15


**Describe the current behavior**
I'm using tflite-gpu in my android application for semantic segmentation. Using my tflite model(all gpu supported ops) i'am able to get proper output with CPU version and tensorflow-lite-gpu 1.14 ;but when i use nightly or 1.15 it loads up opencl backend and gives corrupted output. This backend seems to take longer time to start up(5-10s); however it seems to be faster than corresponding opengl version. When i run the model to get a image output (float:0-1) there seems to be random rectangular blanks within the output. The input is 256x256x3(float) and output os 256x256x1(float). However.  i 'am not facing this issue using a different model with input size 128; even though i use the same back-end.
**Describe the expected behavior**
The tflite model should produce correct output with opencl backend like the  opengl version, regardless of input size.

**Other info / logs**
I'am getting correct output for model with 128 input size regardless of backends and devices; but for the model with 256 output size i'am not getting proper output with opencl backend (gpu-nightly and gpu-1.15)
[Models.zip](https://github.com/tensorflow/tensorflow/files/3992313/Models.zip)

Only the '**opencl gpu delegate with this 256 input sized-model**' produces this corrupted output; other versions (CPU, Opengl-GPU, 128 input-model with Opencl etc.) seems to produce correct result without the rectangular blanks.
![tmap54](https://user-images.githubusercontent.com/1130185/71320296-57515680-24cf-11ea-8c26-66f62c948e94.PNG)
"
35337,ValueError: name for name_scope must be a string when Building up a Custom Model,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

Hi,

I tried to follow tf's doc to build up a model as follows:
https://www.tensorflow.org/guide/keras/custom_layers_and_models

`
					
	class CBR(layers.Layer):
		'''
		Convolution + Batch normalisation + Relu
		'''
		def __int__(self, filterNum, kSize, strSize, padMode, name='cbr', **kwargs):
			super(CBR, self).__init__(name=name, **kwargs)
			self.conv3D = layers.Conv3D(filters=filterNum, kernel_size=kSize, strides=strSize, padding=padMode, data_format='channels_first')
			self.BN = layers.BatchNormalization(axis=1)
		def call(self, inputs):
			x = self.conv3D(inputs)
			x=self.BN(x)
			return layers.Relu(x)
			
	class SimpleUNet(tf.keras.Model, layers.Layer):
		'''
		Serialise basic units so as to build up a double-layered encoder-decoder U-Net
		Input:
			inDim: [mbSize, modaility/channel, tensor dimensions]
			classNum: background included
			name: name for the net
			inputs: 5D tf tensor of [mbSize, modaility/channel, tensor dimensions]. Inputs must be organised into channel first
		Returns:
			outputs: 5D tf tensor of [mbSize, classNum, tensor dimensions]
		'''
		def __init__(self, inDim, classNum, name='SimpleUNet', **kwargs):
			super(SimpleUNet, self).__init__(name=name, **kwargs)
			self.inDim = inDim
			self.classNum = classNum
			dimEnSt1End = np.array(inDim[1:])-2-2
			dimEnSt2Ed = dimEnSt1End/2-2-2
			dimBridgeEnd = (dimEnSt2Ed/2-2-2)*2
			dimDEStd1End = (dimBridgeEnd-2-2)*2
			outDim = dimDEStd1End-2-2-2
			temp = ((dimEnSt2Ed - dimBridgeEnd)/2).astype('int32')
			crop3d1 = tuple(np.tile(temp, (2, 1)).T)
			temp = ((dimEnSt1End - dimDEStd1End)/2).astype('int32')
			crop3d2 = tuple(np.tile(temp, (2, 1)).T)
			# list of basic units used in the model
			self.en_st1_cbr1 = CBR(32, 3, 1, 'valid')
			self.en_st1_cbr2 = CBR(64, 3, 1, 'valid')
			self.en_st2_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')
			self.en_st2_cbr1 = CBR(64, 3, 1, 'valid')
			self.en_st2_cbr2 = CBR(128, 3, 1, 'valid')
			self.bridge_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')
			self.bridge_cbr1 = CBR(128, 3, 1, 'valid')
			self.bridge_cbr2 = CBR(256, 3, 1, 'valid')    
			self.bridge_tconv1 = layers.Conv3DTranspose(256, 2, strides=2, padding='valid', data_format='channels_first')
			self.de_3dcrop1 = layers.Cropping3D(crop3d1, data_format='channels_first')
			self.de_st1_cbr1 = CBR(256, 3, 1, 'valid')
			self.de_st1_cbr2 = CBR(128, 3, 1, 'valid')    
			self.de_st1_tconv1 = layers.Conv3DTranspose(128, 2, strides=2, padding='valid', data_format='channels_first')
			self.de_3dcrop2 = layers.Cropping3D(crop3d2, data_format='channels_first')
			self.de_st2_cbr1 = CBR(64, 3, 1, 'valid')
			self.de_st2_cbr2 = CBR(64, 3, 1, 'valid') 
			self.final_conv3D = layers.Conv3D(filters=self.classNum, kernel_size=3, strides=1, padding='valid', data_format='channels_first')
					
		def call(self, inputs):
			x = self.en_st1_cbr1(inputs)
			xEnSt1End = self.en_st1_cbr2(x)
			x= self.en_st2_mp(xEnSt1End)
			x= self.en_st2_cbr1(x)
			xEnSt2Ed = self.en_st2_cbr2(x)
			x = self.bridge_mp(xEnSt2Ed)
			x = self.bridge_cbr1(x)
			x = self.bridge_cbr2(x)
			xBridgeEnd = self.bridge_tconv1(x)
			xCrop1 = self.de_3dcrop1(xEnSt2Ed)
			x = layers.Concatenate([xBridgeEnd, xCrop1], axis=1)
			x = self.de_st1_cbr1(x)
			x = self.de_st1_cbr2(x)
			xDeSt1End = self.de_st1_tconv1(x)
			xCrop2 = self.de_3dcrop2(xEnSt1End)
			x = layers.Concatenate([xDeSt1End, xCrop2], axis=1)
			x = self.de_st2_cbr1(x)
			x = self.de_st2_cbr2(x)
			x = self.final_conv3D(x)
			outputs = activations.softmax(x, axis=1)
			
			return outputs

`

Then I initialised it, and tried to build it by calling SUNet.build
`
	classNum = 3 
	mbSize = 16 
	inDim = [4, 64, 64, 64] 
	SUNet = SimpleUNet(inDim, classNum) 
	SUNet.build(input_shape=inDim)
`
I strictly followed the example given in tf's doc, but an error was raised when building up it
ValueError: name for name_scope must be a string.
It occurred here when CBR is called for the first time:

`
def __int__(self, filterNum, kSize, strSize, padMode, name='cbr', **kwargs): 
	super(CBR, self).__init__(name=name, **kwargs)
`

I cannot figure out any syntactic mistake. Could anyone give me a hand? Thanks a lot.
Or, is the model cannot be built at this moment until it is actually used when being called in the training?"
35336,No clear document explains how to use pre-trained model,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/applications

## Description of issue (what needs changing):

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 in Docker
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38
- Python version: 3.5
- CUDA/cuDNN version: 10.0 / 7
- GPU model and memory: GTX 1080Ti / 11175MiB


**Describe the current behavior**

Hi authors and developers,

I noticed that tensorflow doesn't provide a clear document explain how to use pre-trained model.

So, I wrote a benchmark which showed the accuracy of pre-trained model with applying imageNet' validation set.

The following is the result:

```
[Testing][pixel vales are from (0,255)][model:ResNet50] - loss: 2.711 - accuracy: 0.457
[Testing][pixel vales are from (0,255)][model:DenseNet121] - loss: 39.000 - accuracy: 0.006
[Testing][pixel vales are from (0,255)][model:MobileNetV2] - loss: 9.979 - accuracy: 0.003

[Testing][pixel vales are from (0,1)][model:ResNet50] - loss: 8.535 - accuracy: 0.001
[Testing][pixel vales are from (0,1)][model:DenseNet121] - loss: 1.895 - acc: 0.599
[Testing][pixel vales are from (0,1)][model:MobileNetV2] - loss: 2.283 - accuracy: 0.523

[Testing][pixel vales are normalized from (-1,1)][model:ResNet50] - loss: 8.313 - acc: 0.001
[Testing][pixel vales are normalized from (-1,1)][model:DenseNet121] - loss: 1.896 - acc: 0.599
[Testing][pixel vales are normalized from (-1,1)][model:MobileNetV2] - loss: 2.287 - acc: 0.524

```

First, we can see the accuracy is not comparable with the original result(Top-1 accuracy is 70% up).

I thought that this issue is I'm not sure which crop and pad method is applied in the original result.

Therefore, I defined a custom function `CenterCrop` to fit the model's input size.

But, we can skip this issues there.

What I want to mention is normalization issue.

If I don't apply any normalization(run_aug=1 in code), pixel's values are defined in **(0, 255)**.

All models' accuracy are near 0.001, except for resNet50 which achieves a meaningful accuracy.

If I do normalization(run_aug=2 in code), pixel's values are defined in **(0, 1)**.

This time, DenseNet121 and MobileNetV2 have a meaningful accuracy.

If I do standard normalization(run_aug=3 in code), pixel's values are defined in **(-1, 1)**.

The results are similar to previous case. But I'm sure why those two cases have same accuracy.

Those behavior let me confused.

Before applying pre-trained model, I have to which normalization method should be applied.

After reading the source code, I found that those applications are import from `keras_application` in `tensorflow`.

[keras-applications](https://github.com/keras-team/keras-applications)

[weight download](https://github.com/fchollet/deep-learning-models)

---

I didn't test other models, such as `ResNet50V2`, `InceptionV3` and `Xception` because their input size are `299` instead of `244` and this is a time consuming task.

However, anyone can modify the test case and do the benchmark.

---

Because of licence issue for ImageNet, I can't provide imagenet in public.

But the following is the minimal test case:

```python
# pip install tensorflow-gpu==1.14.0
# pip pandas
#%%
import time
import numpy as np
import pandas as pd
import tensorflow as tf

from glob import glob

#%%
# input image dimensions
img_h = 224
img_w = 224
channels = 3

# information for dataset
dataset_path = ""dataset-imagenet/""
num_classes = 1000
num_testing = 50000

#%%
class DataGenerator:

    def __init__(self, dataframe, batch_size, run_aug = True):

        self.total_len  = len(dataframe.index)
        self.batch_size = batch_size
        self.run_aug = run_aug
        self.dataframe  = dataframe
        self.on_epoch_end()

    def __build_pipeline(self, file_path, labelY):

        # mapping function in tf
        def preprocess_fn(file_path, labelY):

            def fn_x(img_array):

                img_array = img_array.numpy()

                if self.run_aug == 1:
                    # image's range is [0,255]
                    image = img_array

                if self.run_aug >= 2:
                    # image's range is [0,1]
                    image = img_array / 255.0

                if self.run_aug == 3:
                    # std normalization
                    image[0,:,:] -= 0.485
                    image[1,:,:] -= 0.456
                    image[2,:,:] -= 0.406
                    image[0,:,:] /= 0.229
                    image[1,:,:] /= 0.224
                    image[2,:,:] /= 0.225

                return image

            def fn_y(label):
                return tf.keras.utils.to_categorical(label , num_classes)

            # read image from files
            image = tf.io.read_file(file_path)
            image = tf.image.decode_image(image, channels=channels)
            aug_size = 256
            imageX = tf.compat.v1.image.resize_image_with_pad(image, aug_size, aug_size)
            imageX = tf.image.resize_with_crop_or_pad(image, img_h, img_w)

            # do normalizarion
            [imageX] = tf.py_function(fn_x, [imageX], [tf.float32])
            imageX.set_shape([img_h, img_w, channels])
            imageX = tf.image.random_flip_left_right(imageX)

            [labelY] = tf.py_function(fn_y, [labelY], [tf.float32])
            labelY.set_shape([num_classes])

            return imageX, labelY

        dataset = tf.data.Dataset.from_tensor_slices( (file_path, labelY) )
        dataset = dataset.shuffle(batch_size * 8)
        dataset = dataset.repeat()
        dataset = dataset.map(preprocess_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)
        dataset = dataset.batch(self.batch_size)
        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)

        self.dataset   = dataset

    def  __len__(self):

        return self.total_len // self.batch_size

    def on_epoch_end(self):

        cleanX = np.array(self.dataframe[""File""])
        totalY = np.array(self.dataframe[""One-hot""])

        # run permutation
        rand_idx = np.random.permutation(self.total_len)
        cleanX = cleanX[rand_idx]
        totalY = totalY[rand_idx]

        self.__build_pipeline(cleanX, totalY)

#%%
def build_clf(model_name):

    if model_name == ""ResNet50"":
        clf_model = tf.keras.applications.ResNet50(include_top=True, pooling='max', weights='imagenet')

    if model_name == ""DenseNet121"":
        clf_model = tf.keras.applications.DenseNet121(include_top=True, pooling='max', weights='imagenet')

    if model_name == ""MobileNetV2"":
        clf_model = tf.keras.applications.MobileNetV2(include_top=True, pooling='max', weights='imagenet')

    if model_name == ""InceptionV3"":
        clf_model = tf.keras.applications.InceptionV3(include_top=True, weights='imagenet')


    clf_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    return clf_model

#%%
def list_testing_data(classes, file_path, onehot_map):

    try:
        testing_data = pd.read_pickle('imagenet_test_list.pkl')
        print('[Successful] Testing_data loaded from pickle ...')
    except:
        testing_image_info = []
        for iter_class in classes:
            files = glob(os.path.join(file_path, iter_class, '*.JPEG'))
            for iter_img in files:
                data_info = [iter_img, iter_class]
                testing_image_info.append(data_info)

        testing_data = pd.DataFrame(testing_image_info, columns=['File', 'Class'])
        testing_data[""One-hot""] = testing_data[""Class""].replace(onehot_map, inplace=False)

        testing_data.to_pickle('imagenet_test_list.pkl')

    assert(testing_data.shape[0] == num_testing, ""[Fatal] Mismatched total length of testing data"")
    return testing_data

#%%
if __name__ == '__main__':

    # set GPU
    import os
    if os.environ.get(""CUDA_VISIBLE_DEVICES"") is None:
        os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""

    # Hyperparameters
    batch_size = 100
    epochs = 5

    # load one-hot labels
    file_path = dataset_path + 'val'
    classes = os.listdir(file_path)
    list_class = sorted( list( set(classes) ) )
    onehot_map = dict( zip( list_class, list(range(0, num_classes)) ))

    # load list of validation data, those data should be considered as testing data
    testing_data = list_testing_data(classes, file_path, onehot_map)

    # build data generator
    gen_type1 = DataGenerator(testing_data, batch_size, run_aug=1)
    gen_type2 = DataGenerator(testing_data, batch_size, run_aug=2)
    gen_type3 = DataGenerator(testing_data, batch_size, run_aug=3)
    gen_list = [gen_type1, gen_type2, gen_type3]

    # build model
    model_list = [""ResNet50"", ""DenseNet121"", ""MobileNetV2""]
    
    # print result for type1
    test_gen = gen_type1
    for model_name in model_list:
        model = build_clf(model_name)
        meta_string = '[Testing][pixel vales are from (0,255)][model:{:s}] '.format(model_name)
        prefix_string = ''
        output = model.evaluate(test_gen.dataset, steps = test_gen.__len__())
        for ii in range( len( model.metrics_names) ):
            meta_string = meta_string + '- {:s}{:s}: {:.3f} '.format(prefix_string, model.metrics_names[ii], output[ii])

        print(meta_string)

    # print result for type2
    test_gen = gen_type2
    for model_name in model_list:
        model = build_clf(model_name)
        meta_string = '[Testing][pixel vales are from (0,1)][model:{:s}] '.format(model_name)
        prefix_string = ''
        output = model.evaluate(test_gen.dataset, steps = test_gen.__len__())
        for ii in range( len( model.metrics_names) ):
            meta_string = meta_string + '- {:s}{:s}: {:.3f} '.format(prefix_string, model.metrics_names[ii], output[ii])

        print(meta_string)

    # print result for type3
    test_gen = gen_type3
    for model_name in model_list:
        model = build_clf(model_name)
        meta_string = '[Testing][pixel vales are normalized from (-1,1)][model:{:s}] '.format(model_name)
        prefix_string = ''
        output = model.evaluate(test_gen.dataset, steps = test_gen.__len__())
        for ii in range( len( model.metrics_names) ):
            meta_string = meta_string + '- {:s}{:s}: {:.3f} '.format(prefix_string, model.metrics_names[ii], output[ii])

        print(meta_string)
```

"
35335,Dataset scan loses variable modifications,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, providing source.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.15.2, most likely irrelevant.
- TensorFlow installed from (source or binary): binary from pip
- TensorFlow version (use command below): v1.12.1-21171-g9798f84fa9 2.1.0-dev20191221 (installed via pip install tf-nightly==2.1.0dev20191221)
- Python version: 3.7.2
- CUDA/cuDNN version: using CPU only.

**Describe the current behavior**
While writing a unit test I created a function that iterates a tf.data.Dataset and accumulates the values in a local variable. This worked fine using eager mode, but then I noticed that the returned result was zero when using tf.function.

I've produced a small simple code that reproduces the problem. In particular, returning the accumulator variable produces a result of 0, but accessing the variable directly works fine. Also, using tf.print on the accumulator while iterating the dataset shows the correct value, but printing it after the iteration still within the method shows 0, suggesting perhaps some kind of scoping problem.

Please see the attached source to understand better what I mean.

**Describe the expected behavior**
The result should be the same when using eager mode and tf.function. Also, when using tf.function the result should be the same when returning the variable and when accessing it directly.

**Code to reproduce the issue**
[tf_function_variable.py.txt](https://github.com/tensorflow/tensorflow/files/3992032/tf_function_variable.py.txt)"
35334,Question about tf.keras.backend switch in lambda layer for short circuit computation,"I am using tf.keras.backend.switch inside of a lambda layer in order to swap between multiple different ""expert"" layers based on the output of a ""gating"" layer. When the entire model is used to predict an output, does it evaluate the gating layer first and then ""short circuit"" to only evaluate the single selected expert layer (to save computation time)? If not, how could I implement this?

I am using tensorflow 2.0 gpu and the keras functional api

Update: I created a custom layer with multiple input/output tensors rather than using a Lambda layer, but the question above still stands."
35333,Optimizer within Estimator computes incorrectly small gradient when used with MirroredStrategy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: Anaconda, clean install: `conda create -n tf2 tensorflow-gpu=2.0.0`
- TensorFlow version: 2.0.0
- Python version: 3.7.5
- CUDA/cuDNN version: 10.0.130, 7.6.4
- GPU model and memory: Tesla V100, 16GB

**Describe the current behavior**
A gradient is reported that is only half of the expected gradient when using distributed training with Estimator. See example code below.
**Describe the expected behavior**
The gradient should be twice as large. If somehow this behavior is actually intended, then this needs to be much more loudly documented since it is quite unexpected - right now I cannot find any place where it is documented at all. For example, https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_estimator_limited_support and https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Optimizer#compute_gradients both have no mention of it.

**Code to reproduce the issue**
```
import sys
import os
import tensorflow as tf
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'

def model_fn(features,labels,mode,params):
  if mode == tf.estimator.ModeKeys.PREDICT:
    raise NotImplementedError()
  if mode == tf.estimator.ModeKeys.EVAL:
    raise NotImplementedError()
  if mode == tf.estimator.ModeKeys.TRAIN:
    w = tf.compat.v1.get_variable(initializer=tf.zeros([2]),name=""w"")
    losses = tf.square(w - labels)
    loss = tf.reduce_sum(losses)
    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)
    gradients = optimizer.compute_gradients(tf.reduce_sum(loss))
    global_step=tf.compat.v1.train.get_global_step()
    train_op = optimizer.apply_gradients(gradients,global_step=global_step)
    print_op = tf.print(
      w,
      losses,
      [(grad,var.name) for (grad,var) in gradients],
    )
    return tf.estimator.EstimatorSpec(
      mode,
      loss=loss,
      train_op=tf.group(train_op,print_op)
    )

def input_fn():
  dataset = tf.data.Dataset.from_tensors(({}, [1.,2.]))
  return dataset.repeat(1)

print(""SINGLE GPU"")
estimator = tf.estimator.Estimator(
  model_fn=model_fn,
  model_dir=""test_single_gpu/"",
  params={}
)
estimator.train(input_fn)

def input_fn():
  dataset = tf.data.Dataset.from_tensors(({}, [1.,2.]))
  return dataset.repeat(2)

print(""MULTI GPU"")
strategy = tf.distribute.MirroredStrategy(devices=[""/GPU:0"",""/GPU:1""])
estimator = tf.estimator.Estimator(
  model_fn=model_fn,
  model_dir=""test_multi_gpu/"",
  params={},
  config=tf.estimator.RunConfig(train_distribute = strategy)
)
estimator.train(input_fn)
```

**Other info / logs**
The relevant portion of the output of the above test case is:
```
SINGLE GPU
[0 0] [1 4] [([-2 -4], 'w:0')]
MULTI GPU
[0 0] [1 4] [([-1 -2], 'w:0')]
[0 0] [1 4] [([-1 -2], 'w:0')]
```
The model is attempting to fit two weights to be equal to 1 and 2, respectively, penalizing them by the squared error. The weights are initialized to 0.
* Single GPU case: We can see it reads one data element from the dataset, and correctly computes the loss as as ""1"" and ""4"", which are the squared differences. The gradients are correctly computed as ""-2"" and ""-4"", which indeed the mathematical derivatives of (x-1)^2 and (x-2)^2 at x = 0, respectively. So this is all correct.
* Multi GPU case: We can see each GPU independently and in parallel reads an element from the dataset, reading two elements in total. Each one correctly computes the loss as ""1"" and ""4"" as before on its own element. However, each one separately only computes ""-1"" and ""-2"" as the gradient. This is wrong, each one is a factor of 2 too small.

(edit: corrected linux version, some grammar edits)"
35330,Keras backend functions not working as intended?,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: 4.4.0-18362-Microsoft
- TensorFlow installed from: Anaconda default source
- TensorFlow version: 1.15
- Python version: 3.7.5
- CUDA/cuDNN version: 10.0
- GPU model and memory: GeForce RTX 2060

**Describe the current behavior**

I'm trying to implement a custom loss function based on a custom accuracy function that I'm already using to evaluate my model predictions on the test dataset. The conversion can't be 1:1 because I use numpy ""greater"" and ""equal"" functions that are not differentiable. I created thefore custom functions that approximate the latters but their behavior has some problems

**Describe the expected behavior**

In particular I can test if everything is fine by comparing the results obtained by my original custom accuracy f. and the new loss f. given the same input (my input are tensorflow predictions, I just inglobe them inside K.constant to convert them in tensors). What I noticed is that this line of code

```
eps = sys.float_info.epsilon
return 0.5*(y + 5 + K.sqrt(K.pow(y-5,2) + eps))
```

is problematic. In particular y is an array of float32 values in [1:10] range and the returned array, let's call it 'ret', should have ret[i]=max(5,y[i]) but sometimes the value of **5** becomes **4.9999995** instead. The next portion of my code is based on how many **5** are present and therefore I can't ignore this problem.

The fact is that, let's say a problematic index is 'w' so that ret[w]=4.9999995 instead of 5, if I use the same code with y now equal to only y[w] the returned array is correctly 5. This means that somehow if y is a batch of predictions and not just one something isn't working. This should not be the case because both K.sqrt and K.pow works element wise, it should not matter if y is an array of 1 or multiple values 

Out of almost 20k predictions, around 1k have this same problem and it is deterministic (always the same are problematic). I also tried to use:

```
eps = sys.float_info.epsilon
return 0.5*(y + 5 + np.sqrt(np.pow(y-5,2) + eps))
```

and the problem is gone thefore it is related to Keras backend.

Last info, I tried to use also:

```
eps = sys.float_info.epsilon
return tf.math.ceil(0.5*(y + 5 + K.sqrt(K.pow(y-5,2) + eps)))
```

but this completely ruins the returned value, sometimes real numbers such as 4.5 are rounded to 6 instead of 5

If more informations are needed I can provide them"
35329,Error: Cannot convert 'auto' to EagerTensor of dtype float,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/keras/losses/Reduction?version=stable

## Description of issue (what needs changing):

I intend to build up a custom loss function as follows:


`	from __future__ import absolute_import, division, print_function, unicode_literals
	import functools

	import numpy as np
	import tensorflow as tf


	class GeneralDiceLoss(tf.keras.losses.Loss):
		def __init__(self, reduction=tf.keras.losses.Reduction.AUTO, name='GeneralDiceLoss'):
			super().__init__(reduction=reduction, name=name)
			self.epsilon = 1e-16 
		
		
		def get_config(self):
			config = super(GeneralDiceLoss, self).get_config()
			return config
		
		def call(self, yPred, yTrue):
			#yTrue =tf.dtypes.cast(yTrue, dtype=yPred.dtype)
			# Dot product yPred and yTrue and sum them up for each datum and class
			crossProd=tf.multiply(yPred, yTrue)
			crossProdSum=tf.math.reduce_sum(crossProd, axis=np.arange(2, yTrue.ndim))
			# Calculate weight for each datum and class 
			weight = tf.math.reduce_sum(yTrue, axis=np.arange(2, yTrue.ndim))
			weight = tf.math.divide(1, tf.math.square(weight)+self.epsilon)
			# Weighted sum over classes
			numerator = 2*tf.math.reduce_sum(tf.multiply(crossProdSum, weight), axis=1)
			# Saquared summation 
			yySum = tf.math.reduce_sum(tf.math.square(yPred) + tf.math.square(yTrue), axis=np.arange(2, yTrue.ndim))
			# Weighted sum over classes
			denominator = tf.math.reduce_sum(tf.multiply(weight, yySum), axis=1)
			loss = 1 - tf.math.divide(numerator, denominator+self.epsilon)
			#loss = tf.math.reduce_mean(1 - tf.math.divide(numerator, denominator+self.epsilon))
			
			return loss
`

Then I create variables to have it test
`

	GeneralDiceLoss()
	yPred = tf.random.uniform(shape=(16, 3, 4, 4, 4))
	yTrue = tf.round(tf.random.uniform(shape=(16, 3, 4, 4, 4)))

	loss=GeneralDiceLoss(yPred, yTrue)
`
But I got an error
`

	  File ""...\keras-gpu\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 96, in convert_to_eager_tensor
		return ops.EagerTensor(value, ctx.device_name, dtype)

	TypeError: Cannot convert 'auto' to EagerTensor of dtype float
`

In the doc above, 
1) there is NO clear indication or warning about conversion issue, not to mention there is NO dtype conversion in my code at all. 
2) there is NO clear example indicating which option, AUTO or SUM_OVER_BATCH_SIZE, should be adopted in one's minbatch size is greater than 1. In my case, assume my batch is 16 as exhibted in yPred and yTrue above, shall I use

`
			loss = 1 - tf.math.divide(numerator, denominator+self.epsilon)
`
or 
`
			loss = tf.math.reduce_mean(1 - tf.math.divide(numerator, denominator+self.epsilon))
`
And for which option?

Building up a custom layer/loss function is already a tough task for many practitioners, so could the doc provide more detailed explanations and examples so as to make users' life a little bit easier? Many thanks."
35327,TF1.15 fails to dropout tf.Tensor,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): False
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS (using dockerfile from nvidia/cuda:10.0-cudnn7-devel)
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): 1.15
- Python version: 3.7.3
- CUDA/cuDNN version: CUDA 10.0, cuDNN7
- GPU model and memory: GTX 1080 Ti

**Describe the current behavior**
When I dropout a `tf.Tensor` object by using `tf.layers.dropout`, it fails.

**Describe the expected behavior**
Successfully dropout input object as in tf 1.14.

**Code to reproduce the issue**
```
import tensorflow as tf
e = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
h = tf.layers.dropout(e, noise_shape=tf.shape(e), rate=0.5, training=True)
```
The code above works in tensorflow-gpu==1.14

**Other info / logs**
```
    layers/core.py:226 call
        return super(Dropout, self).call(inputs, training=training)
    keras/layers/core.py:166 call
        lambda: array_ops.identity(inputs))
    keras/utils/tf_utils.py:59 smart_cond
        pred, true_fn=true_fn, false_fn=false_fn, name=name)
    framework/smart_cond.py:54 smart_cond
        return true_fn()
    keras/layers/core.py:160 dropped_inputs
        noise_shape=self._get_noise_shape(inputs),
    keras/layers/core.py:149 _get_noise_shape
        for i, value in enumerate(self.noise_shape):
    framework/ops.py:547 __iter__
        self._disallow_iteration()
    framework/ops.py:543 _disallow_iteration
        self._disallow_in_graph_mode(""iterating over `tf.Tensor`"")
    framework/ops.py:523 _disallow_in_graph_mode
        "" this function with @tf.function."".format(task))

    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.
```"
35326,dataset_ops.py   'NoneType' object has no attribute 'device',"**System information**
- Ubuntu 16.04
- TensorFlow and Tensorflow_Datasets installed from conda
- TensorFlow version 2.0
- Python version: 3.7
- GPU : None

**Describe the current behavior**
when I run the code form [segmentation](https://www.tensorflow.org/tutorials/images/segmentation?hl=en)
after plt.show()
it run well in jupyter but this error occurred in *.py
```
Traceback (most recent call last):
  File ""/home/cirno/anaconda3/envs/pytf/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 3009, in __del__
AttributeError: 'NoneType' object has no attribute 'device'
Exception ignored in: <function _MemoryCacheDeleter.__del__ at 0x7f839a9344d0>
Traceback (most recent call last):
  File ""/home/cirno/anaconda3/envs/pytf/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 2944, in __del__
AttributeError: 'NoneType' object has no attribute 'device'
Exception ignored in: <function _RandomSeedGeneratorDeleter.__del__ at 0x7f839a9347a0>
Traceback (most recent call last):
  File ""/home/cirno/anaconda3/envs/pytf/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 3009, in __del__
AttributeError: 'NoneType' object has no attribute 'device'
```

**Code to reproduce the issue**
[segmentation](https://www.tensorflow.org/tutorials/images/segmentation?hl=en)

**Other info **
in dataset_ops.py
add `import tensorflow as tf`
and
line: 2944 and 3009   `with ops.device(self._device):` modify to  `with tf.device(self._device):`
It looks like it's solved"
35320,"Missing file when using the hexagon delegate, solved now with instructions.","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution :16.04 :
- Mobile device: Samsung Galaxy S9:
- TensorFlow installed from source:
- TensorFlow version: TF mainline
- Python version: 3.6
- Installed using virtualenv? N/A
- Bazel version (if compiling from source): 1.2.1
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: 1080TI



**Describe the problem**
Missing file when using the hexagon delegate. Solved now with the instructions.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
run the hexagon delegate example.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35319,TF2.0 fails post-training uint8 quantization,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): installed TF binary 2.0 with Conda
- TensorFlow version (or github SHA if from source): TF 2.0


**Command used to run the converter or code if you’re using the Python API**

```
converter = tf.lite.TFLiteConverter.from_saved_model('model_mnist.hd5')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

images = tf.cast(X_train, tf.float32)
mnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)
def representative_data_gen():
    for input_value in mnist_ds.take(100):
        yield[input_value]
converter.representative_dataset = representative_data_gen

tflite_quant_model = converter.convert()
with open('model_mnist_quant_uint8.tflite', 'wb') as f:
    f.write(tflite_quant_model)


interpreter = tf.lite.Interpreter(model_path='model_mnist_quant_uint8.tflite')
interpreter.allocate_tensors()

img = X_train[0] * 255
img = img.astype('uint8')
print(interpreter.get_input_details())
interpreter.set_tensor(interpreter.get_input_details()[0]['index'], np.expand_dims(img, axis=0))
```

**The output from the converter invocation**

```
[{'name': 'flatten_input', 'index': 11, 'shape': array([ 1, 28, 28]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-12-5a2fa86c9de2> in <module>
      5 img = img.astype('uint8')
      6 print(interpreter.get_input_details())
----> 7 interpreter.set_tensor(interpreter.get_input_details()[0]['index'], np.expand_dims(img, axis=0))

~\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\lite\python\interpreter.py in set_tensor(self, tensor_index, value)
    344       ValueError: If the interpreter could not set the tensor.
    345     """"""
--> 346     self._interpreter.SetTensor(tensor_index, value)
    347 
    348   def resize_tensor_input(self, input_index, tensor_size):

~\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\lite\python\interpreter_wrapper\tensorflow_wrap_interpreter_wrapper.py in SetTensor(self, i, value)
    134 
    135     def SetTensor(self, i, value):
--> 136         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_SetTensor(self, i, value)
    137 
    138     def GetTensor(self, i):

ValueError: Cannot set tensor: Got tensor of type UINT8 but expected type FLOAT32 for input 11, name: flatten_input 
```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

**Failure details**
The conversion is successful, but the generated model is wrong. The input tensor dtype should be uint8, but is still float32.

I tried the same thing with TF 1.15.0. In this case, every things works as expected. Here is the result with TF 1.15.0
```
[{'name': 'flatten_input', 'index': 11, 'shape': array([ 1, 28, 28], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.003921568859368563, 0)}]
```


**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35318,TensorFlow Lite Micro fully connected int8 test passes illegal filter offset,"An external developer pointed out that the test for the quantized fully connected operation passes in a non-zero weight offset to the kernel for int8 tests:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/kernels/fully_connected_test.cc#L118-L119

The quantization specification promises that int8 kernels will always receive zero weight offsets:
https://www.tensorflow.org/lite/performance/quantization_spec

This failing test is preventing an optimized kernel for a hardware platform from being accepted."
35317,Slicing tensor within a keras.utils.Sequence with multiprocessing=True hangs fit_generator,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04
- TensorFlow installed from (source or binary):  binary
- TensorFlow version (use command below):  2.0.0
- Python version:  3.6.8
- CUDA/cuDNN version:  10.2 / 7.6.5.32-1+cuda10.2
- GPU model and memory:  NVidia Titan RTX 24218 MiB

**Describe the current behavior**

When I attempt to slice a tensor inside a keras.util.Sequence from model.fit_generator with multiprocessing=True, TensorFlow hangs forever without reporting any error or using any CPU or GPU cycles.  It works as expected when multiprocessing=False.

**Describe the expected behavior**

TensorFlow should correctly fit the model just as it does with multiprocessing=False

**Code to reproduce the issue**

In order to reproduce, substitute my_jpeg for some jpeg on your computer (hopefully with dimension greater than 224px).  Note that if you set use_multiprocessing=False in the example below, then this will correctly train the model.

```
from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf
from tensorflow import keras

# In order to reproduce, just use whatever random JPEG you have handy here.
# It should be larger than my_crop in the x and y dimension.
my_jpeg = ""/home/ben/my_jpeg.jpg""
my_crop = 224

# Generates a single crop for TensorFlow.
class DataGenerator(keras.utils.Sequence):
  def __init__(
      self,
      image_location,
      crop_size=224):
    self._image_location = image_location
    self._crop_size = crop_size

  # Just one single batch will be returned, of just one single image.
  def __len__(self):
    return 1

  # Generate one batch of data.
  def __getitem__(self, index):
    # Where the tensors will be stored.
    X = []
    y = [1]

    # Read it.
    image = tf.io.read_file(self._image_location)

    # Load it.
    image = tf.image.decode_jpeg(image, channels=3)

    assert image.shape[2] == 3  # MUST be RGB.
    height = image.shape[0]
    width = image.shape[1]

    # Just take a trivial crop of the image.
    # This is the offending line operation which hangs forever.
    image = image[0:self._crop_size, 0:self._crop_size, :]

    # This line is equivalent to above, and it also hangs with multiprocessing enabled.
    # image = tf.slice(image, [0, 0, 0], [self._crop_size, self._crop_size, 3])

    X.append(tf.dtypes.cast(image, tf.float32))

    # Tensors are not generally assignable, but we can create them from a number of existing ones.
    X = tf.stack(X)
    y = tf.stack(y)

    # Preprocess it.
    X /= 255.0  # Normalize to [0, 1] range.

    return X, y

generator = DataGenerator(my_jpeg, my_crop)

model = tf.keras.applications.ResNet50(input_shape=(my_crop, my_crop, 3))

model.compile(loss='mse')

# use_multiprocessing=False works.
# use_multiprocessing=True hangs.
model.fit_generator(generator, use_multiprocessing=True, workers=2)
```

"
35316,Where is the pip package for TF 1.15?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu (or similar)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): N/A
- TensorFlow version: 1.15
- Python version: python3
- Installed using virtualenv? pip? conda?: virtualenv and pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**
Version 1.15 of TF does not seem to exist on pip, but the documentation [states that it does](https://www.tensorflow.org/install/pip?lang=python3#3.-install-the-tensorflow-pip-package)

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
pip3 install tensorflow==1.15
Collecting tensorflow==1.15
  Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)
No matching distribution found for tensorflow==1.15
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Version 1.15 is required for einsum gradients.
"
35314,Lack of dataset length or cardinality causes `BaseCollectiveExecutor::StartAbort Out of range` issues,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I borrowed someone's code for this particular issue.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source install through Conda.
- TensorFlow version (use command below): 2.0.0
- Python version: python 3.7.
- Bazel version (if compiling from source): NA.
- GCC/Compiler version (if compiling from source): Cuda 10.1
- CUDA/cuDNN version: 7.4
- GPU model and memory: Nvidia GTX 1080

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

There is an existing error https://github.com/tensorflow/tensorflow/issues/32817 the references 
an issue that I had raised before. The problem is that when the batch size does not divide evenly into the total number of examples, you can an error `BaseCollectiveExecutor::StartAbort Out of range.` This is actually a pretty confusing and un-intuitive error message. The code to reproduce this error is below. 

The problem is that this error message relates to a closed issue that I had raised before (https://github.com/tensorflow/tensorflow/issues/26966). There seems to be no good way to compute the cardinality or number of examples in a dataset. There is a function `tf.data.experimental.cardinality` that will find the cardinality of a dataset if say the dataset comes from a pandas dataframe or something, but generally not from some large CSV file or text file. 

Also, say I actually calculate the cardinality of the dataset by manually counting over the batches of examples. The problem is that there is no where to record this information in the Dataset API itself. So I have to either manually hardcode this in a file, or recompute the length each time I run a training loop, which takes a lot of time. 

Is there a better strategy for this? Like is there a way to encode some metadata into a tensorflow Dataset API? Or perhaps a better and more recommended way is to create a Tensorflow Dataset in the style of the https://www.tensorflow.org/datasets , i.e., `tfds`. 

**Describe the expected behavior**

I should not get a warning message for the `BaseCollectiveExecutor::StartAbort Out of range` message because the total number of examples in the dataset does not divide evenly into the batch size. Alternatively, I should be able to compute the total dataset size using the `tf.data.experimental.cardinality` function for csv files. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Code taken from: https://github.com/tensorflow/tensorflow/issues/32817#issuecomment-539200561

data = tf.random.normal((60000,30,4))
ground_truth = tf.ones((60000,1))
dataset = tf.data.Dataset.from_tensor_slices((data, ground_truth)).batch(64)

#predefined model here: input: [?, 30,4] output: [?,1]
model.fit(dataset, epochs=5)

'''
    938/Unknown - 16s 17ms/step - loss: 0.02172019-10-07 14:49:49.928619: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
         [[Shape/_2]]
2019-10-07 14:49:49.928619: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
938/938 [==============================] - 16s 17ms/step - loss: 0.0217
Epoch 2/5
935/938 [============================>.] - ETA: 0s - loss: 2.2229e-062019-10-07 14:49:59.722216: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
2019-10-07 14:49:59.722218: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
         [[Shape/_2]]
'''

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35311,Tensorflow build from source - choose patch version,"**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14
- Python version: 3.5
- Installed using virtualenv? pip? conda?: using conda
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 7.4
- CUDA/cuDNN version: CUDA 10.0, CUDNN 7
- GPU model and memory: GeForce RTX 2080 8 GB



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I'm building tensorflow 1.14 from source because I'm working with an old cpu.

Due to compatibility issues I *MUST* use version 1.14.0, however I always end up with the pip package for version 1.14.1. I read in the documentation that it was related to the bazel version installed so I downgraded to 0.24.1 (as pointed in the docs) but still, i get version 1.14.1"
35310,Execution hangs after particular step in CUDA 10.1 TF 1.14.0,"### System information
- Have I written custom code: Yes
- OS Platform and Distribution: 18.04.3 LTS Ubuntu
- TensorFlow installed : pre installed container
- TensorFlow version : 1.14.0
- Python version: 3.5.2
- Bazel version (if compiling from source): - 
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: Tesla V100-SXM3-32GB
- Exact command to reproduce: Running the command python train_blstm.py

### Describe the problem
I am working on an Bi-LSTM + CTC Loss + WordBeamSearch Architecture for online handwriting recognition. The problem is that the code hangs/stalls after a particular point everytime. I have also let it run for 2 days now but it's still the same. Any help in this regard would be appreciated.

### Source code / logs
Here is the traceback of the issue attached below:
```

2019-12-20 14:18:51.291556: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
W1220 14:18:54.480478 139982147983168 deprecation_wrapper.py:119] From train_blstm_dec_19.py:295: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

W1220 14:18:54.481328 139982147983168 deprecation_wrapper.py:119] From train_blstm_dec_19.py:155: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

data_dir: ./
checkpoints_dir: ../checkpoints/
log_dir1: ../train_log/
restore_path: None
batch_size: 128
total_epoches: 300
hidden_size: 128
num_layers: 2
input_dims: 10
num_classes: 80
save_freq: 5
learning_rate: 0.001
decay_rate: 0.99
momentum: 0.9
max_length: 1940.0
label_pad: 63
if_valid_vr: False
W1220 14:18:59.086257 139982147983168 deprecation_wrapper.py:119] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:30: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

W1220 14:18:59.098966 139982147983168 deprecation_wrapper.py:119] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:31: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W1220 14:18:59.106423 139982147983168 deprecation.py:323] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:36: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W1220 14:18:59.112254 139982147983168 deprecation_wrapper.py:119] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:113: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W1220 14:18:59.112922 139982147983168 deprecation_wrapper.py:119] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:116: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

W1220 14:18:59.113215 139982147983168 deprecation.py:323] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:116: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.
W1220 14:18:59.115090 139982147983168 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/rnn/python/ops/rnn.py:239: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
W1220 14:18:59.115478 139982147983168 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
W1220 14:18:59.192415 139982147983168 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W1220 14:18:59.200585 139982147983168 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
stack_bidirectional_dynamic_rnn: Tensor(""blstm/Reshape:0"", shape=(?, 1940, 2, 128), dtype=float32)
W1220 14:19:00.570831 139982147983168 deprecation_wrapper.py:119] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:122: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

Tensor(""blstm/unstack:0"", shape=(?, 2, 128), dtype=float32)
Tensor(""blstm/add:0"", shape=(?, 128), dtype=float32)
Tensor(""blstm/stack:0"", shape=(1940, ?, 80), dtype=float32)
Tensor(""blstm/stack:0"", shape=(1940, ?, 80), dtype=float32)
W1220 14:19:08.754076 139982147983168 deprecation_wrapper.py:119] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:87: The name tf.nn.ctc_loss is deprecated. Please use tf.compat.v1.nn.ctc_loss instead.

W1220 14:19:08.757078 139982147983168 deprecation_wrapper.py:119] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:89: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

Tensor(""ctc_loss/Mean:0"", shape=(), dtype=float32)
W1220 14:19:08.758585 139982147983168 deprecation_wrapper.py:119] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:93: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W1220 14:20:39.822741 139982147983168 deprecation.py:506] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:101: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.
Instructions for updating:
dim is deprecated, use axis instead
W1220 14:20:39.824999 139982147983168 deprecation_wrapper.py:119] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:41: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

W1220 14:20:39.827233 139982147983168 deprecation_wrapper.py:119] From train_blstm_dec_19.py:203: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

2019-12-20 14:20:39.892933: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-12-20 14:20:40.039163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM3-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.597
pciBusID: 0000:be:00.0
2019-12-20 14:20:40.039210: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-12-20 14:20:40.041126: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-12-20 14:20:40.042838: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-12-20 14:20:40.043132: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-12-20 14:20:40.044996: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-12-20 14:20:40.046033: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-12-20 14:20:40.049918: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-12-20 14:20:40.056223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-12-20 14:20:40.096725: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2700000000 Hz
2019-12-20 14:20:40.146344: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xf2c8a20 executing computations on platform Host. Devices:
2019-12-20 14:20:40.146400: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-12-20 14:20:40.580239: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xcd331c0 executing computations on platform CUDA. Devices:
2019-12-20 14:20:40.580287: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-SXM3-32GB, Compute Capability 7.0
2019-12-20 14:20:40.583875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM3-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.597
pciBusID: 0000:be:00.0
2019-12-20 14:20:40.583911: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-12-20 14:20:40.583946: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-12-20 14:20:40.583968: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-12-20 14:20:40.583978: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-12-20 14:20:40.583987: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-12-20 14:20:40.583997: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-12-20 14:20:40.584007: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-12-20 14:20:40.591236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-12-20 14:20:40.591303: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-12-20 14:20:41.297763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-20 14:20:41.297816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-12-20 14:20:41.297824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-12-20 14:20:41.305712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30466 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM3-32GB, pci bus id: 0000:be:00.0, compute capability: 7.0)
2019-12-20 14:27:51.088076: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10

```"
35309,tf.math.sigmoid precision issues on GPU,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: `2.1.0.dev20191219`
- Python version: 3.6.8
- CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.3
- GPU model and memory: GTX 1060 6GB

**Describe the current behavior**

We compared TensorFlow versions `2.1.0.dev20191203` and `2.1.0.dev20191219` and found some precision differences when using `tf.math.sigmoid`. Is that expected and what is the related commit?

Some results are improved (see last section) but we also find some inconsistent values on GPU when the tensor size is changing.

**Describe the expected behavior**

Sigmoid results should not depend on the tensor size.

**Code to reproduce the issue**

On GPU, going from 3 to 4 elements changes the result:

```python
>>> tf.sigmoid([34.0, 0.0, 0.0])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([1. , 0.5, 0.5], dtype=float32)>
>>> tf.sigmoid([34.0, 0.0, 0.0, 0.0])
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.99999994, 0.5       , 0.5       , 0.5       ], dtype=float32)
```

This is especially problematic when taking the log of the sigmoid output. For reference, this is not an issue on CPU:

```python
>>> tf.sigmoid([34.0, 0.0, 0.0])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([1. , 0.5, 0.5], dtype=float32)>
>>> tf.sigmoid([34.0, 0.0, 0.0, 0.0])
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1. , 0.5, 0.5, 0.5], dtype=float32)>
```

**Other info / logs**

Here is an example of improved precision:

In `2.1.0.dev20191203`:

```python
>>> tf.sigmoid(-20.0)
<tf.Tensor: shape=(), dtype=float32, numpy=0.>
```

In `2.1.0.dev20191219`:

```python
>>> tf.sigmoid(-20.0)
<tf.Tensor: shape=(), dtype=float32, numpy=2.0611537e-09>
```"
35308,Why does the cpu bias_op support only up to five dims?,"I'm currently using tf.1.14 on Windows 10, but it doesn't seem to matter too much.

I have a high-dimensional tensor object which I try to process with a Keras Dense layer. On the GPU everything works perfectly fine, but if I want to run the network on the CPU I get the following error ""Only ranks up to 5 supported:.... "".  A quick look into the C code shows that the dimensionality handling is, indeed, hardcoded:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/bias_op.cc#L102

My question is only why? It seems like it might have something to do with the way `GetBiasValueDims()` is written, but not necessarily."
35307,"I got nan value when I to predict in cpu,this model that I trained it in gpu","when using a trained pix2pix model to predict in cpu ,but I got a nan value .if I use the model that trained in CPU，its work right. 
**this is trained by cup：**
`[[[0.4903388  0.50368387 0.49226668]
  [0.50147295 0.5219532  0.5668285 ]
  [0.5083101  0.56787324 0.45141172]
  ...
  [0.56986064 0.52189714 0.5279328 ]
  [0.5258558  0.5492905  0.435853  ]
  [0.48710588 0.4718867  0.4559199 ]]

 [[0.4601045  0.45254436 0.45107806]
  [0.50491637 0.48829857 0.5550411 ]
  [0.5183163  0.5770218  0.5410818 ]
  ...
  [0.42861402 0.48687014 0.50826234]
  [0.50582343 0.47616833 0.49303797]
  [0.45225543 0.47339994 0.4373474 ]]

 [[0.42919013 0.4317314  0.4555189 ]
  [0.49468514 0.5151671  0.4483238 ]
  [0.50465244 0.48404706 0.4806079 ]
  ...
  [0.4833022  0.54902816 0.40051547]
  [0.4632537  0.45243093 0.49023414]
  [0.48761165 0.45556885 0.46273252]]

 ...

 [[0.43931952 0.4372336  0.4398279 ]
  [0.37421006 0.44697812 0.45707896]
  [0.43218714 0.42247662 0.4385087 ]
  ...
  [0.47185218 0.5173737  0.45031384]
  [0.48594344 0.43559843 0.4615797 ]
  [0.45971525 0.44595918 0.48696217]]

 [[0.45223635 0.41471708 0.40041572]
  [0.46029133 0.4413036  0.41087914]
  [0.44727382 0.42518058 0.3782655 ]
  ...
  [0.44791928 0.49392545 0.5107697 ]
  [0.46841094 0.449826   0.4900933 ]
  [0.44273183 0.4620495  0.4870268 ]]

 [[0.47757632 0.4741067  0.39477357]
  [0.39080092 0.4166201  0.4097267 ]
  [0.43215263 0.43118754 0.40830547]
  ...
  [0.45276994 0.4164047  0.45256376]
  [0.3767779  0.43754327 0.44750926]
  [0.50948536 0.4650667  0.49479547]]`

this is trained by gpu:
`[[[nan nan nan]
  [nan nan nan]
  [nan nan nan]
  ...
  [nan nan nan]
  [nan nan nan]
  [nan nan nan]]

 [[nan nan nan]
  [nan nan nan]
  [nan nan nan]
  ...
  [nan nan nan]
  [nan nan nan]
  [nan nan nan]]

 [[nan nan nan]
  [nan nan nan]
  [nan nan nan]
  ...
  [nan nan nan]
  [nan nan nan]
  [nan nan nan]]

 ...

 [[nan nan nan]
  [nan nan nan]
  [nan nan nan]
  ...
  [nan nan nan]
  [nan nan nan]
  [nan nan nan]]

 [[nan nan nan]
  [nan nan nan]
  [nan nan nan]
  ...
  [nan nan nan]
  [nan nan nan]
  [nan nan nan]]

 [[nan nan nan]
  [nan nan nan]
  [nan nan nan]
  ...
  [nan nan nan]
  [nan nan nan]
  [nan nan nan]]]`"
35306,A possible bug in ConvRNN2D __call__,"Referring to
[ConvRNN2D.__call__](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional_recurrent.py#L294-L341)

L308 ` kwargs['initial_state'] = initial_state` and L317 `kwargs['constants'] = constants` should be added in the else block at L340. The current situation contradicts with the use of `full_input` at L337.

I am not sure if we can simply replicate the code from its parent [RNN.__call__](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/recurrent.py#L640-L700).

I went ahead and tried it, but after loading the saved model weights in a complete new python session the results on validation data doesn't match at all.

I would appreciate a quick fix for local edit at least.

Thanks

"
35305,runnin magic_wand on efr/efm32 microcontroller,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 ( Windows SL)
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source):
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): target : efr32/efm32 (tried to compile with mbed) 

**Describe the problem**
I'm trying to run one of the provided projects on efr/efm32 microcontroller, (i choose magic_wanda example because i have an accelero & it seem's good example to start with). 
I wanted just to generate a binary file first, then make changes to adapt code with external accelerometer, itried this command:  _make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=""CMSIS efm32pg_stk3401"" magic_wand_bin_ but errors appear.
Should i do some big modifications on project to run it on efr32/efm32 micro ?
Thank's for your answers

**Please provide the exact sequence of commands/steps when you ran into the problem**

 _make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=""CMSIS efm32pg_stk3401"" magic_wand_bin

"
35304,How to use tflite c++ api to load a mat input and decode the output?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source):


**Demo code**

```
std::unique_ptr<tflite::FlatBufferModel> model =
			tflite::FlatBufferModel::BuildFromFile(""detect.tflite"");
// Build the interpreter
tflite::ops::builtin::BuiltinOpResolver resolver;
std::unique_ptr<tflite::Interpreter> interpreter;
tflite::InterpreterBuilder(*model, resolver)(&interpreter);

// Resize input tensors, if desired.
interpreter->AllocateTensors();
TfLiteTensor* output_locations = nullptr;
TfLiteTensor* output_classes = nullptr;
TfLiteTensor* num_detections = nullptr;
auto cam = cv::VideoCapture(0);
while (true) {
	cv::Mat image;
	auto success = cam.read(image);
	if (!success) {
		std::cout << ""cam fail"" << std::endl;
		break;
	}
	// resize(image, image, Size(300,300));

	uchar* input = interpreter->typed_input_tensor<uchar>(0);

	// feed input


	memcpy(interpreter->typed_input_tensor<uchar>(0), image.data, image.total() *image.elemSize());

	interpreter->SetNumThreads(1);

	interpreter->Invoke();

	auto output = interpreter->typed_output_tensor<uchar>(0);

        output_locations = interpreter->tensor(interpreter->outputs()[0]);
	output_classes = interpreter->tensor(interpreter->outputs()[1]);

	const float* detection_locations = TensorData<float>(output_locations, batch_index);
	const float* detection_classes =  TensorData<float>(output_classes, batch_index);
	const int num_detections = output_classes->dims->data[1];
	const int num_classes = output_classes->dims->data[2];
			
        std::cout << ""output: ""<< num_detections << std::endl;
```
The tflite model is mobilenet-ssd.
The output 'num_detections' is always 10, no matter  whether there are something or not in front of the camera. 
So I guess the problem is due to the wrong input.
And the function 'Tensordata' is from [here](https://github.com/YijinLiu/tf-cpu/blob/master/benchmark/obj_detect_lite.cc) 

Is there anyone know how to feed tflite model with opencv mat and decode the output?
"
35303,"When Pass experimental_relax_shapes to instance methods decorated with `tf.function`, it can't work.","### System information

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Stretch
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0rc1
- Python version: 3.7.1

### Describe the current behavior
Consider the following source:
```
import tensorflow as tf
@tf.function(experimental_relax_shapes=True)
def fn(x):
    print(tf.shape(x))
    return x * x
```
1:
```
fn(tf.constant([2]))
```
Output:
Tensor(""Shape:0"", shape=(1,), dtype=int32)
Out[9]: <tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>

2:
```
fn(tf.constant([2, 3]))
```
Output:
Tensor(""Shape:0"", shape=(1,), dtype=int32)
Out[10]: <tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 9], dtype=int32)>

### Describe the expected behavior
The experimental_relax_shapes should work also on instance method."
35300,Graphics Card Recommendation,"I am running video-detection application using Yolo and FRCNN model( using Tensorflow backend).
I have to run around 50 parallel video detection simultaneously. Please advice which Graphics card or Combination of Graphics should be able to handle the load of Graphics.

My options are : -
i) 2 x RTX-TITAN
ii) 4 x RTX 2080ti
iii) 8 x GTX 1080ti
"
35299,About bias quantization of hexagon delegate,"As far as I know, Tensorflow Lite uses input_scale * weight_scale as bias scale to quantize bias tensor to int32. However, hexagon use int32_min/in32_max as min/max to quantize bias tensor.
So how do you handle this difference to hexagon delegation? Below is the conv_2d builder of the hexagon delegation. I don't see where the transformation is.
https://github.com/tensorflow/tensorflow/blob/539b7642a928c7fbfb4d896f650f7e6d79c2a5e0/tensorflow/lite/experimental/delegates/hexagon/builders/conv_2d_builder.cc#L234"
35298,OneDeviceStrategy allocates memory on two GPUs,"**System information**
- OS Platform and Distribution: Arch Linux, 5.4.2-arch1-1-ARCH
- TensorFlow installed from: binary
- TensorFlow version: 2.1.0rc1
- Keras version: 2.2.4-tf
- Python version: 3.8
- GPU model and memory: 2x GTX 1080 Ti 11GB""`

**Describe the current behavior**
OneDeviceStrategy is set to use GPU0 but allocates full memory at GPU0 and GPU1 too.
On both GPUs 11GB are allocated.
Using not Strategy (simple plain TF code) - the memory is allocated only at GPU0.

**Describe the expected behavior**
OneDeviceStrategy is set to use GPU0 allocates memory only at GPU0 as the code does if OneDeviceStrategy is not used."
35295,TF Lite GPU delegate gives fuse_auto_input failed error when running TF Lite model that uses only supported GPU operations.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.04

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N.A.

- TensorFlow installed from (source or binary):
Source, 9a5b203

- TensorFlow version (use command below):
2

- Python version:
3.6.8

- Bazel version (if compiling from source):
1.0.0

- GCC/Compiler version (if compiling from source):
7.4.0

- CUDA/cuDNN version:
10.0.130/7.4.2

- GPU model and memory:
RTX 2080 Ti

**Describe the current behavior**
TF Lite GPU delegate gives `fuse_auto_input failed` error when running TF Lite model that only uses supported operations listed in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/gpu.

At first, I suspect that TF Lite GPU delegate expects a model that has no fused operators. Therefore, I tried to prevent TFLiteConverter from fusing operators by commenting the following lines in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/toco_tooling.cc:
```
if (SupportsFusedActivationFunction(output_format)) {
  transformations.Add(new FuseActivationFunctions);
} else {
  transformations.Add(new UnfuseActivationFunctions);
}
```

However, it still gives the same error.

It only works when I add `compile_options.auto_input_fusion = false` to this file (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/gl_delegate.cc), which prevents TF Lite GPU delegate from trying to fuse operators. 

**Describe the expected behavior**
TF Lite GPU delegate should be able to run TF Lite model without needing to modify TensorFlow source code.

**Code to reproduce the issue**

1. git pull https://github.com/google/mediapipe.git
2. Switch this model (https://drive.google.com/file/d/1LwNKYcf_sYDDWNWML-mNjxp8_j_2ofRe/view?usp=sharing) with the `hair_segmentation.tflite` model in MediaPipe
3. Run the hair segmentation pipeline by 
```
bazel build -c opt --copt -DMESA_EGL_NO_X11_HEADERS mediapipe/examples/desktop/hair_segmentation:hair_segmentation_gpu
GLOG_logtostderr=1 bazel-bin/mediapipe/examples/desktop/hair_segmentation/hair_segmentation_gpu     --calculator_graph_config_file=mediapipe/graphs/hair_segmentation/hair_segmentation_mobile_gpu.pbtxt
```

**Other info / logs**
> INFO: Created TensorFlow Lite delegate for GPU.
> ERROR: TfLiteGpuDelegate Prepare: fuse_auto_input failed
> ERROR: Node number 8 (TfLiteGpuDelegate) failed to prepare.
> 
> ERROR: Restored previous execution plan after delegate application failure.
> E1220 10:32:56.170152  5452 demo_run_graph_main_gpu.cc:186] Failed to run the graph: Graph has errors: 
> Calculator::Open() for node ""[TfLiteInferenceCalculator, TfLiteInferenceCalculator with output stream: segmentation_tensor]"" failed: ; (interpreter_->ModifyGraphWithDelegate(delegate_))==(kTfLiteOk)e_calculator.cc:593) 


"
35292,unbounded memory leak in tf.io.gfile.isdir(),"This was discovered in debugging of https://github.com/tensorflow/tensorboard/issues/766 by a combination of @psybuzz, @wchargin, and myself.  From empirical evidence from TensorBoard users it appears that this grows without bound, so in practical usage it only takes a day or so to consume dozens of GB of memory.

Calling `tf.io.gfile.isdir()` leaks memory at a rate of approximately 1 MB per 20,000 calls, and this is reproducible at TF 2.0.0 and latest tf-nightly (`tf-nightly-2.1.0.dev20191219`), on macOS, Ubuntu 16.04, and Linux Debian (a Google workstation), and with python 2.7, 3.5, and 3.7.

Here's our repro script:
```python
import gc
import os
import resource
import time

import tensorflow as tf

print(""PID: %d\n"" % (os.getpid(),))
prev = 0
while True:
  peak = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
  print(""peak memory = %d (+%d) in kb (Linux) or b (macOS)"" % (peak, peak-prev))
  prev = peak
  for _ in range(20000):
    tf.io.gfile.isdir(b""/tmp/nonexistent-file-for-tf-memory-leak"")
  gc.collect()
  time.sleep(1.0)
```

Sample output of `python repro.py`:
```
PID: 153611

peak memory = 226796 (+226796) in kb (Linux) or b (macOS)
peak memory = 228108 (+1312) in kb (Linux) or b (macOS)
peak memory = 229132 (+1024) in kb (Linux) or b (macOS)
peak memory = 229900 (+768) in kb (Linux) or b (macOS)
peak memory = 230924 (+1024) in kb (Linux) or b (macOS)
peak memory = 231948 (+1024) in kb (Linux) or b (macOS)
peak memory = 232716 (+768) in kb (Linux) or b (macOS)
peak memory = 233740 (+1024) in kb (Linux) or b (macOS)
peak memory = 234764 (+1024) in kb (Linux) or b (macOS)
peak memory = 235788 (+1024) in kb (Linux) or b (macOS)
peak memory = 236556 (+768) in kb (Linux) or b (macOS)
peak memory = 237580 (+1024) in kb (Linux) or b (macOS)
...
```

Our initial attempt to find a root cause led us to suspect the fact that `is_directory_v2` uses ScopedTFStatus while the rest of the `gfile` API does not (we spot-checked a few other APIs, including `tf.io.gfile.stat()`, and did not see the same issue).

Here's the code from v2.0.0 (file_io.py was just converted to PyBind11 today so it's possible this actually fixes the issue, but there is not yet a nightly with the change to check):
https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/lib/io/file_io.py#L585-L596

We attempted to debug further by deconstructing the calls to `isdir()` into the two lines, one that creates `ScopedTFStatus` and one that calls `pywrap_tensorflow.IsDirectory()`, and it seemed to be the case that the memory leak is proportional to the number of times `IsDirectory()` is called with a *distinct* `ScopedTFStatus` pointer (calling it over and over with the same status doesn't seem to leak at a proportional rate; reusing the status here seemed fine for testing this because `IsDirectory()` does not actually touch the status in the codepath for a nonexistent file).  So we suspect maybe there's a weird interaction at the SWIG boundary that results in the leak.

Furthermore, it also seems to leak when the argument is an existing filename; the repro uses a nonexistent one for simplicity and because that makes the codepath slightly simpler (since then `IsDirectory()` exits early on file nonexistence via the `access()` syscall and never even calls `stat()`).  Also, the leak still occurs when the `gc.collect()` is omitted; it's also just there to isolate possible causes of the leak."
35283,Android TFlite inconsistent performance when app is not in focus.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 9.0 and 10.0
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 3a (Android 10), tested on Android 9 as well with same bad behaviour
- TensorFlow installed from (source or binary):  tflite .so built from source
- TensorFlow version (use command below): 1.13, 1.14, 1.15, 2.0, 2.1
- Bazel version (if compiling from source): various; 0.21 to 0.29

Problem:
We have an audio processing application that runs in real time, including when the phone's screen is off. Using the built shared objects (tensorflowlite.so) from r1.13 and r1.14 branches, our processing time stays consistent. Locking the phone or minimizing the app does not affect the tflite inference times. Our app also uses a foreground service to make Android give us optimal process scheduling.

However, when using newer tflite shared objects (.so) built from r1.15, r2.0, and r2.1 the inference performance drops when the app is not in focus (either screen was locked, or our app minimized). The behaviour is especially bad when using more threads, e.g: `interpreter->SetNumThreads(2);`

**Describe the current behavior**
Android tflite model inference times are lower when app is not in focus with tflite versions r1.15, r2.0, r2.1. 
**Describe the expected behavior**
Exactly the same inference processing performance when the app is in focus or not.

Is there something in the API of newer tflite versions (r1.15 and newer) that I could play around with to fix this? Any help is greatly appreciated, thank you!
"
35280,tensorflow error issue python3.8.1,"```
Traceback (most recent call last):
  File ""<pyshell#0>"", line 1, in <module>
    import tensorflow
  File ""C:\Users\mark scorp lezeret\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\mark scorp lezeret\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\mark scorp lezeret\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\mark scorp lezeret\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 114
    def TFE_ContextOptionsSetAsync(arg1, async):
                                         ^
SyntaxError: invalid syntax
```"
35277,TF-Lite Micro: Selectively omit data types at compile-time,"My application only requires kTfLiteInt8 kernels.
However, kTfLiteFloat32 and kTfLiteUInt8 kernels are also built into the application.

It would save a considerable amount of code space if there was a way to disable building in the unused data types, e.g.:
```
  switch (input->type) {  // Already know in/out types are same.
#ifndef TFLITE_FLOAT32_DISABLED
    case kTfLiteFloat32:
      return EvalFloat(context, node, params, &data, input, filter, bias,
                       nullptr, nullptr, output);
      break;
#endif 
#infdef TFLITE_INT8_DISABLED
    case kTfLiteInt8:
      return EvalQuantizedPerChannel(context, node, params, &data, input,
                                     filter, bias, output, nullptr);
      break; 
#endif 
#ifndef TFLITE_UINT8_DISABLED
    case kTfLiteUInt8:
      return EvalQuantized(context, node, params, &data, input, filter, bias,
                           nullptr, nullptr, output);
      break;
#endif
    default:
      context->ReportError(context, ""Type %s (%d) not supported."",
                           TfLiteTypeGetName(input->type), input->type);
      return kTfLiteError
```

Or something more elegant ;) 
"
35275,"TensorFlow building error: debug_ops_gpu.cu.cc(47): error: more than one instance of overloaded function ""isinf"" matches the argument list","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 16.04.6`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `N/A`
- TensorFlow installed from (source or binary): `source`
- TensorFlow version: `2.0.0 (master)`
- Python version: `Python 3.5.2`
- Installed using virtualenv? pip? conda?: `apt`
- Bazel version (if compiling from source): `1.1.0`
- GCC/Compiler version (if compiling from source): `gcc 5.4.0`
- CUDA/cuDNN version: `CUDA 10.2 / cuDNN 7.6.5.32-1+cuda10.2`
- GPU model and memory: `NVIDIA Tesla V100`



**Describe the problem**
Observed after 316bd31e02b78a071d2f7f5a87898dd5f125f371 commit.
TensorFlow building fails with the error:
```
[2019-12-19T02:07:55.208Z] tensorflow/core/kernels/debug_ops_gpu.cu.cc(47): error: more than one instance of overloaded function ""isinf"" matches the argument list:
[2019-12-19T02:07:55.208Z]             function ""isinf(float)""
[2019-12-19T02:07:55.208Z]             function ""isinf(double)""
[2019-12-19T02:07:55.208Z]             function ""isinf(long double)""
[2019-12-19T02:07:55.209Z]             argument types are: (const tensorflow::int16)
[2019-12-19T02:07:55.209Z]           detected during:
[2019-12-19T02:07:55.209Z]             instantiation of ""void tensorflow::<unnamed>::CurtHealthKernel(const Tin *, int, Tout *) [with Tin=tensorflow::int16, Tout=float]"" 
[2019-12-19T02:07:55.209Z] (163): here
[2019-12-19T02:07:55.209Z]             instantiation of ""void tensorflow::CurtHealthLaunch<Tin, Tout>::Run(const tensorflow::<unnamed>::GPUDevice &, const Tin *, int, Tout *) [with Tin=tensorflow::int16, Tout=float]"" 
[2019-12-19T02:07:55.209Z] (171): here
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
.tf_configure.bazelrc:
```
build --action_env PYTHON_BIN_PATH=""/usr/bin/python3""
build --action_env PYTHON_LIB_PATH=""/usr/local/lib/python3.5/dist-packages""
build --python_path=""/usr/bin/python3""
build:xla --define with_xla_support=true
build --action_env TF_CUDA_VERSION=""10.2""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env TF_NCCL_VERSION=""2.6.0""
build --action_env TF_CUDA_PATHS=""/hpc/local/oss/cuda10.2/cuda-toolkit,/usr,/usr/local/cuda""
build --action_env CUDA_TOOLKIT_PATH=""/hpc/local/oss/cuda10.2/cuda-toolkit""
build --action_env CUDNN_INSTALL_PATH=""/usr""
build --action_env NCCL_INSTALL_PATH=""<cut>/nccl/stable""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""7.0""
build --action_env LD_LIBRARY_PATH=""<cut>/nccl/stable/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/nccl_rdma_sharp_plugin/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/ucx/lib/ucx:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/ucx/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/sharp/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/hcoll/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/ompi/lib:/hpc/local/oss/cuda10.2/cuda-toolkit/lib64:/hpc/local/oss/cuda10.2/cuda-toolkit/lib64/stubs:/usr/local/nvidia/lib:/usr/local/nvidia/lib64""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc-5""
build --config=cuda
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test --test_env=LD_LIBRARY_PATH
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only
build --action_env TF_CONFIGURE_IOS=""0""
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

CC: @caisq
"
35272,While using tf.import_graph_def: ValueError: Input 1 of node StatefulPartitionedCall,"I am trying to freeze and obfuscate my model but when running:
tf.import_graph_def(graph_def, name='')

I am getting the error:
During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""a.py"", line 129, in <module>
    optimize_model('/home/xxxx/Desktop/Pervasive/Projects/Ofuscar/1576096897/','/home/xxxx/Desktop/Pervasive/Projects/Ofuscar/out/')
  File ""a.py"", line 123, in optimize_model
    convert_graph_def_to_saved_model(output_model_dir, input_nodes, output_nodes, graph_filepath)
  File ""a.py"", line 99, in convert_graph_def_to_saved_model
    tf.import_graph_def(graph_def, name='')
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def
    producer_op_list=producer_op_list)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/importer.py"", line 505, in _import_graph_def_internal
    raise ValueError(str(e))
ValueError: Input 1 of node StatefulPartitionedCall was passed float from xxxx/conv0/weights:0 incompatible with expected resource.

The graph_def was loaded with the function:
def get_graph_def_from_file(graph_filepath):
    with tf.Graph().as_default():
        with tf.gfile.GFile(graph_filepath, 'rb') as f:
            graph_def = tf.GraphDef()
            graph_def.ParseFromString(f.read())
            return graph_def "
35267,Training fails when a multi-output Keras model has one output without a loss function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see minimal example.
- OS Platform and Distribution: Ubuntu 18.04.3 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  N/A
- TensorFlow installed from (source or binary): binary (specifically, `tensorflow/tensorflow:nightly-py3` Docker image)
- TensorFlow version (use command below): 2.1.0-dev20191216
- Python version: 3.6.9
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

A multi-output Keras model compiled so that one output doesn't have a loss function raises an exception when calling `.fit`.

**Describe the expected behavior**

Training should minimise the losses defined for the other output(s).

**Code to reproduce the issue**

```
import numpy as np
import tensorflow as tf
import tensorflow.keras as keras

input_a = keras.layers.Input(shape=(10,), name=""input_a"")
input_b = keras.layers.Input(shape=(20,), name=""input_b"")
output_a = keras.layers.Dense(1, name=""output_a"")(input_a)
output_b = keras.layers.Dense(1, name=""output_b"")(input_b)
model = keras.Model(inputs=[input_a, input_b], outputs=[output_a, output_b])
model.compile(optimizer=""sgd"", loss={""output_a"": None, ""output_b"": ""mse""})

n = 128
input_a = np.ones((n, 10))
input_b = np.ones((n, 20))
output_a = np.ones((n, 1))
output_b = np.ones((n, 1))

dataset = tf.data.Dataset.from_tensor_slices(
    ((input_a, input_b), (output_a, output_b))
).batch(64)

model.fit(dataset)
```

Raises:

```
ValueError: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), for inputs ['output_b'] but instead got the following list of 2 arrays: [<tf.Tensor 'args_2:0' shape=(None, 1) dtype=float64>, <tf.Tensor 'args_3:0' shape=(None, 1) dtype=float64>]...
```"
35264,ran out of memory trying to allocate,"It will take up more than 30gb of memory, happening in tensorflow, tensorflow-gpu, tf-nightly

Code:
```python
import tensorflow as tf
from tensorflow import keras

(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()

x_train = x_train.reshape((-1,28,28,1))
x_test = x_test.reshape((-1,28,28,1))

y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)

input_shape=(224, 224, 3)
inputs=tf.keras.layers.Input(shape=input_shape)

x = tf.keras.layers.Flatten()(inputs)
outputs = tf.keras.layers.Dense(10, activation='softmax')(x)
model = tf.keras.models.Model(inputs=inputs, outputs=outputs)

model.compile(optimizer=keras.optimizers.Adam(),
             loss=""categorical_crossentropy"",
              metrics=['accuracy'])

x_train=tf.image.resize(x_train,input_shape[:2])
x_train=tf.image.grayscale_to_rgb(x_train)

x_train=x_train[:128]
y_train=y_train[:128]
model.fit(x=x_train,y=y_train,batch_size=1)
```


`2019-12-19 22:41:47.467474: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2019-12-19 22:41:52.813348: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-12-19 22:41:52.851093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.755
pciBusID: 0000:41:00.0
2019-12-19 22:41:52.851257: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-12-19 22:41:52.851712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-19 22:41:53.319561: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-12-19 22:41:53.323650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.755
pciBusID: 0000:41:00.0
2019-12-19 22:41:53.323795: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-12-19 22:41:53.324400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-19 22:41:53.989669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-19 22:41:53.989780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-12-19 22:41:53.989838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-12-19 22:41:53.990709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9530 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5)
2019-12-19 22:41:54.080141: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 12042240000 exceeds 10% of system memory.
2019-12-19 22:42:15.504057: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 11.21GiB (rounded to 12042240000).  Current allocation summary follows.
2019-12-19 22:42:15.504241: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (256): 	Total Chunks: 3, Chunks in use: 3. 768B allocated for chunks. 768B in use in bin. 48B client-requested in use in bin.
2019-12-19 22:42:15.504381: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (512): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-12-19 22:42:15.504525: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1024): 	Total Chunks: 1, Chunks in use: 1. 1.3KiB allocated for chunks. 1.3KiB in use in bin. 1.0KiB client-requested in use in bin.
2019-12-19 22:42:15.504675: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-12-19 22:42:15.504821: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-12-19 22:42:15.504964: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8192): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-12-19 22:42:15.505108: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-12-19 22:42:15.505252: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-12-19 22:42:15.505421: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-12-19 22:42:15.505631: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-12-19 22:42:15.505849: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-12-19 22:42:15.506074: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (524288): 	Total Chunks: 1, Chunks in use: 0. 1022.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-12-19 22:42:15.506468: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-12-19 22:42:15.506957: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-12-19 22:42:15.507273: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4194304): 	Total Chunks: 1, Chunks in use: 1. 5.74MiB allocated for chunks. 5.74MiB in use in bin. 5.74MiB client-requested in use in bin.
2019-12-19 22:42:15.507582: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8388608): 	Total Chunks: 3, Chunks in use: 0. 26.26MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-12-19 22:42:15.507965: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-12-19 22:42:15.508284: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-12-19 22:42:15.508723: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-12-19 22:42:15.520336: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-12-19 22:42:15.520521: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-12-19 22:42:15.520749: I tensorflow/core/common_runtime/bfc_allocator.cc:885] Bin for 11.21GiB was 256.00MiB, Chunk State: 
2019-12-19 22:42:15.521083: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 1048576
2019-12-19 22:42:15.521203: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 000000020FC00000 next 1 of size 1280
2019-12-19 22:42:15.521394: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 000000020FC00500 next 4 of size 256
2019-12-19 22:42:15.521591: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 000000020FC00600 next 7 of size 256
2019-12-19 22:42:15.521787: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 000000020FC00700 next 8 of size 256
2019-12-19 22:42:15.521961: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 000000020FC00800 next 18446744073709551615 of size 1046528
2019-12-19 22:42:15.522163: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 8388608
2019-12-19 22:42:15.522353: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 000000020FE00000 next 18446744073709551615 of size 8388608
2019-12-19 22:42:15.522590: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 8388608
2019-12-19 22:42:15.522763: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0000000210600000 next 18446744073709551615 of size 8388608
2019-12-19 22:42:15.523051: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 16777216
2019-12-19 22:42:15.523236: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0000000210E00000 next 6 of size 6021120
2019-12-19 22:42:15.523468: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 00000002113BE000 next 18446744073709551615 of size 10756096
2019-12-19 22:42:15.523719: I tensorflow/core/common_runtime/bfc_allocator.cc:914]      Summary of in-use Chunks by size: 
2019-12-19 22:42:15.523923: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 256 totalling 768B
2019-12-19 22:42:15.524051: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 1280 totalling 1.3KiB
2019-12-19 22:42:15.524201: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 6021120 totalling 5.74MiB
2019-12-19 22:42:15.524454: I tensorflow/core/common_runtime/bfc_allocator.cc:921] Sum Total of in-use chunks: 5.74MiB
2019-12-19 22:42:15.524644: I tensorflow/core/common_runtime/bfc_allocator.cc:923] total_region_allocated_bytes_: 34603008 memory_limit_: 9993660007 available bytes: 9959056999 curr_region_allocation_bytes_: 33554432
2019-12-19 22:42:15.524974: I tensorflow/core/common_runtime/bfc_allocator.cc:929] Stats: 
Limit:                  9993660007
InUse:                     6023168
MaxInUse:                 22799872
NumAllocs:                      20
MaxAllocSize:              8388608

2019-12-19 22:42:15.525331: W tensorflow/core/common_runtime/bfc_allocator.cc:424] *__________________________________________________******************_______________________________
`"
35260,gfile read causes core dump,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04, (running in docker)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Docker image tensorflow/tensorflow:nightly-gpu-py3
- **TensorFlow version (use command below)**: 2.1.0-dev20191106
- **Python version**: 3.6.8
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: 
source_ref is link to image in s3
image_data = tf.io.gfile.GFile(source_ref, 'rb').read()

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
When the read is executing the core gets dumped

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

import tensorflow as tf

source_ref is link to image in s3
image_data = tf.io.gfile.GFile(source_ref, 'rb').read()

free(): invalid pointer
Aborted (core dumped)
"
35259,"Why list, dictionary and tuple are taken value vice versa when declare it in one-line?","I am feeling very strange when I am working on list, dictionary and tuple in python.

When we declare multiple variables in one line like a = b = c = 0 it behaves like separate variables.

If we are update value any of the above variables then it will not affect another variable like below.

a = 10
b = 11
c = 13
But, this thing is not applying with list, tuple and dictionary. If we declare blank list like below.

a = b = c = []

Now, I am appending the value of only a list.

a.append('Testing')
Now, b and c is automatically assigned that value.

This thing also happens with dictionary and tuple also.

Can anyone please help to solve this issue. Please don't suggest to declare it in a different line."
35258,install issue,"I downloaded the win_amd64.whl for python 3.6 in Windows, because when i write 

`pip install tensorflow`

it says it could not find a version that satisfies the requirement from version none
So i downloaded it manually and when i install it with

`pip install tensorflow-2.0.0-cp36-cp36m-win_amd64.whl`

It says that it's not compatible with my platform
What should i do?"
35257,tensorflow 2.0 tf.linalg.normalize yields nan,"### tf.linalg.normalize(np.zeros([10, 4]), ord=1, axis=-1) yields nan as below

(<tf.Tensor: id=58, shape=(10, 4), dtype=float64, numpy=
 array([[nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan]])>,
 <tf.Tensor: id=57, shape=(10, 1), dtype=float64, numpy=
 array([[0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.]])>)


* I know this is caused by divide by 0, so in the future tensorflow should make this operation more numerically stable. "
35254,module 'tensorflow._api.v1.keras.layers' has no attribute 'DenseFeatures',"I want to use feature column as input to my customed model. I do as the answer 
 of the question suggested,   
https://stackoverflow.com/questions/54375298/how-to-use-tensorflow-feature-columns-as-input-to-a-keras-model

feature_layer = tf.keras.layers.DenseFeatures(feature_columns=feature_columns)

but i got the error as below:

AttributeError                            Traceback (most recent call last)
<ipython-input-23-db9a4807151e> in <module>
----> 1 feature_layer = tf.keras.layers.DenseFeatures(feature_columns=feature_columns)
      2 # feature_layer_outputs = feature_layer(feature_layer_inputs)

AttributeError: module 'tensorflow._api.v1.keras.layers' has no attribute 'DenseFeatures'


I use tensorflow 1.14 and keras version 2.2.4
Does anyone could tell me how to fix it, thanks very much!"
35253,tensorflow build fails,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform Windows 10 Pro
- desktop computer:
- TensorFlow installed from source
- TensorFlow version: 2.0
- Python version: 3.7
- Installed using virtualenv? pip? conda?: installed via command line / git clone etc.
- Bazel version (if compiling from source): 1.1.0
- GCC/Compiler version (if compiling from source): Visual Studio 2017 Redistributables
- CUDA/cuDNN version: --- (ROCm selected)
- GPU model and memory:  AMD FX-8800P R7

Build fails, see listing below.

C:\Users\Bludorf\tensorflow>python ./configure.py
WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: Waiting for server process to terminate (waited 5 seconds, waiting at most 60)
WARNING: Waiting for server process to terminate (waited 10 seconds, waiting at most 60)
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 1.1.0 installed.
Please specify the location of python. [Default is C:\Users\Bludorf\AppData\Local\Programs\Python\Python37\python.exe]: 


Found possible Python library paths:
  C:\Users\Bludorf\AppData\Local\Programs\Python\Python37\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\Bludorf\AppData\Local\Programs\Python\Python37\lib\site-packages]

Do you wish to build TensorFlow with XLA JIT support? [y/N]: y
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: y
ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: N
No CUDA support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y
Eigen strong inline overridden.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.

C:\Users\Bludorf\tensorflow>bazel build --config=v2 //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Writing tracer profile to 'C:/users/bludorf/_bazel_bludorf/jkbqqwso/command.profile.gz'
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/Bludorf/AppData/Local/Programs/Python/Python37/python.exe
INFO: Reading rc options for 'build' from c:\users\bludorf\tensorflow\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --config=v2
INFO: Reading rc options for 'build' from c:\users\bludorf\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/Bludorf/AppData/Local/Programs/Python/Python37/python.exe --action_env PYTHON_LIB_PATH=C:/Users/Bludorf/AppData/Local/Programs/Python/Python37/lib/site-packages --python_path=C:/Users/Bludorf/AppData/Local/Programs/Python/Python37/python.exe --config=xla --config=rocm --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:v2 in file c:\users\bludorf\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file c:\users\bludorf\tensorflow\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true
INFO: Found applicable config definition build:xla in file c:\users\bludorf\tensorflow\.tf_configure.bazelrc: --define with_xla_support=true
INFO: Found applicable config definition build:rocm in file c:\users\bludorf\tensorflow\.bazelrc: --crosstool_top=@local_config_rocm//crosstool:toolchain --define=using_rocm=true --define=using_rocm_hipcc=true --action_env TF_NEED_ROCM=1
INFO: Found applicable config definition build:v2 in file c:\users\bludorf\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:windows in file c:\users\bludorf\tensorflow\.bazelrc: --copt=/w --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --incompatible_windows_native_test_wrapper --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\users\bludorf\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):
 - C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_toolchains/repositories/repositories.bzl:37:9
 - C:/users/bludorf/tensorflow/WORKSPACE:37:1
ERROR: An error occurred during the fetch of repository 'io_bazel_rules_docker':
   Traceback (most recent call last):
        File ""C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl"", line 177
                _clone_or_update(ctx)
        File ""C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl"", line 36, in _clone_or_update
                git_repo(ctx, directory)
        File ""C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl"", line 91, in git_repo
                _update(ctx, git_repo)
        File ""C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl"", line 101, in _update
                init(ctx, git_repo)
        File ""C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl"", line 115, in init
                _error(ctx.name, cl, st.stderr)
        File ""C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl"", line 181, in _error
                fail(<1 more arguments>)
error running 'git init C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/io_bazel_rules_docker' while working with @io_bazel_rules_docker:
java.io.IOException: ERROR: src/main/native/windows/process.cc(199): CreateProcessW(""git"" init C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/io_bazel_rules_docker): Das System kann die angegebene Datei nicht finden.
 (error: 2)
ERROR: no such package '@io_bazel_rules_docker//repositories': Traceback (most recent call last):
        File ""C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl"", line 177
                _clone_or_update(ctx)
        File ""C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl"", line 36, in _clone_or_update
                git_repo(ctx, directory)
        File ""C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl"", line 91, in git_repo
                _update(ctx, git_repo)
        File ""C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl"", line 101, in _update
                init(ctx, git_repo)
        File ""C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl"", line 115, in init
                _error(ctx.name, cl, st.stderr)
        File ""C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl"", line 181, in _error
                fail(<1 more arguments>)
error running 'git init C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/io_bazel_rules_docker' while working with @io_bazel_rules_docker:
java.io.IOException: ERROR: src/main/native/windows/process.cc(199): CreateProcessW(""git"" init C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/io_bazel_rules_docker): Das System kann die angegebene Datei nicht finden.
 (error: 2)
ERROR: no such package '@io_bazel_rules_docker//repositories': Traceback (most recent call last):
        File ""C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl"", line 177
                _clone_or_update(ctx)
        File ""C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl"", line 36, in _clone_or_update
                git_repo(ctx, directory)
        File ""C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl"", line 91, in git_repo
                _update(ctx, git_repo)
        File ""C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl"", line 101, in _update
                init(ctx, git_repo)
        File ""C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl"", line 115, in init
                _error(ctx.name, cl, st.stderr)
        File ""C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl"", line 181, in _error
                fail(<1 more arguments>)
error running 'git init C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/io_bazel_rules_docker' while working with @io_bazel_rules_docker:
java.io.IOException: ERROR: src/main/native/windows/process.cc(199): CreateProcessW(""git"" init C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/io_bazel_rules_docker): Das System kann die angegebene Datei nicht finden.
 (error: 2)
INFO: Elapsed time: 7.563s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)

"
35252,Issue - Blas GEMM launch failed,"- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

```
import math
import requests
import datetime
from config import config
from db_analysis import LotteryDatabase
import pandas as pd

from sklearn import model_selection

from sklearn.metrics import (accuracy_score,
                             recall_score,
                             precision_score,
                             f1_score,
                             roc_curve,
                             auc, roc_auc_score,
                             confusion_matrix)

from hyperopt import STATUS_OK, Trials, tpe, hp, fmin, space_eval

from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation

space = {'choice': hp.choice('num_layers',
                                         [{'layers': 'two', },
                                          {'layers': 'three',
                                           'units3': hp.choice('units3', range(64, 256)),
                                           'dropout3': hp.uniform('dropout3', .25, .75)}
                                          ]),
                     'units1': hp.choice('units1', range(64, 256)),
                     'units2': hp.choice('units2', range(64, 256)),

                     'dropout1': hp.uniform('dropout1', .25, .75),
                     'dropout2': hp.uniform('dropout2', .25, .75),

                     'batch_size': hp.choice('batch_size', range(5, 10)),

                     'nb_epochs': 100,
                     'optimizer': hp.choice('optimizer', ['adadelta', 'adam', 'rmsprop']),
                     'activation': 'relu'
                     }

class TestTF:

    def __init__(self):

        self.ldb = LotteryDatabase(config['database']) - connect to sqlite database

        self.x = None
        self.y = None

        self.x_train = None
        self.x_validation = None
        self.y_train = None
        self.y_validation = None

    def main_tf(self):

        dataset = pd.concat(self.generate_df_pieces(self.ldb.conn, 100000, offset=0, ids=5000)) - pull data from sqlite database in chunks total of 1.2 mln samples
        array = dataset.values

        self.x = array[:, :150]
        self.y = array[:, 150]

        self.x_train, self.x_validation, self.y_train, self.y_validation = model_selection.train_test_split(
            self.x, self.y, test_size=0.2, random_state=42)

        bayes_trials = Trials()

        best = fmin(fn=self.keras_objective, space=space, algo=tpe.suggest, max_evals=50, trials=bayes_trials)

        print(best)

        for bt in bayes_trials:
            print(bt['result']['loss'])
            print(bt['result']['params']

    def keras_objective(self, params):

        model = Sequential()
        model.add(Dense(output_dim=params['units1'], input_dim=int(self.x_train.shape[1])))
        model.add(Activation(params['activation']))
        model.add(Dropout(params['dropout1']))

        model.add(Dense(output_dim=params['units2'], init=""glorot_uniform""))
        model.add(Activation(params['activation']))
        model.add(Dropout(params['dropout2']))

        if params['choice']['layers'] == 'three':
            model.add(Dense(output_dim=params['choice']['units3'], init=""glorot_uniform""))
            model.add(Activation(params['activation']))
            model.add(Dropout(params['choice']['dropout3']))

        model.add(Dense(1))
        model.add(Activation('sigmoid'))
        model.compile(loss='binary_crossentropy', optimizer=params['optimizer'])

        model.fit(self.x_train, self.y_train, nb_epoch=params['nb_epochs'], batch_size=params['batch_size'], verbose=0)

        pred_auc = model.predict_proba(self.x_validation, batch_size=10, verbose=0)
        acc = roc_auc_score(self.y_validation, pred_auc)
        # print('AUC:', acc)
        # sys.stdout.flush()

        return {'loss': -acc, 'params': params}


if __name__ == '__main__':
    TF = TestTF()
    TF.main_tf()
```

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7 Ultimate 64bit
- TensorFlow installed from (source or binary): pip install tensorflow-gpu
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: 3.7
- CUDA/cuDNN version:

nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Wed_Oct_23_19:32:27_Pacific_Daylight_Time_2019
Cuda compilation tools, release 10.2, V10.2.89

- GPU model and memory:

GTX 1050Ti 4GB

GPU Engine Specs
CUDA Cores
768
Graphics Clock (MHz)
1290
Processor Clock (MHz)
1392
Graphics Performance
high-6747
Memory Specs
Memory Clock
7 Gbps
Standard Memory Config
4 GB
Memory Interface
GDDR5
Memory Interface Width
128-bit
Memory Bandwidth (GB/sec)
112
Feature Support
Supported Technologies
CUDA, 3D Vision, PhysX, NVIDIA G-SYNC™, Ansel
Thermal and Power Specs
Maximum GPU Temperature (in C)
97
Maximum Graphics Card Power (W)
75
Minimum System Power Requirement (W)
300

**Describe the current behavior**

Testing keras with hyperopt on my model (total shape approx. [1200000, 150]

Receving error below (nvidia-smi below error code):

What I have tried so far:
-installing different CUDAA/cuDNN/tenserflow configurations
-updating packages
-checking for additional nvidia-smi processes
- batch_size manimulation range(1, 64)

```
E:/GitHub Repositories/MYOPM/test.py:139: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=126, units=114)`
  model.add(Dense(output_dim=params['units1'], input_dim=int(self.x_train.shape[1])))

2019-12-19 06:49:34.323001: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-12-19 06:49:34.474001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.392
pciBusID: 0000:01:00.0
2019-12-19 06:49:34.474001: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-12-19 06:49:34.503001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-19 06:49:34.516001: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-12-19 06:49:34.539001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.392
pciBusID: 0000:01:00.0
2019-12-19 06:49:34.539001: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-12-19 06:49:34.546001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-19 06:49:46.336401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-19 06:49:46.336401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-12-19 06:49:46.337401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-12-19 06:49:46.404401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3374 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
WARNING:tensorflow:Large dropout rate: 0.592226 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
E:/Programowanie/GitHub Repositories/MYOPM/test.py:143: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=88, kernel_initializer=""glorot_uniform"")`
  model.add(Dense(output_dim=params['units2'], init=""glorot_uniform""))

E:/GitHub Repositories/MYOPM/test.py:156: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  model.fit(self.x_train, self.y_train, nb_epoch=params['nb_epochs'], batch_size=params['batch_size'], verbose=0)

2019-12-19 06:49:49.410401: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_100.dll'; dlerror: cublas64_100.dll not found
2019-12-19 06:49:49.410401: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR
2019-12-19 06:49:49.515401: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR
2019-12-19 06:49:49.516401: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR
2019-12-19 06:49:49.577401: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR
2019-12-19 06:49:49.578401: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR
2019-12-19 06:49:49.586401: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR
2019-12-19 06:49:49.586401: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR
2019-12-19 06:49:49.587401: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR
2019-12-19 06:49:49.666401: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR
2019-12-19 06:49:49.667401: W tensorflow/stream_executor/stream.cc:1919] attempting to perform BLAS operation using StreamExecutor without BLAS support
2019-12-19 06:49:49.667401: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Blas GEMM launch failed : a.shape=(9, 126), b.shape=(126, 114), m=9, n=114, k=126
	 [[{{node dense_1/MatMul}}]]
Traceback (most recent call last):
  0%|          | 0/50 [00:15<?, ?it/s, best loss: ?]
  File ""E:/GitHub Repositories/MYOPM/test.py"", line 168, in <module>
    TF.main_tf()
  File ""E:/GitHub Repositories/MYOPM/test.py"", line 104, in main_tf
    best = fmin(fn=self.keras_objective, space=space, algo=tpe.suggest, max_evals=50, trials=bayes_trials)
  File ""D:\Programs\Python\lib\site-packages\hyperopt\fmin.py"", line 403, in fmin
    show_progressbar=show_progressbar,
  File ""D:\Programs\Python\lib\site-packages\hyperopt\base.py"", line 651, in fmin
    show_progressbar=show_progressbar)
  File ""D:\Programs\Python\lib\site-packages\hyperopt\fmin.py"", line 422, in fmin
    rval.exhaust()
  File ""D:\Programs\Python\lib\site-packages\hyperopt\fmin.py"", line 276, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.asynchronous)
  File ""D:\Programs\Python\lib\site-packages\hyperopt\fmin.py"", line 241, in run
    self.serial_evaluate()
  File ""D:\Programs\Python\lib\site-packages\hyperopt\fmin.py"", line 141, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File ""D:\Programs\Python\lib\site-packages\hyperopt\base.py"", line 856, in evaluate
    rval = self.fn(pyll_rval)
  File ""E:/Programowanie/GitHub Repositories/MYOPM/test.py"", line 156, in keras_objective
    model.fit(self.x_train, self.y_train, nb_epoch=params['nb_epochs'], batch_size=params['batch_size'], verbose=0)
  File ""D:\Programs\Python\lib\site-packages\keras\engine\training.py"", line 1239, in fit
    validation_freq=validation_freq)
  File ""D:\Programs\Python\lib\site-packages\keras\engine\training_arrays.py"", line 196, in fit_loop
    outs = fit_function(ins_batch)
  File ""D:\Programs\Python\lib\site-packages\tensorflow_core\python\keras\backend.py"", line 3740, in __call__
    outputs = self._graph_fn(*converted_inputs)
  File ""D:\Programs\Python\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1081, in __call__
    return self._call_impl(args, kwargs)
  File ""D:\Programs\Python\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1121, in _call_impl
    return self._call_flat(args, self.captured_inputs, cancellation_manager)
  File ""D:\Programs\Python\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File ""D:\Programs\Python\lib\site-packages\tensorflow_core\python\eager\function.py"", line 511, in call
    ctx=ctx)
  File ""D:\Programs\Python\lib\site-packages\tensorflow_core\python\eager\execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError:  Blas GEMM launch failed : a.shape=(9, 126), b.shape=(126, 114), m=9, n=114, k=126
	 [[node dense_1/MatMul (defined at D:\Programs\Python\lib\site-packages\tensorflow_core\python\framework\ops.py:1751) ]] [Op:__inference_keras_scratch_graph_1201]

Function call stack:
keras_scratch_graph

Thu Dec 19 06:53:46 2019
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 441.22       Driver Version: 441.22       CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 105... WDDM  | 00000000:01:00.0  On |                  N/A |
| 30%   30C    P8    N/A /  75W |    262MiB /  4096MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0       520    C+G   C:\Windows\system32\Dwm.exe                N/A      |
|    0      4016    C+G   ...6)\Google\Chrome\Application\chrome.exe N/A      |
+-----------------------------------------------------------------------------+

```

**Other info / logs**
Using hyperopt setup for keras NN from https://github.com/keras-team/keras/issues/1591
"
35251,LSTMCell initialization issue ( get_initial_state ),"Hi,
When I am using get_initial_state to initialize, I am getting error below . 
Could you please suggest me how to initialize LSTMCell cell? 

lstm_cell = tf.keras.layers.LSTM(units=128)
lstm_cell = tf.nn.RNNCellDropoutWrapper(   lstm_cell,    output_keep_prob=self.dropout_keep_prob)
self._initial_state = **lstm_cell.get_initial_state(128, tf.float32)**

**ValueError**: slice index 0 of dimension 0 out of bounds. for strided_slice (op: StridedSlice) with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>."
35250,Converting to TFLite: Invalid quantization params for op MAXIMUM at index 4 in subgraph 0,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 1.15.0-gpu


**Command used to run the converter or code if you’re using the Python API**

```
import os

import tensorflow as tf
import numpy as np
from PIL import Image


dataset = []
directory_images = './data_test'
directory_saved = './models'
for img in os.listdir(directory_images):
	data = Image.open(os.path.join(directory_images,img)))
	data = np.asarray(data, dtype=np.float32)[np.newaxis, :]
	dataset.append(data)

def representative_dataset_gen():
    for input_value in dataset:
        yield [input_value]

converter = tf.lite.TFLiteConverter.from_saved_model(directory_saved + ""/saved"")
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

tflite_model = converter.convert()

name = directory_saved + ""/tflite_model""
open(name + "".tflite"", ""wb"").write(tflite_model)
```

**The output from the converter invocation**

```
Traceback (most recent call last):
  File ""saved2lite.py"", line 34, in <module>
    tflite_model = converter.convert()
  File ""/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py"", line 993, in convert
    inference_output_type)
  File ""/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py"", line 239, in _calibrate_quantize_model
    inference_output_type, allow_float)
  File ""/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py"", line 78, in calibrate_and_quantize
    np.dtype(output_type.as_numpy_dtype()).num, allow_float)
  File ""/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py"", line 115, in QuantizeModel
    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)
RuntimeError: Invalid quantization params for op MAXIMUM at index 4 in subgraph 0
```

**Also, please include a link to the saved model or GraphDef**

```
https://www.dropbox.com/s/tj96fsm6t6rq8ye/model-r100-arcface-ms1m-refine-v2.zip?dl=0
```

**Failure details**
Conversion fails.


**Any other info / logs**
Full log:
```
2019-12-19 11:49:10.789835: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-12-19 11:49:10.810583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:10.811068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.392
pciBusID: 0000:01:00.0
2019-12-19 11:49:10.811245: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-12-19 11:49:10.812226: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-12-19 11:49:10.813094: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-12-19 11:49:10.813351: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-12-19 11:49:10.814559: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-12-19 11:49:10.815596: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-12-19 11:49:10.817880: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-12-19 11:49:10.818002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:10.818618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:10.819072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-19 11:49:10.819465: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-12-19 11:49:10.843162: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz
2019-12-19 11:49:10.843971: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55724cd4c080 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-12-19 11:49:10.844031: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2019-12-19 11:49:10.926515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:10.927095: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55724cdae1a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2019-12-19 11:49:10.927112: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1
2019-12-19 11:49:10.927282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:10.927748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.392
pciBusID: 0000:01:00.0
2019-12-19 11:49:10.927774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-12-19 11:49:10.927782: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-12-19 11:49:10.927790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-12-19 11:49:10.927797: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-12-19 11:49:10.927804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-12-19 11:49:10.927811: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-12-19 11:49:10.927819: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-12-19 11:49:10.927857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:10.928283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:10.928683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-19 11:49:10.928703: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-12-19 11:49:10.929442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-19 11:49:10.929452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-12-19 11:49:10.929458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-12-19 11:49:10.929674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:10.930285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:10.930708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2934 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
WARNING:tensorflow:From /home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
2019-12-19 11:49:13.849255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:13.849669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.392
pciBusID: 0000:01:00.0
2019-12-19 11:49:13.849711: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-12-19 11:49:13.849721: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-12-19 11:49:13.849729: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-12-19 11:49:13.849737: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-12-19 11:49:13.849744: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-12-19 11:49:13.849752: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-12-19 11:49:13.849759: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-12-19 11:49:13.849796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:13.850140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:13.850456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-19 11:49:13.850478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-19 11:49:13.850484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-12-19 11:49:13.850488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-12-19 11:49:13.850595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:13.850985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:13.851332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2934 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-12-19 11:49:16.749682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:16.750076: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2019-12-19 11:49:16.750135: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-12-19 11:49:16.750521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:16.750902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.392
pciBusID: 0000:01:00.0
2019-12-19 11:49:16.750927: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-12-19 11:49:16.750936: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-12-19 11:49:16.750945: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-12-19 11:49:16.750952: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-12-19 11:49:16.750959: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-12-19 11:49:16.750966: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-12-19 11:49:16.750974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-12-19 11:49:16.751007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:16.751348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:16.751663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-19 11:49:16.751682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-19 11:49:16.751688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-12-19 11:49:16.751692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-12-19 11:49:16.751831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:16.752173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:16.752495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2934 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-12-19 11:49:17.182263: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize
2019-12-19 11:49:17.182289: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.003ms.
2019-12-19 11:49:17.182737: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0ms.
WARNING:tensorflow:From /home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/util.py:249: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
2019-12-19 11:49:18.713005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:18.713562: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2019-12-19 11:49:18.713618: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-12-19 11:49:18.714027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:18.714386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.392
pciBusID: 0000:01:00.0
2019-12-19 11:49:18.714424: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-12-19 11:49:18.714433: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-12-19 11:49:18.714441: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-12-19 11:49:18.714448: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-12-19 11:49:18.714455: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-12-19 11:49:18.714462: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-12-19 11:49:18.714470: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-12-19 11:49:18.714501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:18.714882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:18.715214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-19 11:49:18.715235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-19 11:49:18.715242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-12-19 11:49:18.715261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-12-19 11:49:18.715411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:18.715760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-19 11:49:18.716119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2934 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-12-19 11:49:20.750755: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize
2019-12-19 11:49:20.750831: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 1230 nodes (-2206), 1329 edges (-2360), time = 1332.40198ms.
2019-12-19 11:49:20.750850: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 1230 nodes (0), 1329 edges (0), time = 370.552ms.
Traceback (most recent call last):
  File ""saved2lite.py"", line 34, in <module>
    tflite_model = converter.convert()
  File ""/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py"", line 993, in convert
    inference_output_type)
  File ""/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py"", line 239, in _calibrate_quantize_model
    inference_output_type, allow_float)
  File ""/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py"", line 78, in calibrate_and_quantize
    np.dtype(output_type.as_numpy_dtype()).num, allow_float)
  File ""/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py"", line 115, in QuantizeModel
    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)
RuntimeError: Invalid quantization params for op MAXIMUM at index 4 in subgraph 0
```"
35249,Wrong Accuracy value for Training data in tutorial...,"## URL(s) with the issue:
https://www.tensorflow.org/tutorials/keras/classification/

## Description of issue (what needs changing):

Under ""Train the model"" in ""Build the model"", the accuracy of the model on training data after 10 epochs is 0.91(91%) while it is mentioned as 0.88(88%).

### Clear description

Since, it is already mentioned in the tutorials that the model overfits the training data, thus the accuracy on training data should be more than that on testing data(88.3%).
### Submit a pull request?

If this issue is alright, I'll be glad the submit a PR right away...
Thanks for the help!
"
35248,Flag --incompatible_restrict_string_escapes will break TensorFlow in Bazel 1.2.1,"Incompatible flag --incompatible_restrict_string_escapes will break TensorFlow once Bazel 1.2.1 is released.

Please see the following CI builds for more information:

* [:darwin: (OpenJDK 8)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7"" target=""_blank"">:darwin: (OpenJDK 8)</a>)
* [:windows: (OpenJDK 8)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa"" target=""_blank"">:windows: (OpenJDK 8)</a>)
* [:ubuntu: 18.04 (OpenJDK 11)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1"" target=""_blank"">:ubuntu: 18.04 (OpenJDK 11)</a>)

Questions? Please file an issue in https://github.com/bazelbuild/continuous-integration

**Important**: Please do NOT modify the issue title since that might break our tools.
"
35247,Flag --incompatible_use_platforms_repo_for_constraints will break TensorFlow in Bazel 1.2.1,"Incompatible flag --incompatible_use_platforms_repo_for_constraints will break TensorFlow once Bazel 1.2.1 is released.

Please see the following CI builds for more information:

* [:darwin: (OpenJDK 8)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7"" target=""_blank"">:darwin: (OpenJDK 8)</a>)
* [:windows: (OpenJDK 8)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa"" target=""_blank"">:windows: (OpenJDK 8)</a>)
* [:ubuntu: 18.04 (OpenJDK 11)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1"" target=""_blank"">:ubuntu: 18.04 (OpenJDK 11)</a>)

Questions? Please file an issue in https://github.com/bazelbuild/continuous-integration

**Important**: Please do NOT modify the issue title since that might break our tools.
"
35246,Flag --incompatible_load_cc_rules_from_bzl will break TensorFlow in Bazel 1.2.1,"Incompatible flag --incompatible_load_cc_rules_from_bzl will break TensorFlow once Bazel 1.2.1 is released.

Please see the following CI builds for more information:

* [:darwin: (OpenJDK 8)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7"" target=""_blank"">:darwin: (OpenJDK 8)</a>)
* [:windows: (OpenJDK 8)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa"" target=""_blank"">:windows: (OpenJDK 8)</a>)
* [:ubuntu: 18.04 (OpenJDK 11)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1"" target=""_blank"">:ubuntu: 18.04 (OpenJDK 11)</a>)

Questions? Please file an issue in https://github.com/bazelbuild/continuous-integration

**Important**: Please do NOT modify the issue title since that might break our tools.
"
35245,Flag --incompatible_disable_target_provider_fields will break TensorFlow in Bazel 1.2.1,"Incompatible flag --incompatible_disable_target_provider_fields will break TensorFlow once Bazel 1.2.1 is released.

Please see the following CI builds for more information:

* [:darwin: (OpenJDK 8)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7"" target=""_blank"">:darwin: (OpenJDK 8)</a>)
* [:windows: (OpenJDK 8)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa"" target=""_blank"">:windows: (OpenJDK 8)</a>)
* [:ubuntu: 18.04 (OpenJDK 11)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1"" target=""_blank"">:ubuntu: 18.04 (OpenJDK 11)</a>)

Questions? Please file an issue in https://github.com/bazelbuild/continuous-integration

**Important**: Please do NOT modify the issue title since that might break our tools.
"
35244,Flag --incompatible_no_implicit_file_export will break TensorFlow in Bazel 1.2.1,"Incompatible flag --incompatible_no_implicit_file_export will break TensorFlow once Bazel 1.2.1 is released.

Please see the following CI builds for more information:

* [:darwin: (OpenJDK 8)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7"" target=""_blank"">:darwin: (OpenJDK 8)</a>)
* [:windows: (OpenJDK 8)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa"" target=""_blank"">:windows: (OpenJDK 8)</a>)
* [:ubuntu: 18.04 (OpenJDK 11)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1"" target=""_blank"">:ubuntu: 18.04 (OpenJDK 11)</a>)

Questions? Please file an issue in https://github.com/bazelbuild/continuous-integration

**Important**: Please do NOT modify the issue title since that might break our tools.
"
35243,Flag --incompatible_load_python_rules_from_bzl will break TensorFlow in Bazel 1.2.1,"Incompatible flag --incompatible_load_python_rules_from_bzl will break TensorFlow once Bazel 1.2.1 is released.

Please see the following CI builds for more information:

* [:darwin: (OpenJDK 8)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7"" target=""_blank"">:darwin: (OpenJDK 8)</a>)
* [:windows: (OpenJDK 8)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa"" target=""_blank"">:windows: (OpenJDK 8)</a>)
* [:ubuntu: 18.04 (OpenJDK 11)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1"" target=""_blank"">:ubuntu: 18.04 (OpenJDK 11)</a>)

Questions? Please file an issue in https://github.com/bazelbuild/continuous-integration

**Important**: Please do NOT modify the issue title since that might break our tools.
"
35242,Flag --incompatible_disallow_empty_glob will break TensorFlow in Bazel 1.2.1,"Incompatible flag --incompatible_disallow_empty_glob will break TensorFlow once Bazel 1.2.1 is released.

Please see the following CI builds for more information:

* [:darwin: (OpenJDK 8)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7"" target=""_blank"">:darwin: (OpenJDK 8)</a>)
* [:windows: (OpenJDK 8)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa"" target=""_blank"">:windows: (OpenJDK 8)</a>)
* [:ubuntu: 18.04 (OpenJDK 11)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1"" target=""_blank"">:ubuntu: 18.04 (OpenJDK 11)</a>)

Questions? Please file an issue in https://github.com/bazelbuild/continuous-integration

**Important**: Please do NOT modify the issue title since that might break our tools.
"
35241,add_weight with None name generate a graph that it's not possible to save by checkpoint manager,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: Tesla V100-SXM2-16GB

**Describe the current behavior**
When creating a custom keras.layers.Layer, it is possible to add weight in a lazy fashion through the add_weight() API. One of the possible arguments is the variable name.

According the documentation the name can be omitted, however if we have a Variable with None name and we try to save the model using the CheckpointManager API we can't generate a proper graph taxonomy. Specifically, it is impossible to obtain the path prefix for the custom variable since the ``_escape_local_name()`` function in graph_view.py line 51 would rise a NoneType error.
 
**Describe the expected behavior**
The expected behaviour would rise an error when the custom layer is build. In my opinion I would make the Variable name a mandatory field instead of having a None value by default.

**Code to reproduce the issue**
```
class CustomEmbedding(tf.keras.layers.Layer):
    """"""Construct the embeddings from word, position and token_type embeddings.
    """"""

    def __init__(self, vocab_size, hidden_size,  max_position_embeddings, **kwargs):
        super(CustomEmbedding, self).__init__(**kwargs)
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.max_position_embeddings = max_position_embeddings
        self.pad_idx = 0
        self.max_position_embeddings += 1


    def build(self, input_shape):
        """"""Build shared word embedding layer """"""
        with tf.name_scope(""position_embeddings""):
            self.position_embeddings = self.add_weight(
                # Note that the name is missing
                shape=(self.max_position_embeddings, self.hidden_size),
                initializer=positional_encoding,
                trainable=False,
                dtype=self.dtype)

        with tf.name_scope(""word_embeddings""):
            # Create and initialize weights. The random normal initializer was chosen
            # arbitrarily, and works well.
            self.word_embeddings = self.add_weight(
                ""weight"",
                shape=[self.vocab_size, self.hidden_size],
                initializer=get_initializer(self.initializer_range),
                trainable=True,
                dtype=self.dtype
            )
        super(CustomEmbedding, self).build(input_shape)

    def call(self, inputs):
        input_ids, position_ids = inputs

        inputs_embeds = tf.nn.embedding_lookup(self.word_embeddings, input_ids)
        position_embeddings = tf.nn.embedding_lookup(self.position_embeddings, position_ids)

        embeddings = inputs_embeds + position_embeddings
        return embeddings
```
Assuming to have a model with the over-defined CustomLayer. If we try to save the model after some training epochs
```
ckpt = tf.train.Checkpoint(model=model, optimizer=optim)
ckpt_manager = tf.train.CheckpointManager(ckpt,
    directory=path.join(args.ckpt_path, args.version),
    max_to_keep=args.max_ckp_to_keep)

if ckpt_manager.latest_checkpoint:
    ckpt.restore(ckpt_manager.latest_checkpoint)
    print('Latest checkpoint restored!!')

# training code

if self.val_accuracy.result() > best_model:
    best_model = self.test_accuracy.result()
    ckpt_save_path = ckpt_manager.save() # expected errors
    print('Saving checkpoint for epoch {} at {}'.format(epoch + 1, ckpt_save_path))
``` 
"
35239,Return some sort of verifying data structure for load_weights for tf.keras models,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Currently, whenever user loads keras models using HDF5, user gets no confirmation that models were loaded successfully ([\[1\]](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#load_weights), [\[2\]](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=stable#load_weights) return nothing). This confirmation could be useful for verifying that weights are properly loaded. In case of transfer learning, users typically only want to load specific weights from the HDF5file. This can be currently achieved by using `load_weights(by_name=True)`, however, users don't get any confirmation about which layers were actually loaded.

Also, in case there are no matching layers in between the original model and source model, the `model.load_weights(by_name=True)` fails without raising any exception so there is no way to actually debug what went wrong with the model loading. (Note here that I'm talking about name mismatch not weight mismatch) This significantly affects the ability to write unit tests for models since the tester code cannot actually verify what layers were loaded from HDF5 file.

**Will this change the current api? How?**
Yes, this changes the current API for [\[1\]](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=stable#load_weights) and [\[2\]](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#load_weights) by adding a return type to them. Specifically, we will be returning a particular data structure when loading HDF5 files [here](https://github.com/tensorflow/tensorflow/blob/c49396cf71dacc32195033507b3bbd985b12c255/tensorflow/python/keras/engine/network.py#L1131). My current idea is to return the list of layers that were loaded from HDF5 although I'm open to discussion about the return type.

**Who will benefit with this feature?**
Users who are loading their weights from h5py files and want to verify/check what layers were loaded from HDF5. 

**Any Other info.**
N/A


Let me know if any more clarification/information is needed. I'm willing to contribute by working on this issue."
35228,Configuring issues with tag 2.1.0,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.1.0rc1 (and rc0)
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): bazelisk
- GCC/Compiler version (if compiling from source): 6.3.0
- CUDA/cuDNN version: 10.1 and 7.6.5
- GPU model and memory: 2x NVidia Titan XP 11GB



1. Configuring from both tags (rc0 and rc1) picks the wrong bazel (0.26 instead of 1.1.0) due to misconfigured .bazelversion, which should be cherry-picked from a later commit to make it work. I don't see the reason for this to be necessary and not having a buildable tag. The commit a5f9bcd64453ff3d1f64cb4da4786db3d2da7f82 should be the one but, while I understand this making rc0 not work, I don't understand rc1 also not working since it's a later tag.

2. Configuring out of source, i.e. with a tree made to be able to call `../configure` (hybrid out-of-source) or `../tensorflow/configure` (full out-of-source) results in `/usr/local/bin/python3.6: can't open file 'third_party/gpus/find_cuda_config.py': [Errno 2] No such file or directory`, likely due to 'third_party/gpus/find_cuda_config.py' being an absoulte path instead of a relative one."
35227,Tensorflow build unsuccessful using bazel build,"So I have been following to this guide https://www.tensorflow.org/install/source and https://www.tensorflow.org/install/gpu. My system configuration is


+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1050    Off  | 00000000:01:00.0  On |                  N/A |
| 45%   27C    P8    N/A /  75W |    258MiB /  1997MiB |     10%      Default |
+-------------------------------+----------------------+----------------------+
                                                                              
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1022      G   /usr/lib/xorg/Xorg                           151MiB |
|    0      2686      G   compiz                                       102MiB |
|    0      5660      G   /usr/lib/firefox/firefox                       1MiB |
+-----------------------------------------------------------------------------+

./deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: ""GeForce GTX 1050""
  CUDA Driver Version / Runtime Version          10.2 / 9.0
  CUDA Capability Major/Minor version number:    6.1
  Total amount of global memory:                 1998 MBytes (2094989312 bytes)
  ( 5) Multiprocessors, (128) CUDA Cores/MP:     640 CUDA Cores
  GPU Max Clock rate:                            1455 MHz (1.46 GHz)
  Memory Clock rate:                             3504 Mhz
  Memory Bus Width:                              128-bit
  L2 Cache Size:                                 1048576 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)
  Run time limit on kernels:                     Yes
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Disabled
  Device supports Unified Addressing (UVA):      Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0
  Compute Mode:
     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.2, CUDA Runtime Version = 9.0, NumDevs = 1

nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Sep__1_21:08:03_CDT_2017
Cuda compilation tools, release 9.0, V9.0.176

libcudnn => 7.6.0
#define CUDNN_MAJOR 7
#define CUDNN_MINOR 6
#define CUDNN_PATCHLEVEL 0
--
#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)

#include ""driver_types.h""

NCCL version => 2.4.7
gcc => 5.4.0
pyhton => 2.7.0

dpkg -l | grep TensorRT
ii  graphsurgeon-tf                                            5.1.5-1+cuda9.0                                 amd64        GraphSurgeon for TensorRT package
ii  libnvinfer-dev                                             5.1.5-1+cuda9.0                                 amd64        TensorRT development libraries and headers
ii  libnvinfer-samples                                         5.1.5-1+cuda9.0                                 all          TensorRT samples and documentation
ii  libnvinfer5                                                5.1.5-1+cuda9.0                                 amd64        TensorRT runtime libraries
ii  python-libnvinfer                                          5.1.5-1+cuda9.0                                 amd64        Python bindings for TensorRT
ii  python-libnvinfer-dev                                      5.1.5-1+cuda9.0                                 amd64        Python development package for TensorRT
ii  tensorrt                                                   5.1.5.0-1+cuda9.0                               amd64        Meta package of TensorRT
ii  uff-converter-tf                                           5.1.5-1+cuda9.0                                 amd64        UFF converter for TensorRT package



after running this command
**bazel build --config=opt --config=cuda --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package
and
bazel build --config=opt  --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package**

I got the same error both time mentioned at bottom

**ERROR: /home/user/tensorflow/tensorflow/core/kernels/BUILD:2951:1: output 'tensorflow/core/kernels/_objs/cwise_op_gpu/tensorflow/core/kernels/cwise_op_gpu_mul.cu.pic.o' was not created
ERROR: /home/user/tensorflow/tensorflow/core/kernels/BUILD:2951:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
FAILED: Build did NOT complete successfully**

Any help would be appreciated I am still learning and trying to understand tensorflow and its conepts. It would be great and exciting if anyone could give any suggestions or advice where I am going wrong. Though I am referring to couple of references will keep you guys posted if I come across anything relevant. Let me know if more info is required. Thanks

"
35226,ValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.,"I am not able to run training using tf.distribute.Strategy
However, it works fine without distribution.
Below is the code block for training loop

```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from absl import app
import os
import tensorflow as tf # TF2
import model_timit as model
import kaldi_io
from DataLoader_timit import SequentialLoader
from warprnnt_tensorflow import rnnt_loss
assert tf.__version__.startswith('2')

class Train(object):

  def __init__(self, epochs, decoder,batch_size):
    self.epochs = epochs
    self.decoder = decoder
    self.batch_size = batch_size
    self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0004)
    self.train_loss_metric = tf.keras.metrics.Mean(name='train_loss')
    self.checkpoint = tf.train.Checkpoint(
            decoder=self.decoder,
            optimizer=self.optimizer)

  def loss_function(self, pred,real,xlen,ylen):
    loss_ = rnnt_loss(pred,real,xlen,ylen,0)
    return tf.reduce_sum(loss_) * 1. / self.batch_size

  def train_step(self, inputs):
    loss = 0
    inp, targ,xlen,ylen = inputs

    with tf.GradientTape() as tape:
      xs_1,xs,predictions = self.decoder(
            inp, targ)
      time_dim = tf.shape(predictions)[1]
      loss += self.loss_function(predictions,targ,xlen,ylen)

    batch_loss = (loss / int(targ.shape[1]))
    variables = (self.decoder.trainable_variables)
    gradients = tape.gradient(loss, variables)
    self.optimizer.apply_gradients(zip(gradients, variables))
    #self.optimizer.apply_gradients(list(zip(gradients, variables)))

    self.train_loss_metric(batch_loss)

    return self.train_loss_metric.result().numpy()

class DistributedTrain(Train):
  def __init__(self, epochs, decoder, batch_size, local_batch_size):
    Train.__init__(
        self, epochs, decoder, local_batch_size)

  def training_loop(self, train_ds, test_ds, strategy):
    def distributed_train(inp, targ, xlen, ylen):
      returnstrategy.experimental_run_v2(self.train_step((inp, targ, xlen, ylen)))

    distributed_train = tf.function(distributed_train)
    template = 'Epoch: {}, Train Loss: {}, Test Loss: {}'
    for epoch in range(self.epochs):
      self.train_loss_metric.reset_states()
      for i, (inp, targ, xlen, ylen) in enumerate(train_ds):
        distributed_train(inp, targ, xlen, ylen)

def main(epochs=200, batch_size=16, num_examples=70000, embedding_dim=256, enc_units=1024, dec_units=1024):

  strategy = tf.distribute.MirroredStrategy(devices=[""/gpu:0"",""/gpu:1""])
  num_replicas = strategy.num_replicas_in_sync

  train_ds = SequentialLoader('train', batch_size)
  test_ds = SequentialLoader('test', batch_size)

  with strategy.scope():
    decoder = model.Transducer(39, 62, 250, 3, 0.5,bidirectional=False)
    train_obj = DistributedTrain(10, decoder, batch_size, 8)

    print ('Training ...')
    return train_obj.training_loop(train_ds, test_ds, strategy)

if __name__ == '__main__':
  app.run(main)


```

This is the error.

```
train_timit_distributed.py:97 distributed_train  *
        per_example_loss = strategy.experimental_run_v2(self.train_step((inp, targ, xlen, ylen)))
    train_timit_distributed.py:61 train_step  *
        gradients = tape.gradient(loss, variables)
    /home/ubuntu/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py:996 gradient
        flat_sources = [_handle_or_self(x) for x in flat_sources]
    /home/ubuntu/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py:996 <listcomp>
        flat_sources = [_handle_or_self(x) for x in flat_sources]
    /home/ubuntu/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py:687 _handle_or_self
        x = x.handle
    /home/ubuntu/tf2/lib/python3.6/site-packages/tensorflow_core/python/distribute/values.py:717 handle
        raise ValueError(""`handle` is not available outside the replica context""

    ValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.

```
How to fix this?"
35225, how to inference online with tensorflow2.0? #24 ,"i am trying to inference online with tensorflow2.0. my code is as follows:

>         self.graph = tf.Graph()
> 
>         with self.graph.as_default() as g:
>             self.input_ids = tf.compat.v1.placeholder(tf.int32, [FLAGS.batch_size,
>                                                                  FLAGS.max_seq_length], name=""input_ids"")
>             self.input_mask = tf.compat.v1.placeholder(tf.int32, [FLAGS.batch_size,
>                                                                   FLAGS.max_seq_length], name=""input_mask"")
>             self.p_mask = tf.compat.v1.placeholder(tf.float32, [FLAGS.batch_size,
>                                                                 FLAGS.max_seq_length], name=""p_mask"")
>             self.segment_ids = tf.compat.v1.placeholder(tf.int32, [FLAGS.batch_size,
>                                                                    FLAGS.max_seq_length], name=""segment_ids"")
>             self.cls_index = tf.compat.v1.placeholder(tf.int32, [FLAGS.batch_size], name=""segment_ids"")
>             self.unique_ids = tf.compat.v1.placeholder(tf.int32, [FLAGS.batch_size], name=""unique_ids"")
> 
>             # unpacked_inputs = tf_utils.unpack_inputs(inputs)
>             self.squad_model = ALBertQAModel(
>                 albert_config, FLAGS.max_seq_length, init_checkpoint, FLAGS.start_n_top, FLAGS.end_n_top,
>                 FLAGS.squad_dropout)
> 
>             learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate=1e-5,
>                                                                              decay_steps=10000,
>                                                                              end_learning_rate=0.0)
>             optimizer_fn = AdamWeightDecay
>             optimizer = optimizer_fn(
>                 learning_rate=learning_rate_fn,
>                 weight_decay_rate=0.01,
>                 beta_1=0.9,
>                 beta_2=0.999,
>                 epsilon=1e-6,
>                 exclude_from_weight_decay=['layer_norm', 'bias'])
> 
>             self.squad_model.optimizer = optimizer
>             graph_init_op = tf.compat.v1.global_variables_initializer()
> 
>             y = self.squad_model(
>                 self.unique_ids, self.input_ids, self.input_mask, self.segment_ids, self.cls_index,
>                 self.p_mask, training=False)
>             self.unique_ids, self.start_tlp, self.start_ti, self.end_tlp, self.end_ti, self.cls_logits = y
> 
>             self.sess = tf.compat.v1.Session(graph=self.graph, config=gpu_config)
>             self.sess.run(graph_init_op)
>             with self.sess.as_default() as sess:
>                 self.squad_model.load_weights(FLAGS.model_dir)


This code is executable, but it runs bad result. It looks like the parameters are unloaded.I guess this is probably because I'm not using tf.Session to set default parameters on the model, such as' saver.restore(sess, tf.train. Latest_checkpoint (init_checkpoint)) '.
I've tried several ways to do this, but it hasn't worked.And there are very few examples of online inferencing using tensorflow2.0 on the Internet, and I have trouble finding a solution.  :((((
May i get some help here, thx very much!!"
35224,"Sparse Feature in c++, different shape cause different result","HI :
     I was using c++ api of tensorflow-1.14.0 and found a problem.
     I had a model in pb format and loaded it use c++. The model had a sparse node, so I construct indices, values shape for it.
     Three tensor name for a sparse node:
     DeserializeSparse:0  for indices,
     DeserializeSparse:1   for values,
     DeserializeSparse:2  for shape,

    case 1:
    shape = [1, 1];  indices = [0, 0]; values = [""""]  c++ got a predict result was same as python's result .
    case 2:
    shape = [1, 1];  indices = [0, 0]; values = [""abc""] c++ got a predict result  was different from python's.
    case 3:
    shape = [1, 100] indices = [0, 0]; values = [""abc""] c++ got a predict result was different from case 2.
    
    I found python always got the same answer with different shape but with the same indices and values.
    I thought, there may be some difference between c++ and python, and may be c++ is wrong.
  "
35222,imp.py,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
35221,Crash on Hexagon Delegate,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Mi A2, Pixel3
- TensorFlow version (use command below): 1.15.0
- Python version: 
- Bazel version (if compiling from source): 1.1.0
- GCC/Compiler version (if compiling from source):

**Describe the current behavior**
I've built the dsp delegate aar.
But on my miA2, I always got the following crash in native code.
Is there anything I can do to debug with the cc files?
Also, I tried on Pixel3, it returned this device does not support hexagon delegate.
It doesn't seem to be normal for a snapdragon 845 device.
```
2019-12-18 18:30:38.484 18124-18510/com.ivuu I/tflite: Created TensorFlow Lite delegate for Hexagon.
2019-12-18 18:30:38.491 18124-18510/com.ivuu I/tflite: Initialized TensorFlow Lite runtime.
2019-12-18 18:30:38.952 18124-18510/com.ivuu A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x2 in tid 18510 (Thread-130), pid 18124
```
"
35220,Imputing missing tensor values with the mean is unsuccessful,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-4.14.106+-x86_64-with-debian-buster-sid
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.6

**Describe the current behavior**
I am trying to impute the missing values in a tensor with the sample mean. As the size of my dataset is potentially quite large and the calculation of means require a full pass of the dataset, I am using tf.Transform to perform this computation.

As there is no native support for imputation on tf.Transform, I am implementing this by creating a `SparseTensor`, and specifying a default_value when converting to a dense tensor (as suggested in https://github.com/tensorflow/transform/issues/78#issuecomment-427919062).

However, this does not seem to be successful as the output tensor still contains `nan`.

**Describe the expected behavior**
I expect the missing values in the output tensor to be replaced by the mean of the non-null values.

**Code to reproduce the issue**
```python
import tempfile
from glob import glob

import apache_beam as beam
import pandas as pd
import tensorflow as tf
import tensorflow_transform as tft
import tensorflow_transform.beam as tft_beam
from tensorflow_transform.tf_metadata import dataset_metadata, schema_utils

assert tf.__version__ == ""2.0.0"" and tft.__version__ == ""0.15.0"" and beam.__version__ == ""2.16.0""

def create_raw_data(output_file):
    df = pd.DataFrame({""age"": [35.2, 17.3, None, 25.0]})
    with tf.io.TFRecordWriter(output_file) as writer:
        for _, row in df.iterrows():
            features = tf.train.Features(
                feature={""age"": tf.train.Feature(float_list=tf.train.FloatList(value=[row]))}
            )
            example_proto = tf.train.Example(features=features)
            writer.write(example_proto.SerializeToString())

            
def run_tftransform(input_file, output_file):
    def preprocessing_fn(inputs):
        def _impute(tensor, replacement):
            sparse = tf.sparse.SparseTensor(
                tensor.indices, tensor.values, [tensor.dense_shape[0], 1]
            )
            dense = tf.sparse.to_dense(sp_input=sparse, default_value=replacement)
            dense = tf.squeeze(dense, axis=1)
            return dense

        outputs = inputs.copy()
        mean_age = tft.mean(outputs[""age""])
        outputs[""age""] = _impute(outputs[""age""], mean_age)  # mean is 25.833333333
        return outputs

    RAW_DATA_FEATURE_SPEC = {""age"": tf.io.VarLenFeature(tf.float32)}
    RAW_DATA_METADATA = dataset_metadata.DatasetMetadata(
        schema_utils.schema_from_feature_spec(RAW_DATA_FEATURE_SPEC)
    )

    with beam.Pipeline() as pipeline:
        with tft_beam.Context(temp_dir=tempfile.mkdtemp()):
            raw_data_coder = tft.coders.ExampleProtoCoder(RAW_DATA_METADATA.schema)
            raw_train_data = (
                pipeline | beam.io.ReadFromTFRecord(input_file, coder=raw_data_coder)
            )
            (transformed_train_data, transformed_metadata), transform_fn = (
                (raw_train_data, RAW_DATA_METADATA)| tft_beam.AnalyzeAndTransformDataset(preprocessing_fn)
            )
            transformed_data_coder = tft.coders.ExampleProtoCoder(transformed_metadata.schema)
            _ = (
                transformed_train_data| beam.io.WriteToTFRecord(
                    output_file, coder=transformed_data_coder
                )
            )
            
            
if __name__ == ""__main__"":
    create_raw_data(""raw_data.tfrecord"")
    run_tftransform(""raw_data.tfrecord"", ""transformed_data.tfrecord"")

    TRANSFORMED_FEATURE_SPEC = {""age"": tf.io.FixedLenFeature([], tf.float32)}
    raw_dataset = tf.data.TFRecordDataset(filenames=glob(""transformed_data.tfrecord*""))
    parsed_dataset = raw_dataset.map(lambda x: tf.io.parse_single_example(x, TRANSFORMED_FEATURE_SPEC)).batch(4)

    print(next(iter(parsed_dataset.take(1))))
```

Expected output:
```python
{'age': <tf.Tensor: id=533, shape=(4,), dtype=float32, numpy=array([35.2, 17.3,  25.8333, 25. ], dtype=float32)>}
```
Actual (incorrect) output which still contains `nan` values:
```python
{'age': <tf.Tensor: id=533, shape=(4,), dtype=float32, numpy=array([35.2, 17.3,  nan, 25. ], dtype=float32)>}
```"
35219,Install Tensorflow for Gitlab-CI,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Gitlab-CI, I think that's Linux
- TensorFlow version: 2.0.0
- Python version: 3.8
- Installed using virtualenv? pip? conda?: pip

**Describe the problem**

I need to add Tensorflow to my requirement.txt. First I added ""tensorflow"" without a specific version, but in Gitlab-CI linters give me information that I need to add a specific version, so I added it and it looks like ""tensorflow == 2.0.0-rc0"" but in Gitlab-CI I got that:

```
$ pip install -r requirements.txt
Collecting numpy==1.16.4
  Downloading https://files.pythonhosted.org/packages/d3/4b/f9f4b96c0b1ba43d28a5bdc4b64f0b9d3fbcf31313a51bc766942866a7c7/numpy-1.16.4.zip (5.1MB)
ERROR: Could not find a version that satisfies the requirement tensorflow==2.0.0-rc0 (from -r requirements.txt (line 2)) (from versions: none)
ERROR: No matching distribution found for tensorflow==2.0.0-rc0 (from -r requirements.txt (line 2))
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Create .Gitlab-CI.yml
2. Code for that file:
```
image: ""python:3.8""

before_script:
  - python --version
  - python -c 'import struct;print( 8 * struct.calcsize(""P""))'
  - pip install --upgrade pip
  - pip install --upgrade setuptools
  - pip install -r requirements.txt
```

3. Add file requirements.txt with that:
```
numpy==1.16.4
tensorflow==2.0.0-rc0
```
4. Push branch to repo
5. CI give me error like above.

**Any other info / logs**
tensorflow==2.0.0rc0 and tensorflow==2.0.0 or tensorflow==2.0.0-rc0 give me same error.
"
35216,tf.keras CANNOT use my custom metrics or loss function!,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**

Previously, I used Keras instead of tf.keras to run my code and it can work smoothly.
CCC is the function I implemented.

`
def CCC(y_true, y_pred, sample_weight=None, multioutput='uniform_average'):

    y_true = K.reshape(y_true, (-1,NB_LABELS))
    y_pred = K.reshape(y_pred, (-1,NB_LABELS))
    y_true = K.argmax(y_true, axis=1)
    y_pred = K.argmax(y_pred, axis=1)

    y_true = K.cast(y_true, K.floatx())
    y_pred = K.cast(y_pred, K.floatx())

    # covariance between y_true and y_pred

    s_xy = K.mean((y_true - K.mean(y_true)) * (y_pred - K.mean(y_pred)))
    # means
    x_m = K.mean(y_true)
    y_m = K.mean(y_pred)
    # variances
    s_x_sq = K.var(y_true)
    s_y_sq = K.var(y_pred)

    # condordance correlation coefficient
    ccc = (2.0 * s_xy) / (s_x_sq + s_y_sq + (x_m - y_m) ** 2 + K.epsilon())
    ccc = K.maximum(K.minimum(ccc, 1.0), -1.0)

    return ccc
`

- OS Platform and Distribution: Linux  Ubuntu 18.04.2
- TensorFlow version (use command below): tensorflow 2.1.0rc1   
- Python version: python3.7
- CUDA/cuDNN version:CUDA Version: 10.1   
- GPU model and memory:  Tesla T4 16GB Memory

**Other info / logs**
The reason why I tried to use tf.keras is that I want to use keras-tuner to tune my parameters.
These are errors when combining my code with keras-tuner, but it seems like tf.keras issue not keras-tuner issue..

`
TRAIN STEPS:
2
VAL STEPS:
2
class_weight
{0: 6.695558842629338, 1: 31.333890492832904, 2: 17.24165962151265, 3: 4.737384205783776, 4: 4.5054695617553095, 5: 7.342352322135755, 6: 5.259447664120839, 7: 7.3335032481388405, 8: 1.9969838777884066, 9: 11.858626198083067, 10: 3.5724254090471605}
x_array.shape, y_array.shape, sample_weight_array.shape:
(500, 2048) (500, 11)
WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to  
  ['...']
x_array.shape, y_array.shape, sample_weight_array.shape:
(500, 2048) (500, 11)
WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to  
  ['...']
Train for 2 steps, validate for 2 steps
x_array.shape, y_array.shape, sample_weight_array.shape:
(500, 2048) (500, 11)
1/2 [==============>...............] - ETA: 2sWARNING:tensorflow:Early stopping conditioned on metric 'val_CCC' which is not available. Available metrics are: 
WARNING:tensorflow:Can save best model only with val_CCC available, skipping.
WARNING:tensorflow:Can save best model only with val_CCC available, skipping.
Traceback (most recent call last):
  File ""hp_tunning.py"", line 297, in <module>
    tunning()
  File ""hp_tunning.py"", line 206, in tunning
    train_weight=train_weight)
  File ""/home/i/i0000013/EmoPain2020-FinalVersion/train_tunning.py"", line 144, in train_tunning
    class_weight=class_weight)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/kerastuner/engine/base_tuner.py"", line 122, in search
    self.run_trial(trial, *fit_args, **fit_kwargs)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/kerastuner/engine/multi_execution_tuner.py"", line 95, in run_trial
    history = model.fit(*fit_args, **fit_kwargs, callbacks=callbacks)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 819, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 342, in fit
    total_epochs=epochs)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 128, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 98, in execution_function
x_array.shape, y_array.shape, sample_weight_array.shape:
    distributed_function(input_fn))
(500, 2048) (500, 11)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 568, in __call__
    result = self._call(*args, **kwds)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 632, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2363, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1611, in _filtered_call
    self.captured_inputs)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1692, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 545, in call
    ctx=ctx)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Can not squeeze dim[0], expected a dimension of 1, got 500
	 [[node metrics/CCC_1/Squeeze (defined at /home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/kerastuner/engine/multi_execution_tuner.py:95) ]]
	 [[metrics/mse_1/broadcast_weights/assert_broadcastable/is_valid_shape/else/_47/has_valid_nonscalar_shape/then/_285/has_invalid_dims/_68]]
  (1) Invalid argument:  Can not squeeze dim[0], expected a dimension of 1, got 500
	 [[node metrics/CCC_1/Squeeze (defined at /home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/kerastuner/engine/multi_execution_tuner.py:95) ]]
0 successful operations.
0 derived errors ignored. [Op:__inference_distributed_function_3424]

Function call stack:
distributed_function -> distributed_function
`


These are errors when running the code using tf.keras but that worked fine in Keras....

`
WARNING:tensorflow:From /home/i/i0000013/EmoPain2020-FinalVersion/train.py:117: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
Please use Model.fit, which supports generators.
x_array.shape, y_array.shape, sample_weight_array.shape:
(500, 2048) (500, 11)
WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to  
  ['...']
x_array.shape, y_array.shape, sample_weight_array.shape:
(500, 2048) (500, 11)
WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to  
  ['...']
Train for 2 steps, validate for 2 steps
x_array.shape, y_array.shape, sample_weight_array.shape:
(500, 2048) (500, 11)
1/2 [==============>...............] - ETA: 2sWARNING:tensorflow:Early stopping conditioned on metric `val_CCC` which is not available. Available metrics are: 
WARNING:tensorflow:Can save best model only with val_CCC available, skipping.
Traceback (most recent call last):
  File ""main.py"", line 283, in <module>
    run()
  File ""main.py"", line 196, in run
    train_weight=train_weight)
  File ""/home/i/i0000013/EmoPain2020-FinalVersion/train.py"", line 117, in train
    class_weight=class_weight)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 1306, in fit_generator
    initial_epoch=initial_epoch)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 819, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 342, in fit
    total_epochs=epochs)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 128, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 98, in execution_function
    distributed_function(input_fn))
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 568, in __call__
    result = self._call(*args, **kwds)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 632, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2363, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1611, in _filtered_call
    self.captured_inputs)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1692, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 545, in call
    ctx=ctx)
  File ""/home/i/i0000013/miniconda3/envs/emopain-tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Can not squeeze dim[0], expected a dimension of 1, got 500
	 [[node metrics/CCC_1/Squeeze (defined at /home/i/i0000013/EmoPain2020-FinalVersion/train.py:117) ]]
	 [[metrics/acc_1/broadcast_weights/assert_broadcastable/is_valid_shape/else/_24/has_valid_nonscalar_shape/then/_275/has_invalid_dims/ExpandDims_1/_56]]
  (1) Invalid argument:  Can not squeeze dim[0], expected a dimension of 1, got 500
	 [[node metrics/CCC_1/Squeeze (defined at /home/i/i0000013/EmoPain2020-FinalVersion/train.py:117) ]]
0 successful operations.
0 derived errors ignored. [Op:__inference_distributed_function_3010]

Function call stack:
distributed_function -> distributed_function
`"
35214,tflite.load_delegate() failed when running Demo API on Google Coral mini PCIe,"I am also facing a similar issue.
The demo API gives error at tflite.load_delegate.

```
pi@bpi-iot-ros-ai:~/coral/tflite/python/examples/classification$ python3 classify_image.py --model models/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite --labels models/inat_bird_labels.txt --input images/parrot.jpg
E :248] HIB Error. hib_error_status = 0000000000000001, hib_first_error_status = 0000000000000001
E :248] HIB Error. hib_error_status = 0000000000000001, hib_first_error_status = 0000000000000001
INFO: Initialized TensorFlow Lite runtime.
----INFERENCE TIME----
Note: The first inference on Edge TPU is slow because it includes loading the model into Edge TPU memory.



pi@bpi-iot-ros-ai:~$ uname -a
Linux bpi-iot-ros-ai 5.4.0-bpi-r64 #1 SMP PREEMPT Mon Dec 16 16:00:08 IST 2019 aarch64 aarch64 aarch64 GNU/Linux
pi@bpi-iot-ros-ai:~$ lscpu
Architecture:          aarch64
Byte Order:            Little Endian
CPU(s):                2
On-line CPU(s) list:   0,1
Thread(s) per core:    1
Core(s) per socket:    2
Socket(s):             1
CPU max MHz:           1350.0000
CPU min MHz:           30.0000
pi@bpi-iot-ros-ai:~$ ls -l /usr/lib/aarch64-linux-gnu/libedge*
lrwxrwxrwx 1 root root     17 Sep 17 04:27 /usr/lib/aarch64-linux-gnu/libedgetpu.so.1 -> libedgetpu.so.1.0
-rwxrwxrwx 1 root root 792376 Sep 17 04:27 /usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0
pi@bpi-iot-ros-ai:~$ lspci
00:00.0 PCI bridge: MEDIATEK Corp. Device 3258
01:00.0 System peripheral: Device 1ac1:089a
pi@bpi-iot-ros-ai:~$ ls /dev/apex_0 
/dev/apex_0
pi@bpi-iot-ros-ai:~$ sudo sh -c ""echo 'SUBSYSTEM==\""apex\"", MODE=\""0660\"", GROUP=\""apex\""' >> /etc/udev/rules.d/65-apex.rules""
pi@bpi-iot-ros-ai:~$ sudo groupadd apex
groupadd: group 'apex' already exists
pi@bpi-iot-ros-ai:~$ sudo adduser $USER apex
The user `pi' is already a member of `apex'.
```
"
35213,macOS compiling: hidden symbol `__dso_handle' isn't defined and undefined reference to `operator delete(void*)',"@tensorflow/micro

**System information**
- macOS 10.15 Catalina
- TensorFlow installed from source 
- Tensorflow version (commit SHA if source):2ba0b2ef68d4259e8b02fa8be77a9372020b81b7 (Dec. 16)
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): bluepill

**Describe the problem**
```
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -fno-rtti -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/micro/tools/make/downloads/kissfft -o tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/bin/hello_world tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/micro/examples/hello_world/main.o tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/micro/examples/hello_world/main_functions.o tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/micro/examples/hello_world/sine_model_data.o tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/micro/examples/hello_world/output_handler.o tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/micro/examples/hello_world/constants.o  tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/lib/libtensorflow-microlite.a -T tensorflow/lite/micro/tools/make/targets/bluepill/bluepill.lds -Wl,-Map=tensorflow/lite/micro/tools/make/gen/bluepill.map,--cref -Wl,--gc-sections -lm
tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/micro/examples/hello_world/main_functions.o: In function `setup()':
/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/examples/hello_world/main_functions.cc:47: undefined reference to `__cxa_guard_acquire'
/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/examples/hello_world/main_functions.cc:47: undefined reference to `__cxa_guard_release'
/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/examples/hello_world/main_functions.cc:63: undefined reference to `__cxa_guard_acquire'
/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/examples/hello_world/main_functions.cc:63: undefined reference to `__cxa_guard_release'
/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/examples/hello_world/main_functions.cc:67: undefined reference to `__cxa_guard_acquire'
/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/examples/hello_world/main_functions.cc:67: undefined reference to `__cxa_guard_release'
/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/examples/hello_world/main_functions.cc:83: undefined reference to `__dso_handle'
tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/lib/libtensorflow-microlite.a(greedy_memory_planner.o): In function `tflite::GreedyMemoryPlanner::~GreedyMemoryPlanner()':
/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/memory_planner/greedy_memory_planner.cc:70: undefined reference to `operator delete(void*)'
`__lock___atexit_recursive_mutex' referenced in section `.data.__atexit_recursive_mutex' of /Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/lib/thumb/v7-m/libc.a(lib_a-__call_atexit.o): defined in discarded section `COMMON' of /Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/lib/thumb/v7-m/libc.a(lib_a-lock.o)
/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/bin/ld: tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/bin/hello_world: hidden symbol `__dso_handle' isn't defined
/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/bin/ld: final link failed: Bad value
collect2: error: ld returned 1 exit status
gmake: *** [tensorflow/lite/micro/examples/hello_world/Makefile.inc:42: tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/bin/hello_world] Error 1
```

**Please provide the exact sequence of commands/steps when you ran into the problem**

```
$ /opt/local/bin/gmake -j1 -f tensorflow/lite/micro/tools/make/Makefile TARGET=bluepill hello_world
```
"
35212,tf.load_op_library bug,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): https://github.com/cogaplex-bts/bts
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 1604
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below):1.13.2
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): pip 
- CUDA/cuDNN version: 10.0 
- GPU model and memory: 1060, 6G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
in a process, use full path , twice load:
tf.load_op_library(os.path.join(dname, 'build/libcompute_depth.so'))
the second time, OP_LIST is empty, 
but with relative path, twice  tf.load_op_library not empty
**Describe the expected behavior**
use full path, twice  tf.load_op_library, OP_LIST should not empty
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
I compile https://github.com/cogaplex-bts/bts, met this problem. But I think this is a general problem, reproduce: just in a python file, load the library twice, with full path and relative path   
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
compute_depth_grad_module dict: {'__name__': 'a11935c229913616b7b14d8da52f01ac', '__doc__': 'Python wrappers around TensorFlow ops.\n\nThis file is MACHINE GENERATED! Do not edit.\n', ...  , 'LIB_HANDLE': <Swig Object of type 'TF_Library *' at 0x7ff4ad011f60>, 'OP_LIST': op {
  name: ""ComputeDepth""
  input_arg {
    name: ""input""
    type: DT_FLOAT
  }
  input_arg {
    name: ""focal""
    type: DT_FLOAT
  }
  output_arg {
    name: ""depth""
    type: DT_FLOAT
  }
  attr {
    name: ""upratio""
    type: ""int""
  }
}
op {
  name: ""ComputeDepthGrad""
  input_arg {
    name: ""depth_grad""
    type: DT_FLOAT
  }
  input_arg {
    name: ""input""
    type: DT_FLOAT
  }
  input_arg {
    name: ""focal""
    type: DT_FLOAT
  }
  output_arg {
    name: ""grad_input""
    type: DT_FLOAT
  }
  output_arg {
    name: ""grad_focal""
    type: DT_FLOAT
  }
}
}
{'__name__': 'a11935c229913616b7b14d8da52f01ac', '__doc__': 'Python wrappers around TensorFlow ops.\n\nThis file is MACHINE GENERATED! Do not edit.\n', '__package__': None,  ... , 'LIB_HANDLE': <Swig Object of type 'TF_Library *' at 0x7ff4ad011f60>, 'OP_LIST': op {
  name: ""ComputeDepth""
  input_arg {
    name: ""input""
    type: DT_FLOAT
  }
  input_arg {
    name: ""focal""
    type: DT_FLOAT
  }
  output_arg {
    name: ""depth""
    type: DT_FLOAT
  }
  attr {
    name: ""upratio""
    type: ""int""
  }
}
op {
  name: ""ComputeDepthGrad""
  input_arg {
    name: ""depth_grad""
    type: DT_FLOAT
  }
  input_arg {
    name: ""input""
    type: DT_FLOAT
  }
  input_arg {
    name: ""focal""
    type: DT_FLOAT
  }
  output_arg {
    name: ""grad_input""
    type: DT_FLOAT
  }
  output_arg {
    name: ""grad_focal""
    type: DT_FLOAT
  }
}
}
```

but with full name, second will print:
```
compute_depth_grad_module dict: {'__name__': '670cc8cfec5b6d3b8635f39bd583d769', '__doc__': 'Python wrappers around TensorFlow ops.\n\nThis file is MACHINE GENERATED! Do not edit.\n', '__package__': None, ... , 'LIB_HANDLE': <Swig Object of type 'TF_Library *' at 0x7f6b2a8f3e70>, 'OP_LIST': }
```
"
35211,TensorImage grayscale (single channel) image support ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Huawei
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 2.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: 10.2
- **GPU model and memory**: GTX2080TI
- **Exact command to reproduce**:

### Describe the problem
This is a feature request. I am struggling to make a TFLITE model that works perfectly in Python to  also work in and Android apk. It seems I have problems related with the conversion from Bitmap to a bytebuffer, std, mean and that stuff (I've been using old Android examples that made the conversion Bitmap -> Bytebuffer manually). 

Now I've seen this new TensorImage feature in tensorflow-lite-support and tried it, but it seems it only supports loading RGB images. My model is MNIST-based, so the input shape of the images are (28, 28, 1). I don't see how I can use TensorImage to feed my model in this scenario.

Can you please support grayscale (1 channel) images also please? 

Thanks,

### Source code / logs

"
35210,I need a function which like np.argwhere,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):2.0
- Are you willing to contribute it (Yes/No):No

**Describe the feature and the current behavior/state.**
The current API only includes argmax and argmin for return the index of tensor，i need a function 
which like np.argwhere for inference centernet
**Will this change the current api? How?**
it will not change.only append an api in tf.math
**Who will benefit with this feature?**
everyone
**Any Other info.**
"
35209,Does TF2.0 absolutely support TPU now？,"Our TF code is written with TF2.0 (eager mode). However we heard that TF2.0 cannot run on TPU
 device. So is there any supporting plan ? Or when to release TPU-friendly TF2.X?"
35208,"TypeError: expected bytes, Descriptor found","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (Windows 10)
- TensorFlow installed from (source):
- TensorFlow version: Tensorflow GPU Version 1.14.0
- Python version: 3.6.9
- Installed using conda:
- CUDA/cuDNN version: cuda_10.0.130_411.31_win10 / cudnn-10.0-windows10-x64-v7.4.1.5
- GPU model and memory: nvidia mx 250 with 2GB0



**Describe the problem**
I am folliowing the [Edge Electronics](https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10#1-install-anaconda-cuda-and-cudnn) github repository. At [step 2d. Set up new Anaconda virtual environment](https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10#2d-set-up-new-anaconda-virtual-environment)  I created a virtual environment using `conda create -n tensorflow1 pip python=3.6` 
The other commands I executed are:
`activate tensorflow1`
`python -m pip install --upgrade pip`
`pip install --ignore-installed --upgrade tensorflow-gpu==1.14.0`
After last command, when I try to import tensorflow, it get imported successfully but after running follwoing command,
`conda install -c anaconda protobuf` when I again try to import tensorflow, I get the following error. 
(tensorflow1) C:\>python
Python 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 14:00:49) [MSC v.1915 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\saqib\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\saqib\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""C:\Users\saqib\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\core\framework\graph_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2
  File ""C:\Users\saqib\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\core\framework\node_def_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
  File ""C:\Users\saqib\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\core\framework\attr_value_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
  File ""C:\Users\saqib\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\core\framework\tensor_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
  File ""C:\Users\saqib\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\core\framework\resource_handle_pb2.py"", line 92, in <module>
    __module__ = 'tensorflow.core.framework.resource_handle_pb2'
TypeError: expected bytes, Descriptor found

I have tried everthing like removing the virtual environment and creating the new but still the same error. 
What mistake dou you think I am doing?
Any possible solution will be apppreciated.
Thanks in advance :) 
Looking forward for quick reply
"
35206,"tf.function makes tf.keras.layers.LSTM significantly slower,  no leading to retracing","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution:  Linux Ubuntu 16.04 and 18.04.
- TensorFlow installed from: pip
- TensorFlow version: v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version:3.7.3
- CUDA/cuDNN version: both (Nvidia 410.79, CUDA Version: 10.0) and (418.87.01, 10.1)
- GPU model and memory: both 1080 and V100.

**Describe the current behavior**
  **case** 1. run normally in CPU mode.
  **case** 2. run normally in eager mode in one GPU card, which is several times faster than the case 1.
  **case**  3. run erroneously when using tf.function in train.py:_train_one_batch in one GPU card. I use input signature, and examine there are **no** extra retracing. The speed is much slower than that in the eager mode and almost the same with the CPU mode. I also found the GPU usage is close to 100%.

**Describe the expected behavior**
 The case 3 should have similar speed with the case 2, and be faster than the case 1.

**Code to reproduce the issue**
I provide a reproducible test case.

**Other info / logs**
I
[tf_2_mvp.zip](https://github.com/tensorflow/tensorflow/files/3976188/tf_2_mvp.zip)


"
35205,Object detection producing incorrect results on mobile (ios),"Object detection SSD trained and tested to produce expected results in PC. 
http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz
https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_inception_v2_pets.config

Model works well and produces accurate results on PC. 
After converting to tflite and loading in mobile, the camera view shows incorrect detection (several detection of same class and in fixed place even when moving the camera)."
35204,DLL load failed for Tensorflow-GPU==1.14/1.13 but not Tensorflow-GPU==2.0,"**System information**
CUDA Version: 10.2
CUDNN Version: 7
OS: Windows 10
Conda Version: 4.8
Python version: 3.7.4

I understand that this issue is common, however, I believe my case is sufficiently unique to warrant a new issue. Using the above versions of CUDA and CUDNN, after running `pip install tensorflow-gpu==2.0` and then importing the library in my conda virtual env, everything works. However, after uninstalling `tensorflow-gpu==2.0` and running `pip install tensorflow-gpu==1.14`, when importing I get:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Harry\Anaconda3\envs\tfv1\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Harry\Anaconda3\envs\tfv1\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Harry\Anaconda3\envs\tfv1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Harry\Anaconda3\envs\tfv1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Harry\Anaconda3\envs\tfv1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Harry\Anaconda3\envs\tfv1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Harry\Anaconda3\envs\tfv1\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Harry\Anaconda3\envs\tfv1\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.
```

I am confused because both versions of tensorflow-GPU are supposed to work with these versions of CUDA and CUDNN according to: https://www.tensorflow.org/install/source_windows.

Why I am I not facing any issues with `Tensorflow-GPU==2.0` but I am with `Tensorflow-GPU==1.14`?

If it is a compatibility issue, exactly what downgrades are necessary and why?

I am trying to use an older version of Tensorflow so that I can follow along with: https://github.com/chrisdonahue/wavegan which is not written in version 2 of Tensorflow.
"
35203,Overriding gradients in eager mode with TF 2 after the model is defined,"When I wrote SHAP for TensorFlow 1.x I was able to temporarily override the gradient operators inside `tensorflow.python.framework.ops._gradient_registry._registry`. This allowed me to use the back-propagation mechanisms of TF to compute explainable AI methods on models that had already been defined by the user. I couldn't use the `gradient_override_map` since that would require the user to build their model within my SHAP context...and I need to assume they already have a model they have built. Since I can ""back-propagate"" explanations through non-differentiable functions I also need to temporarily override `tensorflow.python.ops.gradients_impl._IsBackpropagatable` so my gradient handlers get called everywhere and things don't get pruned from the graph. Both of these are not public APIs, but there was no other way I found to get it to work.

Now in TF 2.0 I can't seem to replicate this while in eager mode. It looks like I can get everything to work using `tf.GradientTape` when everything between the input and the output is differentiable. But as soon as something that is not differentiable shows up (like ResourceGather) then it seems that in eager mode overriding `_IsBackpropagatable` no longer ensures that the gradient handlers will get called. I assume this is because the GradientTape doesn't think the ""watched"" tensor is connected to the output when the path through the graph includes non-differentiable operators. One would think that overriding  `_IsBackpropagatable` would fix this, but apparently not in TF 2.0 with eager mode. In fact I never see _IsBackpropagatable getting called at all in TF 2 eager mode. I was trying to figure out what the equivalent function is I can override, but with TF2 the GradientTape does all the primary work now in C++, so it is not as easy to trace through and get at what I need.

Any advice? Thanks!"
35199,Softmax activations don't get converted to Softmax TFLite operator if ndim > 2,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.1
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): tf-nightly==2.1.0.dev20191203

**Command used to run the converter or code if you’re using the Python API**

```
import pathlib

inpt = tf.keras.layers.Input(shape=[256, 256, 3])
out = tf.keras.layers.Lambda(lambda x: tf.keras.activations.softmax(x))(inpt)
out = tf.keras.layers.Lambda(lambda x: tf.nn.softmax(x))(out)
model = tf.keras.Model(inpt, out)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
pathlib.Path('out.tflite').write_bytes(tflite_model)
```

**Failure details**
![image](https://user-images.githubusercontent.com/1422280/71025023-3904e880-20d4-11ea-95fb-29cfea49a44d.png)
This graph shows the difference between the different softmax methods.  When using `tf.keras.activations.softmax`, there is [code](https://github.com/tensorflow/tensorflow/blob/v2.1.0-rc1/tensorflow/python/keras/activations.py#L43-L79) with a workaround for multiple dimensions.  It looks like this was written before the tensorflow op had multi-dimension support. "
35197,ModuleNotFoundError: No module named 'tensorflow.contrib',"Hello, I'm trying to use TF but I have an issue...
When I run my code I have :

    import tflearn
  File ""//anaconda3/lib/python3.7/site-packages/tflearn/__init__.py"", line 4, in <module>
    from . import config
  File ""//anaconda3/lib/python3.7/site-packages/tflearn/config.py"", line 5, in <module>
    from .variables import variable
  File ""//anaconda3/lib/python3.7/site-packages/tflearn/variables.py"", line 7, in <module>
    from tensorflow.contrib.framework.python.ops import add_arg_scope as contrib_add_arg_scope
ModuleNotFoundError: No module named 'tensorflow.contrib'

I believe I'm using tensorflow 2.0. 

The modules in my code : 
import nltk
from nltk.stem.lancaster import LancasterStemmer
import numpy
import tflearn
import tensorflow 
import json 
import random

Can you help me ? 
Thank you."
35195,Tensorflow not detecting/recognizing GPU,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: PC
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: 2.0.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip/conda (both, neither work)
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA 10.0, cuDNN 7
- GPU model and memory: NVIDIA GeForce 940M



I'm using an NVIDIA GeForce 940M, and have followed the instructions to install Tensorflow GPU exactly as given [here](https://www.tensorflow.org/install/gpu):

    # Add NVIDIA package repositories
    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb
    sudo dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb
    sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub
    sudo apt-get update
    wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
    sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
    sudo apt-get update
    
    # Install NVIDIA driver
    sudo apt-get install --no-install-recommends nvidia-driver-418
    # Reboot. Check that GPUs are visible using the command: nvidia-smi
    
    # Install development and runtime libraries (~4GB)
    sudo apt-get install --no-install-recommends \
        cuda-10-0 \
        libcudnn7=7.6.2.24-1+cuda10.0  \
        libcudnn7-dev=7.6.2.24-1+cuda10.0
    # Install TensorRT. Requires that libcudnn7 is installed above.
    sudo apt-get install -y --no-install-recommends libnvinfer5=5.1.5-1+cuda10.0 \
        libnvinfer-dev=5.1.5-1+cuda10.0

This is the output of `nvidia-smi`:

    +-----------------------------------------------------------------------------+
    | NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |
    |-------------------------------+----------------------+----------------------+
    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
    |===============================+======================+======================|
    |   0  GeForce 940M        On   | 00000000:01:00.0 Off |                  N/A |
    | N/A   52C    P0    N/A /  N/A |    238MiB /  4046MiB |      6%      Default |
    +-------------------------------+----------------------+----------------------+
                                                                                   
    +-----------------------------------------------------------------------------+
    | Processes:                                                       GPU Memory |
    |  GPU       PID   Type   Process name                             Usage      |
    |=============================================================================|
    |    0      1027      G   /usr/lib/xorg/Xorg                            24MiB |
    |    0      1184      G   /usr/bin/gnome-shell                          46MiB |
    |    0      1388      G   /usr/lib/xorg/Xorg                           110MiB |
    |    0      1555      G   /usr/bin/gnome-shell                          52MiB |
    +-----------------------------------------------------------------------------+

and `nvcc -v`:

    nvcc: NVIDIA (R) Cuda compiler driver
    Copyright (c) 2005-2018 NVIDIA Corporation
    Built on Sat_Aug_25_21:08:01_CDT_2018
    Cuda compilation tools, release 10.0, V10.0.130

and I've installed `tensorflow-gpu` using both conda and pip (neither work).

The output of

    from tensorflow.python.client import device_lib
    print(device_lib.list_local_devices())

is:

    [name: ""/device:CPU:0""
    device_type: ""CPU""
    memory_limit: 268435456
    locality {
    }
    incarnation: 9340164754758349370
    , name: ""/device:XLA_CPU:0""
    device_type: ""XLA_CPU""
    memory_limit: 17179869184
    locality {
    }
    incarnation: 3967057350071782501
    physical_device_desc: ""device: XLA_CPU device""
    ]

As seen, tensorflow does not recognize the GPU. What do I do?





"
35194,"Failed to convert weights to 8 bit precision: ""Quantize weights tool only supports tflite models with one subgraph"" ","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab (GPU)
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 2.1.0-dev20191217 
- **Python version**: 3
- **Exact command to reproduce**:

```bash
!pip install tf-nightly
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout

model = Sequential()
model.add(GRU(100, activation='relu', return_sequences=False, input_shape=(128,2)))
model.add(Dropout(0.2))
model.add(Dense(11, activation='softmax'))

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.experimental_new_converter = True
tflite_model = converter.convert()
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
tflite_model_quant = converter.convert()
```

### Error message
```bash
E tensorflow/lite/tools/optimize/quantize_weights.cc:351] Quantize weights tool only supports tflite models with one subgraph.
```

### Describe the problem
First, I used the new converter (with the experimental flag converter.experimental_new_converter = True) to convert an RNN model from TensorFlow to TensorFlow Lite's flat buffer format, as suggested in the issue #32608.

This works correctly, but then when I try to perform a post-training weight quantization, I got an error saying that the quantize weights tool only supports tflite models with one subgraph. 

Is there a problem with my procedure? Or is that feature not yet supported? In that case, I would like to request this feature.

Thanks in advance for your help.

### Source code / logs
The attached file can be used to reproduce the error with a trained model (.h5). 
[GRU_1L.zip](https://github.com/tensorflow/tensorflow/files/3974453/GRU_1L.zip)


"
35193,ImportError: DLL load failed TF 1.15,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
- TensorFlow version: 1.15
- Python version: 3.7.5
- Installed using virtualenv? pip? conda?: pip inside conda
- CUDA/cuDNN version: 10.1
- GPU model and memory: Nvidia GTX 750 with 1 GB Memory



I get the error that ""Failed to load the native TensorFlow runtime."" when trying to use Tensorflow with ImageAI.

This is my complete stack-trace:

> Using TensorFlow backend.
> Traceback (most recent call last):
>   File ""C:\Anaconda3\envs\imageai\lib\site-packages\tensorflow\pyth
> on\pywrap_tensorflow.py"", line 58, in <module>
>     from tensorflow.python.pywrap_tensorflow_internal import *
>   File ""C:\Anaconda3\envs\imageai\lib\site-packages\tensorflow\pyth
> on\pywrap_tensorflow_internal.py"", line 28, in <module>
>     _pywrap_tensorflow_internal = swig_import_helper()
>   File ""C:\Anaconda3\envs\imageai\lib\site-packages\tensorflow\pyth
> on\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
> ion)
>   File ""C:\Anaconda3\envs\imageai\lib\imp.py"", line 242, in load_mo
> dule
>     return load_dynamic(name, filename, file)
>   File ""C:\Anaconda3\envs\imageai\lib\imp.py"", line 342, in load_dy
> namic
>     return _load(spec)
> ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""C:\Documents\ImageAI\CustomDetectionTraining.py"", line 1, i
> n <module>
>     from imageai.Detection.Custom import DetectionModelTrainer
>   File ""C:\Anaconda3\envs\imageai\lib\site-packages\imageai\Detecti
> on\__init__.py"", line 2, in <module>
>     from imageai.Detection.keras_retinanet.models.resnet import resnet50_retinan
> et
>   File ""C:\Anaconda3\envs\imageai\lib\site-packages\imageai\Detecti
> on\keras_retinanet\models\resnet.py"", line 19, in <module>
>     import keras
>   File ""C:\Anaconda3\envs\imageai\lib\site-packages\keras\__init__.
> py"", line 3, in <module>
>     from . import utils
>   File ""C:\Anaconda3\envs\imageai\lib\site-packages\keras\utils\__i
> nit__.py"", line 6, in <module>
>     from . import conv_utils
>   File ""C:\Anaconda3\envs\imageai\lib\site-packages\keras\utils\con
> v_utils.py"", line 9, in <module>
>     from .. import backend as K
>   File ""C:\Anaconda3\envs\imageai\lib\site-packages\keras\backend\_
> _init__.py"", line 1, in <module>
>     from .load_backend import epsilon
>   File ""C:\Anaconda3\envs\imageai\lib\site-packages\keras\backend\l
> oad_backend.py"", line 90, in <module>
>     from .tensorflow_backend import *
>   File ""C:\Anaconda3\envs\imageai\lib\site-packages\keras\backend\t
> ensorflow_backend.py"", line 5, in <module>
>     import tensorflow as tf
>   File ""C:\Anaconda3\envs\imageai\lib\site-packages\tensorflow\__in
> it__.py"", line 24, in <module>
>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im
> port
>   File ""C:\Anaconda3\envs\imageai\lib\site-packages\tensorflow\pyth
> on\__init__.py"", line 49, in <module>
>     from tensorflow.python import pywrap_tensorflow
>   File ""C:\Anaconda3\envs\imageai\lib\site-packages\tensorflow\pyth
> on\pywrap_tensorflow.py"", line 74, in <module>
>     raise ImportError(msg)
> ImportError: Traceback (most recent call last):
>   File ""C:\Anaconda3\envs\imageai\lib\site-packages\tensorflow\pyth
> on\pywrap_tensorflow.py"", line 58, in <module>
>     from tensorflow.python.pywrap_tensorflow_internal import *
>   File ""C:\Anaconda3\envs\imageai\lib\site-packages\tensorflow\pyth
> on\pywrap_tensorflow_internal.py"", line 28, in <module>
>     _pywrap_tensorflow_internal = swig_import_helper()
>   File ""C:\Anaconda3\envs\imageai\lib\site-packages\tensorflow\pyth
> on\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
> ion)
>   File ""C:\Anaconda3\envs\imageai\lib\imp.py"", line 242, in load_mo
> dule
>     return load_dynamic(name, filename, file)
>   File ""C:\Anaconda3\envs\imageai\lib\imp.py"", line 342, in load_dy
> namic
>     return _load(spec)
> ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.
> 
> 
> Failed to load the native TensorFlow runtime.
> 
> See https://www.tensorflow.org/install/errors
> 
> for some common reasons and solutions.  Include the entire stack trace
> above this error message when asking for help.
> "
35192,Automatic Mixed Precision and XLA not working with `model.fit_generator`,"### System information
- **OS Platform and Distribution **: `tensorflow/tensorflow:nightly-gpu-py3` Nvidia Docker image (`ec33d38d1b43`)
- **TensorFlow version (use command below)**: 2.1.0-dev20191106
- **Python version**: Python 3.6.8
- **CUDA/cuDNN version**: 10.0
- **GPU model and memory**: Titan RTX 24GB

### Describe the problem
When using `model.fit_generator()`, Automatic Mixed Precision (AMP) and compiling XLA doesn't seem to work. However, it all works fine with `model.fit()`.

Please see below for a complete code sample to reproduce this.

### Source code / logs

```
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import Sequence
import numpy as np
import random


def mixed_precision_test(use_generator):
    # use XLA
    tf.config.optimizer.set_jit(True)

    input_shape = (100, 100, 100)
    n_samples = 1000

    # build the model
    model = Sequential()
    model.add(Dense(16, input_shape=input_shape, activation='relu'))
    model.add(Flatten())
    model.add(Dense(8, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))

    optimiser = Adam(lr=0.001)
     
    # use AMP
    optimiser = tf.train.experimental.enable_mixed_precision_graph_rewrite(
        optimiser)

    model.compile(optimizer=optimiser,
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

    if use_generator:
        class DataGen(Sequence):

            def __len__(self):
                return n_samples

            def __getitem__(self, index):
                _x = np.random.rand(1, *input_shape)
                _x = np.array(_x, dtype='uint8')
                _y = np.array([random.choice([0, 1])], dtype='uint8')
                return _x, _y

        model.fit_generator(generator=DataGen())
    else:
        x = np.random.rand(n_samples, *input_shape)
        x = np.array(x, dtype='uint8')
        y = np.array([random.choice([0, 1]) for _ in range(len(x))],
                     dtype='uint8')

        model.fit(x, y)


if __name__ == '__main__':
    mixed_precision_test(use_generator=False)
    mixed_precision_test(use_generator=True)
```

When setting `use_generator=False`, TensorFlow prints out the following logs:

```
2019-12-17 15:51:43.549128: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1857] Converted 26/409 nodes to float16 precision using 2 cast(s) to float16 (excluding Const and Variable casts)
2019-12-17 15:51:43.755273: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-12-17 15:51:45.048967: I tensorflow/compiler/jit/xla_compilation_cache.cc:242] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
```

indicating that AMP and XLA are working as intended.

When setting `use_generator=True`, those logs are not present and GPU memory consumption is higher, suggesting that no casting to FP16 was performed.
"
35190,"Conv2D accepts strings as 'filters' parameter, but fails to handle them correctly","**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux, Linux-5.4.2-1-MANJARO-x86_64-with-arch-Manjaro-Linux
- TensorFlow installed from (source or binary): `tensorflow` binary using pip 
- TensorFlow version: 2.0.0
- Python version: 3.7.5
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the current behavior**

I create a Conv2D layer with string `""12""` as parameter for `filters`.
Creation of the object does not fail. However trying to create a tensor out of
this object results in the following exception:
```
Traceback (most recent call last):
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3291, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-26-60e14eee1394>"", line 1, in <module>
    a(b)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 817, in __call__
    self._maybe_build(inputs)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 2141, in _maybe_build
    self.build(input_shapes)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py"", line 165, in build
    dtype=self.dtype)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 522, in add_weight
    aggregation=aggregation)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py"", line 744, in _add_variable_with_custom_getter
    **kwargs_for_getter)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py"", line 139, in make_variable
    shape=variable_shape if variable_shape else None)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py"", line 258, in __call__
    return cls._variable_v1_call(*args, **kwargs)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py"", line 219, in _variable_v1_call
    shape=shape)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py"", line 197, in <lambda>
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py"", line 2507, in default_variable_creator
    shape=shape)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py"", line 262, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py"", line 1406, in __init__
    distribute_strategy=distribute_strategy)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py"", line 1537, in _init_from_args
    initial_value() if init_from_fn else initial_value,
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py"", line 119, in <lambda>
    init_val = lambda: initializer(shape, dtype=dtype)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops_v2.py"", line 421, in __call__
    fan_in, fan_out = _compute_fans(scale_shape)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops_v2.py"", line 749, in _compute_fans
    fan_out = shape[-1] * receptive_field_size
TypeError: can't multiply sequence by non-int of type 'float'
```

You can also create a Conv2D object with other strings like `""test""` and it will not fail either.
Using ""`test`"" yields:
```
Traceback (most recent call last):
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3291, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-35-60e14eee1394>"", line 1, in <module>
    a(b)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 817, in __call__
    self._maybe_build(inputs)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 2141, in _maybe_build
    self.build(input_shapes)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py"", line 165, in build
    dtype=self.dtype)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 522, in add_weight
    aggregation=aggregation)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py"", line 744, in _add_variable_with_custom_getter
    **kwargs_for_getter)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py"", line 126, in make_variable
    variable_shape = tensor_shape.TensorShape(shape)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py"", line 776, in __init__
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py"", line 776, in <listcomp>
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py"", line 718, in as_dimension
    return Dimension(value)
  File ""/home/baka/.virtualenvs/cplace-machine-learning-backend/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py"", line 193, in __init__
    self._value = int(value)
ValueError: invalid literal for int() with base 10: 'test'
```

**Describe the expected behavior**

I expect that creation of the Conv2D object fails with a ValueError when used with a string or that it tries to convert the string into int.
If the string cannot be converted to int, an exception should be raised at the creation time of the layer, not when building the tensor. Right now, this is different behavior to parameters like `strides` which fails in the stage of building the layer object when presented with an not int-convertible string.

**Code to reproduce the issue**
```
import tensorflow.python.keras as k
a = k.layers.Conv2D(filters=""12"", kernel_size=3)
b = k.layers.Input((100, 100, 3))
a(b)
```
and
```
import tensorflow.python.keras as k
a = k.layers.Conv2D(filters=""test"", kernel_size=3)
b = k.layers.Input((100, 100, 3))
a(b)
```"
35187,How to solve RuntimeError in Tensorflow 2.0?,"I want to run a TF1.X programm in TF2.0 on a GPU (RTX 2080 TI) in Anaconda, so I changed the code a bit, but I'll get an error in the last line below.

```
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
train_dataset = train_dataset.apply(tf.data.experimental.shuffle_and_repeat(10000, seed=0))
train_dataset = train_dataset.batch(self.batchsize)
train_dataset = train_dataset.apply(tf.data.experimental.prefetch_to_device(device))
```

The error message is 

```
Traceback (most recent call last):
  File ""train.py"", line 77, in <module>
    reward = manager.get_rewards(ModelGenerator, state_space.parse_state_space_list(action))
  File ""C:\Users\user\tf\tf2.0_gpu_test\manager.py"", line 83, in get_rewards
    train_dataset = train_dataset.apply(tf.data.experimental.prefetch_to_device(device))
  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tf_env\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py"", line 1369, in apply
    dataset = transformation_func(self)
  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tf_env\lib\site-packages\tensorflow_core\python\data\experimental\ops\prefetching_ops.py"", line 54, in _apply_fn
    copy_to_device(target_device=device)).prefetch(buffer_size)
  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tf_env\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py"", line 1369, in apply
    dataset = transformation_func(self)
  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tf_env\lib\site-packages\tensorflow_core\python\data\experimental\ops\prefetching_ops.py"", line 78, in _apply_fn
    source_device=source_device).with_options(options)
  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tf_env\lib\site-packages\tensorflow_core\python\data\experimental\ops\prefetching_ops.py"", line 102, in __init__
    self._source_device = ops.convert_to_tensor(source_device)
  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tf_env\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1184, in convert_to_tensor
    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tf_env\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1242, in convert_to_tensor_v2
    as_ref=False)
  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tf_env\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1296, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tf_env\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 286, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tf_env\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 227, in constant
    allow_broadcast=True)
  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tf_env\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 235, in _constant_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tf_env\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 96, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
RuntimeError: Can't copy Tensor with type string to device /job:localhost/replica:0/task:0/device:GPU:0.
```

It would be awesome if someone could help me."
35185,Segmentation fault if importing TF2.1 after sklearn,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10 (buster)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): via `pip install tensorflow==2.1.0-rc1`
- TensorFlow version (use command below): v2.1.0-rc0-47-g064e153 2.1.0-rc1
- Python version: 3.7.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: None
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
TF crashes.

**Describe the expected behavior**
TF doesn't crash.

**Code to reproduce the issue**
Tested in env created with `docker run -it --rm python:3.7.5-slim bash`
```
# pip install tensorflow==2.1.0-rc1 scikit-learn==0.21.3
# python -c ""import sklearn; import tensorflow""
Segmentation fault
```

**Other info / logs**
tensorflow==2.1.0-rc0, tensorflow-cpu==2.1.0-rc1, tf-nightly==2.1.0.dev20191217 expose the issue.
tensorflow==2.0.0 works as expected
`python -c ""import tensorflow; import sklearn""` works as expected (changed import order)

Backtrace:
```
# gdb python
GNU gdb (Debian 8.2.1-2+b3) 8.2.1
Copyright (C) 2018 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
Type ""show copying"" and ""show warranty"" for details.
This GDB was configured as ""x86_64-linux-gnu"".
Type ""show configuration"" for configuration details.
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>.
Find the GDB manual and other documentation resources online at:
    <http://www.gnu.org/software/gdb/documentation/>.

For help, type ""help"".
Type ""apropos word"" to search for commands related to ""word""...
Reading symbols from python...done.
(gdb) r
Starting program: /usr/local/bin/python 
warning: Error disabling address space randomization: Operation not permitted
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
Python 3.7.5 (default, Oct 19 2019, 00:13:43) 
[GCC 8.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import sklearn
[New Thread 0x7f7f5ab88700 (LWP 1983)]
[New Thread 0x7f7f5a387700 (LWP 1984)]
[New Thread 0x7f7f55b86700 (LWP 1985)]
>>> import tensorflow

Thread 1 ""python"" received signal SIGSEGV, Segmentation fault.
0x0000557e44ea5fe0 in ?? ()
(gdb) bt
#0  0x0000557e44ea5fe0 in ?? ()
#1  0x00007f7f07b3ac41 in pybind11::detail::make_new_python_type(pybind11::detail::type_record const&) () from /usr/local/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_events_writer.so
#2  0x00007f7f07b411b9 in pybind11::detail::generic_type::initialize(pybind11::detail::type_record const&) () from /usr/local/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_events_writer.so
#3  0x00007f7f07b41a72 in PyInit__pywrap_events_writer () from /usr/local/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_events_writer.so
#4  0x00007f7f5e467e32 in _PyImport_LoadDynamicModuleWithSpec (spec=spec@entry=0x7f7f524079d0, fp=fp@entry=0x0) at ./Python/importdl.c:159
#5  0x00007f7f5e467bb9 in _imp_create_dynamic_impl (module=<optimized out>, file=<optimized out>, spec=0x7f7f524079d0) at Python/import.c:2170
#6  _imp_create_dynamic (module=<optimized out>, args=<optimized out>, nargs=<optimized out>) at Python/clinic/import.c.h:289
#7  0x00007f7f5e377fe5 in _PyMethodDef_RawFastCallDict (method=0x7f7f5e567840 <imp_methods+320>, self=<optimized out>, args=<optimized out>, nargs=1, kwargs=<optimized out>) at Objects/call.c:530
#8  0x00007f7f5e377cd0 in _PyCFunction_FastCallDict (func=0x7f7f5dcb27d0, args=<optimized out>, nargs=<optimized out>, kwargs=<optimized out>) at Objects/call.c:586
#9  0x00007f7f5e3ed48b in do_call_core (kwdict=0x7f7f523e4960, callargs=0x7f7f523f1090, func=0x7f7f5dcb27d0) at Python/ceval.c:4641
#10 _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3191
#11 0x00007f7f5e3e76c1 in _PyEval_EvalCodeWithName (_co=<optimized out>, globals=<optimized out>, locals=locals@entry=0x0, args=args@entry=0x7f7f5d2391e0, argcount=argcount@entry=2, kwnames=0x0, 
    kwargs=0x7f7f5d2391f0, kwcount=<optimized out>, kwstep=1, defs=0x0, defcount=0, kwdefs=0x0, closure=0x0, name=0x7f7f5dc9f300, qualname=0x7f7f5dc9f300) at Python/ceval.c:3930
#12 0x00007f7f5e378942 in _PyFunction_FastCallKeywords (func=<optimized out>, stack=0x7f7f5d2391e0, nargs=2, kwnames=<optimized out>) at Objects/call.c:433
#13 0x00007f7f5e3ec222 in call_function (kwnames=0x0, oparg=<optimized out>, pp_stack=<synthetic pointer>) at Python/ceval.c:4616
```"
35184,"TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.","from tensorflow.keras.applications.inception_v3 import InceptionV3
model = InceptionV3(weights='imagenet')
model_new = Model(model.input, model.layers[-2].output)

**after running the above line i got the below error:**

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-50-8b5bbb7c43a4> in <module>
      1 # Create a new model, by removing the last layer (output layer) from the inception v3
----> 2 model_new = Model(model.input, model.layers[-2].output)

~\Anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name + '` call to the ' +
     90                               'Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\Anaconda3\lib\site-packages\keras\engine\network.py in __init__(self, *args, **kwargs)
     91                 'inputs' in kwargs and 'outputs' in kwargs):
     92             # Graph network
---> 93             self._init_graph_network(*args, **kwargs)
     94         else:
     95             # Subclassed network

~\Anaconda3\lib\site-packages\keras\engine\network.py in _init_graph_network(self, inputs, outputs, name)
    145         # User-provided argument validation.
    146         # Check for redundancy in inputs.
--> 147         if len(set(self.inputs)) != len(self.inputs):
    148             raise ValueError('The list of inputs passed to the model '
    149                              'is redundant. '

~\Anaconda3\lib\site-packages\tensorflow_core\python\framework\ops.py in __hash__(self)
    711     if (Tensor._USE_EQUALITY and executing_eagerly_outside_functions() and
    712         (g is None or g._building_function)):  # pylint: disable=protected-access
--> 713       raise TypeError(""Tensor is unhashable if Tensor equality is enabled. ""
    714                       ""Instead, use tensor.experimental_ref() as the key."")
    715     else:

TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.

"
35183,tensorflow GPU-util on V100,"I trained a BERT model use 4 V100 GPU on one machine, and I noticed that:
A: batch_size = 60, Volatile GPU-util almost 80%~90%(nvidia-smi), global_step/sec=3.57(tf estimator log) 
B: batch_size = 80, Volatile GPU-util almost 80%~90%(nvidia-smi), global_step/sec=3.21(tf estimator log)
C: batch_size =10, Volatile GPU-util almost 60%~70%(nvidia-smi), global_step/sec=5.2(tf estimator log)

So, i was a little confused about these result. I suppose that throughput of A is 60*3.57=214, B is 80*3.21=256, C is 10*5.2=52. But Volatile GPU-util almost ""same"", which i mean 80%-90%, 80%-90%, 60%-70%. 

To be more specific, for experiment A, i suppose that 80%~90% GPU core was used, which is same  Volatile GPU-util when increase batch_size to 80(experiment B), so why Volatile GPU-util of A and B is almost same? Even reduce batch_size to 10, Volatile GPU-util even 60%~70%.

"
35181,DropoutWrapper and Exploding gradient behaviour for Recurrent Neural Network,"Dear all,
 I have a point about DropoutWrapper and its use with Recurrent Neural Networks.

Due to the possibility that the dropout can be applied to the state or the output (state_keep_prob and output_keep_prob), I found that, during the recurrent process, the state propagated through the time can take values not bounded in the interval [-1, 1]. This is probably due to the way in which the dropout is implemented (at training time with a scaling instead of testing time with expectation). Since the dropout is applied after the activation (i.e. tanh), the feature values will range between -inf and +inf. This point is a bit strange for me since the current implementation can induce exploding gradient issues in the GRU/LSTM process while such cells were introduced to deal with vanishing as well as exploding gradients.

Please, could you supply me some feedback about my issue since, practically, it can impact people that commonly employ such Wrapper that induces behaviours that are divergent w.r.t. the theoretical behaviour of RNN (GRU/LSTM).

All the best
"
35180,Windows build failed - Internal compiler error Visual Studio 2017 - FAILED: Build did NOT complete successfully,"**System information**
- OS Platform and Distribution: Windows 7 Professional SP1 64 bit
- TensorFlow installed from (source or binary): source
- TensorFlow version: tensorflow-master (downloaded 18/12/2019)
- Python version: 3.7
- Installed using virtualenv? pip? conda?: YES, conda
- Bazel version (if compiling from source): 1.1.0
- GCC/Compiler version (if compiling from source): MSVC 14.16.27023 (Visual Studio 2017)
- CUDA/cuDNN version: build for CPU
- GPU model and memory: (Quadro K200M)

**Describe the problem**
Followed this guide: https://www.tensorflow.org/install/source_windows and just build failed.
I tested different combinations (https://www.tensorflow.org/install/source_windows#cpu) and I was just able to compile just the **r1.14 successfully**, by the way.
**r2.0** failed with the same message as master.
But back to master release.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

- Donwloaded the `tensorflow-master.zip` from github
- Installed Bazel version 1.1.0
- cmd to folder `\tensorflow-master`
- `activate python37 environment` (conda)
- Set env var for bazel: `set BAZEL_VC=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC`
- Check Bazel version
```
(python37) C:\Users\username\bin\tensorflow-master>bazel --version
bazel 1.1.0
```
- Configure
```
(python37) C:\Users\ username\bin\tensorflow-master>python ./configure.py
You have bazel 1.1.0 installed.
Please specify the location of python. [Default is C:\Users\ username\Anaconda3\envs\python37\python.exe]:

Found possible Python library paths:
  C:\Users\ username\Anaconda3\envs\python37\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\ username\Anaconda3\envs\python37\lib\site-packages]

Do you wish to build TensorFlow with XLA JIT support? [y/N]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:

Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y
Eigen strong inline overridden.
```
- Launch the compilation: `bazel build //tensorflow:tensorflow_cc.lib`
- After time elapsed the process ended with: `FAILED: Build did NOT complete successfully`

**Any other info / logs**
Just attaching the last rows of the terminal messages

Message has Italian chunks of which I'm trying to provide a translation here:
```
note: vedere il riferimento all'istanza '<Sconosciuto>' della funzione <Sconosciuto> di cui è in corso la compilazione
note: check the reference to instance '<Unknown>' of the function <Unknown> which is compiling
```

```
Errore interno del compilatore in C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\bin\HostX64\x64\cl.exe. Verrà richiesto di inviare una segnalazione errori a Microsoft in un momento successivo.
Internal error of the compiler in C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\bin\HostX64\x64\cl.exe. It will be required to send an error report to Microsoft next.
```

```
ERRORE INTERNO DEL COMPILATORE in 'C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\bin\HostX64\x64\cl.exe' Per altre informazioni, scegliere Supporto tecnico dal menu ? di Visual C++ o aprire il file della Guida relativo al supporto tecnico
INTERNAL ERROR OF THE COMPILER in 'C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\bin\HostX64\x64\cl.exe' For more information, choose Technical support from ? menu of Visual C++ or open the file from the guide related with technical support
```

[tensorflow-master-build-fail-log.txt](https://github.com/tensorflow/tensorflow/files/3971998/tensorflow-master-build-fail-log.txt)

"
35179,Upgrading Boringssl Version in TensorFlow,"I am trying to build Tensorflow using latest Boringssl. However, facing compilation issues.
Is there any plan to upgrade the Boringssl commit currently being used in TensorFlow to a recent commit?"
35178,missing symbols from c++ API,"**System information**
- OS Platform: Windows10
- TensorFlow installed from:source
- TensorFlow version:1.13.0
- Python version: NO
- Bazel version (if compiling from source):0.19
- GCC/Compiler version: msvc14



**Describe the problem**
I want to build tensorflow from the source using the below command 
`bazel build -c opt  --config=mkl --config=monolithic --linkopt=""/FORCE:MULTIPLE"" --define=no_tensorflow_py_deps=true //tensorflow:libtensorflow_cc.so //tensorflow:install_headers   //tensorflow/tools/lib_package:libtensorflow --verbose_failures`

However after the dll file generated, some symbol is not inside of the dll, I will paste some of this error in here(As far as I know this problem is accured because number of symbol is limited in windows, So all functions in tensorflow source can't be exported( I _check the exported function using dumpbin.exe /EXPORT and it's weird because only 3000 symbols is exported )_
`tfwrapper.obj : error LNK2001: unresolved external symbol ""class tensorflow::Output __cdecl tensorflow::ops::Const(class tensorflow::Scope const &,struct tensorflow::Input::Initializer const &)"" (?Const@ops@tensorflow@@YA?AVOutput@2@AEBVScope@2@AEBUInitializer@Input@2@@Z)`


`tfwrapper.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::ops::DecodePng::DecodePng(class tensorflow::Scope const &,class tensorflow::Input,struct tensorflow::ops::DecodePng::Attrs const &)"" (??0DecodePng@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@AEBUAttrs@012@@Z)`


`tfwrapper.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::ops::Placeholder::Placeholder(class tensorflow::Scope const &,enum tensorflow::DataType)"" (??0Placeholder@ops@tensorflow@@QEAA@AEBVScope@2@W4DataType@2@@Z)`

and for solving this issue I work on two idea
 **first one** is using `def_file_filter.py.tpl`, as may you know in this file I have to add missed symbol to it like below(I just paste some of them in here) and reconfigure & recompile the source(I guess). But this solution can't export any of this missing symbol for me

     # Header for the def file.
     def_fp.write(""LIBRARY "" + args.target + ""\n"")
    def_fp.write(""EXPORTS\n"")
    def_fp.write(""\t ??1OpDef@tensorflow@@UEAA@XZ\n"")
	def_fp.write(""\t ??0DecodePng@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@AEBUAttrs@012@@Z\n"")
	def_fp.write(""\t ??6tensorflow@@YAAEAV?$basic_ostream@DU?$char_traits@D@std@@@std@@AEAV12@AEBVStatus@0@@Z\n"")`


So, I can't solve my problem using this method. So I use another Idea that I got this Idea from `ashley tharp` (you can see her great work in this[ link](https://github.com/robosina/stuff/tree/master/ai/tensorflow/build_tensorflow_1.14_source_for_Windows)) and this workaround solve 8 of this problems. Main idea of her is to add `TF_EXPORT` macro to the function names to force symbol to include in the dll file. but my problem is raised in the DecodePng file, there is a function in tensorflow source ->`tensorflow/core/kernels/decode_image_op.cc` the function signature is 
     
     void DecodePng(OpKernelContext* context, StringPiece input)
 I can't find any other. But in examples of the tensorflow we will see this way to read png files

    image_reader = DecodePng(root.WithOpName(""png_reader""), file_reader,
                             DecodePng::Channels(wanted_channels));
this two function is not same and I couldn't find this function in the tensorflow source code so I can't add `TF_EXPORT` macro to it, but how it is possible to use a function that is not present in source code?? So I look in the bazel genereated files and this function is in there.(it is in `bazel-source/bazel-out/x64_windows-opt/genfiles/tensorflow/cc/ops/image_ops.h`)

     DecodePng(const ::tensorflow::Scope& scope, ::tensorflow::Input contents);
     DecodePng(const ::tensorflow::Scope& scope, ::tensorflow::Input contents, const
          DecodePng::Attrs& attrs);

 I don't know exactly how bazel created it and how to add `TF_EXPORT` macro to it(because this files is machine generated files and you can't add TF_EXPORT to it, because it will be overwritten in compile time)"
35177,tensorflow c++ Not found: Op type not registered 'ImageProjectiveTransformV2',"I run my c++ code, it load file .pb have op tf.contrib.image.transform() and out error:
```
Not found: Op type not registered 'ImageProjectiveTransformV2' in binary running on mendel. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.
```

I use tensorflow 1.14 build from source on ubuntu16.04.
How to build to use op ImageProjectiveTransformV2 ?"
35176,How to prevent operator from fusing when converting to TFLite model? ,"GPU delegate does not support operator fusion, so I need to prevent the conversion from fusing the operators.

TFLiteConvertor will automatically fuse operators, which gives problems."
35173,"I do not get the from the file or definition of ""gen_nn_ops"" in the place ""tensorflow.python.ops""","Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
35172,TypeError when using boolean_mask to evaluate hessian matrix,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes (see code snippet below)
- OS Platform and Distribution: both on win10 and Linux Ubuntu 18.04
- TensorFlow installed from: binary (via `pip install tensorflow`)
- TensorFlow version (use command below): `2.0.0`
- Python version: `3.7.3`
- CUDA/cuDNN version: not applicable
- GPU model and memory: not applicable

**minimum code to reproduce the issue**

```Python
import numpy as np
import tensorflow as tf
hfe = lambda x,y: np.max(np.abs(x-y)/(np.abs(x)+np.abs(y)+1e-3))
N0 = 3
np0 = np.random.randn(N0)
np2 = np.sum(np0**3)
hessian_np0 = np.diag(6*np0) #the correct hessian via numpy
tf0 = tf.constant(np0)
with tf.GradientTape() as tape0:
    tape0.watch(tf0)
    with tf.GradientTape() as tape1:
        tape1.watch(tf0)
        # tf1 = tf0 # @label-0 pass 
        # tf1 = tf.gather(tf0*1, tf.range(N0)) # @label-1 pass
        tf1 = tf.boolean_mask(tf0, tf.ones(N0, dtype=tf.bool)) # @label-2 fail TypeError
        tf2 = tf.math.reduce_sum(tf1**3)
    grad_tf0 = tape1.gradient(tf2, tf0)
hessian_tf0 = tape0.jacobian(grad_tf0, tf0) #the hessian via tensorflow
print('relative error:: np vs tf', hfe(hessian_np0, hessian_tf0.numpy()))
```

**current behavior**: both `@label-0 @label-1 @label-2` (see comments in the code snippet) could give the correct `tf1` (just identical with `tf0`), but only `@label-0 @label-1` could give the correct hessian matrix as `hessian_np0` (second order derivative), `@label-2` will raise `TypeError` (see attached [traceback.log](https://github.com/tensorflow/tensorflow/files/3971266/traceback.log))

**expected behavior**: `@label-0 @label-1 @label-2` should give the same hessian matrix"
35171,rnn optimize fail,"Epoch 1/10
2019-12-17 10:30:10.047658: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_gru_5820_6196_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_6314' and '__inference___backward_standard_gru_5820_6196' both implement 'gru_95095eff-9898-4b33-9f85-d96fe50ea8a6' but their signatures do not match.

what's wrong?"
35170,tensorflow-lite-gpu.aar built from source is much slower than downloaded prebuilt library,"### **System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- Mobile device: LG G7
- TensorFlow installed from (source or binary): Source
- TensorFlow version: r2.0
- Python version: 3.5
- Bazel version: 0.24.1
- Android NDK version: r17c
- GCC/Compiler version (if compiling from source): GCC 4.8



### **Describe the problem**
Built Tensorflow lite from source, but the inference is about 3 times slower than original.
(What I mean by 'original' is the prebuilt tensorflow-lite downloaded from Maven repository.)
For example, the model which originally costs 20 ms on GPU takes 50 ms.
I tried different versions of Tensorflow and Android NDK but resulted all same.

Below is the process I've done to build tensorflow lite libraries.


**1. Download Tensorflow source code from github**
```shell
$ git clone https://github.com/tensorflow/tensorflow.git
$ git checkout r2.0
```

**2. Download Android NDK and install standalone toolchains**
Download NDK r17c from [here](https://developer.android.com/ndk/downloads/older_releases.html).
Extract files and move to Android Sdk directory.
```shell
$ cd Android/Sdk/android-ndk-r17c
$ python build/tools/make_standalone_toolchain.py --arch arm64 --api 21
$ export ANDROID_NDK=/path/to/ndk
$ export ANDROID_NDK_HOME=$ANDROID_NDK
```

**3. Download and install Bazel**
Download installer script from [here](https://github.com/bazelbuild/bazel/releases/0.24.1).
```shell
$ cd /path/to/download
$ chmod +x bazel-0.24.1-installer-linux-x86_64.sh
$ ./bazel-0.24.1-installer-linux-x86_64.sh --user
```

**4. Build tensorflow-lite.aar and tensorflow-lite-gpu.aar**
```shell
$ cd tensorflow
$ ./configure
$ bazel build --cxxopt='--std=c++11' -c opt --fat_apk_cpu=arm64-v8a,armeabi-v7a tensorflow/lite/java:tensorflow-lite
$ bazel build --cxxopt='--std=c++11' -c opt --fat_apk_cpu=arm64-v8a,armeabi-v7a tensorflow/lite/java:tensorflow-lite-gpu
```
On `./configure`, set all 'n' for support question and leave others on default.

| Configuration | Value |
| --- | --- |
| XLA JIT support | No |
| OpenCL SYCL support | No |
| ROCm support | No |
| CUDA support | No |
| Fresh released clang | No |
| MPI support | No |
| Bazel comilation option | Default |
| Android ./WORKSPACE configuration | Yes |
| Android NDK API level | 22 |
| Android SDK path | Default |
| Android SDK API level | Default (29) |
| Android build tools version | Default (29.0.2) |

**5. Import libraries to Android project**
Copy the generated `.aar` files into Android project, and change `build.gradle` file:

```gradle
dependencies {
    ...
    implementation files('libs/tensorflow-lite.aar')
    implementation files('libs/tensorflow-lite-gpu.aar')

    // implementation 'org.tensorflow:tensorflow-lite:2.0.0'
    // implementation 'org.tensorflow:tensorflow-lite-gpu:2.0.0'
}
```



Any suggestion or help will be appreciated.
Thanks in advance."
35169,TensorFlow Lite C++ image classification demo result error,"I follow the guidelines of TensorFlow Lite C++ image classification demo. Firstly I clone TensorFlow and use to build label_image for Android armv8 by bazel 1.2.1.

> bazel build -c opt --cxxopt=-std=c++11 --config=android_arm64 \
>   //tensorflow/lite/examples/label_image:label_image


When I run the sample in the guide. I got the error result like below:

> Loaded model ./mobilenet_v1_1.0_224.tflite
> resolved reporter
> INFO: Initialized TensorFlow Lite runtime.
> invoked
> average time: 52.036 ms
> -nan: 1000 1000:toilet tissue, toilet paper, bathroom tissue
> -nan: 999 999:ear, spike, capitulum
> -nan: 998 998:bolete
> -nan: 997 997:hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa
> -nan: 996 996:earthstar

According to the guide, the right result is below:

> Loaded model /tmp/mobilenet_v1_1.0_224.tflite
> resolved reporter
> invoked
> average time: 68.12 ms
> 0.860174: 653 653:military uniform
> 0.0481017: 907 907:Windsor tie
> 0.00786704: 466 466:bulletproof vest
> 0.00644932: 514 514:cornet, horn, trumpet, trump
> 0.00608029: 543 543:drumstick

BTW, The TensorFlow version is the latest commit of the master.
`
commit 27e6c7b49f4558dfc4bd59a9c492bf4f390a77da
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Mon Dec 16 01:03:06 2019 -0800

    compat: Update forward compatibility horizon to 2019-12-16

    PiperOrigin-RevId: 285720630
    Change-Id: Ib744d5f7de70a6c6d73dd1a386712e404d0c2b99
`
"
35168,Provided LSTM example doesn't work.,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): `pip install tensorflow`
- Tensorflow version (commit SHA if source): 1.15.0

**Describe the problem**
The example provided [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/examples/lstm/g3doc) doesn't work.
**Please provide the exact sequence of commands/steps when you ran into the problem**
If I put together the example, it gives this : 
```
# Note this needs to happen before import tensorflow.
import os
os.environ['TF_ENABLE_CONTROL_FLOW_V2'] = '1'
import sys
from absl import app
import argparse
import tensorflow as tf


class MnistLstmModel(object):
  """"""Build a simple LSTM based MNIST model.

  Attributes:
    time_steps: The maximum length of the time_steps, but since we're just using
      the 'width' dimension as time_steps, it's actually a fixed number.
    input_size: The LSTM layer input size.
    num_lstm_layer: Number of LSTM layers for the stacked LSTM cell case.
    num_lstm_units: Number of units in the LSTM cell.
    units: The units for the last layer.
    num_class: Number of classes to predict.
  """"""

  def __init__(self, time_steps, input_size, num_lstm_layer, num_lstm_units,
               units, num_class):
    self.time_steps = time_steps
    self.input_size = input_size
    self.num_lstm_layer = num_lstm_layer
    self.num_lstm_units = num_lstm_units
    self.units = units
    self.num_class = num_class

  def build_model(self):
    """"""Build the model using the given configs.

    Returns:
      x: The input placehoder tensor.
      logits: The logits of the output.
      output_class: The prediction.
    """"""
    x = tf.placeholder(
        'float32', [None, self.time_steps, self.input_size], name='INPUT')
    lstm_layers = []
    for _ in range(self.num_lstm_layer):
      lstm_layers.append(
          # Important:
          #
          # Note here, we use `tf.lite.experimental.nn.TFLiteLSTMCell`
          # (OpHinted LSTMCell).
          tf.lite.experimental.nn.TFLiteLSTMCell(
              self.num_lstm_units, forget_bias=0))
    # Weights and biases for output softmax layer.
    out_weights = tf.Variable(tf.random.normal([self.units, self.num_class]))
    out_bias = tf.Variable(tf.zeros([self.num_class]))

    # Transpose input x to make it time major.
    lstm_inputs = tf.transpose(x, perm=[1, 0, 2])
    lstm_cells = tf.keras.layers.StackedRNNCells(lstm_layers)
    # Important:
    #
    # Note here, we use `tf.lite.experimental.nn.dynamic_rnn` and `time_major`
    # is set to True.
    outputs, _ = tf.lite.experimental.nn.dynamic_rnn(
        lstm_cells, lstm_inputs, dtype='float32', time_major=True)

    # Transpose the outputs back to [batch, time, output]
    outputs = tf.transpose(outputs, perm=[1, 0, 2])
    outputs = tf.unstack(outputs, axis=1)
    logits = tf.matmul(outputs[-1], out_weights) + out_bias
    output_class = tf.nn.softmax(logits, name='OUTPUT_CLASS')

    return x, logits, output_class

def train(model,
          model_dir,
          batch_size=20,
          learning_rate=0.001,
          train_steps=200,
          eval_steps=50,
          save_every_n_steps=100):
  """"""Train & save the MNIST recognition model.""""""
  # Train & test dataset.
  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
  train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
  train_iterator = train_dataset.shuffle(
      buffer_size=1000).batch(batch_size).repeat().make_one_shot_iterator()
  x, logits, output_class = model.build_model()
  test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
  test_iterator = test_dataset.batch(
      batch_size).repeat().make_one_shot_iterator()
  # input label placeholder
  y = tf.placeholder(tf.int32, [
      None,
  ])
  one_hot_labels = tf.one_hot(y, depth=model.num_class)
  # Loss function
  loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(
          logits=logits, labels=one_hot_labels))
  correct = tf.nn.in_top_k(output_class, y, 1)
  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))
  # Optimization
  opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)

  # Initialize variables
  init = tf.global_variables_initializer()
  saver = tf.train.Saver()
  batch_x, batch_y = train_iterator.get_next()
  batch_test_x, batch_test_y = test_iterator.get_next()
  with tf.Session() as sess:
    sess.run([init])
    for i in range(train_steps):
      batch_x_value, batch_y_value = sess.run([batch_x, batch_y])
      _, loss_value = sess.run([opt, loss],
                               feed_dict={
                                   x: batch_x_value,
                                   y: batch_y_value
                               })
      if i % 100 == 0:
        tf.logging.info('Training step %d, loss is %f' % (i, loss_value))
      if i > 0 and i % save_every_n_steps == 0:
        accuracy_sum = 0.0
        for _ in range(eval_steps):
          test_x_value, test_y_value = sess.run([batch_test_x, batch_test_y])
          accuracy_value = sess.run(
              accuracy, feed_dict={
                  x: test_x_value,
                  y: test_y_value
              })
          accuracy_sum += accuracy_value
        tf.logging.info('Training step %d, accuracy is %f' %
                        (i, accuracy_sum / (eval_steps * 1.0)))
        saver.save(sess, model_dir)

def export(model, model_dir, tflite_model_file,
           use_post_training_quantize=True):
  """"""Export trained model to tflite model.""""""
  tf.reset_default_graph()
  x, _, output_class = model.build_model()
  saver = tf.train.Saver()
  sess = tf.Session()
  saver.restore(sess, model_dir)
  # Convert to Tflite model.
  converter = tf.lite.TFLiteConverter.from_session(sess, [x], [output_class])
  converter.post_training_quantize = use_post_training_quantize
  tflite = converter.convert()
  with open(tflite_model_file, 'w') as f:
    f.write(tflite)

def train_and_export(parsed_flags):
  """"""Train the MNIST LSTM model and export to TfLite.""""""
  model = MnistLstmModel(
      time_steps=28,
      input_size=28,
      num_lstm_layer=2,
      num_lstm_units=64,
      units=64,
      num_class=10)
  tf.logging.info('Starts training...')
  train(model, parsed_flags.model_dir)
  tf.logging.info('Finished training, starts exporting to tflite to %s ...' %
                  parsed_flags.tflite_model_file)
  export(model, parsed_flags.model_dir, parsed_flags.tflite_model_file,
         parsed_flags.use_post_training_quantize)
  tf.logging.info(
      'Finished exporting, model is %s' % parsed_flags.tflite_model_file)


def run_main(_):
  """"""Main in the TfLite LSTM tutorial.""""""
  parser = argparse.ArgumentParser(
      description=('Train a MNIST recognition model then export to TfLite.'))
  parser.add_argument(
      '--model_dir',
      type=str,
      help='Directory where the models will store.',
      required=True)
  parser.add_argument(
      '--tflite_model_file',
      type=str,
      help='Full filepath to the exported tflite model file.',
      required=True)
  parser.add_argument(
      '--use_post_training_quantize',
      action='store_true',
      default=True,
      help='Whether or not to use post_training_quantize.')
  parsed_flags, _ = parser.parse_known_args()
  train_and_export(parsed_flags)


def main():
  app.run(main=run_main, argv=sys.argv[:1])

if __name__ == '__main__':
  main()
```

When ran simply like `python example.py --model_dir lstms/ --tflite_model_file lstms/model.tflite`, I get the following error message : 
```
INFO:tensorflow:Starts training...
I1216 23:55:41.555022 140529868711744 doc_example.py:159] Starts training...
INFO:tensorflow:Training step 0, loss is 2.657418
I1216 23:55:45.383375 140529868711744 doc_example.py:120] Training step 0, loss is 2.657418
INFO:tensorflow:Training step 100, loss is 0.867711
I1216 23:55:47.319205 140529868711744 doc_example.py:120] Training step 100, loss is 0.867711
INFO:tensorflow:Training step 100, accuracy is 0.540000
I1216 23:55:47.966933 140529868711744 doc_example.py:132] Training step 100, accuracy is 0.540000
INFO:tensorflow:Finished training, starts exporting to tflite to lstm_doc/model.tflite ...
I1216 23:55:50.603394 140529868711744 doc_example.py:162] Finished training, starts exporting to tflite to lstm_doc/model.tflite ...
INFO:tensorflow:Restoring parameters from lstm_doc/
I1216 23:55:50.832539 140529868711744 saver.py:1284] Restoring parameters from lstm_doc/
2019-12-16 23:55:50.880481: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2019-12-16 23:55:50.880555: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-12-16 23:55:50.900838: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize
2019-12-16 23:55:50.900867: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: Graph size after: 412 nodes (0), 507 edges (0), time = 3.325ms.
2019-12-16 23:55:50.900872: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: Graph size after: 412 nodes (0), 507 edges (0), time = 5.794ms.
2019-12-16 23:55:50.900875: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: hey_rnn_while_body_8743
2019-12-16 23:55:50.900878: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.002ms.
2019-12-16 23:55:50.900882: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0ms.
2019-12-16 23:55:50.900884: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: hey_rnn_while_cond_8742
2019-12-16 23:55:50.900888: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0ms.
2019-12-16 23:55:50.900891: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0ms.
INFO:tensorflow:Froze 26 variables.
I1216 23:55:50.942811 140529868711744 graph_util_impl.py:334] Froze 26 variables.
INFO:tensorflow:Converted 26 variables to const ops.
I1216 23:55:50.946878 140529868711744 graph_util_impl.py:394] Converted 26 variables to const ops.
/home/mparient/.virtualenvs/decibel/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py:846: UserWarning: Property post_training_quantize is deprecated, please use optimizations=[Optimize.DEFAULT] instead.
  "" instead."" % name)
---------------------------------------------------------------------------
ConverterError                            Traceback (most recent call last)
~/code_perso/decibel-light/prod/doc_example.py in <module>
    195 
    196 if __name__ == '__main__':
--> 197   main()

~/code_perso/decibel-light/prod/doc_example.py in main()
    191 
    192 def main():
--> 193   app.run(main=run_main, argv=sys.argv[:1])
    194 
    195 

~/.virtualenvs/decibel/lib/python3.6/site-packages/absl/app.py in run(main, argv, flags_parser)
    297       callback()
    298     try:
--> 299       _run_main(main, args)
    300     except UsageError as error:
    301       usage(shorthelp=True, detailed_error=error, exitcode=error.exitcode)

~/.virtualenvs/decibel/lib/python3.6/site-packages/absl/app.py in _run_main(main, argv)
    248     sys.exit(retval)
    249   else:
--> 250     sys.exit(main(argv))
    251 
    252 

~/code_perso/decibel-light/prod/doc_example.py in run_main(_)
    187       help='Whether or not to use post_training_quantize.')
    188   parsed_flags, _ = parser.parse_known_args()
--> 189   train_and_export(parsed_flags)
    190 
    191 

~/code_perso/decibel-light/prod/doc_example.py in train_and_export(parsed_flags)
    162                   parsed_flags.tflite_model_file)
    163   export(model, parsed_flags.model_dir, parsed_flags.tflite_model_file,
--> 164          parsed_flags.use_post_training_quantize)
    165   tf.logging.info(
    166       'Finished exporting, model is %s' % parsed_flags.tflite_model_file)

~/code_perso/decibel-light/prod/doc_example.py in export(model, model_dir, tflite_model_file, use_post_training_quantize)
    144   converter = tf.lite.TFLiteConverter.from_session(sess, [x], [output_class])
    145   converter.post_training_quantize = use_post_training_quantize
--> 146   tflite = converter.convert()
    147   with open(tflite_model_file, 'w') as f:
    148     f.write(tflite)

~/.virtualenvs/decibel/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py in convert(self)
    981           input_tensors=self._input_tensors,
    982           output_tensors=self._output_tensors,
--> 983           **converter_kwargs)
    984     else:
    985       result = _toco_convert_graph_def(

~/.virtualenvs/decibel/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)
    447       input_data.SerializeToString(),
    448       debug_info_str=debug_info_str,
--> 449       enable_mlir_converter=enable_mlir_converter)
    450   return data
    451 

~/.virtualenvs/decibel/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    198       stdout = _try_convert_to_unicode(stdout)
    199       stderr = _try_convert_to_unicode(stderr)
--> 200       raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
    201   finally:
    202     # Must manually cleanup files.

ConverterError: See console for info.
2019-12-16 23:55:52.418001: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418032: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418038: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418043: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418047: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418051: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418056: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418061: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418066: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418071: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418076: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418081: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418146: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418153: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418158: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418162: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418166: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418171: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418175: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418179: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418184: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418188: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418193: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418199: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ReadVariableOp
2019-12-16 23:55:52.418227: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve
2019-12-16 23:55:52.418236: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-12-16 23:55:52.418479: F tensorflow/lite/toco/tooling_util.cc:1074] Check failed: name.substr(colon_pos + 1).find_first_not_of(""0123456789"") == string::npos (0 vs. 18446744073709551615)Array 'stacked_rnn_cells/InputHint-UnidirectionalSequenceLstm-34aa74ee205711ea831bad6e4148a879-12-None-input_bias/ReadVariableOp:value' has non-digit characters after colon.
Fatal Python error: Aborted

Current thread 0x00007f7ffc1c8740 (most recent call first):
  File ""/home/mparient/.virtualenvs/decibel/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52 in execute
  File ""/home/mparient/.virtualenvs/decibel/lib/python3.6/site-packages/absl/app.py"", line 250 in _run_main
  File ""/home/mparient/.virtualenvs/decibel/lib/python3.6/site-packages/absl/app.py"", line 299 in run
  File ""/home/mparient/.virtualenvs/decibel/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40 in run
  File ""/home/mparient/.virtualenvs/decibel/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89 in main
  File ""/home/mparient/.virtualenvs/decibel/bin/toco_from_protos"", line 8 in <module>
Aborted
```
"
35167,Batch norm internal variables keep changing during inference,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 16.04
- Mobile device if the issue happens on mobile device: Dell Precision Tower 7910
- TensorFlow installed from (source or binary): tensorflow-gpu 1.14.0
- TensorFlow version (use command below): tensorflow-gpu 1.14.0
- Installed from: conda
- Python version: python 3.6
- GCC/Compiler version (if compiling from source): 6.5.0
- CUDA/cuDNN version: 10.1 but nvcc 7.5
- GPU model and memory:Quadro M4000


**Describe the current behavior**
I've encountered weird behavior of batch normalization in TF1.14 (I will update ulteriorly for TF2). The accuracy decrease a lot after the graph freezing. I identified that the origin of this discrepency is due to the batch normalization. A smaller model code to reproduce this behavior is attached at the end of this post. 
After training, I freeze the graph with `tf.graph_util.convert_variables_to_constants()` then optimize with `optimize_for_inference()`. From the results below, you can see that the from training to testing it changed slightly but **just after** freezing, the **moving_avg, beta and gamma changed tremendously** but not the moving_std:

> - [x] Training:   

>0%|          | 0/100 [00:00<=X, X/its] mov_avg: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],  mov_std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.],  beta: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],  gamma: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
> 
> 100%|██████████| 100/100 [00:01<00:00, 79.00it/s] mov_avg:
> [**-0.18366139  0.24028867  0.91940075 -0.07153109**  0.08811506
> -0.5552308
>   0.12759416  0.11020644  0.47183645 -0.91372997 -0.35405988  0.5768641 ],  mov_std: [**0.36972958 0.36972958 0.36972958 0.36972958** 0.36972958 0.36972958
>  0.36972958 0.36972958 0.36972958 0.36972958 0.36972958 0.36972958],  beta: [**0.         0.         0.         0.00995897** 0.         0.
>  0.00995881 0.00995875 0.         0.00989968 0.         0.        ],  gamma: [**1.         1.         1.         1.0009596**  1.         1.
>  **0.99905616 0.99986655 1.         1.0005279**  1.         1.        ]
> 
> - [x] Testing: 

>0%|          | 0/5 [00:00<=X, Xit/s] mov_avg: [**-0.18473879  0.24169827  0.92479414 -0.07199407**  0.08863196
> -0.5584879
>   0.12828277  0.11082216  0.47460434 -0.9191201  -0.35613686  0.5802481 ],  mov_std: [**0.36603227 0.36603227 0.36603227 0.36603227** 0.36603227 0.36603227
>  0.36603227 0.36603227 0.36603227 0.36603227 0.36603227 0.36603227],  beta: [**0.         0.         0.         0.01005934** 0.         0.
>  0.01005919 0.01005913 0.         0.00999967 0.         0.        ],  gamma: [**1.         1.         1.         1.0009888**  1.         1.
>  0.99904144 0.999887   1.         1.0005565  1.         1.        ]
> 
> 100%|██████████| 100/100 [00:00<00:00, 182.68it/s] mov_avg:
> [**-0.18473879  0.24169827  0.92479414 -0.07199407  0.08863196**
> -0.5584879
>   0.12828277  0.11082216  0.47460434 -0.9191201  -0.35613686  0.5802481 ],  mov_std: [**0.36603227 0.36603227 0.36603227 0.36603227 0.36603227** 0.36603227
>  0.36603227 0.36603227 0.36603227 0.36603227 0.36603227 0.36603227],  beta: [**0.         0.         0.         0.01005934 0.**         0.
>  0.01005919 0.01005913 0.         0.00999967 0.         0.        ],  gamma: [**1.         1.         1.         1.0009888  1.**         1.
>  0.99904144 0.999887   1.         1.0005565  1.         1.        ]

> - [x] Optimized (is_training = False) 

>0%|          | 0/100 [00:00<=X, Xit/s] mov_avg: [ **0.31595632 -0.19200048 -0.80097497  0.22726376**
> **-0.2556441**  -0.1720414
>   0.05251308  0.5521817  -0.93566465 -0.14799471 -0.11172786 -0.03040024],  mov_std: [**0.36603227 0.36603227 0.36603227 0.36603227 0.36603227** 0.36603227
>  0.36603227 0.36603227 0.36603227 0.36603227 0.36603227 0.36603227],  beta: [**0.         0.         0.         0.         0.00999963** 0.
>  0.01005759 0.         0.         0.         0.         0.        ],  gamma: [**1.         1.         1.         1.         1.0017155**  1.
>  0.99796325 1.         1.         1.         1.         1.        ]
> 
> 100%|██████████| 100/100 [00:00<00:00, 404.37it/s] mov_avg: [
> 0.31595632 -0.19200048 -0.80097497  0.22726376 -0.2556441  -0.1720414
>   0.05251308  0.5521817  -0.93566465 -0.14799471 -0.11172786 -0.03040024],  mov_std: [0.36603227 0.36603227 0.36603227 0.36603227 0.36603227 0.36603227
>  0.36603227 0.36603227 0.36603227 0.36603227 0.36603227 0.36603227],  beta: [0.         0.         0.         0.         0.00999963 0.
>  0.01005759 0.         0.         0.         0.         0.        ],  gamma: [1.         1.         1.         1.         1.0017155  1.
>  0.99796325 1.         1.         1.         1.         1.        ]


**Describe the expected behavior**
I was expecting these local variables **not change when we switch the training parameter between True/False or while we freeze it**.

**Code to reproduce the issue**

```
import tensorflow as tf
from tqdm import tqdm
import numpy as np
from util import print_nodes_name_shape, check_N_mkdir
from tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference
from tensorflow.python.framework import dtypes
import os


input_ph = tf.placeholder(tf.float32, shape=(None, 2, 2, 1), name='input_ph')
output_ph = tf.placeholder(tf.float32, shape=(None, 2, 2, 3), name='output_ph')
is_training = tf.placeholder(tf.bool, shape=[], name='is_training')

# build a one layer Full layer with BN and save 2 ckpts
with tf.name_scope('model'):
    out = tf.reshape(input_ph, shape=(-1, 2 * 2 * 1), name='flatten')
    with tf.variable_scope('dnn1', reuse=False):
        w1 = tf.get_variable('w1', dtype=tf.float32, shape=[4 * 1, 4 * 3], initializer=tf.initializers.glorot_normal())
        # b1 = tf.get_variable('b1', dtype=tf.float32, shape=[4 * 3], initializer=tf.initializers.glorot_normal())
    # out = tf.matmul(out, w1) + b1
    out = tf.matmul(out, w1)
    out = tf.layers.batch_normalization(out, training=is_training, name='BN')
    logits = tf.nn.relu(out)
    logits = tf.reshape(logits, shape=(-1, 2, 2, 3))

with tf.name_scope('loss'):
    MSE = tf.losses.mean_squared_error(labels=output_ph, predictions=logits)

with tf.name_scope('operations'):
    opt = tf.train.AdamOptimizer(learning_rate=0.0001, name='Adam')
    grads = opt.compute_gradients(MSE)
    train_op = opt.apply_gradients(grads, name='apply_grad')

# train
with tf.Session() as sess:
    # prepare
    graph = tf.get_default_graph()
    print_nodes_name_shape(graph)
    saver = tf.train.Saver()

    # init variables
    sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])
    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    print(update_ops)  # note: [<tf.Operation 'model/BN/cond_2/Merge' type=Merge>, <tf.Operation 'model/BN/cond_3/Merge' type=Merge>]
    saver.save(sess, './dummy/ckpt/step0')
    # train
    for i in tqdm(range(100)):
        inputs = np.ones((8, 2, 2, 1)) + np.random.randn(8, 2, 2, 1)
        outputs = np.arange(8 * 2 * 2 * 3).reshape((8, 2, 2, 3))
        with tf.variable_scope('', reuse=True):
            mov_avg, mov_std, beta, gamma = sess.run([tf.get_variable('BN/moving_mean'),
                                         tf.get_variable('BN/moving_variance'),
                                                      tf.get_variable('BN/beta'),
                                                      tf.get_variable('BN/gamma')])
        if i == 0 or i == 99:
            print('\nmov_avg: {}, \nmov_std: {}, \nbeta: {}, \ngamma: {}'.format(mov_avg, mov_std, beta, gamma))
        _, _ = sess.run([train_op, tf.get_collection(tf.GraphKeys.UPDATE_OPS)], feed_dict={
            input_ph: inputs,
            output_ph: outputs,
            is_training: True,
        })

    for i in tqdm(range(100)):
        inputs = np.ones((8, 2, 2, 1)) + np.random.randn(8, 2, 2, 1)
        outputs = np.arange(8 * 2 * 2 * 3).reshape((8, 2, 2, 3))
        with tf.variable_scope('', reuse=True):
            mov_avg, mov_std, beta, gamma = sess.run([tf.get_variable('BN/moving_mean'),
                                         tf.get_variable('BN/moving_variance'),
                                                      tf.get_variable('BN/beta'),
                                                      tf.get_variable('BN/gamma')])
        if i == 0 or i == 99:
            print('\nmov_avg: {}, \nmov_std: {}, \nbeta: {}, \ngamma: {}'.format(mov_avg, mov_std, beta, gamma))
        _ = sess.run([graph.get_tensor_by_name('model/Reshape:0')], feed_dict={
            input_ph: inputs,
            is_training: False,
        })

        # print moving avg/std
    saver.save(sess, './dummy/ckpt/step100')


def freeze_ckpt_for_inference(ckpt_path=None, conserve_nodes=None):
    # clean graph first
    tf.reset_default_graph()
    # freeze ckpt then convert to pb
    new_input = tf.placeholder(tf.float32, shape=[None, 10, 10, 1], name='new_input')
    new_is_training = tf.placeholder(tf.bool, name='new_is_training')

    restorer = tf.train.import_meta_graph(
        ckpt_path + '.meta',
        input_map={
            'input_ph': new_input,
            'is_training': new_is_training,
        },
        clear_devices=True,
    )

    input_graph_def = tf.get_default_graph().as_graph_def()
    check_N_mkdir('./dummy/pb/')
    check_N_mkdir('./dummy/tb/')

    # freeze to pb
    with tf.Session() as sess:
        # restore variables
        restorer.restore(sess, './dummy/ckpt/step100')
        # convert variable to constant
        output_graph_def = tf.graph_util.convert_variables_to_constants(
            sess=sess,
            input_graph_def=input_graph_def,
            output_node_names=conserve_nodes,
        )

        # save to pb
        with tf.gfile.GFile('./dummy/pb/freeze.pb', 'wb') as f:  # 'wb' stands for write binary
            f.write(output_graph_def.SerializeToString())


def optimize_graph_for_inference(pb_dir=None, conserve_nodes=None):
    tf.reset_default_graph()
    check_N_mkdir(pb_dir)

    # import pb file
    with tf.gfile.FastGFile(pb_dir + 'freeze.pb', ""rb"") as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())

    # optimize graph
    optimize_graph_def = optimize_for_inference(input_graph_def=graph_def,
                                                input_node_names=['new_input', 'new_is_training'],
                                                output_node_names=conserve_nodes,
                                                placeholder_type_enum=[dtypes.float32.as_datatype_enum,
                                                                       dtypes.bool.as_datatype_enum,
                                                                       dtypes.float32.as_datatype_enum,
                                                                       ]
                           )
    with tf.gfile.GFile(pb_dir + 'optimize.pb', 'wb') as f:
        f.write(optimize_graph_def.SerializeToString())

conserve_nodes = ['model/Reshape']
freeze_ckpt_for_inference(ckpt_path='./dummy/ckpt/step100', conserve_nodes=conserve_nodes)
optimize_graph_for_inference(pb_dir='./dummy/pb/', conserve_nodes=conserve_nodes)

# cleaning
tf.reset_default_graph()

# load pb
with tf.gfile.FastGFile('./dummy/pb/optimize.pb', ""rb"") as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())

print('\n Now inference*******************************')
# inference
with tf.Session() as sess:
    tf.graph_util.import_graph_def(
        graph_def,
    )
    # prepare
    G = tf.get_default_graph()
    print_nodes_name_shape(G)
    new_input = G.get_tensor_by_name('import/new_input:0')
    new_is_training = G.get_tensor_by_name('import/new_is_training:0')
    new_output = G.get_tensor_by_name('import/' + conserve_nodes[-1] + ':0')

    # train
    for i in tqdm(range(100)):
        inputs = np.ones((8, 2, 2, 1)) + np.random.randn(8, 2, 2, 1)
        outputs = np.arange(8 * 2 * 2 * 3).reshape((8, 2, 2, 3))
        # print moving avg/std
        with tf.variable_scope('', reuse=True):
            mov_avg, mov_std, beta, gamma = sess.run([G.get_tensor_by_name('import/BN/moving_mean:0'),
                                                      G.get_tensor_by_name('import/BN/moving_variance:0'),
                                                      G.get_tensor_by_name('import/BN/beta:0'),
                                                      G.get_tensor_by_name('import/BN/gamma:0')])
        if i == 0 or i == 99:
            print('\nmov_avg: {}, \nmov_std: {}, \nbeta: {}, \ngamma: {}'.format(mov_avg, mov_std, beta, gamma))
        new_out = sess.run([new_output], feed_dict={
            new_input: inputs,
            new_output: outputs,
            new_is_training: False,
        })
        # print('out: {}'.format(new_out))
```



Is this **an expected behavior?**. I don't think so, because the NN is very sensitive if we change even a very little bit of the beta and gamma. Is this be remedied in the 2.0 version? 
"
35165,25X slower in-graph vs out-of-graph training loop on GPU in TF2.0 ,"
**Describe the current behavior**

When training using a fully in-graph training loop for a custom model ([as described in the documentation](https://www.tensorflow.org/guide/function#define_the_training_loop)), training is 25x slower than when training the same model with an out-of-graph training loop. 

**Describe the expected behavior**

I would think in-graph training should be faster because we are not shuttling data back and forth between the gpu and python runtimes.

**Code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf
import tensorflow.keras as keras
import time
import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


class MyModel(keras.Model):

    def __init__(self):
        super(MyModel, self).__init__()
        self.conv1 = keras.layers.Conv2D(64, [3, 3])
        self.conv2 = keras.layers.Conv2D(64, [3, 3])
        self.flatten = keras.layers.Flatten()

    def call(self, inputs, training):
        images = inputs[0]
        targets = inputs[1]
        x = self.conv1(images)
        x = self.conv2(x)
        x = self.flatten(x)
        loss = tf.reduce_mean(x, axis=1) - tf.reduce_mean(targets, axis=1)
        return loss

def create_dataset():
    X = np.zeros([10, 224, 224, 3], dtype=np.float32)
    Y = np.zeros([10, 1000], dtype=np.float32)
    x_ds = tf.data.Dataset.from_tensor_slices(X)
    y_ds = tf.data.Dataset.from_tensor_slices(Y)
    ds = tf.data.Dataset.zip((x_ds, y_ds))
    ds = ds.batch(32)
    return ds

@tf.function
def train_one_step(model, optim, inputs):
    with tf.GradientTape() as tape:
        loss = model(inputs, training=True)
        loss = tf.reduce_mean(loss)
    grads = tape.gradient(loss, model.trainable_variables)
    grads_and_vars = zip(grads, model.trainable_variables)
    optim.apply_gradients(grads_and_vars)
    return loss


@tf.function
def in_graph_training_loop(model, optim, dataset):
    step = 0
    for inputs in dataset:
        loss = train_one_step(model, optim, inputs)
        step += 1


def out_graph_training_loop(model, optim, dataset):
    step = 0
    for inputs in dataset:
        loss = train_one_step(model, optim, inputs)
        step += 1


def main():
    model = MyModel()
    optim = keras.optimizers.Adam(1e-4)
    dataset = create_dataset()

    for i in range(5):
        t0 = time.time()
        in_graph_training_loop(model, optim, dataset)
        t1 = time.time()
        print('Time for in-graph training loop: %.3f secs' % (t1 - t0))

    print('-' * 20)

    for i in range(5):
        t0 = time.time()
        out_graph_training_loop(model, optim, dataset)
        t1 = time.time()
        print('Time for out-of-graph training loop: %.3f secs' % (t1 - t0))


if __name__ == '__main__':
    main()

```
Output:

```
python train/test.py 
Time for in-graph training loop: 2.064 secs
Time for in-graph training loop: 1.031 secs
Time for in-graph training loop: 1.013 secs
Time for in-graph training loop: 1.018 secs
Time for in-graph training loop: 1.012 secs
--------------------
Time for out-of-graph training loop: 2.079 secs
Time for out-of-graph training loop: 0.014 secs
Time for out-of-graph training loop: 0.041 secs
Time for out-of-graph training loop: 0.041 secs
Time for out-of-graph training loop: 0.041 secs
```

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes but this is minimally divergent from stock examples in documentation
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0
- Python version:  3.6.8
- CUDA/cuDNN version: 10.0
"
35164,"TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.","**Describe the problem**
I am having the the below problem
TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

gen_optimizer = tf.keras.optimizers.Adam(1e-3, beta_1=0.5)
disc_optimizer = tf.keras.optimizers.RMSprop(1e-3)
    
# model
model = VAEGAN(
    enc = encoder,
    dec = decoder,
    vae_disc_function = vaegan_discrim,
    lr_base_gen = 1e-3, # 
    lr_base_disc = 1e-4, # the discriminator's job is easier than the generators so make the learning rate lower
    latent_loss_div=1, # this variable will depend on your dataset - choose a number that will bring your latent loss to ~1-10
    sig_mult = 10, # how binary the discriminator's learning rate is shifted (we squash it with a sigmoid)
    recon_loss_div = .001, # this variable will depend on your dataset - choose a number that will bring your latent loss to ~1-10
)

model.train(example_data)


**Any other info / logs**
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-11-2112af51db2e> in <module>
----> 1 model.train(example_data)

<ipython-input-6-94ca8f8e5589> in train(self, x)
     93 
     94     def train(self, x):
---> 95         enc_gradients, dec_gradients, disc_gradients = self.compute_gradients(x)
     96         self.apply_gradients(enc_gradients, dec_gradients, disc_gradients)
     97 

<ipython-input-6-94ca8f8e5589> in compute_gradients(self, x)
     74     def compute_gradients(self, x):
     75         with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:
---> 76             (_, latent_loss, discrim_layer_recon_loss, gen_fake_loss, disc_fake_loss, disc_real_loss,) = self.compute_loss(x)
     77 
     78             enc_loss = latent_loss + discrim_layer_recon_loss

<ipython-input-6-94ca8f8e5589> in compute_loss(self, x)
     48         # pass through network
     49         q_z = self.dist_encode(x)
---> 50         z = q_z.sample()
     51         p_z = ds.MultivariateNormalDiag(loc=[0.0] * z.shape[-1], scale_diag=[1.0] * z.shape[-1])
     52         xg = self.decode(z)

~/anaconda3/lib/python3.7/site-packages/tensorflow_probability/python/distributions/distribution.py in sample(self, sample_shape, seed, name, **kwargs)
    838       samples: a `Tensor` with prepended dimensions `sample_shape`.
    839     """"""
--> 840     return self._call_sample_n(sample_shape, seed, name, **kwargs)
    841 
    842   def _call_log_prob(self, value, name, **kwargs):

~/anaconda3/lib/python3.7/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py in _call_sample_n(self, sample_shape, seed, name, **kwargs)
    389       # work, it is imperative that this is the last modification to the
    390       # returned result.
--> 391       y = self.bijector.forward(x, **bijector_kwargs)
    392       y = self._set_sample_static_shape(y, sample_shape)
    393 

~/anaconda3/lib/python3.7/site-packages/tensorflow_probability/python/bijectors/bijector.py in forward(self, x, name, **kwargs)
    931       NotImplementedError: if `_forward` is not implemented.
    932     """"""
--> 933     return self._call_forward(x, name, **kwargs)
    934 
    935   def _inverse(self, y):

~/anaconda3/lib/python3.7/site-packages/tensorflow_probability/python/bijectors/bijector.py in _call_forward(self, x, name, **kwargs)
    902       if not self._is_injective:  # No caching for non-injective
    903         return self._forward(x, **kwargs)
--> 904       mapping = self._lookup(x=x, kwargs=kwargs)
    905       if mapping.y is not None:
    906         return mapping.y

~/anaconda3/lib/python3.7/site-packages/tensorflow_probability/python/bijectors/bijector.py in _lookup(self, x, y, kwargs)
   1341     if x is not None:
   1342       # We removed x at caching time. Add it back if we lookup successfully.
-> 1343       mapping = self._from_x[x].get(subkey, mapping).merge(x=x)
   1344     if y is not None:
   1345       # We removed y at caching time. Add it back if we lookup successfully.

~/anaconda3/lib/python3.7/site-packages/tensorflow_probability/python/bijectors/bijector.py in __getitem__(self, key)
    149   def __getitem__(self, key):
    150     weak_key = HashableWeakRef(key, lambda w: self.pop(w, None))
--> 151     return super(WeakKeyDefaultDict, self).__getitem__(weak_key)
    152 
    153   # This is the ""DefaultDict"" part.

~/anaconda3/lib/python3.7/site-packages/tensorflow_probability/python/bijectors/bijector.py in __hash__(self)
    179     x = self()
    180     if not isinstance(x, np.ndarray):
--> 181       return hash(x)
    182     # Note: The following logic can never be reached by the public API because
    183     # the bijector base class always calls `convert_to_tensor` before accessing

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in __hash__(self)
    711     if (Tensor._USE_EQUALITY and executing_eagerly_outside_functions() and
    712         (g is None or g._building_function)):  # pylint: disable=protected-access
--> 713       raise TypeError(""Tensor is unhashable if Tensor equality is enabled. ""
    714                       ""Instead, use tensor.experimental_ref() as the key."")
    715     else:

TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.
"
35161,why TF2.0+TRT6 (keras) slower than TF1+keras about 4 times ?,"I'd like to understand the performance issue I faced :
Why TF2.0+TRT6 (keras) slower than TF1+keras about 4 times ?

GPU K80

before : TF1 + keras (as separate package)
after: TF2 + TRT6 integrated with integrated keras

on the same example TF2 is 4 times slower, when I'm waiting acceleration of training/predition in new version with TRT integration.

"
35160,-D_GLIBCXX_USE_CXX11_ABI=1 increases a lot RAM usage,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from: pip
- TensorFlow version (use command below): 1.13.1
- Python version: python3.7
- CUDA/cuDNN version: 10.1
- GPU model and memory: GTX 1080 ti


**Describe the current behavior**

Since g++7 is now the default version on Ubuntu 18 and most distributions, most builds will use  `_GLIBCXX_USE_CXX11_ABI=1`. It seems also that when tensorflow is built with  `_GLIBCXX_USE_CXX11_ABI=0`, it implies recompiling all other libraries of the project with this flag which can be unconvenient. 

We noticed that building with `_GLIBCXX_USE_CXX11_ABI=1` increases the RAM by a lot. 

**Describe the expected behavior**

Both packages should consume the same amount of RAM.

**Code to reproduce the issue**

You can install tensorflow using `python3.7 -m pip install tensorflow==1.13.1` (related to https://github.com/tensorflow/tensorflow/issues/27078) and makes sure 
`python3.7 -c ""import tensorflow; print(tensorflow.sysconfig.get_compile_flags())""` prints `-D_GLIBCXX_USE_CXX11_ABI=1`. 
Then you can install it in python3.6 `python3.6 -m pip install tensorflow==1.13.1` and make sure ` python3.6 -c ""import tensorflow; print(tensorflow.sysconfig.get_compile_flags())""` prints `D_GLIBCXX_USE_CXX11_ABI=0`.

Now run this script with python3.6 and python3.7 and you will see that the second one consume a lot more (x3 on the model I use). Any `saved_model.pb` should work.

```python
import io
import os
import sys
try:
    from urllib import urlopen
except ImportError:
    from urllib.request import urlopen

import numpy
import psutil
from PIL import Image

import tensorflow as tf
from tensorflow.core.protobuf import saved_model_pb2
from tensorflow.python.platform import gfile
from tensorflow.python.util import compat

process = psutil.Process(os.getpid())

def print_ram(prefix=''):
    print(""RAM"", prefix, process.memory_info().rss / 1024. / 1024.)


if __name__ == '__main__':

    if len(sys.argv) == 1:
        model_filename = 'saved_model.pb'
    else:
        model_filename = sys.argv[1]

    with gfile.FastGFile(model_filename, 'rb') as f:
        data = compat.as_bytes(f.read())
        sm = saved_model_pb2.SavedModel()
        sm.ParseFromString(data)
        if 1 != len(sm.meta_graphs):
            print('More than one graph found. Not sure which to write')
            sys.exit(1)

    img_url = 'https://i.dailymail.co.uk/1s/2019/11/23/09/21370544-7717313-image-a-1_1574501083030.jpg'
    image_data = urlopen(img_url).read()
    decoded_data = numpy.array(Image.open(io.BytesIO(image_data)))
    decoded_data = numpy.expand_dims(decoded_data, axis=0)

    print_ram('before graph import')
    graph = tf.import_graph_def(sm.meta_graphs[0].graph_def)

    print_ram('before device')
    with tf.device(""/device:GPU:0""):
        with tf.Session(graph=graph, config=None) as sess:
            print_ram('after session')
            output = sess.graph.get_tensor_by_name('import/predictions:0')
            print_ram('before run')
            for i in range(10000):
                results = sess.run(output, feed_dict={""import/image_tensor:0"": decoded_data})
                print_ram('after run')
```

**Other info / logs**

python3.6:
```
RAM before graph import 527.55078125
RAM before device 856.59375
2019-12-16 17:51:27.400388: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-12-16 17:51:27.425979: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3399580000 Hz
2019-12-16 17:51:27.426637: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x229fee0 executing computations on platform Host. Devices:
2019-12-16 17:51:27.426655: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
RAM after session 860.25
RAM before run 860.25
RAM after run 948.12109375
RAM after run 998.0
RAM after run 1022.2265625
RAM after run 1038.984375
RAM after run 1038.984375
RAM after run 1056.0
RAM after run 1092.1953125
RAM after run 1097.09375
RAM after run 1097.09375
RAM after run 1097.09375
RAM after run 1097.09375
RAM after run 1097.09375
RAM after run 1116.42578125
RAM after run 1116.42578125
RAM after run 1116.42578125
RAM after run 1116.42578125
RAM after run 1116.42578125
```

python3.7:
```
RAM before graph import 510.33984375
RAM before device 752.87890625
2019-12-16 17:26:00.118658: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-12-16 17:26:00.141979: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3399580000 Hz
2019-12-16 17:26:00.142737: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x265aff0 executing computations on platform Host. Devices:
2019-12-16 17:26:00.142775: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
RAM after session 756.5625
RAM before run 756.5625
RAM after run 2977.8515625
RAM after run 3030.1640625
RAM after run 3047.8046875
RAM after run 3090.08203125
RAM after run 3121.296875
RAM after run 3122.0703125
RAM after run 3123.1015625
RAM after run 3123.1015625
RAM after run 3135.9921875
```"
35159,Cannot successfully serialize and restore model using either 'tf' or 'h5' format,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux on Google Cloud instance
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): 2.1.0.rc1
- Python version: 3.7.3

**Describe the current behavior**

I've attempted to save a TF regression model in 'h5' format as follows:

    model_mean.save(HITTER_MODEL_DIR + 'hitter_model_mean.h5', save_format='h5')

however this fails:

```
ValueError                                Traceback (most recent call last)
<ipython-input-100-bf2d7187a55e> in <module>()
----> 1 model_mean.save(HITTER_MODEL_DIR + 'hitter_model_mean.h5', save_format='h5')

4 frames
/usr/local/lib/python3.6/dist-packages/h5py/_hl/group.py in create_group(self, name, track_order)
     58             name, lcpl = self._e(name, lcpl=True)
     59             gcpl = Group._gcpl_crt_order if track_order else None
---> 60             gid = h5g.create(self.id, name, lcpl=lcpl, gcpl=gcpl)
     61             return Group(gid)
     62 

h5py/_objects.pyx in h5py._objects.with_phil.wrapper()

h5py/_objects.pyx in h5py._objects.with_phil.wrapper()

h5py/h5g.pyx in h5py.h5g.create()

ValueError: Unable to create group (name already exists)
```

I then attempt to save as 'tf' format:

    model_mean.save(HITTER_MODEL_DIR + 'hitter_model_mean', save_format='tf')

which succeeds, however, attempting to load the model fails:

```
tf.keras.models.load_model(HITTER_MODEL_DIR + 'hitter_model_mean')

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-99-37062ae44748> in <module>()
----> 1 tf.keras.models.load_model(HITTER_MODEL_DIR + 'hitter_model_mean')

17 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/function_deserialization.py in restored_function_body(*args, **kwargs)
    260         .format(_pretty_format_positional(args), kwargs,
    261                 len(saved_function.concrete_functions),
--> 262                 ""\n\n"".join(signature_descriptions)))
    263 
    264   concrete_function_objects = []

ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (1 total):
    * Tensor(""inputs:0"", shape=(None, 2), dtype=float32)
  Keyword arguments: {}

Expected these arguments to match one of the following 1 option(s):

Option 1:
  Positional arguments (1 total):
    * [TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs/0')]
  Keyword arguments: {}
```
"
35157,"Caching to file instead of memory of tf.data.dataset using 2 map functions with cache(filename) in between and (x, y) tuple not working","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
None
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
2.0
- Python version:
3.6.8
- Bazel version (if compiling from source):
none
- GCC/Compiler version (if compiling from source):
none
- CUDA/cuDNN version:
none
- GPU model and memory:
none

map(), then caching a dataset of (image shape (224,224,3), and one_hot_encoded class index shape (n)) and subsequent map() works when simply using cache(); however when using file-based caching (which I need because of large dataset) using cache('cache.tmp') fails with the following error:
(I tried making the one-hot same type as image shape (float64) instead of int32 but does not help)
Also not using a tuple but array gives same error.

First I create dataset using tuple: (array of file names (str), array of class indices (str)) 
Then in map( lambda x, y : return (load_image(x) (float64 (224,224,3)), tf.one_hot(int(y), self.class_count, dtype=tf.float64))

afterwards when cache('cache.tmp'); and using a second .map(lambda x, y : return (preprocess(x), y))
the error is produced

Call stack:

14:09:27.225818: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at iterator_ops.cc:929 : Invalid argument: {{function_node __inference_Dataset_map_preprocess_10622}} Expects arg[1] to be double but string is provided
Traceback (most recent call last):
    x, y = next(dataset_iterator)
  File ""C:\Python36\lib\site-packages\tensorflow_core\python\data\ops\iterator_ops.py"", line 622, in __next__
    return self.next()
  File ""C:\Python36\lib\site-packages\tensorflow_core\python\data\ops\iterator_ops.py"", line 666, in next
    return self._next_internal()
  File ""C:\Python36\lib\site-packages\tensorflow_core\python\data\ops\iterator_ops.py"", line 651, in _next_internal
    output_shapes=self._flat_output_shapes)
  File ""C:\Python36\lib\site-packages\tensorflow_core\python\ops\gen_dataset_ops.py"", line 2672, in iterator_get_next_sync
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __inference_Dataset_map_preprocess_10622}} Expects arg[1] to be double but string is provided [Op:IteratorGetNextSync]


**Describe the expected behavior**
Successful caching of dataset

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35156,Item assigment,"I really want to know why tf can't support item assigment like numpy


"
35155,Memory chunk error when train BoostedTreesRegressor in docker container,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS (in docker container)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): pip wheel
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.8 
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
The training is down due to low layer system call (corrupted size vs. prev_size). the process is immediately corrupted without much log to trace.

**Describe the expected behavior**
the estimator (BoostedTreesRegressor) should be trained in normal.

**Code to reproduce the issue**
```
import os
import uuid
import json
from io import StringIO

import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.estimator import BoostedTreesRegressor

def split_data(df: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame):
    """"""Split data into train and test sets

    Args:
        df (DataFrame): pandas dataframe

    Returns:
        X_train, X_test, y_train, y_test
    """"""
    feature_cols = [c for c in df.columns if c.startswith('FEATURE')]
    label_col = [c for c in df.columns if c.startswith('LABEL')]

    return train_test_split(df[feature_cols], df[label_col], test_size=.2, random_state=42)


def make_input_fn(X, y, n_epochs=None, shuffle=True):
    def input_fn():
        dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))
        if shuffle:
            dataset = dataset.shuffle(len(y))
        dataset = dataset.repeat(n_epochs)
        dataset = dataset.batch(len(y))
        return dataset
    return input_fn


def make_serving_receiver_fn(df: pd.DataFrame):
    feature_col_names = [c for c in df.columns if c.startswith('FEATURE')]
    feature_cols = [tf.feature_column.numeric_column(fc) for fc in feature_col_names]
    feature_spec = tf.feature_column.make_parse_example_spec(feature_cols)
    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)


def train_model(df: pd.DataFrame, **params) -> (BoostedTreesRegressor, dict, dict):
    """"""Train Boost Tree

    Args:
        df (DataFrame): pandas dataframe
        params (**dict): parameters for training

    Returns:
        BoostTreesRegressor, saved model dir, a dict containing the evaluation metrics
    """"""
    feature_col_names = [c for c in df.columns if c.startswith('FEATURE')]
    label_col_name = [c for c in df.columns if c.startswith('LABEL')]
    feature_cols = [tf.feature_column.numeric_column(fc_name) for fc_name in feature_col_names]
    default_params = {
        'feature_columns': feature_cols,
        'n_batches_per_layer': 1,
        'model_dir': os.path.join('../output', str(uuid.uuid4())),
    }
    default_params.update(params)
    regressor = BoostedTreesRegressor(**default_params)
    X_train, X_valid, y_train, y_valid = train_test_split(df[feature_col_names], df[label_col_name], test_size=0.2, random_state=42)
    train_input_fn = make_input_fn(X_train, y_train)
    evaluate_input_fn = make_input_fn(X_valid, y_valid, n_epochs=1)
    regressor.train(input_fn=train_input_fn)
    summary = regressor.evaluate(input_fn=evaluate_input_fn)
    receiver_fn = make_serving_receiver_fn(df)
    export_dir = regressor.export_saved_model(regressor.model_dir, receiver_fn)

    summary = {k: float(v) for k, v in summary.items()}

    return regressor, export_dir, summary
```

**Other info / logs**
No problem under environment __OSX 10.14.5__ with __pyenv virtualenv 3.7.0 (default, Nov 22 2019, 12:39:30) \n[Clang 11.0.0 (clang-1100.0.33.12)]__

No problem when use google colab runtime (__Ubuntu 18.04.3 LTS Python 3.6.9 compiled by GCC 8.3.0__). See __[notebook shared](https://colab.research.google.com/drive/1aAjJCWOBd7R0Hb6w4d7UCSnkfk4c-xyE)__

Same problem in Kaggle runtime (__Debian GNU/Linux 9 Python 3.6.6 Anaconda GCC 7.3.0__) using tensorflow 2.0.0.

Full error log in docker using official image __tensorflow/tensorflow:2.0.0-py3__:

```
WARNING: Logging before flag parsing goes to stderr.
I1216 13:01:22.775628 140590360139584 estimator.py:1800] Using default config.
I1216 13:01:22.778364 140590360139584 estimator.py:212] Using config: {'_model_dir': '/app/WINDMIL_PoC_Data_FE/service/estimator/saved_model/8f979f53-4821-4def-bc6e-9692d868e2ea', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fdd837c5d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W1216 13:01:22.809657 140590360139584 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
W1216 13:01:22.811977 140590360139584 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I1216 13:01:22.888739 140590360139584 estimator.py:1147] Calling model_fn.
W1216 13:01:22.968535 140590360139584 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/canned/head.py:437: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
I1216 13:01:23.278414 140590360139584 estimator.py:1149] Done calling model_fn.
I1216 13:01:23.278981 140590360139584 basic_session_run_hooks.py:541] Create CheckpointSaverHook.
W1216 13:01:23.327183 140590360139584 meta_graph.py:448] Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
I1216 13:01:23.468171 140590360139584 monitored_session.py:240] Graph was finalized.
2019-12-16 13:01:23.468761: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-12-16 13:01:23.475253: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz
2019-12-16 13:01:23.476582: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4ed0210 executing computations on platform Host. Devices:
2019-12-16 13:01:23.476657: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
I1216 13:01:23.541273 140590360139584 session_manager.py:500] Running local_init_op.
I1216 13:01:23.561274 140590360139584 session_manager.py:502] Done running local_init_op.
W1216 13:01:23.960061 140590360139584 meta_graph.py:448] Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
I1216 13:01:24.019694 140590360139584 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /app/WINDMIL_PoC_Data_FE/service/estimator/saved_model/8f979f53-4821-4def-bc6e-9692d868e2ea/model.ckpt.
W1216 13:01:24.100714 140590360139584 meta_graph.py:448] Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
I1216 13:01:24.385114 140590360139584 basic_session_run_hooks.py:262] loss = 0.017109463, step = 0
W1216 13:01:24.594182 140590360139584 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
corrupted size vs. prev_size
Aborted
```
"
35154,Cannot train keras model with 2 outputs,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Family
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10 / 7.6.4
- GPU model and memory: GeForce 940M

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

`import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras import Model`


A tf.keras model is created with 2 output heads:

`inputs=Input(shape=(20))
x=Dense(32, activation='relu')(inputs)
output1=Dense(3, activation='softmax')(x)
output2=Dense(6, activation='softmax')(x)

model=Model(inputs=inputs, outputs=[output1, output2])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])`


Then an arbitary sized training sample is created:

`labels=tf.constant([[0 for i in range(50)],[1 for i in range(50)]], shape=[50,2])

sample=[]
for i in range(20):
    if i%2==0:
        sample.append(0)
    else: sample.append(1)

data=tf.constant([sample for i in range(50)], shape=[50,20])`


When the data is called for training,

`model.fit(data, labels, epochs=1, batch_size=5)`

 an error occurs saying that the module expected 2 arrays, but when I swap out the validation dataset with a pair of 2 arrays:

`model.fit(data, (labels1, labels2), epochs=1, batch_size=5)`

another error pops up saying that the x and y dimensions must be the same. To put simply, keras thinks that the pair of validation outputs is an array.

**Describe the expected behavior**
The model.fit method runs and the model is trained for 1 epoch with 50 samples.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
copy the above code in order, with the last model.fit method not usable.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
`Traceback (most recent call last):
  File ""C:\Users\admin\Desktop\working as intended\CONSTANTINOPLE\test\test duo output.py"", line 25, in <module>
    model.fit(data, labels, epochs=1, batch_size=8)
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 224, in fit
    distribution_strategy=strategy)
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 547, in _process_training_inputs
    use_multiprocessing=use_multiprocessing)
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 594, in _process_inputs
    steps=steps)
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2519, in _standardize_user_data
    exception_prefix='target')
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\keras\engine\training_utils.py"", line 531, in standardize_input_data
    str(len(data)) + ' arrays: ' + str(data)[:200] + '...')
ValueError: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [<tf.Tensor: id=286, shape=(50, 2), dtype=int32, numpy=
array([[0, 0],
       [0, 0],
       [0, 0],
       [0, 0],
       [0, 0],
       [0, 0],
       [0, 0],
       [0, 0],
       [0, 0],
       [0...`"
35153,Are there related APIs to determine savedmodel  whether batch is supported?,"There have a savedmodel format model,have some method or API to judge that is support batch?"
35152,Memory leaks when using tf.strings.split in map_func for tf.data.Dataset.map with eager execution.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):  binary
- TensorFlow version (use command below): 2.1.0rc1
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A 
- GCC/Compiler version (if compiling from source): N/A 
- CUDA/cuDNN version: N/A 
- GPU model and memory: N/A 

**Describe the current behavior**
If we use `tf.strings.split` in map_func to process each element in tf.data.Dataset, the used memory grows when we iterate the dataset and the used memory is not freed after iteration. What's more, the used memory continues to grow greatly if we repeatedly create the same tf.data.Dataset instance. However, the used memory keeps stable if we use tf.py_function to implement the split logic.
![image](https://user-images.githubusercontent.com/18071380/70906668-debb4780-2041-11ea-945c-bf2a05cbb637.png)


**Describe the expected behavior**
The used memory when iterating the dataset should be freed and should grow the create the same tf.data.Dataset instance.

**Code to reproduce the issue**
#### Experiment
```python
import psutil
import tensorflow as tf
import pandas as pd
import numpy as np

FEATURE_COUNT = 400

# mock feature names and feature data
def gen_feature_names(feature_count):
    feature_names = []
    for i in range(feature_count):
        feature_names.append(""f{}"".format(i))
    return feature_names


def gen_samples(feature_names, sample_count=5000):
    samples = []
    for _ in range(sample_count):
        feature_str = """"
        for name in feature_names:
            feature_str += ""{};"".format(np.random.random())
        feature_str += str(np.random.randint(0,2))
        yield feature_str
        
def dataset_fn(dataset):
    def _py_parse_data(record):
        record = record.numpy()
        feature_labels = bytes.decode(record).split("";"")
        return feature_labels

    def _parse_data(*record):
        feature_values = record[0:-1]
        features = {}
        for i,feature_name in enumerate(FEATURE_NAMES):
            features[feature_name] = feature_values[i]
        label = tf.strings.to_number(record[-1], tf.int64)
        return features, label
    
    tout = [tf.string] * FEATURE_COUNT
    tout.append(tf.string)
    
    dataset = dataset.map(
        lambda record: tf.py_function(
            _py_parse_data,
            [record],
            tout
        )
    )
    dataset = dataset.map(_parse_data)
    dataset = dataset.shuffle(buffer_size=100)
    return dataset


def dataset_fn_using_split(dataset):
    def _parse_data(record):
        feature_label = tf.strings.split([record], sep=';')[0]
        feature_values = feature_label[0:-1]
        features = {}
        for i, feature_name in enumerate(FEATURE_NAMES):
            features[feature_name] = feature_values[i]
        label = feature_label[-1]

        return features, label
    
    dataset = dataset.map(_parse_data)
    dataset = dataset.shuffle(buffer_size=100)
    return dataset

def create_dataset(feature_names, using_split=True):
    dataset = tf.data.Dataset.from_generator(
        lambda : gen_samples(feature_names), tf.string
    )
    if using_split:
        dataset = dataset_fn_using_split(dataset)
    else:
        dataset = dataset_fn(dataset)
    dataset = dataset.batch(512)
    return dataset


def view_used_mem():
    used_mem = psutil.virtual_memory().used
    print('used memory: {} Mb'.format(used_mem / 1024 / 1024))

FEATURE_NAMES = gen_feature_names(FEATURE_COUNT)

# Test used memory by using tf.strings.split in `map_func`
FEATURE_NAMES = gen_feature_names(FEATURE_COUNT)

start_time = time.time()
for i in range(4):
    print(""loop {}"".format(i))
    view_used_mem()
    dataset = create_dataset(FEATURE_NAMES, using_split=True)
    for batch in dataset:
        pass
print(""Consume time : {}"".format(time.time() - start_time))
print(""end"")
view_used_mem()

# Test used memory by using `tf.py_function`
start_time = time.time()
for i in range(4):
    print(""loop {}"".format(i))
    view_used_mem()
    dataset = create_dataset(FEATURE_NAMES, using_split=False)
    for batch in dataset:
        pass
print(""Consume time : {}"".format(time.time() - start_time))
print(""end"")
view_used_mem()

**Other info / logs**
We encounter this issue when building inputs pipeline using `tf.data.Dataset` in [ElasticDL](https://github.com/sql-machine-learning/elasticdl)
"
35151,No data found in layer when fitting TF 2.0 Keras model with DenseFeatures as input,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-4.14.106+-x86_64-with-debian-buster-sid
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.6

**Describe the current behavior**
I am trying to train a model using:
* TF 2.0 Keras functional API
* Feature columns and `DenseFeatures` as the input layer
* tf.data.Dataset API as the `x` parameter in model.fit()

This raises a ValueError, and I'm not sure why because this doesn't work even with (what I think is) a minimal example. I suspect that this might be due to TF trying to match the feature columns to the inputs by name, but the name of the Input tensor contains a suffix e.g. `age:0`, but I may very well be mistaken.

Might be related to: https://github.com/tensorflow/tensorflow/issues/30143

**Describe the expected behavior**
I expect to be able to train a model using the Keras functional API using data from the Dataset API, with feature_columns being fed into Input layers.

**Code to reproduce the issue**
The following test case does not represent my use case, but it does reproduce the problem. Despite what the simplistic example suggests, I explicitly require both feature_columns and the functional API so suggesting that I use other TF libraries would not be an option, unless I can replicate the same functionality with minimal effort.

```python
import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import DenseFeatures, Dense, Input

def make_model(features):
    feature_columns = [tf.feature_column.numeric_column(key) for key in features]
    nn_input = {key: Input(name=key, shape=(), dtype=tf.float32) for key in features}

    feat = DenseFeatures(feature_columns)(nn_input)
    dense = Dense(16)(feat)
    output = Dense(1)(dense)
    model = tf.keras.Model(inputs=nn_input, outputs=output)
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),
        metrics=[tf.keras.metrics.AUC()],
    )
    return model

features = [""age"", ""income""]
label = ""is_male""

input_dataset = tf.data.Dataset.from_tensor_slices(
    {key: np.ones((1000, 1), dtype=np.float) for key in features}
)
target_dataset = tf.data.Dataset.from_tensor_slices(
    {label: np.ones((1000, 1), dtype=np.int)}
)
complete_dataset = tf.data.Dataset.zip((input_dataset, target_dataset)).shuffle(10000)

model = make_model(features)
model.summary()
model.fit(complete_dataset)
```

This code references the issue created by @durandg12. Thank you.

**Stack trace**
```bash
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
    498           if data[x].__class__.__name__ == 'DataFrame' else data[x]
--> 499           for x in names
    500       ]

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py in <listcomp>(.0)
    498           if data[x].__class__.__name__ == 'DataFrame' else data[x]
--> 499           for x in names
    500       ]

KeyError: 'dense_1'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-1-0081a84113eb> in <module>
     27 model = make_model(features)
     28 model.summary()
---> 29 model.fit(complete_dataset)

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    726         max_queue_size=max_queue_size,
    727         workers=workers,
--> 728         use_multiprocessing=use_multiprocessing)
    729 
    730   def evaluate(self,

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    322                 mode=ModeKeys.TRAIN,
    323                 training_context=training_context,
--> 324                 total_epochs=epochs)
    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    326 

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    121         step=step, mode=mode, size=current_batch_size) as batch_logs:
    122       try:
--> 123         batch_outs = execution_function(iterator)
    124       except (StopIteration, errors.OutOfRangeError):
    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)
     84     # `numpy` translates Tensors to values in Eager mode.
     85     return nest.map_structure(_non_none_constant_value,
---> 86                               distributed_function(input_fn))
     87 
     88   return execution_function

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--> 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    501       # This is the first call of __call__, so we have to initialize.
    502       initializer_map = object_identity.ObjectIdentityDictionary()
--> 503       self._initialize(args, kwds, add_initializers_to=initializer_map)
    504     finally:
    505       # At this point we know that the initialization is complete (or less

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    406     self._concrete_stateful_fn = (
    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 408             *args, **kwds))
    409 
    410     def invalid_creator_scope(*unused_args, **unused_kwds):

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   1846     if self.input_signature:
   1847       args, kwargs = None, None
-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   1849     return graph_function
   1850 

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2148         graph_function = self._function_cache.primary.get(cache_key, None)
   2149         if graph_function is None:
-> 2150           graph_function = self._create_graph_function(args, kwargs)
   2151           self._function_cache.primary[cache_key] = graph_function
   2152         return graph_function, args, kwargs

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2039             arg_names=arg_names,
   2040             override_flat_arg_shapes=override_flat_arg_shapes,
-> 2041             capture_by_value=self._capture_by_value),
   2042         self._function_attributes,
   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    913                                           converted_func)
    914 
--> 915       func_outputs = python_func(*func_args, **func_kwargs)
    916 
    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    357         # the function a weak reference to itself to avoid a reference cycle.
--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    359     weak_wrapped_fn = weakref.ref(wrapped_fn)
    360 

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in distributed_function(input_iterator)
     71     strategy = distribution_strategy_context.get_strategy()
     72     outputs = strategy.experimental_run_v2(
---> 73         per_replica_function, args=(model, x, y, sample_weights))
     74     # Out of PerReplica outputs reduce or pick values to return.
     75     all_outputs = dist_utils.unwrap_output_dict(

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py in experimental_run_v2(self, fn, args, kwargs)
    758       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),
    759                                 convert_by_default=False)
--> 760       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    761 
    762   def reduce(self, reduce_op, value, axis):

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)
   1785       kwargs = {}
   1786     with self._container_strategy().scope():
-> 1787       return self._call_for_each_replica(fn, args, kwargs)
   1788 
   1789   def _call_for_each_replica(self, fn, args, kwargs):

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)
   2130         self._container_strategy(),
   2131         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):
-> 2132       return fn(*args, **kwargs)
   2133 
   2134   def _reduce_to(self, reduce_op, value, destinations):

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    290   def wrapper(*args, **kwargs):
    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
--> 292       return func(*args, **kwargs)
    293 
    294   if inspect.isfunction(func) or inspect.ismethod(func):

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics)
    251   x, y, sample_weights = model._standardize_user_data(
    252       x, y, sample_weight=sample_weight, class_weight=class_weight,
--> 253       extract_tensors_from_dataset=True)
    254   batch_size = array_ops.shape(nest.flatten(x, expand_composites=True)[0])[0]
    255   # If `model._distribution_strategy` is True, then we are in a replica context

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)
   2517           shapes=None,
   2518           check_batch_axis=False,  # Don't enforce the batch size.
-> 2519           exception_prefix='target')
   2520 
   2521       # Generate sample-wise weight values given the `sample_weight` and

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
    501     except KeyError as e:
    502       raise ValueError('No data provided for ""' + e.args[0] + '"". Need data '
--> 503                        'for each key in: ' + str(names))
    504   elif isinstance(data, (list, tuple)):
    505     if isinstance(data[0], (list, tuple)):

ValueError: No data provided for ""dense_1"". Need data for each key in: ['dense_1']
```
"
35150,SavedModelBundle.getSignatures() does not return outputs in correct order.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0.rc0
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Not relevant
- GPU model and memory: Not relevant

**Describe the current behavior**
Loading a SavedModel in C++, the outputs (and probably also inputs, but I do not have models with multiple inputs) from the signature are returned in a random order, which then causes the network to fail when used for inference.

**Describe the expected behavior**
The outputs (and probably inputs) should be returned in the correct order.

**Code to reproduce the issue**
```
std::string model_dir = ""/path/to/my_model"";
std::vector<string> input_names;
std::vector<string> output_names;
tensorflow::SavedModelBundleLite bundle; // Same with SavedModelBundle

// Create default options.
tensorflow::SessionOptions session_options;
tensorflow::RunOptions run_options;

// Load model.
auto status = tensorflow::LoadSavedModel(
	session_options,
	run_options,
	model_dir,
	{tensorflow::kSavedModelTagServe},
	&bundle
);

// Check if model has been loaded correctly.
if (!status.ok()) {
	std::cerr << status.ToString() << std::endl;
	return;
}

// Get model signature.
auto signatures = bundle.GetSignatures();
if (!signatures.contains(""serving_default"")) {
	std::cerr << ""Could not find serving_default in model signatures."" << std::endl;
	return;
}

// Get the inputs names.
for (auto const & input : signatures.at(""serving_default"").inputs()) {
	input_names.push_back(input.second.name());
}

// Get the outputs names.
for (auto const & output : signatures.at(""serving_default"").outputs()) {
	output_names.push_back(output.second.name());
}
std::vector<tensorflow::Tensor>  inputs = getMyInputs();  // Some compatible input vector.
std::vector<tensorflow::Tensor>  outputs;

// Create a vector of pairs for associating inputs to their names.
std::vector<std::pair<std::string, tensorflow::Tensor>> input_pairs;
for (std::size_t i = 0; i < inputs.size(); ++i) {
	input_pairs.push_back({input_names.at(i), inputs.at(i)});
}

// Run the network.
bundle.GetSession()->Run(
	input_pairs,
	output_names,
	{},
	&outputs
);
```

When running this the output order changes, giving a mismatch error (eg. `Check failed: dtype() == expected_dtype (3 vs. 1) float expected, got int32`) after calling `Run`. 


"
35149,TensorFlow Serving cannot handle more than 350 requests/second,"Docker Image: Tensorflow/Serving:latest
Model: Fashion_Mnist (per the official Docs)
Docker Version: 19.03.2, build 6a30dfc

I'm using Docker Container to serve a Tensorflow SavedModel over the REST API Port 8501. The inference latency is 2ms which is great. 

Problem: I'm trying to perform load-testing and notice that the rate of requests stay constant at approximately 350 requests/sec. Beyond which the request fails. Every request has a single batch payload. 

Observation: At the point of failure, the resource usage is only 10%. Server throws error code 404 and 401.

Troubleshooting: 
1. Have tried running `tensorflow_model_server` directly instead of the docker image and face the same issue. 
2. Have served different models (deep_mnist, custom models). Same issue persists.
3. Built a docker image with suitable instruction set. Same issue persists.
4. Deployed the Docker image on a K8 cluster. Set 4 Replicas behind a loadbalancer. Same issue persists. 
5. Pulled different tensorflow/serving image. Same issue persists. 

What have I been doing wrong?"
35148,Reading checkpoint file fail will ignore all checkpoints,"When restoring a checkpoint, if it read checkpoint file fail then all checkpoints will be ignored and train from scratch. I think it should throw exception rather than ignore, otherwise it will train from scratch silently. Especially read fail could happen when using distributed storage.


![image](https://user-images.githubusercontent.com/1125780/70891780-ea4b4600-2022-11ea-9ae0-fcf4572b1997.png)

Link to the code:
https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/checkpoint_management.py#L287"
35147,Size not changed when convert pb file to tflite file,"I tried to convert my quantized pb file to tflite file. 
My code is:
import tensorflow as tf

from tensorflow import lite
FLAGS = tf.app.flags.FLAGS
inputs=[""input_images""]
outputs=[""output_1""]
converter = lite.TFLiteConverter.from_frozen_graph(
    graph_def_file='./q/te.pb',
    input_arrays=inputs,
    output_arrays=outputs
)
#converter.post_training_quantize = False
converter.inference_input_type = lite.constants.QUANTIZED_UINT8
input_arrays = converter.get_input_arrays()
converter.quantized_input_stats = {input_arrays[0]:(0, 255)}
converter.default_ranges_stats = (0, 255)
tflite_quantized_model = converter.convert()
with open('te.tflite', 'wb') as f:
    f.write(tflite_quantized_model)

The pb was frozen by a model that had already been quantized by ""create_training_graph"" and ""create_eval_graph"". The size of my pb file is 83.45MB and the size of my tflite file is 83.15MB. How can I sovle this problem? I just want to quantize my pb file and get a smaller size file."
35146,Missing information when saving model in tf format,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary (pip)
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: Python 3.6.8
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: CUDA 10.0.130_411.31; cuDNN 10.0 v7.6.5.32
- GPU model and memory: NVIDIA Quadro P2000, 4 GB

**Describe the current behavior**
When the model is saved in the default tf format, warnings are logged when trying to serve the model.

Examplary warning logs:
```
WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x000001ED79058730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.
```

When the model is saved in the hdf5 format, the warnings do not occur.

**Describe the expected behavior**
The save formats should be equivalent and behave in the same way.

**Code to reproduce the issue**
Execute the following scripts to create and serve model
1. Run the first script with `format_ext = ''` which saves the model in tf format, **restart the Python console**, serve the model with the second script which creates the aforementioned warnings.
1. When running the scripts with `format_ext = '.h5'`, the model is saved in hdf5 format and no warnings appear.

Model creation:
```python
import os

import tensorflow as tf

format_ext = ''  # '.h5' or empty for tf format
model_path = os.path.join('out', 'mnist-classifier{}'.format(format_ext))

gpus = tf.config.experimental.list_physical_devices('GPU')

tf.config.experimental.set_virtual_device_configuration(
    gpus[0],
    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=512),
     tf.config.experimental.VirtualDeviceConfiguration(memory_limit=512),
     tf.config.experimental.VirtualDeviceConfiguration(memory_limit=512)]
)

strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    inputs = tf.keras.Input(shape=(784,), name='digits')
    x = tf.keras.layers.Dense(64, activation='relu', name='dense_1')(inputs)
    x = tf.keras.layers.Dense(64, activation='relu', name='dense_2')(x)
    outputs = tf.keras.layers.Dense(10, activation='softmax', name='predictions')(x)

    model = tf.keras.Model(inputs=inputs, outputs=outputs)

    model.compile(optimizer=tf.keras.optimizers.RMSprop(),  # Optimizer
                  # Loss function to minimize
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),
                  # List of metrics to monitor
                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

model.save(model_path)
```

Model serving:
```python
import os

import tensorflow as tf

format_ext = ''  # '.h5' or empty for tf format
model_path = os.path.join('out', 'mnist-classifier{}'.format(format_ext))

gpus = tf.config.experimental.list_physical_devices('GPU')

tf.config.experimental.set_virtual_device_configuration(
    gpus[0],
    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=512),
     tf.config.experimental.VirtualDeviceConfiguration(memory_limit=512),
     tf.config.experimental.VirtualDeviceConfiguration(memory_limit=512)]
)

(_, _), (x_test, _) = tf.keras.datasets.mnist.load_data()
x_test = x_test.reshape(10000, 784).astype('float32') / 255

strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    loaded_model = tf.keras.models.load_model(model_path)
    predictions = loaded_model.predict(x_test, batch_size=64)
```

**Other info / logs**
The warnings occur only if more than two vGPUs are used."
35145, How to package My MFCC modules ,"@tensorflow/micro
I want to package the MFCC into my model tflite. like this(tensorflow/lite/android/conv_actions_frozen.tflite)
But I build my own keras layer. And conert to tflite.The Model is a mess where the MFCC func.Why ?"
35144,Low performance in TF2.x Distributed Mirrored Strategy with 4 V100 GPUs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04.3 LTS
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: Python 3.6.8
- CUDA/cuDNN version: Driver Version: 440.33.01, CUDA Version: 10.2, cuDNN 7.6.2
- GPU model and memory: Tesla V100-SXM2-16GB

**Describe the current behavior**
With 4 V100 GPUs in Distributed Mirrored GPU Strategy training single step is around 3x slower than with single V100 GPU.

**Describe the expected behavior**
Single step should be less than 2x slower.

**Code to reproduce the issue**
Training loop (hierarchical VAE in the current configuration):
  https://github.com/olegmyrk/SPADE-Tensorflow/blob/9c1ced5b7b24640aeb7726169e215f5e63d971d3/SPADE.py#L1121

The code is adapted from TF1.x repository:
  https://github.com/olegmyrk/SPADE-Tensorflow/blob/8866a0b1457cbd4be5d6f549f9bf4075d49b2486/SPADE.py#L1045
and is compiled using TF2.x @tf.function annotation.

It uses a dry-run of the model to pre-create variables using tf.compat.v1.variable_scope(scope, reuse=tf.compat.v1.AUTO_REUSE):
  https://github.com/olegmyrk/SPADE-Tensorflow/blob/9c1ced5b7b24640aeb7726169e215f5e63d971d3/SPADE.py#L1144
and then runs the actual training step(s)
  https://github.com/olegmyrk/SPADE-Tensorflow/blob/9c1ced5b7b24640aeb7726169e215f5e63d971d3/SPADE.py#L1180

The total number of mirrored parameters is around 500MB.

With 4 V100 GPUs training step is around 3x slower than with single V100 GPU.

Command:
nohup python3 main.py --dataset CelebAMask-HQ --img_height 256 --img_width 256 --ch 16 --img_ch 3 --phase train --save_freq 10000 --batch_size 18 --gan_type hinge --code_gan_type gan --n_critic 1 --code_num_layers=4 --code_dist_num_layers=0 --sn=False --train_main=true --train_nondet=false --lr 0.0002 --print_freq 100 &> train.CelebAMask-HQ.log &

**Other info / logs**

With CUDA_VISIBLE_DEVICES=0
* Startup time: ~9 min
* GPU utilization: ~90%
* Training step: 0.7 seconds
* Log file: train.CelebAMask-HQ.1xgpu.log
 
With CUDA_VISIBLE_DEVICES=0,1,2,3
* Startup time: ~30min
  * Build variables (dry run): ~10min
  * Build model: ~20min
* GPU utilization: ~50%
* Training step: 2 seconds
* Log file: train.CelebAMask-HQ.4xgpu.log
[train.CelebAMask-HQ.1xgpu.log](https://github.com/tensorflow/tensorflow/files/3965889/train.CelebAMask-HQ.1xgpu.log)
[train.CelebAMask-HQ.4xgpu.log](https://github.com/tensorflow/tensorflow/files/3965890/train.CelebAMask-HQ.4xgpu.log)

"
35143,micro_speech example inference issue,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution
      Answer: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):
     Answer: source
- Tensorflow version (commit SHA if source):
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):
    Answer: x86
**Describe the problem**
   I build the x86 version of micro_speech example, run it with the g_yes_1000ms_sample_data file, but the result is ""Heard silence (0)"", it should be ""yes""
**Please provide the exact sequence of commands/steps when you ran into the problem**
   1. mkdir x86 under micro/examples/micro_speech directory
   2. in the x86 folder, add ""command_responder.cc"":
void RespondToCommand(tflite::ErrorReporter* error_reporter,
                      int32_t current_time, const char* found_command,
                      uint8_t score, bool is_new_command) {
  //if (is_new_command) {
    error_reporter->Report(""Heard %s (%d) @%dms"", found_command, score,
                           current_time);
    sleep(2);
  //}
}
   3. in the x86 folder, add audio_provider.cc:
TfLiteStatus GetAudioSamples(tflite::ErrorReporter* error_reporter,
                             int start_ms, int duration_ms,
                             int* audio_samples_size, int16_t** audio_samples) {

  const int yes_start = (0 * kAudioSampleFrequency) / 1000;
  const int yes_end = (1000 * kAudioSampleFrequency) / 1000;
  const int wraparound = (1000 * kAudioSampleFrequency) / 1000;
  const int start_sample = (start_ms * kAudioSampleFrequency) / 1000;
  for (int i = 0; i < kMaxAudioSampleSize; ++i) {
    const int sample_index = (start_sample + i) % wraparound;
    int16_t sample;
    sample = g_yes_1000ms_sample_data[sample_index - yes_start];
    g_dummy_audio_data[i] = sample;
  }
  *audio_samples_size = kMaxAudioSampleSize;
  *audio_samples = g_dummy_audio_data;

  return kTfLiteOk;
}

int32_t LatestAudioTimestamp()
{
     // This is how we let the outside world know that new audio data has arrived.
     g_latest_audio_timestamp += 1000;
    return g_latest_audio_timestamp;
}

4. build the example:
make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=x86 micro_speech
5. run the example:
   [~/tf-20191211/tensorflow-master/tensorflow/lite/experimental/micro/tools/make/gen/x86_x86_64/bin]./micro_speech
   start_ms=40 duration_ms=30
   start_ms=60 duration_ms=30
   start_ms=80 duration_ms=30
 ...
   Heard silence (0) @1000ms

actually, the result is the same when I port this example to my EVB.
By the way, why the start_ms begin with 40 instead of 0?

Thanks for your support.

ruey-an"
35142,Building from source: ERROR: kernels:training_ops,"**System information**
- **Linux Ubuntu 18.04**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version: v2 (master)
- Python version: python 3.7.5
- Installed using virtualenv? conda:  conda 4.7.12
- Bazel version (if compiling from source): **bazel 1.1.0**
- GCC/Compiler version (if compiling from source): **gcc 8.3.0**
- CUDA/cuDNN version: **CUDA 10.2/cuDNN7.6.5**
- GPU model and memory: GeForce GTX670 4035Mb
- CPU Intel i7860@2.80GHz
- Kernel  Linux-headers-4.15.0-73
- NVCC -  v2.5.6, for CUDA 10.2, Nov 19,2019



**Describe the problem**

It seems Bazel Build is disregarding my choice of **NOT INSTALLING XLA**.

I installed CUDA and cuDNN, NCCL, created a python 3.7.5 environment and decided to build tensorflow from MASTER branch since my  GPU has capabilities 3.0 and the other installs I tried, from docker or conda, where complaining about a minimum capability of 3.5.

There were plenty of documentation for the need of a build from source for 3.0 capability and I decided to do it. 

From git I realized it was necessary to disable XLA. `./configure` asked for **XLA** capabitity and I answered **N** (Y was the default), it correctly detected python, cuda and gcc, I wrote **3.0** for desired capability, (A message showed me the need to disable XLA, what I already did). 

When I did the bazel build it took a huge lot of time and seemed to freeze with something like the piece below.
````bash
./tensorflow/compiler/xla/service/hlo_computation.h:562:3: note: in expansion of macro ‘TF_RET_CHECK’
   TF_RET_CHECK(order.size() == instruction_count());
   ^~~~~~~~~~~~
./tensorflow/compiler/xla/service/hlo_computation.h: In instantiation of ‘tensorflow::Status xla::HloComputation::AcceptOrdered(xla::DfsHloVisitorBase<HloInstructionPtr>*, absl::Span<xla::HloInstruction* const>) const [with HloInstructionPtr = const xla::HloInstruction*]’:
./tensorflow/compiler/xla/service/hlo_computation.h:588:61:   required from here
./tensorflow/compiler/xla/service/hlo_computation.h:562:29: warning: comparison of integer expressions of different signedness: ‘absl::Span<xla::HloInstruction* const>::size_type’ {aka ‘long unsigned int’} and ‘tensorflow::int64’ {aka ‘long long int’} [-Wsign-compare]
   TF_RET_CHECK(order.size() == instruction_count());
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro ‘TF_PREDICT_FALSE’
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/compiler/xla/service/hlo_computation.h:562:3: note: in expansion of macro ‘TF_RET_CHECK’
   TF_RET_CHECK(order.size() == instruction_count());
   ^~~~~~~~~~~~
./tensorflow/compiler/xla/array.h: In instantiation of ‘tensorflow::int64 xla::Array<T>::dim(tensorflow::int64) const [with T = int; tensorflow::int64 = long long int]’:

````


**Provide the exact sequence of commands / steps that you executed before running into the problem**
````bash 
bazel build --config=opt -- config=--local_ram_resources=6000  --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --config=cuda //tensorflow/tools/pip_package:build_pip_package
````

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
The pieces showing the actual error ocurrered as follows:
````
ERROR: /home/lbarosi/Pythonia/tensorflow/tensorflow/core/kernels/BUILD:5735:1: C++ compilation of rule '//tensorflow/core/kernels:training_ops' failed (Exit 1)
x86_64-linux-gnu-gcc-8: fatal error: Killed signal terminated program cc1plus
compilation terminated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/lbarosi/Pythonia/tensorflow/tensorflow/python/tools/BUILD:141:1 C++ compilation of rule '//tensorflow/core/kernels:training_ops' failed (Exit 1)
INFO: Elapsed time: 4520.437s, Critical Path: 1586.97s
INFO: 5492 processes: 5492 local.
FAILED: Build did NOT complete successfully

````

Any help is appreciated. I would like to try as much as possible to stick with this configurations without downgradings of CUDA or cuDNN.
"
35141,tf.where should have optional dtype parameter,"- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No): Yes

```
tf.keras.backend.floatx = tf.float64
f64 = tf.Variable([0,0.2,0.5,0.7,1], dtype=tf.float64)
tf.where(f64 > 0.5, 1., 0. )

<tf.Tensor: id=16, shape=(5,), dtype=float32, numpy=array([0., 0., 0., 1., 1.], dtype=float32)>
```

alternatives for tf.float64 models lead to difficult to read code

```
tf.where(f64 > 0.5, tf.constant(1., dtype=tf.float64), tf.constant(0., dtype=tf.float64) )
tf.cast(tf.where(f64 > 0.5, 1., 0. ), dtype=tf.float64)
```

assuming that tf.where can't respect the float type of the input variable nor the floatx setting, as those change the api, better would be:

```
tf.where(f64 > 0.5, 1., 0. , dtype=tf.float64)
```

"
35140,[TF 2.1.0rc1] Fail to import package on Windows,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.1.0rc1
- Python version: 3.7

**Describe the current behavior**
When trying to import tensorflow on a windows 10 machine with no GPU/CUDA setup I get a `DLL load failed` error. I suspect this is related to 2.1 now containing dynamic kernels (like in 1.15), but it doesn't seem to be falling back appropriately.

**Describe the expected behavior**
Able to import

**Code to reproduce the issue**
```
pip install tensorflow==2.0.0
(win-tf2.0) python -c ""import tensorflow as tf; print(tf.__version__)""
2.0.0
```


```
pip install tensorflow==2.1.0rc1
(win-tf2.1) python -c ""import tensorflow as tf; print(tf.__version__)""
```

```
Traceback (most recent call last):
  File ""C:\Users\SeanM\Miniconda3\envs\win-tf2.1\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\SeanM\Miniconda3\envs\win-tf2.1\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\SeanM\Miniconda3\envs\win-tf2.1\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\SeanM\Miniconda3\envs\win-tf2.1\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\SeanM\Miniconda3\envs\win-tf2.1\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\SeanM\Miniconda3\envs\win-tf2.1\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\SeanM\Miniconda3\envs\win-tf2.1\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\SeanM\Miniconda3\envs\win-tf2.1\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\SeanM\Miniconda3\envs\win-tf2.1\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\SeanM\Miniconda3\envs\win-tf2.1\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\SeanM\Miniconda3\envs\win-tf2.1\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\SeanM\Miniconda3\envs\win-tf2.1\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\SeanM\Miniconda3\envs\win-tf2.1\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\SeanM\Miniconda3\envs\win-tf2.1\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\SeanM\Miniconda3\envs\win-tf2.1\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\SeanM\Miniconda3\envs\win-tf2.1\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\SeanM\Miniconda3\envs\win-tf2.1\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.
```
"
35138,Replacement for experimental_run_tf_function after removal from tf.keras.Model.compile ,"It looks like `experimental_run_tf_function` was removed from `tf.keras.Model.compile` in this commit a few days ago: https://github.com/tensorflow/tensorflow/commit/c73c99ca3e0bacf2bca313f270bb3eae28869530#diff-de9b96ac2d81503324cbbbe21732031fR1159

In [Horovod](http://horovod.ai/), this flag / graph mode is necessary in order for `Optimizer.get_gradients()` to be called, which aggregates gradients across workers.  Since this flag has been removed, distributed training in Horovod with `tf.keras` is not working in our nightly builds.

Is there a workaround to achieve the same behavior with the latest changes on master?

Note that we cannot perform the allreduce aggregation in `apply_gradients` due to interactions with gradient clipping and loss scaling (see https://github.com/horovod/horovod/pull/1347).
"
35137,clone_model changing UpSampling2D interpolation from bilinear to nearest neighbor,"**System information**
Windows 10 (Also happens on Ubuntu 18.04)
Installed from binary
Tensorflow-GPU 2.0.0b1
CUDA 10.0 / CuDNN 7.6
Python 3.6.8
GTX 1060 6GB (also GTX 1080 8GB)

When cloning a model using the clone_model function (AND model_from_json), upsampling interpolation switches from bilinear to nearest neighbor.

Here's some simple code which replicates the issue:

```
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
import matplotlib.pyplot as plt
import numpy as np

#Make bilinear upsampling model
x = Sequential([UpSampling2D(8, interpolation = 'bilinear', input_shape = [5, 5, 3])])

#Clone model
y = clone_model(x)

#Get noise image
z = np.random.uniform(0.0, 1.0, [1, 5, 5, 3])

#X's upsampling
plt.figure(1)
plt.imshow(x.predict(z)[0])

#Y's upsampling
plt.figure(2)
plt.imshow(y.predict(z)[0])

plt.show()
```

Which outputs these two figures:
![Figure_1](https://user-images.githubusercontent.com/8140937/70866792-a3325780-1f33-11ea-99e1-53eff1a8f015.png)
![Figure_2](https://user-images.githubusercontent.com/8140937/70866791-a3325780-1f33-11ea-893c-fe95fafe8f0f.png)

"
35136,Typo on doc,"Line 194 is missing the square bracket:

- **wrong**
`Use distribution to create a linear combination of value with shape batch_size, Tq, dim]:`


- **correct**
`Use distribution to create a linear combination of value with shape [batch_size, Tq, dim]:`

Link to the line code:
https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/dense_attention.py#L194
"
35135,"dataset's map operation performance is O(N) with data shape for O(1) operations, can be orders of magnitude slower than using a dataset from generator.","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 18.04, Windows 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
dataset.map takes 10 times longer when getting 10 times bigger tensors in the dataset, regardless of the function being passed. e.g. `data.map(lambda x: x[0, 0])` would take 100 times longer, if `data` contained tensors of shape `(1000, 1000)` rather than ` (100, 100)`.

**Describe the expected behavior**
I would expect that if the mapped function has an O(1) performance, mapping it on any dataset would still have O(1) performance not O(N).

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
timeit.timeit(""""""
tf.reduce_sum([x for x in data.map(lambda x: x[:10, :10]).repeat().take(1000)])
"""""",
setup=""""""
import tensorflow as tf
tf.autograph.set_verbosity(0, False)
x = tf.ones((1, 10000,1000))
data = tf.data.Dataset.from_tensor_slices([x])
"""""", number=10)
```
takes 10 times longer than 

```
timeit.timeit(""""""
tf.reduce_sum([x for x in data.map(lambda x: x[:10, :10]).repeat().take(1000)])
"""""",
setup=""""""
import tensorflow as tf
tf.autograph.set_verbosity(0, False)
x = tf.ones((1, 1000,1000))
data = tf.data.Dataset.from_tensor_slices([x])
"""""", number=10)
```

Where the only difference is x having 10 times the values in the first example.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35134,how to use different convolutional filter for each data in a minibatch,"Hi,
I want to use a different convolutional filter for each data in a minibatch, that's said:
if the input images have shape: [batch_size, height, width, channel], and I want to use different filters for each image in the minibatch, so the filter shape is: [batch_size, f_width, f_height, in_channel, out_channel].  So each image in the minibatch corresponds to different filters. 

I try to solve it use tf.nn.depthwise_conv2d, but I cannot guarantee it is used in correct way. So I want to know there is some API designed for this situation?

If my statement is unclear, here is a link to the question: https://stackoverflow.com/questions/42068999/tensorflow-convolutions-with-different-filter-for-each-sample-in-the-mini-batch"
35133,Installing tensorflow go failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): n/a
- TensorFlow version: n/a
- Python version: n/a
- Installed using virtualenv? pip? conda?: n/a
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**
Followed https://www.tensorflow.org/install/lang_go
but got this:
go get github.com/tensorflow/tensorflow/tensorflow/go
package github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core: cannot find package ""github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core"" in any of:


go get -d github.com/tensorflow/tensorflow/tensorflow/go
yielded the same result.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35132,Why this simple code causes RAM memory leak? tf.shuffle seems not release memory.,"**System information**
- OS Platform and Distribution Linux Ubuntu 16.04
- Python: 2.7.17 / 3.7.5
- Tensorflow: 1.12.0 / 1.15.0
- Numpy: 1.16.5
- GPU: GeForce RTX 2080 Ti
- CUDA: 9.2

**Describe the current behavior**
CPU memory gradually increase after each epoch until the program restarts, i suspect that dataset.shuffle doesn't release the buffer memory. Tested with tf 1.15, same situation.

**Code to reproduce the issue**
```
import gc
import numpy as np
import tensorflow as tf
def datagenerator():
    for i in range(250):
        for j in range(100):
            yield 'a','b',np.random.randn(300,120)
def f(s1,s2,feat):
    return s1,s2,feat
dataset = tf.data.Dataset.from_generator(datagenerator, (tf.string,tf.string,tf.float32))
dataset = dataset.shuffle(20000)
dataset = dataset.batch(200, drop_remainder=True)
dataset = dataset.map(f)
test_iter = dataset.make_initializable_iterator()
test_next = test_iter.get_next()   

run_config = tf.ConfigProto()
run_config.gpu_options.allow_growth = True
with tf.Session(config=run_config) as sess:

    for i in range(100):

        sess.run(test_iter.initializer)
        
        while True:
            try:
                loss_list = sess.run([test_next])
                print(len(loss_list[0]))
            except tf.errors.OutOfRangeError:
                print(""train epoch %d finish"" % (i+1))
                break
        gc.collect()
```

"
35130,mnist data download failure,"When I use 
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""/tmp/mnist/"", one_hot = True)

It seems that I cannot download the gz files from the googleapis? Error as below
urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:749)>

Then I manually visit the source site, also error xml replied as below:
<Error>
<Code>AccessDenied</Code>
<Message>Access denied.</Message>
<Details>
Anonymous caller does not have storage.objects.get access to cvdf-datasets/mnist.
</Details>
</Error>

So how can I fix the problem? Thanks a lot."
35127,"TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key — when trying to do dictionary mapping inside Dataset.map() function.","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOSx 10.14.5
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 2.0.0
- **Python version**: 3.7.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: try to run the Dataset.map(test) function shown below on a Dataset in the formal of a tuple of two tensors (tensor, tensor).

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
The Dataset.map() function will not allow for the use of dictionary mapping inside of it. It allows for addition/subtraction just fine, but you can't manipulate values inside of a tensor by mapping them to other values. I also tried to use the tf.map_fn() instead of doing it directly, which also returned the same error shown by the stack trace. 

### Source code / logs

Source code: 

The first get_mapped() function returns tuple of tensors in a Dataset object. Then, when trying to map to with the test() function, the error is produced. 

      `def test(self, t1, t2):
             d = {0:1}
        
             t2 = d[t2]

             return t1, t2

         def redo(self): 
             mapped = self.get_mapped()
             return mapped.map(self.test)`


Stack Trace:

     `---------------------------------------------------------------------------
      TypeError                                 Traceback (most recent call last)
      <ipython-input-123-b629a3a02152> in <module>
     ----> 1 a = q.redo()
           2 
           3 for i in a.take(1):
           4     print(i)
           5 

     <ipython-input-121-502779dfc991> in redo(self)
         147     def redo(self):
         148         mapped = self.get_mapped()
      -> 149         return mapped.map(self.test)
         150 
         151 

     tensorflow_core/python/data/ops/dataset_ops.py in map(self, map_func, num_parallel_calls)
        1209     """"""
        1210     if num_parallel_calls is None:
     -> 1211       return MapDataset(self, map_func, preserve_cardinality=True)
        1212     else:
        1213       return ParallelMapDataset(

     tensorflow_core/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func, 
     use_inter_op_parallelism, preserve_cardinality, use_legacy_function)
        3414         self._transformation_name(),
        3415         dataset=input_dataset,
     -> 3416         use_legacy_function=use_legacy_function)
        3417     variant_tensor = gen_dataset_ops.map_dataset(
        3418         input_dataset._variant_tensor,  # pylint: disable=protected-access

     tensorflow_core/python/data/ops/dataset_ops.py in __init__(self, func, transformation_name, 
     dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, 
     use_legacy_function, defun_kwargs)
       2693       resource_tracker = tracking.ResourceTracker()
        2694       with tracking.resource_tracker_scope(resource_tracker):
     -> 2695         self._function = wrapper_fn._get_concrete_function_internal()
        2696         if add_to_graph:
        2697           self._function.add_to_graph(ops.get_default_graph())

     tensorflow_core/python/eager/function.py in _get_concrete_function_internal(self, *args, 
     **kwargs)
        1852     """"""Bypasses error checking when getting a graph function.""""""
        1853     graph_function = self._get_concrete_function_internal_garbage_collected(
     -> 1854         *args, **kwargs)
        1855     # We're returning this concrete function to someone, and they may keep a
        1856     # reference to the FuncGraph without keeping a reference to the

     tensorflow_core/python/eager/function.py in . 
      _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
        1846     if self.input_signature:
        1847       args, kwargs = None, None
     -> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)
        1849     return graph_function
        1850 

     tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
        2148         graph_function = self._function_cache.primary.get(cache_key, None)
        2149         if graph_function is None:
     -> 2150           graph_function = self._create_graph_function(args, kwargs)
        2151           self._function_cache.primary[cache_key] = graph_function
        2152         return graph_function, args, kwargs

     tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, 
     override_flat_arg_shapes)
        2039             arg_names=arg_names,
        2040             override_flat_arg_shapes=override_flat_arg_shapes,
     -> 2041             capture_by_value=self._capture_by_value),
        2042         self._function_attributes,
        2043         # Tell the ConcreteFunction to clean up its graph once it goes out of

     tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, 
     python_func, args, kwargs, signature, func_graph, autograph, autograph_options, 
     add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, 
     override_flat_arg_shapes)
         913                                           converted_func)
         914 
     --> 915       func_outputs = python_func(*func_args, **func_kwargs)
          916 
         917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

     tensorflow_core/python/data/ops/dataset_ops.py in wrapper_fn(*args)
        2687           attributes=defun_kwargs)
        2688       def wrapper_fn(*args):  # pylint: disable=missing-docstring
     -> 2689         ret = _wrapper_helper(*args)
        2690         ret = structure.to_tensor_list(self._output_structure, ret)
        2691         return [ops.convert_to_tensor(t) for t in ret]

     tensorflow_core/python/data/ops/dataset_ops.py in _wrapper_helper(*args)
        2632         nested_args = (nested_args,)
        2633 
     -> 2634       ret = autograph.tf_convert(func, ag_ctx)(*nested_args)
        2635       # If `func` returns a list of tensors, `nest.flatten()` and
        2636       # `ops.convert_to_tensor()` would conspire to attempt to stack

     tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)
         235       except Exception as e:  # pylint:disable=broad-except
         236         if hasattr(e, 'ag_error_metadata'):
     --> 237           raise e.ag_error_metadata.to_exception(e)
         238         else:
         239           raise

     TypeError: in converted code:

         <ipython-input-121-502779dfc991>:145 test  *
             t2 = d[t2]
          venvtf2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:713 __hash__
             raise TypeError(""Tensor is unhashable if Tensor equality is enabled. ""

         TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use . 
    tensor.experimental_ref() as the key.`
"
35124,Poor performance of model.fit and/or model.predict in TF 2.1.0-rc1,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux Kubuntu 18.04, kernel 5.0
- Mobile device: Not verified on mobile devices
- TensorFlow installed from: binary via `pip install tensorflow-gpu`
- TensorFlow version: `2.1.0-rc1`, however affected are also `2.1.0-rc0`, `2.0.0`, `2.0.0-rc2`, `2.0.0-rc1` and `2.0.0-rc0`.
- Python version: 3.6.9
- CUDA version: 10.1 for TF 2.1.0-rc0; 10.0 for the earlier versions of TF
- cuDNN version: 7
- GPU model and memory: Nvidia GeForce GTX 1050 Ti (4GB)
- CPU model: AMD Ryzen 7 1700

**Describe the current behavior**

The code attached below works about 10x slower with TF `2.1.0-rc1` and `2.1.0-rc0` than with `1.14.0`. This code was posted earlier by @ipsec in the issue #33030 in connection with memory leaks in TF `2.0.0`. Now, in `2.1.0-rc*` the memory leaks seem fixed, but the execution of the code in question exhibits poor speed performance. Said that, the performance was even worse in `2.0.0` and pre-releases `2.0.0-rc*`, but the about 10x slowdown in `2.1.0-rc*` compared to `1.14.0` is significant.

Here I provide some more results obtained on my device:

* With TF `1.14.0`, one ""episode"", i.e. one iteration of the `for` loop in the below code, takes ca. 8.5-9.5 seconds in the graph execution mode and ca. 7-7.5 seconds with the eager execution enabled. The memory consumption of the Python process associated with this code is ca. 600 MB.

* With TF from `2.0.0-a0` to `2.0.0-b1` I obtain very similar results.

* With TF from `2.0.0-rc0` to `2.0.0` one episode takes ca. 120 seconds. There is also a memory leak. The memory usage after the 1st episode is ca 2 GB and is growing ca 1.5 GB per episode.

* For TF `2.1.0-rc0` and `2.1.0-rc1` the memory usage is ca. 800 MB and doesn't grow significantly from episode to episode, so the memory leak is fixed. Note that this memory usage is higher than in `1.14.0` but the difference is plausible. Nevertheless, for `2.1.0-rc0` and `2.1.0-rc1` I have measured the execution time to be ca. 75 seconds per episode, so the code execution is still very slow compared to `1.14.0`.

**Describe the expected behavior**

The code attached below should be executed in `2.1.0` comparably fast to `1.14.0`. The difference in the execution time shouldn't be 10-fold.

**Code to reproduce the issue**

This code has already been posted by @ipsec in the issue #33030. Here I only add time measuremets, change verbosity details and reduce the default number of episodes.

```python
import gym
import numpy as np
import matplotlib.pylab as plt

import tensorflow as tf
from tensorflow.keras import layers

import time

env = gym.make('NChain-v0')


def q_learning_keras(env, num_episodes=10):
    # create the keras model
    model = tf.keras.Sequential()
    model.add(layers.InputLayer(batch_input_shape=(1, 5)))
    model.add(layers.Dense(10, activation='sigmoid'))
    model.add(layers.Dense(2, activation='linear'))
    model.compile(loss='mse', optimizer='adam', metrics=['mae'])
    # now execute the q learning
    y = 0.95
    eps = 0.5
    decay_factor = 0.999
    r_avg_list = []
    for i in range(num_episodes):
        s = env.reset()
        eps *= decay_factor
        #if i % 100 == 0:
        print(""Episode {} of {}"".format(i + 1, num_episodes))
        t_start = time.time()
        done = False
        r_sum = 0
        while not done:
            if np.random.random() < eps:
                a = np.random.randint(0, 2)
            else:
                a = np.argmax(model.predict(np.identity(5)[s:s + 1]))
            new_s, r, done, _ = env.step(a)
            target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))
            target_vec = model.predict(np.identity(5)[s:s + 1])[0]
            target_vec[a] = target
            model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)
            s = new_s
            r_sum += r
        print(""Elapsed time: {}"".format(time.time()-t_start))
        r_avg_list.append(r_sum / 1000)
    plt.plot(r_avg_list)
    plt.ylabel('Average reward per game')
    plt.xlabel('Number of games')
    plt.show()
    for i in range(5):
        print(""State {} - action {}"".format(i, model.predict(np.identity(5)[i:i + 1])))


if __name__ == ""__main__"":
    q_learning_keras(env) 
```


"
35122,undeclared inclusion(s) in rule '@nccl_archive//:device_lib',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux centos 6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0.0
- Python version: 
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 5.4
- CUDA/cuDNN version: 10.0.130/7.6.4
- GPU model and memory:



**Describe the problem**

```
ERROR: /home/conda/.cache/bazel/_bazel_conda/6f8a02c56728b792c78882b32cc69e39/external/nccl_archive/BUILD.bazel:53:1: undeclared inclusion(s) in rule '@nccl_archive//:device_lib':
this rule is missing dependency declarations for the following files included by 'external/nccl_archive/src/collectives/device/max_f64_reduce.cu.cc':
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/cuda_runtime.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/crt/host_config.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/builtin_types.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/device_types.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/crt/host_defines.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/driver_types.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/vector_types.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/surface_types.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/texture_types.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/library_types.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/channel_descriptor.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/cuda_runtime_api.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/cuda_device_runtime_api.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/driver_functions.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/vector_functions.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/vector_functions.hpp'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/crt/common_functions.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/crt/math_functions.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/crt/math_functions.hpp'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/cuda_surface_types.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/cuda_texture_types.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/crt/device_functions.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/crt/device_functions.hpp'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/device_atomic_functions.h'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/device_atomic_functions.hpp'
  '/usr/local/cuda-10.0/targets/x86_64-linux/include/crt/device_double_functions.h'

```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```bash
bazel build //tensorflow:libtensorflow_cc.so
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35121,lite/micro: Tensor lifetime incorrectly calculated on multiple use,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): e12ba3de80d9315b7174037081adb482689bc6d6
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): all

**Describe the problem**
 The tensor lifetime may be incorrectly calculated in MicroAllocator::FinishTensorAllocation() if the same sensor is used multiple times as inputs to different operations or is used as input/output or variable of the graph.

The relevant code section:
```c
 // Figure out when the first and last use of each tensor is.
  for (int i = (operators_->size() - 1); i >= 0; --i) {
    const auto* op = operators_->Get(i);
    for (size_t n = 0; n < op->inputs()->size(); ++n) {
      const int tensor_index = op->inputs()->Get(n);
      TensorInfo* current = &tensor_info[tensor_index];
      if ((current->last_used == -1) || (current->last_used > i)) {
        current->last_used = i;
      }
    }
    for (size_t n = 0; n < op->outputs()->size(); ++n) {
      const int tensor_index = op->outputs()->Get(n);
      TensorInfo* current = &tensor_info[tensor_index];
      if ((current->first_created == -1) || (current->first_created < i)) {
        current->first_created = i;
      }
    }
  }
```
Looks just like the condition to update a valid lifetime is accidentally inverted.
Will verify and submit PR

**Please provide the exact sequence of commands/steps when you ran into the problem**
Code review
"
35120,Want to join tensorflow,Can I join tensorflow for contributions 
35118,Expose assign_moving_average via public API,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): `master`
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
`assign_moving_average` does not have a public API (`tf.assign_moving_average`). 

**Will this change the current api? How?**
`tf.assign_moving_average` instead of `tensorflow.python.training.moving_averages.assign_moving_average`

**Who will benefit with this feature?**

We're using it in https://github.com/tensorflow/addons/pull/760, and having a public API would be super useful. 

**Any Other info.**
"
35117,lite/micro: micro_speech example:  input tensor lifetime assumption invalid,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): e12ba3de80d9315b7174037081adb482689bc6d6
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): All

**Describe the problem**
The feature provider accumulates feature slices using the input tensor in the arena as a buffer.
However the lifetime of the input buffer is only the first operation of the model.
As such the feature buffer may be overwritten when the memory is reused for tensors with different lifetimes.
This is the case with the current model and the current greedy memory planner.
As only the front of the feature buffer is currently overwritten - and the front feature slice  is  never reused this does not currently impact the example.  
However using the example as a base of more complex models would trigger this problem.

I will submit a PR to add a buffer to the feature provider.
Alternatively the model could be changed to pass the input through to the output. (To keep the tensor alive) 

**Please provide the exact sequence of commands/steps when you ran into the problem**
Code review
"
35116,Cannot build raspberry pi wheel for python 3.7,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Ubuntu 18.04.3 LTS
- TensorFlow installed from (source or binary): source
- TensorFlow version: v2.1.0-rc1

 I'm building using the docker image for a raspberry pi 3 build.



**Describe the problem**

I'm able to correctly build `tensorflow-2.1.0rc1-cp35-none-linux_armv7l.whl` using:

```
CI_DOCKER_EXTRA_PARAMS=""-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.4""     tensorflow/tools/ci_build/ci_build.sh PI-PYTHON3     tensorflow/tools/ci_build/pi/build_raspberry_pi.sh 
```
I'm trying to do the same but with the python 3.7 docker images `PI-PYTHON37` (`tensorflow/tools/ci_build/Dockerfile.pi-python37`) but it fails with:

```
ERROR: /workspace/tensorflow/lite/python/interpreter_wrapper/BUILD:8:1: C++ compilation of rule '//tensorflow/lite/python/interpreter_wrapper:numpy' failed (Exit 1)
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
In file included from bazel-out/armeabi-py2-opt/bin/external/local_config_python/python_include/Python.h:8:0,
                 from ./tensorflow/lite/python/interpreter_wrapper/numpy.h:49,
                 from tensorflow/lite/python/interpreter_wrapper/numpy.cc:17:
bazel-out/armeabi-py2-opt/bin/external/local_config_python/python_include/pyconfig.h:13:55: fatal error: arm-linux-gnueabihf/python3.5m/pyconfig.h: No such file or directory
 #  include <arm-linux-gnueabihf/python3.5m/pyconfig.h>
                                                       ^
compilation terminated.
INFO: Elapsed time: 339.221s, Critical Path: 36.02s
INFO: 6204 processes: 6204 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
cd tensorflow
git checkout v2.1.0-rc1
CI_DOCKER_EXTRA_PARAMS=""-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.4""     tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37     tensorflow/tools/ci_build/pi/build_raspberry_pi.sh
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35115,Failed to build TF2.0 with TensorRT: undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_,"

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 19.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 1.1.0- (@non-git)
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 8.3.0-6ubuntu1) 8.3.0
- CUDA/cuDNN version: 10.2/7.6.5
- GPU model and memory: 
```console
Device 0: ""GeForce GTX 980M""
  CUDA Driver Version / Runtime Version          10.2 / 10.2
  CUDA Capability Major/Minor version number:    5.2
  Total amount of global memory:                 4035 MBytes (4231331840 bytes)
  (12) Multiprocessors, (128) CUDA Cores/MP:     1536 CUDA Cores
  GPU Max Clock rate:                            1126 MHz (1.13 GHz)
```


```console
WARNING: ....../tensorflow/python/BUILD:107:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: ....../tensorflow/python/keras/api/BUILD:129:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1)
Traceback (most recent call last):
  File ""~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 776, in <module>
    main()
  File ""~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 772, in main
    lazy_loading, args.use_relative_imports)
  File ""~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 629, in create_api_files
    compat_api_versions, lazy_loading, use_relative_imports)
  File ""~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 503, in get_api_init_text
    _, attr = tf_decorator.unwrap(attr)
  File ""~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py"", line 219, in unwrap
    elif _has_tf_decorator_attr(cur):
  File ""~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py"", line 124, in _has_tf_decorator_attr
    hasattr(obj, '_tf_decorator') and
  File ""~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py"", line 62, in __getattr__
    module = self._load()
  File ""~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py"", line 45, in _load
    module = importlib.import_module(self.__name__)
  File ""/usr/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py"", line 28, in <module>
    _wrap_py_utils = swig_import_helper()
  File ""~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_wrap_py_utils', fp, pathname, description)
  File ""/usr/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
  File ""<frozen importlib._bootstrap>"", line 696, in _load
  File ""<frozen importlib._bootstrap>"", line 670, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 583, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 1043, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: ~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: ....../tensorflow/python/tools/BUILD:98:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1)
INFO: Elapsed time: 3.987s, Critical Path: 2.78s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
```


"
35114,STM32F7-Disco Hello World example fails to build,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
NAME=""Ubuntu""
VERSION=""18.04.3 LTS""
ID=ubuntu
PRETTY_NAME=""Ubuntu 18.04.3 LTS""
VERSION_ID=""18.04""
Python versionL 2.7.15+
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): STM32F7-Disco and also tried on K64F based on ARM-M4

**Describe the problem**
I am attempting to build the Hello World example for the STM32F7-Disco from the following link but it is failing to build
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world.

**Please provide the exact sequence of commands/steps when you ran into the problem**
make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=""CMSIS disco_f746ng"" generate_hello_world_mbed_project

cd tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed

mbed config root .

mbed deploy
python -c 'import fileinput, glob;
for filename in glob.glob(""mbed-os/tools/profiles/*.json""):
  for line in fileinput.input(filename, inplace=True):
    print line.replace(""\""-std=gnu++98\"""",""\""-std=c++11\"", \""-fpermissive\"""")'

mbed compile -m DISCO_F746NG -t GCC_ARM

This is the error I seen before making a change to :

`Compile [ 98.7%]: arm_mult_q15.c
[Error] arm_mult_q15.c@101,6: conflicting types for 'arm_mult_q15'
[ERROR] ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Source/BasicMathFunctions/arm_mult_q15.c:101:6: error: conflicting types for 'arm_mult_q15'
 void arm_mult_q15(
      ^~~~~~~~~~~~
In file included from ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Source/BasicMathFunctions/arm_mult_q15.c:29:0:
./mbed-os/cmsis/TARGET_CORTEX_M/arm_math.h:1924:8: note: previous declaration of 'arm_mult_q15' was here
   void arm_mult_q15(
        ^~~~~~~~~~~~

[mbed] ERROR: ""/usr/bin/python"" returned error.
       Code: 1
       Path: ""/home/pramod/tensorflow-master/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed""
       Command: ""/usr/bin/python -u /home/pramod/tensorflow-master/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/make.py -D TF_LITE_STATIC_MEMORY -t GCC_ARM -m DISCO_F746NG --source . --build ./BUILD/DISCO_F746NG/GCC_ARM""
       Tip: You could retry the last command with ""-v"" flag for verbose output
`
Any help regarding this is deeply appreciated.

Thank you"
35113,TensorFlow cannot detect my GPU,"when I run my code, I found it didn't use the GPU, and just used the CPU. 
I check the tf and tf-gpu version is same. my system info as following:

Cuda version (nvcc --version result)
```
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Sat_Aug_25_21:08:01_CDT_2018
Cuda compilation tools, release 10.0, V10.0.130
```

tf version :
```
tensorboard          1.14.0
tensorflow           1.14.0
tensorflow-estimator 1.14.0
tensorflow-gpu       1.14.0
```

other info:
```
python version: Python 3.6.8
OS version: Ubuntu 16.04.5
kernel version: 4.4.0-131-generic
```


GPU model and memory (`nvidia-smi` result)

ps: GPU 1~3 using by pytorch.
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  On   | 00000000:3D:00.0 Off |                    0 |
| N/A   33C    P0    26W / 250W |      0MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE...  On   | 00000000:42:00.0 Off |                    0 |
| N/A   60C    P0   219W / 250W |  13324MiB / 32480MiB |     99%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-PCIE...  On   | 00000000:B1:00.0 Off |                    0 |
| N/A   61C    P0   225W / 250W |  13770MiB / 32480MiB |     99%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-PCIE...  On   | 00000000:B5:00.0 Off |                    0 |
| N/A   61C    P0   222W / 250W |  13392MiB / 32480MiB |     99%      Default |
```

Installed using virtualenv? yes, use pyenv


when I use following command, and just have the cpu, not exist GPU.

```python
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())
```
![-c400](https://user-images.githubusercontent.com/43985465/70850093-79404d00-1ec1-11ea-8a94-6ef16dcf9f97.png)

"
35111,"TypeError: An op outside of the function building code is being passed a ""Graph"" tensor. In my RNNCell Test.","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): Tensorflow2.0-GPU
- Python version: python3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA10 and cudnn
- GPU model and memory: GTX1060 6G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-165-d59a3afffc48> in <module>
      3 for epoch in range(EPOCHS):
      4     for x, y in db_train:
----> 5         train_step(x, y)
      6 
      7     for test_x, test_y in db_test:

c:\users\sha\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\python\eager\def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--> 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

c:\users\sha\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\python\eager\def_function.py in _call(self, *args, **kwds)
    518         # Lifting succeeded, so variables are initialized and we can run the
    519         # stateless function.
--> 520         return self._stateless_fn(*args, **kwds)
    521     else:
    522       canon_args, canon_kwds = \

c:\users\sha\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\python\eager\function.py in __call__(self, *args, **kwargs)
   1821     """"""Calls a graph function specialized to the inputs.""""""
   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1824 
   1825   @property

c:\users\sha\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\python\eager\function.py in _filtered_call(self, args, kwargs)
   1139          if isinstance(t, (ops.Tensor,
   1140                            resource_variable_ops.BaseResourceVariable))),
-> 1141         self.captured_inputs)
   1142 
   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

c:\users\sha\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\python\eager\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1222     if executing_eagerly:
   1223       flat_outputs = forward_function.call(
-> 1224           ctx, args, cancellation_manager=cancellation_manager)
   1225     else:
   1226       gradient_name = self._delayed_rewrite_functions.register()

c:\users\sha\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\python\eager\function.py in call(self, ctx, args, cancellation_manager)
    509               inputs=args,
    510               attrs=(""executor_type"", executor_type, ""config_proto"", config),
--> 511               ctx=ctx)
    512         else:
    513           outputs = execute.execute_with_cancellation(

c:\users\sha\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     74           ""Inputs to eager execution function cannot be Keras symbolic ""
     75           ""tensors, but found {}"".format(keras_symbolic_tensors))
---> 76     raise e
     77   # pylint: enable=protected-access
     78   return tensors

c:\users\sha\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     59     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,
     60                                                op_name, inputs, attrs,
---> 61                                                num_outputs)
     62   except core._NotOkStatusException as e:
     63     if name is not None:

TypeError: An op outside of the function building code is being passed
a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: my_rnn_cell_12/simple_rnn_cell_36/ones_like:0

```
**Describe the expected behavior**
It should train fluently.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

This is the error on [Google Cloab](https://colab.research.google.com/drive/1bLig48fgBRkfihxkU7Sn3bklmVyrrSH-#scrollTo=-RS_curvbwVe)

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Befor I try experts method with @tf.function，I try model.fit()，but it also run error. After I try to find the solutions from the issues, I find that `tf.config.experimental_run_functions_eagerly(False)` can deal this error. But it run very slow. Does it have much better method to deal it?[Google Cloab](https://colab.research.google.com/drive/19waivwxYTpC6S_r5UtPItuks0nW5wufE#scrollTo=HGIY-zFjdnVJ)

Thank you!
"
35110,Error when converting my_frozen_graph.pb to .tflite Google colab,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or GitHub

 SHA if from source):

There is an error when trying to converting .pb that build from tensorflow speech recognition freeze.py file for my own custom data. 

This is the used code for conversion.
```
# Converting a GraphDef from file.
from tensorflow import lite

graph_def_file = ""conv.pb""
input_arrays = [""wav_data""]
output_arrays = [""labels_softmax""]
input_shapes = {""wav_data"" :[1,160,160,3]}
allow_custom_ops=True
converter = lite.TFLiteConverter.from_frozen_graph(
  graph_def_file, input_arrays, output_arrays, input_shapes)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)

```

**Provide the text output from tflite_convert**

```
---------------------------------------------------------------------------
ConverterError                            Traceback (most recent call last)
<ipython-input-14-d875e81402bc> in <module>()
      8 converter = lite.TFLiteConverter.from_frozen_graph(
      9   graph_def_file, input_arrays, output_arrays, input_shapes)
---> 10 tflite_model = converter.convert()
     11 open(""converted_model.tflite"", ""wb"").write(tflite_model)

2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py in convert(self)
    981           input_tensors=self._input_tensors,
    982           output_tensors=self._output_tensors,
--> 983           **converter_kwargs)
    984     else:
    985       result = _toco_convert_graph_def(

/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)
    447       input_data.SerializeToString(),
    448       debug_info_str=debug_info_str,
--> 449       enable_mlir_converter=enable_mlir_converter)
    450   return data
    451 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    198       stdout = _try_convert_to_unicode(stdout)
    199       stderr = _try_convert_to_unicode(stderr)
--> 200       raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
    201   finally:
    202     # Must manually cleanup files.

ConverterError: See console for info.
2019-12-14 06:07:40.498969: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: DecodeWav
2019-12-14 06:07:40.499086: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: AudioSpectrogram
2019-12-14 06:07:40.499112: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Mfcc
2019-12-14 06:07:40.499677: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 18 operators, 30 arrays (0 quantized)
2019-12-14 06:07:40.499947: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 18 operators, 30 arrays (0 quantized)
2019-12-14 06:07:40.501698: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 11 operators, 22 arrays (0 quantized)
2019-12-14 06:07:40.506669: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 10 operators, 21 arrays (0 quantized)
2019-12-14 06:07:40.506844: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 9 operators, 19 arrays (0 quantized)
2019-12-14 06:07:40.506961: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 9 operators, 19 arrays (0 quantized)
2019-12-14 06:07:40.507036: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 9 operators, 19 arrays (0 quantized)
2019-12-14 06:07:40.507128: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.
2019-12-14 06:07:40.507170: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 1694865
2019-12-14 06:07:40.507516: E tensorflow/lite/toco/toco_tooling.cc:481] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, RESHAPE, SOFTMAX. Here is a list of operators for which you will need custom implementations: AudioSpectrogram, DecodeWav, Mfcc.
Traceback (most recent call last):
  File ""/usr/local/bin/toco_from_protos"", line 8, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, RESHAPE, SOFTMAX. Here is a list of operators for which you will need custom implementations: AudioSpectrogram, DecodeWav, Mfcc.


```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35109,"add version information to the dll, such as tensorlow_112.dll","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): msvc2015
- CUDA/cuDNN version: 9.0
- GPU model and memory: NVIDIA GTX 1080TI   X 11G



**Describe the problem**

I can build the tensorflow_cc.dll and use the tensorflow_C++ with GPU.
The problem is that I want to add the version information to the .dll File so that I can use two different version of tensorflow (tensorflow_CC_112.dll and tensorflow_CC_14.dll) in a single .exe.



"
35108,tf.py_function is unusable in map function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7
- CUDA/cuDNN version: 10.0/7
- GPU model and memory: Tesla T4, 16gb


**Describe the current behavior**
Attempting to use `tf.py_function` inside a `tf.data.Dataset.map`. Failure occurs because the  tensor returned from py_function is accessed during build time of the graph, but function is expected to be evaluated during runtime of the graph. 

**Attempt 1**
```
import tensorflow as tf
import numpy as np
#make magical data that requires python
def make_data(i): 
    return np.cast[np.uint8](i) * np.ones([20,256,256,3], dtype=np.float32) / 10.

#clean up magical data
@tf.function 
def make_clean_data(i): 
    ones = tf.py_function(make_data,[i],tf.float32) 
    ones = tf.reshape(ones,ones.get_shape()) 
    ones =tf.image.resize(ones,[224,224]) 
    return ones

ds = tf.data.Dataset.range(10)
ds.map(make_clean_data)
```
Error: 
```
ValueError: in converted code:

    <ipython-input-13-6dd8c09d760a>:8 make_clean_data  *
        ones = tf.reshape(ones,ones.get_shape())
    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:131 reshape
        result = gen_array_ops.reshape(tensor, shape, name)
    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py:8117 reshape
        ""Reshape"", tensor=tensor, shape=shape, name=name)
    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py:545 _apply_op_helper
        (input_name, err))

    ValueError: Tried to convert 'shape' to a tensor and failed. Error: Cannot convert a partially known TensorShape to a Tensor: <unknown>
```

**Attempt 2**
```
import tensorflow as tf
import numpy as np
#make magical data that requires python
def make_data(i): 
    return np.cast[np.uint8](i) * np.ones([20,256,256,3], dtype=np.float32) / 10.

#clean up magical data
@tf.function 
def make_clean_data(i): 
    ones = tf.py_function(make_data,[i],tf.float32) 
    ones.set_shape(ones.get_shape()) 
    ones =tf.image.resize(ones,[224,224]) 
    return ones

ds = tf.data.Dataset.range(10)
ds.map(make_clean_data)
```
Error
```
ValueError: in converted code:

    <ipython-input-14-311bd0ebc814>:9 make_clean_data  *
        ones =tf.image.resize(ones,[224,224])
    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1319 resize_images_v2
        skip_resize_if_same=False)
    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1034 _resize_images_common
        raise ValueError('\'images\' contains no shape.')

    ValueError: 'images' contains no shape.
```
**Attempt 3**
```
import tensorflow as tf
import numpy as np
#make magical data that requires python
def make_data(i): 
    data = np.cast[np.uint8](i) * np.zeros([20,256,256,3], dtype=np.float32) / 10.
    return  data, data.shape

#clean up magical data
@tf.function 
def make_clean_data(i): 
    ones, shape = tf.py_function(make_data,[i],(tf.float32, tf.int32)) 
    ones.set_shape(shape) 
    ones =tf.image.resize(ones,[224,224]) 
    return ones

ds = tf.data.Dataset.range(10)
ds.map(make_clean_data)
```
Error
```
OperatorNotAllowedInGraphError: in converted code:

    <ipython-input-11-a2af44684940>:9 make_clean_data  *
        ones.set_shape(shape)
    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:626 set_shape
        shape = tensor_shape.TensorShape(shape)
    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py:776 __init__
        self._dims = [as_dimension(d) for d in dims_iter]
    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py:776 <listcomp>
        self._dims = [as_dimension(d) for d in dims_iter]
    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:547 __iter__
        self._disallow_iteration()
    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:540 _disallow_iteration
        self._disallow_when_autograph_enabled(""iterating over `tf.Tensor`"")
    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:518 _disallow_when_autograph_enabled
        "" decorating it directly with @tf.function."".format(task))

    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.
```
**Attempt 4**
```
import tensorflow as tf
import numpy as np
#make magical data that requires python
def make_data(i): 
    data = np.cast[np.uint8](i) * np.zeros([20,256,256,3], dtype=np.float32) / 10.
    return  data, data.shape

#clean up magical data
@tf.function 
def make_clean_data(i): 
    ones, shape = tf.py_function(make_data,[i],(tf.float32, tf.int32)) 
    ones = tf.reshape(ones, shape) 
    ones =tf.image.resize(ones,[224,224]) 
    return ones

ds = tf.data.Dataset.range(10)
ds.map(make_clean_data)
```
Error
```
ValueError: in converted code:

    <ipython-input-16-e967bbec561a>:13 make_clean_data  *
        ones =tf.image.resize(ones,[224,224])
    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1319 resize_images_v2
        skip_resize_if_same=False)
    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1034 _resize_images_common
        raise ValueError('\'images\' contains no shape.')

    ValueError: 'images' contains no shape.
```
**Attempt 5**
```
import tensorflow as tf
import numpy as np
#make magical data that requires python
def make_data(i): 
    data = np.cast[np.uint8](i) * np.zeros([20,256,256,3], dtype=np.float32) / 10.
    return  data, data.shape

#clean up magical data
@tf.function 
def make_clean_data(ones, shape): 
    ones.set_shape(shape) 
    ones =tf.image.resize(ones,[224,224]) 
    return ones

ds = tf.data.Dataset.range(10)
ds = ds.map(lambda i:  tf.py_function(make_data,[i],(tf.float32, tf.int32)))
ds.map(make_clean_data)
```
Error
```
OperatorNotAllowedInGraphError: in converted code:

    <ipython-input-12-6bfeded17ffe>:8 make_clean_data  *
        ones.set_shape(shape)
    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:626 set_shape
        shape = tensor_shape.TensorShape(shape)
    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py:776 __init__
        self._dims = [as_dimension(d) for d in dims_iter]
    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py:776 <listcomp>
        self._dims = [as_dimension(d) for d in dims_iter]
    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:547 __iter__
        self._disallow_iteration()
    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:540 _disallow_iteration
        self._disallow_when_autograph_enabled(""iterating over `tf.Tensor`"")
    /home/hollowgalaxy/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:518 _disallow_when_autograph_enabled
        "" decorating it directly with @tf.function."".format(task))

    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.
```"
35107,tf.keras.layers.BatchNormalization() may not work in tf=2.0 and eager model is disable,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 in Docker
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38
- Python version: 3.5
- CUDA/cuDNN version: 10.0 / 7
- GPU model and memory: GTX 1080Ti / 11175MiB

**Describe the current behavior**

Hi authors and developers,

I am developing our project in tf=2.0.0 and eager_mode is disable.

The main reason is tf=1.x will not be maintained but third party libraries have not been ready for tf=2.0 yet.

This issues is a separate issues from [#35050](https://github.com/tensorflow/tensorflow/issues/35050#issuecomment-565395512)

This potential issue is somethine wrong if users do custom training with level API which includes `tf.keras.layers.BatchNormalization()` in tf=2.0 and eager model is disable.

I summary the testcaset as the following:

```python
#%%
import tensorflow as tf
tf.compat.v1.disable_eager_execution()
#tf.compat.v1.disable_v2_behavior()

import numpy as np

batch_size = 100

def download_data():

    # get raw data
    (trainX, trainY), (testX, testY) = tf.keras.datasets.cifar10.load_data()
    trainX = trainX.astype(np.float32)
    testX  = testX.astype(np.float32)

    # ont-hot
    trainY = tf.keras.utils.to_categorical(trainY, 10)
    testY  = tf.keras.utils.to_categorical(testY , 10)

    # get validation sets
    training_size = 45000
    validX = trainX[training_size:,:]
    validY = trainY[training_size:,:]

    trainX = trainX[:training_size,:]
    trainY = trainY[:training_size,:]

    return trainX, trainY, validX, validY, testX, testY

def data_pipeline(dataX, dataY):

        dataset = tf.data.Dataset.from_tensor_slices( (dataX, dataY) )
        dataset = dataset.shuffle(batch_size * 8)
        dataset = dataset.repeat()
        dataset = dataset.batch(batch_size)
        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
        return dataset

class custom_model():
    def __init__(self):

        def Acc():
            acc = tf.keras.metrics.categorical_accuracy(label_ref, clf_out)
            return tf.math.reduce_mean(acc)

        def c_loss():
            loss = tf.keras.losses.categorical_crossentropy(label_ref, clf_out)
            loss = tf.math.reduce_mean(loss)
            return loss

        # create model
        clf_input = tf.keras.layers.Input(shape=(32,32,3), name=""model/input"")
        model = tf.keras.applications.resnet_v2.ResNet50V2(include_top=True, weights=None, input_tensor=clf_input, pooling='max', classes=10)
        #model = tf.keras.applications.vgg16.VGG16(include_top=True, weights=None, input_tensor=clf_input, pooling='max', classes=10)
        model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])

        label_ref = tf.keras.layers.Input(shape=(10,) , name='label_ref')
        clf_out = model(clf_input)

        # using tf.keras.optimizers.Nadam would get error
        #optimizer = tf.keras.optimizers.Nadam(lr=0.0005)
        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)
        self.train_op = optimizer.minimize(c_loss(), var_list=[model.trainable_variables])

        self.clf_model = model
        self.clf_input = clf_input
        self.label_ref = label_ref
        self.op_acc = Acc()
        self.c_loss = c_loss()

if __name__ == '__main__':

    # set GPU
    import os
    if os.environ.get(""CUDA_VISIBLE_DEVICES"") is None:
        os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""

    # reset tf session
    tf.compat.v1.keras.backend.clear_session()
    gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)
    sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))
    tf.compat.v1.keras.backend.set_session(sess) 

    # prepare data
    trainX, trainY, validX, validY, testX, testY = download_data()
    train_gen = data_pipeline(trainX, trainY)
    valid_gen = data_pipeline(validX, validY)
    test_gen = data_pipeline(testX, testY)

    # build targeted model
    model = tf.keras.applications.resnet_v2.ResNet50V2(include_top=True, weights=None, input_shape=(32,32,3), pooling='max', classes=10)
    #model = tf.keras.applications.vgg16.VGG16(include_top=True, weights=None, input_shape=(32,32,3), pooling=None, classes=10)
    model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])

    # fit and evalutate
    model.fit(train_gen,
            steps_per_epoch = trainY.shape[0] // batch_size,
            validation_data = valid_gen,
            validation_steps= validY.shape[0] // batch_size,
            epochs=5,
            verbose=2)
    model.evaluate(testX, testY, verbose=2, batch_size=batch_size)

    # create a new model
    print('Make sure that we create a new model.')
    model = custom_model()
    sess.run(tf.compat.v1.global_variables_initializer())
    model.clf_model.evaluate(testX, testY, verbose=2, batch_size=batch_size)

    # train model
    num_epoch = 5
    total_len = trainY.shape[0] // batch_size
    tf_iter = tf.compat.v1.data.make_initializable_iterator(train_gen)
    tf_next = tf_iter.get_next()
    sess.run(tf_iter.initializer)
    for epoch in range(num_epoch):
        c_loss, acc = 0.0, 0.0
        for ii in range(total_len):
            X, Y = sess.run(tf_next)
            [b_c_loss, b_acc, _] = sess.run([model.c_loss, model.op_acc, model.train_op],
                                                feed_dict={ model.clf_input: X,
                                                            model.label_ref: Y,
                                                            tf.keras.backend.learning_phase(): 1})
            c_loss = c_loss + b_c_loss
            acc = acc + b_acc
        
        c_loss = c_loss / total_len
        acc = acc / total_len
        print('[Training]Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss, acc) )

    print('Show loss and accuracy with keras API')
    model.clf_model.evaluate(trainX, trainY, verbose=2, batch_size=batch_size)
    model.clf_model.evaluate(validX, validY, verbose=2, batch_size=batch_size)
    model.clf_model.evaluate(testX, testY, verbose=2, batch_size=batch_size)

    print('Show loss and accuracy with low level API')
    # evaluate
    num_epoch = 1
    total_len = validY.shape[0] // batch_size
    tf_iter = tf.compat.v1.data.make_initializable_iterator(valid_gen)
    tf_next = tf_iter.get_next()
    sess.run(tf_iter.initializer)
    for epoch in range(num_epoch):
        c_loss_t, acc_t, c_loss_f, acc_f = 0.0, 0.0, 0.0, 0.0
        for ii in range(total_len):
            X, Y = sess.run(tf_next)
            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],
                                        feed_dict={ model.clf_input: X,
                                                    model.label_ref: Y,
                                                    tf.keras.backend.learning_phase(): 1})
            c_loss_t = c_loss_t + b_c_loss
            acc_t = acc_t + b_acc

            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],
                                        feed_dict={ model.clf_input: X,
                                                    model.label_ref: Y,
                                                    tf.keras.backend.learning_phase(): 0})
            c_loss_f = c_loss_f + b_c_loss
            acc_f = acc_f + b_acc

        c_loss_t = c_loss_t / total_len
        c_loss_f = c_loss_f / total_len
        acc_t = acc_t / total_len
        acc_f = acc_f / total_len
        print('[Validation][learning_phase=1] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_t, acc_t) )
        print('[Validation][learning_phase=0] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_f, acc_f) )

    # evaluate
    num_epoch = 1
    total_len = testY.shape[0] // batch_size
    tf_iter = tf.compat.v1.data.make_initializable_iterator(test_gen)
    tf_next = tf_iter.get_next()
    sess.run(tf_iter.initializer)
    for epoch in range(num_epoch):
        c_loss_t, acc_t, c_loss_f, acc_f = 0.0, 0.0, 0.0, 0.0
        for ii in range(total_len):
            X, Y = sess.run(tf_next)
            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],
                                        feed_dict={ model.clf_input: X,
                                                    model.label_ref: Y,
                                                    tf.keras.backend.learning_phase(): 1})
            c_loss_t = c_loss_t + b_c_loss
            acc_t = acc_t + b_acc

            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],
                                        feed_dict={ model.clf_input: X,
                                                    model.label_ref: Y,
                                                    tf.keras.backend.learning_phase(): 0})
            c_loss_f = c_loss_f + b_c_loss
            acc_f = acc_f + b_acc

        c_loss_t = c_loss_t / total_len
        c_loss_f = c_loss_f / total_len
        acc_t = acc_t / total_len
        acc_f = acc_f / total_len
        print('[Testing][learning_phase=1] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_t, acc_t) )
        print('[Testing][learning_phase=0] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_f, acc_f) )

```

The first part of testing case is training model with high leval API and the result is as expected.
```
450/450 - 39s - loss: 1.9658 - accuracy: 0.2993 - val_loss: 1.7215 - val_accuracy: 0.3738
Epoch 2/5
450/450 - 28s - loss: 1.5722 - accuracy: 0.4334 - val_loss: 1.5897 - val_accuracy: 0.4152
Epoch 3/5
450/450 - 27s - loss: 1.3876 - accuracy: 0.4993 - val_loss: 1.4867 - val_accuracy: 0.4770
Epoch 4/5
450/450 - 28s - loss: 1.2564 - accuracy: 0.5477 - val_loss: 1.3498 - val_accuracy: 0.5060
Epoch 5/5
450/450 - 27s - loss: 1.1488 - accuracy: 0.5888 - val_loss: 1.3380 - val_accuracy: 0.5232
10000/10000 - 3s - loss: 1.3523 - accuracy: 0.5289
```

I got a strange loss and the ourput can be seen the following:
```
Make sure that we create a new model.
10000/10000 - 3s - loss: 10.2004 - accuracy: 0.1048
[Training]Epoch: 1/5 - loss: 2.288 - acc: 0.268
[Training]Epoch: 2/5 - loss: 1.513 - acc: 0.448
[Training]Epoch: 3/5 - loss: 1.285 - acc: 0.537
[Training]Epoch: 4/5 - loss: 1.426 - acc: 0.487
[Training]Epoch: 5/5 - loss: 1.306 - acc: 0.535
Show loss and accuracy with keras API
45000/45000 - 9s - loss: nan - accuracy: 0.1002
5000/5000 - 1s - loss: nan - accuracy: 0.0986
10000/10000 - 2s - loss: nan - accuracy: 0.1000
Show loss and accuracy with low level API
[Validation][learning_phase=1] Epoch: 1/1 - loss: 1.163 - acc: 0.585
[Validation][learning_phase=0] Epoch: 1/1 - loss: nan - acc: 0.099
[Testing][learning_phase=1] Epoch: 1/1 - loss: 1.179 - acc: 0.587
[Testing][learning_phase=0] Epoch: 1/1 - loss: nan - acc: 0.100
```

Obviously, after training custom model with low level API, the result would be wrong when setting `tf.keras.backend.learning_phase(): 0`

Also, the result from keras API is wrong too.

`tf.keras.backend.learning_phase(): 0` may affect the behavior of `tf.keras.layers.BatchNormalization()` but I'm not sure whether this is root cause.

I have tried a small custom model without `tf.keras.layers.BatchNormalization()` for MNIST dataset and the result is normal.

The testcase for MNIST as shown in the following:

```python
import tensorflow as tf
tf.compat.v1.disable_eager_execution()
#tf.compat.v1.disable_v2_behavior()

import numpy as np

batch_size = 100

def download_data():

    # get raw data
    (trainX, trainY), (testX, testY) = tf.keras.datasets.mnist.load_data()
    trainX = trainX.astype(np.float32)
    testX  = testX.astype(np.float32)

    # ont-hot
    trainY = tf.keras.utils.to_categorical(trainY, 10)
    testY  = tf.keras.utils.to_categorical(testY , 10)

    # get validation sets
    training_size = 55000
    validX = trainX[training_size:,:]
    validY = trainY[training_size:,:]

    trainX = trainX[:training_size,:]
    trainY = trainY[:training_size,:]

    # expand dimesion
    trainX = np.expand_dims(trainX, axis=3)
    validX = np.expand_dims(validX, axis=3)
    testX  = np.expand_dims(testX , axis=3)

    return trainX, trainY, validX, validY, testX, testY

def data_pipeline(dataX, dataY):

        dataset = tf.data.Dataset.from_tensor_slices( (dataX, dataY) )
        dataset = dataset.shuffle(batch_size * 8)
        dataset = dataset.repeat()
        dataset = dataset.batch(batch_size)
        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
        return dataset

class custom_model():
    def __init__(self):

        def Acc():
            acc = tf.keras.metrics.categorical_accuracy(label_ref, clf_out)
            return tf.math.reduce_mean(acc)

        def c_loss():
            loss = tf.keras.losses.categorical_crossentropy(label_ref, clf_out)
            loss = tf.math.reduce_mean(loss)
            return loss

        # declare variables
        self.init_op = tf.compat.v1.keras.initializers.he_normal()
        model_layers = [ tf.keras.layers.Conv2D(16, (3, 3), padding=""same"", activation=""relu"", kernel_initializer=self.init_op, name=""clf/c1""),
                         tf.keras.layers.Conv2D(32, (3, 3), padding=""same"", activation=""relu"", kernel_initializer=self.init_op, name=""clf/c2""),
                         tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=""clf/p1""),
                         tf.keras.layers.Conv2D(32, (3, 3), padding=""same"", activation=""relu"", kernel_initializer=self.init_op, name=""clf/c3""),
                         tf.keras.layers.Conv2D(64, (3, 3), padding=""same"", activation=""relu"", kernel_initializer=self.init_op, name=""clf/c4""),
                         tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=""clf/p2""),
                         tf.keras.layers.Flatten(name=""clf/f1""),
                         tf.keras.layers.Dense(256, activation=""relu"", kernel_initializer=self.init_op, name=""clf/d1""),
                         tf.keras.layers.Dense(10 , activation=None  , kernel_initializer=self.init_op, name=""clf/d2""),
                         tf.keras.layers.Activation('softmax', name=""clf/a1"")
                        ]

        # clf_model
        clf_input = tf.keras.layers.Input(shape=(28,28,1 ), name=""model/input"")
        clf_out   = clf_input
        for ii in model_layers:
            clf_out = ii(clf_out)
        clf_model = tf.keras.models.Model(inputs=clf_input, outputs=clf_out, name='clf_model')
        clf_model.compile(loss='categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])


        label_ref = tf.keras.layers.Input(shape=(10,) , name='label_ref')
        clf_out = clf_model(clf_input)

        # using tf.keras.optimizers.Nadam would get error
        #optimizer = tf.keras.optimizers.Nadam(lr=0.0005)
        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)
        self.train_op = optimizer.minimize(c_loss(), var_list=[clf_model.trainable_variables])

        self.clf_model = clf_model
        self.clf_input = clf_input
        self.label_ref = label_ref
        self.op_acc = Acc()
        self.c_loss = c_loss()

if __name__ == '__main__':

    # set GPU
    import os
    if os.environ.get(""CUDA_VISIBLE_DEVICES"") is None:
        os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""

    # reset tf session
    tf.compat.v1.keras.backend.clear_session()
    gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)
    sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))
    tf.compat.v1.keras.backend.set_session(sess) 

    # prepare data
    trainX, trainY, validX, validY, testX, testY = download_data()
    train_gen = data_pipeline(trainX, trainY)
    valid_gen = data_pipeline(validX, validY)
    test_gen = data_pipeline(testX, testY)

    # create a new model
    print('Make sure that we create a new model.')
    model = custom_model()
    sess.run(tf.compat.v1.global_variables_initializer())
    model.clf_model.evaluate(testX, testY, verbose=2, batch_size=batch_size)

    # train model
    num_epoch = 5
    total_len = trainY.shape[0] // batch_size
    tf_iter = tf.compat.v1.data.make_initializable_iterator(train_gen)
    tf_next = tf_iter.get_next()
    sess.run(tf_iter.initializer)
    for epoch in range(num_epoch):
        c_loss, acc = 0.0, 0.0
        for ii in range(total_len):
            X, Y = sess.run(tf_next)
            [b_c_loss, b_acc, _] = sess.run([model.c_loss, model.op_acc, model.train_op],
                                                feed_dict={ model.clf_input: X,
                                                            model.label_ref: Y,
                                                            tf.keras.backend.learning_phase(): 1})
            c_loss = c_loss + b_c_loss
            acc = acc + b_acc
        
        c_loss = c_loss / total_len
        acc = acc / total_len
        print('[Training]Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss, acc) )

    print('Show loss and accuracy with keras API')
    model.clf_model.evaluate(trainX, trainY, verbose=2, batch_size=batch_size)
    model.clf_model.evaluate(validX, validY, verbose=2, batch_size=batch_size)
    model.clf_model.evaluate(testX, testY, verbose=2, batch_size=batch_size)

    print('Show loss and accuracy with low level API')
    # evaluate
    num_epoch = 1
    total_len = validY.shape[0] // batch_size
    tf_iter = tf.compat.v1.data.make_initializable_iterator(valid_gen)
    tf_next = tf_iter.get_next()
    sess.run(tf_iter.initializer)
    for epoch in range(num_epoch):
        c_loss_t, acc_t, c_loss_f, acc_f = 0.0, 0.0, 0.0, 0.0
        for ii in range(total_len):
            X, Y = sess.run(tf_next)
            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],
                                        feed_dict={ model.clf_input: X,
                                                    model.label_ref: Y,
                                                    tf.keras.backend.learning_phase(): 1})
            c_loss_t = c_loss_t + b_c_loss
            acc_t = acc_t + b_acc

            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],
                                        feed_dict={ model.clf_input: X,
                                                    model.label_ref: Y,
                                                    tf.keras.backend.learning_phase(): 0})
            c_loss_f = c_loss_f + b_c_loss
            acc_f = acc_f + b_acc

        c_loss_t = c_loss_t / total_len
        c_loss_f = c_loss_f / total_len
        acc_t = acc_t / total_len
        acc_f = acc_f / total_len
        print('[Validation][learning_phase=1] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_t, acc_t) )
        print('[Validation][learning_phase=0] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_f, acc_f) )

    # evaluate
    num_epoch = 1
    total_len = testY.shape[0] // batch_size
    tf_iter = tf.compat.v1.data.make_initializable_iterator(test_gen)
    tf_next = tf_iter.get_next()
    sess.run(tf_iter.initializer)
    for epoch in range(num_epoch):
        c_loss_t, acc_t, c_loss_f, acc_f = 0.0, 0.0, 0.0, 0.0
        for ii in range(total_len):
            X, Y = sess.run(tf_next)
            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],
                                        feed_dict={ model.clf_input: X,
                                                    model.label_ref: Y,
                                                    tf.keras.backend.learning_phase(): 1})
            c_loss_t = c_loss_t + b_c_loss
            acc_t = acc_t + b_acc

            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],
                                        feed_dict={ model.clf_input: X,
                                                    model.label_ref: Y,
                                                    tf.keras.backend.learning_phase(): 0})
            c_loss_f = c_loss_f + b_c_loss
            acc_f = acc_f + b_acc

        c_loss_t = c_loss_t / total_len
        c_loss_f = c_loss_f / total_len
        acc_t = acc_t / total_len
        acc_f = acc_f / total_len
        print('[Testing][learning_phase=1] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_t, acc_t) )
        print('[Testing][learning_phase=0] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_f, acc_f) )
```

Definitely, we got a very normal output:

```
Make sure that we create a new model.
10000/10000 - 1s - loss: 398.0696 - acc: 0.1151
[Training]Epoch: 1/5 - loss: 11.997 - acc: 0.558
[Training]Epoch: 2/5 - loss: 0.474 - acc: 0.849
[Training]Epoch: 3/5 - loss: 0.282 - acc: 0.914
[Training]Epoch: 4/5 - loss: 0.213 - acc: 0.935
[Training]Epoch: 5/5 - loss: 0.181 - acc: 0.945
Show loss and accuracy with keras API
55000/55000 - 1s - loss: 0.1555 - acc: 0.9535
5000/5000 - 0s - loss: 0.1501 - acc: 0.9584
10000/10000 - 0s - loss: 0.1687 - acc: 0.9539
Show loss and accuracy with low level API
[Validation][learning_phase=1] Epoch: 1/1 - loss: 0.150 - acc: 0.958
[Validation][learning_phase=0] Epoch: 1/1 - loss: 0.150 - acc: 0.958
[Testing][learning_phase=1] Epoch: 1/1 - loss: 0.169 - acc: 0.954
[Testing][learning_phase=0] Epoch: 1/1 - loss: 0.169 - acc: 0.954
```

**Describe the expected behavior**

It should work properly.

**Code to reproduce the issue**

Please see the section of **Describe the current behavior**

**Other info / logs**

skip ..."
35106,can dev a lua version,"so many game dev in lua,can't find the lua version of tensorflow"
35105,Tensorboard Embedding Projector Custom Search Bug,"Context: I am using the online version of the [Tensorboard Embedding Projector](https://projector.tensorflow.org/). I uploaded my own checkpoint and metadata TSVs with 681909 rows each. I clicked on the ""CUSTOM"" tab, and then found a bug when I tried to search. 

Current Behavior/Bug: I am unable to click on the search bar that says ""Search by"". When I do click anywhere in the search bar, a drop down appears with the option ""label"". If I click label and try to search again, there is no difference. There is no error message.

Expected Behavior: I would be able to search, or be given an error message indicating why I'm unable to."
35103,Async functions cause tf_upgrade_v2 to crash,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro Linux (kernel 4.19)
- TensorFlow installed from (source or binary): binary? I'm not sure, I used `pip install tensorflow`
- TensorFlow version: 2.0.0
- Python version: Python 3.6.8
- Installed using virtualenv? pip? conda?: pip. This is in a virtualenv environment though.

**Describe the problem**
Running `tf_upgrade_v2` on a file that has async functions causes it to crash. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```tf_upgrade_v2 --infile ./image_classification/test_tf.py --outfile ./image_classification_v2/test_tf.py
Traceback (most recent call last):
  File ""/home/alex/git-repos/bic-bot-py/bin/tf_upgrade_v2"", line 8, in <module>
    sys.exit(main())
  File ""/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/tensorflow_core/tools/compatibility/tf_upgrade_v2_main.py"", line 139, in main
    args.input_file, output_file, upgrade)
  File ""/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/tensorflow_core/tools/compatibility/tf_upgrade_v2_main.py"", line 40, in process_file
    upgrader.process_file(in_filename, out_filename)
  File ""/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/tensorflow_core/tools/compatibility/ast_edits.py"", line 900, in process_file
    temp_file)
  File ""/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/tensorflow_core/tools/compatibility/ast_edits.py"", line 960, in process_opened_file
    self.update_string_pasta("""".join(lines), in_filename))
  File ""/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/tensorflow_core/tools/compatibility/ast_edits.py"", line 916, in update_string_pasta
    t = pasta.parse(text)
  File ""/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/__init__.py"", line 25, in parse
    annotator.visit(t)
  File ""/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py"", line 1201, in visit
    super(AstAnnotator, self).visit(node)
  File ""/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py"", line 133, in visit
    super(BaseVisitor, self).visit(node)
  File ""/home/alex/.pyenv/versions/3.6.8/lib/python3.6/ast.py"", line 253, in visit
    return visitor(node)
  File ""/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py"", line 47, in wrapped
    f(self, node, *args, **kwargs)
  File ""/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py"", line 225, in visit_Module
    self.generic_visit(node)
  File ""/home/alex/.pyenv/versions/3.6.8/lib/python3.6/ast.py"", line 261, in generic_visit
    self.visit(item)
  File ""/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py"", line 1201, in visit
    super(AstAnnotator, self).visit(node)
  File ""/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py"", line 133, in visit
    super(BaseVisitor, self).visit(node)
  File ""/home/alex/.pyenv/versions/3.6.8/lib/python3.6/ast.py"", line 253, in visit
    return visitor(node)
  File ""/home/alex/.pyenv/versions/3.6.8/lib/python3.6/ast.py"", line 261, in generic_visit
    self.visit(item)
  File ""/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py"", line 1201, in visit
    super(AstAnnotator, self).visit(node)
  File ""/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py"", line 133, in visit
    super(BaseVisitor, self).visit(node)
  File ""/home/alex/.pyenv/versions/3.6.8/lib/python3.6/ast.py"", line 253, in visit
    return visitor(node)
  File ""/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py"", line 47, in wrapped
    f(self, node, *args, **kwargs)
  File ""/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py"", line 673, in visit_Return
    self.token('return')
  File ""/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py"", line 1340, in token
    token_val, token.src, token.start[0], token.line))
pasta.base.annotate.AnnotationError: Expected 'return' but found 'async'
line 1: async def f():
```

**Any other info / logs**
test_tf.py:
```
async def f():
    return
```"
35102,Adam implementation differs from paper (applies bias B_2 correction to \epsilon),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: 3.7.3
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: V10.0.130
- GPU model and memory: Tesla V100-SXM2

**Describe the current behavior**
In the Adam paper, we subtract the following quantity from our current gradient [0]:
\alpha * \hat{m_t} / (\sqrt{v_t / (1-\beta^t_2)}  + \epsilon)

<img width=""475"" alt=""Screen Shot 2019-12-13 at 2 32 58 PM"" src=""https://user-images.githubusercontent.com/54961543/70836600-7f490780-1db5-11ea-9669-27c50fe48cae.png"">

The Tensorflow implementation subtracts a subtly different quantity ([1]):
\alpha * \hat{m_t} * \sqrt{1-\beta^T_2} / (\sqrt{v_t} + \epsilon)

<img width=""438"" alt=""Screen Shot 2019-12-13 at 2 34 36 PM"" src=""https://user-images.githubusercontent.com/54961543/70836654-c1724900-1db5-11ea-9260-5fc0678f6f39.png"">

The difference between the two expressions is that in the first, we de-bias only the moving average of the squared gradient, v_t. In the second, this bias correction is also applied to \epsilon. This manifests as scaling up epilson quite a lot in very early training steps, reducing the magnitude of the gradient update.

Note that the same bug was present in PyTorch prior to v1.3. It was fixed in this PR: https://github.com/pytorch/pytorch/pull/22628. That PR description provides a useful visualization.

**Describe the expected behavior**
Implement the algorithm as described in the paper. If the old implementation is necessary to preserve back-compat, providing a flag to trigger the correct implementation would be most helpful.


[0] https://arxiv.org/pdf/1412.6980.pdf, see final 2 lines of Algorithm 1
[1] Notable lines in TF implementation regarding this issue: 
https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/adam.py#L162

https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/adam.py#L245

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc#L373

"
35101,AutoGraph could not transform <bound method TopLevelFeature.decode_example of FeaturesDict,"**System information**
- OS Platform and Distribution: Arch Linux, 5.4.2-arch1-1-ARCH
- TensorFlow installed from: binary
- TensorFlow version: 2.1.0rc0-1
- Keras version: 2.2.4-tf
- Python version: 3.8
- GPU model and memory: 2x GTX 1080 Ti 11GB""`

**Describe the current behavior**
executing Tensorflow's MNIST handwriting example produces warning:

> WARNING:tensorflow:AutoGraph could not transform <bound method TopLevelFeature.decode_example of FeaturesDict({
>     'image': Image(shape=(28, 28, 1), dtype=tf.uint8),
>     'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),
> })> and will run it as-is.
> Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
> Cause: 'arguments' object has no attribute 'defaults'


**Code to reproduce the issue**
 import tensorflow as tf
  import tensorflow_datasets as tfds
  
  from tensorflow.keras.optimizers import Adam
  
  def build_model():
      filters = 48
      units = 24
      kernel_size = 7
      learning_rate = 1e-4
      model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(units, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
      ])
      model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])
      return model
  
  datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)
  mnist_train, mnist_test = datasets['train'], datasets['test']
  
  num_train_examples = info.splits['train'].num_examples
  num_test_examples = info.splits['test'].num_examples
  
  BUFFER_SIZE = 10000
  BATCH_SIZE = 32
  
  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label
  
  train_dataset = mnist_train.map(scale).shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
  eval_dataset = mnist_test.map(scale).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
  
model = build_model()
  
  epochs=2
  model.fit(
          train_dataset,
          validation_data=eval_dataset,
          steps_per_epoch=num_train_examples/epochs,
          validation_steps=num_test_examples/epochs,
          epochs=epochs)"
35100,Error occurred when finalizing GeneratorDataset iterator,"**System information**
- OS Platform and Distribution: Arch Linux, 5.4.2-arch1-1-ARCH
- TensorFlow installed from: binary
- TensorFlow version: 2.1.0rc0-1
- Keras version: 2.2.4-tf
- Python version: 3.8
- GPU model and memory: 2x GTX 1080 Ti 11GB""`

**Describe the current behavior**
executing Tensorflow's MNIST handwriting example produces error:
the error dissapears if the code doesn't use OneDeviceStrategy or MirroredStrategy

> W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled


**Code to reproduce the issue**


 ```
 import tensorflow as tf
  import tensorflow_datasets as tfds
  import time
  
  from tensorflow.keras.optimizers import Adam
  
  def build_model():
      filters = 48
      units = 24
      kernel_size = 7
      learning_rate = 1e-4
      model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(units, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
      ])
      model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])
      return model
  
  datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)
  mnist_train, mnist_test = datasets['train'], datasets['test']
  
  num_train_examples = info.splits['train'].num_examples
  num_test_examples = info.splits['test'].num_examples
  
  strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')
  
  BUFFER_SIZE = 10000
  BATCH_SIZE = 32
  
  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label
  
  train_dataset = mnist_train.map(scale).shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
  eval_dataset = mnist_test.map(scale).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
  
  with strategy.scope():
    model = build_model()
  
  epochs=5
  start = time.perf_counter()
  model.fit(
          train_dataset,
          validation_data=eval_dataset,
          steps_per_epoch=num_train_examples/epochs,
          validation_steps=num_test_examples/epochs,
          epochs=epochs)
  elapsed = time.perf_counter() - start
  print('elapsed: {:0.3f}'.format(elapsed))
```


"
35099,Inconsistency with the shapes for `reduce_sum` and `reduce_logsumexp` for vectorized_map in graph mode,"I don't understand the cause of different handling of shapes for two methods, here is the snippet:

```python
import tensorflow as tf
import tensorflow_probability as tfp

def _log_prob(x):
    x = tf.convert_to_tensor(x, name='x')
    distribution_log_probs = [x for i in range(5)]
    cat_log_probs = [x for i in range(5)]
    final_log_probs = [
        cat_lp + d_lp
        for (cat_lp, d_lp) in zip(cat_log_probs, distribution_log_probs)
    ]
    concat_log_probs = tf.stack(final_log_probs, 0)
    log_sum = tf.reduce_logsumexp(concat_log_probs, axis=[0])
    # log_sum = tf.reduce_sum(concat_log_probs, axis=[0])
    return log_sum

@tf.function(autograph=False)
def f():
    log_prob = tf.vectorized_map(_log_prob, tf.ones((1,5)))
    print(log_prob.shape) # prints (None, 5) for `tf.reduce_logsumexp` and (1, 5) for `tf.reduce_sum`
```

So basically `tf.reduce_logsumexp` gives dynamic shape for the output tensor while `tf.reduce_sum` assigns static shape. Can anybody please give some clear picture on such behaviour and is it expected?

```
tf: 2.0.0
tfp: 0.8.0
```"
35096,Tensorflow lite C++ standalone projects from 'generate_projects' missing sparkfun headers?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.14.6
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.15.0 (same observation for version 2.0), from https://github.com/tensorflow/tensorflow/releases/tag/v1.15.0
- Python version: python3
- GCC/Compiler version (if compiling from source): arm-none-eabi-g++

**Describe the problem**
The standalone projects generated with Tensorflow Lite seem to be missing some headers.

Following this tutorial: 
[https://www.tensorflow.org/lite/microcontrollers/library#generate_projects_for_other_platforms
](https://www.tensorflow.org/lite/microcontrollers/library#generate_projects_for_other_platforms
)

I run the following command (the link seems to be off here in the tutorial, as 'micro' is in another subfolder' experimental'):
`gmake -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=sparkfun_edge generate_projects`

After this command is finished, the files are generated as expected in:
`tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge_cortex-m4/prj`

But when trying to build the micro_speech binary like this:
```
cd tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge_cortex-m4/prj/micro_speech/make
gmake
```

I am getting the following error:
```
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DPART_apollo3 -DAM_PACKAGE_BGA -DAM_PART_APOLLO3 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DNDEBUG -DTF_LITE_MCU_DEBUG_LOG -D __FPU_PRESENT=1 -DARM_MATH_CM4 -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m4 -mthumb -mfpu=fpv4-sp-d16 -mfloat-abi=hard -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -ggdb -O3 -I. -I./third_party/gemmlowp -I./third_party/flatbuffers/include -I./third_party/kissfft  -c tensorflow/lite/experimental/micro/sparkfun_edge/debug_log.cc -o tensorflow/lite/experimental/micro/sparkfun_edge/debug_log.o
tensorflow/lite/experimental/micro/sparkfun_edge/debug_log.cc:22:10: fatal error: am_bsp.h: No such file or directory
 #include ""am_bsp.h""   // NOLINT
          ^~~~~~~~~~
compilation terminated.
make: *** [tensorflow/lite/experimental/micro/sparkfun_edge/debug_log.o] Error 1
```

This is the header file of the sparkfun edge board that I want to build this for.
"
35095,MirroredStrategy compared to OneDeviceStrategy slower ,"**System information**
- OS Platform and Distribution: Arch Linux, 5.4.2-arch1-1-ARCH
- TensorFlow installed from: binary
- TensorFlow version: 2.1.0rc0-1
- Keras version: 2.2.4-tf
- Python version: 3.8
- GPU model and memory: 2x GTX 1080 Ti 11GB""`

**Describe the current behavior**
executing Tensorflow's MNIST handwriting example with MirroredStrategy using two GPUs is slower (286 sec) than OneDeviceStrategy (197 sec) 

**Describe the expected behavior**
using MirroredStrategy should be faster or at least fast as OneDeviceStrategy

**Cod e to reproduce the issue**
hype rparameters were tuned for MirroredStrategy using Keras Tuner

```
  import tensorflow as tf
  import tensorflow_datasets as tfds
  import time
  
  from tensorflow.keras.optimizers import Adam
  
  def build_model():
      filters = 48
      units = 24
      kernel_size = 7
      learning_rate = 1e-4
      model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(units, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
      ])
      model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])
      return model
  
  datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)
  mnist_train, mnist_test = datasets['train'], datasets['test']
  
  num_train_examples = info.splits['train'].num_examples
  num_test_examples = info.splits['test'].num_examples
  
  strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')
  #strategy = tf.distribute.MirroredStrategy()
  print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
  
  BUFFER_SIZE = 10000
  BATCH_SIZE = 32
  
  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label
  
  train_dataset = mnist_train.map(scale).shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
  eval_dataset = mnist_test.map(scale).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
  
  with strategy.scope():
    model = build_model()
  
  epochs=5
  start = time.perf_counter()
  model.fit(
          train_dataset,
          validation_data=eval_dataset,
          steps_per_epoch=num_train_examples/epochs,
          validation_steps=num_test_examples/epochs,
          epochs=epochs)
  elapsed = time.perf_counter() - start
  print('elapsed: {:0.3f}'.format(elapsed))
```


**Other info / logs**
**OneDeviceStrategy**:
> ..
> 12000/12000 [==============================] - 39s 3ms/step - loss: 0.0089 - accuracy: 0.9978 - val_loss: 0.0385 - val_accuracy: 0.9892
> elapsed: 197.177

**MirroredStrategy**:
> ...
> 12000/12000 [==============================] - 56s 5ms/step - loss: 0.0098 - accuracy: 0.9977 - val_loss: 0.0390 - val_accuracy: 0.9884
> elapsed: 286.224"
35093,Custom loss may not work when running keras model with tf.distribute.MirroredStrategy(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 in Docker
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38
- Python version: 3.5
- CUDA/cuDNN version: 10.0 / 7
- GPU model and memory: GTX 1080Ti / 11175MiB

**Describe the current behavior**

Hi authors and developers,

I am developing our project in tf=2.0.0 and eager_mode is disable.

The main reason is tf=1.x will not be maintained but third party libraries have not been ready for tf=2.0 yet.

I got bug when I was running custom loss keras model with tf.distribute.MirroredStrategy()

This bug can be reproduced by the following minimal testcase:

```python
#%%
from distutils.version import LooseVersion
import numpy as np
import tensorflow as tf

# disable eager model for tf=2.x
tf.compat.v1.disable_eager_execution()

batch_size = 100
img_h = 32
img_w = 32
img_min = 0
img_max = 1
channels = 3
num_classes = 10

strategy = tf.distribute.MirroredStrategy()
#%%
def download_data():

    # get raw data
    (trainX, trainY), (testX, testY) = tf.keras.datasets.cifar10.load_data()
    trainX = trainX.astype(np.float32)
    testX  = testX.astype(np.float32)

    # ont-hot
    trainY = tf.keras.utils.to_categorical(trainY, 10)
    testY  = tf.keras.utils.to_categorical(testY , 10)

    # get validation sets
    training_size = 45000
    validX = trainX[training_size:,:]
    validY = trainY[training_size:,:]

    trainX = trainX[:training_size,:]
    trainY = trainY[:training_size,:]

    return trainX, trainY, validX, validY, testX, testY

#%%
class DataGenerator:

    def __init__(self, sess, dataX, dataY, total_len, batch_size):

        super().__init__()

        self.total_len  = total_len
        self.batch_size = batch_size
        self.cleanX = dataX
        self.totalY = dataY
        self.sess = sess
        self.on_epoch_end()

    def __build_pipeline(self, dataX, dataY):

        # create dataset API
        def preprocess_fn(dataX, dataY):
            
            dataX = tf.image.random_flip_left_right(dataX)

            # workaround solution
            if LooseVersion(tf.__version__) < LooseVersion('1.14.0'):
                outputX = dataX
            else:
                outputX = (dataX, dataY)
            return outputX, dataY

        dataset = tf.data.Dataset.from_tensor_slices( (dataX, dataY) )
        dataset = dataset.shuffle(batch_size * 8)
        dataset = dataset.repeat()
        dataset = dataset.batch(batch_size)
        dataset = dataset.map(preprocess_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)
        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)

        self.dataset   = dataset

    def  __len__(self):

        return self.total_len // self.batch_size

    def on_epoch_end(self):

        # run permutation
        rand_idx = np.random.permutation(self.total_len)
        cleanX = self.cleanX[rand_idx]
        totalY = self.totalY[rand_idx]

        self.__build_pipeline(cleanX, totalY)

#%%
# ref: https://keras.io/examples/cifar10_resnet/
def build_clf():
    #with strategy.scope():
    with tf.compat.v1.variable_scope('optimizer'):
        def resnet_layer(inputs,
                        num_filters=16,
                        kernel_size=3,
                        strides=1,
                        activation='relu',
                        batch_normalization=True,
                        conv_first=True):
            """"""2D Convolution-Batch Normalization-Activation stack builder

            # Arguments
                inputs (tensor): input tensor from input image or previous layer
                num_filters (int): Conv2D number of filters
                kernel_size (int): Conv2D square kernel dimensions
                strides (int): Conv2D square stride dimensions
                activation (string): activation name
                batch_normalization (bool): whether to include batch normalization
                conv_first (bool): conv-bn-activation (True) or
                    bn-activation-conv (False)

            # Returns
                x (tensor): tensor as input to the next layer
            """"""
            conv = tf.keras.layers.Conv2D(num_filters,
                        kernel_size=kernel_size,
                        strides=strides,
                        padding='same',
                        kernel_initializer='he_normal',
                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))

            x = inputs
            if conv_first:
                x = conv(x)
                if batch_normalization:
                    x = tf.keras.layers.BatchNormalization()(x)
                if activation is not None:
                    x = tf.keras.layers.Activation(activation)(x)
            else:
                if batch_normalization:
                    x = tf.keras.layers.BatchNormalization()(x)
                if activation is not None:
                    x = tf.keras.layers.Activation(activation)(x)
                x = conv(x)
            return x

        def cw_loss(y_true, y_pred):
            label_mask  = label_ref
            pre_softmax = x
            if LooseVersion(tf.__version__) < LooseVersion('1.14.0'):
                correct_logit = tf.reduce_sum(label_mask * pre_softmax, axis=1, keep_dims=True)
            else:
                correct_logit = tf.reduce_sum(label_mask * pre_softmax, axis=1, keepdims=True)
            distance = tf.nn.relu( pre_softmax - correct_logit + (1-label_mask) * 10)
            inactivate = tf.cast( tf.less_equal(distance, 1e-9), dtype=tf.float32)
            weight = tf.keras.layers.Activation('softmax')(-1e9*inactivate + distance)
            loss = tf.reduce_sum((1-label_mask) * distance * weight, axis=1)
            loss = tf.math.reduce_mean(loss)
            return loss

        # set model's parameters (depth = n * 6 + 2)
        n = 8
        num_filters = 16

        clf_input = tf.keras.layers.Input(shape=(img_h, img_w, channels), name=""model/input"")
        label_ref = tf.keras.layers.Input(shape=(num_classes,), name='label_ref')
        input_list = [clf_input, label_ref]

        x = resnet_layer(inputs=clf_input)
        for stack in range(3):
            for res_block in range(n):
                strides = 1
                if stack > 0 and res_block == 0:  # first layer but not first stack
                    strides = 2  # downsample
                y = resnet_layer(inputs=x,
                                num_filters=num_filters,
                                strides=strides)
                y = resnet_layer(inputs=y,
                                num_filters=num_filters,
                                activation=None)
                if stack > 0 and res_block == 0:  # first layer but not first stack
                    # linear projection residual shortcut connection to match
                    # changed dims
                    x = resnet_layer(inputs=x,
                                    num_filters=num_filters,
                                    kernel_size=1,
                                    strides=strides,
                                    activation=None,
                                    batch_normalization=False)
                x = tf.keras.layers.Add()([x, y])
                x = tf.keras.layers.Activation('relu')(x)
            num_filters *= 2

        x = tf.keras.layers.AveragePooling2D(pool_size=8)(x)
        x = tf.keras.layers.Flatten()(x)
        x = tf.keras.layers.Dense(num_classes , kernel_initializer='he_normal', activation=None)(x)
        y = tf.keras.layers.Activation('softmax')(x)

        optimizer = tf.keras.optimizers.Adam(lr=0.001)
        clf_model = tf.keras.models.Model(inputs=input_list, outputs=y, name='clf_model')
        clf_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', cw_loss])
    clf_model.summary()

    return clf_model

#%%
if __name__ == '__main__':

    # set GPU
    import os
    if os.environ.get(""CUDA_VISIBLE_DEVICES"") is None:
        os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""

    # reset tf session
    tf.compat.v1.keras.backend.clear_session()
    gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)
    sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))
    tf.compat.v1.keras.backend.set_session(sess)

    # Hyperparameters
    batch_size = 100
    epochs = 1

    # prepare data
    trainX, trainY, validX, validY, testX, testY = download_data()
    train_gen = DataGenerator(sess, trainX, trainY, trainY.shape[0], batch_size)
    valid_gen = DataGenerator(sess, validX, validY, validY.shape[0], batch_size)
    test_gen  = DataGenerator(sess, testX, testY, testY.shape[0], batch_size)

    # build model
    model = build_clf()

    # train model
    model.fit(train_gen.dataset,
                    epochs=epochs,
                    steps_per_epoch = train_gen.__len__(),
                    validation_data=valid_gen.dataset,
                    validation_steps= valid_gen.__len__(),
                    verbose=1)

    # print result
    meta_string = '[Testing]'
    prefix_string = ''
    output = model.evaluate(test_gen.dataset, steps = test_gen.__len__())
    for ii in range( len( model.metrics_names) ):
        meta_string = meta_string + '- {:s}{:s}: {:.3f} '.format(prefix_string, model.metrics_names[ii], output[ii])

    print(meta_string)
```

First, this testing case looks good without enabling `tf.distribute.MirroredStrategy()`

There is the output for normal case:

```
Train on 450 steps, validate on 50 steps
2019-12-13 16:20:30.625379: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-12-13 16:20:31.217430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-12-13 16:20:33.007150: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
450/450 [==============================] - 40s 88ms/step - loss: 1.8299 - accuracy: 0.4744 - cw_loss: 9.5022 - val_loss: 1.9870 - val_accuracy: 0.4528 - val_cw_loss: 9.6570
100/100 [==============================] - 3s 26ms/step - loss: 2.0089 - accuracy: 0.4511 - cw_loss: 9.6708
[Testing]- loss: 2.009 - accuracy: 0.451 - cw_loss: 9.671

```

Next, we tried to enable `tf.distribute.MirroredStrategy()` so we modified the testcase by the following patch:

```diff
def build_clf():
-    #with strategy.scope():
-    with tf.compat.v1.variable_scope('optimizer'):
def build_clf():
+    with strategy.scope():
+    #with tf.compat.v1.variable_scope('optimizer'):
```

And we got the error message:

```
Traceback (most recent call last):
  File ""bug.py"", line 233, in <module>
    verbose=1)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py"", line 717, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py"", line 685, in fit
    steps_name='steps_per_epoch')
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py"", line 299, in model_iteration
    batch_outs = f(actual_inputs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/backend.py"", line 3580, in __call__
    run_metadata=self.run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/client/session.py"", line 1472, in __call__
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: You must feed a value for placeholder tensor 'model/input' with dtype float and shape [?,32,32,3]
         [[{{node model/input}}]]
         [[batch_normalization_9/cond/else/_325/FusedBatchNormV3/ReadVariableOp/_2529]]
  (1) Invalid argument: You must feed a value for placeholder tensor 'model/input' with dtype float and shape [?,32,32,3]
         [[{{node model/input}}]]
0 successful operations.
0 derived errors ignored.
```

This error is very similar to the previous issue #34866.

I guess that those two issues may have some strong connection.

**Describe the expected behavior**

It should work properly.

**Code to reproduce the issue**

Please see the section of **Describe the current behavior**

**Other info / logs**

The following message is the result generated by `tf_env_collect.sh`
```
== check python ===================================================
python version: 3.5.2
python branch:
python build version: ('default', 'Oct  8 2019 13:06:37')
python compiler version: GCC 5.4.0 20160609
python implementation: CPython


== check os platform ===============================================
os: Linux
os kernel version: #40~18.04.1-Ubuntu SMP Thu Nov 14 12:06:39 UTC 2019
os release version: 5.0.0-37-generic
os platform: Linux-5.0.0-37-generic-x86_64-with-Ubuntu-16.04-xenial
linux distribution: ('Ubuntu', '16.04', 'xenial')
linux os distribution: ('Ubuntu', '16.04', 'xenial')
mac version: ('', ('', '', ''), '')
uname: uname_result(system='Linux', node='f7f509f1dacf', release='5.0.0-37-generic', version='#40~18.04.1-Ubuntu SMP Thu Nov 14 12:06:39 UTC 2019', machine='x86_64', processor='x86_64')
architecture: ('64bit', 'ELF')
machine: x86_64


== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== check pips ===================================================
numpy                  1.17.4
protobuf               3.11.1
tensorflow-estimator   2.0.1
tensorflow-gpu         2.0.0
tensorflow-probability 0.8.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 2.0.0
tf.version.GIT_VERSION = v2.0.0-rc2-26-g64c3d38
tf.version.COMPILER_VERSION = 7.3.1 20180303
Sanity check: array([1], dtype=int32)
       443:     find library=libpthread.so.0 [0]; searching
       443:      search path=/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64          (LD_LIBRARY_PATH)
       443:       trying file=/usr/local/nvidia/lib/tls/x86_64/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib/tls/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib/x86_64/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib64/tls/x86_64/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib64/tls/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib64/x86_64/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib64/libpthread.so.0
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libpthread.so.0
       443:
       443:     find library=libc.so.6 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libc.so.6
       443:
       443:     find library=libdl.so.2 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libdl.so.2
       443:
       443:     find library=libutil.so.1 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libutil.so.1
       443:
       443:     find library=libexpat.so.1 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libexpat.so.1
       443:
       443:     find library=libz.so.1 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libz.so.1
       443:
       443:     find library=libm.so.6 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libm.so.6
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libpthread.so.0
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libc.so.6
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libm.so.6
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libz.so.1
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libexpat.so.1
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libutil.so.1
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libdl.so.2
       443:
       443:
       443:     initialize program: /usr/local/bin/python
       443:
       443:
       443:     transferring control: /usr/local/bin/python
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_opcode.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libopenblasp-r0-34a18dc3.3.7.so [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/x86_64:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs            (RPATH from file /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/x86_64/libopenblasp-r0-34a18dc3.3.7.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/libopenblasp-r0-34a18dc3.3.7.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/x86_64/libopenblasp-r0-34a18dc3.3.7.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so
       443:
       443:     find library=libgfortran-ed201abd.so.3.0.0 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs         (RPATH from file /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_tests.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/linalg/lapack_lite.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/linalg/_umath_linalg.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libbz2.so.1.0 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libbz2.so.1.0
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libbz2.so.1.0
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_bz2.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=liblzma.so.5 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/liblzma.so.5
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/liblzma.so.5
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_lzma.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libmpdec.so.2 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/usr/lib/x86_64-linux-gnu/libmpdec.so.2
       443:
       443:
       443:     calling init: /usr/lib/x86_64-linux-gnu/libmpdec.so.2
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_decimal.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/fft/_pocketfft_internal.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/mtrand.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/common.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/bounded_integers.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/mt19937.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/bit_generator.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libcrypto.so.1.0.0 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libcrypto.so.1.0.0
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libcrypto.so.1.0.0
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_hashlib.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/philox.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/pcg64.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/sfc64.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/generator.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libtensorflow_framework.so.2 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..            (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/x86_64/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/x86_64/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/x86_64/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../x86_64/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2
       443:
       443:     find library=librt.so.1 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/librt.so.1
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../librt.so.1
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/librt.so.1
       443:
       443:     find library=libstdc++.so.6 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libstdc++.so.6
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libstdc++.so.6
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/usr/lib/x86_64-linux-gnu/libstdc++.so.6
       443:
       443:     find library=libgcc_s.so.1 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libgcc_s.so.1
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libgcc_s.so.1
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libgcc_s.so.1
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libgcc_s.so.1
       443:
       443:
       443:     calling init: /usr/lib/x86_64-linux-gnu/libstdc++.so.6
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/librt.so.1
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2
       443:
       443:     find library=libhdfs.so [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..           (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libhdfs.so
       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libhdfs.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libhdfs.so
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:      search path=/lib/x86_64-linux-gnu/tls/x86_64:/lib/x86_64-linux-gnu/tls:/lib/x86_64-linux-gnu/x86_64:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/tls/x86_64:/usr/lib/x86_64-linux-gnu/tls:/usr/lib/x86_64-linux-gnu/x86_64:/usr/lib/x86_64-linux-gnu:/lib/tls/x86_64:/lib/tls:/lib/x86_64:/lib:/usr/lib/tls/x86_64:/usr/lib/tls:/usr/lib/x86_64:/usr/lib              (system search path)
       443:       trying file=/lib/x86_64-linux-gnu/tls/x86_64/libhdfs.so
       443:       trying file=/lib/x86_64-linux-gnu/tls/libhdfs.so
       443:       trying file=/lib/x86_64-linux-gnu/x86_64/libhdfs.so
       443:       trying file=/lib/x86_64-linux-gnu/libhdfs.so
       443:       trying file=/usr/lib/x86_64-linux-gnu/tls/x86_64/libhdfs.so
       443:       trying file=/usr/lib/x86_64-linux-gnu/tls/libhdfs.so
       443:       trying file=/usr/lib/x86_64-linux-gnu/x86_64/libhdfs.so
       443:       trying file=/usr/lib/x86_64-linux-gnu/libhdfs.so
       443:       trying file=/lib/tls/x86_64/libhdfs.so
       443:       trying file=/lib/tls/libhdfs.so
       443:       trying file=/lib/x86_64/libhdfs.so
       443:       trying file=/lib/libhdfs.so
       443:       trying file=/usr/lib/tls/x86_64/libhdfs.so
       443:       trying file=/usr/lib/tls/libhdfs.so
       443:       trying file=/usr/lib/x86_64/libhdfs.so
       443:       trying file=/usr/lib/libhdfs.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_csv.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/termios.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/fast_tensor_util.so
       443:
       443:     find library=libuuid.so.1 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libuuid.so.1
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libuuid.so.1
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/wrapt/_wrappers.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_json.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libssl.so.1.0.0 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libssl.so.1.0.0
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libssl.so.1.0.0
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_ssl.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libhdf5-49599f4e.so.103.0.0 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls:/usr/local/lib/python3.5/dist-packages/h5py/.libs/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/x86_64/libhdf5-49599f4e.so.103.0.0
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/libhdf5-49599f4e.so.103.0.0
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/x86_64/libhdf5-49599f4e.so.103.0.0
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0
       443:
       443:     find library=libhdf5_hl-db841637.so.100.1.1 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs          (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1
       443:
       443:     find library=libsz-1c7dd0cf.so.2.0.1 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls:/usr/local/lib/python3.5/dist-packages/h5py/.libs/./x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/x86_64/libsz-1c7dd0cf.so.2.0.1
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/libsz-1c7dd0cf.so.2.0.1
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./x86_64/libsz-1c7dd0cf.so.2.0.1
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1
       443:
       443:     find library=libaec-2147abcd.so.0.0.4 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4
       443:
       443:     find library=libz-a147dcb0.so.1.2.3 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/defs.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_objects.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_conv.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5r.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5t.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/utils.cpython-35m-x86_64-linux-gnu.so
       443:
       452:     find library=libc.so.6 [0]; searching
       452:      search path=/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64          (LD_LIBRARY_PATH)
       452:       trying file=/usr/local/nvidia/lib/tls/x86_64/libc.so.6
       452:       trying file=/usr/local/nvidia/lib/tls/libc.so.6
       452:       trying file=/usr/local/nvidia/lib/x86_64/libc.so.6
       452:       trying file=/usr/local/nvidia/lib/libc.so.6
       452:       trying file=/usr/local/nvidia/lib64/tls/x86_64/libc.so.6
       452:       trying file=/usr/local/nvidia/lib64/tls/libc.so.6
       452:       trying file=/usr/local/nvidia/lib64/x86_64/libc.so.6
       452:       trying file=/usr/local/nvidia/lib64/libc.so.6
       452:      search cache=/etc/ld.so.cache
       452:       trying file=/lib/x86_64-linux-gnu/libc.so.6
       452:
       452:
       452:     calling init: /lib/x86_64-linux-gnu/libc.so.6
       452:
       452:
       452:     initialize program: /bin/sh
       452:
       452:
       452:     transferring control: /bin/sh
       452:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5z.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5a.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5s.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5p.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5ac.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_proxy.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5d.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5ds.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5f.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5g.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5i.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5fd.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5pl.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5o.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5l.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/_lib/_ccallback_c.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_sparsetools.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_csparsetools.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_shortest_path.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_tools.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_traversal.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_reordering.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libjpeg-3b10b538.so.9.3.0 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls:/usr/local/lib/python3.5/dist-packages/PIL/.libs/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs            (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/x86_64/libjpeg-3b10b538.so.9.3.0
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/libjpeg-3b10b538.so.9.3.0
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/x86_64/libjpeg-3b10b538.so.9.3.0
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0
       443:
       443:     find library=libopenjp2-b3d7668a.so.2.3.1 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs           (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1
       443:
       443:     find library=libtiff-8267adfe.so.5.4.0 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs           (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0
       443:
       443:     find library=liblzma-6cd627ed.so.5.2.4 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls:/usr/local/lib/python3.5/dist-packages/PIL/.libs/./x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/.            (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/x86_64/liblzma-6cd627ed.so.5.2.4
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/liblzma-6cd627ed.so.5.2.4
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./x86_64/liblzma-6cd627ed.so.5.2.4
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_nd_image.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_ni_label.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libopenblasp-r0-2ecf47d5.3.7.dev.so [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/x86_64:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs            (RPATH from file /usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/x86_64/libopenblasp-r0-2ecf47d5.3.7.dev.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/libopenblasp-r0-2ecf47d5.3.7.dev.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/x86_64/libopenblasp-r0-2ecf47d5.3.7.dev.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flapack.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flinalg.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_solve_toeplitz.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_decomp_update.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_blas.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_lapack.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so
       443:
       443:
       443:     calling fini: /usr/local/bin/python [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libutil.so.1 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libexpat.so.1 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libz.so.1 [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_opcode.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_tests.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/linalg/lapack_lite.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/linalg/_umath_linalg.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_bz2.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libbz2.so.1.0 [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_lzma.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/liblzma.so.5 [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_decimal.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/x86_64-linux-gnu/libmpdec.so.2 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/fft/_pocketfft_internal.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/mtrand.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/common.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/bounded_integers.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/mt19937.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/bit_generator.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_hashlib.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/philox.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/pcg64.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/sfc64.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/generator.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_csv.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/termios.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/fast_tensor_util.so [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libuuid.so.1 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/wrapt/_wrappers.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_json.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_ssl.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libssl.so.1.0.0 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libcrypto.so.1.0.0 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/defs.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_objects.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_conv.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5r.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5t.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/utils.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5z.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5a.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5s.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5p.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5ac.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_proxy.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5d.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5ds.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5f.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5g.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5i.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5fd.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5pl.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5o.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5l.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/_lib/_ccallback_c.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_sparsetools.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_csparsetools.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_shortest_path.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_tools.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_traversal.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_reordering.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_nd_image.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_ni_label.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flapack.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flinalg.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_solve_toeplitz.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_decomp_update.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_blas.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_lapack.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2 [0]
       443:
       443:
       443:     calling fini: /usr/lib/x86_64-linux-gnu/libstdc++.so.6 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libgcc_s.so.1 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/librt.so.1 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libm.so.6 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libdl.so.2 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libpthread.so.0 [0]
       443:

```"
35092,Keyword arguments of the optimizer for learning rate schedule,"This statement raises an error if the learning rate is some learning rate schedule class instance. It does not raise any error if specification is without keyword arguments. 

https://github.com/tensorflow/tensorflow/blob/c10aa0338f928dcc3da2c2660d88c80bcdd01278/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L259"
35090,Subsequent calls to tf.data.Dataset.map() with seeded random operation give same random sequences.,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: CentOS 7
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.14.0
- Python version: 2.7.17
- CUDA/cuDNN version: 10.0

**Describe the current behavior**
In eager mode when setting a random seed, two calls to the same mapping function in tf.data.Dataset.map() would result in two times the same generated random values.

**Describe the expected behavior**
As I haven't explicitly closed or restarted a session, I would expect both calls to output different generated random values. Is tf.data.Dataset.map() creating its own session and running the mapping function subgraph in it? If so, is there a way to force the map() to take the current session as input?

**Code to reproduce the issue**
```
import numpy as np  
import tensorflow as tf
tf.compat.v1.enable_eager_execution()

SEED = 88
tf.compat.v1.random.set_random_seed(SEED)

ds_train = tf.data.Dataset.range(0, 4)
ds_val = tf.data.Dataset.range(0, 4)
ds_train = ds_train.map(
    lambda x: tf.random.uniform([1], 0.2, 5.0)
)
ds_val = ds_val.map(
    lambda x: tf.random.uniform([1], 0.2, 5.0) 
)
for el in ds_train:
    print(el.numpy())
    # --> [0.44027787], [1.7892183], [2.8793733], [3.3438706]
for el in ds_val:
    print(el.numpy())
    # why the same here? --> [0.44027787], [1.7892183], [2.8793733], [3.3438706]
```
"
35089,no attribute 'op' in keras.losses.SparseCategoricalCrossentropy,"**System information**
- OS Platform and Distribution: Jupyter on Windows 10
- TensorFlow installed from: anaconda in venv
- TensorFlow version: 2.0.0
- Python version: 3.7

**Describe the current behavior**
I'm running the example from the [tensorflow documentation for keras.losses.SparseCategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) and get the error:
`'list' object has no attribute 'op'`
If we set `from_logits` to `True` it is working.

**Code to reproduce the issue**
```
cce = tf.keras.losses.SparseCategoricalCrossentropy()
loss = cce( [0, 1, 2], [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])
```"
35088,K.learning_phase() flag does not work in Training  ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

Yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows, Anaconda
- TensorFlow installed from (source or binary):
Source

- TensorFlow version (use command below):
2.0 and keras 2.3.1

- Python version:
3.7

- GPU model and memory:
No GPU

**Describe the current behavior**
I have written a class that will load a tf.keras model, get its config and create a new model with this configuration then using this newly init model, make predictions using dropout to create confidence intervals around the output. 

The function works around the prediction element i.e. the newly initialised model makes a prediction, but the uncertainty element outputs an array of 0's, suggesting that flag is not being passed to the dropout layers during training phase.


**Describe the expected behavior**

I expect an np.array output containing the predictions (n_samples) and the uncertainty (n_samples)


**Code to reproduce the issue**
```

# Create training/test datasets
train_data = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))
test_data = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))
forecast_data = tf.data.Dataset.from_tensor_slices((forecast_ahead_df.values))

# batching
train_batch = train_data.batch(len(X_train)) 
test_batch = test_data.batch(len(X_test))
forecast_batch = forecast_data.batch(len(forecast_ahead_df))


def predict_with_confidence_ints(self, 
                                   X: np.array,
                                   dropout_rate: float, 
                                   n_classes: int,
                                   n_iter: int):
    """"""Load saved .h5 keras model and generate predictions using dropout.
    Args: TODO
    Returns: TODO
    """"""

    # Load saved Keras model (.h5)
    model = tf.keras.models.load_model(self.saved_keras_model,
                                       custom_objects = self.custom_objects) 

    # Load the config of the original model
    conf = model.get_config()
    
    # Add the specified dropout to all layers
    for layer in conf['layers']:
      # Dropout layers
      if layer[""class_name""]==""Dropout"":
        layer[""config""][""rate""] = dropout_rate
      # Recurrent layers with Dropout
      elif ""dropout"" in layer[""config""].keys():
        layer[""config""][""dropout""] = dropout_rate

    # # Create a new model with specified dropout
    if type(model) == tf.keras.Sequential:
      # Sequential
      model_with_dropout = tf.keras.Sequential.from_config(conf)
    else:
      # Functional
      model_with_dropout = tf.keras.models.Model.from_config(conf)
 
    # Get config weights and init new model with same weights
    model_with_dropout.set_weights(model.get_weights())  
    
    # Create function that applies dropout to learning phase
    f = K.function([model_with_dropout.layers[0].input, K.learning_phase()],
               [model_with_dropout.layers[-1].output])

    result = np.zeros((n_iter,) + (x.shape[0], 1))

    for i in range(n_iter):
        result[i, :] = f((x, 1))[0]
    
    predictions_ = result.mean(axis=0)
    uncertainty_ = result.std(axis=0)
    # predictions = pd.Series(predictions_.reshape(-1), name = 'Predictions')
    # std_devs = pd.Series(uncertainty_.reshape(-1), name = 'Uncertainty')
    return predictions_, uncertainty_ 

# Create predictions with dropout

# Get custom objects
custom_objects={'root_mean_squared_error': root_mean_squared_error,
                'glorot_uniform': keras.initializers.glorot_uniform(seed=None)}

# Get saved model
model_path = 'my_saved_keras_model.h5'

# Set dropout rate
dropout = 0.3

# Convert input data to np array
X = np.array(forecast_ahead_df)

# Get predictions 
predictions_new, uncertainties_new = predict_with_confidence_ints(X, dropout, 1, 100)

```

**Other info / logs**
This function was created within a class however I have omitted the __init__ etc for brevity's sake.

Thank you!

"
35087,optimize_for_inference not working for my graph?,"It does not seem to make a difference if I use optimize_for_inference when I generate the pb and pbtxt graphs for my network. The size of the unoptimized graph is 177.775kb and the size for the optimized graph is 177.739kb.

I'm not sure wheter the optimize_for_inference Tool is correctly working for my graph. I currently cannot deploy my model in openCV, which throws an exception [EX] because the graph does not seem to be optimized. 

**I want to access my tensorflow protobuf files with the following openCV function:**
Net ub6net = readNetFromTensorflow (model, config) // PB and PBTXT

**This throws the following Exception:**
Version 1 - optimize_for_inference disabled: **[EX]** Assertion failed in function 'addConstNodes'
Version 2 - optimize_for_inference enabled: **[EX]** Assertion failed in function 'addConstNodes'

___________________________________

This exception gets thrown when the input files are not optimized for inference and it is thrown regardless of the fact that the graph has been optimized or not, indicating that no optimization was applied.

**I would be glad if someone can help me to solve this issue, as it seems to be related to the optimize_for_inference Tool.**

**My Code to generate the pb and pbtxt file:**

```
# -*- coding: utf-8 -*-
# File: export.py

import tensorflow as tf
from tensorflow.python.framework import graph_util
from tensorflow.python.platform import gfile
from tensorflow.python.tools import optimize_for_inference_lib

from ..compat import is_tfv2, tfv1
from ..input_source import PlaceholderInput
from ..tfutils.common import get_tensors_by_names, get_tf_version_tuple
from ..tfutils.tower import PredictTowerContext
from ..utils import logger

__all__ = ['ModelExporter']


class ModelExporter(object):
    """"""Export models for inference.""""""

    def __init__(self, config):
        """"""Initialise the export process.
        Args:
            config (PredictConfig): the config to use.
                The graph will be built with the tower function defined by this `PredictConfig`.
                Then the input / output names will be used to export models for inference.
        """"""
        super(ModelExporter, self).__init__()
        self.config = config

    def export_compact(self, filename, optimize=True, toco_compatible=False):
        """"""Create a self-contained inference-only graph and write final graph (in pb format) to disk.
        Args:
            filename (str): path to the output graph
            optimize (bool): whether to use TensorFlow's `optimize_for_inference`
                to prune and optimize the graph. This does not work on all types of graphs.
            toco_compatible (bool): See TensorFlow's
                `optimize_for_inference
                <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py>`_
                for details. Only available after TF 1.8.
        """"""
        if toco_compatible:
            assert optimize, ""toco_compatible is only effective when optimize=True!""
        self.graph = self.config._maybe_create_graph()
        with self.graph.as_default():
            input = PlaceholderInput()
            input.setup(self.config.input_signature)
            with PredictTowerContext(''):
                self.config.tower_func(*input.get_input_tensors())

            input_tensors = get_tensors_by_names(self.config.input_names)
            output_tensors = get_tensors_by_names(self.config.output_names)

            self.config.session_init._setup_graph()
            # we cannot use ""self.config.session_creator.create_session()"" here since it finalizes the graph
            sess = tfv1.Session(config=tfv1.ConfigProto(allow_soft_placement=True))
            self.config.session_init._run_init(sess)

            dtypes = [n.dtype for n in input_tensors]

            # freeze variables to constants
            frozen_graph_def = graph_util.convert_variables_to_constants(
                sess,
                self.graph.as_graph_def(),
                [n.name[:-2] for n in output_tensors],
                variable_names_whitelist=None,
                variable_names_blacklist=None)

            # prune unused nodes from graph
            if optimize:
                toco_args = () if get_tf_version_tuple() < (1, 8) else (toco_compatible, )
                frozen_graph_def = optimize_for_inference_lib.optimize_for_inference(
                    frozen_graph_def,
                    [n.name[:-2] for n in input_tensors],
                    [n.name[:-2] for n in output_tensors],
                    [dtype.as_datatype_enum for dtype in dtypes],
                    *toco_args)

            tf.train.write_graph(frozen_graph_def, filename, 'ub6_model.pb', as_text=False)
            logger.info(""Output graph written to {}."".format(filename))
            tf.train.write_graph(frozen_graph_def, filename, 'ub6_config.pbtxt', as_text=True)
            logger.info(""Output graph written to {}."".format(filename))
```"
35085,[DOCS] Using `>>>` in documentation is incorrectly formatted on website,"## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/constant_initializer

## Description of issue (what needs changing):
The [example code](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/ops/init_ops_v2.py#L152) uses `>>>`, but on the website, this is incorrectly converted to `&gt;&gt;&gt;`

TF docs link: https://www.tensorflow.org/api_docs/python/tf/constant_initializer

### Submit a pull request?

This is more of a problem with how documentation is generated from comments in the python file. I don't mind taking a look if someone can point out where to get started. "
35084,Memory leak when using py_function inside tf.data.Dataset,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.0
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**

<img width=""368"" alt=""屏幕快照 2019-12-13 下午6 06 20"" src=""https://user-images.githubusercontent.com/22017000/70792077-6c453000-1dd3-11ea-8489-5b6131950515.png"">


**Describe the expected behavior**

The tf.data.Dataset instance should be freed in every step.

**Code to reproduce the issue**

```python
import tensorflow as tf
import os
import numpy as np
import psutil

def _generator():
    for i in range(100):
        yield ""1,2,3,4,5,6,7,8""

def _py_parse_data(record):
    record = record.numpy()
    record = bytes.decode(record)
    rl = record.split("","")
    rl = [str(int(r) + 1) for r in rl]
    return ["","".join(rl)]

def parse_data(record, shape=10):
    sparse_data = tf.strings.split([record], sep="","")
    sparse_data = tf.strings.to_number(sparse_data[0], tf.int64)
    ids_num = tf.cast(tf.size(sparse_data), tf.int64)
    indices = tf.range(0, ids_num, dtype=tf.int64)
    indices = tf.reshape(indices, shape=(-1, 1))
    sparse_data = tf.sparse.SparseTensor(
                indices, sparse_data, dense_shape=(shape,)
    )
    return sparse_data

process = psutil.Process(os.getpid())

step = 0
while (step < 10000):
    t = tf.data.Dataset.from_generator(_generator, output_types=tf.string)
    t = t.map(lambda record: tf.py_function(_py_parse_data, [record], [tf.string]))
    t = t.map(parse_data)
    for d in t:
        a = 1
    if step % 10 == 0:
        print(""Memory : "", process.memory_info().rss)
    step += 1
```

"
35083,Error when setting up virtual devices on system that has multiple physical GPUs,"**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version: 1.15.0, eager mode
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0.130, 7.6.0.64
- GPU model and memory: V100

**Describe the current behavior**

```set_virtual_device_configuration``` throws an exception with multiple physical GPUs in eager mode and ```allow growth```.

**Describe the expected behavior**

Should be able to create virtual devices for multiple physical GPUs.

**Code to reproduce the issue**

``` python
import tensorflow as tf

tf.enable_eager_execution() #required for error
tf.debugging.set_log_device_placement(True)
config = tf.ConfigProto()
config.gpu_options.allow_growth = True #required for error
tf.Session(config=config)

# require multiple physical gpus

physical_devices = tf.config.experimental.list_physical_devices('GPU')

tf.config.experimental.set_virtual_device_configuration(
  physical_devices[0],
  [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=100),
   tf.config.experimental.VirtualDeviceConfiguration(memory_limit=100)])

tf.config.experimental.set_virtual_device_configuration(
  physical_devices[1],
  [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=100),
   tf.config.experimental.VirtualDeviceConfiguration(memory_limit=100)])

logicals = tf.config.experimental.list_logical_devices('GPU')

print(logicals)
```

**Other info / logs**

```
---------------------------------------------------------------------------
AlreadyExistsError                        Traceback (most recent call last)
<ipython-input-1-23cc43a56abd> in <module>
     21    tf.config.experimental.VirtualDeviceConfiguration(memory_limit=100)])
     22 
---> 23 logicals = tf.config.experimental.list_logical_devices('GPU')
     24 
     25 print(logicals)

~/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/config.py in list_logical_devices(device_type)
    345     List of LogicalDevice objects
    346   """"""
--> 347   return context.context().list_logical_devices(device_type=device_type)
    348 
    349 

~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/context.py in list_logical_devices(self, device_type)
   1140   def list_logical_devices(self, device_type=None):
   1141     """"""Return logical devices.""""""
-> 1142     self.ensure_initialized()
   1143 
   1144     devices = []

~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/context.py in ensure_initialized(self)
    490         if self._default_is_async == ASYNC:
    491           pywrap_tensorflow.TFE_ContextOptionsSetAsync(opts, True)
--> 492         context_handle = pywrap_tensorflow.TFE_NewContext(opts)
    493       finally:
    494         pywrap_tensorflow.TFE_DeleteContextOptions(opts)

AlreadyExistsError: TensorFlow device (GPU:1) is being mapped to multiple CUDA devices (0 now, and 1 previously), which is not supported. This may be the result of providing different GPU configurations (ConfigProto.gpu_options, for example different visible_device_list) when creating multiple Sessions in the same process. This is not  currently supported, see https://github.com/tensorflow/tensorflow/issues/19083
```
"
35081,Tensorflow Lite Data Type Error on Android.,"I have converted a super image resolution tensorflow model to tensorflow lite and have deployed it on an android application. Once I am trying to carry out Inference I am getting errors related to the data type. Please have a look at the code snippet below and the error log.

```
       try {
            tflite = new Interpreter(loadModelFile(MainActivity.this, ""model.tflite""));
            Bitmap testImage = getBitmapFromAsset(this,""Image.png"");
            TensorBuffer test_tensor = TensorBuffer.createFixedSize(new int[]{96, 96, 3}, DataType.UINT8);
            Bitmap out =Bitmap.createBitmap(384,
                         384, Bitmap.Config.ARGB_8888);
            TensorBuffer output_tensor = TensorBuffer.createFixedSize(new int[]{384, 384, 3}, DataType.UINT8);
            tflite.run(test_tensor,output_tensor);
            convertTensorBufferToBitmap(output_tensor,out);
        } catch (IOException e) {
            Toast.makeText(this,""Error"",Toast.LENGTH_LONG).show();
            e.printStackTrace();
        }

    static void convertTensorBufferToBitmap(TensorBuffer buffer, Bitmap bitmap) {
        if (buffer.getDataType() != DataType.UINT8) {
            // We will add support to FLOAT format conversion in the future, as it may need other configs.
            throw new UnsupportedOperationException(String.format(
                    ""Converting TensorBuffer of type %s to Bitmap is not supported yet."",
                    buffer.getDataType()));
        }
        int[] shape = buffer.getShape();
        if (shape.length != 3 || shape[0] <= 0 || shape[1] <= 0 || shape[2] != 3) {
            throw new IllegalArgumentException(String.format(
                    ""Buffer shape %s is not valid. 3D TensorBuffer with shape [w, h, 3] is required"",
                    Arrays.toString(shape)));
        }
        int h = shape[0];
        int w = shape[1];
        if (bitmap.getWidth() != w || bitmap.getHeight() != h) {
            throw new IllegalArgumentException(String.format(
                    ""Given bitmap has different width or height %s with the expected ones %s."",
                    Arrays.toString(new int[]{bitmap.getWidth(), bitmap.getHeight()}),
                    Arrays.toString(new int[]{w, h})));
        }
        if (!bitmap.isMutable()) {
            throw new IllegalArgumentException(""Given bitmap is not mutable"");
        }
        // TODO(b/138904567): Find a way to avoid creating multiple intermediate buffers every time.
        int[] intValues = new int[w * h];
        int[] rgbValues = buffer.getIntArray();
        for (int i = 0, j = 0; i < intValues.length; i++) {
            byte r = (byte) rgbValues[j++];
            byte g = (byte) rgbValues[j++];
            byte b = (byte) rgbValues[j++];
            intValues[i] = ((r << 16) | (g << 8) | b);
        }
        bitmap.setPixels(intValues, 0, w, 0, 0, w, h);
    }

    static void convertBitmapToTensorBuffer(Bitmap bitmap, TensorBuffer buffer) {
        int w = bitmap.getWidth();
        int h = bitmap.getHeight();
        int[] intValues = new int[w * h];
        bitmap.getPixels(intValues, 0, w, 0, 0, w, h);
        // TODO(b/138904567): Find a way to avoid creating multiple intermediate buffers every time.
        int[] rgbValues = new int[w * h * 3];
        for (int i = 0, j = 0; i < intValues.length; i++) {
            rgbValues[j++] = ((intValues[i] >> 16) & 0xFF);
            rgbValues[j++] = ((intValues[i] >> 8) & 0xFF);
            rgbValues[j++] = (intValues[i] & 0xFF);
        }
        int[] shape = new int[] {h, w, 3};
        buffer.loadArray(rgbValues, shape);
    }

```

This is a snippet of my code and I still am getting an error:
```

E/AndroidRuntime: FATAL EXCEPTION: main
    Process: com.example.edsr, PID: 14650
    java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.edsr/com.example.edsr.MainActivity}: java.lang.IllegalArgumentException: DataType error: cannot resolve DataType of org.tensorflow.lite.support.tensorbuffer.TensorBufferUint8
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2817)
        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2892)
        at android.app.ActivityThread.-wrap11(Unknown Source:0)
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1593)
        at android.os.Handler.dispatchMessage(Handler.java:105)
        at android.os.Looper.loop(Looper.java:164)
        at android.app.ActivityThread.main(ActivityThread.java:6541)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767)
     Caused by: java.lang.IllegalArgumentException: DataType error: cannot resolve DataType of org.tensorflow.lite.support.tensorbuffer.TensorBufferUint8
        at org.tensorflow.lite.Tensor.dataTypeOf(Tensor.java:255)
        at org.tensorflow.lite.Tensor.throwIfTypeIsIncompatible(Tensor.java:313)
        at org.tensorflow.lite.Tensor.getInputShapeIfDifferent(Tensor.java:218)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:135)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:296)
        at org.tensorflow.lite.Interpreter.run(Interpreter.java:259)
        at com.example.edsr.MainActivity.onCreate(MainActivity.java:97)
        at android.app.Activity.performCreate(Activity.java:6975)
        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1213)
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2770)
        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2892) 
        at android.app.ActivityThread.-wrap11(Unknown Source:0) 
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1593) 
        at android.os.Handler.dispatchMessage(Handler.java:105) 
        at android.os.Looper.loop(Looper.java:164) 
        at android.app.ActivityThread.main(ActivityThread.java:6541) 
        at java.lang.reflect.Method.invoke(Native Method) 
        at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240) 
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767) 
Process 14650 terminated.
```"
35080,Start background prefetching without calling next on dataset iterator,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Currently if you have a dataset with a large shuffle buffer, it doesn't start trying to populate that buffer until the first request to retrieve data from the data set. This is a lost opportunity for parallelism during training script startup - the shuffle buffer could be filling in the background while the model is compiling.  
Since calling next is blocking, its not possible to trigger this overlap to happen. 

**Will this change the current api? How?** Maybe - could add a start_prefetching() method.  The other idea I had was to make it implicit in calling __iter__ - but that is possibly more useful for my specific use case than more generally.

**Who will benefit with this feature?**
Anyone who cares about the startup time of their training script and uses a large shuffle buffer.

**Any Other info.**
"
35079,Add usage examples for tf.audio APIs,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/audio

## Description of issue (what needs changing):
Currently, there are no usage examples for tf.audio APIs , which makes it difficult for new users to implement the same.

### Clear description

For example, why should someone use this method? How is it useful?
**Audio is an area not really explored in machine learning to extent image and text has. While TensorFlow does provide a good amount of documentation for the general Args and Returns of the various functions under tf.audio, since most new users will have very little experience with audio as compared to tf.image**

### Correct links

Is the link to the source code correct?
**Yes**

### Parameters defined

Are all parameters defined and formatted correctly?
**Yes**

### Returns defined

Are return values defined?
**Yes**

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises
**No**

### Usage example

Is there a usage example?
**No**

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?
**Formatted code blocks are present, which are satisfactory.**

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
**Yes, I think I can provide a detailed usage example.**
  "
35076,how to convert NonMaxSuppressionV4 to older versions,"Hi, 
I am training a tensorflow object detection model and am converting it to tensorflow js. Although there is no NonMaxSuppressionV4 in the config.proto file (but has batch_non_max_suppression), I am getting the error 
ValueError: Unsupported Ops in the model before optimization
NonMaxSuppressionV4

when trying to use tensorflowjs_converter to convert the trained model into tensorflow js. Is there a way I can convert my checkpoints into saved_model that doesn't convert nonmaxsuppression to v4 (I think v3 is supported in tensorflowjs)? I am using tensorflow 1.15.0, but back tracking to tensorflow 1.13.0 has not helped either. 

I would appreciate it if someone explains what is happening, or how I can convert the model into supported tensorflow js operators (after creating tensorflowjs with --skip_op_check or before it at the time of model training or exporting it into saved or frozen model)?

Thank you in advance
"
35075,ValueError: Unable to save the object ListWrapper([ListWrapper([None])]) tensorflow 2.0,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 10/ cudnn 7.6.5
- GPU model and memory: V100

**Describe the current behavior**
When saving a model, in my case, HRNet, i met the following error: ValueError: Unable to save the object ListWrapper([ListWrapper([None])])
The error message suggest that ""If you don't need this list checkpointed, wrap it in a tf.contrib.checkpoint.NoDependency object; it will be automatically un-wrapped and subsequently ignored."" However, tf 2.0 has no module named contri.

**Describe the expected behavior**
Save the model correctly.
**Code to reproduce the issue**
You can run the following code to reproduce this issue
https://github.com/zheLim/auto-face-parsing/blob/master/lib/model/seg_hrnet.py
"
35074,"InvalidArgumentError: indices[6] = 3712 is not in [0, 3592)","
  File ""C:\Users\Allen\Anaconda3\lib\site-packages\tensorflow_core\python\client\session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)

InvalidArgumentError: indices[6] = 3712 is not in [0, 3592)"
35073,`tf.vectorized_map` works incorrectly for functions with `tf.stack`,"Description of the bug is pretty much in the title, here is the little snippet:

```python
import tensorflow as tf
import tensorflow_probability as tfp

def log_prob(x):
    x = 0 # we don't care about the input tensor
    final_log_probs = [tf.ones(1)]
    concat_log_probs = tf.stack(final_log_probs, 0)
    return concat_log_probs

print(log_prob(tf.ones(1)).shape) # (1,1)

# vectorized function
log_prob = tf.vectorized_map(log_prob, tf.ones(1))
print(log_prob.shape) # (1,1,1)
```
The returning shapes are different for two types of calls. Maybe the behaviour is documented (limit on `tf.stack`) but it seems to be odd for me.

```
tf: 2.0.0
tfp: 0.8.0
```"
35072,Explain how int8 input and output quantization conversion works in TensorFlow Lite,"We've had feedback from multiple developers that it's hard to figure out how to calculate the right  int8 values for quantized inputs, and understand what int8 values mean as outputs.

For example, when feeding an image to uint8 quantized inputs, the values can be left as in their source 0 to 255 range. For int8 inputs, the developer will typically need to subtract 128 from each value, but this knowledge (and how the offset value is calculated) is not documented. In the same way, users will need to map the -128 to 127 output values to the actual real number range of their outputs, but this process is unclear.

Tagging the @tensorflow/micro team."
35071,TensorFlow Lite Micro MaxPool kernel needs int8 support,"TensorFlow Lite for Microcontrollers has a MAXPOOL operation, but it only supports float and uint8 execution, not int8."
35070,Calculate arena size for TensorFlow Lite Micro models,"

TensorFlow Lite for Microcontrollers doesn't depend on dynamic memory allocation, so it requires users to supply a memory arena when an interpreter is created, as described in this documentation:
https://www.tensorflow.org/lite/microcontrollers/get_started
We need a better way to decide 'tensor_arena_size'. Currently, the above page says 'The size required will depend on the model you are using, and may need to be determined by experimentation.'

The simplest solution might be an offline script that returns the size needed to hold a model's activation buffers, but this won't include miscellaneous or platform-dependent allocations in its total. Another approach might be to return the size needed as an integer along with the error report string from the interpreter creation."
35068,Unable to use optimisation when modifying gradients returned by tf.gradienttape,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux AMI 2018.03
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.0
- Python version: 3.6.8
- CUDA/cuDNN version: 10.1
- GPU model and memory: Tesla K80

My network has multiple outputs and I wanted to be able to use separate loss functions on each output branch.

Initially, I had it setup like this:

```
@tf.function
def train_step(inputs, labels):
    with tf.GradientTape(persistent=True) as tape:
        predictions = model([X, F], training=True)
        losses = [l_f(tf.expand_dims(labels[:,i], axis=-1), predictions[i]) for i, l_f in enumerate(loss_functions)]
    gradients = [tape.gradient(l, model.trainable_variables) for l in losses]
    for g in gradients:
        grads = [gg if gg is not None else tf.zeros_like(model.trainable_variables[i], dtype=tf.float32) for i, gg in enumerate(g)]
        optimizer.apply_gradients(zip(grads, model.trainable_variables)
    del tape
    return losses


def weighted_loss(weights):
    @tf.function
    def loss_func(labels, predictions):
        min_class_filter = tfk.backend.greater(labels, 0.5)

        y_min = tf.boolean_mask(labels, min_class_filter)
        y_max = tf.boolean_mask(labels, tf.math.logical_not(min_class_filter))
        y_pred_min = tf.boolean_mask(predictions, min_class_filter)
        y_pred_max = tf.boolean_mask(predictions, tf.math.logical_not(min_class_filter))

        loss_min_class = tfk.backend.mean(tfk.backend.binary_crossentropy(y_min, y_pred_min))
        loss_max_class = tfk.backend.mean(tfk.backend.binary_crossentropy(y_max, y_pred_max))
        loss_all = tfk.backend.mean(tfk.backend.binary_crossentropy(labels, predictions))
        return weights[0]*loss_min_class + weights[1]*loss_max_class + weights[2]*loss_all
    return loss_func

loss_functions = [weighted_loss(w) for w in target_weights]
```

The loss functions were giving None gradients for the branch of the prediction that was not included in that particular loss, so I replaced those None gradients with zeros_like.

When I run this as written, it takes an extremely long time (10min+) to run a single training step, and I see the following message in the logs:


`E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] function_operator failed: Invalid argument: Input 0 of node model/LSTM_forward_0/zeros_like was passed int32 from model/LSTM_forward_0/StatefulPartitioned Call:9 incompatible with expected variant.
`

When I remove the @tf.function decorator, it runs in about 10% of the time, and I do not see this log warning, but GPU utilisation is still 0%, and it still runs more slowly than I would expect (i.e. much slower than (number of output branches)*(time for updating just one branch))

The way that I had to resolve this, was to stop overwriting the None gradients, which I was able to do like this:

```
@tf.function
def train_step(inputs, labels):
    with tf.GradientTape(persistent=True) as tape:
    predictions = model([X, F], training=True)
    losses = [l_f(labels, predictions, i) for i, l_f in enumerate(loss_functions)]
    gradients = [tape.gradient(l, model.trainable_variables) for l in losses]
    for g in gradients:
        optimizer.apply_gradients(zip(g, model.trainable_variables)
    del tape
    return losses


def weighted_loss(weights):
    @tf.function
    def loss_func(labs, preds, i):
        labels = tf.expand_dims(labs[:,i], axis=-1)
        predictions = preds[i]
        min_class_filter = tfk.backend.greater(labels, 0.5)

        y_min = tf.boolean_mask(labels, min_class_filter)
        y_max = tf.boolean_mask(labels, tf.math.logical_not(min_class_filter))
        y_pred_min = tf.boolean_mask(predictions, min_class_filter)
        y_pred_max = tf.boolean_mask(predictions, tf.math.logical_not(min_class_filter))

        loss_min_class = tfk.backend.mean(tfk.backend.binary_crossentropy(y_min, y_pred_min))
        loss_max_class = tfk.backend.mean(tfk.backend.binary_crossentropy(y_max, y_pred_max))
        loss_all = tfk.backend.mean(tfk.backend.binary_crossentropy(labels, predictions))
        return weights[0]*loss_min_class + weights[1]*loss_max_class + weights[2]*loss_all
    return loss_func

loss_functions = [weighted_loss(w) for w in target_weights]
```
This is fine, and now training at the expected rate, however I can't see why one shouldn't be able to modify the gradients in this way? The zero gradients that I was passing had the correct dtype, which isn't reflected in the warning message that says it was receiving ints, and that was the reason it was failing to optimise.
"
35067,RaggedTensor Hierarchical Model Issue,"I have the following model:

```
   import numpy as np

   def example():

        # Encode each timestep
        in_sentence = Input(shape=(None,),  dtype='int64', name=""Input1"")
        embedded_sentence = Embedding(1000, 300, trainable=False)(in_sentence)
        lstm_sentence = LSTM(300)(embedded_sentence)
        encoded_model = Model(in_sentence, lstm_sentence)

        section_input = Input(shape=(None, None), dtype='int64', name=""Input2"")
        section_encoded = TimeDistributed(encoded_model)(section_input)
        section_encoded = LSTM(300)(section_encoded)
        section_model = Model(section_input, section_encoded)

        document_input = Input(shape=(None, None, None), dtype='int64', name=""Input3"")
        document_encoded = TimeDistributed(section_model)(document_input)
        document_encoded = LSTM(300, return_sequences=True)(document_encoded)
        document_encoded = Dense(1)(document_encoded)
        document_model = Model(document_input, document_encoded)
        document_model.compile(loss='categorical_crossentropy',
                      optimizer='rmsprop',
                      metrics=['accuracy'])

        print(section_model.summary())
        print(document_model.summary())
        return document_model
```
And I call it:

```

y = []
for doc in documents:
    tocs = []
    for token in doc:
        tocs.append(np.random.random_integers(0, high=1))
        y.append(tocs)

output = tf.ragged.constant(y)

input = tf.ragged.constant([[[[4, 5, 2, 6]], [[10, 12, 9], [26, 20, 21, 22], [35, 34, 31]]]])

example().fit(input, output, use_multiprocessing=True, epochs=10, batch_size=1)
```
I get the following error:


Traceback (most recent call last):
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\util\nest.py"", line 318, in assert_same_structure
    expand_composites)
ValueError: The two structures don't have the same nested structure.

First structure: type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([5, None, None, None]), tf.int32, 3, tf.int64)

Second structure: type=Tensor str=Tensor(""Input3:0"", shape=(None, None, None, None), dtype=int64)

More specifically: Substructure ""type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([5, None, None, None]), tf.int32, 3, tf.int64)"" is a sequence, while substructure ""type=Tensor str=Tensor(""Input3:0"", shape=(None, None, None, None), dtype=int64)"" is not

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:/Development/competitions/Kaggle/Tensorflow 2.0 - Q_a_A/tensorflow_2.0/QuestionAnswerContextAttention.py"", line 392, in <module>
    qa_model.fit(input=documents, output=output)
  File ""D:/Development/competitions/Kaggle/Tensorflow 2.0 - Q_a_A/tensorflow_2.0/QuestionAnswerContextAttention.py"", line 163, in fit
    self.example().fit(input, output, use_multiprocessing=True, epochs=10, batch_size=1)
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 224, in fit
    distribution_strategy=strategy)
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 547, in _process_training_inputs
    use_multiprocessing=use_multiprocessing)
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 594, in _process_inputs
    steps=steps)
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2497, in _standardize_user_data
    nest.assert_same_structure(a, b, expand_composites=True)
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\util\nest.py"", line 325, in assert_same_structure
    % (str(e), str1, str2))
ValueError: The two structures don't have the same nested structure.

First structure: type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([5, None, None, None]), tf.int32, 3, tf.int64)

Second structure: type=Tensor str=Tensor(""Input3:0"", shape=(None, None, None, None), dtype=int64)

More specifically: Substructure ""type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([5, None, None, None]), tf.int32, 3, tf.int64)"" is a sequence, while substructure ""type=Tensor str=Tensor(""Input3:0"", shape=(None, None, None, None), dtype=int64)"" is not
Entire first structure:
.
Entire second structure:
.

Process finished with exit code 1


<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35065,Bug in documentation of tf.while_loop.  parallel_iterations doesn't seem to affect performance.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes and no.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
- TensorFlow installed from (source or binary): I used `pip install tensorflow-gpu==2.0.0`
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7.5
- CUDA/cuDNN version: 10.1
- GPU model and memory: 1080 TI and 11170 MiB

**Describe the current behavior**
First as discussed in this [issue](https://github.com/tensorflow/tensorflow/issues/18257). There is a bug in the first example of the documentation of `tf.while_loop`. 

Then, the parallel_iterations argument doesn't seem to parallelize the loop. There is no difference between the run time with `parallel_iterations = 1` or `parallel_iterations = 10`. 
I have a question opened on [Stackoverflow](https://stackoverflow.com/questions/59299060/tf-2-0-while-loop-and-parallel-iterations) .

**Describe the expected behavior**
If the function in iteration n doesn't depend on previous iterations, then I expect that by setting `parallel_iterations = 10,` the loop should finish about 10 times faster than setting `parallel_iterations = 1`
**Code to reproduce the issue**
The code is posted on the [Stackoverflow](https://stackoverflow.com/questions/59299060/tf-2-0-while-loop-and-parallel-iterations). 
"
35064,tensorflow 1.14 not picking up GPU: CUBLAS_STATUS_INTERNAL_ERROR,"I am having problems running tensorflow on my GPU.

My environment is as follows:

* OS: Linux Mint
* GPU: GeForce RTX 2070 Super
* Nvidia Driver Version: 435.21
* CUDA Version: 10.1
* gcc --version: gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
* tensorflow-gpu 1.14
* keras 2.3.1

Both the keras and tensorflow packages are located in a conda environment with no other tensorflow or keras versions to avoid package conflict. First, I get conflicting GPU devices when I run the following quick test:

```
# picks up the GPU it seems
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())
>>> [name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 4346857393168915334
, name: ""/device:XLA_CPU:0""
device_type: ""XLA_CPU""
memory_limit: 17179869184
locality {
}
incarnation: 15716071101553989809
physical_device_desc: ""device: XLA_CPU device""
, name: ""/device:XLA_GPU:0""
device_type: ""XLA_GPU""
memory_limit: 17179869184
locality {
}
incarnation: 6257014534476475142
physical_device_desc: ""device: XLA_GPU device""
]

# but then Keras doesn't pick up the GPU?
from keras import backend as K
K.tensorflow_backend._get_available_gpus()
>>> []
```
This doesn't make much sense as I thought keras uses tensorflow as the backend.

Then I tried running the below program (I got some hints from these two SO posts [[1]](https://datascience.stackexchange.com/a/41958/41929)[[2]](https://stackoverflow.com/a/52132342/4139143))

```
import numpy as np
import tensorflow as tf
from datetime import datetime
import warnings
warnings.filterwarnings(""ignore"")

# Choose which device you want to test on: either 'cpu' or 'gpu'
devices = ['/device:CPU:0', '/device:XLA_GPU:0']

# Choose size of the matrix to be used.
# Make it bigger to see bigger benefits of parallel computation
shapes = [(50, 50), (100, 100), (500, 500), (1000, 1000)]


def compute_operations(device, shape):
    """"""Run a simple set of operations on a matrix of given shape on given device

    Parameters
    ----------
    device : the type of device to use, either 'cpu' or 'gpu'
    shape : a tuple for the shape of a 2d tensor, e.g. (10, 10)

    Returns
    -------
    out : results of the operations as the time taken
    """"""

    # Define operations to be computed on selected device
    with tf.device(device):
        random_matrix = tf.random.uniform(shape=shape, minval=0, maxval=1)
        dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix))
        sum_operation = tf.reduce_sum(dot_operation)

    # Time the actual runtime of the operations
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    start_time = datetime.now()
    with tf.compat.v1.Session(config=config) as session:
            result = session.run(sum_operation)
    elapsed_time = datetime.now() - start_time

    return result, elapsed_time


if __name__ == '__main__':

    # Run the computations and print summary of each run
    for device in devices:
        print(""--"" * 20)

        for shape in shapes:
            _, time_taken = compute_operations(device, shape)

            # Print the result and also the time taken on the selected device
            print(""Input shape:"", shape, ""using Device:"", device, ""took: {:.2f}"".format(time_taken.seconds + time_taken.microseconds/1e6))
            #print(""Computation on shape:"", shape, ""using Device:"", device, ""took:"")
```

But got multiple errors
```
2019-12-12 18:35:05.957991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.8
pciBusID: 0000:42:00.0
...
...
2019-12-12 18:35:06.655452: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory
2019-12-12 18:35:06.655474: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR
2019-12-12 18:35:06.655483: F tensorflow/compiler/xla/service/gpu/gemm_thunk.cc:176] Check failed: stream->parent()->GetBlasGemmAlgorithms(&algorithms) 
Aborted (core dumped)
```

Am I doing something incorrect here? TF was certainly working with my GPU (the program above worked) just a few weeks ago. It seems every time a new TF or keras version is released, I install it and get GPU issues. Any help / advice is much appreciated. Thank you

"
35063,No example provided for using tf.nn.ctc_loss,"## URL with the issue: https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss

## Description of issue:
There's no example provided for using this loss and I cannot make it work.
Following the parameters definitions I created this toy example in tf2.0.0:

```
import functools
import tensorflow as tf
import numpy as np
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Conv2D, Lambda)
from tensorflow.keras.optimizers import Adam

# INPUTS
inputs = Input(shape=[128, 64, 1], batch_size=32)    # [frames, num_labels, channels]
labels = Input(shape=[128], batch_size=32,  dtype=tf.int32)
label_length = tf.constant(np.ones((32)), dtype=tf.int32)
logit_length = tf.constant(np.ones((32)),  dtype=tf.int32)
# MODEL
x = Conv2D(1, kernel_size=(5, 5),  padding='same')(inputs)
logits = Lambda(lambda z: tf.squeeze(z, [-1]))(x)
model = Model(inputs, logits)
model.compile(optimizer=Adam(lr=0.001), loss=tf.nn.ctc_loss(
    labels=labels, logits=logits, label_length=label_length,
    logit_length=logit_length, logits_time_major=False,
    blank_index=-1
))
```

which rises: 

```
~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    469                 dtype=dtype if dtype else None,
    470                 preferred_dtype=default_dtype,
--> 471                 as_ref=input_arg.is_ref)
    472             if input_arg.number_attr and len(
    473                 set(v.dtype.base_dtype for v in values)) > 1:

~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in internal_convert_n_to_tensor(values, dtype, name, as_ref, preferred_dtype, ctx)
   1363             as_ref=as_ref,
   1364             preferred_dtype=preferred_dtype,
-> 1365             ctx=ctx))
   1366   return ret
   1367

~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)
   1262     graph = get_default_graph()
   1263     if not graph.building_function:
-> 1264       raise RuntimeError(""Attempting to capture an EagerTensor without ""
   1265                          ""building a function."")
   1266     return graph.capture(value, name=name)

RuntimeError: Attempting to capture an EagerTensor without building a function.
```

Then, I tried to use it as a handle:

```
ctc_loss = functools.partial(
    tf.nn.ctc_loss,
    labels,         # labels
    logits,         # logits
    label_length,   # label_length
    logit_length,   # logit_length
    False,          # logits_time_major
    -1,             # blank_index
)
model.compile(optimizer=Adam(lr=0.001), loss=ctc_loss)
```

which rises:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-147-2018e8450f34> in <module>
----> 1 model.compile(optimizer=Adam(lr=0.001), loss=ctc_loss)

~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--> 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)
    371
    372       # Creates the model loss and weighted metrics sub-graphs.
--> 373       self._compile_weights_loss_and_weighted_metrics()
    374
    375       # Functions for train, test and predict will

~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--> 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _compile_weights_loss_and_weighted_metrics(self, sample_weights)
   1651       #                   loss_weight_2 * output_2_loss_fn(...) +
   1652       #                   layer losses.
-> 1653       self.total_loss = self._prepare_total_loss(masks)
   1654
   1655   def _prepare_skip_target_masks(self):

~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _prepare_total_loss(self, masks)
   1732             # differentiate between use case where a custom optimizer
   1733             # expects a vector loss value vs unreduced per-sample loss value.
-> 1734             output_loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)
   1735             loss_reduction = losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE
   1736

TypeError: ctc_loss_v2() got an unexpected keyword argument 'sample_weight'
```

Then, I tried to embed it:

```
def my_ctc_loss(
    labels, logits, label_length, logit_length, logits_time_major,
    blank_index, sample_weight
):
    return tf.nn.ctc_loss(
        labels=labels, logits=logits, label_length=label_length,
        logit_length=logit_length, logits_time_major=logits_time_major,
        blank_index=blank_index
    )


ctc_loss_emb = functools.partial(
    my_ctc_loss,
    labels,         # labels
    logits,         # logits
    label_length,   # label_length
    logit_length,   # logit_length
    False,          # logits_time_major
    -1,             # blank_index
    None,           # sample_weight
)
model.compile(optimizer=Adam(lr=0.001), loss=ctc_loss_emb)
```
which rises: 
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-150-f20d10a91540> in <module>
      9     None,           # sample_weight
     10 )
---> 11 model.compile(optimizer=Adam(lr=0.001), loss=ctc_loss_emb)

~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--> 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)
    371
    372       # Creates the model loss and weighted metrics sub-graphs.
--> 373       self._compile_weights_loss_and_weighted_metrics()
    374
    375       # Functions for train, test and predict will

~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--> 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _compile_weights_loss_and_weighted_metrics(self, sample_weights)
   1651       #                   loss_weight_2 * output_2_loss_fn(...) +
   1652       #                   layer losses.
-> 1653       self.total_loss = self._prepare_total_loss(masks)
   1654
   1655   def _prepare_skip_target_masks(self):

~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _prepare_total_loss(self, masks)
   1732             # differentiate between use case where a custom optimizer
   1733             # expects a vector loss value vs unreduced per-sample loss value.
-> 1734             output_loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)
   1735             loss_reduction = losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE
   1736

TypeError: my_ctc_loss() got multiple values for argument 'sample_weight'
```

### Usage example: Not provided

Since it seems that ctc_loss has to be used differently from other losses, it will helpful to have an example that shows how to use it.

### Raises listed and defined: Not defined


Thanks!"
35062, [1.15] Build from source fails on Raspberry Pi,"**System information**
- OS Platform and Distribution: Archlinux
- Mobile device: Raspberry Pi
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.15
- Python version: 3.8
- Installed using virtualenv? pip? conda?: the docker image you provide.
- Bazel version (if compiling from source): the one you put in that image.
- GCC/Compiler version (if compiling from source): the one you put in the image
- CUDA/cuDNN version: No cuda
- GPU model and memory: The computer on which Docker runs has 16gb of ram.

Builds fails while compiling. Build log has been attached. Instructions actually mention that one should checkout a known working version but it does not specify where I could discover which version actually builds on raspberry.

The one listed at this [documentation page](https://www.tensorflow.org/install/source_rpi):

    CI_DOCKER_EXTRA_PARAMS=""-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.4"" \
    tensorflow/tools/ci_build/ci_build.sh PI-PYTHON3 \
    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh


**Any other info / logs**
[tensorflow-1.15-docker-raspberry-armhf-build.log](https://github.com/tensorflow/tensorflow/files/3957401/tensorflow-1.15-docker-raspberry-armhf-build.log).

Actually it just failed on another point, too.
Last part of the latter log:

    peStorage; UniquerT = mlir::detail::TypeUniquer]'
    external/local_config_mlir/lib/IR/Types.cpp:66:66:   required from here
    external/local_config_mlir/include/mlir/Support/StorageUniquer.h:232:34: error: 'getKey' is not a member of 'mlir::detail::OpaqueTypeStorage'
         return ImplTy::getKey(args...);
                                      ^
    external/local_config_mlir/include/mlir/Support/StorageUniquer.h: In instantiation of 'static typename std::enable_if<typename mlir::detail::detector<void, mlir::detail::has_impltype_hash_t, ImplTy, DerivedKey>::value_t:: value, llvm::hash_code>::type mlir::StorageUniquer::getHash(unsigned int, const DerivedKey&) [with ImplTy = mlir::detail::OpaqueTypeStorage; DerivedKey = std::pair<mlir::Identifier, llvm::StringRef>; typename std::enable_if<typename mlir::detail::detector<void, mlir::detail::has_impltype_hash_t, ImplTy, DerivedKey>::value_t:: value, llvm::hash_code>::type = llvm::hash_code]':
    external/local_config_mlir/include/mlir/Support/StorageUniquer.h:141:59:   required from 'Storage* mlir::StorageUniquer::get(std::function<void(Storage*)>, unsigned int, Arg&&, Args&& ...) [with Storage = mlir::detail::OpaqueTypeStorage; Arg = mlir::Identifier&; Args = {llvm::StringRef&}]'
    external/local_config_mlir/include/mlir/IR/TypeSupport.h:104:42:   required from 'static T mlir::detail::TypeUniquer::get(mlir::MLIRContext*, unsigned int, Args&& ...) [with T = mlir::OpaqueType; Args = {mlir::Identifier&, llvm::StringRef&}]'
    external/local_config_mlir/include/mlir/IR/StorageUniquerSupport.h:67:64:   required from 'static ConcreteT mlir::detail::StorageUserBase<ConcreteT, BaseT, StorageT, UniquerT>::get(mlir::MLIRContext*, unsigned int, Args ...) [with Args = {mlir::Identifier, llvm::StringRef}; ConcreteT = mlir::OpaqueType; BaseT = mlir::Type; StorageT = mlir::detail::OpaqueTypeStorage; UniquerT = mlir::detail::TypeUniquer]'
    external/local_config_mlir/lib/IR/Types.cpp:66:66:   required from here
    external/local_config_mlir/include/mlir/Support/StorageUniquer.h:255:63: error: 'hashKey' is not a member of 'mlir::detail::OpaqueTypeStorage'
         return llvm::hash_combine(kind, ImplTy::hashKey(derivedKey));

**EDIT 19/12/17:** As a temporary workaround I installed [PINTO0309](https://github.com/PINTO0309/Tensorflow-bin)'s binaries."
35061,1.15.0 disappeared from pypi,"Hi All,

related ticket :https://github.com/RasaHQ/rasa/issues/4965

I just noticed that this doesn't work anymore:

```
# pip3 install tensorflow~=1.15.0   

Collecting tensorflow~=1.15.0
  Could not find a version that satisfies the requirement tensorflow~=1.15.0 (from versions: 0.12.1, 1.0.0, 1.0.1, 1.1.0rc0, 1.1.0rc1, 1.1.0rc2, 1.1.0, 1.2.0rc0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.3.0rc0, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.4.0rc0, 1.4.0rc1, 1.4.0, 1.4.1, 1.5.0rc0, 1.5.0rc1, 1.5.0, 1.5.1, 1.6.0rc0, 1.6.0rc1, 1.6.0, 1.7.0rc0, 1.7.0rc1, 1.7.0, 1.7.1, 1.8.0rc0, 1.8.0rc1, 1.8.0, 1.9.0rc0, 1.9.0rc1, 1.9.0rc2, 1.9.0, 1.10.0rc0, 1.10.0rc1, 1.10.0, 1.10.1, 1.11.0rc0, 1.11.0rc1, 1.11.0rc2, 1.11.0, 1.12.0rc0, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.12.2, 1.12.3, 1.13.0rc0, 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)
No matching distribution found for tensorflow~=1.15.0
```

but when you browse pypi: https://pypi.org/project/tensorflow/#history , 1.15.0 versions are there, could you see what's going on pls ? "
35060,Increasing predict time every iteration of loop,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 12.04
- TensorFlow version (use command below): conda install tensorflow-gpu=1.14.0 
- Python version: 3.6
- CUDA/cuDNN version: 10.1
- GPU model and memory: Tesla k80

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
model predict time increases for every tensor. 

Using a resnet 32 layer model. No modifications except for input shape.

Call initializer instance with the dtype argument instead of passing it to the constructor
--- 2.172 seconds ---
1
--- 0.3469 seconds ---
1
--- 0.3441 seconds ---
1
--- 0.3465 seconds ---
1
--- 0.3543 seconds ---
1
--- 0.3583 seconds ---
1
--- 0.3638 seconds ---
1
--- 0.3676 seconds ---
1
--- 0.3727 seconds ---
1
--- 0.3779 seconds ---
1
--- 0.3838 seconds ---
1
--- 0.3918 seconds ---
0
--- 0.3958 seconds ---
0
--- 0.4069 seconds ---
0
--- 0.4084 seconds ---
0
--- 0.4144 seconds ---
0
--- 0.4143 seconds ---
0
--- 0.4173 seconds ---
0
--- 0.4180 seconds ---
0
--- 0.4244 seconds ---
0
--- 0.4355 seconds ---
0
--- 0.4313 seconds ---
0
--- 0.4429 seconds ---
0
--- 0.4446 seconds ---
0
--- 0.4435 seconds ---
0
--- 0.4547 seconds ---
0
--- 0.4513 seconds ---
0
--- 0.4641 seconds ---
0
--- 0.4697 seconds ---
0
--- 0.4717 seconds ---
0
--- 0.4737 seconds ---
0
--- 0.4794 seconds ---
0
--- 0.4816 seconds ---
0
--- 0.4845 seconds ---
0
--- 0.4906 seconds ---
0
--- 0.4923 seconds ---
0
--- 0.5027 seconds ---
0
--- 0.5028 seconds ---
0
--- 0.5088 seconds ---
0
--- 0.5185 seconds ---
0
--- 0.5173 seconds ---
0
--- 0.5223 seconds ---
0
--- 0.5292 seconds ---
0
--- 0.5280 seconds ---
0
--- 0.5391 seconds ---
0
--- 0.5429 seconds ---
0
--- 0.5413 seconds ---
0
--- 0.5503 seconds ---
0
--- 0.5559 seconds ---
1
--- 0.5666 seconds ---
1
--- 0.5622 seconds ---
1
--- 0.5678 seconds ---
0
--- 0.5780 seconds ---
0
--- 0.5866 seconds ---
1
--- 0.5863 seconds ---


**Describe the expected behavior**
constant predict times

** code **
```
class PredictPerFeat(object):
   def __init__(self, model, params):

        self.model = tf.keras.models.load_model(model, compile=False)

   def predict(feats):
        start_time = time.time()
        out = self.model.predict_on_batch(feats)
        
        print(""--- %s seconds ---"" % (time.time() - start_time))
        out = int(out > 0.5) # outputs label

        return out

def main(args):
      predperfeat = PredictPerFeat(args.model_path)
      feats = loadfeats(args.feat_path)   

      for i in range(len(feats)):
         # each feature of shape 1, 25, 40, 1
         # also tested for random input 
         # f = tf.convert_to_tensor(np.random.rand(1,25,40,1))
         f = feats[i]
         pred = predperfeat.predict(feats[i])
         print(prediction)
```
    
     


"
35059,No matching distribution found for tensorflow,
35058,4D transpose convolutional layers,"I've been looking for an implementation of a 4D conv layer in tensorflow and found a solution here:
https://github.com/funkey/conv4d
I was wondering if anyone could help me with the transpose version of this 4D conv layer. How would that look like in the implementation? It seems that in this case, frame_results is the output and is the result of summing the convolution of the current input frame with its previous kernel frame. The reverse would be the result of summing the 3D transpose layers? or there is more to it? any comments will be very appreciated, thanks."
35057,TF2.1 Unable to load model in h5-Format,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.11
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0-rc1 CPU
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I create, compile, fit and then save a tf.keras.Model the following way:

```
import numpy as np
import tensorflow as tf
import tensorflow.keras.layers as layers
import tensorflow.keras.backend as K


def density_model(input_shape, n_clusters):

    input_layer = layers.Input(shape=input_shape)

    y = input_layer

    y = layers.Conv1D(32, 10, 2, padding='same', kernel_initializer='he_uniform')(y)
    y = layers.BatchNormalization()(y)
    y = layers.Activation('relu')(y)

    y = layers.Conv1D(32, 5, 2, padding='same', kernel_initializer='he_uniform')(y)
    y = layers.BatchNormalization()(y)
    y = layers.Activation('relu')(y)

    y = layers.Conv1D(16, 3, 2, padding='same', kernel_initializer='he_uniform')(y)
    y = layers.BatchNormalization()(y)
    y = layers.Activation('relu')(y)

    y = layers.Flatten()(y)

    # mixture coefficients
    alpha = layers.Dense(n_clusters, activation=None)(y)
    alpha = layers.Softmax(name='coeff')(alpha)

    # mean
    mu = layers.Dense(n_clusters, activation=None, name='mu')(y)

    # stddev
    sigma = layers.Dense(n_clusters, activation=None)(y)
    sigma = layers.Lambda(tf.keras.backend.exp, name='sigma')(sigma)

    out = layers.Concatenate(axis=1)([alpha, mu, sigma])

    model = tf.keras.models.Model(input_layer, out)

    return model


class LogLikelihood(tf.keras.losses.Loss):
    def __init__(self, n_clusters):
        super(LogLikelihood, self).__init__()
        self.n_clusters = n_clusters

    def call(self, y_true, y_pred):

        eps = 0.00001

        alpha = K.expand_dims(y_pred[:, 0:self.n_clusters], axis=1)
        mu = K.expand_dims(y_pred[:, self.n_clusters:2*self.n_clusters], axis=1)
        sigma = K.expand_dims(y_pred[:, 2*self.n_clusters:3*self.n_clusters], axis=1)

        y = K.expand_dims(y_true, axis=-1)

        # 1/(2*pi)**0.5 = 0.398...
        coeff = 0.4*alpha/sigma
        exponent = -0.5*K.square((y-mu)/sigma)

        density = K.sum(coeff*K.exp(exponent), axis=-1)
        log_density = -K.log(density+eps)
        log_likelihood = K.sum(log_density, axis=-1)

        return K.mean(log_likelihood)

if __name__ == '__main__':

    train_data = np.random.normal(loc=0.0, scale=1.0, size=1000*100).reshape((1000, 100, 1))
    train_target = np.random.normal(loc=1.0, scale=2.0, size=1000*100).reshape((1000, 100))

    model = density_model(input_shape=(100, 1), n_clusters=2)

    model.compile(
        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
        loss=LogLikelihood(n_clusters=2)
    )

    model.fit(x=train_data, y=train_target, batch_size=32, epochs=1, verbose=0)

    model.save('./density_h5', save_format='h5', include_optimizer=False)
```

All works fine until here. Since I am using a custom Loss and I don't want to deal with the trouble of loading it, I am specifying 'include_optimizer=False' when saving. My model is pretty straightforward, apart from using a Lambda-Layer.

However I cannot load the model. 

```
import tensorflow as tf

model = tf.keras.models.load_model('./density_h5', custom_objects={'tf': tf}, compile=False)
```

gives the following error:

Traceback (most recent call last):
  File ""/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 842, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/layers/core.py"", line 593, in call
    outputs.set_shape(self.compute_output_shape(inputs.shape))
  File ""/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/layers/core.py"", line 601, in compute_output_shape
    if all(input_shape[1:]):
  File ""/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py"", line 765, in __bool__
    self._disallow_bool_casting()
  File ""/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py"", line 534, in _disallow_bool_casting
    self._disallow_in_graph_mode(""using a `tf.Tensor` as a Python `bool`"")
  File ""/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py"", line 523, in _disallow_in_graph_mode
    "" this function with @tf.function."".format(task))
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/fischeru/.PyCharmCE2017.2/config/scratches/scratch_9.py"", line 3, in <module>
    model = tf.keras.models.load_model('./density_h5', custom_objects={'tf': tf}, compile=False)
  File ""/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/saving/save.py"", line 146, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
  File ""/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py"", line 168, in load_model_from_hdf5
    custom_objects=custom_objects)
  File ""/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/saving/model_config.py"", line 55, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/layers/serialization.py"", line 102, in deserialize
    printable_module_name='layer')
  File ""/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/utils/generic_utils.py"", line 191, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/network.py"", line 906, in from_config
    config, custom_objects)
  File ""/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/network.py"", line 1852, in reconstruct_from_config
    process_node(layer, node_data)
  File ""/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/network.py"", line 1799, in process_node
    output_tensors = layer(input_tensors, **kwargs)
  File ""/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 854, in __call__
    str(e) + '\n""""""')
TypeError: You are attempting to use Python control flow in a layer that was not declared to be dynamic. Pass `dynamic=True` to the class constructor.
Encountered error:
""""""
using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.
""""""

**Describe the expected behavior**
I expect to be able to load the model. As far as I know the ""custom_objects={'tf': tf}"" should take care of the Lambda-Layer. And since I explicitely did not include the optimizer, my custom loss should also not make any trouble. 

**Code to reproduce the issue**
see above

**Other info / logs**
see anove
"
35056,ValueError: Tried to convert 'y' to a tensor and failed. Error: None values not supported.,"Hello,

I'm trying to implement a Unet segmentation model using Keras with Tensorflow backend. I created a Conda virual environement and installed Tensorflow 2.0.0 package via Conda package manager.
![image](https://user-images.githubusercontent.com/20855725/70704814-9542c380-1cf4-11ea-8b80-df3bb9d48d38.png)
 
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: - 
- TensorFlow installed from (source or binary): source, I guess
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.9
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 10.1 / 7.6.5.32
- GPU model and memory: 1080Ti / 16 Gb

**Describe the current behavior**
Learning process crashes with a following error:
`ValueError: Tried to convert 'y' to a tensor and failed. Error: None values not supported.`
**Describe the expected behavior**
Learning process goes fine =).
**Code to reproduce the issue**
Here's a complete code:
```
import numpy as np
import os
import cv2
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Input, BatchNormalization, Activation, Dense, Dropout
from tensorflow.python.keras.layers.core import Lambda, RepeatVector, Reshape
from tensorflow.python.keras.layers.convolutional import Conv2D, Conv2DTranspose
from tensorflow.python.keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D
from tensorflow.python.keras.layers.merge import concatenate, add
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.python.keras.optimizers import Adam
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img

def data_gen(templates_folder, masks_folder, im_width, batch_size):
    c = 0
    n = os.listdir(templates_folder)  # List of training images
    m = os.listdir(masks_folder)
    # random.shuffle(n)

    while (True):
        img = np.zeros((batch_size, im_width, im_width, 1)).astype('float')
        mask = np.zeros((batch_size, im_width, im_width, 1)).astype('float')

        for i in range(c, c + batch_size):  # initially from 0 to 16, c = 0.

            train_img = cv2.imread(templates_folder + '/' + n[i], cv2.IMREAD_GRAYSCALE) / 255.
            train_img = train_img.reshape(im_width, im_width, 1)

            img[i - c] = train_img  # add to array - img[0], img[1], and so on.
            train_mask = cv2.imread(masks_folder + '/' + m[i], cv2.IMREAD_GRAYSCALE) / 255.
            train_mask = train_mask.reshape(im_width, im_width, 1)  # Add extra dimension for parity with train_img size [512 * 512 * 3]

            mask[i - c] = train_mask

        c += batch_size
        if (c + batch_size >= len(os.listdir(templates_folder))):
            c = 0
            # random.shuffle(n)
        yield img, mask

def conv2d_block(input_tensor, n_filters, kernel_size=3, batchnorm=True):
    # first layer
    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=""he_normal"", padding=""same"")(input_tensor)
    if batchnorm:
        x = BatchNormalization()(x)
    x = Activation(""relu"")(x)
    # second layer
    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=""he_normal"", padding=""same"")(x)
    if batchnorm:
        x = BatchNormalization()(x)
    x = Activation(""relu"")(x)
    return x

def get_unet(input_img, n_filters=16, dropout=0.5, batchnorm=True):
    # contracting path
    c1 = conv2d_block(input_img, n_filters=n_filters * 1, kernel_size=3, batchnorm=batchnorm)
    p1 = MaxPooling2D((2, 2))(c1)
    p1 = Dropout(dropout * 0.5)(p1)

    c2 = conv2d_block(p1, n_filters=n_filters * 2, kernel_size=3, batchnorm=batchnorm)
    p2 = MaxPooling2D((2, 2))(c2)
    p2 = Dropout(dropout)(p2)

    c3 = conv2d_block(p2, n_filters=n_filters * 4, kernel_size=3, batchnorm=batchnorm)
    p3 = MaxPooling2D((2, 2))(c3)
    p3 = Dropout(dropout)(p3)

    c4 = conv2d_block(p3, n_filters=n_filters * 8, kernel_size=3, batchnorm=batchnorm)
    p4 = MaxPooling2D(pool_size=(2, 2))(c4)
    p4 = Dropout(dropout)(p4)

    c5 = conv2d_block(p4, n_filters=n_filters * 16, kernel_size=3, batchnorm=batchnorm)

    # expansive path
    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides=(2, 2), padding='same')(c5)
    u6 = concatenate([u6, c4])
    u6 = Dropout(dropout)(u6)
    c6 = conv2d_block(u6, n_filters=n_filters * 8, kernel_size=3, batchnorm=batchnorm)

    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides=(2, 2), padding='same')(c6)
    u7 = concatenate([u7, c3])
    u7 = Dropout(dropout)(u7)
    c7 = conv2d_block(u7, n_filters=n_filters * 4, kernel_size=3, batchnorm=batchnorm)

    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides=(2, 2), padding='same')(c7)
    u8 = concatenate([u8, c2])
    u8 = Dropout(dropout)(u8)
    c8 = conv2d_block(u8, n_filters=n_filters * 2, kernel_size=3, batchnorm=batchnorm)

    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides=(2, 2), padding='same')(c8)
    u9 = concatenate([u9, c1], axis=3)
    u9 = Dropout(dropout)(u9)
    c9 = conv2d_block(u9, n_filters=n_filters * 1, kernel_size=3, batchnorm=batchnorm)

    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)
    model = Model(inputs=[input_img], outputs=[outputs])
    return model

callbacks = [
    EarlyStopping(patience=10, verbose=1),
    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),
    ModelCheckpoint(""model-prototype.h5"", verbose=1, save_best_only=True, save_weights_only=True)
]

im_width = 1536
im_height = 1536

input_img = Input((im_height, im_width, 1), name='img')
model = get_unet(input_img, n_filters=16, dropout=0.05, batchnorm=True)

model.compile(optimizer=Adam(lr=0.001), loss=""binary_crossentropy"", metrics=[""accuracy""])

train_templates_path = ""E:/train/templates""
train_masks_path = ""E:/train/masks""
valid_templates_path = ""E:/valid/templates""
valid_masks_path = ""E:/valid/masks""

train_generator = data_gen(train_templates_path, train_masks_path, im_width, batch_size = 4)
val_generator = data_gen(valid_templates_path, valid_masks_path, im_width, batch_size = 4)

results = model.fit_generator(train_generator, epochs=5, steps_per_epoch=10, validation_data=val_generator, validation_steps=1, callbacks=callbacks)
```

**Other info / logs**
Here's a complete error log:
```
2019-12-12 15:19:36.384413: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX AVX2
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-12-12 15:19:36.386318: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 16. Tune using inter_op_parallelism_threads for best performance.
Epoch 1/5
Traceback (most recent call last):
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\framework\op_def_library.py"", line 527, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1296, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 286, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 227, in constant
    allow_broadcast=True)
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 265, in _constant_impl
    allow_broadcast=allow_broadcast))
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\framework\tensor_util.py"", line 437, in make_tensor_proto
    raise ValueError(""None values not supported."")
ValueError: None values not supported.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\framework\op_def_library.py"", line 541, in _apply_op_helper
    values, as_ref=input_arg.is_ref).dtype.name
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1296, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 286, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 227, in constant
    allow_broadcast=True)
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 265, in _constant_impl
    allow_broadcast=allow_broadcast))
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\framework\tensor_util.py"", line 437, in make_tensor_proto
    raise ValueError(""None values not supported."")
ValueError: None values not supported.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:/Explorium/python/networks_learning.py"", line 143, in <module>
    results = model.fit_generator(train_generator, epochs=5, steps_per_epoch=10, validation_data=val_generator, validation_steps=1, callbacks=callbacks)
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 1297, in fit_generator
    steps_name='steps_per_epoch')
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\keras\engine\training_generator.py"", line 265, in model_iteration
    batch_outs = batch_function(*batch_data)
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 1017, in train_on_batch
    self._make_train_function()
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2116, in _make_train_function
    params=self._collected_trainable_weights, loss=self.total_loss)
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\keras\optimizers.py"", line 476, in get_updates
    grads = self.get_gradients(loss, params)
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\keras\optimizers.py"", line 92, in get_gradients
    if None in grads:
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\ops\math_ops.py"", line 1336, in tensor_equals
    return gen_math_ops.equal(self, other)
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\ops\gen_math_ops.py"", line 3626, in equal
    name=name)
  File ""C:\Users\E-soft\Anaconda3\envs\Explorium\lib\site-packages\tensorflow_core\python\framework\op_def_library.py"", line 545, in _apply_op_helper
    (input_name, err))
ValueError: Tried to convert 'y' to a tensor and failed. Error: None values not supported.
```
**Note!** Changing imports from 
""from tensorflow.python.keras.models import Model""
to
""from tensorflow.keras.models import Model""
doesn't work. I get another error:
```
Traceback (most recent call last):
  File ""E:/Explorium/python/networks_learning.py"", line 6, in <module>
    from tensorflow.keras.layers.core import Lambda, RepeatVector, Reshape
ModuleNotFoundError: No module named 'tensorflow.keras.layers.core'
```
"
35055,Post-training quantization leaves quantization nodes,"As reported [here](https://towardsdatascience.com/hacking-google-coral-edge-tpu-motion-blur-and-lanczos-resize-9b60ebfaa552), post-training quantization with representative dataset leaves quantization and dequantization nodes in the graph. 
In the post, `edgetpu_compiler` works but the model expects `float32`, which is suboptimal. 
Also all the tensors have `dtype` `int8` instead of `uint8`.
Is this the expected behavior? Or a known issue?

 Is there a robust workaround? 

I can work on an example with DNNs if needed but I just wanted to ask first.
"
35054,Failed to load delegate from libedgetpu.so.1 on PCIe EdgeTPU [SOLVED],"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
++ Linux Ubuntu 18.04.3
- TensorFlow installed from (source or binary):
++ pip install tflite-runtime...whl
- TensorFlow version:
++ tflite-runtime-1.14.0-cp37...
- Python version:
++ 3.7.5
- Installed using virtualenv? pip? conda?:
++ conda
- CUDA/cuDNN version:
++ CPU Only with EdgeTPU
- GPU model and memory:
++ PCIe EdgeTPU

```
Architecture:        x86_64
CPU op-mode(s):      32-bit, 64-bit
Byte Order:          Little Endian
CPU(s):              8
On-line CPU(s) list: 0-7
Thread(s) per core:  2
Core(s) per socket:  4
Socket(s):           1
NUMA node(s):        1
Vendor ID:           GenuineIntel
CPU family:          6
Model:               60
Model name:          Intel(R) Core(TM) i7-4700EQ CPU @ 2.40GHz
Stepping:            3
CPU MHz:             1097.578
CPU max MHz:         3400.0000
CPU min MHz:         800.0000
BogoMIPS:            4789.32
Virtualisation:      VT-x
L1d cache:           32K
L1i cache:           32K
L2 cache:            256K
L3 cache:            6144K
NUMA node0 CPU(s):   0-7
Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer xsave avx f16c rdrand lahf_lm abm cpuid_fault epb invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts md_clear flush_l1d
```


**Describe the problem**

When running `classify_image.py`, I receive the following error
```
(Coral) v4@v4-TPU:~/coral/tflite/python/examples/classification$ python3 classify_image.py --model models/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite --labels models/inat_bird_labels.txt --input images/parrot.jpg
Traceback (most recent call last):
  File ""/home/v4/anaconda3/envs/Coral/lib/python3.7/site-packages/tflite_runtime/interpreter.py"", line 165, in load_delegate
    delegate = Delegate(library, options)
  File ""/home/v4/anaconda3/envs/Coral/lib/python3.7/site-packages/tflite_runtime/interpreter.py"", line 119, in __init__
    raise ValueError(capture.message)
ValueError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""classify_image.py"", line 118, in <module>
    main()
  File ""classify_image.py"", line 95, in main
    interpreter = make_interpreter(args.model)
  File ""classify_image.py"", line 69, in make_interpreter
    {'device': device[0]} if device else {})
  File ""/home/v4/anaconda3/envs/Coral/lib/python3.7/site-packages/tflite_runtime/interpreter.py"", line 168, in load_delegate
    library, str(e)))
ValueError: Failed to load delegate from libedgetpu.so.1
```

**Any other info / logs**
I have seen various solutions, suggesting I add the user to plugdev group (This was on a solution regarding the USB Accelerator, but i tried it anyway)

Another point was that it wasnt plugged in (again for the usb accelerator), which it is:
```
v4@v4-TPU:~$ ls /dev/apex_0
/dev/apex_0
```
```
v4@v4-TPU:~$ lspci -x | grep 089a
04:00.0 Non-VGA unclassified device: Device 1ac1:089a
```
And dmesg: 
```
v4@v4-TPU:~$ dmesg | grep apex
[    7.830468] apex 0000:04:00.0: Apex performance not throttled due to temperature
```
```
v4@v4-TPU:~$ dmesg | grep 089a
[    0.274318] pci 0000:04:00.0: [1ac1:089a] type 00 class 0x0000ff
```
so the device is connected properly.

The Libraries are appropriately installed:
```
(Coral) v4@v4-TPU:~/coral/tflite/python/examples/classification$ ls /usr/lib/x86_64-linux-gnu/libedgetpu.so.1*
/usr/lib/x86_64-linux-gnu/libedgetpu.so.1  /usr/lib/x86_64-linux-gnu/libedgetpu.so.1.0
```
and these are the permissions of the libraries:
```
(Coral) v4@v4-TPU:~/coral/tflite/python/examples/classification$ ll /usr/lib/x86_64-linux-gnu/libedgetpu.so.1*
lrwxrwxrwx 1 root root     17 Sep 16 21:27 /usr/lib/x86_64-linux-gnu/libedgetpu.so.1 -> libedgetpu.so.1.0
-rwxr-xr-x 1 root root 956392 Sep 16 21:27 /usr/lib/x86_64-linux-gnu/libedgetpu.so.1.0
```
NOTE: I did have to add the 2nd and 3rd execute permissions on `libedgetpu.so.1.0`
"
35053, AndroidRuntime: Node number 1 (SPLIT) failed to prepare.,"Hi, when i use my own trained rnn model to inference on my andorid app
the tflite convert is successully with python api 

import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                       tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)

but when i run this model in andorid app ,
tflite.run(input, output);
 it run into the issuses as the log shows:
could anyone take a look at this?


**Any other info / logs**
[16-14-29.810] 01-01 08:49:45.232 12405 12427 E AndroidRuntime: FATAL EXCEPTION: inference
﻿﻿[16-14-29.810] 01-01 08:49:45.232 12405 12427 E AndroidRuntime: Process: com.hse.wifimotiondetection, PID: 12405
﻿﻿[16-14-29.810] 01-01 08:49:45.232 12405 12427 E AndroidRuntime: java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/split.cc Not an even split
﻿﻿[16-14-29.810] 01-01 08:49:45.232 12405 12427 E AndroidRuntime: Node number 1 (SPLIT) failed to prepare.
﻿﻿[16-14-29.854] 01-01 08:49:45.232 12405 12427 E AndroidRuntime: 
﻿﻿[16-14-29.854] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native Method)
﻿﻿[16-14-29.854] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:143)
﻿﻿[16-14-29.854] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:296)
﻿﻿[16-14-29.898] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at org.tensorflow.lite.Interpreter.run(Interpreter.java:259)
﻿﻿[16-14-29.898] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at com.hse.wifimotiondetection.tflite.Classifier.recognizeWiFiData(Classifier.java:282)
﻿﻿[16-14-29.898] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at com.hse.wifimotiondetection.MainActivity$1.run(MainActivity.java:101)
﻿﻿[16-14-29.899] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at android.os.Handler.handleCallback(Handler.java:789)
﻿﻿[16-14-29.942] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at android.os.Handler.dispatchMessage(Handler.java:98)
﻿﻿[16-14-29.942] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at android.os.Looper.loop(Looper.java:164)
﻿﻿[16-14-29.943] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at android.os.HandlerThread.run(HandlerThread.java:65)
﻿﻿[16-14-29.943] 01-01 08:54:34.738 13372 13372 I AndroidRuntime: VM exiting with result code 0.
"
35052,model.run_eagerly=False is much slower than model.run_eagerly=True,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0
- Python version: Python 3.7.5

**Describe the current behavior**
I try to implemente a simple fm algorithm in tensorflow 2.0. I found use keras.fit is very slow in default params.
If I change the model.run_eagerly to True the performance will be better.
Then I tried turn off eager_execution  by tf.compat.v1.disable_eager_execution(), the performance is the same as tf1.14 with estimator.

1. default
```
2019-12-12 15:37:04.152742: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-12-12 15:37:04.174603: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500000000 Hz
2019-12-12 15:37:04.187437: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556f783dc5b0 executing computations on platform Host. Devices:
2019-12-12 15:37:04.187474: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
Epoch 1/1000
    121/Unknown - 31s 256ms/step - loss: 0.6940 - AUC: 0.4324   
```
2. model.run_eagerly=True
```
2019-12-12 15:38:36.014767: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-12-12 15:38:36.038416: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500000000 Hz
2019-12-12 15:38:36.051835: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b826158d40 executing computations on platform Host. Devices:
2019-12-12 15:38:36.051874: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
Epoch 1/1000
     96/Unknown - 7s 72ms/step - loss: 0.6902 - AUC: 0.3739    
```

3. tf.compat.v1.disable_eager_execution()
```
WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.
2019-12-12 15:39:18.986171: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-12-12 15:39:19.008642: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500000000 Hz
2019-12-12 15:39:19.020877: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561448d34bb0 executing computations on platform Host. Devices:
2019-12-12 15:39:19.020914: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From /home/luoxinchen/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Train on 1000 steps
Epoch 1/1000
1000/1000 [==============================] - 1s 1ms/step - loss: 0.6942 - AUC: 0.4701  
Epoch 2/1000
1000/1000 [==============================] - 1s 633us/step - loss: 0.6939 - AUC: 0.4797
Epoch 3/1000
1000/1000 [==============================] - 1s 640us/step - loss: 0.6936 - AUC: 0.4908
Epoch 4/1000
```

**Describe the expected behavior**
I think keras.fit with model.run_eagerly=False will use tf.function to wrap the training loop, and it's performance should be close to the disable eager execution. But it's perform awfully, it even slower than model.run_eagerly=True.

**Code to reproduce the issue**
```python
import os
import sys
import timeit

import numpy as np
import pandas as pd

import tensorflow as tf
from tensorflow import keras

# tf.compat.v1.disable_eager_execution()
tf.config.threading.set_inter_op_parallelism_threads(8)
os.environ['OMP_NUM_THREADS'] = '1'

bucket = int(1e7)

class MyModel(keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()

  def build(self, input_shape):
    self.user_emb = self.add_weight(
        shape=(bucket + 1, 32),
        dtype=tf.float32,
        initializer=tf.keras.initializers.TruncatedNormal(),
        name=""user_emb"")
    self.item_emb = self.add_weight(
        shape=(bucket + 1, 32),
        dtype=tf.float32,
        initializer=tf.keras.initializers.TruncatedNormal(),
        name=""item_emb"")
    self.bias = tf.Variable(0.0)

  def call(self, inputs):
    user_id, item_id = inputs
    user_id = tf.reshape(user_id, [-1])
    item_id = tf.reshape(item_id, [-1])
    out = tf.gather(self.user_emb, user_id) * tf.gather(self.item_emb, item_id)
    out = tf.reduce_sum(out, axis=1, keepdims=True) + self.bias
    out = tf.sigmoid(out)
    return out


def main():

  def py_func(feats):
    label = feats['labels']
    return (feats['user_id'], feats['item_id']), label

  model = MyModel()

  dataset = tf.data.Dataset.from_tensor_slices({
      ""user_id"": np.random.randint(bucket, size=[1000, 1]),
      ""item_id"": np.random.randint(bucket, size=[1000, 1]),
      ""labels"": np.random.randint(2, size=[1000, 1])
  }).map(py_func)

  model.compile(
      keras.optimizers.SGD(0.01), 'binary_crossentropy', metrics=['AUC'])

  # model.run_eagerly = True
  model.fit(
      dataset,
      shuffle=False,
      workers=1,
      epochs=1000)

if __name__ == '__main__':
  main()
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35051,Use bazel to build tensorflow has some error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution :Linux Ubuntu 18.04 
- TensorFlow installed from (source or binary):source
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):bazel-1.0.0 and soft lint to bazel
- GCC/Compiler version (if compiling from source): 7.4.0 
- CUDA/cuDNN version: no 



**Describe the problem**
use bazel to build tensorflow (newest clone from github)

```
bazel build --local_ram_resources=2048 --config=opt //tensorflow/tools/pip_package:build_pip_package 
``` 
Error message : 
```
gcc: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/test/tensorflow/tensorflow/python/tools/BUILD:280:1 C++ compilation of rule '//tensorflow/core/kernels:conv_ops' failed (Exit 4)
INFO: Elapsed time: 13914.473s, Critical Path: 2099.24s
INFO: 3244 processes: 3244 local.
FAILED: Build did NOT complete successfully
```"
35050,tf throw error and not may get correct gradient when eager_mode is disable in tf=2.0.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 in Docker
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38
- Python version: 3.5
- CUDA/cuDNN version: 10.0 / 7
- GPU model and memory: GTX 1080Ti / 11175MiB

**Describe the current behavior**

Hi authors and developers,

I am developing our project in tf=2.0.0 and eager_mode is disable.

The main reason is tf=1.x will not be maintained but third party libraries have not been ready for tf=2.0 yet.

However, I met some strange bugs when I was training custom model.

This bug can be reproduced by the following minimal testcase:

```python
#%%
import tensorflow as tf
tf.compat.v1.disable_eager_execution()
#tf.compat.v1.disable_v2_behavior()

import numpy as np

batch_size = 100

def download_data():

    # get raw data
    (trainX, trainY), (testX, testY) = tf.keras.datasets.cifar10.load_data()
    trainX = trainX.astype(np.float32)
    testX  = testX.astype(np.float32)

    # ont-hot
    trainY = tf.keras.utils.to_categorical(trainY, 10)
    testY  = tf.keras.utils.to_categorical(testY , 10)

    # get validation sets
    training_size = 45000
    validX = trainX[training_size:,:]
    validY = trainY[training_size:,:]

    trainX = trainX[:training_size,:]
    trainY = trainY[:training_size,:]

    return trainX, trainY, validX, validY, testX, testY

def data_pipeline(dataX, dataY):

        dataset = tf.data.Dataset.from_tensor_slices( (dataX, dataY) )
        dataset = dataset.shuffle(batch_size * 8)
        dataset = dataset.repeat()
        dataset = dataset.batch(batch_size)
        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
        return dataset

class custom_model():
    def __init__(self):

        def Acc():
            acc = tf.keras.metrics.categorical_accuracy(label_ref, clf_out)
            return tf.math.reduce_mean(acc)

        def c_loss():
            loss = tf.keras.losses.categorical_crossentropy(label_ref, clf_out)
            loss = tf.math.reduce_mean(loss)
            return loss

        # create model
        clf_input = tf.keras.layers.Input(shape=(32,32,3), name=""model/input"")
        model = tf.keras.applications.resnet_v2.ResNet50V2(include_top=True, weights=None, input_tensor=clf_input, pooling='max', classes=10)
        #model = tf.keras.applications.vgg16.VGG16(include_top=True, weights=None, input_tensor=clf_input, pooling='max', classes=10)
        model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])

        label_ref = tf.keras.layers.Input(shape=(10,) , name='label_ref')
        clf_out = model(clf_input)

        # using tf.keras.optimizers.Nadam would get error
        optimizer = tf.keras.optimizers.Nadam(lr=0.0005)
        #optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)
        self.train_op = optimizer.minimize(c_loss, var_list=[model.trainable_variables])

        self.clf_model = model
        self.clf_input = clf_input
        self.label_ref = label_ref
        self.op_acc = Acc()
        self.c_loss = c_loss()

if __name__ == '__main__':

    # set GPU
    import os
    if os.environ.get(""CUDA_VISIBLE_DEVICES"") is None:
        os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""

    # reset tf session
    tf.compat.v1.keras.backend.clear_session()
    gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)
    sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))
    tf.compat.v1.keras.backend.set_session(sess) 

    # prepare data
    trainX, trainY, validX, validY, testX, testY = download_data()
    train_gen = data_pipeline(trainX, trainY)
    valid_gen = data_pipeline(validX, validY)
    test_gen = data_pipeline(testX, testY)

    # build targeted model
    model = tf.keras.applications.resnet_v2.ResNet50V2(include_top=True, weights=None, input_shape=(32,32,3), pooling='max', classes=10)
    #model = tf.keras.applications.vgg16.VGG16(include_top=True, weights=None, input_shape=(32,32,3), pooling=None, classes=10)
    model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])

    # fit and evalutate
    model.fit(train_gen,
            steps_per_epoch = trainY.shape[0] // batch_size,
            validation_data = valid_gen,
            validation_steps= validY.shape[0] // batch_size,
            epochs=5,
            verbose=2)
    model.evaluate(testX, testY, verbose=2, batch_size=batch_size)

    # create a new model
    print('Make sure that we create a new model.')
    model = custom_model()
    sess.run(tf.compat.v1.global_variables_initializer())
    model.clf_model.evaluate(testX, testY, verbose=2, batch_size=batch_size)

    # train model
    num_epoch = 5
    tf_iter = tf.compat.v1.data.make_initializable_iterator(train_gen)
    tf_next = tf_iter.get_next()
    sess.run(tf_iter.initializer)
    for epoch in range(num_epoch):
        c_loss, acc = 0.0, 0.0
        for ii in range(trainY.shape[0] // batch_size):
            X, Y = sess.run(tf_next)
            [b_c_loss, b_acc, _] = sess.run([model.c_loss, model.op_acc, model.train_op],
                                                feed_dict={ model.clf_input: X,
                                                            model.label_ref: Y,
                                                            tf.keras.backend.learning_phase(): 1})
            c_loss = c_loss + b_c_loss
            acc = acc + b_acc
        
        c_loss = c_loss / (trainY.shape[0] // batch_size)
        acc = acc / (trainY.shape[0] // batch_size)
        print('[Training]Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss, acc) )

    # evaluate
    num_epoch = 1
    tf_iter = tf.compat.v1.data.make_initializable_iterator(valid_gen)
    tf_next = tf_iter.get_next()
    sess.run(tf_iter.initializer)
    for epoch in range(num_epoch):
        c_loss, acc = 0.0, 0.0
        for ii in range(validY.shape[0] // batch_size):
            X, Y = sess.run(tf_next)
            [b_c_loss, b_acc, _] = sess.run([model.c_loss, model.op_acc, model.train_op],
                                                feed_dict={ model.clf_input: X,
                                                            model.label_ref: Y,
                                                            tf.keras.backend.learning_phase(): 0})
            c_loss = c_loss + b_c_loss
            acc = acc + b_acc
        
        c_loss = c_loss / (validY.shape[0] // batch_size)
        acc = acc / (validY.shape[0] // batch_size)
        print('[Validation]Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss, acc) )

    # evaluate
    num_epoch = 1
    tf_iter = tf.compat.v1.data.make_initializable_iterator(test_gen)
    tf_next = tf_iter.get_next()
    sess.run(tf_iter.initializer)
    for epoch in range(num_epoch):
        c_loss, acc = 0.0, 0.0
        for ii in range(testY.shape[0] // batch_size):
            X, Y = sess.run(tf_next)
            [b_c_loss, b_acc, _] = sess.run([model.c_loss, model.op_acc, model.train_op],
                                                feed_dict={ model.clf_input: X,
                                                            model.label_ref: Y,
                                                            tf.keras.backend.learning_phase(): 0})
            c_loss = c_loss + b_c_loss
            acc = acc + b_acc
        
        c_loss = c_loss / (testY.shape[0] // batch_size)
        acc = acc / (testY.shape[0] // batch_size)
        print('[Testing]Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss, acc) )
```

The first part of testing case is training model with high leval API and the result is as expected.
```
450/450 - 36s - loss: 1.9549 - accuracy: 0.2993 - val_loss: 1.7695 - val_accuracy: 0.3776
Epoch 2/5
450/450 - 29s - loss: 1.5775 - accuracy: 0.4314 - val_loss: 1.5351 - val_accuracy: 0.4478
Epoch 3/5
450/450 - 29s - loss: 1.3974 - accuracy: 0.4954 - val_loss: 1.4687 - val_accuracy: 0.4846
Epoch 4/5
450/450 - 30s - loss: 1.2743 - accuracy: 0.5430 - val_loss: 1.3919 - val_accuracy: 0.5096
Epoch 5/5
450/450 - 29s - loss: 1.1646 - accuracy: 0.5820 - val_loss: 1.3872 - val_accuracy: 0.5110
10000/10000 - 3s - loss: 1.4111 - accuracy: 0.5104
```

I met a bug in our custom model which complainted that `No gradients provided for any variable` and reported the following message:
```
Make sure that we create a new model.
Traceback (most recent call last):
  File ""bug.py"", line 107, in <module>
    model = custom_model()
  File ""bug.py"", line 64, in __init__
    self.train_op = optimizer.minimize(c_loss, var_list=[model.trainable_variables])
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py"", line 319, in minimize
    return self.apply_gradients(grads_and_vars, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py"", line 427, in apply_gradients
    grads_and_vars = _filter_grads(grads_and_vars)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py"", line 1025, in _filter_grads
    ([v.name for _, v in grads_and_vars],))
ValueError: No gradients provided for any variable: ['conv1_conv_1/kernel:0', 'conv1_conv_1/bias:0', ... skip ...,  'post_bn_1/gamma:0', 'post_bn_1/beta:0', 'probs_1/kernel:0', 'probs_1/bias:0'].
```

From this error message, I guess that tensorflow may not get the graph for computing gradient correctly.

So I've trid to modify the file `/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py` near line 316:

```diff
-    grads_and_vars = self._compute_gradients(
-        loss, var_list=var_list, grad_loss=grad_loss)
+    var_list = nest.flatten(var_list)
+    grads = self.get_gradients(loss, var_list)
+    grads_and_vars = list(zip(grads, var_list))
    return self.apply_gradients(grads_and_vars, name=name)
```

And I've also modified my testing case:

```diff
-        self.train_op = optimizer.minimize(c_loss, var_list=[model.trainable_variables])
+        self.train_op = optimizer.minimize(c_loss(), var_list=[model.trainable_variables])

```

Finally, tensorflow can compute gradient. But I got a strange result. 

Training loss and accuracy look as the same as the normal case.

But with comparing the the baseline model, validation and testing loss are not  reasonbale.

```
Make sure that we create a new model.
10000/10000 - 4s - loss: 10.8579 - accuracy: 0.0995
[Training]Epoch: 1/5 - loss: 1.697 - acc: 0.396
[Training]Epoch: 2/5 - loss: 1.310 - acc: 0.528
[Training]Epoch: 3/5 - loss: 1.105 - acc: 0.609
[Training]Epoch: 4/5 - loss: 0.955 - acc: 0.664
[Training]Epoch: 5/5 - loss: 0.827 - acc: 0.709
[Validation]Epoch: 1/1 - loss: 6545.217 - acc: 0.152
[Testing]Epoch: 1/1 - loss: 2.103 - acc: 0.219

```

So I doubt this modification may not a proper patch.

Next, I reverted the modification and tried to replace keras.optimizer with `tf.compat.v1.train.AdamOptimizer`:

```diff
        # using tf.keras.optimizers.Nadam would get error
-        optimizer = tf.keras.optimizers.Nadam(lr=0.0005)
-        #optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)
-        self.train_op = optimizer.minimize(c_loss, var_list=[model.trainable_variables])
+        #optimizer = tf.keras.optimizers.Nadam(lr=0.0005)
+        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)
+        self.train_op = optimizer.minimize(c_loss(), var_list=[model.trainable_variables])
```

I still got a strange result. Training loss and accuracy look as the same as the normal case.

But validation and testing loss are `nan` and accuracy are `0.1` which means model is not trained.

 It can be shown as the following.

```
Make sure that we create a new model.
10000/10000 - 3s - loss: 8.4923 - accuracy: 0.1174
[Training]Epoch: 1/5 - loss: 2.546 - acc: 0.198
[Training]Epoch: 2/5 - loss: 1.737 - acc: 0.365
[Training]Epoch: 3/5 - loss: 1.592 - acc: 0.417
[Training]Epoch: 4/5 - loss: 1.385 - acc: 0.496
[Training]Epoch: 5/5 - loss: 1.247 - acc: 0.552
[Validation]Epoch: 1/1 - loss: nan - acc: 0.099
[Testing]Epoch: 1/1 - loss: nan - acc: 0.100
```

In conclusion, I think that when eager_mode is disable in tf=2.0.0,

1. tensorflow throws errors if we use standard optimizer

2. tensorflow may not get the right gradient if we use `tf.compat.v1.train.AdamOptimizer`


**Describe the expected behavior**

It should work properly.

**Code to reproduce the issue**

Please see the section of **Describe the current behavior**

**Other info / logs**

The following message is the result generated by `tf_env_collect.sh`
```
== check python ===================================================
python version: 3.5.2
python branch:
python build version: ('default', 'Oct  8 2019 13:06:37')
python compiler version: GCC 5.4.0 20160609
python implementation: CPython


== check os platform ===============================================
os: Linux
os kernel version: #40~18.04.1-Ubuntu SMP Thu Nov 14 12:06:39 UTC 2019
os release version: 5.0.0-37-generic
os platform: Linux-5.0.0-37-generic-x86_64-with-Ubuntu-16.04-xenial
linux distribution: ('Ubuntu', '16.04', 'xenial')
linux os distribution: ('Ubuntu', '16.04', 'xenial')
mac version: ('', ('', '', ''), '')
uname: uname_result(system='Linux', node='f7f509f1dacf', release='5.0.0-37-generic', version='#40~18.04.1-Ubuntu SMP Thu Nov 14 12:06:39 UTC 2019', machine='x86_64', processor='x86_64')
architecture: ('64bit', 'ELF')
machine: x86_64


== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== check pips ===================================================
numpy                  1.17.4
protobuf               3.11.1
tensorflow-estimator   2.0.1
tensorflow-gpu         2.0.0
tensorflow-probability 0.8.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 2.0.0
tf.version.GIT_VERSION = v2.0.0-rc2-26-g64c3d38
tf.version.COMPILER_VERSION = 7.3.1 20180303
Sanity check: array([1], dtype=int32)
       443:     find library=libpthread.so.0 [0]; searching
       443:      search path=/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64          (LD_LIBRARY_PATH)
       443:       trying file=/usr/local/nvidia/lib/tls/x86_64/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib/tls/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib/x86_64/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib64/tls/x86_64/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib64/tls/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib64/x86_64/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib64/libpthread.so.0
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libpthread.so.0
       443:
       443:     find library=libc.so.6 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libc.so.6
       443:
       443:     find library=libdl.so.2 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libdl.so.2
       443:
       443:     find library=libutil.so.1 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libutil.so.1
       443:
       443:     find library=libexpat.so.1 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libexpat.so.1
       443:
       443:     find library=libz.so.1 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libz.so.1
       443:
       443:     find library=libm.so.6 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libm.so.6
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libpthread.so.0
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libc.so.6
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libm.so.6
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libz.so.1
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libexpat.so.1
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libutil.so.1
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libdl.so.2
       443:
       443:
       443:     initialize program: /usr/local/bin/python
       443:
       443:
       443:     transferring control: /usr/local/bin/python
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_opcode.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libopenblasp-r0-34a18dc3.3.7.so [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/x86_64:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs            (RPATH from file /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/x86_64/libopenblasp-r0-34a18dc3.3.7.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/libopenblasp-r0-34a18dc3.3.7.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/x86_64/libopenblasp-r0-34a18dc3.3.7.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so
       443:
       443:     find library=libgfortran-ed201abd.so.3.0.0 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs         (RPATH from file /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_tests.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/linalg/lapack_lite.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/linalg/_umath_linalg.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libbz2.so.1.0 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libbz2.so.1.0
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libbz2.so.1.0
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_bz2.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=liblzma.so.5 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/liblzma.so.5
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/liblzma.so.5
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_lzma.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libmpdec.so.2 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/usr/lib/x86_64-linux-gnu/libmpdec.so.2
       443:
       443:
       443:     calling init: /usr/lib/x86_64-linux-gnu/libmpdec.so.2
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_decimal.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/fft/_pocketfft_internal.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/mtrand.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/common.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/bounded_integers.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/mt19937.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/bit_generator.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libcrypto.so.1.0.0 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libcrypto.so.1.0.0
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libcrypto.so.1.0.0
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_hashlib.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/philox.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/pcg64.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/sfc64.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/generator.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libtensorflow_framework.so.2 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..            (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/x86_64/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/x86_64/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/x86_64/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../x86_64/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2
       443:
       443:     find library=librt.so.1 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/librt.so.1
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../librt.so.1
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/librt.so.1
       443:
       443:     find library=libstdc++.so.6 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libstdc++.so.6
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libstdc++.so.6
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/usr/lib/x86_64-linux-gnu/libstdc++.so.6
       443:
       443:     find library=libgcc_s.so.1 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libgcc_s.so.1
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libgcc_s.so.1
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libgcc_s.so.1
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libgcc_s.so.1
       443:
       443:
       443:     calling init: /usr/lib/x86_64-linux-gnu/libstdc++.so.6
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/librt.so.1
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2
       443:
       443:     find library=libhdfs.so [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..           (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libhdfs.so
       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libhdfs.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libhdfs.so
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:      search path=/lib/x86_64-linux-gnu/tls/x86_64:/lib/x86_64-linux-gnu/tls:/lib/x86_64-linux-gnu/x86_64:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/tls/x86_64:/usr/lib/x86_64-linux-gnu/tls:/usr/lib/x86_64-linux-gnu/x86_64:/usr/lib/x86_64-linux-gnu:/lib/tls/x86_64:/lib/tls:/lib/x86_64:/lib:/usr/lib/tls/x86_64:/usr/lib/tls:/usr/lib/x86_64:/usr/lib              (system search path)
       443:       trying file=/lib/x86_64-linux-gnu/tls/x86_64/libhdfs.so
       443:       trying file=/lib/x86_64-linux-gnu/tls/libhdfs.so
       443:       trying file=/lib/x86_64-linux-gnu/x86_64/libhdfs.so
       443:       trying file=/lib/x86_64-linux-gnu/libhdfs.so
       443:       trying file=/usr/lib/x86_64-linux-gnu/tls/x86_64/libhdfs.so
       443:       trying file=/usr/lib/x86_64-linux-gnu/tls/libhdfs.so
       443:       trying file=/usr/lib/x86_64-linux-gnu/x86_64/libhdfs.so
       443:       trying file=/usr/lib/x86_64-linux-gnu/libhdfs.so
       443:       trying file=/lib/tls/x86_64/libhdfs.so
       443:       trying file=/lib/tls/libhdfs.so
       443:       trying file=/lib/x86_64/libhdfs.so
       443:       trying file=/lib/libhdfs.so
       443:       trying file=/usr/lib/tls/x86_64/libhdfs.so
       443:       trying file=/usr/lib/tls/libhdfs.so
       443:       trying file=/usr/lib/x86_64/libhdfs.so
       443:       trying file=/usr/lib/libhdfs.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_csv.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/termios.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/fast_tensor_util.so
       443:
       443:     find library=libuuid.so.1 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libuuid.so.1
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libuuid.so.1
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/wrapt/_wrappers.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_json.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libssl.so.1.0.0 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libssl.so.1.0.0
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libssl.so.1.0.0
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_ssl.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libhdf5-49599f4e.so.103.0.0 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls:/usr/local/lib/python3.5/dist-packages/h5py/.libs/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/x86_64/libhdf5-49599f4e.so.103.0.0
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/libhdf5-49599f4e.so.103.0.0
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/x86_64/libhdf5-49599f4e.so.103.0.0
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0
       443:
       443:     find library=libhdf5_hl-db841637.so.100.1.1 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs          (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1
       443:
       443:     find library=libsz-1c7dd0cf.so.2.0.1 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls:/usr/local/lib/python3.5/dist-packages/h5py/.libs/./x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/x86_64/libsz-1c7dd0cf.so.2.0.1
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/libsz-1c7dd0cf.so.2.0.1
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./x86_64/libsz-1c7dd0cf.so.2.0.1
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1
       443:
       443:     find library=libaec-2147abcd.so.0.0.4 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4
       443:
       443:     find library=libz-a147dcb0.so.1.2.3 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/defs.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_objects.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_conv.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5r.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5t.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/utils.cpython-35m-x86_64-linux-gnu.so
       443:
       452:     find library=libc.so.6 [0]; searching
       452:      search path=/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64          (LD_LIBRARY_PATH)
       452:       trying file=/usr/local/nvidia/lib/tls/x86_64/libc.so.6
       452:       trying file=/usr/local/nvidia/lib/tls/libc.so.6
       452:       trying file=/usr/local/nvidia/lib/x86_64/libc.so.6
       452:       trying file=/usr/local/nvidia/lib/libc.so.6
       452:       trying file=/usr/local/nvidia/lib64/tls/x86_64/libc.so.6
       452:       trying file=/usr/local/nvidia/lib64/tls/libc.so.6
       452:       trying file=/usr/local/nvidia/lib64/x86_64/libc.so.6
       452:       trying file=/usr/local/nvidia/lib64/libc.so.6
       452:      search cache=/etc/ld.so.cache
       452:       trying file=/lib/x86_64-linux-gnu/libc.so.6
       452:
       452:
       452:     calling init: /lib/x86_64-linux-gnu/libc.so.6
       452:
       452:
       452:     initialize program: /bin/sh
       452:
       452:
       452:     transferring control: /bin/sh
       452:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5z.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5a.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5s.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5p.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5ac.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_proxy.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5d.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5ds.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5f.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5g.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5i.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5fd.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5pl.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5o.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5l.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/_lib/_ccallback_c.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_sparsetools.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_csparsetools.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_shortest_path.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_tools.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_traversal.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_reordering.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libjpeg-3b10b538.so.9.3.0 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls:/usr/local/lib/python3.5/dist-packages/PIL/.libs/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs            (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/x86_64/libjpeg-3b10b538.so.9.3.0
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/libjpeg-3b10b538.so.9.3.0
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/x86_64/libjpeg-3b10b538.so.9.3.0
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0
       443:
       443:     find library=libopenjp2-b3d7668a.so.2.3.1 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs           (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1
       443:
       443:     find library=libtiff-8267adfe.so.5.4.0 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs           (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0
       443:
       443:     find library=liblzma-6cd627ed.so.5.2.4 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls:/usr/local/lib/python3.5/dist-packages/PIL/.libs/./x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/.            (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/x86_64/liblzma-6cd627ed.so.5.2.4
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/liblzma-6cd627ed.so.5.2.4
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./x86_64/liblzma-6cd627ed.so.5.2.4
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_nd_image.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_ni_label.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libopenblasp-r0-2ecf47d5.3.7.dev.so [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/x86_64:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs            (RPATH from file /usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/x86_64/libopenblasp-r0-2ecf47d5.3.7.dev.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/libopenblasp-r0-2ecf47d5.3.7.dev.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/x86_64/libopenblasp-r0-2ecf47d5.3.7.dev.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flapack.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flinalg.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_solve_toeplitz.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_decomp_update.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_blas.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_lapack.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so
       443:
       443:
       443:     calling fini: /usr/local/bin/python [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libutil.so.1 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libexpat.so.1 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libz.so.1 [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_opcode.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_tests.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/linalg/lapack_lite.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/linalg/_umath_linalg.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_bz2.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libbz2.so.1.0 [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_lzma.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/liblzma.so.5 [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_decimal.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/x86_64-linux-gnu/libmpdec.so.2 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/fft/_pocketfft_internal.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/mtrand.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/common.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/bounded_integers.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/mt19937.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/bit_generator.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_hashlib.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/philox.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/pcg64.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/sfc64.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/generator.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_csv.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/termios.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/fast_tensor_util.so [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libuuid.so.1 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/wrapt/_wrappers.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_json.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_ssl.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libssl.so.1.0.0 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libcrypto.so.1.0.0 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/defs.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_objects.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_conv.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5r.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5t.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/utils.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5z.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5a.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5s.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5p.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5ac.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_proxy.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5d.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5ds.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5f.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5g.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5i.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5fd.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5pl.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5o.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5l.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/_lib/_ccallback_c.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_sparsetools.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_csparsetools.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_shortest_path.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_tools.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_traversal.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_reordering.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_nd_image.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_ni_label.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flapack.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flinalg.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_solve_toeplitz.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_decomp_update.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_blas.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_lapack.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2 [0]
       443:
       443:
       443:     calling fini: /usr/lib/x86_64-linux-gnu/libstdc++.so.6 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libgcc_s.so.1 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/librt.so.1 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libm.so.6 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libdl.so.2 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libpthread.so.0 [0]
       443:

```"
35049,tf.dataset may out of memory,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 in Docker
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38
- Python version: 3.5
- CUDA/cuDNN version: 10.0 / 7
- GPU model and memory: GTX 1080Ti / 11175MiB

**Describe the current behavior**

Hi authors and developers,

I am developing our project in tf=2.0.0 and eager_mode is disable.

The main reason is tf=1.x will not be maintained but third party libraries have not been ready for tf=2.0 yet.

For some resaons, we have to re-generate `trainX` at the end of each epoch in our custom model.

In tf=1.x version, tensorflow provides `placeholder` API so we can feed new `trainX` to `tf.data` and it works very well.

However, `placeholder` API is deprecated in tf=2.0 or above.

I have to re-generate `tf.data` again and again at the end of each epoch.

Finally, our program will be killed eventually because it is out of memory.

**Describe the expected behavior**

It should work properly.

**Code to reproduce the issue**

```python
#%%
import tensorflow as tf
tf.compat.v1.disable_eager_execution()
#tf.compat.v1.disable_v2_behavior()

import numpy as np

batch_size = 100

def download_data():

    # get raw data
    (trainX, trainY), (testX, testY) = tf.keras.datasets.cifar10.load_data()
    trainX = trainX.astype(np.float32)
    testX  = testX.astype(np.float32)

    # ont-hot
    trainY = tf.keras.utils.to_categorical(trainY, 10)
    testY  = tf.keras.utils.to_categorical(testY , 10)

    # get validation sets
    training_size = 45000
    validX = trainX[training_size:,:]
    validY = trainY[training_size:,:]

    trainX = trainX[:training_size,:]
    trainY = trainY[:training_size,:]

    return trainX, trainY, validX, validY, testX, testY

def data_pipeline(dataX, dataY):

    # create dataset API
    def preprocess_fn(dataX, dataY):
        
        dataX = tf.image.random_flip_left_right(dataX)
        return dataX, dataY

    dataset = tf.data.Dataset.from_tensor_slices( (dataX, dataY) )
    dataset = dataset.shuffle(batch_size * 8)
    dataset = dataset.repeat()
    dataset = dataset.batch(batch_size)
    dataset = dataset.map(preprocess_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
    return dataset

if __name__ == '__main__':

    # set GPU
    import os
    if os.environ.get(""CUDA_VISIBLE_DEVICES"") is None:
        os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""

    # reset tf session
    tf.compat.v1.keras.backend.clear_session()
    gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)
    sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))
    tf.compat.v1.keras.backend.set_session(sess) 

    # prepare data
    trainX, trainY, validX, validY, testX, testY = download_data()
    train_gen = data_pipeline(trainX, trainY)
    valid_gen = data_pipeline(validX, validY)
    test_gen = data_pipeline(testX, testY)

    # build targeted model
    model = tf.keras.applications.resnet_v2.ResNet50V2(include_top=True, weights=None, input_shape=(32,32,3), pooling='max', classes=10)
    model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])

    # fit and evalutate
    num_epoch = 20
    for ii in range(num_epoch):
        model.fit(train_gen,
                steps_per_epoch = trainY.shape[0] // batch_size,
                validation_data = valid_gen,
                validation_steps= validY.shape[0] // batch_size,
                epochs=1,
                verbose=2)
        model.evaluate(testX, testY, verbose=2, batch_size=batch_size)

        # update trainX and re-generate train_gen
        trainX = trainX + 0
        train_gen = data_pipeline(trainX, trainY)
```

The following is the output:
```
450/450 - 37s - loss: 1.9472 - accuracy: 0.3077 - val_loss: 1.7661 - val_accuracy: 0.3764
10000/10000 - 3s - loss: 1.7696 - accuracy: 0.3729
Train on 450 steps, validate on 50 steps
450/450 - 37s - loss: 1.5704 - accuracy: 0.4347 - val_loss: 1.6101 - val_accuracy: 0.4224
10000/10000 - 3s - loss: 1.6036 - accuracy: 0.4274
Train on 450 steps, validate on 50 steps
450/450 - 37s - loss: 1.4119 - accuracy: 0.4903 - val_loss: 1.4621 - val_accuracy: 0.4728
10000/10000 - 3s - loss: 1.4667 - accuracy: 0.4759
Train on 450 steps, validate on 50 steps
450/450 - 38s - loss: 1.3042 - accuracy: 0.5313 - val_loss: 1.3688 - val_accuracy: 0.5060
10000/10000 - 3s - loss: 1.3773 - accuracy: 0.5024
Train on 450 steps, validate on 50 steps
450/450 - 36s - loss: 1.2168 - accuracy: 0.5671 - val_loss: 1.3069 - val_accuracy: 0.5330
10000/10000 - 3s - loss: 1.3197 - accuracy: 0.5284
Train on 450 steps, validate on 50 steps
450/450 - 36s - loss: 1.1384 - accuracy: 0.5935 - val_loss: 1.2692 - val_accuracy: 0.5462
10000/10000 - 3s - loss: 1.2831 - accuracy: 0.5437
Train on 450 steps, validate on 50 steps
450/450 - 36s - loss: 1.0762 - accuracy: 0.6156 - val_loss: 1.3297 - val_accuracy: 0.5320
10000/10000 - 3s - loss: 1.3435 - accuracy: 0.5324
Train on 450 steps, validate on 50 steps
450/450 - 38s - loss: 1.0080 - accuracy: 0.6396 - val_loss: 1.3039 - val_accuracy: 0.5404
10000/10000 - 3s - loss: 1.3260 - accuracy: 0.5351
Train on 450 steps, validate on 50 steps
450/450 - 37s - loss: 0.9562 - accuracy: 0.6609 - val_loss: 1.1603 - val_accuracy: 0.5926
10000/10000 - 3s - loss: 1.1833 - accuracy: 0.5848
Train on 450 steps, validate on 50 steps
450/450 - 38s - loss: 0.8957 - accuracy: 0.6823 - val_loss: 1.2314 - val_accuracy: 0.5728
10000/10000 - 3s - loss: 1.2559 - accuracy: 0.5720
Killed
```

**Other info / logs**

The following message is the result generated by `tf_env_collect.sh`
```
== check python ===================================================
python version: 3.5.2
python branch:
python build version: ('default', 'Oct  8 2019 13:06:37')
python compiler version: GCC 5.4.0 20160609
python implementation: CPython


== check os platform ===============================================
os: Linux
os kernel version: #40~18.04.1-Ubuntu SMP Thu Nov 14 12:06:39 UTC 2019
os release version: 5.0.0-37-generic
os platform: Linux-5.0.0-37-generic-x86_64-with-Ubuntu-16.04-xenial
linux distribution: ('Ubuntu', '16.04', 'xenial')
linux os distribution: ('Ubuntu', '16.04', 'xenial')
mac version: ('', ('', '', ''), '')
uname: uname_result(system='Linux', node='f7f509f1dacf', release='5.0.0-37-generic', version='#40~18.04.1-Ubuntu SMP Thu Nov 14 12:06:39 UTC 2019', machine='x86_64', processor='x86_64')
architecture: ('64bit', 'ELF')
machine: x86_64


== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== check pips ===================================================
numpy                  1.17.4
protobuf               3.11.1
tensorflow-estimator   2.0.1
tensorflow-gpu         2.0.0
tensorflow-probability 0.8.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 2.0.0
tf.version.GIT_VERSION = v2.0.0-rc2-26-g64c3d38
tf.version.COMPILER_VERSION = 7.3.1 20180303
Sanity check: array([1], dtype=int32)
       443:     find library=libpthread.so.0 [0]; searching
       443:      search path=/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64          (LD_LIBRARY_PATH)
       443:       trying file=/usr/local/nvidia/lib/tls/x86_64/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib/tls/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib/x86_64/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib64/tls/x86_64/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib64/tls/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib64/x86_64/libpthread.so.0
       443:       trying file=/usr/local/nvidia/lib64/libpthread.so.0
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libpthread.so.0
       443:
       443:     find library=libc.so.6 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libc.so.6
       443:
       443:     find library=libdl.so.2 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libdl.so.2
       443:
       443:     find library=libutil.so.1 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libutil.so.1
       443:
       443:     find library=libexpat.so.1 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libexpat.so.1
       443:
       443:     find library=libz.so.1 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libz.so.1
       443:
       443:     find library=libm.so.6 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libm.so.6
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libpthread.so.0
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libc.so.6
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libm.so.6
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libz.so.1
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libexpat.so.1
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libutil.so.1
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libdl.so.2
       443:
       443:
       443:     initialize program: /usr/local/bin/python
       443:
       443:
       443:     transferring control: /usr/local/bin/python
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_opcode.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libopenblasp-r0-34a18dc3.3.7.so [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/x86_64:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs            (RPATH from file /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/x86_64/libopenblasp-r0-34a18dc3.3.7.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/libopenblasp-r0-34a18dc3.3.7.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/x86_64/libopenblasp-r0-34a18dc3.3.7.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so
       443:
       443:     find library=libgfortran-ed201abd.so.3.0.0 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs         (RPATH from file /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_tests.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/linalg/lapack_lite.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/linalg/_umath_linalg.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libbz2.so.1.0 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libbz2.so.1.0
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libbz2.so.1.0
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_bz2.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=liblzma.so.5 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/liblzma.so.5
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/liblzma.so.5
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_lzma.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libmpdec.so.2 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/usr/lib/x86_64-linux-gnu/libmpdec.so.2
       443:
       443:
       443:     calling init: /usr/lib/x86_64-linux-gnu/libmpdec.so.2
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_decimal.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/fft/_pocketfft_internal.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/mtrand.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/common.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/bounded_integers.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/mt19937.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/bit_generator.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libcrypto.so.1.0.0 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libcrypto.so.1.0.0
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libcrypto.so.1.0.0
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_hashlib.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/philox.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/pcg64.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/sfc64.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/generator.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libtensorflow_framework.so.2 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..            (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/x86_64/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/x86_64/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/x86_64/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../x86_64/libtensorflow_framework.so.2
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2
       443:
       443:     find library=librt.so.1 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/librt.so.1
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../librt.so.1
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/librt.so.1
       443:
       443:     find library=libstdc++.so.6 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libstdc++.so.6
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libstdc++.so.6
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/usr/lib/x86_64-linux-gnu/libstdc++.so.6
       443:
       443:     find library=libgcc_s.so.1 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libgcc_s.so.1
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libgcc_s.so.1
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libgcc_s.so.1
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libgcc_s.so.1
       443:
       443:
       443:     calling init: /usr/lib/x86_64-linux-gnu/libstdc++.so.6
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/librt.so.1
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2
       443:
       443:     find library=libhdfs.so [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..           (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libhdfs.so
       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libhdfs.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libhdfs.so
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:      search path=/lib/x86_64-linux-gnu/tls/x86_64:/lib/x86_64-linux-gnu/tls:/lib/x86_64-linux-gnu/x86_64:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/tls/x86_64:/usr/lib/x86_64-linux-gnu/tls:/usr/lib/x86_64-linux-gnu/x86_64:/usr/lib/x86_64-linux-gnu:/lib/tls/x86_64:/lib/tls:/lib/x86_64:/lib:/usr/lib/tls/x86_64:/usr/lib/tls:/usr/lib/x86_64:/usr/lib              (system search path)
       443:       trying file=/lib/x86_64-linux-gnu/tls/x86_64/libhdfs.so
       443:       trying file=/lib/x86_64-linux-gnu/tls/libhdfs.so
       443:       trying file=/lib/x86_64-linux-gnu/x86_64/libhdfs.so
       443:       trying file=/lib/x86_64-linux-gnu/libhdfs.so
       443:       trying file=/usr/lib/x86_64-linux-gnu/tls/x86_64/libhdfs.so
       443:       trying file=/usr/lib/x86_64-linux-gnu/tls/libhdfs.so
       443:       trying file=/usr/lib/x86_64-linux-gnu/x86_64/libhdfs.so
       443:       trying file=/usr/lib/x86_64-linux-gnu/libhdfs.so
       443:       trying file=/lib/tls/x86_64/libhdfs.so
       443:       trying file=/lib/tls/libhdfs.so
       443:       trying file=/lib/x86_64/libhdfs.so
       443:       trying file=/lib/libhdfs.so
       443:       trying file=/usr/lib/tls/x86_64/libhdfs.so
       443:       trying file=/usr/lib/tls/libhdfs.so
       443:       trying file=/usr/lib/x86_64/libhdfs.so
       443:       trying file=/usr/lib/libhdfs.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_csv.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/termios.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/fast_tensor_util.so
       443:
       443:     find library=libuuid.so.1 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libuuid.so.1
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libuuid.so.1
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/wrapt/_wrappers.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_json.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libssl.so.1.0.0 [0]; searching
       443:      search path=           (LD_LIBRARY_PATH)
       443:      search cache=/etc/ld.so.cache
       443:       trying file=/lib/x86_64-linux-gnu/libssl.so.1.0.0
       443:
       443:
       443:     calling init: /lib/x86_64-linux-gnu/libssl.so.1.0.0
       443:
       443:
       443:     calling init: /usr/lib/python3.5/lib-dynload/_ssl.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libhdf5-49599f4e.so.103.0.0 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls:/usr/local/lib/python3.5/dist-packages/h5py/.libs/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/x86_64/libhdf5-49599f4e.so.103.0.0
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/libhdf5-49599f4e.so.103.0.0
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/x86_64/libhdf5-49599f4e.so.103.0.0
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0
       443:
       443:     find library=libhdf5_hl-db841637.so.100.1.1 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs          (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1
       443:
       443:     find library=libsz-1c7dd0cf.so.2.0.1 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls:/usr/local/lib/python3.5/dist-packages/h5py/.libs/./x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/x86_64/libsz-1c7dd0cf.so.2.0.1
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/libsz-1c7dd0cf.so.2.0.1
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./x86_64/libsz-1c7dd0cf.so.2.0.1
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1
       443:
       443:     find library=libaec-2147abcd.so.0.0.4 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4
       443:
       443:     find library=libz-a147dcb0.so.1.2.3 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/defs.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_objects.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_conv.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5r.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5t.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/utils.cpython-35m-x86_64-linux-gnu.so
       443:
       452:     find library=libc.so.6 [0]; searching
       452:      search path=/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64          (LD_LIBRARY_PATH)
       452:       trying file=/usr/local/nvidia/lib/tls/x86_64/libc.so.6
       452:       trying file=/usr/local/nvidia/lib/tls/libc.so.6
       452:       trying file=/usr/local/nvidia/lib/x86_64/libc.so.6
       452:       trying file=/usr/local/nvidia/lib/libc.so.6
       452:       trying file=/usr/local/nvidia/lib64/tls/x86_64/libc.so.6
       452:       trying file=/usr/local/nvidia/lib64/tls/libc.so.6
       452:       trying file=/usr/local/nvidia/lib64/x86_64/libc.so.6
       452:       trying file=/usr/local/nvidia/lib64/libc.so.6
       452:      search cache=/etc/ld.so.cache
       452:       trying file=/lib/x86_64-linux-gnu/libc.so.6
       452:
       452:
       452:     calling init: /lib/x86_64-linux-gnu/libc.so.6
       452:
       452:
       452:     initialize program: /bin/sh
       452:
       452:
       452:     transferring control: /bin/sh
       452:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5z.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5a.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5s.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5p.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5ac.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_proxy.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5d.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5ds.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5f.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5g.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5i.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5fd.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5pl.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5o.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5l.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/_lib/_ccallback_c.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_sparsetools.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_csparsetools.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_shortest_path.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_tools.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_traversal.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_reordering.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libjpeg-3b10b538.so.9.3.0 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls:/usr/local/lib/python3.5/dist-packages/PIL/.libs/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs            (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/x86_64/libjpeg-3b10b538.so.9.3.0
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/libjpeg-3b10b538.so.9.3.0
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/x86_64/libjpeg-3b10b538.so.9.3.0
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0
       443:
       443:     find library=libopenjp2-b3d7668a.so.2.3.1 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs           (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1
       443:
       443:     find library=libtiff-8267adfe.so.5.4.0 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs           (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0
       443:
       443:     find library=liblzma-6cd627ed.so.5.2.4 [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls:/usr/local/lib/python3.5/dist-packages/PIL/.libs/./x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/.            (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/x86_64/liblzma-6cd627ed.so.5.2.4
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/liblzma-6cd627ed.so.5.2.4
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./x86_64/liblzma-6cd627ed.so.5.2.4
       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_nd_image.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_ni_label.cpython-35m-x86_64-linux-gnu.so
       443:
       443:     find library=libopenblasp-r0-2ecf47d5.3.7.dev.so [0]; searching
       443:      search path=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/x86_64:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs            (RPATH from file /usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so)
       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/x86_64/libopenblasp-r0-2ecf47d5.3.7.dev.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/libopenblasp-r0-2ecf47d5.3.7.dev.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/x86_64/libopenblasp-r0-2ecf47d5.3.7.dev.so
       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flapack.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flinalg.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_solve_toeplitz.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_decomp_update.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_blas.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_lapack.cpython-35m-x86_64-linux-gnu.so
       443:
       443:
       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so
       443:
       443:
       443:     calling fini: /usr/local/bin/python [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libutil.so.1 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libexpat.so.1 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libz.so.1 [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_opcode.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_tests.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/linalg/lapack_lite.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/linalg/_umath_linalg.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_bz2.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libbz2.so.1.0 [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_lzma.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/liblzma.so.5 [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_decimal.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/x86_64-linux-gnu/libmpdec.so.2 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/fft/_pocketfft_internal.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/mtrand.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/common.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/bounded_integers.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/mt19937.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/bit_generator.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_hashlib.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/philox.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/pcg64.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/sfc64.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/generator.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_csv.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/termios.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/fast_tensor_util.so [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libuuid.so.1 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/wrapt/_wrappers.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_json.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/lib/python3.5/lib-dynload/_ssl.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libssl.so.1.0.0 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libcrypto.so.1.0.0 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/defs.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_objects.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_conv.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5r.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5t.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/utils.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5z.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5a.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5s.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5p.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5ac.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_proxy.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5d.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5ds.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5f.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5g.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5i.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5fd.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5pl.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5o.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5l.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/_lib/_ccallback_c.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_sparsetools.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_csparsetools.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_shortest_path.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_tools.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_traversal.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_reordering.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_nd_image.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_ni_label.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flapack.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flinalg.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_solve_toeplitz.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_decomp_update.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_blas.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_lapack.cpython-35m-x86_64-linux-gnu.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0 [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so [0]
       443:
       443:
       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2 [0]
       443:
       443:
       443:     calling fini: /usr/lib/x86_64-linux-gnu/libstdc++.so.6 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libgcc_s.so.1 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/librt.so.1 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libm.so.6 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libdl.so.2 [0]
       443:
       443:
       443:     calling fini: /lib/x86_64-linux-gnu/libpthread.so.0 [0]
       443:

```"
35046,Test,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35045,"add_loss bug when using tf.keras, but it is ok using keras.","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
- TensorFlow version (use command below): 
tensorflow2.0.0 keras 2.3.1
- Python version: 3.6
- CUDA/cuDNN version: 10.0
- GPU model and memory: 1080ti

**Describe the current behavior**
When i use keras, the add_loss behavior is all okay. But when i change to tf.keras, the following error
 is occured.
I suppose that maybe the eager cause this problem, but it still occurs when i disable the eager.
So, any ideas? Thanks a lot.


2019-12-12 13:02:42.334129: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: You must feed a value for placeholder tensor 'Input-Segment' with dtype float and shape [?,?]
	 [[{{node Input-Segment}}]]
2019-12-12 13:02:42.334488: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: You must feed a value for placeholder tensor 'Input-Segment' with dtype float and shape [?,?]
	 [[{{node Input-Segment}}]]
	 [[Embedding-Token/Cast/_8]]
Traceback (most recent call last):
  File ""test.py"", line 33, in <module>
    model.add_loss(cross_entropy)
  File ""/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 1132, in add_loss
    self._graph_network_add_loss(symbolic_loss)
  File ""/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py"", line 1426, in _graph_network_add_loss
    new_nodes, new_layers = _map_subgraph_network(self.inputs, [symbolic_loss])
  File ""/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py"", line 1651, in _map_subgraph_network
    base_layer_utils.create_keras_history(outputs)
  File ""/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py"", line 184, in create_keras_history
    _, created_layers = _create_keras_history_helper(tensors, set(), [])
  File ""/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py"", line 231, in _create_keras_history_helper
    layer_inputs, processed_ops, created_layers)
  File ""/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py"", line 231, in _create_keras_history_helper
    layer_inputs, processed_ops, created_layers)
  File ""/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py"", line 231, in _create_keras_history_helper
    layer_inputs, processed_ops, created_layers)
  File ""/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py"", line 229, in _create_keras_history_helper
    constants[i] = backend.function([], op_input)([])
  File ""/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py"", line 3740, in __call__
    outputs = self._graph_fn(*converted_inputs)
  File ""/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1081, in __call__
    return self._call_impl(args, kwargs)
  File ""/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1121, in _call_impl
    return self._call_flat(args, self.captured_inputs, cancellation_manager)
  File ""/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File ""/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 511, in call
    ctx=ctx)
  File ""/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError:  You must feed a value for placeholder tensor 'Input-Segment' with dtype float and shape [?,?]
	 [[node Input-Segment (defined at /home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_keras_scratch_graph_190]

Function call stack:
keras_scratch_graph

**Code to reproduce the issue**


```
import tensorflow.keras as keras
import tensorflow.keras.backend as K
import tensorflow as tf
tf.compat.v1.disable_eager_execution()
# import keras
# import keras.backend as K
x_in = keras.layers.Input(shape=(None, ), name='Input-Token')
s_in = keras.layers.Input(shape=(None, ), name='Input-Segment')
x, s = x_in, s_in
# Embedding
x = keras.layers.Embedding(input_dim=12,
                           output_dim=12,
                           name='Embedding-Token')(x)
s = keras.layers.Embedding(input_dim=12,
                           output_dim=12,
                           name='Embedding-Segment')(s)
x = keras.layers.Add(name='Embedding-Token-Segment')([x, s])
model = keras.models.Model([x_in, s_in], x)
model.summary()
y_in = model.input[0][:, 1:]
y_mask = model.input[1][:, 1:]
y = model.output[:, :-1]
cross_entropy = K.sparse_categorical_crossentropy(y_in, y)
model.add_loss(cross_entropy)
```

"
35044,Memory leaks when using tf.keras.metrics update_states in multi-threads.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
The memory increases and does not release when I use ` tf.keras.metrics.update_states` in multi-threads on Linux. I view the used memory by ""memory_profiler"" and find that the memory increase after each thread starts.

![image](https://user-images.githubusercontent.com/18071380/70680739-150c6600-1cd4-11ea-9f04-6a1724b72d23.png)


**Describe the expected behavior**
The memory increases greatly after only the first thread starts.

**Code to reproduce the issue**
```python
import threading
import time
import tensorflow as tf
import numpy as np
from memory_profiler import profile

auc = tf.keras.metrics.AUC()
lock = threading.Lock()


def update_metric(metric):
    labels = np.random.randint(0, 2, 20000)
    preds = np.random.random(20000)
    metric.update_state(labels, preds)


def update_metric_mini(metric):
    for i in range(10):
        labels = np.random.randint(0, 2, 2000)
        preds = np.random.random(2000)
        metric.update_state(labels, preds)


def test_mem(metric):
    update_metric(metric)
    # update_metric_mini(metric)


def test_tf_metric():
    with lock:
        test_mem(auc)


@profile
def test_threads():
    t1 = threading.Thread(target=test_tf_metric)
    t2 = threading.Thread(target=test_tf_metric)
    t3 = threading.Thread(target=test_tf_metric)
    t1.start()
    time.sleep(0.1)
    t2.start()
    time.sleep(0.1)
    t3.start()
    time.sleep(0.1)
    t1.join()
    t2.join()
    t3.join()
    time.sleep(1)
    print(""end "")


test_threads()
```
In this test code snippet, if we use the `update_metric_mini` which split the updated outputs and labels to small chunks and update metric with those chunks, the memory increases small too. I wonder whether the `update_states` in each thread creates global variables (e.g. placeholder) for input arguments `outputs` and `labels` and the variable size is proportional to the size of input arguments.

***other info*
We encounter this issue when developing evaluation in [ElasticDL](https://github.com/sql-machine-learning/elasticdl) and resolving the ElasticDL [issue 1568](https://github.com/sql-machine-learning/elasticdl/issues/1568)

"
35043,TF2.0: No way to create local variables in tensorflow 2.0,"In tensorflow 1.x, user can create local variables, whose lifetime spans over multiple session runs, but will not be saved to checkpoints. Those local variables can be used as internal states of session (e.g., the hidden states of an online rnn). In tensorflow 2.x, session is gone in python side, but when deploy the model in a c++ program we still need to use session, and the local variables can be used to store session states of the model. However, in tensorflow 2.x, there is no standard way to create local variables in python, because all variables created in a Module/Layer are automatically tracked and stored into checkpoints."
35042,How to know if savedmodel supports batch input and dynamic length input?,"I use the follwing code:

```
with tf.Session() as sess:
    meta_graph_def = tf.saved_model.loader.load(sess, [""serve""], model_path)
    signature = meta_graph_def.signature_def
    print(signature[""serving_default""])
    if str(-1) in str(signature[""serving_default""]):
        print(""support batch input"")
    else:
        print(""Not support batch input"")
```
the result:
```
inputs {
  key: ""hist_i""
  value {
    name: ""my_input/hist_item_id:0""
    dtype: DT_INT32
    tensor_shape {
      dim {
        size: -1
      }
      dim {
        size: -1
      }
    }
  }
}
inputs {
  key: ""hist_t""
  value {
    name: ""my_input/hist_time:0""
    dtype: DT_INT32
    tensor_shape {
      dim {
        size: -1
      }
      dim {
        size: -1
      }
    }
  }
}
inputs {
  key: ""item_id""
  value {
    name: ""my_input/item_id:0""
    dtype: DT_INT32
    tensor_shape {
      dim {
        size: -1
      }
    }
  }
}
inputs {
  key: ""sl""
  value {
    name: ""my_input/Placeholder:0""
    dtype: DT_INT32
    tensor_shape {
      dim {
        size: -1
      }
    }
  }
}
inputs {
  key: ""user_id""
  value {
    name: ""my_input/user_id:0""
    dtype: DT_INT64
    tensor_shape {
      dim {
        size: -1
      }
    }
  }
}
```
But the code can't to distinguish with batch input and dynamic  length input.Anyone can help me?Thank you very much."
35040,S3 unit test build broken: gtl not declared,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): master @3d79d19aa27abd1e236761a6071acec858acb6a0
- Python version: 3.6
- Bazel version (if compiling from source): 1.1.0
- GCC/Compiler version (if compiling from source): gcc5
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
Test fails to build
```
ERROR: /tensorflow/tensorflow/core/platform/s3/BUILD:96:1: C++ compilation of rule '//tensorflow/core/platform/s3:s3_file_system_test' failed (Exit 1)
In file included from ./tensorflow/core/platform/env.h:27:0,
                 from ./tensorflow/core/platform/s3/s3_file_system.h:20,
                 from tensorflow/core/platform/s3/s3_file_system_test.cc:16:
tensorflow/core/platform/s3/s3_file_system_test.cc: In member function 'tensorflow::Status tensorflow::{anonymous}::S3FileSystemTest::ReadAll(const string&, std::__cxx11::string*)':
tensorflow/core/platform/s3/s3_file_system_test.cc:59:45: error: 'gtl' has not been declared
         reader->Read(0, file_size, &result, gtl::string_as_array(content)));
                                             ^
./tensorflow/core/platform/errors.h:73:37: note: in definition of macro 'TF_RETURN_IF_ERROR'
     ::tensorflow::Status _status = (__VA_ARGS__);        \
                                     ^~~~~~~~~~~
In file included from external/com_google_googletest/googletest/include/gtest/gtest.h:388:0,
                 from ./tensorflow/core/platform/test.h:31,
                 from ./tensorflow/core/lib/core/status_test_util.h:20,
                 from tensorflow/core/platform/s3/s3_file_system_test.cc:18:
tensorflow/core/platform/s3/s3_file_system_test.cc: In member function 'virtual void tensorflow::{anonymous}::S3FileSystemTest_NewRandomAccessFile_Test::TestBody()':
tensorflow/core/platform/s3/s3_file_system_test.cc:83:48: error: 'gtl' has not been declared
       reader->Read(0, content.size(), &result, gtl::string_as_array(&got)));
                                                ^
tensorflow/core/platform/s3/s3_file_system_test.cc:82:3: note: in expansion of macro 'TF_EXPECT_OK'
   TF_EXPECT_OK(
   ^~~~~~~~~~~~
tensorflow/core/platform/s3/s3_file_system_test.cc:89:44: error: 'gtl' has not been declared
   TF_EXPECT_OK(reader->Read(2, 4, &result, gtl::string_as_array(&got)));
                                            ^
tensorflow/core/platform/s3/s3_file_system_test.cc:89:3: note: in expansion of macro 'TF_EXPECT_OK'
   TF_EXPECT_OK(reader->Read(2, 4, &result, gtl::string_as_array(&got)));
   ^~~~~~~~~~~~
Target //tensorflow/core/platform/s3:s3_file_system_test failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 5.538s, Critical Path: 5.20s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
```
**Describe the expected behavior**
Test builds and runs

**Code to reproduce the issue**
bazel test -c opt //tensorflow/core/platform/s3:s3_file_system_test --action_env=S3_TEST_TMPDIR=s3://BUCKET/s3testing-results/bazel/master/

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35037,Testing new tensorflow team,"Testing that issue tagged with @tensorflow/micro goes to the appropriate set of folks.

"
35036,Error when no GPU available in TensorFlow 2.1.0rc1,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0rc1
- Python version: 3.6.9
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

Installing `tensorflow==2.1.0rc1` and trying to `import tensorflow` results in an error if there is no GPU set up on the machine.

**Describe the expected behavior**

In `tensorflow==2.1.0rc0` this simply results in a warning

```
2019-12-11 14:50:44.677649: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2019-12-11 14:50:44.684212: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
```

This should have the same warning message in 2.1.0rc1, rather than crashing.

**Code to reproduce the issue**

```
import tensorflow
```

**Other info / logs**
```
ImportError: Traceback (most recent call last):
  File ""...\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""...\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""...\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""...\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""...\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```
"
35033,Cannot use Model as a Layer,"
I am not able to use Model as a Keras layer. I get the following error message:

**AttributeError: 'Model' object has no attribute 'shape'**


<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35032,cuBLAS failure for large convolutions in V100 GPUs,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.15.0 or 2.0.0
- **Python version**: 3.7.3
- **CUDA/cuDNN version**: 10.0 and 7.6.4
- **GPU model and memory**: Tesla V100 with either 16GB or 32GB
- **Exact command to reproduce**: `python tf_v100_cublas_crash_tf1.py` (from the gist below)

### Describe the problem
If I run a large 1x1 convolution with FP16 on a Tesla V100, cuBLAS crashes with the following kind of message:
```
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal: Blas SGEMM launch failed : m=9584640, n=17, k=17
     [[node graph_final_part/conv/conv2d/Conv2D (defined at ~/.virtualenvs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]
     [[ExpandDims_9/_3631]]
  (1) Internal: Blas SGEMM launch failed : m=9584640, n=17, k=17
     [[node graph_final_part/conv/conv2d/Conv2D (defined at ~/.virtualenvs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]
```
This comes from this place in the TF code:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc#L698

The root cause is such TensorFlow code for example:
```
conv_op = tf.layers.Conv2D(filters=17, kernel_size=(1, 1))
in_t = tf.ones(shape=(10, 1152, 832, 17), dtype=tf.float16)
```
It works again if I:
* Make the input tensor smaller
* Use FP32 instead of FP16
* Run on a CPU instead of the V100 GPU
* Use a GeForce Titan X GPU instead of a V100
* Use TF 1.14 on a V100 GPU (but not TF 1.15.0 or 2.0.0)

### Source code / logs

A small gist with a fully working example of only ~10 lines can be found here:
https://gist.github.com/CNugteren/c40f0f34f2af759b2d900223fefadbfd#file-tf_v100_cublas_crash_tf1-py (TF 1.15.0 version)
https://gist.github.com/CNugteren/c40f0f34f2af759b2d900223fefadbfd#file-tf_v100_cublas_crash_tf2-py (TF 2.0.0 version)

I even tried to mimic the cuBLAS call but that didn't crash, so it seems a TensorFlow bug rather than cuBLAS, unless I made a mistake of course:
https://gist.github.com/CNugteren/c40f0f34f2af759b2d900223fefadbfd#file-tf_v100_cublas_crash_mimic-cu"
35031,ValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.,"**System information**
- Windows 10 866 bit OS,x64 based processor 
- python 3.5.4 (working on virtual env)
- pip installed version of Tensorflow
- TensorFlow version: tensorflow==2.0.0
- other packages:
scikit-learn==0.22
scipy==1.4.0rc2
six==1.12.0
tb-nightly==2.1.0a20191115


**Command used to run the converter or code if you’re using the Python API**
Used the following code to quantize the frozen graph [http://download.tensorflow.org/models/speech_commands_v0.02.zip]()
```
# the above frozen graph was saved in a directory as ""saved_model""
```
code:
import tensorflow as tf
def representative_dataset_gen():
  for _ in range(num_calibration_steps):
    # Get sample input data as a numpy array in a method of your choosing.
    yield [input]

converter = tf.lite.TFLiteConverter.from_saved_model('C:/test/saved_model')
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
converter.representative_dataset = representative_dataset_gen
tflite_quant_model = converter.convert()

**The output from the converter invocation**

```
File ""trained_model_quantize.py"", line 11, in <module>
    tflite_quant_model = converter.convert()
  File ""C:\Users\Desktop\Projects\Environments\google_speech\lib\site-packages\tensorflow_core\lite\python\lite.py"", line 421, in convert
    raise ValueError(""This converter can only convert a single ""
ValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.
```

**Also, please include a link to the saved model or GraphDef**
Tutorial: [https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md]()
 
Graph: [http://download.tensorflow.org/models/speech_commands_v0.02.zip]()
**Objective**
I'm trying to convert this model (frozen graph) which is in normal Float 32 to fixed point 8 bit. "
35030,High memory consumption with model.fit in TF 2.0.0 and 2.1.0-rc0,"**System information**

- Have I written custom code: Yes
- OS Platform and Distribution: Linux Kubuntu 18.04, kernel 5.0
- Mobile device: Not verified on mobile devices
- TensorFlow installed from: binary via `pip install tensorflow-gpu`
- TensorFlow version: `2.1.0-rc0`, however affected are also `2.0.0` and `2.0.0-rc0`, `2.0.0-rc1`, `2.0.0-rc2`
- Python version: 3.6.9
- CUDA version: 10.1 for TF 2.1.0-rc0; 10.0 for the earlier versions of TF
- cuDNN version: 7
- GPU model and memory: Nvidia GeForce GTX 1050 Ti (4GB)
- CPU model: AMD Ryzen 7 1700

**Describe the current behavior**

Model training with the Keras API consumes high amount of system memory with TF `2.0.0` and `2.1.0-rc0`, as well as in `2.0.0-rc0`, `2.0.0-rc1` and `2.0.0-rc2`. It looks like the memory used by `model.fit` is proportional to the size of the training data provided as numpy arrays, with the proportionality constant being approximately 1. In other words, if the numpy arrays `x` and `y` are, say, 8 GB in total, then `model.fit(x,y,...)` will use another 8 GB (plus some overhead). This may suggest that `model.fit` creates unnecessary copies of the data arrays. This is in contrary to TF `1.14.0`, `2.0.0-a0`, `2.0.0-b0` and `2.0.0-b1`, where `model.fit` seems to use some amount of RAM independent of the data size (and much less than 8 GB, at least in the test code attached below).

The same concerns the validation data. If validation data are passed as numpy arrays to `model.fit` via the argument `validation_data`, then the memory use of `model.fit` seems to duplicate the size of the validation data arrays with TF from `2.0.0-rc0` to `2.1.0-rc0`.

In the code attached below, one may change the variable `K` to vary the size of the data and test the above described behaviour. It is straightforward to estimate the data size: e.g. with `K=5000` the data arrays in the below code should be ca. 7.32 GB in total. The whole Python process associated with this code uses approximately this much RAM plus some overhead when running with TF `1.14.0`, `2.0.0-a0`, `2.0.0-b0` or `2.0.0-b1`. But with TF from `2.0.0-rc0` to `2.1.0-rc0` the Python process consumes twice that much RAM. One may comment out the line containing `model.fit` to check that it is the point at which the high memory consumption starts.

**Describe the expected behavior**

The size of the memory used by `model.fit` should not duplicate the size of the training and validation data passed as numpy arrays. It should be more or less independent of the size of the data arrays, similarly as in TF `1.14.0` and in the pre-releases `2.0.0-a0`, `2.0.0-b0` and `2.0.0-b1`. 

**Code to reproduce the issue**

```python
import tensorflow as tf
import numpy as np

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Lambda, Conv2D

print(""Tensorflow version: {}"".format(tf.__version__),flush=True)

K = 5000 # Number of images
N = 512  # Image size

MAX_SIGNAL = 5000 # The values of the training data range from 0 to this

def build_model():
  '''Create a simple test model.'''
  
  inputs = Input((N,N,1))
  s = Lambda(lambda x: x / MAX_SIGNAL) (inputs)
  s = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(s)
  outputs = s

  return Model(inputs=[inputs], outputs=[outputs])

# Generate some random data
x_train = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB
y_train = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB
x_val   = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB
y_val   = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB
# In total, the above arrays should be 7 680 000 kB

model = build_model()

optimizer = tf.keras.optimizers.Adam()
loss = tf.keras.losses.BinaryCrossentropy()

model.compile(optimizer=optimizer, loss=loss)
model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)
```

"
35029,failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): GKE with GPU / Ubuntu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version: **tensorflow-2.0.0-cp38-cp38-linux_x86_64**
- Python version: **3.8**
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1.168-1 / 7.6.4.38-1
- GPU model and memory: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285



**Describe the problem**
The process terminated with theses errors: 
2019-12-11 15:23:31 classification-ld8d9 tensorflow[7] WARNING Entity <bound method ResnetPreprocess.call of <src.model.ResnetPreprocess object at 0x7fb870241d60>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: invalid value for ""node"": expected ""ast.AST"", got ""<class 'NoneType'>""; to visit lists of nodes, use ""visit_block"" instead
2019-12-11 15:24:12.377087: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2019-12-11 15:24:12.377554: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-12-11 15:24:12.377749: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-12-11 15:24:12.377806: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-12-11 15:24:12.570359: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-12-11 15:24:28.471575: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-12-11 15:24:28.471626: W tensorflow/stream_executor/stream.cc:1919] attempting to perform BLAS operation using StreamExecutor without BLAS support
2019-12-11 15:24:28.471658: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Blas GEMM launch failed : a.shape=(32, 25088), b.shape=(25088, 8), m=32, n=8, k=25088
         [[{{node model/dense/MatMul}}]]
Traceback (most recent call last):
  File ""/opt/workspace/app.py"", line 83, in <module>
    training(config[""train""], config[""telegram""], config[""others""])
  File ""/opt/workspace/src/training.py"", line 40, in training
    train_model(dataset_train.dataset, dataset_val.dataset, model, config, n_classes, config_others)
  File ""/opt/workspace/src/training.py"", line 98, in train_model
    loss_train, logits, reg_loss = training_step(x_batch_train, y_batch_train)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/def_function.py"", line 520, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/function.py"", line 1823, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/function.py"", line 1137, in _filtered_call
    return self._call_flat(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/function.py"", line 1223, in _call_flat
    flat_outputs = forward_function.call(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/function.py"", line 506, in call
    outputs = execute.execute(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError:  Blas GEMM launch failed : a.shape=(32, 25088), b.shape=(25088, 8), m=32, n=8, k=25088
         [[node model/dense/MatMul (defined at usr/local/lib/python3.8/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_training_step_9298]

Function call stack:
training_step

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**

"
35028,Loading models with BuildFromBuffer crashed the app.,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Android Q
- Mobile device: Samsung Galaxy
- TensorFlow installed from: source r20
- TensorFlow version: 2.0.0
- Python version: 3.7
- GCC/Compiler version: Android NDK19

In the sample Android project I stored .tflite binary models as constant variables:

models.h:
```
extern const unsigned int  gModel1Size;
extern const unsigned char gModel1Data[];

extern const unsigned int  gModel2Size;
extern const unsigned char gModel2Data[];
```
model1.cpp:
```
const unsigned int gModel1Size = 123;
const unsigned char gModel1Data[] = { 0x00, 0x00, ... };
```
model2.cpp:
```
const unsigned int gModel2Size = 123;
const unsigned char gModel2Data[] = { 0x00, 0x00, ... };
```

I tried to load each model using the BuildFromBuffer() API:
```
mModel = tflite::FlatBufferModel::BuildFromBuffer(modelData, modelSize);
...
```

I have successfully loaded the second model, but the first one produces the following crash:

```
2019-12-11 12:33:47.421 8316-8316/com.test.sample A/libc: Fatal signal 7 (SIGBUS), code 1 (BUS_ADRALN), fault addr 0xc2120f31 in tid 8316 (viewsample), pid 8316 (viewsample)
2019-12-11 12:33:47.473 8375-8375/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
2019-12-11 12:33:47.473 8375-8375/? A/DEBUG: Revision: '27'
2019-12-11 12:33:47.473 8375-8375/? A/DEBUG: ABI: 'arm'
2019-12-11 12:33:47.473 8375-8375/? A/DEBUG: pid: 8316, tid: 8316, name: viewsample  >>> com.test.sample.viewsample <<<
2019-12-11 12:33:47.473 8375-8375/? A/DEBUG: signal 7 (SIGBUS), code 1 (BUS_ADRALN), fault addr 0xc2120f31
2019-12-11 12:33:47.473 8375-8375/? A/DEBUG:     r0  c2120f31  r1  00040ec0  r2  c20e0053  r3  00000012
2019-12-11 12:33:47.473 8375-8375/? A/DEBUG:     r4  c2120f31  r5  00000038  r6  00000000  r7  ffcfac80
2019-12-11 12:33:47.473 8375-8375/? A/DEBUG:     r8  c2120f2d  r9  c2120f31  r10 00041518  r11 ffcfad04
2019-12-11 12:33:47.473 8375-8375/? A/DEBUG:     ip  c23acdf4  sp  ffcfac30  lr  c22bf813  pc  c22bf82a
2019-12-11 12:33:47.688 8375-8375/? A/DEBUG: backtrace:
2019-12-11 12:33:47.688 8375-8375/? A/DEBUG:     #00 pc 000e482a  /data/app/com.test.sample.viewsample-taC72NjjoCTT0dQC8d9LbQ==/lib/arm/libtensorflowlite.so (tflite::InterpreterBuilder::BuildLocalIndexToRegistrationMapping()+78)
2019-12-11 12:33:47.688 8375-8375/? A/DEBUG:     #01 pc 000e523f  /data/app/com.test.sample.viewsample-taC72NjjoCTT0dQC8d9LbQ==/lib/arm/libtensorflowlite.so (tflite::InterpreterBuilder::operator()(std::__ndk1::unique_ptr<tflite::Interpreter, std::__ndk1::default_delete<tflite::Interpreter>>*, int)+50)
```
I swapped the models to check data integrity. Every time the FIRST model produces crashes, no matter which data is in.
It seems this is a data alignment issue. 
I guess that the TF code is not able to get/put the data in the app's right memory successfully that caused this issue.

Then I removed the ""const"" modifier for the data:

`unsigned char gModel1Data[] = {...}`

and cast it inside the BuildFromBuffer API:

`mModel = tflite::FlatBufferModel::BuildFromBuffer((const char*)modelData, modelSize);`

This workaround works fine but looks not natural for the API that should accept const char* by default.
Is it possible to use the model loading this way, or it may surprise me in a crucial moment by undefined behavior?
"
35027,logging broken with Python-3.8: findCaller() takes from 1 to 2 positional arguments but 3 were given,"**System information**System information
- OS Platform and Distribution: Arch Linux, 5.4.2-arch1-1-ARCH
- TensorFlow installed from: binary
- TensorFlow version: 2.1.0rc0-1
- Keras version: 2.2.4-tf
- Python version: 3.8
- GPU model and memory: 2x GTX 1080 Ti 11GB""`

**Describe the current behavior**
execution of MNIST example fails with error:

> TypeError: findCaller() takes from 1 to 2 positional arguments but 3 were given


**Code to reproduce the issue**
```
import tensorflow as tf
import tensorflow_datasets as tfds

from tensorflow.keras.optimizers import Adam

def scale(image, label):
  image = tf.cast(image, tf.float32)
  image /= 255
  return image, label

def build_model():
    filters = 56
    units = 24
    kernel_size = 5
    learning_rate = 1e-2
    model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(units, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])
    return model

datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)
mnist_train, mnist_test = datasets['train'], datasets['test']

num_train_examples = info.splits['train'].num_examples
num_test_examples = info.splits['test'].num_examples

BUFFER_SIZE = 10000
BATCH_SIZE = 128

train_dataset = mnist_train.map(scale).shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
eval_dataset = mnist_test.map(scale).shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

model = build_model()

epochs=5
model.fit(
        train_dataset,
        validation_data=eval_dataset,
        steps_per_epoch=num_train_examples/epochs,
        validation_steps=num_test_examples/epochs,
        epochs=epochs)
```

**Other info / logs**

```
traceback:
Downloading and preparing dataset mnist (11.06 MiB) to /home/graemer/tensorflow_datasets/mnist/1.0.0...
Traceback (most recent call last):
  File ""train.py"", line 23, in <module>
    datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)
  File ""/usr/lib/python3.8/site-packages/tensorflow_datasets/core/api_utils.py"", line 52, in disallow_positional_args_dec
    return fn(*args, **kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow_datasets/core/registered.py"", line 302, in load
    dbuilder.download_and_prepare(**download_and_prepare_kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow_datasets/core/api_utils.py"", line 52, in disallow_positional_args_dec
    return fn(*args, **kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_builder.py"", line 316, in download_and_prepare
    logging.warning(GCS_HOSTED_MSG, self.name)
  File ""/usr/lib/python3.8/site-packages/absl/logging/__init__.py"", line 322, in warning
    log(WARNING, msg, *args, **kwargs)
  File ""/usr/lib/python3.8/site-packages/absl/logging/__init__.py"", line 485, in log
    _absl_logger.log(standard_level, msg, *args, **kwargs)
  File ""/usr/lib/python3.8/site-packages/absl/logging/__init__.py"", line 1047, in log
    super(ABSLLogger, self).log(level, msg, *args, **kwargs)
  File ""/usr/lib/python3.8/logging/__init__.py"", line 1500, in log
    self._log(level, msg, args, **kwargs)
  File ""/usr/lib/python3.8/logging/__init__.py"", line 1565, in _log
    fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)
TypeError: findCaller() takes from 1 to 2 positional arguments but 3 were given
```"
35026,How to use a lstm layer with projection in tf2.0,Is there any way to use projection in tf.keras.layers.lstm mention in [paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf) 
35025,'found gpu 0',"<h3>Is this a error ??</h3>
<pre>2019-12-11 18:49:26.117674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-12-11 18:49:27.156050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-12-11 18:49:27.162481: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-12-11 18:49:27.187878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-11 18:49:27.260258: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-12-11 18:49:27.306165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-12-11 18:49:27.313377: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-12-11 18:49:27.320079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-11 18:49:39.396101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-11 18:49:39.400853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2019-12-11 18:49:39.405363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2019-12-11 18:49:39.458919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2996 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
</pre>"
35023,[MLIR] tf-opt can not run with -debug option,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below): 2.0.0
- Python version:3.7
- Bazel version (if compiling from source):1.0.1
- GCC/Compiler version (if compiling from source):6.4.0
- CUDA/cuDNN version: 10
- GPU model and memory: V100 


**Describe the current behavior**
in current tensorflow dialect (implemented in tensorflow/compiler/mlir)， we use LLVM_DEBUG in various place to print some debug information during compilation , and if we run tf-opt , it's expected to print these informations with -debug option added, this is also the behavior with mlir-opt, but in current master branch , if I pass a -debug option to tf-opt ,an error occured
 ```
#bazel-bin/tensorflow/compiler/mlir/tf-opt -debug
tf-opt: Unknown command line argument '-debug'.  Try: 'bazel-bin/tensorflow/compiler/mlir/tf-opt --help'
tf-opt: Did you mean '--help'?
```
but if I run mlir-opt with -debug:
```
#bin/mlir-opt -debug
Args: bin/mlir-opt -debug
```
I've also compared the main function between tf-opt(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tf_mlir_opt_main.cc ) and mlir-opt  (https://github.com/tensorflow/mlir/blob/master/tools/mlir-opt/mlir-opt.cpp ) and see no apparent difference between the two files



**Describe the expected behavior**

tf-opt can run with -debug so I can use the debug info to debug the compilation process


"
35022,Errors of building XLA AOT example,"**System information**
- Have I written custom code: No
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0.0
- Python version: 3.6.7
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: 10.0/7.5
- GPU model and memory:RTX2080Ti with 11GB

**Describe the current behavior**
I followed this [guide](https://tensorflow.google.cn/xla/tfcompile) to test xla example, all the code are the same with this guide. In the last step, when building the cc_binary, it gives some errors
```
bazel build //tensorflow/compiler/aot/tests:my_binary --verbose_failures
```
```
INFO: Analyzed target //tensorflow/compiler/aot/tests:my_binary (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /home/zoud/workspace/local/tf_build/tensorflow-2.0.0-cc/tensorflow/compiler/aot/tests/BUILD:16:1: Linking of rule '//tensorflow/compiler/aot/tests:my_binary' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/zoud/.cache/bazel/_bazel_zoud/8f979226e66ca56e3b01def87b6ccec0/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    LD_LIBRARY_PATH=:/usr/local/cuda/lib64:/home/zoud/program/TensorRT-5.1.5.0/lib \
    PATH=/usr/lib/jvm/jdk1.8.0_221/bin:/usr/lib/jvm/jdk1.8.0_221/jre/bin:/home/zoud/program/anaconda3/envs/tf_2.0.0_src/bin:/home/zoud/program/anaconda3/condabin:/usr/lib/jvm/jdk1.8.0_221/bin:/usr/lib/jvm/jdk1.8.0_221/jre/bin:/home/zoud/bin:/home/zoud/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin:/home/zoud/program/MATLAB/R2017b/bin:/home/zoud/program/upx-3.95-amd64_linux:/home/zoud/program/bazel-0.26.1:/usr/local/cuda/bin:/home/zoud/program/MATLAB/R2017b/bin:/home/zoud/program/upx-3.95-amd64_linux:/home/zoud/program/bazel-0.26.1 \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/zoud/program/anaconda3/envs/tf_2.0.0_src/bin/python \
    PYTHON_LIB_PATH=/home/zoud/program/anaconda3/envs/tf_2.0.0_src/lib/python3.6/site-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.5 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/k8-opt/bin/tensorflow/compiler/aot/tests/my_binary -pthread -Wl,-no-as-needed -pie -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -fno-canonical-system-headers -B/usr/bin -Wl,--gc-sections -Wl,@bazel-out/k8-opt/bin/tensorflow/compiler/aot/tests/my_binary-2.params)
Execution platform: @bazel_tools//platforms:host_platform
/usr/bin/ld: bazel-out/k8-opt/bin/external/com_google_absl/absl/strings/libstrings.a(charconv.o): undefined reference to symbol 'nanf@@GLIBC_2.2.5'
//lib/x86_64-linux-gnu/libm.so.6: error adding symbols: DSO missing from command line
collect2: error: ld returned 1 exit status
Target //tensorflow/compiler/aot/tests:my_binary failed to build
INFO: Elapsed time: 0.308s, Critical Path: 0.12s
INFO: 0 processes.
FAILED: Build did NOT complete successfully

```
Maybe my GCC version is too low? I noticed official environment of building TF2 is GCC7.3.

Another question, I don't understand the meaning of step1, why modify the `tf2xla.proto`, this file seems never be used in the following steps.
"
35018,Can't convert Albert to tflite by missing IdentityN support,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab
- TensorFlow installed from (source or binary): colab
- TensorFlow version (or github SHA if from source): 1.15


**Provide the text output from tflite_convert**

```
ConverterError: See console for info.
2019-12-11 07:22:00.278074: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: IdentityN
2019-12-11 07:22:00.421599: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2198 operators, 3544 arrays (0 quantized)
2019-12-11 07:22:00.563590: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 2194 operators, 3535 arrays (0 quantized)
2019-12-11 07:22:00.753664: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2194 operators, 3535 arrays (0 quantized)
2019-12-11 07:22:01.305107: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 1366 operators, 2266 arrays (0 quantized)
2019-12-11 07:22:02.013834: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 1293 operators, 2193 arrays (0 quantized)
2019-12-11 07:22:02.126476: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 1256 operators, 2125 arrays (0 quantized)
2019-12-11 07:22:02.230528: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 4: 1252 operators, 2120 arrays (0 quantized)
2019-12-11 07:22:02.347386: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 5: 1316 operators, 2260 arrays (0 quantized)
2019-12-11 07:22:02.459624: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 6: 1316 operators, 2260 arrays (0 quantized)
2019-12-11 07:22:02.583975: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 7: 1379 operators, 2410 arrays (0 quantized)
2019-12-11 07:22:02.710596: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 8: 1379 operators, 2410 arrays (0 quantized)
2019-12-11 07:22:02.837758: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 9: 1378 operators, 2420 arrays (0 quantized)
2019-12-11 07:22:02.964471: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 10: 1378 operators, 2420 arrays (0 quantized)
2019-12-11 07:22:03.097633: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 11: 1330 operators, 2335 arrays (0 quantized)
2019-12-11 07:22:03.219935: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 12: 1324 operators, 2329 arrays (0 quantized)
2019-12-11 07:22:03.341469: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 13: 1388 operators, 2469 arrays (0 quantized)
2019-12-11 07:22:03.473109: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 14: 1388 operators, 2469 arrays (0 quantized)
2019-12-11 07:22:03.608375: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 15: 1451 operators, 2619 arrays (0 quantized)
2019-12-11 07:22:03.746363: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 16: 1451 operators, 2619 arrays (0 quantized)
2019-12-11 07:22:03.888602: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 17: 1450 operators, 2629 arrays (0 quantized)
2019-12-11 07:22:04.021441: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 18: 1450 operators, 2629 arrays (0 quantized)
2019-12-11 07:22:04.166493: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 19: 1402 operators, 2544 arrays (0 quantized)
2019-12-11 07:22:04.296326: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 20: 1396 operators, 2538 arrays (0 quantized)
2019-12-11 07:22:04.435268: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 21: 1460 operators, 2678 arrays (0 quantized)
2019-12-11 07:22:04.576370: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 22: 1460 operators, 2678 arrays (0 quantized)
2019-12-11 07:22:04.727069: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 23: 1523 operators, 2828 arrays (0 quantized)
2019-12-11 07:22:04.884741: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 24: 1523 operators, 2828 arrays (0 quantized)
2019-12-11 07:22:05.035517: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 25: 1522 operators, 2838 arrays (0 quantized)
2019-12-11 07:22:05.185234: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 26: 1522 operators, 2838 arrays (0 quantized)
2019-12-11 07:22:05.345311: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 27: 1474 operators, 2753 arrays (0 quantized)
2019-12-11 07:22:05.490007: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 28: 1468 operators, 2747 arrays (0 quantized)
2019-12-11 07:22:05.664208: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 29: 1532 operators, 2887 arrays (0 quantized)
2019-12-11 07:22:05.816073: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 30: 1532 operators, 2887 arrays (0 quantized)
2019-12-11 07:22:05.979330: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 31: 1595 operators, 3037 arrays (0 quantized)
2019-12-11 07:22:06.146208: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 32: 1595 operators, 3037 arrays (0 quantized)
2019-12-11 07:22:06.311167: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 33: 1594 operators, 3047 arrays (0 quantized)
2019-12-11 07:22:06.469774: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 34: 1594 operators, 3047 arrays (0 quantized)
2019-12-11 07:22:06.641870: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 35: 1546 operators, 2962 arrays (0 quantized)
2019-12-11 07:22:06.794954: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 36: 1540 operators, 2956 arrays (0 quantized)
2019-12-11 07:22:06.961088: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 37: 1604 operators, 3096 arrays (0 quantized)
2019-12-11 07:22:07.126460: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 38: 1604 operators, 3096 arrays (0 quantized)
2019-12-11 07:22:07.298514: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 39: 1667 operators, 3246 arrays (0 quantized)
2019-12-11 07:22:07.480185: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 40: 1667 operators, 3246 arrays (0 quantized)
2019-12-11 07:22:07.682372: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 41: 1666 operators, 3256 arrays (0 quantized)
2019-12-11 07:22:07.867497: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 42: 1666 operators, 3256 arrays (0 quantized)
2019-12-11 07:22:08.062627: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 43: 1618 operators, 3171 arrays (0 quantized)
2019-12-11 07:22:08.239084: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 44: 1612 operators, 3165 arrays (0 quantized)
2019-12-11 07:22:08.428755: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 45: 1676 operators, 3305 arrays (0 quantized)
2019-12-11 07:22:08.615966: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 46: 1676 operators, 3305 arrays (0 quantized)
2019-12-11 07:22:08.811813: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 47: 1739 operators, 3455 arrays (0 quantized)
2019-12-11 07:22:09.014879: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 48: 1739 operators, 3455 arrays (0 quantized)
2019-12-11 07:22:09.220450: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 49: 1738 operators, 3465 arrays (0 quantized)
2019-12-11 07:22:09.423087: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 50: 1738 operators, 3465 arrays (0 quantized)
2019-12-11 07:22:09.634820: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 51: 1690 operators, 3380 arrays (0 quantized)
2019-12-11 07:22:09.829015: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 52: 1684 operators, 3374 arrays (0 quantized)
2019-12-11 07:22:10.034856: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 53: 1748 operators, 3514 arrays (0 quantized)
2019-12-11 07:22:10.242489: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 54: 1748 operators, 3514 arrays (0 quantized)
2019-12-11 07:22:10.449573: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 55: 1811 operators, 3664 arrays (0 quantized)
2019-12-11 07:22:10.656973: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 56: 1811 operators, 3664 arrays (0 quantized)
2019-12-11 07:22:10.887171: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 57: 1810 operators, 3674 arrays (0 quantized)
2019-12-11 07:22:11.106195: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 58: 1810 operators, 3674 arrays (0 quantized)
2019-12-11 07:22:11.342466: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 59: 1762 operators, 3589 arrays (0 quantized)
2019-12-11 07:22:11.555873: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 60: 1756 operators, 3583 arrays (0 quantized)
2019-12-11 07:22:11.779618: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 61: 1820 operators, 3723 arrays (0 quantized)
2019-12-11 07:22:12.002172: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 62: 1820 operators, 3723 arrays (0 quantized)
2019-12-11 07:22:12.230472: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 63: 1883 operators, 3873 arrays (0 quantized)
2019-12-11 07:22:12.475085: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 64: 1883 operators, 3873 arrays (0 quantized)
2019-12-11 07:22:12.715383: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 65: 1882 operators, 3883 arrays (0 quantized)
2019-12-11 07:22:12.956508: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 66: 1882 operators, 3883 arrays (0 quantized)
2019-12-11 07:22:13.200556: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 67: 1834 operators, 3798 arrays (0 quantized)
2019-12-11 07:22:13.431959: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 68: 1828 operators, 3792 arrays (0 quantized)
2019-12-11 07:22:13.676081: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 69: 1892 operators, 3932 arrays (0 quantized)
2019-12-11 07:22:13.936701: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 70: 1892 operators, 3932 arrays (0 quantized)
2019-12-11 07:22:14.198596: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 71: 1955 operators, 4082 arrays (0 quantized)
2019-12-11 07:22:14.455744: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 72: 1955 operators, 4082 arrays (0 quantized)
2019-12-11 07:22:14.709946: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 73: 1954 operators, 4092 arrays (0 quantized)
2019-12-11 07:22:14.973819: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 74: 1954 operators, 4092 arrays (0 quantized)
2019-12-11 07:22:15.247743: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 75: 1906 operators, 4007 arrays (0 quantized)
2019-12-11 07:22:15.508479: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 76: 1900 operators, 4001 arrays (0 quantized)
2019-12-11 07:22:15.768632: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 77: 1964 operators, 4141 arrays (0 quantized)
2019-12-11 07:22:16.042654: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 78: 1964 operators, 4141 arrays (0 quantized)
2019-12-11 07:22:16.315995: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 79: 2027 operators, 4291 arrays (0 quantized)
2019-12-11 07:22:16.599728: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 80: 2027 operators, 4291 arrays (0 quantized)
2019-12-11 07:22:16.876653: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 81: 2026 operators, 4301 arrays (0 quantized)
2019-12-11 07:22:17.155476: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 82: 2026 operators, 4301 arrays (0 quantized)
2019-12-11 07:22:17.455543: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 83: 1978 operators, 4216 arrays (0 quantized)
2019-12-11 07:22:17.732213: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 84: 1972 operators, 4210 arrays (0 quantized)
2019-12-11 07:22:18.041275: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 85: 2036 operators, 4350 arrays (0 quantized)
2019-12-11 07:22:18.337704: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 86: 2036 operators, 4350 arrays (0 quantized)
2019-12-11 07:22:18.645178: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 87: 2099 operators, 4500 arrays (0 quantized)
2019-12-11 07:22:18.960635: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 88: 2099 operators, 4500 arrays (0 quantized)
2019-12-11 07:22:19.269232: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 89: 2098 operators, 4510 arrays (0 quantized)
2019-12-11 07:22:19.562883: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 90: 2098 operators, 4510 arrays (0 quantized)
2019-12-11 07:22:19.889353: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 91: 2050 operators, 4425 arrays (0 quantized)
2019-12-11 07:22:20.190117: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 92: 2044 operators, 4416 arrays (0 quantized)
2019-12-11 07:22:20.501514: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 93: 2108 operators, 4556 arrays (0 quantized)
2019-12-11 07:22:20.810962: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 94: 2108 operators, 4556 arrays (0 quantized)
2019-12-11 07:22:21.129394: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 95: 2171 operators, 4706 arrays (0 quantized)
2019-12-11 07:22:21.450294: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 96: 2171 operators, 4706 arrays (0 quantized)
2019-12-11 07:22:21.782083: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 97: 2170 operators, 4716 arrays (0 quantized)
2019-12-11 07:22:22.105969: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 98: 2170 operators, 4716 arrays (0 quantized)
2019-12-11 07:22:22.442836: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 99: 2158 operators, 4698 arrays (0 quantized)
2019-12-11 07:22:22.764761: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 100: 2155 operators, 4692 arrays (0 quantized)
2019-12-11 07:22:23.096868: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 2155 operators, 4692 arrays (0 quantized)
2019-12-11 07:22:23.324249: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 2155 operators, 4692 arrays (0 quantized)
2019-12-11 07:22:23.549716: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 5111808 bytes, theoretical optimal value: 4718592 bytes.
2019-12-11 07:22:23.613548: I tensorflow/lite/toco/toco_tooling.cc:439] Estimated count of arithmetic ops: 22884682880 ops, equivalently 11442341440 MACs
2019-12-11 07:22:23.613795: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 11065920
2019-12-11 07:22:23.618965: E tensorflow/lite/toco/toco_tooling.cc:481] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CAST, DIV, FULLY_CONNECTED, GATHER, MEAN, MUL, PACK, POW, RESHAPE, RSQRT, SLICE, SOFTMAX, SQUARED_DIFFERENCE, SUB, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: IdentityN.
Traceback (most recent call last):
  File ""/usr/local/bin/toco_from_protos"", line 8, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CAST, DIV, FULLY_CONNECTED, GATHER, MEAN, MUL, PACK, POW, RESHAPE, RSQRT, SLICE, SOFTMAX, SQUARED_DIFFERENCE, SUB, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: IdentityN.

```

Also, please include a link to a GraphDef or the model if possible.
-> I am using ""bert-for-tf2"" to load albert_base.
```
def load_pretrained_albert():
    model_name = ""albert_base""
    model_dir = bert.fetch_tfhub_albert_model(model_name, "".models"")
    model_params = bert.albert_params(model_name)
    l_bert = bert.BertModelLayer.from_params(model_params, name=""albert"")

    # use in Keras Model here, and call model.build()
    max_seq_len = 128

    l_input_ids = Input(shape=(max_seq_len,), dtype='int32', name=""l_input_ids"")
    #l_token_type_ids = Input(shape=(max_seq_len,), dtype='int32')

    output = l_bert(l_input_ids)                              # output: [batch_size, max_seq_len, hidden_size]
    pooled_output = AveragePooling1D(pool_size=max_seq_len, data_format=""channels_last"")(output)
    pooled_output = Flatten()(pooled_output)


    model = Model(inputs=[l_input_ids], outputs=[pooled_output])
    model.build(input_shape=(None, max_seq_len))

    bert.load_albert_weights(l_bert, model_dir)

    return model
```



Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35017,tensorflow 2.0 use ParameterServerStrategy error,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**

* **use tensorflow docker hub images:  tensorflow / tensorflow : 2.0.0-gpu**

**Describe the current behavior**
* when i use tf2.0 distributed training with ParameterServerStrategy,  the error occurred follow:
```
2019-12-11 14:53:27.569524: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-12-11 14:53:27.619245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:2f:00.0
2019-12-11 14:53:27.621064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:86:00.0
2019-12-11 14:53:27.622349: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-12-11 14:53:27.627608: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-12-11 14:53:27.632513: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-12-11 14:53:27.634123: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-12-11 14:53:27.640466: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-12-11 14:53:27.643992: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-12-11 14:53:27.656264: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-12-11 14:53:27.660960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1
2019-12-11 14:53:27.661734: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-12-11 14:53:27.676662: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600000000 Hz
2019-12-11 14:53:27.684246: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x565309993340 executing computations on platform Host. Devices:
2019-12-11 14:53:27.684288: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-12-11 14:53:27.955807: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5653099f5a60 executing computations on platform CUDA. Devices:
2019-12-11 14:53:27.955947: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
2019-12-11 14:53:27.955978: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla P100-PCIE-16GB, Compute Capability 6.0
2019-12-11 14:53:27.958412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:2f:00.0
2019-12-11 14:53:27.959282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:86:00.0
2019-12-11 14:53:27.959338: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-12-11 14:53:27.959361: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-12-11 14:53:27.959381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-12-11 14:53:27.959391: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-12-11 14:53:27.959413: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-12-11 14:53:27.959423: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-12-11 14:53:27.959434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-12-11 14:53:27.962606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1
2019-12-11 14:53:27.962639: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-12-11 14:53:27.964634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-11 14:53:27.964653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1
2019-12-11 14:53:27.964663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N Y
2019-12-11 14:53:27.964679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   Y N
2019-12-11 14:53:27.967224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 6.0)
2019-12-11 14:53:27.968872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15216 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:86:00.0, compute capability: 6.0)
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Traceback (most recent call last):
  File ""tf_mnist_ps_worker.py"", line 136, in <module>
    main()
  File ""tf_mnist_ps_worker.py"", line 132, in main
    app.train(args)
  File ""tf_mnist_ps_worker.py"", line 85, in train
    model = Net().model
  File ""tf_mnist_ps_worker.py"", line 32, in __init__
    tf.keras.layers.Dense(10, activation='softmax')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/tracking/base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/sequential.py"", line 114, in __init__
    self.add(layer)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/tracking/base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/sequential.py"", line 196, in add
    output_tensor = layer(self.outputs[0])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 817, in __call__
    self._maybe_build(inputs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 2141, in _maybe_build
    self.build(input_shapes)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/layers/core.py"", line 1027, in build
    trainable=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 2311, in __setattr__
    if val.trainable:
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/variables.py"", line 477, in trainable
    raise NotImplementedError
NotImplementedError
```

**Code to reproduce the issue**
* my demo code:
```
import os
import json
import argparse

import tensorflow as tf
from tensorflow.keras import datasets
from tensorflow.keras import layers, models
from tensorflow.keras import optimizers

class Net(object):
    def __init__(self):
        model = models.Sequential()
        model.add(layers.Conv2D(
            32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
        model.add(layers.MaxPooling2D((2, 2)))
        model.add(layers.Conv2D(64, (3, 3), activation='relu'))
        model.add(layers.MaxPooling2D((2, 2)))
        model.add(layers.Conv2D(64, (3, 3), activation='relu'))

        model.add(layers.Flatten())
        model.add(layers.Dense(64, activation='relu'))
        model.add(layers.Dense(10, activation='softmax'))

        model.summary()

        self.model = model


# inital dateset
class DataSet(object):
    def __init__(self):
        data_path = os.path.dirname(os.path.realpath(__file__)) \
                    + '/../../datasets/mnist/mnist.npz'
        (train_images, train_labels), (test_images, test_labels) = \
            datasets.mnist.load_data(path=data_path)
        train_images = train_images.reshape((60000, 28, 28, 1))
        test_images = test_images.reshape((10000, 28, 28, 1))

        train_images, test_images = train_images / 255.0, test_images / 255.0

        self.train_images, self.train_labels = train_images, train_labels
        self.test_images, self.test_labels = test_images, test_labels


# train and val
class Train:
    def __init__(self):
        self.data = DataSet()

    def train(self, args):
        # Define the checkpoint directory to store the checkpoints
        checkpoint_dir = args.train_dir
        # Name of the checkpoint files
        checkpoint_path = os.path.join(checkpoint_dir, ""ckpt_{epoch}"")

        callbacks = [
            tf.keras.callbacks.TensorBoard(log_dir='./logs'),
            tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                               save_weights_only=True),
        ]

        with strategy.scope():
            model = Net().model

            model.compile(optimizer=optimizers.Adam(),
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])

        model.fit(self.data.train_images, self.data.train_labels,
                  batch_size=args.batch_size,
                  epochs=args.epochs,
                  callbacks=callbacks,
                  validation_data=(self.data.test_images, self.data.test_labels))

        # EVAL
        model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
        eval_loss, eval_acc = model.evaluate(
            self.data.test_images, self.data.test_labels, verbose=2)
        print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))


def main():
    # training params settings
    parser = argparse.ArgumentParser(description='Tensorflow 2.0 MNIST Example,'
                                                 ' use Mirrorstrategy')
    parser.add_argument('--batch_size', '-b', type=int, default=64,
                        help='input batch size for training (default: 64)')
    parser.add_argument('--test_batchsize', '-tb', type=int, default=1000,
                        help='input batch size for testing (default: 1000)')
    parser.add_argument('--epochs', '-e', type=int, default=10,
                        help='number of epochs to train (default: 10)')
    parser.add_argument('--learning_rate', '-lr', type=float, default=0.01,
                        help='learning rate (default: 0.01)')
    parser.add_argument('--momentum', type=float, default=0.5,
                        help='SGD momentum (default: 0.5)')
    parser.add_argument('--log_interval', type=int, default=10,
                        help='how many batches to wait before logging training status')
    parser.add_argument('--save_model', '-sm', action='store_true', default=False,
                        help='For Saving the current Model')

    args = parser.parse_args()

    app = Train()
    app.train(args)


if __name__ == ""__main__"":
   main()
```

* and my cmd line:

```
TF_CONFIG='{""cluster"": {""worker"": [""10.240.208.106:12345"", ""10.240.208.108:12345""], ""ps"": [""10.240.208.106:12346"", ""10.240.208.108:12346""]}, ""task"": {""index"": 1, ""type"": ""worker""}}' python tf_mnist_ps_worker.py
```

"
35014,Build error due to missing dependency declarations in double-conversion,"**System information**
- OS Platform and Distribution : CentOS-8.0
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0
- Python version: 3.7.4
- Installed using virtualenv? pip? conda?: from source using spack
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): gcc-7.4.0
- CUDA/cuDNN version: CUDA@10.1.243/cuDNN@7.6.5.32-10.1
- GPU model and memory: 2x NVIDIA Quadro P4000

**Describe the problem**

I used [`spack`](https://github.com/spack/spack/) to build all dependencies for tensorflow including bazel. As per the build logs from spack, the following procedure was attempted to install tensorflow from source : 
```
'./configure'
```
```
'bazel' '--nohome_rc' '--nosystem_rc' '--output_user_root=/tmp/spack/tf' 'build' '--color=no' '--jobs=16' '--config=opt' '--config=cuda' '--config=noaws' '--config=nohdfs' '--config=noignite' '--config=nokafka' '--config=v2' '--cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0' '//tensorflow/tools/pip_package:build_pip_package'
```
Full error log from spack : [error_log](https://pastebin.com/n6qfsrVr)

The error comes from `'@double_conversion//:double-conversion': ` : 
```
[sajid@xrmlite ~]$ cat error_log | grep -a3b3 ERROR
80532-INFO: Deleting stale sandbox base /tmp/spack/tf/ba2957b26f360b54b039cbd58767fa4e/sandbox
80621-[6 / 38] [Prepa] BazelWorkspaceStatusAction stable-status.txt
80683-[48 / 1,263] [Prepa] Executing genrule @local_config_nccl//:nccl-files [for host] ... (6 actions, 5 running)
80792:ERROR: /tmp/spack/tf/ba2957b26f360b54b039cbd58767fa4e/external/double_conversion/BUILD.bazel:12:1: undeclared inclusion(s) in rule '@double_conversion//:double-conversion':
80965-this rule is missing dependency declarations for the following files included by 'external/double_conversion/double-conversion/double-conversion.cc':
81115-  '/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.4.0/gettext-0.20.1-yjkttko6qwne6igwp2cabop2d4p2g3ff/include/libintl.h'
81256-[75 / 1,297] Executing genrule @local_config_cuda//cuda:cuda-lib [for host]; 2s local ... (14 actions, 2 running)
[sajid@xrmlite ~]$
```

Additional Info : 

The full dependency tree for building `py-tensoflow` is show below. As can be seen, `gettext` is a dependency for `python` built from source. This `python` is both a dependency of `bazel` and the `python` we want to eventually link `py-tensorflow` against. 

```
[sajid@xrmlite ~]$ spack spec -I  py-tensorflow cuda_arch=61  ^/ppi2muw  ^cuda@10.1.243 ^python@3.7.4 %gcc@7.4.0

.....

Concretized
--------------------------------
 -   py-tensorflow@2.0.0%gcc@7.4.0~android~aws~computecpp+cuda cuda_arch=61 ~dynamic_kernels+gcp~gdr~hdfs~ignite~ios~jemalloc~kafka~mkl~monolithic~mpi+nccl~ngraph~numa~opencl patches=c49766a976e5c25c3827036828df0a2630e511bd783ac9cdcc6fc13068b22fac ~rocm~tensorrt~verbs~xla arch=linux-centos8-broadwell
[+]      ^bazel@0.26.1%gcc@7.4.0 patches=3e6448a0dde42bbd72568d29c5646d370dd62ca300cdd10a630908c086844167,75fad08d2a118372ed86696832942c7903bb716af28e5af969d8e20857655cf4,aa926467d3fc2bcd338ccc6355bc9f56adfd18dd3b4e1813a4ce8daee9e34600 arch=linux-centos8-broadwell
[+]          ^jdk@1.8.0_131-b11%gcc@7.4.0 arch=linux-centos8-broadwell
[+]          ^python@3.7.4%gcc@7.4.0+bz2+ctypes+dbm+lzma~nis~optimizations+pic+pyexpat+pythoncmd+readline+shared+sqlite3+ssl~tix~tkinter~ucs4~uuid+zlib arch=linux-centos8-broadwell
[+]              ^bzip2@1.0.8%gcc@7.4.0+shared arch=linux-centos8-broadwell
[+]              ^expat@2.2.9%gcc@7.4.0+libbsd arch=linux-centos8-broadwell
[+]                  ^libbsd@0.10.0%gcc@7.4.0 arch=linux-centos8-broadwell
[+]              ^gdbm@1.18.1%gcc@7.4.0 arch=linux-centos8-broadwell
[+]                  ^readline@8.0%gcc@7.4.0 arch=linux-centos8-broadwell
[+]                      ^ncurses@6.1%gcc@7.4.0~symlinks~termlib arch=linux-centos8-broadwell
[+]              ^gettext@0.20.1%gcc@7.4.0+bzip2+curses+git~libunistring+libxml2+tar+xz arch=linux-centos8-broadwell
[+]                  ^libxml2@2.9.9%gcc@7.4.0~python arch=linux-centos8-broadwell
[+]                      ^libiconv@1.16%gcc@7.4.0 arch=linux-centos8-broadwell
[+]                      ^xz@5.2.4%gcc@7.4.0 arch=linux-centos8-broadwell
[+]                      ^zlib@1.2.11%gcc@7.4.0+optimize+pic+shared arch=linux-centos8-broadwell
[+]                  ^tar@1.32%gcc@7.4.0 arch=linux-centos8-broadwell
[+]              ^libffi@3.2.1%gcc@7.4.0 arch=linux-centos8-broadwell
[+]              ^openssl@1.1.1d%gcc@7.4.0+systemcerts arch=linux-centos8-broadwell
[+]              ^sqlite@3.30.1%gcc@7.4.0~column_metadata+fts~functions~rtree arch=linux-centos8-broadwell
[+]      ^cuda@10.1.243%gcc@7.4.0 arch=linux-centos8-broadwell
[+]      ^cudnn@7.6.5.32-10.1-linux-x64%gcc@7.4.0 arch=linux-centos8-broadwell
[+]      ^curl@7.63.0%gcc@7.4.0~darwinssl~gssapi~libssh~libssh2~nghttp2 arch=linux-centos8-broadwell
[+]      ^nccl@2.4.8-1%gcc@7.4.0 patches=42778c78eb9875dacddf5eca20f7f6a077773fcbee41e51174f81b3143684b6d arch=linux-centos8-broadwell
[+]      ^py-absl-py@0.7.0%gcc@7.4.0 arch=linux-centos8-broadwell
[+]          ^py-setuptools@41.4.0%gcc@7.4.0 arch=linux-centos8-broadwell
[+]          ^py-six@1.12.0%gcc@7.4.0 arch=linux-centos8-broadwell
[+]      ^py-astor@0.8.0%gcc@7.4.0 patches=edc5eeddabe153b08e938f52edaeb2d880ee3128082967f310db0f98510fe6e0 arch=linux-centos8-broadwell
[+]      ^py-gast@0.2.2%gcc@7.4.0 arch=linux-centos8-broadwell
[+]      ^py-google-pasta@0.1.8%gcc@7.4.0 arch=linux-centos8-broadwell
[+]      ^py-grpcio@1.25.0%gcc@7.4.0 arch=linux-centos8-broadwell
[+]          ^c-ares@1.15.0%gcc@7.4.0 build_type=RelWithDebInfo arch=linux-centos8-broadwell
[+]              ^cmake@3.15.5%gcc@7.4.0~doc+ncurses+openssl+ownlibs patches=3387faf4a71efe81c0fa17410b270ca7d352081ac88d2322df3da9bb6a6a3f2d ~qt arch=linux-centos8-broadwell
[+]          ^py-cython@0.29.13%gcc@7.4.0 arch=linux-centos8-broadwell
[+]      ^py-keras-applications@1.0.8%gcc@7.4.0 arch=linux-centos8-broadwell
[+]      ^py-keras-preprocessing@1.1.0%gcc@7.4.0 arch=linux-centos8-broadwell
[+]      ^py-numpy@1.17.4%gcc@7.4.0+blas+lapack arch=linux-centos8-broadwell
[+]          ^openblas@0.3.7%gcc@7.4.0+avx2~avx512 cpu_target=auto ~ilp64+pic+shared threads=none ~virtual_machine arch=linux-centos8-broadwell
[+]      ^py-opt-einsum@3.1.0%gcc@7.4.0 arch=linux-centos8-broadwell
[+]      ^py-protobuf@3.11.0%gcc@7.4.0~cpp arch=linux-centos8-broadwell
[+]      ^py-termcolor@1.1.0%gcc@7.4.0 arch=linux-centos8-broadwell
[+]      ^py-wheel@0.33.1%gcc@7.4.0 arch=linux-centos8-broadwell
[+]      ^py-wrapt@1.11.2%gcc@7.4.0 arch=linux-centos8-broadwell
[+]      ^swig@4.0.0%gcc@7.4.0 arch=linux-centos8-broadwell
[+]          ^pcre@8.42%gcc@7.4.0~jit+multibyte+utf arch=linux-centos8-broadwell
[+]          ^pkgconf@1.6.3%gcc@7.4.0 arch=linux-centos8-broadwell
```

The environment for building `py-tensorflow` is described in [`spack-build-env.txt`](https://pastebin.com/MuWi6BAK). 

Contents of [`.bazelrc`](https://pastebin.com/6TTEMMSu)

The spack [build recipe](https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/py-tensorflow/package.py) for `py-tensorlfow`. 

Downstream tracker : https://github.com/spack/spack/issues/14105

"
35013,ValueError while passing a SparseTensor input to the Dense layer.,"**System information**
- OS Platform and Distribution: MacOS Mojave
- TensorFlow installed from: conda install tensorflow
- TensorFlow version: 2.0.0
- Python version: 3.6

I have generated a smaller version of my issue:
```
from tensorflow.keras import layers
inputs = layers.Input(shape=(256,), sparse=False, name='name_sparse')
x = layers.Dense(32, name=""my_layer"")(inputs)
print(x)
```
Output
`Tensor(""my_layer/Identity:0"", shape=(None, 32), dtype=float32)`

But if I change `sparse=True`, I get a ValueError: 
`The last dimension of the inputs to Dense should be defined. Found None.`

**Other info / logs**
I am able to run it on TF1.14 with Keras 2.2.4.
"
35012,Gradient of matmul in while_loop works when run eagerly but not as tf.function,"**System information**
- Have I written custom code: Example provided below
- OS Platform and Distribution: Both Windows 10 and Google Colab
- TensorFlow installed from binary
- TensorFlow version: Both 2.0.0 and 2.1.0-rc0
- Python version: 3.6 and 3.7.4

**Describe the current behavior**
If I multiply tensors together in a while_loop I can compute higher order gradients when running eagerly, but not when running inside a function with the @tf.function decorator. If I manually unroll the loop, it works either way, so the problem must be related to the interaction between tf.function and tf.while_loop.

> TypeError: in converted code:
> 
>     <ipython-input-17-50c1c9a72b0f>:17 func  *
>         d3y_dx3 = t.gradient(d2y_dx2, x)
>     /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py:999 gradient
>         if not backprop_util.IsTrainable(t):
>     /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop_util.py:30 IsTrainable
>         dtype = dtypes.as_dtype(dtype)
>     /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/dtypes.py:725 as_dtype
>         (type_value,))
> 
>     TypeError: Cannot convert value None to a TensorFlow DType.

**Describe the expected behavior**
The output of this function should be the same regardless of the @tf.function decorator:
> [[0 0 4]]
> [[[-2 1 10]]]
> [[[8 2 20]]]
> [[[-18 6 30]]]
> [[[24 24 24]]]

**Code to reproduce the issue**
Run without and then with @tf.function:
```
import tensorflow as tf

x = tf.Variable([[[-1.0]], [[0.0]], [[1.0]]])

# @tf.function
def func():
    with tf.GradientTape(persistent=True) as t:
        t.watch(x)
        pv = x
        
        n = 4
        c = lambda i, _: i < n
        b = lambda i, pv: (i+1, tf.concat([x, tf.matmul(pv, x)], axis=1))
        _, pv = tf.while_loop(c, b, (tf.constant(1), pv), shape_invariants=(tf.constant(1).shape, tf.TensorShape([3, None, 1])))

        # manual loop unrolling works fine both ways:
        # pv = tf.concat([x, tf.matmul(pv, x)], axis=1)
        # pv = tf.concat([x, tf.matmul(pv, x)], axis=1)
        # pv = tf.concat([x, tf.matmul(pv, x)], axis=1)
        
        y = tf.reduce_sum(pv, axis=1)
        dy_dx = t.gradient(y, x)
        d2y_dx2 = t.gradient(dy_dx, x)
        d3y_dx3 = t.gradient(d2y_dx2, x)
        d4y_dx4 = t.gradient(d3y_dx3, x)
    del t

    tf.print(tf.transpose(y)) # transpose to print on one line
    tf.print(tf.transpose(dy_dx))
    tf.print(tf.transpose(d2y_dx2))
    tf.print(tf.transpose(d3y_dx3))
    tf.print(tf.transpose(d4y_dx4))

func()
```

"
35011,"Derivative of pow(x, y) can give nan when x=0 for higher order derivatives","**System information**
- Have I written custom code: Example provided below
- OS Platform and Distribution: Both Windows 10 and Google Colab
- TensorFlow installed from binary
- TensorFlow version: Both 2.0.0 and 2.1.0-rc0
- Python version: 3.7.4

**Describe the current behavior**
The output of pow(x, y) is not correct for higher order derivatives when x is 0 and when y is other than a single value. For case 2 below I get:
[0 0 4]
[[-2 1 10]]
[[8 -nan 20]]
[[-18 -nan 30]]
[[24 -nan 24]]
[[0 -nan 0]]

For case 1 the nan only shows up in the 5th derivative, which is still not great, but not as bad:
[1 0 1]
[[-4 0 4]]
[[12 0 12]]
[[-24 0 24]]
[[24 24 24]]
[[-0 -nan 0]]

**Describe the expected behavior**
The output of this example for case 1 should be:
[1 0 1]
[[-4 0 4]]
[[12 0 12]]
[[-24 0 24]]
[[24 24 24]]
[[-0 0 0]]

The output of this example for case 2 should be:
[0 0 4]
[[-2 1 10]]
[[8 0 20]]
[[-18 0 30]]
[[24 0 24]]
[[0 0 0]]

**Code to reproduce the issue**

```
import tensorflow as tf

x = tf.Variable([[-1.0], [0.0], [1.0]])

with tf.GradientTape(persistent=True) as t:
    t.watch(x)
    # case 1: y = x^4
    # y = tf.reduce_sum(tf.pow(x, 4), axis=1) # gives nan for 5th derivative at x=0
    # case 2: y = x + x^2 + x^3 + x^4
    y = tf.reduce_sum(tf.pow(x, [[1, 2, 3, 4]]), axis=1) # gives nan for 2nd to 5th derivative at x=0
    dy_dx = t.gradient(y, x)
    d2y_dx2 = t.gradient(dy_dx, x)
    d3y_dx3 = t.gradient(d2y_dx2, x)
    d4y_dx4 = t.gradient(d3y_dx3, x)
    d5y_dx5 = t.gradient(d4y_dx4, x)
del t

tf.print(y)
tf.print(tf.transpose(dy_dx)) # transpose only to fit on one line when printed
tf.print(tf.transpose(d2y_dx2))
tf.print(tf.transpose(d3y_dx3))
tf.print(tf.transpose(d4y_dx4))
tf.print(tf.transpose(d5y_dx5))
```
"
35010,Memory leak with tf.py_function in eager mode using TF 2.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.6
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.3 64bit
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
I customize a Keras layer and use `tf.py_function` in the `call` of the layer. The used memory keeps increasing linearly with the number of calling the layer with inputs in eager mode. What's 

![image](https://user-images.githubusercontent.com/18071380/70585663-d5744a00-1bff-11ea-8d41-3649d035e9be.png)

**Describe the expected behavior**
The memory should keep stable.

**Code to reproduce the issue**
```python
import psutil
import numpy as np
import tensorflow as tf


class TestLayer(tf.keras.layers.Layer):
    def __init__(
        self,
        output_dim,
        **kwargs
    ):
        super(TestLayer, self).__init__(**kwargs)
        self.output_dim = output_dim

    def build(self, input_shape):
        self.built = True

    def call(self, inputs):
        batch_embedding = tf.py_function(
            self.mock_output, inp=[inputs], Tout=tf.float64,
        )
        return batch_embedding

    def mock_output(self, inputs):
        shape = inputs.shape.as_list()
        batch_size = shape[0]
        return tf.constant(np.random.random((batch_size,self.output_dim)))


test_layer = TestLayer(1000)

for i in range(1000):
    test_layer.call(tf.constant(np.random.randint(0,100,(256,10))))
    if i % 100 == 0:
        used_mem = psutil.virtual_memory().used
        print('used memory: {} Mb'.format(used_mem / 1024 / 1024))
```

**other info**
We encounter this issue when developing a customized embedding layer in [ElasticDL](https://github.com/sql-machine-learning/elasticdl) and resolving the ElasticDL [issue 1567](https://github.com/sql-machine-learning/elasticdl/issues/1567)

"
35008,convert to tflite failed using tf.lite.TFLiteConverter.from_saved_model and validate failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
when i convert saved lstm tensorflow model to the tflite file and meets the error.
this is piece of code :
def create_LSTM_model(inputs):
    W = {
        
        'hidden': tf.Variable(tf.keras.backend.random_normal([N_FEATURES, N_HIDDEN_UNITS])),
        'output': tf.Variable(tf.keras.backend.random_normal([N_HIDDEN_UNITS, N_CLASSES]))
        'output': tf.Variable(init_w_output)
    }
    biases = {
        'hidden': tf.Variable(tf.keras.backend.random_normal([N_HIDDEN_UNITS], mean=1.0))
        'output': tf.Variable(tf.keras.backend.random_normal([N_CLASSES]))
    }

    X = tf.transpose(inputs, [1, 0, 2])
    X = tf.reshape(X, [-1, N_FEATURES])
    hidden = tf.nn.relu(tf.matmul(X, W['hidden']) + biases['hidden'])
    hidden = tf.split(hidden, N_TIME_STEPS, 0)

    def get_a_cell(lstm_size, keep_prob):
        lstm = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(lstm_size)
        drop = tf.compat.v1.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)
        return drop

    # lstm_layers = [tf.compat.v1.nn.rnn_cell.BasicLSTMCell(N_HIDDEN_UNITS, forget_bias=1.0) for _ in range(2)]
    lstm_size = N_HIDDEN_UNITS
    keep_prob = 0.9
    num_layers = 2
    # lstm_layers = tf.compat.v1.nn.rnn_cell.MultiRNNCell([get_a_cell(lstm_size, keep_prob) for _ in range(num_layers)])
    lstm_layers = tf.compat.v1.nn.rnn_cell.MultiRNNCell([get_a_cell(lstm_size, keep_prob) for _ in range(num_layers)])

    outputs, _ = tf.compat.v1.nn.static_rnn(lstm_layers, hidden, dtype=tf.float32)
    # keras.layers.RNN  outputs, _ = tf.keras.layers.RNN(lstm_layers, hidden, dtype=tf.float32)

    # Get output for the last time step
    lstm_last_output = outputs[-1]

    return tf.matmul(lstm_last_output, W['output']) + biases['output']


tf.compat.v1.reset_default_graph()

X = tf.compat.v1.placeholder(tf.float32, [None, N_TIME_STEPS, N_FEATURES], name=""input_X"")
Y = tf.compat.v1.placeholder(tf.float32, [None, N_CLASSES], name='input_Y')

pred_Y = create_LSTM_model(X)
tf.identity(pred_Y, name=""pred_Y"")

pred_softmax = tf.nn.softmax(pred_Y, name=""pred_softmax"")

train_count = len(X_train)
L2_LOSS = 0.0015

l2 = L2_LOSS * \
     sum(tf.nn.l2_loss(tf_var) for tf_var in tf.compat.v1.trainable_variables())

loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred_Y, labels=Y)) + l2
# loss=tf.reduce_mean(tf.square(tf.reshape(pred,[-1,2])-tf.reshape(Y, [-1,2])))
tf.identity(loss, name=""saved_loss"")


global_step = tf.Variable(0, trainable=False, name='global_step')

learning_rate = tf.compat.v1.train.exponential_decay(
    LEARNING_RATE_BASE,
    global_step,
    train_count / BATCH_SIZE, LEARNING_RATE_DECAY)

tf.identity(learning_rate, name=""learning_rate"")

optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)
tf.identity(loss, name=""saved_optimizer"")

correct_pred = tf.equal(tf.argmax(pred_softmax, 1), tf.argmax(Y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32), name='output_accuracy')

history = dict(train_loss=[],
               train_acc=[],
               test_loss=[],
               test_acc=[])

sess = tf.compat.v1.InteractiveSession()
sess.run(tf.compat.v1.global_variables_initializer())

export_simple_path = 'model_simple/'
if os.path.exists(export_simple_path):
    shutil.rmtree(export_simple_path)

tf.compat.v1.saved_model.simple_save(sess,
            export_simple_path,
            inputs={""input_X"": X},
            outputs={""pred_softmax"": pred_softmax})
use this to save the model
and then i convert the .pb model to tflite with python API

import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                       tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
but got the error:
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, FULLY_CONNECTED, GREATER_EQUAL, LOGISTIC, MUL, SOFTMAX, SPLIT, TANH. Here is a list of operators for which you will need custom implementations: RandomUniform.
Traceback (most recent call last):
  File ""c:\users\d\.conda\envs\ai\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\d\.conda\envs\ai\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\d\.conda\envs\ai\Scripts\toco_from_protos.exe\__main__.py"", line 7, in <module>
  File ""c:\users\d\.conda\envs\ai\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 89, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""c:\users\d\.conda\envs\ai\lib\site-packages\tensorflow_core\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""c:\users\d\.conda\envs\ai\lib\site-packages\absl\app.py"", line 299, in run
    _run_main(main, args)
  File ""c:\users\d\.conda\envs\ai\lib\site-packages\absl\app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""c:\users\d\.conda\envs\ai\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 52, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, FULLY_CONNECTED, GREATER_EQUAL, LOGISTIC, MUL, SOFTMAX, SPLIT, TANH. Here is a list of operators for which you will need custom implementations: RandomUniform.

and then i add the
converter.allow_custom_ops=True

then i can convert to tflite (lstm.tflite) successfully
but then i use below command to validate my model then i got the errors:

interpreter = tf.lite.Interpreter(model_path=""lstm.tflite"")

interpreter.allocate_tensors()

>>> interpreter = tf.lite.Interpreter(model_path=""D:\Feature\model_simple_1207\lstm_har_1207.tflite"")
INFO: Initialized TensorFlow Lite runtime.
>>> interpreter.allocate_tensors()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\d\.conda\envs\ai\lib\site-packages\tensorflow_core\lite\python\interpreter.py"", line 244, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File ""C:\Users\d\.conda\envs\ai\lib\site-packages\tensorflow_core\lite\python\interpreter_wrapper\tensorflow_wrap_interpreter_wrapper.py"", line 106, in AllocateTensors
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
**R**untimeError: Encountered unresolved custom op: RandomUniform.Node number 14 (RandomUniform) failed to prepare.****

could anyone  hlep with the issue?

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**

"
35007,Attention/AdditiveAttention Issue,"I have the following functions to return two models:


```
def get_question_model(self, embedding):
        question_input = Input(shape=(None,), name='question_input')
        question_embedding = embedding(question_input)

        cnn_1d = Conv1D(128, 4, padding='same', activation='relu', strides=1)(question_embedding)
        cnn_1d = AveragePooling1D(pool_size=3)(cnn_1d)

        model = Model(question_input, cnn_1d)
        return model

def get_sentence_model(self, embedding, question_model):
        sentence_input = Input(shape=(None,),  name='sentence_input')
        sentence_embedding = embedding(sentence_input)
        cnn_1d = Conv1D(128, 4, padding=""same"", activation='relu', strides=1)(sentence_embedding)
        cnn_1d = AveragePooling1D(pool_size=3)(cnn_1d)

        sentence_attention = AdditiveAttention()([cnn_1d, question_model])
        model = Model(sentence_input, sentence_attention)
        model.shape = model.output_shape
        return model
```

And the functions are called:
```
   self.question_model = self.get_question_model(embedding)
   self.sentence_model = self.get_sentence_model(embedding, self.question_model)
```
#####

I get the following issue:


```
2019-12-10 18:17:01.848676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9371 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:83:00.0, compute capability: 7.0)
Traceback (most recent call last):
  File ""D:/Development/competitions/Kaggle/Tensorflow 2.0 - Q_a_A/tensorflow_2.0/QuestionAnswerContextAttention.py"", line 376, in <module>
    qa_model.fit(input=input, output=y)
  File ""D:/Development/competitions/Kaggle/Tensorflow 2.0 - Q_a_A/tensorflow_2.0/QuestionAnswerContextAttention.py"", line 152, in fit
    self.model = self.get_model()
  File ""D:/Development/competitions/Kaggle/Tensorflow 2.0 - Q_a_A/tensorflow_2.0/QuestionAnswerContextAttention.py"", line 123, in get_model
    self.sentence_model = self.get_sentence_model(embedding, self.question_model)
  File ""D:/Development/competitions/Kaggle/Tensorflow 2.0 - Q_a_A/tensorflow_2.0/QuestionAnswerContextAttention.py"", line 84, in get_sentence_model
    sentence_attention = AdditiveAttention()([cnn_1d, question_model])
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 887, in __call__
    self._maybe_build(inputs)
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 2141, in _maybe_build
    self.build(input_shapes)
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\keras\layers\dense_attention.py"", line 406, in build
    v_shape = tensor_shape.TensorShape(input_shape[1])
**TypeError: 'NoneType' object is not subscriptable**
```






"
35004,BinaryCrossentropy incorrect partial reduction of loss when reduction='none',"**System information**
 custom code
- OS Platform and Distribution
ubuntu 18.04
- TensorFlow installed from (source or binary):
pip install tensorflow-gpu
- TensorFlow version (use command below):
v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version:
3.6.9

**Describe the current behavior**
BinaryCrossentropy does a reduction on the last dimension even if you pass in reduction='none'

**Describe the expected behavior**
It should not do any reduction

**Workaround**
use tf.nn.sigmoid_cross_entropy_with_logits()

**Code to reproduce the issue**
```
import numpy as np

import tensorflow as tf

def main():
    y = np.array([0., 0., 1., 1.]).reshape((1, 1, 1, -1))
    x = np.array([1., 1., 1., 0.]).reshape((1, 1, 1, -1))
    print(y.shape)
    print(x.shape)

    bce_reduce = tf.keras.losses.BinaryCrossentropy()
    loss = bce_reduce(y, x)
    print('correct: fully reduced loss (reduction=default)', loss.numpy())

    bce = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)
    loss = bce(y, x)
    print('incorrect: should be not be reduced (reduction=none): ', loss.numpy())  # Loss: 11.522857
    print('incorrect: reduced along last dimension:', loss.shape)

    # should be the same as:
    correct_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=x)
    print('correct: unreduced_loss (using sigmoid_cross_entropy_with_logits)', correct_loss)
    print('correct: unreduced_loss_shape', correct_loss.shape)


main()
```
output:
```
correct: fully reduced loss (reduction=default) 11.568711280822754
incorrect: should be not be reduced (reduction=none):  [[[11.56871128]]]
incorrect: reduced along last dimension: (1, 1, 1)
correct: unreduced_loss (using sigmoid_cross_entropy_with_logits) tf.Tensor([[[[1.31326169 1.31326169 0.31326169 0.69314718]]]], shape=(1, 1, 1, 4), dtype=float64)
correct: unreduced_loss_shape (1, 1, 1, 4)

```
"
35003,The label_image example fails on VGG19 network,"I call it with the 224x224 image of orange:
```
./label_image \
        --image orange.bmp \
        --labels imagenet_class_index.txt \
        --tflite_model vgg19_tf1_14.tflite
```
and it outputs this:
```
NFO: Initialized TensorFlow Lite runtime.
invoked 
average time: 1180.25 ms 
0.0352545: 669 mosquito_net
0.0291345: 794 shower_curtain
0.0180968: 549 envelope
0.0168044: 999 toilet_tissue
0.0155437: 314 cockroach
```

It might be that input normalization is a problem, but ```label_image -h``` doesn't say anything about normalization.
"
35002,Inconsistent loading of saved model,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Linux
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7.3

**Describe the current behavior**

I have several models saved using `model.save` as hdf5 objects. However, when I try to load them from a script via:

```python
tf.keras.models.load_model(DATA_DIR + ""models/{}.h5"".format(filename))
```

I get an I/O error:

```
""Unable to open object (file read failed: time = Tue Dec 10 16:36:14 2019\n, filename = '/home/fonnesbeck/models/Slider-rhb-rhp-attack.h5', file descriptor = 14, errno = 5, error message = 'Input/output error', buf = 0x7f642c733da0, total read size = 272, bytes this sub-read = 272, bytes actually read = 18446744073709551615, offset = 8443360)""
```

Yet, when I manually go in and import the file manually from the terminal, it works fine:

```
>>> import tensorflow as tf
>>> tf.keras.models.load_model('Slider-rhb-rhp-attack.h5')
2019-12-10 16:34:45.948071: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions tha
t this TensorFlow binary was not compiled to use: AVX2 FMA
2019-12-10 16:34:46.355657: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2019-12-10 16:34:46.356406: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557d96a7f470 executing 
computations on platform Host. Devices:
2019-12-10 16:34:46.356451: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, De
fault Version
<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fa474307400>
```

Even stranger, after having done the manual import, running the script works fine. this seems to happen with every new file.
"
34998,Classification of Forum for Microcontrollers Vs Android/other OS Apps,"Hi Experts,

I found the issues reported are mixed of platforms used like Desktop, Android, micontroller and other OS. Is there feature to classify it only based on micro-controller based APPS.

It will help better understand the types of problems and issues.
"
34997,ValueError: You are trying to load a weight file containing 5 layers into a model with 0 layers,"I have defined a custom model and custom train using tf2.0
Not able to load weights of the model from checkpoint or .h5 file."
34996,Compilation issue on Example program of TF Lite,"**System information**
- OS Platform and Distribution : Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:Desktop
- TensorFlow installed from (source or binary): Source https://github.com/tensorflow/tensorflow.git
- TensorFlow version: Release 1.15.0
- Python version:
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Default compilation of example folder (Hello World) leads to missing header file reference and shows error.


**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Install Ubuntu 18.04 OS
2. Download git using command ""git clone https://github.com/tensorflow/tensorflow.git""
3. Install build essentials using command ""sudo apt install build-essential""
4. Install curl library using command ""sudo apt install curl""
5. Issue command ""make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test""

**Any other info / logs**
make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test
g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -fno-rtti -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/logical.cc -o tensorflow/lite/micro/tools/make/gen/linux_x86_64/obj/tensorflow/lite/micro/kernels/logical.o
In file included from ./tensorflow/lite/kernels/internal/reference/binary_function.h:18:0,
                 from tensorflow/lite/micro/kernels/logical.cc:16:
./tensorflow/lite/kernels/internal/common.h:24:10: fatal error: fixedpoint/fixedpoint.h: No such file or directory
 #include ""fixedpoint/fixedpoint.h""
          ^~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
tensorflow/lite/micro/tools/make/Makefile:257: recipe for target 'tensorflow/lite/micro/tools/make/gen/linux_x86_64/obj/tensorflow/lite/micro/kernels/logical.o' failed
make: *** [tensorflow/lite/micro/tools/make/gen/linux_x86_64/obj/tensorflow/lite/micro/kernels/logical.o] Error 1"
34992,How to convert ByteBuffer to Bitmap Image?,"I am working on creating an image super-resolution application that uses a TensorFlow Lite model. The model gives the output Image in the form of ByteBuffer and I convert the ByteBuffer to Bitmap. Next, I display this Bitmap but nothing shows up. The code I am using can be seen below:

```
ByteBuffer out = ByteBuffer.allocate(4 * 384 * 384 * 3);
tflite.run(byteBuffer,out);

byte[] imageBytes= new byte[out.remaining()];
out.get(imageBytes);
final Bitmap outPut_Image = BitmapFactory.decodeByteArray(imageBytes,0,imageBytes.length);
//Toast.makeText(this,tflite.toString(),Toast.LENGTH_LONG).show();
Toast.makeText(this, ""Working"",Toast.LENGTH_LONG).show();
ImageView imageView = (ImageView) this.findViewById(R.id.imageView2);
imageView.setImageBitmap(outPut_Image);
```
Please advise me what I am doing wrong here."
34991,_pywrap_tensorflow_internal.so: cannot open shared object file: No such file or directory,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): [Ubuntu 18.04.3 LTS](https://wiki.odroid.com/odroid-c2/os_images/ubuntu/v3.1) on ODROID C2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not a mobile but the hardware is ODROID C2
- TensorFlow installed from (source or binary): source (cross-compiled for ODROID C2)
- TensorFlow version: 2.0.0
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: Using virtualenv, installed using pip, with a bazel built .whl file
- Bazel version (if compiling from source): 0.26.0
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: N/A
- GPU model and memory: Mali450MP3 (But GPU not used)



**Describe the problem**
Bazel build from source and pip install successful, but doing `import tensorflow` results in following error -
```
ImportError: /home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: cannot open shared object file: No such file or directory
```
The file is not missing, it exists.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
- Setup a docker container for Ubuntu 18.04.3 and open a terminal into it
- Follow all instructions from [official build from source docs](https://www.tensorflow.org/install/source)
    - Use `python3-dev  python3-pip`
    - Use `virtualenv` to install python dependencies
    - Install Bazel binary directly (no bazelisk), version 0.26.0 and add it to your $PATH
    - Do not use GPU support
    - Clone Tensorflow from official Github repo, and use tag v2.0.0
    - During `./configure`, say Yes to `XLA JIT` and No to everything else
    - During `./configure`, Set the optimization flag to `-march=armv8-a+crc`
    - Use Bazel build command : `bazel build --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package`
- Transfer the built `.whl` to ODROID C2 and rename `tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl` to `tensorflow-2.0.0-cp36-cp36m-linux_aarch64.whl`
- Setup Python on the device using `sudo apt-get install python3-dev python3-pip`
- Setup a virtualenv using `virtualenv -p python3.6 ./venv` and activate it
- Install Tensorflow using `pip install tensorflow-2.0.0-cp36-cp36m-linux_aarch64.whl`
    - Install HDF5 system package dependency using `sudo apt-get install pkg-config libhdf5-100 libhdf5-dev`
- Run Python interpreter and execute `import tensorflow`
- You will get the above error regarding `_pywrap_tensorflow_internal.so`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

- The `/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so` file exists and is 223M in size
- Output of `ldd /home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so` gives `not a dynamic executable`
- Full error Traceback
```
>>> import tensorflow
Traceback (most recent call last):
  File ""/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/rmc/code/venv/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/rmc/code/venv/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow/__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/home/rmc/code/venv/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/rmc/code/venv/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/rmc/code/venv/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```
"
34990,"Tensorboard: Can'r reach localhost:6006 after ""fuser 6006/tcp -k""","Hi!

I just installed Tensorboard and everything worked fine. I noticed that the old loss-curves where kept in the plots and after reading online I took the suggestion to kill the process by:

fuser 6006/tcp -k

When I nor run Tensorboard and get:
Reusing TensorBoard on port 6006 (pid 48869), started 1:40:42 ago. (Use '!kill 48869' to kill it.)
Please visit http://localhost:6006 in a web browser.

The webpage cannot be reached (Unable to connect). I have probably screwed something up that a reboot did not solve... Can someone help me to un-screw myself?

Thanks!
btw, first post so sorry if the format is off!"
34989,Dataset from TFRecord has unknown shapes.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 8
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: 1080Ti 12G * 4

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I follow the [tf2.0 tutorial](https://www.tensorflow.org/tutorials/load_data/tfrecord) to generate and read the dataset into tf.Dataset. However, the read serialized record data has unknown shapes and unable to call ```strategy.experimental_distribute_dataset``` to use mirror strategy.

Note that I can load data normally using 
```for f0, f1, f2, f3 in train_dataset:```
 for single GPU training.

**Describe the expected behavior**

The loaded tf.Dataset should have correct shapes and be able to be distributed using ```strategy.experimental_distribute_dataset```. The features all have fixed shapes, but I don't know how to define them.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
raw_train_dataset = tf.data.TFRecordDataset('path/to/the/record')

def read_tfrecord(serialized_example):
    feature_description = {
        'feature0': tf.io.FixedLenFeature([], tf.string),
        'feature1': tf.io.FixedLenFeature([], tf.string),
        'feature2': tf.io.FixedLenFeature([], tf.string),
        'feature3': tf.io.FixedLenFeature([], tf.string)
    }

    example = tf.io.parse_single_example(serialized_example, feature_description)
    f0 = tf.io.parse_tensor(example['feature0'], tf.uint8)
    f1 = tf.io.parse_tensor(example['feature1'], tf.float32)
    f2 = tf.io.parse_tensor(example['feature2'], tf.int16)
    f3 = tf.io.parse_tensor(example['feature3'], tf.uint8)

    return f0, f1, f2, f3

train_dataset = raw_train_dataset.map(read_tfrecord)
train_dataset = strategy.experimental_distribute_dataset(train_dataset)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Before running the last line of code, the printed ```train_dataset``` is 
```<MapDataset shapes: (<unknown>, <unknown>, <unknown>, <unknown>), types: (tf.uint8, tf.float32, tf.int16, tf.uint8)>```. 
And after running the last line to distribute the dataset for mirror strategy, it raises an error ```ValueError: Cannot take the length of shape with unknown rank.```.
"
34988,whl is not a supported wheel on this platform,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 19.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): checked out from git, tag r1.15
- TensorFlow version: 1.15
- Python version: python 3.6.8
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.0 / 7.6.3.30-1+cuda10.0
- GPU model and memory: GTX 1080 Ti
- CPU model: Intel i9-9900K


**Describe the problem**
I have compiled successfully without any error, but when I try to install the wheel package I got the following error.
`ERROR: tensorflow-1.15.0-cp36-cp36m-linux_x86_64-optimized.whl is not a supported wheel on this platform.`


**Provide the exact sequence of commands / steps that you executed before running into the problem**
`bazel build -c opt --copt=-march=native --copt=-mfpmath=both --copt=-mfma --copt=-msse4.2 --copt=-msse4.1 --copt=-mavx --copt=-mavx2 --config=cuda -k //tensorflow/tools/pip_package:build_pip_package`

`./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`

`python3 -m venv tf`
`. tf/bin/activate`
`pip install --upgrade pip`
`pip install /tmp/tensorflow_pkg/tensorflow-1.15.0-cp36-cp36m-linux_x86_64.whl`

`ERROR: tensorflow-1.15.0-cp36-cp36m-linux_x86_64-optimized.whl is not a supported wheel on this platform.`"
34987,The quantized model has low accuracy on Android,"I use these code `https://github.com/tensorflow/models/tree/master/research/slim` train mobilenet V2 model. Then I freeze graph `mobilenet_v2.pb`. The last run this command to get  tensorflow lite model.
```shell
bazel build tensorflow/lite/toco:toco

bazel-bin/tensorflow/lite/toco/toco \
  --input_file=/mnt/d/tmp/mobilenet_v2.pb \
  --output_file=/mnt/d/tmp/mobilenet_v2.tflite \
  --inference_type=QUANTIZED_UINT8 \                                                                    
  --input_arrays=input \
  --output_arrays=MobilenetV2/Predictions/Reshape_1 \
  --input_shapes=1,224,224,3 \
  --mean_values=128 \
  --std_values=128 \
  --change_concat_input_ranges=false \
  --allow_custom_ops
```

```java
imgData = ByteBuffer.allocateDirect(ddims[2] * ddims[3] * 3);

Interpreter.Options options = new Interpreter.Options();
options.setNumThreads(NUM_THREADS);
tflite = new Interpreter(file, options);

    private ByteBuffer getScaledMatrix(Bitmap bitmap) {
        imgData.rewind();
        bitmap.getPixels(intValues, 0, ddims[2], 0, 0, ddims[2], ddims[3]);
        for (int i = 0; i < ddims[2]; ++i) {
            for (int j = 0; j < ddims[3]; ++j) {
                int pixelValue = intValues[i * ddims[2] + j];
                imgData.put((byte) ((pixelValue >> 16) & 0xFF));
                imgData.put((byte) ((pixelValue >> 8) & 0xFF));
                imgData.put((byte) (pixelValue & 0xFF));
            }
        }
        if (bitmap.isRecycled()) {
            bitmap.recycle();
        }
        return imgData;
    }


ByteBuffer inputData = getScaledMatrix(bmp);
byte[][] labelProbArray = new byte[1][NUM_CLASS];
tflite.run(inputData, labelProbArray);
```
**But the accuracy of the results is very low.**

The accuracy of the non-quantized model did not decrease."
34986,"how to understand "" Do not call this op with the   output of `softmax`, as it will produce incorrect results. ""","So why?
[tf.nn.softmax_cross_entropy_with_logits_v2](https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/ops/nn_ops.py#L3108-L3224)"
34984,DLL Load Failed in tensorflow-gpu,"Whenever i try to run my python script, i keep getting this error. My environment specifications are:

Python: 3.7.4
CUDA: 9.0
Tensorflow-gpu: 1.14.0

Error:

Traceback (most recent call last):
  File ""O:\WinPython\WPy64-3741\python-3.7.4.amd64\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""O:\WinPython\WPy64-3741\python-3.7.4.amd64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""O:\WinPython\WPy64-3741\python-3.7.4.amd64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""O:\WinPython\WPy64-3741\python-3.7.4.amd64\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""O:\WinPython\WPy64-3741\python-3.7.4.amd64\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""demo_cli.py"", line 3, in <module>
    from synthesizer.inference import Synthesizer
  File ""C:\Users\DELL\Desktop\Project\Voice_Cloning\Real-Time-Voice-Cloning-master\synthesizer\inference.py"", line 1, in <module>
    from synthesizer.tacotron2 import Tacotron2
  File ""C:\Users\DELL\Desktop\Project\Voice_Cloning\Real-Time-Voice-Cloning-master\synthesizer\tacotron2.py"", line 3, in <module>
    from synthesizer.models import create_model
  File ""C:\Users\DELL\Desktop\Project\Voice_Cloning\Real-Time-Voice-Cloning-master\synthesizer\models\__init__.py"", line 1, in <module>
    from .tacotron import Tacotron
  File ""C:\Users\DELL\Desktop\Project\Voice_Cloning\Real-Time-Voice-Cloning-master\synthesizer\models\tacotron.py"", line 1, in <module>
    import tensorflow as tf
  File ""O:\WinPython\WPy64-3741\python-3.7.4.amd64\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""O:\WinPython\WPy64-3741\python-3.7.4.amd64\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""O:\WinPython\WPy64-3741\python-3.7.4.amd64\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""O:\WinPython\WPy64-3741\python-3.7.4.amd64\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""O:\WinPython\WPy64-3741\python-3.7.4.amd64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""O:\WinPython\WPy64-3741\python-3.7.4.amd64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""O:\WinPython\WPy64-3741\python-3.7.4.amd64\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""O:\WinPython\WPy64-3741\python-3.7.4.amd64\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.
"
34983,ValueError: tf.function-decorated function tried to create variables on non-first call,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0
- Python version:  3.6.7
- CUDA/cuDNN version: 7.6.0
- GPU model and memory: 1660Ti

**Describe the current behavior**
Error raised when I tried to train a model with function decorated with @tf.function by two different optimizer sequentially
ValueError: tf.function-decorated function tried to create variables on non-first call

**Describe the expected behavior**
The model can be trained by two different optimizer sequentially

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras.layers import Flatten, Dense
    from tensorflow.keras import Model
    
    try: 
        gpus= tf.config.experimental.list_physical_devices('GPU')
        tf.config.experimental.set_memory_growth(gpus[0], True)
    except:
        print('Failed to configure GPU')
    
    cifar10 = tf.keras.datasets.cifar10
    
    (x_train, y_train), (x_test, y_test) = cifar10.load_data()
    x_train = x_train / 255.0
    
    
    class MyModel(Model):
        def __init__(self):
            super(MyModel, self).__init__()
            self.flatten = Flatten()
            self.d1 = Dense(20, activation='relu')
            self.d2 = Dense(10, activation='softmax')
    
        def call(self, x):
            x = self.flatten(x)
            x = self.d1(x)
            return self.d2(x)
    
    
    optimizer = keras.optimizers.Adam(0.001)
    optimizer2 = keras.optimizers.SGD(0.01)
    
    model = MyModel()
    
    
    @tf.function  # it will work correctly if this line is disabled but the training becomes very slow
    def train_step(model, data, labels, optimizer):
        with tf.GradientTape() as tape:
            pred = model(data)
            loss = keras.losses.SparseCategoricalCrossentropy()(labels, pred)
    
        grads = tape.gradient(loss, model.trainable_variables)
    
        optimizer.apply_gradients(zip(grads, model.trainable_variables))
    
    
    train_step(model, x_train[:10], y_train[:10], optimizer)  # this works
    train_step(model, x_train[:10], y_train[:10], optimizer2)  # it can't work


**Other info / logs**

    ---------------------------------------------------------------------------
    ValueError                                Traceback (most recent call last)
    <ipython-input-7-f4e941ca2116> in <module>
          1 train_step(model, x_train[:10], y_train[:10], optimizer)
    ----> 2 train_step(model, x_train[:10], y_train[:10], optimizer2)
    
    ~/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
        455 
        456     tracing_count = self._get_tracing_count()
    --> 457     result = self._call(*args, **kwds)
        458     if tracing_count == self._get_tracing_count():
        459       self._call_counter.called_without_tracing()
    
    ~/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
        485       # In this case we have created variables on the first call, so we run the
        486       # defunned version which is guaranteed to never create variables.
    --> 487       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
        488     elif self._stateful_fn is not None:
        489       # Release the lock early so that multiple threads can perform the call
    
    ~/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
       1820   def __call__(self, *args, **kwargs):
       1821     """"""Calls a graph function specialized to the inputs.""""""
    -> 1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
       1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
       1824 
    
    ~/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
       2148         graph_function = self._function_cache.primary.get(cache_key, None)
       2149         if graph_function is None:
    -> 2150           graph_function = self._create_graph_function(args, kwargs)
       2151           self._function_cache.primary[cache_key] = graph_function
       2152         return graph_function, args, kwargs
    
    ~/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
       2039             arg_names=arg_names,
       2040             override_flat_arg_shapes=override_flat_arg_shapes,
    -> 2041             capture_by_value=self._capture_by_value),
       2042         self._function_attributes,
       2043         # Tell the ConcreteFunction to clean up its graph once it goes out of
    
    ~/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
        913                                           converted_func)
        914 
    --> 915       func_outputs = python_func(*func_args, **func_kwargs)
        916 
        917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,
    
    ~/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)
        356         # __wrapped__ allows AutoGraph to swap in a converted function. We give
        357         # the function a weak reference to itself to avoid a reference cycle.
    --> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)
        359     weak_wrapped_fn = weakref.ref(wrapped_fn)
        360 
    
    ~/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)
        903           except Exception as e:  # pylint:disable=broad-except
        904             if hasattr(e, ""ag_error_metadata""):
    --> 905               raise e.ag_error_metadata.to_exception(e)
        906             else:
        907               raise
    
    ValueError: in converted code:
    
        <ipython-input-2-db5119c2db0a>:27 train_step  *
            optimizer.apply_gradients(zip(grads, model.trainable_variables))
        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:433 apply_gradients
            _ = self.iterations
        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:541 __getattribute__
            return super(OptimizerV2, self).__getattribute__(name)
        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:655 iterations
            aggregation=tf_variables.VariableAggregation.ONLY_FIRST_REPLICA)
        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:805 add_weight
            aggregation=aggregation)
        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py:744 _add_variable_with_custom_getter
            **kwargs_for_getter)
        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py:139 make_variable
            shape=variable_shape if variable_shape else None)
        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:258 __call__
            return cls._variable_v1_call(*args, **kwargs)
        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:219 _variable_v1_call
            shape=shape)
        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:65 getter
            return captured_getter(captured_previous, **kwargs)
        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py:413 invalid_creator_scope
            ""tf.function-decorated function tried to create ""
    
        ValueError: tf.function-decorated function tried to create variables on non-first call.

How should I correct it if possible? Or it's still a bug?
related issue  #27120
Thanks!"
34982,Can I train with keras float16 and convert it to tensorflow-lite float16 model,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
34981,[Java-tensorflow-seed] Restoring saved model with dropout applied from java program,"
**This is my first issue, and I read the guide to post. I am not sure whether it is a new feature request. Otherwise, please correct me.**


I have a pre-trained model with dropout applied and I want to restore it from java program.

With my application, in a inference session, I need to turn on dropout and repeat feeding input to the model multiple times and get an array of predictions. Another feeding session, I get another array.

In python, I set seed as follow: 
`slim.dropout(input, keep_prob, is_training = phase_train, scope='dropout_mask', seed=1)`

In java, I want the same inference (same array) but it is not, even though I reinitialized the session every time. 
I know that the dropout mask is changed over time.
I guess I need ""seed"" variable as what we have in python API. But I couldn't find any reference.

What I tried: To get the same prediction list, I need to load the model and initialize the session multiple times. But it is not optimal as it takes time and can cause memory related issue.

Is there any ""seed"" variable in java API or similar things to control the output?



"
34980,Missing symbols from C++ API,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): stock example
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ArchLinux x86_64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.1.0-rc0
- Python version: 3.8
- Bazel version (if compiling from source): 1.2.0
- GCC/Compiler version (if compiling from source): 9.2
- CUDA/cuDNN version: 10.2.89/7.6.5.32
- GPU model and memory: NVidia GTX 980

**Describe the current behavior**
Trying to compile an example results in broken linking:
```c++
// basic.cpp
// tensorflow/cc/example/example.cc

#include ""tensorflow/cc/client/client_session.h""
#include ""tensorflow/cc/ops/standard_ops.h""
#include ""tensorflow/core/framework/tensor.h""

int main() {
  namespace tf = tensorflow;
  namespace tfo = tensorflow::ops;
  tf::Scope root = tf::Scope::NewRootScope();
  // Matrix A = [3 2; -1 0]
  auto A = tfo::Const(root, { {3.f, 2.f}, {-1.f, 0.f} });
  // Vector b = [3 5]
  auto b = tfo::Const(root, { {3.f, 5.f} });
  // v = Ab^T
  auto v = tfo::MatMul(root.WithOpName(""v""), A, b, tfo::MatMul::TransposeB(true));
  std::vector<tf::Tensor> outputs;
  tf::ClientSession session(root);
  // Run and fetch v
  TF_CHECK_OK(session.Run({v}, &outputs));
  // Expect outputs[0] == [19; -3]
  LOG(INFO) << outputs[0].matrix<float>();
  return 0;
}
```
trying to compile with `g++ basic.cpp -I/usr/include/tensorflow  -ltensorflow_cc -ltensorflow_framework  -o basic` fails:
```bash
make -C tensorflow/basic run
make[1]: Entering directory '/home/gizdov/Git/arch-package-tests/tensorflow/basic'
g++ basic.cpp -I/usr/include/tensorflow  -ltensorflow_cc -ltensorflow_framework  -o basic
/usr/bin/ld: /tmp/ccidGpWX.o: in function `main':
basic.cpp:(.text+0x363): undefined reference to `tensorflow::ClientSession::ClientSession(tensorflow::Scope const&)'
/usr/bin/ld: basic.cpp:(.text+0x3ea): undefined reference to `tensorflow::ClientSession::Run(std::vector<tensorflow::Output, std::allocator<tensorflow::Output> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const'
/usr/bin/ld: basic.cpp:(.text+0x52b): undefined reference to `tensorflow::ClientSession::~ClientSession()'
/usr/bin/ld: basic.cpp:(.text+0x70d): undefined reference to `tensorflow::ClientSession::~ClientSession()'
collect2: error: ld returned 1 exit status
make[1]: *** [Makefile:6: basic] Error 1
make[1]: Leaving directory '/home/gizdov/Git/arch-package-tests/tensorflow/basic'
make: *** [Makefile:11: tensorflow/basic] Error 2
```

**Describe the expected behavior**
Correct symbols should be exposed in `libtensorflow_cc.so` so that linking can work properly.

**Code to reproduce the issue**
source included above, `Makefile` provided below
```Makefile
CXX=g++
BINS=basic
INCL_DIR=$(shell pkg-config tensorflow_cc --cflags)
LIBS=$(shell pkg-config tensorflow_cc --libs)
$(BINS): $(BINS).cpp
	$(CXX) $< $(INCL_DIR) $(LIBS) -o $@
.PHONY: clean run
run: $(BINS)
	./$<
clean:
	rm -rf $(BINS)
```

**Other info / logs**
Provided are `nm` dumps with all available symbols that are exposed in `libtensorflow.so`, `libtensorflow_cc` and `libtensorflow_framework`:
[libtensorflow.so.2.1.0rc0-nm-symbol-dump.log](https://github.com/tensorflow/tensorflow/files/3942585/libtensorflow.so.2.1.0rc0-nm-symbol-dump.log)
[libtensorflow_cc.so.2.1.0rc0-nm-symbol-dump.log](https://github.com/tensorflow/tensorflow/files/3942586/libtensorflow_cc.so.2.1.0rc0-nm-symbol-dump.log)
[libtensorflow_framework.so.2.1.0rc0-nm-symbol-dump.log](https://github.com/tensorflow/tensorflow/files/3942587/libtensorflow_framework.so.2.1.0rc0-nm-symbol-dump.log)

"
34977,Cannot build submodel with layer of nested model as output ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: 3.6.8
- CUDA/cuDNN version: 10
- GPU model and memory: Nvidia GTX 1060 6GB

**Describe the current behavior**
(see also the StackOverflow on this: https://stackoverflow.com/questions/59191855/how-to-build-a-submodel-in-keras-functional-api-ending-at-a-layer-of-a-nested-m)

Usually when using the functional API to define a model one can build a submodel starting and ending at any of the original models layers. However, when models become many and large, it can become very convenient to encapsulate some parts of the models into different python functions. These then build another model (potentially multi input/output, hence not Sequentials), which can be used in the functional API just like a layer. The graph will differ due to the additional Input layer, but the computations should be the same. However, when trying to build a submodel from such a nested model that has an output which originally was a layer of the nested model, a  ""Graph disconnected"" error is thrown.

**Describe the expected behavior**
The Graph very clearly is connected, given that the nested model in its entirety is an executable, connected graph. It should therefore be possible to build submodels, no matter how deeply nested the model is, as it should always be possible to select subgraphs.

**Code to reproduce the issue**

```python
import os
import tensorflow as tf
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# NOT NESTED
inp = tf.keras.Input((4,))
y = tf.keras.layers.Dense(4, name=""od_1"")(inp)
y = tf.keras.layers.Dense(2, name=""od_2"")(y)
y = tf.keras.layers.Dense(4, name=""id_1"")(y)
y = tf.keras.layers.Dense(10, name=""od_3"")(y)
y = tf.keras.layers.Dense(10, name=""od_4"")(y)
final_model = tf.keras.Model(inputs=[inp], outputs=[y])
final_model.summary()

sub_model = tf.keras.Model(inputs=[final_model.input], outputs=[final_model.get_layer(""id_1"").output])
sub_model.summary()

# NESTED
inp_1 = tf.keras.Input(shape=(2,))
x = tf.keras.layers.Dense(4, name=""id_1"")(inp_1)
inner_model = tf.keras.Model(inputs=[inp_1], outputs=[x], name=""inner_model"")

inp_outer = tf.keras.Input((4,))
y = tf.keras.layers.Dense(4, name=""od_1"")(inp_outer)
y = tf.keras.layers.Dense(2, name=""od_2"")(y)
y = inner_model(y)
y = tf.keras.layers.Dense(10, name=""od_3"")(y)
y = tf.keras.layers.Dense(10, name=""od_4"")(y)
final_model = tf.keras.Model(inputs=[inp_outer], outputs=[y])
final_model.summary()

sub_model = tf.keras.Model(inputs=[final_model.input], outputs=[final_model.get_layer(""inner_model"").get_layer(""id_1"").output])
sub_model.summary()

```

**Other info / logs**
```
    Traceback (most recent call last):
      File ""/home/***/test_submodel_acces.py"", line 35, in <module>
        sub_model = tf.keras.Model(inputs=[final_model.input], outputs=[final_model.get_layer(""inner_model"").get_layer(""id_1"").output])
      File ""/home/***/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 146, in __init__
        super(Model, self).__init__(*args, **kwargs)
      File ""/home/***/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py"", line 167, in __init__
        self._init_graph_network(*args, **kwargs)
      File ""/home/***/venv/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py"", line 457, in _method_wrapper
        result = method(self, *args, **kwargs)
      File ""/home/***/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py"", line 320, in _init_graph_network
        self.inputs, self.outputs)
      File ""/home/***/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py"", line 1625, in _map_graph_network
        str(layers_with_complete_input))
    ValueError: Graph disconnected: cannot obtain value for tensor Tensor(""input_2:0"", shape=(None, 2), dtype=float32) at layer ""input_2"". The following previous layers were accessed without issue: []
```"
34976,"C:\Users\david\Anaconda3\envs\amajiration\lib\site-packages\tensorflow\python\framework\dtypes.py:523: FutureWarning: Passing (type, 1) or  '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.   _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])","C:\Users\david\Anaconda3\envs\amajiration\lib\site-packages\tensorflow\python\framework\dtypes.py:523: FutureWarning: Passing (type, 1) or 
'1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])

I got this when I ran the test code I don't know what it means"
34975,[TF 2.0 API Docs] `tf.keras.callbacks.LearningRateScheduler` (Very small update),"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
Doc Link:
https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler
Code Link:
https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/callbacks.py#L1311-L1358

## Description of issue (what needs changing):

### Parameters

The next API for `scheduler` parameter of `LearningRateScheduler` takes in 2 parameters, `epoch` and `lr` (learning rate) instead of just `epoch`, this is evident in the `on_epoch_begin` of the `LearningRateScheduler` method.

The documentation for this method is still outdated, the docs and the example code still shows the `scheduler` function takes in only `epoch` instead of both `epoch` and `lr`. I think the doc should be updated to reflect the new API.

Proposed change to the doc:

1) update the description of `scheduler`: 

`
schedule: a function that takes an epoch index as input (integer, indexed from 0) and current learning rate and returns a new learning rate as output (float).
`
(copied from the doc from keras.io)

2) update the example usage to include a `scheduler` that utilize the current learning rate as well.

I hope this is helpful! Happy to contribute if needed.

### Submit a pull request? Yes, if this should be updated.

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
34968,Classification signature on tf serving.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `no`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 18.04`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `no`
- TensorFlow installed from (source or binary): `no`
- TensorFlow version (use command below): `2.0.0`
- Python version: `3.7.3`
- Bazel version (if compiling from source): `-`
- GCC/Compiler version (if compiling from source): `-`
- CUDA/cuDNN version: `-`
- GPU model and memory: `-`

**Describe the current behavior**

It seem currently *not* possible to export a `classify` signature to use within `Tensorflow Serving`.

**Describe the expected behavior**
It should be possible to export a `classify` signature to use within `Tensorflow Serving`.
I was able to experience the issue also by exporting directly a `tf.Module` and a subclassed `tf.Model`. In [this](https://github.com/tensorflow/serving/issues/1420#issuecomment-552475340) comment I was also wondering about both the `inference` and `classify` signatures being *deprecated* since it looks like in [_generate_signatures](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/save.py#L466) the `method_name` is assigned to `signature_constants.PREDICT_METHOD_NAME`.

**Code to reproduce the issue**
```python
import tensorflow as tf
import numpy as np

model = tf.keras.Sequential([
    tf.keras.layers.Dense(2, activation=""softmax""),
])
model.compile(optimizer=""Adam"", loss=""categorical_crossentropy"", metrics=[""accuracy""])
X = [0.42]
Y = [[0, 1]]
model.fit(X, Y, epochs=1)
pred_before = model.predict(X)
print(f""*** pred : {pred_before} ***"")
model.save(""./mymodel/1"", save_format=""tf"")
model = tf.keras.models.load_model(""./mymodel/1"")
model.predict([X])
print(f""*** signatures : {model.signatures} ***"")
pred_after = model([X])
np.testing.assert_almost_equal(pred_before, pred_after)
print(model.predict([X]))
```

```bash
docker run -p 8501:8501 -v /path/to/mymodel:/models/mymodel -e MODEL_NAME=mymodel --name serving_tmp tensorflow/serving
```

```
curl -XPOST http://localhost:8501/v1/models/mymodel:classify -d '{""examples"": [{}]}'
# { ""error"": ""Expected classification signature method_name to be tensorflow/serving/classify. Was: tensorflow/serving/predict"" }
```

**Other info / logs**
```
{ ""error"": ""Expected classification signature method_name to be tensorflow/serving/classify. Was: tensorflow/serving/predict"" }
```

If I modify `tensorflow_core/python/saved_model/save.py@469`
```python
# from
method_name=signature_constants.PREDICT_METHOD_NAME)

# to
method_name=signature_constants.CLASSIFY_METHOD_NAME)
``` 

I'm then able to get what expected:
```
curl -XPOST http://localhost:8501/v1/models/mymodel:classify -d '{""examples"": [{}]}'
{ ""error"": ""No classification inputs found in SignatureDef: inputs {\n  key: \""text\""\n  value {\n    name: \""serving_default_text:0\""\n    dtype: DT_FLOAT\n    tensor_shape {\n      dim {\n        size: -1\n      }\n      dim {\n        size: -1\n      }\n    }\n  }\n}\noutputs {\n  key: \""probabilities\""\n  value {\n    name: \""StatefulPartitionedCall_2:0\""\n    dtype: DT_FLOAT\n    tensor_shape {\n      dim {\n        size: -1\n      }\n      dim {\n        size: 99\n      }\n    }\n  }\n}\nmethod_name: \""tensorflow/serving/classify\""\n"" }
```"
34966,Document about masking does not have previous masks information,"Greetings,

While the document about [masking](https://www.tensorflow.org/guide/keras/masking_and_padding) is super good, I found it misses an important point: how the mask associated with the previous mask in compute_mask(input, previous_mask)

Specifically, let us assume we have two inputs A and B. I wrote a custom Add layers:

```
class CustomAddingWithMasking(tf.keras.layers.Layer):
    def __init__(self, masking_boolean, **kwargs):
        super(CustomAddingWithMasking, self).__init__(**kwargs)

    def call(self, inputs):
        return inputs[0] + inputs[1]
    
    def compute_mask(self, inputs, mask=None):
        return mask
```

Here, I want to compute the sum of two tensors. Let us also assume that A and B have their own masks, which could be different to each other. Because we technically have two ""previous masks"" (from A and B separately), I don't know how the mask parameter in compute_mask was received. Is it the OR (or AND?) operation between mask of A and mask of B?

Those things are not clear as well as not documented well."
34965,Keras Callback log entry wrong documented,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback


> on_epoch_end: logs include `acc` and `loss`, and
>    optionally include `val_loss`
>    (if validation is enabled in `fit`), and `val_acc`
>    (if validation and accuracy monitoring are enabled).

and

> on_batch_end: logs include `loss`, and optionally `acc`

This is correct for the original Keras implementation. However, in tf2.keras callbacks get `accuracy` and `val_accuracy` instead of the short documented versions `acc`/`val_acc`. Either the implementation is wrong or the documentation.
"
34964,Add K Medoids Estimator to tf canned estimators,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
https://en.wikipedia.org/wiki/K-medoids
Basically same as https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/canned/kmeans.py except when updating centroid use median rather than mean.

**Will this change the current api? How?**
No, it should just add functionality

**Who will benefit with this feature?**
Anyone who wants to use K-medoids clustering

**Any Other info.**
"
34963,R2.1 pip install failed to find a correct tensorflow-estimator,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source code build and then pip install
- TensorFlow version: R2.1 (b2474d99ae962ce66d6db896606488797152dc4d)
- Python version: 2.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.29.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CPU(MKL)


**ERROR: No matching distribution found for tensorflow-estimator<2.2.0,>=2.1.0 (from tensorflow==2.1.0rc1)**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
```
Target //tensorflow/tools/pip_package:build_pip_package up-to-date:
  bazel-bin/tensorflow/tools/pip_package/build_pip_package
[0m[91mINFO: Elapsed time: 1088.439s, Critical Path: 237.61s
INFO: 19007 processes: 19007 local.
[0m[91mINFO: Build completed successfully, 19988 total actions
[0m[91mINFO: Build completed successfully, 19988 total actions
[0mMon Dec 9 09:18:14 UTC 2019 : === Preparing sources in dir: /tmp/tmp.fqqQwu5J91
/tensorflow /tensorflow
/tensorflow
/tmp/tmp.fqqQwu5J91/tensorflow/include /tensorflow
/tensorflow
Mon Dec 9 09:18:21 UTC 2019 : === Building wheel
[91mwarning: no files found matching 'README'
[0m[91mwarning: no files found matching '*.pyd' under directory '*'
[0m[91mwarning: no files found matching '*.pd' under directory '*'
[0m[91mwarning: no files found matching '*.dylib' under directory '*'
[0m[91mwarning: no files found matching '*.dll' under directory '*'
[0m[91mwarning: no files found matching '*.lib' under directory '*'
[0m[91mwarning: no files found matching '*.csv' under directory '*'
warning: no files found matching '*.h' under directory 'tensorflow_core/include/tensorflow'
[0m[91mwarning: no files found matching '*' under directory 'tensorflow_core/include/third_party'
[0mMon Dec 9 09:18:57 UTC 2019 : === Output wheel file is in: /tmp/pip
[91mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support
[0mProcessing /tmp/pip/tensorflow-2.1.0rc1-cp27-cp27mu-linux_x86_64.whl
Collecting gast==0.2.2
  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz
Collecting grpcio>=1.8.6
  Downloading https://files.pythonhosted.org/packages/0c/47/35cc9f6fd43f8e5ed74fcc6dd8a0cb2e89c118dd3ef7a8ff25e65bf0909f/grpcio-1.25.0-cp27-cp27mu-manylinux2010_x86_64.whl (2.4MB)
Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.1.0rc1) (1.16.5)
Collecting protobuf>=3.8.0
  Downloading https://files.pythonhosted.org/packages/a3/27/9a375cc85b90c93ab847370c31fe1302a763b6cd57d54daee20171d720ff/protobuf-3.11.1-cp27-cp27mu-manylinux1_x86_64.whl (1.3MB)
Requirement already satisfied, skipping upgrade: wheel; python_version < ""3"" in /usr/lib/python2.7/dist-packages (from tensorflow==2.1.0rc1) (0.30.0)
Collecting wrapt>=1.11.1
  Downloading https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz
Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.1.0rc1) (1.1.0)
Collecting backports.weakref>=1.0rc1; python_version < ""3.4""
  Downloading https://files.pythonhosted.org/packages/88/ec/f598b633c3d5ffe267aaada57d961c94fdfa183c5c3ebda2b6d151943db6/backports.weakref-1.0.post1-py2.py3-none-any.whl
Collecting six>=1.12.0
  Downloading https://files.pythonhosted.org/packages/65/26/32b8464df2a97e6dd1b656ed26b2c194606c16fe163c695a992b36c11cdf/six-1.13.0-py2.py3-none-any.whl
Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.1.0rc1) (1.0.8)
Collecting functools32>=3.2.3; python_version < ""3""
  Downloading https://files.pythonhosted.org/packages/c5/60/6ac26ad05857c601308d8fb9e87fa36d0ebf889423f47c3502ef034365db/functools32-3.2.3-2.tar.gz
Collecting termcolor>=1.1.0
  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz
Collecting absl-py>=0.7.0
  Downloading https://files.pythonhosted.org/packages/3b/72/e6e483e2db953c11efa44ee21c5fdb6505c4dffa447b4263ca8af6676b62/absl-py-0.8.1.tar.gz (103kB)
Collecting opt-einsum>=2.3.2
  Downloading https://files.pythonhosted.org/packages/f6/d6/44792ec668bcda7d91913c75237314e688f70415ab2acd7172c845f0b24f/opt_einsum-2.3.2.tar.gz (59kB)
Collecting tensorboard<2.2.0,>=2.1.0
  Downloading https://files.pythonhosted.org/packages/e4/c5/94a66686b86adfb3ef6f34837d0a7cc2efdc995bc39cad64a9b3e103f0d5/tensorboard-2.1.0-py2-none-any.whl (3.8MB)
[91mERROR: Could not find a version that satisfies the requirement tensorflow-estimator<2.2.0,>=2.1.0 (from tensorflow==2.1.0rc1) (from versions: 1.10.6, 1.10.7, 1.10.8, 1.10.9, 1.10.10, 1.10.11, 1.10.12, 1.13.0rc0, 1.13.0, 1.14.0rc0, 1.14.0rc1, 1.14.0, 1.15.0, 1.15.1, 2.0.0, 2.0.1, 2.1.0rc0)
[0m[91mERROR: No matching distribution found for tensorflow-estimator<2.2.0,>=2.1.0 (from tensorflow==2.1.0rc1)
```
"
34962,TypeError: unhashable type: 'ListWrapper' after adding losses to tf.keras model,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.14, 1.15, 2.0, 2.1rc0
- Keras version: 2.2.4-tf
- Python version: 3.6
- CUDA/cuDNN version: 10.0 and 10.1 / 7.6.5
- GPU model and memory: RTX 2060 6GB

**Describe the current behavior**
In project based on Mask_RCNN I got `TypeError: unhashable type: 'ListWrapper'` during training in add_loss(loss) method:
```

File ""C:\Project\project.py"", line 41, in build_Model
	maskRcnn = MaskRCNN(mode, config, model_dir)
File ""C:\Project\project.py"", line 58, in __init__
	maskRcnn.compile()
File ""C:\Project\MaskRCNN\model.py"", line 2175, in compile
	self.keras_model.add_loss(loss)
File ""C:\python36\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 1132, in add_loss
	self._graph_network_add_loss(symbolic_loss)
File ""C:\python36\lib\site-packages\tensorflow_core\python\keras\engine\network.py"", line 1433, in _graph_network_add_loss
	self._insert_layers(new_layers, new_nodes)
File ""C:\python36\lib\site-packages\tensorflow_core\python\keras\engine\network.py"", line 1392, in _insert_layers
	layer_set = set(self._layers)
File ""C:\python36\lib\site-packages\tensorflow_core\python\training\tracking\data_structures.py"", line 547, in __hash__
	raise TypeError(""unhashable type: 'ListWrapper'"")
TypeError: unhashable type: 'ListWrapper'
```

It's what happens when I use  tf.keras instead of keras.
"
34961,EfficientDet: Are you going to implement this in the object detection API?,"**System information**
- TensorFlow version (you are using): 2.0

**Describe the feature and the current behavior/state.**
I read about this new Network topology: [https://arxiv.org/pdf/1911.09070.pdf](https://arxiv.org/pdf/1911.09070.pdf ) 

It seems to work precisely and fast. Are you going to implement this new topology to your framework?

**Will this change the current api? How?**
Possible

**Who will benefit with this feature?**
Everyone who works with object detectors!

**Any Other info.**
If the answer is yes: will there be a quantized version for TPU inference?

Thanks in advance,
Timo"
34960,"the model of Tensorflow2.0 lite has low accuracy, is there anything wrong?","Hello, I create new model with the program as follows:
https://github.com/tensorflow/hub/blob/master/tensorflow_hub/tools/make_image_classifier/make_image_classifier.py
and after the whole train process, the accuracy is more than 93%
However, I  run the program as follows to test the model:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/python/label_image.py
with the same dataset as validacation datasets. But the accuracy is very low, less than 80%.
Is there anything wrong? Thanks very much for your help.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (or github SHA if from source):
2.0.0


**Command used to run the converter or code if you’re using the Python API**

make_image_classifier --image_dir /home/ioz/bird/images/ --tfhub_module /home/tensorflow-2.0/resnet101v4/ --image_size 224 --saved_model_dir /home/ioz/bird/models/ --labels_output_file /home/ioz/bird/m
odels/class_labels.txt --tflite_output_file /home/ioz/bird/models/resnet101_20_0.001True_model.tflite --learning_rate 0.001 --train_epochs 20 --do_fine_tuning True

"
34957,How to use SparseTensor in capi?,"HI :
    How to use SparseTensor in capi?"
34956,RunMetaData,"Hello!
I want to know how to use tf.RunMetaData() in tf.contrib.tpu.TPUEstimator(), because I want to analyze the time in ops.
Thank you!"
34955,DepthwiseConv in lite/kernels/internal/reference/depthwiseconv_float.h can be optimized better,"These two lines https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/reference/depthwiseconv_float.h#L62 can be moved up, because they only depend on out_x/out_y, there's no need to compute them per input depth/depth_multiplier.
"
34954,Keras MobilenetV3 weights ,"**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Tensorflow MobilenetV3 code and checkpoints(.ckpt, .pb, .tflite) have been provided. What changes need to be incorporated to use the model, in Keras. As of now, Keras supports applications till MobilenetV2, and not MobilenetV3. If provided, it will be easier for model loading and usage as it is. 

**Will this change the current api? How?**
No. It will be an update to the current scenario.

**Who will benefit with this feature?**
Users of Tensorflow and Keras

**Any Other info.**
NIL"
34953,A question about PSNR implementation in tensorflow 2.0,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
  
I am working on some image processes using PSNR ( [tf.image.psnr](https://www.tensorflow.org/api_docs/python/tf/image/psnr)), but always I
got results larger than what I expected. Then I check the source code and I
find


```
psnr_val = math_ops.subtract(

20 * math_ops.log(max_val) / math_ops.log(10.0),

np.float32(10 / np.log(10)) * math_ops.log(mse),

name='psnr')
```

Compare to the standard PSNR algorithm, its formula as follows:

![image](https://user-images.githubusercontent.com/14346086/70408961-03814f00-1a85-11ea-8df2-8c0f470e86f0.png)


I wonder why there is np.log(10) in the tensorflow implementation rather
than np.log10(10)? Or is there anything I get wrong?

Thanks for any help.
"
34952,tensorflow1.14.0 correspond protobuf version？,"ImportError: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringE

I think the problem is caused by incompatible versions？So i need protobuf version that conrrespond tensorflow1.14.0"
34950,load_weights h5file failed (cause by BN layer) in tensorflow2.0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-rc1-51-g2646d23074 2.0.0-rc2
- Python version: 3.6
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: NO
- GPU model and memory: NO


**Describe the current behavior**
I save my model using .h5 format(model.save_weights(""./save_weights/resNet.h5"")). And I load this weights(model.load_weights('./save_weights/resNet.h5', by_name=True)) then failed. this error is shape mismatch.
i did some tests:
**(1) when i save model using ckpt format, i can successfully load weights.
(2) when i save model that no use bn layer using h5 format, i can successfully load weights.**

**Describe the expected behavior**
i think i should load model weights(include bn layer) using .h5 format.

**Code to reproduce the issue**
(1) **i tracked the problem. the problem is in ""\tensorflow_core\python\keras\saving\hdf5_format.py"" file(754 line, ""if K.int_shape(symbolic_weights[i]) != weight_values[i].shape:"")**.
(2) when i load model weights, i find the **variables order(layers order) is different between ""symbolic_weights"" and ""weight_values""** . so you can't simply use list index to load weights. 
(3) the main reason is caused by bn layer. i check the varables ""symbolic_weights"" and ""weight_values"",  the bn parameters order in ""weight_values"" is like [gamma, bata, mean, variance, gamma, bata, mean, variance] but in ""symbolic_weights"" is like [gamma, bata, gamma, bata, mean, variance, mean, variance].

**Other info / logs**
Traceback (most recent call last):
  File ""E:\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\network.py"", line 1175, in load_weights
    saving.load_weights_from_hdf5_group_by_name(f, self.layers)
  File ""E:\Anaconda3\lib\site-packages\tensorflow_core\python\keras\saving\hdf5_format.py"", line 760, in load_weights_from_hdf5_group_by_name
    str(weight_values[i].shape) + '.')
ValueError: Layer #4 (named ""block1""), weight <tf.Variable 'block1/unit_1/conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32, numpy=
array([[[[ 0.00050041, -0.06431282, -0.00577746, ..., -0.05891485,
          -0.02970275, -0.06618612],
.......
-0.04930668, -0.02927044]]]], dtype=float32)> has shape (3, 3, 64, 64), but the saved weight has shape (64,).
"
34949,How to use TF_VARIANT in c_api?,"Hi :
      How to use TF_VARIANT in c_api?"
34947,"polyval gives TypeError when run inside tf.function with Tensor coeffs, but not when run eagerly","This is my first issue, so let me know if it is reported in the wrong place. Thanks! 

**System information**
- Have I written custom code: Example Python code to reproduce provided below
- OS Platform and Distribution: Windows 10 Pro 1903
- TensorFlow installed from binary with pip
- TensorFlow version: 'v2.0.0-rc2-26-g64c3d382ca'; '2.0.0'
- Python version: 3.7.4

**Describe the current behavior**
tf.math.polyval works correctly when executing eagerly, but when it is called from within a function that has the @tf.function decorator then a TypeError is raised:

> TypeError: len is not well defined for symbolic Tensors. (eye/diag:0) Please call `x.shape` rather than `len(x)` for shape information.

**Describe the expected behavior**
The output of this example should be the same regardless of eager execution or tf.function decoration:
```
[1 1 5]
[[-2 1 10]]
[[8 2 20]]
[[-18 6 30]]
[[24 24 24]]
```

**Code to reproduce the issue**
```
import tensorflow as tf

x = tf.Variable([[-1.0], [0.0], [1.0]])

@tf.function
def func():
    with tf.GradientTape(persistent=True) as t:
        t.watch(x)
        coeffs = tf.eye(5)
        pv = tf.math.polyval(coeffs, x)
        y = tf.reduce_sum(pv, axis=1)
        dy_dx = t.gradient(y, x)
        d2y_dx2 = t.gradient(dy_dx, x)
        d3y_dx3 = t.gradient(d2y_dx2, x)
        d4y_dx4 = t.gradient(d3y_dx3, x)
    del t

    tf.print(y)
    tf.print(tf.transpose(dy_dx))
    tf.print(tf.transpose(d2y_dx2))
    tf.print(tf.transpose(d3y_dx3))
    tf.print(tf.transpose(d4y_dx4))

func()
```

"
34946,"Does TensorFlow 2.x still need the bazel build option of --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" ?","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No.
- TensorFlow installed from (source or binary): source
- TensorFlow version:  v2.1.0-rc0 
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): bazel 1.2.1
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
- CUDA/cuDNN version: CUDA 10.0 / cuDNN 7
- GPU model and memory: 1080Ti, ~11GB

**Describe the problem**

According to the official doc in section **TensorFlow 2.x** from https://www.tensorflow.org/install/source#tensorflow_2x, to build the TF 2.X via bazel 

```
bazel build //tensorflow/tools/pip_package:build_pip_package
```
Do we need the bazel option of --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" ?

The [subsection: Bazel build options](https://www.tensorflow.org/install/source#bazel_build_options) under section **TensorFlow 1.x** states that 

> The official TensorFlow packages are built with GCC 4 and use the older ABI. For GCC 5 and later, make your build compatible with the older ABI using: --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"". ABI compatibility ensures that custom ops built against the official TensorFlow package continue to work with the GCC 5 built package.

However,  `The official TensorFlow packages ` link includes TF 2.X. 
So, does TensorFlow 2.x still need the bazel option of --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" ?


**Provide the exact sequence of commands / steps that you executed before running into the problem**

> The official TensorFlow packages are built with GCC 4 and use the older ABI. For GCC 5 and later, make your build compatible with the older ABI using: --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"". ABI compatibility ensures that custom ops built against the official TensorFlow package continue to work with the GCC 5 built package.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
34944,"SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'keras_learning_phase:0' shape=() dtype=bool>]","Hi,
 I am writing Encoder-Decoder architecture with Bahdanau Attention using tf.keras with TensorFlow 2.0. Below is my code This is working with TensorFlow 1.15 but getting the error in 2.0. you can check the code in colab notebook [here](https://colab.research.google.com/drive/12Vq1t9xOtrPmXV2Sj_GSoE9bxH7FgaKs). 
```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Input, Dense, Conv2D, BatchNormalization, Activation, Dropout, GRU, Embedding
from tensorflow.keras.models import Model
from tensorflow.keras import activations
from tensorflow.keras.layers import Layer
from tensorflow.keras import layers
import tensorflow as tf
from tensorflow.keras.layers import GRU, concatenate, Lambda

ENCODER_SEQ_LEN = 30
DECODER_SEQ_LEN = 20
VOCAB_SIZE = 500
units = 16

tf.keras.backend.clear_session()
class Encoder(Model):
    def __init__(self, vocab_size, embedding_dim, input_length, units):
        super(Encoder, self).__init__()
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.input_length = input_length
        self.units = units
        self.embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=50, input_length=self.input_length,
                           mask_zero=False, name=""embedding_layer_encoder"")
        self.gru = GRU(self.units, return_state=True, return_sequences=True, name=""Encoder_GRU"")
    @tf.function
    def call(self, inputs, training=True):
        x_embedd = self.embedding(inputs)
        gru_output, gru_state = self.gru(x_embedd)
        return gru_output, gru_state
    
class BahdanauAttention(tf.keras.layers.Layer):
    def __init__(self, units):
        super(BahdanauAttention, self).__init__()
        self.W1 = tf.keras.layers.Dense(units)
        self.W2 = tf.keras.layers.Dense(units)
        self.V = tf.keras.layers.Dense(1)

    def call(self, query, values):
        # hidden shape == (batch_size, hidden size)
        # # hidden_with_time_axis shape == (batch_size, 1, hidden size)
        # # we are doing this to perform addition to calculate the score
        hidden_with_time_axis = tf.expand_dims(query, 1)
        # score shape == (batch_size, max_length, 1)
        # # we get 1 at the last axis because we are applying score to self.V
        # # the shape of the tensor before applying self.V is (batch_size, max_length, units)
        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))
        # attention_weights shape == (batch_size, max_length, 1)
        attention_weights = tf.nn.softmax(score, axis=1)
        # context_vector shape after sum == (batch_size, hidden_size)
        context_vector = attention_weights * values
        context_vector = tf.reduce_sum(context_vector, axis=1)
        return context_vector

class onestepDecoder(Model):
    def __init__(self, vocab_size, embedding_dim, dec_units, att_units):
        super(onestepDecoder, self).__init__()
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.dec_units = dec_units
        self.att_units = att_units
        self.embedd = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim,
                      input_length=1, mask_zero=False, name=""Decoder_Embedding_layer"")
        self.att_layer = BahdanauAttention(units=self.att_units) #name='Attention')
        self.dense = Dense(self.vocab_size, activation=""softmax"", name=""DenseOut"")
        self.gru = GRU(units=self.dec_units, return_state=True, name=""DecGRU"")
    @tf.function
    def call(self, input_decoder, input_state, encoder_outputs, training=True):
        x_embedd = self.embedd(input_decoder)
        context_vector = self.att_layer(input_state, encoder_outputs )
        concat = tf.concat([tf.expand_dims(context_vector, 1), x_embedd], axis=-1)
        decoder_output, Decoder_state = self.gru(concat, initial_state=input_state)
        output = self.dense(decoder_output)
        return (output, Decoder_state)
class Decoder(Model):
    def __init__(self, vocab_size, embedding_dim, dec_units, att_units):
        super(Decoder, self).__init__()
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.dec_units = dec_units
        self.att_units = att_units
        self.stepdec = onestepDecoder(self.vocab_size, self.embedding_dim, self.dec_units, self.att_units)
    @tf.function
    def call(self, input_decoder, input_state, encoder_outputs):
        all_outputs= tf.TensorArray(tf.float32, size=input_decoder.shape[1], name=""output_arrays"")
        for timestep in range(input_decoder.shape[1]):
            output, input_state = self.stepdec(input_decoder[:,timestep:timestep+1], input_state, encoder_outputs)
            all_outputs = all_outputs.write(timestep, output)
        all_outputs = tf.transpose(all_outputs.stack(), [1, 0, 2])
        return all_outputs

encoder_input = Input(shape=(ENCODER_SEQ_LEN,), name='encoder_input_final')
decoder_input = Input(shape=(DECODER_SEQ_LEN,), name=""Decoder_inout_final"")
encoder = Encoder(vocab_size=VOCAB_SIZE, embedding_dim=50, input_length=ENCODER_SEQ_LEN, units=16)
x_gru_out, x_gru_state = encoder(encoder_input)
decoder = Decoder(vocab_size=VOCAB_SIZE, embedding_dim=50, dec_units=16, att_units=20)
all_outputs = decoder(decoder_input, x_gru_state, x_gru_out)
encoder_decoder = Model([encoder_input, decoder_input], outputs=all_outputs)
encoder_decoder.compile(optimizer='adam',loss='sparse_categorical_crossentropy')

x = np.random.randint(0, 499, size=(2000, ENCODER_SEQ_LEN))
y = np.random.randint(0, 499, size=(2000, DECODER_SEQ_LEN))

encoder_decoder.fit(x=[x,y], y=y, epochs=1,verbose=1,batch_size=32)
```
**Error**:
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     60                                                op_name, inputs, attrs,
---> 61                                                num_outputs)
     62   except core._NotOkStatusException as e:

TypeError: An op outside of the function building code is being passed
a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: keras_learning_phase:0

During handling of the above exception, another exception occurred:

_SymbolicException                        Traceback (most recent call last)
11 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     73       raise core._SymbolicException(
     74           ""Inputs to eager execution function cannot be Keras symbolic ""
---> 75           ""tensors, but found {}"".format(keras_symbolic_tensors))
     76     raise e
     77   # pylint: enable=protected-access

_SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'keras_learning_phase:0' shape=() dtype=bool>]
"
34942,"Memory leak with tf.shuffle, doesn't release buffer memory","**System information**
- OS Platform and Distribution Linux Ubuntu 16.04
- Python: 2.7.17 |Anaconda, Inc.| (default, Oct 21 2019, 19:04:46) [GCC 7.3.0]
- Tensorflow: 1.12.0
- Numpy: 1.16.5
- GPU: GeForce RTX 2080 Ti
- CUDA: 9.2

**Describe the current behavior**
CPU memory gradually increase after each epoch until the program restarts, i suspect that dataset.shuffle doesn't release the buffer memory. Tested with tf 1.15, same situation.

**Code to reproduce the issue**
```
import numpy as np
import tensorflow as tf
class ASRDataGenerator(object):
    def __init__(self,num):
        self.num = num
    def __call__(self):
        for i in range(self.num):
            for j in range(106):
                yield 'a','b',np.random.randn(100,120)

class TFASRDataSet(object):
    def __init__(self,num,batch_size):

        self.num = num
        self.batch_size = batch_size
        self.asrDataGenerator = ASRDataGenerator(num)
        
    def setDataSetIterator(self):
        
        dataset = tf.data.Dataset.from_generator(self.asrDataGenerator, (tf.string,tf.string,tf.float32))
        dataset = dataset.shuffle(30000)
        dataset = dataset.map(lambda s1,s2,feat: [s1,s2,feat])
        dataset = dataset.batch(self.batch_size, drop_remainder=True)
        self.iterator = dataset.make_initializable_iterator()
        
test_tfASRDataSet = TFASRDataSet(248,192)
test_tfASRDataSet.setDataSetIterator()
test_iter = test_tfASRDataSet.iterator
test_next = test_iter.get_next()   

run_config = tf.ConfigProto()
run_config.gpu_options.allow_growth = True
run_config.allow_soft_placement = True

with tf.Session(config=run_config) as sess:

    for i in range(100):

        sess.run(test_iter.initializer)
        
        while True:
            try:
                loss_list = sess.run([test_next])
                print(len(loss_list[0]))
            except tf.errors.OutOfRangeError:
                print(""train epoch %d finish"" % (i+1))
                break
```"
34940,Autograph issues with TF documented examples,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)** No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux 5.4.2-arch1-1 x86_64
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 2.0.0
- **Python version**: 3.8.0
- **Bazel version (if compiling from source)**: 1.1.0
- **GCC/Compiler version (if compiling from source)**: 9.2.0
- **CUDA/cuDNN version**: 10.1.168-4 / 7.6.5.32-1
- **GPU model and memory**: Nvidia GTX 1050 Ti, 4096 Mb
- **Exact command to reproduce**: 
import tensorflow as tf

tf.autograph.set_verbosity(10, alsologtostdout=True)


@tf.function
def square_if_positive(x):
    if x > 0:
        x = x * x
    else:
        x = 0
    return x


print('square_if_positive(2) = {}'.format(square_if_positive(tf.constant(2))))
print('square_if_positive(-2) = {}'.format(square_if_positive(tf.constant(-2))))

### Describe the problem
""OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function."" pops up anywhere I try to use a conditional, not just this example.

### Source code / logs
2019-12-08 10:56:40.479540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2019-12-08 10:56:41.527181: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-12-08 10:56:41.543972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-08 10:56:41.544502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1
coreClock: 1.455GHz coreCount: 6 deviceMemorySize: 3.94GiB deviceMemoryBandwidth: 104.43GiB/s
2019-12-08 10:56:41.544518: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2019-12-08 10:56:41.545902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2019-12-08 10:56:41.547153: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2019-12-08 10:56:41.547367: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2019-12-08 10:56:41.548737: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2019-12-08 10:56:41.549551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2019-12-08 10:56:41.552590: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-12-08 10:56:41.552690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-08 10:56:41.553243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-08 10:56:41.553652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2019-12-08 10:56:41.554122: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
2019-12-08 10:56:41.573643: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 3101990000 Hz
2019-12-08 10:56:41.573908: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561688dfa0c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-12-08 10:56:41.573927: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2019-12-08 10:56:41.574088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-08 10:56:41.574746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1
coreClock: 1.455GHz coreCount: 6 deviceMemorySize: 3.94GiB deviceMemoryBandwidth: 104.43GiB/s
2019-12-08 10:56:41.574774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2019-12-08 10:56:41.574795: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2019-12-08 10:56:41.574808: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2019-12-08 10:56:41.574820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2019-12-08 10:56:41.574832: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2019-12-08 10:56:41.574844: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2019-12-08 10:56:41.574857: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-12-08 10:56:41.574913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-08 10:56:41.575579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-08 10:56:41.576106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2019-12-08 10:56:41.576137: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2019-12-08 10:56:42.040932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-08 10:56:42.040956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2019-12-08 10:56:42.040967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2019-12-08 10:56:42.041100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-08 10:56:42.041410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-08 10:56:42.041696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-08 10:56:42.041971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3386 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-12-08 10:56:42.043375: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56169a227030 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2019-12-08 10:56:42.043385: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1
Converted call: <function square_if_positive at 0x7f4849b981f0>
    args: (<tf.Tensor 'x:0' shape=() dtype=int32>,)
    kwargs: {}

Entity <function square_if_positive at 0x7f4849b981f0> is not cached for subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7f4810de6d00>, frozenset())
Converting <function square_if_positive at 0x7f4849b981f0>
Source code of <function square_if_positive at 0x7f4849b981f0>:

@tf.function
def square_if_positive(x):
    if x > 0:
        x = x * x
    else:
        x = 0
    return x


Error transforming entity <function square_if_positive at 0x7f4849b981f0>
WARNING: AutoGraph could not transform <function square_if_positive at 0x7f4849b981f0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Bad argument number for keyword: 1, expecting 2
Caught error in <function square_if_positive at 0x7f4849b981f0> (converted=False)
Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 539, in converted_call
    converted_f = conversion.convert(target_entity, program_ctx)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 358, in convert
    converted_entity_info = _convert_with_cache(entity, program_ctx,
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 273, in _convert_with_cache
    nodes, converted_name, entity_info = convert_entity_to_ast(
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 506, in convert_entity_to_ast
    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 705, in convert_func_to_ast
    node = node_to_graph(node, context)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 735, in node_to_graph
    node = converter.apply_(node, context, function_scopes)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/core/converter.py"", line 399, in apply_
    node = converter_module.transform(node, context)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py"", line 132, in transform
    return FunctionBodyTransformer(ctx).visit(node)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/core/converter.py"", line 345, in visit
    return super(Base, self).visit(node)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/transformer.py"", line 480, in visit
    result = super(Base, self).visit(node)
  File ""/usr/lib/python3.8/ast.py"", line 360, in visit
    return visitor(node)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py"", line 114, in visit_FunctionDef
    wrapped_body = templates.replace(
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/templates.py"", line 270, in replace
    node = ReplaceTransformer(replacements).visit(node)
  File ""/usr/lib/python3.8/ast.py"", line 360, in visit
    return visitor(node)
  File ""/usr/lib/python3.8/ast.py"", line 436, in generic_visit
    value = self.visit(value)
  File ""/usr/lib/python3.8/ast.py"", line 360, in visit
    return visitor(node)
  File ""/usr/lib/python3.8/ast.py"", line 445, in generic_visit
    new_node = self.visit(old_value)
  File ""/usr/lib/python3.8/ast.py"", line 360, in visit
    return visitor(node)
  File ""/usr/lib/python3.8/ast.py"", line 436, in generic_visit
    value = self.visit(value)
  File ""/usr/lib/python3.8/ast.py"", line 360, in visit
    return visitor(node)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/templates.py"", line 201, in visit_Name
    new_nodes = self._prepare_replacement(node, node.id)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/templates.py"", line 140, in _prepare_replacement
    new_nodes = ast_util.copy_clean(repl, preserve_annos=self.preserved_annos)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py"", line 76, in copy_clean
    return CleanCopier(preserve_annos).copy(node)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py"", line 54, in copy
    new_fields[f] = self.copy(getattr(node, f))
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py"", line 41, in copy
    return [self.copy(n) for n in node]
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py"", line 41, in <listcomp>
    return [self.copy(n) for n in node]
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py"", line 55, in copy
    new_node = type(node)(**new_fields)
  File ""/usr/lib/python3.8/site-packages/gast/gast.py"", line 10, in create_node
    assert nbparam in (0, len(Fields)), \
AssertionError: Bad argument number for keyword: 1, expecting 2
WARNING:tensorflow:AutoGraph could not transform <function square_if_positive at 0x7f4849b981f0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Bad argument number for keyword: 1, expecting 2
Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 539, in converted_call
    converted_f = conversion.convert(target_entity, program_ctx)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 358, in convert
    converted_entity_info = _convert_with_cache(entity, program_ctx,
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 273, in _convert_with_cache
    nodes, converted_name, entity_info = convert_entity_to_ast(
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 506, in convert_entity_to_ast
    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 705, in convert_func_to_ast
    node = node_to_graph(node, context)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 735, in node_to_graph
    node = converter.apply_(node, context, function_scopes)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/core/converter.py"", line 399, in apply_
    node = converter_module.transform(node, context)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py"", line 132, in transform
    return FunctionBodyTransformer(ctx).visit(node)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/core/converter.py"", line 345, in visit
    return super(Base, self).visit(node)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/transformer.py"", line 480, in visit
    result = super(Base, self).visit(node)
  File ""/usr/lib/python3.8/ast.py"", line 360, in visit
    return visitor(node)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py"", line 114, in visit_FunctionDef
    wrapped_body = templates.replace(
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/templates.py"", line 270, in replace
    node = ReplaceTransformer(replacements).visit(node)
  File ""/usr/lib/python3.8/ast.py"", line 360, in visit
    return visitor(node)
  File ""/usr/lib/python3.8/ast.py"", line 436, in generic_visit
    value = self.visit(value)
  File ""/usr/lib/python3.8/ast.py"", line 360, in visit
    return visitor(node)
  File ""/usr/lib/python3.8/ast.py"", line 445, in generic_visit
    new_node = self.visit(old_value)
  File ""/usr/lib/python3.8/ast.py"", line 360, in visit
    return visitor(node)
  File ""/usr/lib/python3.8/ast.py"", line 436, in generic_visit
    value = self.visit(value)
  File ""/usr/lib/python3.8/ast.py"", line 360, in visit
    return visitor(node)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/templates.py"", line 201, in visit_Name
    new_nodes = self._prepare_replacement(node, node.id)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/templates.py"", line 140, in _prepare_replacement
    new_nodes = ast_util.copy_clean(repl, preserve_annos=self.preserved_annos)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py"", line 76, in copy_clean
    return CleanCopier(preserve_annos).copy(node)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py"", line 54, in copy
    new_fields[f] = self.copy(getattr(node, f))
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py"", line 41, in copy
    return [self.copy(n) for n in node]
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py"", line 41, in <listcomp>
    return [self.copy(n) for n in node]
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py"", line 55, in copy
    new_node = type(node)(**new_fields)
  File ""/usr/lib/python3.8/site-packages/gast/gast.py"", line 10, in create_node
    assert nbparam in (0, len(Fields)), \
AssertionError: Bad argument number for keyword: 1, expecting 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 344, in _call_unconverted
    return f(*args, **kwargs)
  File ""/home/michal/Projects/RLtat/the-mayor/l2tf.py"", line 8, in square_if_positive
    if x > 0:
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/ops.py"", line 760, in __bool__
    self._disallow_bool_casting()
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/ops.py"", line 525, in _disallow_bool_casting
    self._disallow_when_autograph_enabled(
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/ops.py"", line 511, in _disallow_when_autograph_enabled
    raise errors.OperatorNotAllowedInGraphError(
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.
Traceback (most recent call last):
  File ""/home/michal/Projects/RLtat/the-mayor/l2tf.py"", line 15, in <module>
    print('square_if_positive(2) = {}'.format(square_if_positive(tf.constant(2))))
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/def_function.py"", line 568, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/def_function.py"", line 615, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/def_function.py"", line 496, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/function.py"", line 2375, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/function.py"", line 2706, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/function.py"", line 2586, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/func_graph.py"", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/def_function.py"", line 439, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/func_graph.py"", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in converted code:

    /home/michal/Projects/RLtat/the-mayor/l2tf.py:8 square_if_positive
        if x > 0:
    /usr/lib/python3.8/site-packages/tensorflow_core/python/framework/ops.py:760 __bool__
        self._disallow_bool_casting()
    /usr/lib/python3.8/site-packages/tensorflow_core/python/framework/ops.py:525 _disallow_bool_casting
        self._disallow_when_autograph_enabled(
    /usr/lib/python3.8/site-packages/tensorflow_core/python/framework/ops.py:511 _disallow_when_autograph_enabled
        raise errors.OperatorNotAllowedInGraphError(

    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.
"
34939,Title,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
34938,Error when adding tensorflow op with cmake using c ++ 17,"Error when adding tensorflow op with cmake using c ++ 17，how do I set tensorflow to support c ++ 17?

tensorflow.python.framework.errors_impl.NotFoundError: /home/user/Desktop/sum_each_op/lib/libsum_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefESt17basic_string_viewIcSt11char_traitsIcEESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteISA_EE
"
34937,ZeroDivisionError: integer division or modulo by zero,"I am using Ubuntu 17.10 and python 2.7 
Trained the neural network and now trying to predict the unknown data using predict.py but getting an error as stated below: 
Traceback (most recent call last):
File ""predict.py"", line 68, in <module>
    main(dataset_npz_filepath, result_csv_filepath)
  File ""predict.py"", line 36, in main
    for output, pfd_filepath in zip(output_generator, pfd_filepaths):
  File ""/home/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 617, in predict
    input_fn, ModeKeys.PREDICT)
  File ""/home/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 993, in _get_features_from_input_fn
    result = self._call_input_fn(input_fn, mode)
  File ""/home/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1113, in _call_input_fn
    return input_fn(**kwargs)
  File ""/home/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/inputs/numpy_io.py"", line 198, in input_fn
    num_epochs=num_epochs)
  File ""/home/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py"", line 484, in _enqueue_data
    num_epochs=num_epochs))
  File ""/home/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py"", line 220, in __init__
    self._epoch_end = (self._trav - 1) % self._max
ZeroDivisionError: integer division or modulo by zero
"
34936,"Train time per sample grows with dataset tensor size, even though it is always cropped to the same size","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0
- Python version: 3.7.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 
- GPU model and memory:

**Describe the current behavior**
When using `dataset.from_tensor_slices` on a tensor with shape (1, 10000, 1000) then `.map(tf.image.random_crop((20, 20)))`, training is 100 times faster par one iteration than when using  `dataset.from_tensor_slices` on a tensor with shape (1, 100000, 10000)

**Describe the expected behavior**
I expect that as long as the tensor can be kept in RAM, its size would not affect performance (putting aside tensors small enough to be kept in CPU cache)

**Code to reproduce the issue**

This is the simplest form of data and model that reproduces this.. not very interesting, but I really can't see any reason why this would be so much slower when the ones and zeros are bigger.

```python
ones = tf.ones((1, 10000, 1000))
zeros = tf.zeros((1, 10000, 1000))
data = tf.data.Dataset.from_tensor_slices(
    (
        ones,
        tf.ones((1, 1))
    )
).concatenate(
    tf.data.Dataset.from_tensor_slices(
        (
            zeros,
            tf.zeros((1, 1))
        )
    )
).map(lambda x, y: (tf.image.random_crop(x, (20, 20)), y)).repeat().batch(10)

x = tf.keras.layers.Input((20, 20))
y = tf.keras.layers.Dense(1, tf.keras.activations.sigmoid)(x)
model = tf.keras.models.Model(inputs=[x], outputs=[y])
model.compile(loss='mean_squared_error', optimizer='ADAM')
model.fit(x=data, epochs=100, steps_per_epoch=100, validation_data=validation, validation_steps=3)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
34935,Tutorial for Data Augmentation using tf.image,"Hi, my name is Rachin Kalakheti and i am a participant of Google Code-in 2019. I felt overwhelmed to know Tensorflow is also one of the organization for this year. So, there was a task to create a notebook tutorial on Data Augmentation using tf.image. I see that currently there is no tutorial regarding the same topic. So, I would like to contribute to the community by adding my tutorial to the  Tensorflow repo. Therefore I am seeking guidance as to discuss this further.
Link to my notebook tutorial: https://colab.research.google.com/drive/1skGIQhwifJY6HWO6ZnbFe4un-VuJ3VW5

Thank you!"
34933,"Gast 0.2.2 working, 0.3.3 not working","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, but tested for both
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Conda
- TensorFlow version (use command below): 1.14, 1.15
- Python version: 3.6,3.7,3.8
- CUDA/cuDNN version: Conda
- GPU model and memory: Geforce GTX 1070 & 1080ti, 8GB/11GB respectivelyl


**Describe the current behavior**
Using Gast 0.3.3, what happens is that the training will hang randomly when using tensorflow-gpu. This can be fixed by pip install gast==0.2.2
**Describe the expected behavior**
Training and not hanging
**Code to reproduce the issue**
All ML training that utilizes gpu

This issue still hasn't been dealt with.
"
34932,Severe TPU/CPU behaviour discrepency,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):No
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.1.0-dev20191203
- Python version: 3.5
- GPU model and memory: TPU (nightly-2.x)

**Describe the current behavior**
When training using a TPU backend, if a `@tf.function` `function` code is defined before connecting to a TPU cluster. Calling the function (as `Dataset.map(function)`) results in a python kernel crash, without any errors or other info. 

This is especially severe (IMHO), since, as the training code base grows, more autograph functions are defined in modules instead of on the main program. As it's natural to import modules at the start of the program, if the TPU connection is initiated after the imports, using the @tf.fucntion code defined in the modules results in a kernel crash.
If the same code is, however, being run just on the CPU, it works as expected.
This leads to a somewhat frustrating experience of everything working on a CPU dev env, and then crashing inexplicably when connecting to a TPU.
The unintuitive solution is to run the TPU connection boilerplate before any imports.

**Describe the expected behavior**
1. connecting to a TPU shouldn't create an implicit scope, if a scope is required it should be with a `with` idiom
2. It should be better documented if all autograph function definitions should be defined after connecting to a TPU
3. If a code executed within a TPU scope depends on a code defined outside, it should fail gracefully and informatively (not crash the kernel)

**Code to reproduce the issue**

---
Fail case:
```
import tensorflow as tf

@tf.function
def test_func(a):
    return a**3

train, test = tf.keras.datasets.fashion_mnist.load_data()
images, labels = train
images = images/255
ds = tf.data.Dataset.from_tensor_slices((images))

TPU_IP = '10.0.3.2' #this requires a working nightly-2.x TPU cluster (2v-8)
tpu_address = 'grpc://' + TPU_IP + ':8470'
resolver=tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_address)
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)

dsf=ds.map(test_func)
```
---
Working case:
```
import tensorflow as tf

TPU_IP = '10.0.3.2' #this requires a working nightly-2.x TPU cluster (2v-8)
tpu_address = 'grpc://' + TPU_IP + ':8470'
resolver=tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_address)
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)

train, test = tf.keras.datasets.fashion_mnist.load_data()
images, labels = train
images = images/255
ds = tf.data.Dataset.from_tensor_slices((images))

@tf.function
def test_func(a):
    return a**3

dsf=ds.map(test_func)
```
---
The reproduction code doesn't use imports, but 
```
@tf.function
def test_func(a):
    return a**3
```
Would typically run as part of the import code, and not the main program.

**Other info / logs**
No error logs produced"
34931,TensorFlow broadcasting results in error of Incompatible shapes,"Greetings,

Another day, another potentially ""deep"" bug with TF I found! :-) 

I have three tensors: A, B, C. Here A is of shape (None, 64, 64), B is of shape: (None, 64) and C is of shape (None, 64). What I wanted to do is to have an element-wise multiplication followed by an element-wise addition.

Thanks to TF broadcasting, the code I needed to write is simply:

A * B + C

With this, the output should have shape  (None, 64, 64). All these things are a part of my model, and everything works well. Specifically, by working well I mean I can compile the model and print model summary() successfully.

Unfortunately, when I fit the the model to my training data, I got a problem of Incompatible shapes as bellows.

```

  File ""/home/cuong.hoang/anaconda2/envs/py36_env_tensor_gpu_pip_local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 780, in fit
    steps_name='steps_per_epoch')
  File ""/home/cuong.hoang/anaconda2/envs/py36_env_tensor_gpu_pip_local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 363, in model_iteration
    batch_outs = f(ins_batch)
  File ""/home/cuong.hoang/anaconda2/envs/py36_env_tensor_gpu_pip_local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 3292, in __call__
    run_metadata=self.run_metadata)
  File ""/home/cuong.hoang/anaconda2/envs/py36_env_tensor_gpu_pip_local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1458, in __call__
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: Incompatible shapes: [32,64] vs. [32,64,64]
	 [[{{node encoder_layer/mul_5}}]]
	 [[loss/mul/_5175]]
  (1) Invalid argument: Incompatible shapes: [32,64] vs. [32,64,64]
	 [[{{node encoder_layer/mul_5}}]]
0 successful operations.
0 derived errors ignored.
```

I am pretty sure my code correct, and the problem, I think is because of broadcasting. Moving forward, I decided to write by myself the code to make B and C has size (None, 64, 64) by repeating. The code is pretty simple, for B, as example:
```
B = K.expand_dims(B, axis=-1)
B = K.repeat_elements(B, 64, axis=2)
```

With this, the model compiles OK, and more importantly, when I fit the model to data, it runs OK as well.

So, I believe this is a bug of TensorFlow broadcasting. I am curious why it happens and did anyone get any similar error before?

This is with Tensorflow 1.14. If you want the code to reproduce the error just let me know."
34929,Distributed training not working properly when i got 1 ps 2 ro more workers,"tf 1.15  python 3.7.2
Hello,
When I am running simple distributed demo with 1 ps, 2 workers.
the worker 2 can stop properly, but the chef worker can't stop properly, here is error from chef worker 'RuntimeError: Run called even after should_stop requested.'

here is my code, 

import tensorflow as tf
from absl import app as absl_app
from absl import flags
import numpy as np

tf.logging.set_verbosity(tf.logging.INFO)

flags.DEFINE_string(""ps_hosts"", ""localhost:2222"", ""ps hosts"")
flags.DEFINE_string(""worker_hosts"", ""localhost:2223,localhost:2224"", ""worker hosts"")
flags.DEFINE_string(""job_name"", ""worker"", ""'ps' or'worker'"")
flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")
FLAGS = flags.FLAGS


def main(argv):
    ps_hosts = FLAGS.ps_hosts.split("","")
    worker_hosts = FLAGS.worker_hosts.split("","")
    global_step = tf.train.get_or_create_global_step()
    cluster = tf.train.ClusterSpec({""ps"": ps_hosts, ""worker"": worker_hosts})
    server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)
    if FLAGS.job_name == ""ps"":
        server.join()
    is_chief = (FLAGS.task_index == 0)
    with tf.device(
            tf.train.replica_device_setter(
                worker_device=""/job:worker/task:{0}"".format(FLAGS.task_index),
                ps_device=""/job:ps/cpu:0"",
                cluster=cluster)):
        x = tf.placeholder(tf.float32, shape=[None, 1], name='x')
        y = tf.placeholder(tf.float32, shape=[None, 1], name='y')
        w = tf.Variable(tf.constant(0.0))
        learning_rate = tf.train.exponential_decay(0.1, global_step, 10, 2, staircase=False)
        loss = tf.pow(w * x - y, 2)
        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)
        with tf.train.MonitoredTrainingSession(
                master=server.target,
                is_chief=is_chief,
                hooks=[tf.train.StopAtStepHook(last_step=2000)]
        ) as mon_sess:
            while not mon_sess.should_stop():
                mon_sess.run(train_op, feed_dict={x: np.linspace(1, 2, 10).reshape([10, 1]),
                                                  y: np.linspace(1, 2, 10).reshape([10, 1])})
                print(mon_sess.run(learning_rate))
                print(mon_sess.run(global_step))


if __name__ == ""__main__"":
    absl_app.run(main)




'''python demo.py --job_name=ps task_index=0
python demo.py --job_name=worker task_index=0
python demo.py --job_name=worker task_index=1'''

Is there anyone can offer me a one true simple with no error tf distributed demo?
thanks~!

"
34928,Tensorflow 1.14.0 and python3.6.9 compatibility issues？,"when I write `import tensorflow as tf`,the tensorflow version is 1.14.0,and python version is 3.6.9,the result show exception:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringE

But use tensorflow 1.14.0 and python 3.6.8,the result is normal.

"
34927,Bad example in Sparse_Categorical_CrossEntropy,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/losses.py#L493

## Description of issue (what needs changing):

The example will cause errors:

cce = tf.keras.losses.SparseCategoricalCrossentropy()
loss = cce(
  [0, 1, 2],
  [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])
print('Loss: ', loss.numpy())  # Loss: 0.3239

Need to change to

cce = tf.keras.losses.SparseCategoricalCrossentropy()
loss = cce(
  [0, 1, 2],
  tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]]))
print('Loss: ', loss.numpy())  # Loss: 0.3239

In addition [.5, .89, .6] should be [0.05, 0.89, 0.06] to be consistent with similar example (
https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy?hl=en). Thus Loss should be updated to 0.0945

"
34926,tensorflow lite build issue on Windows,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows-10-10.0.18362-SP0
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.0.0
- Python version: 3.6.8
- Bazel version (if compiling from source): 1.1.0
- GCC/Compiler version (if compiling from source): 8.1.0



**Describe the problem**

I was trying to build tensorflow lite from source with `bazel build //tensorflow/lite:libtensorflowlite.so`
After build error I've tried to add `features = [""windows_export_all_symbols""]` to build_def.bzl, but nothing changes

**Any other info / logs**
Error without verbose with this command `bazel build //tensorflow/lite:libtensorflowlite.so`:
```
LINK : warning LNK4044: эхЁрёяючэрээ√щ ярЁрьхЄЁ ""/s""; шуэюЁшЁєхЄё 
ERROR: C:/users/hell_/documents/tensorflow/tensorflow/lite/BUILD:452:1: output 'tensorflow/lite/libtensorflowlite.so.if.lib' was not created
ERROR: C:/users/hell_/documents/tensorflow/tensorflow/lite/BUILD:452:1: not all outputs were created or valid
Target //tensorflow/lite:libtensorflowlite.so failed to build
INFO: Elapsed time: 0.943s, Critical Path: 0.38s
INFO: 1 process: 1 local.
FAILED: Build did NOT complete successfully
```
And error running `bazel build -c opt - verbose_failures //tensorflow/lite:libtensorflowlite.so`:
```
Skipping 'Ч': Bad target pattern 'Ч': package names may contain A-Z, a-z, 0-9, or any of ' !""#$%&'()*+,-./;<=>?[]^_`{|}~' (most 7-bit ascii characters except 0-31, 127, ':', or '\')
ERROR: Bad target pattern 'Ч': package names may contain A-Z, a-z, 0-9, or any of ' !""#$%&'()*+,-./;<=>?[]^_`{|}~' (most 7-bit ascii characters except 0-31, 127, ':', or '\')
INFO: Elapsed time: 0.272s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
```
Thank you"
34925,sparse_matrix_sparse_matmul gives wrong result when run on GPU,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 2.1.0-rc0
- **Python version**: 3.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 10.1.243
- **GPU model and memory**: Titan X
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
When using sparse matmul with 2 sparse matrices on GPU, the result is wrong (has bad indices and some entries are 0 that should not be). The code below fails with  

> tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[256] = [0,0,0] is out of 
order. Many sparse ops require sorted indices.
>    Use `tf.sparse.reorder` to create a correctly ordered copy.

  The same code runs without any errors on CPU. 


### Source code / logs

```python
import tensorflow as tf
from tensorflow.python.ops.linalg.sparse import sparse


a = tf.random.uniform([5, 300, 200])
a = tf.nn.dropout(a, 0.5)
sa = tf.sparse.from_dense(a)
csr_sa = sparse.CSRSparseMatrix(sa)
smm = sparse.matmul(csr_sa, csr_sa, transpose_b=True)
smm = sparse.csr_sparse_matrix_to_sparse_tensor(smm._matrix, type=tf.dtypes.float32)
smm = tf.sparse.SparseTensor(indices=smm.indices, values=smm.values, dense_shape=smm.dense_shape)
mm = tf.linalg.matmul(a, a, transpose_b=True)
tf.debugging.assert_near(mm, tf.sparse.to_dense(smm))
```"
34924,Support EIGEN_USE_MKL_ALL macro for building tensorflow,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): tf1.14, tf2.0
- Are you willing to contribute it (Yes/No): No, I am not familiar with bazel



**Describe the feature and the current behavior/state.**
If my understanding is correct, the compiling flag `--config=mkl` for bazel only enables mkl-dnn supports which could replace several basic ops like matrix multiplication with mkl implementation using jit. However, it seems to me that this flag doesn't enable mkl linkage with Eigen. Therefore, all linear ops beyond several covered in mkl-dnn are still executed in plain eigen single-threaded implementation which is too slow to use (there can be O(10) or even O(100) speed difference for large matrix and eigh, svd, qr ops, as previously noted in https://github.com/tensorflow/tensorflow/issues/7128, https://github.com/tensorflow/tensorflow/issues/13222, etc.). 

Currently, in tensorflow.bzl, `DEIGEN_USE_VML` flag is set when compiled with --config=mkl. As explained in https://github.com/tensorflow/tensorflow/issues/30592, this indicates eigen is not enabled with mkl at all. But a simple replacement of this flag with `DEIGEN_USE_MKL_ALL` leads to failure of the compiling with the error complaining <mkl.h> not found. Also as noted https://github.com/tensorflow/tensorflow/pull/12219, MKL optimized Tensorflow does not support EIGEN_USE_MKL_ALL. I know little about bazel setup, so I don't know whether turn on such support is involved or as simple as some small tweaks.

In sum, tuning tf building system to ""really"" enable mkl behind tf is of great importance and it is vital for the speed of a large range of matrix ops. And this should be the expected behavior for `--config=mkl` flag after all. Currently, so called ""intel optimized"" or ""mkl enabled"" tensorflow is somehow confusing.

**Will this change the current api? How?**
Not for the user level API.

**Who will benefit with this feature?**
Anyone using tf in his/her workflow including matrix operations like EIG, SVD, QR etc on CPU. (One can argue that there is no problem for GPU implementation, but cusolver implementations for SVD and QR can still be much slower than mkl cpu implementations. So fast CPU implementations are critical for these matrix decomposition types op).

**Any Other info.**
"
34922,Tensorflow Lite does not include GPU ,"**System information**
- OS Platform and Distribution: Linux Mint 19
- TensorFlow installed from (source or binary): source
- TensorFlow version: current master (094da7eaa9d802ee676170f2df58ded528da52d9)
- Installed using: make (as shown here: https://www.tensorflow.org/lite/guide/build_arm64)
- GCC/Compiler version (if compiling from source): gcc 7.4.0
- CUDA/cuDNN version: V9.1.85
- GPU model and memory: GeForce GTX 1660 6GB

**Describe the problem**
The Makefile (tensorflow/tensorflow/lite/tools/make/Makefile) for TF Lite does not include any possibility to include gpu delegates for x86.

```bash
>> ./download_dependencies.sh
>> ./build.lib.sh
```

Then e.g. this will result in an undefined reference error: 
```cpp
TfLiteGpuDelegateV2Create()
```

**What I have tried**
I adapted some files and tried to get it working, but with little success:
```bash
# for cl support
>> sudo apt-get install nvidia-cuda-toolkit
# egl
>> sudo apt-get install libegl1-mesa-dev
# gles
>> sudo apt-get install libgles2-mesa-dev

# add fp16 to download_dependencies.sh
FP16_URL=""https://github.com/Maratyszcza/FP16/archive/master.zip""
...
download_and_extract ""${FP16_URL}"" ""${DOWNLOADS_DIR}/fp16""

# add to Makefile
INCLUDES := \
...
-I$(MAKEFILE_DIR)/downloads/fp16/include \
...

CORE_CC_ALL_SRCS := \
...
$(wildcard tensorflow/lite/delegates/*.cc) \
$(wildcard tensorflow/lite/delegates/gpu/**/*.cc) \
...
```

Results in error:
```
In file included from /usr/include/EGL/eglplatform.h:124:0,
                 from /usr/include/EGL/egl.h:39,
                 from ./tensorflow/lite/delegates/gpu/cl/egl_sync.h:19,
                 from tensorflow/lite/delegates/gpu/cl/egl_sync.cc:16:
./tensorflow/lite/delegates/gpu/common/status.h:47:7: error: expected identifier before ‘int’
 class Status {
```
"
34921,Cann't transform TF.EagerTensor to python datatype in map_fun,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Win10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.0.0
- Python version:3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.0
- GPU model and memory:GTX1060 6Gb

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I want to use `dataset = tf.data.Dataset.list_files('./*.mat')` to create a dataset, when i using `next(iter(dataset))` and `map_fun` to parse the data, it is ok. The `map_fun` is as below:
```python
def map_fun(filename):
	filename = filename.numpy()
	data = sio.loadmat(filename)
	x = tf.cast(data['X'],dtype = tf.float32)
	label = tf.cast(data['label'].reshape(-1,), dtype = tf.int8)
	return x,label
```
Because tensorflow doesn't support to read .mat file directly,so l need to use `scipy.io.loadmat` to load .mat file.But when I use the `map` method to process lots of .mat files such as:
```python
db = dataset.map(map_fun)
```
it is not work due to `.numpy()` method shouldn't be used based on eager mode.

**Describe the expected behavior**
Can using tf.data.Dataset.map to process a lot of .mat files so that can be create a dataset.
I think i should transfrom the `TF.EagerTensor` to python data type(in my case, str) so that `scipy.io.loadmat` could work.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
34920,Unrolled LSTM terrible performance when tf.function is used,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Windows 10
- TensorFlow installed from : binary
- TensorFlow version : 2.0
- Python version: 3.6.9
- CUDA/cuDNN version: no CUDA
- GPU model and memory: no GPU

Well I thought I knew what tf.function does, but these two pieces of code confused me:

This does not use tf.function:
```
import time
import tensorflow as tf

rnn = tf.keras.layers.LSTM(256, return_state=True, unroll=True)

def go():
    return rnn(tf.zeros((1, 300, 256)))

st = time.time()
with tf.GradientTape() as tape:
    x = go()
print(f'graph: {time.time() - st}') # --> graph: 0.20923495292663574
st = time.time()
gradient = tape.gradient(x[-1], rnn.trainable_weights) # --> gradient: 0.4629485607147217
print(f'gradient: {time.time() - st}')
```

While this does:
```
import time
import tensorflow as tf

rnn = tf.keras.layers.LSTM(256, return_state=True, unroll=True)

@tf.function
def go():
    return rnn(tf.zeros((1, 300, 256)))

st = time.time()
with tf.GradientTape() as tape:
    x = go()
print(f'graph: {time.time() - st}') # --> graph: 12.843713283538818
st = time.time()
gradient = tape.gradient(x[-1], rnn.trainable_weights) # --> gradient: 2.7147161960601807
print(f'gradient: {time.time() - st}')
```

This difference in performance only happens when unroll=True. Is this expected?
"
34912,AssertionError: Could not compute output Tensor,"I am trying to use functional API to build an encoder-decoder model for grammatical error correction. Given incorrect sentences (x) and correct sentences (y):

```
x = tf.random.uniform(shape=(1000,8), minval=2, maxval=voc_len, dtype=tf.dtypes.int32)
y = tf.random.uniform(shape=(1000,6), minval=2, maxval=voc_len, dtype=tf.dtypes.int32)

x = np.array(tf.concat([tf.ones([1000,1], tf.int32), x, tf.zeros([1000,1], tf.int32)], 1))
y = np.array(tf.concat([tf.ones([1000,1], tf.int32), y, tf.zeros([1000,1], tf.int32)], 1))

input_x = Input(shape=(None,), name='inc_sent')
input_y = Input(shape=(None,), name='cor_sent')

emb_enc = Embedding(input_dim=voc_len+1, output_dim=emb_size, name='inc_emb')(input_x)
emb_dec = Embedding(input_dim=voc_len+1, output_dim=emb_size, name='cor_emb')(input_y)

gru_enc_l1 = GRU(units, return_sequences = True, return_state = True, recurrent_initializer='glorot_uniform', name='inc_gru_l1')
gru_dec_l1 = GRU(units, return_sequences = True, return_state = True, recurrent_initializer='glorot_uniform', name='cor_gru_l1')

o_enc1, h_enc1 = gru_enc_l1(emb_enc, initial_state = tf.zeros([1000,100]))
o_dec1, h_dec1 = gru_dec_l1(emb_dec, initial_state = h_enc1)

# I would like to inspect o_dec1:
model = Model(inputs = [input_x, input_y], outputs = o_dec1)

model(x,y)
```
The error shows `AssertionError: Could not compute output Tensor(""cor_gru_l1_28/Identity:0"", shape=(1000, None, 100), dtype=float32)`

How can I fix it? Please let me know. Thank you in advance"
34909,Non-deterministic access to Random Number Generator in tf.data.Dataset.map with num_parallel_calls > 1,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.7
- CUDA/cuDNN version: 10.0
- GPU model and memory: Tesla K80

**Describe the current behavior**
Currently, when a `tf.random.experimental.Generator` is passed to an operator used in `tf.data.Dataset.map` with `num_parallel_calls` > 1, this random number generator is accessed in a non-deterministic order, which makes that the output of the dataset is non-reproducible (see MVCE below).

**Describe the expected behavior**
The expected behavior is that even when parallelized inside `.map`, Random Number Generators are called in a deterministic way, so that the `data.Dataset` overall pipeline gets fully reproducible.

I do not know whether this is a bug, a desired behavior, or an unavoidable side-effect of parallelizing the operator passed to `.map`, but the overall consequence is that an operator which leverages random operations (like data augmentation, typically) can not be parallelized over `tf.data.dataset` pipelines if one wants to keep its experiments reproducible. 

I am interested in any trick / workaround which would authorize me to keep both high - performance pipelines and experiments reproducibility.

**Code to reproduce the issue**
See below a simple MVCE: It is self explanatory. 

The `test_sequential` creates a dummy `dataset` of length 10, maps an op which pulls 1 number from a RNG  (not parallelized). The 10 resulting values are concatenated into a 10-length vector. It then repeats the overall process and compares the vector from the first draft to the one of the second draft.
The `test_parallel` does exactly the same. However, the op pulling numbers from the RNG is parallelized over 4 threads. When comparing the two 10-length vectors, they are non equal often.

**`test_parallel` is run 10 times which should be enough to highlight some discrepancies (shuffled values) between the two 10-length generated vectors.**
The `test_sequential` shows that the RNG is indeed reproducible when accessed to in a sequential way.

```
import tensorflow as tf
print('TF Version', tf.__version__)
import numpy as np

def draw_samples(num_parallel_calls):

    def mapper(x, rng):
        return rng.uniform(shape=(), minval=0, maxval=10, dtype=tf.int32)

    seed = 12345
    algo = 1
    state = tf.random.experimental.create_rng_state(seed, 1)
    rng = tf.random.experimental.Generator(state=state, alg=algo)

    ds = tf.data.Dataset.from_tensor_slices(tf.range(10)).map(lambda x: mapper(x, rng), num_parallel_calls=num_parallel_calls).batch(10)

    return next(iter(ds)).numpy()

def test_answer_sequential():
    r = [draw_samples(num_parallel_calls=1) for _ in range(2)]
    print('\nSequential')
    print('\n', r[0].tolist(), '\n', r[1].tolist())


def test_answer_parallel():
    r = [draw_samples(num_parallel_calls=4) for _ in range(2)]
    if not np.allclose(*r):
      print('\nNon deterministic access when spread on 4 threads:')
      print('\tFirst draw ', r[0].tolist(), '\n\tSecond draw', r[1].tolist())
    else:
      print('Ok by chance')

test_answer_sequential()
for _ in range(10):
  test_answer_parallel()
```
"
34908,Saving custom tensor fails,"Hello everyone,

There is a bug with saving and loading of custom tensors (variables) in TF. It was discovered at https://github.com/GPflow/GPflow/issues/1127. I have created the MWE in colab [here](https://colab.research.google.com/drive/1REEmDStqGGvl9EmYRa6KB44YXT7EVZkt). There are two errors when `tf.save_model` applies to custom tensors:

* The loading for custom tensor (variable) restores broken tf.Variable object
* Saving of tf.Module with custom tensors (variables) fails

Details in [colab notebook](https://colab.research.google.com/drive/1REEmDStqGGvl9EmYRa6KB44YXT7EVZkt)

PS: referencing @alextp and @dynamicwebpaige, because they know what I mean by custom tensor.

Thanks,
Artem "
34907,"[TF 2.1rc0] The `{predict,train,test}_on_batch` trace functions with fixed batch size","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Stretch
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.1.0rc0
- Python version: 3.5.3

**Describe the current behavior**

When using the updated `predict_on_batch` method wrapped now in `tf.function`, the traced calls have a fixed batch size. Therefore, if batch size changes regularly, the function is retraced all the time. For example, the following

```python
import numpy as np
import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Dense(1, input_shape=[1])
])
model.compile(optimizer=tf.optimizers.Adam(),
              loss=tf.losses.MeanSquaredError())

for i in range(1, 300):
    model.predict_on_batch(np.ones([i, 1]))
```

gives

```
WARNING:tensorflow:5 out of the last 5 calls to <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f2185a37630> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.
WARNING:tensorflow:6 out of the last 6 calls to <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f2185a37630> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.
...
```

The same issue happend with `train_on_batch` and `test_on_batch`.

**Describe the expected behavior**

The `{predict,train,test}_on_batch` methods are traced with undefined batch size.

Alternatively, the behaviour could be configurable. Each of `{predict,train,test}_on_batch` could take `undefined_batch_size=True/False` argument; personally I vote for a default value of `undefined_batch_size=True`.

**And I just realized:** if `*_on_batch` methods are used with sequences (for example sentences in NLP), then the retracing would happen on every call. So instead, it would be better to allow passing `experimental_relax_shapes` to the `*_on_batch` or even to `.compile` call.

**Code to reproduce the issue**
Available above.

**Other info / logs**"
34906,tensorflow gpu inference  only one thread is busy," PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND
  **5708 root      20   0 21.518g 1.820g 794224 S 82.0  0.7   6:38.31 server**
  5722 root      20   0 21.518g 1.820g 794224 S  8.0  0.7   0:36.46 server
  5747 root      20   0 21.518g 1.820g 794224 S  8.0  0.7   0:36.61 server
  5712 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.35 server
  5714 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.49 server
  5716 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.57 server
  5718 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.36 server
  5721 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.52 server
  5723 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.42 server
  5724 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.47 server
  5725 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.60 server
  5726 root      20   0 21.518g 1.820g 794224 R  7.7  0.7   0:36.48 server
  5728 root      20   0 21.518g 1.820g 794224 R  7.7  0.7   0:36.35 server
  5730 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.46 server
  5731 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.37 server
  5734 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.42 server
  5736 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.43 server
  5737 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.35 server
  5741 root      20   0 21.518g 1.820g 794224 R  7.7  0.7   0:36.58 server

tensorflow gpu inference  only one thread is busy, and the inference is so slow ,and the gpu useage is low, something bug?

tensorflow gpu version is 1.13.1"
34905,`experimental_relax_shapes` argument of `tf.function` does not work on instance methods,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Stretch
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0, 2.1.0rc0
- Python version: 3.5.3

**Describe the current behavior**
Consider the following source:
```python
import numpy as np
import tensorflow as tf

class A:
    @tf.function(experimental_relax_shapes=True)
    def f(self, data):
        return tf.reduce_sum(data)

@tf.function(experimental_relax_shapes=True)
def f(data):
    return tf.reduce_sum(data)

a = A()
for i in range(100):
    print(a.f(np.ones(i)))
```

Then all calls of `a.f` cause retracing of A.f, even with `experimental_relax_shapes=True`.

Note that if
```python
    print(f(np.ones(i)))
```
is used instead of `a.f`, the `experimental_relax_shapes` kicks in and the retracing stops after third call to `f`.

**Describe the expected behavior**

The `experimental_relax_shapes` should work also on instance method.

**Code to reproduce the issue**

Given above.

**Other info / logs**

The problem is that when creating the instance method wrapper in
  https://github.com/tensorflow/tensorflow/blob/746e018d181f52b04f77811b1fbdf9bccdbd1d83/tensorflow/python/eager/function.py#L3179
several parameters including experimental_relax_shapes are not copied, see
  https://github.com/tensorflow/tensorflow/blob/746e018d181f52b04f77811b1fbdf9bccdbd1d83/tensorflow/python/eager/function.py#L3221-L3225"
34904,TF Dataset - group_by_window without iterating the dataset first (input_window_size),"I have described this in StackOverflow, but may be I have to post my question also here:
https://stackoverflow.com/questions/59216451/tf-data-group-by-window-without-iterating-the-complete-dataset-first

`tf.data.experimental.group_by_window` seems to always iterate the complete Dataset before outputing something. Is this an implementation issue, or am I doing something wrong?

I'm trying to reduce consecutive frames of fixed size into a single tensor grouped by the example they belong to, i.e. having an input dataset like:
```
id | feature_frame
------------------
 0 | [0, 1, 2, 3]
 0 | [4, 3, 2, 1]
 1 | [3, 1, 0, 0]
 2 | [7, 7, 1, 2]
 2 | [2, 7, 1, 2]
 2 | [4, 7, 1, 3]
```

how to turn it into (without iterating to the end of the dataset):
```
id| features_batch
------------------
0 | [[0,1,2,3],[4,3,2,1]]
1 | [[3,1,0,0]]
2 | [[7,7,1,2],[2,7,1,2],[4,7,1,3]]
```

@jsimsa  - you have answer similar questions (#30585) in the past, It would be great if you can help here!

Source code to reproduce (change the `max_len=100` in `ReducerTestCase.test_reducer()` to make it work as expected):
```python


class FramesDS:

    def example_frames_ds(self, max_len=None):
        def gen():
            example_count = 0
            while True:
                example_id = np.random.randint(low=np.iinfo(np.int64).min,
                                               high=np.iinfo(np.int64).max, dtype='int64')
                frame_count = np.random.randint(low=2, high=10)
                print(""{:4d}| {}: frame_count:{}"".format(example_count, example_id, frame_count))
                example_count += 1

                max_seq_len = 8
                frames  = np.random.randint(low=0, high=9, size=(frame_count, max_seq_len))
                example_ids = [example_id] * frame_count
                yield example_ids, frames
        ds = tf.data.Dataset.from_generator(gen,
                                            output_types=(tf.int64, tf.int32),
                                            output_shapes=(tf.TensorShape([None, ]), tf.TensorShape([None, None])))
        if max_len is not None:
            ds = ds.take(max_len)
        return ds


class ReducerTestCase(unittest.TestCase):
    def test_reducer(self):

        max_len = None   # set to 100 to make it work

        ds = FramesDS().example_frames_ds(max_len)

        def key_fn(example_id, frame):
            return example_id

        def init_fn(example_id):
            return example_id, tf.zeros([0,], dtype=tf.int32)

        def reduce_fn(state, rinput):
            state_eid, frames = state
            example_id, frame = rinput
            tf.assert_equal(state_eid, example_id)
            frames = tf.concat([tf.reshape(frames, (tf.shape(frames)[0],
                                                    tf.shape(frame)[-1])),
                                tf.expand_dims(frame, axis=0)], axis=0)
            return example_id, frames

        def fin_fn(example_id, frames):
            return example_id, frames

        reducer = tf.data.experimental.Reducer(init_func=init_fn,
                                               reduce_func=reduce_fn,
                                               finalize_func=fin_fn)

        ds = ds.unbatch().batch(8)
        ds = ds.unbatch()

        def window_reduce_fn(key, ds):
            ds = ds.apply(tf.data.experimental.group_by_reducer(key_func=key_fn, reducer=reducer))
            return ds

        ds = ds.apply(tf.data.experimental.group_by_window(key_func=key_fn,
                                                           reduce_func=window_reduce_fn,
                                                           window_size=20))

        for example_id, frames in tqdm(ds):
            print(""{}: {}"".format(example_id, frames.shape))

```"
34901,keras saved model cannot be loaded (no custom layer),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip install tensorflow-gpu
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.2
- CUDA/cuDNN version: 10.0.130 / 7.6.1
- GPU model and memory: GTX 1060 MAX-Q 6GB

**Describe the current behavior**

A keras `Sequential` model trained and saved cannot be re-loaded by `tf.keras.models.load_model`.
The complete error message:

```
TypeError                                 Traceback (most recent call last)
c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\framework\tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
    540     try:
--> 541       str_values = [compat.as_bytes(x) for x in proto_values]
    542     except TypeError:

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\framework\tensor_util.py in <listcomp>(.0)
    540     try:
--> 541       str_values = [compat.as_bytes(x) for x in proto_values]
    542     except TypeError:

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\util\compat.py in as_bytes(bytes_or_text, encoding)
     70     raise TypeError('Expected binary or unicode string, got %r' %
---> 71                     (bytes_or_text,))
     72

TypeError: Expected binary or unicode string, got -1

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-65-6d5b848d58f1> in <module>
----> 1 tf.keras.models.load_model(""model.h5"")

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\keras\saving\save.py in load_model(filepath, custom_objects, compile)
    144   if (h5py is not None and (
    145       isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):
--> 146     return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
    147
    148   if isinstance(filepath, six.string_types):

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\keras\saving\hdf5_format.py in load_model_from_hdf5(filepath, custom_objects, compile)
    166     model_config = json.loads(model_config.decode('utf-8'))
    167     model = model_config_lib.model_from_config(model_config,
--> 168                                                custom_objects=custom_objects)
    169
    170     # set weights

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\keras\saving\model_config.py in model_from_config(config, custom_objects)
     53                     '`Sequential.from_config(config)`?')
     54   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top
---> 55   return deserialize(config, custom_objects=custom_objects)
     56
     57

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\keras\layers\serialization.py in deserialize(config, custom_objects)
    100       module_objects=globs,
    101       custom_objects=custom_objects,
--> 102       printable_module_name='layer')

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\keras\utils\generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    189             custom_objects=dict(
    190                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +
--> 191                 list(custom_objects.items())))
    192       with CustomObjectScope(custom_objects):
    193         return cls.from_config(cls_config)

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\keras\engine\sequential.py in from_config(cls, config, custom_objects)
    368       layer = layer_module.deserialize(layer_config,
    369                                        custom_objects=custom_objects)
--> 370       model.add(layer)
    371     if not model.inputs and build_input_shape:
    372       model.build(build_input_shape)

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\training\tracking\base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--> 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\keras\engine\sequential.py in add(self, layer)
    194       # If the model is being built continuously on top of an input layer:
    195       # refresh its output.
--> 196       output_tensor = layer(self.outputs[0])
    197       if len(nest.flatten(output_tensor)) != 1:
    198         raise TypeError('All layers in a Sequential model '

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py in __call__(self, inputs, *args, **kwargs)
    840                     not base_layer_utils.is_in_eager_or_tf_function()):
    841                   with auto_control_deps.AutomaticControlDependencies() as acd:
--> 842                     outputs = call_fn(cast_inputs, *args, **kwargs)
    843                     # Wrap Tensors in `outputs` in `tf.identity` to avoid
    844                     # circular dependencies.

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\keras\layers\pooling.py in call(self, inputs, mask)
    641       input_shape = inputs.shape.as_list()
    642       broadcast_shape = [-1, input_shape[steps_axis], 1]
--> 643       mask = array_ops.reshape(mask, broadcast_shape)
    644       inputs *= mask
    645       return backend.sum(inputs, axis=steps_axis) / math_ops.reduce_sum(

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\ops\array_ops.py in reshape(tensor, shape, name)
    129     A `Tensor`. Has the same type as `tensor`.
    130   """"""
--> 131   result = gen_array_ops.reshape(tensor, shape, name)
    132   tensor_util.maybe_set_static_shape(result, shape)
    133   return result

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\ops\gen_array_ops.py in reshape(tensor, shape, name)
   8115   # Add nodes to the TensorFlow graph.
   8116   _, _, _op = _op_def_lib._apply_op_helper(
-> 8117         ""Reshape"", tensor=tensor, shape=shape, name=name)
   8118   _result = _op.outputs[:]
   8119   _inputs_flat = _op.inputs

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\framework\op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    528           except TypeError as err:
    529             if dtype is None:
--> 530               raise err
    531             else:
    532               raise TypeError(

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\framework\op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    525                 dtype=dtype,
    526                 as_ref=input_arg.is_ref,
--> 527                 preferred_dtype=default_dtype)
    528           except TypeError as err:
    529             if dtype is None:

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\framework\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)
   1294
   1295     if ret is None:
-> 1296       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1297
   1298     if ret is NotImplemented:

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\framework\constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    284                                          as_ref=False):
    285   _ = as_ref
--> 286   return constant(v, dtype=dtype, name=name)
    287
    288

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\framework\constant_op.py in constant(value, dtype, shape, name)
    225   """"""
    226   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--> 227                         allow_broadcast=True)
    228
    229

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\framework\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    263       tensor_util.make_tensor_proto(
    264           value, dtype=dtype, shape=shape, verify_shape=verify_shape,
--> 265           allow_broadcast=allow_broadcast))
    266   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
    267   const_tensor = g.create_op(

c:\users\kylec\appdata\local\programs\python\python36\lib\site-packages\tensorflow_core\python\framework\tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
    543       raise TypeError(""Failed to convert object of type %s to Tensor. ""
    544                       ""Contents: %s. Consider casting elements to a ""
--> 545                       ""supported type."" % (type(values), values))
    546     tensor_proto.string_val.extend(str_values)
    547     return tensor_proto

TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [-1, None, 1]. Consider casting elements to a supported type.
```

Also if I switch to use functional API the result remains the same.

**Describe the expected behavior**

A saved model should be able to re-loaded without error.

**Code to reproduce the issue**

```python
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences

(x_train, y_train), (x_test, y_test) = imdb.load_data(path=""imdb.npz"",
                                                      num_words=None,
                                                      skip_top=0,
                                                      maxlen=None,
                                                      seed=113,
                                                      start_char=1,
                                                      oov_char=2,
                                                      index_from=3)

x_train = pad_sequences(x_train, padding=""post"")
maxlen = x_train.shape[1]
vocab_size = x_train.max() + 1

model = tf.keras.Sequential([
  tf.keras.layers.Input(shape=(maxlen,), name=""sequence""),
  tf.keras.layers.Embedding(vocab_size, 32, mask_zero=True, name=""word_embedding""),
  tf.keras.layers.GlobalAveragePooling1D(name=""doc_embedding""),
  tf.keras.layers.Dense(16, activation=""relu"", name=""relu""),
  tf.keras.layers.Dense(1, activation=""sigmoid"", name=""sigmoid"")
], name=""nn_classifier"")
model.compile(optimizer=""adam"", loss=""binary_crossentropy"", metrics=[""accuracy""])
metrics = model.fit(x=x_train, y=y_train, batch_size=256, epochs=1)
model.save(""model.h5"")

tf.keras.models.load_model(""model.h5"")  # Failed.
```

"
34900,[TF2.0] can't model.save(save_format='h5') NotImplementedError,"I cant use ```model.save()``` like this link
https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model
python=3.7.5
tensorflow-gpu=2.0.0
transformers=2.2.1


```
input_layer = Input(shape = (512,), dtype='int64') 
bert = TFBertModel.from_pretrained('bert-base-chinese')(input_layer)
bert = bert[0]   
dropout = Dropout(0.1)(bert)
flat = Flatten()(dropout)
classifier = Dense(units=5, activation=""softmax"")(flat)               
model = Model(inputs=input_layer, outputs=classifier)
model.summary()

optimizer = tf.keras.optimizers.Adam(learning_rate=5e-6, epsilon=1e-08, clipnorm=1.0)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss, metrics=[metric])
```
```
>
Model: ""model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 512)]             0         
_________________________________________________________________
tf_bert_model (TFBertModel)  ((None, 512, 768), (None, 102267648 
_________________________________________________________________
dropout_37 (Dropout)         (None, 512, 768)          0         
_________________________________________________________________
flatten (Flatten)            (None, 393216)            0         
_________________________________________________________________
dense (Dense)                (None, 5)                 1966085   
=================================================================
Total params: 104,233,733
Trainable params: 104,233,733
Non-trainable params: 0
```

```
model.save('model/my_model.h5')
```
```
>
NotImplementedError
```
But I did have InputLayer





I have no choice to use ```tf.keras.models.save_model()```
```
tf.keras.models.save_model(
    model,
    ""model/model_bert_eland_softmax_2"",
    overwrite=True,
    include_optimizer=True,
)
```



But when I loaded it,  I have to add an inputlayer and I losed my model layers structures.
```
input_layer = Input(shape = (512,), dtype='int64')  
load_model = tf.keras.models.load_model('model/model_bert_eland_softmax_2')(input_layer)
new_model = Model(inputs=input_layer, outputs=load_model)

# Show the model architecture
new_model.summary()
```
```
>
Model: ""model_1""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 512)]             0         
_________________________________________________________________
model (Model)                (None, 5)                 104233733 
=================================================================
Total params: 104,233,733
Trainable params: 104,233,733
Non-trainable params: 0
```

Is it a bug?

Is it possible to use ```tf.keras.models.save_model()``` or ```model.save()```
and still keep all layers of my model like this link?
https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model"
34899,Fit of Undefined in train TensorFlowJS - Transfer learning audio recognizer,"In the example of 'Transfer learning audio recognizer' there is an error on lines.
**Uncaught (in promise) TypeError: Cannot read property 'fit' of undefined**

there is an error highlighted like shown :

![Ekran Resmi 2019-12-06 17 09 11](https://user-images.githubusercontent.com/25597808/70328972-566fc200-184b-11ea-9add-c31964511b35.png)
"
34898,tf.execute_volume_patches misses one dimension when used with placeholders and None,"Tested in with tf 1.12.0 and tf 2.0. Below code for use with tf 2.0.

p = tf.keras.backend.placeholder((1,None,None,None,1))
kernel = [1,3,3,3,1]
strides = [1,1,1,1,1]
tf.extract_volume_patches(p, kernes, strides, 'SAME')
>>><tf.Tensor 'ExtractVolumePatches:0' shape=(1, None, None, 27) dtype=float32>

p = tf.keras.backend.placeholder((1,120,130,140,1))
kernel = [1,3,3,3,1]
strides = [1,1,1,1,1]
tf.extract_volume_patches(p, kernes, strides, 'SAME')
>>><tf.Tensor 'ExtractVolumePatches:0' shape=(1, 120, 130, 140, 27) dtype=float32>

Is there a reason behind this behaviour?
"
34897,TFLITE C++ ,"Hi,

We are able to run TFLITE CPP application(label_image) on ARM processor which uses CPU.
Can we run same TFLITE CPP application on any GPU like NVIDIA, POWERVR.

Thanks,
Ram."
34896,Tensorflow Lite Micro fails to build for Arm Mbed OS,"**System information**
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1c8c3f2015
- GCC/Compiler version (if compiling from source):

**Describe the problem**
Buidling Tensorflow Lite Micro for Arm Mbed OS results in the following compile error:
`[Fatal Error] scoped_profiling_label_wrapper.h@38,39: profiling/instrumentation.h: No such file or directory
[ERROR] In file included from ./tensorflow/lite/kernels/internal/reference/integer_ops/mul.h:20:0,
                 from ./tensorflow/lite/experimental/micro/kernels/mul.cc:20:
./tensorflow/lite/kernels/internal/scoped_profiling_label_wrapper.h:38:39: fatal error: profiling/instrumentation.h: No such file or directory
 #include ""profiling/instrumentation.h""
                                       ^
compilation terminated.`

It's related to this [commit](https://github.com/tensorflow/tensorflow/commit/27aac0fe54bf121abed62757891ae4580eb12fc5#diff-90548b139ee2b479dbf2f409aef40ff3) that adds some profiling code.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
$ make -f tensorflow/lite/experimental/micro/tools/make/Make generate_network_tester_test_mbed_project
$ cd tensorflow/lite/experimental/micro/tools/make/gen/linux_x86_64/prj/network_tester_test/mbed
$ mbed config root .
$ mbed deploy
$ mbed compile -m DISCO_F746NG -t GCC_ARM
"
34895,SavedModel format for tf.estimator class in Tensorflow 2.0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os mojave 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):  2.0.0
- Python version: 2.7.10 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I'm moving to TF 2.0 with its very nice dataset functionalities, but I got stuck when I want to save the model in the SavedModel format.

I'm using the estimator class to do a linear regression, and after training this is how I'd set up the export in TF 1:
```
columns = [('hour', tf.int64),
           ('domain', tf.string),
           ('device_type', tf.string)]
feature_placeholders = {
 name: tf.placeholder(dtype, [1], name=name + ""_placeholder"")
 for name, dtype in columns
}
```
I have three features with different datatypes, and I use the placeholder method to concatenate them into a dict that is then served using the tf.estimator.export.build_raw_serving_input_receiver_fn() method, and finally exported using the estimator.export_saved_model to my model directory:

```
export_input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(
    feature_placeholders)
estimator.export_saved_model(model_dir, export_input_fn)
```
All tutorials online uses this series of steps, but tf.placeholder() doesn't exist in TF 2.0, so how can I do this?
"
34893,"load_weights dont work in tf.keras: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ""mobilenet_model"": Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.14, 1.15, 2.0, 2.1rc0
- Keras version: 2.2.4-tf and 2.2.4
- Python version: 3.6
- CUDA/cuDNN version: 10.0 and 10.1 / 7.6.5
- GPU model and memory: RTX 2060 6GB

When I use the **tf.keras** and try to load the weights from mobilenet modeI get an error""
`W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ""mobilenet_model"": Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?`

When use only **keras** it's ok.

**Simple code to reproduce the issue**
```
import os

use_tfkeras = True

if use_tfkeras:
    from tensorflow.compat.v1.keras.applications.mobilenet import MobileNet
else:
    from keras.applications.mobilenet import MobileNet


def save_mobilenet_weights(alpha, filename):
    mobilenet = MobileNet(alpha=alpha, input_tensor=None, include_top=False, weights='imagenet', pooling=None)
    if use_tfkeras:
        mobilenet.save_weights(filepath=os.path.abspath(filename), overwrite=True, save_format='h5')
    else:
        mobilenet.save_weights(filepath=os.path.abspath(filename), overwrite=True)

def load_mobilenet_weights(alpha, filename):
    mobilenet = MobileNet(alpha=alpha, input_tensor=None, include_top=False, weights=None, pooling=None)
    mobilenet.load_weights(os.path.abspath(filename))


alpha = 0.75
filename = 'mobilenet_model'

save_mobilenet_weights(alpha, filename)
load_mobilenet_weights(alpha, filename)
```
"
34891,Cannot execute the substraction op with the broadcast mechanism,"```python
import tensorflow as tf
import numpy as np
x = tf.constant(np.random.random((500, 6)))
y = x - x[:,0]
```
This code raises the issue of not supporting [500,6] - [500] in the `sub` OP.
I suppose that it should compute correctly, using the broadcast automatically?
Is it a bug? 
version: tf2.0"
34890,tf.keras.models.Sequential does not support run_eagarly,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v1.12.1-16986-g6c32a22 2.1.0-dev20191029
- Python version: 3.6.8

**Describe the current behavior**
`tf.keras.models.Sequential` doesn't support `run_eagarly` as mentioned in the [docs](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#run_eagerly).

**Describe the expected behaviour**
Either Sequential model accepts `run_eagarly` as a param and changes its behaviour, or we modify the docs. 

**Code to reproduce the issue**

```python
import tensorflow as tf

model = tf.keras.models.Sequential(
    layers=[tf.keras.layers.Dense(input_shape=(3, ), units=1)], 
    run_eagerly=True)
```


**Other info / logs**

```
Traceback (most recent call last):
  File ""tst.py"", line 5, in <module>
    run_eagerly=True)
  File ""/home/squadrick/.local/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'run_eagerly'
```"
34889,documentation,"Porting the original website from bootstrap3  to bootstrap4

Section to change:
- [ ] [_alumni.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_alumni.html)
- [ ] [_events-participate.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_events-participate.html)
- [ ] [_events.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_events.html)
- [ ] [_home.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_home.html)
- [ ] [_intro.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_intro.html)
- [ ] [_open-source.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_open-source.html)
- [ ] [_team.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_team.html)
- [ ] [_webinars.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_webinars.html)


"
34888,Failed to get convolution algorithm. This is probably because cuDNN failed to initialize,"
[error_log.txt](https://github.com/tensorflow/tensorflow/files/3930650/error_log.txt)
Getting ""Failed to get Convolution aglorithm"" error 
tensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04**
- TensorFlow installed from (source or binary): **Binary [https://www.tensorflow.org/install/gpu](url)**
- TensorFlow version: **tensorboard==2.0.2, tensorflow-datasets==1.3.0, tensorflow-estimator==2.0.1, tensorflow-gpu==2.0.0, tensorflow-metadata==0.15.1**
- Python version: **Python 3.6.9**
- Installed using virtualenv? pip? conda?: **pip**
- Bazel version (if compiling from source): **Build label: 1.2.1**
- GCC/Compiler version (if compiling from source): **gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0** 
- CUDA/cuDNN version: 
**`nvidia-smi` command gives me CUDA version as 10.2 and `nvcc --version` command gives me Cuda compilation tools, release 9.1, V9.1.85**
- GPU model and memory: **GeForce GTX 1660 Ti/PCIe/SSE2  and Memory 15.6 GiB**

**Problem**
`tensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.`

Earlier my GPU was not detected and had to go back and forth on drivers and CUDA library versions. I have followed the steps for Ubuntu 18.04 command set in Tensor Flow GPU installation site. I could detect the Graphics card, but having above issue. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I have run one of the image classification example give in TensorFlow tutorials and was getting the above error. I have attached the complete log. 

**Any other info / logs**
Included the logs and the code 
[image_classifier.txt](https://github.com/tensorflow/tensorflow/files/3930678/image_classifier.txt)
[error_log.txt](https://github.com/tensorflow/tensorflow/files/3930680/error_log.txt)


Stuck with this error and not able use the GPU for Tensor Flow computations with above system configuration. Please help me in this regard.

Thanks
Viswanath.B"
34886,ValueError: Received a scalar value '512' as shape; require a statically known scalar with value '-1' to describe an unknown shape.,"**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from :pip
- TensorFlow version : 1.3
- Python version: 2.7


**Code to reproduce the issue**
    stfts = tf.signal.rfft(frames, fft_length=512)

Traceback (most recent call last):
  File ""pre_process_1.py"", line 264, in <module>    feature_2,wav_lens_1,wav_lens,waveforms_0,waveforms_1,waveforms_2,waveforms,frames,stfts,stft_lens,masks = extract_features_tf(feature_1,target_sample_rate=16000)
  File ""pre_process_1.py"", line 86, in extract_features_tf    filter_banks_1,wav_lens_1,wav_lens,waveforms_0,waveforms_1,waveforms_2,waveforms,frames,stfts,stft_lens,masks = common_audio_tf.compute_mel_filterbank_features(signal, sample_rate=target_sample_rate,dither=0.0, frame_length=25, window_fn=None, lower_edge_hertz=0.0, upper_edge_hertz=8000.0, num_mel_bins=64)
  File ""/home/fan/anaconda3/envs/tf_asr/lib/python2.7/site-packages/tensor2tensor/layers/common_audio_tf.py"", line 130, in compute_mel_filterbank_features
    stfts = tf.signal.rfft(frames, fft_length=512)
  File ""/home/fan/anaconda3/envs/tf_asr/lib/python2.7/site-packages/tensorflow/python/ops/signal/fft_ops.py"", line 123, in _rfft
    input_tensor = _maybe_pad_for_rfft(input_tensor, fft_rank, fft_length)
  File ""/home/fan/anaconda3/envs/tf_asr/lib/python2.7/site-packages/tensorflow/python/ops/signal/fft_ops.py"", line 64, in _maybe_pad_for_rfft
    fft_shape = _tensor_util.constant_value_as_shape(fft_length)
  File ""/home/fan/anaconda3/envs/tf_asr/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 888, in constant_value_as_shape
    ""scalar with value '-1' to describe an unknown shape."" % value)
ValueError: Received a scalar value '512' as shape; require a statically known scalar with value '-1' to describe an unknown shape.


"
34878,"Illegal ambiguous match on configurable attribute ""deps"" in //tensorflow/core:ops","**System information**
- OS Platform and Distribution: macOS Catalina 10.15.1
- TensorFlow installed from: source
- TensorFlow version: 2.0.0
- Python version: 3.7.4
- Installed using: [spack](https://spack.io)
- Bazel version: 0.26.1
- Compiler version: Clang 11.0.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**

I'm trying to build a minimal TF installation, but I don't seem to be able to disable GCP support. I get the following error during build time:
```
/private/var/folders/21/hwq39zyj4g36x6zjfyl5l8080000gn/T/Adam/spack-stage/spack-stage-py-tensorflow-2.0.0-ve5t7rb2qt4igekam6hdrwnm6vgts22q/spack-src/tensorflow/core/BUILD:1359:12: Illegal ambiguous match on configurable attribute ""deps"" in //tensorflow/core:ops:
//tensorflow:api_version_2
//tensorflow:no_gcp_support
Multiple matches are not allowed unless one is unambiguously more specialized.
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

First, I set the following environment variables:
```bash
TF_CONFIGURE_IOS=0; export TF_CONFIGURE_IOS
TF_DOWNLOAD_CLANG=0; export TF_DOWNLOAD_CLANG
TF_ENABLE_XLA=0; export TF_ENABLE_XLA
TF_NEED_AWS=0; export TF_NEED_AWS
TF_NEED_CUDA=0; export TF_NEED_CUDA
TF_NEED_GCP=0; export TF_NEED_GCP
TF_NEED_GDR=0; export TF_NEED_GDR
TF_NEED_HDFS=0; export TF_NEED_HDFS
TF_NEED_IGNITE=0; export TF_NEED_IGNITE
TF_NEED_JEMALLOC=0; export TF_NEED_JEMALLOC
TF_NEED_KAFKA=0; export TF_NEED_KAFKA
TF_NEED_MKL=0; export TF_NEED_MKL
TF_NEED_MPI=0; export TF_NEED_MPI
TF_NEED_NGRAPH=0; export TF_NEED_NGRAPH
TF_NEED_OPENCL=0; export TF_NEED_OPENCL
TF_NEED_OPENCL_SYCL=0; export TF_NEED_OPENCL_SYCL
TF_NEED_ROCM=0; export TF_NEED_ROCM
TF_NEED_S3=0; export TF_NEED_S3
TF_NEED_VERBS=0; export TF_NEED_VERBS
TF_SET_ANDROID_WORKSPACE=0; export TF_SET_ANDROID_WORKSPACE
```
Then, I ran:
```console
$ ./configure
$ bazel --nohome_rc --nosystem_rc build --color=no --jobs=4 --config=opt --config=noaws --config=nogcp --config=nohdfs --config=noignite --config=nokafka --config=nonccl --config=v2 //tensorflow/tools/pip_package:build_pip_package
```
The bug seems to occur regardless of whether or not `--config=v2` is added, as that is the default. Using `--config=nogcp --config=v1` seems to work, and not using `--config=nogcp` also seems to work. Is GCP support required in v2?

**Any other info / logs**

[build log](https://github.com/tensorflow/tensorflow/files/3928307/spack-build-out.txt)
[build environment](https://github.com/tensorflow/tensorflow/files/3928308/spack-build-env.txt)

This work is being done in conjunction with https://github.com/spack/spack/pull/13112

I've seen #25644 #24447 #11807 #14522 and other similar issues, but they look specific to CUDA/NCCL or are due to older versions of Bazel."
34877,Getting Bus error in Ubuntu Mate ,"WARNING:tensorflow:From /home/odroid/tensorflow1/models/research/object_detection/utils/label_map_util.py:138: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.

WARNING:tensorflow:From Object_detection_image.py:64: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.

WARNING:tensorflow:From Object_detection_image.py:70: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

Bus error

**Getting this Bus error in Ubuntu Mate on Odroid while running the object detection webcam script  , Using Python 3.5 and tensorflow version 1.14.0
Any idea how to solve that ??"
34876,Memory leak in fit() with callback,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>
** Coding Language **
Python

**System information**

- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from (source or binary): pip3
- TensorFlow version : 2.0.0
- TensorFlow Keras version: '2.2.4-tf'
- Python version: 3.6
- GPU model and memory:

![image](https://user-images.githubusercontent.com/7545134/70257890-11129e00-1740-11ea-944b-291d5fed4c30.png)

**Describe the current behavior**
CPU Ram progressively gets consumed as the fit() works. Each EPOCH consumes more and more memory. This memory leak only happens when a callback is assigned, any callback eg: tensorboard.

The memory consumption can be seen in both ways:
-  System memory (vmstat)
-  Printing resource memory usage in code (print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss))

Attached the vmstat print to screen:
![image](https://user-images.githubusercontent.com/7545134/70258980-196bd880-1742-11ea-9cfe-6d6fa684ac3b.png)


**Code to reproduce the issue**
BATCH_SIZE = 24
learning_rate = 0.000005
decay_rate = 1e-6
EPOCHS = 4000
CLIP_VALUE = 1.5
l1reg = 0.01
l2reg = 0.00

model = Sequential()

model.add(LSTM(512, input_shape=(train_x.shape[1], train_x.shape[2]), activation='relu',
               return_sequences=True, bias_regularizer=L1L2(l1=l1reg, l2=l2reg)))
model.add(Dropout(0.3))
model.add(BatchNormalization())

model.add(LSTM(512, activation='relu', return_sequences = True))
model.add(Dropout(0.3))
model.add(BatchNormalization())

model.add(LSTM(256, activation='relu'))
model.add(Dropout(0.25))
model.add(BatchNormalization())

model.add(Flatten())
model.add(Dense(24, activation='relu'))

model.add(Flatten())
model.add(Dense(2, activation='softmax'))
opt = tf.keras.optimizers.Adam(lr=learning_rate, decay=decay_rate, clipvalue=CLIP_VALUE)

model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=opt,
    metrics=['accuracy'],
)

NAME = f""PARAM-{train_x.shape[2]}-FIRSTTEST-{int(time.time())}""
print(NAME)
print('-----------------------------------------')
tensorboard = TensorBoard(log_dir=f""{ML_LOG_DIR}/{NAME}"")
filepath = ""RNN_Final-{epoch:02d}-{val_accuracy:.3f}""
checkpoint = ModelCheckpoint(""{}/{}.model"".format(ML_MODEL_DIR, filepath, monitor='val_accuracy',
                            verbose=1, save_best_only=True, mode='max')) # saves only the best ones

history = model.fit(
        train_x,
        train_y,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(test_x, test_y)
        callbacks=[tensorboard, checkpoint]
)

**Other info / logs**
The problem goes away if the callbacks are removed.
"
34874,Skipping optimization warning when using tanh activation in LSTM layers,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 19.10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: 3.7
- GPU model and memory: no GPU

**Describe the current behavior**
When training a model containing an LSTM layer with activation function 'tanh', I receive the following warnings:

> W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_2431_2613' and '__inference___backward_standard_lstm_2770_3255_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_3336' both implement 'lstm_c366749f-d70a-4931-b887-2ea0c126b791' but their signatures do not match.
> W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_3844' and '__inference_standard_lstm_3733_specialized_for_model_lstm_StatefulPartitionedCall_at___inference_distributed_function_4076' both implement 'lstm_3433d132-ad4b-4073-bc24-8ed871518bdc' but their signatures do not match.

However, it feels like the model is still training, despite the warning. The warnings disappear when changing the activation function.

**Describe the expected behavior**
The warning shouldn't appear.

**Code to reproduce the issue**
```
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM
from tensorflow.keras.models import Model

n_trn, n_vld = 10_000, 100
x_trn = np.random.randint(2, size=(n_trn, 10, 2))
y_trn = x_trn
x_vld = np.random.randint(2, size=(n_vld, 10, 2))
y_vld = x_vld

train_data = tf.data.Dataset.from_tensor_slices((x_trn, y_trn)).repeat().batch(100)
valid_data = tf.data.Dataset.from_tensor_slices((x_vld, y_vld)).repeat().batch(100)

x = Input(shape=(10, 2))
y = tf.math.abs(LSTM(2, return_sequences=True, dropout=0.3, activation='tanh')(x))

model = Model(inputs=[x], outputs=y)
model.compile(optimizer='adam', loss='binary_crossentropy')
model.fit(train_data, steps_per_epoch=100, epochs=100, validation_data=valid_data, validation_steps=1)
```
"
34873,Using GPU delegate causes app to crash,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'


**Command used to run the converter or code if you’re using the Python API**

```
model = tf.keras.models.load_model('my_conv.h5')
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]
tflite_model = converter.convert()
open(""custom_cnn_f16.tflite"", ""wb"").write(tflite_model)
```

**The output from the converter invocation**

```
It successfully converts the model to TFlite f16
```

**Also, please include a link to the saved model or GraphDef**

```
[Link](https://drive.google.com/file/d/1-0R5fPIWIM7N3Kv442QGTxVJ5Dz0ebZ-/view?usp=sharing)

```

The model is a simple CNN which takes a 50x50x1 grayscale image and outputs probabilities for 10 classes. 

**Failure details**
I want to run the Float16 version of the model using TFLite on the GPU (Samsung S10+) however using the GPU delegate on this model causes the app to crash. I have tested on other devices and this model crashes on all phones when running on the GPU.


**Any other info / logs**
```
2019-12-05 18:06:30.715 3830-3830/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
2019-12-05 18:06:30.716 3830-3830/? A/DEBUG: Build fingerprint: 'xxxxxxxxxxxxxxxx/release-keys'
2019-12-05 18:06:30.716 3830-3830/? A/DEBUG: Revision: '26'
2019-12-05 18:06:30.716 3830-3830/? A/DEBUG: ABI: 'arm64'
2019-12-05 18:06:30.716 3830-3830/? A/DEBUG: pid: 2979, tid: 3198, name: inference  >>> com.test.app <<<
2019-12-05 18:06:30.716 3830-3830/? A/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0
2019-12-05 18:06:30.716 3830-3830/? A/DEBUG: Cause: null pointer dereference
2019-12-05 18:06:30.716 3830-3830/? A/DEBUG:     x0  0000000000000000  x1  0000000000000000  x2  00000070516c44e0  x3  00000070516c43f0
2019-12-05 18:06:30.716 3830-3830/? A/DEBUG:     x4  00000000000000ba  x5  00000070546a5288  x6  000000704d0bb540  x7  000000704d0bb560
2019-12-05 18:06:30.716 3830-3830/? A/DEBUG:     x8  185cb8064dde48fc  x9  185cb8064dde48fc  x10 0000000000000000  x11 00000070546a51d0
2019-12-05 18:06:30.716 3830-3830/? A/DEBUG:     x12 000000704d0bb580  x13 000000704d0bb5a0  x14 00000000ffffffff  x15 0000000000000000
2019-12-05 18:06:30.716 3830-3830/? A/DEBUG:     x16 00000070f06f3bd0  x17 00000070f068898c  x18 00000000ffffffff  x19 000000706424f000
2019-12-05 18:06:30.716 3830-3830/? A/DEBUG:     x20 00000070516c43b0  x21 00000070516c43f0  x22 00000070516c44e0  x23 00000070642a9fe0
2019-12-05 18:06:30.716 3830-3830/? A/DEBUG:     x24 00000070516c4430  x25 00000070516c7588  x26 0000007064253b88  x27 00000070516c7588
2019-12-05 18:06:30.716 3830-3830/? A/DEBUG:     x28 000000704d1803c0  x29 00000070516c4390
2019-12-05 18:06:30.716 3830-3830/? A/DEBUG:     sp  00000070516c42c0  lr  0000007039a729f0  pc  0000007039a729f4
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG: backtrace:
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #00 pc 00000000000c49f4  /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/lib/arm64/libtensorflowlite_gpu_jni.so
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #01 pc 000000000001ba8c  /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/lib/arm64/libtensorflowlite_gpu_jni.so
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #02 pc 000000000017e52c  /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/lib/arm64/libtensorflowlite_jni.so
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #03 pc 000000000017e0a4  /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/lib/arm64/libtensorflowlite_jni.so
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #04 pc 000000000017de68  /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/lib/arm64/libtensorflowlite_jni.so
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #05 pc 000000000001b5d4  /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/lib/arm64/libtensorflowlite_gpu_jni.so
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #06 pc 000000000017fb08  /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/lib/arm64/libtensorflowlite_jni.so
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #07 pc 0000000000182fa0  /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/lib/arm64/libtensorflowlite_jni.so
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #08 pc 000000000000f214  /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_applyDelegate+40)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #09 pc 00000000005545e0  /system/lib64/libart.so (art_quick_generic_jni_trampoline+144)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #10 pc 000000000054b84c  /system/lib64/libart.so (art_quick_invoke_static_stub+604)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #11 pc 00000000000d00b8  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+232)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #12 pc 000000000027ec54  /system/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+344)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #13 pc 0000000000279da4  /system/lib64/libart.so (bool art::interpreter::DoCall<true, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+752)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #14 pc 000000000051d854  /system/lib64/libart.so (MterpInvokeStaticRange+148)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #15 pc 000000000053e014  /system/lib64/libart.so (ExecuteMterpImpl+15380)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #16 pc 000000000046472e  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/base.apk_2979_2979 (deleted) (org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates+122)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #17 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #18 pc 0000000000258374  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #19 pc 0000000000278c78  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #20 pc 000000000051be90  /system/lib64/libart.so (MterpInvokeDirect+296)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #21 pc 000000000053dc94  /system/lib64/libart.so (ExecuteMterpImpl+14484)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #22 pc 00000000004649a4  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/base.apk_2979_2979 (deleted) (org.tensorflow.lite.NativeInterpreterWrapper.init+140)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #23 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #24 pc 0000000000258374  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #25 pc 0000000000279d88  /system/lib64/libart.so (bool art::interpreter::DoCall<true, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+724)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #26 pc 000000000051d6b4  /system/lib64/libart.so (MterpInvokeDirectRange+244)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #27 pc 000000000053df94  /system/lib64/libart.so (ExecuteMterpImpl+15252)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #28 pc 000000000046468c  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/base.apk_2979_2979 (deleted) (org.tensorflow.lite.NativeInterpreterWrapper.<init>+128)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #29 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #30 pc 0000000000258374  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #31 pc 0000000000278c78  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #32 pc 000000000051be90  /system/lib64/libart.so (MterpInvokeDirect+296)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #33 pc 000000000053dc94  /system/lib64/libart.so (ExecuteMterpImpl+14484)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #34 pc 0000000000464002  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/base.apk_2979_2979 (deleted) (org.tensorflow.lite.Interpreter.<init>+10)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #35 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #36 pc 0000000000258374  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #37 pc 0000000000278c78  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #38 pc 000000000051be90  /system/lib64/libart.so (MterpInvokeDirect+296)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #39 pc 000000000053dc94  /system/lib64/libart.so (ExecuteMterpImpl+14484)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #40 pc 0000000000044cf2  /dev/ashmem/dalvik-classes2.dex extracted in memory from /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/base.apk!classes2.dex_2979_2979 (deleted) (com.test.app.tflite.Classifier.runModel+302)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #41 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #42 pc 0000000000258374  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #43 pc 0000000000278c78  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #44 pc 000000000051ab60  /system/lib64/libart.so (MterpInvokeVirtual+584)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #45 pc 000000000053db94  /system/lib64/libart.so (ExecuteMterpImpl+14228)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #46 pc 000000000003fe80  /dev/ashmem/dalvik-classes2.dex extracted in memory from /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/base.apk!classes2.dex_2979_2979 (deleted) (com.test.app.home.HomeFragment$run$1.run+128)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #47 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #48 pc 0000000000258374  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #49 pc 0000000000278c78  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #50 pc 000000000051bacc  /system/lib64/libart.so (MterpInvokeInterface+1392)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #51 pc 000000000053dd94  /system/lib64/libart.so (ExecuteMterpImpl+14740)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #52 pc 0000000000dbd78c  /system/framework/boot-framework.vdex (android.os.Handler.handleCallback+4)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #53 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #54 pc 0000000000258374  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #55 pc 0000000000278c78  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #56 pc 000000000051c054  /system/lib64/libart.so (MterpInvokeStatic+204)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #57 pc 000000000053dd14  /system/lib64/libart.so (ExecuteMterpImpl+14612)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #58 pc 0000000000c658a8  /system/framework/boot-framework.vdex (android.os.Handler.dispatchMessage+8)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #59 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #60 pc 0000000000258374  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #61 pc 0000000000278c78  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #62 pc 000000000051ab60  /system/lib64/libart.so (MterpInvokeVirtual+584)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #63 pc 000000000053db94  /system/lib64/libart.so (ExecuteMterpImpl+14228)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #64 pc 0000000000c6e4ee  /system/framework/boot-framework.vdex (android.os.Looper.loop+406)
2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #65 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #66 pc 0000000000258374  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #67 pc 0000000000278c78  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #68 pc 000000000051c054  /system/lib64/libart.so (MterpInvokeStatic+204)
2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #69 pc 000000000053dd14  /system/lib64/libart.so (ExecuteMterpImpl+14612)
2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #70 pc 0000000000c6540c  /system/framework/boot-framework.vdex (android.os.HandlerThread.run+56)
2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #71 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #72 pc 000000000050b850  /system/lib64/libart.so (artQuickToInterpreterBridge+1032)
2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #73 pc 00000000005546fc  /system/lib64/libart.so (art_quick_to_interpreter_bridge+92)
2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #74 pc 000000000054b588  /system/lib64/libart.so (art_quick_invoke_stub+584)
2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #75 pc 00000000000d0098  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+200)
2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #76 pc 0000000000454970  /system/lib64/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*)+104)
2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #77 pc 0000000000455a3c  /system/lib64/libart.so (art::InvokeVirtualOrInterfaceWithJValues(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jmethodID*, jvalue*)+424)
2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #78 pc 00000000004807f0  /system/lib64/libart.so (art::Thread::CreateCallback(void*)+1260)
2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #79 pc 0000000000084148  /system/lib64/libc.so (__pthread_start(void*)+64)
2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #80 pc 0000000000023b28  /system/lib64/libc.so (__start_thread+68)
```
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
34872,ImportError: libcuda.so.1: cannot open shared object file: No such file or directory,"hi im beginner in tensorflow! i want  run my tensorflow code in ipm but i got this error :

Traceback (most recent call last):
  File ""/share/Application/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/share/Application/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/share/Application/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/share/Application/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/share/Application/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcuda.so.1: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main.py"", line 5, in <module>
    import VDSR.vdsr as vdsr
  File ""../VDSR/vdsr.py"", line 1, in <module>
    import tensorflow as tf
  File ""/share/Application/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/share/Application/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/share/Application/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/share/Application/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/share/Application/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/share/Application/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/share/Application/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/share/Application/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcuda.so.1: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime."
34871,tf.string tensors are not converted to numpy string arrays,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Bug appears on Linux Ubuntu 18.04, but also in Colab Notebooks
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6

**Describe the current behavior**

[Colab Notebook to reproduce](https://colab.research.google.com/drive/13zEPEBbfOR0U5TVc4dABxtT6NUB6TftL)

When casting a tensor of type `tf.string` to a numpy array in eager mode, the ndarray does not have a correct string dtype, but is converted to an array of arbitrary objects.

```
tf_str = tf.constant(['Hello World', 'B'])
>> <tf.Tensor: id=1, shape=(2,), dtype=string, numpy=array([b'Hello World', b'B'], dtype=object)>
tf_str.numpy()
>> array([b'Hello World', b'B'], dtype=object)
tf_str.numpy().astype(str)  # expicit casting to string
>> array(['Hello World', 'B'], dtype='<U11')
```

**Describe the expected behavior**
It would be expected that a `tf.string` datatype is converted to a [numpy string datatype](https://docs.scipy.org/doc/numpy/reference/arrays.dtypes.html#string-dtype-note).

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

[Colab Notebook to reproduce](https://colab.research.google.com/drive/13zEPEBbfOR0U5TVc4dABxtT6NUB6TftL)
"
34868,Some of the operators in the model are not supported by the standard TensorFlow Lite runtime,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04
- TensorFlow installed from (source or binary):
pip install tensorflow
- TensorFlow version (or github SHA if from source):
2.1.0rc0


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, FULLY_CONNECTED, LOGISTIC, MUL, RESHAPE, STRIDED_SLICE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
34867,saved_model.go,"    where is the package ""github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core""?
   I can't build the program."
34866,Keras Custom Loss/Model Compilation,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10 1909
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
Binary, Pip
- TensorFlow version (use command below):
1.14
- Python version:
3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
10/7.6.4
- GPU model and memory:
RTX 2080 with 8GB VRAM, 16GB DRAM DDR4

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

_for 1_: unknown 1.14.0

**Describe the current behavior**
Model compilation fails with the following error.

    Traceback (most recent call last):
      File "".\vgg_loss.py"", line 103, in <module>
        main()
      File "".\vgg_loss.py"", line 97, in main
        model.compile(optimizer='adam', loss=some_loss, metrics=['accuracy'])
      File ""C:\Users\Intel\Anaconda3\lib\site-packages\tensorflow\python\training\tracking\base.py"", 
    line 457, in _method_wrapper
        result = method(self, *args, **kwargs)
      File ""C:\Users\Intel\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", 
    line 337, in compile
        self._compile_weights_loss_and_weighted_metrics()
      File ""C:\Users\Intel\Anaconda3\lib\site-packages\tensorflow\python\training\tracking\base.py"", 
    line 457, in _method_wrapper
        result = method(self, *args, **kwargs)
      File ""C:\Users\Intel\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", 
    line 1710, in _compile_weights_loss_and_weighted_metrics
        self.total_loss = self._prepare_total_loss(masks)
    otal_loss
        per_sample_losses = loss_fn.call(y_true, y_pred)
      File ""C:\Users\Intel\Anaconda3\lib\site-packages\tensorflow\python\keras\losses.py"", line 215, 
    in call
        return self.fn(y_true, y_pred, **self._fn_kwargs)
      File "".\vgg_loss.py"", line 86, in some_loss
        return mse(vgg_model.predict(y_pred, steps=1), vgg_model.predict(y_true, steps=1))
      File ""C:\Users\Intel\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", 
    line 1078, in predict       callbacks=callbacks)
        batch_outs = f(actual_inputs)
        run_metadata=self.run_metadata)
        run_metadata_ptr)
    tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
       (0) Invalid argument: You must feed a value for placeholder tensor 'input_node' with dtype 
         float 
    and shape [1,512,512,3]
              [[{{node input_node}}]]
              [[block4_conv3/Relu/_217]]
       (1) Invalid argument: You must feed a value for placeholder tensor 'input_node' with dtype float 
     and shape [1,512,512,3]
              [[{{node input_node}}]]
     0 successful operations.
     0 derived errors ignored.


**Describe the expected behavior**
The code should compile properly.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
I am attaching the [link](https://github.com/jtdutta1/Fastest/blob/master/vgg_loss.py) to the code.
Also download the vgg model whose link is provided in the code. I am also referencing [here](https://drive.google.com/open?id=1OsKx6CPacs7V7d-1cNCI2bFIty2BVQEz) again.

Be sure to run it as 
  
    python vgg_loss.py -p <path_to_vgg_model>

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
34865,[TF2.0]how to save model to get .pb in this example?,"
https://www.tensorflow.org/tutorials/text/nmt_with_attention
In this [example](https://www.tensorflow.org/tutorials/text/nmt_with_attention) ,how to save model to get .pb ? 

I tried this way, but have some bugs.
what should I do? 
```
tf.saved_model.save(decoder, ""./decoder_out/1"")
tf.saved_model.save(encoder, ""./encoder_out/1"")


encoder = tf.saved_model.load('./encoder_out/1')
decoder = tf.saved_model.load('./decoder_out/1')

sentence = u'¿todavia estan en casa?'
attention_plot = np.zeros((max_length_targ, max_length_inp))
sentence = preprocess_sentence(sentence)
    
inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]
inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=max_length_inp,padding='post')
    
    
inputs = tf.convert_to_tensor(inputs)
result = ''
hidden = [tf.zeros((1,units))]
enc_out, enc_hidden = encoder(inputs, hidden)


---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-59-ba6b7bf7814b> in <module>
     10 result = ''
     11 hidden = [tf.zeros((1,units))]
---> 12 enc_out, enc_hidden = encoder(inputs, hidden)

TypeError: '_UserObject' object is not callable

```"
34863,use 'grpc + verbs' appears (core dumped),"When I change the protocol to 'grpc + verbs' for RDMA training, the program will have a core dump:

INFO tensorflow 140500786698048 Create CheckpointSaverHook.
INFO tensorflow 140500786698048 Start Tensorflow server.
tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job chief
...
tensorflow / contrib / verbs / rdma.cc: 159] Check failed: num_devs_with_active_port> 0 There is no active port in the system
(core dumped)

My underlying environment is ROCE，25worker+16ps
Is it my environmental problem? The program works when I use protocol to 'grpc'.
I want to know what is causing this error and how to fix it？"
34862, tf.feature_column.shared_embeddings supports eager mode ,"**System information**
- TensorFlow version (you are using): TF 2.0.0
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
Using tensorflow-2.0.0 with eager mode,  tf.feature_column.shared_embeddings  can not support eager mode. 

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
tf.feature_column.shared_embeddings is a common feature column API，we can use it for multiple category_column with embedding parameters.

**Any Other info.**
"
34861,tensorflow gredient Tape,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
34860,Can not load tflite model converted from pytorch,"**System information**
- Win10:
- TensorFlow version=1.15:


**Code to generate tflite model**
Generate the tflite model with cmd
```
python tflite_convert.py 
--graph_def_file=./epoch10_model_toco.pb 
--output_file=./lite.tflite 
--inference_type=QUANTIZED_UINT8 
--mean_values=0 --std_dev_values=1 
--input_arrays=0 --output_arrays=Reshape_136 --input_shape=1,3,120,18,2 
--default_ranges_min=0 --default_ranges_max=6 --post_training_quantize
```

**Code to load tflite model**

```
import tensorflow as tf

# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""converted_model.tflite"")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
print(input_details)
output_details = interpreter.get_output_details()
print(output_details)

# Test model on random input data.
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)
```

**Error Message**

```
Traceback (most recent call last):
  File ""demo.py"", line 97, in <module>
    main()
  File ""demo.py"", line 59, in main
    interpreter = tf.lite.Interpreter(model_path=PATH_TO_TFLITE)
  File ""C:\Users\Administrator\.conda\envs\stgcn\lib\site-packages\tensorflow_core\lite\python\interpreter.py"", line 206, in __init__
    model_path))
ValueError: Input array not provided for operation 'reshape'.
```

**If i convert .pb to .tflite without uint8 quantized**

```
python tflite_convert.py 
--graph_def_file=./epoch10_model_toco.pb 
--output_file=./lite.tflite 
--input_arrays=0 --output_arrays=Reshape_136 --input_shape=1,3,120,18,2 
```

Then load the tflite without quantized as the same.
Still i got another error message.

```
Traceback (most recent call last):
  File ""demo.py"", line 97, in <module>
    main()
  File ""demo.py"", line 60, in main
    interpreter.allocate_tensors()
  File ""C:\Users\Administrator\.conda\envs\stgcn\lib\site-packages\tensorflow_core\lite\python\interpreter.py"", line 244, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File ""C:\Users\Administrator\.conda\envs\stgcn\lib\site-packages\tensorflow_core\lite\python\interpreter_wrapper\tensorflow_wrap_interpreter_wrapper.py"", line 106, in AllocateTensors
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: tensorflow/lite/kernels/transpose.cc Transpose op only supports 1D-4D input arrays.Node number 0 (TRANSPOSE) failed to prepare.
```

How can i convert the .pb to the .tflite, which could be loaded.
Is there something wrong in the .tflite layer?How to correct that?"
34859,tf.train.AdamOptimizer doesn't work with custom TPU training loop,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.15
- Python version: 3.x
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
Run this Colab notebook with a TPU accelerator: https://colab.research.google.com/drive/1bsgSNK3aK9sETlplIPVpAa-yc4q1S3sA

When running the above notebook with `tf.train.AdamOptimizer`, we get:
```
ValueError: in converted code:

    <ipython-input-22-807c7cf92c68>:21 simple_model_fn  *
        train_op = tf.train.AdamOptimizer().minimize(y)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/optimizer.py:413 minimize
        name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/optimizer.py:569 apply_gradients
        self._distributed_apply, args=(grads_and_vars, global_step, name))
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py:1940 merge_call
        return self._merge_call(merge_fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py:1947 _merge_call
        return merge_fn(self._strategy, *args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/optimizer.py:717 _distributed_apply
        non_slot_devices, finish, args=(self, update_ops), group=False)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py:1577 update_non_slot
        return self._update_non_slot(colocate_with, fn, args, kwargs, group)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/tpu_strategy.py:580 _update_non_slot
        result = fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/optimizer.py:713 finish
        return self._finish(update_ops, ""update"")
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/adam.py:228 _finish
        beta1_power, beta2_power = self._get_beta_accumulators()
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/adam.py:115 _get_beta_accumulators
        return (self._get_non_slot_variable(""beta1_power"", graph=graph),
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/optimizer.py:868 _get_non_slot_variable
        if hasattr(non_slot, ""_distributed_container""):
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py:827 __getattr__
        return super(TPUVariableMixin, self).__getattr__(name)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py:389 __getattr__
        return getattr(self.get(), name)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py:834 get
        return super(TPUVariableMixin, self).get(device=device)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py:324 get
        return self._device_map.select_for_device(self._values, device)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py:219 select_for_device
        (device, self._devices, device_util.current()))

    ValueError: Device /job:worker/replica:0/task:0/device:CPU:0 not found in ('/job:worker/replica:0/task:0/device:TPU:0', '/job:worker/replica:0/task:0/device:TPU:1', '/job:worker/replica:0/task:0/device:TPU:2', '/job:worker/replica:0/task:0/device:TPU:3', '/job:worker/replica:0/task:0/device:TPU:4', '/job:worker/replica:0/task:0/device:TPU:5', '/job:worker/replica:0/task:0/device:TPU:6', '/job:worker/replica:0/task:0/device:TPU:7') (current device /job:worker/replica:0/task:0/device:CPU:0)
```

This code runs just fine with `tf.train.MomentumOptimizer` and `tf.keras.optimizers.Adam` (run same code with the `optimizer_type` form variable set to `KerasAdam` or `Momentum`).

**Describe the expected behavior**
Code should run without error using `tf.train.AdamOptimizer`, just like it does for the other optimizers.

**Code to reproduce the issue**
https://colab.research.google.com/drive/1bsgSNK3aK9sETlplIPVpAa-yc4q1S3sA"
34858,SparseCategoricalCrossEntropy example contains a mistake in terms of input,"## URL(s) with the issue:

https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/losses.py#L493

## Description of issue (what needs changing):

### Clear description

Entries [1,0] and [1,2] of a tensor linked to should be 10 times smaller in order for the second entry to sum up to 1.

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? 

No, the change seems too small for an expensive TF CI to run. 
"
34857,"How to add ""TFLite_Detection_Postprocess"" op in Keras model when using tf2.x","In tf1.x, I used `export_tflite_ssd_graph.py` function in the `object_detection` module to convert model adding`TFLite_Detection_Postprocess` op, but now, I write model use Keras in tf2.x, and there are no documents to tell how to do the conversion."
34856,Push up Switch ops in grappler,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.14
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
If we have a chain of ops  `-> A -> B -> Switch ->`, both A and B will need to be evaluated regardless of the outcome of the Switch op. In this case, we can push up the Switch op so it becomes ` -> Switch -> A -> B -> `. That way, neither A or B needs to be evaluated if the wrong branch of the Switch is chosen. This would simplify the use of `tf.cond` by not requiring all conditional ops be created in the supplied true_fn or false_fn.

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
Users of tf.cond.

**Any Other info.**
"
34854,What logging level do deprecation alerts belong to in python-tensorflow?,"Hello everyone, 

I was looking around the documentation and can't seem to find the specific logging level that deprecation warnings live on (e.g. `The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.`).

From the documentation here https://www.tensorflow.org/api_docs/python/tf/compat/v1/logging - I see that there are 5 distinct levels. `DEBUG = 10`, `ERROR = 40`, `FATAL = 50`, `INFO = 20`, `WARN = 30`.

But does anyone know the specific level that deprecation alerts belong to? And if so, is the level static across different versions of tensorflow? Or does the level fluctuate between versions?

All the best, Alan.

"
34851,Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA,"I have

```
$ module list
Currently Loaded Modulefiles:
  1) python3/3.6.5       2) cuda/10.0           3) tensorflow/1.13.1   4) miniconda/4.7.5
```


and I get this error:

```
$ python mouse.py 
DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)
/projectnb/ivcgroup/jalal/mouse10k/openfield-Pranav-2019-12-04/training-datasets/iteration-0/UnaugmentedDataSet_openfieldDec4  already exists!
/projectnb/ivcgroup/jalal/mouse10k/openfield-Pranav-2019-12-04/labeled-data/m3v1mp4/CollectedData_Pranav.h5  not found (perhaps not annotated)
Annotation data was not found by splitting video paths (from config['video_sets']). An alternative route is taken...
The following folders were found: ['m4s1']
/projectnb/ivcgroup/jalal/mouse10k/openfield-Pranav-2019-12-04/dlc-models/iteration-0/openfieldDec4-trainset95shuffle1  already exists!
/projectnb/ivcgroup/jalal/mouse10k/openfield-Pranav-2019-12-04/dlc-models/iteration-0/openfieldDec4-trainset95shuffle1//train  already exists!
/projectnb/ivcgroup/jalal/mouse10k/openfield-Pranav-2019-12-04/dlc-models/iteration-0/openfieldDec4-trainset95shuffle1//test  already exists!
The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!
Config:
{'all_joints': [[0], [1], [2], [3]],
 'all_joints_names': ['snout', 'leftear', 'rightear', 'tailbase'],
 'batch_size': 1,
 'bottomheight': 400,
 'crop': True,
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_openfieldDec4/openfield_Pranav95shuffle1.mat',
 'dataset_type': 'imgaug',
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': '/projectnb/dnn-motion/jalal/.conda/envs/deeplabcut/lib/python3.6/site-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'leftwidth': 400,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_openfieldDec4/Documentation_data-openfield_95shuffle1.pickle',
 'min_input_size': 64,
 'minsize': 100,
 'mirror': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 4,
 'optimizer': 'sgd',
 'pos_dist_thresh': 17,
 'project_path': '/projectnb/ivcgroup/jalal/mouse10k/openfield-Pranav-2019-12-04',
 'regularize': False,
 'rightwidth': 400,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': '/projectnb/ivcgroup/jalal/mouse10k/openfield-Pranav-2019-12-04/dlc-models/iteration-0/openfieldDec4-trainset95shuffle1/train/snapshot',
 'stride': 8.0,
 'topheight': 400,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
Starting with imgaug pose-dataset loader.
Batch Size is 1
Initializing ResNet
WARNING:tensorflow:From /share/pkg.7/tensorflow/1.13.1/install/lib/site-packages/../python3.6-gpu/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /share/pkg.7/tensorflow/1.13.1/install/lib/site-packages/../python3.6-gpu/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Loading ImageNet-pretrained resnet_50
2019-12-04 23:41:20.555301: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA
Aborted
```


How can I fix it?

```
$ uname -a
Linux scc-c08 3.10.0-957.27.2.el7.x86_64 #1 SMP Mon Jul 29 17:46:05 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux


$ lsb_release -a
LSB Version:	:core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch
Distributor ID:	CentOS
Description:	CentOS Linux release 7.5.1804 (Core) 
Release:	7.5.1804
Codename:	Core
```
"
34850, Error: slice index 0 of dimension 0 out of bounds,"@ravikyram 
Hi, I am working on tensorflow 2.0  and getting an error at line 3 while running the model  

1) lstm_cell =tf.keras.layers.LSTM(units=hidden_unit)
2) lstm_cell = tf.nn.RNNCellDropoutWrapper(lstm_cell, output_keep_prob=self.dropout_keep_prob)
3) self._initial_state = lstm_cell.get_initial_state(128, tf.float32)

Got ERROR at line 3
ValueError: slice index 0 of dimension 0 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.
may I know why I am getting this error?
"
34849,TF 2.0 - WARNING - dense_features is casting an input tensor from dtype float64 to the layer's dtype of float32,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution Ubuntu Linux 18.04 x64

- TensorFlow installed from (source or binary): Installed from Anaconda

- TensorFlow version (use command below):
`python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
unknown 2.0.0
I am using TF 2.0.0.
- Python version: python 3.7.4
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: Cuda release 10.1, V10.1.168; cudnn 7.6.0
- GPU model and memory: Nvidia GTX 1080 11GB.

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

This is both a code issue and a documentation problem--but mostly a code problem. 
I was looking at the tutorial(https://www.tensorflow.org/tutorials/structured_data/feature_columns#numeric_columns) and saw that the tutorial itself is generating warnings. So that suggests some problems in the code as well as the tutorial. 

**Describe the expected behavior**

I would expect the tutorial to generate no warnings--and hence demonstrate proper code functionality. As it is, it is not clear whether the warnings are generated from a bug in the code, or from spurious warnings, etc. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
from __future__ import absolute_import, division, print_function, unicode_literals

import numpy as np
import pandas as pd

import tensorflow as tf

from tensorflow import feature_column
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

URL = 'https://storage.googleapis.com/applied-dl/heart.csv'
dataframe = pd.read_csv(URL)
train, test = train_test_split(dataframe, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
print(len(train), 'train examples')
print(len(val), 'validation examples')
print(len(test), 'test examples')

def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()
  labels = dataframe.pop('target')
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)
  return ds

batch_size = 5 # A small batch sized is used for demonstration purposes
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

# We will use this batch to demonstrate several types of feature columns
example_batch = next(iter(train_ds))[0]

# A utility method to create a feature column
# and to transform a batch of data
def demo(feature_column):
  feature_layer = layers.DenseFeatures(feature_column)
  print(feature_layer(example_batch).numpy())

age = feature_column.numeric_column(""age"")
demo(age) # <-- SHOULD TRIGGER OR DISPLAY THE WARNING
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
No other materials provided."
34848,TF2.0 - Cannot convert EagerTensor to datatype and casting,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): Installed from Anaconda
- TensorFlow version (use command below):
`python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
unknown 2.0.0
I am using TF 2.0.0.
- Python version: python 3.7.4
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: Cuda release 10.1, V10.1.168; cudnn 7.6.0

- GPU model and memory: Nvidia GTX 1080 11GB.

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
The code provided generates an error:
```
TypeError: Cannot convert 1.0 to EagerTensor of dtype int64
```
The cause of the error is not clear but the setup of the code basically follows the Tensorflow tutorial.


**Describe the expected behavior**
The `model.fit()` function should run and complete training. Instead this error gets generated and the training terminates early. I only get like step or such. 

**Code to reproduce the issue**
Note that the error gets generated towards the bottom of the `main()` function, where I invoke the `fit()` function. 

```
#!/usr/bin/python

from __future__ import absolute_import, division, print_function, unicode_literals

# import comet_ml in the top of your file
from comet_ml import Experiment
from comet_ml import Optimizer
import numpy as np
import pandas as pd
import tensorflow as tf
import datetime
from tensorflow import feature_column
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

# Add the following code anywhere in your machine learning file

def main():

    # Import and setup data

    URL = 'https://storage.googleapis.com/applied-dl/heart.csv'
    dataframe = pd.read_csv(URL)

    # Setup training test split

    train, test = train_test_split(dataframe, test_size=0.2)
    train, val = train_test_split(train, test_size=0.2)
    print(f'The training set is of length: {len(train)}')
    batch_size = 32

    train_ds = df_to_dataset(train, batch_size=batch_size)
    val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
    test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

    size_train_ds = get_dataset_length(df_to_dataset(train, batch_size=1, repeat=1))
    size_test_ds = get_dataset_length(df_to_dataset(test, batch_size=1, repeat=1))
    size_validation_ds = get_dataset_length(df_to_dataset(val, batch_size=1, repeat=1))

    print(f' training data length is: {size_train_ds}')
    print(f' test data length is: {size_test_ds}')
    print(f' validation data length is: {size_validation_ds}')
    # Configure experiment and hyperparameters to test with.

    hparams = {""nodes"":128, ""thal_embedding_cols"": 3}

    loss = fit_model(train_ds,  # <-- ERROR GETS GENERATED AROUND HERE.
                     test_ds,
                     val_ds,
                     epochs=100,
                     batch_size=batch_size,
                     number_training_examples=size_train_ds,
                     number_testing_examples=size_test_ds,
                     number_validation_examples=size_validation_ds,
                     hyper_parameters=hparams)


    return 0

def df_to_dataset(dataframe, shuffle=True, repeat=None, batch_size=32):
    dataframe = dataframe.copy()
    labels = dataframe.pop('target')
    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
    if shuffle:
        ds = ds.shuffle(buffer_size=len(dataframe))
    ds = ds.batch(batch_size).repeat(repeat)
    return ds

def get_dataset_length(ds):
    """"""
    This function will get the number of examples in a tf.data.Dataset. Make
    sure that the batch size for the dataset is set to 1, otherwise this
    function will count the number of batches in the data. 
    """"""
    return ds.reduce(np.int64(0), lambda x, _: x + 1)

def define_features(hyper_parameters):

    feature_columns = []
    # numeric cols
    for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:
        feature_columns.append(feature_column.numeric_column(header))
        

    # bucketized cols
    age = feature_column.numeric_column(""age"")
    age_buckets = feature_column.bucketized_column(age,
                                                   boundaries=[18, 25, 30, 35,
                                                               40, 45, 50, 55,
                                                               60, 65])
    #feature_columns.append(age_buckets)

    # indicator cols
    thal = feature_column.categorical_column_with_vocabulary_list(
        'thal', ['fixed', 'normal', 'reversible'])
    thal_one_hot = feature_column.indicator_column(thal)
    #feature_columns.append(thal_one_hot)

    # embedding cols
    thal_embedding = feature_column.embedding_column(thal,
                                                     dimension= \
                                                     hyper_parameters['thal_embedding_cols'])
    #feature_columns.append(thal_embedding)

    # crossed cols
    crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)
    crossed_feature = feature_column.indicator_column(crossed_feature)
    #feature_columns.append(crossed_feature)
    return feature_columns

def build_model(features, hyper_parameters):
    '''
    documentation string
    '''
    feature_layer = tf.keras.layers.DenseFeatures(features)

    model = tf.keras.Sequential([
        feature_layer,
        layers.Dense(hyper_parameters['nodes'], activation='relu'),
        layers.Dense(hyper_parameters['nodes'], activation='relu'),
        layers.Dense(hyper_parameters['nodes'], activation='relu'),
        layers.Dense(hyper_parameters['nodes'], activation='relu'),
        layers.Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer='adam',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

    return model


def fit_model(
              train_dataset,
              test_dataset,
              validation_dataset,
              epochs=None,
              batch_size=None,
              number_training_examples=None,
              number_testing_examples=None,
              number_validation_examples=None,
              hyper_parameters=None):


    model = build_model(define_features(hyper_parameters), hyper_parameters)

    log_dir = ""logs/fit/"" + datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

    model.fit(train_dataset,
              validation_data = test_dataset,
              epochs = epochs,
              steps_per_epoch = number_training_examples//batch_size,
              validation_steps= number_testing_examples//batch_size,
              callbacks = [tensorboard_callback])

    # score = model.evaluate(x_test, y_test, verbose=0)[1]
    loss, accuracy = model.evaluate(validation_dataset,
                                    steps=number_validation_examples//batch_size,
                                    verbose=0)
    print(loss)
    return loss

if __name__ == ""__main__"":
    main()

```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

NA"
34846,Feed Input tensor Data from CUDA memory,"Hello 

This feature request is for the C_API

**System information**
- TensorFlow version (you are using): 1.13.1
- Are you willing to contribute it (Yes/No): No
- OS : Windows 10
- Compiler : VS2015

**Describe the feature and the current behavior/state.**
I want the C API to provide a way to feed an input Tensor directly from preallocated CUDA memory without copy data through PCI-Express 

Current Behaviour  : Need to copy the data to CPU side, then the data is sent back to the GPU for inference.

**Who will benefit with this feature?**
 Anyone who want a performance improvement using Tensorflow C_API for inference
"
34845,"Mask-RCNN fails in TOCO Conversion: ""Exception: Merge of two inputs that differ on more than one predicate""","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.1.0-dev20191203

On the TF-Lite mailing list it was [announced](https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!msg/tflite/C7Ag0sUrLYg/tHYfDHmNAAAJ) that the new MLIR-based TOCO converter:
```Enables conversion of new classes of models, including Mask R-CNN, ...```
However I was unable to get a version of Mask-RCNN converted to TF-Lite. It would be really helpful if additional notebooks like the [LSTM example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/keras_lstm.ipynb) could be added. I attempted to create one below for others.

**Command used to run the converter or code if you’re using the Python API**
See the notebook modeled after the LSTM example: https://github.com/parvizp/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/maskrcnn.ipynb

**The output from the converter invocation**
```
ConverterError: See console for info.
2019-12-04 18:11:17.034818: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:106] Ignored output_format.
2019-12-04 18:11:17.034844: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:112] Ignored drop_control_dependency.
Traceback (most recent call last):
  File ""/n_mounts/scr_ncore/parvizp/Git/tensorflow_public/tensorflow/lite/examples/experimental_new_converter/env/bin/toco_from_protos"", line 8, in <module>
    sys.exit(main())
  File ""/n_mounts/scr_ncore/parvizp/Git/tensorflow_public/tensorflow/lite/examples/experimental_new_converter/env/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 93, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/n_mounts/scr_ncore/parvizp/Git/tensorflow_public/tensorflow/lite/examples/experimental_new_converter/env/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/n_mounts/scr_ncore/parvizp/Git/tensorflow_public/tensorflow/lite/examples/experimental_new_converter/env/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/n_mounts/scr_ncore/parvizp/Git/tensorflow_public/tensorflow/lite/examples/experimental_new_converter/env/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/n_mounts/scr_ncore/parvizp/Git/tensorflow_public/tensorflow/lite/examples/experimental_new_converter/env/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 56, in execute
    enable_mlir_converter)
Exception: Merge of two inputs that differ on more than one predicate {s(BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater:0,else), s(BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/pred_id:0,then)} and {s(BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater:0,else), s(BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/pred_id:0,else), s(BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/Greater:0,else)}
	for node {{node BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/Merge}}
```

**Also, please include a link to the saved model or GraphDef**

http://download.tensorflow.org/models/object_detection/mask_rcnn_resnet50_atrous_coco_2018_01_28.tar.gz

"
34844,Tensorboard example not reproducing GPU profile,"When running this notebook on Colab:

https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras

I am not getting the same profile output for profiling: on my side no GPU device is shown.

This is what is expected:
[![enter image description here][1]][1]

This is what I am getting:
[![enter image description here][2]][2]


  [1]: https://i.stack.imgur.com/o5w0G.png
  [2]: https://i.stack.imgur.com/mDlXb.png"
34841,Segmentation fault and corrupted output during inference with large 3D U-Net,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution: Linux CLE 7.0UP00
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.0.0
- Python version: 3.7.4

**Describe the current behavior**
I have a 3D U-Net model that I am trying to use for very large inputs (arrays as large as 1024^3). Obviously, I cannot fit this in GPU memory so I am running the network on CPU compute nodes which have a very large amount of RAM available (~350 GB). I have a ""bare-bones"" script which performs only the forward pass using the 3D convolution methods from `tf.nn`, but I am only able to process arrays of size  <= 640^3 without issues. If I go for the next feasible size (which is 704^3 since I can only increase spatial dimensions of the input by multiples of 64), the following behavior occurs:

* If using tensorflow with the Intel MKL library, which is optimized for running on CPUs on my system, the network runs without crashing but the generated output is corrupted (see attached images). The corruption seems to only occur along the outermost spatial axis of the output array.

* If using tensorflow-gpu or regular tensorflow (without Intel MKL), the program crashes with a segmentation fault

Though the input sizes are very large, I would expect tensorflow to be able to complete the computations without issue since I have enough RAM available. The difference in behavior with or without Intel MKL is also perplexing.

**Code to reproduce the issue**
Below is a script that performs the U-Net computation on random data and reproduces the segfault/bug:
```
import tensorflow as tf
import numpy as np

size=704
dtype = np.float32
sample = np.random.normal(size=(1,size,size,size,4)).astype(dtype)
print(sample.shape)

convs = [(4, 4, 4, 4, 64),
         (4, 4, 4, 64, 128),
         (4, 4, 4, 128, 256),
         (4, 4, 4, 256, 512),
         (4, 4, 4, 512, 512),
         (4, 4, 4, 512, 512)]
Tconvs = [(4, 4, 4, 512, 512),
          (4, 4, 4, 512, 1024),
          (4, 4, 4, 256, 1024),
          (4, 4, 4, 128, 512),
          (4, 4, 4, 64, 256),
          (4, 4, 4, 5, 128)]

x = tf.constant(sample)
skips = []
shapes = []

# Downsampling
for kernelshape in convs:
    bias = np.zeros(shape=(kernelshape[-1]))
    kernel = np.random.normal(loc=0.0, scale=0.02, size=kernelshape)
    x = tf.nn.conv3d(x, filters=kernel.astype(dtype), strides=[1,2,2,2,1], padding='SAME')
    x = tf.nn.bias_add(x, bias.astype(dtype))
    x = tf.nn.leaky_relu(x, alpha=0.3)
    skips.append(x)
    shapes.append(x.shape)
    print(x.shape)

# Upsampling with skip connections
skips = list(reversed(skips[:-1]))
for kernelshape, skip in zip(Tconvs, skips):
    bias = np.zeros(shape=(kernelshape[3]))
    kernel = np.random.normal(loc=0.0, scale=0.02, size=kernelshape)
    out_shape = np.array(x.shape.as_list())
    out_shape[1:4] = 2*out_shape[1:4]
    out_shape[-1] = kernel.shape[3]
    x = tf.nn.conv3d_transpose(x, filters=kernel.astype(dtype), strides=2, padding='SAME', output_shape=out_shape)
    x = tf.nn.bias_add(x, bias.astype(dtype))
    x = tf.nn.relu(x)
    x = tf.concat([x, skip], axis=-1)
    print(x.shape)

# Final upsampling conv
kernelshape = Tconvs[-1]
bias = np.zeros(shape=(kernelshape[3]))
kernel = np.random.normal(loc=0.0, scale=0.02, size=kernelshape)
out_shape = np.array(x.shape.as_list())
out_shape[1:4] = 2*out_shape[1:4]
out_shape[-1] = kernel.shape[3]
x = tf.nn.conv3d_transpose(x, filters=kernel.astype(dtype), strides=2, padding='SAME', output_shape=out_shape)
x = tf.nn.bias_add(x, bias.astype(dtype))
x = tf.math.tanh(x)

print(x.shape)
x = x.numpy().astype(np.float32)
```

**Other info / logs**
The attached images show the output of the U-Net (running on random noise) for the problematic size (704^3), via 2D slices along the x-y and x-z planes: 
![image](https://user-images.githubusercontent.com/48932392/70189520-613e2180-16a8-11ea-970f-27bdd58546f1.png)
The images should look like uniform noise, but instead have a sharp change occur along the x-axis after index~520.

On my actual dataset, the corruption looks like: 
![image](https://user-images.githubusercontent.com/48932392/70190128-fc83c680-16a9-11ea-9fb1-984be9673381.png)
"
34840,"Tensorflow installs locally, but does not on Heroku - python django pipenv","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
pipenv lock
Installs fine with ""*"" for tensorflow, but then pushing to remote heroku server fails install 

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

An error occurred while installing tensorflow==2.0.0! Will try again.

Could not find a version that satisfies the requirement tensorflow==2.0.0 (from -r /tmp/pipenv-7q4timpj-requirements/pipenv-4z54yb09-requirement.txt (line 1)) (from versions: 0.12.1, 1.0.0, 1.0.1, 1.1.0rc0, 1.1.0rc1, 1.1.0rc2, 1.1.0, 1.2.0rc0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.3.0rc0, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.4.0rc0, 1.4.0rc1, 1.4.0, 1.4.1, 1.5.0rc0, 1.5.0rc1, 1.5.0, 1.5.1, 1.6.0rc0, 1.6.0rc1, 1.6.0, 1.7.0rc0, 1.7.0rc1, 1.7.0, 1.7.1, 1.8.0rc0, 1.8.0rc1, 1.8.0, 1.9.0rc0, 1.9.0rc1, 1.9.0rc2, 1.9.0, 1.10.0rc0, 1.10.0rc1, 1.10.0, 1.10.1, 1.11.0rc0, 1.11.0rc1, 1.11.0rc2, 1.11.0, 1.12.0rc0, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.12.2, 1.12.3, 1.13.0rc0, 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)
remote:        No matching distribution found for tensorflow==2.0.0 (from -r /tmp/pipenv-7q4timpj-requirements/pipenv-4z54yb09-requirement.txt (line 1))"
34834,layer.output raises AttributeError because inbound nodes lost after call to activation function,"**System information**
- custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
ubuntu 18.04
- TensorFlow installed from (source or binary):
pip install tensorflow-gpu
- TensorFlow version (use command below):

v2.0.0-rc2-26-g64c3d38 2.0.0
and 
v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1

- Python version:
3.6.9

**Describe the current behavior**
calling layer.output on a keras layer that is called on the output of an activation function does not setup the inbound nodes properly and so one cannot call the layer.output method.

**Describe the expected behavior**
layer.output should return the output tensor

**Code to reproduce the issue**
```

import tensorflow as tf



class MyModel(tf.keras.Model):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        self.dense0 = tf.keras.layers.Dense(10, name='dense0', input_shape=(5, 5, 1))
        self.dense1 = tf.keras.layers.Dense(10, name='dense1')
        self.dense2 = tf.keras.layers.Dense(10, name='dense2')

    def call(self, x):
        x = self.dense0(x)
        # if you use this line it works
        x = tf.keras.layers.ReLU()(x)
        x = self.dense1(x)

        print('correct:', self.dense1.inbound_nodes)

        # if you use this line it doesn't work
        relu = tf.keras.activations.get('relu')
        x = relu(x)
        x = self.dense2(x)

        print('incorrect:', self.dense2.inbound_nodes)
        return x


def main():
    my_model = MyModel()

    inp = tf.keras.Input(shape=(5, 5, 1))
    out = my_model(inp)

    my_model.compile(optimizer='adam',
                     loss='sparse_categorical_crossentropy')

    for l in my_model.layers:
        try:
            print(l.output)
        except AttributeError:
            print('EXCEPTION: {}.output raises attribute error'.format(l.name))


if __name__=='__main__':
    main()

```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

UPDATED: to include input_shape which does not solve the problem."
34833,Error building from source,"<em>I'm getting this error after running ""bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package ""</em>

**System information**
- Windows 10
- TensorFlow version: 1.13
- Python version: 3.6
- Installed using  conda:
- Bazel version : 0.21.0
- GCC/Compiler version : visual 2015


WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
c:\tensorflow-build\tensorflow/.bazelrc
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
INFO: Invocation ID: c11f1d92-ce5b-430a-9113-7db1a5079995
INFO: Build options --copt and --define have changed, discarding analysis cache.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:499:1: In rule 'decorator_utils_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:499:1: In rule 'decorator_utils_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:34:1: In rule 'base_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:34:1: In rule 'base_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:54:1: In rule 'tracking_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:54:1: In rule 'tracking_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:81:1: In rule 'data_structures_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:81:1: In rule 'data_structures_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:132:1: In rule 'util_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:132:1: In rule 'util_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:168:1: In rule 'util_with_v1_optimizers_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:168:1: In rule 'util_with_v1_optimizers_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:510:1: In rule 'tf_export_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:510:1: In rule 'tf_export_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:521:1: In rule 'deprecation_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:521:1: In rule 'deprecation_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:532:1: In rule 'dispatch_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:532:1: In rule 'dispatch_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:543:1: In rule 'keyword_args_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:543:1: In rule 'keyword_args_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:1240:1: In rule 'function_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:1240:1: In rule 'function_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:2868:1: In rule 'sparse_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:2868:1: In rule 'sparse_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:2895:1: In rule 'sort_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:2895:1: In rule 'sort_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3266:1: In rule 'gradient_checker_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3266:1: In rule 'gradient_checker_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3266:1: In rule 'gradient_checker_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3266:1: In rule 'gradient_checker_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3282:1: In rule 'gradient_checker_v2_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3282:1: In rule 'gradient_checker_v2_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3282:1: In rule 'gradient_checker_v2_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3282:1: In rule 'gradient_checker_v2_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3298:1: In rule 'gradients_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3298:1: In rule 'gradients_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3298:1: In rule 'gradients_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3298:1: In rule 'gradients_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3345:1: In rule 'image_grad_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3345:1: In rule 'image_grad_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3345:1: In rule 'image_grad_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3345:1: In rule 'image_grad_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3358:1: In rule 'image_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3358:1: In rule 'image_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3358:1: In rule 'image_ops_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3358:1: In rule 'image_ops_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3432:1: In rule 'nn_batchnorm_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3432:1: In rule 'nn_batchnorm_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3432:1: In rule 'nn_batchnorm_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3432:1: In rule 'nn_batchnorm_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3467:1: In rule 'nn_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3467:1: In rule 'nn_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3467:1: In rule 'nn_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3467:1: In rule 'nn_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3487:1: In rule 'nn_xent_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3487:1: In rule 'nn_xent_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3487:1: In rule 'nn_xent_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3487:1: In rule 'nn_xent_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3804:1: In rule 'function_utils_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3804:1: In rule 'function_utils_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4375:1: In rule 'localhost_cluster_performance_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4375:1: In rule 'localhost_cluster_performance_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4375:1: In rule 'localhost_cluster_performance_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4375:1: In rule 'localhost_cluster_performance_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4399:1: In rule 'sync_replicas_optimizer_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4399:1: In rule 'sync_replicas_optimizer_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4442:1: In rule 'session_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4442:1: In rule 'session_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'adadelta_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'adadelta_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'adagrad_da_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'adagrad_da_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'adagrad_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'adagrad_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'basic_loops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'basic_loops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'coordinator_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'coordinator_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'device_setter_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'device_setter_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'ftrl_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'ftrl_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'gradient_descent_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'gradient_descent_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'learning_rate_decay_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'learning_rate_decay_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'learning_rate_decay_v2_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'learning_rate_decay_v2_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'momentum_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'momentum_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'optimizer_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'optimizer_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'proximal_adagrad_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'proximal_adagrad_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'proximal_gradient_descent_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'proximal_gradient_descent_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'quantize_training_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'quantize_training_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'queue_runner_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'queue_runner_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'rmsprop_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'rmsprop_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'slot_creator_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'slot_creator_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'tensorboard_logging_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'tensorboard_logging_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'training_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'training_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4752:1: In rule 'saver_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4752:1: In rule 'saver_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4752:1: In rule 'saver_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4752:1: In rule 'saver_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4825:1: In rule 'saver_large_variable_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4825:1: In rule 'saver_large_variable_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4846:1: In rule 'saver_large_partitioned_variable_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4846:1: In rule 'saver_large_partitioned_variable_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4865:1: In rule 'session_manager_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4865:1: In rule 'session_manager_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4865:1: In rule 'session_manager_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4865:1: In rule 'session_manager_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4908:1: In rule 'basic_session_run_hooks_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4908:1: In rule 'basic_session_run_hooks_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4985:1: In rule 'warm_starting_util_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4985:1: In rule 'warm_starting_util_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5004:1: In rule 'monitored_session_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5004:1: In rule 'monitored_session_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5068:1: In rule 'input_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5068:1: In rule 'input_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5296:1: In rule 'layers_normalization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5296:1: In rule 'layers_normalization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5296:1: In rule 'layers_normalization_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5296:1: In rule 'layers_normalization_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5379:1: In rule 'batch_norm_benchmark', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5379:1: In rule 'batch_norm_benchmark', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5379:1: In rule 'batch_norm_benchmark_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5379:1: In rule 'batch_norm_benchmark_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5399:1: In rule 'concat_benchmark', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5399:1: In rule 'concat_benchmark', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5399:1: In rule 'concat_benchmark_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5399:1: In rule 'concat_benchmark_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5416:1: In rule 'control_flow_ops_benchmark', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5416:1: In rule 'control_flow_ops_benchmark', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5416:1: In rule 'control_flow_ops_benchmark_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5416:1: In rule 'control_flow_ops_benchmark_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5449:1: In rule 'split_benchmark', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5449:1: In rule 'split_benchmark', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5449:1: In rule 'split_benchmark_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5449:1: In rule 'split_benchmark_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5467:1: In rule 'transpose_benchmark', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5467:1: In rule 'transpose_benchmark', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5467:1: In rule 'transpose_benchmark_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5467:1: In rule 'transpose_benchmark_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5486:1: In rule 'matmul_benchmark', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5486:1: In rule 'matmul_benchmark', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5486:1: In rule 'matmul_benchmark_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5486:1: In rule 'matmul_benchmark_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5507:1: In rule 'matmul_benchmark_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5507:1: In rule 'matmul_benchmark_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5507:1: In rule 'matmul_benchmark_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5507:1: In rule 'matmul_benchmark_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5529:1: In rule 'session_benchmark', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5529:1: In rule 'session_benchmark', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5529:1: In rule 'session_benchmark_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5529:1: In rule 'session_benchmark_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5709:1: In rule 'memory_optimizer_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5709:1: In rule 'memory_optimizer_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5735:1: In rule 'constant_folding_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5735:1: In rule 'constant_folding_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5735:1: In rule 'constant_folding_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5735:1: In rule 'constant_folding_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5758:1: In rule 'layout_optimizer_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5758:1: In rule 'layout_optimizer_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5758:1: In rule 'layout_optimizer_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5758:1: In rule 'layout_optimizer_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:1360:1: Target '//tensorflow/python:framework_for_generated_wrappers_v2' contains an error and its package is in error and referenced by '//tensorflow/python:test_ops'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:1360:1: Target '//tensorflow/python:framework/test_ops.py' contains an error and its package is in error and referenced by '//tensorflow/python:test_ops'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:32:1: In rule 'batch_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:32:1: In rule 'batch_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:132:1: In rule 'dataset_constructor_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:132:1: In rule 'dataset_constructor_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:151:1: In rule 'filter_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:151:1: In rule 'filter_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:171:1: In rule 'fixed_length_record_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:171:1: In rule 'fixed_length_record_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:190:1: In rule 'flat_map_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:190:1: In rule 'flat_map_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:215:1: In rule 'group_by_reducer_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:215:1: In rule 'group_by_reducer_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:234:1: In rule 'group_by_window_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:234:1: In rule 'group_by_window_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:273:1: In rule 'interleave_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:273:1: In rule 'interleave_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:294:1: In rule 'map_and_batch_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:294:1: In rule 'map_and_batch_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:313:1: In rule 'numa_map_and_batch_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:313:1: In rule 'numa_map_and_batch_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:332:1: In rule 'map_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:332:1: In rule 'map_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:393:1: In rule 'padded_batch_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:393:1: In rule 'padded_batch_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:413:1: In rule 'parallel_interleave_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:413:1: In rule 'parallel_interleave_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:434:1: In rule 'parallel_map_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:434:1: In rule 'parallel_map_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:459:1: In rule 'parse_example_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:459:1: In rule 'parse_example_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:517:1: In rule 'sample_from_datasets_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:517:1: In rule 'sample_from_datasets_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:553:1: In rule 'sequence_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:553:1: In rule 'sequence_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:590:1: In rule 'shuffle_and_repeat_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:590:1: In rule 'shuffle_and_repeat_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:608:1: In rule 'shuffle_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:608:1: In rule 'shuffle_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:648:1: In rule 'stats_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:648:1: In rule 'stats_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:669:1: In rule 'textline_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:669:1: In rule 'textline_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:688:1: In rule 'tf_record_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:688:1: In rule 'tf_record_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:707:1: In rule 'unbatch_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:707:1: In rule 'unbatch_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:618:1: In rule 'framework_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:618:1: In rule 'framework_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:976:1: In rule 'session_debug_grpc_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:976:1: In rule 'session_debug_grpc_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:976:1: In rule 'session_debug_grpc_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:976:1: In rule 'session_debug_grpc_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1003:1: In rule 'grpc_large_data_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1003:1: In rule 'grpc_large_data_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1003:1: In rule 'grpc_large_data_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1003:1: In rule 'grpc_large_data_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1029:1: In rule 'dist_session_debug_grpc_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1029:1: In rule 'dist_session_debug_grpc_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1029:1: In rule 'dist_session_debug_grpc_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1029:1: In rule 'dist_session_debug_grpc_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1123:1: In rule 'examples_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1123:1: In rule 'examples_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/BUILD:23:1: In rule 'evaluation_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/BUILD:23:1: In rule 'evaluation_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/BUILD:70:1: In rule 'learning_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/BUILD:70:1: In rule 'learning_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/BUILD:168:1: In rule 'summaries_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/BUILD:168:1: In rule 'summaries_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:37:1: In rule 'tfe_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:37:1: In rule 'tfe_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:37:1: In rule 'tfe_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:37:1: In rule 'tfe_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:69:1: In rule 'datasets_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:69:1: In rule 'datasets_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:69:1: In rule 'datasets_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:69:1: In rule 'datasets_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:114:1: In rule 'saver_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:114:1: In rule 'saver_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:114:1: In rule 'saver_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:114:1: In rule 'saver_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:151:1: In rule 'metrics_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:151:1: In rule 'metrics_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:188:1: In rule 'evaluator_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:188:1: In rule 'evaluator_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:220:1: In rule 'network_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:220:1: In rule 'network_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:253:1: In rule 'remote_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:253:1: In rule 'remote_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:253:1: In rule 'remote_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:253:1: In rule 'remote_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/labeled_tensor/BUILD:66:1: In rule 'core_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/labeled_tensor/BUILD:66:1: In rule 'core_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/labeled_tensor/BUILD:168:1: In rule 'ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/labeled_tensor/BUILD:168:1: In rule 'ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:325:1: In rule 'ragged_tensor_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:325:1: In rule 'ragged_tensor_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:353:1: In rule 'ragged_eager_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:353:1: In rule 'ragged_eager_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:367:1: In rule 'ragged_range_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:367:1: In rule 'ragged_range_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:380:1: In rule 'ragged_tensor_bounding_shape_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:380:1: In rule 'ragged_tensor_bounding_shape_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:393:1: In rule 'ragged_row_lengths_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:393:1: In rule 'ragged_row_lengths_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:408:1: In rule 'ragged_gather_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:408:1: In rule 'ragged_gather_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:427:1: In rule 'ragged_batch_gather_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:427:1: In rule 'ragged_batch_gather_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:446:1: In rule 'ragged_gather_nd_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:446:1: In rule 'ragged_gather_nd_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:465:1: In rule 'ragged_row_splits_to_segment_ids_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:465:1: In rule 'ragged_row_splits_to_segment_ids_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:478:1: In rule 'ragged_segment_ids_to_row_splits_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:478:1: In rule 'ragged_segment_ids_to_row_splits_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:491:1: In rule 'ragged_from_tensor_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:491:1: In rule 'ragged_from_tensor_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:507:1: In rule 'ragged_to_sparse_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:507:1: In rule 'ragged_to_sparse_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:531:1: In rule 'ragged_from_sparse_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:531:1: In rule 'ragged_from_sparse_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:548:1: In rule 'ragged_to_tensor_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:548:1: In rule 'ragged_to_tensor_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:563:1: In rule 'ragged_segment_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:563:1: In rule 'ragged_segment_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:581:1: In rule 'ragged_reduce_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:581:1: In rule 'ragged_reduce_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:600:1: In rule 'ragged_map_flat_values_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:600:1: In rule 'ragged_map_flat_values_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:619:1: In rule 'ragged_const_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:619:1: In rule 'ragged_const_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:635:1: In rule 'ragged_constant_value_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:635:1: In rule 'ragged_constant_value_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:654:1: In rule 'convert_to_tensor_or_ragged_tensor_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:654:1: In rule 'convert_to_tensor_or_ragged_tensor_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:671:1: In rule 'ragged_boolean_mask_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:671:1: In rule 'ragged_boolean_mask_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:689:1: In rule 'ragged_concat_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:689:1: In rule 'ragged_concat_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:708:1: In rule 'ragged_stack_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:708:1: In rule 'ragged_stack_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:723:1: In rule 'ragged_tile_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:723:1: In rule 'ragged_tile_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:740:1: In rule 'ragged_util_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:740:1: In rule 'ragged_util_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:756:1: In rule 'ragged_expand_dims_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:756:1: In rule 'ragged_expand_dims_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:770:1: In rule 'ragged_where_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:770:1: In rule 'ragged_where_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:784:1: In rule 'ragged_dispatch_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:784:1: In rule 'ragged_dispatch_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:810:1: In rule 'ragged_operators_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:810:1: In rule 'ragged_operators_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:848:1: In rule 'ragged_tensor_shape_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:848:1: In rule 'ragged_tensor_shape_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/tpu/BUILD:316:1: In rule 'datasets_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/tpu/BUILD:316:1: In rule 'datasets_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/tpu/BUILD:396:1: In rule 'topology_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/tpu/BUILD:396:1: In rule 'topology_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/python/BUILD:27:1: In rule 'interpreter_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/python/BUILD:27:1: In rule 'interpreter_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/python/BUILD:70:1: In rule 'lite_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/python/BUILD:70:1: In rule 'lite_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/python/BUILD:127:1: In rule 'convert_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/python/BUILD:127:1: In rule 'convert_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/python/BUILD:172:1: In rule 'convert_saved_model_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/python/BUILD:172:1: In rule 'convert_saved_model_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/pyct/common_transformers/BUILD:33:1: In rule 'anf_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/pyct/common_transformers/BUILD:33:1: In rule 'anf_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:145:1: In rule 'flat_map_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:145:1: In rule 'flat_map_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:166:1: In rule 'from_generator_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:166:1: In rule 'from_generator_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:248:1: In rule 'interleave_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:248:1: In rule 'interleave_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:267:1: In rule 'iterator_checkpoint_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:267:1: In rule 'iterator_checkpoint_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:314:1: In rule 'iterator_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:314:1: In rule 'iterator_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:314:1: In rule 'iterator_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:314:1: In rule 'iterator_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:370:1: In rule 'map_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:370:1: In rule 'map_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:402:1: In rule 'multi_device_iterator_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:402:1: In rule 'multi_device_iterator_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:402:1: In rule 'multi_device_iterator_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:402:1: In rule 'multi_device_iterator_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:654:1: In rule 'window_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:654:1: In rule 'window_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/core/BUILD:54:1: In rule 'converter_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/core/BUILD:54:1: In rule 'converter_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/core/BUILD:65:1: In rule 'errors_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/core/BUILD:65:1: In rule 'errors_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/core/BUILD:80:1: In rule 'function_wrapping_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/core/BUILD:80:1: In rule 'function_wrapping_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/core/BUILD:90:1: In rule 'naming_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/core/BUILD:90:1: In rule 'naming_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/predictor/BUILD:109:1: In rule 'saved_model_predictor_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/predictor/BUILD:109:1: In rule 'saved_model_predictor_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/predictor/BUILD:125:1: In rule 'predictor_factories_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/predictor/BUILD:125:1: In rule 'predictor_factories_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/predictor/BUILD:137:1: In rule 'core_estimator_predictor_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/predictor/BUILD:137:1: In rule 'core_estimator_predictor_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/predictor/BUILD:150:1: In rule 'contrib_estimator_predictor_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/predictor/BUILD:150:1: In rule 'contrib_estimator_predictor_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/nets/BUILD:44:1: In rule 'alexnet_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/nets/BUILD:44:1: In rule 'alexnet_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/nets/BUILD:190:1: In rule 'overfeat_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/nets/BUILD:190:1: In rule 'overfeat_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/nets/BUILD:304:1: In rule 'vgg_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/nets/BUILD:304:1: In rule 'vgg_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/api/generator/BUILD:38:1: In rule 'create_python_api_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/api/generator/BUILD:38:1: In rule 'create_python_api_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/api/generator/BUILD:52:1: In rule 'tensorflow_doc_srcs_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/api/generator/BUILD:52:1: In rule 'tensorflow_doc_srcs_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/api/generator/BUILD:68:1: In rule 'output_init_files_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/api/generator/BUILD:68:1: In rule 'output_init_files_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/session_bundle/BUILD:150:1: In rule 'gc_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/session_bundle/BUILD:150:1: In rule 'gc_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/session_bundle/BUILD:221:1: In rule 'session_bundle_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/session_bundle/BUILD:221:1: In rule 'session_bundle_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/specs/BUILD:38:1: In rule 'specs_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/specs/BUILD:38:1: In rule 'specs_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/specs/BUILD:52:1: In rule 'summaries_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/specs/BUILD:52:1: In rule 'summaries_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/BUILD:237:1: In rule 'print_selective_registration_header_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/BUILD:237:1: In rule 'print_selective_registration_header_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/BUILD:260:1: In rule 'saved_model_cli_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/BUILD:260:1: In rule 'saved_model_cli_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/tensor_forest/hybrid/BUILD:206:1: In rule 'decisions_to_data_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/tensor_forest/hybrid/BUILD:206:1: In rule 'decisions_to_data_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/examples/tutorials/mnist/BUILD:92:1: In rule 'fully_connected_feed_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/examples/tutorials/mnist/BUILD:92:1: In rule 'fully_connected_feed_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:99:1: In rule 'tensor_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:99:1: In rule 'tensor_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:99:1: In rule 'tensor_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:99:1: In rule 'tensor_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:111:1: In rule 'backprop_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:111:1: In rule 'backprop_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:111:1: In rule 'backprop_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:111:1: In rule 'backprop_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:132:1: In rule 'core_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:132:1: In rule 'core_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:132:1: In rule 'core_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:132:1: In rule 'core_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:149:1: In rule 'function_argument_naming_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:149:1: In rule 'function_argument_naming_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:149:1: In rule 'function_argument_naming_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:149:1: In rule 'function_argument_naming_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:163:1: In rule 'function_defun_collection_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:163:1: In rule 'function_defun_collection_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:163:1: In rule 'function_defun_collection_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:163:1: In rule 'function_defun_collection_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:178:1: In rule 'function_gradients_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:178:1: In rule 'function_gradients_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:178:1: In rule 'function_gradients_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:178:1: In rule 'function_gradients_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:195:1: In rule 'function_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:195:1: In rule 'function_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:195:1: In rule 'function_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:195:1: In rule 'function_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:260:1: In rule 'execution_callbacks_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:260:1: In rule 'execution_callbacks_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:284:1: In rule 'graph_only_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:284:1: In rule 'graph_only_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:284:1: In rule 'graph_only_ops_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:284:1: In rule 'graph_only_ops_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:357:1: In rule 'benchmarks_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:357:1: In rule 'benchmarks_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:357:1: In rule 'benchmarks_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:357:1: In rule 'benchmarks_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:378:1: In rule 'tape_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:378:1: In rule 'tape_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:396:1: In rule 'ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:396:1: In rule 'ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:396:1: In rule 'ops_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:396:1: In rule 'ops_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:419:1: In rule 'pywrap_tfe_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:419:1: In rule 'pywrap_tfe_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:447:1: In rule 'memory_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:447:1: In rule 'memory_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:447:1: In rule 'memory_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:447:1: In rule 'memory_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:496:1: In rule 'def_function_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:496:1: In rule 'def_function_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:524:1: In rule 'wrap_function_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:524:1: In rule 'wrap_function_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/boosted_trees/BUILD:120:1: In rule 'gbdt_batch_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/boosted_trees/BUILD:120:1: In rule 'gbdt_batch_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:10:1: In rule 'bucket_by_sequence_length_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:10:1: In rule 'bucket_by_sequence_length_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:62:1: In rule 'csv_dataset_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:62:1: In rule 'csv_dataset_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:83:1: In rule 'dense_to_sparse_batch_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:83:1: In rule 'dense_to_sparse_batch_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:99:1: In rule 'directed_interleave_dataset_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:99:1: In rule 'directed_interleave_dataset_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:151:1: In rule 'group_by_reducer_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:151:1: In rule 'group_by_reducer_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:172:1: In rule 'group_by_window_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:172:1: In rule 'group_by_window_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:194:1: In rule 'ignore_errors_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:194:1: In rule 'ignore_errors_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:211:1: In rule 'indexed_dataset_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:211:1: In rule 'indexed_dataset_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:226:1: In rule 'make_batched_features_dataset_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:226:1: In rule 'make_batched_features_dataset_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:247:1: In rule 'make_csv_dataset_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:247:1: In rule 'make_csv_dataset_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:266:1: In rule 'make_tf_record_dataset_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:266:1: In rule 'make_tf_record_dataset_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:283:1: In rule 'map_and_batch_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:283:1: In rule 'map_and_batch_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:348:1: In rule 'cardinality_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:348:1: In rule 'cardinality_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:380:1: In rule 'parallel_interleave_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:380:1: In rule 'parallel_interleave_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:472:1: In rule 'rejection_resample_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:472:1: In rule 'rejection_resample_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:499:1: In rule 'restructured_dataset_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:499:1: In rule 'restructured_dataset_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:538:1: In rule 'shuffle_and_repeat_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:538:1: In rule 'shuffle_and_repeat_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:558:1: In rule 'sleep_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:558:1: In rule 'sleep_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:659:1: In rule 'unbatch_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:659:1: In rule 'unbatch_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/constrained_optimization/BUILD:41:1: In rule 'candidates_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/constrained_optimization/BUILD:41:1: In rule 'candidates_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/constrained_optimization/BUILD:70:1: In rule 'external_regret_optimizer_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/constrained_optimization/BUILD:70:1: In rule 'external_regret_optimizer_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/constrained_optimization/BUILD:84:1: In rule 'swap_regret_optimizer_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/constrained_optimization/BUILD:84:1: In rule 'swap_regret_optimizer_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:56:1: In rule 'train_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:56:1: In rule 'train_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:157:1: In rule 'losses_impl_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:157:1: In rule 'losses_impl_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:194:1: In rule 'tuple_losses_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:194:1: In rule 'tuple_losses_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:232:1: In rule 'conditioning_utils_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:232:1: In rule 'conditioning_utils_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:262:1: In rule 'random_tensor_pool_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:262:1: In rule 'random_tensor_pool_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:299:1: In rule 'virtual_batchnorm_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:299:1: In rule 'virtual_batchnorm_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:334:1: In rule 'clip_weights_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:334:1: In rule 'clip_weights_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:372:1: In rule 'classifier_metrics_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:372:1: In rule 'classifier_metrics_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:404:1: In rule 'eval_utils_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:404:1: In rule 'eval_utils_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:436:1: In rule 'summaries_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:436:1: In rule 'summaries_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:468:1: In rule 'head_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:468:1: In rule 'head_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:505:1: In rule 'gan_estimator_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:505:1: In rule 'gan_estimator_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:556:1: In rule 'stargan_estimator_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:556:1: In rule 'stargan_estimator_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:608:1: In rule 'sliced_wasserstein_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:608:1: In rule 'sliced_wasserstein_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/summary/BUILD:13:1: In rule 'summary_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/summary/BUILD:13:1: In rule 'summary_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/summary/BUILD:33:1: In rule 'summary_ops_graph_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/summary/BUILD:33:1: In rule 'summary_ops_graph_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/saved_model/BUILD:316:1: In rule 'save_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/saved_model/BUILD:316:1: In rule 'save_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/saved_model/BUILD:350:1: In rule 'load_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/saved_model/BUILD:350:1: In rule 'load_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/toco/python/BUILD:72:1: In rule 'toco_from_protos_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/toco/python/BUILD:72:1: In rule 'toco_from_protos_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/data/BUILD:60:1: In rule 'dataset_data_provider_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/data/BUILD:60:1: In rule 'dataset_data_provider_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/data/BUILD:179:1: In rule 'tfexample_decoder_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/data/BUILD:179:1: In rule 'tfexample_decoder_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:53:1: In rule 'util_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:53:1: In rule 'util_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:65:1: In rule 'select_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:65:1: In rule 'select_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:77:1: In rule 'match_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:77:1: In rule 'match_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:89:1: In rule 'subgraph_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:89:1: In rule 'subgraph_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:101:1: In rule 'reroute_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:101:1: In rule 'reroute_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:114:1: In rule 'edit_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:114:1: In rule 'edit_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:127:1: In rule 'transform_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:127:1: In rule 'transform_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/receptive_field/BUILD:62:1: In rule 'graph_compute_order_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/receptive_field/BUILD:62:1: In rule 'graph_compute_order_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/receptive_field/BUILD:78:1: In rule 'parse_layer_parameters_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/receptive_field/BUILD:78:1: In rule 'parse_layer_parameters_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/receptive_field/BUILD:94:1: In rule 'receptive_field_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/receptive_field/BUILD:94:1: In rule 'receptive_field_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/cluster_resolver/BUILD:33:1: In rule 'cluster_resolver_initialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/cluster_resolver/BUILD:33:1: In rule 'cluster_resolver_initialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:21:1: In rule 'dct_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:21:1: In rule 'dct_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:34:1: In rule 'fft_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:34:1: In rule 'fft_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:50:1: In rule 'mel_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:50:1: In rule 'mel_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:61:1: In rule 'mfcc_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:61:1: In rule 'mfcc_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:75:1: In rule 'reconstruction_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:75:1: In rule 'reconstruction_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:92:1: In rule 'shape_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:92:1: In rule 'shape_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:130:1: In rule 'window_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:130:1: In rule 'window_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/compiler/BUILD:73:1: In rule 'xla_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/compiler/BUILD:73:1: In rule 'xla_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/examples/tutorials/mnist:package' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/lite/python:interpreter_test_data' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/lite/python:tflite_convert' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/lite/toco/python:toco_from_protos' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/autograph/core:test_lib' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/autograph/pyct/common_transformers:common_transformers' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python:cond_v2' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python:distributed_framework_test_lib' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python:meta_graph_testdata' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python:spectral_ops_test_util' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python:util_example_parser_configuration' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/data/experimental/kernel_tests/serialization:dataset_serialization_test_base' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/data/experimental/kernel_tests:stats_dataset_test_base' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/data/kernel_tests:test_base' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/debug:debug_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/eager:eager_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/kernel_tests/signal:test_util' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/ops/ragged:ragged_test_util' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/saved_model:saved_model' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/tools:tools_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/tools/api/generator:create_python_api' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python:test_ops' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python:while_v2' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/boosted_trees:boosted_trees_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/cluster_resolver:cluster_resolver_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/compiler:xla' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/constrained_optimization:constrained_optimization_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/eager/python:evaluator' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/gan:gan' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/graph_editor:graph_editor_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/labeled_tensor:labeled_tensor_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/predictor:predictor_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/receptive_field:receptive_field_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/session_bundle:session_bundle_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/slim:slim' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/slim/python/slim/data:data_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/slim/python/slim/nets:nets_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/specs:specs' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/summary:summary_test_util' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/tensor_forest/hybrid:hybrid_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/tpu:tpu' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python:pywrap_tensorflow_import_lib_file' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Cannot compute config conditions
INFO: Elapsed time: 1,698s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (6 packages loaded, 56 targets configured)
    currently loading: tensorflow/core
    Fetching @six_archive; fetching
"
34828,Missing default floating point values in single_image_random_dot_stereograms_ops.cc,"- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow version: 1.12+ gpu installed with pip
- CUDA/cuDNN version: 9.0/7.0
- GPU model and memory: GTX 1060 6GB / NVIDIA-SMI 390.116
- Exact command to reproduce: use code below in python3 script
- Mobile device: N/A

Code to reproduce:

```python
import carla
import tensorflow as tf
vol = tf.contrib.util.make_tensor_proto([256, 256])
```

Run with (carla egg package attached):

``` shell
PYTHONPATH=carla-0.9.6-py3.5-linux-x86_64.egg:$PYTHONPATH  python ./above_script.py
```



Observe:

```
Traceback (most recent call last):
  File ""above_script.py"", line 3, in <module>
    vol = tf.contrib.util.make_tensor_proto([256, 256])
  File ""/home/pawel.ziecina/.virtualenvs/myvenv/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py"", line 53, in __getattr__
    module = self._load()
  File ""/home/pawel.ziecina/.virtualenvs/myvenv/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py"", line 42, in _load
    module = importlib.import_module(self.__name__)
  File ""/home/pawel.ziecina/.virtualenvs/myvenv/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/home/pawel.ziecina/.virtualenvs/myvenv/lib/python3.6/site-packages/tensorflow/contrib/__init__.py"", line 57, in <module>
    from tensorflow.contrib import image
  File ""/home/pawel.ziecina/.virtualenvs/myvenv/lib/python3.6/site-packages/tensorflow/contrib/image/__init__.py"", line 70, in <module>
    from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms
  File ""/home/pawel.ziecina/.virtualenvs/myvenv/lib/python3.6/site-packages/tensorflow/contrib/image/python/ops/single_image_random_dot_stereograms.py"", line 27, in <module>
    ""_single_image_random_dot_stereograms.so""))
  File ""/home/pawel.ziecina/.virtualenvs/myvenv/lib/python3.6/site-packages/tensorflow/contrib/util/loader.py"", line 56, in load_op_library
    ret = load_library.load_op_library(path)
  File ""/home/pawel.ziecina/.virtualenvs/myvenv/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py"", line 77, in load_op_library
    exec(wrappers, module.__dict__)
  File ""<string>"", line 28
    def single_image_random_dot_stereograms(depth_values, hidden_surface_removal=True, convergence_dots_size=8, dots_per_inch=72, eye_separation=, mu=, normalize=True, normalize_max=, normalize_min=, border_level=, number_colors=256, output_image_shape=[1024, 768, 1], output_data_window=[1022, 757], name=None):
                                                                                                                                                 ^
SyntaxError: invalid syntax


```

Related with #21164 - there was bug with wrong decimal separator, here we observe missing default value for float type attrributes in generated python wrapper. 

Other observations:
1. Issue doesn't occur when switch order of imports i.e. import carla after tensorflow import
1. Issue doesn't occur on tf 1.11 - probably it was introduced with #22044

[carla_egg.tar.gz](https://github.com/tensorflow/tensorflow/files/3922153/carla_egg.tar.gz)"
34827,Bogus cache keys in Network._output_shape_cache can cause wrong output shapes (and gradual but unbounded memory growth),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: 3.7.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

Increasing amounts of memory get allocated and not garbage-collected on successive calls to `model.predict_on_batch(...)`, because output tensor shapes are cached into `Network._output_shape_cache` with cache keys based on Python object identity (which is based on the memory address of the object, in CPython), and cache entries are never expired (the cache is just a standard Python dict). This happens at the end of `Network._run_internal_graph`:

```python
      cache_key = generic_utils.object_list_uid(input_shapes)
      self._output_shape_cache[cache_key] = nest.pack_sequence_as(
          self._nested_outputs, output_shapes)
```

This is seen in the output of my script (see further below), which shows memory growth like this:

```
...
Notable memory usage diff over last 3000 predictions:
+177 B     (+0 B    per prediction)   +3 objs (0.00 per prediction) at:
    kerasbug.py:37
    .../tensorflow_core/python/keras/engine/training.py:1135
    .../tensorflow_core/python/keras/engine/training_v2_utils.py:370
    .../tensorflow_core/python/keras/engine/base_layer.py:891
    .../tensorflow_core/python/keras/engine/network.py:708
    .../tensorflow_core/python/keras/engine/network.py:877
    .../tensorflow_core/python/keras/utils/generic_utils.py:564
    .../tensorflow_core/python/keras/utils/generic_utils.py:564

model._output_shape_cache has grown to 300, new elements are:
('5492540304', TensorShape([None, 3]))
('5492540688', TensorShape([None, 3]))
('5492539600', TensorShape([None, 3]))
```

In practice, at least in my simple script, the growth rate _does_ decrease with time and the overall memory use _probably_ levels off, because input shape objects get garbage collected and their memory gets reused so new input shape objects _often_ get allocated at the same memory addresses, so their cache keys are already present in the cache. But there is no guarantee of this reuse of memory addresses and thus object identities (and its likelihood probably depends on what else is going on in the process), and thus there is no real guarantee of an upper bound to the growth of this cache.

Additionally, this cache key scheme hints at a likely functional bug: an input shape object that gets created at the same memory address as a previous input shape object will get the same object identity, and can thus lead to the same cache key in the output shape cache, even if it is not the same input shape. Such a case can cause `compute_output_shape` to return the wrong output shape (one that was valid for a _previous_ set of input shapes, still present in the cache). I have not tried to write code with varying input shapes to manifest this suspected bug.

**Describe the expected behavior**

I expect memory growth on successive model predictions to have an upper bound, when I  repeatedly call `predict_on_batch`.

I expect the output tensor shape returned by `Network.compute_output_shape` to be unaffected by the shapes of inputs of _previous_ runs. (But I haven't demonstrated a failure of this expectation, just conjectured how it would happen.)

**Code to reproduce the issue**

The following script shows the (theoretically) unbounded memory growth.

```python
import gc
import time
from itertools import islice
import tracemalloc
from os.path import basename

import tensorflow as tf
import numpy as np

TRACEMALLOC_EVERY = 3000
TRACEMALLOC_IGNORE = ""tracemalloc.py:"",

input_tensor = tf.keras.Input(shape=(3,), name='input')
output_tensor = tf.keras.layers.Dense(3, name='output')(input_tensor)
model = tf.keras.Model(inputs=[input_tensor], outputs=[output_tensor])

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['categorical_accuracy'])


def random_input():
    return np.random.random_sample([1, 3])

model.predict_on_batch(random_input())

tracemalloc.start(50)

last_snapshot = tracemalloc.take_snapshot()
already_printed_from_output_shape_cache = []

while True:
    for _ in range(TRACEMALLOC_EVERY):
        model.predict_on_batch(random_input())

    gc.collect()
    time.sleep(0.1)
    snapshot = tracemalloc.take_snapshot()
    top_stats = snapshot.compare_to(last_snapshot, 'traceback')
    last_snapshot = snapshot

    notable_mem_usage_diff = ""\n"".join(
        f""{tracemalloc._format_size(stat.size_diff, True):10} ""
        f""({tracemalloc._format_size(stat.size_diff // TRACEMALLOC_EVERY, True):7} per prediction) ""
        f""{stat.count_diff:+4} objs ""
        f""({stat.count_diff / TRACEMALLOC_EVERY:.2f} per prediction) at:\n""
        + ""\n"".join(""    %s"" % tb for tb in stat.traceback)
        for stat in islice(
            (s for s in top_stats
             if not any(i in str(tb) for tb in s.traceback for i in TRACEMALLOC_IGNORE)
             and not all(basename(__file__) + "":"" in str(tb) for tb in s.traceback)
             and abs(s.size_diff) > 0  # // TRACEMALLOC_EVERY >= 8
             ),
            20
        )
    )
    if notable_mem_usage_diff:
        print(""Notable memory usage diff over last %d predictions:\n%s\n"" % (TRACEMALLOC_EVERY, notable_mem_usage_diff))
    else:
        print(""No notable memory usage diff over last %d predictions\n"" % (TRACEMALLOC_EVERY,))
    new_cache_elements = [item for item in model._output_shape_cache.items()
                          if item not in already_printed_from_output_shape_cache]
    if new_cache_elements:
        print(""model._output_shape_cache has grown to %d, new elements are:\n%s\n"" % (
            len(model._output_shape_cache),
            ""\n"".join(str(item) for item in new_cache_elements)
        ))
        already_printed_from_output_shape_cache.extend(new_cache_elements)
```

**Other info / logs**

As a sidenote, we are calling `predict_on_batch` instead of `predict` as a workaround for https://github.com/tensorflow/tensorflow/issues/33009 (another memory leak issue)."
34826,[Dask] [TF 2.0] ValueError: TypeError: len() of unsized object,"**Describe the current behavior**
```python
In [6]: tf.convert_to_tensor(value)                                                                                                                                                                                                                                                                                          
2019-12-04 13:15:11.908174: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-12-04 13:15:12.647383: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz
2019-12-04 13:15:12.648044: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5644649bc070 executing computations on platform Host. Devices:
2019-12-04 13:15:12.648118: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-12-04 13:15:12.654239: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-6-cb9026824ca5> in <module>
----> 1 tf.convert_to_tensor(value)

/opt/anaconda/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)
   1240       name=name,
   1241       preferred_dtype=dtype_hint,
-> 1242       as_ref=False)
   1243 
   1244 

/opt/anaconda/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)
   1294 
   1295     if ret is None:
-> 1296       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1297 
   1298     if ret is NotImplemented:

/opt/anaconda/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    284                                          as_ref=False):
    285   _ = as_ref
--> 286   return constant(v, dtype=dtype, name=name)
    287 
    288 

/opt/anaconda/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in constant(value, dtype, shape, name)
    225   """"""
    226   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--> 227                         allow_broadcast=True)
    228 
    229 

/opt/anaconda/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    233   ctx = context.context()
    234   if ctx.executing_eagerly():
--> 235     t = convert_to_eager_tensor(value, ctx, dtype)
    236     if shape is None:
    237       return t

/opt/anaconda/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
     94       dtype = dtypes.as_dtype(dtype).as_datatype_enum
     95   ctx.ensure_initialized()
---> 96   return ops.EagerTensor(value, ctx.device_name, dtype)
     97 
     98 

ValueError: TypeError: len() of unsized object
Traceback (most recent call last):

  File ""/opt/anaconda/envs/tf/lib/python3.7/site-packages/dask/array/core.py"", line 1165, in __len__
    raise TypeError(""len() of unsized object"")

TypeError: len() of unsized object
```

**Describe the expected behavior**
Should just return a tf.EagerTensor

**Code to reproduce the issue**
I uploaded a Test-Dask-array which produces this error:
https://drive.google.com/file/d/1NtKsv3P37MyYzs8OCbzxSVuL-hA79IPE/view?usp=sharing
Please `bunzip2` it before running the following code:
```python
import pickle
import tensorflow as tf
with open('test.pkl', 'rb') as handle:
        value = pickle.load(handle)

tf.convert_to_tensor(value)
```

**_The part where it fails is `value[0, 0]`, which is a Dask scalar array.
I have no idea why Tensorflow tries to fetch scalar values from a Dask array instead of just calling `np.asarray(value)` on it._**

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Centos 7
- TensorFlow installed from (source or binary):
conda-forge
- TensorFlow version (use command below):
unknown 2.0.0
- Python version:
3.7.3"
34825,Compiling tensorflow 2.0.0 on Jetson AGX Xavier,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): https://github.com/tensorflow/tensorflow/archive/v2.0.0.tar.gz
- TensorFlow version: 2.0.0
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.0.326 / 7.5
- GPU model and memory: Jetson AGX Xavier (512-core Volta GPU with Tensor Cores) 16GB



**Describe the problem**
I'm compiling tensorflow 2.0.0 on Jetson AGX Xavier.
I'm following the tutorial on: https://jkjung-avt.github.io/build-tensorflow-2.0.0/
but compiling blaze fails with:
```
ERROR: /mnt/nvme/tensorflow_install/jetson_nano/src/tensorflow-2.0.0/tensorflow/python/keras/api/BUILD:46:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1)
Traceback (most recent call last):
  File ""/mnt/nvme/.cache/bazel/_bazel_nvidia/98cdf810b71446efc44591ffddb09367/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/mnt/nvme/.cache/bazel/_bazel_nvidia/98cdf810b71446efc44591ffddb09367/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/mnt/nvme/.cache/bazel/_bazel_nvidia/98cdf810b71446efc44591ffddb09367/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/core/framework/graph_pb2.py"", line 7, in <module>
    from google.protobuf import descriptor as _descriptor
ModuleNotFoundError: No module named 'google.protobuf'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 13210.120s, Critical Path: 451.46s
INFO: 11876 processes: 11876 local.
FAILED: Build did NOT complete successfully
```
but on the other hand if I'm running that line alone it works no problem:
```
/mnt/nvme/tensorflow_install/jetson_nano$ python3
Python 3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from google.protobuf import descriptor as _descriptor
>>> exit()
```

I'm assuming something wrong with my enviroment, but how can I check it?"
34823,Profiler API and Service are not available,"## URL(s) with the issue:

https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras#other_ways_for_profiling

## Description of the issue (what needs changing):

The guide presents the profiler API and service as other ways of profiling.
Both of these use modules inside the `tensorflow.python` package.
However, this package does not seem to exist as exhibited by the error message:

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-14-c7b3d51d1060> in <module>()
----> 1 with tf.python.eager.profiler.Profiler('logdir_path'):
      2   # do your training here
      3   pass
      4 
      5 

AttributeError: module 'tensorflow' has no attribute 'python'
```

### Submit a pull request?

I would be pleased to submit a pull request, but do not know if there is any TF2.0 compatible way to use profile API or service."
34821,MultiWorkerMirroredStategy cannot run,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (Linux Ubuntu 18.04.3 LTS):
- TensorFlow installed from (binary):
- TensorFlow version (2.0.0):
- Python version: 3.6.8
- CUDA/cuDNN version: 10.1
- GPU model and memory: Tesla P100

**Describe the current behavior**
When I run the tutorial code in [Multi worker training with Keras](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras). Train fails after the first epoch. I guess that the `CollectiveOp` fails. `TF_CONFIG` was  generated by kubeflow, and I can assure that it is correct.

**Describe the expected behavior**
Should run normally.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import tensorflow_datasets as tfds
import tensorflow as tf
from absl import app, flags
import os
import json

tfds.disable_progress_bar()
FLAGS = flags.FLAGS

flags.DEFINE_string('input_data_path', default='hdfs://30.78.5.52:9000/data/public/dataset/tensorflow_datasets', help='HDFS Input data path')
flags.DEFINE_string('checkpoint_dir', default=None, help='HDFS checkpoint dir')

BUFFER_SIZE = 10000
BATCH_SIZE = 64

def make_datasets_unbatched():
  # Scaling MNIST data from (0, 255] to (0., 1.]
  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label

  datasets, info = tfds.load(name='mnist',
                            with_info=True,
                            as_supervised=True,
                            download=False,
                            data_dir=FLAGS.input_data_path)

  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)


def build_and_compile_cnn_model():
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')
  ])
  model.compile(
      loss=tf.keras.losses.sparse_categorical_crossentropy,
      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
      metrics=['accuracy'])
  return model

def main(argv):
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

  NUM_WORKERS = 2
  # Here the batch size scales up by number of workers since
  # `tf.data.Dataset.batch` expects the global batch size. Previously we used 64,
  # and now this becomes 128.
  GLOBAL_BATCH_SIZE = 64 * NUM_WORKERS
  # Replace the `filepath` argument with a path in the file system
  # accessible by all workers.
  callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=FLAGS.checkpoint_dir)]
  with strategy.scope():
    # Creation of dataset, and model building/compiling need to be within
    # `strategy.scope()`.
    train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)
    multi_worker_model = build_and_compile_cnn_model()
  multi_worker_model.fit(x=train_datasets, epochs=2, callbacks=callbacks)



if __name__ == '__main__':
  app.run(main)

```

**Other info / logs**
After the first epoch, the following error occurred.
```
    469/Unknown - 10s 20ms/step - loss: 2.1859 - accuracy: 0.31432019-12-04 18:43:39.653487: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
         [[GroupCrossDeviceControlEdges_0/Identity_4/_19]]
2019-12-04 18:43:39.653688: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
2019-12-04 18:43:40.073912: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence
         [[{{node IteratorGetNext}}]]
         [[GroupCrossDeviceControlEdges_0/Identity_4/_19]]
2019-12-04 18:43:40.074084: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
         [[{{node IteratorGetNext}}]]
         [[GroupCrossDeviceControlEdges_0/Identity_4/_19]]
2019-12-04 18:43:40.074306: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Out of range: [_Derived_]End of sequence
         [[{{node IteratorGetNext}}]]
         [[GroupCrossDeviceControlEdges_0/Identity_4/_19]]
2019-12-04 18:43:40.074408: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
         [[{{node IteratorGetNext}}]]
         [[GroupCrossDeviceControlEdges_0/Identity_4/_19]]
         [[CollectiveReduce]]
         [[CollectiveReduce/_2]]
2019-12-04 18:43:40.074591: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
         [[{{node IteratorGetNext}}]]
         [[GroupCrossDeviceControlEdges_0/Identity_4/_19]]
         [[CollectiveReduce]]
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 668, in on_start
    yield
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 372, in fit
    prefix='val_')
  File ""/usr/lib/python3.6/contextlib.py"", line 88, in __exit__
    next(self.gen)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 685, in on_epoch
    self.callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py"", line 298, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py"", line 963, in on_epoch_end
    self._save_model(epoch=epoch, logs=logs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py"", line 1012, in _save_model
    self.model.save(filepath, overwrite=True)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py"", line 975, in save
    signatures, options)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py"", line 115, in save_model
    signatures, options)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/save.py"", line 74, in save
    save_lib.save(model, filepath, signatures, options)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/save.py"", line 883, in save
    _ = _SaveableView(checkpoint_graph_view)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/save.py"", line 164, in __init__
    self.checkpoint_view.objects_ids_and_slot_variables())
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/graph_view.py"", line 425, in objects_ids_and_slot_variables
    object_names=object_names)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/graph_view.py"", line 96, in _serialize_slot_variables
    or hasattr(trackable, ""_create_or_restore_slot_variable"")):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py"", line 389, in __getattr__
    return getattr(self.get(), name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py"", line 322, in get
    return self._get_cross_replica()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py"", line 1237, in _get_cross_replica
    self, axis=None)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 805, in reduce
    return self._extended._reduce(reduce_op, value)  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 1436, in _reduce
    device_util.current() or ""/device:CPU:0""))[0]
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py"", line 490, in _reduce_to
    reduce_op, value, destinations=destinations)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py"", line 282, in reduce
    destinations)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py"", line 1025, in reduce_implementation
    all_reduced = self._batch_all_reduce(reduce_op, [per_replica_value])[0]
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py"", line 1091, in _batch_all_reduce
    dense_results = self._do_batch_all_reduce_dense(reduce_op, dense_values)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py"", line 1120, in _do_batch_all_reduce_dense
    ""Id"")
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/cross_device_utils.py"", line 365, in build_collective_reduce
    return collective_all_reduce()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py"", line 526, in _call
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py"", line 1141, in _filtered_call
    self.captured_inputs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py"", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py"", line 511, in call
    ctx=ctx)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.OutOfRangeError:  [_Derived_]End of sequence
         [[{{node IteratorGetNext}}]]
         [[GroupCrossDeviceControlEdges_0/Identity_4/_19]]
         [[CollectiveReduce]]
         [[CollectiveReduce/_2]] [Op:__inference_collective_all_reduce_2649]

Function call stack:
collective_all_reduce


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""multi_worker_mirrored_strategy_keras_mnist.py"", line 81, in <module>
    app.run(main)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""multi_worker_mirrored_strategy_keras_mnist.py"", line 76, in main
    multi_worker_model.fit(x=train_datasets, epochs=2, callbacks=callbacks)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py"", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py"", line 789, in fit
    *args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py"", line 776, in wrapper
    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py"", line 853, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py"", line 360, in _run_single_worker
    return worker_fn(strategy)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py"", line 771, in _worker_fn
    return method(model, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 372, in fit
    prefix='val_')
  File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 671, in on_start
    self.callbacks._call_end_hook(mode)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py"", line 258, in _call_end_hook
    self.on_train_end()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py"", line 375, in on_train_end
    callback.on_train_end(logs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py"", line 940, in on_train_end
    self._training_state.delete_backup()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/distribute/multi_worker_training_state.py"", line 161, in delete_backup
    tracking.AutoTrackable.__delattr__(self._model, CKPT_SAVED_EPOCH)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/tracking.py"", line 94, in __delattr__
    super(AutoTrackable, self).__delattr__(name)
AttributeError: _ckpt_saved_epoch
2019-12-04 18:43:40.494701: W tensorflow/core/common_runtime/eager/context.cc:290] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.

```"
34820,[Bug] InternalError: cudaGetDevice() failed. Status: cudaGetErrorString symbol not found. On Cuda 10.0.130,"**System information**
- OS Platform and Distribution: (Windows 10)
- TensorFlow installed from : Binary
- TensorFlow version: TF-GIT: v2.0.0-rc2-26-g64c3d382ca  TF : 2.0.0
- Python version: 3.6
- CUDA/cuDNN version: 10.0.130
- GPU model and memory:  GeForce Gtx 1070 


**Describe the current behavior**
similar to this issue just on CUDA 10.0.130:
#https://github.com/tensorflow/tensorflow/issues/32381
**Describe the expected behavior**


```
import tensorflow as tf
mnist = tf.keras.datasets.mnist
print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU')))
(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])


model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)
```

**Other info / logs**


```
Num GPUs Available:  1
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
<ipython-input-2-187cc8e8751d> in <module>
      9   tf.keras.layers.Dense(128, activation='relu'),
     10   tf.keras.layers.Dropout(0.2),
---> 11   tf.keras.layers.Dense(10, activation='softmax')
     12 ])
     13 

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\training\tracking\base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--> 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\keras\engine\sequential.py in __init__(self, layers, name)
    112       tf_utils.assert_no_legacy_layers(layers)
    113       for layer in layers:
--> 114         self.add(layer)
    115 
    116   @property

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\training\tracking\base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--> 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\keras\engine\sequential.py in add(self, layer)
    194       # If the model is being built continuously on top of an input layer:
    195       # refresh its output.
--> 196       output_tensor = layer(self.outputs[0])
    197       if len(nest.flatten(output_tensor)) != 1:
    198         raise TypeError('All layers in a Sequential model '

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py in __call__(self, inputs, *args, **kwargs)
    815           # Build layer if applicable (if the `build` method has been
    816           # overridden).
--> 817           self._maybe_build(inputs)
    818           cast_inputs = self._maybe_cast_inputs(inputs)
    819 

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py in _maybe_build(self, inputs)
   2139         # operations.
   2140         with tf_utils.maybe_init_scope(self):
-> 2141           self.build(input_shapes)
   2142       # We must set self.built since user defined build functions are not
   2143       # constrained to set self.built.

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\keras\layers\core.py in build(self, input_shape)
   1025         constraint=self.kernel_constraint,
   1026         dtype=self.dtype,
-> 1027         trainable=True)
   1028     if self.use_bias:
   1029       self.bias = self.add_weight(

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)
    520         collections=collections_arg,
    521         synchronization=synchronization,
--> 522         aggregation=aggregation)
    523     backend.track_variable(variable)
    524 

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\training\tracking\base.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)
    742         dtype=dtype,
    743         initializer=initializer,
--> 744         **kwargs_for_getter)
    745 
    746     # If we set an initializer and the variable processed it, tracking will not

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\keras\engine\base_layer_utils.py in make_variable(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)
    137       synchronization=synchronization,
    138       aggregation=aggregation,
--> 139       shape=variable_shape if variable_shape else None)
    140 
    141 

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\ops\variables.py in __call__(cls, *args, **kwargs)
    256   def __call__(cls, *args, **kwargs):
    257     if cls is VariableV1:
--> 258       return cls._variable_v1_call(*args, **kwargs)
    259     elif cls is Variable:
    260       return cls._variable_v2_call(*args, **kwargs)

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\ops\variables.py in _variable_v1_call(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)
    217         synchronization=synchronization,
    218         aggregation=aggregation,
--> 219         shape=shape)
    220 
    221   def _variable_v2_call(cls,

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\ops\variables.py in <lambda>(**kwargs)
    195                         shape=None):
    196     """"""Call on Variable class. Useful to force the signature.""""""
--> 197     previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
    198     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access
    199       previous_getter = _make_getter(getter, previous_getter)

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\ops\variable_scope.py in default_variable_creator(next_creator, **kwargs)
   2505         synchronization=synchronization,
   2506         aggregation=aggregation,
-> 2507         shape=shape)
   2508   else:
   2509     return variables.RefVariable(

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\ops\variables.py in __call__(cls, *args, **kwargs)
    260       return cls._variable_v2_call(*args, **kwargs)
    261     else:
--> 262       return super(VariableMetaclass, cls).__call__(*args, **kwargs)
    263 
    264 

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)
   1404           aggregation=aggregation,
   1405           shape=shape,
-> 1406           distribute_strategy=distribute_strategy)
   1407 
   1408   def _init_from_args(self,

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)
   1535           with ops.name_scope(""Initializer""), device_context_manager(None):
   1536             initial_value = ops.convert_to_tensor(
-> 1537                 initial_value() if init_from_fn else initial_value,
   1538                 name=""initial_value"", dtype=dtype)
   1539           if shape is not None:

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\keras\engine\base_layer_utils.py in <lambda>()
    117           (type(init_ops.Initializer), type(init_ops_v2.Initializer))):
    118         initializer = initializer()
--> 119       init_val = lambda: initializer(shape, dtype=dtype)
    120       variable_dtype = dtype.base_dtype
    121   if use_resource is None:

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\ops\init_ops_v2.py in __call__(self, shape, dtype)
    435     else:
    436       limit = math.sqrt(3.0 * scale)
--> 437       return self._random_generator.random_uniform(shape, -limit, limit, dtype)
    438 
    439   def get_config(self):

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\ops\init_ops_v2.py in random_uniform(self, shape, minval, maxval, dtype)
    798       op = random_ops.random_uniform
    799     return op(
--> 800         shape=shape, minval=minval, maxval=maxval, dtype=dtype, seed=self.seed)
    801 
    802   def truncated_normal(self, shape, mean, stddev, dtype):

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\ops\random_ops.py in random_uniform(shape, minval, maxval, dtype, seed, name)
    235     maxval = 1
    236   with ops.name_scope(name, ""random_uniform"", [shape, minval, maxval]) as name:
--> 237     shape = tensor_util.shape_tensor(shape)
    238     minval = ops.convert_to_tensor(minval, dtype=dtype, name=""min"")
    239     maxval = ops.convert_to_tensor(maxval, dtype=dtype, name=""max"")

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\framework\tensor_util.py in shape_tensor(shape)
    962       # not convertible to Tensors becasue of mixed content.
    963       shape = tuple(map(tensor_shape.dimension_value, shape))
--> 964   return ops.convert_to_tensor(shape, dtype=dtype, name=""shape"")
    965 
    966 

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\framework\ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)
   1182   preferred_dtype = deprecation.deprecated_argument_lookup(
   1183       ""dtype_hint"", dtype_hint, ""preferred_dtype"", preferred_dtype)
-> 1184   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
   1185 
   1186 

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\framework\ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)
   1240       name=name,
   1241       preferred_dtype=dtype_hint,
-> 1242       as_ref=False)
   1243 
   1244 

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\framework\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)
   1294 
   1295     if ret is None:
-> 1296       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1297 
   1298     if ret is NotImplemented:

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\framework\constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    284                                          as_ref=False):
    285   _ = as_ref
--> 286   return constant(v, dtype=dtype, name=name)
    287 
    288 

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\framework\constant_op.py in constant(value, dtype, shape, name)
    225   """"""
    226   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--> 227                         allow_broadcast=True)
    228 
    229 

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\framework\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    233   ctx = context.context()
    234   if ctx.executing_eagerly():
--> 235     t = convert_to_eager_tensor(value, ctx, dtype)
    236     if shape is None:
    237       return t

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\framework\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
     93     except AttributeError:
     94       dtype = dtypes.as_dtype(dtype).as_datatype_enum
---> 95   ctx.ensure_initialized()
     96   return ops.EagerTensor(value, ctx.device_name, dtype)
     97 

~\AppData\Local\Continuum\anaconda3\envs\tftestclean\lib\site-packages\tensorflow_core\python\eager\context.py in ensure_initialized(self)
    490         if self._default_is_async == ASYNC:
    491           pywrap_tensorflow.TFE_ContextOptionsSetAsync(opts, True)
--> 492         self._context_handle = pywrap_tensorflow.TFE_NewContext(opts)
    493       finally:
    494         pywrap_tensorflow.TFE_DeleteContextOptions(opts)

InternalError: cudaGetDevice() failed. Status: cudaGetErrorString symbol not found..
```
"
34819,Java: The result of  SavedModelBundle is different every time!,"The model was trained by python and was saved using tf.train.Saver.

I use the SavedModelBundle api to load and predict in java. However, the result is different every time. It seems some tensor was not loaded randomly. And in the same main programe，the result is same notever how many times I load the model. But in different programs, the result may be different. 

PS: model maybe partial load. Because some median result was right, but unstable.

python==1.14 java==1.14

My code:

```
//load model
            String modelDir = ""/data/model"";
            File file = new File(modelDir);
            SavedModelBundle savedModelBundle = null;
            if (file.exists() && file.isDirectory()) {
                savedModelBundle = SavedModelBundle.load(modelDir, ""serve"");
                System.out.println(""load succ"");
            } else {
                System.out.println(""load fail"");
            }
            Session session = savedModelBundle.session();
//feed
            ModelFeature modelFeature = new ModelFeature();
            buildModelFeature(modelFeature);
            Session.Runner tfrunner = session.runner();
            for (FeatureItem item : modelFeature.featureItems) {
                Tensor tensor;
                if (item.getValue().getClass().equals(String[][].class)) {
                    byte[][][] tmpString = new byte[1][1][1];
                    tmpString[0][0] = ((String)item.getValue()[0][0]).getBytes();
                    tensor = Tensor.create(tmpString, String.class);
                } else {
                    tensor = Tensor.create(item.getValue());
                }
                tfrunner = tfrunner.feed(item.getName(), tensor);
                System.out.println(""succ to create tensor name = "" + item.getName());
            }
//fetch
            //List<Tensor<?>> tensorList = tfrunner.fetch(output).run();
            List<Tensor<?>> tensorList = tfrunner.fetch(""concat_input_layer_1/concat:0"").run();
//print
            for (int ade = 0; ade < tensorList.size(); ade++) {
                Tensor out;
                System.out.println(""tensorList size is "" + tensorList.size());
                out = tensorList.get(ade);
                System.out.println(out);

                final long[] shape = out.shape();
                float[][] trans = new float[(int)shape[0]][(int)shape[1]];
                out.copyTo(trans);
                for (int i = 0; i < shape[0]; i++){
                    for (int j = 0; j < shape[1]; j++){
                        System.out.println(""final result = "" + trans[i][j]);
                    }
                }
                out.close();
            }}         
```"
34817,"YOLOBOX ops, YoloNMS ops","Use tf2.0 project: yolov3-tf2
I train the model, and want to covert it to .tflite file. Then I get error:
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, DIV, EXP, EXPAND_DIMS, FILL, LEAKY_RELU, LOGISTIC, MUL, PACK, PAD, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, SPLIT_V, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: CombinedNonMaxSuppression, Size.


The YoloBoxes and YoloNms ops is not supported."
34815,[bug] Lambda multiple-layers-different-shapes (ValueError: Dimensions must be equal),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux tfbug 4.9.0-11-amd64 #1 SMP Debian 4.9.189-3+deb9u2 (2019-11-11) x86_64 GNU/Linux
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A
- **TensorFlow installed from (source or binary)**:
binary
```bash
pip install --upgrade tf-nightly 
```
- **TensorFlow version (use command below)**:
v1.12.1-19580-gc397ed9 2.1.0-dev20191203
(also tried in 2.0.0-stable)
- **Python version**:
Python 3.5.3
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
```bash
python lambda_bug.py
```
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
When there are multiple layers wrapped in a Lambda, where the unit of the 1st layer is not the same as the inputs, error occurs (ValueError: Dimensions must be equal).
If there are multiple layers wrapped in a Lambda, where the unit of the 1st layer is the same as the inputs, no error occurs (refer to #bug#fine).
If there is only a single layer wrapped in a Lambda, no error occurs (refer to model_lambda_single).
If layer(s) are not wrapped in a Lambda, no error occurs (refer to model_function_,model_bare_).
As a prototype counterpart of subclassed layer, Lambda should be able to wrap multiple layers
Hence, it is convincing that Lambda is not relaying the shape correctly.


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

>SOURCE CODE: lambda_bug.py
import tensorflow as tf
from tensorflow.keras import Input, layers
D=123
C=4
#bare
def model_bare_single():
    model_in = Input((D,),name='model_in')
    model_out = layers.Dense(C,name='lyr_0')(model_in)
    model = tf.keras.Model(inputs=model_in,outputs=model_out,name='model_bare_single')
    return model
def model_bare_multiple():
    model_in = Input((D,),name='model_in')
    model_io = layers.Dense(C,name='lyr_0')(model_in)
    model_out = layers.Dense(C,name='lyr_1')(model_io)
    model = tf.keras.Model(inputs=model_in,outputs=model_out,name='model_bare_multiple')
    return model
#function
def function_single(ins):
    outs = layers.Dense(C,name='lyr_0')(ins)
    return outs
def model_function_single():
    model_in = Input((D,),name='model_in')
    model_out = function_single(model_in)
    model = tf.keras.Model(inputs=model_in,outputs=model_out,name='model_function_single')
    return model
def function_multiple(ins):
    ios = layers.Dense(C,name='lyr_0')(ins)
    outs = layers.Dense(C,name='lyr_1')(ios)
    return outs
def model_function_multiple():
    model_in = Input((D,),name='model_in')
    model_out = function_multiple(model_in)
    model = tf.keras.Model(inputs=model_in,outputs=model_out,name='model_function_multiple')
    return model
#lambda
def lambda_single(ins):
    outs = layers.Dense(C,name='lyr_0')(ins)
    return outs
def model_lambda_single():
    model_in = Input((D,),name='model_in')
    model_out = layers.Lambda(lambda_single,name='lambda_single')(model_in)
    model = tf.keras.Model(inputs=model_in,outputs=model_out,name='model_lambda_single')
    return model
def lambda_multiple(ins):
    ios = layers.Dense(C,name='lyr_0',input_shape=(D,))(ins)#bug
    #ios = layers.Dense(D,name='lyr_0')(ins)#fine
    outs = layers.Dense(C,name='lyr_1')(ios)
    return outs
def model_lambda_multiple():#bug
    model_in = Input((D,),name='model_in')
    model_out = layers.Lambda(lambda_multiple,name='lambda_multiple',output_shape=(C,))(model_in)
    model = tf.keras.Model(inputs=model_in,outputs=model_out,name='model_lambda_multiple')
    return model
def main():
    model_bare_single().summary()
    model_bare_multiple().summary()
    model_function_single().summary()
    model_function_multiple().summary()
    model_lambda_single().summary()
    model_lambda_multiple().summary()
    print(tf.__version__)
    return
if __name__ == '__main__':
    main()

>LOGS
...
Traceback (most recent call last):
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py"", line 1619, in _creat
e_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 4 and 123 for 'lamb
da_multiple/lyr_1/MatMul' (op: 'MatMul') with input shapes: [?,4], [123,4].
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""lambda_bug.py"", line 69, in <module>
    main()
  File ""lambda_bug.py"", line 65, in main
    model_lambda_multiple().summary()
  File ""lambda_bug.py"", line 56, in model_lambda_multiple
    model_out = layers.Lambda(lambda_multiple,name='lambda_multiple',output_shape=(C,))(model_in)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 773, 
in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/keras/layers/core.py"", line 827, in cal
l
    return self.function(inputs, **arguments)
  File ""lambda_bug.py"", line 52, in lambda_multiple
    outs = layers.Dense(C,name='lyr_1')(ios)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 773, 
in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/keras/layers/core.py"", line 1089, in ca
ll
    outputs = gen_math_ops.mat_mul(inputs, self.kernel)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/ops/gen_math_ops.py"", line 5626, in mat
_mul
    name=name)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 742,
 in _apply_op_helper
    attrs=attr_protos, op_def=op_def)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/framework/func_graph.py"", line 595, in 
_create_op_internal
    compute_device)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py"", line 3314, in _creat
e_op_internal
    op_def=op_def)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py"", line 1786, in __init
__
    control_input_ops)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py"", line 1622, in _creat
e_c_op
    raise ValueError(str(e))
ValueError: Dimensions must be equal, but are 4 and 123 for 'lambda_multiple/lyr_1/MatMul' (op: 'MatMul') with inpu
t shapes: [?,4], [123,4]."
34814,"tflite load model failed use static library, how to get the error info, now only ok or error code?","i have a tflite model with two type float and float16, run with gpu(use c++ interface). float run ok and both model run ok use python, but float16 failed when use c++ code at code: auto iRetCode = m_pDeeplabInterp->ModifyGraphWithDelegate(m_pGPUDelegate); but this only return one error code 1, but no any other information, i can't find any way to do more thing to check whether is model has ops not support by tflite or my convert pipeline has error."
34813,gru convert tflite err(KeyError: 'kernel'),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10, notebook(anconda)
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source):
tensorflow2.0.0 (use pip install)


**Command used to run the converter or code if you’re using the Python API**
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert() # where is wrong

 Copy and paste here the exact command
```python
input_tensor = tf.keras.Input(shape=(28, 28))
x = tf.keras.layers.GRU(154)(input_tensor)
x = tf.keras.layers.Flatten()(x)
out_tensor = tf.keras.layers.Dense(10, activation=""relu"")(x)
model = tf.keras.Model(input_tensor, out_tensor)
model.summary()

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert() # where is wrong
open(""converted_keras_model.tflite"", ""wb"").write(tflite_model)
```
**The output from the converter invocation**
KeyError: 'kernel'
```
Copy and paste the output here.
KeyError                                  Traceback (most recent call last)
<ipython-input-7-b14fd83e63c7> in <module>()
     13 
     14 converter = tf.lite.TFLiteConverter.from_keras_model(model)
---> 15 tflite_model = converter.convert()
     16 open(""converted_keras_model.tflite"", ""wb"").write(tflite_model)

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\lite\python\lite.py in convert(self)
    403 
    404     frozen_func = _convert_to_constants.convert_variables_to_constants_v2(
--> 405         self._funcs[0], lower_control_flow=False)
    406     input_tensors = [
    407         tensor for tensor in frozen_func.inputs

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\framework\convert_to_constants.py in convert_variables_to_constants_v2(func, lower_control_flow)
    412 
    413   # Get mapping from function name to argument types.
--> 414   function_data = _get_control_flow_function_data(node_defs, tensor_data)
    415 
    416   # Get variable data for all nodes in `node_defs`.

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\framework\convert_to_constants.py in _get_control_flow_function_data(node_defs, tensor_data)
    260         if arg_types[idx] == dtypes.resource:
    261           input_name = node.input[idx]
--> 262           arg_types[idx] = get_resource_type(input_name)
    263           output_shapes[idx] = get_resource_shape(input_name)
    264 

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\framework\convert_to_constants.py in get_resource_type(node_name)
    226 
    227   def get_resource_type(node_name):
--> 228     numpy_type = tensor_data[node_name][""data""].dtype
    229     return dtypes.as_dtype(numpy_type).as_datatype_enum
    230 

KeyError: 'kernel'



**Failure details**
conversion is  wrong.



**Any other info / logs**

"
34812,The checkpoint generated by tensorflow2.0 training does not have a meta file. How should it be converted to a pb file?,"When T1, the meta file is converted to a pb file, and then the pb file is converted to a tflite file. Now T2 does not generate a meta file, what should I do? Can anyone help me?"
34810,Using Dataset API gives different results than passing in data directly,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I started from a stock Keras example on https://keras.io/examples/mnist_siamese/ and changed the `from keras` to from `tensorflow.keras`   
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): NGC Tensorflow 19.11 docker Image (Also happens on pip install tensorflow-gpu)
- TensorFlow version (use command below): 2.0.0 and 1.14
- Python version: 3.6.8
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.2 and CUDA 10.1 / cudnn 7.6.5 and cudnn 7.6.3
- GPU model and memory: Tesla V100

**Describe the current behavior**
When I run the example on https://keras.io/examples/mnist_siamese/ and changed the `from keras` to from `tensorflow.keras` it executes exactly how you'd expect and trains up to ~98% for training and validation. The fit portion of code passes in the data directly via 

```
model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,
          batch_size=128,
          epochs=epochs,
          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))
```

If I instead create a dataset using the following 

```
tr_ds = tf.data.Dataset.from_tensor_slices(((tr_pairs[:, 0], tr_pairs[:, 1]), tr_y))
tr_ds = tr_ds.batch(128)
te_ds = tf.data.Dataset.from_tensor_slices(((te_pairs[:, 0], te_pairs[:, 1]), te_y))
te_ds = te_ds.batch(128)
```

and then train by passing in the datasets into the model.fit. The training and validation loss and accuracy behave significantly worse.  

**Describe the expected behavior**

I'd expect the the Dataset API to give the same result as passing in the data directly.

**Code to reproduce the issue**

1. Copy code from example into a notebook or other python environment.
2. Change `import keras` calls to `import tensorflow.keras`
    2.1 If using TF 2.0 change tr_y and te_y to floats by adding `.astype(np.float32)` to the values in model.fit. If you don't do this you will get `TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int64 of argument 'x'.` 
3. Run to see expected good results
4. Before the `rms = RMSProp()` line add the following
```
tr_ds = tf.data.Dataset.from_tensor_slices(((tr_pairs[:, 0], tr_pairs[:, 1]), tr_y))
tr_ds = tr_ds.batch(128)
te_ds = tf.data.Dataset.from_tensor_slices(((te_pairs[:, 0], te_pairs[:, 1]), te_y))
te_ds = te_ds.batch(128)
```
5. Comment out the included `model.fit` and replace it with 
```
model.fit(tr_ds,
          epochs=epochs,
          validation_data=te_ds
     )
```
6. Train and watch val_loss and val_accuracy not behave as expected.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I've attached a txt file of the source I used to exhibit this behavior (it wouldn't let me upload the file as a .py)
[pairs_mnist.txt](https://github.com/tensorflow/tensorflow/files/3918658/pairs_mnist.txt)"
34809,Typo in tf.keras.layers.Attention docs example,"## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention

## Description of issue (what needs changing):
In the code example, it currently reads:
```python
# Variable-length int sequences.
query_input = tf.keras.Input(shape=(None,), dtype='int32')
value_input = tf.keras.Input(shape=(None,), dtype='int32')

# Embedding lookup.
token_embedding = tf.keras.layers.Embedding(max_tokens, dimension)
# Query embeddings of shape [batch_size, Tq, dimension].
query_embeddings = token_embedding(query_input)
# Value embeddings of shape [batch_size, Tv, dimension].
value_embeddings = token_embedding(query_input)
```

The last line should instead be:
```
value_embeddings = token_embedding(value_input)
```"
34808,grpc+verbs not being used,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N.A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.15
- Python version: 3.5.2, 3.6.8 (different machines)
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): GCC 7.4
- CUDA/cuDNN version: NA
- GPU model and memory: NA



**Describe the problem**
Running code with tf.estimator.RunConfig(...., protocol=""grpc+verbs"") does not throw any error, but does not appear to use RDMA (logs do not show any mention of RDMA). Running the code without this protocol (i.e just on gRPC) appears to work fine, which leads me to think that it's a build / installation issue. We also observe no speed-up by using RDMA, which suggests that RDMA is not being used. We are using a 1x parameter server, 1x chief, 1x evaluator, 15x worker setup (Parameter server, chief, and evaluator are on the same node and share the same filesystem; the workers are on separate machines and do not share the same file system).

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I followed the guide [here](https://www.tensorflow.org/install/source) to compile from source. However, running the ""./configure"" step does **not** give an option to install with verbs support. Hence, I built Tensorflow with the following command:
`bazel build --config=verbs //tensorflow/tools/pip_package:build_pip_package`. Everything else followed the given guide.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Sample parameter server logs:
`2019-12-03 13:26:22.885842: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
2019-12-03 13:26:22.899183: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job chief -> {0 -> 192.168.9.20:1235}
2019-12-03 13:26:22.899214: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job ps -> {0 -> localhost:1236}
2019-12-03 13:26:22.899232: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -> {0 -> 192.168.9.16:1234, 1 -> 192.168.9.17:1234, 2 -> 192.168.9.18:1234, 3 -> 192.168.9.23:1234, 4 -> 192.168.9.24:1234, 5 -> 192.168.9.25:1234, 6 -> 192.168.9.26:1234, 7 -> 192.168.9.27:1234, 8 -> 192.168.9.28:1234, 9 -> 192.168.9.29:1234, 10 -> 192.168.9.30:1234, 11 -> 192.168.9.31:1234, 12 -> 192.168.9.32:1234, 13 -> 192.168.9.105:1234, 14 -> 192.168.9.106:1234}
2019-12-03 13:26:22.903725: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:1236
2019-12-03 13:26:22.903796: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:369] Server already started (target: grpc://localhost:1236)
2019-12-03 13:26:28.930441: I tensorflow/core/distributed_runtime/worker.cc:203] Cancellation requested for RunGraph.
2019-12-03 13:26:29.073805: I tensorflow/core/distributed_runtime/worker.cc:203] Cancellation requested for RunGraph.
2019-12-03 13:26:29.308595: I tensorflow/core/distributed_runtime/worker.cc:203] Cancellation requested for RunGraph.
2019-12-03 13:26:29.609207: I tensorflow/core/distributed_runtime/worker.cc:203] Cancellation requested for RunGraph.
2019-12-03 13:26:29.994561: I tensorflow/core/distributed_runtime/worker.cc:203] Cancellation requested for RunGraph.
2019-12-03 13:26:30.430739: I tensorflow/core/distributed_runtime/worker.cc:203] Cancellation requested for RunGraph.
2019-12-03 13:26:30.971334: I tensorflow/core/distributed_runtime/worker.cc:203] Cancellation requested for RunGraph.
2019-12-03 13:26:59.046948: I tensorflow/core/distributed_runtime/worker.cc:203] Cancellation requested for RunGraph.
2019-12-03 13:26:59.278155: I tensorflow/core/distributed_runtime/worker.cc:203] Cancellation requested for RunGraph.`"
34806,build tensorflow/python.tools:optimize_for_inference failed,"\
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14
- Python version: 3.7
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.26.0
- GCC/Compiler version (if compiling from source):6.4.0
- CUDA/cuDNN version:  cuda 10 cudnn 7 
- GPU model and memory: p40


i was trying to optimize tensorflow on cpu, that's why i followed a website to use tool: optimize_for_inference to decrease the size of my pb model.  I cloned tensorflow, get into the tensorflow directory,  used  
bazel build tensorflow/python.tools:optimize_for_inference to build optimize_for_inference script
.but i got some error like :
![image](https://user-images.githubusercontent.com/51428350/70078703-0927f200-15b8-11ea-9917-18f9b2ddf6b6.png)

![image](https://user-images.githubusercontent.com/51428350/70078751-20ff7600-15b8-11ea-94e6-a4a0dc874b32.png)

i am not sure what these problems are, could anyone help me out? thanks so much

"
34805,Please add Tested build configurations for Tensorflow 1.15 and Tensorflow 1.15-gpu,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue: N/A

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/install/source

## Description of issue (what needs changing): Please add Tested build configurations for Tensorflow 1.15 and Tensorflow 1.15-gpu

### Clear description

For example, why should someone use this method? How is it useful?
I cannot find the test build configurations for Tensorflow 1.15. It is useful for someone who is trying to build Tensorflow 1.15. 



"
34804,Object detection evaluation done on only a subset of test images,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Google Colab**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **Default installation on Google Colab**
- TensorFlow version (use command below): **v1.15.0-0-g590d6eef7e 1.15.0**
- Python version: **Python 3.6.8**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **Google Colab**

**Describe the current behavior**
Evaluation is done on precisely 496 images
On every evaluation i get this:
    
    I1129 04:00:20.238537 139729717991168 coco_evaluation.py:205] Performing evaluation on 496 images.

**Describe the expected behavior**
Evaluation is done on 1000 images

**Code to reproduce the issue**
Training ssd_mobilenet_v2, using object_detection/model_main.py running the following:

    !python3 object_detection/model_main.py --pipeline_config_path=pipeline.config --model_dir=ssd_mobilenet/ --num_train_steps=200000 --sample_1_of_n_eval_examples=1 --alsologtostderr

Inside pipeline.config:
    
    num_examples: 1000

Also inside test images directory there is precisely 1000 annotated images.
I did not specify the number 496 anywhere and it seem to be changing according to how many images there actually is inside test directory.

"
34801,Can not run tensorflow gpu 2.0 with cuda 10.0 on arm64 boards (jetson nano),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution: Linux Ubuntu 18.04.3 arm64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0.0
- Python version: Python 3.6.8
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: cuda=10.0.326-1/ cudnn=7.5.0.56-1+cuda10.0
- GPU model and memory: nvidia jetson nano , 4GB of memory

**Describe the current behavior**
Can not run code on gpu as it stuck at libcublas then crashes
```
>>> tf.debugging.set_log_device_placement(True)
>>> a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
2019-12-03 15:14:11.076714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero
2019-12-03 15:14:11.077411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: NVIDIA Tegra X1 major: 5 minor: 3 memoryClockRate(GHz): 0.9216
pciBusID: 0000:00:00.0
2019-12-03 15:14:11.077849: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-12-03 15:14:11.078090: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-12-03 15:14:11.078270: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-12-03 15:14:11.078453: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-12-03 15:14:11.078617: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-12-03 15:14:11.078793: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-12-03 15:14:11.079122: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-12-03 15:14:11.079542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero
2019-12-03 15:14:11.080004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero
2019-12-03 15:14:11.080247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-03 15:14:11.083934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero
2019-12-03 15:14:11.084130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: NVIDIA Tegra X1 major: 5 minor: 3 memoryClockRate(GHz): 0.9216
pciBusID: 0000:00:00.0
2019-12-03 15:14:11.084244: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-12-03 15:14:11.084317: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-12-03 15:14:11.084378: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-12-03 15:14:11.084443: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-12-03 15:14:11.084501: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-12-03 15:14:11.084567: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-12-03 15:14:11.084623: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-12-03 15:14:11.084836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero
2019-12-03 15:14:11.085143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero
2019-12-03 15:14:11.085284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-03 15:14:11.085383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-03 15:14:11.085441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-12-03 15:14:11.085484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-12-03 15:14:11.085724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero
2019-12-03 15:14:11.086047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero
2019-12-03 15:14:11.086225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2270 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X1, pci bus id: 0000:00:00.0, compute capability: 5.3)
>>> b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
>>> c = tf.matmul(a, b)
2019-12-03 15:14:16.993486: I tensorflow/core/common_runtime/eager/execute.cc:574] Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0
2019-12-03 15:14:17.047313: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-12-03 15:18:10.046885: F tensorflow/stream_executor/cuda/cuda_driver.cc:175] Check failed: err == cudaSuccess || err == cudaErrorInvalidValue Unexpected CUDA error: unknown error
Aborted

```

**Describe the expected behavior**
It should calculate the value of the tensor C.

**Code to reproduce the issue**
Install tensorflow 2.0 on jetson nano
I provide the python wheel here in my post here
https://pythops.com/post/compile-deeplearning-libraries-for-jetson-nano
then run this
```
a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
c = tf.matmul(a, b)
print(c)
```"
34799,tf custom model serialization issue,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Debian
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary using pip install tensorflow
- TensorFlow version (use command below):  v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: 3.7
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory:  NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Reconstructing custom model from json produces the following error

  File ""/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 1841, in reconstruct_from_config
    for layer_data in config['layers']:
KeyError: 'layers'

**Describe the expected behavior**

model should load without issues as this is a pretty trivial case

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
from tensorflow import keras
import numpy as np 

class TM(keras.Model):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def build(self, input_shape):
        self.lm = self.add_weight(shape=[2,5], initializer=""uniform"", name=""wts"")
        super().build(input_shape)

    def call(self, x):
        return x @ self.lm

    def get_config(self):
        return {}        

# serialize arch to json and weights to h5
x = np.random.rand(5,2)
m = TM()
y = m(x) #build
json_config = m.to_json()

# error on loading back
m2 = keras.models.model_from_json(json_config, custom_objects= {""TM"": TM})
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

  File ""training/test_model_class.py"", line 25, in <module>
    m2 = keras.models.model_from_json(json_config, custom_objects= {""TM"": TM})
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/model_config.py"", line 96, in model_from_json
    return deserialize(config, custom_objects=custom_objects)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/serialization.py"", line 102, in deserialize
    printable_module_name='layer')
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/generic_utils.py"", line 191, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 906, in from_config
    config, custom_objects)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 1841, in reconstruct_from_config
    for layer_data in config['layers']:
KeyError: 'layers'"
34796,bazel build tensorflow lite with with_select_tf_ops Error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:r1.14
- Bazel version (if compiling from source):0.24.1



**Describe the problem**

i set a crosstool for bazel to build tensorflow lite, it work well when build `libtensorflowlite.so`.
but when i add the tensorflow ops delegate library dependency (`tensorflow/lite/delegates/flex:delegate`) to the build dependencies, i got an error:

```
ERROR:/home/wang/.cache/bazel/_bazel_wang/4b298145d4306eeb9a64ca7837a4593b/external/nasm/BUILD.bazel:8:1: C++ compilation of rule '@nasm//:nasm' failed (Exit 1): gcc failed: error executing command 
  (cd /home/wang/.cache/bazel/_bazel_wang/4b298145d4306eeb9a64ca7837a4593b/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/wang/.local/bin:/home/wang/bin:/home/wang/android-ndk-r17c:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/wang/.local/bin:/home/wang/bin:/usr/ARM/toolchain/gcc-linaro-7.4.1-2019.02-x86_64_aarch64-linux-gnu/bin \
    PWD=/proc/self/cwd \
  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=2' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -isystem /usr/include -g0 -O3 -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/host/bin/external/nasm/_objs/nasm/assemble.d '-frandom-seed=bazel-out/host/bin/external/nasm/_objs/nasm/assemble.o' -DHAVE_SNPRINTF -DHAVE_SYS_TYPES_H -iquote external/nasm -iquote bazel-out/host/genfiles/external/nasm -iquote bazel-out/host/bin/external/nasm -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools -isystem external/nasm/asm -isystem bazel-out/host/genfiles/external/nasm/asm -isystem bazel-out/host/bin/external/nasm/asm -isystem external/nasm/include -isystem bazel-out/host/genfiles/external/nasm/include -isystem bazel-out/host/bin/external/nasm/include -isystem external/nasm/output -isystem bazel-out/host/genfiles/external/nasm/output -isystem bazel-out/host/bin/external/nasm/output -isystem external/nasm/x86 -isystem bazel-out/host/genfiles/external/nasm/x86 -isystem bazel-out/host/bin/external/nasm/x86 -g0 '-march=native' -w '-std=c99' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nasm/asm/assemble.c -o bazel-out/host/bin/external/nasm/_objs/nasm/assemble.o)
Execution platform: @bazel_tools//platforms:host_platform
external/nasm/asm/assemble.c: In function 'assert_no_prefix':
external/nasm/asm/assemble.c:265:20: error: 'ERR_NONFATAL' undeclared (first use in this function)
         nasm_error(ERR_NONFATAL, ""invalid %s prefix"",
                    ^
external/nasm/asm/assemble.c:265:20: note: each undeclared identifier is reported only once for each function it appears in
external/nasm/asm/assemble.c: In function 'warn_overflow':
external/nasm/asm/assemble.c:295:16: error: 'ERR_WARNING' undeclared (first use in this function)
     nasm_error(ERR_WARNING | ERR_PASS2 | ERR_WARN_NOV,
                ^
external/nasm/asm/assemble.c:295:30: error: 'ERR_PASS2' undeclared (first use in this function)
     nasm_error(ERR_WARNING | ERR_PASS2 | ERR_WARN_NOV,
                              ^
external/nasm/asm/assemble.c:295:42: error: 'ERR_WARN_NOV' undeclared (first use in this function)
     nasm_error(ERR_WARNING | ERR_PASS2 | ERR_WARN_NOV,
                                          ^
external/nasm/asm/assemble.c: In function 'out':
external/nasm/asm/assemble.c:402:24: error: 'ERR_NONFATAL' undeclared (first use in this function)
             nasm_error(ERR_NONFATAL,
                        ^
external/nasm/asm/assemble.c:406:24: error: 'ERR_WARNING' undeclared (first use in this function)
             nasm_error(ERR_WARNING | ERR_WARN_ZEXTRELOC,
                        ^
external/nasm/asm/assemble.c:406:38: error: 'ERR_WARN_ZEXTRELOC' undeclared (first use in this function)
             nasm_error(ERR_WARNING | ERR_WARN_ZEXTRELOC,
                                      ^
external/nasm/asm/assemble.c: In function 'out_reladdr':
external/nasm/asm/assemble.c:473:20: error: 'ERR_NONFATAL' undeclared (first use in this function)
         nasm_error(ERR_NONFATAL, ""invalid use of self-relative expression"");
                    ^
external/nasm/asm/assemble.c: In function 'jmp_match':
external/nasm/asm/assemble.c:527:20: error: 'ERR_WARNING' undeclared (first use in this function)
         nasm_error(ERR_WARNING | ERR_WARN_BND | ERR_PASS2 ,
                    ^
external/nasm/asm/assemble.c:527:34: error: 'ERR_WARN_BND' undeclared (first use in this function)
         nasm_error(ERR_WARNING | ERR_WARN_BND | ERR_PASS2 ,
```
what should i do?
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
34795,AttributeError: 'ImageDataGenerator' object has no attribute 'shape',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): KDE neon, based on Ubuntu 18.04 LTS
- TensorFlow installed from (source or binary): via Anaconda 3
- TensorFlow version (use command below): 2.0.0 
- Python version: 3.7.5

**Anaconda environment**
Name                    Version                   Build  Channel
_libgcc_mutex             0.1                        main  
_tflow_select             2.3.0                       mkl  
absl-py                   0.8.1                    py37_0  
astor                     0.8.0                    py37_0  
blas                      1.0                         mkl  
c-ares                    1.15.0            h7b6447c_1001  
ca-certificates           2019.10.16                    0  
certifi                   2019.9.11                py37_0  
gast                      0.2.2                    py37_0  
google-pasta              0.1.8                      py_0  
grpcio                    1.16.1           py37hf8bcb03_1  
h5py                      2.9.0            py37h7918eee_0  
hdf5                      1.10.4               hb1b8bf9_0  
intel-openmp              2019.4                      243  
keras-applications        1.0.8                      py_0  
keras-preprocessing       1.1.0                      py_1  
libedit                   3.1.20181209         hc058e9b_0  
libffi                    3.2.1                hd88cf55_4  
libgcc-ng                 9.1.0                hdf63c60_0  
libgfortran-ng            7.3.0                hdf63c60_0  
libprotobuf               3.10.1               hd408876_0  
libstdcxx-ng              9.1.0                hdf63c60_0  
markdown                  3.1.1                    py37_0  
mkl                       2019.4                      243  
mkl-service               2.3.0            py37he904b0f_0  
mkl_fft                   1.0.15           py37ha843d7b_0  
mkl_random                1.1.0            py37hd6b4f25_0  
ncurses                   6.1                  he6710b0_1  
numpy                     1.17.3           py37hd14ec0e_0  
numpy-base                1.17.3           py37hde5b4d6_0  
openssl                   1.1.1d               h7b6447c_3  
opt_einsum                3.1.0                      py_0  
pip                       19.3.1                   py37_0  
protobuf                  3.10.1           py37he6710b0_0  
python                    3.7.5                h0371630_0  
readline                  7.0                  h7b6447c_5  
scipy                     1.3.1            py37h7c811a0_0  
setuptools                41.6.0                   py37_0  
six                       1.13.0                   py37_0  
sqlite                    3.30.1               h7b6447c_0  
tensorboard               2.0.0              pyhb230dea_0  
tensorflow                2.0.0           mkl_py37h66b46cc_0  
tensorflow-base           2.0.0           mkl_py37h9204916_0  
tensorflow-estimator      2.0.0              pyh2649769_0  
tensorflow-mkl            2.0.0                h4fcabd2_0  
termcolor                 1.1.0                    py37_1  
tk                        8.6.8                hbc83047_0  
webencodings              0.5.1                    py37_1  
werkzeug                  0.16.0                     py_0  
wheel                     0.33.6                   py37_0  
wrapt                     1.11.2           py37h7b6447c_0  
xz                        5.2.4                h14c3975_4  
zlib                      1.2.11               h7b6447c_3

**Describe the current behavior**
Using only Keras methods I get the error:
> AttributeError: 'ImageDataGenerator' object has no attribute 'shape'

This seems to indicate that Keras code is expecting other data scructures in an object that it created itself.

**Describe the expected behavior**

No error message.

**Code to reproduce the issue**
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import MaxPooling2D, Conv2D, Dense
from tensorflow.keras.preprocessing.image import ImageDataGenerator

model = Sequential([
    Conv2D(filters=32,
           input_shape=(10,10,3),
           kernel_size=(4,4),
           activation='relu'),

    MaxPooling2D(pool_size=(2, 2)),


    Dense(5),
])

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

imageGenerator = ImageDataGenerator()

imageGenerator.flow_from_directory(
    directory='training data',
    target_size=(10, 10),
    batch_size=32,
    class_mode='categorical',
)

model.fit_generator(imageGenerator, steps_per_epoch=55)
```

The folder *training data* contains three folders each containing JPG images for the respective class.

**Output**
```
/home/user/anaconda3/envs/project/bin/python3.7 /home/user/.PyCharm2019.3/config/scratches/scratch_2.py
2019-12-03 14:11:23.807238: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-12-03 14:11:23.834757: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2593900000 Hz
2019-12-03 14:11:23.835094: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558c62e752d0 executing computations on platform Host. Devices:
2019-12-03 14:11:23.835114: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-12-03 14:11:23.835399: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Found 6126 images belonging to 3 classes.
Traceback (most recent call last):
  File ""/home/user/.PyCharm2019.3/config/scratches/scratch_2.py"", line 31, in <module>
    model.fit_generator(imageGenerator, steps_per_epoch=55)
  File ""/home/user/anaconda3/envs/project/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 1297, in fit_generator
    steps_name='steps_per_epoch')
  File ""/home/user/anaconda3/envs/project/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py"", line 144, in model_iteration
    shuffle=shuffle)
  File ""/home/user/anaconda3/envs/project/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py"", line 477, in convert_to_generator_like
    num_samples = int(nest.flatten(data)[0].shape[0])
AttributeError: 'ImageDataGenerator' object has no attribute 'shape'

Process finished with exit code 1
```
"
34794,@local_config_cc//:toolchain' does not contain a toolchain for cpu 'ios_x86_64',"I am trying to import mediapipe project using Bazel. But when try to sync the project by clicking on bazel->sync->sync bazel  I get following issues. I am using ubuntu LTS 18 version and bazel version is 0.29.1. I am new to mediapipe so please provide detail answer for the issue.
   
     Error:Error:line: 47in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite 
     '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'ios_x86_64'

    Error:Error:While resolving toolchains for target @bazel_tools//tools/objc:dummy_lib: no 
    matching toolchains found for types @bazel_tools//tools/cpp:toolchain_type 
  "
34793,Sequence pad support for Ragged Tensor when call to_tensor,"**System information**
- TensorFlow version (you are using): 2.0.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Right now, we can get a dense tensor by calling `to_tensor` of a RaggedTensor and the num_cols of the dense tensor is the max length of the tensors in the RaggedTensor. However, the max length may vary if we get the RaggedTensor by `tf.string.splits` for different batch data. So, we can not use the dense tensor for `tf.keras.layers.Embedding` and `tf.keras.layers.Dense` layers because the num_cols is not fixed.

**Will this change the current api? How?**
In order to support padding for `to_tensor`, we need add three arguments for `to_tensor` API, like:
```python
def to_tensor(self, default_value=None, name=None, 
    maxlen=None, padding='pre', truncating='pre'):
```
maxlen: Int, maximum length of all sequences where the sequence is a tensor in the RaggedTensor .
padding: String, 'pre' or 'post':
    pad either before or after each sequence.
truncating: String, 'pre' or 'post':
    remove values from sequences larger than
    `maxlen`, either at the beginning or at the end of the sequences.

Those arguments are the same as `tf.keras.preprocessing.sequence.pad_sequences` in https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences

**Who will benefit with this feature?**
In NLP and time series modeling, the format of feature is the string with many values separated by a separator like ""19,39,94,30"" or ""there, is, a, dog"" and the number of values of different samples is different. In this case, we need to use `tf.strings.split()` to get a RaggedTensor and convert it to a dense tensor with fixed num_cols.

**Any Other info.**
Here is a demo with maxlen.
```python
import numpy as np

from tensorflow.python import tf2
from tensorflow.python.client import session
from tensorflow.python.framework import composite_tensor
from tensorflow.python.framework import constant_op
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import ops
from tensorflow.python.framework import sparse_tensor
from tensorflow.python.framework import tensor_shape
from tensorflow.python.framework import tensor_spec
from tensorflow.python.framework import tensor_util
from tensorflow.python.framework import type_spec
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import check_ops
from tensorflow.python.ops import control_flow_ops
from tensorflow.python.ops import gen_ragged_conversion_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.ops.ragged import ragged_config
from tensorflow.python.ops.ragged import ragged_tensor_value
from tensorflow.python.ops.ragged import ragged_util
from tensorflow.python.ops.ragged import segment_id_ops
from tensorflow.python.util.tf_export import tf_export

def is_ragged(value):
  """"""Returns true if `value` is a ragged tensor or ragged tensor value.""""""
  return isinstance(value,
                    (tf.RaggedTensor, ragged_tensor_value.RaggedTensorValue))

def to_tensor(ragged_tensor, default_value=None, name=None, maxlen=None):
    self = ragged_tensor
    print(""self : "", self)
    with ops.name_scope(name, ""RaggedToTensor"", [self, default_value]):
      if default_value is not None:
        default_value = ops.convert_to_tensor(
            default_value, name=""default_value"", dtype=self.dtype)

      # If ragged_rank > 1, then recursively convert the ragged values into a
      # `Tensor` before we proceed.
      values = self.values
      if is_ragged(values):
        values = values.to_tensor(default_value)

      # Tile the default value, if necessary.
      if default_value is not None:
        if values.shape.ndims is not None:
          default_value.shape.with_rank_at_most(values.shape.ndims - 1)
        if (values.shape.ndims is None or default_value.shape.ndims is None or
            values.shape.ndims != default_value.shape.ndims + 1):
          value_shape = array_ops.shape(values)[1:]
          default_value = array_ops.broadcast_to(default_value, value_shape)
        default_value.shape.assert_is_compatible_with(values.shape[1:])

      # Get the expected dense shape ([nrows, ncols] + value_shape).
      rt_row_lengths = [self.row_splits[1:] - self.row_splits[:-1]]
      print('rt_row_lengths : ', rt_row_lengths)
      nrows = array_ops.shape(self.row_splits,
                              out_type=self._row_splits.dtype)[0] - 1
      ncols = math_ops.maximum(math_ops.reduce_max(rt_row_lengths), 0) 
      if maxlen is not None:
        ncols = tf.constant(maxlen, dtype=tf.int64)
        
      values_shape = array_ops.shape(values, out_type=self._row_splits.dtype)
      value_shape = values_shape[1:]
      nvals = values_shape[0]

      # Build a default value if none was supplied.
      if default_value is None:
        default_value = array_ops.zeros(value_shape, dtype=values.dtype)
      default_value.shape.assert_is_compatible_with(values.shape[1:])
      default_value.set_shape(values.shape[1:])

      # Get the row start indices, and expand to shape=[nrows, 1].
      starts = array_ops.expand_dims(self.row_splits[:-1], 1)

      # Get the row limit indices, and expand to shape=[nrows, 1].
      limits = array_ops.expand_dims(self.row_splits[1:], 1)

      # Get the column indices, and expand to shape=[1, ncols].
      columns = array_ops.expand_dims(math_ops.range(0, ncols), 0)

      # Build a list containing the values plus the default value.  We will use
      # tf.gather to collect values from this list for the `Tensor` (using
      # nvals as the index for the default value).
      values_and_default = array_ops.concat(
          [values, array_ops.stack([default_value])], axis=0)

      # Construct a matrix ""indices"" pointing into values_and_default.  I.e.,
      # output[r, c] = values_and_default[indices[r, c].
      nondefault_index = starts + columns
      has_value = nondefault_index < limits
      default_index = array_ops.fill(array_ops.stack([nrows, ncols]), nvals)
        
      indices = array_ops.where(has_value, nondefault_index, default_index)
      tensor = array_ops.gather(values_and_default, indices)

      # Gather the results into a `Tensor`.
      return tensor
    
rt = tf.ragged.constant([[9, 8, 7], [], [6, 5], [4]])
print(to_tensor(rt, maxlen=10))
```
"
34791,variable scope is changed by force when reopened (with regard to use Adam) (with new example),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): 1.12
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: 1080ti

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
The same issue with #30813

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Please compare the results of following codes

code 1:
```
import tensorflow as tf
class generator:
    def __init__(self):
        with tf.variable_scope('myscope'):
            self.a = tf.get_variable('a', [4,3,2], tf.float32)
            self.b = tf.get_variable('b', [4,3,2], tf.float32)
            self.c = tf.reduce_sum(self.a + self.b)
            self.d = tf.train.AdamOptimizer(name = 'adamopt')
            self.e = self.d.compute_gradients(self.c, tf.trainable_variables())
            self.f = self.d.apply_gradients(self.e)
        for item in tf.global_variables():
            if 'beta' in item.name:
                print item
    
    def compute(self):
        with tf.variable_scope('myscope'):
            self.e1 = self.d.compute_gradients(self.c, tf.trainable_variables())
            self.f1 = self.d.apply_gradients(self.e1)
        print '------------------------'
        for item in tf.global_variables():
            if 'beta' in item.name:
                print item
    
    def compute1(self):
        with tf.variable_scope('myscope'):
            self.e2 = self.d.compute_gradients(self.c, tf.trainable_variables())
            self.f2 = self.d.apply_gradients(self.e2)
        print '---------------------------'
        for item in tf.global_variables():
            if 'beta' in item.name:
                print item

g = generator()
g.compute()
g.compute1()
```

The printed result will be:
```
<tf.Variable 'myscope/beta1_power:0' shape=() dtype=float32_ref>
<tf.Variable 'myscope/beta2_power:0' shape=() dtype=float32_ref>
------------------------
<tf.Variable 'myscope/beta1_power:0' shape=() dtype=float32_ref>
<tf.Variable 'myscope/beta2_power:0' shape=() dtype=float32_ref>
---------------------------
<tf.Variable 'myscope/beta1_power:0' shape=() dtype=float32_ref>
<tf.Variable 'myscope/beta2_power:0' shape=() dtype=float32_ref>
```

However

code 2:
```
import tensorflow as tf
class generator:
    def __init__(self):
        with tf.variable_scope('myscope'):
            self.a = tf.get_variable('a', [4,3,2], tf.float32)
            self.b = tf.get_variable('b', [4,3,2], tf.float32)
            self.c = tf.reduce_sum(self.a + self.b)
            self.d = tf.train.AdamOptimizer(name = 'adamopt')
    
    def compute(self):
        with tf.variable_scope('myscope'):
            self.e1 = self.d.compute_gradients(self.c, tf.trainable_variables())
            self.f1 = self.d.apply_gradients(self.e1)
        print '------------------------'
        for item in tf.global_variables():
            if 'beta' in item.name:
                print item
    
    def compute1(self):
        with tf.variable_scope('myscope'):
            self.e2 = self.d.compute_gradients(self.c, tf.trainable_variables())
            self.f2 = self.d.apply_gradients(self.e2)
        print '---------------------------'
        for item in tf.global_variables():
            if 'beta' in item.name:
                print item

g = generator()
g.compute()
g.compute1()
```

results in:
```
------------------------
<tf.Variable 'myscope_1/beta1_power:0' shape=() dtype=float32_ref>
<tf.Variable 'myscope_1/beta2_power:0' shape=() dtype=float32_ref>
---------------------------
<tf.Variable 'myscope_1/beta1_power:0' shape=() dtype=float32_ref>
<tf.Variable 'myscope_1/beta2_power:0' shape=() dtype=float32_ref>
```

The only difference between two code is that the first code block doesn't operate 'apply_gradients' operation within the first opening of the variable scope. And I found that this results from the fact that the `beta` is made with `variable_scope.variable` operation. Is this a bug? or an intended results?


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
34790,Dynamic scatter of TensorArray not working in tf.function + eager,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 19.10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7

**Describe the current behavior**
This may be related to #34683, but this is explicitly about the scatter method, I believe it is independent.

The following code creates a TensorArray in a while loop. Every time, a variable-length piece is added. Rules: I _need_ to use scatter and tf.Range like this, the example is a simplification.

The following works with:
 - Graph mode
 - Graph mode with tf.function (no autograph)
 - Eager mode

It breaks in Eager Mode with tf.function (no autograph)


**Describe the expected behavior**

It fails saying that the shape could not be determined, since the scatter unstacks the values internally.

**Code to reproduce the issue**

```
import tensorflow as tf

use_eager = True  # switch to make it run

if not use_eager:
    tf.compat.v1.disable_eager_execution()

stop_at = 1000

empty_sample = tf.TensorArray(dtype=tf.float32, size=stop_at, dynamic_size=True,
                              clear_after_read=True,  # we read only once at end to tensor
                              element_shape=()
                              )

@tf.function(autograph=False)  # if removed, works
def body(sample, length):
    n_to_draw = tf.cast(tf.random.poisson(shape=(), lam=30), dtype=tf.int32)
    rnd = tf.random.uniform(shape=(n_to_draw,), dtype=tf.float32, maxval=1) + 3.
    new_length = length + n_to_draw
    indices = tf.range(length, new_length)
    new_sample = sample.scatter(indices=indices, value=rnd)
    return new_sample, new_length


def cond(sample, length):
    return tf.less(length, stop_at)


sampled = tf.while_loop(cond=cond, body=body, loop_vars=[empty_sample, 0], back_prop=False)[0]
reshaped_sample = tf.reshape(sampled.stack(), shape=(-1,))  # make a read
print(reshaped_sample)

if not use_eager:
    with tf.compat.v1.Session() as sess:
        print(sess.run(reshaped_sample))
```
**Other info / logs**
This is the actual error log
```
  File ""/home/jonas/.PyCharm2019.3/config/scratches/scratch_27.py"", line 21, in body
    new_sample = sample.scatter(indices=indices, value=rnd)
  File ""/home/jonas/anaconda3/envs/zfit37tf2/lib/python3.7/site-packages/tensorflow_core/python/util/tf_should_use.py"", line 198, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""/home/jonas/anaconda3/envs/zfit37tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/tensor_array_ops.py"", line 1168, in scatter
    return self._implementation.scatter(indices, value, name=name)
  File ""/home/jonas/anaconda3/envs/zfit37tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/tensor_array_ops.py"", line 873, in scatter
    for index, val in zip(indices, array_ops.unstack(value)):
  File ""/home/jonas/anaconda3/envs/zfit37tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py"", line 1333, in unstack
    raise ValueError(""Cannot infer num from shape %s"" % value_shape)
ValueError: Cannot infer num from shape (None,)
```"
34789,GRUCell is not compatible with its own initial state,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: 2.1.0rc0
- Python version: 3.5.2

**Describe the current behavior**

The initial state returned by `tf.keras.layers.GRUCell.get_initial_state()` can not be passed to the first cell call without error. It raises an `InvalidArgumentError` error.

**Describe the expected behavior**

RNN cells should accept their own initial states.

**Code to reproduce the issue**

```python
import tensorflow as tf
batch_size = 4
cell = tf.keras.layers.GRUCell(20)
initial_state = cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)
output, state = cell(tf.random.uniform([batch_size, 10]), initial_state)
```

**Other info / logs**

```
Traceback (most recent call last):
  File ""test/gru_incompat.py"", line 5, in <module>
    output, state = cell(tf.random.uniform([batch_size, 10]), initial_state)
  File ""/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/lib/python3.5/site-packages/tensorflow_core/python/keras/layers/recurrent.py"", line 1846, in call
    matrix_inner = K.dot(h_tm1, self.recurrent_kernel)
  File ""/lib/python3.5/site-packages/tensorflow_core/python/keras/backend.py"", line 1678, in dot
    out = math_ops.matmul(x, y)
  File ""/lib/python3.5/site-packages/tensorflow_core/python/util/dispatch.py"", line 180, in wrapper
    return target(*args, **kwargs)
  File ""/lib/python3.5/site-packages/tensorflow_core/python/ops/math_ops.py"", line 2797, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""/lib/python3.5/site-packages/tensorflow_core/python/ops/gen_math_ops.py"", line 5631, in mat_mul
    _ops.raise_from_not_ok_status(e, name)
  File ""/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py"", line 6598, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: In[0] is not a matrix. Instead it has shape [20] [Op:MatMul] name: gru_cell/MatMul/
```"
34788,Inconsistent cpu/gpu results of gather_nd,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.2
- GPU model and memory: 11G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When I set device to gpu, the code runs correctly. When I set device to cpu, the code raises error: tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[4] = [4, -1] does not index into param shape [5,1] [Op:GatherNd].
**Describe the expected behavior**
Consistent results on cpu and gpu.
**Code to reproduce the issue**
```
import os

import tensorflow as tf
import numpy as np

np.random.seed(2222222)

with tf.device('cpu:0'):
  a = np.random.rand(5, 1)
  print(a)

  b = tf.gather_nd(
      a,
      [[0, -1], [1, -1], [2, -1], [3, -1], [4, -1]])

  print(b.numpy())
```
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
34787,Bug: tensors built by tf.keras layers cannot use numpy() to obtain the array. ,Bug: tensors built by tf.keras layers cannot use numpy() to obtain the array. So can i get the value of such tensors??
34786,memory leak on tf2.0 with tf.signal.frame ,"The memory doubles every time I run the tape.gradients part.  I think it's connected to the timedistributedlayers...?

```
###### create model 

    inputs = tf.keras.Input(shape=(6, *data_loader_train[0][0][0].shape), name='img') ## (108, 192, 3)
    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu'))(inputs)
    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu'))(x)
    block_1_output = layers.TimeDistributed(layers.MaxPooling2D(2))(x)

    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu', padding='same'))(block_1_output)
    block_3_output = layers.add([x, block_1_output])
    block_3_output = layers.TimeDistributed(layers.MaxPooling2D(2))(block_3_output)

    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu'))(block_3_output)
    x = layers.TimeDistributed(layers.GlobalAveragePooling2D())(x)

    x = layers.Flatten()(x)
    x = layers.Dense(16, activation='relu')(x)
    x = layers.Dense(1)(x)
    counts = tf.keras.activations.softplus(x)

    model = tf.keras.Model(inputs, counts, name='toy_resnet')
    model.summary()

    ### run model
```

```
####### running this part doubles memory every two times ##########
for x_ in batch(np.random.uniform(size=(100,6,108,192,3)).astype(np.float32), 10):
     with tf.GradientTape() as tape:
             count_ = tf.reduce_sum(model(x_))
```"
34785,How does TensorFlow calculate the gradients of an FFT layer?,"Hi, if I insert the function, i.e., tf.fft(input, name=None), into a neural network, how does TensorFlow calculate the gradients in backpropagation?

I didn't find a documentation about this. I am using TensorFlow 1.0.

Does anyone know？"
34784,Boosted trees using Estimators codes crash,"When I test the codes in https://www.tensorflow.org/tutorials/estimator/boosted_trees, it crashed. and I see the error of ""malloc(): memory corruption (fast): 0x00007f4f84066f50 ***"". The detailed error messages can be found here(https://gist.github.com/fancyerii/c805604e94b76988771e4ff045aeb303)

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1/7.0
- GPU model and memory: Quadro P3000



**Code to reproduce the issue**
https://gist.github.com/fancyerii/538d4033bc94115c561a146b78beeac9

**Other info / logs**
https://gist.github.com/fancyerii/c805604e94b76988771e4ff045aeb303
"
34782,"[tf.keras] Mixed precision policy ""mixed_bfloat16"" not supported in Keras compile","**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Co-lab
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): TF 2.1-rc0
- Python version: 3.6

**Describe the current behavior**

We are porting a GPU based model to CloudTPU. We are using Keras **mixed_float16** mixed-precision policy to enable TensorCore on GPU. Without any code change, we are trying to use **mixed_bfloat16** for CloudTPU for maximal performance.

**Describe the expected behavior**

``model.compile`` with **mixed_bfloat16** policy to enable mixed-precision training on CloudTPU.

**Code to reproduce the issue**

Colab link: https://colab.research.google.com/drive/1-SBnqhsmyjVNJxntB8ZZdqRUk7h7tRs8

```
def compile_keras_model(dtype):
  policy = tf.keras.mixed_precision.experimental.Policy(dtype)
  tf.compat.v2.keras.mixed_precision.experimental.set_policy(policy)

  optimizer = tf.optimizers.SGD(learning_rate=0.1, momentum=0.9)

  model = tf.keras.applications.resnet50.ResNet50(weights=None)
  model.compile(loss='sparse_categorical_crossentropy',
                optimizer=optimizer, 
                metrics=['sparse_categorical_accuracy'])
  return model
  
gpu_model = compile_keras_model('mixed_float16')
tpu_model = compile_keras_model('mixed_bfloat16')
```

**Other info / logs**

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-5-ad9cf91bd793> in <module>()
     12 
     13 gpu_model = compile_keras_model('mixed_float16')
---> 14 tpu_model = compile_keras_model('mixed_bfloat16')

12 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)
     59           ""allowed values: %s"" %
     60           (param_name, dtypes.as_dtype(dtype).name,
---> 61            "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
     62 
     63 

TypeError: Value passed to parameter 'x' has DataType bfloat16 not in list of allowed values: float16, float32, float64, complex64, complex128
```"
34781,obfuscate_names encrypts inputs,"I'm try work on with Graph Transform Tool. 
And found with the flag `obfuscate_names`, he inputs the encrypting. Outputs not encrypting.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below): 1.14.0
- Python version: 3.7.2
- CUDA/cuDNN version: 10.1
- GPU model and memory: 1060

"
34780,[FusedBatchNormGradOp] Actual shapes of outputs NOT consistent with the shapes derived by shapeFn,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
> NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
> Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):
> Binary
- TensorFlow version (use command below):
> 1.12.0
- Python version:
> 3.6.3

**Describe the current behavior**
For the operator FusedBatchNormGradOp, the `shape derived by shapeFn` is inconsistent with the `run-time shape of the actual output` when set `is_training = False` , and this difference is caused by the obvious code. I wonder if this is a bug? In fact, I rely on the shape derived by Tensorflow to build my network...

**Describe the expected behavior**
The shapefn derivation gives the same result as the runtime, unless it's an so called `unknown shape operator`.

**Code to reproduce the issue**
- Just as an example:
> the comment line illustrates the problem
```python
import numpy as np
import tensorflow as tf
from tensorflow.python.ops import gen_nn_ops

y_backprop=np.random.random((32,32,2,1024)).astype(np.float32)
x=np.random.random((32,32,2,1024)).astype(np.float32)
scale=np.random.random((1024)).astype(np.float32)
reserve_space_1=np.random.random((1024)).astype(np.float32)
reserve_space_2=np.random.random((1024)).astype(np.float32)
fused_batch_norm_grad_result = gen_nn_ops.fused_batch_norm_grad(y_backprop, x, scale, reserve_space_1, reserve_space_2, epsilon=0.0001, data_format=""NHWC"", is_training=False)

print(fused_batch_norm_grad_result)
# printed: (32, 32, 2, 1024), (1024,), (1024,), (1024,), (1024,)
fused_batch_norm_grad_result=tf.Session().run(fused_batch_norm_grad_result)
for t in fused_batch_norm_grad_result:
    print(np.shape(t))
# printed: (32, 32, 2, 1024), (1024,), (1024,), (), ()
```

**Other info / logs**
The code that causes this problem is easily found in the Compute method of FusedBatchNormGradOp as follows:
```c
    ctx->SetOutput(1, scale_backprop);
    ctx->SetOutput(2, offset_backprop);
    ctx->SetConstantOutput(3, Tensor());
    ctx->SetConstantOutput(4, Tensor());
```
However, the following derivation is made in shapefn:
```c
  // Set the correct shapes for reserve_spaces
  // so that gradients can be performed when
  // the op is in a symbolic condition.
  if (is_training) {
    c->set_output(3, c->Vector(0));
    c->set_output(4, c->Vector(0));
  } else {
    c->set_output(3, c->Vector(channel_dim));
    c->set_output(4, c->Vector(channel_dim));
  }
```
I noticed the comment above that says this branch is to set the proper shape for reserve_spaces, but I can't understand what that really means...

"
34778,Can't get a tape.gradient when tf.signal.frame is used.,"Tensorflow 2.0.
I believe this is related to the tf.gather problems that have been noted already.  

```
    inputs = tf.keras.Input(shape=data_loader_train[0][0][0].shape, name='img') ## (108, 192, 3)
    x = layers.Conv2D(32, 3, activation='relu')(inputs)
    x = layers.Conv2D(16, 3, activation='relu')(x)
    block_1_output = layers.MaxPooling2D(2)(x)

    x = layers.Conv2D(16, 3, activation='relu', padding='same')(block_1_output)
    # x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)
    block_2_output = layers.add([x, block_1_output])
    block_2_output = layers.MaxPooling2D(2)(block_2_output)

    x = layers.Conv2D(16, 3, activation='relu', padding='same')(block_2_output)
    # x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)
    block_3_output = layers.add([x, block_2_output])
    block_3_output = layers.MaxPooling2D(2)(block_3_output)

    x = layers.Conv2D(32, 3, activation='relu')(block_3_output)
    x = layers.GlobalAveragePooling2D()(x)

### if this part is included we get an error ##########
    # x = layers.Flatten()(x)
    x = tf.signal.frame(x,args.num_frames,1, axis=0)
    x = layers.Flatten()(x)
##############################
    x = layers.Dense(16, activation='relu')(x)
    x = layers.Dense(1)(x)
    counts = tf.keras.activations.softplus(x)
    # x = layers.Dropout(0.5)(x)
    # outputs = layers.Dense(10, activation='softmax')(x)

    model = tf.keras.Model(inputs, counts, name='toy_resnet')
```
Now if we do this....
```
            with tf.GradientTape() as tape:
                loss = tf.math.squared_difference(tf.reduce_sum(model(x_mb)), y_mb)

            grads = tape.gradient(loss, model.trainable_variables)
```
we get the following error at ""grads = ...""
Error = AssertionError: Expected all args to be Tensors or Variables; but got CompositeTensor: [<tensorflow.python.framework.indexed_slices.IndexedSlices object
"
34777,"Build issue ""None of the libraries match their SONAME"" (libcudnn)","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Slackware Linux 64 -current (14.2+) 2019-12-02
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
Source
- TensorFlow version:
HEAD 882a1c8ed7
- Python version:
Python 3.7.5
- Installed using virtualenv? pip? conda?:
Directly from OS
- Bazel version (if compiling from source):
Binary download bazel 1.1.0
- GCC/Compiler version (if compiling from source):
gcc (GCC) 8.3.1 20191031
- CUDA/cuDNN version:
cuda_10.2.89_440.33.01_linux.run
cudnn-10.2-linux-x64-v7.6.5.32.tgz
nccl_2.5.6-2+cuda10.2_x86_64.txz
- GPU model and memory:
MSI Graphic Cards GT 1030 2G LP OC

**Describe the problem**
Trying to build TF shows error `Cuda Configuration Error: None of the libraries match their SONAME: /opt/nvidia/cuda/lib64/libcudnn.so.7`

**Provide the exact sequence of commands / steps that you executed before running into the problem**
- Installed nvidia kernel modules and driver 440.31
- Installed cuda, cudnn, nccl, tensorrt to `/opt/nvidia/cuda  /opt/nvidia/nccl  /opt/nvidia/tensorrt`
- Installed bazel binary as `/usr/bin/bazel`
- Downloaded latest TF source from GitHub
- Used TF's `./configure` command to config the build
- Ran build `bazel build //tensorflow/tools/pip_package:build_pip_package`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
[.tf_configure.bazelrc](https://github.com/tensorflow/tensorflow/files/3913908/tf_configure.bazelrc.txt)
[tf-build-err.txt](https://github.com/tensorflow/tensorflow/files/3913909/tf-build-err.txt)
"
34776,StackedRNNCells has an invalid example,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/keras/layers/StackedRNNCells

## Description of issue (what needs changing):

Documentation example does not actually use `StackedRNNCells`. There is no example for the class being documented. Ideally there would be both an example of the class and an example showing how the same behaviour would be implemented without the class.
"
34775,tf.keras.models.load_model from saved model not loading as keras model?,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0rc0
- Python version: 3.7.3
- CUDA/cuDNN version: 10.1/7.6.4
- GPU model and memory: RTX Titan
2.1.0-rc0
v1.12.1-17734-gc6daad3

**Describe the current behavior**
The documents from [official-tensorflow-website](https://www.tensorflow.org/tutorials/keras/save_and_load) written as when using ```tf.keras.models.load_model```  on a saved model format folder will be loaded as Keras model.

My model was trained on 2.0.0b0

**Describe the expected behavior**
However, when I loaded the model, it loaded as saved_model.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
```
>>> new_model = tf.keras.models.load_model('saved_model/01')
2019-12-03 08:13:44.584942: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-12-03 08:13:44.681183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:
pciBusID: 0000:17:00.0 name: TITAN RTX computeCapability: 7.5
coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.62GiB deviceMemoryBandwidth: 625.94GiB/s
2019-12-03 08:13:44.683339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties:
pciBusID: 0000:65:00.0 name: TITAN RTX computeCapability: 7.5
coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.62GiB deviceMemoryBandwidth: 625.94GiB/s
2019-12-03 08:13:44.683357: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2019-12-03 08:13:44.683380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2019-12-03 08:13:44.684694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2019-12-03 08:13:44.684963: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2019-12-03 08:13:44.686456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2019-12-03 08:13:44.687384: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2019-12-03 08:13:44.687422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-12-03 08:13:44.695355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1
2019-12-03 08:13:44.695566: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-12-03 08:13:44.727235: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz
2019-12-03 08:13:44.730301: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6317a70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-12-03 08:13:44.730343: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2019-12-03 08:13:45.044582: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x637d2f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2019-12-03 08:13:45.044634: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN RTX, Compute Capability 7.5
2019-12-03 08:13:45.044649: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): TITAN RTX, Compute Capability 7.5
2019-12-03 08:13:45.047700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:
pciBusID: 0000:17:00.0 name: TITAN RTX computeCapability: 7.5
coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.62GiB deviceMemoryBandwidth: 625.94GiB/s
2019-12-03 08:13:45.050560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties:
pciBusID: 0000:65:00.0 name: TITAN RTX computeCapability: 7.5
coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.62GiB deviceMemoryBandwidth: 625.94GiB/s
2019-12-03 08:13:45.050603: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2019-12-03 08:13:45.050625: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2019-12-03 08:13:45.050653: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2019-12-03 08:13:45.050680: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2019-12-03 08:13:45.050712: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2019-12-03 08:13:45.050746: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2019-12-03 08:13:45.050765: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-12-03 08:13:45.061373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1
2019-12-03 08:13:46.263485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-03 08:13:46.263516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 1
2019-12-03 08:13:46.263521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N Y
2019-12-03 08:13:46.263524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 1:   Y N
2019-12-03 08:13:46.266356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22629 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:17:00.0, compute capability: 7.5)
2019-12-03 08:13:46.267761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 22629 MB memory) -> physical GPU (device: 1, name: TITAN RTX, pci bus id: 0000:65:00.0, compute capability: 7.5)
WARNING:tensorflow:From /misc/home/usr16/cheesiang_leow/.virtualenvs/tensorflow-2.1/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
>>> new_model
<tensorflow.python.training.tracking.tracking.AutoTrackable object at 0x7f9c3775b198>
>>> new_model.summary()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'AutoTrackable' object has no attribute 'summary'
```

Is this because my model uses the Functional API ? The model API from "
34774,DenseToDenseSetOperation: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): conda -c anaconda
- TensorFlow version (use command below): 1.14
- Python version: 3.6
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): 7.4
- CUDA/cuDNN version: cuda10.1/cudnn7.6.4
- GPU model and memory: Quadro P5000

**Describe the current behavior**
tf.data.Dataset throws a error since I pass to tf114 from tf113. My following code can reproduce this error. 

**Describe the expected behavior**
I explain my code here. The Dataset takes a list of .h5 file names in string then shuffle this list.  A wrapper written with tf.py_func() will load the .h5 file (I know it's deprecated in TF2, but since it works well in my previous code). It's also my question. Is this bug exists only in TF1 or it does in TF2? Should I move to TF2?

**Code to reproduce the issue**
I wrote a small Seg-Net network to to reproduce the same bug. Everything goes well until I change the output and input of the NN by tf.data.

```
import tensorflow as tf
import numpy as np
from tqdm import tqdm
from tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference
from tensorflow.python.framework import dtypes
import os

def up_2by2_ind(input_layer, ind, name=''):
    with tf.name_scope(name):
        in_shape = input_layer.get_shape().as_list()
        out_shape = [tf.cast(tf.shape(input_layer), dtype=tf.int64)[0], in_shape[1] * 2, in_shape[2] * 2, in_shape[3]]

        # prepare
        _pool = tf.reshape(input_layer, [-1])
        _range = tf.reshape(tf.range(out_shape[0], dtype=ind.dtype), [out_shape[0], 1, 1, 1])
        tmp = tf.ones_like(ind) * _range
        tmp = tf.reshape(tmp, [-1, 1])
        _ind = tf.reshape(ind, [-1, 1])
        _ind = tf.concat([tmp, _ind], 1)

        # scatter
        unpool = tf.scatter_nd(_ind, _pool, [out_shape[0], out_shape[1] * out_shape[2] * out_shape[3]])

        # reshape
        unpool = tf.reshape(unpool, out_shape)
        return unpool

###################### input pipeline
def wrapper(a, b):
    return tf.py_func(
        wrawrapper,
        [a, b],
        [tf.float32, tf.float32],
    )

def wrawrapper(a, b):
    return np.ones((1, 50, 50, 1), dtype=np.float32), np.ones((1, 50, 50, 1), dtype=np.float32)

a_ph = tf.placeholder(tf.string, shape=[None], name='a_ph')
b_ph = tf.placeholder(tf.int32, shape=[None], name='b_ph')

batch = tf.data.Dataset.from_tensor_slices((a_ph, b_ph))
batch = batch.shuffle(tf.cast(tf.shape(a_ph)[0], tf.int64))
batch = batch.map(wrapper).prefetch(10).repeat()
it = tf.data.Iterator.from_structure(batch.output_types, batch.output_shapes)
iter_init_op = it.make_initializer(batch, name='iter_init_op')
X_it, y_it = it.get_next()

dropout = tf.placeholder(tf.float32, [], name='dropout')
BN_phase = tf.placeholder(tf.bool, [], name='BN_phase')
save_summary_step = 20
save_model_step = 100

##################### train graph on gpu1
with tf.device('/device:GPU:0'):
    with tf.name_scope('model'):
        with tf.name_scope('conv'):
            with tf.variable_scope('conv', reuse=False):
                w1 = tf.get_variable('w', shape=[3, 3, 1, 1], initializer=tf.initializers.glorot_normal())
            out1 = tf.nn.conv2d(X_it, w1, strides=[1, 1, 1, 1], padding='SAME', name='conv')
            with tf.variable_scope('conv', reuse=False):
                out1 = tf.layers.batch_normalization(out1, training=BN_phase, name='batch_norm')
            out1 = tf.nn.relu(out1, 'relu')
            out1, ind1 = tf.nn.max_pool_with_argmax(out1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='maxpool')
        with tf.name_scope('dnn'):
            flat = tf.reshape(out1, [1, 625])
            with tf.variable_scope('dnn2', reuse=False):
                w2 = tf.get_variable('w2', shape=[625, 625], initializer=tf.initializers.glorot_normal())
            dnn_out = tf.matmul(flat, w2)
            dnn_out = tf.nn.dropout(dnn_out, keep_prob=dropout, name='do')
            dnn_out = tf.nn.relu(dnn_out, name='relu')
            dnn_out = tf.reshape(dnn_out, shape=[1, 25, 25, 1], name='dnn')

        with tf.name_scope('deconv'):
            out1 = up_2by2_ind(dnn_out, ind1, 'up1')
            with tf.variable_scope('deconv', reuse=False):
                w3 = tf.get_variable('w', shape=[3, 3, 1, 1], initializer=tf.initializers.glorot_normal())
            out1 = tf.nn.conv2d(out1, w3, strides=[1, 1, 1, 1], padding='SAME', name='deconv')
            with tf.variable_scope('deconv', reuse=False):
                out1 = tf.layers.batch_normalization(out1, training=BN_phase, name='batch_norm')
            logits = tf.nn.relu(out1, 'logits')

        #todo: here with tabulation with tf.name_scope('operation'):
    with tf.name_scope('operation'):
        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
        with tf.control_dependencies(update_ops):
            mse = tf.losses.mean_squared_error(labels=y_it, predictions=logits)
        opt = tf.train.AdamOptimizer(learning_rate=0.0001, name='Adam')

        grads = opt.compute_gradients(mse)
        train_op = opt.apply_gradients(grads, name='apply_grad')

with tf.name_scope('train_metrics'):
    acc_val_op, acc_update_op = tf.metrics.accuracy(labels=y_it, predictions=logits)
    summ_acc = tf.summary.merge([tf.summary.scalar('accuracy', acc_val_op)])
    grad_sum = tf.summary.merge([tf.summary.histogram('{}/grad'.format(g[1].name), g[0]) for g in grads])

with tf.name_scope('train_summary'):
    merged = tf.summary.merge([summ_acc, grad_sum, tf.summary.histogram(""weights"", w1)])

###################### test graph on gpu2
with tf.device('/device:GPU:1'):
    with tf.name_scope('model'):
        with tf.name_scope('conv'):
            with tf.variable_scope('conv', reuse=True):
                w1 = tf.get_variable('w', shape=[3, 3, 1, 1], initializer=tf.initializers.glorot_normal())
            out1 = tf.nn.conv2d(X_it, w1, strides=[1, 1, 1, 1], padding='SAME', name='conv')
            with tf.variable_scope('conv', reuse=True):
                out1 = tf.layers.batch_normalization(out1, training=BN_phase, name='batch_norm')
            out1 = tf.nn.relu(out1, 'relu')
            out1, ind1 = tf.nn.max_pool_with_argmax(out1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='maxpool')
        with tf.name_scope('dnn'):
            flat = tf.reshape(out1, [1, 625])
            with tf.variable_scope('dnn2', reuse=True):
                w2 = tf.get_variable('w2', shape=[625, 625], initializer=tf.initializers.glorot_normal())
            dnn_out = tf.matmul(flat, w2)
            dnn_out = tf.nn.dropout(dnn_out, keep_prob=dropout, name='do')
            dnn_out = tf.nn.relu(dnn_out, name='relu')
            dnn_out = tf.reshape(dnn_out, shape=[1, 25, 25, 1], name='dnn')

        with tf.name_scope('deconv'):
            out1 = up_2by2_ind(dnn_out, ind1, 'up1')
            with tf.variable_scope('deconv', reuse=True):
                w3 = tf.get_variable('w', shape=[3, 3, 1, 1], initializer=tf.initializers.glorot_normal())
            out1 = tf.nn.conv2d(out1, w3, strides=[1, 1, 1, 1], padding='SAME', name='deconv')
            with tf.variable_scope('conv', reuse=True):
                out1 = tf.layers.batch_normalization(out1, training=BN_phase, name='batch_norm')
            logits = tf.nn.relu(out1, 'logits')


with tf.name_scope('test_metrics'):
    acc_val_op2, acc_update_op2 = tf.metrics.accuracy(labels=y_it, predictions=logits)
    summ_acc2 = tf.summary.merge([tf.summary.scalar('accuracy', acc_val_op2)])

with tf.name_scope('test_summary'):
    merged2 = tf.summary.merge([summ_acc2, tf.summary.histogram(""weights"", w1), tf.summary.histogram(""weights"", w2)])


##############################################
with tf.Session() as sess:
    sess.run([tf.global_variables_initializer(), tf.local_variables_initializer(), iter_init_op],
             feed_dict={a_ph: ['a'], b_ph: [10]})
    model_saver = tf.train.Saver(max_to_keep=100000)
    train_writer = tf.summary.FileWriter('./dummy/gpus/train/', sess.graph)
    test_writer = tf.summary.FileWriter('./dummy/gpus/test/', sess.graph)
    for i in tqdm(range(1000)):
        if i % save_summary_step == 0:
            _, rlt, summary, _ = sess.run([train_op, out1, merged, acc_update_op], feed_dict={
                dropout: 1,
                BN_phase: True,
            }
                                          )
            # print('train:', rlt)
            train_writer.add_summary(summary, global_step=i)
        else:
            _, rlt = sess.run([train_op, out1], feed_dict={dropout: 1, BN_phase: True})
            # print('train:', rlt)
        if i % save_model_step == 0:
            model_saver.save(sess, './dummy/ckpt/step{}'.format(i))
            if i != 0:
                for j in tqdm(range(5)):
                    rlt, summary = sess.run([logits, merged2], feed_dict={
                        dropout: 1,
                        BN_phase: False,
                    }
                                            )
                    # print('test:', rlt)
                    test_writer.add_summary(summary, global_step=j)
```

**Other info / logs**
```
Traceback (most recent call last):
  File ""/home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1356, in _do_call
    return fn(*args)
  File ""/home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1339, in _run_fn
    self._extend_graph()
  File ""/home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1374, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation operation/mean_squared_error/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='' supported_device_types_=[CPU] possible_devices_=[]
DenseToDenseSetOperation: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  operation/mean_squared_error/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation (DenseToDenseSetOperation) /device:GPU:0

Op: DenseToDenseSetOperation
Node attrs: T=DT_INT32, validate_indices=true, set_operation=""a-b""
Registered kernels:
  device='CPU'; T in [DT_STRING]
  device='CPU'; T in [DT_UINT16]
  device='CPU'; T in [DT_UINT8]
  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_INT8]

	 [[{{node operation/mean_squared_error/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/media/snippet.py"", line 1930, in <module>
    feed_dict={a_ph: ['a'], b_ph: [10]})
  File ""/home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""/home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation operation/mean_squared_error/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='' supported_device_types_=[CPU] possible_devices_=[]
DenseToDenseSetOperation: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  operation/mean_squared_error/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation (DenseToDenseSetOperation) /device:GPU:0

Op: DenseToDenseSetOperation
Node attrs: T=DT_INT32, validate_indices=true, set_operation=""a-b""
Registered kernels:
  device='CPU'; T in [DT_STRING]
  device='CPU'; T in [DT_UINT16]
  device='CPU'; T in [DT_UINT8]
  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_INT8]

	 [[node operation/mean_squared_error/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation (defined at media/snippet.py:1875) ]]

Original stack trace for 'operation/mean_squared_error/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation':
  File ""media/snippet.py"", line 1875, in <module>
    mse = tf.losses.mean_squared_error(labels=y_it, predictions=logits)
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py"", line 646, in mean_squared_error
    losses, weights, scope, loss_collection, reduction=reduction)
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py"", line 181, in compute_weighted_loss
    weights_broadcast_ops.assert_broadcastable(weights, losses),)):
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/weights_broadcast_ops.py"", line 132, in assert_broadcastable
    name=""is_valid_shape"")
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1988, in cond
    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1814, in BuildCondBranch
    original_result = fn()
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/weights_broadcast_ops.py"", line 131, in <lambda>
    weights_rank, weights_shape, values_rank, values_shape),
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/weights_broadcast_ops.py"", line 57, in _has_valid_nonscalar_shape
    name=scope)
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1977, in cond
    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1814, in BuildCondBranch
    original_result = fn()
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/weights_broadcast_ops.py"", line 55, in <lambda>
    lambda: _has_valid_dims(weights_shape, values_shape),
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/weights_broadcast_ops.py"", line 40, in _has_valid_dims
    invalid_dims = sets.set_difference(weights_shape_2d, valid_dims)
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/sets_impl.py"", line 273, in set_difference
    return _set_operation(a, b, ""a-b"" if aminusb else ""b-a"", validate_indices)
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/sets_impl.py"", line 132, in _set_operation
    a, b, set_operation, validate_indices)
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/gen_set_ops.py"", line 98, in dense_to_dense_set_operation
    name=name)
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()

```"
34773,tf.math.tanh produces values strictly smaller than -1,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab default environment
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 1.15.0
- Python version: 2.4.6

**Describe the current behavior**

```
import tensorflow as tf
import numpy as np

tf.enable_v2_behavior()

x = np.float(-8.51089)
tf.math.tanh(x)
=> <tf.Tensor: shape=(), dtype=float32, numpy=-1.0000001>
```
Note the result is less than -1.0.

**Describe the expected behavior**

Expected tanh to stay within the interval [-1.0, 1.0].
"
34772,tf.pad is limited to six dimensions,"```
> A = tf.zeros([1] * 7)
> tf.pad(A, [[0, 0]] * 6 + [[0, 1]])
...
UnimplementedError: inputs rank not in [0,6]: 7 [Op:Pad]
> tf.pad(A[0], [[0, 0]] * 5 + [[0, 1]])
<tf.Tensor: id=50, shape=(1, 1, 1, 1, 1, 2), dtype=float32, numpy=array([[[[[[0., 0.]]]]]], dtype=float32)>
```"
34771,tf.keras uses Eager execution or Graph execution in tf 2.0 ?,"Hello, i'm doing a research work and i'd be glad to know if in tf 2.0, tensorflow.keras uses eager execution vs graph execution instead."
34768,Error when saving a stacked LSTM model to .h5 file,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
I have a simple and functional custom code to reproduce the issue.

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.04

- TensorFlow installed from (source or binary):
conda install tensorflow-gpu=2.0.0

- TensorFlow version (use command below):

python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""

v2.0.0-rc2-26-g64c3d38 2.0.0

- Python version:
Python 3.7.4

- CUDA/cuDNN version:
 CUDA Version: 10.1
#define CUDNN_MAJOR 7

- GPU model and memory:
nvidia Quadro GV100 - 32478MiB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)

output `env_info.sh` attached. 

**Describe the current behavior**

When saving a stacked LSTM model to .h5 file, the following error is encountered:

`OSError: Unable to create link (name already exists)`

**Describe the expected behavior**

Expect to be able to save a stacked LSTM model to .h5 file with no error.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf
import numpy as np

def Model_Functional_API():

    inputs = tf.keras.Input(shape=(3, 2))
    encoder_cell = [
        tf.keras.layers.LSTMCell(10),
        tf.keras.layers.LSTMCell(10),
        tf.keras.layers.LSTMCell(10)
    ]
    encoder = tf.keras.layers.RNN(encoder_cell, return_sequences=True)
    encoder_outputs = encoder(inputs)
    projection_layer = tf.keras.layers.Dense(2)
    preds = projection_layer(encoder_outputs)
    model = tf.keras.Model(inputs,preds)

    return model

model = Model_Functional_API()
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss='mean_squared_error')

data_x = np.random.random([64,3,2])
data_y = np.random.random([64,3,2])

model.fit(data_x,data_y,batch_size=64,epochs=2)

model.save('saved_model_h5/my_model.h5')
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

`tf_env.txt` and full error log are attached:
[error_log.txt](https://github.com/tensorflow/tensorflow/files/3912957/error_log.txt)
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3912958/tf_env.txt)

"
34767,Error when retraining a saved LSTM model,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
I have a simple and functional custom code to reproduce the issue.

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.04

- TensorFlow installed from (source or binary):
conda install tensorflow-gpu=2.0.0

- TensorFlow version (use command below):

python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""

v2.0.0-rc2-26-g64c3d38 2.0.0

- Python version:
Python 3.7.4

- CUDA/cuDNN version:
 CUDA Version: 10.1
#define CUDNN_MAJOR 7

- GPU model and memory:
nvidia Quadro GV100 - 32478MiB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)

output `env_info.sh` attached. 

**Describe the current behavior**

1. Train a simple LSTM model using tf.keras API
2. save the model to file as `SavedModel` format. `model.save('saved_model', save_format='tf')`
3. In a separate script (without access to the model definition), load the saved model: `model = tf.keras.models.load_model('saved_model1')`
4. continue training the reloaded model: `model.fit(data_x,data_y,batch_size=64,epochs=2)`
The following error encountered:

`LookupError: No gradient defined for operation 'while' (op type: While)`

**Describe the expected behavior**

expect the LSTM `SavedModel` can be loaded and retrained in a separated script without access to the model definition.

**Code to reproduce the issue**
Script to train and save the model:
```
import tensorflow as tf
import numpy as np

def Model_Functional_API():

    inputs = tf.keras.Input(shape=(3, 2))
    encoder = tf.keras.layers.LSTM(10,return_sequences=True)
    encoder_outputs = encoder(inputs)
    projection_layer = tf.keras.layers.Dense(2)
    preds = projection_layer(encoder_outputs)
    model = tf.keras.Model(inputs,preds)

    return model

def Model_Sequence():

    model = tf.keras.Sequential()
    model.add(tf.keras.layers.LSTM(10,return_sequences=True))
    model.add(tf.keras.layers.Dense(2))

    return model

# model = Model_Functional_API()
model = Model_Sequence()
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss='mean_squared_error')

data_x = np.random.random([64,3,2])
data_y = np.random.random([64,3,2])

model.fit(data_x,data_y,batch_size=64,epochs=2)

model.save('saved_model', save_format='tf')
# model.save('saved_model.h5')
```
Script to load and retrain the model (this is where the error encountered):
```
import tensorflow as tf
import numpy as np

model = tf.keras.models.load_model('saved_model')
# model = tf.keras.models.load_model('saved_model.h5')

data_x = np.random.random([64,3,2])
data_y = np.random.random([64,3,2])

model.fit(data_x,data_y,batch_size=64,epochs=2)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

`tf_env.txt` and the ouput error message are attached.
[error_log.txt](https://github.com/tensorflow/tensorflow/files/3912802/error_log.txt)
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3912803/tf_env.txt)

 "
34766,Error with tensorflow lite interpreter,"Hi, I am trying to create a wrapper library that takes tensorflow lite model, returns interpreter, which is used later to invoke on specific inputs

I am using the mimimal.cc example code and modified it to my needs:

In order to use interpreter from one function to the next, make sure to keep other components like model and resolver alive with it.
"
34765,Keras' predict method should be compatible with TensorFlow Probability,"**System information**
- TensorFlow version (you are using): 2.
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**

Currently, I cannot call the [`predict`](https://keras.io/models/model/) method to perform predictions, when using a [`DistributionLambda`](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DistributionLambda) as the output layer of a Keras model. See, for example, the issues https://github.com/tensorflow/probability/issues/427, https://github.com/tensorflow/probability/issues/538 and https://github.com/tensorflow/tensorflow/issues/31695. So, currently, the apparent alternative is to use  `my_model(input)`, which is a syntax that is not even documented anywhere (AFAIK), to get the predictions.

**Will this change the current api? How?**

I don't know exactly because I am not very familiar with the Keras APIs.

**Who will benefit with this feature?**

Everyone interested in Bayesian deep learning."
34763,MLIR-based convertor fails to convert Universal Sentence Encoder model,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (or github SHA if from source): tf-nightly 2.1.0.dev20191202 also tried tf-nightly-gpu 2.1.0.dev20191202


**Command used to run the converter or code if you’re using the Python API**

```
saved_model_dir = '... local path to https://tfhub.dev/google/universal-sentence-encoder-large/4 '
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.experimental_new_converter = True
tflite_model = converter.convert()
```

**The output from the converter invocation**

```
2019-12-02 11:06:35.679573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3020 MB memory) -> physical GPU (device: 0, name: Quadro M2200, pci bus id: 0000:01:00.0, compute capability: 5.2)
2019-12-02 11:06:36.258120: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize
2019-12-02 11:06:36.258307: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 3003 nodes (0), 3003 edges (0), time = 149.21ms.
2019-12-02 11:06:36.258515: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 3003 nodes (0), 3003 edges (0), time = 144.701ms.
2019-12-02 11:06:36.258724: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_pruned_16231
2019-12-02 11:06:36.258972: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2019-12-02 11:06:36.259294: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2019-12-02 11:06:39.543465: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2019-12-02 11:06:39.544669: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-12-02 11:06:39.546506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: Quadro M2200 computeCapability: 5.2
coreClock: 1.036GHz coreCount: 8 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 82.08GiB/s
2019-12-02 11:06:39.546854: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2019-12-02 11:06:39.547033: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2019-12-02 11:06:39.547210: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2019-12-02 11:06:39.547386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2019-12-02 11:06:39.547560: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2019-12-02 11:06:39.547739: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2019-12-02 11:06:39.547922: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-12-02 11:06:39.548392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2019-12-02 11:06:39.548564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-02 11:06:39.548735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2019-12-02 11:06:39.548842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2019-12-02 11:06:39.549462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3020 MB memory) -> physical GPU (device: 0, name: Quadro M2200, pci bus id: 0000:01:00.0, compute capability: 5.2)
2019-12-02 11:06:39.665521: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize
2019-12-02 11:06:39.665699: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 3003 nodes (0), 3003 edges (0), time = 39.697ms.
2019-12-02 11:06:39.665895: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 3003 nodes (0), 3003 edges (0), time = 35.538ms.
Traceback (most recent call last):
  File ""C:/Users/User/PycharmProjects/tf_models/main.py"", line 70, in <module>
    tflite_model = converter.convert()
  File ""C:\Users\User\anaconda3\envs\tf_models\lib\site-packages\tensorflow_core\lite\python\lite.py"", line 474, in convert
    **converter_kwargs)
  File ""C:\Users\User\anaconda3\envs\tf_models\lib\site-packages\tensorflow_core\lite\python\convert.py"", line 475, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""C:\Users\User\anaconda3\envs\tf_models\lib\site-packages\tensorflow_core\lite\python\convert.py"", line 215, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2019-12-02 11:06:42.830207: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-12-02 11:06:42.830437: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
...
...
2019-12-02 11:06:43.425042: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-12-02 11:06:43.425241: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-12-02 11:06:43.425462: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-12-02 11:06:43.426297: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: StatefulPartitionedCall
2019-12-02 11:06:43.427219: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""WrapDatasetVariant"" device_type: ""CPU""') for unknown op: WrapDatasetVariant
2019-12-02 11:06:43.427479: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""WrapDatasetVariant"" device_type: ""GPU"" host_memory_arg: ""input_handle"" host_memory_arg: ""output_handle""') for unknown op: WrapDatasetVariant
2019-12-02 11:06:43.427895: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""UnwrapDatasetVariant"" device_type: ""CPU""') for unknown op: UnwrapDatasetVariant
2019-12-02 11:06:43.428165: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""UnwrapDatasetVariant"" device_type: ""GPU"" host_memory_arg: ""input_handle"" host_memory_arg: ""output_handle""') for unknown op: UnwrapDatasetVariant
2019-12-02 11:06:43.428599: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)


```

**Also, please include a link to the saved model or GraphDef**

```
https://tfhub.dev/google/universal-sentence-encoder-large/4
```
"
34762,AttributeError: module 'tensorflow.python.framework.op_def_registry' has no attribute 'register_op_list',"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 2.0.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.27.1
- **GCC/Compiler version (if compiling from source)**: 7.4.0
- **CUDA/cuDNN version**: 10.2
- **GPU model and memory**: NVIDIA 970
- **Exact command to reproduce**: python3 legacy/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Trying tensorflow's trainning appears an error message about a non existed attribute **(register_op_list')** . I checked in the file that is supposed to have it and it really doesn't have it. 
I looked for that attribute in every single path I thought it could be but I didn't get anything at all.
I have no clue of how I can resolve this.


To resolve the contrib issue of tf2.0 i used [tf-slim](https://github.com/adrianc-a/tf-slim) but if you have a way to resolve it completely that doesn't trigger this error please let me know because I'm quite sure that everything comes because of the ""contrib"" problem.
Thanks




### Source code / logs

**### python3 legacy/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config**

2019-12-02 13:59:36.908351: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
Traceback (most recent call last):
  File ""legacy/train.py"", line 48, in <module>
    from tensorflow.contrib import framework as contrib_framework
  File ""/home/adele/.local/lib/python3.6/site-packages/tensorflow/contrib/__init__.py"", line 31, in <module>
    from tensorflow.contrib import cloud
  File ""/home/adele/.local/lib/python3.6/site-packages/tensorflow/contrib/cloud/__init__.py"", line 24, in <module>
    from tensorflow.contrib.cloud.python.ops.bigquery_reader_ops import *
  File ""/home/adele/.local/lib/python3.6/site-packages/tensorflow/contrib/cloud/python/ops/bigquery_reader_ops.py"", line 21, in <module>
    from tensorflow.contrib.cloud.python.ops import gen_bigquery_reader_ops
  File ""/home/adele/.local/lib/python3.6/site-packages/tensorflow/contrib/cloud/python/ops/gen_bigquery_reader_ops.py"", line 369, in <module>
    _op_def_lib = _InitOpDefLibrary(b""\n\355\001\n\016BigQueryReader\032\024\n\rreader_handle\030\007\200\001\001\""\027\n\tcontainer\022\006string\032\002\022\000\""\031\n\013shared_name\022\006string\032\002\022\000\""\024\n\nproject_id\022\006string\""\024\n\ndataset_id\022\006string\""\022\n\010table_id\022\006string\""\027\n\007columns\022\014list(string)\""\027\n\020timestamp_millis\022\003int\""\034\n\016test_end_point\022\006string\032\002\022\000\210\001\001\n\331\001\n GenerateBigQueryReaderPartitions\032\016\n\npartitions\030\007\""\024\n\nproject_id\022\006string\""\024\n\ndataset_id\022\006string\""\022\n\010table_id\022\006string\""\027\n\007columns\022\014list(string)\""\027\n\020timestamp_millis\022\003int\""\025\n\016num_partitions\022\003int\""\034\n\016test_end_point\022\006string\032\002\022\000"")
  File ""/home/adele/.local/lib/python3.6/site-packages/tensorflow/contrib/cloud/python/ops/gen_bigquery_reader_ops.py"", line 277, in _InitOpDefLibrary
    _op_def_registry.register_op_list(op_list)
**AttributeError: module 'tensorflow.python.framework.op_def_registry' has no attribute 'register_op_list'**"
34761,TFLite object detection output values,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.


Hi, 

I am using TFLite in my project, which includes object detection in python. I am working on a raspberry pi so TFLite was suitable for this. I could successsfully invoke the interpreter and get some numbers as the output:

```
[[[ 1.66415479e-02  5.48024022e-04  8.67791831e-01  3.35325867e-01]
  [ 7.41335377e-02  3.22245747e-01  9.64617252e-01  9.71388936e-01]
  [-2.11861148e-03  5.41743517e-01  2.60241032e-01  7.02846169e-01]
  [-5.67546487e-03  3.26282382e-01  8.59034657e-01  6.30770981e-01]
  [ 7.27111334e-03  7.90268779e-01  2.86753297e-01  9.56545353e-01]
  [ 2.07318692e-03  7.96441555e-01  5.48386931e-01  9.96111989e-01]
  [-1.04907183e-02  2.38761827e-01  6.75976276e-01  7.01156497e-01]
  [ 3.12007014e-02  1.34294275e-02  5.82291842e-01  3.10949832e-01]
  [-1.95578858e-03  7.05318868e-01  9.18281525e-02  7.96184599e-01]
  [-5.43205580e-03  3.23292404e-01  6.34427786e-01  5.68508685e-01]]]
```

How do I convert these numbers to classes and then to thier location in the image?

Here is my code if I did something wrong:

```python
import tensorflow as tf 
import numpy as np
import cv2

interpreter = tf.lite.Interpreter(model_path=""/content/drive/My Drive/detect.tflite"")
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
print(input_details)
print(output_details)

input_shape = input_details[0]['shape']
im = cv2.imread(""/content/drive/My Drive/doggy.jpg"")
im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)
im_rgb = cv2.resize(im_rgb, (input_shape[1], input_shape[2]))
input_data = np.expand_dims(im_rgb, axis=0)
print(input_data.shape)

interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data.shape)
print()
print(output_data)
```
Output:
```
[{'name': 'normalized_input_image_tensor', 'index': 175, 'shape': array([  1, 300, 300,   3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.0078125, 128)}]
[{'name': 'TFLite_Detection_PostProcess', 'index': 167, 'shape': array([ 1, 10,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'TFLite_Detection_PostProcess:1', 'index': 168, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'TFLite_Detection_PostProcess:2', 'index': 169, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'TFLite_Detection_PostProcess:3', 'index': 170, 'shape': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]

(1, 300, 300, 3)

(1, 10, 4)

[[[ 1.66415479e-02  5.48024022e-04  8.67791831e-01  3.35325867e-01]
  [ 7.41335377e-02  3.22245747e-01  9.64617252e-01  9.71388936e-01]
  [-2.11861148e-03  5.41743517e-01  2.60241032e-01  7.02846169e-01]
  [-5.67546487e-03  3.26282382e-01  8.59034657e-01  6.30770981e-01]
  [ 7.27111334e-03  7.90268779e-01  2.86753297e-01  9.56545353e-01]
  [ 2.07318692e-03  7.96441555e-01  5.48386931e-01  9.96111989e-01]
  [-1.04907183e-02  2.38761827e-01  6.75976276e-01  7.01156497e-01]
  [ 3.12007014e-02  1.34294275e-02  5.82291842e-01  3.10949832e-01]
  [-1.95578858e-03  7.05318868e-01  9.18281525e-02  7.96184599e-01]
  [-5.43205580e-03  3.23292404e-01  6.34427786e-01  5.68508685e-01]]]
```

Thanks"
34760,"ValueError: Dimensions must be equal, but are 32 and 64 for 'conv2/Conv2D' (op: 'Conv2D') with input shapes: [?,?,?,32], [3,3,64,128].","Hello There,
I am Working on a project of converting given image to its latex form . 
I have cloned this github repo->
**https://github.com/yixuanzhou/image2latex.git**

I am using Python-2 and tensorflow_version = 0.12.1 with a GPU backend on **Google-Colab**.
I have resotred  the Weights  correctly and tried to run the file **predict.py** Without calling the function **predict** inside that file I am getting no error . But when I am calling that function with an input image ,it showed me an error that Data Format must be NHWC and the file **ops.py** under the tflib folder in that repo used data format of NCHW . So i simply changed the data format from NCHW to NHWC.
After that when I run the program **predict.py** I am getting this error :

Traceback (most recent call last):
  File ""predict.py"", line 32, in <module>
    ctx = tflib.network.im2latex_cnn(X,NUM_FEATS_START,False)
  File ""/content/image2latex/tflib/network.py"", line 79, in im2latex_cnn
    X = tf.nn.relu(tflib.ops.conv2d('conv2', X, 3, 1, num_feats, num_feats*2, pad = 'SAME', bias=False))
  File ""/content/image2latex/tflib/ops.py"", line 202, in conv2d
    out = tf.nn.conv2d(input, filters, strides=[1, 1, stride, stride], padding=pad, data_format='NHWC')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 396, in conv2d
    data_format=data_format, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2242, in create_op
    set_shapes_for_outputs(ret)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1617, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1568, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py"", line 610, in call_cpp_shape_fn
    debug_python_shape_fn, require_shape_fn)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py"", line 675, in _call_cpp_shape_fn_impl
    raise ValueError(err.message)
ValueError: Dimensions must be equal, but are 32 and 64 for 'conv2/Conv2D' (op: 'Conv2D') with input shapes: [?,?,?,32], [3,3,64,128].

I have also tried a solution proposed on StackOverflow :
-> **https://stackoverflow.com/questions/37689423/convert-between-nhwc-and-nchw-in-tensorflow**

Kindly help me regarding this issue .
-->Thanks<--

 "
34759,Using tensorflow gpu 2.1 with Cuda 10.2,"- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from: `pip`
- TensorFlow version: 2.1.0rc0
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: `pip`
- CUDA/cuDNN version: 10.2
- GPU model and memory: Quadro P5000, 16GB

**Describe the problem**

I want to use `tensorflow-gpu==2.1.0rc0` with cuda 10.2 and it seems that it can't work right now.
When I use `tensorflow-gpu=2.0.0` it works perfectly fine.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
mkdir tests2 &&\
cd tests2 &&\
virtualenv -p /usr/bin/python3.6 venv &&\
source venv/bin/activate &&\
pip install tensorflow-gpu==2.1.0rc0 &&\
python -c 'import tensorflow'
```
Which gives the following warnings:
```
2019-12-02 15:23:46.869198: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64
2019-12-02 15:23:46.869227: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2019-12-02 15:23:47.516321: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64
2019-12-02 15:23:47.516433: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64
2019-12-02 15:23:47.516449: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
```

**Any other info / logs**
When I do `locate libcudart.so`, I get the following:
```
/usr/local/cuda-10.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so
/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0
/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130
/usr/local/cuda-10.2/doc/man/man7/libcudart.so.7
/usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so
/usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2
/usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2.89
```

`locate libnvinfer_plugin.so` is empty.


"
34758,Docker Install instructions page is broken now that v2.0 is the latest image,"
##  URL(s) with the issue:
https://www.tensorflow.org/install/docker

## Documentation page with the issue:
https://www.tensorflow.org/install/docker

## Description of issue (what needs changing):
Since the latest docker builds are now tensorflow v2.x instead of v1.x the python example script on this page doesn't work out of the box. If the user copy and pastes the commands from this page they'll get a ""AttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'"" error when trying to verify their docker container.
The python example script can be changing to this to fix it:

```
import tensorflow.compat.v1 as tf
tf.enable_eager_execution();
print(tf.reduce_sum(tf.random_normal([1000, 1000])))
```

A better alternative however would be to change it to a TF v2.0 native example.

"
34757,Multiple inputs for iOS benchmark app,"Hi.
I'm trying to find a way in the ""benchmark_params.json"":
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/ios/README.md

To define multiple inputs to the network. Is it even possible?

"
34756,Cannot load model in TF2.1.0 AttributeError: 'h5py.h5f.FileID' object has no attribute 'endswith',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.1.0rc0
- Keras version: 2.2.4-tf
- Python version: 3.6.1
- CUDA/cuDNN version: 10.1 / 7.6.5
- GPU model and memory: RTX 2060 6GB

**Describe the current behavior**
I cannot load a model that has been trained with previous versions of the TensorFlow 1.x

```
Traceback (most recent call last):
  File ""C:\project\model.py"", line 56, in load_weights
    weights = model.load_weights(file, by_name=True)
  File ""C:\python36\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 234, in load_weights
    return super(Model, self).load_weights(filepath, by_name, skip_mismatch)
  File ""C:\python36\lib\site-packages\tensorflow_core\python\keras\engine\network.py"", line 1183, in load_weights
    if _is_hdf5_filepath(filepath):
  File ""C:\python36\lib\site-packages\tensorflow_core\python\keras\engine\network.py"", line 1500, in _is_hdf5_filepath
    return (filepath.endswith('.h5') or filepath.endswith('.keras') or
AttributeError: 'h5py.h5f.FileID' object has no attribute 'endswith'
```"
34753,libcublas.so.10.0: cannot open shared object file,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu server 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==1.13.1
- TensorFlow version:1.13.1
- Python version:3.6.9
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:it is saying nvidia-smi cuda 10 but on nvcc -version it is not showing 
- GPU model and memory:p40



cannot run 
import tensorflow 

>>> import tensorflow
Traceback (most recent call last):
  File ""/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/fast/anaconda3/envs/imageai/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/fast/anaconda3/envs/imageai/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/fast/anaconda3/envs/imageai/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/fast/anaconda3/envs/imageai/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.
*


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
34752,"MultiWorkerMirroredStrategy  Performance is low (2gpu, 2node)  X1.3   speed-up","System information

    Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
    OS Platform and Distribution: Ubuntu 18.04
    TensorFlow installed from (source or binary): pip install tensorflow-gpu
    TensorFlow version (use command below): 2.0
    Python version: 3.6.9
    CUDA/cuDNN version: 10/7.6.4.38
    GPU model and memory: Tesla P4  8G

Describe the current behavior
I  run the code described below:

**TEST 1:   (two machine)**

os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': [""server1:12345"", ""server2:12345""]
    },
    'task': {'type': 'worker', 'index': 0}
})

In the other machine

os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': [""server1:12345"", ""server2:12345""]
    },
    'task': {'type': 'worker', 'index': 1}
})

When the script start processing the first epoch it crashes,

**Describe the expected behavior**

15s/epoch  is so slow

<img width=""1162"" alt=""图片"" src=""https://user-images.githubusercontent.com/12653212/69707374-942a6780-1134-11ea-8dd1-994fd7e41451.png"">


**TEST 2:   (one machine)**

os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': [""server1:12345""]
    },
    'task': {'type': 'worker', 'index': 0}
})

**Describe the expected behavior**

5s/epoch      same as use  strategy = tf.distribute.MirroredStrategy()  one GPU card

<img width=""1072"" alt=""图片"" src=""https://user-images.githubusercontent.com/12653212/69707387-9b517580-1134-11ea-976a-73c9d36fdc2b.png"">


**CODE**

```
import ssl
import os
import json
import argparse
import time

import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds

ssl._create_default_https_context = ssl._create_unverified_context


def configure_cluster(worker_hosts=None, task_index=-1):
    """"""Set multi-worker cluster spec in TF_CONFIG environment variable.
    Args:
      worker_hosts: comma-separated list of worker ip:port pairs.
    Returns:
      Number of workers in the cluster.
    """"""
    tf_config = json.loads(os.environ.get('TF_CONFIG', '{}'))
    if tf_config:
        num_workers = len(tf_config['cluster'].get('worker', []))
    elif worker_hosts:
        workers = worker_hosts.split(',')
        num_workers = len(workers)
        if num_workers > 1 and task_index < 0:
            raise ValueError('Must specify task_index when number of workers > 1')
        task_index = 0 if num_workers == 1 else task_index
        os.environ['TF_CONFIG'] = json.dumps({
            'cluster': {
                'worker': workers
            },
            'task': {'type': 'worker', 'index': task_index}
        })
    else:
        num_workers = 1
    return num_workers


parser = argparse.ArgumentParser(description='TensorFlow Benchmark',
                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument('--num-epochs', type=int, default=5, help='input batch size')
parser.add_argument('--batch-size-per-replica', type=int, default=32, help='input batch size')
parser.add_argument('--worker-method', type=str, default=""NCCL"")
parser.add_argument('--worker-hosts', type=str, default=""localhost:23456"")
parser.add_argument('--worker-index', type=int, default=0)

args = parser.parse_args()

worker_num = configure_cluster(args.worker_hosts, args.worker_index)
batch_size = args.batch_size_per_replica * worker_num
print('Batch Size: %d' % batch_size)

gpus = tf.config.experimental.list_physical_devices('GPU')
print(""Physical GPU Devices Num:"", len(gpus))
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

if args.worker_method == ""AUTO"":
    communication = tf.distribute.experimental.CollectiveCommunication.AUTO
elif args.worker_method == ""RING"":
    communication = tf.distribute.experimental.CollectiveCommunication.RING
else:
    communication = tf.distribute.experimental.CollectiveCommunication.NCCL

strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
    communication=communication)


# logical_gpus = tf.config.experimental.list_logical_devices('GPU')
# print(""Logical GPU Devices Num:"", len(gpus))


def resize(image, label):
    image = tf.image.resize(image, [128, 128]) / 255.0
    return image, label


# if as_supervised is True，return image abd label
dataset, info = tfds.load(""tf_flowers"", split=tfds.Split.TRAIN, with_info=True, as_supervised=True)
dataset = dataset.map(resize).repeat().shuffle(1024).batch(batch_size)

# options = tf.data.Options()
# options.experimental_distribute.auto_shard = False
# dataset = dataset.with_options(options)

def build_and_compile_cnn_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, [3, 3], activation='relu'),
        tf.keras.layers.Conv2D(64, [3, 3], activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Dropout(0.25),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(info.features['label'].num_classes, activation='softmax')
    ])
    model.compile(
        opt=tf.keras.optimizers.Adam(learning_rate=0.0001),
        loss=tf.keras.losses.sparse_categorical_crossentropy,
        metrics=[tf.keras.metrics.sparse_categorical_accuracy]
    )
    return model


with strategy.scope():
    multi_worker_model = build_and_compile_cnn_model()
print(""Now training the distributed model"")


class TimeHistory(tf.keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.times = []
        self.totaltime = time.time()

    def on_train_end(self, logs={}):
        self.totaltime = time.time() - self.totaltime

    def on_epoch_begin(self, batch, logs={}):
        self.epoch_time_start = time.time()

    def on_epoch_end(self, batch, logs={}):
        self.times.append(time.time() - self.epoch_time_start)


time_callback = TimeHistory()
steps_per_epoch = 100
print('Running benchmark...')
multi_worker_model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=args.num_epochs, callbacks=[time_callback])
per_epoch_time = np.mean(time_callback.times[1:])
print(""per_epoch_time:"", per_epoch_time)
img_sec = batch_size * steps_per_epoch / per_epoch_time
print(""Result:  {:.1f} pic/sec"".format(img_sec))


```

In TEST 2:    only 1 worker,   440pic/sec  （batch_szie = 128）

In TEST 1:  2 workers,     610 pic/sec   （batch_szie = 128*2）   [Expect  440 *2 = 800+]

**Question1:** 
with dist MultiWorkerMirroredStrategy  worker nums > 1,   why Training is so slow

Expect

"
34750,estimator API may not save the latest variable value,"**System information**
- TensorFlow version :tf1.10.0

**Describe the current behavior**

hello,I build a model with estimator API, According to this tutorial: https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator ,I use parameter server distribute strategy to train my model. I devide the training data into several parts on average, the chief get one part, each worker get one part also. I didn't set the max train step,so the training will end when the data used up. in practical, I found that some worker is slower than chief(because of mechine performace),so the chief finish early than some workers. when the chief finish, it will dump the last checkpoint, and the training result(by other unfinished worker) after this time node will be lost.I think it may not expected.
**Describe the expected behavior**
I expect the model satisfy “all the training data used exactly once”，no mater which mechine run faster. Is there any way to make the chief wait until all worker finished，then dump the last checkpoint？

"
34749,how to convert tensorflow1.13 trained model ckpt to tflite use tensorflow2.0?,"i have a model trained use tensorflow1.13 and i can export model to pb from checkpoints and then convert to tflite file by tf.lite.TFLiteConverter.from_frozen_graph. but in tensorflow 2.0 there is no this method. my code trained use slim, so is very hard to change to tf2.0 retrain the model, are there any method to read from the original ckpt files to convert tf2.0 tflite file? "
34748,TFLite micro hello_world sketch fails to build,"**System information**
- OS Platform and Distribution: 4.19.67-2rodete2-amd64
- Target platform: Arduino 1.8.10 w/ `nano33ble` target
- TensorFlow version: 1.15.0-ALPHA

**Describe the problem**

`hello_world` sketches fails to build due to a missing header file (not included in the Arduino libraries distribution).


**Provide the exact sequence of commands / steps that you executed before running into the problem**

- Install `Arduino_TensorFlowLite@1.15.0-ALPHA`
- Open `hello_world` example sketch
- Click the `Verify` ✓ button.

**Any other info / logs**

```
/usr/local/google/home/proppy/.arduino15/packages/arduino/tools/arm-none-eabi-gcc/7-2017q4/bin/arm-none-eabi-g++ -c -w -g -Os -nostdlib @/usr/local/google/home/proppy/.arduino15/packages/arduino/hardware/mbed/1.1.3/variants/ARDUINO_NANO33BLE/defines.txt @/usr/local/google/home/proppy/.arduino15/packages/arduino/hardware/mbed/1.1.3/variants/ARDUINO_NANO33BLE/cxxflags.txt -DARDUINO_ARCH_NRF52840 -mcpu=cortex-m4 -w -x c++ -E -CC -DARDUINO=10810 -DARDUINO_ARDUINO_NANO33BLE -DARDUINO_ARCH_MBED -I/usr/local/google/home/proppy/.arduino15/packages/arduino/hardware/mbed/1.1.3/cores/arduino -I/usr/local/google/home/proppy/.arduino15/packages/arduino/hardware/mbed/1.1.3/variants/ARDUINO_NANO33BLE -I/usr/local/google/home/proppy/Arduino/libraries/Arduino_TensorFlowLite/src -I/usr/local/google/home/proppy/.arduino15/packages/arduino/hardware/mbed/1.1.3/cores/arduino/api/deprecated -iprefix/usr/local/google/home/proppy/.arduino15/packages/arduino/hardware/mbed/1.1.3/cores/arduino @/usr/local/google/home/proppy/.arduino15/packages/arduino/hardware/mbed/1.1.3/variants/ARDUINO_NANO33BLE/includes.txt /tmp/arduino_build_987011/sketch/hello_world.ino.cpp -o /dev/null
Alternatives for tensorflow/lite/c/common.h: []
ResolveLibrary(tensorflow/lite/c/common.h)
  -> candidates: []
Multiple libraries were found for ""TensorFlowLite.h""
In file included from /tmp/arduino_modified_sketch_134477/hello_world.ino:21:0:
 Used: /usr/local/google/home/proppy/Arduino/libraries/Arduino_TensorFlowLite
output_handler.h:19:10: error: tensorflow/lite/c/common.h: No such file or directory
 Not used: /usr/local/google/home/proppy/Arduino/libraries/tensorflow_lite
 #include ""tensorflow/lite/c/common.h""
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
```

/cc @dansitu @khanhlvg"
34744,Nested TensorArray using tf.function. Write->concat->write,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

 **Yes**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

**Windows 10 Pro 1809. OS Build: 17763.503**

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:

**Not applicable**

- TensorFlow installed from (source or binary):

**binary, via pip
pip3 install --user --upgrade tensorflow-gpu**

- TensorFlow version (use command below):

**v2.0.0-rc2-26-g64c3d382ca 2.0.0**

- Python version:

**3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]**

- Bazel version (if compiling from source):

**Not applicable**

- GCC/Compiler version (if compiling from source):

**Not applicable**

- CUDA/cuDNN version:

**Cuda compilation tools, release 10.0, V10.0.130
cuDNN: cudnn-10.0-windows10-x64-v7.6.5.32**

- GPU model and memory:

**GTX 1060 Ti, 6Gb**

**Describe the current behavior**

I have two functions, one calling another, in the inner function, I'm accumulating some data into a TensorArray, then return TensorArray.concat() result to the outer function. In the outer function I have a loop that calls the inner function. On each call, it retrieves the result of the inner function and writes it to it's own TensorArray. The operations could be summarized as write->concat->write. The whole program runs some random number of iterations before crashing, likely due to some non-deterministic execution of the graph, perhaps something to do with control dependencies?

Observations:
- The higher the number of iterations in the outer function (currently 50), the more quickly it fails.
- It appears 3 is a minimum number of iterations inside the outer function for the issue to occur
- Issue occurs only on GPU (CPU does fine, using ``os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1"")``. It does not occur in eager execution mode.
- Code generates a series of different errors, sporadically (see below)
- The functions do not need to be nested for the issue to occur (see reduced code version below)
- You can remove ``infer_shape=False, element_shape=(3,)`` and see it generate a different error:
``tensorflow.python.framework.errors_impl.InvalidArgumentError:  Trying to concat list with only uninitialized tensors but element_shape_except_first_dim_ is not fully defined: []	 
[[{{node while/body/_1/TensorListConcatV2}}]] [Op:__inference_computeElement_71]
``
- It may take many runs to encounter an error (up to a 1000 on my machine)

**Describe the expected behavior**

Code producing same result and running indefinitely.

**Code to reproduce the issue**
```python
@tf.function
def computeElement_byBin():
	c = tf.TensorArray(tf.int64, size=1, infer_shape=False, element_shape=(3,))
	const = tf.cast(tf.constant([1, 2, 3]), tf.int64)
	c = c.write(0, const)
	c_c = c.concat()
	return c_c

@tf.function
def computeElement():
	c = tf.TensorArray(tf.int64, size=50, infer_shape=False, element_shape=(3,))
	for x in tf.range(50):
		byBinVariant = computeElement_byBin()
		c = c.write(x, byBinVariant)

	return tf.constant([1, 2, 3])

k=0
while True:
	k+=1
	r = computeElement()
	print('iteration: %s, result: %s'%(k,r))
```

Here's a reduced version of code, without nesting (behaves similarly):

``` python
@tf.function
def computeElement():
	tensorArray1 = tf.TensorArray(tf.int32, size=50, infer_shape=False, element_shape=(3,))
	tensorArray2 = tf.TensorArray(tf.int32, size=1, infer_shape=False, element_shape=(3,))
	const = tf.constant([1, 2, 3])
	for x in tf.range(50):
		tensorArray2 = tensorArray2.write(0, const)
		concat = tensorArray2.concat()
		tensorArray1=tensorArray1.write(x, concat)
	return tf.constant(1)

k=0
while True:
	k+=1
	r = computeElement()
	print('iteration: %s, result: %s'%(k,r))
```

**Other info / logs**
Output (example):

```
2019-12-01 23:29:49.834579: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2019-12-01 23:29:50.917651: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-12-01 23:29:50.988907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce GTX 1660 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.77
pciBusID: 0000:01:00.0
2019-12-01 23:29:50.992256: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-12-01 23:29:50.995402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-01 23:29:50.996693: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-12-01 23:29:50.999987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce GTX 1660 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.77
pciBusID: 0000:01:00.0
2019-12-01 23:29:51.001975: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-12-01 23:29:51.003600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-01 23:29:51.501197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-01 23:29:51.503618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2019-12-01 23:29:51.505070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2019-12-01 23:29:51.506907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4629 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)

iteration: 1, result: tf.Tensor([1 2 3], shape=(3,), dtype=int32)
iteration: 2, result: tf.Tensor([1 2 3], shape=(3,), dtype=int32)
iteration: 3, result: tf.Tensor([1 2 3], shape=(3,), dtype=int32)
iteration: 4, result: tf.Tensor([1 2 3], shape=(3,), dtype=int32)
iteration: 5, result: tf.Tensor([1 2 3], shape=(3,), dtype=int32)
iteration: 6, result: tf.Tensor([1 2 3], shape=(3,), dtype=int32)
r=outer()
  File ""Python\Python37\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""Python\Python37\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 494, in _call
    results = self._stateful_fn(*args, **kwds)
  File ""Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1823, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1141, in _filtered_call
    self.captured_inputs)
  File ""Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File ""Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py"", line 511, in call
    ctx=ctx)
  File ""Python\Python37\lib\site-packages\tensorflow_core\python\eager\execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Tried to set a tensor with incompatible shape at a list index. Item element shape: [3,3] list shape: [3]
	 [[{{node while/body/_1/TensorArrayV2Write/TensorListSetItem}}]] [Op:__inference_computeElement_79]
```

Other errors that code may generate:
```
Process finished with exit code -1073740791 (0xC0000409)
(No stack trace)
```
or
```
2019-12-01 23:10:55.194128: F tensorflow/core/framework/tensor_shape.cc:445] Check failed: end <= dims() (1 vs. 0)

Process finished with exit code -1073740791 (0xC0000409)
```

Cheers!"
34743,"Will TF2.0 build with Python3 ONLY, without Python2? ","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 19.10
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0
- Python version: 3.7.5
- Installed using virtualenv? pip? conda?: pip 
- Bazel version (if compiling from source):1.1.0
- GCC/Compiler version (if compiling from source): 9.2.1
- CUDA/cuDNN version:10.2/7.6.5
- GPU model and memory: GeForce GTX 980M/4035MiB



```console
ERROR: ~/Downloads/....../tensorflow/python/keras/api/BUILD:129:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1)
Traceback (most recent call last):
  File ""~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 771, in <module>
    main()
  File ""~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 767, in main
    lazy_loading, args.use_relative_imports)
  File ""~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 625, in create_api_files
    api_version, compat_api_versions, lazy_loading, use_relative_imports)
  File ""~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 502, in get_api_init_text
    _, attr = tf_decorator.unwrap(attr)
  File ""~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py"", line 219, in unwrap
    elif _has_tf_decorator_attr(cur):
  File ""~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py"", line 124, in _has_tf_decorator_attr
    hasattr(obj, '_tf_decorator') and
  File ""~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py"", line 62, in __getattr__
    module = self._load()
  File ""~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py"", line 45, in _load
    module = importlib.import_module(self.__name__)
  File ""/usr/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py"", line 28, in <module>
    _wrap_py_utils = swig_import_helper()
  File ""~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_wrap_py_utils', fp, pathname, description)
  File ""/usr/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
  File ""<frozen importlib._bootstrap>"", line 696, in _load
  File ""<frozen importlib._bootstrap>"", line 670, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 583, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 1043, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: ~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_
----------------
Note: The failure of target //tensorflow/python/keras/api:create_tensorflow.python_api_2_keras_python_api_gen_compat_v2 (with exit code 1) may have been caused by the fact that it is a Python 2 program that was built in the host configuration, which uses Python 3. You can change the host configuration (for the entire build) to instead use Python 2 by setting --host_force_python=PY2.

If this error started occurring in Bazel 0.27 and later, it may be because the Python toolchain now enforces that targets analyzed as PY2 and PY3 run under a Python 2 and Python 3 interpreter, respectively. See https://github.com/bazelbuild/bazel/issues/7899 for more information.
----------------
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: ~/....../tensorflow/tools/pip_package/BUILD:40:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1)
INFO: Elapsed time: 12747.266s, Critical Path: 300.17s
INFO: 16213 processes: 16213 local.
FAILED: Build did NOT complete successfully
➜  tensorflow git:(master) ✗ 
```"
34742,libtensorflow_framework.so.2 segfault in OPENSSL_strcasecmp,"**System information**
- OS Platform and Distribution Ubuntu 18.04 on jetson xavier
- TensorFlow installed from source
- TensorFlow version: 2.0.0
- Python version: 3.6.9
- Bazel version: 0.26.1
- GCC/Compiler version: 7.4.0
- CUDA/cuDNN version: 10.0.326-1/ 7.5.0.56 installed from nvidia repo

after compiling tensorflow 2.0 from source my application segfaults this way:

```
(gdb) bt
#0  0x0000007fa56656a8 in OPENSSL_strcasecmp () at /usr/local/lib/libtensorflow_framework.so.2
#1  0x0000007fa5623d44 in EVP_get_cipherbyname () at /usr/local/lib/libtensorflow_framework.so.2
#2  0x0000007f77b0258c in  () at /usr/lib/aarch64-linux-gnu/libssl.so.1.1
#3  0x0000007f77b5c88c in  () at /usr/lib/aarch64-linux-gnu/libssl.so.1.1
```

openssl version is 1.1.1-1ubuntu2.1~18.04.5.

I'm using tensorflow inside a Qt application but probably this make no difference.

is there any way to prevent this issue and/or remove openssl dependency while building?

libtensorflow_framework.so.2 is not linked to openssl

```
ldd /usr/local/lib/libtensorflow_framework.so.2 
linux-vdso.so.1 (0x0000007f84044000)
librt.so.1 => /lib/aarch64-linux-gnu/librt.so.1 (0x0000007f826d2000)
libpthread.so.0 => /lib/aarch64-linux-gnu/libpthread.so.0 (0x0000007f826a6000)
libdl.so.2 => /lib/aarch64-linux-gnu/libdl.so.2 (0x0000007f82691000)
libm.so.6 => /lib/aarch64-linux-gnu/libm.so.6 (0x0000007f825d7000)
libstdc++.so.6 => /usr/lib/aarch64-linux-gnu/libstdc++.so.6 (0x0000007f82444000)
libgcc_s.so.1 => /lib/aarch64-linux-gnu/libgcc_s.so.1 (0x0000007f82420000)
libc.so.6 => /lib/aarch64-linux-gnu/libc.so.6 (0x0000007f822c7000)
/lib/ld-linux-aarch64.so.1 (0x0000007f84019000)
```

thanks
"
34741,NameError: name 'x_train' is not defined,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom Code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (GPU)
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): 1.15.0
- Python version: 3.6


**Describe the current behavior**
Error when using `fit_generator()`:  
`ModuleNotFoundError: No module named 'tensorflow_core.compat'`

**Describe the expected behavior**
The model should begin training.

**Code to reproduce the issue**
```
from google.colab import drive
drive.mount('/content/drive')

!pip install --quiet tensorflow==2.0.0-rc0
!pip install --quiet neural-structured-learning

from __future__ import absolute_import, division, print_function, unicode_literals

# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import datasets, layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator

print(tf.__version__)


import numpy as np
import matplotlib.pyplot as plt
import os
import cv2
from tqdm import tqdm
import pathlib
import random

train_data_dir = ""/content/drive/My Drive/Resize/train""
train_label_dir = pathlib.Path(train_data_dir)

test_data_dir = ""/content/drive/My Drive/Resize/test""
test_label_dir = pathlib.Path(test_data_dir)



CATEGORIES = np.array([item.name for item in train_label_dir.glob('*') if item.name != ""LICENSE.txt""])
class_names = CATEGORIES
print(CATEGORIES)


def createdataset(DATADIR, label_dir, CATEGORIES, img_size):
    image_count = len(list(label_dir.glob('*/*.jpg')))
    print(image_count)

    # for category in CATEGORIES:  # do dogs and cats
    #     path = os.path.join(DATADIR,category)  # create path to dogs and cats
    #     for img in os.listdir(path):  # iterate over each image per dogs and cats
    #         img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array
    #     #     break  # we just want one for now so break
    #     # break  #...and one more!

    IMG_SIZE = img_size

    datalist = []

    for category in CATEGORIES:  # do dogs and cats

        path = os.path.join(DATADIR,category)  # create path to dogs and cats
        class_num = np.where(CATEGORIES == category)

        for img in tqdm(os.listdir(path)):  # iterate over each image per dogs and cats
            try:
                img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_COLOR)  # convert to array
                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size
                datalist.append([new_array, class_num])  # add this to our training_data
            except Exception as e:  # in the interest in keeping the output clean...
                pass
            #except OSError as e:
            #    print(""OSErrroBad img most likely"", e, os.path.join(path,img))
            #except Exception as e:
            #    print(""general exception"", e, os.path.join(path,img))
    return datalist


training_dataset = createdataset(train_data_dir, train_label_dir, CATEGORIES, 200)
testing_dataset = createdataset(test_data_dir, test_label_dir, CATEGORIES, 200)

print(len(training_dataset))
print(len(testing_dataset))


def dataset(datasets):
    xdata = []
    ylabels = []
    # random.shuffle(datasets)
    for datas,labels in datasets:
        xdata.append(datas)
        ylabels.append(labels)
    return xdata, ylabels

train_images, train_labels = dataset(training_dataset)
test_images, test_labels = dataset(testing_dataset)
print(len(train_images))
print(len(train_labels))
print(len(test_images))
print(len(test_labels))



train_images = np.array(train_images)
test_images = np.array(test_images)
train_labels = np.array(train_labels)
test_labels = np.array(test_labels)

print(train_images.shape)
print(test_images.shape)

# print(train_labels)
# print(test_labels)



train_images = train_images / 255.0
test_images = test_images / 255.0

# plt.figure(figsize=(10,10))
# for i in range(25):
#     plt.subplot(5,5,i+1)
#     plt.xticks([])
#     plt.yticks([])
#     plt.grid(False)
#     plt.imshow(train_images[i], cmap=plt.cm.binary)
#     plt.xlabel(class_names[train_labels[i]])
# plt.show()


epochs = 5
batch_size = 50

datagen = ImageDataGenerator(
    featurewise_center=True,
    featurewise_std_normalization=True,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True)

print(""Augmenting the shitty dataset"")
datagen.fit(train_images, augment=True, rounds=5)





model = models.Sequential()

model.add(layers.Conv2D(32, (3, 3), input_shape=(200,200,3)))
model.add(layers.BatchNormalization(axis=-1))
model.add(layers.Activation('relu'))
model.add(layers.Conv2D(32, (3, 3)))
model.add(layers.BatchNormalization(axis=-1))
model.add(layers.Activation('relu'))
model.add(layers.MaxPooling2D(pool_size=(2,2)))

model.add(layers.Conv2D(64,(3, 3)))
model.add(layers.BatchNormalization(axis=-1))
model.add(layers.Activation('relu'))
model.add(layers.Conv2D(64, (3, 3)))
model.add(layers.BatchNormalization(axis=-1))
model.add(layers.Activation('relu'))
model.add(layers.MaxPooling2D(pool_size=(2,2)))

model.add(layers.Flatten())

# Fully connected layer
model.add(layers.Dense(512))
model.add(layers.BatchNormalization())
model.add(layers.Activation('relu'))
model.add(layers.Dropout(0.2))
model.add(layers.Dense(4))

model.add(layers.Activation('softmax'))


model.summary()

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),
                    steps_per_epoch=len(x_train) / 32, epochs=epochs)


test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)

print('\nTest accuracy:', test_acc)
```

**Other info / logs**
```
ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2882, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-22-ce52d11f71b2>"", line 1, in <module>
    model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),
NameError: name 'x_train' is not defined

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 1823, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'NameError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py"", line 1132, in get_records
    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py"", line 313, in wrapped
    return f(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py"", line 358, in _fixed_getinnerframes
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
  File ""/usr/lib/python3.6/inspect.py"", line 1490, in getinnerframes
    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
  File ""/usr/lib/python3.6/inspect.py"", line 1448, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File ""/usr/lib/python3.6/inspect.py"", line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File ""/usr/lib/python3.6/inspect.py"", line 733, in getmodule
    if ismodule(module) and hasattr(module, '__file__'):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'tensorflow_core.compat'
```"
34740,Autograph failure with tf.ragged.boolean_mask,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, providing source
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.15.1, most likely irrelevant.
- TensorFlow installed from (source or binary): binary from pip
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: 3.7.2
- CUDA/cuDNN version: using CPU only.

**Describe the current behavior**
I have a function computing a loss for a Keras model that produces multiple outputs of different sizes, and chooses one of them based on an index received as input from a tf.data.Dataset. Since the desired outputs have different sizes, they are stored in the dataset as ragged tensors and tf.ragged.boolean_mask is used to perform appropriate selections based on the index.

**Describe the expected behavior**
The function works fine in eager mode, but raises the following exception when trying to apply @tf.function on it:

> 2019-12-01 22:06:26.103694: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Input to reshape is a tensor with 388 values, but the requested shape has 408
> Traceback (most recent call last):
>   File ""ragged_bug.py"", line 50, in <module>
>     loss, grads = get_loss(model, inputs, outputs, index)
>   File "".../lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 457, in __call__
>     result = self._call(*args, **kwds)
>   File "".../lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 494, in _call
>     results = self._stateful_fn(*args, **kwds)
>   File "".../lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1823, in __call__
>     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
>   File "".../lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1141, in _filtered_call
>     self.captured_inputs)
>   File "".../lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1224, in _call_flat
>     ctx, args, cancellation_manager=cancellation_manager)
>   File "".../lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 511, in call
>     ctx=ctx)
>   File "".../lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py"", line 67, in quick_execute
>     six.raise_from(core._status_to_exception(e.code, message), None)
>   File ""<string>"", line 3, in raise_from
> tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input to reshape is a tensor with 388 values, but the requested shape has 408
> 	 [[node RaggedMask_1/RaggedMask/boolean_mask/Reshape (defined at .../lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_get_loss_1681]

I can confirm that making all model output sizes the same removes the problem, suggesting an issue with the use of ragged tensors in this situation. Also, the actual exception values change on every run since they depend on the random data being generated.

**Code to reproduce the issue**
[ragged_bug.py.txt](https://github.com/tensorflow/tensorflow/files/3908709/ragged_bug.py.txt)

"
34739,Embedding visualization in TensorFlow 2.0 not supported,"This is the way I create a Tensorboard callback in the Keras:

```
from keras.callbacks import TensorBoard
tbCallBack = TensorBoard(log_dir='./Graph', 
                         histogram_freq=1, 
                         embeddings_freq=1, 
                         embeddings_data=embedding_matrix, 
                         write_graph=True, 
                         write_images=True)
```

What I get is:

```
/usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v2.py:102: UserWarning: The TensorBoard callback does not support embeddings display when using TensorFlow 2.0. Embeddings-related arguments are ignored.
  warnings.warn('The TensorBoard callback does not support '
```

When I launched Tensorboard, `Projector` is under `Inactive`

"
34738,Delete init_op option,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

**System information**
- TensorFlow version (you are using): 1.15.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
I am using HiAI-DDK in order to deploy my model on mobile phone. The model transform failed with debug information that the _init_op_ is not supported. I think this should be due to the redundant part in the initialization procedure for trained model. So I wonder if there exists the feature that allows me to delete a node in a graph (especially tf_rep). 
I am using onnx-tf to do the transformation. 

**Will this change the current api? How?**
Add node deleting interface, (not sure)

**Who will benefit with this feature?**
Engineers working with model deployment.

**Any Other info.**
"
34737,"tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 294912 values, but the requested shape has 73728","I downloaded the precomputed image features from 
[https://github.com/kuanghuei/SCAN](url)

and then tried to store them in a tfrecord file and trying to read them from that file but then i get this array. any help will be appreciated.
"
34736,2.1.0rc0 doesn't find GPU libraries,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 1909
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.1.0rc0
- Python version: 3.7.5
- Installed using virtualenv? pip? conda?: pip/virtualenv
- CUDA/cuDNN version: Cuda 10.0, cudnn v7.6.4
- GPU model and memory: rtx 2060 6gb



**Describe the problem**

2.1.0rc0 and nightly builds doesn't see any of the libraries below, and it defaults to use CPU. 


**Any other info / logs**


2019-12-01 20:12:55.597639: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2019-12-01 20:12:55.601941: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
timestamp divided..
timestamp divided..
(1112,)
2019-12-01 20:12:59.141649: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-12-01 20:12:59.168743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2060 computeCapability: 7.5
coreClock: 1.695GHz coreCount: 30 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s
2019-12-01 20:12:59.176962: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2019-12-01 20:12:59.181864: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found
2019-12-01 20:12:59.186511: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found
2019-12-01 20:12:59.191583: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found
2019-12-01 20:12:59.197264: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found
2019-12-01 20:12:59.202050: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_10.dll'; dlerror: cusparse64_10.dll not found
2019-12-01 20:12:59.216645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-12-01 20:12:59.219553: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing 
libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2019-12-01 20:12:59.232902: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not 
compiled to use: AVX2
2019-12-01 20:12:59.236973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-01 20:12:59.246142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]
<RepeatDataset shapes: ((None, 720, 1), (None,)), types: (tf.float64, tf.float64)>
(720, 1)
Train for 200 steps, validate for 50 steps
"
34735,tensorflow-gpu 2.0 cannot be imported,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Ubuntu 18.04)
- TensorFlow installed from (source or binary)
- TensorFlow version: tensorflow-gpu 2.0.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: installed using pip
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0 / 7.6.4
- GPU model and memory: GeForce GTX, driveri 410.104

i have tensorflow-estimator 2.0 & tensorflow-gpu 2.0 installed, and it shows under 'pip list' and 'conda list'.

but import tensorflow is not working as attached picture;


![capture](https://user-images.githubusercontent.com/46334448/69915721-9888b480-1495-11ea-8593-f93413cc9ccd.JPG)

please help..!! thank you all for your help in advance!"
