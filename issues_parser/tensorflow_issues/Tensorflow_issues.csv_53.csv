Issue Number,Issue Title,Issue Body
19382,Tensor Forest generates TWO sets of files while training,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS
- **TensorFlow installed from (source or binary)**: Tensorflow CPU installed with pip
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**:  3.6.4

### Describe the problem
I am building a simple classification model using pre-made Tensor Forest Estimator. While training, my classifier generates one graph and one file with checkpoints but TWO sets of model data, here are the files generated after model fitting the training data:

```
checkpoint                      
events.out.tfevents.1526645293.Iulias-MacBook-Air.local 
graph.pbtxt 
model.ckpt-1.data-00000-of-00002
model.ckpt-1.data-00001-of-00002
model.ckpt-1.index
model.ckpt-1.meta
model.ckpt-299.data-00000-of-00002                  
model.ckpt-299.data-00001-of-00002          
model.ckpt-299.index            
model.ckpt-299.meta
```
While training I'm also getting multiple warnings from TF about errors during serializing, like:

WARNING:tensorflow:Error encountered when serializing resources. Type is unsupported, or the types of the items don't match field type in CollectionDef. '_Resource' object has no attribute 'name'

Could it be the reason?

My code is below:

```
import pickle
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
import tensorflow as tf
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from tensorflow.contrib.tensor_forest.client import eval_metrics
import itertools
resale_postback_df = pd.read_pickle('./stats')

features_list = ['feature_1', 'feature_2','feature_3','feature_4','feature_6', 'feature_7', 'feature_8']
labels_list = ['tf_labels']

print(labels_test.values)

features_train, features_test, labels_train, labels_test = train_test_split(resale_postback_df[features_list], 
                                                                            resale_postback_df.tf_labels,
                                                                           test_size=0.3,random_state = 42, shuffle = True)
#defining custom input function for Tensor Forest
def generate_tf_input_fn(x_input,y_input,num_epochs=None):
    #this is the function we are generating
    def _input_fn_():
        # generate a standard input function
        train_input_fn = tf.estimator.inputs.pandas_input_fn(
            x= x_input,  
            y= y_input,
            num_epochs=num_epochs,
            shuffle=True
        )
        #execute the standard input function 
        x, y = train_input_fn()
        # expand the shape of the results (necessary for Tensor Forest)
        for name in x:
            x[name] = tf.expand_dims(x[name], 1, name= name) 
        return x, y

    return _input_fn_

train_input_fn = generate_tf_input_fn(features_train,labels_train,num_epochs=None)
test_input_fn = generate_tf_input_fn(features_test, labels_test, num_epochs=None)

tf.logging.set_verbosity(0)
hparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(num_classes=len(resale_postback_df.tf_labels.unique()),
                                          regression= False,
                                          num_features=len(features_list),
                                          num_trees=50,
                                          min_split_samples=2,
                                          verbose = 0).fill()

SAVE_PATH = './save'

classifier = tf.contrib.tensor_forest.client.random_forest.TensorForestEstimator(hparams,model_dir = SAVE_PATH)
classifier.fit(input_fn = train_input_fn)
```
Can somebody explain to me hpw to solve this problem ?
"
19380,Tensorflow Lite AllocateTensors() fails with custom model,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
- **Python version**: 
('v1.8.0-0-g93bc2e2072', '1.8.0')
- **Bazel version (if compiling from source)**:
```
Build label: 0.13.0
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Mon Oct 18 21:33:40 +50297 (1525078013620)
Build timestamp: 1525078013620
Build timestamp as int: 1525078013620

```
- **GCC/Compiler version (if compiling from source)**:
gcc version 5.5.0 20171010 (Ubuntu 5.5.0-12ubuntu1~16.04) 
- **CUDA/cuDNN version**:
None
- **GPU model and memory**:
None
- **Exact command to reproduce**:

### Python-Code:
```
from keras import Input, Model
from keras.layers import Conv2D
import keras.backend as K
import tensorflow as tf
from tensorflow.python.framework import graph_util
# manually put back imported modules
import tempfile
import subprocess

tf.contrib.lite.tempfile = tempfile
tf.contrib.lite.subprocess = subprocess

K.set_learning_phase(0)
input = Input((416, 416, 3),name='Placeholder')
conv1 = Conv2D(16, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False)(input)
model = Model(input, conv1)
netin = [K.placeholder(name=""Input"", dtype=tf.float32, shape=(416,416,3))]
netout = [K.identity(model.outputs[0],""Prediction"")]
sess = K.get_session()

constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, [""Prediction""])

tflite_model = tf.contrib.lite.toco_convert(constant_graph, netin, netout)
open('model' + '.tflite', ""wb"").write(tflite_model)

```

### C-Code:

```
//based on tensorflow/contrib/lite/examples/minimal/minimal.cc

#include ""tensorflow/contrib/lite/model.h""
#include ""tensorflow/contrib/lite/interpreter.h""
#include ""tensorflow/contrib/lite/kernels/register.h""
#include ""tensorflow/contrib/lite/kernels/kernel_util.h""
#include <cstdio>
#include <iostream>
#include ""exp.h""
#include ""opencv2/highgui.hpp""
#include ""opencv2/core.hpp""
// Usage: minimal <tflite model>

using namespace tflite;

#define TFLITE_MINIMAL_CHECK(x) \
  if(!(x)) {                                                    \
    fprintf(stderr, ""Error at %s:%d\n"",  __FILE__, __LINE__); \
    exit(1); \
  }


int main(int argc, char *argv[]) {
  if(argc != 3) {
    fprintf(stderr, ""Usage: <model> <image>\n"");
    return 1;
  }else{
      std::cout << ""Reading model from: "" << argv[1] << std::endl;
      std::cout << ""Reading image from: "" << argv[2] << std::endl;
  }
  const char* filename = argv[1];
    const char* imagefile = argv[2];

  // Load model
  std::unique_ptr<tflite::FlatBufferModel> model
      = tflite::FlatBufferModel::BuildFromFile(filename);
  TFLITE_MINIMAL_CHECK(model != nullptr);



  // Build the interpreter
  tflite::ops::builtin::BuiltinOpResolver resolver;
  resolver.AddCustom(""Exp"", Register_EXP());

  InterpreterBuilder builder(*model.get(), resolver);
  std::unique_ptr<Interpreter> interpreter;
  builder(&interpreter);
  TFLITE_MINIMAL_CHECK(interpreter != nullptr);
    cv::Mat cvimg = cv::imread(imagefile);
    cv::imshow(""Input"",cvimg);
    cv::waitKey(1);

  // Allocate tensor buffers.
    TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);
    // Fill input buffers

    int input = interpreter->inputs()[0];
    memcpy(interpreter->typed_input_tensor<float>(input), cvimg.data, cvimg.total() * cvimg.elemSize());

  // Run inference
    TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);

  // Read output buffers
  // TODO(user): Insert getting data out code.
    //int output = interpreter->outputs()[0];
    std::cout << interpreter->typed_output_tensor<float>(0) << std::endl;
  return 0;
}
```
### Describe the problem

I convert a keras model (very simplified to reproduce error) to tflite. I verified the graph using tensorboard and it seems fine. However, when I want to run it in C++ the following error occurs during the call of interpreter->AllocateTensors():

```
tensorflow/contrib/lite/kernels/conv.cc:189 input->dims->size != 4 (0 != 4)
```
This does not happen when loading a model from [here](https://www.tensorflow.org/mobile/tflite/demo_android). Is this a bug in the graph conversion in the model loader or am I doing sth wrong?
"
19378,tf.gfile.Glob returns wrong values on Windows systems,"### System information
Environment:
- **Python 3.6.2**
- **Tensorflow 'v1.8.0-0-g93bc2e2072' 1.8.0**
- **OS : Windows 7**

- **Exact command to reproduce**:

```
 > ls -R
.:
example/

./example:
subdir/

./example/subdir:
file1  file2


> python
Python 3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> tf.gfile.Glob(""example/*"")
['example\\subdir', 'example\\subdir\\file1', 'example\\subdir\\file2']

```

### Describe the problem
Behaviour of _tf.gfile.Glob_ is different on Windows and Linux-based (POSIX) systems.
On Windows, the result includes all files and sub-directories of the matching pattern (=> recursive)
On Linux, the result includes only files and directories matched by the pattern (=> not recursive)

As an example, given the directory structure above, I have the following result:

* Windows
```python
>>> tf.gfile.Glob(""example/*"")
['example\\subdir', 'example\\subdir\\file1', 'example\\subdir\\file2']
```

* Linux-based systems
```python
>>> tf.gfile.Glob(""example/*"")
['example/subdir']
```

The result returned on Windows platform is clearly wrong.

This has an impact on my application because I do something like `[my_list]=tf.gfile.Glob(pattern)` and it doesn't work on Windows whereas it works on Linux.
Error on Windows is `ValueError: too many values to unpack `


### Source code
I tracked down the issue in TF code and it is related to how _GetMatchingPaths_ works. See difference between [Windows implementation](https://github.com/tensorflow/tensorflow/blob/e7f158858479400f17a1b6351e9827e3aa83e7ff/tensorflow/core/platform/windows/windows_file_system.cc) and [POSIX implementation](https://github.com/tensorflow/tensorflow/blob/e7f158858479400f17a1b6351e9827e3aa83e7ff/tensorflow/core/platform/posix/posix_file_system.cc).
Both of them refer to [internal::GetMatchingPaths](https://github.com/tensorflow/tensorflow/blob/6ba9573a702ea9e1290517cce4e5f73a14552ad9/tensorflow/core/platform/file_system_helper.cc) which seems to be the root cause of the issue.


"
19376,The channel dimension of the inputs is `None` when tf.layers.conv2d after tf.slice with tf.shape instead of constant value,"Here is my sample code to test the case:
```python
import tensorflow as tf
import numpy as np

if __name__ == '__main__':
    n_channel = 3
    n_layer = 5
    input_tensor = tf.placeholder(tf.float32, shape=[None, None, None, n_channel])
    input_image = np.random.rand(4, 512, 512,n_channel)

    slice = tf.slice(input_tensor, [0,0,0,0], [-1, tf.shape(input_tensor)[1], tf.shape(input_tensor)[2], -1])
    conv = tf.layers.conv2d(slice,filters=16,kernel_size=3,strides=1,padding='valid')

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        output_image = sess.run(conv,feed_dict={input_tensor:input_image})
    print(output_image.shape)
```
Here is the error:
```python-traceback
Traceback (most recent call last):
  File ""test_concat_then_conv.py"", line 11, in <module>
    conv = tf.layers.conv2d(slice,filters=16,kernel_size=3,strides=1,padding='valid')
  File ""/home/python3.6/site-packages/tensorflow/python/layers/convolutional.py"", line 619, in conv2d
    return layer.apply(inputs)
  File ""/home/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 825, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/home/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 696, in __call__
    self.build(input_shapes)
  File ""/home/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py"", line 133, in build
    raise ValueError('The channel dimension of the inputs '
ValueError: The channel dimension of the inputs should be defined. Found `None`.
```
This happen only when I used `tf.layers.conv2d`. It works with `tf.nn.conv2d`.
I wonder what is the root of this problem?"
19375,python3: Relink issue,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 78.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 9.2
- **GPU model and memory**:  GeForce GTX 980 Ti - 6GB
- **Exact command to reproduce**:
python3 
import tensorflow as tf

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

== cat /etc/issue ===============================================
Linux blue 4.15.0-20-generic #21-Ubuntu SMP Tue Apr 24 06:16:15 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""18.04 LTS (Bionic Beaver)""
VERSION_ID=""18.04""
VERSION_CODENAME=bionic

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 7.3.0-16ubuntu3) 7.3.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux blue 4.15.0-20-generic #21-Ubuntu SMP Tue Apr 24 06:16:15 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.14.3)
protobuf (3.5.2.post1)
tensorflow-gpu (1.8.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libcufft.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri May 18 09:03:01 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 980 Ti  Off  | 00000000:05:00.0 Off |                  N/A |
| 22%   60C    P0    71W / 250W |      0MiB /  6080MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 980 Ti  Off  | 00000000:06:00.0 Off |                  N/A |
| 22%   57C    P0    67W / 250W |      0MiB /  6083MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX 980 Ti  Off  | 00000000:09:00.0 Off |                  N/A |
|  0%   50C    P0    59W / 250W |      0MiB /  6083MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.2/doc/man/man7/libcudart.7
/usr/local/cuda-9.2/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.88
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a


You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

Can't run tensorflow because of this error:

python3: Relink `/lib/x86_64-linux-gnu/libudev.so.1' with `/lib/x86_64-linux-gnu/librt.so.1' for IFUNC symbol `clock_gettime'
Segmentation fault (core dumped)

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
19374,TensorFlow import issue with Deepfix.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.0.1
- **Python version**:  python -V
Python 2.7.14 :: Anaconda, Inc.

- **Bazel version (if compiling from source)**:
.........................................................................
Build label: 0.11.1
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue May 14 07:48:23 +50148 (1520362424903)
Build timestamp: 1520362424903
Build timestamp as int: 1520362424903

- **GCC/Compiler version (if compiling from source)**:
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

The output of above script is as follows

`ujjval@ujjval-VPCEH18FG:~/Downloads/tensorflow-master/tools$ ./tf_env_collect.shCollecting system information...
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Wrote environment to tf_env.txt. You can review the contents of that file.
and use it to populate the fields in the github issue template.

cat tf_env.txt`


You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.0.0-65-g4763edf-dirty', '1.0.1')

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I have some problems in importing the Tensorflow in Python. I am using Ubuntu 16.04. I am trying to perform Deepfix experiments. 
As given on the Deepfix website [https://bitbucket.org/iiscseal/deepfix](url) I have tried running the script
source init.sh
but it gives me the error. Here is my output to the command

`~/iiscseal-deepfix-7a4f0b6122d4$ source init.sh
Setting up a new virtual environment...


CondaValueError: prefix already exists: /home/ujjval/miniconda2/envs/deepfix

done!
Requirement already satisfied: subprocess32 in /home/ujjval/miniconda2/envs/deepfix/lib/python2.7/site-packages (3.5.0)
Requirement already satisfied: tensorflow-gpu==1.0.1 in /home/ujjval/miniconda2/envs/deepfix/lib/python2.7/site-packages (1.0.1)
Requirement already satisfied: regex in /home/ujjval/miniconda2/envs/deepfix/lib/python2.7/site-packages (2018.2.21)
Requirement already satisfied: mock>=2.0.0 in /home/ujjval/miniconda2/envs/deepfix/lib/python2.7/site-packages (from tensorflow-gpu==1.0.1) (2.0.0)
Requirement already satisfied: six>=1.10.0 in /home/ujjval/miniconda2/envs/deepfix/lib/python2.7/site-packages (from tensorflow-gpu==1.0.1) (1.11.0)
Requirement already satisfied: numpy>=1.11.0 in /home/ujjval/miniconda2/envs/deepfix/lib/python2.7/site-packages (from tensorflow-gpu==1.0.1) (1.14.3)
Requirement already satisfied: protobuf>=3.1.0 in /home/ujjval/miniconda2/envs/deepfix/lib/python2.7/site-packages (from tensorflow-gpu==1.0.1) (3.5.2.post1)
Requirement already satisfied: wheel in /home/ujjval/miniconda2/envs/deepfix/lib/python2.7/site-packages (from tensorflow-gpu==1.0.1) (0.31.0)
Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /home/ujjval/miniconda2/envs/deepfix/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow-gpu==1.0.1) (1.0.2)
Requirement already satisfied: pbr>=0.11 in /home/ujjval/miniconda2/envs/deepfix/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow-gpu==1.0.1) (4.0.2)
Requirement already satisfied: setuptools in /home/ujjval/miniconda2/envs/deepfix/lib/python2.7/site-packages (from protobuf>=3.1.0->tensorflow-gpu==1.0.1) (39.1.0)
mkdir: cannot create directory ‘temp’: File exists
mkdir: cannot create directory ‘logs’: File exists
mkdir: cannot create directory ‘data/results’: File exists

Downloading DeepFix dataset...
Preprocessing DeepFix dataset...
Make sure that your tensorflow is version 1.0.1 before proceeding, checking your tensorflow version now!

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/ujjval/miniconda2/envs/deepfix/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/ujjval/miniconda2/envs/deepfix/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/ujjval/miniconda2/envs/deepfix/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/ujjval/miniconda2/envs/deepfix/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/home/ujjval/miniconda2/envs/deepfix/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
`

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
19373, No gradient defined for operation 'SparseSlice',"i am working on solve a indeterminate equation optimization with sparse_slice functions and met this error.
it's strange all sparse functions have gradient except this one. any suggestion how can i avoid this problem?

### Source code / logs
import tensorflow as tf
sparse_values = tf.identity(tf.Variable(tf.constant([1., 2., 3.])))
sparse_indices = tf.constant([[0, 0], [1, 1], [2, 2]], dtype=tf.int64)
sparse_matrix = tf.SparseTensor(sparse_indices, sparse_values, [3, 3])
sparse_slice = tf.sparse_slice(sparse_matrix,[0,0], [2,2])
multiplied = tf.sparse_tensor_dense_matmul(sparse_slice, tf.eye(2))
loss = tf.reduce_sum(multiplied)
gradients = tf.gradients(loss, [sparse_values])
with tf.Session() as session:
    tf.global_variables_initializer().run()
    print(session.run(gradients))

"
19372,Can not allocate memory for the interpreter in Tensorflow_lite. When i initialize the Interpreter Object in android.,"
------------------------

### System information
TensorFlow version :1.8.0

### Describe the problem
Purpose:Convert a custom model to a quantize tensorflow lite model.

[This is the model structure](https://drive.google.com/file/d/1XV1FZa0ChoJGuR8n-gxK4Ry67CVn5aVm/view?usp=sharing)
The model structure is similar to mobilenet_v1

_Step：_
1. Quantize Training (add tf.contrib.quantize.create_training_graph() in my code）

2. Generate .pb (add tf.contrib.quantize.create_eval_graph() in my code)

3. Use the toco to convert my pb file to tflite. The command:
bazel-bin/tensorflow/contrib/lite/toco/toco \
--input_file=/home/admin_pc/model_test/output_224_qg.pb \
--output_file=/home/admin_pc/model_test/mobilenet_qg.tflite \
--input_fromat=TENSORFLOW_GRAPHDEF \
--output_format=TFLITE \
--inference_type=QUANTIZED_UINT8 \
--input_array=image \
--output_array=Openpose/concat_stage7 \
--input_shape=1,224,224,3 \
--std_value=128 --mean_value=128 

    It was succeed. But some warning occured,just like this:
    tensorflow/contrib/lite/toco/graph_transformations/hardcode_min_max.cc:120] Tweaking the MinMax of the output array of {Concatenation operator with output Openpose/MConv_Stage2_concat}, because we want all inputs and outputs of a Concatenation operator to have the same MinMax so that it can be implemented as a pure byte-copy, no arithmetic.
    tensorflow/contrib/lite/toco/graph_transformations/hardcode_min_max.cc:105] Tweaking the MinMax of array Conv2d_3_pool, which is an input to {Concatenation operator with output feat_concat}, because we want all inputs and outputs of a Concatenation operator to have the same MinMax so that it can be implemented as a pure byte-copy, no arithmetic.

4. I add the quantize tflite to my android project. And run,some error occured when i init the Interpreter.
java.lang.NullPointerException: Can not allocate memory for the interpreter

I don't know it's a bug or my fault.
"
19371,Failed to compile r1.8 ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.8
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: 0.13.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: CUDA 9.2.88+patch 1, cuDNN 7.1.3
- **GPU model and memory**: gtx 1050 ti 4G, gtx 1050 ti 4G
- **Exact command to reproduce**:
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures

### Describe the problem
I tried to upgrade from r1.3 to r1.8 from source and learned my CUDA version is a bit old. Then I got the latest version from nvidia's website, which is CUDA 9.2.88+patch 1, cuDNN 7.1.3, and the driver 396.26. Running the above bazel build command, I got a failure as the provided logs in the below. It seems 
CUDA 9.2 libraries are not in defined references. 

### Source code / logs
ERROR: /home/alan/tensorflow/tensorflow/contrib/tensor_forest/hybrid/BUILD:49:1: Linking of rule '//tensorflow/contrib/tensor_forest/hybrid:gen_training_ops_py_wrappers_cc' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/alan/.cache/bazel/_bazel_alan/2190512db8dfcdc4b6b4ea3fdcfc66e7/execroot/org_tensorflow && \
  exec env - \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/tensorflow/contrib/tensor_forest/hybrid/gen_training_ops_py_wrappers_cc '-Wl,-rpath,$ORIGIN/../../../../_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow' '-Wl,-rpath,$ORIGIN/../../../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' -Lbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow -Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..,-rpath,$ORIGIN/../..,-rpath,$ORIGIN/../../..' -Wl,-z,notext -Wl,-z,notext -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 -pthread -Wl,-no-as-needed -B/usr/bin/ -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,--gc-sections -Wl,-S -Wl,@bazel-out/host/bin/tensorflow/contrib/tensor_forest/hybrid/gen_training_ops_py_wrappers_cc-2.params)
/usr/bin/ld: warning: libcublas.so.9.2, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libcudnn.so.7, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libcufft.so.9.2, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libcurand.so.9.2, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftMakePlanMany@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDtrsm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasIzamax_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZhpr2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreate@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreatePoolingDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDzasum_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `curandDestroyGenerator@libcurand.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreateConvolutionDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnConvolutionBiasActivationForward@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasIsamax_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDestroyConvolutionDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSsbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSgemm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZherk_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSspr_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDsymm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasIdamax_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDestroyFilterDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftPlan3d@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSetStream_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasGemmEx@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZhpmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCsyrk_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasIsamin_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCscal_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftExecD2Z@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSrotg_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnLRNCrossChannelForward@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSsyrk_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreateRNNDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDestroyRNNDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnConvolutionBackwardData@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftSetWorkArea@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreateFilterDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZher2k_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDrotm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCsyr2k_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnLRNCrossChannelBackward@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftMakePlan2d@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDgbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSsyr2k_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasStpsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasStrmm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDestroyLRNDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDestroy@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDznrm2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCtrsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZgemmBatched@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZdscal_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDdot_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDropoutGetStatesSize@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetConvolutionForwardWorkspaceSize@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSgemv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetActivationDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDsbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetPoolingNdDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetRNNWorkspaceSize@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasChbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasStpmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCgemv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetConvolutionForwardAlgorithm@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCtrsm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnConvolutionBackwardBias@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftMakePlan1d@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCsymm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDtrmm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSgemmBatched@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSgbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCtbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDasum_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasScopy_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDrotmg_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetTensorNdDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZgemv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCaxpy_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasChpr2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnAddTensor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZgemm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZtrmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDestroyActivationDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDrotg_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZgerc_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZhemv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDsyrk_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetConvolutionNdDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZtbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSrotmg_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDgemmBatched@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnPoolingBackward@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDspmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnRNNBackwardData@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetFilterNdDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCrotg_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCher_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftExecR2C@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSdot_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZrotg_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCsscal_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetProperty@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetConvolutionBackwardFilterWorkspaceSize@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasChemm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDtpmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDscal_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDger_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasStrsm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetRNNTrainingReserveSize@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDsyr_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSsyr2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `curandSetStream@libcurand.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasChemv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDrot_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSaxpy_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `curandGenerateUniformDouble@libcurand.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `curandSetPseudoRandomGeneratorSeed@libcurand.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `curandGenerateUniform@libcurand.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetLRNDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnBatchNormalizationForwardTraining@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDnrm2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZtrsm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDestroyTensorDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDestroyDropoutDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSsymv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDtpsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSspmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreateTensorDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCgeru_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSrot_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetStream@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZtrmm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCtpsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasIdamin_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasIcamin_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDsymv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSrotm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZhpr_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCherk_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasChpmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnBatchNormalizationForwardInference@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCdotu_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreateDropoutDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZtpmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZsyr2k_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCgerc_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZscal_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCtpmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `curandGenerateNormal@libcurand.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSsyr_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftDestroy@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZhemm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftCreate@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZgbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSetMathMode@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDtrmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnDestroyPoolingDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDtbsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZhbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreateActivationDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSspr2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSger_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSasum_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSsymm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDcopy_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftPlanMany@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDspr_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDswap_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnRNNForwardInference@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasStrmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `curandGenerateNormalDouble@libcurand.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCreate_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasIzamin_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftPlan1d@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftSetStream@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftExecZ2Z@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetConvolutionBackwardDataAlgorithm@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnActivationForward@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetRNNLinLayerMatrixParams@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetRNNDescriptor_v6@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetConvolutionBackwardDataWorkspaceSize@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasChpr_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `curandSetGeneratorOffset@libcurand.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnBatchNormalizationBackward@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasStbsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSetPointerMode_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetDropoutDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZgeru_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasStrsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCswap_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDgemv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDspr2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCher2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCdotc_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDestroy_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftSetAutoAllocation@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDsyr2k_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCcopy_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnConvolutionForward@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnConvolutionBackwardFilter@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCgemm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCgbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZher2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetConvolutionBackwardFilterAlgorithm@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZswap_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetRNNLinLayerBiasParams@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCtrmm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetFilterNdDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetRNNMatrixMathType@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSscal_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZsyrk_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetRNNParamsSize@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasScnrm2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZsymm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZher_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasIcamax_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetConvolutionNdForwardOutputDim@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftMakePlan3d@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSnrm2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasScasum_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCher2k_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDtbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSgemmEx@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetTensor4dDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZtpsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasSswap_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnRNNBackwardWeights@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZtrsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZtbsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCtbsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZdotc_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasStbmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZdrot_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftExecC2R@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnRNNForwardTraining@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDgemm_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnTransformTensor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCtrmv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftExecC2C@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCsrot_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZdotu_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDtrsv_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnSetConvolutionMathType@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnPoolingForward@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDaxpy_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftPlan2d@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftExecZ2D@libcufft.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `curandCreateGenerator@libcurand.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZaxpy_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreateLRNDescriptor@libcudnn.so.7'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasGetMathMode@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDsyr2_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZcopy_v2@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCgemmBatched@libcublas.so.9.2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasGetPointerMode_v2@libcublas.so.9.2'
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 12.553s, Critical Path: 2.90s
INFO: 48 processes, local.
FAILED: Build did NOT complete successfully
"
19370,Error with version of protocol buf,"Helo ,  the error is shwon as
 [libprotobuf FATAL google/protobuf/stubs/common.cc:61] This program requires version 3.5.0 of the Protocol Buffer runtime library, but the installed version is 2.6.1.  Please update your library.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in ""bazel-out/arm-opt/genfiles/tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.cc"".)
terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  This program requires version 3.5.0 of the Protocol Buffer runtime library, but the installed version is 2.6.1.  Please update your library.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in ""bazel-out/arm-opt/genfiles/tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.cc"".)
Aborted (core dumped)

why 2.6.1?
nvidia@tegra-ubuntu:~$ protoc --version
libprotoc 3.5.1

nvidia@tegra-ubuntu:~$ pip list
protobuf (3.5.2.post1)

WhAT happend? Thank u!


"
19369,Error with  'std::bad_alloc',"Helo ,
I want to run the demo with 
https://github.com/anishathalye/neural-style

Thisi is basic information:
tensorflow 1.8 GPU
PYTHON_BIN_PATH=/usr/bin/python
GCC_HOST_COMPILER_PATH=/usr/bin/gcc
CUDA_TOOLKIT_PATH=/usr/local/cuda-9.0
TF_CUDA_VERSION=9.0
TF_CUDA_COMPUTE_CAPABILITIES=5.3,6.2
CUDNN_INSTALL_PATH=/usr/local/include/tf/bazel-genfiles/external/local_config_cuda/cuda/cuda/lib
TF_CUDNN_VERSION=7.0.5

free -h
              total        used        free      shared  buff/cache   available
Mem:           7.7G        2.8G        2.9G         48M        1.9G        4.7G
Swap:            0B          0B          0B

As  I  run with :: 
nvidia@tegra-ubuntu:~/neural-style$ python neural_style.py --content ./examples/123.jpg --styles ./examples/2-style2.jpg  --output zcq.jpg --iterations 500

The error shows:
nvidia@tegra-ubuntu:~/neural-style$ python neural_style.py --content ./examples/123.jpg --styles ./examples/2-style2.jpg  --output zcq.jpg --iterations 500
/usr/lib/python2.7/dist-packages/scipy/misc/pilutil.py:479: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
  if issubdtype(ts, int):
/usr/lib/python2.7/dist-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  elif issubdtype(type(size), float):
2018-05-18 02:28:34.668653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:865] ARM64 does not support NUMA - returning NUMA node zero
2018-05-18 02:28:34.668832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: NVIDIA Tegra X2 major: 6 minor: 2 memoryClockRate(GHz): 1.3005
pciBusID: 0000:00:00.0
totalMemory: 7.66GiB freeMemory: 2.39GiB
2018-05-18 02:28:34.668886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-05-18 02:28:35.548358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-05-18 02:28:35.548468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2018-05-18 02:28:35.548501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2018-05-18 02:28:35.548709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1950 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)
2018-05-18 02:28:38.167544: W tensorflow/core/framework/allocator.cc:101] Allocation of 314572800 exceeds 10% of system memory.
2018-05-18 02:28:38.625368: W tensorflow/core/framework/allocator.cc:101] Allocation of 314572800 exceeds 10% of system memory.
terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc
Aborted (core dumped)

How to fix this problem??  Thank u!"
19368,tf.control_dependencies fails to update the dependent op.,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Redhat 7.3
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.8
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: 0.12
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: 9.1/7.0.5
- **GPU model and memory**: P5000/16G
- **Exact command to reproduce**: N/A


### Describe the problem

During the test of tf.metrics, two pieces of code snippets are believed to be equivalent outputting totally different results.

### Source code / logs
Code snippet 1:

```
import tensorflow as tf

a = tf.constant([1.], tf.float32)
mean_a, mean_a_uop = tf.metrics.mean(a)

sess = tf.InteractiveSession()
tf.global_variables_initializer().run()
tf.local_variables_initializer().run()

for _ in range(10):
  sess.run(mean_a_uop)
  print(sess.run(mean_a))

```
outputs:
```
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

```

Code snippet 2:
```
import tensorflow as tf

a = tf.constant([1.], tf.float32)
mean_a, mean_a_uop = tf.metrics.mean(a)

with tf.control_dependencies([mean_a_uop]):
  mean_a = tf.identity(mean_a)

sess = tf.InteractiveSession()
tf.global_variables_initializer().run()
tf.local_variables_initializer().run()

for _ in range(10):
  print(sess.run(mean_a))

```

outputs:

```
0.0
2.0
1.5
1.3333334
1.25
1.2
1.1666666
1.1428572
1.125
1.1111112

```
"
19367,Abnormal feature function,"#### Expected behavior
Supposedly, When I click ""Save Feature"" the image should be saved. instead of connecting to the ""Sharing Features"" that are not working and do not contain any data / images.

#### Actual behavior
When I click ""Save Feature"", the App takes me to the ""Sharing Feature"" which is not working and does not contain any data / images.

#### How to reproduce
1. Instal App [here](https://play.google.com/store/apps/details?id=com.deerslab.stylized1)
2. Run the app
3. Choose the effect you like
4. Click ""SAVE feature"", you will be taken to ""Share Feature"" which does not contain data and can not be shared.

***
***

* Browser : Redmi Note 5A
* System Operating : 7.1.0 Nougat
* App Version : 1.02

#### Recording Of The Bug
https://youtu.be/MXN-WgNp17Q

#### Proof Of Work Done
http://www.github.com/rezamusic881"
19363,Offer TensorFlow Docker Images Based on 18.04,"Nvidia is now shipping docker images based on ubuntu 18.04 ([see](https://github.com/NVIDIA/nvidia-docker/issues/666#issuecomment-390039509)), it would be great if TensorFlow started offering builds against these.

Ubuntu 18.04 has added Python 3.6.
```
Have I written custom code N/A
OS Platform and Distribution Docker
TensorFlow installed from Docker
TensorFlow version N/A
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce N/A
```"
19362,ImportError: libnvidia-fatbinaryloader.so.384.111: cannot open shared object file: No such file or directory,"### System information
- **Have I written custom code (as opposed to off a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 LTS**:
- **TensorFlow installed from (source or binary) from official wesite:
- **TensorFlow version (use command below)**: 
- **Python version : 2.7**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version: 9.0**:
- **GPU model and memory: GEFORCE GTX 1050(with GPU.While installation, it didnt**:
4 GB)**:
- **Exact command to reproduce: I installed tensor flow with pip(with GPU).While installation, it didnt recieve any errors but running I am getting following errors**:



### Describe the problem

when I tried importing Tensor flow I got the following errors:
 
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/deepak/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libnvidia-fatbinaryloader.so.384.111: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.


Does Anyone have experienced it?
please do help me!!!


"
19361,Feature request: forwards autodifferentiation for gradient computation,"Does TensorFlow use forwards instead of reverse (i.e. backprop) autodiff where appropriate? 

For graphs with fewer inputs (layer input values plus model parameters) than outputs (as  occurs in convolutional upsampling architectures now popular in GANs and auto-encoders), I believe forwards auto-differentiation will be a faster way to compute gradients. The Jacobian matrices get progressively wider as you go from right to left (up the chain of functions), so if you start multiplying from the right (forwards), rather than from the left (backprop) you reduce the number of operations.

But maybe I'm missing something."
19360,tf.split's -1 support doesn't handle zero dimensions,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab
- **TensorFlow installed from (source or binary)**: Colab
- **TensorFlow version (use command below)**: ('unknown', '1.7.0') and 1.8.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: `tf.split(tf.zeros([0]), [0, -1], axis=-1)`

### Describe the problem
The variable size version of `tf.split` (`SplitV` in C++) allows one of the sizes to be -1.  The corresponding output will expand as necessary so that the total output size matches the input.

Unfortunately, the -1 support currently assumes the -1 dimension corresponds to **positive** size.  It should handle zero as well.  E.g., this should work, but it doesn't:

```
>>> tf.split(tf.zeros([0]), [0, -1], axis=-1)
Traceback (most recent call last):
  File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1592, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Sum of output sizes must match the size of the original Tensor along the split dimension or the sum of the positive sizes must be less if it contains a -1 for 'split_3' (op: 'SplitV') with input shapes: [0], [2], [] and with computed input tensors: input[1] = <0 -1>, input[2] = <-1>.
```

By comparison, the positive case works fine:

```
>>> tf.split(tf.zeros([1]), [0, -1], axis=-1)
[<tf.Tensor 'split_4:0' shape=(0,) dtype=float32>, <tf.Tensor 'split_4:1' shape=(?,) dtype=float32>]
```"
19359,Getting wrong predictions on a model loaded in java vs python using the tensorflow “saved model” api,"### System information
- **custom code**:
- **OS Platform and Distribution**: MacOS High Sierra v10.13.3
- **TensorFlow installed from binary**: pip3 install
- **TensorFlow version **: v1.8.0-0-g93bc2e2072 1.8.0
- **Python version**: 3.6.1
- Have I written custom code: Yes
 - Bazel version: n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a
- Exact command to reproduce: In the 'Code' section of this post

### Problem
I'm trying to load a model in Java that was trained in python and saved using the saved model api (`from tensorflow.python.saved_model`).

I'm able to load it in a separate Python script and from Java but the predictions are wrong in the Java version.

I wrote a quick example project with a simple model that demonstrates the ""bug"" (I'm hoping my misunderstanding).

**Python: OrTraining.py**

Save model after training using Saved Model Api.

    builders = saved_model_builder.SavedModelBuilder(export_path)
    builders.add_meta_graph_and_variables(sess, [""or""], signature_def_map={
        ""predict"": tf.saved_model.signature_def_utils.predict_signature_def(
            inputs= {""images"": x_placeholder},
            outputs= {""scores"": hypothesis_function})
        })
    builders.save()

https://github.com/JsFlo/DebuggingSavedModelJava/blob/master/OrTraining.py

**Python: OrLoadSavedModel.py**

Load model in a separate script using the Saved Model Api.

    with tf.Session(graph=tf.Graph()) as sess:
    tf.saved_model.loader.load(sess, [""or""], ""orTrainingModels"")
    graph = tf.get_default_graph()
    print(graph.get_operations())
    x_placeholder = graph.get_tensor_by_name(""or_inputs:0"")
    hypothesis_function = graph.get_tensor_by_name(""hypothesis_output:0"")
    # sess.run(""init"")
    print(sess.run(hypothesis_function, feed_dict={x_placeholder: np.array([
        np.array([1, 0]),
        np.array([0, 1]),
        np.array([0, 0]),
        np.array([1, 1]),
    ])}))

https://github.com/JsFlo/DebuggingSavedModelJava/blob/master/OrLoadSavedModel.py

**Java: OrLoadSavedModel.java**

Load

     SavedModelBundle savedModelBundle = SavedModelBundle.load(""./orTrainingModels"", ""or"");
     Session session = savedModelBundle.session();

Run

    Tensor result = session.runner()
                .feed(""or_inputs"", tensorInput)
                .fetch(""hypothesis_output"")
                .run().get(0);

https://github.com/JsFlo/DebuggingSavedModelJava/blob/master/src/main/java/OrLoadSavedModel.java

Both the java version and the python version load and run the graphs without a problem but the java version doens't output the correct predictions.

At first I thought it was because the weights/bias weren't being loaded but I'm able to ""run"" the weights/bias operation in the java version and see that it has the correct weights that I see in the python script after training.

Check weights in java (https://github.com/JsFlo/DebuggingSavedModelJava)

    Tensor result = session.runner()
                .fetch(""da_weights"")
                .run().get(0);

### Code
https://github.com/JsFlo/DebuggingSavedModelJava

I run **python3 OrTraining.py** which will ""train"" (really small model) on my cpu and output a folder called `orTrainingModels` which has the output of the training SavedModel API. (You don't have to run this since the folder is already in the repo but if you did want to then remove the folder first)

    .
    ├── saved_model.pb
    └── variables
        ├── variables.data-00000-of-00001
        └── variables.index

I then run **python3 OrLoadingSaveModel.py** which will output the results of a small (4) validation/test set . I see the correct results in this. 

I run the java version **(OrLoadSavedModel.java)** by opening up the project in Intellij IDEA and clicking play or running **./gradlew clean build :run**

The Java version seems to load the correct weights and bias but the outputs are wrong. Running the java version should output some debugging logs (weights/bias/operations/output)."
19354,building tensorflow r1.4  RelWithDebInfo on Window 10 Fails,"Hi Guys,

I was able to build Tensorflow r1.4 in Release on Windows 10 with the following successfully.
Python 3.5
Cuda 8.0, Cudnn 6.0
Visual Studio 2015

However, I clone the Tensorflow repository, checked out r1.4 and try to build RelWithDebInfo with the following commands:

cmake .. -A x64 -DCMAKE_BUILD_TYPE=RelWithDebInfo -DSWIG_EXECUTABLE=""C:\\swigwin-3.0.12\\swig.exe"" -DPYTHON_EXECUTABLE=""C:\\Python35\\python.exe"" -DPYTHON_LIBRARIES=""C:\\Python35\\libs\\python35.lib"" -DPYTHON_INCLUDE_DIR=""C:\\Python35\\include"" -DNUMPY_INCLUDE_DIR=""C:\\Python35\\Lib\\site-packages\\numpy\\core\\include"" -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=""C:\\cudnn_80_60""

MSBuild /p:Configuration=RelWithDebInfo tf_tutorials_example_trainer.vcxproj

Cmake was able to generate the visual studio files.

However, I ended up with the following compilation error:
""C:\tensorflow_1_4\tensorflow\tensorflow\contrib\cmake\build\tf_tutorials_example_trainer.vcxproj"" (default target) (1) ->
""C:\tensorflow_1_4\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj"" (default target) (114) ->
(ClCompile target) ->
  C:\tensorflow_1_4\tensorflow\tensorflow\core\kernels\training_ops.cc(3248): error C2471: cannot update program database 'C:\tensorflow_1_4\tensorflow\tensorflow\contrib\cmake\b
uild\tf_core_kernels.dir\RelWithDebInfo\tf_core_kernels.pdb' [C:\tensorflow_1_4\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]

Can I please get some help on this?  Thank you very much in advance!"
19353,CMAKE+CPU on Ubuntu 18.04 build failure,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master 51ef16057b4625e0a3e2943a9f1bbf856cf098ca
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.13.0 (Irrelevant)
- **GCC/Compiler version (if compiling from source)**: 7.3.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
```
$ mkdir build && cd build
$ cmake -DCMAKE_BUILD_TYPE=Release ../tensorflow/contrib/cmake
$ make -j 4

```

### Describe the problem

While trying to build tensorflow with cmake (CPU only) it looks like build is broken. From the log it seems that there are some eager dependency issues. 

### Source code / logs

```
# mkdir build && cd build
# cmake -DCMAKE_BUILD_TYPE=Release ../tensorflow/contrib/cmake
# make -j 4

[ 99%] Building CXX object CMakeFiles/summarize_graph.dir/home/ubuntu/tensorflow/tensorflow/tools/graph_transforms/summarize_graph_main.cc.o
[ 99%] Linking CXX executable summarize_graph
CMakeFiles/tf_core_cpu.dir/home/ubuntu/tensorflow/tensorflow/core/common_runtime/eager/eager_operation.cc.o: In function `tensorflow::EagerOperation::AddInput(tensorflow::TensorHandle*)':
eager_operation.cc:(.text+0x23b): undefined reference to `tensorflow::AttrBuilder::NumInputs(int)'
CMakeFiles/tf_core_cpu.dir/home/ubuntu/tensorflow/tensorflow/core/common_runtime/eager/execute.cc.o: In function `tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::gtl::InlinedVector<tensorflow::TensorHandle*, 2>*, int*)':
execute.cc:(.text+0x2bf4): undefined reference to `tensorflow::AttrBuilder::CacheKey(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'
execute.cc:(.text+0x32f4): undefined reference to `tensorflow::AttrBuilder::BuildNodeDef()'
execute.cc:(.text+0x4e3a): undefined reference to `tensorflow::OpDefForOp(char const*, tensorflow::OpDef const**)'
collect2: error: ld returned 1 exit status
CMakeFiles/summarize_graph.dir/build.make:2164: recipe for target 'summarize_graph' failed
make[2]: *** [summarize_graph] Error 1
CMakeFiles/Makefile2:2214: recipe for target 'CMakeFiles/summarize_graph.dir/all' failed
make[1]: *** [CMakeFiles/summarize_graph.dir/all] Error 2
Makefile:129: recipe for target 'all' failed
make: *** [all] Error 2
```
"
19352,Failed to build version 1.8,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: bazel build --config=opt//tensorflow/tools/pip_package:build_pip_package

### Describe the problem
Trying to build tensorflow v1.8 from sources got linking error.

ERROR: /home/alexey/tensorflow-1.8.0/tensorflow/python/BUILD:1582:1: Executing genrule //tensorflow/python:sparse_ops_pygenrule failed (Exit 127)
bazel-out/host/bin/tensorflow/python/gen_sparse_ops_py_wrappers_cc: symbol lookup error: bazel-out/host/bin/tensorflow/python/gen_sparse_ops_py_wrappers_cc: undefined symbol: _ZN10tensorflow8str_util8EndsWithENS_11StringPieceES1_

### Steps to reproduce
1) Go to https://github.com/tensorflow/tensorflow
2) Click ""Branch"" button, select ""Tags"" tab and choose v1.8.0 tag
3) Click ""Clone or download"" > Download ZIP
4) Unpack the archive to home folder and cd to tensorflow-1.8.0
5) Run configure answering all options NO, except to jmalloc and python version/path (see the logs below)
6) Build CPU version of tensorflow with ""bazel build --config=opt//tensorflow/tools/pip_package:build_pip_package"" command
7) Wait for 2400 seconds and get the linking error mentioned above

P.S. configure's inputs:

alexey@Inspiron-7559:~/tensorflow-1.8.0$ ./configure
WARNING: Running Bazel server needs to be killed, because the startup options are different.
You have bazel 0.11.1 installed.
Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3.5

Found possible Python library paths:
  /usr/local/lib/python3.5/dist-packages
  /usr/lib/python3/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]

Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: 
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n
No Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n
No Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n
No Amazon S3 File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n
No Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: n
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: n
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
Configuration finished
"
19351,Bidirectional beam search tensorflow implmentation,"###  Problem Description :

I read the paper named ""Bidirectional Beam Search: Forward-Backward Inference in Neural Sequence Models for Fill-in-the-Blank Image Captioning""  [Paper-Link](https://arxiv.org/pdf/1705.08759.pdf).
I checked the official Tensorflow website for a feature similar to this one but didn't find any.
So is it possible we add this feature to Tensorflow or are there already similar implemented ones?

"
19350,negative condition in tf.cond gets evaluated,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 17.10
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.8.0
- **Python version**: 
2.7.14
- **Bazel version (if compiling from source)**: none
- **GCC/Compiler version (if compiling from source)**: none
- **CUDA/cuDNN version**: none
- **GPU model and memory**: none
- **Exact command to reproduce**: none

### Describe the problem
when the output tensor of tf.cond is evaluated, it evaluates the tensors returned from both true and false conditions. This creates an unwanted behavior when used in combination with tf.data.Iterator.get_next. In such a situation both true and false tensors get iterated to the next element. 

### Source code / logs
```
import tensorflow as tf

iterate = tf.placeholder(tf.bool, shape=())

dataset = tf.data.Dataset.from_tensor_slices(range(1, 10))
iterator = dataset.make_one_shot_iterator()

data = iterator.get_next()

empty_data = tf.constant(0)

input_data = tf.cond(iterate,
                     true_fn=lambda: data,
                     false_fn=lambda: empty_data)

with tf.Session() as sess:
    print sess.run([input_data], feed_dict={iterate: True})
    print sess.run([input_data], feed_dict={iterate: False})
    print sess.run([input_data], feed_dict={iterate: False})
    print sess.run([input_data], feed_dict={iterate: True})
    print sess.run([input_data], feed_dict={iterate: False})
    print sess.run([input_data], feed_dict={iterate: False})
    print sess.run([input_data], feed_dict={iterate: False})
    print sess.run([input_data], feed_dict={iterate: True})
```
Output
```
[1]
[0]
[0]
[4]
[0]
[0]
[0]
[8]
```
Expected Output
```
[1]
[0]
[0]
[2]
[0]
[0]
[0]
[3]
```
"
19348,"Improve java documentation by explaining ""magic"" constants like """"save/control_dependency""""","In your examples [here](https://github.com/tensorflow/models/blob/master/samples/languages/java/training/src/main/java/Train.java) there is a statement:
```
sess.runner()
     .feed(""save/Const"", checkpointPrefix)
     .addTarget(""save/control_dependency"")
     .run();
```
Can you explain somewhere in documentation what is the ""magic"" constants used there and list all the variants of them somewhere?"
19347,Segmentation fault (core dumped) on tf.Session(),"I just installed TensorFlow and to test the installation, I tried the following code and as soon as I initiate the TF Session, I am getting the ***Segmentation fault (core dumped)*** error.

    bafhf@remote-server:~$ python
    Python 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) 
    [GCC 7.2.0] on linux
    Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
    >>> import tensorflow as tf
    /home/bafhf/anaconda3/envs/ismll/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
      from ._conv import register_converters as _register_converters
    >>> tf.Session()
    2018-05-15 12:04:15.461361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1349] Found device 0 with properties: 
    name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
    pciBusID: 0000:04:00.0
    totalMemory: 11.17GiB freeMemory: 11.10GiB
    Segmentation fault (core dumped)

My **nvidia-smi** is:

    Tue May 15 12:12:26 2018       
    +-----------------------------------------------------------------------------+
    | NVIDIA-SMI 390.30                 Driver Version: 390.30                    |
    |-------------------------------+----------------------+----------------------+
    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
    |===============================+======================+======================|
    |   0  Tesla K80           On   | 00000000:04:00.0 Off |                    0 |
    | N/A   38C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |
    +-------------------------------+----------------------+----------------------+
    |   1  Tesla K80           On   | 00000000:05:00.0 Off |                    2 |
    | N/A   31C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |
    +-------------------------------+----------------------+----------------------+
                                                                               
    +-----------------------------------------------------------------------------+
    | Processes:                                                       GPU Memory |
    |  GPU       PID   Type   Process name                             Usage      |
    |=============================================================================|
    |  No running processes found                                                 |
    +-----------------------------------------------------------------------------+

And **nvcc --version** is:

```
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Sep__1_21:08:03_CDT_2017
Cuda compilation tools, release 9.0, V9.0.176

```

Also **gcc --version** is:

    gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
    Copyright (C) 2015 Free Software Foundation, Inc.
    This is free software; see the source for copying conditions.  There is NO
    warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

Following is my **PATH**:

    /home/bafhf/bin:/home/bafhf/.local/bin:/usr/local/cuda/bin:/usr/local/cuda/lib:/usr/local/cuda/extras/CUPTI/lib:/home/bafhf/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin

and the **LD_LIBRARY_PATH**:

    /usr/local/cuda/bin:/usr/local/cuda/lib:/usr/local/cuda/extras/CUPTI/lib

<br>I am running this on a server and I don't have root privileges. Still I managed to install everything as per the instructions on the official website.<br>

Seems like the GPU is allocating memory for the process for a second and then the core segmentation dumped error is thrown.

![terminal output](https://user-images.githubusercontent.com/34688139/40171744-7a623402-59cc-11e8-857a-f1c0b2286f2a.gif)

I downgraded my tensorflow version from v1.8 to v1.5. The issue still remains.

<br>Is there any way address or debug this issue?
"
19344,compile error with CUDA8.0,"

### System information

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.7
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.13
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:

### Describe the problem
met compile error with:
tensorflow/core/kernels/gather_nd_op_gpu.cu.cc(45): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherSliceOpKernel< ::std::complex<double> , long long, (int)4> "") is not allowed

tensorflow/core/kernels/gather_nd_op_gpu.cu.cc(45): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherSliceOpKernel< ::std::complex<double> , long long, (int)5> "") is not allowed

tensorflow/core/kernels/gather_nd_op_gpu.cu.cc(45): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherSliceOpKernel< ::std::complex<double> , long long, (int)6> "") is not allowed

tensorflow/core/kernels/gather_nd_op_gpu.cu.cc(45): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherSliceOpKernel< ::std::complex<double> , long long, (int)7> "") is not allowed

80 errors detected in the compilation of ""/tmp/tmpxft_0000502d_00000000-7_gather_nd_op_gpu.cu.cpp1.ii"".
ERROR: /home/carmark/github/tensorflow/tensorflow/core/kernels/BUILD:690:1: output 'tensorflow/core/kernels/_objs/gather_nd_op_gpu/tensorflow/core/kernels/gather_nd_op_gpu.cu.pic.o' was not created
ERROR: /home/carmark/github/tensorflow/tensorflow/core/kernels/BUILD:690:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1050.472s, Critical Path: 77.58s
INFO: 4023 processes, local.
FAILED: Build did NOT complete successfully


"
19343,AttributeError: module 'tensorflow.contrib' has no attribute 'distribute',"When I include mirroring strategy using following code -

# Build the Estimator
distribution = tf.contrib.distribute.MirroredStrategy()
config = tf.estimator.RunConfig(train_distribute=distribution)
model = tf.estimator.Estimator(model_fn,config=config)

I get the error - AttributeError: module 'tensorflow.contrib' has no attribute 'distribute'

I tried this with both Tensorflow 1.7 and 1.8 - I get the same issue. I am using Python 3.6 https://colab.research.google.com/ .
"
19342,Install of Tensorflow Lite error - :app:compiledebugjavawithjavac,"Hi,
I am trying to test out the Tensorflow Lite Android demo on Android Studio, but I am always getting the build error: :app:compiledebugjavawithjavac. I set the JDK path and rebuilt the project. I have done everything the instructions have said. I recently installed the latest Android Studio 3.1.2 just before importing the Tensorflow Lite demo. So how can I fix this?

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows - on Android Studio
- **TensorFlow installed from (source or binary)**: Android Studio Demo Code
- **TensorFlow version (use command below)**: NA
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: 9.0 / 7.0
- **GPU model and memory**: GeForce GTX 750
- **Exact command to reproduce**: NA"
19341,Eager: Variable collection for training specific varibles,"### System information
- **TensorFlow version (use command below)**: 1.6.0v GPU
- **Python version**: 3.6.4

### Describe the problem
When exploring the eager execution mode, I built neural network model with `tf.layers.Dense` API under the `tf.variable_scope(""MLP"")`. 

Once I would like to get variables with `tf.get_collection` to train specific variables, a ValueError was raised. 

> ValueError: When Eager Execution is enabled, variable collections are not supported.

It means that the `var_list` could not be collected to train specific variables, although fine-tune is quite common in the application of neural network model. Is there any solution?
"
19340,[TfLite]how can i run test for tflite(contrib/lite/testing/)?,"if i did some optimization work for tflite kernels, for example, i modify the Conv() operation.
How can i test it? i find that there is a ""testing"" in contrib/lite/, and there is something related to test, but there there is no document for what the testing is and how to compile and run it.
Is there any instructions and document available? if i really make some improvement to tflite kernel and want to contribute, i want to make it be fully tested.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
 now. No change, it is a question for how to test tflite kernels.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
master
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
0.11.1
- **GCC/Compiler version (if compiling from source)**:
gcc-5.4.0
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
it is a question for how to use testing in tflite. 

"
19339,Failed to load the native TensorFlow runtime.,"CPU: 3.1 GHz Intel Core i7
GPU: Intel Iris Graphics 6100 1536 MB
Python Version: 2.7

Hello, I hope someone will be able to help me with this, I am interested in seeing what Tensorflow 
 could do, but while attempting to run a test program for Tensorflow I receive this message: 

Traceback (most recent call last):
  File ""classify_image.py"", line 46, in <module>
    import tensorflow as tf
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 51, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: dlopen(/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): no suitable image found.  Did find:
	/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: mach-o, but wrong architecture


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help"
19337,tf.clip_by_value crashes for empty tensors on GPU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.4 LTS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: `b'v1.8.0-0-g93bc2e2' 1.8.0`
- **Python version**: `Python 3.6.1 :: Continuum Analytics, Inc.`
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7.0.5
- **GPU model and memory**: Tesla K80, 12 GB
- **Exact command to reproduce**: `./bug.py`

### Describe the problem
`tf.clip_by_value` has four different kernel code paths for different shape inputs.  None of them check for empty inputs, which is necessary before calling into a cuda kernel since the infinite wisdom of cuda is that zero length loops should crash horribly.  One of the four places a check is necessary:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_op_clip.cc#L48

It ran across this in real code, but it's slightly delicate to reproduce this crash.  Therefore, I'm not sure that the check is technically necessary before all of the paths.  But it's necessary in at least one, as exhibited by the code below.  Also the code should probably be restructured: currently the same error code is repeated three times.  With that restructuring, there would also only need to be one emptiness check.

### Source code / logs

This code reproduces the bug for me.  Note that it runs fine on CPU, since zero length loops on CPUs do not crash horribly:

```
#!/usr/bin/env python3

import numpy as np
import tensorflow as tf

z = tf.placeholder(dtype=tf.float32, shape=None)
x = tf.clip_by_value(z, 1, z)
with tf.Session():
  print(x.eval(feed_dict={z: np.zeros((7,0))}).shape)
```

Resulting output:

```
root@azero-bug:~/tmp$ ./bug.py 
/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
2018-05-16 22:07:46.191147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 4761:00:00.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2018-05-16 22:07:46.191226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-05-16 22:07:46.514559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-05-16 22:07:46.514640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2018-05-16 22:07:46.514661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2018-05-16 22:07:46.514923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10755 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 4761:00:00.0, compute capability: 3.7)
2018-05-16 22:07:46.631461: F ./tensorflow/core/util/cuda_launch_config.h:127] Check failed: work_element_count > 0 (0 vs. 0)
Aborted (core dumped)
```"
19333,Bug in EluGradGrad,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary - latest pip tf_nightly
- **TensorFlow version (use command below)**: v1.8.0-1674-gd8fac4cb80 1.9.0-dev20180515
- **Python version**: Python 3.6.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA Version 9.1.85
- **GPU model and memory**: GeForce GTX 970
- **Exact command to reproduce**: See script attached

### Describe the problem

There seems to be an issue with EluGradGrad that I have uncovered. Please see the post here: https://github.com/renmengye/tensorflow-forward-ad/issues/2#issuecomment-389321546

### Source code / logs

Running this script will demonstrate the incorrect values:

```python
import tensorflow as tf
import numpy as np

def fwd_gradients(ys, xs, d_xs):
    dummy = tf.zeros_like(ys)
    g = tf.gradients(ys, xs, grad_ys=dummy, name=""gradients"")
    return tf.gradients(g, dummy, grad_ys=d_xs, name=""jvp"")

def my_elu(x):
    return tf.where(x >= 0.0, x, tf.exp(x) - 1.0)

def main():
    print(tf.__version__)

    sess = tf.InteractiveSession()
    init = tf.global_variables_initializer()
    
    # activation = my_elu # Works correctly tf.nn.relu (or any other non-elu activation)
    activation = tf.nn.elu

    x_size = 3
    y_size = x_size

    # Single ELU or RELU op
    X = tf.placeholder(tf.float64, shape=[x_size]) # Input
    Y = activation(X) # Output

    # Define vjp and jvp
    Vx = tf.placeholder(tf.float64, shape=[x_size]) # Jac-vector product input V
    Vy = tf.placeholder(tf.float64, shape=[y_size]) # vector-jac product input V
    jvp = fwd_gradients(Y, X, d_xs=Vx)
    vjp = tf.gradients(Y, X, grad_ys=Vy)

    # Compute jacobians
    x = np.ones(x_size) - 1.5 # Bug only occurs in x < 0 region
    # x = np.random.normal(-1, 1, x_size)
    tf_jac, numeric_jac = tf.test.compute_gradient(X, [x_size], Y, [y_size], x_init_value=x)
    vjp_jac = np.array([sess.run(vjp, feed_dict={X: x, Vy: v})[0] for v in np.identity(y_size)])
    jvp_jac = np.array([sess.run(jvp, feed_dict={X: x, Vx: v})[0] for v in np.identity(x_size)])

    # Print results as maximum absolute error
    print(""Numeric jac:"", numeric_jac)
    print(""jvp jac:"", jvp_jac)
    print(""tf error:"", np.max(np.abs(numeric_jac - tf_jac)))   # ~0.0
    print(""vjp error:"", np.max(np.abs(numeric_jac - vjp_jac))) # ~0.0
    print(""jvp error:"", np.max(np.abs(numeric_jac - jvp_jac))) # LARGE! for ELU

    sess.close()

if __name__ == '__main__':
    main()
```

The solution is to edit the implementation of `_EluGradGrad` [here](https://github.com/tensorflow/tensorflow/blob/f318765ad5a50b2fbd7cc08dd4ebc249b3924270/tensorflow/python/ops/nn_grad.py#L364).

I've created a pull request that references this issue."
19331,Undefined symbol when importing tensorflow,"### System information
- **OS Platform and Distribution**: Linux Ubuntu 18.04
- **TensorFlow installed from**: usual pip install
- **TensorFlow version**: 1.8.0
- **Python version**: 3.6.5
- **Exact command to reproduce**: python -c ""import tensorflow""

### Describe the problem
I'm installing Tensorflow CPU version via the usual pip command:
```bash
pip install tensorflow
```
And when I try to import tensorflow I get the following error message:
```bash
Traceback (most recent call last):
  File ""/home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow34_CallableOptions_default_instance_E

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/jplu/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow34_CallableOptions_default_instance_E


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
[1]    15355 segmentation fault (core dumped)  python -c ""import tensorflow""
```
Any idea of what is going wrong?"
19325,Alternative to image_tensor_mapped in Tensoreflow-Lite,"Hi there, For my iOS app using Tensorflow the pre-processing was done by using image_tensor_mapped. i.e.

```
auto image_tensor_mapped = image_tensor.tensor<float, 4>();

uint32_t *pixels = (uint32_t *) calloc(width * height, sizeof(uint32_t));

uint8_t *bufptr = (uint8_t *)pixels;
for (i = 0; i < height; i++){
   for (j = 0; j < width; j++){
        blue = bufptr[1] / 255.0;
        green = bufptr[2] / 255.0;
        red = bufptr[3] / 255.0;
            
        image_tensor_mapped(0, i, j, 0) = blue;
        image_tensor_mapped(0, i, j, 1) = green;
        image_tensor_mapped(0, i, j, 2) = red;
            
        bufptr += 4;
    }
}
```
What will be the alternative for image_tensor_mapped in Tensorflow-lite(tflite)


Have I written custom code - No
OS Platform and Distribution - MacOS
TensorFlow installed from Pod
TensorFlow version - r1.8
TensorFlowLite version - 0.1.7
Bazel version - 0.11.0
CUDA/cuDNN version  - N/A
GPU model and memory - N/A
Exact command to reproduce - N/A
"
19324,[feature request] large scale embedding for sparse features,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux CentOS 7
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.0 release
- **Python version**: python2.7
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: gcc4.8.5
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: -

In my scene, training data is very large, we have 10e10 unique values(string) for embedding in one sparse column. In short, model structure is “input->embedding->NNs”. I set hash_bucket_size to 15e10 for low hash collision. sample code:
```
col0=tf.contrib.layers.sparse_column_with_hash_bucket(""feature_id"", hash_bucket_size=1.5e11)
cols=[tf.feature_column.embedding_column(categorical_column=col0, dimension=8)]
```
in above example, Tensorflow new a Variable with shape [1.5e11, 8]
Some problems:
1. memory waste, more than 0.5e11\*8\*4Bytes memory not used. 
2. hash collision, though enlarging the `hash_bucket_size`, it might occurs conflict. I don't know how much it influence on model. 

Is there any suggestions in Tensorflow in this case ?
In my opinion: 
Define a new Variable for embedding, need not define the first demension. It will malloc memory for this Variable when the new value is embedding_lookup. It will solve the two problems mentioned above.

Thanks"
19323,Error while building tensorflow using cmake,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux
- **TensorFlow installed from (source or binary)**: NA
- **TensorFlow version (use command below)**: NA
- **Python version**:  NA
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: GCC version 6.2.1 
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: `cmake ../tensorflow/contrib/cmake -DCMAKE_INSTALL_PREFIX=/usr -DLIB_INSTALL_DIR=/usr/lib -DINCLUDE_INSTALL_DIR=/usr/include ; make`


### Describe the problem
While building tensorflow using cmake, files like `tensorflow/core/example/feature.pb_text.cc` , `tensorflow/core/framework/attr_value.pb_text.cc` are generated with incorrect header includes.
(They are missing `#include <algorithm>` ). Similar issue can be found in [this pull request](https://github.com/tensorflow/tensorflow/pull/19321)
### Source code / logs
```
[ 1011s] /home/abuild/rpmbuild/BUILD/tensorflow-1.8.0/build/tensorflow/core/example/feature.pb_text.cc: In function 'void tensorflow::internal::AppendProtoDebugString(tensorflow::strings::ProtoTextOutput*, const tensorflow::Features&)':
[ 1011s] /home/abuild/rpmbuild/BUILD/tensorflow-1.8.0/build/tensorflow/core/example/feature.pb_text.cc:464:5: error: 'stable_sort' is not a member of 'std'
[ 1011s]      std::stable_sort(keys.begin(), keys.end());
[ 1011s]      ^~~
[ 1011s] /home/abuild/rpmbuild/BUILD/tensorflow-1.8.0/build/tensorflow/core/example/feature.pb_text.cc: In function 'void tensorflow::internal::AppendProtoDebugString(tensorflow::strings::ProtoTextOutput*, const tensorflow::FeatureLists&)':
[ 1011s] /home/abuild/rpmbuild/BUILD/tensorflow-1.8.0/build/tensorflow/core/example/feature.pb_text.cc:708:5: error: 'stable_sort' is not a member of 'std'
[ 1011s]      std::stable_sort(keys.begin(), keys.end());
[ 1011s]      ^~~
[ 1011s] [ 30%] Building CXX object CMakeFiles/tf_core_framework.dir/tensorflow/core/framework/attr_value.pb_text.cc.o
[ 1012s] CMakeFiles/tf_core_framework.dir/build.make:2614: recipe for target 'CMakeFiles/tf_core_framework.dir/tensorflow/core/example/feature.pb_text.cc.o' failed
[ 1012s] make[2]: *** [CMakeFiles/tf_core_framework.dir/tensorflow/core/example/feature.pb_text.cc.o] Error 1
[ 1012s] make[2]: *** Waiting for unfinished jobs....
[ 1013s] /home/abuild/rpmbuild/BUILD/tensorflow-1.8.0/build/tensorflow/core/framework/attr_value.pb_text.cc: In function 'void tensorflow::internal::AppendProtoDebugString(tensorflow::strings::ProtoTextOutput*, const tensorflow::NameAttrList&)':
[ 1013s] /home/abuild/rpmbuild/BUILD/tensorflow-1.8.0/build/tensorflow/core/framework/attr_value.pb_text.cc:698:5: error: 'stable_sort' is not a member of 'std'
[ 1013s]      std::stable_sort(keys.begin(), keys.end());
[ 1013s]      ^~~
```
"
19322,tf.gfile.GFile deadlocks on HDFS access after a fork(),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

Accessing HDFS via `tf.gfile` functions leads to a deadlock (?) if the HDFS has been initialized, i.e. there has been at least a single HDFS call from the parent process. Not sure if this is a `libhdfs` bug, or a TF one. If the former, then maybe #16919 could help.

### Source code / logs

#### Case 1: HDFS is not initialized in the parent process

```python
>>> import tensorflow as tf
>>> def f():
...     print(tf.gfile.ListDirectory(""hdfs://root/user/[...]""))
...
>>> from multiprocessing import Process
>>> p = Process(target=f)
>>> p.start()
>>> p.join()
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

['foo', 'bar', 'boo']
```

#### Case 2: HDFS is initialized in the parent process

```python
>>> f()
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
['foo', 'bar', 'boo']
>>> p = Process(target=f)
>>> p.start()
>>> p.join()  # Never returns.
```"
19320,NULL for interpreter->typed_tensor<uint8_t>(input),"Hi there, for my iOS app i'm using tensorflow-lite, 
I'm facing this issue after initializing the interpreter
In,
**int input =interpreter->inputs()[0];**
**uint8_t \*input_index = interpreter->typed_tensor<uint8_t>(input);**

input_index = NULL and input = 27

May I know what is **input** and what are the possibilities that **input_index** will be **NULL**


Have I written custom code - No
OS Platform and Distribution - MacOS
TensorFlow installed from Pod
TensorFlow version - r1.8
TensorFlowLite version - 0.1.7
Bazel version - 0.11.0
CUDA/cuDNN version  - N/A
GPU model and memory - N/A
Exact command to reproduce - N/A"
19319,Can't import interpreter from tensorflow.contrib.lite.python ,"![image](https://user-images.githubusercontent.com/22855898/40106717-0ef62cda-5929-11e8-806e-8e0ed2b14254.png)
tensorflow 1.8.0
tensorflow.contrib.lite has no attribute 'python', thus it's impossible to import interpreter from tensorflow.contrib.lite.python, which is in your tensorflow/tensorflow/contrib/lite/python/interpreter_test.py"
19318,Simple Keras model does not compile in float64 mode,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra 10.12.6
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: CPU only
- **GPU model and memory**: CPU only
- **Exact command to reproduce**: Run code below

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
When setting the backend to `float64`, a simple model does not compile anymore. I have debugged into the issue - the problem arises in the call to `weighted(y_true, y_pred, weights, mask=None)`, defined in `tensorflow.python.keras._impl.keras.engine.training_utils.py`. There, `weights` is

`Tensor(""dense_1_sample_weights:0"", shape=(?,), dtype=float32)`

while `y_true` and `y_pred` have both the type `float64`. This causes the subsequent multiplication to error. The weights come from line `401`, the `compile` function in `tensorflow/python/keras/_impl/keras/engine/training.py`:

```
sample_weights.append(array_ops.placeholder_with_default(
                [1.], shape=[None], name=name + '_sample_weights'))
```

But I have not found a way to make this `placeholder_with_default` function return `float64` weights.

**When using `keras` instead of `tensorflow.python.keras` it works** (for Keras version 2.1.6).

### Source code / logs
Minimum reproducible test case:
```
from tensorflow.python.keras import Model
from tensorflow.python.keras import backend as K
from tensorflow.python.keras.layers import Input, Dense

K.set_floatx('float64')

input_ = Input((5,))
output = Dense(1)(input_)
model = Model(input_, output)
model.compile('rmsprop', 'mse')
```

Stack trace:
```
Traceback (most recent call last):
  File ""/Users/kilian/dev/tmp/src/test/float64.py"", line 13, in <module>
    model.compile('rmsprop', 'mse')
  File ""/Users/kilian/dev/tmp/venv/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py"", line 428, in compile
    output_loss = weighted_loss(y_true, y_pred, sample_weight, mask)
  File ""/Users/kilian/dev/tmp/venv/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training_utils.py"", line 454, in weighted
    score_array *= weights
  File ""/Users/kilian/dev/tmp/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 970, in binary_op_wrapper
    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=""y"")
  File ""/Users/kilian/dev/tmp/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1014, in convert_to_tensor
    as_ref=False)
  File ""/Users/kilian/dev/tmp/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1104, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/Users/kilian/dev/tmp/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 947, in _TensorTensorConversionFunction
    (dtype.name, t.dtype.name, str(t)))
ValueError: Tensor conversion requested dtype float64 for Tensor with dtype float32: 'Tensor(""dense_1_sample_weights:0"", shape=(?,), dtype=float32)'
```"
19317,TfLiteCameraDemo performance with and without NNAPI,"Hi , 
I running TfLiteCameraDemo.apk on Google Pixel 2 8.1 Android.
when NNAPI = false apk display 90-80 ms per frame 
when NNAPI = true apk display 170- 200 ms per frame.

Is that NNAPI current performance ? match slower than Tflite path !?

Thanks.


------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
SetUSENNAPI(true);
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
ubuntu 16
- **TensorFlow installed from (source or binary)**:
sources
- **TensorFlow version (use command below)**:
master
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
0.10
- **GCC/Compiler version (if compiling from source)**:
gcc
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a

Thanks."
19316,[TF-Lite] densenet.pb at /contrib/lite/g3doc/models.md cannot be loaded,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.8
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**: 4.9
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**: 

### Describe the problem
The densenet.pb provided at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md
cannot be converted by toco command line. It can also not be loaded by GFile API.

### Source code / logs
> bazel-bin/tensorflow/contrib/lite/toco/toco \
--input_file=""densenet.pb"" \
--output_file=""quant_densenet.tflite"" \
--input_format=TENSORFLOW_GRAPHDEF \
--output_format=TFLITE \
--inference_input_type=QUANTIZED_UINT8 \
--inference_type=QUANTIZED_UINT8 \
--input_shape=1,224,224,3 \
--input_array=""input"" \
--output_array=""final_dense"" \
--allow_custom_ops \
--default_ranges_min=0.0 \
--default_ranges_max=6.0 \
--mean_value=127.5 \
--std_value=1.0 

>[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/text_format.cc:288] Error parsing text-format tensorflow.GraphDef: 1:1: Invalid control characters encountered in text.
>[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/text_format.cc:288] Error parsing text-format tensorflow.GraphDef: 1:2: Interpreting non ascii codepoint 139.
>[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/text_format.cc:288] Error parsing text-format tensorflow.GraphDef: 1:2: Expected identifier, got: ‹
>2018-05-16 15:19:01.232421: F tensorflow/contrib/lite/toco/import_tensorflow.cc:2230] Check failed: ParseFromStringEitherTextOrBinary(input_file_contents, tf_graph.get())
Aborted (core dumped)

if using GFile python API to load the pb file:

```
def load_pb_graph(frozen_graph_file):
    with tf.gfile.GFile(frozen_graph_file, 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
    with tf.Graph().as_default() as graph:
        tf.import_graph_def(graph_def, name=""prefix"")

    return graph, graph_def
```

>Traceback (most recent call last):
  File ""GraphCheckerPB.py"", line 24, in <module>
    graph, graph_def = load_pb_graph(in_pb_file)
  File ""GraphCheckerPB.py"", line 14, in load_pb_graph
    graph_def.ParseFromString(f.read())
google.protobuf.message.DecodeError: Error parsing message

"
19315,AttributeError: 'module' object has no attribute 'Exporter',"i ran retrain.py 
i got the code from tensorflow.org


Traceback (most recent call last):
  File ""retrain.py"", line 133, in <module>
    import tensorflow_hub as hub
  File ""/home/pi/.local/lib/python2.7/site-packages/tensorflow_hub/__init__.py"", line 26, in <module>
    from tensorflow_hub.estimator import LatestModuleExporter
  File ""/home/pi/.local/lib/python2.7/site-packages/tensorflow_hub/estimator.py"", line 61, in <module>
    class LatestModuleExporter(tf.estimator.Exporter):
AttributeError: 'module' object has no attribute 'Exporter'
"
19313,BUG: Arithmetic computations that should be identical are not?,"### System information
System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.13.4
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): v1.8.0-0-g93bc2e2072 1.8.0
Python version: 3.5.4
Bazel version (if compiling from source): 0.11.1
GCC/Compiler version (if compiling from source): 9.1.0
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A

### Describe the problem

I'm running two versions of a NN which I expect to produce identical results, but they don't.

The first version contains as part of the computation:

`x = [0.0*a + 1.0*b, 1.0*a + 0.0*b]`

The second contains instead:

`x = [b, a]`

The two runs produce different results. (a and b are both matrices).

I'm using a constant seed for the random noise input, and I've verified that multiple runs of either of the two above cases produce identical results. Just changing from one case to the other produces different results. The results are not broken in either case (i.e. both cases produce reasonable output), just different. 

This seems like a bug to me.

Thanks

EDIT: 

I've also found that doing:

x = 1.0 * a
x = 1.0 * (1.0 * a)
x = tf.identity(a)

All give different results.

Looking at this: https://github.com/tensorflow/tensorflow/issues/14675
Apparently the problem is I've made a change to the computation graph. 

I still think this is a bug so I'm going to leave this here. It's very frustrating when obvious things don't work as expected. 


"
19312,std::to_string issue when compile tensorflow lite with android ndk,"I add some code in tflitedemo, and will use std::to_string, but i failed with error that
error: no member named 'to_string' in namespace 'std'

the compiling command of tflite indicates it will use gcc-4.9 include files
external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang .... -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/x86_64/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward

the android ndk i am using is android-ndk-r16b
in fact it has to_string definition in /home/chao/Android/android-ndk-r16b/sources/cxx-stl/llvm-libc++/include/string

how can i make tflite to compile based on llvm-libc++ not gnu-libstdc++?

i update with the system information

**System information**
    **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**
    add std::to_string() in kernel/internal/conv.cc as:
      _auto str = std::to_string(input->dims->size);
       std::cout << str;_
    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**
    ubuntu 16
    **TensorFlow installed from (source or binary):**
    sources
    **TensorFlow version (use command below):**
    master
    **Python version:**
    2.7
    **Bazel version (if compiling from source):**
    0.11.1
    **CUDA/cuDNN version:**
    N/A
    **GPU model and memory:**
    N/A
    **Exact command to reproduce:**
    bazel build --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt='--std=c++11' //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --force_pic --cpu=x86_64  --verbose_failures --fat_apk_cpu=x86_64
    **Android NDK:**
    android-ndk-r15c



"
19310,What packages I should use to build my Relationship advisor algorithm?,"Hey guys,

Good day!

I'm trying to build a Relationship advisor using Machine Learning. 

**Project Description:** The system will be presented to the couples who are going to be in a relationship / going to apply divorce / going to marry. They will be asked a set of questions by the system about their habits, lifestyle, behaviour etc., Based on the answers provided by both of them, the system will evaluate them and tell them whether they are made for each other or not based on the percentage of matching between them.

In case, if the couples are going to apply divorce, the system will analyse them and also suggest some ways to the couples to solve their problem. If not, it will tell them to apply divorce so that they could lead a happier life separately.

**Query:** So, the above description explains what I want to build. 
Please suggest some packages inside tensorflow through which I can achieve the system I want.
"
19305,Inconsistent behaviour when `Dataset.repeats()` and `max_steps` differ,"### System information
(This issue is about an inconsistent behaviour between running TF with and without GPU. I will update the system information when it is clear to me which system that is not behaving. 

### Describe the problem
I get inconsistent behaviour when I try to train a model with/without GPU. The problem can be summarized with these few lines of code:
```
    train_spec = tf.estimator.TrainSpec(
        input_fn=lambda: make_dataset_iter(
            n_repeats=2),  # *** THIS HAS PRECEDENCE ON GPU ***
        max_steps=N_SAMPLES * 3)  # *** THIS HAS PRECEDENCE WITHOUT GPU ***
```
With a `make_dataset_iter` function that produces 20 samples (`N_SAMPLES=20`) I will get a model that is trained for 60 global steps (data repeated 3 times) when I run locally without GPU, and 40 global steps (data repeated 2 times) when I run it in a Docker container on a GPU cluster. Not yet sure whether it is a GPU thing, or something else related to the way I run it in the cloud.

Note that this also is a problem when either `max_steps` or `Dataset.repeats()` are set to the default value `None` and the other is set to something specific. This is probably the use-case where most people will experience this inconsistency.

Which of the two are the expected behaviours?

Full code and logs can be found below.

### Source code / logs
Minimal code reproducing the issue:
```
import tensorflow as tf


N_SAMPLES = 20


def lr_model_fn(features, labels, mode):
    """"""Simple logistic regression model""""""
    prediction = tf.layers.dense(features, units=1)
    loss = tf.losses.mean_squared_error(prediction, labels)

    return tf.estimator.EstimatorSpec(
        mode=mode,
        predictions=prediction,
        loss=loss,
        train_op=tf.train.AdamOptimizer().minimize(
            loss=loss,
            global_step=tf.train.get_global_step(),
        )
    )


def make_dataset_iter(n_repeats=1):
    """"""Turns an instance of a generator into a Dataset""""""
    dataset = tf.data.Dataset.from_generator(
        generator=lambda: zip(range(N_SAMPLES), range(N_SAMPLES)),
        output_types=(tf.float32))
    dataset = dataset.repeat(n_repeats)
    dataset = dataset.make_one_shot_iterator()
    dataset = dataset.get_next()
    features, labels = dataset[0], dataset[1]
    return tf.reshape(features, [-1, 1]), tf.reshape(labels, [-1, 1])


def run_experiment():
    # Create the estimator
    estimator = tf.estimator.Estimator(
        model_fn=lr_model_fn,
    )
    # Build train/eval specs
    train_spec = tf.estimator.TrainSpec(
        input_fn=lambda: make_dataset_iter(
            n_repeats=2),  # *** THIS HAS PRECEDENCE ON GPU ***
        max_steps=N_SAMPLES * 3)  # *** THIS HAS PRECEDENCE WITHOUT GPU ***
    eval_spec = tf.estimator.EvalSpec(
        input_fn=make_dataset_iter)
    # Run training
    tf.estimator.train_and_evaluate(
        estimator,
        train_spec,
        eval_spec
    )


if __name__ == ""__main__"":
    tf.logging.set_verbosity(tf.logging.INFO)
    run_experiment()

```
[Logs from the model running on GPU cluster](https://github.com/tensorflow/tensorflow/files/2006677/with_gpu_log.txt)
[Logs from my local machine](https://github.com/tensorflow/tensorflow/files/2006678/without_gpu_log.txt)

"
19304,"With TF C-API, got ""GPU device not registered"" and no-op for SessionRun.","Please go to Stack Overflow for help and support:

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.7.0/1.8.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:n/a
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:9.0/7.0
- **GPU model and memory**:1080Ti
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
Output:
```== cat /etc/issue ===============================================
Linux yangl-contenttech 4.13.0-41-generic #46~16.04.1-Ubuntu SMP Thu May 3 10:06:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.4 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux yangl-contenttech 4.13.0-41-generic #46~16.04.1-Ubuntu SMP Thu May 3 10:06:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                         1.14.3                
protobuf                      3.5.2.post1           
tensorflow-gpu                1.8.0                 

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)
/home/yangl/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64/:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Tue May 15 13:54:31 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |
| 27%   48C    P0    72W / 250W |   1489MiB / 11177MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1168      G   /usr/lib/xorg/Xorg                           870MiB |
|    0      2504      G   compiz                                       437MiB |
|    0      3240      G   ...-token=F3F41E0025601B32775ADF60FE0AE568   178MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.252
/usr/local/cuda-9.0/targets/aarch64-linux/lib/libcudart_static.a
/usr/local/cuda-9.0/targets/aarch64-linux/lib/libcudart.so.9.0.252
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
```

You can obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

```
/home/yangl/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
('v1.8.0-0-g93bc2e2072', '1.8.0')
```

### Describe the problem
I am trying to load and run graph model (mobilenet v2) with Tensorflow C-API. Python script downloaded from following link works on my desktop.
`https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v2.py`
The c-api library and framework library are downloaded via Bazel:
```
new_http_archive(
    name = ""libtensorflow"",
    build_file = ""third_party/libtensorflow.BUILD"",
    sha256 = ""05f5b543d5054f3c1ddc4f4caff7e3a6a96985579ef2d3dd9340ab94b1262f54"",
    
    url = ""https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-1.8.0.tar.gz"",
    )
cc_library(
    name = ""libtensorflow"",
    srcs = [
        ""lib/libtensorflow.so"",
        ""lib/libtensorflow_framework.so"",
    ],
    hdrs = [
        ""include/tensorflow/c/c_api.h"",
        ""include/tensorflow/c/c_api_experimental.h"",
    ],
    strip_include_prefix = ""include"",
    visibility = [""//visibility:public""],
)
```
With following code calling into C-API I see non-changed output buffer after SessionRun() and dubious console log:
```
2018-05-15 15:42:11.572574: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-05-15 15:42:11.695012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 10.92GiB freeMemory: 9.26GiB
2018-05-15 15:42:11.695035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-05-15 15:42:12.104753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-05-15 15:42:12.104773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2018-05-15 15:42:12.104777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2018-05-15 15:42:12.104976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8958 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Hello from TensorFlow C library version 1.8.0
inf starting
2018-05-15 15:42:12.195162: E tensorflow/core/grappler/clusters/utils.cc:127] Not found: TF GPU device with id 0 was not registered
Max value of 0 at 0
Max value of 0 at 0
Max value of 0 at 0
Max value of 0 at 0
Max value of 0 at 0
Max value of 0 at 0
Max value of 0 at 0
Max value of 0 at 0
Max value of 0 at 0
Max value of 0 at 0

```

The model is downloaded here: `https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet`
`https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.4_224.tgz`

The image is downloaded here:
`https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_example.ipynb`
and processed with python code for loading in c++:
```
import numpy as np
from PIL import Image
img = np.array(Image.open('panda.jpeg').resize((224, 224))).astype(np.float) / 128 - 1
reimg=np.reshape(img, [224,672])
np.savetxt('panda.ndarray',reimg)
```

### Source code / logs
```#include <string.h>
#include <fstream>
#include <iostream>
#include <memory>
#include <string>

#include ""gflags/gflags.h""
#include ""tensorflow/c/c_api.h""

#define ASSERT(expr, ...)      \
  if (!(expr)) {               \
    char buf[1024];            \
    sprintf(buf, __VA_ARGS__); \
    std::cerr << buf;          \
    std::abort();              \
  }

DEFINE_string(model_filename,
              ""mobilenet_v2_1.4_224_frozen.pb"",
              ""Filename for model to load"");

DEFINE_int32(num_repeat, 10, ""Run inference many times."");

DEFINE_string(np_filename, ""panda.ndarray"",
              ""Filename for (text) image file to load"");

void free_buffer(void* data, size_t length) { free(data); }

void Deallocator(void* data, size_t length, void* arg) { free(data); }

TF_Buffer* read_file(const char* file) {
  FILE* f = fopen(file, ""rb"");
  ASSERT(f != nullptr, ""Model File Not Ready"");
  fseek(f, 0, SEEK_END);
  long fsize = ftell(f);
  fseek(f, 0, SEEK_SET);  // same as rewind(f);

  void* data = malloc(fsize);
  std::ignore = fread(data, fsize, 1, f);
  fclose(f);

  TF_Buffer* buf = TF_NewBuffer();
  buf->data = data;
  buf->length = fsize;
  buf->data_deallocator = free_buffer;
  return buf;
}

int main(int argc, char** argv) {
  printf(""Hello from TensorFlow C library version %s\n"", TF_Version());

  // Creates buffer for graph def by reading pb file
  std::unique_ptr<TF_Buffer, std::function<void(TF_Buffer*)>> p_model_buffer(
      read_file(FLAGS_model_filename.c_str()), [](TF_Buffer* p) { TF_DeleteBuffer(p); });
  ASSERT(
      p_model_buffer != nullptr && p_model_buffer->data != nullptr && p_model_buffer->length != 0,
      ""Reading Graph Model pb file Failure"");

  // Creates error status for checking
  std::unique_ptr<TF_Status, std::function<void(TF_Status*)>> p_status(
      TF_NewStatus(), [](TF_Status* p) { TF_DeleteStatus(p); });
  ASSERT(p_status != nullptr, ""Status Creation Failure"");

  // Creates graph instance
  std::unique_ptr<TF_Graph, std::function<void(TF_Graph*)>> p_graph(
      TF_NewGraph(), [](TF_Graph* p) { TF_DeleteGraph(p); });
  ASSERT(p_graph != nullptr, ""Graph Creation Failure"");

  // Creates session option
  std::unique_ptr<TF_SessionOptions, std::function<void(TF_SessionOptions*)>> p_session_opts(
      TF_NewSessionOptions(), [](TF_SessionOptions* p) { TF_DeleteSessionOptions(p); });
  ASSERT(p_session_opts != nullptr, ""Create SessionOptions Failure"");

  // Creates options for importing graph_def
  std::unique_ptr<TF_ImportGraphDefOptions, std::function<void(TF_ImportGraphDefOptions*)>>
      p_import_graph_def_options(TF_NewImportGraphDefOptions(), [](TF_ImportGraphDefOptions* p) {
        TF_DeleteImportGraphDefOptions(p);
      });
  ASSERT(p_import_graph_def_options != nullptr, ""Create Import Graph Def Option Failure"");

  // Imports graph def into graph
  TF_GraphImportGraphDef(p_graph.get(), p_model_buffer.get(), p_import_graph_def_options.get(),
                         p_status.get());
  ASSERT(TF_GetCode(p_status.get()) == TF_Code::TF_OK, ""Import Failure %s"",
         TF_Message(p_status.get()));

  // Grab output ops from importing result
  TF_Operation* ops_in = TF_GraphOperationByName(p_graph.get(), ""input"");
  ASSERT(ops_in != nullptr, ""Getting Input Operation Failure"");
  TF_Operation* ops_out =
      TF_GraphOperationByName(p_graph.get(), ""MobilenetV2/Predictions/Reshape_1"");
  ASSERT(ops_out != nullptr, ""Getting Output Operation Failure"");
  TF_Output tf_output_in{.oper = ops_in, .index = 0};
  TF_Output tf_output_out{.oper = ops_out, .index = 0};

  // Create Tensors for input and output
  std::vector<std::shared_ptr<TF_Tensor>> input_tensor_vector;
  std::vector<std::shared_ptr<TF_Tensor>> output_tensor_vector;
  // Input tensor is fp32 image of 224x224 with bgr channels, as
  // img = np.array(PIL.Image.open('panda.jpeg').resize((224, 224))).astype(np.float) / 128 - 1
  int64_t input_dims[] = {1, 224, 224, 3};
  ASSERT(sizeof(float) == 4, ""Float32 should be 4 bytes."");
  size_t input_num_bytes = 224 * 224 * 3 * sizeof(float);
  // Output tensor is fp32 1001x1 one-hot for classes of objects
  int64_t output_dims[] = {1, 1001};
  size_t output_num_bytes = 1001 * sizeof(float);

  float* input_buffer = static_cast<float*>(malloc(input_num_bytes));
  ASSERT(input_buffer != nullptr, ""Input Tensor Memory Allocation Failure."");
  TF_Tensor* input_tensor =
      TF_NewTensor(TF_FLOAT, input_dims, 4, input_buffer, input_num_bytes, &Deallocator, 0);
  ASSERT(input_tensor != nullptr, ""Input Tensor Creation Failure"");
  input_tensor_vector.push_back(
      std::shared_ptr<TF_Tensor>(input_tensor, [](TF_Tensor* p) { TF_DeleteTensor(p); }));

  float* output_buffer = static_cast<float*>(malloc(output_num_bytes));
  ASSERT(output_buffer != nullptr, ""Output Tensor Memory Allocation Failure."");
  memset(output_buffer, 0, output_num_bytes);
  TF_Tensor* output_tensor =
      TF_NewTensor(TF_FLOAT, output_dims, 2, output_buffer, output_num_bytes, &Deallocator, 0);
  ASSERT(output_tensor != nullptr, ""Output Tensor Creation Failure"");
  output_tensor_vector.push_back(
      std::shared_ptr<TF_Tensor>(output_tensor, [](TF_Tensor* p) { TF_DeleteTensor(p); }));

  // Prepare input image data
  {
    std::ifstream img(FLAGS_np_filename);
    ASSERT(img, ""Read input image tensor failure."");
    float* p_data = static_cast<float*>(TF_TensorData(input_tensor_vector[0].get()));
    for (int i = 0; i < 224; ++i) {
      for (int j = 0; j < 224 * 3; ++j) {
        int offset = i * 224 * 3 + j;
        img >> p_data[offset];
      }
    }
  }
  {
    // Initializes config proto with magic numbers got from following python code:
    // config = tf.ConfigProto(allow_soft_placement = True)
    // serialized = config.SerializeToString() # '8\x01'
    char config_proto[] = {'8', 0x01};
    TF_SetConfig(p_session_opts.get(), config_proto, 2, p_status.get());
    ASSERT(TF_GetCode(p_status.get()) == TF_Code::TF_OK, ""Set Config Failure %s"",
           TF_Message(p_status.get()));

    // Creates session instance for loaded graph model
    std::unique_ptr<TF_Session, std::function<void(TF_Session*)>> p_session(
        TF_NewSession(p_graph.get(), p_session_opts.get(), p_status.get()), [&](TF_Session* p) {
          TF_CloseSession(p, p_status.get());
          TF_DeleteSession(p, p_status.get());
        });
    ASSERT(p_session != nullptr, ""Session Creation Failure %s"", TF_Message(p_status.get()));

    // Empty RunOptions for inference.
    TF_Buffer* run_options = nullptr;
    constexpr int ninputs = 1;
    constexpr int noutputs = 1;
    constexpr int ntargets = 0;
    TF_Tensor* output_tensors[1] = {output_tensor_vector[0].get()};
    TF_Tensor* const input_tensors[1] = {input_tensor_vector[0].get()};

    std::cerr << ""inf starting"" << std::endl;
    for (int i = 0; i < FLAGS_num_repeat; ++i) {
      TF_SessionRun(p_session.get(), run_options,
                    // Input tensors
                    &tf_output_in, input_tensors, ninputs,
                    // Output tensors
                    &tf_output_out, output_tensors, noutputs,
                    // Target operations
                    nullptr, ntargets,
                    // RunMetadata
                    nullptr,  // only valid run_metadata value
                    p_status.get());
      ASSERT(p_status != nullptr, ""Inference Failure %s"", TF_Message(p_status.get()));

      // print out max prob and corresponding idx
      const float* p_data = static_cast<const float*>(TF_TensorData(output_tensor_vector[0].get()));
      float v_max = p_data[0];
      int i_max = 0;
      for (int i = 1; i < 1001; ++i) {
        if (p_data[i] > v_max) {
          v_max = p_data[i];
          i_max = i;
        }
      }
      std::cout << ""Max value of "" << v_max << "" at "" << i_max << std::endl;
    }
  }

  return 0;
}
```"
19303,Model checkpoint issue: serialization error for tf.string,"TF version: 1.8
Installation: tensorflow/tensorflow:latest-gpu
OS details: 
Distributor ID:	Ubuntu
Description:	Ubuntu 16.04.4 LTS
Release:	16.04
Codename:	xenial

AWS instance: p2.xlarge
GPU details:
```
[name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 16086859869116902206
, name: ""/device:GPU:0""
device_type: ""GPU""
memory_limit: 11285974221
locality {
  bus_id: 1
  links {
  }
}
incarnation: 13890740079777279899
physical_device_desc: ""device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7""
]
```

Custom code:
```python
use_model = hub.Module(""http://tfhub.dev/google/universal-sentence-encoder/1"", trainable=True);
sess.run(tf.global_variables_initializer());
sess.run(tf.tables_initializer());

def USEEmbedding(x):
    return use_model(tf.squeeze(tf.cast(x, tf.string)), 
                      signature=""default"", as_dict=True)[""default""]
 
input_text = layers.Input(shape=(1,), dtype=tf.string)
embedding = layers.Lambda(USEEmbedding, output_shape=(512,))(input_text)
dense = layers.Dense(1024, activation='relu')(embedding)
bnorm = layers.BatchNormalization()(dense)
pred = layers.Dense(2000, activation='softmax')(bnorm)

model = Model(inputs=[input_text], outputs=pred)

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

callbacks = [keras.callbacks.EarlyStopping(monitor='val_acc',
                                               min_delta=1e-3,
                                               patience=8,
                                               verbose=0,
                                               mode='auto'),
             keras.callbacks.ModelCheckpoint('../models/best-weights.h5',
                                                 monitor='val_acc',
                                                 verbose=1,
                                                 save_best_only=True,
                                                 mode='auto'),
             keras.callbacks.TensorBoard(log_dir='../tb-logs', histogram_freq=0,
                                         write_graph=True, write_images=False)]

train_text = [' '.join(t.split()[0:20]) for t in train_x.sentences.tolist()]
train_text = np.array(train_text, dtype=object)[:, np.newaxis]
valid_text = [' '.join(t.split()[0:20]) for t in valid_x.sentences.tolist()]
valid_text = np.array(valid_text, dtype=object)[:, np.newaxis]

model.fit(train_text , train['labels'].tolist(),
          validation_data=(valid_text, valid['labels'].tolist()),
          epochs=100, batch_size=256,
          callbacks=callbacks, shuffle=True)
```

```
Error:

TypeError                                 Traceback (most recent call last)
<ipython-input-81-e546b70750dc> in <module>()
      2           validation_data=(valid_text, valid['labels'].tolist()),
      3           epochs=100, batch_size=256,
----> 4           callbacks=callbacks, shuffle=True)

/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1703                               initial_epoch=initial_epoch,
   1704                               steps_per_epoch=steps_per_epoch,
-> 1705                               validation_steps=validation_steps)
   1706 
   1707     def evaluate(self, x=None, y=None,

/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc in _fit_loop(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)
   1254                             for l, o in zip(out_labels, val_outs):
   1255                                 epoch_logs['val_' + l] = o
-> 1256             callbacks.on_epoch_end(epoch, epoch_logs)
   1257             if callback_model.stop_training:
   1258                 break

/usr/local/lib/python2.7/dist-packages/keras/callbacks.pyc in on_epoch_end(self, epoch, logs)
     75         logs = logs or {}
     76         for callback in self.callbacks:
---> 77             callback.on_epoch_end(epoch, logs)
     78 
     79     def on_batch_begin(self, batch, logs=None):

/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/callbacks.pyc in on_epoch_end(self, epoch, logs)
    466               self.model.save_weights(filepath, overwrite=True)
    467             else:
--> 468               self.model.save(filepath, overwrite=True)
    469           else:
    470             if self.verbose > 0:

/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc in save(self, filepath, overwrite, include_optimizer)
   2589         """"""
   2590         from ..models import save_model
-> 2591         save_model(self, filepath, overwrite, include_optimizer)
   2592 
   2593     def save_weights(self, filepath, overwrite=True):

/usr/local/lib/python2.7/dist-packages/keras/models.pyc in save_model(model, filepath, overwrite, include_optimizer)
    125             'class_name': model.__class__.__name__,
    126             'config': model.get_config()
--> 127         }, default=get_json_type).encode('utf8')
    128 
    129         model_weights_group = f.create_group('model_weights')

/usr/lib/python2.7/json/__init__.pyc in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, encoding, default, sort_keys, **kw)
    249         check_circular=check_circular, allow_nan=allow_nan, indent=indent,
    250         separators=separators, encoding=encoding, default=default,
--> 251         sort_keys=sort_keys, **kw).encode(obj)
    252 
    253 

/usr/lib/python2.7/json/encoder.pyc in encode(self, o)
    205         # exceptions aren't as detailed.  The list call should be roughly
    206         # equivalent to the PySequence_Fast that ''.join() would do.
--> 207         chunks = self.iterencode(o, _one_shot=True)
    208         if not isinstance(chunks, (list, tuple)):
    209             chunks = list(chunks)

/usr/lib/python2.7/json/encoder.pyc in iterencode(self, o, _one_shot)
    268                 self.key_separator, self.item_separator, self.sort_keys,
    269                 self.skipkeys, _one_shot)
--> 270         return _iterencode(o, 0)
    271 
    272 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,

/usr/local/lib/python2.7/dist-packages/keras/models.pyc in get_json_type(obj)
    102             return obj.__name__
    103 
--> 104         raise TypeError('Not JSON Serializable:', obj)
    105 
    106     from . import __version__ as keras_version

TypeError: ('Not JSON Serializable:', tf.string)
```"
19302,Image Classification: Usage with C++ API (python_api_gen failed),"### System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
**No custom code**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
**Ubuntu 16.04**
- TensorFlow installed from (source or binary):
**Installed from source**
- TensorFlow version (use command below):
**Tensorflow 1.8**
- Python version: 
**3**
- Bazel version (if compiling from source):
**Build label: 0.13.0**
- GCC/Compiler version (if compiling from source):
**gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609**
- CUDA/cuDNN version:
**Cuda 9.0, cuDNN 7.0**
- GPU model and memory:
**GTX 1060 3gb**
- Exact command to reproduce:
----as taken from: https://www.tensorflow.org/tutorials/image_recognition

bazel build tensorflow/examples/label_image/...

curl -L ""https://storage.googleapis.com/download.tensorflow.org/mode /inception_v3_2016_08_28_frozen.pb.tar.gz"" |
  tar -C tensorflow/examples/label_image/data -xz

bazel build tensorflow/examples/label_image/...


### Describe the problem

I am trying to run the image classification example with the c++ api; however, when I  do bazel build, the following error occurs. I had completed the python api example with no problems. Please help.

Aaron

### Source code / logs
ERROR: /home/aaron/Software/tensorflow/tensorflow/tools/api/generator/BUILD:27:1: Executing genrule //tensorflow/tools/api/generator:python_api_gen failed (Exit 1)
Traceback (most recent call last):
  File ""/home/aaron/.cache/bazel/_bazel_aaron/f08049efc03facba46c4fd0cb426b554/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/tools/api/generator/create_python_api.py"", line 26, in <module>
    from tensorflow.python.util import tf_decorator
  File ""/home/aaron/.cache/bazel/_bazel_aaron/f08049efc03facba46c4fd0cb426b554/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/home/aaron/.cache/bazel/_bazel_aaron/f08049efc03facba46c4fd0cb426b554/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""/usr/local/lib/python3.5/dist-packages/google/protobuf/__init__.py"", line 37, in <module>
    __import__('pkg_resources').declare_namespace(__name__)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2927, in <module>
    @_call_aside
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2913, in _call_aside
    f(*args, **kwargs)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2952, in _initialize_master_working_set
    add_activation_listener(lambda dist: dist.activate())
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 956, in subscribe
    callback(dist)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2952, in <lambda>
    add_activation_listener(lambda dist: dist.activate())
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2515, in activate
    declare_namespace(pkg)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2097, in declare_namespace
    _handle_ns(packageName, path_item)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2047, in _handle_ns
    _rebuild_mod_path(path, packageName, module)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2066, in _rebuild_mod_path
    orig_path.sort(key=position_in_sys_path)
AttributeError: '_NamespacePath' object has no attribute 'sort'
INFO: Elapsed time: 4009.117s, Critical Path: 120.96s
INFO: 7023 processes, local.

"
19301,"Broken link on ""Simple text classifier with TF-Hub"" tutorial","In the Feature Columns part of the page, i.e.

> Feature columns
> 
> TF-Hub provides a feature column that applies a module on the given text feature and passes further the outputs of the module. In this tutorial we will be using the nnlm-en-dim128 module. For the purpose of this tutorial, the most important facts are:

The link in text ""feature column"" is broken."
19299,resource_mgr.h violates its own thread safety annotations,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.13.4
- **TensorFlow installed from (source or binary)**: Source (ff6be80a1ec3c353ebd0d17e2f0b46d9097310db)
- **TensorFlow version (use command below)**: `b'v1.8.0-1663-gb2511764a7' 1.8.0`
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**: `0.13.0-homebrew`
- **GCC/Compiler version (if compiling from source)**: `Apple LLVM version 9.1.0 (clang-902.0.39.1)`
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

Line https://github.com/tensorflow/tensorflow/blob/de4a6e646be56ca59c78dd6f92f8f6bcc7196696/tensorflow/core/framework/resource_mgr.h#L527 accesses `resource_` without holding `mutex_`.  This is causing a warning for some custom op code of mine:

```
Installing collected packages: debate
  Found existing installation: debate 0.0.1
    Uninstalling debate-0.0.1:
      Successfully uninstalled debate-0.0.1
  Running setup.py develop for debate
    Complete output from command /Users/irving/anaconda/envs/openai/bin/python -c ""import setuptools, tokenize;__file__='/Users/irving/openai/debate/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" develop --no-deps:
    running develop
    running egg_info
    writing debate.egg-info/PKG-INFO
    writing dependency_links to debate.egg-info/dependency_links.txt
    writing requirements to debate.egg-info/requires.txt
    writing top-level names to debate.egg-info/top_level.txt
    file debate.py (for module debate) not found
    reading manifest file 'debate.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    writing manifest file 'debate.egg-info/SOURCES.txt'
    running build_ext
    building 'debate_ops' extension
    g++ -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/irving/anaconda/envs/openai/include -arch x86_64 -I/Users/irving/anaconda/envs/openai/include -arch x86_64 -I/Users/irving/anaconda/envs/openai/include/python3.6m -c debate/search.cc -o build/temp.macosx-10.7-x86_64-3.6/debate/search.o -std=c++1z -Wall -Werror -Wno-sign-compare -Wthread-safety -stdlib=libc++ -I/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
    In file included from debate/search.cc:7:
    /Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/include/tensorflow/core/framework/resource_mgr.h:527:22: error: passing variable 'resource_' by reference requires holding mutex 'mutex_' [-Werror,-Wthread-safety-reference]
      ctx->set_output(0, resource_);
                         ^
    /Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/include/tensorflow/core/framework/resource_mgr.h:334:12: note: in instantiation of member function 'tensorflow::ResourceHandleOp<debate::(anonymous namespace)::Search>::Compute' requested here
      explicit ResourceHandleOp(OpKernelConstruction* context);
               ^
    debate/search.cc:463:1: note: in instantiation of member function 'tensorflow::ResourceHandleOp<debate::(anonymous namespace)::Search>::ResourceHandleOp' requested here
    REGISTER_RESOURCE_HANDLE_KERNEL(Search);
    ^
    /Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/include/tensorflow/core/framework/resource_mgr.h:350:27: note: expanded from macro 'REGISTER_RESOURCE_HANDLE_KERNEL'
                              ResourceHandleOp<Type>)
                              ^
    1 error generated.
    error: command 'g++' failed with exit status 1
```

The code is at least *almost* correct, since

1. The use of `resource_` outside the lock is read only
2. `resource_` is only written to inside the lock.
3. The unlocked read occurs only if `initialized_` is set to true.

However, it looks like there is still a race condition:

1. Two threads could both detect that `initialized_` is false at the same time.
2. They both try to grab the lock.
3. One of them succeeds, does some initialization, and allocates the resource.
4. Once the first thread releases the lock, the second thread allocates the resource a second time.
5. Possibly (I'm not sure) the two threads see different values when they call `set_output`.

The moral of this story is probably to not ignore thread safety warnings."
19298,java/sbt. Cannot register 2 metrics with the same name on cross tests,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: osx 10.11.6
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**: NA
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: https://github.com/ravwojdyla/minimal-repo-tf-crashing

### Describe the problem
We have an issue open in scio (https://github.com/spotify/scio/issues/1137) which depends on java TF artifacts, please read full description in the link above. A TLDR: in cross test TF crushes with:

```
Cannot register 2 metrics with the same name: /tensorflow/cc/saved_model/load_attempt_count
```

This problem started to manifest itself from version 1.4.0 (previous versions were not affected), and continues to be a problem. The minimal reproduction repo is here: https://github.com/ravwojdyla/minimal-repo-tf-crashing and to reproduce you run `sbt +test`.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
19297,[Feature Request] GCS and S3 support on windows,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Will be required.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows, all versions
- **TensorFlow installed from (source or binary)**: source and binary,
- **TensorFlow version (use command below)**: master, and all releases
- **Python version**: python 3+
- **Bazel version (if compiling from source)**: - 
- **GCC/Compiler version (if compiling from source)**: - 
- **CUDA/cuDNN version**: - 
- **GPU model and memory**: - 
- **Exact command to reproduce**: Install tf on windows, and try anything that reads from GCS or S3

### Describe the problem
Currently, TF has no support for GCS and S3 on windows. Because they depend on curl, and we have not worked to make curl build and work on windows. Someone needs to dive in and work through the problems, and the rest should be just removing the windows exceptions for GCS and S3 support in configure.py.

This issue is open for community contributions."
19296,tf.gfile.Open has different semantics from open,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

`tf.gfile.Open` in `""w""` mode delays the creation of the file until the first write. This is different from the buitin [`open`](https://docs.python.org/3/library/functions.html#open), which creates a file right away. I am not sure if this is a bug, or the intended behaviour, but I think should be clarified in the documention of `tf.gfile.Open`.

### Source code / logs

```python
>>> import tensorflow as tf
>>> tf.gfile.Open(""hdfs://root/tmp/user/test"", ""wb"").close()
>>> tf.gfile.Exists(""hdfs://root/tmp/user/test"")
# ...
False
>>> tf.gfile.Open(""/tmp/test"", ""wb"").close()
>>> tf.gfile.Exists(""/tmp/test"")
False
```"
19295,[FR] Add warm_start_from in model_to_estimator,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Docker nightly
- **TensorFlow installed from (source or binary)**: Docker nightly
- **TensorFlow version (use command below)**: Docker nightly
- **Python version**: Docker nightly
- **Bazel version (if compiling from source)**: Docker nightly
- **GCC/Compiler version (if compiling from source)**: Docker nightly
- **CUDA/cuDNN version**: Docker nightly
- **GPU model and memory**:  N/A
- **Exact command to reproduce**: N/A

### Describe the problem
Please add `warm_start_from` parameter in `tf.model_to_estimator`

### Source code / logs
N/A"
19293,Feature request: Mask R-CNN support on TensorFlow Lite,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux openSUSE Leap 42.3
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
This is a feature request about supporting Mask R-CNN on TensorFlow Lite. Thanks.

### Source code / logs
Currently, the following operators are not supported by TensorFlow Lite: `ResizeNearestNeighbor`, `Stack`, and `TensorFlowShape`.

`Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: ResizeNearestNeighbor, Stack, TensorFlowShape.`

"
19292,tf.layers.Layer does not preserver the keras metadata entered into a tensor/placeholder,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.8.0-rc1
- **Python version**:  3.6
- **Bazel version (if compiling from source)**: 0.12.0
- **GCC/Compiler version (if compiling from source)**: 5.4.1
- **CUDA/cuDNN version**: 7.1
- **GPU model and memory**: 1080Ti
- **Exact command to reproduce**:  Any command like `tf.keras.layers.TimeDistributed(tf.layers.Conv2D(64, 3, padding='valid', activation=tf.nn.relu)(x))` where `x` is any tensor that has keras metadata like `_keras_shape` and/or `_keras_history`,

### Describe the problem
 wrapping any tensor which contains keras metadata such as `_keras_shape` or `_keras_histroy` with `tf.layers.Layer` or passing it through `tf.layers.Conv2D` does not preserve the keras metadata instead it discards them. In addition the `tf.layers.Layer` does not implement any `_add_inbound_node` method which is really confusing and leads to many errors with shape mismatches.
"
19290,Multiple TITAN V introduce new error: CUB segmented reduce error,"### System information
- Keras code (with tf backend) that works perfectly on multiples of Titan Xp or 1080 Ti and tested on multiple TF versions both on Windows and Ubuntu
- Windows 10
- TensorFlow  1.8.0 (b'v1.8.0-0-g93bc2e2072' 1.8.0)
- Python 3.6.5
- CUDA 9.0/ cuDNN 7.0
- Two TITAN V

### Describe the problem
We upgraded our hardware from two Titan Xp to two Titan V. Running the exact same training script that was running perfectly on two GPUs now produces the error:
```
tensorflow.python.framework.errors_impl.InternalError: CUB segmented reduce errorinvalid configuration argument
```

If we run the same script on a single GPU it runs fine. If we run on two GPUs with batch size '1' it also works fine. This could be a Keras issue, but I don't find anything in the Trackback to indicate that. The training starts but at Epoch 2 it crashes after going over some batches.

Also, a minor note, the word 'errorinvalid' has no space in between.

### Source code / logs
Full trackback:
```
Traceback (most recent call last):
  File ""C:\Users\User\AppData\Local\conda\conda\envs\py3tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""C:\Users\User\AppData\Local\conda\conda\envs\py3tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\Users\User\AppData\Local\conda\conda\envs\py3tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: CUB segmented reduce errorinvalid configuration argument
         [[Node: loss/DEC_CONV1C_loss/Mean_1 = Mean[T=DT_FLOAT, Tidx=DT_INT32, _class=[""loc:@training/Adam/gradients/loss/DEC_CONV1C_loss/Mean_1_grad/truediv""], keep_dims=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](loss/DEC_CONV1C_loss/Mean, training/Adam/gradients/loss/DEC_CONV1C_loss/Mean_1_grad/mod)]]
         [[Node: training/Adam/gradients/DEC_CONV1C_1/concat_grad/Slice_1/_429 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_1072_training/Adam/gradients/DEC_CONV1C_1/concat_grad/Slice_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:1""]()]]
```
The error prints this twice."
19289,Error in model load weights,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: from nightly-devel-gpu-py3 and nightly-gpu-py3 docker images
- **TensorFlow installed from (source or binary)**: from nightly-devel-gpu-py3 and nightly-gpu-py3 docker images
- **TensorFlow version (use command below)**: from nightly-devel-gpu-py3 and nightly-gpu-py3 docker images
- **Python version**: from nightly-devel-gpu-py3 and nightly-gpu-py3 docker images
- **Bazel version (if compiling from source)**: from nightly-devel-gpu-py3 and nightly-gpu-py3 docker images
- **GCC/Compiler version (if compiling from source)**: from nightly-devel-gpu-py3 and nightly-gpu-py3 docker images
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: tesla k80
- **Exact command to reproduce**: N/A


### Describe the problem
Running with nightly (both nightly-devel-gpu-py3 and nightly-gpu-py3) is not possible load weights on the keras model.
In the example belowe I have saved the weights, they are saved correctly in the file system and then I have tryed to load them, but I've got an error.

### Source code / logs
Here the snippet:

```
import tensorflow as tf
from tensorflow import keras as ks


def main():
    input_rgb = ks.layers.Input(shape=(1, 5, 5, 3), name=""input_rgb"")
    x = ks.layers.Dense(1, activation='relu', name=""Dense_1"")(input_rgb)
    x = ks.layers.Dense(1, activation='sigmoid', name=""sigmoid"")(x)
    model = ks.models.Model(inputs=[input_rgb], outputs=[x])
    model.compile(
        loss={'sigmoid': 'binary_crossentropy'},
        optimizer=tf.keras.optimizers.Adam())
    model.save_weights('/tmp/test_weights.h5')

    model.load_weights('/tmp/test_weights.h5', by_name=True)


if __name__ == ""__main__"":
    main()
```

And here the output:

```
2018-05-15 09:56:21.404382: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /tmp/test_weights.h5: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
```"
19288,The usage of `Dataset.from_generator()` function,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 7
- **TensorFlow installed from (source or binary)**:binary 
- **TensorFlow version (use command below)**:1.7
- **Python version**: 3.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8
- **GPU model and memory**: 1080Ti
- **Exact command to reproduce**:


### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I tried to use `Dataset.from_generator` function to return a dataset. 
```python
    ds = Dataset.from_generator(data,
                                (tf.float32,tf.float32, tf.int64, tf.bool),
                                (tf.TensorShape([None, None, num_features]),
                                 tf.TensorShape([None, None]),
                                 tf.TensorShape([None]),
                                 tf.TensorShape([])
                                 ))
```
but I met the error, which is against the API guide.

`
 File ""/opt/python/lib/python3.4/site-packages/tensorflow/python/data/util/nest.py"", line 365, in assert_shallow_structure
    ""Input has type: %s."" % type(input_tree))
TypeError: If shallow structure is a sequence, input must also be a sequence. Input has type: <class 'list'>.`

I also tried numpy array and tensorflow tensor but I met the identical error.

What should I return to use this API?
Should this API support numpy array and tensorflow tensor?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
19287,"How to convert ""Where"" and ""NonMaxSuppressionV2"" ops into tflite","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source  
- **TensorFlow version (use command below)**:
- **Python version**:  Python 2.7.12
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**:  gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I try to convert ssdlite mobilenev v2 into tflite, shows some ops not supported, model is downloaded from http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz

And I try to add the unsupportted ops into tflite, but I found that, for ""Where"" and ""NonMaxSuppressionV2"" , the output shape is not confirmed when call ""AllocateTensors"" at prepare stage.
As the output shape is depend on the input data & shape.
How shoud I do with the two ops ?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

bazel-bin/tensorflow/contrib/lite/toco/toco 
--input_file=./tf_model/ssdlite_mobilenet/ssdlite_mobilenet_v2_coco_2018_05_09/frozen_inference_graph.pb 
--input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE 
--output_file=./ssdlite_mobilenet/ssdlite_mobilenet_v2_coco_2018_05_09/frozen_inference_graph.tflite --inference_type=FLOAT 
--input_type=FLOAT --input_arrays=image_tensor 
--output_arrays=detection_boxes,detection_scores,num_detections,detection_classes --input_shapes=1,300,300,3

2018-05-15 09:10:15.881400: W tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:251] --input_type is deprecated. It was an ambiguous flag that set both --input_data_types and --inference_input_type. If you are trying to complement the input file with information about the type of input arrays, use --input_data_type. If you are trying to control the quantization/dequantization of real-numbers input arrays in the output file, use --inference_input_type.
2018-05-15 09:10:15.940730: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayV3
2018-05-15 09:10:15.940788: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.940799: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayV3
2018-05-15 09:10:15.940808: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.940818: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.940826: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayV3
2018-05-15 09:10:15.940833: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.940842: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.940849: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.940873: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayScatterV3
2018-05-15 09:10:15.940881: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.940890: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.940901: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: LoopCond
2018-05-15 09:10:15.940916: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Exit
2018-05-15 09:10:15.940923: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArraySizeV3
2018-05-15 09:10:15.940933: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Exit
2018-05-15 09:10:15.940939: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArraySizeV3
2018-05-15 09:10:15.940948: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayReadV3
2018-05-15 09:10:15.940967: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayWriteV3
2018-05-15 09:10:15.940979: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayWriteV3
2018-05-15 09:10:15.940999: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayGatherV3
2018-05-15 09:10:15.941012: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayGatherV3
2018-05-15 09:10:15.941022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.941030: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.941036: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.941723: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.941755: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.941862: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayV3
2018-05-15 09:10:15.941874: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.941883: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayV3
2018-05-15 09:10:15.941892: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.941901: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayV3
2018-05-15 09:10:15.941909: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.941917: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayV3
2018-05-15 09:10:15.941925: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.941933: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayV3
2018-05-15 09:10:15.941943: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.941954: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.941961: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayV3
2018-05-15 09:10:15.941969: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.941978: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.941987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayV3
2018-05-15 09:10:15.941995: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.942004: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.942012: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayV3
2018-05-15 09:10:15.942020: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.942028: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.942034: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.942058: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayScatterV3
2018-05-15 09:10:15.942067: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.942089: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayScatterV3
2018-05-15 09:10:15.942098: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.942120: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayScatterV3
2018-05-15 09:10:15.942129: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.942151: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayScatterV3
2018-05-15 09:10:15.942159: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.942169: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Enter
2018-05-15 09:10:15.942180: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: LoopCond
2018-05-15 09:10:15.942192: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Exit
2018-05-15 09:10:15.942199: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArraySizeV3
2018-05-15 09:10:15.942209: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Exit
2018-05-15 09:10:15.942216: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArraySizeV3
2018-05-15 09:10:15.942224: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Exit
2018-05-15 09:10:15.942231: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArraySizeV3
2018-05-15 09:10:15.942239: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Exit
2018-05-15 09:10:15.942245: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArraySizeV3
2018-05-15 09:10:15.942255: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayReadV3
2018-05-15 09:10:15.942263: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayReadV3
2018-05-15 09:10:15.942270: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayReadV3
2018-05-15 09:10:15.942278: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayReadV3
2018-05-15 09:10:15.942285: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942293: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942300: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942307: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942316: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942323: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942329: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942336: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942343: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942350: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942357: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942364: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942377: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942384: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942393: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942400: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942406: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942413: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942419: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942427: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942434: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942440: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942447: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942453: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942460: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942467: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942474: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942481: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942487: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942493: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942501: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942507: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942514: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942520: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942527: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942533: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942541: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942547: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942554: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942560: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942566: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942574: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942580: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942587: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942593: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942600: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942606: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942614: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942621: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942627: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942634: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942640: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942647: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942654: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942660: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942667: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942673: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942680: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942694: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942700: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942706: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942713: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942719: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942733: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942740: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942746: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942752: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942759: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942766: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942773: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942779: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942785: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942792: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942799: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942806: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942812: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942819: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942825: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942832: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942839: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942845: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942852: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942858: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942864: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942871: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942878: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942885: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942907: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Unpack
2018-05-15 09:10:15.942944: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Equal
2018-05-15 09:10:15.943742: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.943796: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.943831: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.943846: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.943860: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.943910: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.943945: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.943959: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.943973: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.944020: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.944055: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.944068: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.944082: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.944128: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.944160: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.944174: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.944187: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.944234: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.944270: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.944284: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.944297: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.944344: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.944379: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.944392: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.944406: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.944453: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.944486: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.944501: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.944514: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.944560: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.944593: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.944606: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.944620: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.944665: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.944698: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.944712: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.944726: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.944775: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.944810: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.944824: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.944840: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.944886: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.944920: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.944934: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.944948: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.944994: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.945028: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.945041: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.945055: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.945119: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.945152: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.945166: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.945179: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.945225: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.945260: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.945274: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.945289: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.945336: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.945369: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.945383: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.945398: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.945443: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.945477: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.945490: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.945504: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.945550: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.945583: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.945595: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.945609: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.945653: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.945686: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.945698: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.945712: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.945765: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.945799: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.945812: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.945827: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.945873: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.945906: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.945919: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.945933: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.945980: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.946012: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.946025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.946039: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.946084: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.946116: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.946128: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.946142: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.946188: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.946221: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.946233: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.946249: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.946295: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.946328: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.946341: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.946355: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.946401: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.946435: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.946447: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.946462: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.946508: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.946540: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.946553: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.946566: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.946613: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.946645: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.946657: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.946671: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.946718: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.946752: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.946765: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.946780: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.946832: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.946864: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.946878: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.946892: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.946938: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.946971: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.946984: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.946997: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.947043: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.947076: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.947089: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.947102: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.947149: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.947183: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.947197: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.947211: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.947257: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.947291: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.947305: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.947319: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.947365: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.947398: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.947411: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.947425: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.947471: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.947504: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.947517: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.947530: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.947576: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.947608: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.947622: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.947635: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.947683: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.947716: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.947730: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.947743: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.947790: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.947823: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.947837: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.947850: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.947897: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.947930: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.947943: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.947958: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948004: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948036: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.948050: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.948064: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948110: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948142: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.948155: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.948169: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948216: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948250: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.948264: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.948279: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948324: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948358: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.948371: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.948386: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948432: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948465: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.948478: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.948492: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948538: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948570: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.948583: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.948596: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948642: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948676: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.948689: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.948703: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948749: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948783: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.948796: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.948810: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948856: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948890: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.948903: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.948918: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948964: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.948996: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.949008: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.949024: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.949068: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.949101: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.949113: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.949127: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.949174: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.949207: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.949220: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.949234: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.949280: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.949313: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.949326: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.949341: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.949387: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.949420: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.949434: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.949447: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.949494: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.949527: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.949541: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.949554: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.949599: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.949631: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.949644: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.949657: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.949703: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.949741: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.949755: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.949769: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.949815: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.949848: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.949862: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.949876: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.949922: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.949956: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.949970: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.949983: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.950029: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.950061: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.950075: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.950088: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.950134: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.950166: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.950181: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.950194: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.950240: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.950273: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.950287: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.950300: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.950346: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.950380: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.950393: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.950407: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.950454: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.950486: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.950499: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.950513: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.950558: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.950591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.950604: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.950618: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.950663: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.950698: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.950712: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.950726: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.950772: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.950805: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.950819: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.950834: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.950880: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.950915: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.950928: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.950942: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.950989: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.951022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.951033: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.951048: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.951094: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.951126: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.951138: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.951152: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.951199: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.951232: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.951245: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.951259: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.951305: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.951339: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.951352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.951367: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.951414: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.951447: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.951459: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.951473: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.951519: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.951551: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.951563: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.951577: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.951623: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.951655: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.951668: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.951684: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.951730: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.951763: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.951776: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.951790: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.951838: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.951872: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.951886: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.951899: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.951945: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.951977: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.951990: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.952004: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.952050: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.952082: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.952095: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.952108: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.952154: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.952188: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.952203: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.952216: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.952264: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.952297: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.952311: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.952324: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.952370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.952403: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.952417: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.952431: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.952477: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.952509: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.952523: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.952536: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.952581: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.952654: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.952668: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.952682: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.952728: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.952761: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.952775: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.952788: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.952834: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.952868: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.952882: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.952896: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.952942: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.952974: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.952987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.953000: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.953046: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.953092: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.953106: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.953119: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.953162: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.953193: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.953207: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.953220: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.953266: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.953300: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.953314: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.953327: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.953374: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Where
2018-05-15 09:10:15.953406: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: NonMaxSuppressionV2
2018-05-15 09:10:15.953420: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: ZerosLike
2018-05-15 09:10:15.953484: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Size
2018-05-15 09:10:15.953503: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: Equal
2018-05-15 09:10:15.953602: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayWriteV3
2018-05-15 09:10:15.953690: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayWriteV3
2018-05-15 09:10:15.953781: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayWriteV3
2018-05-15 09:10:15.953832: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayWriteV3
2018-05-15 09:10:15.953854: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayGatherV3
2018-05-15 09:10:15.953869: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayGatherV3
2018-05-15 09:10:15.953884: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayGatherV3
2018-05-15 09:10:15.953896: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1326] Converting unsupported operation: TensorArrayGatherV3
2018-05-15 09:10:16.114762: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 4337 operators, 7123 arrays (0 quantized)
2018-05-15 09:10:16.403415: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 4311 operators, 7076 arrays (0 quantized)
2018-05-15 09:10:16.760136: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 4311 operators, 7076 arrays (0 quantized)
2018-05-15 09:10:16.929347: F tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:95] Check failed: other_op->type == OperatorType::kTensorFlowMerge
Aborted (core dumped)"
19286,[tf-gpu1.7][tf-serving] Error with cuDNN version incompatible issue when load a ckpt model to do the inference,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat 4.8.5-16, Linux version 3.10.0-693.5.2.el7.x86_64
- **TensorFlow installed from (source or binary)**: pip (python 2.7)
- **TensorFlow version (use command below)**: tensorflow-gpu==1.7.0
- **Python version**:  python 2.7
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: gcc 5.3
- **CUDA/cuDNN version**: CUDA9.0, cuDNN7.0.5
- **GPU model and memory**: Tesla P4, 8GB
- **Exact command to reproduce**: NA

### Describe the problem
I established a tensorflow-serving environment by gRPC, which load a model in MetaGraph/ckpt format and create a session to do inference. It worked fine under tensorflow-gpu 1.4.1, but now Error happened when I changed to tensorflow-gpu 1.7.0. 

Here is the log when server side starts up:
```
$ python classification_server.py -m inception_v3.ckpt-85000 -p 10000
2018-05-15 14:44:53.703251: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-05-15 14:44:53.992419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:84:00.0
totalMemory: 11.90GiB freeMemory: 11.74GiB
2018-05-15 14:44:54.221179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 1 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:88:00.0
totalMemory: 11.90GiB freeMemory: 11.74GiB
2018-05-15 14:44:54.222130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0, 1
2018-05-15 14:44:55.050563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-05-15 14:44:55.050618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 1 
2018-05-15 14:44:55.050667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N Y 
2018-05-15 14:44:55.050677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 1:   Y N 
2018-05-15 14:44:55.051199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2437 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:84:00.0, compute capability: 6.1)
2018-05-15 14:44:55.098488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 2437 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:88:00.0, compute capability: 6.1)
Tensor(""InceptionV3/Logits/global_pool:0"", shape=(?, 1, 1, 2048), dtype=float32)
[Tue May 15 14:44:57 2018]server port 10000
[Tue May 15 14:44:57 2018]config visual_model 
[Tue May 15 14:44:57 2018]server start at 10000

```
Here is the log on the server side when a client asks for image classification service:
```
2018-05-15 14:45:10.095470: E tensorflow/stream_executor/cuda/cuda_dnn.cc:396] Loaded runtime CuDNN library: 7103 (compatibility version 7100) but source was compiled with 7005 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.
2018-05-15 14:45:10.096881: F tensorflow/core/kernels/conv_ops.cc:712] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) 
Aborted (core dumped)
```
What does client side gets:
```
$ python client.py  -p 127.0.0.1:10000
Traceback (most recent call last):
  File ""client.py"", line 52, in <module>
    run(options.port, options.feat_len)
  File ""client.py"", line 26, in run
    reply = stub.GetFeatureByImgs(tmp2)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/grpc/_channel.py"", line 487, in __call__
    return _end_unary_response_blocking(state, call, False, deadline)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/grpc/_channel.py"", line 437, in _end_unary_response_blocking
    raise _Rendezvous(state, None, None, deadline)
grpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Socket closed)>
```

I checked the CUDA+cuDNN version in below way to ensure that I am using CUDA9.0 and cuDNN 7.0.5. Not sure why Tensorflow says I'm using cuDNN 7.1.3.

```
$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Sep__1_21:08:03_CDT_2017
Cuda compilation tools, release 9.0, V9.0.176
```
```
$ python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib() + ""/python/_pywrap_tensorflow_internal.so"")' | xargs ldd
/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
	linux-vdso.so.1 =>  (0x00007ffd3b9f7000)
	libtensorflow_framework.so => /home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so (0x00007f5927b53000)
	libcublas.so.9.0 => /home/web_server/xiaolun/cuda-9.0/lib64/libcublas.so.9.0 (0x00007f592440e000)
	libcusolver.so.9.0 => /home/web_server/xiaolun/cuda-9.0/lib64/libcusolver.so.9.0 (0x00007f591f812000)
	libcudart.so.9.0 => /home/web_server/xiaolun/cuda-9.0/lib64/libcudart.so.9.0 (0x00007f591f5a5000)
	libdl.so.2 => /lib64/libdl.so.2 (0x00007f591f392000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x00007f591f175000)
	libgomp.so.1 => /home/web_server/gcc-5.3/lib64/libgomp.so.1 (0x00007f591ef56000)
	librt.so.1 => /lib64/librt.so.1 (0x00007f591ed4e000)
	libm.so.6 => /lib64/libm.so.6 (0x00007f591ea4b000)
	libstdc++.so.6 => /home/web_server/gcc-5.3/lib64/libstdc++.so.6 (0x00007f591e6bd000)
	libgcc_s.so.1 => /home/web_server/gcc-5.3/lib64/libgcc_s.so.1 (0x00007f591e4a7000)
	libc.so.6 => /lib64/libc.so.6 (0x00007f591e0e3000)
	/lib64/ld-linux-x86-64.so.2 (0x000055944f876000)
	libcuda.so.1 => /lib64/libcuda.so.1 (0x00007f591d265000)
	libcudnn.so.7 => /home/web_server/xiaolun/cuda-9.0/lib64/libcudnn.so.7 (0x00007f59093f9000)
	libcufft.so.9.0 => /home/web_server/xiaolun/cuda-9.0/lib64/libcufft.so.9.0 (0x00007f5901358000)
	libcurand.so.9.0 => /home/web_server/xiaolun/cuda-9.0/lib64/libcurand.so.9.0 (0x00007f58fd3f3000)
	libnvidia-fatbinaryloader.so.384.90 => /lib64/libnvidia-fatbinaryloader.so.384.90 (0x00007f58fd1a1000)
```

Moreover, I tried to load a frozen model to do the inference in tensorflow-serving, and it worked fine.
Has anyone encountered a similar problem? Any idea will be welcome.

Thanks,


### Source code / logs

"
19284,Training bug in mobilenet v1 extractor for Faster r-cnn,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
custom kitti format dataset (good performance in Inception, Resnet extractor)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**: 1.6.0
- **Python version**: 3.4
- **Bazel version (if compiling from source)**: No
- **GCC/Compiler version (if compiling from source)**: GCC 4.8.4
- **CUDA/cuDNN version**: 9, 7
- **GPU model and memory**: Tesla 40
- **Exact command to reproduce**:

### Describe the problem
It seems like training bug with Faster R-CNN Mobilenet.
I've been training Faster R-CNN with mobilenet feature extractor for 400k iteration, batch 8, learning rate 3e-03(mentioned in HuangMurphy_2017_Speed,accuracy trade-offs for modern convolutional object detectors). But it's mAP is ""zero"".  I'm using my own dataset and it's going well with InceptionV2, Resnet50, 101. mAP of InceptionV2, Resnet50, 101 is 0.7. So it's not about hyperparameter tuning problem.


### Source code / logs
here is loss of inception v2, which has good mAP.
![image](https://user-images.githubusercontent.com/39238559/40033921-76956e36-5835-11e8-966f-347359f9588d.png)

here is loss of mobilenet. Second stage loss is strangely low (loss is zero almost of time)
![image](https://user-images.githubusercontent.com/39238559/40033856-352ec6ae-5835-11e8-973e-44c2806cacea.png)

here is mAP of mobilenet. How can it be a zero?
![image](https://user-images.githubusercontent.com/39238559/40033951-b43a1142-5835-11e8-8166-c4315a83a17c.png)

here is my tensorboard distributions. Compared to Incepction V2, mobilenet has No change in second stage conv2d_12, conv2d_13 - moving_mean, moving_variance. I think it could be a clue of cause.
![image](https://user-images.githubusercontent.com/39238559/40034051-1937993e-5836-11e8-860c-e722bb71725b.png)

"
19283,Tensor with GPUBFCAllocator generate Segfaults on tensorflow::ops,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.8
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: 0.18
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9.0 / 7.1
- **GPU model and memory**: nvidia p6000
- **Exact command to reproduce**: 


### Describe the problem
Using the C++ api, when a tensorflow::Tensor is allocated using GPUBFCAllocator, all further operations (such as tensorflow::op::Identity) generate a segfault. Memory is properly allocated (tested with cudaMemcpy). If the allocator is removed, or switched to tensorflow::cpu_allocator(), the error does not occur, but the memory is not allocated on the gpu (memcpy fails). My final goal is to obtain the cuda pointer of the tensor to fill the tensor with data manually. 

### Source code / logs
Reproducible code: 
```
  auto root = tensorflow::Scope::NewRootScope().WithDevice(""/gpu:0"");
  tensorflow::ClientSession* session = new tensorflow::ClientSession(root);
  
  tensorflow::VisitableAllocator* gpu_allocator;
  gpu_allocator = new tensorflow::GPUBFCAllocator(tensorflow::CudaGpuId(0), sizeof(float) * 512 * 640 * 3, ""bfc_gpu_0"");
  gpu_allocator = new tensorflow::GPUcudaMallocAllocator(gpu_allocator, tensorflow::CudaGpuId(0));

  void* myp = gpu_allocator->AllocateRaw(1,512*640*3*4);
  tensorflow::Tensor *myTensor = new tensorflow::Tensor(gpu_allocator,tensorflow::DT_FLOAT, tensorflow::TensorShape({1, 512, 640, 3}));

  checkCudaErrors(cudaMemcpy(tensorflow::DMAHelper::base(myTensor), myp, sizeof(float) * 512 * 640 *3, cudaMemcpyDeviceToDevice));

  auto identity = tensorflow::ops::Identity(root.WithOpName(""init""), *myTensor);
```

"
19282,docker image temsorflow:latest-gpu-py3  failed to import tensorflow,"Im running tensorflow with cuda in a docker container as an instance of tensorflow/tensorflow:latest-gpu-py3.

When `import tensorflow` in a python3 terminal, it throws an error because libcuda.so.1 not in library path.

These commands fix it. 
```
cd /usr/local/cuda/lib64 \
 && mv stubs/libcuda.so ./ \
 && ln -s libcuda.so libcuda.so.1 \
 && ldconfig
```

Please fix that bug in docker image.
"
19281,Tensorflow takes up all system memory in distributed training ,"**System Information:**

- OS Platform: Redhat 6.6
- TensorFlow installed from binary
- TensorFlow version: 1.6.1
- CUDA version: 9.1
- cuDNN version: 9.1

output for print(tf.GIT_VERSION, tf.VERSION):
('v1.6.0-rc1-3-g690ce9c6cc', '1.6.0-rc0')

**Problem:**
I'm running distributed training with TensorFlow GPU build in a managed cluster. Each box has 4 GPUs and multi tenancy is enabled in this cluster. The problem is each TensorFlow job running inside a container inside the node would allocate **ALL** available system RAM to the process and ends in exhausting available virtual memory. For example, the box has 4 GPUs and 400 GB mem. When we allocate 4 jobs in the same node each asking for a single GPU, each container running the tf job would allocate a 400GB vm, in total 2TB mem and blow away the box :(

**Stack trace:**
```
INFO:root: Before calling tf.train.Server(cluster_spec, job_name, task_index): mem usage: 159284 kB, vmem 1228872 kB
2018-05-15 01:38:39.088741: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-05-15 01:38:44.083881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:05:00.0
totalMemory: 11.17GiB freeMemory: 11.10GiB
2018-05-15 01:38:44.083950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2018-05-15 01:38:44.386139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-05-15 01:38:44.386178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
2018-05-15 01:38:44.386187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
2018-05-15 01:38:44.390272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:0 with 10763 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
2018-05-15 01:38:44.589474: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> ltx1-hcl4471.grid.linkedin.com:9521}
2018-05-15 01:38:44.589514: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> ltx1-hcl4471.grid.linkedin.com:10011, 1 -> localhost:16944}
2018-05-15 01:38:44.591463: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:333] Started server with target: grpc://localhost:16944
INFO:root: After calling tf.train.Server(cluster_spec, job_name, task_index): mem usage: 838408 kB, vmem 292753900 kB
```

This is killing the multi-tenancy for a managed cluster, please help!"
19280,cuda libraries cannot be found during linking,"Hi All,

I am trying to build tensorflow 1.8 with bazel 0.13, gcc 4.8.5, cuda 9.0, Anaconda3 5.1.0 on Scientific LInux 7.

This is HPC environment and applications packages are installed in non-standard locations.
I set up environment so that I can use all the above software from command line.
In addition I specify path to cuda and cudnn during configuration and use --action_env.
Nevertheless, the build process fails at linking stage because it cannot find cuda libraries:

bazel build --config=opt --config=cuda --action_env=LD_LIBRARY_PATH=$LD_LIBRARY_PATH //tensorflow/tools/pip_package:build_pip_package
...
ERROR: /home/ivy2/tmp11/tensorflow/tensorflow/python/BUILD:1473:1: Linking of
rule '//tensorflow/python:gen_io_ops_py_wrappers_cc' faile
d (Exit 1)
/usr/bin/ld: warning: libcublas.so.9.0, needed by
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Uio_Uops_Upy_Uwrappers_U
cc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or
-rpath-link)


The reason I am building tensorflow from sources is because I got a request for C++ interface to tensorflow. My understand is that I need to build from source to get it. Is that right or is there a simpler way?

Thank you,
Igor


==========
Have I written custom code

N/A

OS Platform and Distribution

Scientific Linux 7

TensorFlow installed from

N/A

TensorFlow version

1.8

Bazel version

0.13

CUDA/cuDNN version

9/7

GPU model and memory

Tesla K80, 64G RAM

Exact command to reproduce

bazel build --config=opt --config=cuda --action_env=LD_LIBRARY_PATH=$LD_LIBRARY_PATH //tensorflow/tools/pip_package:build_pip_package
"
19279,Feature Request: serialising checkpoints and models to byte arrays,"### System information
- **Have I written custom code**: Yes
- **OS Platform and Distribution**: Linux, Oracle Linux 7.4
- **TensorFlow installed from**: source
- **TensorFlow version**: 1.7 & 1.8
- **Python version**: 2.7 & 3.6
- **Bazel version**: 0.11.1
- **GCC/Compiler version**: 4.8.5
- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7
- **GPU model and memory**: Titan X
- **Exact command to reproduce**: N/A

### Describe the problem
I'm wrapping Tensorflow in a higher level Java prediction API, using the provided Tensorflow Java API. This higher level API controls the serialisation of models in a uniform way, allowing a user to serialise models to disk, or over the network, and to load models as resources from jar files or non-disk sources.

I can train and test a model from Java, after creating the graph protobuf in Python, however the checkpointing and saved model functionality is extremely fragile as it requires specific structures in the filesystem. Currently I can either serialise a path to disk which means the model requires both the path to be valid as well as the serialised blob the high level API creates, or I can tar up the checkpoint/model bundle directory and serialise that as a byte array before untar-ing it to a temp directory and loading. Neither of these options are particularly satisfactory.

I had originally considered adding an extra couple of tensor endpoints to Saver which accepted a byte array wrapped in a tensor, deserialising that into the session, and one which emitted a byte array which was a serialised form of that session, but given how far down the assumption of filesystem access is in a saver I'm not sure that's the right approach.

I'm now thinking that adding another Saver class which has the byte array operations would be the best way to go. I'd like to contribute this back to tensorflow, so I thought it best to discuss the design first (and even if this kind of contribution is acceptable).

My ultimate goal is to add a Java implementation of freeze_graph.py, as that operation would make prediction time deployment much simpler, but there would still be a checkpointing issue while a model is training. Given #17390 I'm not sure if the Java API has enough operations implemented to freeze a model yet.
"
19277,SSD mobilenet inference is slower w/ MKL,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.8.0 (could repro the same problem from head too)
- **Python version**: 
N/A
- **Bazel version (if compiling from source)**:
0.13.0
- **GCC/Compiler version (if compiling from source)**:
5.4.0
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
OMP_NUM_THREADS=1 bazel run --config=mkl --config=opt --config=monolithic //tensorflow/tools/benchmark:benchmark_model -- --graph=ssd_mobilenet_v2_coco_2018_03_29_frozen.pb --show_flops --input_layer=image_tensor --input_layer_type=uint8 --input_layer_shape=1,1920,1080,3 --output_layer=num_detections,detection_classes,detection_scores,detection_boxes --num_threads=1

### Describe the problem
w/ MKL, benchmark_model got 18.98B FLOPs/second, w/o MKL, it got 25.61B.
From the benchmark_model results, we could see that _MklConv2DWithBias is the culprit.
I am using MKL 2018.2.199 and mkldnn 0.14 on a i7-5557U CPU.

A unrelated question: @agramesh1 I filed the same bug on [mkldnn](https://github.com/intel/mkl-dnn/issues/234). They told me you have some plan to implement MKL version of DepthwiseConv2dNative. Do you have a timeline for it? I am eager to try it..

benchmark_model results:

 w/ MKL

             [Node type]  [count]  [avg ms]    [avg %]    [cdf %]  [mem KB][times called]
       _MklConv2DWithBias       12   143.936    42.710%    42.710% 56555.531       12
               _MklConv2D       43    48.872    14.502%    57.212% 43206.305       43
      DepthwiseConv2dNative       21    27.330     8.110%    65.321% 17180.992       21
                  _MklMul       43    12.751     3.784%    69.105% 32070.080       43
                  _MklAdd       53    12.137     3.601%    72.706% 33662.238       53
                     Cast      183    11.584     3.437%    76.143% 24883.217      183
        _MklInputConversion       96    10.990     3.261%    79.404% 32261.969       96
                    Const     1602    10.360     3.074%    82.479%     0.000     1602
                      Mul      127     7.063     2.096%    84.574%     0.004      127
                      Add      119     6.541     1.941%    86.515%    15.344      119
                    Relu6       47     3.715     1.102%    87.618%     0.000       47
                  Minimum      451     3.680     1.092%    88.709%     0.000      451
                   Gather      546     3.607     1.070%    89.780%     0.000      546
                    Slice       93     3.543     1.051%    90.831%  1380.240       93
       TensorArrayScatterV3        5     3.261     0.968%    91.799% 25604.012        5
              _MklReshape      107     3.183     0.944%    92.743%   810.972      107
                    Where      180     2.560     0.760%    93.503%     1.440      180
             _MklConcatV2       98     2.260     0.671%    94.173%   782.604       98
                  Greater      183     2.183     0.648%    94.821%   172.533      183

w/o MKL

              [Node type]  [count]  [avg ms]    [avg %]    [cdf %]  [mem KB][times called]
                   Conv2D       55   108.692    47.515%    47.515% 32798.539       55
    DepthwiseConv2dNative       21    28.022    12.250%    59.765% 17180.992       21
                      Mul      170    18.866     8.247%    68.012%     0.008      170
                      Add      172    18.460     8.070%    76.082%    15.344      172
                     Cast      183    12.116     5.297%    81.378% 24883.217      183
                   Gather      546     4.815     2.105%    83.483%     0.000      546
     TensorArrayScatterV3        5     3.971     1.736%    85.219% 25604.012        5
                    Relu6       47     3.619     1.582%    86.801%     0.000       47
                    Slice       93     2.915     1.274%    88.075%  1380.240       93
                  Minimum      451     2.877     1.258%    89.333%     0.000      451
                    Const      476     2.802     1.225%    90.558%     0.000      476
                  Maximum      360     2.761     1.207%    91.765%     0.000      360
                  Reshape      287     2.318     1.013%    92.778%     0.000      287
                    Where      180     1.793     0.784%    93.562%     1.440      180
                      Sub      190     1.749     0.765%    94.327%     0.008      190
                    Split      180     1.749     0.765%    95.091%     0.000      180
                  Greater      183     1.220     0.533%    95.625%   172.533      183
                 ConcatV2       99     1.057     0.462%    96.087%   730.868       99
                    Shape      112     1.001     0.438%    96.524%     0.952      112
                  Squeeze       92     0.978     0.428%    96.952%     0.000       92
           ResizeBilinear        1     0.885     0.387%    97.339%  1080.000        1
             StridedSlice      113     0.878     0.384%    97.722%     0.432      113
      NonMaxSuppressionV2       90     0.820     0.358%    98.081%     0.000       90
                Transpose        3     0.706     0.309%    98.390%    92.016        3
                    Enter       26     0.434     0.190%    98.579%     0.000       26
                     Pack       19     0.433     0.189%    98.769%    30.908       19
                ZerosLike       92     0.362     0.158%    98.927%     0.004       92
                  BiasAdd       12     0.326     0.143%    99.069%     0.000       12
            NextIteration        8     0.308     0.135%    99.204%     0.000        8


### Source code / logs
I run benchmark_model with MKLDDN_VERBOSE=1 and got this log.
https://drive.google.com/file/d/12ClzFKiOryge6So-trXrvEyEk6ycOheY/view?usp=sharing"
19275,tensorflow lite for android doesn't work with gradle version 4.4,"I am using mobilenetv2 pre-trained models to inference images. tensorflow lite for android takes 3 times more time to inference with gradle 4.4. if you change the gradle version to 4.1, it works as expected.  "
19274,tf.matching_files fails when there is a file with wrong permissions in a subdirectory ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu18.04
- **TensorFlow installed from (source or binary)**: pip installed binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: see below

```python
import tensorflow as tf
from tensorflow.python.framework.graph_util import convert_variables_to_constants

graph = tf.Graph()
with graph.as_default():
    output1 = tf.matching_files('/some/dir', name='output1', )
    sess = tf.InteractiveSession(graph=graph)
    print(sess.run([output1]))
```

### Describe the problem

This simple script fails if some dir contains a file without read access somewhere in subfolders of `/some/dir`. So for instance if I don't have permission to read `/some/dir/subfolder/filebelongingtosomeoneelse`, the script fails (while I have permissions for subfolder). 
Strange thing is that neither read access needed to know its filename, nor it is actually will be listed in the output (so it should be skipped during scanning). 

"
19272,Gradient Penalty won't execute,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary (pip install inside conda)
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 2.7.9
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0 / 7
- **GPU model and memory**: Nvidia Titan X
- **Exact command to reproduce**:

I use the following code for Gradient Penalty. The code works fine without gradient penalty but with gradient penalty, the code hangs at the mentioned line (mentioned with -------> ). For some reason, incorporating the gradient in the loss term isn't letting me take the gradient. Is it a bug?

    def dropout(inp):
            return tf.nn.dropout(inp,0.8)

    def mnist_svhn(X,noise,reuse=False):
        with tf.variable_scope(""Encoder/mnist-svhn"",reuse=reuse,initializer=initializer):
            h0 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(X,32,5,1)))
            h1 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h0,64,4,2)))
            h2 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h1,128,4,1)))
            h3 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h2,256,4,2)))
            h4 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h3,256,3,1)))
            h5 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h4,256,1,1)))
            h6 = tf.layers.conv2d(h5,256,1,1)

            h7 = tf.concat([h6,noise],axis=-1)

            h8 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h7,256,4,1)))
            h9 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h8,128,4,2)))
            h10 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h9,64,4,1)))
            h11 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h10,32,4,2)))
            h12 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h11,3,5,1)))
            h13 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h12,32,1,1)))
            h14 = tf.layers.conv2d_transpose(h13,3,1,1,activation=tf.nn.sigmoid)

        print h14

    def svhn_mnist(X,noise,reuse=False):
        with tf.variable_scope(""Encoder/svhn-mnist"",reuse=reuse,initializer=initializer):
            h0 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(X,32,5,1)))
            h1 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h0,64,4,2)))
            h2 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h1,128,4,1)))
            h3 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h2,256,4,2)))
            h4 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h3,256,4,1)))
            h5 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h4,256,1,1)))
            h6 = tf.layers.conv2d(h5,256,1,1)

            h7 = tf.concat([h6,noise],axis=-1)

            h8 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h7,256,3,1)))
            h9 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h8,128,4,2)))
            h10 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h9,64,4,1)))
            h11 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h10,32,4,2)))
            h12 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h11,32,5,1)))
            h13 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h12,32,1,1)))
            h14 = tf.layers.conv2d_transpose(h13,1,1,1,activation=tf.nn.sigmoid)

        return h14

    def discriminator(X,y,reuse=False):
        with tf.variable_scope(""Discriminator"",reuse=reuse,initializer=initializer):
            d0 = dropout(lrelu(tf.layers.conv2d(X,32,5,1)))
            d1 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(d0,64,4,2))))
            d2 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(d1,128,4,1))))
            d3 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(d2,256,4,2))))
            d4 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(d3,512,4,1))))

            h0 = dropout(lrelu(tf.layers.conv2d(y,32,5,1)))
            h1 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h0,64,4,2))))
            h2 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h1,128,4,1))))
            h3 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h2,256,4,2))))
            h4 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h3,512,3,1))))

            final = tf.concat([d4,h4],axis=-1)

            f0 = dropout(lrelu(tf.layers.conv2d(final,1024,1,1)))
            f1 = dropout(lrelu(tf.layers.conv2d(f0,1024,1,1)))
            f2 = dropout(tf.layers.conv2d(f1,1,1,1))

        return tf.reshape(f2,[-1,1])

    def sig_loss(a,b):
        return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=a,labels=b),axis=1)

    def get_vars(name):
        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=name)

    def l2_loss(a,b):
        return tf.reduce_sum(tf.square(a-b),axis=1)

    X_mnist = tf.placeholder(tf.float32,[None,28,28,1])
    X_svhn = tf.placeholder(tf.float32,[None,32,32,3])

    epsilon = tf.placeholder(tf.float32,[None,1])

    z_mnist = tf.placeholder(tf.float32,[None,1,1,32])
    z_svhn = tf.placeholder(tf.float32,[None,1,1,32])

    Xtr_svhn = mnist_svhn(X_mnist,z_mnist)
    Xtr_mnist = svhn_mnist(X_svhn,z_svhn)

    sv = discriminator(X_svhn,Xtr_mnist)
    mn = discriminator(Xtr_svhn,X_mnist,True)

    G_opt = tf.train.AdamOptimizer(learning_rate=1e-4,beta1=0.5)
    D_opt = tf.train.AdamOptimizer(learning_rate=1e-4,beta1=0.5)

    x_hat = epsilon * X_svhn + (1-epsilon) * Xtr_svhn
    grad_1 = tf.gradients(discriminator(x_hat,svhn_mnist(x_hat,z_svhn,True),True),x_hat)[0]
    x_hat = epsilon * X_mnist + (1-epsilon) * Xtr_mnist
    grad_2 = tf.gradients(discriminator(mnist_svhn(x_hat,z_mnist,True),x_hat,True),x_hat)[0]

    slopes_1 = tf.sqrt(tf.reduce_sum(tf.square(grad_1),reduction_indices=-1))
    slopes_2 = tf.sqrt(tf.reduce_sum(tf.square(grad_2),reduction_indices=-1))
    grad_penalty = tf.reduce_mean((slopes_1 - 1.)**2) * 5
    grad_penalty += tf.reduce_mean((slopes_2 - 1.)**2) * 5

    disc_loss = tf.reduce_mean(sig_loss(sv,tf.ones_like(sv))) + \
                tf.reduce_mean(sig_loss(mn,tf.zeros_like(mn))) + grad_penalty

    gen_loss = tf.reduce_mean(sig_loss(mn,tf.ones_like(mn))) + \
                tf.reduce_mean(sig_loss(sv,tf.zeros_like(sv)))

    g_grad = G_opt.compute_gradients(gen_loss,var_list=get_vars(""Encoder""),colocate_gradient_with_ops=True)
    -------> d_grad = D_opt.compute_gradients(disc_loss,var_list=get_vars(""Discriminator""),colocate_gradient_with_ops=True)

    d_step = D_opt.apply_gradients(d_grad)
    g_step = G_opt.apply_gradients(g_grad)"
19270,Different dynamic rnn output depending on batch_size,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, own code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 6.7 and Windows 10
- **TensorFlow installed from (source or binary)**:  by pip install
- **Bazel version**:  N/A
- **TensorFlow installed from (source or binary)**:  by pip install
- **TensorFlow version (use command below)**: ('v1.4.0-rc0-21-g1e25994', '1.4.0-rc1')
 on CentOS ; 1.8.0 on Windows
- **Python version**: 2.7 on CentOS ; 3 on Windows
- **CUDA/cuDNN version**: CUDA 8.0/cuDNN 6 on CentOS; CUDA 9.0/cuDNN 7.1 on Windows
- **GPU model and memory**: NVidia Tesla K80 on CentOS ; GeForce GTX 1050 Ti 4GB on Windows
- **Exact command to reproduce**: Example below, tested in jupyter-notebook from anaconda 5.1 (i think so)
ipykernel==4.8.2
ipython==5.6.0
jupyter==1.0.0
jupyter-client==5.2.3
jupyter-console==5.2.0
jupyter-core==4.4.0
jupyter-tensorboard==0.1.6


### Describe the problem
**Context**:  I'm trying to create a syntactic parser with NN classifier which defines state of next parsing step. Batch size is dynamic and equals to parsing steps count during training, but on prediction i feed into NN only 1 state per parsing step, so batch size=1. When i found that i lost some prediction accuracy i started to dig and that's what i found.
**Problem**: hidden states of dynamic LSTM  are a bit different when batch size is 1 and >1. The difference between them is small, about 0.0000001, but since i have several LSTM in NN, it affects the output of network. And interesting that if batch size is 2, 3 or more, the outputs are equal, but they are not if batch size is 1. And last one, it's ok with input with small dimensions, like [batch_size, 4, 4], but not when i have [batch size, 4, >10]
I wrote some test example to represent this. It worked for me on 2 systems.

### Source code / logs
```
import numpy as np
import tensorflow as tf

config = tf.ConfigProto(allow_soft_placement=True)
config.gpu_options.allow_growth = True
sess = tf.InteractiveSession(config=config)

model_config = {}
model_config[""n_hidden""]=10
model_config[""lstmStacks""]=2
wordsCount = 4
```
```
gatherOut = tf.placeholder(shape=(None,None,model_config[""n_hidden""]), dtype=tf.float32)
inp_l1_length = tf.placeholder(shape=(None, ), dtype=tf.int32,name=""inp_l1_length"")

cell = tf.contrib.rnn.LSTMCell(model_config[""n_hidden""])
lstm_layer, lstm_states = tf.nn.dynamic_rnn(cell, gatherOut, sequence_length=inp_l1_length, dtype=tf.float32)    
listOut = lstm_states[1]

sess.run(tf.global_variables_initializer())
```
```
inpArr = np.random.uniform(high=1,low=0,size=(1,wordsCount, model_config[""n_hidden""]))

res1 = sess.run(listOut, feed_dict= {
    gatherOut: inpArr,
    inp_l1_length: [1]
})

res2 = sess.run(listOut, feed_dict= {
    gatherOut: np.tile(inpArr,(2,1,1)),
    inp_l1_length: [1,1]
})

res3 = sess.run(listOut, feed_dict= {
    gatherOut: np.tile(inpArr,(3,1,1)),
    inp_l1_length: [1,1,1]
})
```
```
(res1[0]==res2[0]).all()  # False
(res2[:2]==res3[:2]).all()  # True
(res2[-1]==res3[-1]).all()  # True
```"
19269,tf.contrib.data.parallel_interleave with tf.data.Dataset.from_generator,"I think this is a feature request (or maybe I miss something). When data is coming from multiple remote sources multiple tf.data.Dataset.from_generator should run in parallel. Therefore tf.contrib.data.parallel_interleave is already available, see this small code example.

```
def generator(n):
  # returns n-th generator function

def dataset(n):
  return tf.data.Dataset.from_generator(generator(n))

ds = tf.data.Dataset.range(N).apply(tf.contrib.data.parallel_interleave(dataset, cycle_lenght=N))

# where N is the number of generators you use

```


### Describe the problem
This problem with the code above is that generator cannot look like this:
```
def generator(n):
  return consumers[n] # A python list with n items
```

Because then the error 'TypeError: list indices must be integers or slices, not Tensor' will occur. However it would be really nice to have support for multiple from_generator pipelines in parallel.

@mrry Do you have an idea whether this is indeed a feature request or that it is possible with the current codebase?
"
19268,Error when freeze the graph,"Information:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary 1.5
- **Python version**: 3.5
- **CUDA/cuDNN version**: No

Hello,

I use a model, in my code I save the model with that command:
`                writer = tf.summary.FileWriter(""/tmp/model4/MTCNN/"", self.sess.graph)
                saver = tf.train.Saver() #saver load pretrain model
                save_path = tf.train.Saver().save(self.sess, ""/tmp/model4/MTCNN/model.ckpt"")`

It generates me these files:
```
/tmp/model4/MTCNN/checkpoint
/tmp/model4/MTCNN/events.out.tfevents.1526300275.xavier-Inspiron
/tmp/model4/MTCNN/model.ckpt.data-00000-of-00001
/tmp/model4/MTCNN/model.ckpt.index
/tmp/model4/MTCNN/model.ckpt.meta
```

I reload this saved model and graph with that code:

```
               self.sess = tf.Session()
                model_path = ""/tmp/model4/MTCNN/model.ckpt""
                model_path_p = ""/tmp/model4/MTCNN/""
                self.sess.run(tf.global_variables_initializer())
                new_saver = tf.train.import_meta_graph(model_path + '.meta')
                new_saver.restore(self.sess,tf.train.latest_checkpoint(model_path_p))
```

And this works properly. Now I want to freeze my model in pb file with that command:
`python3 ./tensorflow/python/tools/freeze_graph.py --input_checkpoint=/tmp/model4/MTCNN/model.ckpt.data-00000-of-00001 --input_meta_graph_def=/tmp/model4/MTCNN/model.ckpt.meta  --output_graph=frozen_graph.pb --output_node_names='pnet/conv4-2/BiasAdd'`

And I get that errors:

`2018-05-14 14:35:20.597234: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-05-14 14:35:20.598060: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /tmp/model4/FaceFeature/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
Traceback (most recent call last):
  File ""./tensorflow/python/tools/freeze_graph.py"", line 360, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""./tensorflow/python/tools/freeze_graph.py"", line 254, in main
    FLAGS.saved_model_tags, FLAGS.checkpoint_version)
  File ""./tensorflow/python/tools/freeze_graph.py"", line 244, in freeze_graph
    saved_model_tags.split("",""), checkpoint_version=checkpoint_version)
  File ""./tensorflow/python/tools/freeze_graph.py"", line 119, in freeze_graph_with_def_protos
    reader = pywrap_tensorflow.NewCheckpointReader(input_checkpoint)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader
    return CheckpointReader(compat.as_bytes(filepattern), status)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /tmp/model4/FaceFeature/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
`
"
19267,Op type not registered 'NoOp' from toco,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: linux ubuntu 17.10
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.6
- **Python version**: python 3.6.3
- **Bazel version (if compiling from source)**: bazel-0.11.1-installer-linux-x86_64.sh
- **GCC/Compiler version (if compiling from source)**: gcc/g++ 6.4.0
- **CUDA/cuDNN version**: CUDA:9.1 cuDNN 7.1
- **GPU model and memory**: gtx 1060 6GB
- **Exact command to reproduce**:
bazel run --config=opt --copt=-msse4.1 --copt=-msse4.2   //tensorflow/contrib/lite/toco:toco -- --input_file=/home/andy/data/models/Best_model/MFN.pb   --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --output_file=/home/andy/data/models/Best_model/MFN.tflite   --inference_type=FLOAT   --inference_input_type=FLOAT   --input_arrays=input   --output_arrays=embeddings   --input_shapes=1,112,112,3

### Describe the problem
I had trained and frozen a MobileNet_v2 pretrain model which is under slim architecture to tensorflow graphdef protocol, when I use  toco convert to tflite(command as flow item 1),  I got a error as flow item 2. hold for someone kindly help me solve it, thanks.

1. toco cmd
`bazel run --config=opt --copt=-msse4.1 --copt=-msse4.2   //tensorflow/contrib/lite/toco:toco -- --input_file=/home/andy/data/models/Best_model/MFN.pb   --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --output_file=/home/andy/data/models/Best_model/MFN.tflite   --inference_type=FLOAT   --inference_input_type=FLOAT   --input_arrays=input   --output_arrays=embeddings   --input_shapes=1,112,112,3`

2. error logs
`2018-05-14 19:35:27.695827: F tensorflow/core/graph/graph.cc:283] Non-OK-status: status status: Not found: Op type not registered 'NoOp' in binary running on andy. Make sure the Op and Kernel are registered in the binary running in this process.`
"
19266,tensorflow-gpu-1.8: failed call to cuInit: CUDA_ERROR_UNKNOWN,"### System information
- **No custom code, Validate you installation test from tensorflow website**: Validate your installation test code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.4
- **TensorFlow installed from (source or binary)**: with `pip install --upgrade tensorflow-gpu`
- **TensorFlow version (use command below)**: 1.8 gpu
- **Python version**: 2.7
- **CUDA/cuDNN version**: 9.0 / 7
- **GPU model and memory**: Nvidia GeForce GTX 1080 Ti 11GB
- **Exact command to reproduce**:
`# Python
import tensorflow as tf
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
print(sess.run(hello))
`
### Describe the problem
I installed tensorflow and then tried to run the ""validate your installation"" program from the website.
`# Python
import tensorflow as tf
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
print(sess.run(hello))
`
After `sess = tf.Session()` im recieving the following error:
`2018-05-14 13:19:32.098811: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_UNKNOWN
2018-05-14 13:19:32.098874: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: censored
2018-05-14 13:19:32.098889: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: censored
2018-05-14 13:19:32.098935: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 390.48.0
2018-05-14 13:19:32.098992: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 390.48.0
2018-05-14 13:19:32.099006: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 390.48.0`

This Issue https://github.com/tensorflow/tensorflow/issues/16860 did not resolve my problem.

Hopefully you can help me with this issue.
"
19265,"`tf.nn.embedding_lookup`: unexpected behaviour with sharded `params` in ""div"" mode","The following code shows what appears to be a [misbehaviour](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup) when trying to lookup sharded embeddings in ""div"" mode:

```python
def test_shard_embedding_lookup(index):
   embeddings_1 = tf.constant([[0,0,0],[1,1,1],[2,2,2],[3,3,3]])
   embeddings_2 = tf.constant([[4,4,4],[5,5,5]])
   indices = tf.constant([index])
    
   with tf.Session() as sess:
      ret = sess.run(\
                  tf.nn.embedding_lookup(\
                     [embeddings_1, embeddings_2],\
                     indices,\
                     partition_strategy='div'\
                  )\
                )

   return ret

test_shard_embedding_lookup(2) # [2,2,2]
test_shard_embedding_lookup(3) # [4,4,4] instead of [3,3,3]
```

Tested with

- Python 3.6.5
- TF 1.8

"
19263,Could you please share the frozen pb which is been used in tflite example?,"Hi,
In the tflite example, uses a ssd mobilenet model, I can only find the tflite model: mobilenet_ssd.tflite.
Could you please share me the frozen tf model ?
I tried http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz , but failed to convert it to tflite, some ops are not supported."
19262,PosixFileSystem::CreateDir should create directory respecting umask,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See below

### The Problem
When using the EventFileWriter to write event files to a non-existent directory, the directory will be created but will not have group or other permissions, even if the parent directory has the sticky bit set and umask is set to 0777.

The problem originates from [posix_file_system.cc:244](https://github.com/tensorflow/tensorflow/blob/e7f158858479400f17a1b6351e9827e3aa83e7ff/tensorflow/core/platform/posix/posix_file_system.cc#L244). Here the mask is set to 0755, which may not be desired. Instead setting this to 0777 will respect the umask, thus work as expected.

### Source code / logs
The following code will create the directory ""nonexistent-directory"" with permissions 0755 even when umask is set to 0775 or 0755:
```
from tensorflow.python.training.summary_io import SummaryWriterCache
SummaryWriterCache.get(""nonexistent-directory"")
```

### Workaround
As a workaround, the directory can be created before the `SummaryWriterCache` is used:
```
from tensorflow.python.training.summary_io import SummaryWriterCache
import os
os.makedirs(""nonexistent-directory"", exist_ok=True)
SummaryWriterCache.get(""nonexistent-directory"")
```
In this case the umask is respected correctly.

### Fix
Replacing [posix_file_system.cc:244](https://github.com/tensorflow/tensorflow/blob/e7f158858479400f17a1b6351e9827e3aa83e7ff/tensorflow/core/platform/posix/posix_file_system.cc#L244) by `if (mkdir(TranslateName(name).c_str(), 0777) != 0) {` should fix the problem."
19261,tf windows dll could you help me,"I want to use tensorflow on the windows platform. Why is it so hard?

According to the official website's instructions, cmake came out of the vc project, but compiled a bunch of errors

i use tf r1.2 ！ please help me"
19260,[Lite] tf.strided_slice sometimes computes wrong indices,"### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: Ubuntu 16.04.2 LTS
- **TensorFlow installed from**: source
- **TensorFlow version**: v1.8.0-1520-g1f03f82 1.8.0
- **Python version**: 3.5.2
- **Bazel version**: 0.13.0

### Problem
I tried using the [TOCO tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/toco) on a graph that contains a [`strided_slice`](https://www.tensorflow.org/api_docs/python/tf/strided_slice) op.
The code determining the fixed size of this op, fails at an assertion and throws an error (see below).

### Logs
2018-05-14 00:42:56.816500: F [tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1305](https://github.com/tensorflow/tensorflow/blob/1f03f829285ca0fbd47a99350e9f5d99aa10e9b9/tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc#L1305)] Check failed: dim_size > 0 (-1 vs. 0)Output size for an axis must be greater than 0. Axis 0 computes to size -1 for StridedSlice op with output ""stft/frame/strided_slice"".

### Minimum Reproducible Example
[Source files](https://github.com/tensorflow/tensorflow/files/1999035/mre.zip)
(set `TF_ROOT` in `freeze` and `toco`)
```
./mre.py
./freeze
./toco
```
This produces a directory ""model"" with the graph, weights and frozen graph.
The offending error is thrown by last step."
19253,Entropy for multinomial distribution,"Have I written custom code: No
OS Platform and Distribution: N/A
TensorFlow installed from: N/A
TensorFlow version: 1.8.0
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A

Currently there does not appear to be an implementation of entropy for the Multinomial distribution. Is there a reason for this?

[Multinomial](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/ops/distributions/multinomial.py) inherits from [Distribution](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/ops/distributions/distribution.py). However, Multinomial does not have override the the dummy entropy function in the Distribution class
"
19252,Prebuilt binary APK TFLCameraDemo does not Install (Error analyzing Package; Android 7 and 5.1.1),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
19250,"In eager model, tfe.Checkpoint does not restore Variables that are not instance properties of tf.keras.Model instance","## System information

+ Have I written custom code : Yes
+ OS Platform and Distribution: Win7 X64
+ TensorFlow installed from (source or binary): binary
+ TensorFlow version (use command below): 1.8.0.dev20180329
+ Python version: 3.5
+ Bazel version (if compiling from source): N/A
+ GCC/Compiler version (if compiling from source):N/A
+ CUDA/cuDNN version: N/A
+ GPU model and memory: N/A
+ Exact command to reproduce: N/A 


## Problem

I wrote a Text CNN in eager model:
```python
class TextCnn(tf.keras.Model):
    def __init__(self):
        ......
        self.conv_funcs = [tf.layers.Conv2D(filter_num,
                                            [filter_size, embedding_size],
                                            activation=tf.nn.relu,
                                            name='conv_{}'.format(filter_size))
                           for filter_size, filter_num in filter_size_num_list]

```
I set the list of Conv2D layers as an instance property(self.conv_funcs), and when I restore the model by tfe.Checkpoint, the weights of the Conv2D layers are not restored.

However, I add the following code and the tfe.Checkpoint successfully restore the weights of the Conv2D layers:
```python
        self.conv_2 = self.conv_funcs[0]
        self.conv_3 = self.conv_funcs[1] 
```

Is it a bug that tfe.Checkpoint only restore the weights of instance properties of tf.keras.Model?
"
19249,build tensorflow on tx2 with cuda8.0 and cudnn6.0,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Tx2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.6, r1.7, r1.8
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:.13.0- (@non-git)
- **GCC/Compiler version (if compiling from source)**:5.4.0 
- **CUDA/cuDNN version**: 8.0 / 6.0
- **GPU model and memory**:  Denver2 8GB
- **Exact command to reproduce**: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherOpKernel< ::Eigen::half, int, (bool)0> "") is not allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherOpKernel< ::Eigen::half, long long, (bool)1> "") is not allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherOpKernel< ::Eigen::half, long long, (bool)0> "") is not allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherOpKernel<float, int, (bool)1> "") is not allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherOpKernel<float, int, (bool)0> "") is not allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherOpKernel<float, long long, (bool)1> "") is not allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherOpKernel<float, long long, (bool)0> "") is not allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherOpKernel<double, int, (bool)1> "") is not allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherOpKernel<double, int, (bool)0> "") is not allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherOpKernel<double, long long, (bool)1> "") is not allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherOpKernel<double, long long, (bool)0> "") is not allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherOpKernel< ::std::complex<float> , int, (bool)1> "") is not allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherOpKernel< ::std::complex<float> , int, (bool)0> "") is not allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherOpKernel< ::std::complex<float> , long long, (bool)1> "") is not allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherOpKernel< ::std::complex<float> , long long, (bool)0> "") is not allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherOpKernel< ::std::complex<double> , int, (bool)1> "") is not allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherOpKernel< ::std::complex<double> , int, (bool)0> "") is not allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherOpKernel< ::std::complex<double> , long long, (bool)1> "") is not allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_expect"") from a __global__ function(""tensorflow::GatherOpKernel< ::std::complex<double> , long long, (bool)0> "") is not allowed

20 errors detected in the compilation of ""/tmp/tmpxft_0000294e_00000000-9_gather_functor_gpu.cu.compute_52.cpp1.ii"".
ERROR: /home/nvidia/tensorflow/tensorflow/core/kernels/BUILD:1201:1: output 'tensorflow/core/kernels/_objs/gather_functor_gpu/tensorflow/core/kernels/gather_functor_gpu.cu.pic.o' was not created
ERROR: /home/nvidia/tensorflow/tensorflow/core/kernels/BUILD:1201:1: not all outputs were created or valid
```

and this is my configure settings

```
You have bazel 0.13.0- (@non-git) installed.
Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3.5


Found possible Python library paths:
  /usr/local/lib/python3.5/dist-packages
  /usr/lib/python3/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]

Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: 
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: 
Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: 
Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: 
Amazon S3 File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: 
Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: 
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: 
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: 
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: 8


Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 6


Please specify the location where cuDNN 6 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:/usr/libaarch64-linux-gnu


Do you wish to build TensorFlow with TensorRT support? [y/N]: 
No TensorRT support will be enabled for TensorFlow.

Please specify the NCCL version you want to use. [Leave empty to default to NCCL 1.3]: 


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]


Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 


Do you wish to build TensorFlow with MPI support? [y/N]: 
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
```
"
19247,Could TensorFlow support tf.nn.separable_conv1d in future release?,"Dear TensorFlower:

Currently TensorFlow only has tf.nn.separable_conv2d support and it is convenient to use.

Could it is possible in future release tf.nn.separable_conv1d be added? Since I am intensively using TensorFlow for processing bio-signals, so it is more attractive for TensorFlow having such support at this regard.

Best regards

The following fields are filled due to request, since it is not a issue but more like a feature, not sure the information below is helpful or not.

Have I written custom code: No
OS Platform and Distribution: RedHat 7.3
TensorFlow installed from: Customer build from source code
TensorFlow version: 1.8
Bazel version: 0.12
CUDA/cuDNN version:9.1/7.0.5
GPU model and memory: P5000/16G
Exact command to reproduce: N/A

"
19246,TensorFlow toco - convert from *.pb to *.tflite error,"When try to convert *.pb model to *.tflite (for running on android) using toco converter of TensorFlow, I got the following error:

2018-05-11 20:38:10.841641: I tensorflow/cc/saved_model/loader.cc:242] Loading SavedModel with tags: { serve }; from: /home/user_name/Downloads/model_directory
2018-05-11 20:38:10.847390: I tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: fail. Took 6042 microseconds.
2018-05-11 20:38:10.848098: F tensorflow/contrib/lite/toco/toco_saved_model.cc:50] **Non-OK-status:** tensorflow::LoadSavedModel(tensorflow::SessionOptions(), tensorflow::RunOptions(), model_path, tags, bundle) **status: Not found:** Could not find meta graph def matching supplied tags: { serve }. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`Failed to load exported model from /home/user_name/Downloads/model_directory. **Ensure the model contains the required tags 'serve'.**

**Any idea how to solve it?**

Machine details:
OS Platform and Distribution - ubuntu x64, 
python version is 3.5.2, 
TensorFlow installed from - pip, 
TensorFlow version - cpu version 1.8.0, 
Bazel version - 0.13.0, 
CUDA/cuDNN version - no cuda, 
GPU model and memory - no gpu, 
Exact command to reproduce - no need
"
19245,An error occurred while starting the kernel in CNN runing,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win 8.1
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:  1.7.0 
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**: anaconda
- **CUDA/cuDNN version**: 9.0.0  
- **GPU model and memory**:GTX 740m
- **Exact command to reproduce**:

### Describe the problem
I can run NN code but i can't run CNN code.
when i run a CNN code, the kernel suddenly dies. I see this error:
|An error occurred while starting the kernel|

```python
### Source code / logs
from keras.datasets import mnist
def plot_history(net_history):
history = net_history.history
import matplotlib.pyplot as plt
losses = history['loss']
val_losses = history['val_loss']
accuracies = history['acc']
val_accuracies = history['val_acc']
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.plot(losses)
plt.plot(val_losses)
plt.legend(['loss', 'val_loss'])
plt.figure()
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.plot(accuracies)
plt.plot(val_accuracies)
plt.legend(['acc', 'val_acc'])
#========================Load data
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

#================Data attributes
print(""train_images dimentions: "", train_images.ndim)
print(""train_images shape: "", train_images.shape)
print(""train_images type: "", train_images.dtype)

X_train = train_images.reshape(60000, 784)
X_test = test_images.reshape(10000, 784)

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

X_train /= 255
X_test /= 255

from keras.utils import np_utils
Y_train = np_utils.to_categorical(train_labels)
Y_test = np_utils.to_categorical(test_labels)

#==============Creating our model====================================
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import SGD
from keras.losses import categorical_crossentropy

myModel = Sequential()
myModel.add(Dense(500, activation='relu', input_shape=(784,)))
myModel.add(Dropout(20))
myModel.add(Dense(100, activation='relu'))
myModel.add(Dropout(20))
myModel.add(Dense(10, activation='softmax'))

myModel.summary()
myModel.compile(optimizer=SGD(lr=0.001), loss=categorical_crossentropy, metrics=['accuracy'])

#=====Train our model=============================================
network_history = myModel.fit(X_train, Y_train, batch_size=128, epochs=20, validation_split=0.2)
plot_history(network_history)

#=============Evaluation========================================
test_loss, test_acc = myModel.evaluate(X_test, Y_test)
test_labels_p = myModel.predict(X_test)
import numpy as np
test_labels_p = np.argmax(test_labels_p, axis=1)

#=========== Change layers config================
myModel.layers[0].name = 'Layer_0'
myModel.layers[0].trainable = False
myModel.layers[0].get_config()
```"
19244,tf.contrib.data.prefetch_to_device not compatible with tf.data.Iterator.from_structure,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
tensorflow-gpu binary
- **Bazel Version**:
N/A
- **TensorFlow version (use command below)**:
v1.8.0-0-g93bc2e2072 1.8.0
- **Python version**: 
3.6.3
- **CUDA/cuDNN version**:
CUDA 9.0 cuDNN 7.0.3
- **GPU model and memory**:
GTX 1070 8 GB VRAM
- **Exact command to reproduce**:

```python
import tensorflow as tf

class MyData(object):
    def __call__(self):
         return range(100)

expected_shapes = []
expected_types = tf.int32
iterator = tf.data.Iterator.from_structure(output_types=expected_types, output_shapes=expected_shapes)
dataset = tf.data.Dataset.from_generator(MyData(), output_types=expected_types, output_shapes=expected_shapes)

prefetch_op = tf.contrib.data.prefetch_to_device(device=""/gpu:0"")
dataset = dataset.apply(prefetch_op)
initializer = iterator.make_initializer(dataset)
```

### Describe the problem

This raises `NotImplementedError: prefetch_to_device() must be the last transformation in a dataset pipeline`. 

It is not possible to apply this to the dataset after the initializer has been created, since a new dataset is returned, instead of it being modified in place.

If one reads through [this testcase](https://github.com/tensorflow/tensorflow/commit/4681562607bf4001ecd61492f1e7567be9212c6f), it is clear that it works when creating the iterator from the dataset.

It is  **not** clear from [the documentation of `make_initializer`](https://www.tensorflow.org/api_docs/python/tf/data/Iterator#make_initializer) that this function is a transformation of the dataset and thus counts as an additional step after prefetching. 
I am not sure if this is a bug/was overlooked, or is known to be not implemented.

**Proposed short term solution**:
1. Mention in the [documentation of `prefetch_to_device`](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/data/prefetch_to_device), that it is not supported in combination with `make_initializer`.
2. Mention in the documentation of `make_initializer` that this operation modifies the dataset
(although I don't think this is the correct choice of words, the issue is with a call to `dataset._as_variant_tensor` in [`make_initializer` line 308](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/iterator_ops.py)).

**Proposed longterm solution**:
1. _This is already a TODO in line 289 of [prefetching_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/data/python/ops/prefetching_ops.py)_:
Implement `_as_variant_tensor` for `_PrefetchToDeviceDataset`.

### Reason why this is needed:

Creating the data pipeline using `from_structure` and  `make_initializer` allows to dynamically switch the input source to the network, e.g. between training and testing set after an epoch without having to reinitialize the graph or fall back to using feed dicts.

### Source code / logs

Exact stack trace of the error:
```bash
Traceback (most recent call last):
  File ""test.py"", line 14, in <module>
    initializer = iterator.make_initializer(dataset)
  File ""/home/veith/.pyenv/versions/3.6.3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 308, in make_initializer
    dataset._as_variant_tensor(), self._iterator_resource, name=name)  # pylint: disable=protected-access
  File ""/home/veith/.pyenv/versions/3.6.3/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/prefetching_ops.py"", line 291, in _as_variant_tensor
    raise NotImplementedError(""`prefetch_to_device()` must be the last ""
NotImplementedError: `prefetch_to_device()` must be the last transformation in a dataset pipeline.
```
"
19242,Can TFLite treat input/output tensors as flat buffers just like TFMobile used to?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**:  3.5.4
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**:  feature?

### Describe the problem
TF Mobile allows treating input/output as flat arrays (e.g. float[]). Why does the TF Lite design deviate? Moreover, TF Lite treats inputs differently from outputs which feels odd, especially when chaining models.

Is there a way to get the old behavior back? Essentially, treating input/output tensor as flat buffers (appropriately sized) enables abstracting over the exact network input/output shapes in app code, making it more reusable. One can think of the network + shape as an existential package, which is quite convenient.

### Source code / logs
[TF Lite Android example](https://github.com/googlecodelabs/tensorflow-for-poets-2/blob/end_of_first_codelab/android/tflite/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java#L103) requires instantiating `float[][]` which is hard-coding network output shape into the app. 
"
19241,[Eager] Fix for determining input / output shape of the model prior to Model.fit(),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Home Edition
- **TensorFlow installed from (source or binary)**: Windows binary
- **TensorFlow version (use command below)**: Github version 'v1.8.0-0-g93bc2e2072' 1.8.0TF (GPU) 
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 9.0 / cuDNN 7.0.5
- **GPU model and memory**: NVIDIA GTX 980M
- **Exact command to reproduce**: Provided below as a standalone script

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
The issue is that in Eager mode, the two attributes of the Model subclass, `inputs` and `outputs` is undetermined until the first call to `Model.fit(...)`.

When attempting to determine this prior to fitting the model, in the script at location **tensorflow/python/keras/_impl/keras/engine/training.py**, the entire input dataset `X` (passed to .fit(...)) is provided as the input `x` in Line 684 `if not self.inputs: self._set_inputs(x)` inside `_standardize_user_data`.

Due to this, in eager execution mode, this call is deferred to `_eager_set_inputs(inputs)`. Here, `inputs` is the entire dataset numpy matrix / tensor, and a `Model.call(inputs)` is performed at line 909. 

Since the entire dataset is unable to fit in GPU memory for smaller GPU devices, it causes an OOM error.

However, to determine the input / output shape/s of a model, a single sample tensor is sufficient.

The below fix shows that the solution is adequate, and can be implemented by simply extracting a single sample of the entire dataset to determine the input / output shape/s of a model during eager execution.

**Note : **
In order to provide indicators to identify where the error occurs, I modified the above mentioned script to print 2 logs to the console, to describe the shape of the ""inputs"" parameter inside `_eager_set_inputs(inputs)`. The lines `Inside training._eager_set_inputs ...` and `***** Providing entire dataset into call *****` are the above logs.

### Source code / logs

```python
import os
import numpy as np

import tensorflow as tf
from tensorflow.python.keras.datasets import mnist
from tensorflow.contrib.eager.python import tfe

# enable eager mode
tf.enable_eager_execution()
tf.set_random_seed(0)
np.random.seed(0)

# constants
batch_size = 128
epochs = 10
num_classes = 10

# dataset loading
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train.reshape((-1, 28, 28, 1))
x_test = x_test.reshape((-1, 28, 28, 1))

# one hot encode the labels. convert back to numpy as we cannot use a combination of numpy
# and tensors as input to keras
y_train_ohe = tf.one_hot(y_train, depth=num_classes).numpy()
y_test_ohe = tf.one_hot(y_test, depth=num_classes).numpy()

print('x train', x_train.shape)
print('y train', y_train_ohe.shape)
print('x test', x_test.shape)
print('y test', y_test_ohe.shape)

class CNN(tf.keras.Model):

    def __init__(self, num_classes):
        super(CNN, self).__init__()

        self.cnn1 = tf.keras.layers.Conv2D(16, (5, 5), padding='same', strides=(2, 2))
        self.bn1 = tf.keras.layers.BatchNormalization()
        self.cnn2 = tf.keras.layers.Conv2D(32, (5, 5), padding='same', strides=(2, 2))
        self.bn2 = tf.keras.layers.BatchNormalization()
        self.pool = tf.keras.layers.GlobalAveragePooling2D()
        self.classifier = tf.keras.layers.Dense(num_classes)

    def call(self, inputs, training=None, mask=None):
        # Used to print out the input shape of the entire dataset prior to training loop
        print(inputs.shape)

        x = self.cnn1(inputs)
        x = self.bn1(x)
        x = tf.nn.relu(x)  # layer 1
        x = tf.nn.relu(self.bn2(self.cnn2(x))) # layer 2
        x = self.pool(x)
        output = self.classifier(x)

        # softmax op does not exist on the gpu, so always use cpu
        with tf.device('/cpu:0'):
            output = tf.nn.softmax(output)

        return output


device = '/cpu:0' if tfe.num_gpus() == 0 else '/gpu:0'

with tf.device(device):
    # build model and optimizer
    model = CNN(num_classes)
    model.compile(optimizer=tf.train.AdamOptimizer(0.001), loss='categorical_crossentropy',
                  metrics=['accuracy'])
    
    # suggested fix ; can be incorporated inside `_eager_set_inputs` or `_set_input`
    # Fix = Use exactly one sample from the provided input dataset to determine 
    # input/output shape/s for the model

    # dummy_x = np.zeros((1, 28, 28, 1))
    # model._set_inputs(dummy_x)

    # train
    model.fit(x_train, y_train_ohe, batch_size=batch_size, epochs=epochs,
              validation_data=(x_test, y_test_ohe), verbose=2)

    # evaluate on test set
    scores = model.evaluate(x_test, y_test_ohe, batch_size, verbose=2)
    print(""Final test loss and accuracy :"", scores)

```

Truncated log without the fix : (Only tensor allocation summary dump after OOM is truncated)
```python
2018-05-12 00:22:44.208127: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-05-12 00:22:45.385242: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX 980M major: 5 minor: 2 memoryClockRate(GHz): 1.1265
pciBusID: 0000:01:00.0
totalMemory: 4.00GiB freeMemory: 3.32GiB
2018-05-12 00:22:45.385822: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1435] Adding visible gpu devices: 0
2018-05-12 00:22:45.838116: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-05-12 00:22:45.838626: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:929]      0 
2018-05-12 00:22:45.838912: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:942] 0:   N 
2018-05-12 00:22:45.839762: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3050 MB memory) -> physical GPU (device: 0, name: GeForce GTX 980M, pci bus id: 0000:01:00.0, compute capability: 5.2)
x train (60000, 28, 28, 1)
y train (60000, 10)
x test (10000, 28, 28, 1)
y test (10000, 10)

Inside training._eager_set_inputs ip shape/s [(60000, 28, 28, 1)]  <= Here the input is the entire dataset
***** Providing entire dataset into call *****
(60000, 28, 28, 1)

2018-05-12 00:22:58.125845: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 717.77MiB.  Current allocation summary follows.

<<< Truncated >>>

2018-05-12 00:22:58.229153: W T:\src\github\tensorflow\tensorflow\core\framework\op_kernel.cc:1318] OP_REQUIRES failed at fused_batch_norm_op.cc:263 : Resource exhausted: OOM when allocating tensor with shape[60000,16,14,14] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File ""D:/Users/Yue/PycharmProjects/eager-tutorials/tutorials/04_cnn.py"", line 83, in <module>
    validation_data=(x_test, y_test_ohe), verbose=2)
  File ""D:\Users\Yue\Anaconda3\lib\site-packages\tensorflow\python\keras\_impl\keras\engine\training.py"", line 1147, in fit
    batch_size=batch_size)
  File ""D:\Users\Yue\Anaconda3\lib\site-packages\tensorflow\python\keras\_impl\keras\engine\training.py"", line 685, in _standardize_user_data
    self._set_inputs(x)
  File ""D:\Users\Yue\Anaconda3\lib\site-packages\tensorflow\python\keras\_impl\keras\engine\training.py"", line 871, in _set_inputs
    self._eager_set_inputs(inputs)
  File ""D:\Users\Yue\Anaconda3\lib\site-packages\tensorflow\python\keras\_impl\keras\engine\training.py"", line 914, in _eager_set_inputs
    ops.convert_to_tensor(inputs, dtype=K.floatx()))
  File ""D:/Users/Yue/PycharmProjects/eager-tutorials/tutorials/04_cnn.py"", line 55, in call
    x = self.bn1(x)
  File ""D:\Users\Yue\Anaconda3\lib\site-packages\tensorflow\python\keras\_impl\keras\engine\base_layer.py"", line 314, in __call__
    output = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""D:\Users\Yue\Anaconda3\lib\site-packages\tensorflow\python\layers\base.py"", line 717, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""D:\Users\Yue\Anaconda3\lib\site-packages\tensorflow\python\keras\_impl\keras\layers\normalization.py"", line 113, in call
    output = super(BatchNormalization, self).call(inputs, training=training)
  File ""D:\Users\Yue\Anaconda3\lib\site-packages\tensorflow\python\layers\normalization.py"", line 501, in call
    outputs = self._fused_batch_norm(inputs, training=training)
  File ""D:\Users\Yue\Anaconda3\lib\site-packages\tensorflow\python\layers\normalization.py"", line 396, in _fused_batch_norm
    training, _fused_batch_norm_training, _fused_batch_norm_inference)
  File ""D:\Users\Yue\Anaconda3\lib\site-packages\tensorflow\python\layers\utils.py"", line 206, in smart_cond
    pred, true_fn=true_fn, false_fn=false_fn, name=name)
  File ""D:\Users\Yue\Anaconda3\lib\site-packages\tensorflow\python\framework\smart_cond.py"", line 56, in smart_cond
    return false_fn()
  File ""D:\Users\Yue\Anaconda3\lib\site-packages\tensorflow\python\layers\normalization.py"", line 393, in _fused_batch_norm_inference
    data_format=self._data_format)
  File ""D:\Users\Yue\Anaconda3\lib\site-packages\tensorflow\python\ops\nn_impl.py"", line 904, in fused_batch_norm
    name=name)
  File ""D:\Users\Yue\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 3804, in _fused_batch_norm
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[60000,16,14,14] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:FusedBatchNorm]

```

Partial log with the aforementioned fix
```python
2018-05-12 00:27:23.607020: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-05-12 00:27:24.748056: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX 980M major: 5 minor: 2 memoryClockRate(GHz): 1.1265
pciBusID: 0000:01:00.0
totalMemory: 4.00GiB freeMemory: 3.32GiB
2018-05-12 00:27:24.748771: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1435] Adding visible gpu devices: 0
2018-05-12 00:27:25.196921: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-05-12 00:27:25.197231: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:929]      0 
2018-05-12 00:27:25.197570: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:942] 0:   N 
2018-05-12 00:27:25.197993: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3050 MB memory) -> physical GPU (device: 0, name: GeForce GTX 980M, pci bus id: 0000:01:00.0, compute capability: 5.2)
x train (60000, 28, 28, 1)
y train (60000, 10)
x test (10000, 28, 28, 1)
y test (10000, 10)

Inside training._eager_set_inputs ip shape/s [(1, 28, 28, 1)]  <= Here the ""dataset"" is the dummy batch
***** Providing entire dataset into call *****
(1, 28, 28, 1)

Train on 60000 samples, validate on 10000 samples
Epoch 1/10
(128, 28, 28, 1)
(128, 28, 28, 1)
(128, 28, 28, 1)
(128, 28, 28, 1)
(128, 28, 28, 1)
... repeated for the entire training loop over the batch.
```
"
19240,error compilation with openmp enabled  in tensorflow lite compilation,"Hi , 
I building tensorflow lite shared library .
I added flag -fopenmp to build_def.bzl to enable openmp:

```
def tflite_copts():
  """"""Defines compile time flags.""""""
  copts = [
      ""-DFARMHASH_NO_CXX_STRING"",
  ] + select({
          str(Label(""//tensorflow:android_arm64"")): [
              ""-std=c++11"",
              ""-O3"",
              ""-fopenmp"",
          ],
          str(Label(""//tensorflow:android_arm"")): [
              ""-mfpu=neon"",
              ""-mfloat-abi=softfp"",
              ""-std=c++11"",
              ""-O3"",
          ],
          str(Label(""//tensorflow:android_x86"")): [
              ""-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK"",
          ],
          str(Label(""//tensorflow:ios_x86_64"")): [
              ""-msse4.1"",
          ],
          ""//conditions:default"": [],
  }) + select({
      str(Label(""//tensorflow:with_default_optimizations"")): [],
      ""//conditions:default"": [""-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK""],
  })
```

And I got error compilation,

```
tensorflow/contrib/lite/kernels/BUILD:43:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels:eigen_support' failed (Exit 1)
In file included from tensorflow/contrib/lite/kernels/eigen_support.cc:17:
In file included from ./third_party/eigen3/Eigen/Core:1:
external/eigen_archive/Eigen/Core:275:10: fatal error: 'omp.h' file not found
#include <omp.h>
         ^
1 error generated.
Target //tensorflow/contrib/lite:libtensorflowLite.so failed to build
```

Please advise how to fix it.
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes,
1. added openmp flag.
2. I changed BUILD file in tensorflow lite to create shared library
```
cc_binary(
    name = ""libtensorflowLite.so"",
    linkshared=1,

    deps = [
        "":framework"",
	""//tensorflow/contrib/lite/kernels:builtin_ops""
    ],
)
```
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16
- **TensorFlow installed from (source or binary)**:
sources
- **TensorFlow version (use command below)**:
master
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
0.10
- **GCC/Compiler version (if compiling from source)**:
latest
- **CUDA/cuDNN version**:
no
- **GPU model and memory**:
no
- **Exact command to reproduce**:
`bazel build  //tensorflow/contrib/lite:libtensorflowLite.so --crosstool_top=//external:android/crosstool --cpu=arm64-v8a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=""-std=c++11""`
"
19238,Reproducibility Problem In tensorflow,"If you open a GitHub issue, here is our policy:
------------------------

### System information
- **Have I written custom code - yes**:
- **google colab**:
- **google colab default notebook**:
- **'1.7.0'**:
- **Python version: '1.7.0'**: 
- **Bazel version : not applicable**:
- **GCC/Compiler version : not applicable**:
- **CUDA/cuDNN version : google colab default version**:
- **GPU model and memory : google colab's k80 gpu**:
- **Exact command to reproduce : code given**:

https://colab.research.google.com/drive/1KXAa4OVXht0ZhfN73u5AqqfYKM2uqvN8#scrollTo=gtSwh3KykmxG

I see some problem regarding Reproducibility of result. Here I have given a notebook. Each of the cell  contains same code with same seed of tensorflow except that in the middle cell contains only single placeholder extra that has nothing to do with the graph input. But I don't know why random number generation is different for just declaring a single placeholder extra. Is that an expected behavior? "
19236,add output_shape to tf.layers.conv2d_transpose,"### System information
- **Have I written custom code (N/A)**:
- **OS Platform and Distribution (Windows 10)**:
- **TensorFlow installed from (binary)**:
- **TensorFlow version (1.8.0)**:
- **Python version 3.6.1**: 
- **Bazel version (N/A)**:
- **GCC/Compiler version (N/A)**:
- **CUDA/cuDNN version** N/A:
- **GPU model and memory** N/A:
- **Exact command to reproduce** N/A:

### Describe the problem
Currently tf.layers.conv2d_transpose doesn't have an output_shape parameter like tf.nn.conv2d_transpose does. As a result when using strides > 1 the output shape is not defined 

https://github.com/tensorflow/tensorflow/issues/2118#issuecomment-215488127

Would it be possible to have this added in to the layers api so we can specify output shape.

"
19235,Bottleneck retrained model predicts one class after applying transforms and opimize_for_inference. ,"
Summary : 

**Have I written custom code** - Mostly using the retrain script from the tensorflow for poets tutorial. I added custom code to validate network but didn't change the training procedure. 
**OS Platform and Distribution** -  Windows 7 x64. Training and evaluation is done on a windows machine using Anconda python distribution. Once I freeze the model I then copy it to a linux box with CentOS 7 x86_64 to do the transformation. The transformed model is then copied back to the windows box for further analysis before deployment.
**TensorFlow installed from** - anaconda cloud 
**TensorFlow version** - 1.2.1 
**Bazel version** - 0.8.1
**CUDA/cuDNN version** - 8 / 7 
**GPU model and memory** - GTX 970 4GB 
**Exact command to reproduce** - N/A

I have asked the same question on stackoverflow 

https://stackoverflow.com/questions/50243492/inception-v3-retrained-model-incorrect-predictions-after-using-optimize-for-infe/50263837#50263837

and opencv help 

http://answers.opencv.org/question/191168/inception-v3-retrained-model-incorrect-predictions-after-using-optimize_for_inference-and-graph_transform-tools/

I can really use some suggestions: 

1. I followed the tensorflow for poets tutorial to retrain inception V3 for 3-class image task 

https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0

2. The network fits the data well and i'm able to validate the frozen model using tensorflow python which predicts unseen samples at 97 % accuracy 

3. The frozen model has to be integrated to a legacy application which uses opencv::DNN module. To get the forzen model ready I applied 
  - optimize_for_inference 
  - graph_transforms with "" remove_nodes(op=PlaceholderWithDefault) strip_unused_nodes(type=float, shape=\""1,299,299,3\"") sort_by_execution_order"" flags 

4. Loaded transformed model with opencv and run some more validation before deployment. The transformed version could not match (not even close) the trained original model.  It mostly predicts a single class. 

I used opencv blobFromImage function with swapRB set to true. 

I tried to normalize input by subtracting ImageNet mean as well as my training set mean. Unsuccessful in both cases. 


I just have no idea what is causing for the model to be so different after its transformed. 

Thanks for your time 


"
19233,`foldl` disallows mixing different types of `elems`,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Redhat
- **TensorFlow installed from (source or binary)**:
conda-forge
- **TensorFlow version (use command below)**:
1.6
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A (cpu only)
- **GPU model and memory**:
N/A (cpu only)
- **Exact command to reproduce**:
### Describe the problem
### Source code / logs
Look at the following two code snippets. 

```python
score = tf.foldl(lambda s, x: transitions[x[0], x[1]] + x[2][x[0]] + s, (label[1:], label[:-1], logits), initializer=np.zeros((), dtype=np.float64))
```

```python
score = tf.reduce_sum(tf.map_fn(lambda x: transitions[x[0], x[1]] + x[2][x[0]], (label[1:], label[:-1], logits), dtype=tf.float64))
```

Long story short, the 1st one does not compile, while the second one does. From my view, these 2 operations essentially do the same thing. That said, from the error trace, it seems to me `foldl` disallows mixing different types for the argument of `elems=`, since indeed `label` is of type `tf.int32`, while `logits` is of type `tf.float64`. However the documentation does not indicate that is the case. Could you confirm if that is true ? If yes, can we update the documentation to make it explicit ?

```python
XXX/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py in foldl(fn, elems, initializer, parallel_iterations, back_prop, swap_memory, name)
    107 
    108     # Convert elems to tensor array.
--> 109     elems = ops.convert_to_tensor(elems, name=""elems"")
    110     n = array_ops.shape(elems)[0]
    111     elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,

XXX/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)
    944       name=name,
    945       preferred_dtype=preferred_dtype,
--> 946       as_ref=False)
    947 
    948 

XXX/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)
   1034 
   1035     if ret is None:
-> 1036       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1037 
   1038     if ret is NotImplemented:

XXX/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_conversion_function(v, dtype, name, as_ref)
   1018   if dtype is not None and dtype != inferred_dtype:
   1019     return NotImplemented
-> 1020   return _autopacking_helper(v, inferred_dtype, name or ""packed"")
   1021 
   1022 

XXX/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_helper(list_or_tuple, dtype, name)
    960           raise TypeError(""Cannot convert a list containing a tensor of dtype ""
    961                           ""%s to %s (Tensor is: %r)"" % (elem.dtype, dtype,
--> 962                                                         elem))
    963         converted_elems.append(elem)
    964         must_pack = True

TypeError: Cannot convert a list containing a tensor of dtype <dtype: 'float64'> to <dtype: 'int32'> (Tensor is: <tf.Tensor 'loss/BiasAdd:0' shape=(?, 5) dtype=float64>)
```"
19231,.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
19228,optimize_for_inference.py has conversion error for float16 Conv2D,"### System information
- **Have I written custom code**: No
- **OS Platform and Distribution**: Linux Mint 18.3
- **TensorFlow installed from**: source
- **TensorFlow version**: v1.8.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 0.13.0
- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)
- **CUDA/cuDNN version**: Cuda compilation tools, release 9.1, V9.1.85
- **GPU model and memory**: 1080 Ti

### Describe the problem
`optimize_for_inference.py` has conversion error for `float16` `Conv2D`.

I have a model with all floats as `float16`. After optimization by `optimize_for_inference.py` and then loaded in C++, the following error occurs.

### Source code / logs
```
Invalid argument: No OpKernel was registered to support Op 'FusedPadConv2D' with these attrs.  Registered devices: [CPU,GPU,XLA_CPU,XLA_GPU], Registered kernels:
  device='CPU'; T in [DT_FLOAT]

	 [[Node: Conv2D = FusedPadConv2D[T=DT_HALF, mode=""REFLECT"", padding=""VALID"", strides=[1, 1, 1, 1]](MirrorPad_5, MirrorPad_6/paddings, Deep-Q-learning/conv1)]]
```

The format of the output of `Registered kernels` is so weird.

I have read the source code of `optimize_for_inference.py` but found no clue.
"
19223,"Misleading ""cupti.h"" error message","i am trying to build tensorflow 1.7.1 with cuda support on debian.
While doing so, i received the error message
```
Cuda Configuration Error: Cannot find cupti.h under /usr/lib/cuda
```
i symlinked cupti.h to the written location, even copied it there but nothing happened. after reading the buildscript ```third_party/gpus/cuda_configure.bzl```, it became clear that the script did actually not search for cupti.h in ```/usr/lib/cuda/``` but in two subfolders below that one. This error message is disleading and should display the actual path that bazel uses to look up the files."
19222,Support float16 for depthwise convolutions on GPU and CPU,"The current implementation supports only bfloat16, float32, float64, but not float16.

Stack trace:
```
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py"", line 536, in _DepthwiseConv2dNativeGrad
    data_format=op.get_attr(""data_format"")),
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 2325, in depthwise_conv2d_native_backprop_input
    name=name)
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 609, in _apply_op_helper
    param_name=input_name)
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 60, in _SatisfiesTypeConstraint
    "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
TypeError: Value passed to parameter 'filter' has DataType float16 not in list of allowed values: bfloat16, float32, float64
```

"
19220,"upsampling op for 5-D tensor with shape [batch, height, width, depth, channels] (feature request)","My target is 3D medical image.
For 4-D tensor B with shape [batch, height, width, channels] use tf.image.resize_* for upsampling.
For 5-D tensor A with shape [batch, height, width, depth, channels], tf.nn.conv3d_transpose can be used for upsampling, but I don't want extra weights for training.

Is there an direct op for this? instead of

1.  Note: This seem to be wrong !
     [batch, height, width, depth, channels] = tf.shape(A)
     A1 = tf.reshape(A, [batch, height, width * depth, channels]) 
     A2 = tf.image.resize_bilinear(A1, [2 * height, 4 * width * depth])
     A3 = tf.reshape(A2, [batch, 2 * height, 2 * width, 2 * depth, channels]

2. split the last dimension -> reshape -> resize_bilinear -> reshape -> concat.

3. tf.nn.con3d_transpose()
"
19219,unorderable types: str() < tuple() in /tensorflow/python/feature_column/feature_column.py,"Google Tensorflow forum guy asked me to open this issue:

### System information
- **Have I written custom code **:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 14.04 LTS
- **TensorFlow installed from (source or binary)**:
pip3 install tensorflow --upgrade
- **TensorFlow version (use command below)**:
TF 1.7.0.
- **Python version**: 
Python 3.4.3 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:



### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

run time error in python: unorderable types: str() < tuple() in /tensorflow/python/feature_column/feature_column.py

### Source code / logs
https://github.com/werowe/tripAdvisorNeuralNetworkTensorFlow/blob/master/tripadvisorNN.py
data:   https://raw.githubusercontent.com/werowe/tripAdvisorNeuralNetworkTensorFlow/master/tripAdvisorFL.csv


dictionary {'Nrhotelreviews': <tf.Tensor 'DecodeCSV:2' shape=() dtype=int32>, 'Casino': <tf.Tensor 'DecodeCSV:11' shape=() dtype=int32>, 'Travelertype': <tf.Tensor 'DecodeCSV:6' shape=() dtype=int32>, 'Helpfulvotes': <tf.Tensor 'DecodeCSV:3' shape=() dtype=int32>, 'Usercontinent': <tf.Tensor 'DecodeCSV:16' shape=() dtype=int32>, 'Tenniscourt': <tf.Tensor 'DecodeCSV:9' shape=() dtype=int32>, 'Usercountry': <tf.Tensor 'DecodeCSV:0' shape=() dtype=int32>, 'Reviewmonth': <tf.Tensor 'DecodeCSV:18' shape=() dtype=int32>, 'Hotelname': <tf.Tensor 'DecodeCSV:13' shape=() dtype=int32>, 'Nrrooms': <tf.Tensor 'DecodeCSV:15' shape=() dtype=int32>, 'Freeinternet': <tf.Tensor 'DecodeCSV:12' shape=() dtype=int32>, 'Nrreviews': <tf.Tensor 'DecodeCSV:1' shape=() dtype=int32>, 'Gym': <tf.Tensor 'DecodeCSV:8' shape=() dtype=int32>, 'Memberyears': <tf.Tensor 'DecodeCSV:17' shape=() dtype=int32>, 'Periodofstay': <tf.Tensor 'DecodeCSV:5' shape=() dtype=int32>, 'Pool': <tf.Tensor 'DecodeCSV:7' shape=() dtype=int32>, 'Reviewweekday': <tf.Tensor 'DecodeCSV:19' shape=() dtype=int32>, 'Spa': <tf.Tensor 'DecodeCSV:10' shape=() dtype=int32>, 'Hotelstars': <tf.Tensor 'DecodeCSV:14' shape=() dtype=int32>}  label =  Tensor(""DecodeCSV:4"", shape=(), dtype=int32)
INFO:tensorflow:Calling model_fn.
Traceback (most recent call last):
  File ""<stdin>"", line 3, in <module>
  File ""/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py"", line 355, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py"", line 824, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py"", line 805, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 354, in _model_fn
    config=config)
  File ""/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 184, in _dnn_model_fn
    logits = logit_fn(features=features, mode=mode)
  File ""/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 92, in dnn_logit_fn
    features=features, feature_columns=feature_columns)
  File ""/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/feature_column/feature_column.py"", line 274, in input_layer
    trainable, cols_to_vars)
  File ""/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/feature_column/feature_column.py"", line 192, in _internal_input_layer
    for column in sorted(feature_columns, key=lambda x: x.name):
TypeError: unorderable types: str() < tuple()
"
19216,import tensorflow as tf error,"installed from anaconda using
`pip  install  tensorflow`

OS Platform:
Windows 7 64bit
intel processor
intel hd graphic graphic card
python version 3.6.4

Have I written custom code: No
Bazel version : I don't have
CUDA/cuDNN version : I don't have
GPU model and memory :N\A
Exact command to reproduce : import tensorflow as tf using jupyter


```
(base) C:\Users\david>pip install tensorflow
Collecting tensorflow
  Downloading https://files.pythonhosted.org/packages/f4/88/980d7032b7408fcca5b0
b8d420fcd97919197a9e7acf280ab74fc7db6993/tensorflow-1.8.0-cp36-cp36m-win_amd64.w
hl (34.4MB)
    100% |¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦| 34.4MB 13kB/s
Collecting astor>=0.6.0 (from tensorflow)
  Downloading https://files.pythonhosted.org/packages/b2/91/cc9805f1ff7b49f62013
6b3a7ca26f6a1be2ed424606804b0fbcf499f712/astor-0.6.2-py2.py3-none-any.whl
Collecting gast>=0.2.0 (from tensorflow)
  Downloading https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e
789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz
Requirement already satisfied: six>=1.10.0 in c:\programdata\anaconda3\lib\site-
packages (from tensorflow)
Collecting tensorboard<1.9.0,>=1.8.0 (from tensorflow)
  Downloading https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b
2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl (3.1
MB)
    100% |¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦| 3.1MB 682kB/s
Requirement already satisfied: numpy>=1.13.3 in c:\programdata\anaconda3\lib\sit
e-packages (from tensorflow)
Collecting termcolor>=1.1.0 (from tensorflow)
  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2
a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz
Requirement already satisfied: wheel>=0.26 in c:\programdata\anaconda3\lib\site-
packages (from tensorflow)
Collecting grpcio>=1.8.6 (from tensorflow)
  Downloading https://files.pythonhosted.org/packages/80/7e/d5ee3ef92822b01e3a27
4230200baf2454faae64e3d7f436b093ff771a17/grpcio-1.11.0-cp36-cp36m-win_amd64.whl
(1.4MB)
    100% |¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦| 1.4MB 787kB/s
Collecting protobuf>=3.4.0 (from tensorflow)
  Downloading https://files.pythonhosted.org/packages/32/cf/6945106da76db9b62d11
b429aa4e062817523bb587018374c77f4b63200e/protobuf-3.5.2.post1-cp36-cp36m-win_amd
64.whl (958kB)
    100% |¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦| 962kB 787kB/s
Collecting absl-py>=0.1.6 (from tensorflow)
  Downloading https://files.pythonhosted.org/packages/90/6b/ba04a9fe6aefa56adafa
6b9e0557b959e423c49950527139cb8651b0480b/absl-py-0.2.0.tar.gz (82kB)
    100% |¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦| 92kB 708kB/s
Collecting bleach==1.5.0 (from tensorboard<1.9.0,>=1.8.0->tensorflow)
  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d
4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl
Requirement already satisfied: werkzeug>=0.11.10 in c:\programdata\anaconda3\lib
\site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow)
Collecting markdown>=2.6.8 (from tensorboard<1.9.0,>=1.8.0->tensorflow)
  Downloading https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5
788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl (7
8kB)
    100% |¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦| 81kB 682kB/s
Collecting html5lib==0.9999999 (from tensorboard<1.9.0,>=1.8.0->tensorflow)
  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfa
f19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)
    100% |¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦| 890kB 787kB/s
Requirement already satisfied: setuptools in c:\programdata\anaconda3\lib\site-p
ackages (from protobuf>=3.4.0->tensorflow)
Building wheels for collected packages: gast, termcolor, absl-py, html5lib
  Running setup.py bdist_wheel for gast ... done
  Stored in directory: C:\Users\david\AppData\Local\pip\Cache\wheels\9a\1f\0e\3c
de98113222b853e98fc0a8e9924480a3e25f1b4008cedb4f
  Running setup.py bdist_wheel for termcolor ... done
  Stored in directory: C:\Users\david\AppData\Local\pip\Cache\wheels\7c\06\54\bc
84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6
  Running setup.py bdist_wheel for absl-py ... done
  Stored in directory: C:\Users\david\AppData\Local\pip\Cache\wheels\23\35\1d\48
c0a173ca38690dd8dfccfa47ffc750db48f8989ed898455c
  Running setup.py bdist_wheel for html5lib ... done
  Stored in directory: C:\Users\david\AppData\Local\pip\Cache\wheels\50\ae\f9\d2
b189788efcf61d1ee0e36045476735c838898eef1cad6e29
Successfully built gast termcolor absl-py html5lib
Installing collected packages: astor, gast, html5lib, bleach, markdown, protobuf
, tensorboard, termcolor, grpcio, absl-py, tensorflow
  Found existing installation: html5lib 1.0.1
    Uninstalling html5lib-1.0.1:
      Successfully uninstalled html5lib-1.0.1
  Found existing installation: bleach 2.1.2
    Uninstalling bleach-2.1.2:
      Successfully uninstalled bleach-2.1.2
Successfully installed absl-py-0.2.0 astor-0.6.2 bleach-1.5.0 gast-0.2.0 grpcio-
1.11.0 html5lib-0.9999999 markdown-2.6.11 protobuf-3.5.2.post1 tensorboard-1.8.0
 tensorflow-1.8.0 termcolor-1.1.0
You are using pip version 9.0.1, however version 10.0.1 is available.
You should consider upgrading via the 'python -m pip install --upgrade pip' comm
and.
```




```
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     13         try:
---> 14             return importlib.import_module(mname)
     15         except ImportError:

C:\ProgramData\Anaconda3\lib\importlib\__init__.py in import_module(name, package)
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 

C:\ProgramData\Anaconda3\lib\importlib\_bootstrap.py in _gcd_import(name, package, level)

C:\ProgramData\Anaconda3\lib\importlib\_bootstrap.py in _find_and_load(name, import_)

C:\ProgramData\Anaconda3\lib\importlib\_bootstrap.py in _find_and_load_unlocked(name, import_)

C:\ProgramData\Anaconda3\lib\importlib\_bootstrap.py in _load_unlocked(spec)

C:\ProgramData\Anaconda3\lib\importlib\_bootstrap.py in module_from_spec(spec)

C:\ProgramData\Anaconda3\lib\importlib\_bootstrap_external.py in create_module(self, spec)

C:\ProgramData\Anaconda3\lib\importlib\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)

ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

ModuleNotFoundError                       Traceback (most recent call last)
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
     16             return importlib.import_module('_pywrap_tensorflow_internal')
---> 17     _pywrap_tensorflow_internal = swig_import_helper()
     18     del swig_import_helper

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     15         except ImportError:
---> 16             return importlib.import_module('_pywrap_tensorflow_internal')
     17     _pywrap_tensorflow_internal = swig_import_helper()

C:\ProgramData\Anaconda3\lib\importlib\__init__.py in import_module(name, package)
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 

ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-306-64156d691fe5> in <module>()
----> 1 import tensorflow as tf

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 # pylint: disable=wildcard-import
     26 from tensorflow.tools.api.generator.api import *  # pylint: disable=redefined-builtin

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\ProgramData\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\ProgramData\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```


"
19214,for loop not working in tf.contrib.autograph,"The following example for autograph is not working (see my system-info below):

```
from tensorflow.contrib import autograph

def computation():
    for k in range(4):
        print(k)
    return tf.no_op()

computation_autographd = autograph.to_graph(computation, verbose=True)
```

If I remove the for loop, it works fine.
With the loop, it fails with:

```
File ""/opt/project/examples/_dict_in_autograph_example.py"", line 66, in <module>
verbose=True)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/autograph/impl/api.py"", line 245, in to_graph
    compiled_node, compiled_src = compiler.ast_to_object(module)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/autograph/pyct/compiler.py"", line 75, in ast_to_object
    return imp.load_source(module_name, f.name), source
  File ""/usr/lib/python3.5/imp.py"", line 172, in load_source
    module = _load(spec)
  File ""<frozen importlib._bootstrap>"", line 693, in _load
  File ""<frozen importlib._bootstrap>"", line 673, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 661, in exec_module
  File ""<frozen importlib._bootstrap_external>"", line 767, in get_code
  File ""<frozen importlib._bootstrap_external>"", line 727, in source_to_code
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
  File ""/tmp/tmpqdvecllf.py"", line 21
    () = __ops.for_loop(autograph_utils.dynamic_builtin(range, 4),
       ^
SyntaxError: can't assign to ()

Process finished with exit code 1
```

python3 --version
Python 3.5.2

python3 -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.8.0-0-g93bc2e2072 1.8.0

== cat /etc/issue ===============================================
Linux 5a7b67a966f3 4.9.87-linuxkit-aufs #1 SMP Wed Mar 14 15:12:16 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux 5a7b67a966f3 4.9.87-linuxkit-aufs #1 SMP Wed Mar 14 15:12:16 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ==================================================="
19211,TensorRT engine requires consistent batch size,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, custom SSD model
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: nvidia gpu cloud docker container 18.04 for V100
                                                                                from source for 1080 Ti
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9 / 7
- **GPU model and memory**: V100 (16GB) 1080Ti (11GB)
- **Exact command to reproduce**: N/A 

### Source code / logs
Run TF-TRT with custom SSD and got the following message:
F tensorflow/contrib/tensorrt/shape_fn/trt_shfn.cc:52] TensorRT engine requires consistent batch size

After commenting out the line 52 in trt_shfn.cc and recompile with bazel, the model ran properly and output expected results. Suspect that this dimension check has a bug and does not cover all the cases. 

Use the code example from [here](https://developer.download.nvidia.com/devblogs/tftrt_sample.tar.xz) and modify to load custom SSD model.

"
19208,rnn_cell not Checkpointable,"tf-nightly: 1.8.0.dev20180329 (latest Windows 10 build)

Sample Code:
```python
import tensorflow as tf
import tensorflow.contrib.eager as tfe
from tensorflow.python.training import checkpointable


class MyModel(checkpointable.Checkpointable):

  def __init__(self):
    self.cell = tf.nn.rnn_cell.DropoutWrapper(
        tf.nn.rnn_cell.BasicLSTMCell(10), 0.5)
    self.cell2 = tf.nn.rnn_cell.BasicLSTMCell(10)


model = MyModel()
state = tf.nn.rnn_cell.LSTMStateTuple(
    c=tf.constant([[10] * 10] * 10, dtype=tf.float32),
    h=tf.constant([[10] * 10] * 10, dtype=tf.float32))
model.cell(tf.constant([[10]] * 10, dtype=tf.float32), state)
model.cell2(tf.constant([[10]] * 10, dtype=tf.float32), state)

checkpoint_path = 'modeldir/test/'
checkpoint = tfe.Checkpoint(model=model)
saver = tfe.CheckpointableSaver(checkpoint)

from tensorflow.contrib.eager.python import checkpointable_utils
print(checkpointable_utils._serialize_object_graph(saver._root_checkpointable))

```

Prints:
```
({'model/cell2/kernel/.ATTRIBUTES/VARIABLE_VALUE': <tf.Variable 'basic_lstm_cell_1/kernel:0' shape=(11, 40) dtype=float32_ref>, 'model/cell2/bias/.ATTRIBUTES/VARIABLE_VALUE': <tf.Variable 'basic_lstm_cell_1/bias:0' shape=(40,) dtype=float32_ref>}, nodes {
  children {
    node_id: 1
    local_name: ""model""
  }
}
nodes {
  children {
    node_id: 2
    local_name: ""cell""
  }
  children {
    node_id: 3
    local_name: ""cell2""
  }
}
nodes {
}
nodes {
  children {
    node_id: 4
    local_name: ""kernel""
  }
  children {
    node_id: 5
    local_name: ""bias""
  }
}
nodes {
  attributes {
    name: ""VARIABLE_VALUE""
    full_name: ""basic_lstm_cell_1/kernel""
    checkpoint_key: ""model/cell2/kernel/.ATTRIBUTES/VARIABLE_VALUE""
  }
}
nodes {
  attributes {
    name: ""VARIABLE_VALUE""
    full_name: ""basic_lstm_cell_1/bias""
    checkpoint_key: ""model/cell2/bias/.ATTRIBUTES/VARIABLE_VALUE""
  }
}
)
```

Expected to contain `bias` and `kernel` for both `cell` and `cell2` but only contains them for the cell without `DropoutWrapper`."
19207,AttributeError: 'SsdAnchorGenerator' object has no attribute 'height_stride',"### System information
- I am trying to use export_inference_graph.py within tensorflow/research/object_detection
- Windows 10
- Tensorflow installed from binary
- Tensorflow Version b'v1.8.0-0-g93bc2e2072' 1.8.0
- Python 3.6.4
### Describe the problem
I am trying to train a detector using a custom data set.  I have run through the training and produced the model.ckpt files (meta, index, etc).  The issue I'm running into is creating the frozen inference graph.  Running export_inference_graph.py always fails out with the error:

AttributeError: 'SsdAnchorGenerator' object has no attribute 'height_stride'

After some mild searching online, I didn't uncover any hits or paths to go down to address this.  What might I be missing?

### Source code / logs
tensorflow\research\object_detection> python export_inference_graph.py --input_type i
mage_tensor --pipeline_config_path training/ssd_mobilenet_v1_coco.config --trained_checkpoint_prefix training/model.ckpt
-89076 --output_directory inference_graph
C:\ProgramData\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""export_inference_graph.py"", line 147, in <module>
    tf.app.run()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""export_inference_graph.py"", line 143, in main
    FLAGS.output_directory, input_shape)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\exporter.py"", line 447, in export_inference_graph
    is_training=False)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\builders\model_builder.py"", line 98, in build
    add_background_class)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\builders\model_builder.py"", line 178, in _build_ssd_model
    ssd_config.anchor_generator)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\builders\anchor_generator_builder.py"", line 59, in build
    if ssd_anchor_generator_config.height_stride:
AttributeError: 'SsdAnchorGenerator' object has no attribute 'height_stride'

### Edit 5/14/2018:
Have I written custom code: not for this instance.
OS Platform : Windows 10
Basel Version: NA
Cuda version l: NA
Gpu Model: NA
Command: tensorflow\research\object_detection> python export_inference_graph.py --input_type i
mage_tensor --pipeline_config_path training/ssd_mobilenet_v1_coco.config --trained_checkpoint_prefix training/model.ckpt
-89076 --output_directory inference_graph"
19206,Can't pass eager tensors to Model.train_on_batch,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
OSX, and Fedora 28

- **TensorFlow installed from (source or binary)**:
Binary

- **TensorFlow version (use command below)**:
1.8.0

- **Python version**: 
3.6

Bazel version
N/A

CUDA/cuDNN version
N/A

GPU model and memory
N/A

Exact command to reproduce
See description of the problem below.


### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I can't pass eager tensors to Model.train_on_batch. I had to pass numpy arrays instead."
19203,Build Tensorflow 1.7 on Cuda 8.0,"Hi I am trying to build tensorflow r1.7 branch with the following settings:

* cuda 8.0
* cudnn 7.0
* gcc 5.4

but i got the following error:

```
tensorflow/core/kernels/neon/neon_depthwise_conv_op.cc:138:42:   required from here
./tensorflow/core/kernels/neon/depthwiseconv_float.h:602:7: warning: variable 'fixed_input_depth' set
 but not used [-Wunused-but-set-variable]
   int fixed_input_depth = 0;
       ^
INFO: From Compiling tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc:
tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc(113): warning: function ""tensorflow::<unnamed>::IdentityOp::operator()"" was declared but never referenced

tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc(155): warning: function ""tensorflow::<unnamed>::BoundedOutputIterator::operator*"" was declared but never referenced

tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc(165): warning: function ""tensorflow::<unnamed>::BoundedOutputIterator::operator+"" was declared but never referenced

tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc(171): warning: function ""tensorflow::<unnamed>::BoundedOutputIterator::operator-"" was declared but never referenced

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_
expect"") from a __global__ function(""tensorflow::GatherOpKernel< ::Eigen::half, int, (bool)1> "") is n
ot allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_
expect"") from a __global__ function(""tensorflow::GatherOpKernel<float, int, (bool)1> "") is not allowe
d

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_
expect"") from a __global__ function(""tensorflow::GatherOpKernel<double, int, (bool)1> "") is not allow
ed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_
expect"") from a __global__ function(""tensorflow::GatherOpKernel< ::std::complex<float> , int, (bool)1
> "") is not allowed

./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(""__builtin_
expect"") from a __global__ function(""tensorflow::GatherOpKernel< ::std::complex<double> , int, (bool)
1> "") is not allowed

5 errors detected in the compilation of ""/tmp/tmpxft_00004427_00000000-7_dynamic_partition_op_gpu.cu.
cpp1.ii"".
ERROR: /home/ubuntu/tensorflow/tensorflow/core/kernels/BUILD:1702:1: output 'tensorflow/core/kernels/
_objs/dynamic_partition_op_gpu/tensorflow/core/kernels/dynamic_partition_op_gpu.cu.pic.o' was not cre
ated
ERROR: /home/ubuntu/tensorflow/tensorflow/core/kernels/BUILD:1702:1: not all outputs were created or
valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 770.936s, Critical Path: 154.34s
INFO: 1470 processes, local.
FAILED: Build did NOT complete successfully
```"
19200,Difference in output between CPU and GPU ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: TF 1.7 and TF 1.8 (v1.8.0-0-g93bc2e2072 1.8.0)
- **Python version**: Python 3.5.4
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: CUDA 9.0, cudnn 7.0.5
- **GPU model and memory**: GPU GTX1080
- **Exact command to reproduce**: n/a

**Problem**
I see a very different output when executing a training on CPU vs. GPU. The problem does not seem to be related to initialization or numerical precision (see investigation): same code, same data, same machine, just switching between GPU and CPU execution by adding `os.environ[""CUDA_VISIBLE_DEVICES""]=""""` gives fundamentally different results (the loss function differs by several orders of magnitude at the end of the first run of the first epoch).

**Investigation**
I gave names to all the variables and operations in order to track what was the first operation in the graph where the results differ between CPU and GPU execution. I could actually identify it. In my case, it is a `tf.multiply` (pixel-wise multiplication of complex64 Tensors). I can see that the inputs of that operation are identical (+/- numerical precision), while the outputs are really different: the magnitude of the complex output is consistent but the phase is very different (the real and imaginary parts are both very different). I can tell that the CPU output is the right one.

Unfortunately, I am not able to reproduce the problem in a unit test (but am still working on it). If I feed the same inputs to `tf.multiply` in a unit test, the CPU and GPU outputs are consistent. I am wondering if the CPU/GPU execution may be affecting the order in which the graph is build/executed, and if there may be interferences somehow.

**Seeding and reproducibility**
Regarding the seeds, they are all set (graph wise and operation wise), but anyway, the diverging operation does not depend on a seed in this case.

The problem is reproducible from one run to the other.

**Environment dependency**
The problem occurs with both TF 1.7 and TF 1.8. However, if I switch the environment (same machine, same OS, same code, same data, but python 2.7, TF 1.4), the problem disappears, i.e. the CPU and GPU outputs are consistent.

Any idea or debugging experiment suggestion is very welcome.
"
19198,__hadd() is ambiguous when EIGEN_CUDA_ARCH >= 530,"### System information
- **Have I written custom code**: I change the CUDA capabilities to 6.1 and 7.0.
- **OS Platform and Distribution**: Windows 10
- **TensorFlow installed from (source or binary)**: I'm compiling the source code.
- **TensorFlow version (use command below)**: branch `r1.8`, 8753e2ebde6c58b56675cc19ab7ff83072824a62
- **Python version**: 3.6.0
- **Bazel version (if compiling from source)**: No
- **GCC/Compiler version (if compiling from source)**: VS 2017(v141), but v140 for CUDA host compiler
- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7.1
- **GPU model and memory**: 1080 Ti, Titan V
- **Exact command to reproduce**: cmake-gui, enable GPU, and change CUDA host compiler to v140

### Describe the problem
[__hadd()](https://github.com/eigenteam/eigen-git-mirror/blob/2bdd9e80c43ee491fb0cb299940995e094b1647b/Eigen/src/Core/arch/CUDA/Half.h#L212) is ambiguous when [EIGEN_CUDA_ARCH >= 530](https://github.com/eigenteam/eigen-git-mirror/blob/2bdd9e80c43ee491fb0cb299940995e094b1647b/Eigen/src/Core/arch/CUDA/Half.h#L204).

The following is where the ambiguity comes from found in VS 2017:
![image](https://user-images.githubusercontent.com/6009211/39873593-f19954bc-549d-11e8-8bb6-3c02e33c44c5.png)

### Source code / logs
`tf_core_gpu_kernels` compilation fails because of this problem:
```
42>Building NVCC (Device) object CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/Release/tf_core_gpu_kernels_generated_check_numerics_op_gpu.cu.cc.obj
42>check_numerics_op_gpu.cu.cc
42>e:\program\ml\tensorflow_build_05-10-01\external\eigen_archive\eigen\src/Core/arch/CUDA/Half.h(212): error : more than one instance of overloaded function ""__hadd"" matches the argument list:
42>            function ""__hadd(int, int)""
42>            function ""__hadd(__half, __half)""
42>            argument types are: (const Eigen::half, const Eigen::half)
```

I'm confused that nobody has every post such an issue.
Nobody has ever tried changing CUDA capabilities to >=5.3?
Or is there something wrong with my environment?

It seems that this is a pure Eigen issue...
"
19196,the program is not running on GPU,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- 
    OS Platform and Distribution: windows 10
    **TensorFlow installed from **: pip2 install tensorflow-gpu
    TensorFlow version: ('v1.4.0')
    Python version: Python 3.6.2
    CUDA/cuDNN version: Cuda compilation tools, release 8.0, V8.0.61


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

problem 
while i run my program and check the performance on task manager it is not using GPU 1 Which is NVIDIA 940MX, it runs on INTEL GRAPHICS 


i checked it by writing this line of code on my editor
import tensorflow as tf
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))

 the out put message is the following 
2018-05-10 14:26:55.271501: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2018-05-10 14:26:55.955641: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1030] Found device 0 with properties:
name: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 1.2415
pciBusID: 0000:01:00.0
totalMemory: 2.00GiB freeMemory: 1.66GiB
2018-05-10 14:26:55.955771: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)
Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0
2018-05-10 14:26:56.084191: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\direct_session.cc:299] Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0


"
19195,Build TF lite benchmark model failed,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:('v1.7.0-3-g024aecf414', '1.7.0')
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**:0.13.0
- **GCC/Compiler version (if compiling from source)**:c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
- **CUDA/cuDNN version**:No
- **GPU model and memory**:No
- **Exact command to reproduce**:

I want to build tensorflow lite benchmark model for Android, so I change the WORKSPACE like follows:
```
android_sdk_repository(
    name = ""androidsdk"",
    api_level = 26,
    # Ensure that you have the build_tools_version below installed in the
    # SDK manager as it updates periodically.
    build_tools_version = ""26.0.1"",
    # Replace with path to Android SDK on your system
    path = ""/home/libin11/Android/Sdk"",
)

android_ndk_repository(
    name=""androidndk"",
    path=""/home/libin11/opt/android-ndk-r14b/"",
    # This needs to be 14 or higher to compile TensorFlow.
    # Please specify API level to >= 21 to build for 64-bit
    # archtectures or the Android NDK will automatically select biggest
    # API level that it supports without notice.
    # Note that the NDK version is not the API level.
    api_level=21)
```
And then build:
```
bazel build --config android_arm64 --config monolithic --cxxopt=-std=c++11 //tensorflow/contrib/lite/tools:benchmark_model
```
But it failed after building awhile:
```
ERROR: /home/libin11/workspace/tensorflow/tensorflow/contrib/lite/tools/BUILD:31:1: C++ compilation of rule '//tensorflow/contrib/lite/tools:benchmark_model' failed (Exit 1)
tensorflow/contrib/lite/tools/benchmark_model.cc:306:37: error: unknown type name 'int64'; did you mean 'tensorflow::int64'?
                      int num_runs, int64* total_time_us) {
                                    ^~~~~
                                    tensorflow::int64
./tensorflow/core/platform/default/integral_types.h:27:19: note: 'tensorflow::int64' declared here
typedef long long int64;
                  ^
tensorflow/contrib/lite/tools/benchmark_model.cc:425:3: error: unknown type name 'int64'; did you mean 'tensorflow::int64'?
  int64 initialization_start_us = Env::Default()->NowMicros();
  ^~~~~
  tensorflow::int64
./tensorflow/core/platform/default/integral_types.h:27:19: note: 'tensorflow::int64' declared here
typedef long long int64;
                  ^
tensorflow/contrib/lite/tools/benchmark_model.cc:436:3: error: unknown type name 'int64'; did you mean 'tensorflow::int64'?
  int64 initialization_end_us = Env::Default()->NowMicros();
  ^~~~~
  tensorflow::int64
./tensorflow/core/platform/default/integral_types.h:27:19: note: 'tensorflow::int64' declared here
typedef long long int64;
                  ^
tensorflow/contrib/lite/tools/benchmark_model.cc:447:3: error: unknown type name 'int64'; did you mean 'tensorflow::int64'?
  int64 warmup_time_us = 0;
  ^~~~~
  tensorflow::int64
./tensorflow/core/platform/default/integral_types.h:27:19: note: 'tensorflow::int64' declared here
typedef long long int64;
                  ^
tensorflow/contrib/lite/tools/benchmark_model.cc:458:3: error: unknown type name 'int64'; did you mean 'tensorflow::int64'?
  int64 no_stat_time_us = 0;
  ^~~~~
  tensorflow::int64
./tensorflow/core/platform/default/integral_types.h:27:19: note: 'tensorflow::int64' declared here
typedef long long int64;
                  ^
5 errors generated.
Target //tensorflow/contrib/lite/tools:benchmark_model failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 332.588s, Critical Path: 68.13s
INFO: 513 processes, local.
FAILED: Build did NOT complete successfully
```
So I change int64 from benchmark_model.cc to int64_t and rebuild..
And it failed again:
```
ERROR: /home/libin11/workspace/tensorflow/tensorflow/contrib/lite/tools/BUILD:31:1: Linking of rule '//tensorflow/contrib/lite/tools:benchmark_model' failed (Exit 1)
bazel-out/arm64-v8a-opt/bin/tensorflow/core/libandroid_tensorflow_lib_lite.lo(logging.o): In function `tensorflow::internal::LogMessage::GenerateLogMessage()':
/proc/self/cwd/tensorflow/core/platform/default/logging.cc:65: undefined reference to `__android_log_write'
bazel-out/arm64-v8a-opt/bin/external/protobuf_archive/libprotobuf_lite.a(common.o): In function `google::protobuf::internal::DefaultLogHandler(google::protobuf::LogLevel, char const*, int, std::string const&)':
/proc/self/cwd/external/protobuf_archive/src/google/protobuf/stubs/common.cc:142: undefined reference to `__android_log_write'
/proc/self/cwd/external/protobuf_archive/src/google/protobuf/stubs/common.cc:150: undefined reference to `__android_log_write'
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target //tensorflow/contrib/lite/tools:benchmark_model failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 28.330s, Critical Path: 27.94s
INFO: 5 processes, local.
FAILED: Build did NOT complete successfully
```
So I add ""-llog"" to benchmark_model's linkopts. Finally, it worked:
```
$ bazel build --config android_arm64 --config monolithic --cxxopt=-std=c++11 //tensorflow/contrib/lite/tools:benchmark_model
WARNING: /home/libin11/.cache/bazel/_bazel_libin11/ee99114ce55f575758aad31c3fa3e774/external/protobuf_archive/WORKSPACE:1: Workspace name in /home/libin11/.cache/bazel/_bazel_libin11/ee99114ce55f575758aad31c3fa3e774/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:batch_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_volume_patch.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there
INFO: Analysed target //tensorflow/contrib/lite/tools:benchmark_model (1 packages loaded).
INFO: Found 1 target...
Target //tensorflow/contrib/lite/tools:benchmark_model up-to-date:
  bazel-bin/tensorflow/contrib/lite/tools/benchmark_model
INFO: Elapsed time: 23.555s, Critical Path: 18.58s
INFO: 1 process, local.
INFO: Build completed successfully, 2 total actions
```
So my question is, is this the right way to build tensorflow lite benchmark model?"
19194,"TypeError: `pred` must be a Tensor, a Variable, or a Python bool","Anaconda3\lib\site-packages\tensorflow\python\layers\utils.py"", line 234, in constant_value
    raise TypeError('`pred` must be a Tensor, a Variable, or a Python bool.')
TypeError: `pred` must be a Tensor, a Variable, or a Python bool."
19193,Feature request: tf.pad to pad an image with different values correspond to different channels respectively,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source)**: source
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: GTX 1080, 8G

### Describe the problem

**tf.pad** has a parameter `constant_values` to control the padding values. However, the value need to be a single scalar and it is used to pad all the channels equally.
Then if we want to pad the imagenet mean values [123.68, 116.779, 103.939] to 3 channels of a RGB image tensor respectively, we cannot achieve it by tf.pad.

So is it possible to extend tf.pad to be able to pad 3 channels with different values respectively?
"
19192,classifier.predict can't stop if input_fn return a sample tuple,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.6
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

```python
In [7]: import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)
v1.8.0-0-g93bc2e2072 1.8.0
```

# Describe the problem
If `input_fn` return a sample tuple, it won't stop and predict again and again
```python
result = classifier.predict(lambda: (ev_data,))
```
But, if `input_fn` return a `tf.data.Dataset`, it work well.
```python
result = classifier.predict(lambda: eval_input_fn(ev_data, labels=None, batch_size=1))
```
After debug, i found a while loop can't stop in here [`while not mon_sess.should_stop()`](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/estimator/estimator.py#L508)

But it seem like tuple is supported[](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/estimator/estimator.py#L456)
So, the return of `input_fn` must be `tf.data.Dataset` type? Or, should I give it a [end-of-input exception](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/estimator/estimator.py#L445)by myself?

### Source code / logs
Here is my code
```python
import tensorflow as tf


def eval_input_fn(features, labels, batch_size):
    """"""An input function for evaluation or prediction""""""
    features = dict(features)
    if labels is None:
        # No labels, use only features.
        inputs = features
    else:
        inputs = (features, labels)

    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices(inputs)

    # Batch the examples
    assert batch_size is not None, ""batch_size must not be None""
    dataset = dataset.batch(batch_size)

    # Return the dataset.
    return dataset


train_x = {'x1': [0, 1, 0, 1], 'x2': [0, 0, 1, 1]}
label = [0, 1, 1, 0]

ev_data = {'x1': [0.5, 0.5, 0.5, 1], 'x2': [0.5, 0, 1, 1]}

train_steps = 1000
my_feature_columns = []
for key in train_x.keys():
    my_feature_columns.append(tf.feature_column.numeric_column(key=key))

classifier = tf.estimator.DNNClassifier(
    feature_columns=my_feature_columns,
    hidden_units=[4, 4],
    n_classes=2)

classifier.train(lambda: (train_x, label), steps=train_steps)

result = classifier.predict(lambda: eval_input_fn(ev_data, labels=None, batch_size=1))
# result = classifier.predict(lambda: (ev_data,)) # it won't stop
for r in result:
    print(r)
```
If I return `tf.data.Dataset`, here is output:
```text
INFO:tensorflow:Using default config.
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpyu6blxd6
INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpyu6blxd6', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5c9dfcdd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpyu6blxd6/model.ckpt.
INFO:tensorflow:loss = 2.571034, step = 1
INFO:tensorflow:global_step/sec: 792.849
INFO:tensorflow:loss = 1.2854626, step = 101 (0.126 sec)
INFO:tensorflow:global_step/sec: 1621.83
INFO:tensorflow:loss = 0.58735335, step = 201 (0.062 sec)
INFO:tensorflow:global_step/sec: 1585.9
INFO:tensorflow:loss = 0.37925595, step = 301 (0.063 sec)
INFO:tensorflow:global_step/sec: 1579.13
INFO:tensorflow:loss = 0.286068, step = 401 (0.063 sec)
INFO:tensorflow:global_step/sec: 1656.07
INFO:tensorflow:loss = 0.23221238, step = 501 (0.060 sec)
INFO:tensorflow:global_step/sec: 1624.02
INFO:tensorflow:loss = 0.1968127, step = 601 (0.062 sec)
INFO:tensorflow:global_step/sec: 1565.82
INFO:tensorflow:loss = 0.1710473, step = 701 (0.064 sec)
INFO:tensorflow:global_step/sec: 1619.16
INFO:tensorflow:loss = 0.15148035, step = 801 (0.062 sec)
INFO:tensorflow:global_step/sec: 1600.48
INFO:tensorflow:loss = 0.13629705, step = 901 (0.063 sec)
INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpyu6blxd6/model.ckpt.
INFO:tensorflow:Loss for final step: 0.12395525.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from /tmp/tmpyu6blxd6/model.ckpt-1000
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
{'logits': array([-2.8130803], dtype=float32), 'logistic': array([0.05662142], dtype=float32), 'probabilities': array([0.94337857, 0.05662142], dtype=float32), 'class_ids': array([0]), 'classes': array([b'0'], dtype=object)}
{'logits': array([2.2674427], dtype=float32), 'logistic': array([0.90614456], dtype=float32), 'probabilities': array([0.09385548, 0.90614456], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)}
{'logits': array([1.0914161], dtype=float32), 'logistic': array([0.7486483], dtype=float32), 'probabilities': array([0.2513517, 0.7486483], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)}
{'logits': array([-2.8130803], dtype=float32), 'logistic': array([0.05662142], dtype=float32), 'probabilities': array([0.94337857, 0.05662142], dtype=float32), 'class_ids': array([0]), 'classes': array([b'0'], dtype=object)}
```
But if I return a sample tuple, the output whould stop like this:
```
...
...
...
INFO:tensorflow:Done running local_init_op.
{'logits': array([-2.8130803], dtype=float32), 'logistic': array([0.05662142], dtype=float32), 'probabilities': array([0.94337857, 0.05662142], dtype=float32), 'class_ids': array([0]), 'classes': array([b'0'], dtype=object)}
{'logits': array([2.2674427], dtype=float32), 'logistic': array([0.90614456], dtype=float32), 'probabilities': array([0.09385548, 0.90614456], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)}
{'logits': array([1.0914161], dtype=float32), 'logistic': array([0.7486483], dtype=float32), 'probabilities': array([0.2513517, 0.7486483], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)}
{'logits': array([-2.8130803], dtype=float32), 'logistic': array([0.05662142], dtype=float32), 'probabilities': array([0.94337857, 0.05662142], dtype=float32), 'class_ids': array([0]), 'classes': array([b'0'], dtype=object)}
{'logits': array([-2.8130803], dtype=float32), 'logistic': array([0.05662142], dtype=float32), 'probabilities': array([0.94337857, 0.05662142], dtype=float32), 'class_ids': array([0]), 'classes': array([b'0'], dtype=object)}
{'logits': array([2.2674427], dtype=float32), 'logistic': array([0.90614456], dtype=float32), 'probabilities': array([0.09385548, 0.90614456], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)}
{'logits': array([1.0914161], dtype=float32), 'logistic': array([0.7486483], dtype=float32), 'probabilities': array([0.2513517, 0.7486483], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)}
{'logits': array([-2.8130803], dtype=float32), 'logistic': array([0.05662142], dtype=float32), 'probabilities': array([0.94337857, 0.05662142], dtype=float32), 'class_ids': array([0]), 'classes': array([b'0'], dtype=object)}
{'logits': array([-2.8130803], dtype=float32), 'logistic': array([0.05662142], dtype=float32), 'probabilities': array([0.94337857, 0.05662142], dtype=float32), 'class_ids': array([0]), 'classes': array([b'0'], dtype=object)}
{'logits': array([2.2674427], dtype=float32), 'logistic': array([0.90614456], dtype=float32), 'probabilities': array([0.09385548, 0.90614456], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)}
{'logits': array([1.0914161], dtype=float32), 'logistic': array([0.7486483], dtype=float32), 'probabilities': array([0.2513517, 0.7486483], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)}
{'logits': array([-2.8130803], dtype=float32), 'logistic': array([0.05662142], dtype=float32), 'probabilities': array([0.94337857, 0.05662142], dtype=float32), 'class_ids': array([0]), 'classes': array([b'0'], dtype=object)}
.....more
```
"
19191,GPU cannot use in C++ API when using libtensorflow_cc.so,"My purpose is using C++ api with libtensorflow_cc.so for detection. 

First , compile the libtensorflow_cc.so. 
I do like;
1   ./configure.       I choose CUDA support, and cuda 9.1 and cudnn 7 is used.
2 bazel the tensorflow .     bazel build -c opt --config=cuda //tensorlfow:libtensorflow_cc.so.
At last ,complete sucessfully.

My code like:

#include <tensorflow/core/protobuf/meta_graph.pb.h>
#include <tensorflow/core/public/session.h>
#include <tensorflow/core/graph/default_device.h>
#include <tensorflow/core/graph/graph_def_builder.h>

tensorflow::GraphDef GraphDef;
tensorflow::Session* Session = nullptr;

1 void LoadGraph()
2 {
3 // Read in the protobuf graph we exported
4 tensorflow::Status Status;
5
6 Status = tensorflow::ReadBinaryProto(tensorflow::Env::Default(), ""my_model.pb"", &GraphDef);
7 if (!Status.ok())
8 {
9 printf(""Error reading graph definition from %s: %s\n"", ""my_model.pb"", Status.ToString().c_str());
10 return false;
11 }
12
13 Session = tensorflow::NewSession(tensorflow::SessionOptions());
14 if (Session == nullptr)
15 {
16 printf(""Could not create Tensorflow session.\n"");
17 return false;
18 }
19
20 graph::SetDefaultDevice(""/device:GPU:0"",&GraphDef);

21 // Add the graph to the session
22 Status = Session->Create(GraphDef);
23 if (!Status.ok())
24 {
25 printf(""Error creating graph: %s\n"", Status.ToString().c_str());
26 return false;
27 }
28 }

But the code will be get an error.   The line 25 will be run.  That is say in line 23, Status.ok() = false.

Segmentation fault(core dumped)..   I do not know how to deal with it.

Can anyone help me? Thank you very much!  Please to me soon !

"
19186,[tf.keras] Bug with Stateful Metrics & Fit Generator,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: n/a

The stateful metrics integration with fit generator is not working. This can be demonstrated by taking the Keras metrics tests from the latest release and changing the imports to TensorFlow.keras (see https://github.com/keras-team/keras/blob/master/tests/keras/metrics_test.py):

```
import tensorflow as tf
from tensorflow.python.keras import backend as K

import numpy as np

class BinaryTruePositives(tf.keras.layers.Layer):
    """"""Stateful Metric to count the total true positives over all batches.
    Assumes predictions and targets of shape `(samples, 1)`.
    # Arguments
        name: String, name for the metric.
    """"""

    def __init__(self, name='true_positives', **kwargs):
        super(BinaryTruePositives, self).__init__(name=name, **kwargs)
        self.stateful = True
        self.true_positives = K.variable(value=0, dtype='int32')

    def reset_states(self):
        K.set_value(self.true_positives, 0)

    def __call__(self, y_true, y_pred):
        """"""Computes the number of true positives in a batch.
        # Arguments
            y_true: Tensor, batch_wise labels
            y_pred: Tensor, batch_wise predictions
        # Returns
            The total number of true positives seen this epoch at the
                completion of the batch.
        """"""
        y_true = K.cast(y_true, 'int32')
        y_pred = K.cast(K.round(y_pred), 'int32')
        correct_preds = K.cast(K.equal(y_pred, y_true), 'int32')
        true_pos = K.cast(K.sum(correct_preds * y_true), 'int32')
        current_true_pos = self.true_positives * 1
        self.add_update(K.update_add(self.true_positives,
                                     true_pos),
                        inputs=[y_true, y_pred])
        return current_true_pos + true_pos

# Test on simple model
inputs = tf.keras.Input(shape=(2,))
outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='out')(inputs)
model = tf.keras.Model(inputs, outputs)

model.compile(optimizer='sgd',
              loss='binary_crossentropy',
              metrics=['acc', BinaryTruePositives()])

samples = 1000
x = np.random.random((samples, 2))
y = np.random.randint(2, size=(samples, 1))

val_samples = 10
val_x = np.random.random((val_samples, 2))
val_y = np.random.randint(2, size=(val_samples, 1))

# Test fit and evaluate
history = model.fit(x, y, validation_data=(val_x, val_y), epochs=2, batch_size=10)
outs = model.evaluate(x, y, batch_size=10)
preds = model.predict(x)

def ref_true_pos(y_true, y_pred):
    return np.sum(np.logical_and(y_pred > 0.5, y_true == 1))

# Test correctness (e.g. updates should have been run)
np.testing.assert_allclose(outs[2], ref_true_pos(y, preds), atol=1e-5)

# Test correctness of the validation metric computation
val_preds = model.predict(val_x)
val_outs = model.evaluate(val_x, val_y, batch_size=10)
np.testing.assert_allclose(val_outs[2], ref_true_pos(val_y, val_preds), atol=1e-5)
np.testing.assert_allclose(val_outs[2], history.history['val_true_positives'][-1], atol=1e-5)

# Test with generators
gen = [(np.array([x0]), np.array([y0])) for x0, y0 in zip(x, y)]
val_gen = [(np.array([x0]), np.array([y0])) for x0, y0 in zip(val_x, val_y)]
history = model.fit_generator(iter(gen), epochs=1, steps_per_epoch=samples,
                              validation_data=iter(val_gen), validation_steps=val_samples)
outs = model.evaluate_generator(iter(gen), steps=samples)
preds = model.predict_generator(iter(gen), steps=samples)

# Test correctness of the metric re ref_true_pos()
np.testing.assert_allclose(outs[2], ref_true_pos(y, preds), atol=1e-5)

# Test correctness of the validation metric computation
val_preds = model.predict_generator(iter(val_gen), steps=val_samples)
val_outs = model.evaluate_generator(iter(val_gen), steps=val_samples)
np.testing.assert_allclose(val_outs[2], ref_true_pos(val_y, val_preds), atol=1e-5)
np.testing.assert_allclose(val_outs[2], history.history['val_true_positives'][-1], atol=1e-5)
```

In addition the progress bar is not working with fit generator. @fchollet 

"
19184,tf.image.crop_and_resize() return a wrong  values on the Jetson TX2,"same as https://github.com/tensorflow/tensorflow/issues/13890
And I have tried tensorflow 1.4.0 , tensorflow 1.4.1 ,tensorflow 1.7 , all of them got a wrong value,and when I use cpu to calculate it ，it returns a right value .
"
19182,pandas_input_fn taking a DataFrame or dict of Series for y,"# Pandas Input Function for Multilabel Inputs

`tf.estimator.inputs.pandas_input_fn` should be able to take a DataFarme or a dict with key to Series for y. This would allow multi-label inputs. This is similar to the numpy_input_fn behavior implemented in issue #12610.

I'd be happy to give the implementation a shot."
19181,Arrow format for Tensor import/export for cross-platform usage,"This is regarding a feature request in Tensorflow.
Is there any support currently (or will be in the future) for Tensor porting using [Apache Arrow](https://arrow.apache.org/) for cross-platform usage (e.g., Spark-Tensorflow) in python/java?

"
19180,tf.constant() processes float16 100 times slower than float32,"```python
import tensorflow as tf
import numpy as np
import time

images = tf.keras.datasets.mnist.load_data()[0][0]

imagesFloat16 = images.astype(np.float16);
imagesFloat32 = images.astype(np.float32);

start = time.time()
tf.data.Dataset.from_tensor_slices(imagesFloat32)
end = time.time()
print(""float32: {0:.4f} sec"".format(end - start)) # float32: 1.2561 sec

start = time.time()
tf.data.Dataset.from_tensor_slices(imagesFloat16)
end = time.time()
print(""float16: {0:.4f} sec"".format(end - start)) # float16: 110.0273 sec
```

> float32: 1.2561 sec
> float16: 110.0273 sec"
19178,Race condition when initializing variables in control_dependencies,"### System information

```
== cat /etc/issue ===============================================
Darwin Mikes-MBP.enversion.com 16.7.0 Darwin Kernel Version 16.7.0: Mon Nov 13 21:56:25 PST 2017; root:xnu-3789.72.11~1/RELEASE_X86_64 x86_64
Mac OS X 10.12.6

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.0.0 (clang-900.0.39.2)
Target: x86_64-apple-darwin16.7.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

== uname -a =====================================================
Darwin Mikes-MBP.enversion.com 16.7.0 Darwin Kernel Version 16.7.0: Mon Nov 13 21:56:25 PST 2017; root:xnu-3789.72.11~1/RELEASE_X86_64 x86_64

== check pips ===================================================

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```

### Describe the problem

Given how `Variable` and `control_dependencies` are documented, I would expect the following to work:

```
with tf.Session() as sess:
    v = tf.Variable(tf.zeros([100,100]))
    with tf.control_dependencies([v.initializer]):
        d = tf.Print(v, [v])
    print(sess.run(d))
```

But this seems to create a race condition. Sometimes it executes fine, sometimes it errors with `FailedPreconditionError: Attempting to use uninitialized value Variable`. It is unclear to me if the use of initializers in `control_dependencies` is valid. If it is not, the documentation should probably better reflect this.

### Source code / logs

Full traceback:
```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1322, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value Variable
         [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[""loc:@Variable/Assign""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Variable)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test.py"", line 105, in <module>
    print(sess.run(d))
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value Variable
         [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[""loc:@Variable/Assign""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Variable)]]

Caused by op 'Variable/read', defined at:
  File ""test.py"", line 101, in <module>
    v = tf.Variable(tf.zeros([100,100]))
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 235, in __init__
    constraint=constraint)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 397, in _init_from_args
    self._snapshot = array_ops.identity(self._variable, name=""read"")
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 142, in identity
    return gen_array_ops.identity(input, name=name)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3187, in identity
    ""Identity"", input=input, name=name)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

FailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable
         [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[""loc:@Variable/Assign""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Variable)]]
```
"
19176,selu alpha: why a different value from the ref paper?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **TensorFlow version (use command below)**: 1.8.0

### Describe the problem

In the source, I see that the value of the `alpha` constant chosen for the `selu` activation is 1.7580993408473768599402175208123.

However, in the [reference paper ](https://arxiv.org/abs/1706.02515) given in the [documentation](https://www.tensorflow.org/api_docs/python/tf/nn/selu), the value is 1.673263242354377.

Why the difference? If there is published work about this update could you please add it to the documentation. If not could you please motivate this choice?

### Source code / logs

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/relu_op_functor.h#L139
"
19175,1-D convolution of tensorflow do not support float64 exactly.,"In the description of tf.nn.conv1d, it said ""value: A 3D Tensor. Must be of type float32 or float64"". But the float64 isn't supported correctly. Some one could try the code below:

import numpy as np
import tensorflow as tf
a = np.random.randn(12, 228, 1)
w = np.random.randn(2, 1, 18)
sess = tf.Session()
c = sess.run(tf.nn.conv1d(tf.cast(a, tf.float64), tf.cast(w, tf.float64), stride=1, padding='SAME'))
sess.close()

I have test it in tensorflow 1.4 and 1.7,  they raised the error : 
Value passed to parameter 'input' has DataType float64 not in list of allowed values: float16, float32"
19174,GPU not used and failed call to cuInit: CUDA_ERROR_NO_DEVICE,"-----------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.4.0
- **Python version**: 2.7.14
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:8.0/6.0
- **GPU model and memory**:GeForce GTX 1080 Ti - 12GB
- **NVIDIA driver**: 390.48

Hi, when I test tensorflow in my machine, run such code : 
```
import tensorflow as tf
import os
a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
c = tf.matmul(a, b)
os.environ['CUDA_VISIBLE_DEVICES'] = ""0""
sess = tf.Session()
print (sess.run(c))
sess.close()
```
And output are as follow:
```
2018-05-09 14:01:26.787648: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-05-09 14:01:26.787669: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-05-09 14:01:26.787673: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-05-09 14:01:26.787676: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-05-09 14:01:26.787679: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-05-09 14:01:26.788347: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE
2018-05-09 14:01:26.788365: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: axadl-System-Product-Name
2018-05-09 14:01:26.788369: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: axadl-System-Product-Name
2018-05-09 14:01:26.788387: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Invalid argument: expected %d.%d or %d.%d.%d form for driver version; got ""1""
2018-05-09 14:01:26.788403: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:369] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  390.48  Thu Mar 22 00:42:57 PDT 2018
GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9) 
""""""
2018-05-09 14:01:26.788413: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 390.48.0
[[22. 28.]
 [49. 64.]]
```

And I input ""nvidia-smi"":
![default](https://user-images.githubusercontent.com/28421690/39800867-e1919820-539b-11e8-8403-ea41e672f010.png)
And I input ""nvcc -V"":
![default](https://user-images.githubusercontent.com/28421690/39800880-f265c450-539b-11e8-9f34-2ba41e1bc653.png)
Then I test cuda ""sudo ./1_Utilities/deviceQuery/deviceQuery"": 
```
./1_Utilities/deviceQuery/deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: ""GeForce GTX 1080 Ti""
  CUDA Driver Version / Runtime Version          9.1 / 8.0
  CUDA Capability Major/Minor version number:    6.1
  Total amount of global memory:                 11170 MBytes (11713052672 bytes)
  (28) Multiprocessors, (128) CUDA Cores/MP:     3584 CUDA Cores
  GPU Max Clock rate:                            1683 MHz (1.68 GHz)
  Memory Clock rate:                             5505 Mhz
  Memory Bus Width:                              352-bit
  L2 Cache Size:                                 2883584 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)
  Run time limit on kernels:                     Yes
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Disabled
  Device supports Unified Addressing (UVA):      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 2 / 0
  Compute Mode:
     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.1, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GTX 1080 Ti
Result = PASS
```

In addition, I have tried https://github.com/tensorflow/tensorflow/issues/255, but it still occur mistakes as above.
Can anyone help me?
Thanks so much!"
19173,Tensorflow AttributeError: module 'bottleneck' has no attribute '__version__',"**Tensorflow version==1.7**
**Tensorhub version==1.0**

**I am trying to learn tensorflow serving using small tutorial on [Medium blog](https://medium.com/@utsumukiMutsuki/using-inception-v3-from-tensorflow-hub-for-transfer-learning-a931ff884526)  as soon as I am running the basic program its returning this error.**

  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *  # pylint: disable=redefined-builtin
  File ""/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 82, in <module>
    from tensorflow.python.estimator import estimator_lib as estimator
  File ""/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator_lib.py"", line 37, in <module>
    from tensorflow.python.estimator.inputs import inputs
  File ""/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/inputs.py"", line 22, in <module>
    from tensorflow.python.estimator.inputs.numpy_io import numpy_input_fn
  File ""/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/numpy_io.py"", line 26, in <module>
    from tensorflow.python.estimator.inputs.queues import feeding_functions
  File ""/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py"", line 40, in <module>
    import pandas as pd
  File ""/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/pandas/__init__.py"", line 42, in <module>
    from pandas.core.api import *
  File ""/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/pandas/core/api.py"", line 9, in <module>
    from pandas.core.categorical import Categorical
  File ""/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/pandas/core/categorical.py"", line 36, in <module>
    from pandas.core.base import (PandasObject,
  File ""/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/pandas/core/base.py"", line 20, in <module>
    import pandas.core.nanops as nanops
  File ""/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/pandas/core/nanops.py"", line 30, in <module>
    ver = bn.__version__
AttributeError: module 'bottleneck' has no attribute '__version__'
"
19172,how to parse training examples with 2 dimension in csv file with tf.decode_csv,"I used tf.decode_csv in tensorflow as decoder to parse training examples in a tab-delimited file into cnn models. For every training example, the features are 2 dimensions (100 columns, 2000 rows). After reading the document in tensorflow official site, I still have two questions.

1. how to create record_defaults for my case (2 dimensional data) and how to stack the features of 2 dimensionality? The following is my code to do that, but it gave me errors.

filename_queue = tf.train.string_input_producer([file], num_epochs)

key, value = tf.TextLineReader().read(filename_queue)

record_defaults = [[1.0] for col in range(0, 100)]

content = tf.decode_csv(value, record_defaults = record_defaults, field_delim = '\t')

features = tf.stack(content[0:100])

 2. I am doing binary (0, 1) classification. Where do I put the labels for training examples? in the 2001th row? (For every training example, the first 2000 rows for features, and the 2001th row for label)

Thanks for your time!


Have I written custom code    Yes
OS Platform and Distribution    ubuntu 16.04 LTS
TensorFlow installed from          pip3 install tensorflow-gpu
TensorFlow version       the latest version
Bazel version     NA
CUDA/cuDNN version     CUDA Toolkit 9.0  
GPU model and memory     GeForce GTX 1080
Exact command to reproduce   NA
"
19171,Obtaining different results on declaring unused placeholders,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: GeForce GTX 1080 Ti - 12GB
- **Exact command to reproduce**: [Code link](https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5)

### Describe the problem
I am getting different results on declaring some unused placeholders in the code even with the same seed. After setting the same seed in both the files, the results obtained are different although the only difference in the files is an addition of some unused placeholders in [placeholder_reproduce_unusedplc.py](https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5#file-placeholder_reproduce_unusedplc-py). I am unable to understand the cause of this difference because the input and the computational graph is same for both the files. Also, with different seeds, sometimes the code with additional placeholders gives worse results.

Difference between the two files (Line #52-#57 [placeholder_reproduce_unusedplc.py](https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5#file-placeholder_reproduce_unusedplc-py)):
```
X1 = tf.placeholder(""float"", [None, timesteps, num_input])
Y1 = tf.placeholder(""float"", [None, num_classes])
X2 = tf.placeholder(""float"", [None, timesteps, num_input])
Y2 = tf.placeholder(""float"", [None, num_classes])
X3 = tf.placeholder(""float"", [None, timesteps, num_input])
Y3 = tf.placeholder(""float"", [None, num_classes])
X4 = tf.placeholder(""float"", [None, timesteps, num_input])
Y4 = tf.placeholder(""float"", [None, num_classes])
```
### Source code / logs
Running [placeholder_reproduce.py](https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5#file-placeholder_reproduce-py) gives (same everytime):

> Step 1, Minibatch Loss= 2.5037, Training Accuracy= 0.164
> Step 200, Minibatch Loss= 2.1293, Training Accuracy= 0.305
> Step 400, Minibatch Loss= 1.8812, Training Accuracy= 0.453
> Step 600, Minibatch Loss= 1.8287, Training Accuracy= 0.398
> Step 800, Minibatch Loss= 1.7506, Training Accuracy= 0.430
> Step 1000, Minibatch Loss= 1.6265, Training Accuracy= 0.508
> Step 1200, Minibatch Loss= 1.4767, Training Accuracy= 0.516
> Step 1400, Minibatch Loss= 1.4777, Training Accuracy= 0.547
> Step 1600, Minibatch Loss= 1.3804, Training Accuracy= 0.586
> Step 1800, Minibatch Loss= 1.2695, Training Accuracy= 0.672
> Step 2000, Minibatch Loss= 1.2690, Training Accuracy= 0.594
> Step 2200, Minibatch Loss= 1.2799, Training Accuracy= 0.570
> Step 2400, Minibatch Loss= 1.1228, Training Accuracy= 0.680
> Step 2600, Minibatch Loss= 1.0570, Training Accuracy= 0.695
> Step 2800, Minibatch Loss= 1.0974, Training Accuracy= 0.625
> Step 3000, Minibatch Loss= 1.1049, Training Accuracy= 0.641
> Step 3200, Minibatch Loss= 1.0421, Training Accuracy= 0.688
> Step 3400, Minibatch Loss= 1.0742, Training Accuracy= 0.672
> Step 3600, Minibatch Loss= 1.0896, Training Accuracy= 0.648
> Step 3800, Minibatch Loss= 0.8617, Training Accuracy= 0.734
> Step 4000, Minibatch Loss= 0.9472, Training Accuracy= 0.719
> Step 4200, Minibatch Loss= 0.7870, Training Accuracy= 0.773
> Step 4400, Minibatch Loss= 0.9419, Training Accuracy= 0.656
> Step 4600, Minibatch Loss= 0.7562, Training Accuracy= 0.750
> Step 4800, Minibatch Loss= 0.8001, Training Accuracy= 0.758
> Step 5000, Minibatch Loss= 0.9771, Training Accuracy= 0.711
> Step 5200, Minibatch Loss= 0.7151, Training Accuracy= 0.805
> Step 5400, Minibatch Loss= 0.7159, Training Accuracy= 0.789
> Step 5600, Minibatch Loss= 0.7710, Training Accuracy= 0.797
> Step 5800, Minibatch Loss= 0.7368, Training Accuracy= 0.766
> Step 6000, Minibatch Loss= 0.7593, Training Accuracy= 0.758
> Step 6200, Minibatch Loss= 0.6506, Training Accuracy= 0.773
> Step 6400, Minibatch Loss= 0.8102, Training Accuracy= 0.766
> Step 6600, Minibatch Loss= 0.5647, Training Accuracy= 0.820
> Step 6800, Minibatch Loss= 0.6096, Training Accuracy= 0.828
> Step 7000, Minibatch Loss= 0.6203, Training Accuracy= 0.844
> Step 7200, Minibatch Loss= 0.5551, Training Accuracy= 0.820
> Step 7400, Minibatch Loss= 0.5007, Training Accuracy= 0.836
> Step 7600, Minibatch Loss= 0.5582, Training Accuracy= 0.844
> Step 7800, Minibatch Loss= 0.6226, Training Accuracy= 0.789
> Step 8000, Minibatch Loss= 0.5149, Training Accuracy= 0.812
> Step 8200, Minibatch Loss= 0.5257, Training Accuracy= 0.844
> Step 8400, Minibatch Loss= 0.4988, Training Accuracy= 0.844
> Step 8600, Minibatch Loss= 0.5633, Training Accuracy= 0.805
> Step 8800, Minibatch Loss= 0.4969, Training Accuracy= 0.797
> Step 9000, Minibatch Loss= 0.4822, Training Accuracy= 0.844
> Step 9200, Minibatch Loss= 0.4551, Training Accuracy= 0.844
> Step 9400, Minibatch Loss= 0.4117, Training Accuracy= 0.906
> Step 9600, Minibatch Loss= 0.4025, Training Accuracy= 0.883
> Step 9800, Minibatch Loss= 0.5490, Training Accuracy= 0.812
> Step 10000, Minibatch Loss= 0.4854, Training Accuracy= 0.836
> Optimization Finished!
> Testing Accuracy: 0.8515625

Running [placeholder_reproduce_unusedplc.py](https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5#file-placeholder_reproduce_unusedplc-py) gives (same everytime):
> Step 1, Minibatch Loss= 3.9436, Training Accuracy= 0.055
> Step 200, Minibatch Loss= 2.0941, Training Accuracy= 0.234
> Step 400, Minibatch Loss= 1.8377, Training Accuracy= 0.422
> Step 600, Minibatch Loss= 1.7426, Training Accuracy= 0.391
> Step 800, Minibatch Loss= 1.7220, Training Accuracy= 0.383
> Step 1000, Minibatch Loss= 1.6012, Training Accuracy= 0.469
> Step 1200, Minibatch Loss= 1.4531, Training Accuracy= 0.531
> Step 1400, Minibatch Loss= 1.4427, Training Accuracy= 0.516
> Step 1600, Minibatch Loss= 1.2995, Training Accuracy= 0.594
> Step 1800, Minibatch Loss= 1.2551, Training Accuracy= 0.664
> Step 2000, Minibatch Loss= 1.2198, Training Accuracy= 0.648
> Step 2200, Minibatch Loss= 1.2236, Training Accuracy= 0.609
> Step 2400, Minibatch Loss= 1.0227, Training Accuracy= 0.680
> Step 2600, Minibatch Loss= 0.9933, Training Accuracy= 0.695
> Step 2800, Minibatch Loss= 0.9887, Training Accuracy= 0.656
> Step 3000, Minibatch Loss= 1.0707, Training Accuracy= 0.578
> Step 3200, Minibatch Loss= 0.9540, Training Accuracy= 0.680
> Step 3400, Minibatch Loss= 0.9850, Training Accuracy= 0.695
> Step 3600, Minibatch Loss= 1.0233, Training Accuracy= 0.664
> Step 3800, Minibatch Loss= 0.8106, Training Accuracy= 0.727
> Step 4000, Minibatch Loss= 0.8919, Training Accuracy= 0.719
> Step 4200, Minibatch Loss= 0.7129, Training Accuracy= 0.773
> Step 4400, Minibatch Loss= 0.8317, Training Accuracy= 0.734
> Step 4600, Minibatch Loss= 0.6762, Training Accuracy= 0.789
> Step 4800, Minibatch Loss= 0.6951, Training Accuracy= 0.773
> Step 5000, Minibatch Loss= 0.8617, Training Accuracy= 0.727
> Step 5200, Minibatch Loss= 0.6173, Training Accuracy= 0.828
> Step 5400, Minibatch Loss= 0.6464, Training Accuracy= 0.797
> Step 5600, Minibatch Loss= 0.6643, Training Accuracy= 0.828
> Step 5800, Minibatch Loss= 0.6795, Training Accuracy= 0.758
> Step 6000, Minibatch Loss= 0.6252, Training Accuracy= 0.812
> Step 6200, Minibatch Loss= 0.5862, Training Accuracy= 0.812
> Step 6400, Minibatch Loss= 0.7126, Training Accuracy= 0.812
> Step 6600, Minibatch Loss= 0.4639, Training Accuracy= 0.891
> Step 6800, Minibatch Loss= 0.5116, Training Accuracy= 0.859
> Step 7000, Minibatch Loss= 0.5322, Training Accuracy= 0.852
> Step 7200, Minibatch Loss= 0.4902, Training Accuracy= 0.844
> Step 7400, Minibatch Loss= 0.4027, Training Accuracy= 0.867
> Step 7600, Minibatch Loss= 0.4654, Training Accuracy= 0.875
> Step 7800, Minibatch Loss= 0.5647, Training Accuracy= 0.812
> Step 8000, Minibatch Loss= 0.4110, Training Accuracy= 0.852
> Step 8200, Minibatch Loss= 0.3976, Training Accuracy= 0.844
> Step 8400, Minibatch Loss= 0.3916, Training Accuracy= 0.867
> Step 8600, Minibatch Loss= 0.5487, Training Accuracy= 0.797
> Step 8800, Minibatch Loss= 0.4265, Training Accuracy= 0.875
> Step 9000, Minibatch Loss= 0.4590, Training Accuracy= 0.859
> Step 9200, Minibatch Loss= 0.4135, Training Accuracy= 0.906
> Step 9400, Minibatch Loss= 0.3457, Training Accuracy= 0.914
> Step 9600, Minibatch Loss= 0.4067, Training Accuracy= 0.844
> Step 9800, Minibatch Loss= 0.4394, Training Accuracy= 0.859
> Step 10000, Minibatch Loss= 0.3586, Training Accuracy= 0.875
> Optimization Finished!
> Testing Accuracy: 0.890625
"
19169,Problem of fusing CONV-BN-RELU together of TF-Lite,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.7
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**: 4.9
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: 16GB
- **Exact command to reproduce**:

### Describe the problem
BN-RELU-CONV structure is becoming more popular recently. However, for TF-Lite, if there are frequent BN-RELU-CONV structures in the graph, the (BN-RELU-CONV) * N sequence will be matched as BN-RELU-(CONV-BN-RELU) * (N-1) - CONV sequence. To make TOCO tools work, it is simply to add an additional 1x1 CONV op at the very beginning, such like conv-(BN-RELU-CONV) * N. Then, it will be  conv-BN-RELU-(CONV-BN-RELU) * (N-1) -CONV and TOCO can fuse them. However, the shapes between CONV in first block and BN-RELU in second block could be not matched. Thus, it will be crashed during the runtime. The single way to solve this problem is to add an 1x1 conv in front of every BN-RELU-CONV strcture. However, this changes too much from the original model. Thus, in my opinion, fusion of CONV-BN-RELU ops should not be hard coded. 

I am not sure that this problem only occurs in TF-Lite. It seems that the BN folding method of TF graph_transform will also match the corresponding CONV-BN structure at first.

### Source code / logs
For example, DenseNet:
Runtime Error:
> java.lang.NullPointerException: Can not allocate memory for the given inputs: third_party/tensorflow/contrib/lite/kernels/mul.cc:48 NumDimensions(input1) != NumDimensions(input2) (4 != 1)
"
19168,tf.control_dependencies() not work in a custom_gradient function,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: r1.8
- **Python version**: 2.7.14
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 8.0/7.0
- **GPU model and memory**: GTX1080, 8G
- **Bazel version**: N/A
- **Exact command to reproduce**: python test_script.py

### Describe the problem
The output for the code below, is not correct. The first value of variable ""assign_var"" is not assigned with 2. The expected output for the 2nd line is: [2 0]. Thus, the approach [here][1] cannot work with ""@tf.custom_gradient"". I don't know if this is a bug or there is a solution for this. Thanks in advance!

### Source code / logs
@tf.custom_gradient
def test_func(x):
    i = tf.constant(0)
    id_refs = tf.Variable(tf.zeros((1), tf.int32))
    assign_var = tf.Variable(tf.zeros((2), tf.int32))
    cond = lambda i, id_refs: tf.less(i, 8)

    def _body(i, id_refs):
      a = tf.assign(assign_var[0], 2)
      id_refs = tf.concat([id_refs, [i]], 0)
      with tf.control_dependencies([a]):
        return i+1, id_refs

    _, gradRefs = tf.while_loop(cond, _body, [i, id_refs], shape_invariants= \
      [i.get_shape(), tf.TensorShape([None])])
    def grad(dy):
        return tf.constant(0)
    return assign_var, grad

x = tf.constant(100.)
y = test_func(x)
dy = tf.gradients(y, x)

init_op = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init_op)
    print(sess.run(dy))
    print(sess.run(y))

Output:
[0]
[0 0]
"
19167,Windows 10 GPU JNI Compile Error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Only modified path variables
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 10
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.8 or master ( tried both)
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.10.0
- **GCC/Compiler version (if compiling from source)**: 3.6.3
- **CUDA/cuDNN version**: V9.0 / 7.0
- **GPU model and memory**: GTX 1070 6G (Notebook)
- **Exact command to reproduce**:  
msys64 :swigwin-3.0.10
tensorflow/tools/ci_build/windows/libtensorflow_gpu.sh
 

### Describe the problem
I need a GPU version of tensorflow_jni.dll but I did not find that trying to compile with an error.
Compilation is perfectly normal in cpu mode, but errors occur in gpu mode, and every time changes error * .o file

### Source code / logs
ERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: not all outputs were created or valid
ERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/javanano/javanano_enum.o' was not created
ERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/php/php_generator.o' was not created
ERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/java/java_message_field_lite.o' was not created
ERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/javanano/javanano_primitive_field.o' was not created
ERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/java/java_lazy_message_field.o' was not created
ERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/cpp/cpp_string_field.o' was not created

### Source code / logs (ALL)
wangxiaoming@MSI MINGW64 /d/projects/tensorflow
$ /d/projects/tensorflow/tensorflow/tools/ci_build/windows/libtensorflow_gpu.sh
+++ dirname /d/projects/tensorflow/tensorflow/tools/ci_build/windows/libtensorflow_gpu.sh
++ cd /d/projects/tensorflow/tensorflow/tools/ci_build/windows
++ pwd
+ SCRIPT_DIR=/d/projects/tensorflow/tensorflow/tools/ci_build/windows
+ source /d/projects/tensorflow/tensorflow/tools/ci_build/windows/bazel/common_env.sh
++ export TMPDIR=C:/tmp
++ TMPDIR=C:/tmp
++ mkdir -p C:/tmp
++ export BAZEL_SH=C:/tools/msys64/usr/bin/bash
++ BAZEL_SH=C:/tools/msys64/usr/bin/bash
++ export PYTHON_BASE_PATH=ProgramData/Anaconda3
++ PYTHON_BASE_PATH=ProgramData/Anaconda3
++ export PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/python.exe
++ PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/python.exe
++ export PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/lib/site-packages
++ PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/lib/site-packages
++ export PATH=/c/ProgramData/Anaconda3:/mingw64/bin:/usr/local/bin:/usr/bin:/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin
++ PATH=/c/ProgramData/Anaconda3:/mingw64/bin:/usr/local/bin:/usr/bin:/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin
++ export PATH=/c/ProgramData/Anaconda3/Scripts:/c/ProgramData/Anaconda3:/mingw64/bin:/usr/local/bin:/usr/bin:/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin
++ PATH=/c/ProgramData/Anaconda3/Scripts:/c/ProgramData/Anaconda3:/mingw64/bin:/usr/local/bin:/usr/bin:/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin
++ export 'PATH=/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/bin:/c/ProgramData/Anaconda3/Scripts:/c/ProgramData/Anaconda3:/mingw64/bin:/usr/local/bin:/usr/bin:/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin'
++ PATH='/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/bin:/c/ProgramData/Anaconda3/Scripts:/c/ProgramData/Anaconda3:/mingw64/bin:/usr/local/bin:/usr/bin:/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin'
++ export 'PATH=/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/extras/CUPTI/libx64:/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/bin:/c/ProgramData/Anaconda3/Scripts:/c/ProgramData/Anaconda3:/mingw64/bin:/usr/local/bin:/usr/bin:/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin'
++ PATH='/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/extras/CUPTI/libx64:/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/bin:/c/ProgramData/Anaconda3/Scripts:/c/ProgramData/Anaconda3:/mingw64/bin:/usr/local/bin:/usr/bin:/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin'
+ source /d/projects/tensorflow/tensorflow/tools/ci_build/windows/bazel/bazel_test_lib.sh
++ failing_cpu_cc_tests='    //tensorflow/core/kernels:control_flow_ops_test +     //tensorflow/core:example_example_parser_configuration_test +     //tensorflow/core:lib_core_status_test +     //tensorflow/core:lib_monitoring_collection_registry_test +     //tensorflow/core:lib_strings_numbers_test +     //tensorflow/core/platform/hadoop:hadoop_file_system_test +     //tensorflow/core:platform_file_system_test +     //tensorflow/core:platform_logging_test +     //tensorflow/core:util_sparse_sparse_tensor_test +     //tensorflow/cc:framework_gradient_checker_test +     //tensorflow/cc:framework_gradients_test +     //tensorflow/cc:gradients_array_grad_test +     //tensorflow/cc:gradients_math_grad_test +     //tensorflow/cc:gradients_nn_grad_test +     //tensorflow/cc/saved_model:loader_test '
++ broken_cpu_cc_tests='    //tensorflow/cc:framework_cc_ops_test +     //tensorflow/core/platform/cloud:time_util_test +     //tensorflow/core/platform/cloud:oauth_client_test +     //tensorflow/core/platform/cloud:http_request_test +     //tensorflow/core/platform/cloud:google_auth_provider_test +     //tensorflow/core/platform/cloud:gcs_file_system_test +     //tensorflow/core/kernels/cloud:bigquery_table_accessor_test +     //tensorflow/core/kernels/hexagon:graph_transferer_test +     //tensorflow/core/kernels:remote_fused_graph_execute_utils_test +     //tensorflow/core/kernels:requantize_op_test +     //tensorflow/core/kernels:requantization_range_op_test +     //tensorflow/core/kernels:quantized_reshape_op_test +     //tensorflow/core/kernels:quantized_pooling_ops_test +     //tensorflow/core/kernels:quantized_matmul_op_test +     //tensorflow/core/kernels:quantized_conv_ops_test +     //tensorflow/core/kernels:quantized_concat_op_test +     //tensorflow/core/kernels:quantized_bias_add_op_test +     //tensorflow/core/kernels:quantized_batch_norm_op_test +     //tensorflow/core/kernels:quantized_activation_ops_test +     //tensorflow/core/kernels:quantize_op_test +     //tensorflow/core/kernels:quantize_down_and_shrink_range_op_test +     //tensorflow/core/kernels:quantize_and_dequantize_op_test_gpu +     //tensorflow/core/kernels:quantize_and_dequantize_op_test +     //tensorflow/core/kernels:quantization_utils_test +     //tensorflow/core/kernels:debug_ops_test +     //tensorflow/core/distributed_runtime/rpc:rpc_rendezvous_mgr_test_gpu +     //tensorflow/core/distributed_runtime/rpc:rpc_rendezvous_mgr_test +     //tensorflow/core/distributed_runtime/rpc:grpc_tensor_coding_test +     //tensorflow/core/distributed_runtime/rpc:grpc_session_test_gpu +     //tensorflow/core/distributed_runtime/rpc:grpc_session_test +     //tensorflow/core/distributed_runtime/rpc:grpc_channel_test_gpu +     //tensorflow/core/distributed_runtime/rpc:grpc_channel_test +     //tensorflow/core/distributed_runtime:remote_device_test_gpu +     //tensorflow/core/distributed_runtime:remote_device_test +     //tensorflow/core/distributed_runtime:executor_test_gpu +     //tensorflow/core/distributed_runtime:executor_test +     //tensorflow/core/debug:debug_gateway_test +     //tensorflow/core/debug:debug_grpc_io_utils_test +     //tensorflow/core:util_reporter_test +     //tensorflow/core:util_memmapped_file_system_test +     //tensorflow/core:platform_subprocess_test +     //tensorflow/core:platform_profile_utils_cpu_utils_test +     //tensorflow/core:lib_jpeg_jpeg_mem_unittest +     //tensorflow/core/debug:debug_io_utils_test '
++ extra_failing_gpu_cc_tests='    //tensorflow/core:lib_core_threadpool_test +     //tensorflow/core:cuda_libdevice_path_test +     //tensorflow/core:common_runtime_direct_session_test +     //tensorflow/core:common_runtime_direct_session_with_tracking_alloc_test +     //tensorflow/core:device_tracer_test +     //tensorflow/core:ops_math_grad_test '
++ exclude_cpu_cc_tests='    //tensorflow/core/kernels:control_flow_ops_test +     //tensorflow/core:example_example_parser_configuration_test +     //tensorflow/core:lib_core_status_test +     //tensorflow/core:lib_monitoring_collection_registry_test +     //tensorflow/core:lib_strings_numbers_test +     //tensorflow/core/platform/hadoop:hadoop_file_system_test +     //tensorflow/core:platform_file_system_test +     //tensorflow/core:platform_logging_test +     //tensorflow/core:util_sparse_sparse_tensor_test +     //tensorflow/cc:framework_gradient_checker_test +     //tensorflow/cc:framework_gradients_test +     //tensorflow/cc:gradients_array_grad_test +     //tensorflow/cc:gradients_math_grad_test +     //tensorflow/cc:gradients_nn_grad_test +     //tensorflow/cc/saved_model:loader_test  +     //tensorflow/cc:framework_cc_ops_test +     //tensorflow/core/platform/cloud:time_util_test +     //tensorflow/core/platform/cloud:oauth_client_test +     //tensorflow/core/platform/cloud:http_request_test +     //tensorflow/core/platform/cloud:google_auth_provider_test +     //tensorflow/core/platform/cloud:gcs_file_system_test +     //tensorflow/core/kernels/cloud:bigquery_table_accessor_test +     //tensorflow/core/kernels/hexagon:graph_transferer_test +     //tensorflow/core/kernels:remote_fused_graph_execute_utils_test +     //tensorflow/core/kernels:requantize_op_test +     //tensorflow/core/kernels:requantization_range_op_test +     //tensorflow/core/kernels:quantized_reshape_op_test +     //tensorflow/core/kernels:quantized_pooling_ops_test +     //tensorflow/core/kernels:quantized_matmul_op_test +     //tensorflow/core/kernels:quantized_conv_ops_test +     //tensorflow/core/kernels:quantized_concat_op_test +     //tensorflow/core/kernels:quantized_bias_add_op_test +     //tensorflow/core/kernels:quantized_batch_norm_op_test +     //tensorflow/core/kernels:quantized_activation_ops_test +     //tensorflow/core/kernels:quantize_op_test +     //tensorflow/core/kernels:quantize_down_and_shrink_range_op_test +     //tensorflow/core/kernels:quantize_and_dequantize_op_test_gpu +     //tensorflow/core/kernels:quantize_and_dequantize_op_test +     //tensorflow/core/kernels:quantization_utils_test +     //tensorflow/core/kernels:debug_ops_test +     //tensorflow/core/distributed_runtime/rpc:rpc_rendezvous_mgr_test_gpu +     //tensorflow/core/distributed_runtime/rpc:rpc_rendezvous_mgr_test +     //tensorflow/core/distributed_runtime/rpc:grpc_tensor_coding_test +     //tensorflow/core/distributed_runtime/rpc:grpc_session_test_gpu +     //tensorflow/core/distributed_runtime/rpc:grpc_session_test +     //tensorflow/core/distributed_runtime/rpc:grpc_channel_test_gpu +     //tensorflow/core/distributed_runtime/rpc:grpc_channel_test +     //tensorflow/core/distributed_runtime:remote_device_test_gpu +     //tensorflow/core/distributed_runtime:remote_device_test +     //tensorflow/core/distributed_runtime:executor_test_gpu +     //tensorflow/core/distributed_runtime:executor_test +     //tensorflow/core/debug:debug_gateway_test +     //tensorflow/core/debug:debug_grpc_io_utils_test +     //tensorflow/core:util_reporter_test +     //tensorflow/core:util_memmapped_file_system_test +     //tensorflow/core:platform_subprocess_test +     //tensorflow/core:platform_profile_utils_cpu_utils_test +     //tensorflow/core:lib_jpeg_jpeg_mem_unittest +     //tensorflow/core/debug:debug_io_utils_test '
++ exclude_gpu_cc_tests='    //tensorflow/core:lib_core_threadpool_test +     //tensorflow/core:cuda_libdevice_path_test +     //tensorflow/core:common_runtime_direct_session_test +     //tensorflow/core:common_runtime_direct_session_with_tracking_alloc_test +     //tensorflow/core:device_tracer_test +     //tensorflow/core:ops_math_grad_test  +     //tensorflow/core/kernels:control_flow_ops_test +     //tensorflow/core:example_example_parser_configuration_test +     //tensorflow/core:lib_core_status_test +     //tensorflow/core:lib_monitoring_collection_registry_test +     //tensorflow/core:lib_strings_numbers_test +     //tensorflow/core/platform/hadoop:hadoop_file_system_test +     //tensorflow/core:platform_file_system_test +     //tensorflow/core:platform_logging_test +     //tensorflow/core:util_sparse_sparse_tensor_test +     //tensorflow/cc:framework_gradient_checker_test +     //tensorflow/cc:framework_gradients_test +     //tensorflow/cc:gradients_array_grad_test +     //tensorflow/cc:gradients_math_grad_test +     //tensorflow/cc:gradients_nn_grad_test +     //tensorflow/cc/saved_model:loader_test  +     //tensorflow/cc:framework_cc_ops_test +     //tensorflow/core/platform/cloud:time_util_test +     //tensorflow/core/platform/cloud:oauth_client_test +     //tensorflow/core/platform/cloud:http_request_test +     //tensorflow/core/platform/cloud:google_auth_provider_test +     //tensorflow/core/platform/cloud:gcs_file_system_test +     //tensorflow/core/kernels/cloud:bigquery_table_accessor_test +     //tensorflow/core/kernels/hexagon:graph_transferer_test +     //tensorflow/core/kernels:remote_fused_graph_execute_utils_test +     //tensorflow/core/kernels:requantize_op_test +     //tensorflow/core/kernels:requantization_range_op_test +     //tensorflow/core/kernels:quantized_reshape_op_test +     //tensorflow/core/kernels:quantized_pooling_ops_test +     //tensorflow/core/kernels:quantized_matmul_op_test +     //tensorflow/core/kernels:quantized_conv_ops_test +     //tensorflow/core/kernels:quantized_concat_op_test +     //tensorflow/core/kernels:quantized_bias_add_op_test +     //tensorflow/core/kernels:quantized_batch_norm_op_test +     //tensorflow/core/kernels:quantized_activation_ops_test +     //tensorflow/core/kernels:quantize_op_test +     //tensorflow/core/kernels:quantize_down_and_shrink_range_op_test +     //tensorflow/core/kernels:quantize_and_dequantize_op_test_gpu +     //tensorflow/core/kernels:quantize_and_dequantize_op_test +     //tensorflow/core/kernels:quantization_utils_test +     //tensorflow/core/kernels:debug_ops_test +     //tensorflow/core/distributed_runtime/rpc:rpc_rendezvous_mgr_test_gpu +     //tensorflow/core/distributed_runtime/rpc:rpc_rendezvous_mgr_test +     //tensorflow/core/distributed_runtime/rpc:grpc_tensor_coding_test +     //tensorflow/core/distributed_runtime/rpc:grpc_session_test_gpu +     //tensorflow/core/distributed_runtime/rpc:grpc_session_test +     //tensorflow/core/distributed_runtime/rpc:grpc_channel_test_gpu +     //tensorflow/core/distributed_runtime/rpc:grpc_channel_test +     //tensorflow/core/distributed_runtime:remote_device_test_gpu +     //tensorflow/core/distributed_runtime:remote_device_test +     //tensorflow/core/distributed_runtime:executor_test_gpu +     //tensorflow/core/distributed_runtime:executor_test +     //tensorflow/core/debug:debug_gateway_test +     //tensorflow/core/debug:debug_grpc_io_utils_test +     //tensorflow/core:util_reporter_test +     //tensorflow/core:util_memmapped_file_system_test +     //tensorflow/core:platform_subprocess_test +     //tensorflow/core:platform_profile_utils_cpu_utils_test +     //tensorflow/core:lib_jpeg_jpeg_mem_unittest +     //tensorflow/core/debug:debug_io_utils_test '
+ cd /d/projects/tensorflow/tensorflow/tools/ci_build/windows/../../../..
+ '[' '!' -e WORKSPACE ']'
+ export TF_BAZEL_TARGETS=//tensorflow:libtensorflow.so
+ TF_BAZEL_TARGETS=//tensorflow:libtensorflow.so
+ export 'TF_BAZEL_TARGETS=//tensorflow:libtensorflow.so //tensorflow/tools/lib_package:clicenses_generate'
+ TF_BAZEL_TARGETS='//tensorflow:libtensorflow.so //tensorflow/tools/lib_package:clicenses_generate'
+ export 'TF_BAZEL_TARGETS=//tensorflow:libtensorflow.so //tensorflow/tools/lib_package:clicenses_generate //tensorflow/java:libtensorflow_jni.so'
+ TF_BAZEL_TARGETS='//tensorflow:libtensorflow.so //tensorflow/tools/lib_package:clicenses_generate //tensorflow/java:libtensorflow_jni.so'
+ export 'TF_BAZEL_TARGETS=//tensorflow:libtensorflow.so //tensorflow/tools/lib_package:clicenses_generate //tensorflow/java:libtensorflow_jni.so //tensorflow/tools/lib_package:jnilicenses_generate'
+ TF_BAZEL_TARGETS='//tensorflow:libtensorflow.so //tensorflow/tools/lib_package:clicenses_generate //tensorflow/java:libtensorflow_jni.so //tensorflow/tools/lib_package:jnilicenses_generate'
+ run_configure_for_gpu_build
+ export TF_NEED_CUDA=1
+ TF_NEED_CUDA=1
+ export TF_CUDA_VERSION=9.0
+ TF_CUDA_VERSION=9.0
+ export 'CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0'
+ CUDA_TOOLKIT_PATH='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0'
+ export TF_CUDNN_VERSION=7.0
+ TF_CUDNN_VERSION=7.0
+ export 'CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0'
+ CUDNN_INSTALL_PATH='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0'
+ export TF_CUDA_COMPUTE_CAPABILITIES=6.1
+ TF_CUDA_COMPUTE_CAPABILITIES=6.1
+ '[' -z '' ']'
+ export TF_ENABLE_XLA=0
+ TF_ENABLE_XLA=0
+ export TF_NEED_VERBS=0
+ TF_NEED_VERBS=0
+ export TF_NEED_MKL=0
+ TF_NEED_MKL=0
+ export TF_NEED_GCP=0
+ TF_NEED_GCP=0
+ export TF_NEED_HDFS=0
+ TF_NEED_HDFS=0
+ export TF_NEED_OPENCL_SYCL=0
+ TF_NEED_OPENCL_SYCL=0
+ export USE_MSVC_WRAPPER=1
+ USE_MSVC_WRAPPER=1
+ echo ''
+ ./configure
WARNING: Running Bazel server needs to be killed, because the startup options are different.
You have bazel 0.10.0 installed.
Invalid python path: C:/ProgramData/Anaconda3/python cannot be found.
Please specify the location of python. [Default is C:\ProgramData\Anaconda3\python.exe]:

Do you wish to build TensorFlow with GDR support? [y/N]: No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with MPI support? [y/N]: No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]:

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
Configuration finished
+ bazel build -c opt --copt=/arch:AVX tensorflow:libtensorflow.so tensorflow/tools/lib_package:clicenses_generate tensorflow/java:libtensorflow_jni.so tensorflow/tools/lib_package:jnilicenses_generate
...................
Loading:
Loading: 0 packages loaded
WARNING: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/WORKSPACE:1: Workspace name in C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions
DEBUG: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:197:9: @//tensorflow/tools/lib_package:libtensorflow_jni: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:197:9: @//tensorflow/tools/lib_package:common_deps: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:197:9: @//tensorflow/tools/lib_package:cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:197:9: @//tensorflow/tools/lib_package:clib: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:197:9: @//tensorflow/tools/lib_package:clicenses: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
Analyzing: 4 targets (6 packages loaded)
Analyzing: 4 targets (42 packages loaded)
Analyzing: 4 targets (55 packages loaded)
Analyzing: 4 targets (59 packages loaded)
Analyzing: 4 targets (66 packages loaded)
WARNING: D:/projects/tensorflow/tensorflow/core/BUILD:1895:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in D:/projects/tensorflow/tensorflow/tensorflow.bzl:1181:30
WARNING: D:/projects/tensorflow/tensorflow/core/BUILD:1895:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in D:/projects/tensorflow/tensorflow/tensorflow.bzl:1181:30
INFO: Analysed 4 targets (70 packages loaded).
INFO: Found 4 targets...
Building: no action
[0 / 24] [-----] BazelWorkspaceStatusAction stable-status.txt
INFO: From Compiling external/protobuf_archive/src/google/protobuf/compiler/java/java_doc_comment.cc [for host]:
▒÷▒: cl [ ѡ▒▒... ] ▒ļ▒▒▒... [ /link ▒▒▒▒ѡ▒▒... ]
▒▒▒▒ x64 ▒▒ Microsoft (R) C/C++ ▒Ż▒▒▒▒▒▒▒ 19.00.24215.1 ▒▒
▒▒Ȩ▒▒▒▒(C) Microsoft Corporation▒▒▒▒▒▒▒▒▒▒Ȩ▒▒▒▒

INFO: From Compiling external/protobuf_archive/src/google/protobuf/compiler/javanano/javanano_enum.cc [for host]:
▒÷▒: cl [ ѡ▒▒... ] ▒ļ▒▒▒... [ /link ▒▒▒▒ѡ▒▒... ]
▒▒▒▒ x64 ▒▒ Microsoft (R) C/C++ ▒Ż▒▒▒▒▒▒▒ 19.00.24215.1 ▒▒
▒▒Ȩ▒▒▒▒(C) Microsoft Corporation▒▒▒▒▒▒▒▒▒▒Ȩ▒▒▒▒

ERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/java/java_doc_comment.o' was not created
INFO: From Compiling external/protobuf_archive/src/google/protobuf/compiler/java/java_message_field_lite.cc [for host]:
▒÷▒: cl [ ѡ▒▒... ] ▒ļ▒▒▒... [ /link ▒▒▒▒ѡ▒▒... ]
▒▒▒▒ x64 ▒▒ Microsoft (R) C/C++ ▒Ż▒▒▒▒▒▒▒ 19.00.24215.1 ▒▒
▒▒Ȩ▒▒▒▒(C) Microsoft Corporation▒▒▒▒▒▒▒▒▒▒Ȩ▒▒▒▒

ERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: not all outputs were created or valid
ERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/javanano/javanano_enum.o' was not created
ERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/php/php_generator.o' was not created
ERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/java/java_message_field_lite.o' was not created
ERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/javanano/javanano_primitive_field.o' was not created
ERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/java/java_lazy_message_field.o' was not created
ERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/cpp/cpp_string_field.o' was not created
INFO: Elapsed time: 11.237s, Critical Path: 0.46s
FAILED: Build did NOT complete successfully

"
19166,densenet.pb is invalid protobuf,"`densenet.pb` in [densenet_2018_04_27.tgz](https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/densenet_2018_04_27.tgz) linked in [models.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md) seems to be not a `GraphDef`?

```
from tensorflow.core.framework import graph_pb2
from google.protobuf import text_format

with open('densenet.pb', 'rb') as input_file:
    graph_def = graph_pb2.GraphDef()
    graph_def.ParseFromString(input_file.read())
```

```
    graph_def.ParseFromString(input_file.read())
google.protobuf.message.DecodeError: Error parsing message
```"
19163,Memory leak in tensorflow newsession c++ api,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.8
- **Python version**:  3.5
- **Bazel version (if compiling from source)**:0.12
- **GCC/Compiler version (if compiling from source)**:7.3
- **CUDA/cuDNN version**: 9.1.85,7.1.3
- **GPU model and memory**: Titan x geforce
- **Exact command to reproduce**:

Code:
```
class Temp
{
 public:
  Temp(){
std::unique_ptr<tensorflow::Session> session(
      tensorflow::NewSession(tensorflow::SessionOptions()));
};
};

int main(int argc, char **argv)
{
  Temp temp;
  return 0;
}
```

### Describe the problem
I am just trying to create tensorflow session and getting memory leak.

### Source code / logs: (Valgrind)
```
==13704== 4,391 (24 direct, 4,367 indirect) bytes in 1 blocks are definitely lost in loss record 87,919 of 88,141
==13704==    at 0x4C30216: operator new(unsigned long) (vg_replace_malloc.c:334)
==13704==    by 0x73D47A7: tensorflow::RegisterXlaDeviceKernels(char const*, char const*) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)
==13704==    by 0x6CD32C1: tensorflow::XlaCpuDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)
==13704==    by 0xBAA034C: tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)
==13704==    by 0xB93A246: tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)
==13704==    by 0xBAED531: tensorflow::NewSession(tensorflow::SessionOptions const&) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)
==13704==    by 0x4E5E1D5: Temp::Temp() (temp.cpp:7)
```


```
4,388 (24 direct, 4,364 indirect) bytes in 1 blocks are definitely lost in loss record 87,918 of 88,141
==13704==    at 0x4C30216: operator new(unsigned long) (vg_replace_malloc.c:334)
==13704==    by 0x73D47A7: tensorflow::RegisterXlaDeviceKernels(char const*, char const*) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)
==13704==    by 0x73CF019: tensorflow::XlaGpuDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)
==13704==    by 0xBAA034C: tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)
==13704==    by 0xB93A246: tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)
==13704==    by 0xBAED531: tensorflow::NewSession(tensorflow::SessionOptions const&) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)
==13704==    by 0x4E5E1D5: Temp::Temp() (temp.cpp:7)
```
"
19162,Documentation of raw_rnn is wrong,"In the [official documentation][1] of tf.nn.raw_rnn we have emit structure as the third output of loop_fn when the loop_fn is run for the first time

later on the emit_structure is used to copy tf.zeros_like(emit_structure) to the minibatch entries that are finished by `emit = tf.where(finished, tf.zeros_like(emit_structure), emit)`

my lack of understanding is: emit structure is `None` so `tf.where(finished, tf.zeros_like(emit_structure), emit)` is going to throw a ValueError as `tf.zeros_like(None)` does so. Can somebody please fill in what i am missing here?

basically even the example implementation of dynamic_rnn using raw_rnn is simple wrong
to quote exact parts:
in dynamic_rnn implementation shown
```
def loop_fn(time, cell_output, cell_state, loop_state):
  emit_output = cell_output  # == None for time == 0
```
and in raw_rnn we have:
```
time = tf.constant(0, dtype=tf.int32)
(finished, next_input, initial_state, emit_structure, loop_state) = loop_fn(
    time=time, cell_output=None, cell_state=None, loop_state=None)
.
.
.
# Emit zeros and copy forward state for minibatch entries that are finished.
  state = tf.where(finished, state, next_state)
  emit = tf.where(finished, tf.zeros_like(emit_structure), emit)
```

This will fail at the first time step itself. But i have gone through code, this is not how it is implemented. In case of None for emit_structure  cell_output is used.

raw_rnn is really powerful and once i have understood it there is no going back to dynamic_rnn or any other rnn unfolding mechanism. But the documentation is making is really tough to understand this amazing api. An improvement is in order

Have I written custom code N/A
OS Platform and Distribution windows 10
TensorFlow installed from https://www.tensorflow.org/install/install_windows
TensorFlow version 1.7
Bazel version NA
CUDA/cuDNN version NA
GPU model and memory NA
Exact command to reproduce tf.zeros_like(None)


  [1]: https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn"
19159,[Feature Request] Implementation of cosine_distance_with_negative_samples loss,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.5.0
- **Python version**: 
- **Bazel version (if compiling from source)**:No
- **GCC/Compiler version (if compiling from source)**:No
- **CUDA/cuDNN version**:CPU
- **GPU model and memory**:CPU
- **Exact command to reproduce**:Feature request

### Describe the problem
Tensorflow current does not have [CNTK's negative sampling loss](https://www.cntk.ai/pythondocs/cntk.losses.html#cntk.losses.cosine_distance_with_negative_samples). I tried to get the same functionality using tf.nn.sampled_softmax_loss, but the weights parameter of the function needs to be variable, instead of an op[If weights is an arbitrary op, I get sparse gradient warning].

Is it possible to implement this loss natively without doing an embedding lookup on weights?"
19158,TFLite memory alignment errors with some armv7 Android devices,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
  Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Built on
  Mac OS X 10.13.4 and `Linux Ubuntu 17.10`, Android NDK revision 16.1.4479499,
  targeting various Android devices with TFLite.
- **TensorFlow installed from (source or binary)**:
  Binary
- **TensorFlow version (use command below)**:
  1.7.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See [my repo, `tsob/TFLite_bug_test`](https://github.com/tsob/TFLite_bug_test) for a demonstration of this issue. Clone the repo, open in Android Studio, and run the tests.

### Describe the problem

When using TFLite to run inference on a saved `.tflite` model, we see a memory
alignment error on certain armv7 devices (_e.g._ Galaxy S3, Galaxy Nexus) which
causes crashes pretty consistently. The same app runs fine on armv8, apparently,
as well as _some_ armv7 devices (Galaxy S4).

For example, on Galaxy S3 we see:

```
D/CrashAnrDetector(  351): Build: samsung/d2uc/d2att:4.3/JSS15J/I747UCUEMJB:user/release-keys
D/CrashAnrDetector(  351): Hardware: MSM8960
D/CrashAnrDetector(  351): Revision: 16
D/CrashAnrDetector(  351): Bootloader: I747UCUEMJB
D/CrashAnrDetector(  351): Radio: unknown
D/CrashAnrDetector(  351): Kernel: Linux version 3.0.31-2024954 (se.infra@R0210-11) (gcc version 4.7 (GCC) ) #1 SMP PREEMPT Thu Oct 31 01:05:53 KST 2013
D/CrashAnrDetector(  351):
D/CrashAnrDetector(  351): *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
D/CrashAnrDetector(  351): Build fingerprint: 'samsung/d2uc/d2att:4.3/JSS15J/I747UCUEMJB:user/release-keys'
D/CrashAnrDetector(  351): Revision: '16'
D/CrashAnrDetector(  351): pid: 6048, tid: 6067, name: roidJUnitRunner  >>> com.example.tsob.tflite_bug_test <<<
D/CrashAnrDetector(  351): signal 7 (SIGBUS), code 128 (SI_KERNEL), fault addr 00000000
D/CrashAnrDetector(  351):     r0 5f7d6bd8  r1 00000004  r2 5f7f0250  r3 00000080
D/CrashAnrDetector(  351):     r4 5f6ab6e4  r5 00000000  r6 5f7d6bd8  r7 5f540888
D/CrashAnrDetector(  351):     r8 00000001  r9 5f7f0240  sl 00000000  fp 5f7f0040
D/CrashAnrDetector(  351):     ip 5f7f0040  sp 5f540830  lr 00000080  pc 5f743394  cpsr 00000030
D/CrashAnrDetector(  351):     d0  0000000000000000  d1  4082819d00000000
D/CrashAnrDetector(  351):     d2  000000004147b36c  d3  0000000041476116
D/CrashAnrDetector(  351):     d4  4113563800000000  d5  4114b6d100000000
D/CrashAnrDetector(  351):     d6  0000000000000000  d7  000000003fe5492c
D/CrashAnrDetector(  351):     d8  0000000000000000  d9  0000000000000000
D/CrashAnrDetector(  351):     d10 0000000000000000  d11 0000000000000000
D/CrashAnrDetector(  351):     d12 0000000000000000  d13 0000000000000000
D/CrashAnrDetector(  351):     d14 0000000000000000  d15 0000000000000000
D/CrashAnrDetector(  351):     d16 0000000000000000  d17 0000000000000000
D/CrashAnrDetector(  351):     d18 c0e0bc0ec1100562  d19 c167ff6a40ab03aa
D/CrashAnrDetector(  351):     d20 c07bb5aac01af9fd  d21 c0e3bae1c0916f36
D/CrashAnrDetector(  351):     d22 40bb1fa74085e307  d23 41476116c139dbc3
D/CrashAnrDetector(  351):     d24 3fc74721cad6b0ed  d25 3fc2f112df3e5244
D/CrashAnrDetector(  351):     d26 40026bb1bbb55516  d27 4000000000000000
D/CrashAnrDetector(  351):     d28 40008df2d49d41f1  d29 3fb0f4a31edab38b
D/CrashAnrDetector(  351):     d30 3ff0000000000000  d31 3f4de16b9c24a98f
D/CrashAnrDetector(  351):     scr 80000010
D/CrashAnrDetector(  351):
```

On Galaxy Nexus, we have:

```
F/libc    ( 4638): Fatal signal 7 (SIGBUS) at 0x00000000 (code=128), thread 4651 (roidJUnitRunner)
I/DEBUG   (  124): *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
I/DEBUG   (  124): Build fingerprint: 'google/mysid/toro:4.2.2/JDQ39/573038:user/release-keys'
I/DEBUG   (  124): Revision: '10'
I/DEBUG   (  124): pid: 4638, tid: 4651, name: roidJUnitRunner  >>> com.example.tsob.tflite_bug_test <<<
I/DEBUG   (  124): signal 7 (SIGBUS), code 128 (?), fault addr 00000000
I/DEBUG   (  124):     r0 5d2c6f88  r1 00000004  r2 5a01c250  r3 00000080
I/DEBUG   (  124):     r4 59e866d4  r5 00000000  r6 5d2c6f88  r7 5e61d990
I/DEBUG   (  124):     r8 00000001  r9 5a01c240  sl 00000000  fp 5a01c040
I/DEBUG   (  124):     ip 5a01c040  sp 5e61d938  lr 00000080  pc 5c97f394  cpsr 00000030
I/DEBUG   (  124):     d0  6620746f00000000  d1  732e736b726f7775
I/DEBUG   (  124):     d2  7262696c203a296e  d3  62696c2220797261
I/DEBUG   (  124):     d4  c07bb5aac01af9fd  d5  c0e3bae1c0916f36
I/DEBUG   (  124):     d6  40bb1fa74085e307  d7  41476116c139dbc3
I/DEBUG   (  124):     d8  0000000000000000  d9  0000000000000000
I/DEBUG   (  124):     d10 0000000000000000  d11 0000000000000000
I/DEBUG   (  124):     d12 0000000000000000  d13 0000000000000000
I/DEBUG   (  124):     d14 0000000000000000  d15 0000000000000000
I/DEBUG   (  124):     d16 0000000000000000  d17 0000000000000000
I/DEBUG   (  124):     d18 ff7fffffff7fffff  d19 ff7fffffff7fffff
I/DEBUG   (  124):     d20 41007c5e41512bac  d21 0000000040a5953d
I/DEBUG   (  124):     d22 41007c5e41512bac  d23 0000000040a5953d
I/DEBUG   (  124):     d24 0000000000000000  d25 0000000000000000
I/DEBUG   (  124):     d26 0000002f0000002f  d27 0000002f0000002f
I/DEBUG   (  124):     d28 0000000000000005  d29 0001000000010000
I/DEBUG   (  124):     d30 000000000000000a  d31 000000000000000e
I/DEBUG   (  124):     scr 80000090
I/DEBUG   (  124):
```


### Source code / logs
See [`tsob/TFLite_bug_test`](https://github.com/tsob/TFLite_bug_test) for relevant info, as well as the code to reproduce this issue."
19156,Tensorflow silently renaming variables on implicitly defined scope,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**:  N/A
- **GPU model and memory**:  N/A
- **Exact command to reproduce**:
```python
import tensorflow as tf

tf.logging.set_verbosity(tf.logging.DEBUG)

n_input = 5
input = tf.placeholder(tf.float32, (None, n_input), 'input')
output = tf.placeholder(tf.float32, (None, 1), 'output')
check = tf.placeholder(tf.float32, (None, 1), 'check')

with tf.variable_scope(""scope1"",
                       initializer=tf.random_normal_initializer(mean=0, stddev=1)):
    w = tf.get_variable(name=""w"", shape=(n_input, 1))
    y_predicted = tf.matmul(input, w)

cost = tf.losses.mean_squared_error(check, y_predicted)
cost = tf.reduce_mean(cost, name=""mean_squared_error"")

tf.get_default_graph().get_tensor_by_name(""mean_squared_error:0"")
```

### Describe the problem
When using losses from `tf.losses` various scopes a defined implicitly. For example for `tf.losses.mean_squared_error` the scope `mean_squared_error` is defined. If I now try to define a variable named `mean_squared_error`, e.g., for later showing results in Tensorboard, then I get a `KeyError`:
```
KeyError: ""The name 'mean_squared_error:0' refers to a Tensor which does not exist. The operation, 'mean_squared_error', does not exist in the graph.""
```
This is due to Tensorflow silently renaming my explicitly defined variable because of the implicitly defined scope (by `tf.losses.mean_squared_error`). This is rather counter intuitive and potentially can easily result in unexpected behavior. I suggest throwing an error or at least printing a warning in such a case. "
19155,[Bug] Silent memory failure,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes (see below)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Lubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.7.0-3-g024aecf414 1.7.0
- **Python version**: Python 3.5.2
 **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**:  Driver Version: 390.30 CUDA Driver Version = 9.1
- **GPU model and memory**: NVidia GTX 1080Ti (11Go)
- **Exact command to reproduce**: copy paste the given code and call the only function (if running on a  GPU with different memory please fiddle with the size of the arrays accordingly)

### Describe the problem
When run on GPU the program runs fine except results are silently zeroed for elements in the last 25% of result. It only happens when I it uses a lot of memory but instead of failing it tries to compute and silently mess the results.

### Source code / logs

```
import tensorflow as tf
def bugTensorflow():
    #bug with GTX 1080 Ti
    #Lubuntu 16.04
    #Driver Version: 390.30
    #CUDA Driver Version = 9.1
    nbpoints1 = 100000
    nbpoints2 = 4000
    dim = 3

    #works fine on cpu and bugs on second gpu as well
    #with tf.device(""/cpu:0""):
    #with tf.device(""/gpu:1""):

    with tf.device(""/gpu:0""):
        points = tf.random_normal( (nbpoints1,1,dim) )
        traj = tf.random_normal((1,nbpoints2,dim) )
        diff= points-traj
        dist = tf.reduce_sum( diff*diff,axis=-1)
    sess = tf.Session()
    with(sess.as_default()):
        _dist = sess.run(dist)
        print(""dist.shape"")
        print(dist.shape)
        print(""dist[0]"")
        print(_dist[0])
        print(""dist[75000]"")
        print(_dist[75000])
        print(""tensorflow version"")
        print(tf.__version__)
```
Results : 

> dist.shape
> (100000, 4000)
> dist[0]
> [ 3.918147   8.78706    3.0947132 ...  0.8662497  0.6857513 11.894537 ]
> dist[75000]
> [0. 0. 0. ... 0. 0. 0.]
> tensorflow version
> 1.7.0
> 




"
19151,Keras metrics raise RuntimeError with MirroredStrategy,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Docker built on nvidia/cuda:9.0-devel-ubuntu16.04
- **TensorFlow installed from**: binary
- **TensorFlow version**: v1.8.0-0-g93bc2e2072 1.8.0
- **Cuda version**: 9.0
- **Bazel version**: N/A (binary install)
- **Python version**: Python 3.5.2
- **GPU model and memory**:  4x1080 w/8G / See output from tf_env_collect.sh below
- **Exact command to reproduce**: `python3 simple_tfkeras_example.py ./simple_model/`

### Describe the problem

Trying to adapt Keras code to be distributed using `tf.contrib.distribute.MirroredStrategy`, however, using `metrics` in the Keras model causes an Exception to be raised:

```
    ""Use DistributionStrategy.update() to modify a MirroredVariable."")
RuntimeError: Use DistributionStrategy.update() to modify a MirroredVariable.
Exception ignored in: <generator object get_controller at 0x7f7be0048f68>
```

If the `strategy` is set to `None` or `OneDeviceStrategy`, the metrics perform as expected.  When `strategy` is `MirroredStrategy`, the exception is raised.  If the metrics are removed from the `model.compile` call, the code works as expected.

This appears to be a bug, or an unimplemented feature, with the understanding that `MirroredStrategy` is new in TF 1.8.

### Source code / logs
`simple_tfkeras_example.py` (slightly modified from the [examples](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/examples/simple_tfkeras_example.py))

```
# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
""""""An example tf.keras model that is trained using MirroredStrategy.""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from sys import argv
import numpy as np
import tensorflow as tf

sz = 16
nchannels = 5
sh = (sz,sz,sz,nchannels)
def input_fn():
  x = np.random.random((2,sz,sz,sz,nchannels))
  y = np.random.randint(2, size=(2,sz,sz,sz,nchannels))
  x = tf.cast(x, tf.float32)
  dataset = tf.data.Dataset.from_tensor_slices((x, y))
  dataset = dataset.repeat(10)
  dataset = dataset.batch(32)
  return dataset


def main(args):
  if len(args) < 2:
    print('You must specify  model_dir for checkpoints such as'
          ' /tmp/tfkeras_example./')
    return

  print('Using %s to store checkpoints.' % args[1])

  # These strategies work
  # strategy = None
  # strategy = tf.contrib.distribute.OneDeviceStrategy('/device:GPU:0')
  
  # These strategies fail with:
  # ""Use DistributionStrategy.update() to modify a MirroredVariable."")
  # RuntimeError: Use DistributionStrategy.update() to modify a MirroredVariable.
  # Exception ignored in: <generator object get_controller at 0x7f7be0048f68>
  strategy = tf.contrib.distribute.MirroredStrategy(
      ['/device:GPU:0', '/device:GPU:1'])
  strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=4)
  
  config = tf.estimator.RunConfig(train_distribute=strategy)
  optimizer = tf.train.GradientDescentOptimizer(0.2)

  optimizer = tf.train.AdadeltaOptimizer()
  
  model = tf.keras.Sequential()
  l = tf.keras.layers
  model.add(l.Conv3D(5,3, padding='same', activation='relu', input_shape=sh))
  # model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(10,)))
  # model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

  # works correctly with OneDeviceStrategy and MirroredStrategy
  metrics = None
  # works correctly with OneDeviceStrategy, but not with MirroredStrategy
  metrics = ['binary_accuracy']
    
  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)
  model.summary()
  tf.keras.backend.set_learning_phase(True)
  keras_estimator = tf.keras.estimator.model_to_estimator(
      keras_model=model, config=config, model_dir=args[1])

  keras_estimator.train(input_fn=input_fn, steps=1000)
  eval_result = keras_estimator.evaluate(input_fn=input_fn)
  print('Eval result: {}'.format(eval_result))

if __name__ == '__main__':
  tf.app.run(argv=argv)
```

### Output of `tf_env_collect.sh`

```
== cat /etc/issue ===============================================
Linux c036785af2e5 3.10.0-693.17.1.el7.x86_64 #1 SMP Thu Jan 25 04:11:40 CST 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.4 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux c036785af2e5 3.10.0-693.17.1.el7.x86_64 #1 SMP Thu Jan 25 04:11:40 CST 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy           1.14.3
protobuf        3.5.2.post1
tensorflow-gpu  1.8.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)
/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Tue May  8 15:47:06 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1080    Off  | 00000000:84:00.0 Off |                  N/A |
| 27%   35C    P0    39W / 180W |      0MiB /  8114MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 1080    Off  | 00000000:85:00.0 Off |                  N/A |
| 27%   37C    P0    38W / 180W |      0MiB /  8114MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX 1080    Off  | 00000000:88:00.0 Off |                  N/A |
| 27%   27C    P0    39W / 180W |      0MiB /  8114MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX 1080    Off  | 00000000:89:00.0 Off |                  N/A |
| 27%   32C    P0    38W / 180W |      0MiB /  8114MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a
```


"
19150,error while converting *.pb file to *.tflite with toco,"I used the following example to create tensorflow model: http://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/
You can download the code from here: https://github.com/sankit1/cv-tricks.com/tree/master/Tensorflow-tutorials/tutorial-2-image-classifier
Also I used ""2. Freezing the graph"" section from http://cv-tricks.com/how-to/freeze-tensorflow-models/
to create a *.pb file of my model.
I have Ubuntu 16.04 and tensorflow cpu version 1.8.0 from binary. python version is 3.5.2 and bazel 0.13.0. 
I'm tried to convert *.pb file with toco command line tool, as describe on ""Convert a TensorFlow SavedModel to TensorFlow Lite"" at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md#savedmodel
and got he following error: 
(venv) user@user-desktop:~/PycharmProjects/tensorflow_tutorial/tensorflow$ **bazel run -c opt   tensorflow/contrib/lite/toco:toco --   --savedmodel_directory=/home/user/PycharmProjects/tensorflow_tutorial/tutorial-2-image-classifier   --output_file=/home/user/PycharmProjects/tensorflow_tutorial/tutorial-2-image-classifier/dogs-cats-model.tflite**
WARNING: /home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/external/protobuf_archive/WORKSPACE:1: Workspace name in /home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions
INFO: Analysed target //tensorflow/contrib/lite/toco:toco (0 packages loaded).
INFO: Found 1 target...
WARNING: failed to create one or more convenience symlinks for prefix 'bazel-':
  cannot create symbolic link bazel-out -> /home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/execroot/org_tensorflow/bazel-out:  /home/user/PycharmProjects/tensorflow_tutorial/tensorflow/bazel-out (File exists)
  cannot create symbolic link bazel-out -> /home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/execroot/org_tensorflow/bazel-out:  /home/user/PycharmProjects/tensorflow_tutorial/tensorflow/bazel-out (File exists)
  cannot create symbolic link bazel-tensorflow -> /home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/execroot/org_tensorflow:  /home/user/PycharmProjects/tensorflow_tutorial/tensorflow/bazel-tensorflow (File exists)
  cannot create symbolic link bazel-bin -> /home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/execroot/org_tensorflow/bazel-out/k8-opt/bin:  /home/user/PycharmProjects/tensorflow_tutorial/tensorflow/bazel-bin (File exists)
  cannot create symbolic link bazel-testlogs -> /home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/execroot/org_tensorflow/bazel-out/k8-opt/testlogs:  /home/user/PycharmProjects/tensorflow_tutorial/tensorflow/bazel-testlogs (File exists)
cannot create symbolic link bazel-genfiles -> /home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/execroot/org_tensorflow/bazel-out/k8-opt/genfiles:  /home/user/PycharmProjects/tensorflow_tutorial/tensorflow/bazel-genfiles (File exists)
Target //tensorflow/contrib/lite/toco:toco up-to-date:
/home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/lite/toco/toco
INFO: Elapsed time: 0.271s, Critical Path: 0.00s
INFO: 0 processes.
INFO: Build completed successfully, 1 total action
INFO: Running command line: /home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/lite/toco/toco '--savedmodel_directory=/home/user/PycharmProjects/tensorflow_tutorial/tutorial-2-image-classifier' '--output_file=/home/user/PycharmProjects/tensorflow_tutorial/tutorial-2-image-classifier/dogs-cats-model.tflite'
2018-05-07 01:33:13.776954: F tensorflow/contrib/lite/toco/toco_saved_model.cc:34] **Check failed: tensorflow::MaybeSavedModelDirectory(model_path) Model is not saved in the supported SavedModel format.**

The function which throw this error is MaybeSavedModelDirectory at
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/toco_saved_model.cc
I took a look at the implementation of it on 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/loader.cc
Actually it looking for *.pb or *.pbtxt file on the model directory. I got this file at this location so why I get this error?
"
19147,when oficial Ruby language?,Is any real data to start oficial Ruby port of tensorflow?
19145,Linalg.LinearOperators do not give any performance improvement,"### Describe the problem

The performance gaurantees are not visible for the Linalg.LinearOperators (e.g., [DiagOperator](https://www.tensorflow.org/api_docs/python/tf/linalg/LinearOperatorDiag))   I have implemented a basic case below as per the documentation. Is there any reason why the performance of `diag` operator is same as full matrix ? 

Cuda and other versions not included as the issue is reproducible across GPU and CPU modes.


### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

Yes, but using only preliminary Linalg operations. 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux xxxx-xxxop 4.10.0-28-generic #32~16.04.2-Ubuntu SMP Thu Jul 20 10:19:48 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.4 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

- **TensorFlow installed from (source or binary)**:
Binary

- **TensorFlow version (use command below)**:
== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)


- **Python version**: 
Python 3.6.0 |Continuum Analytics, Inc.| (default, Dec 23 2016, 12:22:00) 

- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
Check the source code.


### Source code / logs



```python
import numpy as np
import tensorflow as tf
rng = np.random.RandomState(1)
```
```python
class Data:
    num_data = 10
    num_ind = 50
    D_in = 100
    D_out = 2

    Xmu = rng.randn(num_data, D_in)
    Xcov = rng.randn(num_data, D_in, D_in)
    Xcov = Xcov @ np.transpose(Xcov, (0, 2, 1))
    Z = rng.randn(num_ind, D_in)
```


```python
N = Data.num_data
Xmu = tf.convert_to_tensor(Data.Xmu)
Xcov = tf.convert_to_tensor(Data.Xcov)
```
```python
C = tf.cholesky(Xcov)

Z_tiled = tf.tile(tf.expand_dims(tf.transpose(Data.Z), 0), [N, 1, 1])
```
```python
C_operator = tf.linalg.LinearOperatorLowerTriangular(C)
C_diag_operator = tf.linalg.LinearOperatorDiag(tf.matrix_diag_part(C))
```

```python
def compute_matrix_solve(C, Z_tiled):
#     return tf.matrix_triangular_solve(C, Z_tiled, lower=True)  # NxDxM
    return tf.matrix_solve(C, Z_tiled)  # NxDxM

def compute_matrix_matmul(C, Z_tiled):
    mat = tf.matmul(C, Z_tiled)
    return mat 

def compute_operator_solve(C_operator, Z_tiled):
    operator_value = C_operator.solve(Z_tiled)
    return operator_value

def compute_operator_matmul(C_operator, Z_tiled):
    operator_value = C_operator.matmul(Z_tiled)
    return operator_value
```


```python
with tf.Session() as sess:
    mat_solve = sess.run(compute_matrix_solve(C, Z_tiled))
    op_solve = sess.run(compute_operator_solve(C_operator, Z_tiled))
    
    tf_mat_mul = sess.run(compute_matrix_matmul(C, Z_tiled))
    op_matmul = sess.run(compute_operator_matmul(C_operator, Z_tiled))

np.testing.assert_allclose(mat_solve, op_solve)
np.testing.assert_allclose(tf_mat_mul, op_matmul)
```


```python
with tf.Session() as sess:
    %timeit sess.run(compute_matrix_solve(C, Z_tiled))
    %timeit sess.run(compute_operator_solve(C_operator, Z_tiled))
    %timeit sess.run(compute_operator_solve(C_diag_operator, Z_tiled))
```

    69.1 ms ± 1.33 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
    69.4 ms ± 515 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    73.7 ms ± 1.55 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)



```python
with tf.Session() as sess:
    %timeit sess.run(compute_matrix_matmul(C, Z_tiled))
    %timeit sess.run(compute_operator_matmul(C_operator, Z_tiled))
    %timeit sess.run(compute_operator_matmul(C_diag_operator, Z_tiled))
    
```

    58.5 ms ± 447 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    60.8 ms ± 808 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    63.8 ms ± 1.78 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)




"
19144,TF 1.8 benchmark distributed run failed,"System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Linux Ubuntu 16.04
TensorFlow installed from source with verbs support
TensorFlow version:
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.8.0-1-g8753e2e', '1.8.0')
Python version: 2.7 
Bazel version: 0.9 ,0.10
GCC/Compiler version: 5.4
CUDA/cuDNN version: 9.1 ,7.0
GPU model and memory: P100, 16GB
Exact command to reproduce:

On first node:
CUDA_VISIBLE_DEVICES='' python ~/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --model inception3 --batch_size 64 --ps_hosts 11.11.11.47:10001,11.11.11.48:10002 --worker_hosts 11.11.11.47:20001,11.11.11.48:20002 --task_index 1 --job_name ps --num_gpus 1 --variable_update=distributed_replicated --cross_replica_sync=True &

python ~/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --model inception3 --batch_size 64 --ps_hosts 11.11.11.47:10001,11.11.11.48:10002 --worker_hosts 11.11.11.47:20001,11.11.11.48:20002 --task_index 1 --job_name worker --num_gpus 1 --variable_update=distributed_replicated --cross_replica_sync=True


On second node:
CUDA_VISIBLE_DEVICES='' python ~/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --model inception3 --batch_size 64 --ps_hosts 11.11.11.47:10001,11.11.11.48:10002 --worker_hosts 11.11.11.47:20001,11.11.11.48:20002 --task_index 0 --job_name ps --num_gpus 1 --variable_update=distributed_replicated --cross_replica_sync=True &

python ~/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --model inception3 --batch_size 64 --ps_hosts 11.11.11.47:10001,11.11.11.48:10002 --worker_hosts 11.11.11.47:20001,11.11.11.48:20002 --task_index 0 --job_name worker --num_gpus 1 --variable_update=distributed_replicated --cross_replica_sync=True
Problem
I'm running benchmark ( master branch) on two hosts with error:
2018-05-08 14:55:30.598759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-05-08 14:55:30.979461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 1 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:06:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-05-08 14:55:31.325185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 2 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:07:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-05-08 14:55:31.676291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 3 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:08:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-05-08 14:55:31.685309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1, 2, 3
2018-05-08 14:55:32.760916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-05-08 14:55:32.760978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 3 
2018-05-08 14:55:32.760986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y Y 
2018-05-08 14:55:32.760991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y Y 
2018-05-08 14:55:32.760997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N Y 
2018-05-08 14:55:32.761002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 3:   Y Y Y N 
2018-05-08 14:55:32.762303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 15133 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
2018-05-08 14:55:33.097816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:1 with 15133 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:06:00.0, compute capability: 6.0)
2018-05-08 14:55:33.431088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:2 with 15133 MB memory) -> physical GPU (device: 2, name: Tesla P100-PCIE-16GB, pci bus id: 0000:07:00.0, compute capability: 6.0)
2018-05-08 14:55:33.735570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:3 with 15133 MB memory) -> physical GPU (device: 3, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)
2018-05-08 14:55:34.010006: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> 11.11.11.47:10001, 1 -> 11.11.11.48:10002}
2018-05-08 14:55:34.010056: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:20001, 1 -> 11.11.11.48:20002}
2018-05-08 14:55:34.016202: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:20001
TensorFlow:  1.8
Model:       inception3
Dataset:     imagenet (synthetic)
Mode:        training
SingleSess:  False
Batch size:  128 global
             64 per device
Num batches: 100
Num epochs:  0.01
Devices:     ['/job:worker/task:0/gpu:0']
Data format: NCHW
Layout optimizer: False
Optimizer:   sgd
Variables:   distributed_replicated
Sync:        True
==========
Generating model
W0508 14:55:41.537986 139727489328896 tf_logging.py:126] From /root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py:1525: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
2018-05-08 14:55:53.359721: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1
2018-05-08 14:56:00.360362: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: OS Error
I0508 14:56:00.440299 139727489328896 tf_logging.py:116] Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.UnavailableError'>, OS Error
Traceback (most recent call last):
  File ""/root/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py"", line 60, in <module>
    app.run(main)  # Raises error on invalid flags, unlike tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 274, in run
    _run_main(main, argv)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 238, in _run_main
    sys.exit(main(argv))
  File ""/root/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py"", line 56, in main
    bench.run()
  File ""/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py"", line 1306, in run
    return self._benchmark_cnn()
  File ""/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py"", line 1535, in _benchmark_cnn
    start_standard_services=start_standard_services) as sess:
  File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 1000, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 828, in stop
    ignore_live_threads=ignore_live_threads)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 989, in managed_session
    start_standard_services=start_standard_services)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 726, in prepare_or_wait_for_session
    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 285, in prepare_session
    sess.run(init_op, feed_dict=init_feed_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnavailableError: OS Error

"
19143,[Feature Request] Control the progress of Iterator.get_next(),"I just noticed that Iterator.get_next() did not change my batch samples in a tf.while_loop.
However, I'd like to run the training operation in a tf.while_loop, so I need to control the progress of Iterator.get_next() directly.


**My Feature Request:**
It would be cool, if Iterator would provide an operation which can be called to advance its state.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**:  3.6.5
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: None
"
19142,"[TF r1.8][TensorRT 4.0.0.3] Output ""munmap_chunk(): invalid pointer""","## **System information**
- **HaveI written custom code (as opposed to using a stock example script provided in TensorFlow):** no

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Ubuntu 18.04

- **TensorFlow installed from (source or binary):** source

- **TensorFlow version (use command below):** 1.8.0

- **Bazel version (if compiling from source):** 0.10.1

- **CUDA/cuDNN version:** 9.0/7.1.3

- **GPU model and memory:** GTX 1080 8gb

- **Exact command to reproduce:**
python [test_tftrt.py](https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/contrib/tensorrt/test/test_tftrt.py)

## **Source code / logs**
`2018-05-08 16:11:39.979209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero`
`2018-05-08 16:11:39.979516: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1`
`2018-05-08 16:11:40.197749: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2660] Max batch size= 100 max workspace size= 33554432`
`2018-05-08 16:11:40.197774: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2666] starting build engine`
`munmap_chunk(): invalid pointer`
`Aborted (core dumped)`"
19141,[r1.7][TensorRT] Questions about the calibration in INT8 mode of TensorRT optimization,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat 4.8.5-16, Linux version 3.10.0-693.5.2.el7.x86_64
- **TensorFlow installed from (source or binary)**: pip (python 2.7)
- **TensorFlow version (use command below)**: tensorflow-gpu==1.7.0
- **Python version**:  python 2.7
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: gcc 5.3
- **CUDA/cuDNN version**: CUDA9.0, cuDNN7.0.5
- **GPU model and memory**: Tesla P4, 8GB
- **Exact command to reproduce**: NA

### Describe the problem
I tried to evaluate the accuracy of TF integrated TRT INT8 optimization in Python. However, I followed the procedure as the example test: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/test/test_tftrt.py
but the inference accuracy is not good (FP32: 70%, INT8-without-calib: 16%, INT8-with-calib: 35%).

I tried to trace the code but on the API level, but I'm a little confused about how does the calibration work. looks like the function run_calibration() just created a session and accepted the calib dataset to run the session, and then the return value was abandoned. 

I checked this introduction:https://devblogs.nvidia.com/int8-inference-autonomous-vehicles-tensorrt/, seems that the TRT python INT8 calibration will generate a calibration cache in file and then be used in the later inference. But in this TFTRT, the calibration didn't generate a calibration cache.

Another observation is that, I previously tried the TensorRT optimization in C++ independently, which is not integrated in the Tensorflow. At that time, the accuracy lose on INT8 mode optimization is about 10% (FP32: 70%, INT8-with-calib: 60%). Note that my C++ and Python experiments are using the same model and same calibration dataset and the same test dataset, so from my point of view, perhaps TRT INT8 Python API (at least the TF1.7 integrated version) seems somehow different from the TRT original C++ INT8 API. 

So let me conclude my questions:
1) Is the TF integrated TRT INT8 optimization in Python has a different back-end process compared from the independent TRT INT8 optimization in C++?
2) Could you share a hint of how does the python version INT8 calibration is realized? Shall the calibration data be generated so that can be loaded in later inference?

### Source code / logs
Please refer to the test:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/test/test_tftrt.py

@samikama "
19140,[XLA] input data type support for DT_STRING?,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  CentOS 7.4.1708
- **TensorFlow installed from (source or binary)**:  source
- **TensorFlow version (use command below)**:  1.7.0
- **Python version**:   2.7
- **Bazel version (if compiling from source)**:  0.11.1
- **GCC/Compiler version (if compiling from source)**:  4.8.5
- **CUDA/cuDNN version**:  cuda9 & cuddn6
- **GPU model and memory**:  
- **Exact command to reproduce**:  


### Describe the problem
What I want to do here is that using XLA to speedup my model inference performance.  so I have a model(think it is just a wide and deep model) trained with Estimator, exported as savedModel format with feature_column, then transform to frozen graph.  Then I followed the [AOT tutorial](https://www.tensorflow.org/performance/xla/tfcompile):

* prepare my frozen graph.pb
* write a graph_config.pbtxt
* edit BUILD file to add cc_library of my own
* build

in the meantime, in order to make it work, I have to add 3 more dependency in tf_compile library section of BUILD file(tensorflow/compiler/aot/BUILD). like below:

```
        ""//tensorflow/core/kernels:example_parsing_ops"",
        ""//tensorflow/core/kernels:lookup_table_op"",
        ""//tensorflow/core/kernels:logging_ops"",
```

then, after all dependency error resolved. Following error messages showed up:

```
INVALID ARGUMENTS: Unsupported type in DataTypeToPrimitiveType string
```

So I checked the code here [tensorflow/compiler/tf2xla/type_util.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/type_util.cc) and here [tensorflow/compiler/xla/xla_data.proto](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/xla_data.proto); found that XLA actually does not support DT_STRING now. So I would like to know is it possible to support string? why?


### Source code / logs

my graph.config.pbtxt is like below:

```
feed {
  id { node_name: ""input_example_tensor"" }
  shape {
    dim { size: 1 }
  }
}

fetch {
  id { node_name: ""head/predictions/probabilities"" }
}
```

bazel build error message:

```
ERROR: /data/tf/tensorflow-1.7.0/tensorflow/compiler/aot/tests/BUILD:155:1: Executing genrule //tensorflow/compiler/aot/tests:gen_feed_graph failed (Exit 1)
2018-05-08 17:49:19.657209: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
INVALID ARGUMENTS: Unsupported type in DataTypeToPrimitiveType string

tfcompile performs ahead-of-time compilation of a TensorFlow graph,
resulting in an object file compiled for your target architecture, and a
header file that gives access to the functionality in the object file.
A typical invocation looks like this:

   $ tfcompile --graph=mygraph.pb --config=myfile.pbtxt --cpp_class=""mynamespace::MyComputation""

usage: bazel-out/host/bin/tensorflow/compiler/aot/tfcompile
Flags:
	--graph=""""                       	string	Input GraphDef file.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.
	--config=""""                      	string	Input file containing Config proto.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.
	--dump_fetch_nodes=false         	bool	If set, only flags related to fetches are processed, and the resulting fetch nodes will be dumped to stdout in a comma-separated list.  Typically used to format arguments for other tools, e.g. freeze_graph.
	--target_triple=""x86_64-pc-linux""	string	Target platform, similar to the clang -target flag.  The general format is <arch><sub>-<vendor>-<sys>-<abi>.  http://clang.llvm.org/docs/CrossCompilation.html#target-triple.
	--target_cpu=""""                  	string	Target cpu, similar to the clang -mcpu flag.  http://clang.llvm.org/docs/CrossCompilation.html#cpu-fpu-abi
	--target_features=""""             	string	Target features, e.g. +avx2, +neon, etc.
	--entry_point=""entry""            	string	Name of the generated function.  If multiple generated object files will be linked into the same binary, each will need a unique entry point.
	--cpp_class=""""                   	string	Name of the generated C++ class, wrapping the generated function.  The syntax of this flag is [[<optional_namespace>::],...]<class_name>.  This mirrors the C++ syntax for referring to a class, where multiple namespaces may precede the class name, separated by double-colons.  The class will be generated in the given namespace(s), or if no namespaces are given, within the global namespace.
	--out_function_object=""out_model.o""	string	Output object file containing the generated function for the TensorFlow model.
	--out_header=""out.h""             	string	Output header file name.
	--out_metadata_object=""out_helper.o""	string	Output object file name containing optional metadata for the generated function.
	--out_session_module=""""          	string	Output session module proto.
	--gen_name_to_index=false        	bool	Generate name-to-index data for Lookup{Arg,Result}Index methods.
	--gen_program_shape=false        	bool	Generate program shape data for the ProgramShape method.
	--xla_generate_hlo_graph=""""      	string	HLO modules matching this regex will be dumped to a .dot file throughout various stages in compilation.
	--xla_hlo_graph_addresses=false  	bool	With xla_generate_hlo_graph, show addresses of HLO ops in graph dump.
	--xla_hlo_graph_path=""""          	string	With xla_generate_hlo_graph, dump the graphs into this path.
	--xla_hlo_dump_as_graphdef=false 	bool	Dump HLO graphs as TensorFlow GraphDefs.
	--xla_hlo_graph_sharding_color=false	bool	Assign colors based on sharding assignments when generating the HLO graphs.
	--xla_hlo_tfgraph_device_scopes=false	bool	When generating TensorFlow HLO graphs, if the HLO instructions are assigned to a specific device, prefix the name scope with ""devX"" with X being the device ordinal.
	--xla_log_hlo_text=""""            	string	HLO modules matching this regex will be dumped to LOG(INFO).
	--xla_generate_hlo_text_to=""""    	string	Dump all HLO modules as text into the provided directory path.
	--xla_enable_fast_math=true      	bool	Enable unsafe fast-math optimizations in the compiler; this may produce faster code at the expense of some accuracy.
	--xla_llvm_enable_alias_scope_metadata=true	bool	In LLVM-based backends, enable the emission of !alias.scope metadata in the generated IR.
	--xla_llvm_enable_noalias_metadata=true	bool	In LLVM-based backends, enable the emission of !noalias metadata in the generated IR.
	--xla_llvm_enable_invariant_load_metadata=true	bool	In LLVM-based backends, enable the emission of !invariant.load metadata in the generated IR.
	--xla_llvm_disable_expensive_passes=false	bool	In LLVM-based backends, disable a custom set of expensive optimization passes.
	--xla_backend_optimization_level=3	int32	Numerical optimization level for the XLA compiler backend.
	--xla_disable_hlo_passes=""""      	string	Comma-separated list of hlo passes to be disabled. These names must exactly match the passes' names; no whitespace around commas.
	--xla_embed_ir_in_executable=false	bool	Embed the compiler IR as a string in the executable.
	--xla_dump_ir_to=""""              	string	Dump the compiler IR into this directory as individual files.
	--xla_eliminate_hlo_implicit_broadcast=true	bool	Eliminate implicit broadcasts when lowering user computations to HLO instructions; use explicit broadcast instead.
	--xla_cpu_multi_thread_eigen=true	bool	When generating calls to Eigen in the CPU backend, use multi-threaded Eigen mode.
	--xla_gpu_cuda_data_dir=""./cuda_sdk_lib""	string	If non-empty, speficies a local directory containing ptxas and nvvm libdevice files; otherwise we use those from runfile directories.
	--xla_gpu_ftz=false              	bool	If true, flush-to-zero semantics are enabled in the code generated for GPUs.
	--xla_gpu_disable_multi_streaming=false	bool	If true, multi-streaming in the GPU backend is disabled.
	--xla_dump_optimized_hlo_proto_to=""""	string	Dump Hlo after all hlo passes are executed as proto binary into this directory.
	--xla_dump_unoptimized_hlo_proto_to=""""	string	Dump HLO before any hlo passes are executed as proto binary into this directory.
	--xla_dump_per_pass_hlo_proto_to=""""	string	Dump HLO after each pass as an HloProto in binary file format into this directory.
	--xla_test_all_output_layouts=false	bool	Let ClientLibraryTestBase::ComputeAndCompare* test all permutations of output layouts. For example, with a 3D shape, all permutations of the set {0, 1, 2} are tried.
	--xla_test_all_input_layouts=false	bool	Let ClientLibraryTestBase::ComputeAndCompare* test all permutations of *input* layouts. For example, for 2 input arguments with 2D shape and 4D shape, the computation will run 2! * 4! times for every possible layouts
	--xla_hlo_profile=false          	bool	Instrument the computation to collect per-HLO cycle counts
	--xla_dump_computations_to=""""    	string	Dump computations that XLA executes into the provided directory path
	--xla_dump_executions_to=""""      	string	Dump parameters and results of computations that XLA executes into the provided directory path
	--xla_backend_extra_options=""""   	string	Extra options to pass to a backend; comma-separated list of 'key=val' strings (=val may be omitted); no whitespace around commas.
	--xla_reduce_precision=""""        	string	Directions for adding reduce-precision operations. Format is 'LOCATION=E,M:OPS;NAMES' where LOCATION is the class of locations in which to insert the operations (e.g., 'OP_OUTPUTS'), E and M are the exponent and matissa bit counts respectively, and OPS and NAMES are comma-separated (no spaces) lists of the operation types and names to which to attach the reduce-precision operations.  The NAMES string and its preceding ';' may be omitted.  This option may be repeated to define multiple sets of added reduce-precision operations.
	--xla_gpu_use_cudnn_batchnorm=false	bool	Allows the GPU backend to implement batchnorm HLOs using cudnn, rather than expanding them to a soup of HLOs.
Target //tensorflow/compiler/aot/tests:feed_binary failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 12.475s, Critical Path: 3.04s
FAILED: Build did NOT complete successfully
```"
19139,Can't quantize nodes In while,"I have similar errors with [#7162](https://github.com/tensorflow/tensorflow/issues/7162) when quantize_nodes for [Tensor2Tensor Transformer](https://github.com/tensorflow/tensor2tensor) model.
It's no problem after freeze_graph, it's also no problem with quantize_weights. But when I qutize_nodes, it will:
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'transformer/while/body/parallel_0/body/decoder/layer_prepostprocess/layer_norm/add_1/eightbit' has inputs from different frames. The input 'transformer/while/body/parallel_0/body/decoder/layer_prepostprocess/layer_norm/add_1_eightbit/transformer/while/body/parallel_0/body/decoder/layer_prepostprocess/layer_norm/add_1/Enter/quantize' is in frame 'transformer/while/while_context'. The input 'transformer/while/body/parallel_0/body/decoder/layer_prepostprocess/layer_norm/mul_1/eightbit/requantize' is in frame ''.
```
I have tried TF1.4 and TF1.7, and the error is the same.
Can anyone help me?"
19138,precision and recall values kept unchanged for some training steps.,"### System information
- Have I written custom code: No, i just use a canned estimator DNNClassifier.
- OS Platform and Distribution: macOS High Sierra
- TensorFlow installed from: pip
- TensorFlow version: 1.8.0
- Bazel version: N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A
- Exact command to reproduce: N/A

### Describe the problem
I'm using google ml-engine to train a model for ctr-prediction(the training dataset is Imbalanced).
Following is the training log screenshot from ml-engine.
![pr-auc](https://user-images.githubusercontent.com/17157194/39740932-52b01a7c-52ca-11e8-8952-820424e38357.jpg)
The problem is: the **precision** and **recall** values kept unchanged for some training steps as i annotated in the picture(auctually these two values kept unchanged for 2 epochs in this picture). I'm pretty sure this is a bug, but i don't know where is wrong.
Does any one can check the calculation of these two metrics(precision, recall) for me?"
19134,When I try to import the kernel always gets dumped. ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
19129,Linker errors building Tensorflow master on Windows 10,"Hi Guys, 
I am trying to build tensorflow master.  

I am running Windows 10, Python 3.5, Visual Studio 2015, CUDA 9.0 and cudnn 9.0 windows10 v7.1.

I follow the instruction on the README file and was able to get cmake to create the visual studio solutions/project files as follows:
**cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=""C:\\swigwin-3.0.12\\swig.exe"" -DPYTHON_EXECUTABLE=""C:\\Python35\\python.exe"" -DPYTHON_LIBRARIES=""C:\\Python35\\libs\\python35.lib"" -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=""C:\\cudnn_90_712""**

However, when I tried to build with the following command:
**MSBuild /p:Configuration=Release tf_tutorials_example_trainer.vcxproj**

I got some linker errors:
""C:\temp\tensorflow\tensorflow\contrib\cmake\build_ex\tf_tutorials_example_trainer.vcxproj"" (default target) (1) ->
(Link target) ->
  eager_operation.obj : error LNK2019: unresolved external symbol ""public: class tensorflow::AttrBuilder & __cdecl tensorflow::AttrBuilder::NumInputs(int)"" (?NumInputs@AttrBuilder@tensorflow@@QEAAAEAV12@H
@Z) referenced in function ""public: void __cdecl tensorflow::EagerOperation::AddInput(class tensorflow::TensorHandle *)"" (?AddInput@EagerOperation@tensorflow@@QEAAXPEAVTensorHandle@2@@Z) [C:\temp\tensorfl
ow\tensorflow\contrib\cmake\build_ex\tf_tutorials_example_trainer.vcxproj]
  execute.obj : error LNK2019: unresolved external symbol ""class tensorflow::Status __cdecl tensorflow::OpDefForOp(char const *,class tensorflow::OpDef const * *)"" (?OpDefForOp@tensorflow@@YA?AVStatus@1@P
EBDPEAPEBVOpDef@1@@Z) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHand
le *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z) [C:\temp\tensorflow\tensorflow\contrib\cmake\build_ex\tf_
tutorials_example_trainer.vcxproj]
  execute.obj : error LNK2019: unresolved external symbol ""public: struct tensorflow::Fprint128 __cdecl tensorflow::AttrBuilder::CacheKey(class std::basic_string<char,struct std::char_traits<char>,class s
td::allocator<char> > const &)const "" (?CacheKey@AttrBuilder@tensorflow@@QEBA?AUFprint128@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z) referenced in function ""class tensorflow::S
tatus __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEa
gerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z) [C:\temp\tensorflow\tensorflow\contrib\cmake\build_ex\tf_tutorials_example_trainer.vcxproj]
  execute.obj : error LNK2019: unresolved external symbol ""public: class tensorflow::NodeDef const & __cdecl tensorflow::AttrBuilder::BuildNodeDef(void)"" (?BuildNodeDef@AttrBuilder@tensorflow@@QEAAAEBVNod
eDef@2@XZ) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,i
nt *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z) [C:\temp\tensorflow\tensorflow\contrib\cmake\build_ex\tf_tutorials_e
xample_trainer.vcxproj]
  C:\temp\tensorflow\tensorflow\contrib\cmake\build_ex\Release\tf_tutorials_example_trainer.exe : fatal error LNK1120: 4 unresolved externals [C:\temp\tensorflow\tensorflow\contrib\cmake\build_ex\tf_tutor
ials_example_trainer.vcxproj]

Do you guys know what might have been wrong?  

Does the Windows build instructions work with tensorflow master?

If not, is there another tag/version/snapshot of tensorflow that would work?

Thank you very much for your help in advance!

Thanks,
Ben

"
19128,build with python3 binary uses wrong path to find Python.h (fix inside),"I spent few days fighting issue with building tensorflow where it was unable to find Python.h, and seen number of similar issues.
Python.h was installed as a part of python3-devel, but build system wasnt unable to find it.
Finally i figured out that in tensorflow/third_party/py/python_configure.bzl function _get_python_bin was looking for binary by name python, even once i did configure i told it to use /usr/bin/python3

so i did a workaround by changing python_bin_path=repository_ctx.which(""python"") to look for python3 instead.

but i guess configure should do that for me.

i had unsupported fedora28 but its not the only distro having both python and python3 binaries"
19127,Unable to build tensorflow using CMake without GPU,"-Have I written custom code (as opposed to using a stock example script provided in TensorFlow) : NO

    OS Platform and Distribution : WINDOWS 10
    -TensorFlow installed from (source or binary) : git clone

    TensorFlow version (use command below) :
commit eeab2c867faa0f10dfea8635d1e87009844f902e (HEAD -> master, origin/master, origin/HEAD)
Merge: daecc72653 9a4879660f
Author: Shanqing Cai <cais@google.com>
Date:   Fri May 4 21:34:58 2018 -0400

    Merge pull request #19090 from caisq/branch_195443326

    Branch 195443326



    Python version : 3.6.4

    Bazel version (if compiling from source) : NO using CMAKE 3.11.0

    GCC/Compiler version (if compiling from source): VS 2017 version 15.6.3

    CUDA/cuDNN version: no only cpu version

    GPU model and memory : NOT USE

    Exact command to reproduce :

Use cmake : cmakecache is [CMakeCache.txt](https://github.com/tensorflow/tensorflow/files/1979573/CMakeCache.tx
I open my own issue : previous one was issue #19004 and @ctraina does not answer
t)

full error message : 
[tferror.txt](https://github.com/tensorflow/tensorflow/files/1979582/tferror.txt)


Error message 
```
Severity	Code	Description	Project	File	Line	Suppression State
Error	LNK2019	unresolved external symbol ""class tensorflow::Status __cdecl tensorflow::OpDefForOp(char const *,class tensorflow::OpDef const * *)"" (?OpDefForOp@tensorflow@@YA?AVStatus@1@PEBDPEAPEBVOpDef@1@@Z) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	tf_tutorials_example_trainer	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK1120	4 unresolved externals	transform_graph	F:\lib\build\tensorflow\Release\transform_graph.exe	1	
Error	LNK1120	4 unresolved externals	benchmark_model	F:\lib\build\tensorflow\Release\benchmark_model.exe	1	
Error	LNK1120	4 unresolved externals	tf_tutorials_example_trainer	F:\lib\build\tensorflow\Release\tf_tutorials_example_trainer.exe	1	
Error	LNK1120	4 unresolved externals	tf_label_image_example	F:\lib\build\tensorflow\Release\tf_label_image_example.exe	1	
Error	LNK1120	4 unresolved externals	compare_graphs	F:\lib\build\tensorflow\Release\compare_graphs.exe	1	
Error	LNK1120	4 unresolved externals	grpc_tensorflow_server	F:\lib\build\tensorflow\Release\grpc_tensorflow_server.exe	1	
Error	LNK1120	4 unresolved externals	summarize_graph	F:\lib\build\tensorflow\Release\summarize_graph.exe	1	
Error	LNK1181	cannot open input file '\pywrap_tensorflow_internal.lib'	_periodic_resample_op	F:\lib\build\tensorflow\LINK	1	
Error	LNK1181	cannot open input file '\pywrap_tensorflow_internal.lib'	_nearest_neighbor_ops	F:\lib\build\tensorflow\LINK	1	
Error	LNK1181	cannot open input file '\pywrap_tensorflow_internal.lib'	_beam_search_ops	F:\lib\build\tensorflow\LINK	1	
Error	LNK1181	cannot open input file '\pywrap_tensorflow_internal.lib'	_gru_ops	F:\lib\build\tensorflow\LINK	1	
Error	LNK1181	cannot open input file '\pywrap_tensorflow_internal.lib'	_lstm_ops	F:\lib\build\tensorflow\LINK	1	
Error	LNK2019	unresolved external symbol ""class tensorflow::Status __cdecl tensorflow::OpDefForOp(char const *,class tensorflow::OpDef const * *)"" (?OpDefForOp@tensorflow@@YA?AVStatus@1@PEBDPEAPEBVOpDef@1@@Z) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	benchmark_model	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK2019	unresolved external symbol ""class tensorflow::Status __cdecl tensorflow::OpDefForOp(char const *,class tensorflow::OpDef const * *)"" (?OpDefForOp@tensorflow@@YA?AVStatus@1@PEBDPEAPEBVOpDef@1@@Z) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	transform_graph	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK2019	unresolved external symbol ""class tensorflow::Status __cdecl tensorflow::OpDefForOp(char const *,class tensorflow::OpDef const * *)"" (?OpDefForOp@tensorflow@@YA?AVStatus@1@PEBDPEAPEBVOpDef@1@@Z) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	tf_label_image_example	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK2019	unresolved external symbol ""class tensorflow::Status __cdecl tensorflow::OpDefForOp(char const *,class tensorflow::OpDef const * *)"" (?OpDefForOp@tensorflow@@YA?AVStatus@1@PEBDPEAPEBVOpDef@1@@Z) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	compare_graphs	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK2019	unresolved external symbol ""class tensorflow::Status __cdecl tensorflow::OpDefForOp(char const *,class tensorflow::OpDef const * *)"" (?OpDefForOp@tensorflow@@YA?AVStatus@1@PEBDPEAPEBVOpDef@1@@Z) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	grpc_tensorflow_server	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK2019	unresolved external symbol ""class tensorflow::Status __cdecl tensorflow::OpDefForOp(char const *,class tensorflow::OpDef const * *)"" (?OpDefForOp@tensorflow@@YA?AVStatus@1@PEBDPEAPEBVOpDef@1@@Z) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	summarize_graph	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK2019	unresolved external symbol ""public: class tensorflow::AttrBuilder & __cdecl tensorflow::AttrBuilder::NumInputs(int)"" (?NumInputs@AttrBuilder@tensorflow@@QEAAAEAV12@H@Z) referenced in function ""public: void __cdecl tensorflow::EagerOperation::AddInput(class tensorflow::TensorHandle *)"" (?AddInput@EagerOperation@tensorflow@@QEAAXPEAVTensorHandle@2@@Z)	tf_tutorials_example_trainer	F:\lib\build\tensorflow\eager_operation.obj	1	
Error	LNK2019	unresolved external symbol ""public: class tensorflow::AttrBuilder & __cdecl tensorflow::AttrBuilder::NumInputs(int)"" (?NumInputs@AttrBuilder@tensorflow@@QEAAAEAV12@H@Z) referenced in function ""public: void __cdecl tensorflow::EagerOperation::AddInput(class tensorflow::TensorHandle *)"" (?AddInput@EagerOperation@tensorflow@@QEAAXPEAVTensorHandle@2@@Z)	benchmark_model	F:\lib\build\tensorflow\eager_operation.obj	1	
Error	LNK2019	unresolved external symbol ""public: class tensorflow::AttrBuilder & __cdecl tensorflow::AttrBuilder::NumInputs(int)"" (?NumInputs@AttrBuilder@tensorflow@@QEAAAEAV12@H@Z) referenced in function ""public: void __cdecl tensorflow::EagerOperation::AddInput(class tensorflow::TensorHandle *)"" (?AddInput@EagerOperation@tensorflow@@QEAAXPEAVTensorHandle@2@@Z)	transform_graph	F:\lib\build\tensorflow\eager_operation.obj	1	
Error	LNK2019	unresolved external symbol ""public: class tensorflow::AttrBuilder & __cdecl tensorflow::AttrBuilder::NumInputs(int)"" (?NumInputs@AttrBuilder@tensorflow@@QEAAAEAV12@H@Z) referenced in function ""public: void __cdecl tensorflow::EagerOperation::AddInput(class tensorflow::TensorHandle *)"" (?AddInput@EagerOperation@tensorflow@@QEAAXPEAVTensorHandle@2@@Z)	tf_label_image_example	F:\lib\build\tensorflow\eager_operation.obj	1	
Error	LNK2019	unresolved external symbol ""public: class tensorflow::AttrBuilder & __cdecl tensorflow::AttrBuilder::NumInputs(int)"" (?NumInputs@AttrBuilder@tensorflow@@QEAAAEAV12@H@Z) referenced in function ""public: void __cdecl tensorflow::EagerOperation::AddInput(class tensorflow::TensorHandle *)"" (?AddInput@EagerOperation@tensorflow@@QEAAXPEAVTensorHandle@2@@Z)	compare_graphs	F:\lib\build\tensorflow\eager_operation.obj	1	
Error	LNK2019	unresolved external symbol ""public: class tensorflow::AttrBuilder & __cdecl tensorflow::AttrBuilder::NumInputs(int)"" (?NumInputs@AttrBuilder@tensorflow@@QEAAAEAV12@H@Z) referenced in function ""public: void __cdecl tensorflow::EagerOperation::AddInput(class tensorflow::TensorHandle *)"" (?AddInput@EagerOperation@tensorflow@@QEAAXPEAVTensorHandle@2@@Z)	grpc_tensorflow_server	F:\lib\build\tensorflow\eager_operation.obj	1	
Error	LNK2019	unresolved external symbol ""public: class tensorflow::AttrBuilder & __cdecl tensorflow::AttrBuilder::NumInputs(int)"" (?NumInputs@AttrBuilder@tensorflow@@QEAAAEAV12@H@Z) referenced in function ""public: void __cdecl tensorflow::EagerOperation::AddInput(class tensorflow::TensorHandle *)"" (?AddInput@EagerOperation@tensorflow@@QEAAXPEAVTensorHandle@2@@Z)	summarize_graph	F:\lib\build\tensorflow\eager_operation.obj	1	
Error	LNK2019	unresolved external symbol ""public: class tensorflow::NodeDef const & __cdecl tensorflow::AttrBuilder::BuildNodeDef(void)"" (?BuildNodeDef@AttrBuilder@tensorflow@@QEAAAEBVNodeDef@2@XZ) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	tf_tutorials_example_trainer	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK2019	unresolved external symbol ""public: class tensorflow::NodeDef const & __cdecl tensorflow::AttrBuilder::BuildNodeDef(void)"" (?BuildNodeDef@AttrBuilder@tensorflow@@QEAAAEBVNodeDef@2@XZ) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	benchmark_model	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK2019	unresolved external symbol ""public: class tensorflow::NodeDef const & __cdecl tensorflow::AttrBuilder::BuildNodeDef(void)"" (?BuildNodeDef@AttrBuilder@tensorflow@@QEAAAEBVNodeDef@2@XZ) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	transform_graph	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK2019	unresolved external symbol ""public: class tensorflow::NodeDef const & __cdecl tensorflow::AttrBuilder::BuildNodeDef(void)"" (?BuildNodeDef@AttrBuilder@tensorflow@@QEAAAEBVNodeDef@2@XZ) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	tf_label_image_example	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK2019	unresolved external symbol ""public: class tensorflow::NodeDef const & __cdecl tensorflow::AttrBuilder::BuildNodeDef(void)"" (?BuildNodeDef@AttrBuilder@tensorflow@@QEAAAEBVNodeDef@2@XZ) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	compare_graphs	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK2019	unresolved external symbol ""public: class tensorflow::NodeDef const & __cdecl tensorflow::AttrBuilder::BuildNodeDef(void)"" (?BuildNodeDef@AttrBuilder@tensorflow@@QEAAAEBVNodeDef@2@XZ) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	grpc_tensorflow_server	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK2019	unresolved external symbol ""public: class tensorflow::NodeDef const & __cdecl tensorflow::AttrBuilder::BuildNodeDef(void)"" (?BuildNodeDef@AttrBuilder@tensorflow@@QEAAAEBVNodeDef@2@XZ) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	summarize_graph	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK2019	unresolved external symbol ""public: struct tensorflow::Fprint128 __cdecl tensorflow::AttrBuilder::CacheKey(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)const "" (?CacheKey@AttrBuilder@tensorflow@@QEBA?AUFprint128@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	tf_tutorials_example_trainer	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK2019	unresolved external symbol ""public: struct tensorflow::Fprint128 __cdecl tensorflow::AttrBuilder::CacheKey(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)const "" (?CacheKey@AttrBuilder@tensorflow@@QEBA?AUFprint128@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	benchmark_model	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK2019	unresolved external symbol ""public: struct tensorflow::Fprint128 __cdecl tensorflow::AttrBuilder::CacheKey(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)const "" (?CacheKey@AttrBuilder@tensorflow@@QEBA?AUFprint128@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	transform_graph	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK2019	unresolved external symbol ""public: struct tensorflow::Fprint128 __cdecl tensorflow::AttrBuilder::CacheKey(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)const "" (?CacheKey@AttrBuilder@tensorflow@@QEBA?AUFprint128@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	tf_label_image_example	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK2019	unresolved external symbol ""public: struct tensorflow::Fprint128 __cdecl tensorflow::AttrBuilder::CacheKey(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)const "" (?CacheKey@AttrBuilder@tensorflow@@QEBA?AUFprint128@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	compare_graphs	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK2019	unresolved external symbol ""public: struct tensorflow::Fprint128 __cdecl tensorflow::AttrBuilder::CacheKey(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)const "" (?CacheKey@AttrBuilder@tensorflow@@QEBA?AUFprint128@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	grpc_tensorflow_server	F:\lib\build\tensorflow\execute.obj	1	
Error	LNK2019	unresolved external symbol ""public: struct tensorflow::Fprint128 __cdecl tensorflow::AttrBuilder::CacheKey(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)const "" (?CacheKey@AttrBuilder@tensorflow@@QEBA?AUFprint128@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	summarize_graph	F:\lib\build\tensorflow\execute.obj	1	
```

"
19126,Build Tensorflow C++ Windows with MSVC2013,"I have successfully built Tensorflow C++ Windows with MSVC2015, but currently I am interested in building Tensorflow C++ Windows with MSVC2013. May I know has anybody done this before, or does it doable?

Thanks!"
19125,proto issue: anchor_generator.proto,"When I tried to test the model from the tensorflow/models/research folder using the command below:                 

> python object_detection/builders/model_builder_test.py

It throws out error saying import is not done.

```
Traceback (most recent call last):
  File ""builders/model_builder_test.py"", line 21, in <module>
    from object_detection.builders import model_builder
  File ""D:\Workspace\Tensorflow\models\research\object_detection\builders\model_builder.py"", line 17, in <module>
    from object_detection.builders import anchor_generator_builder
  File ""D:\Workspace\Tensorflow\models\research\object_detection\builders\anchor_generator_builder.py"", line 21, in <module>
    from object_detection.protos import anchor_generator_pb2
  File ""D:\Workspace\Tensorflow\models\research\object_detection\protos\anchor_generator_pb2.py"", line 27, in <module>
    dependencies=[object__detection_dot_protos_dot_grid__anchor__generator__pb2.DESCRIPTOR,object__detection_dot_protos_dot_ssd__anchor__generator__pb2.DESCRIPTOR,object__detection_dot_protos_dot_multiscale__anchor__generator__pb2.DESCRIPTOR,])
  File ""C:\Users\M1043107\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\descriptor.py"", line 829, in __new__
    return _message.default_pool.AddSerializedFile(serialized_pb)
TypeError: Couldn't build proto file into descriptor pool!
Invalid proto descriptor for file ""object_detection/protos/anchor_generator.proto"":
  object_detection/protos/anchor_generator.proto: Import ""object_detection/protos/grid_anchor_generator.proto"" has not been loaded.
  object_detection.protos.AnchorGenerator.grid_anchor_generator: ""object_detection.protos.GridAnchorGenerator"" seems to be defined in ""grid_anchor_generator.proto"", which is not imported by ""object_detection/protos/anchor_generator.proto"".  To use it here, please add the necessary import.
```

Any solution?
"
19124,/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark] Error 1,"
/home/pi/Downloads/tensorFlow/tfsource/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(mutex.o): In function `tensorflow::condition_variable::notify_one()':
mutex.cc:(.text+0xa8): undefined reference to `nsync::nsync_cv_signal(nsync::nsync_cv_s_*)'
/home/pi/Downloads/tensorFlow/tfsource/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(mutex.o): In function `tensorflow::condition_variable::notify_all()':
mutex.cc:(.text+0xac): undefined reference to `nsync::nsync_cv_broadcast(nsync::nsync_cv_s_*)'
collect2: error: ld returned 1 exit status
tensorflow/contrib/makefile/Makefile:730: recipe for target '/home/pi/Downloads/tensorFlow/tfsource/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark' failed
make: *** [/home/pi/Downloads/tensorFlow/tfsource/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark] Error 1

Please,who can help me make this issue………………………………"
19123,Error converting from tflite to PB,"bazel run --config=opt   //tensorflow/contrib/lite/toco:toco --   --input_file=/root/tensorflow/tensorflow/mobilenet_v1_1.0_224.tflite   --output_file=/root/tensorflow/tensorflow/tflite_2_pb.pb   --input_format=TFLITE   --output_format=TENSORFLOW_GRAPHDEF   --input_shape=1,224,224,3   --input_array=input   --output_array=MobilenetV1/Predictions/Reshape_1


**It shows below error:** 

INFO: Analysed target //tensorflow/contrib/lite/toco:toco (0 packages loaded).
INFO: Found 1 target...
Target //tensorflow/contrib/lite/toco:toco up-to-date:
  bazel-bin/tensorflow/contrib/lite/toco/toco
INFO: Elapsed time: 0.198s, Critical Path: 0.00s
INFO: Build completed successfully, 1 total action

INFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/root/tensorflow/tensorflow/mobilenet_v1_1.0_224.tflite' '--output_file=/root/tensorflow/tensorflow/tflite_2_pb.pb' '--input_format=TFLITE' '--output_format=TENSORFLOW_GRAPHDEF' '--input_shape=1,224,224,3' '--input_array=input' '--output_array=MobilenetV1/Predictions/Reshape_1'
2018-05-07 16:07:38.775341: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 31 operators, 89 arrays (0 quantized)
2018-05-07 16:07:38.775647: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 31 operators, 89 arrays (0 quantized)
2018-05-07 16:07:38.775663: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:48] Check failed: dilation_width_factor >= 1 (0 vs. 1)
"
19122,Windows: Adding a new op,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:windows7
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.8.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:6.4.0
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I use pip to install tensorflow, but there is no library I need to compile my op.
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
TF_CFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))') ) TF_LFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))') ) g++ -std=c++11 -shared zero_out.cc -o zero_out.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -O2 /usr/bin/ld: 找不到 -ltensorflow_framework collect2: 错误：ld 返回 1"
19120,Cannot build tensorflow on arm with the latest bazel. _pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow9ConcatCPUINS_8bfloat16EEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2EiE11ConstMatrixESt14default_deleteIS9_EESaISC_EEPNS8_6MatrixE,"The system:

Have I written custom code  
- No

OS Platform and Distribution 
- Ubuntu 18.04 LTS on odroid ux4
- OS image is here https://wiki.odroid.com/odroid-xu4/os_images/linux/ubuntu_4.14/20180501
- kernel 4.14
- arm-linux-gnueabihf

TensorFlow installed from
- git https://github.com/tensorflow

TensorFlow version
- v1.8.0

Bazel version
- bazel release 0.13.0
- bazel release 0.11.0

CUDA/cuDNN version
- No

GPU model and memory
- Mali no Cuda

Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/arm-linux-gnueabihf/7/lto-wrapper

gcc version 7.3.0 (Ubuntu/Linaro 7.3.0-16ubuntu3)
**the same issue with gcc 4.8 and 5.5 **

**Exact command to reproduce**

build -c opt --local_resources 2024,8,1.0  --copt=""-mcpu=cortex-a15"" --copt=""-Ofast""   tensorflow/tools/pip_package:build_pip_package  --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}"" --verbose_failures

```
ERROR: /home/jora/frameworks/tensorflow/tensorflow/tools/api/generator/BUILD:27:1: Executing genrule //tensorflow/tools/api/generator:python_api_gen failed (Exit 1): bash failed: error executing command
  (cd /home/jora/.cache/bazel/_bazel_jora/7dfd049478ee24381bd18645ae9bd0df/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH='' \
    PATH=/home/jora/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/app/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/bitwise/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/compat/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/contrib/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/contrib/stat_summarizer/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/data/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/distributions/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/distributions/bijectors/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/errors/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/estimator/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/estimator/export/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/estimator/inputs/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/feature_column/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/gfile/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/graph_util/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/image/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/initializers/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/activations/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/densenet/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/inception_resnet_v2/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/inception_v3/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/mobilenet/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/nasnet/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/resnet50/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/vgg16/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/vgg19/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/xception/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/backend/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/callbacks/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/constraints/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/boston_housing/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/cifar10/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/cifar100/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/fashion_mnist/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/imdb/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/mnist/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/reuters/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/estimator/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/initializers/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/layers/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/losses/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/metrics/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/models/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/optimizers/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/preprocessing/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/preprocessing/image/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/preprocessing/sequence/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/preprocessing/text/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/regularizers/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/utils/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/wrappers/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/wrappers/scikit_learn/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/layers/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/linalg/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/logging/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/losses/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/manip/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/math/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/metrics/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/nn/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/nn/rnn_cell/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/profiler/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/python_io/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/resource_loader/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/builder/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/constants/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/loader/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/main_op/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/signature_constants/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/signature_def_utils/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/tag_constants/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/utils/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/sets/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/spectral/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/summary/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/sysconfig/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/test/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/train/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/train/queue_runner/__init__.py bazel-out/arm-py3-opt/genfiles/tensorflow/tools/api/generator/api/user_ops/__init__.py')
Traceback (most recent call last):
  File ""/home/jora/.cache/bazel/_bazel_jora/7dfd049478ee24381bd18645ae9bd0df/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/jora/.cache/bazel/_bazel_jora/7dfd049478ee24381bd18645ae9bd0df/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/jora/.cache/bazel/_bazel_jora/7dfd049478ee24381bd18645ae9bd0df/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /home/jora/.cache/bazel/_bazel_jora/7dfd049478ee24381bd18645ae9bd0df/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow9ConcatCPUINS_8bfloat16EEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2EiE11ConstMatrixESt14default_deleteIS9_EESaISC_EEPNS8_6MatrixE

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/jora/.cache/bazel/_bazel_jora/7dfd049478ee24381bd18645ae9bd0df/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/tools/api/generator/create_python_api.py"", line 26, in <module>
    from tensorflow.python.util import tf_decorator
  File ""/home/jora/.cache/bazel/_bazel_jora/7dfd049478ee24381bd18645ae9bd0df/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/jora/.cache/bazel/_bazel_jora/7dfd049478ee24381bd18645ae9bd0df/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/jora/.cache/bazel/_bazel_jora/7dfd049478ee24381bd18645ae9bd0df/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/jora/.cache/bazel/_bazel_jora/7dfd049478ee24381bd18645ae9bd0df/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/jora/.cache/bazel/_bazel_jora/7dfd049478ee24381bd18645ae9bd0df/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /home/jora/.cache/bazel/_bazel_jora/7dfd049478ee24381bd18645ae9bd0df/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow9ConcatCPUINS_8bfloat16EEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2EiE11ConstMatrixESt14default_deleteIS9_EESaISC_EEPNS8_6MatrixE


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 8968.754s, Critical Path: 2194.12s
INFO: 3040 processes, local.
FAILED: Build did NOT complete successfully
```"
19119,Cannot build tensorflow on Windows 10 CUDA,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.8 or master ( tried both)
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.13.0
- **Cmake version (if compiling from source)**: 3.10.1
- **CUDA/cuDNN version**: 9.1/7.1
- **GPU model and memory**: GTX 1080 Ti
- **Exact command to reproduce**: 
cmake -T host=x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=c:/swigwin-3.0.12/swig.exe -Dtensorflow_ENABLE_GPU=ON -Dtensorflow_CUDA_VERSION=9.1 -Dtensorflow_CUDNN_VERSION=7.1 -DCUDNN_HOME=""C:\cudnn-9.1-windows10-x64-v7.1\cuda"" ..

Notes: -DCUDNN_HOME line could be skipped since it is already installed inside cuda 9.1 folder but tried different ones too

MSBuild /p:Configuration=Release /verbosity:detailed tf_python_build_pip_package.vcxproj

Build fails, I suppose warnings are fine, but what am I missing?
I need to build tensorflow because my CPU doesn't use AVX instructions (Intel Xeon 5670) and since I want to use GPU for this I only want to focus on CUDA build.

I have also tried with bazel:
````
$ ./configure
You have bazel 0.13.0 installed.
Please specify the location of python. [Default is D:\Miniconda3\python.exe]:


Found possible Python library paths:
  D:\Miniconda3\lib\site-packages
Please input the desired Python library path to use.  Default is [D:\Miniconda3\                               lib\site-packages]

Do you wish to build TensorFlow with XLA JIT support? [y/N]:
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]:
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]:
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to d                               efault to CUDA 9.0]: 9.1


Please specify the location where CUDA 9.1 toolkit is installed. Refer to README                               .md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/                               CUDA/v9.1]:


Please specify the cuDNN version you want to use. [Leave empty to default to cuD                               NN 7.0]: 7.1


Please specify the location where cuDNN 7 library is installed. Refer to README.                               md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/C                               UDA/v9.1]:


Please specify a list of comma-separated Cuda compute capabilities you want to b                               uild with.
You can find the compute capability of your device at: https://developer.nvidia.                               com/cuda-gpus.
Please note that each additional compute capability significantly increases your                                build time and binary size. [Default is: 3.5,5.2]6.1


Do you wish to build TensorFlow with MPI support? [y/N]:
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""-                               -config=opt"" is specified [Default is /arch:AVX]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:                               
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--con                               fig=<>"" to your build command. See tools/bazel.rc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
Configuration finished

atro@atro-PC MINGW64 /c/tensorflow (master)
$

atro@atro-PC MINGW64 /c/tensorflow (master)
$ bazel build --config=opt --config=win-cuda //tensorflow/tools/pip_package:buil                               d_pip_package
Starting local Bazel server and connecting to it...
.................
WARNING: The following configs were expanded more than once: [win-cuda]. For rep                               eatable flags, repeats are counted twice and may lead to unexpected behavior.
Loading:
Loading: 0 packages loaded
Loading: 0 packages loaded
    currently loading: tensorflow/tools/pip_package
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages l                               oaded)
WARNING: C:/users/atro/_bazel_atro/x1e5egqw/external/protobuf_archive/WORKSPACE:                               1: Workspace name in C:/users/atro/_bazel_atro/x1e5egqw/external/protobuf_archiv                               e/WORKSPACE (@com_google_protobuf) does not match the name given in the reposito                               ry's definition (@protobuf_archive); this will cause a build error in future ver                               sions
WARNING: C:/users/atro/_bazel_atro/x1e5egqw/external/grpc/WORKSPACE:1: Workspace                                name in C:/users/atro/_bazel_atro/x1e5egqw/external/grpc/WORKSPACE (@com_github                               _grpc_grpc) does not match the name given in the repository's definition (@grpc)                               ; this will cause a build error in future versions
WARNING: C:/users/atro/_bazel_atro/x1e5egqw/external/absl_py/WORKSPACE:1: Worksp                               ace name in C:/users/atro/_bazel_atro/x1e5egqw/external/absl_py/WORKSPACE (@io_a                               bseil_py) does not match the name given in the repository's definition (@absl_py                               ); this will cause a build error in future versions
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (76 packages                                loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (124 packages                                loaded)
WARNING: C:/users/atro/_bazel_atro/x1e5egqw/external/grpc/BUILD:1960:1: in srcs                                attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//t                               hird_party/nanopb:pb_common.c' directly. You should either move the file to this                                package or depend on an appropriate rule there. Since this rule was created by                                the macro 'grpc_generate_one_off_targets', the error might have been caused by t                               he macro implementation in C:/users/atro/_bazel_atro/x1e5egqw/external/grpc/baze                               l/grpc_build_system.bzl:172:12
WARNING: C:/users/atro/_bazel_atro/x1e5egqw/external/grpc/BUILD:1960:1: in srcs                                attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//t                               hird_party/nanopb:pb_decode.c' directly. You should either move the file to this                                package or depend on an appropriate rule there. Since this rule was created by                                the macro 'grpc_generate_one_off_targets', the error might have been caused by t                               he macro implementation in C:/users/atro/_bazel_atro/x1e5egqw/external/grpc/baze                               l/grpc_build_system.bzl:172:12
WARNING: C:/users/atro/_bazel_atro/x1e5egqw/external/grpc/BUILD:1960:1: in srcs                                attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//t                               hird_party/nanopb:pb_encode.c' directly. You should either move the file to this                                package or depend on an appropriate rule there. Since this rule was created by                                the macro 'grpc_generate_one_off_targets', the error might have been caused by t                               he macro implementation in C:/users/atro/_bazel_atro/x1e5egqw/external/grpc/baze                               l/grpc_build_system.bzl:172:12
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (134 packages                                loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (145 packages                                loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (149 packages                                loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (160 packages                                loaded)
WARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule /                               /tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depen                               ds on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longe                               r supported. Switch to SavedModel immediately.
WARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule /                               /tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depen                               ds on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supp                               orted. Switch to SavedModel immediately.
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (256 packages                                loaded)
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (257 pack                               ages loaded).
INFO: Found 1 target...
Building: no action
[2 / 19] [-----] BazelWorkspaceStatusAction stable-status.txt
ERROR: C:/tensorflow/tensorflow/core/BUILD:1800:1: C++ compilation of rule '//tensorflow/core:lib_hash_crc32c_accelerate_internal' failed (Exit 2): cl.exe failed: error executing command
  cd C:/users/atro/_bazel_atro/x1e5egqw/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.15063.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.15063.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.15063.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.15063.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.15063.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.15063.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\VCPackages;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\WINDOWS\Microsoft.NET\Framework64\;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET TEMP=C:\Users\atro\AppData\Local\Temp
    SET TMP=C:\Users\atro\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe /c tensorflow/core/lib/hash/crc32c_accelerate.cc /Fobazel-out/host/bin/tensorflow/core/_objs/lib_hash_crc32c_accelerate_internal/tensorflow/core/lib/hash/crc32c_accelerate.o /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/host/genfiles /Iexternal/bazel_tools /Ibazel-out/host/genfiles/external/bazel_tools /showIncludes /MD /O2 /DNDEBUG -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -w -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /U_HAS_EXCEPTIONS /D_HAS_EXCEPTIONS=1 /EHsc /DNOGDI /DTF_COMPILE_LIBRARY
tensorflow/core/lib/hash/crc32c_accelerate.cc(16): fatal error C1083: Cannot open include file: 'stddef.h': No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 20.360s, Critical Path: 0.32s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
````

### Source code / logs
Attached is the complete msbuild log. 
[msbuild.zip](https://github.com/tensorflow/tensorflow/files/1978158/msbuild.zip)
"
19115,[Example Request]: Add seq2seq in eager examples,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NA
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: NA
- **TensorFlow installed from (source or binary)**: NA
- **TensorFlow version (use command below)**: NA
- **Python version**:  NA
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

existing tf.contrib.seq2seq in not compatible with eager mode. Can you add a seq2seq example with the eager mode? Something similar to [this](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html). "
19114,Add support to identify elements in tensors,"Problem description:
I want to create symmetric matrix, which contains only n(n + 1) / 2 trainable parameters.
In general case i want to create tensor, where some elements are the same as trainable parameters.
Why is it so important?
1) PointNet article about 3D point cloud classification: contains symmetric matrices for estimation
2) Symmetric differential equations
2) Completeness of framework as a tool for optimization tasks

One possible way to do it is to stack and concatenate different variables, but i think, low level possibility of this feature has a better perfomance

Sorry for my English."
19113,[AdamOptimizer]Failed precondition: Attempting to use uninitialized value model_2/beta1_power,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu
- **TensorFlow version (use command below)**: r1.3
- **Python version**: 3.6

### Describe the problem
I write a model class that use a AdamOptimizer to update the parameters. It is a joint model so that there are two losses. It works well as I update all the parameters by the joint loss. But then I want to updated a subset of parameters by each of the loss respectively, that is , I had to call the apply_gradients() **twice**, it raised errors when I restore the model from .ckpt file: 'Failed precondition: Attempting to use uninitialized value model_2/beta1_power'.


### Source code / logs
This is the original source code when I update the model with the whole loss.

		taggingloss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels = self.tag, logits = self.tagginglogits))
		classifyloss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels = self.label, logits = self.logits))
		self.loss = (1 - alpha) * classifyloss + alpha * taggingloss
		# Gradients
		params = tf.trainable_variables()
		opt = tf.train.AdamOptimizer(learning_rate = self.learning_rate)
		gradient = tf.gradients(self.loss, params)
		clipped_gradients, norm = tf.clip_by_global_norm(gradient, max_gradient_norm)
		self.gradient_norm = norm
		self.update = opt.apply_gradients(zip(clipped_gradients, params), global_step = self.global_step)
		save_list = params
		save_list.append(self.global_step)
		self.saver = tf.train.Saver(save_list)

then I  revised it into:

		# Gradients
		params = tf.trainable_variables()
		opt = tf.train.AdamOptimizer(learning_rate = self.learning_rate)
		tag_gradient = tf.gradients(taggingloss, params)
		tag_clipped_gradients, tag_norm = tf.clip_by_global_norm(tag_gradient, max_gradient_norm)
		self.tag_gradient_norm = tag_norm
		self.tag_update = opt.apply_gradients(zip(tag_clipped_gradients, params), global_step = self.global_step)
		
		classify_gradient = tf.gradients(classifyloss, params)
		classify_clipped_gradients, classify_norm = tf.clip_by_global_norm(classify_gradient, max_gradient_norm)
		self.classify_gradient_norm = classify_norm
		self.classify_update = opt.apply_gradients(zip(classify_clipped_gradients, params), global_step = self.global_step)

		save_list = tf.trainable_variables()
		save_list.append(self.global_step)
		self.saver = tf.train.Saver(save_list)

The rest are all the same, except that, I used to run the self.update operation in the train() call. Now I first run self.tag_update in the tagging() call and then run self.classify_update in the train() call.

I have read the closed issue of https://github.com/tensorflow/tensorflow/issues/8057. it said in it that 

> I understood here such a thing: variables beta1_power and beta2_power are specific to each call to apply_gradients, but not to the whole graph. So if we want to call apply_gradients twice, two separate pairs of beta accumulators should be created, even if both calls are made within the single graph. This does not fit into the concept of ""graph slots"". Definitely, we should separate these slots by graphs, but we cannot simply key these slots by graphs only.

But I still have no idea how ti fix it."
19112,Feeding to a list of placeholders leads to the same values for the whole list placeholders,"Maybe it is not a bug, correct me if I misunderstand somewhere

Here's my code:

```
with tf.Graph().as_default():
    list_placeholder = [tf.placeholder(dtype=tf.int32, shape=[2])] * 3
    with tf.Session(config=config) as sess:
        real_values = sess.run(list_placeholder, feed_dict={
            list_placeholder[0]: [0, 0],
            list_placeholder[1]: [1, 1],
            list_placeholder[2]: [2, 2],
        })
        print(real_values)
```

It hopefully prints out something like `[0,0], [1,1], [2,2]`, because `list_placeholders` contain 3 separate elements. The actual output is:
`[array([2, 2], dtype=int32), array([2, 2], dtype=int32), array([2, 2], dtype=int32)]`

Some specs of system:
* Ubuntu 16.04
* I encountered this issue on both python 2.7 and python 3.6
* Tensorflow version: 1.4.1
"
19110,Add to documentation: how to install Tensorflow Lite onto Raspberry Pi 3B+ Raspian Stretch,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Raspberry Pi 3B+, Raspian Stretch
- **TensorFlow installed from (source or binary)**:
What I'm asking about. 
- **TensorFlow version (use command below)**:
Tensorflow Version 1.7.0
- **Python version**: 
Python 3.5
- **Bazel version (if compiling from source)**:
What I'm asking about. 
- **GCC/Compiler version (if compiling from source)**:
What I'm asking about. 
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
Broadcom VideoCore IV @ 250 MHz (BCM2837: 3D part of GPU @ 300 MHz, video part of GPU @ 400 MHz)
- **Exact command to reproduce**:
Addition to documentation: Installation of Tensorflow Lite on Raspberry Pi 3B+ Raspian Stretch. 

### Describe the problem
I would like instructions on how to install Tensorflow Lite onto Raspberry Pi 3B+ with Raspbian Stretch OS. To the best of my knowledge, the documentation doesn't yet cover installation for Raspbian Stretch. I posted on the Raspberry Pi Stack Exchange (https://raspberrypi.stackexchange.com/questions/83498/how-do-i-install-tensorflow-lite-on-raspbian-stretch) and I was directed here. My goal is to deploy a Tensorflow neural network onto the Raspberry Pi 3B+ with Tensorflow Lite. "
19109,IOError: [Errno 21] Is a directory: '/tmp/speech_dataset/',"I'm following the speech recognition tutorial from TensorFlow, and when I'm running the following command, which downloads the dataset provided by TensorFlow, it runs perfectly.

`python tensorflow/examples/speech_commands/train.py`

However, when I'm changing the defaults, so that it points to my dataset, it throws the following error:

```
Traceback (most recent call last):
  File ""/home/users2/lmn/.local/lib/python2.7/site-packages/tensorflow/examples/speech_commands/train.py"", line 428, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/users2/lmn/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/home/users2/lmn/.local/lib/python2.7/site-packages/tensorflow/examples/speech_commands/train.py"", line 106, in main
    FLAGS.testing_percentage, model_settings)
  File ""/home/users2/lmn/.local/lib/python2.7/site-packages/tensorflow/examples/speech_commands/input_data.py"", line 158, in __init__
    self.maybe_download_and_extract_dataset(data_url, data_dir)
  File ""/home/users2/lmn/.local/lib/python2.7/site-packages/tensorflow/examples/speech_commands/input_data.py"", line 204, in maybe_download_and_extract_dataset
    tarfile.open(filepath, 'r:gz').extractall(dest_directory)
  File ""/usr/lib64/python2.7/tarfile.py"", line 1693, in open
    return func(name, filemode, fileobj, **kwargs)
  File ""/usr/lib64/python2.7/tarfile.py"", line 1740, in gzopen
    fileobj = gzip.GzipFile(name, mode, compresslevel, fileobj)
  File ""/usr/lib64/python2.7/gzip.py"", line 94, in __init__
    fileobj = self.myfileobj = __builtin__.open(filename, mode or 'rb')
IOError: [Errno 21] Is a directory: '/tmp/speech_dataset/' 

The command I'm running is:
`python tensorflow/examples/speech_commands/train.py --data_url=path/to/data/ --sample_rate=20000 --wanted_words=one,two,three,four,five,six,seven,eight,nine`
```

Now, the error says that '/tmp/speech_dataset/' is a directory, but it is expecting a file, I guess. When I looked at `train.py` file, found the following code:

```
parser.add_argument(
      '--data_dir',
      type=str,
      default='/tmp/speech_dataset/',
      help=""""""\
      Where to download the speech training data to.
      """""")
```
The `--data-dir `argument defines where the files from the downloaded dataset should be stored. However, I'm not changing that argument at all, nor does the code need to save any data, since I already have the data on my computer, where I define them at `--data-url` argument. It seems to me that this is a bug in the code.

Does anyone has experience with speech recognition on TensorFlow and know where the problem might be?"
19108,Set the weights in tf.layers with other variables but not as initializers.,"### System information
- **Have I written custom code**: N/A (There is no example of meta learning with tf.layers)
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**: Installed tf-nightly with pip 
- **TensorFlow version**: v1.8.0-rc1-934-g291d85be42 1.9.0-dev20180426
- **Python version**: 3.5.2 
- **Bazel version**: N/A
- **GCC/Compiler version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
Nowadays, with advances in deep learning, the researchers sometimes need to set the weights of their model based on some formula (not initialize them but initialize something which calculates that formula) and then use that to backpropagate through the variables which computed that formula. For example in meta learning, I have a model, let's say it is just one layer tf.layers.dense(). I want to compute gradients and then use that gradients to build updated model graph and compute gradients on that. The backpropagation should tell me how should I update my first model weights to adapt well. Look at the code below:

### Source code / logs

    train = tf.placeholder(dtype=tf.float32, shape=(None, 4), name='train')
    train_out = tf.placeholder(dtype=tf.float32, shape=(None, 1), name='train_out')

    validation = tf.placeholder(dtype=tf.float32, shape=(None, 4))
    validation_out = tf.placeholder(dtype=tf.float32, shape=(None, 1))

    with tf.variable_scope('model'):
        model_out = tf.layers.dense(train, 1, activation=tf.nn.relu)

    with tf.variable_scope('loss'):
        loss = tf.square(model_out - train_out)

    with tf.variable_scope('gradients'):
        optimizer = tf.train.AdamOptimizer()
        grads = optimizer.compute_gradients(loss)

    with tf.variable_scope('updated_model'):
        updated_vars = {
            grad_info[1].name[6:]: grad_info[1] - 0.1 * grad_info[0] \
            for grad_info in grads if grad_info[0] is not None
            }

        # meta_out = tf.nn.relu(tf.matmul(validation, updated_vars['dense/kernel:0']) + updated_vars['dense/bias:0'])
        meta_out = tf.layers.dense(validation, 1, activation=tf.nn.relu, 
    kernel_initializer=updated_vars['dense/kernel:0'], bias_initializer=updated_vars['dense/bias:0'])


    with tf.variable_scope('meta_loss'):
        meta_loss = tf.square(meta_out - validation_out)

    with tf.variable_scope('meta_optimizer'):
        model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='model')
        meta_optimizer = tf.train.AdamOptimizer()
        meta_train_op = meta_optimizer.minimize(meta_loss, var_list=model_vars)


In this code, I have commented one line. In that line, I did not use tf.layers but instead implemented what should happen in that layer myself. I think we need to be able to do that with tf.layers as well. 
Please notice that we cannot use initializer because initializers should be set when we run tf.global_variables_initializer() and have another meaning(Which is to set the value of a variable). Here we do not want to initial those weights of layers with values but just with variables which we calculated in some other way and be able to backpropagate through them.

I think one way to solve this is to allow creating tf.layers' models with setting kernel and bias directly instead of using initializers. I would be more than happy if I could help with this issue."
19104,Illegal instruction (core dumped),"I have install Tensorflow via using this commands:
sudo apt-get install python3-pip python3-dev  :->  this for the 'pip' installation
pip3 install tensorflow   :-> this for a tensorflow installation

I am using ubantu 16.04 64 bit version Configuration as below :-
![screenshot from 2018-05-05 10-30-59](https://user-images.githubusercontent.com/30696388/39659867-076002a2-5050-11e8-8ee3-361881b92e3b.png)


still when as importing the tensorflow in python3 it gives ""Illegal instruction (core dumped)"" error
as :-1: 
![screenshot from 2018-05-05 10-37-31](https://user-images.githubusercontent.com/30696388/39659879-55f635da-5050-11e8-8fb4-03b0fce0571b.png)

"
19103,Build Pip Package Failed on Windows v1.8.0 with GPU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No!!
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: v1.8.0
- **Python version**:  3.6
- **Bazel version (if compiling from source)**: CMake
- **GCC/Compiler version (if compiling from source)**: VS 2015
- **CUDA/cuDNN version**: 9.1
- **GPU model and memory**: GTX-1070 8GB
- **Exact command to reproduce**: 
git pull
git checkout refs/tags/v1.8.0
cd tensorflow\contrib\cmake
mkdir build
cd build
cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^
-DSWIG_EXECUTABLE=C:\swigwin-3.0.12\swig.exe ^
-DPYTHON_EXECUTABLE=C:\Anaconda3\python.exe ^
-DPYTHON_LIBRARIES=C:\Anaconda3\libs\python36.lib ^
-Dtensorflow_ENABLE_GPU=ON ^
-DCUDNN_HOME=""%CUDA_PATH%"" ^
-Dtensorflow_CUDA_VERSION=9.1 ^
-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2

MSBuild /p:Configuration=Release tf_tutorials_example_trainer.vcxproj

MSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj

After this command, following error was occured. Sorry, this is Japanese System message.
But, problem is seems missing the ""tensorflow/python/framework/cpp_shape_inference.pb.h""

""C:\repo\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.
vcxproj"" (既定のターゲット) (1) ->
""C:\repo\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.v
cxproj"" (既定のターゲット) (2) ->
""C:\repo\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal_s
tatic.vcxproj"" (既定のターゲット) (3) ->
""C:\repo\tensorflow\tensorflow\contrib\cmake\build\tf_c_python_api.vcxproj"" (既 定
のターゲット) (134) ->
(ClCompile ターゲット) ->
  C:\repo\tensorflow\tensorflow\c\python_api.cc(19): fatal error C1083: include
 ファイルを開けません。'tensorflow/python/framework/cpp_shape_inference.pb.h':No such file
 or directory [C:\repo\tensorflow\tensorflow\contrib\cmake\build\tf_c_python_ap
i.vcxproj]

    90 個の警告 (Note: 90's Warning)
    1 エラー (Note: Error)

Best regards.
"
19100,Installation error on macos for 1.8.0/1.7.0/1.6.0 (Illegal instruction: 4),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.13.4
- **TensorFlow installed from (source or binary)**: binary (tensorflow-1.8.0-cp36-cp36m-macosx_10_11_x86_64.whl)
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: pip3 install tensorflow && python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I tried to install tensorflow and the module does not load. Same problem for all version up to 1.5.0 which then works fine. 
(with version 1.5.0)
```
 python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
/Users/marco/coding/crypto/_python_env/_mac/nn/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
v1.5.0-0-g37aa430d84 1.5.0
```
### Source code / logs
I run a verbose import as attached for the latest version (`python3 -v -m tensorflow 2&> verbose_import.txt`).
[verbose_import.txt](https://github.com/tensorflow/tensorflow/files/1976200/verbose_import.txt)

"
19093,tf.Variable uses about twice the memory (on the GPU),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary with pip using Anaconda
- **TensorFlow version (use command below)**: tensorflow-gpu 1.8.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: V9.0.176
- **GPU model and memory**: Quadro M2000M, 4G
- **Exact command to reproduce**: Please see problem description. Thank you.

### Describe the problem
This is almost the same to #13433 but on GPU. I think this is a bug/feature request. If not, please let me know. I use the following python code for testing both `tf.Variable` and `tf.get_variable`. The variable `v` should be 800MB.

    import tensorflow as tf

    sess_conf = tf.ConfigProto()
    sess_conf.gpu_options.allow_growth = True

    # Test the following for Tensor and Variable
    v = tf.Variable(tf.random_uniform((1024*1024, 100), dtype=tf.float64))
    # v = tf.get_variable('v', shape=(1024*1024, 100), initializer=tf.random_normal_initializer, dtype=tf.float64, trainable=False)

    init = tf.global_variables_initializer()
    with tf.Session(config=sess_conf) as sess:
        sess.run(init)

 And I use `nvidia-smi dmon -c 10 -s m` to monitor the memory usage. For both cases got:

    # gpu    fb  bar1
    # Idx    MB    MB
        0    42   227
        0    42   227
        0    42   227
        0    42   227
        0   102   227
        0  2163   227
        0  2163   227
        0  2163   227
        0  2163   227
        0  2163   227

Could you please let me know which part is wrong? Or is this an expected behavior? Thank you.
"
19086,[Feature request] SavedModelBuilder collection export,"Hello!
As far as I'm concerned, there is no way to export custom collection with `SavedModelBuilder`.
Current implementation uses `tf_saver.Saver.export_meta_graph ` method with empty `collection_list` parameter.

My suggestion is to add the `collection_list` parameter to the  `SavedModelBuilder.add_meta_graph_and_variables` and `SavedModelBuilder.add_meta_graph` methods. In this way, user can specify custom collections, that could be useful after model import."
19084,tensorflow.nn.dynamic_rnn with variable as init_state cannot work with Estimator.train,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows10
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.8.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I wrote my own function to generate the init state variable

```python
def get_initial_cell_state(cell, batch_size, dtype):
  state_size = cell.state_size
  i = 0
  def get_state_shape(s):
    c = _concat(1, s, static=True)
    nonlocal i
    name = ""init_state_"" + str(i)
    i = i + 1
    size = tf.get_variable(name, shape=c, dtype=dtype, initializer=tf.initializers.zeros)
    size = tf.tile(size, [batch_size] + [1] * (len(c) - 1))
    return size
  return nest.map_structure(get_state_shape, state_size)
```

And use it as below:

```python
  rnn_output, _ = tf.nn.dynamic_rnn(
    cell=cell,
    inputs=inputs,
    initial_state=get_initial_cell_state(cell, batch_size=batch_size, dtype=tf.float32),
    parallel_iterations=128,
    dtype=tf.float32
  )
```

It's compilable and runnable. But got this error:

`WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.`

I use Estimator to train. If I don't use this init state variable, everything goes fine.
"
19083,GPU remapping using visible_device_list is broken,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0
I have also tried this on 16.0 and 17.0, it crashes both of them. 
13.0 and 15.0 are fine.

- **Python version**:   3.6.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: Both 8.0, and 9.1 (with 9.0 libraries)
- **GPU model and memory**:   GeForce GTX 1080 Ti.  with 11178 MiB
- **Exact command to reproduce**:

import tensorflow as tf
G =tf.Graph()
sess1 = tf.Session(graph=G, config=tf.ConfigProto(log_device_placement=False,gpu_options=tf.GPUOptions(allow_growth=True,visible_device_list='0')))
sess2 = tf.Session(graph=G, config=tf.ConfigProto(log_device_placement=False,gpu_options=tf.GPUOptions(allow_growth=True,visible_device_list='1')))

Running the second tf.Session command crashes with the following error:

F tensorflow/core/common_runtime/gpu/gpu_id_manager.cc:45] Check failed: cuda_gpu_id.value() == result.first->second (1 vs. 0)Mapping the same TfGpuId to a different CUDA GPU id. TfGpuId: 0 Existing mapped CUDA GPU id: 0 CUDA GPU id being tried to map to: 1


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

The GPU remapping using visible_device_list is broken. This works fine in Tensorflow 1.3 and 1.5, but is completely broken (crashes the program) in 1.6, 1.7 and 1.8.
As far as I can tell from reading tensorflow/include/tensorflow/core/common_runtime/gpu/gpu_id.h
this mechanism is supposed to still work the same way it used to.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

import tensorflow as tf
G =tf.Graph()
sess1 = tf.Session(graph=G, config=tf.ConfigProto(log_device_placement=False,gpu_options=tf.GPUOptions(allow_growth=True,visible_device_list='0')))
sess2 = tf.Session(graph=G, config=tf.ConfigProto(log_device_placement=False,gpu_options=tf.GPUOptions(allow_growth=True,visible_device_list='1')))

F tensorflow/core/common_runtime/gpu/gpu_id_manager.cc:45] Check failed: cuda_gpu_id.value() == result.first->second (1 vs. 0)Mapping the same TfGpuId to a different CUDA GPU id. TfGpuId: 0 Existing mapped CUDA GPU id: 0 CUDA GPU id being tried to map to: 1
"
19081,tensorflow/python/ops/init_ops.py line 466 - initialization scaling incorrect?,"### System information : NOT APPLICABLE
Have I written custom code: No and N/A
OS Platform and Distribution: GNU/Linux Gentoo and N/A
TensorFlow installed from: https://www.tensorflow.org/install/install_sources
TensorFlow version:  1.6.0 but relevant to stable version on GitHub
Bazel version: 0.11.1 and N/A
CUDA/cuDNN version: 9.1/7.1 and N/A
GPU model and memory: Titan XP x2 - 12GB x2 and N/A
Exact command to reproduce: N/A
### Source code / logs:
tensorflow/python/ops/init_ops.py - lines 465-469:
```
    if self.distribution == ""normal"":
      stddev = math.sqrt(scale)
      return random_ops.truncated_normal(
          shape, 0.0, stddev, dtype, seed=self.seed)
```
### Describe the problem:
According to He et al (2015) [http://arxiv.org/abs/1502.0185] (Eq. 10) the correct scaling for the standard deviation for kernel initialization is sqrt(2/n). However line 466 of tensorflow/python/ops/init_ops.py reads:

`stddev = math.sqrt(scale)`

where the default `scale` is 1/n (n depending on fan mode), corresponding to sqrt(1/n) rather than sqrt(2/n) - therefore giving half the correct scaling.

Also - since a truncated (-2SD to +2SD) normal distribution is used rather than a full distribution, shouldn't the scale be multiplied by 1.137? Perhaps the simplest fix is therefore:

`stddev = math.sqrt(2.274*scale)`
 "
19080,Make canned estimators compute PR-AUC with right method.,"As we are sure about the error produced by computing *PR-AUC* with **trapezoidal** rule according to [this commit](https://github.com/tensorflow/tensorflow/commit/1f5324ca69bc1017972eef8e418691cff9a86dd7).
And also i found there is no proper workaround to make a canned estimator(Let's say [DNNClassifier](https://github.com/tensorflow/tensorflow/blob/2dc7575123ffa0e6413fc3d2700968ef25f049de/tensorflow/python/estimator/canned/dnn.py#L194)) to compute PR-AUC with the right method **careful_interpolation**.
Then, i did a simple change to this embarrassed situation: https://github.com/tensorflow/tensorflow/pull/19079

Does anyone can confirm this for me?


#### Update template content:

- Have I written custom code: No, i just use a canned estimator DNNClassifier.
- OS Platform and Distribution: macOS High Sierra
- TensorFlow installed from: pip
- TensorFlow version: 1.8.0
- Bazel version: N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A
- Exact command to reproduce: N/A"
19078,Getting an error while converting frozen inference graph (*.pb) to tflite graph with toco,"I've trained the R-FCN model on my own training set. Now, I'd like to convert my frozen inference graph form the *.pb format to the *.tflite format to use it on an android mobile phone.

After training, I exported the frozen inference graph with the following command:

```
python3 export_inference_graph.py 
--pipeline_config_path=""training/ckpt/rfcn-69/pipeline.config"" 
--trained_checkpoint_prefix=""training/ckpt/rfcn-69/model.ckpt-300000"" 
--output_directory=""training/ckpt/rfcn-69/""
```
Afterwards I run the transform_graph util to quantize the graph. I noticed that it doesn't matter wether I run the transform_graph or not. In the end I'm getting the same error with both of the graphs.

```
bazel run tensorflow/tools/graph_transforms/transform_graph -- 
--in_graph=""/git/bda/frozen_graphs/rfcn-69/frozen_inference_graph.pb"" 
--out_graph=""/git/bda/frozen_graphs/rfcn-69/quantized_graph_2.pb"" 
--inputs=image_tensor 
--outputs=""num_detections,detection_boxes,detection_scores,detection_classes"" 
--transforms='fold_old_batch_norms quantize_weights strip_unused_nodes sort_by_execution_order obfuscate_names merge_duplicate_nodes'
```
Finally, I try to convert the quantized graph to an tflite graph with the toco util.

```
bazel run --config=opt  tensorflow/contrib/lite/toco:toco -- 
--input_file=/git/bda/frozen_graphs/rfcn-69/quantized_graph_2.pb 
--output_file=/git/bda/frozen_graphs/out.tflite --inference_type=FLOAT 
--input_shape=1,600,1024,3 
--input_array=image_tensor 
--output_arrays=num_detections,detection_boxes,detection_scores,detection_classes
```
But, the operation fails with the following error.

```
2018-04-11 13:19:58.364591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArrayV3
2018-04-11 13:19:58.364606: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArrayV3
2018-04-11 13:19:58.364656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArrayReadV3
2018-04-11 13:19:58.364753: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Where
2018-04-11 13:19:58.364851: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Where
2018-04-11 13:19:58.364969: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Dequantize
2018-04-11 13:19:58.365001: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArraySizeV3
2018-04-11 13:19:58.365022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Dequantize
2018-04-11 13:19:58.366115: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Where
2018-04-11 13:20:00.620101: F tensorflow/contrib/lite/toco/tooling_util.cc:821] Check failed: d >= 1 (0 vs. 1)
```
I've got no clue what could be the problem. Any help would be highly appreciated.

Thanks in advance."
19077,CSV parsing flow (TextLineDataset -> decode_csv) doesn't parse linebreak in quotes correctly,"
Using tensorflow 1.8, parsing the following file with decode_csv:
""a
b"",0
raise the error ""Quoted field has to end with quote followed by delim or end"".

To reproduce:

```
file = ['example.csv']

dataset = tf.data.TextLineDataset(file)
dataset = dataset.map(lambda record: tf.decode_csv(
    record,
    record_defaults=[[""""],[0]]
))
iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()

with tf.Session() as sess:
    print(sess.run(next_element))
```

I think it is because the linebreak in the quote is not escaped. This behavior seems in contradiction with the RFC2048 specifications (section 2.6)."
19076,why ldd  libtensorflow_cc.so can not find library related to tensorrt ?,"I have build tensorflow c++ library from source code of tensorflow1.7-release vision, but when i ldd this so, i can not find any information about tenosrrt, why? And when i configure, i have designated the TRT library's path, and i use bazel to build ,the command is like this:
bazel build --config=cuda --config=monolithic //tensorflow:libtensorflow_cc.so

"
19074,Tensorboard is down after upgrading to the tensorflow 1.8.0,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:
binary

- **TensorFlow version (use command below)**:
1.8.0

- **Python version**: 
2.7.12

- **Bazel version (if compiling from source)**:
No

- **GCC/Compiler version (if compiling from source)**:
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609

- **CUDA/cuDNN version**:
CUDA 9.0
cuDNN 7.0

- **GPU model and memory**:
Nvidia Geforece GTX 1080

- **Exact command to reproduce**:
type 

> sudo pip install tensorboard

and then get

> tensorflow 1.4.1 requires tensorflow-tensorboard<0.5.0,>=0.4.0rc1, which is not installed.

But my tensorflow or tensorflow object detection api works.

### Describe the problem
Cannot install tensorboard from pip.

### Source code / logs
N/A
"
19073,Tensorflow-GPU costing 3130% cpu resouces not for computation,"My host has 16 core cpu (with x2 hyper-thread), when I run resnet50 (weights & sync on gpu) benchmark for Tensorflow CUDA version, it actually not need CPU to do any heavy job since training and weights merge are done at gpu device. But it costs nearly full CPU resources not for computing but for long pulling queries on `cuEventQuery`, and 99% of `cuEventQuery` replies NOT_FINISHED, this is extremely bad for cpu utilization, and block all CPU cores from doing other tasks.

According to my profiling, for Resnet50, Tensorflow costs only 14840 time of model forward and backward, but it gathers all threads to do full-time queries, and should cost 23564854 times to do useless `cuEventQuery` due to not ready, which is incredibly heavy and triggers 3130% CPU cost on Tensorflow GPU computational version."
19071,MobileNet v2 vs MobileNet v1 - bug with retrain.py and label_image.py examples from Tensorflow for Poets v2 transfer learning example!,"### System information
We have reproduced this on Mac OSX, a windows box running a Linux VM, and on a Canadian research cluster all with the same result.
 - mac was v1.7 recompiled to take advantage of CPU: ('v1.7.0-1321-gd82b2f71b6', '1.7.0')
 - stock tensorflow code and examples

### Description
We are using Tensorflow v1.7.  We train **MobileNet v2** (from tfHub - feature/1 version) by running the code Tensorflow provided in **retrain.py** (from examples). When training is done the validation set is run (for us about 700 images) and it reports say **80% validation accuracy**.   Then it saves the model.  We load the saved model using **label_image.py** (also from tensorflow examples) and see how it's doing on a small validation set that we've withheld.  Instead of seeing the validation of 80% roughly confirmed, we instead see a validation accuracy of about 35%.  The clincher is that when use the same code and data and instead use **MobileNet v1** we do see that training validation is confirmed by validation reported using label_image.py . 

When calling label_image we change the input_layer from ""input"" for MobileNet v1 to ""Placeholder"" for MobileNet v2

Can you confirm this? What is going on?

### Logs:
From retrain.py:
...
INFO:tensorflow:Initialize variable module/MobilenetV2/expanded_conv_9/project/weights:0 from checkpoint /var/folders/x8/f10sgc052q75r1thg5hlw4dm0000gn/T/tfhub_modules/33f8428fe83945b8b3d46d79168a0e2818e65e8a/variables/variables with MobilenetV2/expanded_conv_9/project/weights
INFO:tensorflow:Restoring parameters from /tmp/_retrain_checkpoint
INFO:tensorflow:Final test accuracy = 74.0% (N=696)
...
then from using label_image.py:

it gets 35% is correct

Have I written custom code: no
OS Platform and Distribution: macosx latest
TensorFlow installed from: pip
TensorFlow version: 1.7
Bazel version: Build label: 0.11.1-homebrew
   Build target: bazel-out/darwin- 
   opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
   Build time: Sun May 31 16:57:39 +50150 (1520426998659)
   Build timestamp: 1520426998659
   Build timestamp as int: 1520426998659

CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: see above
"
19069,Variable initialization under estimator + dynamic_rnn + MirroredStrategy (DistributionStrategy),"**System information**
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 and Windows 10
TensorFlow installed from (source or binary): Binary (pip)
TensorFlow version (use command below): 1.8.0
Python version: 3.6
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: GTX 1080 8 GiB (Windows) or two 8 GiB Tesla M60 (Ubuntu 16.04)
Exact command to reproduce: See python code (below)

**Describe the problem**
I am trying to transition my TensorFlow sequence2sequence codebase (RNN) into using the Estimator API, using the new tf.contrib.distribute.DistributionStrategy API. My provided runnable code snippet (below) works for a tf.contrib.distribute.OneDeviceStrategy, but not for a tf.contrib.distribute.MirroredStrategy.

The problem lies with initialization of the variables associated with RNN layers and the way these variables are handled/initialized using dynamic_rnn. 
The first tower seems to work fine, as it correctly uses the default callable initializer. The problem seems to be that the following towers are initialized directly with the tensors resulting from the initializers called on the first tower, which the dynamic_rnn (tf.nn.bidirectional_dynamic_rnn) API does not like. 

I have found three possible issues:

- (Possibly major?) Unable to initialize RNN parameters/variables, beyond the first tower, using tf.nn.bidirectional_dynamic_rnn and tf.contrib.rnn.LSTMCell together with a tf.contrib.distribute.MirroredStrategy.
-  (Minor) Optimizer API, such as tf.train.AdamOptimizer, seems to return a tensor containing the updated global_step tensor instead of a no_op, as it normally does, when running under a distribution strategy. (Seems this is an easy fix, e.g. by using tf.group (see below))
- (Minor) Error message: Would it not be more meaningful if the below error message ""... use a lambda as the initializer"" said something along the lines of: ""... use a callable, e.g. an Initializer (tf.keras.initializers.Initializer) or a lambda, as the initializer"" ?

Is there something i might have missed?
 
**Source code / logs**

    import tensorflow as tf

    class RNNModel(object):
    
      def __init__(self, hparams):
        return
    
      def __call__(self, features, labels, mode, params):
    
        inputs = features[0]
    
        print(inputs)
    
        cell_fw = tf.contrib.rnn.LSTMCell(  # tf.contrib.rnn.BasicLSTMCell
          300)
        cell_bw = tf.contrib.rnn.LSTMCell(  # tf.contrib.rnn.BasicLSTMCell
          300)
    
        (outputs, output_states) = tf.nn.bidirectional_dynamic_rnn(
          cell_fw,
          cell_bw,
          inputs,
          sequence_length=tf.convert_to_tensor([2, 2]),
          initial_state_fw=None,
          initial_state_bw=None,
          dtype=tf.float32,
          parallel_iterations=None,
          swap_memory=False,
          time_major=False,
          scope=None
        )
    
        model_output = tf.concat(outputs, axis=-1)
        mock_loss = 10 - tf.reduce_sum(model_output)
    
        train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mock_loss,
                                                                             global_step=tf.train.get_or_create_global_step())
        train_op = tf.group(train_op)
    
        if mode == tf.estimator.ModeKeys.TRAIN:
          return tf.estimator.EstimatorSpec(mode, loss=mock_loss, train_op=train_op)
        elif mode == tf.estimator.ModeKeys.EVAL:
          return tf.estimator.EstimatorSpec(mode, loss=mock_loss, eval_metric_ops=None)
        elif mode == tf.estimator.ModeKeys.PREDICT:
          predictions = {
            'mock': 1,
          }
          return tf.estimator.EstimatorSpec(mode, predictions=predictions)
    
    
    def train(hparams):
      model = RNNModel(hparams)
    
      distribution_strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=2)
      #distribution_strategy = tf.contrib.distribute.OneDeviceStrategy(tf.DeviceSpec(device_type=""GPU"", device_index=0))
    
      checkpointing_config = tf.estimator.RunConfig(
        save_checkpoints_secs=20 * 60,  # Save checkpoints every 20 minutes.
        keep_checkpoint_max=10,  # Retain the 10 most recent checkpoints.
        train_distribute=distribution_strategy
      )
    
      estimator = tf.estimator.Estimator(
        model_fn=model,
        model_dir=""out/"",
        params=hparams,
        config=checkpointing_config,
      )
    
      def create_mock_dataset():
        mock_data = tf.convert_to_tensor([[[0.0, 1.0, 2.0, 3.0, 4.0], [0.0, 1.0, 2.0, 3.0, 4.0]],
                                          [[0.0, 1.0, 2.0, 3.0, 4.0], [0.0, 1.0, 2.0, 3.0, 4.0]]])
    
        mock_data_set = tf.data.Dataset.from_tensors(mock_data)
        mock_data_set = mock_data_set.map(lambda mock_data: (((mock_data,)), ()) )
    
        return mock_data_set
    
      num_train_steps = 100
      estimator.train(create_mock_dataset, hooks=None,
                      steps=num_train_steps)
    
    if __name__ == '__main__':
      train({""mock"": ""mock""})

The code produces the following  output:
```
Traceback (most recent call last):
File ""C:\Python36\lib\runpy.py"", line 193, in _run_module_as_main
""main"", mod_spec)
File ""C:\Python36\lib\runpy.py"", line 85, in _run_code
exec(code, run_globals)
File ""C:\Users\marhl\nmt\tfDeepNLP\models\seq2seq\run_seq2seq.py"", line 105, in 
tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
File ""C:\Python36\lib\site-packages\tensorflow\python\platform\app.py"", line 126, in run
_sys.exit(main(argv))
File ""C:\Users\marhl\nmt\tfDeepNLP\models\seq2seq\run_seq2seq.py"", line 96, in main
run_main(FLAGS, default_hparams, train_fn, inference_fn, hparams_creator)
File ""C:\Users\marhl\nmt\tfDeepNLP\models\seq2seq\run_seq2seq.py"", line 88, in run_main
train_fn(hparams)
File ""C:\Users\marhl\nmt\tfDeepNLP\models\seq2seq\train_estimator.py"", line 470, in s2s_train
estimator.train(input_fn=train_input_fn, hooks=train_hooks,steps=num_train_steps)
File ""C:\Python36\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 363, in train
loss = self._train_model(input_fn, hooks, saving_listeners)
File ""C:\Python36\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 841, in _train_model
return self._train_model_distributed(input_fn, hooks, saving_listeners)
File ""C:\Python36\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 884, in _train_model_distributed
self.config)
File ""C:\Python36\lib\site-packages\tensorflow\python\training\distribute.py"", line 756, in call_for_each_tower
return self._call_for_each_tower(fn, *args, **kwargs)
File ""C:\Python36\lib\site-packages\tensorflow\contrib\distribute\python\mirrored_strategy.py"", line 254, in _call_for_each_tower
coord.join(threads)
File ""C:\Python36\lib\site-packages\tensorflow\python\training\coordinator.py"", line 389, in join
six.reraise(*self._exc_info_to_raise)
File ""C:\Python36\lib\site-packages\six.py"", line 693, in reraise
raise value
File ""C:\Python36\lib\site-packages\tensorflow\python\training\coordinator.py"", line 297, in stop_on_exception
yield
File ""C:\Python36\lib\site-packages\tensorflow\contrib\distribute\python\mirrored_strategy.py"", line 465, in run
self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
File ""C:\Python36\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 831, in _call_model_fn
model_fn_results = self._model_fn(features=features, **kwargs)
File ""C:\Users\marhl\nmt\tfDeepNLP\models\seq2seq\s2s_base_model.py"", line 157, in call
res = self.build_graph(hparams, features, labels)
File ""C:\Users\marhl\nmt\tfDeepNLP\models\seq2seq\hierarchical_model.py"", line 83, in build_graph
encoded_outputs, encoded_state = self._build_encoder(hparams, features)
File ""C:\Users\marhl\nmt\tfDeepNLP\models\seq2seq\hierarchical_model.py"", line 202, in _build_encoder
return self._build_flat_encoder(hparams, features)
File ""C:\Users\marhl\nmt\tfDeepNLP\models\seq2seq\hierarchical_model.py"", line 250, in _build_flat_encoder
num_bi_residual_layers=num_bi_residual_layers))
File ""C:\Users\marhl\nmt\tfDeepNLP\models\seq2seq\hierarchical_model.py"", line 768, in _build_bidirectional_rnn
time_major=self.time_major)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\rnn.py"", line 412, in bidirectional_dynamic_rnn
time_major=time_major, scope=fw_scope)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\rnn.py"", line 627, in dynamic_rnn
dtype=dtype)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\rnn.py"", line 824, in _dynamic_rnn_loop
swap_memory=swap_memory)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 3224, in while_loop
result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2956, in BuildLoop
pred, body, original_loop_vars, loop_vars, shape_invariants)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2893, in _BuildLoop
body_result = body(*packed_vars_for_body)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 3194, in 
body = lambda i, lv: (i + 1, orig_body(*lv))
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\rnn.py"", line 793, in _time_step
skip_conditionals=True)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\rnn.py"", line 248, in _rnn_step
new_output, new_state = call_cell()
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\rnn.py"", line 781, in 
call_cell = lambda: cell(input_t, state)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py"", line 232, in call
return super(RNNCell, self).call(inputs, state)
File ""C:\Python36\lib\site-packages\tensorflow\python\layers\base.py"", line 717, in call
outputs = self.call(inputs, *args, **kwargs)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py"", line 1292, in call
cur_inp, new_state = cell(cur_inp, cur_state)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py"", line 1099, in call
output, new_state = self._cell(inputs, state, scope=scope)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py"", line 339, in call
*args, **kwargs)
File ""C:\Python36\lib\site-packages\tensorflow\python\layers\base.py"", line 699, in call
self.build(input_shapes)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py"", line 767, in build
partitioner=maybe_partitioner)
File ""C:\Python36\lib\site-packages\tensorflow\python\layers\base.py"", line 546, in add_variable
partitioner=partitioner)
File ""C:\Python36\lib\site-packages\tensorflow\python\training\checkpointable.py"", line 436, in _add_variable_with_custom_getter
**kwargs_for_getter)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 1317, in get_variable
constraint=constraint)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 1079, in get_variable
constraint=constraint)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 417, in get_variable
return custom_getter(**custom_getter_kwargs)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 1720, in wrapped_custom_getter
*args, **kwargs)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py"", line 235, in _rnn_get_variable
variable = getter(*args, **kwargs)
File ""C:\Python36\lib\site-packages\tensorflow\python\training\distribute.py"", line 575, in disable_partitioned_variables
return getter(*args, **kwargs)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 394, in _true_getter
use_resource=use_resource, constraint=constraint)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 786, in _get_single_variable
use_resource=use_resource)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 2220, in variable
use_resource=use_resource)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 2198, in 
return lambda **kwargs: captured_getter(captured_previous, **kwargs)
File ""C:\Python36\lib\site-packages\tensorflow\contrib\distribute\python\shared_variable_creator.py"", line 69, in create_new_variable
v = next_creator(*args, **kwargs)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 2198, in 
return lambda **kwargs: captured_getter(captured_previous, **kwargs)
File ""C:\Python36\lib\site-packages\tensorflow\python\training\distribute.py"", line 568, in creator_with_resource_vars
return self._create_variable(*args, **kwargs)
File ""C:\Python36\lib\site-packages\tensorflow\contrib\distribute\python\mirrored_strategy.py"", line 118, in _create_variable
v = next_creator(*args, **kwargs)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 2210, in 
previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 2182, in default_variable_creator
constraint=constraint)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 282, in init
constraint=constraint)
File ""C:\Python36\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 421, in _init_from_args
""initializer."" % name)
ValueError: Initializer for variable encoder/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/replica_1/ is from inside a control-flow construct, such as a loop or conditional. When creating a variable inside a loop or conditional, use a lambda as the initializer.
```"
19068, Could not find a version that satisfies the requirement tensorflow (from versions: ) No matching distribution found for tensorflow,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
19066,Segfault Custom Op using KenLM,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0 / 7
- **GPU model and memory**: TitanX
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

When I use KenLM in my C++ test suite it runs just fine but when I try to use it from inside a custom op I get a Segmentation Fault. Any help would be greatly appreciated. Thx!

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```c
#include ""lm/model.hh""

#include ""ctc_beam.hh""

#ifndef CTC_SCORER_HH
#define CTC_SCORER_HH

class CTCBeamScorer {
    public:
    CTCBeamScorer(std::string lm_path, float alpha, float beta) : alpha_(alpha), beta_(beta) {
        model_ = lm::ngram::LoadVirtual(lm_path.c_str());
        vocabulary_ = &model_->BaseVocabulary();
    }
    ~CTCBeamScorer(void) {
        delete model_;
    }

    void expand_beam(const CTCBeam* beam, int from_label, int to_label) { }

    void expand_beam_end(CTCBeam* beam) const { }

    float get_beam_score(const CTCBeam* beam) const { return 0.0f; }

    void initialize_beam(CTCBeam* beam) const { }

    private:
    const float alpha_ = 0.0;
    const float beta_ = 0.0;
    lm::base::Model* model_ = nullptr;
    const lm::base::Vocabulary* vocabulary_ = nullptr;
};

#endif // CTC_SCORER_HH
```
*THIS WORKS*
```c
#include <gtest/gtest.h>

#include ""../src/ctc_scorer.hh""

TEST(ctc_scorer_tests, score) {
    CTCBeamScorer* scorer = nullptr;
    try {
        scorer = new CTCBeamScorer(""/tmp/test.mmap"", 0.0f, 0.0f);
        ASSERT_FALSE(scorer == nullptr);
    } catch(const std::exception &e) {
        std::cerr << e.what() << std::endl;
    }

    delete scorer;
}
```

```
g++ -Wall -O3 -std=c++11 -g -I/home/thomas/projects/thomas/ops/thirdparty/kenlm -DKENLM_MAX_ORDER=6 -DHAVE_ZLIB=1 -DHAVE_BZLIB=1 -DHAVE_XZLIB=1 -o tests/test_runner tests/test_runner.cc tests/ctc_scorer_tests.cc src/ctc_scorer.cc thirdparty/kenlm/util/string_piece.o thirdparty/kenlm/util/exception.o thirdparty/kenlm/util/spaces.o thirdparty/kenlm/util/pool.o thirdparty/kenlm/util/file_piece.o thirdparty/kenlm/util/usage.o thirdparty/kenlm/util/parallel_read.o thirdparty/kenlm/util/bit_packing.o thirdparty/kenlm/util/float_to_string.o thirdparty/kenlm/util/ersatz_progress.o thirdparty/kenlm/util/integer_to_string.o thirdparty/kenlm/util/mmap.o thirdparty/kenlm/util/murmur_hash.o thirdparty/kenlm/util/read_compressed.o thirdparty/kenlm/util/scoped.o thirdparty/kenlm/util/file.o thirdparty/kenlm/lm/value_build.o thirdparty/kenlm/lm/config.o thirdparty/kenlm/lm/model.o thirdparty/kenlm/lm/trie.o thirdparty/kenlm/lm/sizes.o thirdparty/kenlm/lm/search_hashed.o thirdparty/kenlm/lm/lm_exception.o thirdparty/kenlm/lm/virtual_interface.o thirdparty/kenlm/lm/binary_format.o thirdparty/kenlm/lm/trie_sort.o thirdparty/kenlm/lm/vocab.o thirdparty/kenlm/lm/search_trie.o thirdparty/kenlm/lm/read_arpa.o thirdparty/kenlm/lm/quantize.o thirdparty/kenlm/lm/bhiksha.o thirdparty/kenlm/util/double-conversion/strtod.o thirdparty/kenlm/util/double-conversion/fast-dtoa.o thirdparty/kenlm/util/double-conversion/double-conversion.o thirdparty/kenlm/util/double-conversion/cached-powers.o thirdparty/kenlm/util/double-conversion/bignum.o thirdparty/kenlm/util/double-conversion/fixed-dtoa.o thirdparty/kenlm/util/double-conversion/bignum-dtoa.o thirdparty/kenlm/util/double-conversion/diy-fp.o -lgtest -lpthread -lboost_program_options -lboost_system -lboost_thread -lz -lbz2 -llzma
```

*THIS DOES NOT WORK*
```c
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/op_kernel.h""
#include ""tensorflow/core/framework/shape_inference.h""

#include ""ctc_scorer.hh""

using namespace tensorflow;

REGISTER_OP(""CTCKenLMBeamSearchDecoder"")
    .Input(""scores: float32"")
    .Output(""output: float32"")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* context) {
      context->set_output(0, context->input(0));
      return Status::OK();
    });

class CTCKenLMBeamSearchDecoderOp : public OpKernel {
    public:
    explicit CTCKenLMBeamSearchDecoderOp(OpKernelConstruction* context) : OpKernel(context) {
        CTCBeamScorer* scorer = nullptr;
        try {
            scorer = new CTCBeamScorer(""/tmp/test.mmap"", 0.0f, 0.0f);
        } catch(const std::exception &e) {
            std::cerr << e.what() << std::endl;
        }

        delete scorer;
    }

    ~CTCKenLMBeamSearchDecoderOp(void) {
        
    }

    void Compute(OpKernelContext* context) override {
        // Grab the input tensor
        const Tensor& scores_tensor = context->input(0);
        auto scores = scores_tensor.tensor<float, 3>();

        // Create an output tensor
        Tensor* output_tensor = NULL;
        OP_REQUIRES_OK(context, context->allocate_output(0, scores_tensor.shape(), &output_tensor));
        auto output = output_tensor->tensor<float, 3>();

        // Copy the scores to the output.
        const auto scores_shape = scores_tensor.shape();
        for (unsigned int x_idx = 0; x_idx < scores_shape.dim_size(0); x_idx++) {
            for (unsigned int y_idx = 0; y_idx < scores_shape.dim_size(1); y_idx++) {
                for (unsigned int z_idx = 0; z_idx < scores_shape.dim_size(2); z_idx++) {
                    output(x_idx, y_idx, z_idx) = scores(x_idx, y_idx, z_idx);
                }
            }
        }
    }
};

REGISTER_KERNEL_BUILDER(Name(""CTCKenLMBeamSearchDecoder"").Device(DEVICE_CPU), CTCKenLMBeamSearchDecoderOp);
```

```
g++ -Wall -O3 -std=c++11 -g -I/home/thomas/projects/thomas/ops/thirdparty/kenlm -DKENLM_MAX_ORDER=6 -DHAVE_ZLIB=1 -DHAVE_BZLIB=1 -DHAVE_XZLIB=1 -I/usr/local/lib/python3.5/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -shared -o libs/ctc_decoder.so src/ctc_decoder_op.cc src/ctc_scorer.cc thirdparty/kenlm/util/string_piece.o thirdparty/kenlm/util/exception.o thirdparty/kenlm/util/spaces.o thirdparty/kenlm/util/pool.o thirdparty/kenlm/util/file_piece.o thirdparty/kenlm/util/usage.o thirdparty/kenlm/util/parallel_read.o thirdparty/kenlm/util/bit_packing.o thirdparty/kenlm/util/float_to_string.o thirdparty/kenlm/util/ersatz_progress.o thirdparty/kenlm/util/integer_to_string.o thirdparty/kenlm/util/mmap.o thirdparty/kenlm/util/murmur_hash.o thirdparty/kenlm/util/read_compressed.o thirdparty/kenlm/util/scoped.o thirdparty/kenlm/util/file.o thirdparty/kenlm/lm/value_build.o thirdparty/kenlm/lm/config.o thirdparty/kenlm/lm/model.o thirdparty/kenlm/lm/trie.o thirdparty/kenlm/lm/sizes.o thirdparty/kenlm/lm/search_hashed.o thirdparty/kenlm/lm/lm_exception.o thirdparty/kenlm/lm/virtual_interface.o thirdparty/kenlm/lm/binary_format.o thirdparty/kenlm/lm/trie_sort.o thirdparty/kenlm/lm/vocab.o thirdparty/kenlm/lm/search_trie.o thirdparty/kenlm/lm/read_arpa.o thirdparty/kenlm/lm/quantize.o thirdparty/kenlm/lm/bhiksha.o thirdparty/kenlm/util/double-conversion/strtod.o thirdparty/kenlm/util/double-conversion/fast-dtoa.o thirdparty/kenlm/util/double-conversion/double-conversion.o thirdparty/kenlm/util/double-conversion/cached-powers.o thirdparty/kenlm/util/double-conversion/bignum.o thirdparty/kenlm/util/double-conversion/fixed-dtoa.o thirdparty/kenlm/util/double-conversion/bignum-dtoa.o thirdparty/kenlm/util/double-conversion/diy-fp.o -lboost_program_options -lboost_system -lboost_thread -lz -lbz2 -llzma -L/usr/local/lib/python3.5/dist-packages/tensorflow -ltensorflow_framework
```
"
19063,slim.conv2d_transpose  has no output_shape,"slim.conv2d_transpose  has no output_shape param, So I can't control the output_shape"
19062,train_and_evaluate does not preserve the Dataset iterator state across train/eval,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: see below

### Describe the problem

When the input function is based on the one-shot dataset iterator, the training phase always starts from the beginning of the iterator. That is, the iterator state gets reset in between the train/eval phases. Therefore, if the dataset is big enough, then the training would only see a subset of the data, which can be processed in `eval_spec.throttle_secs`. 

I think the issue is caused by the fact that the graph is persisted before transitioning to the next phase, and restored upon reentering training. However, I find the behaviour a bit counterintuitive, so if it is not a bug, it should be mentioned in the `train_and_evaluate` docs.

### Source code / logs

Here is a small example demonstrating the issue:

```python
import tensorflow as tf


def input_fn(data):
    dataset = tf.data.Dataset.from_tensor_slices(data)
    dataset = dataset.batch(batch_size=1)
    x = dataset.make_one_shot_iterator().get_next()
    return {""x"": tf.Print(x, [x])}, x


if __name__ == ""__main__"":
    model = tf.estimator.LinearRegressor(feature_columns=[
        tf.feature_column.numeric_column(""x"")
    ])

    train_spec = tf.estimator.TrainSpec(
        input_fn=lambda: input_fn(list(range(2**20))),
        max_steps=2)
    eval_spec = tf.estimator.EvalSpec(
        input_fn=lambda: input_fn([42]),
        steps=1,
        start_delay_secs=1,
        throttle_secs=1)

    tf.logging.set_verbosity(""INFO"")
    tf.train.create_global_step()
    tf.estimator.train_and_evaluate(model, train_spec, eval_spec)
```

The code produces the following log output (I've omitted irrelevant lines):

```
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 1 secs (eval_spec.throttle_secs) or training is finished.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
[0]
INFO:tensorflow:Saving checkpoints for 1 into /var/folders/wr/s7brqkzj74v4pmdwkwn19321l34xg2/T/tmpgtq1ap9_/model.ckpt.
INFO:tensorflow:loss = 0.0, step = 1
INFO:tensorflow:Loss for final step: 0.0.
...
INFO:tensorflow:Starting evaluation at 2018-05-03-16:21:51
...
INFO:tensorflow:Finished evaluation at 2018-05-03-16:21:51
...
INFO:tensorflow:Restoring parameters from /var/folders/wr/s7brqkzj74v4pmdwkwn19321l34xg2/T/tmpgtq1ap9_/model.ckpt-1
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
[0]
INFO:tensorflow:Saving checkpoints for 2 into /var/folders/wr/s7brqkzj74v4pmdwkwn19321l34xg2/T/tmpgtq1ap9_/model.ckpt.
INFO:tensorflow:loss = 0.0, step = 2
INFO:tensorflow:Loss for final step: 0.0.
...
```
"
19061,Tensorflow serving pages missing on deploy page,"From the [TensorFlow Deploy page](https://www.tensorflow.org/deploy/), the links for TensorFlow serving all give 404 page not found errors. 

For example, try the [Installation page](https://www.tensorflow.org/setup)

For example, try the [Serving a TensorFlow Model](https://www.tensorflow.org/serving_basic)"
19060,data.prefetch_to_device does not work with tf.data.Iterator.from_structure(),"it works with make_one_shot_iterator(), I can see that the training time is 2x lower, when using the make_initializable_iterator() the time is the same as without prefetching to device.
how to use the new feature when dataset switch (train/val/test) is needed for evaluation purpose?
code parts
//not working
trainDataset = tf.data.Dataset.from_tensor_slices((trainFeatures,trainLabels,trainLengths,trainMasks))
trainDataset = trainDataset.batch(batchSize)
trainDataset = trainDataset.apply(tf.contrib.data.shuffle_and_repeat(100,nEpochs))
iterator = tf.data.Iterator.from_structure(trainDataset.output_types, trainDataset.output_shapes)
train_init_op = iterator.make_initializer(trainDataset)
trainDataset = trainDataset.apply(tf.contrib.data.prefetch_to_device('/gpu:0'))

//working
trainDataset = tf.data.Dataset.from_tensor_slices((trainFeatures,trainLabels,trainLengths,trainMasks))
trainDataset = trainDataset.batch(batchSize)
trainDataset = trainDataset.apply(tf.contrib.data.shuffle_and_repeat(100,nEpochs))
trainDataset = trainDataset.apply(tf.contrib.data.prefetch_to_device('/gpu:0'))
iterator = trainDataset.make_one_shot_iterator()"
19059,wrong,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
19058,CNN features feed to LSTM Tensorflow,"Hello Guys
So recently i am working on a project which i am supposed to take images as input to a CNN and extract the features and feed them to LSTM for training. I am using 2 Layer CNN for feature extraction and im taking the features form fully connected layer and trying to feed them to LSTM. Problem is when i want to feed the FC layer to LSTM as input i get error regarding to wrong dimension. my FC layer is a Tensor with (128,1024) dimension. I tried to reshape it like this tf.reshape(fc,[-1]) which gives me a tensor ok (131072, )
dimension and still wont work. Could anyone give me any ideas of how im suppose to feed the FC to LSTM?here i just write part of my code and teh error i get.
# Convolution Layer with 32 filters and a kernel size of 5
        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)
        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2
        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)

        # Convolution Layer with 32 filters and a kernel size of 5
        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)
        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2
        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)

        # Flatten the data to a 1-D vector for the fully connected layer
        fc1 = tf.contrib.layers.flatten(conv2)

        # Fully connected layer (in contrib folder for now)
        fc1 = tf.layers.dense(fc1, 1024)
        # Apply Dropout (if is_training is False, dropout is not applied)
        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)
        s = tf.reshape(fc1, [1])
	rnn_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)
	outputs, states = rnn.static_rnn(rnn_cell, s, dtype=tf.float32)
	return tf.matmul(outputs[-1], rnn_weights['out']) + rnn_biases['out']
	here is the error:
ValueError: Cannot reshape a tensor with 131072 elements to shape [1] (1 elements) for 'ConvNet/Reshape' (op: 'Reshape') with input shapes: [128,1024], [1] and with input tensors computed as partial shapes: input[1] = [1].
"
19056,Android tensorflow repository weird permissions,"when using `implementation 'org.tensorflow:tensorflow-android:+'` in gradle file of my android project

it adds those permissions to apk

```
uses-permission: name='android.permission.WRITE_EXTERNAL_STORAGE'
uses-permission: name='android.permission.READ_PHONE_STATE'
uses-permission: name='android.permission.READ_EXTERNAL_STORAGE'
```

once I remove  `implementation 'org.tensorflow:tensorflow-android:+'`  permissions are gone

what is that?"
19054,Regularization loss will duplicated when reusing PartitionedVariable in tf.layers,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 2.7.11
- **Bazel version (if compiling from source)**: 0.10.0
- **GCC/Compiler version (if compiling from source)**: 4.8.5
- **CUDA/cuDNN version**: CUDA 8.0/ cuDNN 7
- **GPU model and memory**: N/A
- **Exact command to reproduce**: see below

### Describe the problem
When reusing a variable in current variable scope, we should always reuse its regularization loss computation. But it will declare regularization loss multiple times when reusing PartitionedVariables in tf.layers. I have found that there is no special treatment for reusing regularization loss of PartitionedVariables in tf.layers .
### Source code / logs
Here is the small script can reproduce the result.

```
import tensorflow as tf
partitioner = tf.fixed_size_partitioner(3)
l2_regularizer = tf.contrib.layers.l2_regularizer(0.001)
for i in xrange(2):
  with tf.variable_scope(tf.get_variable_scope(), partitioner=partitioner, reuse=False if i == 0 else True):
    inputs_tensor = tf.constant(1.0, shape=[100, 100])
    logits = tf.layers.dense(inputs_tensor, 256, use_bias=False, name=""fc"", kernel_regularizer=l2_regularizer)
print (tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))
```

This short program should get result 3 because the PartitionedVariable has 3 shards. However, it got 6. 

A pull request has been submitted here to fix this bug: https://github.com/tensorflow/tensorflow/pull/19053
"
19051,After tensorflow-lite-0.1.7 update Image classification using inception-v3 model stops working,"Image classification using inceptionv3_slim_2016.tflite is not working after the recent update of 0.1.7 version. Cannot allocate memory for the interpreter.
It gives the below exception :

java.lang.RuntimeException: Unable to start activity ComponentInfo{android.example.com.tflitecamerademo/com.example.android.tflitecamerademo.CameraActivity}: java.lang.NullPointerException: Internal error: Cannot allocate memory for the interpreter"
19049,Memory Leak when tf.Session run on the sliced tensor,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.5.0-0-g37aa430d84 1.5.0
- **Python version**: 3.6
- **CUDA/cuDNN version**: release 9.0, V9.0.176
- **GPU model and memory**: 1070 8G

### Describe the Feature Request
For current version of Tensorflow, it will leak memory if we evaluate the sliced tensor as attached code and images, which isn't intuitive and hard to debug. It seems like the unused part of tensor will leak memory because there is no python object reference on it. I am not sure whether it could fixed or not, maybe it could show warning at least.

### Source code / logs

`import tensorflow as tf`
`from pympler.tracker import SummaryTracker`
`tracker = SummaryTracker()`
`a = tf.zeros([3,4,5])`
`sess = tf.Session()`

`def leak_version():`
`____return sess.run(a[0,0])`

`def safe_version():`
`____return sess.run(a)[0,0]` 

`for i in range(10):`
`____tracker.print_diff()`
`____# b = safe_version()`
`____b = leak_version()`

#### Trace of the Leak
![image](https://user-images.githubusercontent.com/5878561/39568787-ed1af628-4ef5-11e8-80a0-b1e532b8c81c.png)

#### Leak Version : visualize the object reference graph on object 'b' with depth=5 by Library objgraph
![image](https://user-images.githubusercontent.com/5878561/39569067-adc03b18-4ef6-11e8-8d68-b5dee8a89eb0.png)

#### Safe Version : visualize the object reference graph on object 'b' with depth=5 by Library objgraph
![image](https://user-images.githubusercontent.com/5878561/39568928-57ccc000-4ef6-11e8-871d-7507a6dfefb4.png)

"
19048,Android camera demo,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
19047,keras call convert_model from theano error.,"    convert_model(model)
  File ""../utils.py"", line 31, in convert_model
    ops.append (tf.assign (layer.kernel, converted_w).op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py"", line 273, in assign
    if ref.dtype._is_ref_dtype:
AttributeError: 'str' object has no attribute '_is_ref_dtype'
"
19046,Building from source: No such file or directory,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master branch pull yesterday (077ae6d)
- **Python version**: Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19)  [GCC 7.2.0] on linux
- **Bazel version (if compiling from source)**: 0.13.0
- **GCC/Compiler version (if compiling from source)**: 4.8.4
- **CUDA/cuDNN version**: only CPU
- **GPU model and memory**: only CPU
- **Exact command to reproduce**: according to tutorial ""bazel build --config=opt --verbose_failures //tensorflow/tools/pip_package:build_pip_package""

### Describe the problem
When I build from source according to the tutorial, I get the following error (see Logs below).

### Source code / logs

**My configuration:**

Please specify the location of python. [Default is /home/rodin_maxim93/programs/miniconda3/bin/python]: 
Found possible Python library paths:
  /home/rodin_maxim93/programs/miniconda3/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is 

[/home/rodin_maxim93/programs/miniconda3/lib/python3.6/site-packages]

Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: 
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n
No Google Cloud Platform support will be enabled for TensorFlow.
Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n
No Hadoop File System support will be enabled for TensorFlow.
Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n
No Amazon S3 File System support will be enabled for TensorFlow.
Do you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n
No Apache Kafka Platform support will be enabled for TensorFlow.
Do you wish to build TensorFlow with XLA JIT support? [y/N]: 
No XLA JIT support will be enabled for TensorFlow.
Do you wish to build TensorFlow with GDR support? [y/N]: 
No GDR support will be enabled for TensorFlow.
Do you wish to build TensorFlow with VERBS support? [y/N]: 
No VERBS support will be enabled for TensorFlow.
Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.
Do you wish to build TensorFlow with CUDA support? [y/N]: 
No CUDA support will be enabled for TensorFlow.
Do you wish to download a fresh release of clang? (Experimental) [y/N]: 
Clang will not be downloaded.
Do you wish to build TensorFlow with MPI support? [y/N]: 
No MPI support will be enabled for TensorFlow.
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 
Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.
Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
Configuration finished


**The ERROR:**

```
ERROR: /home/rodin_maxim93/programs/distr/tensorflow/tensorflow/core/BUILD:2015:1: Executing genrule //tensorflow/core:version_info_gen failed (Exit 127): bash failed: error executing command 
  (cd /home/rodin_maxim93/.cache/bazel/_bazel_rodin_maxim93/7b13afc3c7dd373845b8c107b3b3738b/execroot/org_tensorflow && \exec env - \
 PATH=/home/rodin_maxim93/programs/miniconda3/bin:/home/rodin_maxim93/programs/miniconda3/bin:/usr/local/sbin:/

usr/local/bin:/usr/s
bin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; tensorflow/tools/git/gen_git_source.py --

generate external
/local_config_git/gen/spec.json external/local_config_git/gen/head external/local_config_git/gen/branch_ref ""bazel-

out/host/genfiles/t
ensorflow/core/util/version_info.cc"" --git_tag_override=${GIT_TAG_OVERRIDE:-}')
: No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 139.686s, Critical Path: 18.66s
INFO: 955 processes, local.
FAILED: Build did NOT complete successfully


```
"
19045,tf.boolean_mask always return empty results while the mask has true value,"### System information
-  CentOS Linux release 7.4.1708 (Core)
-  TensorFlow installed from source 
-  TensorFlow version: 1.3.0
-  Python version: 3.6.3
-  gcc version 4.8.5 20150623 (Red Hat 4.8.5-16) (GCC) 
-  Cuda compilation tools, release 9.1, V9.1.85
-  GPU GeForce GTX 1080Ti *2

### Describe the problem
I am trying to use the function of tf.boolean_mask to select value from a 5D tensor. To make things easier, I just make my mask as same shape of target tensor, that is (1,12,125,179,1), but it will return zero dimension tensor as result.

Then I tried with the example tensorflow provided in official documentation. I found there's some problem with the function.
This is the document and example in tensorflow document (Since I cannot use axis argument in my environment, I think I might close to this case): https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/boolean_mask
In my system, the example tensorflow documentation provided would return zero dimension tensor instead of one dimension as the example case.
Here's the example code and result:
![1](https://user-images.githubusercontent.com/18610064/39559423-aca5170e-4ec8-11e8-94dc-d4f29e9876ad.png)


### Code
As to my case, it not so different from the example, the case just from 2D to 5D.
1 make the mask same shape with target tensor, result (none) shape tensor:
![2](https://user-images.githubusercontent.com/18610064/39559462-f9d241e6-4ec8-11e8-8e4b-ae1eb57a7bc6.png)
2 make the mask 1D less than target tensor, result (none, 1) shape tensor:
![3](https://user-images.githubusercontent.com/18610064/39559495-3a5864a2-4ec9-11e8-907b-fec0627792a4.png)

It's easy to reproduce with random np.array and tf.tensor.

It would be appreciated that you could spend some time to take a look at it. Thanks.
"
19043,Code documetnantion. Probably misprint in function parameter description.,"https://github.com/tensorflow/tensorflow/blob/a44996a84b24c43cca40c685a009fd59275755ab/tensorflow/contrib/slim/python/slim/learning.py#L573

I believe there was a typo in the description in the penultimate word (""and""), which is probably worth replacing with ""are"".
Example:  
log_every_n_steps: The frequency, in terms of global steps, that the loss and global step **are** logged."
19042,Anyone gotten Facebook's AlphaGo to compile yet,"How much stronger is it than leela zero on only one playout?


https://github.com/gcp/leela-zero/issues/1311

https://github.com/pytorch/ELF/issues/1"
19041,"Saving Estimators, loading from checkpoints and accessing placeholders.","I posted this question here on stackoverflow (https://stackoverflow.com/questions/50011969/tensorflow-estimator-api-input-tensor-names) and someone else posted a similar question earlier (https://stackoverflow.com/questions/48335699/tensorflow-what-are-the-input-nodes-for-tf-estimator-models), yet no satisfying answers. 

When using an estimator in tensorflow and passing the inputs using `tf.estimator.inputs.numpy_input_function()`, I would like to find a way to access the input placeholders (one for features and one for labels).
I train the model using `tf.estimator.estimator.train()` and save the checkpoint files in `model_dir`. 
Later, I would like to load the model and variables using the following snippet:
`sess = tf.Session()`
`saver = tf.train.import_meta_graph(model_dir+'/model.ckpt-1.meta')`
`saver.restore(sess, tf.train.latest_checkpoint(model_dir))`
`predictions = graph.get_tensor_by_name('softmax_tensor:0')`

Then do `sess.run(predictions,....)`. But there's no way to pull out the inputs' placeholder for the `feed_dict` parameter of `sess.run()`.

Another related issue I have: I would also like to add placeholders to an estimator other than the features and labels placeholders. For example if I have dropout, I would like the dropout_rate or the `tf.layers.dropout()` function's `training` param to be placeholders. This way after loading the model from chkpt as above, I can turn off dropout. 


Have I written custom code Y
OS Platform and Distribution Ubuntu 18.04
TensorFlow installed from pip
TensorFlow version r1.8.0
Bazel version N/A
CUDA/cuDNN version 9.0/7.0.1
GPU model and memory GTX 1080TI
Exact command to reproduce N/A


"
19038,TensorFlow 1.8 upgrade causes Keras CI build to stall,"Here's a detailed description of the issue: https://github.com/keras-team/keras/issues/10100

**Summary:**

- Keras runs its CI tests on Travis
- Keras has two builds for its TensorFlow backend tests (Py 2.7 and Py 3.6)
- TF dependency is installed via `pip install tensorflow` (CPU version)
- A few days ago this started installing TF 1.8
- Both TF backend builds immediately started timing out after 25-50 min (high variance) with the error ""No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself."". Normally the TF tests take 17-18 min.
- No test started failing, this is entirely a build stalling issue.

**Reverting to TF 1.7 fixes the issue.** The exact cause of the stalling is completely unknown.

In addition, we know that:

- The failure is stochastic; sometimes one of the two TF builds (for Python 2.7 and 3.6) fails while the other passes. Most common case is that both fail, indicating that P(failure) is high.
- There are cases where TF Py 2.7 fails while 3.6 passes and cases where 3.6 fails while 2.7 passes, so the issue is not specific to a Python version.
- The failure is **not** due to an increase in the time taken by the tests. Disabling all large tests did not resolve the issue.

**Such stalling problems on Travis typically arise from either:**

- hanging process
- excessive RAM consumption leading to freezing execution

It is likely that TF 1.8 suffers from one of these two issues.


"
19029,Invalid License Error While Running bazel build for tensorflow,"I am trying to compile tensorflow for Macbook, but every time I am getting same error.

When I Run this command

`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`

I get this error


`/private/var/tmp/_bazel_cagrigider/2c548436011ab6b7308cd0cc70a91ed3/external/bazel_tools/tools/cpp/BUILD:3:1: invalid license type: 'notice'`

[Full Output Of Terminal](https://i.stack.imgur.com/S9MtY.png)

Mac OS 10.13.3

Xcode version = 9.2

Tensorflow version = 1.8

Bazel version = 0.13.0

Thanks."
19028,Is it possible to parepare two set of weights for feedforward and backprop separately?,"Currently I'm working on the network quantization and I try to implement the ideas of the paper [""Mixed precision training""](https://arxiv.org/abs/1710.03740) using tensorflow. In this paper, the author prepare two set of weights (with different data type) for feedforward and backprop separately. I want to know if current tensorflow can support this kind of feature. Thanks.

![screenshot from 2018-05-02 10-14-31](https://user-images.githubusercontent.com/13360081/39538427-a48b88fa-4df1-11e8-99ee-e2ee284b94a9.png)

"
19027,Speed regression on tensorflow version > 1.4.1,"We have observed some massive slowdowns in some Keras/tensorflow models with tensorflow versions newer than 1.4.1.
Not sure if the issue is with tensorflow or with the way keras creates the tensorflow models, so I am cross-posting these issue to both repos (apologies for that!).

Here is a script reproducing the issue:

## Setup
```python
from pandas import get_dummies
from sklearn.datasets import make_classification

from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.callbacks import EarlyStopping

X, y = make_classification(n_samples=10000, n_features=10, n_informative=8,
                           n_classes=5, n_clusters_per_class=1, random_state=0)

model = Sequential()
model.add(Dense(10, input_dim=10, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(5, activation='sigmoid'))
model.add(Dropout(0.1))
model.add(Dense(5, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])

yk = get_dummies(y)
```

## Fitting

```python
model.fit(X, yk, epochs=10, batch_size=50, 
               callbacks=[EarlyStopping(monitor='loss', patience=2)])   
```

Here are some timings for the fit method:
* Tensorflow 1.4.1:  **2.91 s ± 452 ms** per loop  (obtained using ipython's `%%timeit` magic, 7 loops)
* Tensorflow 1.5.0:  CPU times: user **2min 19s**, sys: 5min 22s, total: 7min 41s Wall time: 1min 2s
* Tensorflow 1.6.0: CPU times: user **5min 5s**, sys: 12min 31s, total: 17min 36s Wall time: 2min 37s
* Tensorflow 1.7.0: CPU times: user **5min 5s**, sys: 12min 39s, total: 17min 45s Wall time: 2min 39s

So, it seems there was a massive slowdown in version 1,5, and then a further one in 1.6 (which similar speed in 1.7). All the tests are run on a conda environment with python 3.6.5 and keras 2.1.5, with the corresponding tensorflow versions all coming from the anaconda `defaults` channel.

The GPU accelerated version of keras/tensorflow (`keras-gpu` conda package) does not present the issue.

Thanks in advance!"
19026,Tensorflow lite interpreter->AllocateTensors() fails with  it->size != alloc.size (15840 != 0),"=### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Raspberry Pi 3
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.7.0
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
0.10.0
- **GCC/Compiler version (if compiling from source)**:
g++ 6.3.0
- **CUDA/cuDNN version**:
None
- **GPU model and memory**:

- **Exact command to reproduce**:

### Describe the problem

I was able to build tensorflow-lite.a lib and test out examples without any problem. However, when i tried loading my own model (converted from frozen .pb graph), I get following error. function interpreter->AllocateTensors()  fails for some reason. Not able to understand why?

tensorflow/contrib/lite/simple_memory_arena.cc:82 it->size != alloc.size (15840 != 0)

"
19024,Bug : creating variables from restored variables get reinitialized but not always,"

I've been experiencing the bug on two configurations :
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 / Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: conda tensorflow-GPU / conda tensorflow (CPU)
- **TensorFlow version (use command below)**: 1.6 / 1.4
- **Python version**:  Python 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA Toolkit 9.1 and cuDNN 7.1.2 / None
- **GPU model and memory**: 1080ti 11GB / 
- **Exact command to reproduce**:

```
x = {
    '1' : tf.Variable(tf.random_normal([3, 3]), trainable=True, name='x1')
    }
saver = tf.train.Saver(var_list=x)

with tf.Session() as sess:
    
    
    
    sess.run(tf.global_variables_initializer())

    print(sess.run(x))
    saver.save(sess, './savedModels/bug/x')
```

> 
> {'1': array([[-2.80644059, -0.7185123 ,  0.70223355],
>        [ 1.06445408, -0.72174907,  1.29832721],
>        [ 1.52049255, -1.19468224, -1.02100158]], dtype=float32)}

```
for i in range(0,10):
    with tf.Session() as sess:

        new_saver = tf.train.import_meta_graph('./savedModels/bug/x.meta')
        new_saver.restore(sess, './savedModels/bug/x')

        x = {
            '1' : tf.Variable(tf.get_default_graph().get_tensor_by_name(""x1:0""), trainable=False)
            }
        sess.run(tf.global_variables_initializer())
        print(sess.run(x['1']))
```

> 
> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x
> [[ 0.49538991 -0.11817512 -0.10268462]
>  [ 0.51088262  1.28533709  0.05063328]
>  [-1.30132782  0.41913262  1.3775363 ]]
**> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x
> [[-2.80644059 -0.7185123   0.70223355]
>  [ 1.06445408 -0.72174907  1.29832721]
>  [ 1.52049255 -1.19468224 -1.02100158]]**
> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x
> [[-2.29186296  0.36665305  0.28915456]
>  [ 0.35180676  0.10379237  1.17195833]
>  [-1.05604255  0.01500762  2.31890965]]
> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x
> [[ 0.79247308  0.43540171 -1.15054667]
>  [-1.51901126  0.4132176  -1.45383906]
>  [-1.19301605 -0.21523039  1.42999935]]
> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x
> [[-0.03097171 -1.22662902 -0.30857435]
>  [-0.49384364 -0.98362756  0.12052365]
>  [-0.78354359  0.58901048 -1.8879807 ]]
> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x
> [[-0.91630346 -1.20698965 -0.10588568]
>  [-2.06214571 -0.81664461  0.58493197]
>  [ 0.40064785 -1.33241594  2.23627448]]
**> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x
> [[-2.80644059 -0.7185123   0.70223355]
>  [ 1.06445408 -0.72174907  1.29832721]
>  [ 1.52049255 -1.19468224 -1.02100158]]**
> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x
> [[-1.16766882  0.48798651 -1.84950495]
>  [-0.90750635  0.68074739 -0.86705917]
>  [ 0.6242311  -0.75273407  0.95616102]]
> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x
> [[ 0.02900926 -0.71752155  1.40167475]
>  [-1.76051104 -0.38902128 -0.42455733]
>  [ 0.61904657 -0.45060599  1.10379994]]
> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x
> [[-1.51588821 -2.11397028  0.33227748]
>  [ 0.34245569 -2.17953372  0.60712498]
>  [ 1.37212336  0.4941766   0.47627288]]

-------------------------------------------

Restoring the variable without using a sess.run() to ensure the constance, and then running the initializer should either:
-give the same result because variable are stored as numbers
-give different numbers because it is stored as a variable and thus, get initialized too.

In any case, we shouldn't find back the variables from time to time, it should be extremely unlikely to find back EXACTLY the same values when initialized, or, we should always find the same values.

NOTA BENE: 
I only find the values back when I use tensorflow 1.4 ( I haven't been able to check this on 1.7 yet), has this bug been fixed in the way I described above since?
"
19023,Doesn`t work with Golang version until 1.7,"### System information
- OS Platform and Distribution: `Linux Ubuntu 16.04` 
- TensorFlow version: `1.7.0` 
- TensorFlow installed from `binary`
- Have I written custom code: `not`, thats because i can`t install the library
- Bazel version: bazel is `not installed`
- CUDA/cuDNN version: `i have not` CUDA/cuDNN
- GPU model and memory: GPU - `AMD MULLINS (DRM 2.50.0 / 4.13.0-37-generic, LLVM 5.0.0)`, RAM - `6.8 gb`
- Exact command to reproduce: `go get github.com/tensorflow/tensorflow/tensorflow/go`

### Tthe problem
I installed TensorFlow as shown in the [instruction](https://www.tensorflow.org/install/install_go), but with `go get github.com/tensorflow/tensorflow/tensorflow/go` failed:
```
# github.com/tensorflow/tensorflow/tensorflow/go
could not determine kind of name for C. CBytes
```
but it was only in Golang version below 1.7, BUT to use it I can't because I need another library gocv (and other binding OpenCV for Golang) work only in version 1.6. 
### Question
Is there any possibility to use TensorFlow with Golang 1.6?
P.S. [env collect](https://www.dropbox.com/s/vawhuyh1d58a3k6/tf_env.txt?dl=0)"
19022,Documentation format mistake,"### Describe the problem


here, the `exeption_mode` ought to be an arg, while Markdown recognizes it as a list member
![image](https://user-images.githubusercontent.com/11363529/39514973-4726e0bc-4e2b-11e8-9cb3-dc70f6273e5e.png)

![image](https://user-images.githubusercontent.com/11363529/39514893-0d7b9ee8-4e2b-11e8-98d6-40acb1171315.png)

api doc address: 
https://www.tensorflow.org/api_docs/python/tf/enable_eager_execution

the problem source code:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L5426

### System information
v1.8.0-0-g93bc2e2072 1.8.0

It's not a problem of my environment, it's a problem of the official api doc
"
19021,unsupported operation SquaredDifference & Pow,"Hi,

There are some unsupported operation

2018-05-02 16:44:35.396116: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: SquaredDifference
2018-05-02 16:44:35.396179: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: Pow
2018-05-02 16:44:35.396269: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: SquaredDifference
2018-05-02 16:44:35.396311: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: Pow
2018-05-02 16:44:35.396454: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: SquaredDifference
2018-05-02 16:44:35.396494: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: Pow
2018-05-02 16:44:35.396635: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: SquaredDifference
2018-05-02 16:44:35.396675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: Pow
2018-05-02 16:44:35.396792: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: SquaredDifference
2018-05-02 16:44:35.396818: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: Pow
2018-05-02 16:44:35.396909: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: SquaredDifference
2018-05-02 16:44:35.396936: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: Pow
2018-05-02 16:44:35.397025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: SquaredDifference
2018-05-02 16:44:35.397051: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: Pow
2018-05-02 16:44:35.397145: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: SquaredDifference
2018-05-02 16:44:35.397172: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: Pow
"
19020,tf.data leaves hash table not initialized,"tensorflow version: 1.6

### bug description: 
  when using hash_table in ""tensorflow.python.ops.gen_lookup_ops""  in tf.data.Dataset.map function
because  tf.data.Dataset.map do not use the default graph, the hash_table can not be initialized.

### Exception: 
  FailedPreconditionError (see above for traceback): Table not initialized.

### code:
from __future__ import absolute_import, division, print_function
import tensorflow as tf
try:
    from tensorflow.python.ops.gen_lookup_ops import hash_table as _hash_table
    from tensorflow.python.ops.gen_lookup_ops import initialize_table as _initialize_table
    from tensorflow.python.ops.gen_lookup_ops import initialize_table_from_text_file as _initialize_table_from_text_file
    from tensorflow.python.ops.gen_lookup_ops import lookup_table_find as _lookup_table_find
    from tensorflow.python.ops.gen_lookup_ops import lookup_table_size as _lookup_table_size
except:
    from tensorflow.python.ops.gen_lookup_ops import _hash_table
    from tensorflow.python.ops.gen_lookup_ops import _initialize_table
    from tensorflow.python.ops.gen_lookup_ops import _initialize_table_from_text_file
    from tensorflow.python.ops.gen_lookup_ops import _lookup_table_find
    from tensorflow.python.ops.gen_lookup_ops import _lookup_table_size    

def look_up(input_tensor, look_up_table_ref, default_value=tf.constant(0, dtype=tf.int64), name=None):
    i_shape = input_tensor.get_shape()
    r = _lookup_table_find(look_up_table_ref, keys=input_tensor, default_value=default_value)
    r.set_shape(i_shape)
    return r

def string2int64_via_map(input_tensor, keys, values, default_value=0, table_ref=None): 
    from tensorflow.python.framework import ops
    with tf.name_scope('string2int64_via_map'):
        key_type = tf.string
        value_type = tf.int64
        keys = tf.convert_to_tensor(keys, dtype=key_type)
        values = tf.convert_to_tensor(values, dtype=value_type)
        default_value = tf.convert_to_tensor(default_value, dtype=value_type)
        if(table_ref is None):
            table_ref = _hash_table(key_dtype=key_type, value_dtype=value_type)
        init_op = _initialize_table(table_ref, keys, values)
        ops.add_to_collection(ops.GraphKeys.TABLE_INITIALIZERS, init_op)
        indices = _lookup_table_find(table_ref, keys=input_tensor, default_value=default_value)
        
        print(""graph in string2int64_via_map: \t%s"" % tf.get_default_graph())
        return indices, table_ref


def __test_string2int64_via_map():
    ''' this function works well
    '''
    print('__test_string2int64_via_map()')
    keys = ['s1', 's2', 's3']
    values = [10, 20, 30]
    input_tensor = ['s2', 's4', 's6', 's8', 's3']
    default_value = 0
    
    indices, _ = string2int64_via_map(input_tensor, keys, values, default_value)
    with tf.Session() as sess:
        sess.run(tf.tables_initializer())
        rr = sess.run(indices)
        print(rr)
        import sys
        sys.stdout.flush()

def test_dataset_using_hashmap():
    features = ['s1', 's2', 's3']
    labels = [0, 1, 2]
    sess = tf.Session()
    
    def mapfun_works(txt, label):
        return (tf.string_join([""prefix:"", txt], separator=''), label)
    
    def mapfun_using_hash_table(txt, label):
        keys = ['s1', 's2', 's3']
        values = [10, 20, 30]
        default_value = 0
        indices, _ = string2int64_via_map(txt, keys, values, default_value)
        return (indices, label)
        
    mapfunc = mapfun_using_hash_table  # change to mapfun1 works
    
    def train_input_fn(features, labels, batch_size):
        dataset = tf.data.Dataset.from_tensor_slices((features, labels))
        dataset = dataset.shuffle(10).repeat().batch(batch_size)
        dataset = dataset.map(mapfunc, num_parallel_calls=1)
        return dataset
    
    dataset = train_input_fn(features, labels, batch_size=4)
    it = dataset.make_initializable_iterator()
    sess.run(it.initializer)
    sess.run(tf.tables_initializer())  # not work because the hash table is in another graph
    
    print(""graph in main: \t\t\t%s"" % tf.get_default_graph())
    print(sess.run(it.get_next()))


if __name__ == '__main__':
    __test_string2int64_via_map()
    print('\n------------------------------\n')
    test_dataset_using_hashmap()

"
19018,java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.nmanoharlal.emoji2/com.example.nmanoharlal.emoji2.MainActivity}: java.lang.IllegalStateException: Attempting to use uninitialized value conv1d_1/W,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
When using model.pb with the TensorFlowInferenceInterface, we are getting above exception.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
19017,Strang behavior in memory copy with control dependencies.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I have costom code.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: `redhat-7.2`
- **TensorFlow installed from (source or binary)**: `binary`
- **TensorFlow version (use command below)**: `1.4.0`
- **Python version**: `2.7`
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: `CUDA-8.0`, `cuDNN-6.0`
- **GPU model and memory**: `Nvidia - K80`
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
There are two different ways to apply memory copy from CPU to GPU:0 in the following code:
```
  with tf.control_dependencies([update_op]):
    # Method-1 :
    # with tf.device('/GPU:0'):
    #   a_cpu_to_gpu = a_cpu.read_value()
    # train_ops.append(vars[i].assign(a_cpu_to_gpu).op)

    # Method-2 :
    train_ops.append(vars[i].assign(a_cpu).op)
```
`a_cpu` is a variable in CPU device. `vars[i]` is a variable in GPU:0 device. The first one (Method-1) is to read the value in GPU:0 and then assign it to `a_cpu_to_gpu`. the second one (Method-2) is directly assign the variable in CPU to the variable in GPU:0.

When I enable the `trace_level`, I find that the timeline of those two codes are different:

Method-1
![image](https://user-images.githubusercontent.com/7370869/39508070-8e8204a4-4e13-11e8-97e3-f166403f0bef.png)

Method-2
![image](https://user-images.githubusercontent.com/7370869/39507750-05f705d6-4e12-11e8-8aec-ff88db046e77.png)

In `Method-1`, I think the read_value op `a_cpu.read_value()` only depends on the i-th `update_op`. So the MEMCPYHtoD (green bar) should be right after the the i-th update_op, which means right after the MEMCPYDtoH (purple bar). I don't understand why they appear in the end of the timeline.

In `Method-2`, MEMCPYHtoD (green bar) appears at the beginning of the timeline. `vars[i].assign(a_cpu).op` depends on `update_op`, how can the memcpy be executed before the `update_op` ?

I have no clue why it behaves like this, can anyone tell me why the memcpy behaves like that in `Method-1` and `Method-2` and what's the difference between them.

Thanks.

### Source code / logs
```
import tensorflow as tf
import os

from tensorflow.python.client import timeline
slim = tf.contrib.slim

train_ops = []

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)

net = tf.random_normal(shape=(32, 16, 16, 128))

with tf.device('/GPU:0'):

  for i in range(10):
    net = slim.conv2d(net, 128, [11, 11], padding='SAME', scope='conv_%s' % i)

  loss = tf.reduce_mean(net, name='loss_func')
  grad = tf.gradients(loss, tf.global_variables(), gate_gradients=True, name='my_gradients')

vars = tf.global_variables()
num_vars = len(vars)

for i in range(num_vars - 1, -1, -1):

  with tf.device('/CPU:0'):
    a_cpu = tf.get_variable('a_cpu_%s' % i, initializer=vars[i].initial_value)
    update_op = optimizer.apply_gradients([(grad[i], a_cpu)], name='apply_%s' % i)

  with tf.control_dependencies([update_op]):
    # Method-1 :
    # with tf.device('/GPU:0'):
    #   a_cpu_to_gpu = a_cpu.read_value()
    # train_ops.append(vars[i].assign(a_cpu_to_gpu).op)

    # Method-2 :
    train_ops.append(vars[i].assign(a_cpu).op)


with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  for i in range(10):
    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
    run_metadata = tf.RunMetadata()
    sess.run(train_ops, options=run_options, run_metadata=run_metadata)
    trace_filename = os.path.join('/tmp/delete_me/trace', 'trace-global-step-%d.json' % i)
    if not tf.gfile.Exists(os.path.dirname(trace_filename)):
      os.makedirs(os.path.dirname(trace_filename))
    trace = timeline.Timeline(step_stats=run_metadata.step_stats)
    with tf.gfile.Open(trace_filename, 'w') as trace_file:
      trace_file.write(trace.generate_chrome_trace_format(show_memory=False))
```
"
19016,"No OpKernel was registered to support Op 'CudnnRNNParamsToCanonical' with these attrs.  Registered devices: [CPU], Registered kernels:   <no registered kernels>","
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.6.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.1.0/7.0
- **GPU model and memory**:GTX 1080, 11GB
- **Exact command to reproduce**:
Code used to export the model on CPU/GPU:
```
with tf.Graph().as_default() as graph:
        inputs, outputs = create_graph()


        # Create a saver using variables from the above newly created graph
        saver = tf.train.Saver(tf.global_variables())

        with tf.Session() as sess:
            # Restore the model from last checkpoints
            ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)
            saver.restore(sess, ckpt.model_checkpoint_path)

            # (re-)create export directory
            export_path = os.path.join(
                tf.compat.as_bytes(FLAGS.export_dir),
                tf.compat.as_bytes(str(FLAGS.export_version)))
            if os.path.exists(export_path):
                shutil.rmtree(export_path)

            # create model builder
            builder = tf.saved_model.builder.SavedModelBuilder(export_path)

            input_node = graph.get_tensor_by_name('input_node:0')
            input_lengths = graph.get_tensor_by_name('input_lengths:0')
            outputs = graph.get_tensor_by_name('output_node:0')

            # create tensors info
            predict_tensor_inputs_info = tf.saved_model.utils.build_tensor_info(input_node)
            predict_tensor_inputs_length_info = tf.saved_model.utils.build_tensor_info(input_lengths)
            predict_tensor_scores_info = tf.saved_model.utils.build_tensor_info(outputs)

            # build prediction signature
            prediction_signature = (
                tf.saved_model.signature_def_utils.build_signature_def(
                    inputs={'input': predict_tensor_inputs_info,'input_len':predict_tensor_inputs_length_info},
                    outputs={'output': predict_tensor_scores_info},
                    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME
                )
            )

            # save the model
            builder.add_meta_graph_and_variables(
                sess, [tf.saved_model.tag_constants.SERVING],
                signature_def_map={
                    'infer': prediction_signature
                })

            builder.save()
```

### Describe the problem
Trained one model on GPU using CudnnLSTM (tf.contrib.cudnn_rnn.CudnnLSTM). When trying to export using saved_model API on CPU getting the below error. Export works on GPU and it works for predictions also only on GPU. We want to use the model for prediction on CPU also. So, how can we export the trained model on GPU so that it can be used on CPU ?

### Error log:

```
File ""/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py"", line 501, in _create_saveable
    name=""%s_saveable"" % self.trainable_variables[0].name.split("":"")[0])
  File ""/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py"", line 262, in __init__
    weights, biases = self._OpaqueParamsToCanonical()
  File ""/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py"", line 315, in _OpaqueParamsToCanonical
    direction=self._direction)
  File ""/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/ops/gen_cudnn_rnn_ops.py"", line 769, in cudnn_rnn_params_to_canonical
    name=name)
  File ""/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3290, in create_op
    op_def=op_def)
  File ""/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1654, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'CudnnRNNParamsToCanonical' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

	 [[Node: CudnnRNNParamsToCanonical = CudnnRNNParamsToCanonical[T=DT_FLOAT, direction=""bidirectional"", dropout=0, input_mode=""linear_input"", num_params=16, rnn_mode=""lstm"", seed=0, seed2=0, _device=""/device:GPU:0""](CudnnRNNParamsToCanonical/num_layers, CudnnRNNParamsToCanonical/num_units, CudnnRNNParamsToCanonical/input_size, cudnn_lstm/opaque_kernel/read)]]
```"
19015,Building from source with Python3.6 raises `java.lang.RuntimeException`,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**:  master branch, commit d0f5bc1
- **Python version**: Python 3.6
- **Bazel version (if compiling from source)**:

Build label: 0.13.0
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Mon Oct 18 21:33:40 +50297 (1525078013620)
Build timestamp: 1525078013620
Build timestamp as int: 1525078013620

- **GCC/Compiler version (if compiling from source)**:

gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609

- **CUDA/cuDNN version**:

CUDA 9.0 / cuDNN 7.0 

- **GPU model and memory**:

GeForce GTX TITAN X 12 Gb

- **Exact command to reproduce**:

`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

In `./configure` I use the default settings for everything except I use `/usr/bin/python3.6` and CUDA.

```
~/tensorflow master* ⇣ ethanbro@rldl2
❯ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```

This raises the following error:
```
Starting local Bazel server and connecting to it...
..............
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: /home/ethanbro/.cache/bazel/_bazel_ethanbro/5e4124942ab73951559485c7924c6c9e/external/protobuf_archive/WORKSPACE:1: Workspace name in /home/ethanbro/.cache/bazel/_bazel_ethanbro/5e4124942ab73951559485c7924c6c9e/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions
Unhandled exception thrown during build; message: Unrecoverable error while evaluating node 'PACKAGE:tensorflow/core/kernels' (requested by nodes '//tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@216e49db true (673307310)', '//tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@6511864 false (1063568825)', '//tensorflow/core/grappler/clusters:single_machine com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@216e49db true (1846558957)', '//tensorflow/core/grappler/costs:measuring_cost_estimator com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@216e49db true (1957631410)')
INFO: Elapsed time: 3.346s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (130 packages loaded)
    currently loading: tensorflow/core ... (2 packages)
java.lang.RuntimeException: Unrecoverable error while evaluating node 'PACKAGE:tensorflow/core/kernels' (requested by nodes '//tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@216e49db true (673307310)', '//tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@6511864 false (1063568825)', '//tensorflow/core/grappler/clusters:single_machine com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@216e49db true (1846558957)', '//tensorflow/core/grappler/costs:measuring_cost_estimator com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@216e49db true (1957631410)')
	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:424)
	at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:355)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: Invalid EvalException:
java.lang.InterruptedException
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:485)
	at com.google.common.util.concurrent.AbstractFuture$TrustedFuture.get(AbstractFuture.java:85)
	at com.google.common.util.concurrent.ForwardingFuture.get(ForwardingFuture.java:62)
	at com.google.devtools.build.lib.vfs.UnixGlob$GlobFuture.get(UnixGlob.java:444)
	at com.google.devtools.build.lib.vfs.UnixGlob$GlobFuture.get(UnixGlob.java:432)
	at com.google.devtools.build.lib.packages.GlobCache.fromFuture(GlobCache.java:219)
	at com.google.devtools.build.lib.packages.GlobCache.getGlobUnsorted(GlobCache.java:161)
	at com.google.devtools.build.lib.packages.GlobCache.globUnsorted(GlobCache.java:248)
	at com.google.devtools.build.lib.packages.PackageFactory$LegacyGlobber.fetch(PackageFactory.java:305)
	at com.google.devtools.build.lib.skyframe.PackageFunction$SkyframeHybridGlobber$HybridToken.resolve(PackageFunction.java:1080)
	at com.google.devtools.build.lib.skyframe.PackageFunction$SkyframeHybridGlobber$HybridToken.access$600(PackageFunction.java:1046)
	at com.google.devtools.build.lib.skyframe.PackageFunction$SkyframeHybridGlobber.fetch(PackageFunction.java:1003)
	at com.google.devtools.build.lib.packages.PackageFactory.callGlob(PackageFactory.java:569)
	at com.google.devtools.build.lib.packages.SkylarkNativeModule.glob(SkylarkNativeModule.java:92)
	at sun.reflect.GeneratedMethodAccessor99.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:360)
	at com.google.devtools.build.lib.syntax.BuiltinCallable.call(BuiltinCallable.java:130)
	at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:462)
	at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:440)
	at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:719)
	at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:839)
	at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:823)
	at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)
	at com.google.devtools.build.lib.syntax.BinaryOperatorExpression.doEval(BinaryOperatorExpression.java:251)
	at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)
	at com.google.devtools.build.lib.syntax.Eval.execAssignment(Eval.java:50)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:171)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)
	at com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:210)
	at com.google.devtools.build.lib.syntax.Eval.execIfBranch(Eval.java:62)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:177)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)
	at com.google.devtools.build.lib.syntax.Eval.execIf(Eval.java:119)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:193)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)
	at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)
	at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:462)
	at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:440)
	at com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:854)
	at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:826)
	at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:180)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)
	at com.google.devtools.build.lib.syntax.BuildFileAST.execTopLevelStatement(BuildFileAST.java:225)
	at com.google.devtools.build.lib.syntax.BuildFileAST.exec(BuildFileAST.java:198)
	at com.google.devtools.build.lib.packages.PackageFactory.evaluateBuildFile(PackageFactory.java:1659)
	at com.google.devtools.build.lib.packages.PackageFactory.createPackageFromAst(PackageFactory.java:1298)
	at com.google.devtools.build.lib.skyframe.PackageFunction.loadPackage(PackageFunction.java:1234)
	at com.google.devtools.build.lib.skyframe.PackageFunction.compute(PackageFunction.java:476)
	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:347)
	at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:355)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.google.devtools.build.lib.syntax.EvalException.<init>(EvalException.java:112)
	at com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:209)
	at com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:217)
	at com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:393)
	at com.google.devtools.build.lib.syntax.BuiltinCallable.call(BuiltinCallable.java:130)
	at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:462)
	at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:440)
	at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:719)
	at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:839)
	at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:823)
	at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)
	at com.google.devtools.build.lib.syntax.BinaryOperatorExpression.doEval(BinaryOperatorExpression.java:251)
	at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)
	at com.google.devtools.build.lib.syntax.Eval.execAssignment(Eval.java:50)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:171)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)
	at com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:210)
	at com.google.devtools.build.lib.syntax.Eval.execIfBranch(Eval.java:62)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:177)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)
	at com.google.devtools.build.lib.syntax.Eval.execIf(Eval.java:119)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:193)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)
	at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)
	at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:462)
	at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:440)
	at com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:854)
	at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:826)
	at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:180)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)
	at com.google.devtools.build.lib.syntax.BuildFileAST.execTopLevelStatement(BuildFileAST.java:225)
	at com.google.devtools.build.lib.syntax.BuildFileAST.exec(BuildFileAST.java:198)
	at com.google.devtools.build.lib.packages.PackageFactory.evaluateBuildFile(PackageFactory.java:1659)
	at com.google.devtools.build.lib.packages.PackageFactory.createPackageFromAst(PackageFactory.java:1298)
	at com.google.devtools.build.lib.skyframe.PackageFunction.loadPackage(PackageFunction.java:1234)
	at com.google.devtools.build.lib.skyframe.PackageFunction.compute(PackageFunction.java:476)
	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:347)
	... 4 more
java.lang.RuntimeException: Unrecoverable error while evaluating node 'PACKAGE:tensorflow/core/kernels' (requested by nodes '//tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@216e49db true (673307310)', '//tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@6511864 false (1063568825)', '//tensorflow/core/grappler/clusters:single_machine com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@216e49db true (1846558957)', '//tensorflow/core/grappler/costs:measuring_cost_estimator com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@216e49db true (1957631410)')
	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:424)
	at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:355)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: Invalid EvalException:
java.lang.InterruptedException
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:485)
	at com.google.common.util.concurrent.AbstractFuture$TrustedFuture.get(AbstractFuture.java:85)
	at com.google.common.util.concurrent.ForwardingFuture.get(ForwardingFuture.java:62)
	at com.google.devtools.build.lib.vfs.UnixGlob$GlobFuture.get(UnixGlob.java:444)
	at com.google.devtools.build.lib.vfs.UnixGlob$GlobFuture.get(UnixGlob.java:432)
	at com.google.devtools.build.lib.packages.GlobCache.fromFuture(GlobCache.java:219)
	at com.google.devtools.build.lib.packages.GlobCache.getGlobUnsorted(GlobCache.java:161)
	at com.google.devtools.build.lib.packages.GlobCache.globUnsorted(GlobCache.java:248)
	at com.google.devtools.build.lib.packages.PackageFactory$LegacyGlobber.fetch(PackageFactory.java:305)
	at com.google.devtools.build.lib.skyframe.PackageFunction$SkyframeHybridGlobber$HybridToken.resolve(PackageFunction.java:1080)
	at com.google.devtools.build.lib.skyframe.PackageFunction$SkyframeHybridGlobber$HybridToken.access$600(PackageFunction.java:1046)
	at com.google.devtools.build.lib.skyframe.PackageFunction$SkyframeHybridGlobber.fetch(PackageFunction.java:1003)
	at com.google.devtools.build.lib.packages.PackageFactory.callGlob(PackageFactory.java:569)
	at com.google.devtools.build.lib.packages.SkylarkNativeModule.glob(SkylarkNativeModule.java:92)
	at sun.reflect.GeneratedMethodAccessor99.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:360)
	at com.google.devtools.build.lib.syntax.BuiltinCallable.call(BuiltinCallable.java:130)
	at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:462)
	at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:440)
	at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:719)
	at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:839)
	at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:823)
	at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)
	at com.google.devtools.build.lib.syntax.BinaryOperatorExpression.doEval(BinaryOperatorExpression.java:251)
	at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)
	at com.google.devtools.build.lib.syntax.Eval.execAssignment(Eval.java:50)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:171)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)
	at com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:210)
	at com.google.devtools.build.lib.syntax.Eval.execIfBranch(Eval.java:62)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:177)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)
	at com.google.devtools.build.lib.syntax.Eval.execIf(Eval.java:119)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:193)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)
	at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)
	at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:462)
	at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:440)
	at com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:854)
	at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:826)
	at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:180)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)
	at com.google.devtools.build.lib.syntax.BuildFileAST.execTopLevelStatement(BuildFileAST.java:225)
	at com.google.devtools.build.lib.syntax.BuildFileAST.exec(BuildFileAST.java:198)
	at com.google.devtools.build.lib.packages.PackageFactory.evaluateBuildFile(PackageFactory.java:1659)
	at com.google.devtools.build.lib.packages.PackageFactory.createPackageFromAst(PackageFactory.java:1298)
	at com.google.devtools.build.lib.skyframe.PackageFunction.loadPackage(PackageFunction.java:1234)
	at com.google.devtools.build.lib.skyframe.PackageFunction.compute(PackageFunction.java:476)
	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:347)
	at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:355)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.google.devtools.build.lib.syntax.EvalException.<init>(EvalException.java:112)
	at com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:209)
	at com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:217)
	at com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:393)
	at com.google.devtools.build.lib.syntax.BuiltinCallable.call(BuiltinCallable.java:130)
	at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:462)
	at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:440)
	at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:719)
	at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:839)
	at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:823)
	at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)
	at com.google.devtools.build.lib.syntax.BinaryOperatorExpression.doEval(BinaryOperatorExpression.java:251)
	at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)
	at com.google.devtools.build.lib.syntax.Eval.execAssignment(Eval.java:50)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:171)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)
	at com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:210)
	at com.google.devtools.build.lib.syntax.Eval.execIfBranch(Eval.java:62)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:177)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)
	at com.google.devtools.build.lib.syntax.Eval.execIf(Eval.java:119)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:193)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)
	at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)
	at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:462)
	at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:440)
	at com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:854)
	at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:826)
	at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:180)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)
	at com.google.devtools.build.lib.syntax.BuildFileAST.execTopLevelStatement(BuildFileAST.java:225)
	at com.google.devtools.build.lib.syntax.BuildFileAST.exec(BuildFileAST.java:198)
	at com.google.devtools.build.lib.packages.PackageFactory.evaluateBuildFile(PackageFactory.java:1659)
	at com.google.devtools.build.lib.packages.PackageFactory.createPackageFromAst(PackageFactory.java:1298)
	at com.google.devtools.build.lib.skyframe.PackageFunction.loadPackage(PackageFunction.java:1234)
	at com.google.devtools.build.lib.skyframe.PackageFunction.compute(PackageFunction.java:476)
	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:347)
	... 4 more
```
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Here is the .tf_configure.bazelrc:
```
build --action_env PYTHON_BIN_PATH=""/usr/bin/python3.6""
build --action_env PYTHON_LIB_PATH=""/usr/local/lib/python3.6/dist-packages""
build --python_path=""/usr/bin/python3.6""
build --define with_jemalloc=true
build:gcp --define with_gcp_support=true
build:hdfs --define with_hdfs_support=true
build --define with_s3_support=true
build --define with_kafka_support=true
build:xla --define with_xla_support=true
build:gdr --define with_gdr_support=true
build:verbs --define with_verbs_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_CUDA=""1""
build --action_env CUDA_TOOLKIT_PATH=""/usr/local/cuda""
build --action_env TF_CUDA_VERSION=""9.0""
build --action_env CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env TF_NCCL_VERSION=""1""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
build --action_env LD_LIBRARY_PATH=""/home/ethanbro/.mujoco/mjpro150/bin:/usr/local/cuda-9.0/lib64:""
build --action_env TF_CUDA_CLANG=""0""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
build --config=cuda
test --config=cuda
build --define grpc_no_ares=true
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
build --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
```

Here is the content of tf_env.txt:
```

== cat /etc/issue ===============================================
Linux rldl2 4.4.0-122-generic #146-Ubuntu SMP Mon Apr 23 15:34:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux rldl2 4.4.0-122-generic #146-Ubuntu SMP Mon Apr 23 15:34:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                              1.13.3                
protobuf                           3.4.0                 
tensorflow-tensorboard             0.1.8                 

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""tensorflow/python/pywrap_tensorflow.py"", line 25, in <module>
    from tensorflow.python.platform import self_check
ImportError: No module named platform

== env ==========================================================
LD_LIBRARY_PATH /home/ethanbro/.mujoco/mjpro150/bin:/usr/local/cuda-9.0/lib64:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Tue May  1 23:26:41 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.111                Driver Version: 384.111                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX TIT...  Off  | 00000000:02:00.0 Off |                  N/A |
| 22%   22C    P8    14W / 250W |     12MiB / 12207MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX TIT...  Off  | 00000000:03:00.0 Off |                  N/A |
| 22%   23C    P8    15W / 250W |     11MiB / 12207MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX TIT...  Off  | 00000000:82:00.0 Off |                  N/A |
| 22%   22C    P8    15W / 250W |     11MiB / 12207MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX TIT...  Off  | 00000000:83:00.0 Off |                  N/A |
| 22%   22C    P8    15W / 250W |     11MiB / 12207MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1585      G   /usr/lib/xorg/Xorg                            11MiB |
|    1      1585      G   /usr/lib/xorg/Xorg                            10MiB |
|    2      1585      G   /usr/lib/xorg/Xorg                            10MiB |
|    3      1585      G   /usr/lib/xorg/Xorg                            10MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
/usr/local/MATLAB/R2015a/bin/glnxa64/libcudart.so.6.5.14
```"
19014,How to quantize Mobilenet v2 ? ,"## System information

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
- TensorFlow installed from (source or binary):Source
- TensorFlow version (use command below):1.8.0rc0
- Python version: 2.7.12
- Bazel version (if compiling from source): 0.12.0
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version:cuda-9.0/7.0
- GPU model and memory:GeForce GTX 1080/8105MiB
- Phone: xiaomi5 (Snapdragon 820)

## Describe the problem
Using **tf.contrib.quantize.create_training_graph()** and **tf.contrib.quantize.create_eval_graph()**,  I 
quantized training  Mobilenet v2,  and  export inference graph,  but  when toco pb to tflite,  I encountered the following error:

2018-05-02 10:26:27.215975: F tensorflow/contrib/lite/toco/tooling_util.cc:1464] Array MobilenetV2/expanded_conv_2/add, which is an input to the Conv operator producing the output array MobilenetV2/expanded_conv_3/expand/Relu6, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.

This layer MobilenetV2/expanded_conv_2/add  cannot be quantized ? 

Who can explain why ? 

## Source code / logs

bazel run --config=opt \
>   //tensorflow/contrib/lite/toco:toco -- \
>   --input_file=${TRAIN_DIR}/frozen_graph.pb \
>   --output_file=${TRAIN_DIR}/tflite_model.tflite \
>   --input_format=TENSORFLOW_GRAPHDEF \
>   --output_format=TFLITE \
>   --inference_type=QUANTIZED_UINT8 \
>   --input_shape=1,224,224,3 \
>   --input_array=input \
>   --output_array=MobilenetV2/Predictions/Reshape_1 \
>   --std_value=127 \
>   --mean_value=128 \
>   --dump_graphviz=${TRAIN_DIR}

```
INFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/home/apuser/tmp/models/flowers/mobilenet_v2_1.0_224_quan/frozen_graph.pb' '--output_file=/home/apuser/tmp/models/flowers/mobilenet_v2_1.0_224_quan/tflite_model.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=QUANTIZED_UINT8' '--input_shape=1,224,224,3' '--input_array=input' '--output_array=MobilenetV2/Predictions/Reshape_1' '--std_value=127' '--mean_value=128' '--dump_graphviz=/home/apuser/tmp/models/flowers/mobilenet_v2_1.0_224_quan'
2018-05-02 10:26:26.887728: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1207 operators, 1752 arrays (0 quantized)
2018-05-02 10:26:26.934465: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1207 operators, 1752 arrays (0 quantized)
2018-05-02 10:26:27.211718: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 120 operators, 228 arrays (1 quantized)
2018-05-02 10:26:27.214013: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 120 operators, 228 arrays (1 quantized)
2018-05-02 10:26:27.215023: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 67 operators, 175 arrays (1 quantized)
2018-05-02 10:26:27.215975: F tensorflow/contrib/lite/toco/tooling_util.cc:1464] Array MobilenetV2/expanded_conv_2/add, which is an input to the Conv operator producing the output array MobilenetV2/expanded_conv_3/expand/Relu6, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.
```

"
19013,[Feature Request] Reuse curl handle in CurlHttpRequest class,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.7.0-3-g024aecf414 1.7.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.12.0-homebrew
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.1.0 (clang-902.0.39.1)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
In the [libcurl programming tutorial](https://curl.haxx.se/libcurl/c/libcurl-tutorial.html) under the section ""Persistence Is The Way to Happiness"" it notes that re-cycling the same easy handle can give several benefits. This could lead to performance improvements when using remote filesystems that utilise this class (e.g. gcs file system and I've been working on azure blob storage). Currently it seems that a single instance of the `CurlHttpRequest` and hence curl handle can only be used once and must be instantiated per request.

Is this worth investigating to integrate some sort of 'reset' method to this class along with [curl_easy_reset](https://curl.haxx.se/libcurl/c/curl_easy_reset.html) plus extract defaults set in the constructor into a 'setup defaults' method? Plus some profiling to ensure the change is better?"
19012,how many steps that are required to train the model>,I've trained the model for more than 12 hours and it is still training. How many steps that machine needs to complete the process?
19010,Tensorflow r1.8  ImportError: dlopen: cannot load any more object with static TLS,"Have I written custom code: No
OS Platform and Distribution: Debian 8.10
TensorFlow installed from: Source
TensorFlow version: r1.8
Bazel version: 0.13.0
CUDA/cuDNN version: n/a
GPU model and memory: n/a
Exact command to reproduce:

Importing Tensorflow r1.8 **after** some libraries (specifically cv2) results in: 
`ImportError: dlopen: cannot load any more object with static TLS`



A temporary fix seems to be to import Tensorflow before any other libraries. 
Has this shown up before? Any other information I can provide?


https://github.com/Kaggle/docker-python/issues/206
https://www.kaggle.com/kmader/library-overload



```
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
/opt/conda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

/opt/conda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

/opt/conda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

/opt/conda/lib/python3.6/imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

/opt/conda/lib/python3.6/imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: dlopen: cannot load any more object with static TLS

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-2-41389fad42b5> in <module>()
----> 1 import tensorflow as tf

/opt/conda/lib/python3.6/site-packages/tensorflow/__init__.py in <module>()
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 # pylint: disable=wildcard-import
     26 from tensorflow.tools.api.generator.api import *  # pylint: disable=redefined-builtin

/opt/conda/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

/opt/conda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/conda/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/conda/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen: cannot load any more object with static TLS


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

"
19009,"Incorrect results if set ""-c 100"" in TFLite label_image example","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution: Android:
- **TensorFlow installed from binary:
- **TensorFlow version: 1.6:
- **Python version 2.7: 
- **Bazel version: 0.11.1:
- **CUDA/cuDNN version: N/A
- **GPU model and memory: N/A
- **GCC/Compiler version: gcc-5:
- **Exact command to reproduce**:
label_image -b 128 -s 128 -i example.bmp -l labels_mobilenet_quant_v1_224.txt -m mobilenet_quant_v1_224.tflite -c 100

### Describe the problem
Incorrect results if set ""-c 100"" in TFLite label_image example. If only loop once, the result is correct, but if loop more than once, then the results somehow are all incorrect.

### Source code / logs
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/label_image/label_image.cc#L194
"
19007,Using different optimizers on a trained model leads to poor test accuracy.,"I have trained a convolutional neural network model using `AdamOptimizer`. The test accuracy is pretty good. This was expected since the validation loss was decreasing while training.

Now I restored the model weights using `saver.restore()` method, but this time I built the model with `RMSPropOptimizer` just for experimenting. Please note that I **DID NOT RETRAIN THE MODEL**. To my surprise, the test accuracy was horrible with the new optimizer. The accuracy becomes good again when I use the previously used `AdamOptimizer`.

This is peculiar in the sense that the train_op should not affect the test data results.

Below is the sample code I used for calculating the test accuracy. Please note I did not run the `train_op` here.

```
accuracies = []

# Data generator
for batchx, batchy in dataLoader.next_validation():
    # Run the graph.
    pred = sess.run(model['prediction'], 
                        feed_dict={
                            model['sensor_data']: batchx,
                            model['label']: batchy
                        })
        
    accuracies.append(np.count_nonzero(pred == label) / pred.shape[0] * 100)

accuracies = np.array(accuracies)
print(""Average Validation set accuracy: {} %"".format(accuracies.mean()))
```

Am I missing something here? Or is this a possible bug?"
19006,TF serving for TensorFlow 1.7 or more,"Dear all,

My machine is a MacBook Pro with  macOS HighSierra 10.13.1 and I am using tf 1.7.  I am trying to use tensorflow serving without any results since the [doc](https://www.tensorflow.org/serving/docker) is not updated for Tensorflow 1.7. The doc says to 

```
git clone --recurse-submodules https://github.com/tensorflow/serving
cd serving/tensorflow
./configure
cd ..
bazel test tensorflow_serving/...
```
But there is no `serving/tensorflow` folder in the container. Following the doc I did:
1) Create the container

```
docker build --pull -t $USER/tensorflow-serving-devel -f Dockerfile.devel .
```
Where the docker file is the one provided by the doc
2) Run it
```
docker run -it $USER/tensorflow-serving-devel
```
3)Try to build tf serving

```
root@5db9a099bd77:/# git clone --recurse-submodules https://github.com/tensorflow/serving
Cloning into 'serving'...

remote: Counting objects: 6476, done.
remote: Compressing objects: 100% (16/16), done.
remote: Total 6476 (delta 7), reused 17 (delta 7), pack-reused 6452
Receiving objects: 100% (6476/6476), 2.43 MiB | 1.07 MiB/s, done.
Resolving deltas: 100% (4628/4628), done.
Checking connectivity... done.
root@5db9a099bd77:/# cd serving/tensorflow
bash: cd: serving/tensorflow: No such file or directory
```

So, basically, how can I run my model in production? I also think that you should maintain a docker image with TF serving already built on it to make it easier for the community to use it.

Cheers,

Francesco Saverio Zuppichini"
19005,TFLite conversation failed,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MAC OSX
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.7
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.11.0
- **CUDA/cuDNN version**: 9/7
- **GPU model and memory**: 1080Ti 12G
- **Exact command to reproduce**: N/A

I wrote a code for mobile net and generated model. Converted model from keras(.h5) to frozen graph(.pb) Summary of graph is 
```
$ ./bazel-bin/tensorflow/tools/graph_transforms/summarize_graph   --in_graph=s.pb
Found 1 possible inputs: (name=input_1, type=float(1), shape=[?,224,224,3]) 
No variables spotted.
Found 1 possible outputs: (name=dense_1/Sigmoid, op=Sigmoid) 
Found 4254887 (4.25M) const parameters, 0 (0) variable parameters, and 4 control_edges
Op types used: 164 Switch, 154 Const, 84 Mul, 56 Add, 28 Sub, 28 Merge, 27 Relu, 27 Minimum, 27 FusedBatchNorm, 27 Rsqrt, 15 Conv2D, 14 Pad, 13 DepthwiseConv2dNative, 3 Shape, 2 BiasAdd, 2 StridedSlice, 2 Pack, 2 Reshape, 2 RealDiv, 1 Max, 1 RandomUniform, 1 MatMul, 1 Floor, 1 Sigmoid, 1 Mean, 1 Exp, 1 Sum, 1 Placeholder
To use with tensorflow/tools/benchmark:benchmark_model try these arguments:
bazel run tensorflow/tools/benchmark:benchmark_model -- --graph=s.pb --show_flops --input_layer=input_1 --input_layer_type=float --input_layer_shape=-1,224,224,3 --output_layer=dense_1/Sigmoid
```
When I try to convert it into tflite it gives me following error:
```
$ ./bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=s.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=train-1.tflite --inference_type=FLOAT --input_arrays=input_1 --output_arrays=dense_1/Sigmoid --v=2
2018-05-01 14:13:50.005925: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1236] Converting unsupported operation: RandomUniform
2018-05-01 14:13:50.233921: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 640 operators, 986 arrays (0 quantized)
2018-05-01 14:13:50.454878: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 639 operators, 984 arrays (0 quantized)
2018-05-01 14:13:50.696498: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 639 operators, 984 arrays (0 quantized)
2018-05-01 14:13:50.861230: F tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:86] Check failed: mean_shape.dims() == multiplier_shape.dims() 
Aborted
```"
19004,Windows build fails with unresolved externals,"I followed the instructions to build on Windows (Windows 10, Visual Studio 2017 version 15.6.7, CPU only), but the following projects fail to build:
tf_python_api
grpc_tensorflow_server
benchmark_model
tf_tutorials_example_trainer
tf_label_image_example
compare_graphs
transform_graph
summarize_graph

With the same four unresolved externals:
LNK2019	unresolved external symbol ""public: class tensorflow::AttrBuilder & __cdecl tensorflow::AttrBuilder::NumInputs(int)"" (?NumInputs@AttrBuilder@tensorflow@@QEAAAEAV12@H@Z) referenced in function ""public: void __cdecl tensorflow::EagerOperation::AddInput(class tensorflow::TensorHandle *)"" (?AddInput@EagerOperation@tensorflow@@QEAAXPEAVTensorHandle@2@@Z)

LNK2019	unresolved external symbol ""class tensorflow::Status __cdecl tensorflow::OpDefForOp(char const *,class tensorflow::OpDef const * *)"" (?OpDefForOp@tensorflow@@YA?AVStatus@1@PEBDPEAPEBVOpDef@1@@Z) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)	

LNK2019	unresolved external symbol ""public: struct tensorflow::Fprint128 __cdecl tensorflow::AttrBuilder::CacheKey(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)const "" (?CacheKey@AttrBuilder@tensorflow@@QEBA?AUFprint128@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)

LNK2019	unresolved external symbol ""public: class tensorflow::NodeDef const & __cdecl tensorflow::AttrBuilder::BuildNodeDef(void)"" (?BuildNodeDef@AttrBuilder@tensorflow@@QEAAAEBVNodeDef@2@XZ) referenced in function ""class tensorflow::Status __cdecl tensorflow::EagerExecute(class tensorflow::EagerOperation *,class tensorflow::gtl::InlinedVector<class tensorflow::TensorHandle *,2> *,int *)"" (?EagerExecute@tensorflow@@YA?AVStatus@1@PEAVEagerOperation@1@PEAV?$InlinedVector@PEAVTensorHandle@tensorflow@@$01@gtl@1@PEAH@Z)
"
19003,Dockerfile.gpu keeps the pip package,"```
$ docker run --rm tensorflow/tensorflow:nightly-gpu-py3 sh -c 'ls -lh /*.whl'
-rw-rw-r-- 1 root root 207M May  1 04:54 /tf_nightly_gpu-1.9.0.dev20180429-cp35-cp35m-manylinux1_x86_64.whl
```
This is 200MB we should get rid of.

In Dockerfile.gpu we have this:
https://github.com/tensorflow/tensorflow/blob/d0f5bc17560fc97bcc7de9164aa3b237a8d5221d/tensorflow/tools/docker/Dockerfile.gpu#L47-L56

In `parameterized_docker_build.sh` this block is replaced this way:
https://github.com/tensorflow/tensorflow/blob/d0f5bc17560fc97bcc7de9164aa3b237a8d5221d/tensorflow/tools/docker/parameterized_docker_build.sh#L260-L263

We don't have the `RUN rm -f /_PIP_FILE_` but it doesn't really matter for the size of the image. Since the `COPY` is necessarily a different layer than the `RUN`, see my commit message here: https://github.com/tensorflow/tensorflow/commit/9da73dedfc14861a7efcd44a9943d28ced038dc5

There is no simple solution here, multi-stage builds won't help, I will spare you the black magic technique since we have another option right in front of us:
https://github.com/tensorflow/tensorflow/blob/d0f5bc17560fc97bcc7de9164aa3b237a8d5221d/tensorflow/tools/docker/Dockerfile.gpu#L53-L55
What prevents us from modifying the parameters passed to `parameterized_docker_build.sh` to download the pip package? I think a 200MB improvement is worth it. 

cc @gunan "
19002,Could anybody help check this problem? I've been stuck in two days and didn't find the solution.,"I'm using the tf.Dataset API as follows: Suppose I have 10 `*.tfrecords` files (named `trn-001.tfrecords`,`trn-002.tfrecords`,etc.). There are 128 samples in each of these tfrecords. Suppose the samples in the first 5 tfrecords belong to class 0 and the remaining 5 tfrecords belong to class 1. In my case, the data provider should first random select 2 tfrecords for each class from those 5 tfrecords, then select fixed number of samples from each of the selected tfrecords and combine them to generate one balanced batch data to feed into the network for training. For example, for class 0, select `trn-000.tfrecords` and `trn-001.tfrecords`; for class 1, select `trn-006.tfrecords` and `trn-009.tfrecords`; then for each of the four selected tfrecords, randomly select 1 sample from each of them and combine the `1+1+1+1` samples to generate one batch data. My script to initialize the dataset operations is like this:

    trn0_filenames = ['trn-000.tfrecords','trn-001.tfrecords','trn-002.tfrecords','trn-003.tfrecords','trn-004.tfrecords']
    trn1_filenames = ['trn-005.tfrecords','trn-006.tfrecords','trn-007.tfrecords','trn-008.tfrecords','trn-009.tfrecords']

    half_batch_size = int(BATCH_SIZE / 2)
    trn0_count = len(trn0_filenames)
    trn1_count = len(trn1_filenames)
    features0_ops = []
    labels0_ops = []
    for trn0_filename in trn0_filenames:
        feature_op, label_op, = dataset_input_from_tfrecords([trn0_filename], batch_size=1, num_epochs=50000, shuffle=True, image_mean=None)
        features0_ops.append(feature_op)
        labels0_ops.append(label_op)
    
    features1_ops = []
    labels1_ops = []
    for trn1_filename in trn1_filenames:
        feature_op, label_op = dataset_input_from_tfrecords([trn1_filename], batch_size=1, num_epochs=50000, shuffle=True, image_mean=None)
        features1_ops.append(feature_op)
        label_ops.append(label_op)

Then during training, the script to generate the batch data is like this:

    features = np.zeros((BATCH_SIZE, HEIGHT, WIDTH, 3), dtype=np.float32)
    labels = np.zeros((BATCH_SIZE,), dtype=np.int64)
    
    while not coord.should_stop():
        ind0 = np.random.choice(trn0_count, half_batch_size)
        ind1 = np.random.choice(trn1_count, half_batch_size)
        batch_idx = 0
        for idx in ind0:
            temp1,temp2 = sess.run([features0_ops[idx],
                                                labels0_ops[idx]])
            features[batch_idx] = temp1[0]
            labels[batch_idx] = temp2[0]
            batch_idx+=1
        for idx in ind1:
            temp1,temp2 = sess.run([features1_ops[idx],
                                    labels1_ops[idx]])
            features[batch_idx] = temp1[0]
            labels[batch_idx] = temp2[0]
            batch_idx+=1

and then feed the batch data into network for training:

    summary,_,loss_total,loss,loss_reg = sess.run([summary_op, train_op, 
                                   loss_total_op,loss_op,loss_reg_op], 
                                  feed_dict={x_tensor: features, y_tensor: labels})


The problem is: during training, I found that the memory usage of the process is consistently increasing. Please check the following screenshot. The process 3208 is the running process. At first, the memory usage is about `10%`, but at the moment when I taken this picture, the memory usage has increased to `41.6%`. So my question is: is this a memory problem? What's wrong? I cannot find the problem. If the memory usage is increasing to some certain amount, the computer will be halt down then I need to restart.

[![memory usage][1]][1]



  [1]: https://i.stack.imgur.com/WaPkI.png"
19001,multiple traceback error  (import tensorflow as tf)..... [I am using Python 3.6.5 x64),"Issue with (import tensorflow as tf)..... details of error given below [I am using Python 3.6.5 x64)
```
Traceback (most recent call last):
  File ""C:\Users\anilt\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\anilt\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\anilt\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\anilt\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\anilt\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\anilt\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#0>"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\anilt\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\anilt\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\anilt\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\anilt\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\anilt\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\anilt\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\anilt\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\anilt\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\anilt\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```"
19000,Feature Request: get Estimator from binary model (.pb file),"### System information
- **Have I written custom code**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS High Sierra 10.13.4
- **TensorFlow installed from (source or binary)**: binary (through pip)
- **TensorFlow version (use command below)**: 1.7
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: not using CUDA, just CPU for now
- **GPU model and memory**: - 
- **Exact command to reproduce**: -

### Describe the problem
I am using the Estimator interface to train and evaluate  my model. It is really cool.
However, since I am developing for mobile, after training I must also freeze, optimize, quantize the model and then port it to TFLite.  
In these steps, my model must be in binary format (.pb).

It would be nice to build an Estimator from a .pb file to allow evaluation of binary models.
For instance, I need to check how much accuracy I am losing after the quantization step.
However, I believe that at the moment it is not possible to instantiate an Estimator from a binary model.

I have done some research before opening this issue:
- I have searched the TF documentation but did not find anything about loading a binary model into an Estimator;
- I have tried to ask this question on StackOverflow three weeks ago (here is [the link](https://stackoverflow.com/questions/49736537/load-a-frozen-model-into-a-tensorflow-estimator)), but got no answers.

As a consequence, at the moment I am just using the old GraphDef and feed_dict interface to perform evaluation and prediction with binary models. 
However using different interfaces for literally the same tasks just seems a bit... off.

Estimators probably use GraphDef under the hood. So it should be really easy to allow instantiation from a binary model.

Am I missing something? If not, can you implement this feature please?

Thanks for your support!
Andrea"
18998,"numeric_column with shape other than (1,)","```
import tensorflow as tf
import pandas as pd
import numpy as np

fcs = [tf.feature_column.numeric_column('images',shape=(1,1),dtype=tf.DType)]
mydf=pd.DataFrame({'images':[[-1,-1],[-2,-2],[-3,-3],[2,2],[3,3],[4,4]],'labels':[0,0,0,1,1,1]})

infn = tf.estimator.inputs.pandas_input_fn(x=mydf,y=mydf['labels'],batch_size=6,num_epochs=None,shuffle=False)
estimator = tf.estimator.LinearClassifier(feature_columns=fcs)
estimator.train(input_fn=infn,steps=200)
```

error i get is 

```
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InternalError'>, Unable to get element as bytes.
INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpe64_mf9a/model.ckpt.

InternalError: Unable to get element as bytes.
```

if i change my Dataframe to 

```
mydf=pd.DataFrame({'images':[-1,-2,-3,2,3,4],'labels':[0,0,0,1,1,1]})
``` 
it works"
18997,Please Tell me ， Can I brazel Tensorflow Android in windows paltom ? ,"I brazel Android Apk in windows 10, but Always makes Errors ， Can I brazel Tensorflow Android in windows paltom ? "
18996,Problems when implementing double backpropagation on BatchNormalization layer in residual block of ResNet,"### System information
- **OS Platform and Distribution: Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-87-generic x86_64)**:
- **TensorFlow version: 1.7.0**:
- **Python version: 3.6.4**: 
- **GCC/Compiler version: 7.2.0**:
- **CUDA/cuDNN version: CUDA 9.1.85**:
- **Exact command to reproduce: python train_adv_cifar.py**:

### Describe the problem
I met a problem when implementing double back-propagation when training my ResNet-20-V1 model to classify CIFAR10 images. Double back-propagation means to add a regularization term to the normal loss function. The regularization term is usually the gradient of the normal loss function W.R.T. the input tensor. The following codes are for reference. The problem happens when the following sentence is executed.
`train_step = tf.train.AdamOptimizer(learning_rate=0.0002, epsilon=1e-4).minimize(total_loss, global_step=global_step)`
![image](https://user-images.githubusercontent.com/30309087/39464412-b3b3c84e-4d4f-11e8-93ce-3655c568ec14.png)

All the BN layers in the residual block have such problems. 
And then it goes like this:
![image](https://user-images.githubusercontent.com/30309087/39464422-c6fb2e24-4d4f-11e8-9f9c-a46908a0e50e.png)
The above traceback is very long and it seems like the program is trapped there in a loop when constructing gradients in Tf Graph.
However, if I remove the BN layers in the residual block, the program works well. I also have BN layers as sequential parts of my model architecture and it works well. The residual block itself also works well. The problem happens only when there are BN layers in the residual block. But in custom ResNet architectures, there are usually BN layers in residual block. I couldn't figure out the solution.

### Source code / logs
Here is my main code:
`
import sys
import tensorflow as tf
import numpy as np

from keras.preprocessing.image import ImageDataGenerator
from cifar_model_tf import Model_cifar

model = Model_cifar(mode='train')

x_nat = tf.placeholder(tf.float32,(None,32,32,3))

y = tf.placeholder(tf.float32,(None,10))

lamda = 100
logits_nat = model._build_model(x_nat)
preds_nat = tf.nn.softmax(logits_nat)

loss_1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits_nat))
loss_2 = tf.nn.l2_loss(tf.gradients(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits_nat)), x_nat)[0])
total_loss = loss_1 + loss_2 * lamda

train_step = tf.train.AdamOptimizer(learning_rate=0.0002, epsilon=1e-4).minimize(total_loss, global_step=global_step)

`

And here is the code in my model file ""cifar_model_tf.py"":
`import numpy as np
import tensorflow as tf

class Model_cifar(object):
  """"""ResNet model.""""""
  def __init__(self, mode='eval'):
    """"""ResNet constructor.""""""
    self.mode = mode
     
  def _stride_arr(self, stride):
    """"""Map a stride scalar to the stride array for tf.nn.conv2d.""""""
    return [1, stride, stride, 1]

  def _build_model(self, x_input):
    with tf.variable_scope('model', reuse=tf.AUTO_REUSE):
      with tf.variable_scope('input'):
        ch = x_input.get_shape().as_list()[3]
        x = self._conv('init_conv', x_input, 3, ch, 16, self._stride_arr(1))
        x = self._batch_norm('init_bn', x)
        x = self._relu(x)
      res_func = self._residual
      filters = [16, 32, 64]
      with tf.variable_scope('unit_1'):
        with tf.variable_scope('unit_1_1'):
          x = res_func(x, filters[0], filters[0], self._stride_arr(1))
        with tf.variable_scope('unit_1_2'):
          x = res_func(x, filters[0], filters[0], self._stride_arr(1))
        with tf.variable_scope('unit_1_3'):
          x = res_func(x, filters[0], filters[0], self._stride_arr(1))
      with tf.variable_scope('unit_2'):
        with tf.variable_scope('unit_2_1'):
          x = res_func(x, filters[0], filters[1], self._stride_arr(2))
        with tf.variable_scope('unit_2_2'):
          x = res_func(x, filters[1], filters[1], self._stride_arr(1))
        with tf.variable_scope('unit_2_3'):
          x = res_func(x, filters[1], filters[1], self._stride_arr(1))
      with tf.variable_scope('unit_3'):
        with tf.variable_scope('unit_3_1'):
          x = res_func(x, filters[1], filters[2], self._stride_arr(2))
        with tf.variable_scope('unit_3_2'):
          x = res_func(x, filters[2], filters[2], self._stride_arr(1))
        with tf.variable_scope('unit_3_3'):
          x = res_func(x, filters[2], filters[2], self._stride_arr(1))
      
      with tf.variable_scope('unit_last'):
        x = self._avg_pool(x, 8)
      with tf.variable_scope('logit'):
        x = self._fully_connected(x, 10)
      
      return x 
  
  def _batch_norm(self, name, x):
    """"""Batch normalization.""""""
    with tf.name_scope(name):
      return tf.layers.batch_normalization(
          inputs=x,
          training=(self.mode == 'train'))
  

  def _residual(self, x, in_filter, out_filter, stride):
    """"""Residual unit with 2 sub layers.""""""
    orig_x = x
    with tf.variable_scope('sub1'):
      x = self._conv('conv1', x, 3, in_filter, out_filter, stride)
      x = self._batch_norm('bn1', x)
      x = self._relu(x)
    with tf.variable_scope('sub2'):
      x = self._conv('conv2', x, 3, out_filter, out_filter, self._stride_arr(1))
      #x = self._batch_norm('bn2', x)
    with tf.variable_scope('sub_add'):
      if in_filter != out_filter:
        y = self._conv('conv_match', orig_x, 1, in_filter, out_filter, stride)
      else:
        y = orig_x
      z = x + y
      z = self._relu(z)

    return z

  def _conv(self, name, x, filter_size, in_filters, out_filters, strides):
    """"""Convolution.""""""
    with tf.variable_scope(name):
      n = filter_size * filter_size * out_filters
      kernel = tf.get_variable('DW', [filter_size, filter_size, in_filters, out_filters],tf.float32, initializer=tf.keras.initializers.he_normal(), regularizer=tf.keras.regularizers.l2(l=1e-4))
      bias = tf.get_variable('biases', [out_filters], initializer=tf.constant_initializer())
      conv = tf.nn.conv2d(x, kernel, strides, padding='SAME')
      result = conv + bias
      return result

  def _relu(self, x, leakiness=0.0):
    """"""Relu, with optional leaky support.""""""
    return tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')

  def _fully_connected(self, x, out_dim):
    """"""FullyConnected layer for final output.""""""
    num_non_batch_dimensions = len(x.shape)
    prod_non_batch_dimensions = 1
    for ii in range(num_non_batch_dimensions - 1):
      prod_non_batch_dimensions *= int(x.shape[ii + 1])
    x = tf.reshape(x, [tf.shape(x)[0], -1])
    w = tf.get_variable(
        'DW', [prod_non_batch_dimensions, out_dim],
        initializer=tf.keras.initializers.he_normal())
    b = tf.get_variable('biases', [out_dim],
                        initializer=tf.constant_initializer())
    result = tf.nn.xw_plus_b(x, w, b)
    return result

  def _avg_pool(self, x, pool_size):
    return tf.nn.avg_pool(x, ksize=[1,pool_size,pool_size,1], strides=[1,pool_size,pool_size,1], padding='VALID')`



"
18995,compiling c++ code with TFLite library gives error with cstring,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: N/A (using only source files)
- **TensorFlow version (use command below)**: N/A
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.13.0
- **GCC/Compiler version (if compiling from source)**: g++ 5.4.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:


### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I'm trying to compile custom c++ code with command
`g++ -std=c++11 -Itensorflow/contrib/lite -I. -Lbazel-bin/tensorflow/contrib/lite -tflite test.cpp -o test`

and it gives some error that some functions in cstring cannot be found.

When I delete line `#include <cstring>` from flatbuffers/base.h, errors from cstring disappear but others remain. Including cstring in flatbuffers.h also gives same errors.
I'm using flatbuffers cloned from git google flatbuffers repository. Is this can be a problem?


I've made c++ tensorflow lite library with command
`bagel build //tensorflow/contrib/lite:framework`
with lite/BUILD including
`cc_binary(
    name = ""libtflite.so"",
    deps = [
        "":framework"",
        ""//tensorflow/contrib/lite/kernels:builtin_ops""],
    ]
)
`


Thanks.


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

#### C++ Code
```
  1 #include <stdio.h>
  2 
  3 #include ""tensorflow/contrib/lite/kernels/register.h""
  4 #include ""tensorflow/contrib/lite/model.h""
  5 #include ""tensorflow/contrib/lite/string_util.h""
  6 #include ""tensorflow/contrib/lite/tools/mutable_op_resolver.h""
  7 
  8 
  9 
 10 int main(){
 11     const char* graph_path = ""xorGate.lite"";
 12     const int num_threads = 1;
 13     std::string input_layer_type = ""float"";
 14     float x,y;
 15 
 16     std::unique_ptr<tflite::FlatBufferModel> model(
 17         tflite::FlatBufferModel::BuildFromFile(graph_path));
 18 
 19     if(!model){
 20         printf(""Failed to mmap model\n"");
 21         exit(0);
 22     }
 23 
 24     tflite::ops::builtin::BuiltinOpResolver resolver;
 25     std::unique_ptr<tflite::Interpreter> interpreter;
 26     tflite::InterpreterBuilder(*model, resolver)(&interpreter);
 27 
 28     if(!interpreter){
 29         printf(""Failed to construct interpreter\n"");
 30         exit(0);
 31     }
 32 
 33     if(num_threads != 1){
 34         interpreter->SetNumThreads(num_threads);
 35     }
 36 
 37     float* input = interpreter->typed_input_tensor<float>(0);
 38 
 39     if(interpreter->AllocateTensors() != kTfLiteOk){
 40         printf(""Failed to allocate tensors\n"");
 41         exit(0);
 42     }
 43 
 44     //read two numbers
 45     std::printf(""Type two float numbers : "");
 46     std::scanf(""%f %f"", &x, &y);
 47     input[0] = x;
 48     input[1] = y;
 49 
 50     if(interpreter->Invoke() != kTfLiteOk){
 51         std::printf(""Failed to invoke!\n"");
 52         exit(0);
 53     }
 54     float* output = interpreter->typed_output_tensor<float>(0);
 55     printf(""output = %f\n"", output[0]);
 56     return 0;
 57 }
```

#### Log
```
In file included from ./flatbuffers/base.h:2:0,
                 from ./flatbuffers/flatbuffers.h:18,
                 from ./tensorflow/contrib/lite/schema/schema_generated.h:21,
                 from ./tensorflow/contrib/lite/model.h:40,
                 from tensorflow/contrib/lite/kernels/register.h:20,
                 from test.cpp:1:
/usr/include/c++/5/cstring:75:11: error: ‘::memchr’ has not been declared
   using ::memchr;
/usr/include/c++/5/cstring:76:11: error: ‘::memcmp’ has not been declared
   using ::memcmp;
/usr/include/c++/5/cstring:77:11: error: ‘::memcpy’ has not been declared
   using ::memcpy;
/usr/include/c++/5/cstring:78:11: error: ‘::memmove’ has not been declared
   using ::memmove;
/usr/include/c++/5/cstring:79:11: error: ‘::memset’ has not been declared
   using ::memset;
/usr/include/c++/5/cstring:80:11: error: ‘::strcat’ has not been declared
   using ::strcat;

...
same error from different functions
...
   
In file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,
                 from ./tensorflow/contrib/lite/model.h:40,
                 from tensorflow/contrib/lite/kernels/register.h:20,
                 from test.cpp:1:
./flatbuffers/flatbuffers.h: In member function ‘bool flatbuffers::String::operator<(const flatbuffers::String&) const’:
./flatbuffers/flatbuffers.h:346:12: error: ‘strcmp’ is not a member of ‘std’
     return std::strcmp(c_str(), o.c_str()) < 0;
            ^
./flatbuffers/flatbuffers.h: In member function ‘void flatbuffers::Allocator::memcpy_downward(uint8_t*, size_t, uint8_t*, size_t, size_t, size_t)’:
./flatbuffers/flatbuffers.h:387:23: error: ‘memcpy’ was not declared in this scope
            in_use_back);
                       ^
./flatbuffers/flatbuffers.h: In member function ‘void flatbuffers::vector_downward::push(const uint8_t*, size_t)’:
./flatbuffers/flatbuffers.h:628:39: error: ‘memcpy’ was not declared in this scope
     memcpy(make_space(num), bytes, num);
                                       ^
./flatbuffers/flatbuffers.h: In member function ‘void flatbuffers::vector_downward::fill_big(size_t)’:
./flatbuffers/flatbuffers.h:652:57: error: ‘memset’ was not declared in this scope
     memset(make_space(zero_pad_bytes), 0, zero_pad_bytes);
                                                         ^
./flatbuffers/flatbuffers.h: In member function ‘flatbuffers::uoffset_t flatbuffers::FlatBufferBuilder::EndTable(flatbuffers::uoffset_t)’:
./flatbuffers/flatbuffers.h:982:62: error: ‘memcmp’ was not declared in this scope
         if (vt1_size != vt2_size || memcmp(vt2, vt1, vt1_size)) continue;
                                                              ^
./flatbuffers/flatbuffers.h: In member function ‘flatbuffers::Offset<flatbuffers::String> flatbuffers::FlatBufferBuilder::CreateString(const char*)’:
./flatbuffers/flatbuffers.h:1061:40: error: ‘strlen’ was not declared in this scope
     return CreateString(str, strlen(str));
                                        ^
./flatbuffers/flatbuffers.h: In member function ‘flatbuffers::Offset<flatbuffers::String> flatbuffers::FlatBufferBuilder::CreateString(char*)’:
./flatbuffers/flatbuffers.h:1068:40: error: ‘strlen’ was not declared in this scope
     return CreateString(str, strlen(str));
                                        ^
./flatbuffers/flatbuffers.h: In member function ‘flatbuffers::Offset<flatbuffers::String> flatbuffers::FlatBufferBuilder::CreateSharedString(const char*)’:
./flatbuffers/flatbuffers.h:1124:46: error: ‘strlen’ was not declared in this scope
     return CreateSharedString(str, strlen(str));
                                              ^
In file included from ./flatbuffers/base.h:12:0,
                 from ./flatbuffers/flatbuffers.h:18,
                 from ./tensorflow/contrib/lite/schema/schema_generated.h:21,
                 from ./tensorflow/contrib/lite/model.h:40,
                 from tensorflow/contrib/lite/kernels/register.h:20,
                 from test.cpp:1:
./flatbuffers/flatbuffers.h: In member function ‘void flatbuffers::FlatBufferBuilder::Finish(flatbuffers::uoffset_t, const char*, bool)’:
./flatbuffers/flatbuffers.h:1563:48: error: ‘strlen’ was not declared in this scope
       FLATBUFFERS_ASSERT(strlen(file_identifier) == kFileIdentifierLength);
                                                ^
./flatbuffers/flatbuffers.h:1563:7: note: in expansion of macro ‘FLATBUFFERS_ASSERT’
       FLATBUFFERS_ASSERT(strlen(file_identifier) == kFileIdentifierLength);
       ^
In file included from ./tensorflow/contrib/lite/schema/schema_generated.h:21:0,
                 from ./tensorflow/contrib/lite/model.h:40,
                 from tensorflow/contrib/lite/kernels/register.h:20,
                 from test.cpp:1:
./flatbuffers/flatbuffers.h: In member function ‘bool flatbuffers::FlatBufferBuilder::StringOffsetCompare::operator()(const flatbuffers::Offset<flatbuffers::String>&, const flatbuffers::Offset<flatbuffers::String>&) const’:
./flatbuffers/flatbuffers.h:1604:64: error: ‘strncmp’ was not declared in this scope
                      (std::min)(stra->size(), strb->size()) + 1) < 0;
                                                                ^
./flatbuffers/flatbuffers.h: In function ‘bool flatbuffers::BufferHasIdentifier(const void*, const char*, bool)’:
./flatbuffers/flatbuffers.h:1676:58: error: ‘strncmp’ was not declared in this scope
                  FlatBufferBuilder::kFileIdentifierLength) == 0;
                                                          ^
./flatbuffers/flatbuffers.h: In function ‘int flatbuffers::LookupEnum(const char**, const char*)’:
./flatbuffers/flatbuffers.h:2119:10: error: ‘strcmp’ is not a member of ‘std’
     if (!std::strcmp(*p, name)) return static_cast<int>(p - names);
```"
18994,[r1.7][TensorRT] Will it be able to use TRT to optimized the GraphDef that gained from a TF saved_model?,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat 4.8.5-16, Linux version 3.10.0-693.5.2.el7.x86_64
- **TensorFlow installed from (source or binary)**: pip (python 2.7)
- **TensorFlow version (use command below)**: tensorflow-gpu==1.7.0
- **Python version**:  python 2.7
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: gcc 5.3
- **CUDA/cuDNN version**: CUDA9.0, cuDNN7.0.5
- **GPU model and memory**: Tesla P4, 8GB
- **Exact command to reproduce**: NA

### Describe the problem
I tried to load the saved_model and optimize it by integrated TensorRT to do the prediction. The TensorRT will optimize the GraphDef and eventually get about 50% throughput speed-up. I verified this on frozen model, but when trying to apply that on the saved_model, I encountered an error during the graph conversion like below:
```
2018-05-01 11:36:30.553143: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1
2018-05-01 11:36:41.562979: W tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2524] Type conversion failed for InceptionV3/Conv2d_2a_3x3/BatchNorm/moving_mean
2018-05-01 11:36:41.563063: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:412] subgraph conversion error for subgraph_index:0 due to: ""Invalid argument: Unsupported data type float_ref"" SKIPPING......( 803 nodes)
Tensor(""input:0"", shape=(?, 299, 299, 3), dtype=float32)
```
I printed out the GraphDef but not found key name ""float_ref"", so not quite sure how to address this issue. 

### Source code / logs
Some essential part of my script:
```
    with tf.Graph().as_default():                                                                                                            
        with tf.Session(graph=tf.Graph()) as sess:
            if args.image:
                import numpy as np
                func = TestKit.preprocess_func['tensorflow'][args.network]
                img = func(args.image)
                img = np.expand_dims(img, axis = 0)                                                                  
            inp, out = importer.import_graph_def(graph_def=sess.graph_def, return_elements=['input','InceptionV3/Logits/SpatialSqueeze'])
            t0 = time.time()                                                                                                                                   
            print sess.graph_def
            f32_graph = trt.create_inference_graph(    #<<<<< Breakpoint
                input_graph_def=sess.graph_def,
                #outputs=[logits],                                                                                                                                              
                outputs=['InceptionV3/Logits/SpatialSqueeze'],
                #  max_batch_size = int(input_dims[0]),                                                                                                                         
                max_batch_size = 1,
                max_workspace_size_bytes=1 << 20,
                precision_mode=""FP32"",  # TRT Engine precision ""FP32"",""FP16"" or ""INT8""                                                                                          
                minimum_segment_size=2  # minimum number of nodes in an engine      
            )                                                                                                                                         
        features_tensor = sess.graph.get_tensor_by_name(""InceptionV3/Logits/SpatialSqueeze:0"")
        data_input = sess.graph.get_tensor_by_name(""input:0"")
        print data_input
    init_g = tf.global_variables_initializer()
    init_l = tf.local_variables_initializer()
    g = ops.Graph()
```

Any idea will be welcome.
Thanks,

"
18993,Why not support passing extras tensors from input_fn to model_fn in estimator?,"Sometimes input_fn may create some tensors that are useful in model_fn, these tensors can't be create outside input_fn to ensure they are in the same graph as tensors in models. 

It seems that we can put those tensors to 'labels' returned by input_fn, but because these tensors are same across batch samples, so it will cause problems when we use replicate_model_fn. (replicate_model_fn will split each tensors in 'features' or 'labels' to each GPU, but those tensors stay same across all batch samples )"
18990,Is the expected_shape argument of tf.Variable still useable/useful?,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
Installed with Anaconda
- **TensorFlow version (use command below)**:
b'v1.8.0-0-g93bc2e2072' 1.8.0
- **Python version**: 
3.6.5

I know this might be a small docs fix which doesn't fit the issue policy. But I'm not sure and I don't really know how to fix it. I'm sorry. But it seems that the `expected_shape` argument of `tf.Variable` has been deprecated for a long time (since v1.0.0). Is it still useable/useful? Is there a reason [it is still in the documentation](https://www.tensorflow.org/api_docs/python/tf/Variable#__init__)?

https://github.com/tensorflow/tensorflow/blob/8753e2ebde6c58b56675cc19ab7ff83072824a62/tensorflow/python/ops/variables.py#L284"
18988,Tensorflow input pipeline & performance - images - memory,"Best,

# system properties:

- windows 10
- intel core i7-6820HQ CPU | 2.70GHZ 8CPUs
- 16GB ram
- 64bit 
- NVIDIA Quadro M1000M
   - approx. total memory: 10093 MB
   - Display memory (VRAM): 2019 MB
   - Shared Memory: 8073 MB
 
- Tensorflow 1.8
- Python 3.5.2
 
- images (i've 36k images):
   - train: 3000 x (720x1280x3)
   - valid: 500 x (720x1280x3)
   - test : 500 x (720x1280x3)

 
### My story & strugles
First of all, I would like to say that I really like machine learning, specially neural networks. But most of the time, when I'm working with Tensorflow, I've the feeling that it is backstabbing me the entirely time. (like for example, the speed of those releases... (1.8 :O )) & Sometimes I even don't know any more if I'm doing it right or wrong? (Or can I do it better?)

Therefore, my main question is: **How do you create a proper input pipeline!**
preferable with tensorflow gpu

Because come one, it should be easy as $*%€k no? Especially, **can't you cover 90% of all the input pipelines into 1, 2 or 3 template pipelines?** (I think it is +/- possible, (a giant image with a cat is still an image | matrix))
and if it is possible, why can't we have an optimized template/base for it?


as you would have noticed, I've provided my system properties and info about the data which I'm using. My goals are:

 - Create a GAN-network (GPU)(doesn't have to be a gan for this question)
 - Use the TF-estimator api (with custom features)
 - Use TF-records !
 - Use TF-dataset !

But Unfortunately, most of the time, I'll receive errors like for example that I'm out of memory :'(
And the more I look things up, the more I start to hesitate...


## Step 1: create TF-records
In my first step, I create a tf-record (train). And as you can see, I loop over the images (from a certain folder) and write all the data into 1 tf-record.
```python 
# Helper-function for wrapping an integer so it can be saved to the TFRecords file
def wrap_int64(value):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

# Helper-function for wrapping raw bytes so they can be saved to the TFRecords file.

def wrap_bytes(value):
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))


#from skimage.transform import rescale, resize, downscale_local_mean

def convert(image_paths, out_path):
    # Args:
    # image_paths   List of file-paths for the images.
    # labels        Class-labels for the images.
    # out_path      File-path for the TFRecords output file.

    print(""Converting: "" + out_path)

    # Number of images. Used when printing the progress.
    num_images = len(image_paths)

    # Open a TFRecordWriter for the output-file.
    with tf.python_io.TFRecordWriter(out_path) as writer:

        # Iterate over all the image-paths and class-labels.
        for i, path in enumerate(image_paths):

            # Print the percentage-progress.
            print_progress(count=i+1, total=num_images)

            # Load the image-file using matplotlib's imread function.
            img_bytes_sharp = load_images(path)

            # Convert the image to raw bytes.
            img_bytes_sharp = tf.compat.as_bytes(img_bytes_sharp.tostring())

            # Create a dict with the data we want to save in the
            # TFRecords file. You can add more relevant data here.
            data = \
                {
                    'x': wrap_bytes(img_bytes_sharp)
                }

            # Wrap the data as TensorFlow Features.
            feature = tf.train.Features(feature=data)

            # Wrap again as a TensorFlow Example.
            example = tf.train.Example(features=feature)

            # Serialize the data.
            serialized = example.SerializeToString()

            # Write the serialized data to the TFRecords file.
            writer.write(serialized)
```

about the tf-record:
- size: 6 GB
- 3000 images
- preprocessed:
   - RGB values between: 0 and 1
    - type: float32



## Step 2: Load TF-records (parser)

```python
def parse(serialized):
    # Define a dict with the data-names and types we expect to
    # find in the TFRecords file.
    # It is a bit awkward that this needs to be specified again,
    # because it could have been written in the header of the
    # TFRecords file instead.
    features = \
        {
            'x': tf.FixedLenFeature([], tf.string)
        }

    # Parse the serialized data so we get a dict with our data.
    parsed_example = tf.parse_single_example(serialized=serialized,
                                             features=features)


    # Decode the raw bytes so it becomes a tensor with type.
    image_x = tf.decode_raw(parsed_example['x'], tf.float32)

    # The type is now uint8 but we need it to be float.
    #image_x = tf.cast(image_x, tf.float32)

    return image_x
```



## Step 2+1: Load TF-records (for real)
```python
def input_fn(filenames, train, batch_size=32, buffer_size=2048):
    # Args:
    # filenames:   Filenames for the TFRecords files.
    # train:       Boolean whether training (True) or testing (False).
    # batch_size:  Return batches of this size.
    # buffer_size: Read buffers of this size. The random shuffling
    #              is done on the buffer, so it must be big enough.

    # Create a TensorFlow Dataset-object which has functionality
    # for reading and shuffling data from TFRecords files.
    dataset = tf.data.TFRecordDataset(filenames=filenames)

    # Parse the serialized data in the TFRecords files.
    # This returns TensorFlow tensors for the image and labels.
    dataset = dataset.map(parse)

    if train:
        # If training then read a buffer of the given size and
        # randomly shuffle it.
        dataset = dataset.shuffle(buffer_size=buffer_size)

        # Allow infinite reading of the data.
        num_repeat = None
    else:
        # If testing then don't shuffle the data.

        # Only go through the data once.
        num_repeat = 1

    # Repeat the dataset the given number of times.
    dataset = dataset.repeat(num_repeat)

    # Get a batch of data with the given size.
    dataset = dataset.batch(batch_size)

    # Create an iterator for the dataset and the above modifications.
    iterator = dataset.make_one_shot_iterator()

    # Get the next batch of images and labels.
    images_batch = iterator.get_next()

    # The input-function must return a dict wrapping the images.
    x = {'image': images_batch}

    return x
```



# But but but but
although, I think that the above set-up is quite clear, as soon as I get rid of the mnist dataset (32x32 images), I receive memory issues. (can't even perform a batch-size of 2)
+ Also when I'm trying to solve this and watch/read the tensorflow summit videos (2018), I even wonder if I'm doing it correctly at the first place :s

For example:  
![alt image from ppt summet](https://i.stack.imgur.com/OdHI0.png ""code"")


1. First of all, how to deal with memory issues? I really can understand that I've a memory issue, when TF tries to store, the whole tf-record 6-7gig in its memory (video-card memory)? but I also would think that it is smarter then that ... (doesn't it work like a generator? add only x values in memory + their location)
    1.1 I really would like to keep the tf-records because they promises us that it is faster then e.g. the placeholders (actually it is also easier to use)

2. In the image, you see at the beginning: `Dataset.list_files` the question which I've with this is. Is this just 1 file, or does this mean that each image which I've is a **new** tf.record? (do I've) to create 3000 tf records?)(and is this the reason why I might have memory issues?)

3. The image returns a dataset and not a iterator (like in my piece of code), any clue where they might do it (this is necessary right?), when they are using the tf-estimator api?




And that is basically it.
**underlying question i**s: How can I work & play with Tensorflow|tf-records|tf-estimator **on** _BIG_ images. (even bigger than 720p)


_extra info:_
(https://www.youtube.com/watch?v=SxOsJPaxHME https://www.tensorflow.org/versions/master/performance/datasets_performance)
"
18985,No module named tensorFlow when trying to import it through Neptune,"
### System information
- I am following the instructions found [here](https://blog.deepsense.ai/region-of-interest-pooling-in-tensorflow-example) in order to create a roi pooling using Neptune and tensorflow:
-  Linux Ubuntu 16.04
- **TensorFlow installed from** binary, using virtual environment
- **TensorFlow version (use command below)**:1.7
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA
- **GPU model and memory**: working with CPU
- **Exact command to reproduce**: 
```neptune run             --parameter im_folder:$PWD/../data/images             --parameter roidb:$PWD/../data/roidb             --parameter pretrained_path:$PWD/../data/vgg16-20160129.tfmodel```


### Describe the problem

When trying to run the previous command, I get the `ImportError: No module named tensorflow` error while the environment is activated. If I try to import it with a  normal python command it works perfectly.


### Source code / logs
```
(tensorflow3) ignacio@ignacio-Swift-SF314-52:~/Escritorio/nacho/AVIVA/roi-pooling/code$ neptune run             --parameter im_folder:$PWD/../data/images             --parameter roidb:$PWD/../data/roidb             --parameter pretrained_path:$PWD/../data/vgg16-20160129.tfmodel
>
> Experiment enqueued, id: SAN-43
>
> To browse the experiment, follow:
> https://app.neptune.ml/dashboard/experiment/d28dc701-8477-4dee-9281-def81feddc8f?getStartedState=folded
>

Calculated experiment snapshot size: 44.82 kB   
Sending sources to server: 100%|█████████████████████████████████████████████████████████| 44.8k/44.8k [00:00<00:00, 170kB/s]
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/deepsense/neptune/job_wrapper.py"", line 107, in <module>
    execute()
  File ""/usr/local/lib/python2.7/dist-packages/deepsense/neptune/job_wrapper.py"", line 103, in execute
    execfile(job_filepath, job_globals)
  File ""main.py"", line 4, in <module>
    from trainer import Trainer
  File ""trainer.py"", line 8, in <module>
    import tensorflow as tf
ImportError: No module named tensorflow
Calculated experiment snapshot size: 0 Bytes   
Process exited with return code 1.
```
"
18984, returned non-zero exit status -9," I am using gcloud ml-engine. I can run tensorflow package locally and on the cloud with non-distributed version. When I run in the distributed using STANDARD_1 it gives me the following error:
{
 insertId:  ""tfdl1gg1hisuso""  
 jsonPayload: {
  created:  1525101438.307529   
  levelname:  ""ERROR""   
  lineno:  889   
  message:  ""Command '['python', '-m', u'trainer_1_4.task', u'--train-data-file', u'gs://main-175118-accern/tf_data/train_1*', u'--num-epochs', u'1', u'--train-batch-size', u'10', u'--eval-batch-size', u'10', u'--eval-data-file', u'gs://main-175118-accern/tf_data/test_1*', '--job-dir', u'gs://main-175118-accern/accern_1_4']' returned non-zero exit status -9""   
  pathname:  ""/runcloudml.py""   
 }
 labels: {
  compute.googleapis.com/resource_id:  ""1659456639612915557""   
  compute.googleapis.com/resource_name:  ""cmle-training-worker-7bc72ccf46-0-qz5zj""   
  compute.googleapis.com/zone:  ""us-central1-f""   
  ml.googleapis.com/job_id:  ""accern_1_4_dist_1""   
  ml.googleapis.com/job_id/log_area:  ""root""   
  ml.googleapis.com/task_name:  ""worker-replica-0""   
  ml.googleapis.com/trial_id:  """"   
 }
 logName:  ""projects/main-175118/logs/worker-replica-0""  
 receiveTimestamp:  ""2018-04-30T15:17:23.426890073Z""  
 resource: {
  labels: {…}   
  type:  ""ml_job""   
 }
 severity:  ""ERROR""  
 timestamp:  ""2018-04-30T15:17:18.307528972Z""  
}"
18982,TensorFlow Error : java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: windows,"Hello everyone I have this error when I compile my project tensorflow on netbeans

```
Exception in thread ""main"" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: windows, architecture: x86. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.
 at org.tensorflow.NativeLibrary.load(NativeLibrary.java:77)
 at org.tensorflow.TensorFlow.init(TensorFlow.java:66) at org.tensorflow.TensorFlow.(TensorFlow.java:70)
 at org.tensorflow.Graph.(Graph.java:258) at HelloTF.main(HelloTF.java:8)
 C:\Users\HP Notebook\AppData\Local\NetBeans\Cache\8.2\executor-enter codeheresnippets\run.xml:53: Java returned: 1 BUILD FAILED (total time: 0 seconds)
```"
18980,error when comiling Tensorflow 1.8,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow) N/A:
- **OS Platform and Distribution:Linux Debian 9.1:
- **TensorFlow installed from (source or binary) bazel:
- **TensorFlow version (use command below) v1.8.0:
- **Python version 3.7: 
- **Bazel version (if compiling from source)0.11:
- **GCC/Compiler version (if compiling from source) 6.3 and 4.9:
- **CUDA/cuDNN version 7.1:
- **GPU model and memory GTX 1080 TI, 11GB, 48GB:
- **Exact command to reproduce: gcc:

### Describe the problem
I could not able to install Tensorflow V1.8.0 on my machine. I used different gcc and g++ versions 4.9 and 6.1 in Debian 9. Furthermore, I got the same errors.

### Error:
ERROR: /home/pm/local/cpp/TENSOR_FLOW_180/tensorflow/tensorflow/core/kernels/BUILD:1864:1: output 'tensorflow/core/kernels/_objs/eye_functor_gpu/tensorflow/core/kernels/eye_functor_gpu.cu.pic.o' was not created
ERROR: /home/pm/local/cpp/TENSOR_FLOW_180/tensorflow/tensorflow/core/kernels/BUILD:1864:1: not all outputs were created or valid
Target //tensorflow:libtensorflow_cc.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 66.847s, Critical Path: 57.74s
FAILED: Build did NOT complete successfully


"
18979,TOCO converter requires min/max information for taking output from intermediate nodes?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:No.
- **TensorFlow version (use command below)**:v1.7.0-3-g024aecf414 1.7.0
- **Python version**: 3.5.4
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:9.0
- **GPU model and memory**:Nvidia 840M
- **Exact command to reproduce**:N/A

### Describe the problem
I have a quantized mobilenet downloaded from the official repository([here](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md)) , I want to collect output from the last pointwise convolutional layer of the network and run on a mobile.

The quantized frozen model contains additional fc,softmax etc layers that are of no use for my application.

To convert my quantized and frozen graph to specified input & output i run the below command
```
toco \
  --input_file=mobilenet_v1_1.0_224_quant_frozen.pb \
  --output_file=corrected_mobilenet_v1_1.0_224_frozen.tflite \
  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \
  --inference_type=QUANTIZED_UINT8 \
  --input_shape=""1,224,224,3"" \
  --input_array=input \
  --output_array=MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Conv2D_Fold \
  --std_value=127.5 --mean_value=127.5
```
and receive the following output from the converter
```
WARNING:tensorflow:From /home/sam/.virtualenvs/tf6/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.
2018-04-30 16:31:24.735099: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 598 operators, 889 arrays (0 quantized)
2018-04-30 16:31:24.753483: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 571 operators, 851 arrays (0 quantized)
2018-04-30 16:31:24.773414: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 571 operators, 851 arrays (0 quantized)
2018-04-30 16:31:25.122835: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 54 operators, 109 arrays (1 quantized)
2018-04-30 16:31:25.123855: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 54 operators, 109 arrays (1 quantized)
2018-04-30 16:31:25.124206: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 28 operators, 83 arrays (1 quantized)
2018-04-30 16:31:25.124569: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 28 operators, 83 arrays (1 quantized)
2018-04-30 16:31:25.155063: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:169] Array MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Conv2D_Fold does not have MinMax information, and is not a constant array. Cannot proceed with quantization.
Aborted (core dumped)
```

Why does this quantized graph requires min max information for intermediate nodes when it can resolve the final nodes correctly without additional min/max information?

I also tried manually giving default min/max information but i suffer a huge performance drop if i do this.




"
18978,Tensorflow on imx6,"Hi,
I am trying to port tensorflow on imx6 board,
First i installed bazel and tensorflow from source it got installed but when i did import tensorflow i was getting this:
 File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python2.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python2.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so

Then in tried to install using tensorflow makefile it also got installed but when i ran my own .pb file it gave this error:
 [[Node: MultipleGridAnchorGenerator/assert_equal/Equal = Equal[T=DT_INT32](MultipleGridAnchorGenerator/assert_equal/x, MultipleGridAnchorGenerator/strided_slice_2)]]
2018-04-30 03:21:07.874606: I tensorflow/tools/benchmark/benchmark_model.cc:468] Initialized session in 2.16979s
can anyone tell  how to install tensorflow on imx6"
18976,TFlite TransposeConv op supported but unregistered,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.13.3
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: v1.8.0-rc1-1107-g8a428cd 1.8.0-rc1
- **Python version**: 3.5.4
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**: 9.1.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

I'm trying to convert my custom model to tflite using tf.contrib.lite.toco_convert. 
The model has some TransposeConv in it. I expect these to convert as looking at commit https://github.com/tensorflow/tensorflow/commit/58fe7d26afa435560e7a0d8ca6fc8d670d2477da there does seem to be quite a bit of code in place to support transposeconv, including the actual implementation and graph transformations. However when I run the conversion it tells me it's not supported (see below output).


It seems like this op is supported but simply unregistered for some reason, which might be a bug or maybe there is some mysterious reason it cannot be registered?

Thanks



### Source code / logs

2018-04-30 11:11:07.554671: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 204 operators, 314 arrays (0 quantized)
2018-04-30 11:11:07.557853: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 204 operators, 314 arrays (0 quantized)
2018-04-30 11:11:07.562549: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 52 operators, 108 arrays (0 quantized)
2018-04-30 11:11:07.566535: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 48 operators, 103 arrays (0 quantized)
2018-04-30 11:11:07.567162: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 43 operators, 95 arrays (0 quantized)
2018-04-30 11:11:07.567617: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 43 operators, 95 arrays (0 quantized)
2018-04-30 11:11:07.568085: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:329] Total transient array allocated size: 934336 bytes, theoretical optimalvalue: 933120 bytes.
2018-04-3011:11:07.569157: F tensorflow/contrib/lite/toco/tflite/export.cc:315] Some of the operators in the model are not supported by the standard TensorFlow Literuntime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need customimplementations: TransposeConv.
"
18975,``sequence_bucketized_column`` is missing,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A 
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

TensorFlow 1.8 introduced a new API for sequence feature columns. The API covers categorical, numeric columns, but lacks `sequence_bucketized_column`. It should not be hard to implement, so if that's OK with you, I can do it myself and submit a PR.

### Source code / logs

N/A.
"
18974,Permission denied: '/Library/Python/2.7/site-packages/pbr-4.0.2.dist-info',"pip install tensorflow
Collecting tensorflow
  Using cached https://files.pythonhosted.org/packages/9b/1e/d89f1369b5b8045e5aedf43718b45d2396d3c61e9cc56123c24b7758dd9f/tensorflow-1.8.0-cp27-cp27m-macosx_10_11_x86_64.whl
Requirement already satisfied: numpy>=1.13.3 in /Library/Python/2.7/site-packages (from tensorflow) (1.14.3)
Collecting mock>=2.0.0 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl
Collecting enum34>=1.1.6 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/c5/db/e56e6b4bbac7c4a06de1c50de6fe1ef3810018ae11732a50f15f62c7d050/enum34-1.1.6-py2-none-any.whl
Collecting wheel (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/1b/d2/22cde5ea9af055f81814f9f2545f5ed8a053eb749c08d186b369959189a8/wheel-0.31.0-py2.py3-none-any.whl
Collecting astor>=0.6.0 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/b2/91/cc9805f1ff7b49f620136b3a7ca26f6a1be2ed424606804b0fbcf499f712/astor-0.6.2-py2.py3-none-any.whl
Collecting backports.weakref>=1.0rc1 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/88/ec/f598b633c3d5ffe267aaada57d961c94fdfa183c5c3ebda2b6d151943db6/backports.weakref-1.0.post1-py2.py3-none-any.whl
Collecting termcolor>=1.1.0 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz
Collecting gast>=0.2.0 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz
Collecting grpcio>=1.8.6 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/47/75/c8b6a98b3a8de062e9454a3b796344bb5a8f36caa558578c8bbc584ec901/grpcio-1.11.0-cp27-cp27m-macosx_10_11_x86_64.whl
Requirement already satisfied: six>=1.10.0 in /Library/Python/2.7/site-packages/six-1.11.0-py2.7.egg (from tensorflow) (1.11.0)
Collecting absl-py>=0.1.6 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/90/6b/ba04a9fe6aefa56adafa6b9e0557b959e423c49950527139cb8651b0480b/absl-py-0.2.0.tar.gz
Collecting protobuf>=3.4.0 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/c7/15/e21b9597043ecdc586b76b29608b30212658d239d66407621a642aedb41f/protobuf-3.5.2.post1-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl
Collecting tensorboard<1.9.0,>=1.8.0 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/4d/1e/3bfb48ff165e331c0c5fdca5de79d497fa5d71f3cb2eee2733ff22e898df/tensorboard-1.8.0-py2-none-any.whl
Collecting pbr>=0.11 (from mock>=2.0.0->tensorflow)
  Using cached https://files.pythonhosted.org/packages/e1/ba/f95e3ec83f93919b1437028e989cf3fa5ff4f5cae4a1f62255f71deddb5b/pbr-4.0.2-py2.py3-none-any.whl
Collecting funcsigs>=1; python_version < ""3.3"" (from mock>=2.0.0->tensorflow)
  Using cached https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl
Requirement already satisfied: futures>=2.2.0 in /Library/Python/2.7/site-packages/futures-3.2.0-py2.7.egg (from grpcio>=1.8.6->tensorflow) (3.2.0)
Requirement already satisfied: setuptools in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from protobuf>=3.4.0->tensorflow) (18.5)
Collecting markdown>=2.6.8 (from tensorboard<1.9.0,>=1.8.0->tensorflow)
  Using cached https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl
Collecting bleach==1.5.0 (from tensorboard<1.9.0,>=1.8.0->tensorflow)
  Using cached https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl
Collecting werkzeug>=0.11.10 (from tensorboard<1.9.0,>=1.8.0->tensorflow)
  Using cached https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl
Collecting html5lib==0.9999999 (from tensorboard<1.9.0,>=1.8.0->tensorflow)
  Using cached https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz
Installing collected packages: pbr, funcsigs, mock, enum34, wheel, astor, backports.weakref, termcolor, gast, protobuf, grpcio, absl-py, markdown, html5lib, bleach, werkzeug, tensorboard, tensorflow
Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/Library/Python/2.7/site-packages/pbr-4.0.2.dist-info'
Consider using the `--user` option or check the permissions."
18971,Problems about loading SavedModelBundle,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04.3 LTS
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.6
- **Python version**: 
2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
CUDA Version 9.0.176
- **GPU model and memory**:
Tesla K80, total memory 11.17GiB
- **Exact command to reproduce**:

### Describe the problem
- **Definitions**:
**Update N** - Update status of proposed issues.
**Additional N** - New relevant issues met during development.
- **Background**: 
I saved a trained model under Tensorflow 1.0 using Saver.save(), then updated some variables and loaded it into Tensorflow 1.6. Now I saved it again using SavedModelBundle APIs and want to load it in Java. 
- **Current Status**: 
The model has been exported as .pb and variables files, with tags and SignatureDefs. But when I tried to SavedModelBundle.load() it in Java, it seemed to be blocked until manually terminated. Prompts after termination was like:
![error screenshot - github](https://user-images.githubusercontent.com/22216143/39414218-aaa3f24e-4c67-11e8-802a-93900c7b173a.jpg)
**Update 1**: (transfer the import procedure from Win to Ubuntu)
Exception in thread ""main"" java.lang.IllegalArgumentException: Cannot assign a device for operation ... : Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Registered kernels:
  device='CPU'

- **Further Description**: 
What I want to emphasize is that, speaking of the SignatureDef settings, I attempted to give several inputs and outputs at one time, in which case some outputs may also be inputs again.
More clearly in Seq2Seq model, I have outputs from the encoder which may also be inputs to the decoder. It also means that I want to fetch, modify and feed some intermediate variables of the model instead of playing with a simple model just with 1 input and 1 output. Hence the situation now is a little complex.
I read the documents but still have not found information describing this situation. Also searching the community gave no suitable answer. Therefore, could you help me to figure out if my implementation is right? or how to use the APIs to serve this situation?
Thank you!
**Update 1**: 
It seems like SavedModelBundle failed to register GPU device for operations? or there are some configurations I should do before using it? 
Besides, I have set ""**tf.ConfigProto(allow_soft_placement=True)**"".
Thanks.

### Source code / logs
Python codes:
``` python
saved_model_builder = tf.saved_model.builder.SavedModelBuilder(export_dir)
a = tf.saved_model.utils.build_tensor_info(model.a)
b = tf.saved_model.utils.build_tensor_info(model.b)
c = tf.saved_model.utils.build_tensor_info(model.c)
d = tf.saved_model.utils.build_tensor_info(model.d)
e = tf.saved_model.utils.build_tensor_info(model.e)
signature = tf.saved_model.signature_def_utils.build_signature_def(
        inputs = {
            ""a"": a,
            ""b"": b,
            ""c"": c},
        outputs = {
            ""d"": d,
            ""e"": e,
            ""a"": a,
            ""b"": b},
        method_name = tf.saved_model.signature_constants.PREDICT_METHOD_NAME)
asset_file = tf.constant(FLAGS.vocab_path, name=""Vocabulary"")
tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, asset_file)
saved_model_builder.add_meta_graph_and_variables(
        self._sess, [tf.saved_model.tag_constants.SERVING],
        signature_def_map = {tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature},
            assets_collection = tf.get_collection(tf.GraphKeys.ASSET_FILEPATHS))
saved_model_builder.save()
```
Java code:
``` java
private static final String MODEL_TAG = ""serve"";
SavedModelBundle model = SavedModelBundle.load(modelDir, MODEL_TAG);
```"
18970,v,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
18969,Error (windows 10 - Anaconda),"problemes to install
------------
During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Usuario\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Usuario\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Usuario\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Usuario\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Usuario\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 985, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 968, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 957, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 938, in create_module
  File ""<frozen importlib._bootstrap_external>"", line 938, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Uma rotina de inicialização da biblioteca de vínculo dinâmico (DLL) falhou.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Usuario\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Usuario\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Usuario\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Usuario\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
18967,r,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
18966,.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
18964,Add profile binary to dev image,"Why `tensorflow/core/profiler:profiler` was not added at least in dev images?

I.e. for the gpu one:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu#L103"
18962,bug of tf.extract_image_patches,"### System information
- **OS Platform and Distribution**: Linux Ubuntu  16.04
- **TensorFlow installed from pip**:
- **TensorFlow version**: 1.5
- **Python version**: 2.7 
- **CUDA/cuDNN version**: 8.0
- **Have I written custom code**: N/A
- **Bazel version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### the problem
I have a input ""x"" which size is (batch, width, height, chanel), because the height of data is variable. so x.get_shape() is (?, 20, ?, 64). I use the following code to do something like im2col:
`im2col = tf.extract_image_patches(x, ksizes=[1, 1, KERNEL_SIZE, 1], strides=[1, 1, 1, 1], rates=[1, 1, 1, 1], padding=""SAME"")
`
the output ""im2col"" lost some shape, so I use the following code to set_shape:
 `im2col.set_shape([shape1, shape2, shape3, KERNEL_SIZE * shape4])`
and then reshape im2col to (shape1, shape2, shape3, KERNEL_SIZE, shape4)

In the process of building graph, errors arise when processing optimization algorithms (Adam).

> TypeError: unsupported operand type(s) for /: 'NoneType' and 'long'

I'm sure the error is produced by tf.extract_image_patches. If I replace tf.extract_image_patches with tf.tile (to make the output size of tf.tile same as tf.extract_image_patches), graph will be built sucessfully.

### Error message
Traceback (most recent call last):
  File ""/home/anaconda2/bin/t2t-trainer"", line 32, in <module>
    tf.app.run()
  File ""/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 129, in run
    _sys.exit(main(argv))
  File ""/home/anaconda2/bin/t2t-trainer"", line 28, in main
    t2t_trainer.main(argv)
  File ""/home/anaconda2/lib/python2.7/site-packages/tensor2tensor/bin/t2t_trainer.py"", line 338, in main
    execute_schedule(exp)
  File ""/home/anaconda2/lib/python2.7/site-packages/tensor2tensor/bin/t2t_trainer.py"", line 288, in execute_schedule
    getattr(exp, FLAGS.schedule)()
  File ""/home/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 635, in train_and_evaluate
    self.train(delay_secs=0)
  File ""/home/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 377, in train
    saving_listeners=self._saving_listeners)
  File ""/home/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 824, in _call_train
    saving_listeners=saving_listeners)
  File ""/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 314, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 743, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 725, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/anaconda2/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py"", line 886, in wrapping_model_fn
    use_tpu=use_tpu)
  File ""/home/anaconda2/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py"", line 980, in estimator_model_fn
    loss, num_async_replicas=num_async_replicas)
  File ""/home/anaconda2/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py"", line 984, in estimator_spec_train
    train_op = self.optimize(loss, num_async_replicas=num_async_replicas)
  File ""/home/anaconda2/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py"", line 401, in optimize
    loss, lr, self.hparams, use_tpu=common_layers.is_on_tpu())
  File ""/home/anaconda2/lib/python2.7/site-packages/tensor2tensor/utils/optimize.py"", line 68, in optimize
    colocate_gradients_with_ops=True)
  File ""/home/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py"", line 241, in optimize_loss
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/home/anaconda2/lib/python2.7/site-packages/tensor2tensor/utils/optimize.py"", line 110, in compute_gradients
    gradients = self._opt.compute_gradients(loss, var_list, **kwargs)
  File ""/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 456, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py"", line 608, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py"", line 374, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py"", line 608, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_grad.py"", line 724, in _ExtractImagePatchesGrad
    cols_out = int(ceil(cols_in / stride_h))
TypeError: unsupported operand type(s) for /: 'NoneType' and 'long'

"
18961,CUDA 9.1 Support,"I am using NVIDIA GeForce GTX 1050 and installed NVIDIA 388.19. I installed cuDNN 7.0.5 and CUDA 9.1. As of my understanding, I know that, tensorflow is not supported in CUDA 9.1. My question is when I can expect the next build/release of TF to support CUDA 9.1. For the time being, shall I make a link from CUDA 9.0 to CUDA 9.1 and expect to work? Or is there any better way to solve the problem?"
18960,Please clarify the documentation on eager execution and available host language functionalities,"The documentation on eager execution [here](https://www.tensorflow.org/programmers_guide/eager), particularly for the ""fizzbuzz"" example, does not reflect what tensorflow does. Specifically, all the comparisons num % x == y  do not work as the reader is expecting them to. According to me, it is not true that ""all the functionality of the host language is available"" because in order to make that code work I need to convert all the comparisons into num.numpy() % x == y (I have enabled eager execution)."
18958,[eature request,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
18956,Fail to build cmake with gpu for _lstm and _rnn ops in windows,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: github master (should be 1.8?)
- **Python version**:  3.6
- **Bazel version (if compiling from source)**: CMake build
- **GCC/Compiler version (if compiling from source)**: MSVC2015
- **CUDA/cuDNN version**:  CUDA 9.0 with CUDNN 7.0.5
- **GPU model and memory**: GTX 1060 6GB
- **Exact command to reproduce**:

cmake \
    	-DCMAKE_INSTALL_PREFIX=../install/ \
    	-DCMAKE_BUILD_TYPE=Release \
        -Dtensorflow_BUILD_SHARED_LIB=ON \
    	-Dtensorflow_BUILD_ALL_KERNELS=ON \
    	-Dtensorflow_BUILD_CONTRIB_KERNELS=ON \
    	-Dtensorflow_BUILD_CC_EXAMPLE=ON \
    	-Dtensorflow_BUILD_PYTHON_BINDINGS=ON \
    	-Dtensorflow_ENABLE_GRPC_SUPPORT=ON \
    	-Dtensorflow_ENABLE_SSL_SUPPORT=OFF \
    	-Dtensorflow_BUILD_CC_TESTS=OFF \
    	-Dtensorflow_BUILD_PYTHON_TESTS=OFF \
        -Dtensorflow_ENABLE_GPU=ON ..

### Describe the problem
In the [contrib/rnn](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/BUILD) build, the gpu resource requires `blas_gemm.h` . I found this is missed in the cmake 
[relevent position](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/tf_python.cmake)

 I have built pass in older version of tensorflow release (1.5) but not in 1.8. The cuda object built in rnn should be linked with blas_gemm? How can I modified the cmake files to do this?

### Source code / logs
The cmake file that fails to build

```cmake
if(WIN32)
    # include contrib/rnn as .so
    #
    set(tf_gru_srcs
        ""${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/blas_gemm.cc""
        ""${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/blas_gemm.h""
        ""${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/gru_ops.cc""
        ""${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/gru_ops.h""
        ""${tensorflow_source_dir}/tensorflow/contrib/rnn/ops/gru_ops.cc""
    )
    set(tf_gru_gpu_srcs
        ""${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/gru_ops_gpu.cu.cc""
    )

    set(tf_lstm_srcs
        ""${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/blas_gemm.cc""
        ""${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/blas_gemm.h""
        ""${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/lstm_ops.cc""
        ""${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/lstm_ops.h""
        ""${tensorflow_source_dir}/tensorflow/contrib/rnn/ops/lstm_ops.cc""
    )
    set(tf_lstm_gpu_srcs
        ""${tensorflow_source_dir}/tensorflow/contrib/rnn/kernels/lstm_ops_gpu.cu.cc""
    )

    AddUserOps(TARGET _gru_ops
        SOURCES ""${tf_gru_srcs}""
        GPUSOURCES ${tf_gru_gpu_srcs}
        DEPENDS pywrap_tensorflow_internal tf_python_ops
        DISTCOPY ${CMAKE_CURRENT_BINARY_DIR}/tf_python/tensorflow/contrib/rnn/python/ops/)

    AddUserOps(TARGET _lstm_ops
        SOURCES ""${tf_lstm_srcs}""
        GPUSOURCES ${tf_lstm_gpu_srcs}
        DEPENDS pywrap_tensorflow_internal tf_python_ops
        DISTCOPY ${CMAKE_CURRENT_BINARY_DIR}/tf_python/tensorflow/contrib/rnn/python/ops/)
endif(WIN32)
```

Bazel BUILD file in contrib/rnn:

```BAZEL
tf_custom_op_library(
    name = ""python/ops/_lstm_ops.so"",
    srcs = [
        ""kernels/blas_gemm.cc"",
        ""kernels/blas_gemm.h"",
        ""kernels/lstm_ops.cc"",
        ""kernels/lstm_ops.h"",
        ""ops/lstm_ops.cc"",
    ],
    gpu_srcs = [
        ""kernels/blas_gemm.h"",
        ""kernels/lstm_ops_gpu.cu.cc"",
        ""kernels/lstm_ops.h"",
    ],
    deps = [""//tensorflow/core/kernels:eigen_helpers""],
)

...

tf_custom_op_library(
    name = ""python/ops/_gru_ops.so"",
    srcs = [
        ""kernels/blas_gemm.cc"",
        ""kernels/blas_gemm.h"",
        ""kernels/gru_ops.cc"",
        ""kernels/gru_ops.h"",
        ""ops/gru_ops.cc"",
    ],
    gpu_srcs = [
        ""kernels/blas_gemm.h"",
        ""kernels/gru_ops_gpu.cu.cc"",
        ""kernels/gru_ops.h"",
    ],
    deps = [""//tensorflow/core/kernels:eigen_helpers""],
)

```"
18952,Feature request: Option to create dataset from a subset of the columns in the CSV file using tf.contrib.data.make_csv_dataset(),"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**:  3.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**:NA
- **Exact command to reproduce**:
tf.contrib.data.make_csv_dataset()

### Describe the problem

The `tf.contrib.data.make_csv_dataset()` is a very useful feature which allows us to convert CSV files directly into at dataset without having to use Pandas library (like shown [here](https://stackoverflow.com/questions/49116343/dataset-api-flat-map-method-producing-error-for-same-code-which-works-with-ma/49140725#49140725)). However it is missing an important feature which Pandas had, that is to read a subset of the columns in the CSV file.
For example the following code:
```
dataset=tf.contrib.data.make_csv_dataset(file_pattern='./data/power_data/MISO_power_data1.csv',batch_size=24,shuffle=False)
dataset = dataset.batch(4)
X_iter = dataset.make_one_shot_iterator()
X_batch = X_iter.get_next()
X_batch
```
results in following dataset:
```
{'Actual_Load_MWh': <tf.Tensor 'IteratorGetNext_9:0' shape=(?, ?) dtype=float32>,
 'Hour_Ending': <tf.Tensor 'IteratorGetNext_9:1' shape=(?, ?) dtype=int32>,
 'Market_Day': <tf.Tensor 'IteratorGetNext_9:2' shape=(?, ?) dtype=int32>,
 'Wind_MWh': <tf.Tensor 'IteratorGetNext_9:3' shape=(?, ?) dtype=float32>}
```
However I don't want feature columns for 'Hour_Ending'  and  'Market_Day' in my dataset (since they are not relevant training data) . This could be done in Pandas using code below:
```
df_input=pd.read_csv('./data/power_data/MISO_power_data1.csv',
                         usecols=['Wind_MWh', 'Actual_Load_MWh'], nrows=24)
```
I know the easy solution would be to create a CSV file having only the feature columns I want. But it would be a great utility feature to add before `make_csv_dataset()` migrates out of contrib into core TF. I can submit a PR for this if required."
18947,tf.contrib.data.prefetch_to_device throws an error when used with tf.data.Iterator.from_structure,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I'm using **tf.data.Iterator.from_structure** mechanism to switch between training/validation datasets during training
However, when **tf.contrib.data.prefetch_to_device** is added at the end of the dataset pipeline, the code crashes with the following error:
```NotImplementedError: `prefetch_to_device()` must be the last transformation in a dataset pipeline.```
Explicit one shot iterator works fine

### Source code / logs
```
with tf.Graph().as_default():
    with tf.Session() as sess:
        dataset = tf.data.Dataset.range(10).apply(tf.contrib.data.prefetch_to_device('/cpu:0'))

        # 1) works fine
        iterator = dataset.make_one_shot_iterator()
        print sess.run(iterator.get_next())
        
        # 2) fails
        iterator = tf.data.Iterator.from_structure((tf.int64), ([]))
        sess.run(iterator.make_initializer(dataset))
        print sess.run(iterator.get_next())
```
```
0

---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-34-dbae58b82de0> in <module>()
      6 
      7         iterator = tf.data.Iterator.from_structure((tf.int64), ([]))
----> 8         sess.run(iterator.make_initializer(dataset))
      9         print sess.run(iterator.get_next())

/usr/local/lib/python2.7/dist-packages/tensorflow/python/data/ops/iterator_ops.pyc in make_initializer(self, dataset, name)
    306     with ops.colocate_with(self._iterator_resource):
    307       return gen_dataset_ops.make_iterator(
--> 308           dataset._as_variant_tensor(), self._iterator_resource, name=name)  # pylint: disable=protected-access
    309 
    310   def get_next(self, name=None):

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/data/python/ops/prefetching_ops.pyc in _as_variant_tensor(self)
    289     # TODO(mrry): Investigate support for chaining further transformations after
    290     # the prefetch, including GPU support.
--> 291     raise NotImplementedError(""`prefetch_to_device()` must be the last ""
    292                               ""transformation in a dataset pipeline."")
    293 

NotImplementedError: `prefetch_to_device()` must be the last transformation in a dataset pipeline.
```

I guess, prefetching is still under development? Is there a temporary workaround for this kind of scenario?"
18946,"A preliminary issue,Techniques for object detection","A preliminary issue about tensorflow for Android object tracking technology
Is it using opencv? Thanks for your answer"
18945,How to change batch_size when load a frozen model?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
18943,ssd_mobilenet_v2 was slower than ssd_mobilenet_v1 in the tflite ?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:Source
- **TensorFlow version (use command below)**:1.8.0rc0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.12.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**:cuda-9.0/7.0
- **GPU model and memory**:GeForce GTX 1080/8105MiB
- **Phone**: xiaomi5 (Snapdragon 820)

### Describe the problem
Using tflite's [benchmark_model](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/tools/benchmark_model.cc) tool, I tested the performance of the ssd_mobilenet_v1 and ssd_mobilenet_v2  models , and found that ssd_mobilenet_v2 was slower than ssd_mobilenet_v1, about 10ms, when setting to 4 threads.

But SSDLite presented in [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) is 35% faster than Mobilenet V1 SSD on a Google Pixel phone CPU (200ms vs. 270ms) at the same accuracy.

Who can explain why ?
### Source code / logs
ssd_mobilenet_v1 : 
```
 ./benchmark_model \
>   --graph=""mobilenet_ssd_v1_quan.tflite"" \
>   --input_layer=""Preprocessor/sub"" \
>   --input_layer_shape=""1,300,300,3"" \
>   --input_layer_type=""uint8"" \
>   --run_delay=""-1.0"" \
>   --output_layer=""concat,concat_1"" \
>   --num_runs=50 \
>   --num_threads=4 \
>   --warmup_runs=1 \
>   --use_nnapi=false
WARNING: linker: /data/local/tmp/benchmark_model: unused DT entry: type 0xf arg 0x82e
Graph: [mobilenet_ssd_v1_quan.tflite]
Input layers: [Preprocessor/sub]
Input shapes: [1,300,300,3]
Input types: [uint8]
Output layers: [concat,concat_1]
Num runs: [50]
Inter-run delay (seconds): [-1.0]
Num threads: [4]
Warmup runs: [1]
Use nnapi : [0]
Enable profiling : [0]
nnapi error: unable to open library libneuralnetworks.so
Initialized session in 0.02646s
Running benchmark for 1 iterations: 
count=1 min=220752 max=220752 avg=220752 std=0
Running benchmark for 50 iterations: 
count=50 min=75066 max=193842 avg=82848.9 std=19942
Average inference timings in us: 82848 , Warmup: 220752,
``` 
ssd_mobilenet_v2 :
```
./benchmark_model \
>   --graph=""mobilenet_ssd_v2_quan.tflite"" \
>   --input_layer=""Preprocessor/sub"" \
>   --input_layer_shape=""1,300,300,3"" \
>   --input_layer_type=""uint8"" \
>   --run_delay=""-1.0"" \
>   --output_layer=""concat,concat_1"" \
>   --num_runs=50 \
>   --num_threads=4 \
>   --warmup_runs=1 \
>   --use_nnapi=false
WARNING: linker: /data/local/tmp/benchmark_model: unused DT entry: type 0xf arg 0x82e
Graph: [mobilenet_ssd_v2_quan.tflite]
Input layers: [Preprocessor/sub]
Input shapes: [1,300,300,3]
Input types: [uint8]
Output layers: [concat,concat_1]
Num runs: [50]
Inter-run delay (seconds): [-1.0]
Num threads: [4]
Warmup runs: [1]
Use nnapi : [0]
Enable profiling : [0]
nnapi error: unable to open library libneuralnetworks.so
Initialized session in 0.033135s
Running benchmark for 1 iterations: 
count=1 min=110434 max=110434 avg=110434 std=0
Running benchmark for 50 iterations: 
count=50 min=78189 max=285788 avg=90162.9 std=29644
Average inference timings in us: 90162 , Warmup: 110434, 
```"
18941,question when run distributed tensorflow,"when I run the distributed tensorflow , I met this problem described below on one of the\ worker:


Traceback (most recent call last):
  File ""dis_convae.py"", line 277, in <module>
    _, cost,step = sess.run([train_op_1, model_one_cost,global_step], feed_dict={model_one_X: input_})
  File ""/home/master/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 778, in run
    run_metadata_ptr)
  File ""/home/master/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 982, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/master/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1032, in _do_run
    target_list, options, run_metadata)
  File ""/home/master/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1052, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef missing attr 'use_nesterov' from Op<name=ApplyAdam; signature=var:Ref(T), m:Ref(T), v:Ref(T), beta1_power:T, beta2_power:T, lr:T, beta1:T, beta2:T, epsilon:T, grad:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; attr=use_nesterov:bool,default=false>; NodeDef: Adam/update_Variable_1/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[""loc:@Variable_1""], use_locking=false, _device=""/job:ps/replica:0/task:0/device:CPU:0""](Variable_1, Variable_1/Adam, Variable_1/Adam_1, beta1_power/read, beta2_power/read, Adam/learning_rate_S147, Adam/beta1_S149, Adam/beta2_S151, Adam/epsilon_S153, gradients/Add_grad/tuple/control_dependency_1_S155)

Does anyone know how to solve it?"
18936,the problem when downgrading the tensorflow version from 1.7 to 1.6,"I got the following error message when trying to roll back the tensorflow version from 1.7 to 1.6.  What might be the cause?

`conda install tensorflow==1.6`

```
Fetching package metadata ...........
Solving package specifications: .

Package plan for installation in environment /data/virtualE/deeplab:

The following packages will be DOWNGRADED:

    tensorboard:         1.7.0-py36hf484d3e_0 --> 1.6.0-py36hf484d3e_1
    tensorflow:          1.7.0-0              --> 1.6.0-0
    tensorflow-base:     1.7.0-py36hff88cb2_0 --> 1.6.0-py36hff88cb2_0
    tensorflow-gpu:      1.7.0-0              --> 1.6.0-0
    tensorflow-gpu-base: 1.7.0-py36h8a131e3_0 --> 1.6.0-py36h8a131e3_0

tensorboard-1. 100% |##################################################################################################| Time: 0:00:00  39.64 MB/s
tensorboard-1. 100% |##################################################################################################| Time: 0:00:00 502.27 MB/s
tensorboard-1. 100% |##################################################################################################| Time: 0:00:00 498.78 MB/s

CondaError: CondaError: Failed to write to /tmp/conda/4/pkgs/tensorboard-1.6.0-py36hf484d3e_1.tar.bz2
  errno: 28
CondaError: CondaError: Failed to write to /tmp/conda/4/pkgs/tensorboard-1.6.0-py36hf484d3e_1.tar.bz2
  errno: 28
CondaError: CondaError: Failed to write to /tmp/conda/4/pkgs/tensorboard-1.6.0-py36hf484d3e_1.tar.bz2
  errno: 28
```"
18934,Patch Request: Move CROSSTOOL_nvcc.tpl to c++14,"CUDA9.0 is already supporting C++14 now:
https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cpp14

```
diff --git a/third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl b/third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl
index 05290d6..237d001 100644
--- a/third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl
+++ b/third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl
@@ -51,9 +51,9 @@ toolchain {
   # path, combined with the rest of our Bazel configuration causes our
   # compilation to use those files.
   tool_path { name: ""gcc"" path: ""clang/bin/crosstool_wrapper_driver_is_not_gcc"" }
-  # Use ""-std=c++11"" for nvcc. For consistency, force both the host compiler
-  # and the device compiler to use ""-std=c++11"".
-  cxx_flag: ""-std=c++11""
+  # Use ""-std=c++14"" for nvcc. For consistency, force both the host compiler
+  # and the device compiler to use ""-std=c++14"".
+  cxx_flag: ""-std=c++14""
   linker_flag: ""-Wl,-no-as-needed""
   linker_flag: ""-lstdc++""
   linker_flag: ""-B/usr/bin/""
diff --git a/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl b/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl
index 2558f46..5b1e65c 100755
--- a/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl
+++ b/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl
@@ -167,7 +167,7 @@ def InvokeNvcc(argv, log=False):
   undefines = ''.join([' -U' + define for define in undefines])
   std_options = GetOptionValue(argv, 'std')
   # currently only c++11 is supported by Cuda 7.0 std argument
-  nvcc_allowed_std_options = [""c++11""]
+  nvcc_allowed_std_options = [""c++11"",""c++14""]
   std_options = ''.join([' -std=' + define
       for define in std_options if define in nvcc_allowed_std_options])
```"
18932,is it possible using tf.contrib.model_pruning.masked_conv2d instead of tf.layers.conv2d?,
18931,CMake build without GRPC and Python bindings fails,"### System information
- OS: Linux Ubuntu 14.04
- Tensorflow: installed from source ([master branch](8a428cdd350a56b9f7de2ed55b3fbe7b6ad6b257))
- GCC 4.8.4
- CMake 3.8.2
- No GPU

### Problem
The CMake build fails on Ubuntu when GRPC support is disabled.

### Command
```
cmake \
    	-DCMAKE_INSTALL_PREFIX=../test/ \
    	-DCMAKE_BUILD_TYPE=Release \
        -Dtensorflow_BUILD_SHARED_LIB=ON \
    	-Dtensorflow_BUILD_ALL_KERNELS=ON \
    	-Dtensorflow_BUILD_CONTRIB_KERNELS=OFF \
    	-Dtensorflow_BUILD_CC_EXAMPLE=OFF \
    	-Dtensorflow_BUILD_PYTHON_BINDINGS=OFF \
    	-Dtensorflow_ENABLE_GRPC_SUPPORT=OFF \
    	-Dtensorflow_ENABLE_SSL_SUPPORT=OFF \
    	-Dtensorflow_BUILD_CC_TESTS=OFF \
    	-Dtensorflow_BUILD_PYTHON_TESTS=OFF \
        -Dtensorflow_ENABLE_GPU=OFF ..
make
```

### Logs
```
[...]
make[2]: *** No rule to make target `../grpc', needed by `tensorflow/core/debug/debug_service.grpc.pb.cc'.
Stop.
make[1]: *** [CMakeFiles/tf_protos_cc.dir/all] Error 2
```
"
18930,ERROR: in target '//external:cc_toolchain': no such package ',"ERROR: in target '//external:cc_toolchain': no such package '@local_config_cc//': Traceback (most recent call last):
        File ""C:/users/wondercupid/appdata/local/temp/_bazel_wondercupid/dlqxfaaz/external/bazel_tools/tools/cpp/cc_configure.bzl"", line 37
                configure_windows_toolchain(repository_ctx)
        File ""C:/users/wondercupid/appdata/local/temp/_bazel_wondercupid/dlqxfaaz/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 302, in configure_windows_toolchain
                tpl(repository_ctx, ""CROSSTOOL"", {""%{cpu...}"": """"})
        File ""C:/users/wondercupid/appdata/local/temp/_bazel_wondercupid/dlqxfaaz/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 314, in tpl
                _get_escaped_windows_msys_crosstool_content(repository_ctx)
        File ""C:/users/wondercupid/appdata/local/temp/_bazel_wondercupid/dlqxfaaz/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 42, in _get_escaped_windows_msys_crosstool_content
                auto_configure_fail((""Could not determine MSYS/Cygwi...))
        File ""C:/users/wondercupid/appdata/local/temp/_bazel_wondercupid/dlqxfaaz/external/bazel_tools/tools/cpp/lib_cc_configure.bzl"", line 84, in auto_configure_fail
                fail((""\n%sAuto-Configuration Error:%...)))

Auto-Configuration Error: Could not determine MSYS/Cygwin root from BAZEL_SH (e:/msys32/usr/bin)
INFO: Elapsed time: 7.287s
FAILED: Build did NOT complete successfully (2 packages loaded)"
18929,TensorFlow Tutorials should have a Jupyter notebook friendly option,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
Nope
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 (latest 04/27/2018)
- **TensorFlow installed from (source or binary)**:
python -m pip install tensorflow
- **TensorFlow version (use command below)**:
1.7.0
- **Python version**: 
3.6.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0/1.7
- **GPU model and memory**: GeoForce GTX 1050Ti (4GB)
- **Exact command to reproduce**:
I opened jupyter notebook in my main python installation: 
C:\Users\ (me) \AppData\Local\Programs\Python\Python36\Scripts\jupyter-notebook.exe
I downloaded the master models folder, put it in my jupyter directory and went to
http://localhost:8888/tree/models-master/official/wide_deep
Then I created a new notebook (in the wide_deep directory) and tried to do the very first shell command

! data_download.py


### Describe the problem
The TensorFlow tutorial code (https://www.tensorflow.org/tutorials/wide) can't be run from Jupyter notebook. TensorFlow is a tool mainly for data scientists and we use Jupyter notebook a lot! The official tutorial/sample code should have a Jupyter friendly option.

### Source code / logs
Traceback (most recent call last):
  File ""data_download.py"", line 71, in <module>
    tf.app.run(argv=[sys.argv[0]] + unparsed)
  File ""C:\Users\shado\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\platform\app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""data_download.py"", line 60, in main
    tf.gfile.MkDir(FLAGS.data_dir)
  File ""C:\Users\shado\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 358, in create_dir
    pywrap_tensorflow.CreateDir(compat.as_bytes(dirname), status)
  File ""C:\Users\shado\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a directory: /tmp/census_data; No such file or directory
"
18925,About newlly added eager_operation.h,"In the latest Eager core update, I see that eager_operation is moved to [tensorflow/core/common_runtime/eager](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/common_runtime/eager). However this file require c_api as one of the dependency.

The c_api is suppose to be built after cpu_core_lib is generated, I am now a bit confuse as I am trying to rewrite the CMake files in order to fit the new eager structure.

Any hel?
"
18924,build Error  Tabulations are not allowed for identation. Use spaces instead.,"ERROR: E:/demoall/demofour/tf/tensorflow/WORKSPACE:60:5: Tabulations are not allowed for identation. Use spaces instead.
ERROR: E:/demoall/demofour/tf/tensorflow/WORKSPACE:61:2: Tabulations are not allowed for identation. Use spaces instead.
ERROR: E:/demoall/demofour/tf/tensorflow/WORKSPACE:56:2: indentation error
ERROR: E:/demoall/demofour/tf/tensorflow/WORKSPACE:61:6: syntax error at 'outdent': expected expression
ERROR: error loading package 'external': Failed to parse WORKSPACE file
ERROR: error loading package 'external': Failed to parse WORKSPACE file
INFO: Elapsed time: 1.773s
FAILED: Build did NOT complete successfully (0 packages loaded)"
18923,Missing Numerous models in TF-Slim from Tensorflow/models,"Hello dear friends,

If you have a look to: [tensorflow/tensorflow =>/tensorflow/contrib/slim/python/slim/nets](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim/python/slim/nets), you may notice that numerous models are missing:

* cifarnet.py
* cyclegan.py
* dcgan.py
* inception_resnet_v2.py
* lenet.py
* mobilenet_v1.py
* pix2pix.py
* nasnet/nasnet.py
* nasnet/pnasnet.py
* mobilenet/mobilenet.py  # Might be redundant with mobilenet_v1.py
* mobilenet/mobilenet_v2.py

All of them are available here : [tensorflow/models =>/research/slim/nets](https://github.com/tensorflow/models/tree/master/research/slim/nets)

Why some of them are given with TF and not the others ?

Have a nice day,

Jonathan"
18922,Why the leaky_relu is better？,"
def lrelu(x, leak=0.2, name=""lrelu""):
     with tf.variable_scope(name):
         f1 = 0.5 * (1 + leak)
         f2 = 0.5 * (1 - leak)
         return f1 * x + f2 * abs(x)

I don't know why is better than tf.maximum(0.2*x,x)
Could someone tell me why?
I really want to know."
18921,Resume checkpoint problem. Question in stackoverflow no answer,"I want to restore model parameters in the checkpoint file.
But I found
`NotFoundError (see above for traceback): Key wavenet/dilated_stack/layer10/gc_filter not found in checkpoint
`

I tried to read the model parameters, but found that I could not read it. The error was strange. I couldn't find the same error on google. I didn't seem to be able to do anything about it.

```
UnimplementedError Traceback (most recent call last)
<ipython-input-21-bef7e856f26a> in <module>()
     10 for key in (var_to_shape_map):
     11 print(""tensor_name: "", key)
---> 12 print(reader.get_tensor(key))

~/PSnet/env/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py in get_tensor(self, tensor_str)
    144 from tensorflow.python.util import compat
    145 return CheckpointReader_GetTensor(self, compat.as_bytes(tensor_str),
--> 146 status)
    147
    148 CheckpointReader_swigregister = _pywrap_tensorflow.CheckpointReader_swigregister

/usr/lib/python3.5/contextlib.py in __exit__(self, type, value, traceback)
     64 if type is None:
     65 try:
---> 66 next(self.gen)
     67 except StopIteration:
     68 return

~/PSnet/env/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py in raise_exception_on_not_ok_status()
    464 None, None,
    465 compat.as_text(pywrap_tensorflow.TF_Message(status)),
--> 466 pywrap_tensorflow.TF_GetCode(status))
    467 finally:
    468 pywrap_tensorflow.TF_DeleteStatus(status)

UnimplementedError: Unsupported tf type half
```"
18920,Feature request: Gradient for number of loop iterations for automatic differentiation of physics simulations,"## System information
**Not relevant.**

Have I written custom code **yes**
OS Platform and Distribution **different linux systems**
TensorFlow installed from **source and pip** (on different systems)
TensorFlow version **different versions, all >= 1.5.0**
Bazel version **different versions on different systems**
CUDA/cuDNN version **different versions, all >= 8**
GPU model and memory **GTX 1080, GTX 1060, Tesla P40** (potentially many others on our cluster, but not tested yet)
Exact command to reproduce **run [bug_test.py](https://github.com/AlexHarn/tf-ice-model-optimization/blob/1a9f9e11d55277bd595266410bf10a54bbd8b5f0/bug_test.py)**

## Describe the problem
### Motivation
(you might want to skip this)
Tensorflow is a powerful framework that can potentially be used for much more than static computations like those needed to train neural networks. We (a couple of physicists) are trying to use Tensorflow to get a gradient on some physical parameters to fit those parameters to measured data by propagating the gradient through the entire physics simulation to replicate the data. This is extremely relevant for a lot of people and the idea is not new.  Instead of performing extremely time consuming (in terms of computational and human work time) grid searches on high dimensional parameter spaces we would like to be able to directly perform gradient descent on such spaces in an automated fashion by using Tensorflows automatic differentiation. You can find our work in progress repository [here](https://github.com/AlexHarn/tf-ice-model-optimization).

### The Problem
Like explicitly stated [here](http://download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf) on page 15 ""This means that we assume that `pred` is not trainable"", the number of loop iterations is just a constant while backpropagating the gradient. This leads to some extremely unexpected behavior like described in this [issue](https://github.com/AlexHarn/tf-ice-model-optimization/issues/1) (at least unexpected for someone who is a Tensorflow newbie like I am). For our plans to work it is mandatory for the gradient to include information on how the number of loop iterations would have changed, if the trainable variables changed. 

### Example
(You might want to skip this)
In our project we have a trainable variable, which describes the mean distance between scattering events of photons. Tensorflow is able to propagate the gradient through the simulation (while loop), but the gradient only ""thinks"" that the photons will reach further points when the scattering length is longer, but ""is not aware"" of the fact that there will be less iterations. 

### Request
I do not know if this is even possible with how Tensorflow works, but we would love to see a way to make this work. Tensorflow would be a really great tool for us to use. So is this something you might work on in the future? 

Also: does anyone maybe have any idea of how to solve or get around this right now with the current implementation? We have put quite some thought into this ourselves already, but until now we have not found a working solution.

## Source code
For a work in progress example of how this would be useful you can take a look at the [master](https://github.com/AlexHarn/tf-ice-model-optimization) and [experimental](https://github.com/AlexHarn/tf-ice-model-optimization/tree/experimental) branches of our repo. 

For a stripped down minimal one dimensional example of the simulation you can take a look at [minimal_1d.py](https://github.com/AlexHarn/tf-ice-model-optimization/blob/gradient-sign-bug/minimal_1d.py), this in its current state however actually converges. For some more information on the unexpected behavior take a look at my [issue](https://github.com/AlexHarn/tf-ice-model-optimization/issues/1) where I reference commits, which demonstrate what's happening.

A minimal example, which does not include any physics and purely demonstrates the unexpected behavior is given [here](https://github.com/AlexHarn/tf-ice-model-optimization/blob/1a9f9e11d55277bd595266410bf10a54bbd8b5f0/bug_test.py).

All of the code includes detailed comments. 

If you are interested in the background and general idea you can take a look at my [slides](https://drive.google.com/file/d/17WdIcpIzFDZmQvD-DCqonmDvz_S0-Arz/view?usp=sharing), even though I am not sure how much of a help those will be for someone from outside. Slides 5 to 8 might be helpful for everyone though. Especially slide 5 which shows how we fit the data using simulations. For the general concept testing right now the data is also given by the same simulation with ""true"" parameters.

We really hope to get some form of feedback here and I am sorry if this is a trivial or stupid request in the eyes of someone more experienced with Tensorflow."
18919,"Tf lite: Array activation1, which is an input to the Div operator producing the output array dropout/div, is lacking min/max data","### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS X High Sierra
- **TensorFlow version (use command below)**: 1.8.0rc1
- **Python version**: 3.5

I'm trying to save a very simple NN model to tflite format, with weight quantization, following this documentation: https://www.tensorflow.org/performance/quantization.

However, when converting with toco, I get this error:

Array Relu, which is an input to the Div operator producing the output array dropout/div, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\n""

This is the graph:

![image](https://user-images.githubusercontent.com/8360740/39358075-28e18702-4a15-11e8-82de-8f2d705b1f78.png)

The code to reproduce:

```
    inputs = tf.placeholder(tf.float32, shape=(1, train_X.shape[1]), name='inputs')
    label = tf.placeholder(tf.float32, shape=(1, num_classes), name='labels')

    # First layer
    hid1_size = 128
    w1 = tf.Variable(tf.random_normal([hid1_size, train_X.shape[1]], stddev=0.01), name='w1')
    b1 = tf.Variable(tf.constant(0.1, shape=(hid1_size, 1)), name='b1')
    y1 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(w1, tf.transpose(inputs)), b1, name=""layer1""), ""activation1""),
                       keep_prob=0.5)

    # Second layer
    hid2_size = 256
    w2 = tf.Variable(tf.random_normal([hid2_size, hid1_size], stddev=0.01), name='w2')
    b2 = tf.Variable(tf.constant(0.1, shape=(hid2_size, 1)), name='b2')
    y2 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(w2, y1), b2, name=""layer2""), name=""activation2""), keep_prob=0.5)

    # Output layer
    wo = tf.Variable(tf.random_normal([num_classes, hid2_size], stddev=0.01), name='wo')
    bo = tf.Variable(tf.random_normal([num_classes, 1]), name='bo')
    yo = tf.transpose(tf.add(tf.matmul(wo, y2), bo), name=""logits"")

    # Loss function and optimizer
    lr = tf.placeholder(tf.float32, shape=(), name='learning_rate')
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=yo, labels=label), name=""loss"")

    # Call the training rewrite which rewrites the graph in-place with
    # FakeQuantization nodes and folds batchnorm for training. It is
    # often needed to fine tune a floating point model for quantization
    # with this training tool. When training from scratch, quant_delay
    # can be used to activate quantization after training to converge
    # with the float graph, effectively fine-tuning the model.
    #tf.contrib.quantize.create_training_graph(quant_delay=3)

    optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)

    # Prediction
    pred = tf.nn.softmax(yo, name=""prediction"")
    pred_label = tf.argmax(pred, 1)
    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(label, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))

    # Create operation which will initialize all variables
    init = tf.global_variables_initializer()

    # Configure GPU not to use all memory
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True

    # Start a new tensorflow session and initialize variables
    sess = tf.InteractiveSession(config=config)

    writer = tf.summary.FileWriter(""tensorboard"", sess.graph)

    sess.run(init)

    # This is the main training loop: we train for 50 epochs with a learning rate of 0.05 and another
    # 50 epochs with a smaller learning rate of 0.01
    for learning_rate in [0.05, 0.01]:
        for epoch in range(50):
            avg_cost = 0.0

            # For each epoch, we go through all the samples we have.
            for i in range(train_X.shape[0]):
                # Finally, this is where the magic happens: run our optimizer, feed the current example into X and the current target into Y
                _, c = sess.run([optimizer, loss], feed_dict={lr: learning_rate,
                                                              inputs: train_X[i, None],
                                                              label: train_y[i, None]})
                avg_cost += c
            avg_cost /= train_X.shape[0]

            # Print the cost in this epcho to the console.
            if epoch % 10 == 0:
                print(""Epoch: {:3d}    Train Cost: {:.4f}"".format(epoch, avg_cost))

    # Call the eval rewrite which rewrites the graph in-place with
    # FakeQuantization nodes and fold batchnorm for eval.
    #tf.contrib.quantize.create_eval_graph()

    writer.close()

    graph_path = os.path.join(""models"", ""model.pbtxt"")
    checkpoint_path = os.path.join(""models"", ""model.ckpt"")

    # Save the checkpoint and eval graph proto to disk for freezing and providing to TFLite.
    with open(graph_path, 'w') as f:
        f.write(str(sess.graph.as_graph_def()))

    saver = tf.train.Saver()
    saver.save(sess, checkpoint_path)

    frozen_graphdef = tf.graph_util.convert_variables_to_constants(
        sess, sess.graph_def, [""prediction""])
    open(""models/frozen_model.pb"", ""w"").write(str(frozen_graphdef))

    tflite_model = tf.contrib.lite.toco_convert(
        frozen_graphdef, [inputs], [pred], inference_type=tf.contrib.lite.QUANTIZED_UINT8,
        quantized_input_stats=[(127.5, 127.5)])
    open(""models/converted_model.tflite"", ""wb"").write(tflite_model)
```

Not sure if this is the problem, but could it be that the quantization scripts (quantize.create_training_graph quantize.create_eval_graph) are not detecting the first layer, not fake quantizing it and for this reason I get an error at activation1 when converting?"
18918,Unable to save a checkpoint,"Hello

I found a interesting project on github which recognize face in realtime:
https://github.com/vudung45/FaceRec

In that project there are two part, the face detection and the face recognition. The two graph are stored in a global graph and I want to export that global graph.

I add that code in mtcnn_detect.py:

```
        def __init__(self, face_rec_graph, model_path = ""models"", threshold = [0.6, 0.7, 0.7], factor = 0.709, scale_factor = 1):
        '''
        :param face_rec_sess: FaceRecSession
        :param threshold: detection threshold
        :param factor: default 0.709 image pyramid -- magic number
        :param model_path:
        '''
        self.threshold = threshold
        self.factor = factor
        self.scale_factor = scale_factor;
        with face_rec_graph.graph.as_default():
            print(""Loading MTCNN Face detection model"")
            self.sess = tf.Session()
            if not model_path:
                model_path, _ = os.path.split(os.path.realpath(__file__))

            with tf.variable_scope('pnet'):
                data = tf.placeholder(tf.float32, (None, None, None, 3), 'input')
                pnet = PNet({'data': data})
                pnet.load(os.path.join(model_path, 'det1.npy'), self.sess)
            with tf.variable_scope('rnet'):
                data = tf.placeholder(tf.float32, (None, 24, 24, 3), 'input')
                rnet = RNet({'data': data})
                rnet.load(os.path.join(model_path, 'det2.npy'), self.sess)
            with tf.variable_scope('onet'):
                data = tf.placeholder(tf.float32, (None, 48, 48, 3), 'input')
                onet = ONet({'data': data})
                onet.load(os.path.join(model_path, 'det3.npy'), self.sess)

            self.pnet = lambda img: self.sess.run(('pnet/conv4-2/BiasAdd:0', 'pnet/prob1:0'), feed_dict={'pnet/input:0': img})
            self.rnet = lambda img: self.sess.run(('rnet/conv5-2/conv5-2:0', 'rnet/prob1:0'), feed_dict={'rnet/input:0': img})
            self.onet = lambda img: self.sess.run(('onet/conv6-2/conv6-2:0', 'onet/conv6-3/conv6-3:0', 'onet/prob1:0'),
                                            feed_dict={'onet/input:0': img})
            print(""MTCNN Model loaded"")


            ###############what I added #################
            writer = tf.summary.FileWriter(""/tmp/model3/"", face_rec_graph.graph)
            saver = tf.train.Saver() #saver load pretrain model
            save_path = saver.save(self.sess, ""/tmp/model3/model.ckpt"")
            print(""Model saved in path: %s"" % save_path)
```



When I launch the program I get that error:

   ```
 MTCNN Model loaded
  


    Traceback (most recent call last):
          File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1323, in _do_call
            return fn(*args)
          File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1302, in _run_fn
            status, run_metadata)
          File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
            c_api.TF_GetCode(self.status.status))
        tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value InceptionResnetV1/Block8/Branch_0/Conv2d_1x1/BatchNorm/beta
        	 [[Node: save_1/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/SaveV2/tensor_names, save_1/SaveV2/shape_and_slices, InceptionResnetV1/Block8/Branch_0/Conv2d_1x1/BatchNorm/beta, InceptionResnetV1/Block8/Branch_0/Conv2d_1x1/BatchNorm/moving_mean, InceptionResnetV1/Block8/Branch_0/Conv2d_1x1/BatchNorm/moving_variance, InceptionResnetV1/Block8/Branch_0/Conv2d_1x1/weights, InceptionResnetV1/Block8/Branch_1/Conv2d_0a_1x1/BatchNorm/beta, InceptionResnetV1/Block8/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean, InceptionResnetV1/Block8/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance, InceptionResnetV1/Block8/Branch_1/Conv2d_0a_1x1/weights, InceptionResnetV1/Block8/Branch_1/Conv2d_0b_1x3/BatchNorm/beta, InceptionResnetV1/Block8/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_mean, InceptionResnetV1/Block8/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_variance,

     
    ....
    
    
    Caused by op 'save_1/SaveV2', defined at:
      File ""./main.py"", line 156, in <module>
        face_detect = MTCNNDetect(FRGraph, scale_factor=2); #scale_factor, rescales image for faster detection
      File ""/home/xavier/Téléchargements/FaceRec-master/mtcnn_detect.py"", line 51, in __init__
        saver = tf.train.Saver() #saver load pretrain model
      File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py"", line 1218, in __init__
        self.build()
      File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py"", line 1227, in build
        self._build(self._filename, build_save=True, build_restore=True)
      File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py"", line 1263, in _build
        build_save=build_save, build_restore=build_restore)
      File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py"", line 748, in _build_internal
        save_tensor = self._AddSaveOps(filename_tensor, saveables)
      File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py"", line 296, in _AddSaveOps
        save = self.save_op(filename_tensor, saveables)
      File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py"", line 239, in save_op
        tensors)
      File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1163, in save_v2
        shape_and_slices=shape_and_slices, tensors=tensors, name=name)
      File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
        op_def=op_def)
      File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
        op_def=op_def)
      File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
        self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access



```"
18917,tf.contrib.image.connected_components works incorrectly with 3D numpy arrays,"### System information
Have I written custom code: N/A
OS Platform and Distribution: Linux Ubuntu 16.04
Bazel version: N/A
TensorFlow installed from binary
TensorFlow version: v1.7.0-3-g024aecf414 1.7.0
Python version: 3.6
CUDA/cuDNN version: CUDA 9.0 / cuDNN 7.1.1
GPU model and memory: GeForce GTX 1060 6GB

### Exact command to reproduce:
`import numpy as np`
`from tensorflow.contrib.image import connected_components`
`a = np.zeros((2,3,4))`
`a[0,0,0]=1`
`a[0,0,1]=1`
`a[1,0,1]=1`
`print((tf.Session().run(connected_components(a))))`

> [[[1 1 0 0]
  [0 0 0 0]
  [0 0 0 0]]
 [[0 2 0 0]
  [0 0 0 0]
  [0 0 0 0]]]

It gives two components instead of one. "
18916,Does tensorflow have 2D short time Fourier transform? ,Tensorflow have tf.contrib.signal.stft which i think implements 1D stft. Can this be used fpr the 2D case?
18915,Tensorflow lite version 0.1.7 is too slow,"
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: android
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: no
- **Python version**: no
- **Bazel version (if compiling from source)**: no
- **GCC/Compiler version (if compiling from source)**: no
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**: no

I used the Tensor Flowlight library on my device. Until then, the operation speed was 300ms. However, it took more than a second after using the 0.1.7 version of the Tensor Flow Lite version released this week. If i use the current 0.0.0-nightly version, it takes 300ms again. The question is whether or not you have removed the option that the 0.1.7 version of the library uses neon.

Ask https://bintray.com/google/tensorflow/tensorflow-lite to post your question here and upload it here. Even if it does not fit the form, please understand and ask for confirmation.

"
18914,Dose Titan Xp work in Windows10?,"Hi i bought new GPU - Titan Xp for learning, so i set the system like below:
CPU-AMD Ryzen 51600 Six-Core Processor
GPU-Titan Xp
Tensorflow-1.5.0 GPU
CUDA-9.0
cuDNN-7.0

But i found that when the calculation begins, the data looks not stacked on GPU, so that it occurs error message.

does Titan XP work in Windows 10 to use tensorflow gpu?"
18913,Accessing CuDNN autotuner in built-in Keras,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
```bash
$ uname -mrs
Linux 4.14.0-49.el7a.ppc64le ppc64le
```
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.6.0
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA 9.1, CuDNN 7.0.5
- **GPU model and memory**: Tesla V100:
```
== nvidia-smi ===================================================
Thu Apr 26 18:35:34 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.19                 Driver Version: 396.19                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000004:04:00.0 Off |                    0 |
| N/A   29C    P0    52W / 300W |      0MiB / 15360MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+
```
- **Exact command to reproduce**: N/A

We would like to be able to have access to the CuDNN autotuner in `tf.keras` module to access optimal algorithms for a given hardware (or, perhaps passing a custom convolutional algorithm from config). In TensorFlow, I can specify to use the CuDNN autotuner by setting: 
```bash
os.environ['TF_CUDNN_USE_AUTOTUNE'] = ""1""
```
(currently enabled by default), which improves performance significantly on Volta GPUs and especially with FP16.

However, I am unable to access this performance improvement when running pure `tf.keras`, where setting this environmental variable does not have any effect. 

### Source code / logs

Following simple example could be used to reproduce the issue: https://gist.github.com/ASvyatkovskiy/8d1dd622e447d9d8de1ec4e238e0dbaa"
18911,Check failed: dtype() == expected_dtype (9 vs. 3),"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source (Today's version)
- **TensorFlow version (use command below)**: v1.8.0-rc1-1107-g8a428cd 1.8.0-rc1
- **Python version**: Python3
- **Bazel version (if compiling from source)**: 0.12
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9.1/7.1.3
- **GPU model and memory**: Geforce gtx Titan X
- **Exact command to reproduce**: 

```
 std::vector<tensorflow::Tensor> finalOutput;

  std::string InputName  = ""inp"";
  std::string OutputName = ""out"";
  tensorflow::Status run_status =
      session->Run({{InputName, input_tensor}}, {OutputName}, {}, &finalOutput);

  for (int y = 0; y < height; y++)
  {
    for (int x = 0; x < width; x++)
    {
     std::cout << finalOutput[0].tensor<int, 4>()(0, y, x, 0); // Error: Check failed: dtype() == expected_dtype (9 vs. 3)
    }
  }
```

### Describe the problem
I have trained network in python. I froze the model from python and writing inference model in c++. I am using above code to run the inference on frozen graph. It works as I got the tensor output but I am unable to read this tensor file by using above method.

**Based on types.proto, I am comparing DT_INT64(frozen model) with DT_INT32(c++ inference model).**

**Things I have tried:**
1. Specify tf.int32 in argmax layer. (Last layer in frozen model) **-> It works** (But I don't want to modify the network architecture)
2. Instead of  finalOutput[0].tensor<int, 4>, I have tried  finalOutput[0].tensor<long int, 4>,  finalOutput[0].tensor<int64_t, 4> but compilation issue as eigen might not be supporting this. (Just a guess)

### Source code / logs

Error: Check failed: dtype() == expected_dtype (9 vs. 3)
"
18908,[tf.keras] tf.keras.utils.plot_model does not work with Sequential API,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 & OSX
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7.0 & 1.8.0.rc1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a 
- **Exact command to reproduce**: See code/logs.


### Describe the problem
tf.keras.utils.plot_model does not work with a model specified in the Sequential API.

### Source code / logs
```
import tensorflow as tf

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(10, input_dim=50))

tf.keras.utils.plot_model(model, ""sequential.png"")
```

Error tensorflow 1.7.0:
```
Traceback (most recent call last):
  File ""plot_tf.py"", line 6, in <module>
    tf.keras.utils.plot_model(model, ""sequential.png"")
  File ""/Users/removed/.local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/utils/vis_utils.py"", line 149, in plot_model
    dot = model_to_dot(model, show_shapes, show_layer_names, rankdir)
  File ""/Users/removed/.local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/utils/vis_utils.py"", line 123, in model_to_dot
    if node_key in model._container_nodes:
AttributeError: 'Model' object has no attribute '_container_nodes'
```

Error tensorflow 1.8.0.rc1
```
Traceback (most recent call last):
  File ""plot_tf.py"", line 6, in <module>
    tf.keras.utils.plot_model(model, ""sequential.png"")
  File ""/Users/removed/.local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/utils/vis_utils.py"", line 149, in plot_model
    dot = model_to_dot(model, show_shapes, show_layer_names, rankdir)
  File ""/Users/removed/.local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/utils/vis_utils.py"", line 80, in model_to_dot
    model = model.model
AttributeError: 'Sequential' object has no attribute 'model'
```

"
18906,Planning Ticket  CUDA 9.2 + cuDNN 7.1,"Updated 02-JUNE-2018

I have done some testing with CUDA 9.2/cuDNN 7.1.4/NCCL 2.x with tf_cnn_benchmarks.

Below you will see that if you upgrade to cuDNN 7.1.4 and a newer device driver you get some pretty nice gains and no need to compile from source as well as using CUDA 9.0.  Compiling from source and using NCCL 2.x (Hierarchal Copy was almost the same) I was able to get 6,800+ 8xV100s ResNet50v1 FP16 with synthetic data and about 6,500+ real data sustained over a full run for under 5 hours.  

I suspect many of you saw NVIDIA announce 1K and 1.3K for 1xV100 ResNet50.  That was with **unreleased** libraries and we are working to ensure we can hit those numbers when the libraries are available or with our own tricks.

I apologize for any short hand I use below in describing the runs.  I am happy to answer questions or make clarifications.  This testing was slightly informal but recent full scale testing yielded similar results.  

**CUDA 9.0**
[Recommended driver] 6,197.70 CUDA 9.0 + 384.13 (v1.8.0-1386-g2dc7575) hierarchical copy
6,620 CUDA 9.0 + 390.59 (v1.8.0-1386-g2dc7575) NCCL
6,613.11 CUDA 9.0 + 396.26 (v1.8.0-2215-gf528eba) NCCL
6,564.9 CUDA 9.0 + 396.26 (v1.8.0-2215-gf528eba) hierarchical copy
6,541.25 CUDA 9.0 + 390.59 (1.9.0.dev20180523) hierarchical copy
6,276.02 CUDA 9.0 + 384.13 (1.9.0.dev20180523) hierarchical copy
6,197.70 CUDA 9.0 + 384.13 (v1.8.0-1386-g2dc7575) hierarchical copy

**CUDA 9.1**
[Recommended driver]  6,227.64 CUDA 9.1 + 390.59 (v1.8.0-2215-gf528eba) hierarchical copy
6,210.28 CUDA 9.1 + 396.26 (v1.8.0-2215-gf528eba) hierarchical copy
6,118.21 CUDA 9.1 + 396.26 (v1.8.0-2215-gf528eba) NCCL

**CUDA 9.2**
[Recommended driver]  6,696.39 CUDA 9.2 + 396.26 (v1.8.0-2215-gf528eba) NCCL
6,606.57 CUDA 9.2 + 396.26 (v1.8.0-2215-gf528eba) hierarchical copy
6,738.32 CUDA 9.2 + 396.26 (v1.8.0-2215-gf528eba) NCCL SGD

**Full test runs with CUDA 9.2**
top_1 ranges between 75.7% and 76% and does not seem to be based on the hyper parameters.  I was focused on testing NCCL vs. hierarchical copy.  I have a minor concern about my validation command, but this is still good info.
Hierarchical copy:  6441.64  Accuracy @ 1 = 0.7584 Accuracy @ 5 = 0.9267 [49920 examples] 
NCCL (repacking:2):  6490.62  Accuracy @ 1 = 0.7572 Accuracy @ 5 = 0.9265 [49920 examples]
NCCL (repacking:8):  6490.62  Accuracy @ 1 = 0.7582 Accuracy @ 5 = 0.9268 [49920 examples]
My first hierarchical copy run was 76% exactly. 

**Note:**  I would like to move to CUDA 9.2 as the default but the driver is not widely available for easy apt-get install.  I am working long-term to get the build team to support a secondary CUDA build.  I am also very open to feedback as I do not see any easy path forward to moving to newer CUDA versions faster.  You can always (usually) install a new cuDNN.  We are compiling with 7.0 but you see gains by installing 7.1.4 as you see above if you upgrade your driver."
18905,"the error message of ""OP_REQUIRES failed at mkl_concat_op.cc:784 : Aborted: Operation received an exception:Status: 3, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:781            INFO:tensorflow:Error reported to Coordinator: Operation received an exception:Status: 3, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:781    ""","When running the deeplab model (train.py) included in tensorflow, it keeps generating the log message such as following. I don't know what does it mean.
```

2018-04-26 13:37:54.472896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0                                                        
2018-04-26 13:37:54.472907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N                                                        
INFO:tensorflow:Restoring parameters from /user/DL-Phase3/DeepLab/ADE20K/train_on_train_set/train/model.ckpt-0                  
INFO:tensorflow:Running local_init_op.                                                                                                            
INFO:tensorflow:Done running local_init_op.                                                                                                       
INFO:tensorflow:Starting Session.                                                                                                                 
INFO:tensorflow:Saving checkpoint to path /user/DL-Phase3/DeepLab/ADE20K/train_on_train_set/train/model.ckpt                    
INFO:tensorflow:Starting Queues.                                                                                                                  
2018-04-26 13:38:17.513798: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at mkl_concat_op.cc:784 : Aborted: Operation received an exception:Status: 3, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:781           
INFO:tensorflow:Error reported to Coordinator: Operation received an exception:Status: 3, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:781                                                                                             
         [[Node: concat = _MklConcatV2[N=5, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ResizeBilinear_1, aspp0/Relu, aspp1_pointwise/Relu, aspp2_pointwise/Relu, aspp3_pointwise/Relu, concat/axis, DMT/_313, aspp0/Relu:1, aspp1_pointwise/Relu:1, aspp2_pointwise/Relu:1, aspp3_pointwise/Relu:1, DMT/_314)]]                                                                              

Caused by op 'concat', defined at:
  File ""train.py"", line 386, in <module>
    tf.app.run()                        
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))                                                                                                        
  File ""train.py"", line 278, in main                                                                                             
    clones = model_deploy.create_clones(config, model_fn, args=model_args)                                                       
  File ""/tmp/test/models/research/slim/deployment/model_deploy.py"", line 193, in create_clones                                   
    outputs = model_fn(*args, **kwargs)                                                                                          
  File ""train.py"", line 207, in _build_deeplab                                                                                   
    fine_tune_batch_norm=FLAGS.fine_tune_batch_norm)                                                                             
  File ""/tmp/test/models/research/deeplab/model.py"", line 296, in multi_scale_logits                                             
    fine_tune_batch_norm=fine_tune_batch_norm)                                                                                   
  File ""/tmp/test/models/research/deeplab/model.py"", line 461, in _get_logits                                                    
    fine_tune_batch_norm=fine_tune_batch_norm)                                                                                   
  File ""/tmp/test/models/research/deeplab/model.py"", line 424, in _extract_features                                              
    concat_logits = tf.concat(branch_logits, 3)                                                                                  
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1181, in concat
    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)                                                               
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 949, in concat_v2
    ""ConcatV2"", values=values, axis=axis, name=name)                                                                                        
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper                                                                                                                                          
    op_def=op_def)                                                                                                                                
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3290, in create_op         
    op_def=op_def)                                                                                                                                
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1654, in __init__          
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access                                                            

AbortedError (see above for traceback): Operation received an exception:Status: 3, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:781                                                                                                    
         [[Node: concat = _MklConcatV2[N=5, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ResizeBilinear_1, aspp0/Relu, aspp1_pointwise/Relu, aspp2_pointwise/Relu, aspp3_pointwise/Relu, concat/axis, DMT/_313, aspp0/Relu:1, aspp1_pointwise/Relu:1, aspp2_pointwise/Relu:1, aspp3_pointwise/Relu:1, DMT/_314)]]                                                                              
Traceback (most recent call last):                                                                                                                
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1327, in _do_call         
    return fn(*args)                                                                                                                              
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1312, in _run_fn          
    options, feed_dict, fetch_list, target_list, run_metadata)                                                                                    
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1420, in _call_tf_sessionrun                                                                                                                                                
    status, run_metadata)                                                                                                                         
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__   
    c_api.TF_GetCode(self.status.status))                                                                                                         
tensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:781                                                                                      
         [[Node: concat = _MklConcatV2[N=5, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ResizeBilinear_1, aspp0/Relu, aspp1_pointwise/Relu, aspp2_pointwise/Relu, aspp3_pointwise/Relu, concat/axis, DMT/_313, aspp0/Relu:1, aspp1_pointwise/Relu:1, aspp2_pointwise/Relu:1, aspp3_pointwise/Relu:1, DMT/_314)]]                                                                              

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception                                                                                                                                             
    yield                                                                                                                                         
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 495, in run         
    self.run_loop()                                                                                                                               
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 1030, in run_loop    
    self._sv.global_step])                                                                                                                        
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 905, in run               
    run_metadata_ptr)                                                                                                                             
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1140, in _run             
    feed_dict_tensor, options, run_metadata)                                                                                                      
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run          
    run_metadata)                                                                                                                                 
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call         
    raise type(e)(node_def, op, message)                                                                                                          
tensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:781                                                                                      
         [[Node: concat = _MklConcatV2[N=5, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ResizeBilinear_1, aspp0/Relu, aspp1_pointwise/Relu, aspp2_pointwise/Relu, aspp3_pointwise/Relu, concat/axis, DMT/_313, aspp0/Relu:1, aspp1_pointwise/Relu:1, aspp2_pointwise/Relu:1, aspp3_pointwise/Relu:1, DMT/_314)]]                                                                              

Caused by op 'concat', defined at:
  File ""train.py"", line 386, in <module>
    tf.app.run()                        
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))                                                                                                        
  File ""train.py"", line 278, in main                                                                                             
    clones = model_deploy.create_clones(config, model_fn, args=model_args)                                                       
  File ""/tmp/test/models/research/slim/deployment/model_deploy.py"", line 193, in create_clones                                   
    outputs = model_fn(*args, **kwargs)                                                                                          
  File ""train.py"", line 207, in _build_deeplab                                                                                   
    fine_tune_batch_norm=FLAGS.fine_tune_batch_norm)                                                                             
  File ""/tmp/test/models/research/deeplab/model.py"", line 296, in multi_scale_logits                                             
    fine_tune_batch_norm=fine_tune_batch_norm)                                                                                   
  File ""/tmp/test/models/research/deeplab/model.py"", line 461, in _get_logits                                                    
    fine_tune_batch_norm=fine_tune_batch_norm)                                                                                   
  File ""/tmp/test/models/research/deeplab/model.py"", line 424, in _extract_features                                              
    concat_logits = tf.concat(branch_logits, 3)                                                                                  
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1181, in concat
    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)                                                               
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 949, in concat_v2
    ""ConcatV2"", values=values, axis=axis, name=name)                                                                                        
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper                                                                                                                                          
    op_def=op_def)                                                                                                                                
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3290, in create_op         
    op_def=op_def)                                                                                                                                
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1654, in __init__          
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access                                                            

AbortedError (see above for traceback): Operation received an exception:Status: 3, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:781                                                                                                    
         [[Node: concat = _MklConcatV2[N=5, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ResizeBilinear_1, aspp0/Relu, aspp1_pointwise/Relu, aspp2_pointwise/Relu, aspp3_pointwise/Relu, concat/axis, DMT/_313, aspp0/Relu:1, aspp1_pointwise/Relu:1, aspp2_pointwise/Relu:1, aspp3_pointwise/Relu:1, DMT/_314)]]                                                                              

2018-04-26 13:38:18.049156: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at mkl_concat_op.cc:784 : Aborted: Operation received an exception:Status: 3, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:781           
INFO:tensorflow:Retrying training!                                                                                                                
2018-04-26 13:38:18.592880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1394] Ignoring visible gpu device (device: 0, name: Quadro 5000, pci bus id: 0000:05:00.0, compute capability: 2.0) with Cuda compute capability 2.0. The minimum required Cuda capability is 3.0.                   
2018-04-26 13:38:18.592940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:                                                                                                                                                 
2018-04-26 13:38:18.592950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0                                                        
2018-04-26 13:38:18.592958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N                                                        
INFO:tensorflow:Restoring parameters from /user/DL-Phase3/DeepLab/ADE20K/train_on_train_set/train/model.ckpt-0                  
INFO:tensorflow:Running local_init_op.                                                                                                            
INFO:tensorflow:Done running local_init_op.                                                                                                       
INFO:tensorflow:Starting Session.                                                                                                                 
INFO:tensorflow:Saving checkpoint to path /user/DL-Phase3/DeepLab/ADE20K/train_on_train_set/train/model.ckpt                    
INFO:tensorflow:Starting Queues.                                                                                                                  
2018-04-26 13:38:42.322907: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at mkl_concat_op.cc:784 : Aborted: Operation received an exception:Status: 3, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:781           
INFO:tensorflow:Error reported to Coordinator: Operation received an exception:Status: 3, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:781                                                                                             
         [[Node: concat = _MklConcatV2[N=5, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ResizeBilinear_1, aspp0/Relu, aspp1_pointwise/Relu, aspp2_pointwise/Relu, aspp3_pointwise/Relu, concat/axis, DMT/_313, aspp0/Relu:1, aspp1_pointwise/Relu:1, aspp2_pointwise/Relu:1, aspp3_pointwise/Relu:1, DMT/_314)]]                                                                              

Caused by op 'concat', defined at:
  File ""train.py"", line 386, in <module>
    tf.app.run()                        
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))                                                                                                        
  File ""train.py"", line 278, in main                                                                                             
    clones = model_deploy.create_clones(config, model_fn, args=model_args)                                                       
  File ""/tmp/test/models/research/slim/deployment/model_deploy.py"", line 193, in create_clones                                   
    outputs = model_fn(*args, **kwargs)                                                                                          
  File ""train.py"", line 207, in _build_deeplab                                                                                   
    fine_tune_batch_norm=FLAGS.fine_tune_batch_norm)                                                                             
  File ""/tmp/test/models/research/deeplab/model.py"", line 296, in multi_scale_logits                                             
    fine_tune_batch_norm=fine_tune_batch_norm)                                                                                   
  File ""/tmp/test/models/research/deeplab/model.py"", line 461, in _get_logits                                                    
    fine_tune_batch_norm=fine_tune_batch_norm)                                                                                   
  File ""/tmp/test/models/research/deeplab/model.py"", line 424, in _extract_features                                              
    concat_logits = tf.concat(branch_logits, 3)                                                                                  
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1181, in concat
    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)                                                               
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 949, in concat_v2
    ""ConcatV2"", values=values, axis=axis, name=name)                                                                                        
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper                                                                                                                                          
    op_def=op_def)                                                                                                                                
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3290, in create_op         
    op_def=op_def)                                                                                                                                
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1654, in __init__          
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access                                                            

AbortedError (see above for traceback): Operation received an exception:Status: 3, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:781                                                                                                    
         [[Node: concat = _MklConcatV2[N=5, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ResizeBilinear_1, aspp0/Relu, aspp1_pointwise/Relu, aspp2_pointwise/Relu, aspp3_pointwise/Relu, concat/axis, DMT/_313, aspp0/Relu:1, aspp1_pointwise/Relu:1, aspp2_pointwise/Relu:1, aspp3_pointwise/Relu:1, DMT/_314)]]                                                                              
Traceback (most recent call last):                                                                                                                
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1327, in _do_call         
    return fn(*args)                                                                                                                              
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1312, in _run_fn          
    options, feed_dict, fetch_list, target_list, run_metadata)                                                                                    
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1420, in _call_tf_sessionrun                                                                                                                                                
    status, run_metadata)                                                                                                                         
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__   
    c_api.TF_GetCode(self.status.status))                                                                                                         
tensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:781                                                                                      
         [[Node: concat = _MklConcatV2[N=5, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ResizeBilinear_1, aspp0/Relu, aspp1_pointwise/Relu, aspp2_pointwise/Relu, aspp3_pointwise/Relu, concat/axis, DMT/_313, aspp0/Relu:1, aspp1_pointwise/Relu:1, aspp2_pointwise/Relu:1, aspp3_pointwise/Relu:1, DMT/_314)]]                                                                              

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception                                                                                                                                             
    yield                                                                                                                                         
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 495, in run         
    self.run_loop()                                                                                                                               
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 1030, in run_loop    
    self._sv.global_step])                                                                                                                        
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 905, in run               
    run_metadata_ptr)                                                                                                                             
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1140, in _run             
    feed_dict_tensor, options, run_metadata)                                                                                                      
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run          
    run_metadata)                                                                                                                                 
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call         
    raise type(e)(node_def, op, message)                                                                                                          
tensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:781                                                                                      
         [[Node: concat = _MklConcatV2[N=5, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ResizeBilinear_1, aspp0/Relu, aspp1_pointwise/Relu, aspp2_pointwise/Relu, aspp3_pointwise/Relu, concat/axis, DMT/_313, aspp0/Relu:1, aspp1_pointwise/Relu:1, aspp2_pointwise/Relu:1, aspp3_pointwise/Relu:1, DMT/_314)]]

Caused by op 'concat', defined at:
  File ""train.py"", line 386, in <module>
    tf.app.run()
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""train.py"", line 278, in main
    clones = model_deploy.create_clones(config, model_fn, args=model_args)
  File ""/tmp/test/models/research/slim/deployment/model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""train.py"", line 207, in _build_deeplab
    fine_tune_batch_norm=FLAGS.fine_tune_batch_norm)
  File ""/tmp/test/models/research/deeplab/model.py"", line 296, in multi_scale_logits
    fine_tune_batch_norm=fine_tune_batch_norm)
  File ""/tmp/test/models/research/deeplab/model.py"", line 461, in _get_logits
    fine_tune_batch_norm=fine_tune_batch_norm)
  File ""/tmp/test/models/research/deeplab/model.py"", line 424, in _extract_features
    concat_logits = tf.concat(branch_logits, 3)
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1181, in concat
    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 949, in concat_v2
    ""ConcatV2"", values=values, axis=axis, name=name)
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3290, in create_op
    op_def=op_def)
  File ""/user/virtualE/deeplab/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1654, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

AbortedError (see above for traceback): Operation received an exception:Status: 3, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:781
         [[Node: concat = _MklConcatV2[N=5, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ResizeBilinear_1, aspp0/Relu, aspp1_pointwise/Relu, aspp2_pointwise/Relu, aspp3_pointwise/Relu, concat/axis, DMT/_313, aspp0/Relu:1, aspp1_pointwise/Relu:1, aspp2_pointwise/Relu:1, aspp3_pointwise/Relu:1, DMT/_314)]]

2018-04-26 13:38:42.846252: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at mkl_concat_op.cc:784 : Aborted: Operation received an exception:Status: 3, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:781
INFO:tensorflow:Retrying training!
2018-04-26 13:38:43.479830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1394] Ignoring visible gpu device (device: 0, name: Quadro 5000, pci bus id: 0000:05:00.0, compute capability: 2.0) with Cuda compute capability 2.0. The minimum required Cuda capability is 3.0.
2018-04-26 13:38:43.479882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-04-26 13:38:43.479896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2018-04-26 13:38:43.479906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
INFO:tensorflow:Restoring parameters from /user/DL-Phase3/DeepLab/ADE20K/train_on_train_set/train/model.ckpt-0
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Starting Session.
INFO:tensorflow:Saving checkpoint to path /user/DL-Phase3/DeepLab/ADE20K/train_on_train_set/train/model.ckpt
INFO:tensorflow:Starting Queues.


```"
18904,"Relevant fluctuations in activations depending on batch size, at test time","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**: 
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: GeForce GTX 1080 8GB
- **Exact command to reproduce**:

### Describe the problem
I'm using the slim implementation of resnet-v1-50 . The logits seem to change depending on the test batch_size used.

I would expect some instability due to parallelism mechanisms on GPU, but I found out this behavior after the model's accuracy changed when batch_size changed. Thus, some fluctuations might be so big that they change predictions, which is not irrelevant.

If this is not a bug, is there a way to prevent this / what is the recommended way to evaluate our models?

### Source code / logs
I attach source code that tests this scenario. It was tested on tf version 1.3 .
to run: python test_batch_size.py --batch_size=1

It prints the logits for the first frame of the batch. It saves the frame so you can be sure it's the very same frame. It loads all the weights from a checkpoint, and batchnorm is disabled with is_training=False.

please vary the batch_size argument, and see that the activations change.
https://www.dropbox.com/s/ns9j84t02zifdaa/test_batch_size.zip?dl=0"
18903,Feature: tf.contrib.rnn Downsampling Wrapper ,"### Feature description
-  On sequential inputs with many timesteps (e.g. speech), it is common to down-sample the higher order representations in a multi-layer recurrent network to improve the computation time and eventually the attention weights learning [1]. The MultiRNNCell class in TensorFlow simplifies the construction of multi-layered RNNs but gives users no control to post-process the outputs of intermediate layers. Would it be possible to add a new cell wrapper implementing RNN layer output sub-sampling, or more general used-defined post-processing ? The simplest case could be down-sampling the outputs by a factor of 2, as in [1], section 3.1, but without concatenation.

Thanks,
George 

[1] https://arxiv.org/abs/1508.01211"
18902,Read the model from external storage instead of assets (runtime) [android],"Would be great to read the model from external storage instead of assets.
Assets is ok when your model doesn't change. But what if we need to load the model dynamically on runtime. 
The usage example is for assets:
`TensorFlowImageClassifier.create(-->AssetManager<--,...)`


"
18901,contrib/pi_examples's Makefile is missing lib nsync.a,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Raspbian Stretch
- **TensorFlow installed from (source or binary)**: source (not installed)
- **TensorFlow version (use command below)**: 1.8.0rc1
- **Python version**: -
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: 4.8
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: follow instructions here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/pi_examples

### Describe the problem
Attempting to build the sample with `make` will fail in the linking stage with an `undefined reference to `nsync::nsync_cv_signal(nsync::nsync_cv_s_*)'`.
Adding `-Ltensorflow/contrib/makefile/downloads/nsync/builds/default.linux.c++11` to `LDFLAGS` (taken from the env var TARGET_NSYNC_LIB) and `-l:nsync.a` to `LIBS` fixes the problem.
So far I hardcoded the paths, so I can't really make a PR (and I'm not experienced enough with shell scripts / makefiles to propose a decent fix).
"
18899,pi_examples/label_image.cc has wrong include order for libjpg.h,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Raspbian stretch
- **TensorFlow installed from (source or binary)**: source (not installed though)
- **TensorFlow version (use command below)**: 1.8.0rc1
- **Python version**: -
- **Bazel version (if compiling from source)**: Built using makefiles
- **GCC/Compiler version (if compiling from source)**: 4.8
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: Follow instructions in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/pi_examples

### Describe the problem
As mentioned in [this issue](https://github.com/libjpeg-turbo/libjpeg-turbo/issues/17), `#include <libjpg.h>` has to be after `#include <stdio.h>` or else it won't have definitions for `FILE`, `size_t`, etc.
In the current version of label_image.cc the order of the includes is wrong ad thus the build fails.
Swapping the two includes fixes the problem (I'll provide a PR as soon as I have time, if someone else doesn't beat me to it first)"
18898,[r1.7][TensorRT] INT8 mode calibration is not worked,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat 4.8.5-16, Linux version 3.10.0-693.5.2.el7.x86_64
- **TensorFlow installed from (source or binary)**: pip (python 2.7)
- **TensorFlow version (use command below)**: tensorflow-gpu==1.7.0
- **Python version**: python 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: gcc 5.3
- **CUDA/cuDNN version**:  CUDA9.0, cuDNN7.0.5
- **GPU model and memory**: Tesla P4, 8GB
- **Exact command to reproduce**: My own script is coded based on the official test ""tensorflow/tensorflow/contrib/tensorrt/test/test_tftrt.py"", so I think you can reproduce this problem with this test. 

### Describe the problem
I tried to use Tensorflow 1.7 to do the prediction under Python environment, which can integrate the TensorRT to optimize the GraphDef.

The optimization in FP32 mode is successfully done, but when I tried the INT8 mode, I'm confused about how to do the calibration. I checked the examples both from tensorflow source code and the NVidia dev guide but still not sure. 

Below is part of the example that contained within tensorflow.
```
def run_calibration(gdef, dumm_inp):
  """"""Run given calibration graph multiple times.""""""
  gpu_options = cpb2.GPUOptions(per_process_gpu_memory_fraction=0.50)
  ops.reset_default_graph()
  g = ops.Graph()
  with g.as_default():
    inp, out = importer.import_graph_def(
        graph_def=gdef, return_elements=[""input"", ""output""])
    inp = inp.outputs[0]
    out = out.outputs[0]
  with csess.Session(
      config=cpb2.ConfigProto(gpu_options=gpu_options), graph=g) as sess:
    # run over real calibration data here, we are mimicking a calibration set of
    # 30 different batches. Use as much calibration data as you want
    for _ in range(30):
      val = sess.run(out, {inp: dumm_inp})
  return val
############################
 int8_calib_gdef = trt.create_inference_graph(
     input_graph_def=orig_graph,
     outputs=[""output""],
     max_batch_size=inp_dims[0],
     max_workspace_size_bytes=1 << 25,
     precision_mode=""INT8"",  # TRT Engine precision ""FP32"",""FP16"" or ""INT8""
     minimum_segment_size=2  # minimum number of nodes in an engine
 )
 _ = run_calibration(int8_calib_gdef, dummy_input)
 int8_graph = trt.calib_graph_to_infer_graph(int8_calib_gdef)
 o5 = run_graph(int8_graph, dummy_input)
```
Above code is copied from tensorflow/tensorflow/contrib/tensorrt/test/test_tftrt.py
The function run_calibration seems just create a session and run 30 times with the same input, and the return of run_calibration seems not used at all. 

How could the calibration be done in this way? 

Thanks,"
18897,Tensorflow model training working slow," I have arond 1500 images  with 7 classes i know its less but i am rookie in machine learning and object detection. Due to technical limitation i am currently training the model  using tensorflow cpu version. the machine is taking 20 second/step. and it took 3 days to reach 11000 steps. i want to reach atleast 40000 steps.  My question is it always take that long time for training the model in tensorflow-cpu or is there some problem? please answer my question as soon as possible

I am using tensorflow-cpu version 1.5 the reason of using this version is that in tensorflow-1.7 i am getting the depracation warning and  was not able to train the model on my laptop

If i am doing something wrong please guide me 
thanking you in anticipation"
18895,'concave points_mean' is not a valid scope name,"I am running my models in **Google's Colaboratory Cloud Platform** and I never confronted this error message.

Python Version (Colaboratory Notebook): `Python3`
Hardware Accelerator: `GPU`

(before creating this issue I did my research but found no useful content)

My code has something like this,
```
periods = 10
steps_per_period = steps / periods
  
# Create linear_classifier object and configure it.
my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)
linear_classifier = tf.estimator.LinearClassifier(
    feature_columns = construct_feature_columns(training_examples),
    optimizer = my_optimizer
)
```
and inside a `for` loop, I am training my model like this,
```
for period in range(0, periods):
    linear_classifier.train(
        input_fn = training_input_fn,
        steps = steps_per_period
    )
```
The error below:
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-44-b9598a3db7f4> in <module>()
      6     training_targets = training_targets,
      7     validation_examples = validation_examples,
----> 8     validation_targets = validation_targets
      9 )

<ipython-input-43-0ca23946204e> in train_linear_classfication_model(learning_rate, steps, batch_size, training_examples, training_targets, validation_examples, validation_targets)
     43     linear_classifier.train(
     44         input_fn = training_input_fn,
---> 45         steps = steps_per_period
     46     )
     47 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    353 
    354     saving_listeners = _check_listeners_type(saving_listeners)
--> 355     loss = self._train_model(input_fn, hooks, saving_listeners)
    356     logging.info('Loss for final step: %s.', loss)
    357     return self

/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
    822       worker_hooks.extend(input_hooks)
    823       estimator_spec = self._call_model_fn(
--> 824           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
    825 
    826       if self._warm_start_settings:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)
    803 
    804     logging.info('Calling model_fn.')
--> 805     model_fn_results = self._model_fn(features=features, **kwargs)
    806     logging.info('Done calling model_fn.')
    807 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/canned/linear.py in _model_fn(features, labels, mode, config)
    316           optimizer=optimizer,
    317           partitioner=partitioner,
--> 318           config=config)
    319 
    320     super(LinearClassifier, self).__init__(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/canned/linear.py in _linear_model_fn(features, labels, mode, head, feature_columns, optimizer, partitioner, config)
    156     logit_fn = _linear_logit_fn_builder(
    157         units=head.logits_dimension, feature_columns=feature_columns)
--> 158     logits = logit_fn(features=features)
    159 
    160     def _train_op_fn(loss):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/canned/linear.py in linear_logit_fn(features)
     97         feature_columns=feature_columns,
     98         units=units,
---> 99         cols_to_vars=cols_to_vars)
    100     bias = cols_to_vars.pop('bias')
    101     if units > 1:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column.py in linear_model(features, feature_columns, units, sparse_combiner, weight_collections, trainable, cols_to_vars)
    423     for column in sorted(feature_columns, key=lambda x: x.name):
    424       with variable_scope.variable_scope(
--> 425           None, default_name=column._var_scope_name):  # pylint: disable=protected-access
    426         ordered_columns.append(column)
    427         weighted_sum = _create_weighted_sum(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in __enter__(self)
   1901 
   1902     try:
-> 1903       return self._enter_scope_uncached()
   1904     except:
   1905       if self._graph_context_manager is not None:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in _enter_scope_uncached(self)
   2001           self._default_name)
   2002       try:
-> 2003         current_name_scope_name = current_name_scope.__enter__()
   2004       except:
   2005         current_name_scope.__exit__(*sys.exc_info())

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in __enter__(self)
   5773       try:
   5774         self._name_scope = g.name_scope(self._name)
-> 5775         return self._name_scope.__enter__()
   5776       except:
   5777         self._g_manager.__exit__(*sys.exc_info())

/usr/lib/python3.6/contextlib.py in __enter__(self)
     79     def __enter__(self):
     80         try:
---> 81             return next(self.gen)
     82         except StopIteration:
     83             raise RuntimeError(""generator didn't yield"") from None

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in name_scope(self, name)
   3977         # (viz. '-', '\', '/', and '_').
   3978         if not _VALID_SCOPE_NAME_REGEX.match(name):
-> 3979           raise ValueError(""'%s' is not a valid scope name"" % name)
   3980       else:
   3981         # Scopes created in the root must match the more restrictive

ValueError: 'concave points_mean' is not a valid scope name
```
**Update**: I tried variables like `steps_per_period` as `global scope` and reran the model, but still the same issue persisting."
18894,TfLite Image classification score is not consistent it keeps increasing for same image untill it reaches to some saturation(actual score),"With same instance of 'interpreter' score is getting increased for same image until it reaches at some saturation.
`Interpreter tflite = new Interpreter(loadModelFile(context));`

Create Instance for ImageClassifier and use the same instance to classify Frame and run inference for the same image.
```
ImageClassifier(Activity activity) throws IOException {
    tflite = new Interpreter(loadModelFile(activity));
    labelList = loadLabelList(activity);
    imgData =
        ByteBuffer.allocateDirect(
            DIM_BATCH_SIZE
                * getImageSizeX()
                * getImageSizeY()
                * DIM_PIXEL_SIZE
                * getNumBytesPerChannel());
    imgData.order(ByteOrder.nativeOrder());
    filterLabelProbArray = new float[FILTER_STAGES][getNumLabels()];
    Log.d(TAG, ""Created a Tensorflow Lite Image Classifier."");
  }
```

Classifies a frame for the same image. Same image can be picked up from the Sd card.

```
private void classifyImage() {
    if (classifier == null || getActivity() == null || cameraDevice == null) {
      showToast(""Uninitialized Classifier or invalid context."");
      return;
    }
    String imgPath =  ""/storage/emulated/0/DCIM/test.jpg"";
    Log.d(""Image Path is %s"", imgPath);
    Bitmap bitmap = BitmapFactory.decodeFile(imgPath);
    Bitmap newbitmap = Bitmap.createScaledBitmap(bitmap, 299, 299, false);
    String textToShow = classifier.classifyFrame(newbitmap);
    bitmap.recycle();
    showToast(textToShow);
  }
```

At the first time when application gets launched score the image classification is 0.06 and then again if we called classifyImage() on some event click score gets increased to 0.13 and with same process it keeps increasing until it reached to 0.86(saturation).

I am not sure why its happening but it happened for both type of TfLite models inceptionV3 and MobileNet."
18893,Use ghost batch normalization with slim,"Posted in Stack Overflow:
https://stackoverflow.com/questions/49967489/use-ghost-batch-normalization-with-slim

### Details

OS Platform and Distribution: N/A
TensorFlow installed from: N/A
TensorFlow version: 1.8
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: use slim.batch_norm

### Describe the problem

normalization_layers.BatchNormalization takes an argument virtual_batch_size that tells the layer to use ""Ghost Batch Normalization"", which creates virtual sub-batches which are each normalized separately.

slim.batch_norm doesn't expose this argument. is there a way to use ghost BN with slim?

Thanks.
"
18891,Android Example won't run: AndroidRuntime: FATAL EXCEPTION NullPointerException,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android 6.0.1
- **TensorFlow installed from (source or binary)**: not used
- **TensorFlow version (use command below)**: not used
- **Python version**:  not used
- **Bazel version (if compiling from source)**: not used
- **GCC/Compiler version (if compiling from source)**: not used
- **CUDA/cuDNN version**: not used
- **GPU model and memory**: not used
- **Exact command to reproduce**: download android example project https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android, set `def nativeBuildSystem = 'none'`, try to start **Detector Activity**

### Describe the problem
**AndroidRuntime: FATAL EXCEPTION NullPointerException**

### Source code / logs
```
04-26 09:43:01.808 2417-2475/? E/AndroidRuntime: FATAL EXCEPTION: inference
    Process: org.tensorflow.demo, PID: 2417
    java.lang.NullPointerException: Attempt to invoke interface method 'java.util.List org.tensorflow.demo.Classifier.recognizeImage(android.graphics.Bitmap)' on a null object reference
        at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:289)
        at android.os.Handler.handleCallback(Handler.java:742)
        at android.os.Handler.dispatchMessage(Handler.java:95)
        at android.os.Looper.loop(Looper.java:157)
        at android.os.HandlerThread.run(HandlerThread.java:61)
```
"
18890,failed to allocate tensors on mobile device,"### System information
I minimize the model to very simple model with one input, convolution layer and pool is the output.
I work on Ubuntu 16.04
I compile TensorFlow Lite from source with Bazel
TensorFlow version is 1.8
Python 3.6
Bazel version 0.10.0
GCC/Compiler version - gcc (Ubuntu 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609
Working on CPU


### Describe the problem
I am trying to run my Tensorflow model on samsung galaxy s9, 
I trained model with TensorFlow freeze it and convert it to .tflite model file with toco.
I am running CPP program on the phone according to documentation and load the .tflite model file:

    std::unique_ptr<tflite::FlatBufferModel> model(tflite::FlatBufferModel::BuildFromFile(modelFile));
    if (!model) {
        printf(""Failed to mmap model %s\n"", g_modelFile);
    }
    model->error_reporter();
    tflite::ops::builtin::BuiltinOpResolver resolver;
    std::unique_ptr<tflite::Interpreter> interpreter;
    tflite::InterpreterBuilder(*model, resolver)(&interpreter);
    if (!interpreter) {
        printf(""Failed to construct interpreter\n"");
    }
    interpreter->UseNNAPI(false);
    if (interpreter->AllocateTensors() != kTfLiteOk) {
        printf(""Failed to allocate tensors!\n"");
    }


I succeed to initialize the model but failed to run **interpreter->AllocateTensors()**
There is no print of why it failed to allocate the tensors, just my print that the function failed

    if (interpreter->AllocateTensors() != kTfLiteOk) {
        printf(""Failed to allocate tensors!\n"");
    }

I minimized my model to include only reshape, convolution layer and pool and still failed to run AllocateTensors

Output with debug information:

resolved reporter
tensors size: 8
nodes size: 3
inputs: 1
input(0) name: patches
input(1) name: (null)
Reshape, 237552, 1, 0.000000, 0
Reshape/shape, 16, 2, 0.000000, 0
conv1/Relu, 1861632, 1, 0.000000, 0
conv1/convolution_bias, 128, 1, 0.000000, 0
conv1/kernel, 3456, 1, 0.000000, 0
input/patches-input, 237552, 1, 0.000000, 0
output, 465408, 1, 0.000000, 0
patches, 4, 1, 0.000000, 0

**Failed to allocate tensors!**"
18889,AttributeError: 'module' object has no attribute 'Exporter',"When I try to import tensorflow hub in python 2, I get the following error saying that 'module' object has no attribute 'Exporter'.

AttributeErrorTraceback (most recent call last)
<ipython-input-1-31fb71834c8c> in <module>()
      1 # Install TF-Hub.
      2 import tensorflow as tf
----> 3 import tensorflow_hub as hub
      4 import matplotlib.pyplot as plt
      5 import numpy as np

/usr/local/lib/python2.7/dist-packages/tensorflow_hub/__init__.py in <module>()
     24 import tensorflow as tf
     25 
---> 26 from tensorflow_hub.estimator import LatestModuleExporter
     27 from tensorflow_hub.estimator import register_module_for_export
     28 from tensorflow_hub.feature_column import image_embedding_column

/usr/local/lib/python2.7/dist-packages/tensorflow_hub/estimator.py in <module>()
     59 
     60 
---> 61 class LatestModuleExporter(tf.estimator.Exporter):
     62   """"""Regularly exports registered modules into timestamped directories.
     63 

AttributeError: 'module' object has no attribute 'Exporter'`"
18887,TypeError: Input 'input_sizes' of 'Conv3DBackpropInputV2' Op has type int64 that does not match expected type of int32,"Please go to Stack Overflow for help and support:
deconv_shape1 = layer3.get_shape()
    de_W1 = tf.Variable(tf.truncated_normal(shape=(4, 4, 4, deconv_shape1[4].value, 2), mean = mu, stddev = sigma))
    de_b1 = tf.Variable(tf.zeros(deconv_shape1[4].value))
    output_shape=x.get_shape().as_list()
    output_shape[1] *= 2
    output_shape[2] *= 2
    output_shape[3] *= 2 
    output_shape[4] = deconv_shape1[4].value
    output_shape=np.asarray(output_shape)
    output_shape=tf.convert_to_tensor(output_shape)
    print(type(output_shape))
    x = tf.nn.conv3d_transpose(x, de_W1, output_shape, strides=[1, 2, 2, 2, 1], padding=""SAME"")
    x = tf.nn.bias_add(x,de_b1)
    first_down_layer=x

I am getting these error on tensorflow 1.4 and python 2.7. The above code works fine on tensorflow 1.7 and python 3.6. I am a  newbie to tensorflow, please help ?

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
18886,Master branch fails to build,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 18.2 Sonya (based on Ubuntu 16.04)
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: Github master (most recent commit: adf045607cc4126366ebb84ee2109f88c6ab25f)
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.12.0
- **GCC/Compiler version (if compiling from source)**: 7.3.0 (6.4.0 used for CUDA compiler)
- **CUDA/cuDNN version**: CUDA 9.1, cuDNN 7.1, NCCL 2.1
- **GPU model and memory**: GTX1080Ti 11GB
- **Exact command to reproduce**: bazel build --config=opt --config=cuda //tensorflow/tools/... --verbose_failures

```
$ cat .tf_configure.bazelrc
build --action_env PYTHON_BIN_PATH=""/usr/bin/python3""
build --action_env PYTHON_LIB_PATH=""/usr/local/lib/python3.5/dist-packages""
build --force_python=py3
build --host_force_python=py3
build --python_path=""/usr/bin/python3""
build:gcp --define with_gcp_support=true
build:hdfs --define with_hdfs_support=true
build:s3 --define with_s3_support=true
build:kafka --define with_kafka_support=true
build:xla --define with_xla_support=true
build:gdr --define with_gdr_support=true
build:verbs --define with_verbs_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_CUDA=""1""
build --action_env CUDA_TOOLKIT_PATH=""/usr/local/cuda""
build --action_env TF_CUDA_VERSION=""9.1""
build --action_env CUDNN_INSTALL_PATH=""/usr/local/cuda-9.1""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env NCCL_INSTALL_PATH=""/usr/local/cuda-9.1""
build --action_env TF_NCCL_VERSION=""2""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
build --action_env LD_LIBRARY_PATH="":/usr/local/cuda/extras/CUPTI/lib:/usr/local/cuda/extras/CUPTI/lib""
build --action_env TF_CUDA_CLANG=""0""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc-6""
build --config=cuda
test --config=cuda
build --define grpc_no_ares=true
build:opt --copt=-march=native
build:opt --copt=-mtune=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
build --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
```

### Describe the problem
Build experiences linker errors while building `tensorflow/tools/api/lib`.

### Source code / logs
Output from build
```
ERROR: /home/bidski/Projects/tensorflow/tensorflow/tools/api/lib/BUILD:14:1: Linking of rule '//tensorflow/tools/api/lib:api_objects_proto_cc' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/bidski/.cache/bazel/_bazel_bidski/f3205cee2972cb6d709c5d3c6ebef87c/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda-9.1 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc-6 \
    LD_LIBRARY_PATH=:/usr/local/cuda/extras/CUPTI/lib:/usr/local/cuda/extras/CUPTI/lib \
    NCCL_INSTALL_PATH=/usr/local/cuda-9.1 \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/local/lib/python3.5/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_CUDA_VERSION=9.1 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION=2 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/libapi_objects_proto_cc.so -Wl,-no-as-needed -B/usr/bin/ -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,--gc-sections -Wl,@bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/libapi_objects_proto_cc.so-2.params)
/usr/lib/gcc/x86_64-linux-gnu/6/../../../x86_64-linux-gnu/Scrt1.o: In function `_start':
(.text+0x20): undefined reference to `main'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `void google::protobuf::internal::arena_destruct_object<google::protobuf::internal::InternalMetadataWithArenaBase<google::protobuf::UnknownFieldSet, google::protobuf::internal::InternalMetadataWithArena>::Container>(void*)':
api_objects.pb.cc:(.text._ZN6google8protobuf8internal21arena_destruct_objectINS1_29InternalMetadataWithArenaBaseINS0_15UnknownFieldSetENS1_25InternalMetadataWithArenaEE9ContainerEEEvPv[_ZN6google8protobuf8internal21arena_destruct_objectINS1_29InternalMetadataWithArenaBaseINS0_15UnknownFieldSetENS1_25InternalMetadataWithArenaEE9ContainerEEEvPv]+0xc): undefined reference to `google::protobuf::UnknownFieldSet::ClearFallback()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMember::SerializeWithCachedSizes(google::protobuf::io::CodedOutputStream*) const':
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIMember24SerializeWithCachedSizesEPN6google8protobuf2io17CodedOutputStreamE+0x6d): undefined reference to `google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::io::CodedOutputStream*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIMember24SerializeWithCachedSizesEPN6google8protobuf2io17CodedOutputStreamE+0x85): undefined reference to `google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::io::CodedOutputStream*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIMember24SerializeWithCachedSizesEPN6google8protobuf2io17CodedOutputStreamE+0x52): undefined reference to `google::protobuf::internal::WireFormat::SerializeUnknownFields(google::protobuf::UnknownFieldSet const&, google::protobuf::io::CodedOutputStream*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMethod::SerializeWithCachedSizes(google::protobuf::io::CodedOutputStream*) const':
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIMethod24SerializeWithCachedSizesEPN6google8protobuf2io17CodedOutputStreamE+0x6e): undefined reference to `google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::io::CodedOutputStream*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIMethod24SerializeWithCachedSizesEPN6google8protobuf2io17CodedOutputStreamE+0x86): undefined reference to `google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::io::CodedOutputStream*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIMethod24SerializeWithCachedSizesEPN6google8protobuf2io17CodedOutputStreamE+0x9d): undefined reference to `google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::io::CodedOutputStream*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIMethod24SerializeWithCachedSizesEPN6google8protobuf2io17CodedOutputStreamE+0x52): undefined reference to `google::protobuf::internal::WireFormat::SerializeUnknownFields(google::protobuf::UnknownFieldSet const&, google::protobuf::io::CodedOutputStream*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMember::InternalSerializeWithCachedSizesToArray(bool, unsigned char*) const':
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIMember39InternalSerializeWithCachedSizesToArrayEbPh+0x4c): undefined reference to `google::protobuf::io::CodedOutputStream::WriteStringWithSizeToArray(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned char*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIMember39InternalSerializeWithCachedSizesToArrayEbPh+0x64): undefined reference to `google::protobuf::io::CodedOutputStream::WriteStringWithSizeToArray(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned char*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIMember39InternalSerializeWithCachedSizesToArrayEbPh+0x3c): undefined reference to `google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(google::protobuf::UnknownFieldSet const&, unsigned char*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMethod::InternalSerializeWithCachedSizesToArray(bool, unsigned char*) const':
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIMethod39InternalSerializeWithCachedSizesToArrayEbPh+0x4d): undefined reference to `google::protobuf::io::CodedOutputStream::WriteStringWithSizeToArray(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned char*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIMethod39InternalSerializeWithCachedSizesToArrayEbPh+0x65): undefined reference to `google::protobuf::io::CodedOutputStream::WriteStringWithSizeToArray(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned char*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIMethod39InternalSerializeWithCachedSizesToArrayEbPh+0x7c): undefined reference to `google::protobuf::io::CodedOutputStream::WriteStringWithSizeToArray(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned char*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIMethod39InternalSerializeWithCachedSizesToArrayEbPh+0x3c): undefined reference to `google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(google::protobuf::UnknownFieldSet const&, unsigned char*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMember::ByteSizeLong() const':
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIMember12ByteSizeLongEv+0x85): undefined reference to `google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(google::protobuf::UnknownFieldSet const&)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMethod::ByteSizeLong() const':
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIMethod12ByteSizeLongEv+0xb5): undefined reference to `google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(google::protobuf::UnknownFieldSet const&)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIObject::ByteSizeLong() const':
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIObject12ByteSizeLongEv+0x9d): undefined reference to `google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(google::protobuf::UnknownFieldSet const&)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIModule::ByteSizeLong() const':
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIModule12ByteSizeLongEv+0xc5): undefined reference to `google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(google::protobuf::UnknownFieldSet const&)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIClass::ByteSizeLong() const':
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api10TFAPIClass12ByteSizeLongEv+0x115): undefined reference to `google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(google::protobuf::UnknownFieldSet const&)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIObject::SerializeWithCachedSizes(google::protobuf::io::CodedOutputStream*) const':
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIObject24SerializeWithCachedSizesEPN6google8protobuf2io17CodedOutputStreamE+0x6e): undefined reference to `google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(int, google::protobuf::MessageLite const&, google::protobuf::io::CodedOutputStream*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIObject24SerializeWithCachedSizesEPN6google8protobuf2io17CodedOutputStreamE+0x86): undefined reference to `google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(int, google::protobuf::MessageLite const&, google::protobuf::io::CodedOutputStream*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIObject24SerializeWithCachedSizesEPN6google8protobuf2io17CodedOutputStreamE+0x9d): undefined reference to `google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::io::CodedOutputStream*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIObject24SerializeWithCachedSizesEPN6google8protobuf2io17CodedOutputStreamE+0x52): undefined reference to `google::protobuf::internal::WireFormat::SerializeUnknownFields(google::protobuf::UnknownFieldSet const&, google::protobuf::io::CodedOutputStream*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIModule::SerializeWithCachedSizes(google::protobuf::io::CodedOutputStream*) const':
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIModule24SerializeWithCachedSizesEPN6google8protobuf2io17CodedOutputStreamE+0x39): undefined reference to `google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(int, google::protobuf::MessageLite const&, google::protobuf::io::CodedOutputStream*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIModule24SerializeWithCachedSizesEPN6google8protobuf2io17CodedOutputStreamE+0x69): undefined reference to `google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(int, google::protobuf::MessageLite const&, google::protobuf::io::CodedOutputStream*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIModule24SerializeWithCachedSizesEPN6google8protobuf2io17CodedOutputStreamE+0xa0): undefined reference to `google::protobuf::internal::WireFormat::SerializeUnknownFields(google::protobuf::UnknownFieldSet const&, google::protobuf::io::CodedOutputStream*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIClass::SerializeWithCachedSizes(google::protobuf::io::CodedOutputStream*) const':
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api10TFAPIClass24SerializeWithCachedSizesEPN6google8protobuf2io17CodedOutputStreamE+0x39): undefined reference to `google::protobuf::internal::WireFormatLite::WriteString(int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::io::CodedOutputStream*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api10TFAPIClass24SerializeWithCachedSizesEPN6google8protobuf2io17CodedOutputStreamE+0x69): undefined reference to `google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(int, google::protobuf::MessageLite const&, google::protobuf::io::CodedOutputStream*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api10TFAPIClass24SerializeWithCachedSizesEPN6google8protobuf2io17CodedOutputStreamE+0x99): undefined reference to `google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(int, google::protobuf::MessageLite const&, google::protobuf::io::CodedOutputStream*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api10TFAPIClass24SerializeWithCachedSizesEPN6google8protobuf2io17CodedOutputStreamE+0xd0): undefined reference to `google::protobuf::internal::WireFormat::SerializeUnknownFields(google::protobuf::UnknownFieldSet const&, google::protobuf::io::CodedOutputStream*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIModule::InternalSerializeWithCachedSizesToArray(bool, unsigned char*) const':
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIModule39InternalSerializeWithCachedSizesToArrayEbPh+0x168): undefined reference to `google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(google::protobuf::UnknownFieldSet const&, unsigned char*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIClass::InternalSerializeWithCachedSizesToArray(bool, unsigned char*) const':
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api10TFAPIClass39InternalSerializeWithCachedSizesToArrayEbPh+0x48): undefined reference to `google::protobuf::io::CodedOutputStream::WriteStringWithSizeToArray(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned char*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api10TFAPIClass39InternalSerializeWithCachedSizesToArrayEbPh+0x1a8): undefined reference to `google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(google::protobuf::UnknownFieldSet const&, unsigned char*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIObject::InternalSerializeWithCachedSizesToArray(bool, unsigned char*) const':
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIObject39InternalSerializeWithCachedSizesToArrayEbPh+0x13c): undefined reference to `google::protobuf::io::CodedOutputStream::WriteStringWithSizeToArray(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned char*)'
api_objects.pb.cc:(.text._ZNK11third_party10tensorflow5tools3api11TFAPIObject39InternalSerializeWithCachedSizesToArrayEbPh+0x6e): undefined reference to `google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(google::protobuf::UnknownFieldSet const&, unsigned char*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `void google::protobuf::internal::RepeatedPtrFieldBase::MergeFromInnerLoop<google::protobuf::RepeatedPtrField<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >::TypeHandler>(void**, void**, int, int)':
api_objects.pb.cc:(.text._ZN6google8protobuf8internal20RepeatedPtrFieldBase18MergeFromInnerLoopINS0_16RepeatedPtrFieldINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEE11TypeHandlerEEEvPPvSE_ii[_ZN6google8protobuf8internal20RepeatedPtrFieldBase18MergeFromInnerLoopINS0_16RepeatedPtrFieldINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEE11TypeHandlerEEEvPPvSE_ii]+0x90): undefined reference to `google::protobuf::internal::ArenaImpl::AllocateAlignedAndAddCleanup(unsigned long, void (*)(void*))'
api_objects.pb.cc:(.text._ZN6google8protobuf8internal20RepeatedPtrFieldBase18MergeFromInnerLoopINS0_16RepeatedPtrFieldINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEE11TypeHandlerEEEvPPvSE_ii[_ZN6google8protobuf8internal20RepeatedPtrFieldBase18MergeFromInnerLoopINS0_16RepeatedPtrFieldINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEE11TypeHandlerEEEvPPvSE_ii]+0xfa): undefined reference to `google::protobuf::Arena::OnArenaAllocation(std::type_info const*, unsigned long) const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto::InitDefaultsTFAPIMember()':
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto23InitDefaultsTFAPIMemberEv+0x26): undefined reference to `vtable for google::protobuf::internal::FunctionClosure0'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto23InitDefaultsTFAPIMemberEv+0x45): undefined reference to `google::protobuf::GoogleOnceInitImpl(long*, google::protobuf::Closure*)'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto23InitDefaultsTFAPIMemberEv+0x4d): undefined reference to `google::protobuf::internal::FunctionClosure0::~FunctionClosure0()'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto23InitDefaultsTFAPIMemberEv+0x65): undefined reference to `google::protobuf::internal::FunctionClosure0::~FunctionClosure0()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto::InitDefaultsTFAPIMethod()':
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto23InitDefaultsTFAPIMethodEv+0x26): undefined reference to `vtable for google::protobuf::internal::FunctionClosure0'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto23InitDefaultsTFAPIMethodEv+0x45): undefined reference to `google::protobuf::GoogleOnceInitImpl(long*, google::protobuf::Closure*)'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto23InitDefaultsTFAPIMethodEv+0x4d): undefined reference to `google::protobuf::internal::FunctionClosure0::~FunctionClosure0()'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto23InitDefaultsTFAPIMethodEv+0x65): undefined reference to `google::protobuf::internal::FunctionClosure0::~FunctionClosure0()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto::InitDefaultsTFAPIModule()':
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto23InitDefaultsTFAPIModuleEv+0x26): undefined reference to `vtable for google::protobuf::internal::FunctionClosure0'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto23InitDefaultsTFAPIModuleEv+0x45): undefined reference to `google::protobuf::GoogleOnceInitImpl(long*, google::protobuf::Closure*)'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto23InitDefaultsTFAPIModuleEv+0x4d): undefined reference to `google::protobuf::internal::FunctionClosure0::~FunctionClosure0()'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto23InitDefaultsTFAPIModuleEv+0x65): undefined reference to `google::protobuf::internal::FunctionClosure0::~FunctionClosure0()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto::InitDefaultsTFAPIClass()':
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto22InitDefaultsTFAPIClassEv+0x26): undefined reference to `vtable for google::protobuf::internal::FunctionClosure0'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto22InitDefaultsTFAPIClassEv+0x45): undefined reference to `google::protobuf::GoogleOnceInitImpl(long*, google::protobuf::Closure*)'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto22InitDefaultsTFAPIClassEv+0x4d): undefined reference to `google::protobuf::internal::FunctionClosure0::~FunctionClosure0()'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto22InitDefaultsTFAPIClassEv+0x65): undefined reference to `google::protobuf::internal::FunctionClosure0::~FunctionClosure0()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto::InitDefaultsTFAPIObject()':
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto23InitDefaultsTFAPIObjectEv+0x26): undefined reference to `vtable for google::protobuf::internal::FunctionClosure0'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto23InitDefaultsTFAPIObjectEv+0x45): undefined reference to `google::protobuf::GoogleOnceInitImpl(long*, google::protobuf::Closure*)'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto23InitDefaultsTFAPIObjectEv+0x4d): undefined reference to `google::protobuf::internal::FunctionClosure0::~FunctionClosure0()'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto23InitDefaultsTFAPIObjectEv+0x65): undefined reference to `google::protobuf::internal::FunctionClosure0::~FunctionClosure0()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto::AddDescriptorsImpl()':
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto18AddDescriptorsImplEv+0x2a): undefined reference to `google::protobuf::DescriptorPool::InternalAddGeneratedFile(void const*, int)'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto18AddDescriptorsImplEv+0x3e): undefined reference to `google::protobuf::MessageFactory::InternalRegisterGeneratedFile(char const*, void (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&))'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto::protobuf_AssignDescriptorsOnce()':
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto30protobuf_AssignDescriptorsOnceEv+0x26): undefined reference to `vtable for google::protobuf::internal::FunctionClosure0'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto30protobuf_AssignDescriptorsOnceEv+0x45): undefined reference to `google::protobuf::GoogleOnceInitImpl(long*, google::protobuf::Closure*)'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto30protobuf_AssignDescriptorsOnceEv+0x4d): undefined reference to `google::protobuf::internal::FunctionClosure0::~FunctionClosure0()'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto30protobuf_AssignDescriptorsOnceEv+0x65): undefined reference to `google::protobuf::internal::FunctionClosure0::~FunctionClosure0()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto::protobuf_RegisterTypes(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)':
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto22protobuf_RegisterTypesERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE+0x17): undefined reference to `google::protobuf::internal::RegisterAllTypes(google::protobuf::Metadata const*, int)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto::AddDescriptors()':
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto14AddDescriptorsEv+0x26): undefined reference to `vtable for google::protobuf::internal::FunctionClosure0'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto14AddDescriptorsEv+0x45): undefined reference to `google::protobuf::GoogleOnceInitImpl(long*, google::protobuf::Closure*)'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto14AddDescriptorsEv+0x4d): undefined reference to `google::protobuf::internal::FunctionClosure0::~FunctionClosure0()'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto14AddDescriptorsEv+0x65): undefined reference to `google::protobuf::internal::FunctionClosure0::~FunctionClosure0()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto::protobuf_AssignDescriptors()':
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto26protobuf_AssignDescriptorsEv+0x68): undefined reference to `google::protobuf::internal::AssignDescriptors(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::internal::MigrationSchema const*, google::protobuf::Message const* const*, unsigned int const*, google::protobuf::MessageFactory*, google::protobuf::Metadata*, google::protobuf::EnumDescriptor const**, google::protobuf::ServiceDescriptor const**)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMember::SharedCtor()':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMember10SharedCtorEv+0x3): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMember::SharedDtor()':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMember10SharedDtorEv+0x17): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMember::~TFAPIMember()':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMemberD2Ev+0x47): undefined reference to `google::protobuf::UnknownFieldSet::ClearFallback()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMethod::SharedCtor()':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMethod10SharedCtorEv+0x3): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMethod::SharedDtor()':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMethod10SharedDtorEv+0x17): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMethod::~TFAPIMethod()':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMethodD2Ev+0x47): undefined reference to `google::protobuf::UnknownFieldSet::ClearFallback()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIModule::~TFAPIModule()':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIModuleD2Ev+0x6a): undefined reference to `google::protobuf::UnknownFieldSet::ClearFallback()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIClass::~TFAPIClass()':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClassD2Ev+0x72): undefined reference to `google::protobuf::UnknownFieldSet::ClearFallback()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIObject::SharedCtor()':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIObject10SharedCtorEv+0x3): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIObject::SharedDtor()':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIObject10SharedDtorEv+0xe): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIObject::~TFAPIObject()':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIObjectD2Ev+0x47): undefined reference to `google::protobuf::UnknownFieldSet::ClearFallback()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `google::protobuf::internal::InternalMetadataWithArenaBase<google::protobuf::UnknownFieldSet, google::protobuf::internal::InternalMetadataWithArena>::~InternalMetadataWithArenaBase()':
api_objects.pb.cc:(.text._ZN6google8protobuf8internal29InternalMetadataWithArenaBaseINS0_15UnknownFieldSetENS1_25InternalMetadataWithArenaEED2Ev[_ZN6google8protobuf8internal29InternalMetadataWithArenaBaseINS0_15UnknownFieldSetENS1_25InternalMetadataWithArenaEED5Ev]+0x3a): undefined reference to `google::protobuf::UnknownFieldSet::ClearFallback()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto::InitDefaultsTFAPIMemberImpl()':
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto27InitDefaultsTFAPIMemberImplEv+0x1b): undefined reference to `google::protobuf::internal::VerifyVersion(int, int, char const*)'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto27InitDefaultsTFAPIMemberImplEv+0x20): undefined reference to `google::protobuf::internal::InitProtobufDefaults()'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto27InitDefaultsTFAPIMemberImplEv+0x37): undefined reference to `google::protobuf::internal::OnShutdownDestroyMessage(void const*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto::InitDefaultsTFAPIMethodImpl()':
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto27InitDefaultsTFAPIMethodImplEv+0x1b): undefined reference to `google::protobuf::internal::VerifyVersion(int, int, char const*)'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto27InitDefaultsTFAPIMethodImplEv+0x20): undefined reference to `google::protobuf::internal::InitProtobufDefaults()'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto27InitDefaultsTFAPIMethodImplEv+0x37): undefined reference to `google::protobuf::internal::OnShutdownDestroyMessage(void const*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto::InitDefaultsTFAPIObjectImpl()':
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto27InitDefaultsTFAPIObjectImplEv+0x1b): undefined reference to `google::protobuf::internal::VerifyVersion(int, int, char const*)'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto27InitDefaultsTFAPIObjectImplEv+0x20): undefined reference to `google::protobuf::internal::InitProtobufDefaults()'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto27InitDefaultsTFAPIObjectImplEv+0x41): undefined reference to `google::protobuf::internal::OnShutdownDestroyMessage(void const*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto::InitDefaultsTFAPIModuleImpl()':
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto27InitDefaultsTFAPIModuleImplEv+0x1b): undefined reference to `google::protobuf::internal::VerifyVersion(int, int, char const*)'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto27InitDefaultsTFAPIModuleImplEv+0x20): undefined reference to `google::protobuf::internal::InitProtobufDefaults()'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto27InitDefaultsTFAPIModuleImplEv+0x41): undefined reference to `google::protobuf::internal::OnShutdownDestroyMessage(void const*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto::InitDefaultsTFAPIClassImpl()':
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto26InitDefaultsTFAPIClassImplEv+0x1b): undefined reference to `google::protobuf::internal::VerifyVersion(int, int, char const*)'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto26InitDefaultsTFAPIClassImplEv+0x20): undefined reference to `google::protobuf::internal::InitProtobufDefaults()'
api_objects.pb.cc:(.text._ZN63protobuf_tensorflow_2ftools_2fapi_2flib_2fapi_5fobjects_2eproto26InitDefaultsTFAPIClassImplEv+0x41): undefined reference to `google::protobuf::internal::OnShutdownDestroyMessage(void const*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `void google::protobuf::Arena::Own<third_party::tensorflow::tools::api::TFAPIMember>(third_party::tensorflow::tools::api::TFAPIMember*)':
api_objects.pb.cc:(.text._ZN6google8protobuf5Arena3OwnIN11third_party10tensorflow5tools3api11TFAPIMemberEEEvPT_[_ZN6google8protobuf5Arena3OwnIN11third_party10tensorflow5tools3api11TFAPIMemberEEEvPT_]+0x12): undefined reference to `google::protobuf::internal::ArenaImpl::AddCleanup(void*, void (*)(void*))'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `void google::protobuf::Arena::Own<third_party::tensorflow::tools::api::TFAPIMethod>(third_party::tensorflow::tools::api::TFAPIMethod*)':
api_objects.pb.cc:(.text._ZN6google8protobuf5Arena3OwnIN11third_party10tensorflow5tools3api11TFAPIMethodEEEvPT_[_ZN6google8protobuf5Arena3OwnIN11third_party10tensorflow5tools3api11TFAPIMethodEEEvPT_]+0x12): undefined reference to `google::protobuf::internal::ArenaImpl::AddCleanup(void*, void (*)(void*))'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `void google::protobuf::Arena::Own<third_party::tensorflow::tools::api::TFAPIModule>(third_party::tensorflow::tools::api::TFAPIModule*)':
api_objects.pb.cc:(.text._ZN6google8protobuf5Arena3OwnIN11third_party10tensorflow5tools3api11TFAPIModuleEEEvPT_[_ZN6google8protobuf5Arena3OwnIN11third_party10tensorflow5tools3api11TFAPIModuleEEEvPT_]+0x12): undefined reference to `google::protobuf::internal::ArenaImpl::AddCleanup(void*, void (*)(void*))'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `void google::protobuf::Arena::Own<third_party::tensorflow::tools::api::TFAPIClass>(third_party::tensorflow::tools::api::TFAPIClass*)':
api_objects.pb.cc:(.text._ZN6google8protobuf5Arena3OwnIN11third_party10tensorflow5tools3api10TFAPIClassEEEvPT_[_ZN6google8protobuf5Arena3OwnIN11third_party10tensorflow5tools3api10TFAPIClassEEEvPT_]+0x12): undefined reference to `google::protobuf::internal::ArenaImpl::AddCleanup(void*, void (*)(void*))'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `void google::protobuf::Arena::Own<third_party::tensorflow::tools::api::TFAPIObject>(third_party::tensorflow::tools::api::TFAPIObject*)':
api_objects.pb.cc:(.text._ZN6google8protobuf5Arena3OwnIN11third_party10tensorflow5tools3api11TFAPIObjectEEEvPT_[_ZN6google8protobuf5Arena3OwnIN11third_party10tensorflow5tools3api11TFAPIObjectEEEvPT_]+0x12): undefined reference to `google::protobuf::internal::ArenaImpl::AddCleanup(void*, void (*)(void*))'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `google::protobuf::internal::InternalMetadataWithArenaBase<google::protobuf::UnknownFieldSet, google::protobuf::internal::InternalMetadataWithArena>::mutable_unknown_fields_slow()':
api_objects.pb.cc:(.text._ZN6google8protobuf8internal29InternalMetadataWithArenaBaseINS0_15UnknownFieldSetENS1_25InternalMetadataWithArenaEE27mutable_unknown_fields_slowEv[_ZN6google8protobuf8internal29InternalMetadataWithArenaBaseINS0_15UnknownFieldSetENS1_25InternalMetadataWithArenaEE27mutable_unknown_fields_slowEv]+0x38): undefined reference to `google::protobuf::internal::ArenaImpl::AllocateAlignedAndAddCleanup(unsigned long, void (*)(void*))'
api_objects.pb.cc:(.text._ZN6google8protobuf8internal29InternalMetadataWithArenaBaseINS0_15UnknownFieldSetENS1_25InternalMetadataWithArenaEE27mutable_unknown_fields_slowEv[_ZN6google8protobuf8internal29InternalMetadataWithArenaBaseINS0_15UnknownFieldSetENS1_25InternalMetadataWithArenaEE27mutable_unknown_fields_slowEv]+0xa0): undefined reference to `google::protobuf::Arena::OnArenaAllocation(std::type_info const*, unsigned long) const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMethod::Clear()':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMethod5ClearEv+0x6c): undefined reference to `google::protobuf::UnknownFieldSet::ClearFallback()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMember::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*)':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMember27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x75): undefined reference to `google::protobuf::internal::WireFormat::SkipField(google::protobuf::io::CodedInputStream*, unsigned int, google::protobuf::UnknownFieldSet*)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMember27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x96): undefined reference to `google::protobuf::io::CodedInputStream::ReadTagFallback(unsigned int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMember27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0xd0): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMember27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0xda): undefined reference to `google::protobuf::internal::WireFormatLite::ReadBytes(google::protobuf::io::CodedInputStream*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMember27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x11b): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMethod::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*)':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMethod27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x83): undefined reference to `google::protobuf::internal::WireFormat::SkipField(google::protobuf::io::CodedInputStream*, unsigned int, google::protobuf::UnknownFieldSet*)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMethod27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0xa6): undefined reference to `google::protobuf::io::CodedInputStream::ReadTagFallback(unsigned int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMethod27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0xe0): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMethod27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x10c): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMethod27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x134): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMethod27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x13e): undefined reference to `google::protobuf::internal::WireFormatLite::ReadBytes(google::protobuf::io::CodedInputStream*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIObject::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*)':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIObject27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x69): undefined reference to `google::protobuf::internal::WireFormat::SkipField(google::protobuf::io::CodedInputStream*, unsigned int, google::protobuf::UnknownFieldSet*)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIObject27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x8a): undefined reference to `google::protobuf::io::CodedInputStream::ReadTagFallback(unsigned int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIObject27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0xc3): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIObject27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0xd1): undefined reference to `google::protobuf::internal::WireFormatLite::ReadBytes(google::protobuf::io::CodedInputStream*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIObject27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x15e): undefined reference to `google::protobuf::io::CodedInputStream::ReadVarintSizeAsIntFallback()'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIObject27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x170): undefined reference to `google::protobuf::io::CodedInputStream::IncrementRecursionDepthAndPushLimit(int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIObject27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x1a2): undefined reference to `google::protobuf::io::CodedInputStream::DecrementRecursionDepthAndPopLimit(int)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIModule::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*)':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIModule27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x66): undefined reference to `google::protobuf::internal::WireFormat::SkipField(google::protobuf::io::CodedInputStream*, unsigned int, google::protobuf::UnknownFieldSet*)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIModule27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x8b): undefined reference to `google::protobuf::io::CodedInputStream::ReadTagFallback(unsigned int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIModule27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x115): undefined reference to `google::protobuf::internal::ArenaImpl::AllocateAlignedAndAddCleanup(unsigned long, void (*)(void*))'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIModule27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x167): undefined reference to `google::protobuf::io::CodedInputStream::ReadVarintSizeAsIntFallback()'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIModule27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x17d): undefined reference to `google::protobuf::io::CodedInputStream::IncrementRecursionDepthAndPushLimit(int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIModule27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x1ec): undefined reference to `google::protobuf::io::CodedInputStream::DecrementRecursionDepthAndPopLimit(int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIModule27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x258): undefined reference to `google::protobuf::internal::ArenaImpl::AllocateAlignedAndAddCleanup(unsigned long, void (*)(void*))'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIModule27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x2d4): undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::Reserve(int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIModule27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x2f0): undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::Reserve(int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIModule27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x314): undefined reference to `google::protobuf::Arena::OnArenaAllocation(std::type_info const*, unsigned long) const'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIModule27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x335): undefined reference to `google::protobuf::Arena::OnArenaAllocation(std::type_info const*, unsigned long) const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIClass::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*)':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x71): undefined reference to `google::protobuf::internal::WireFormat::SkipField(google::protobuf::io::CodedInputStream*, unsigned int, google::protobuf::UnknownFieldSet*)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x96): undefined reference to `google::protobuf::io::CodedInputStream::ReadTagFallback(unsigned int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x11d): undefined reference to `google::protobuf::internal::ArenaImpl::AllocateAlignedAndAddCleanup(unsigned long, void (*)(void*))'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x159): undefined reference to `google::protobuf::internal::WireFormatLite::ReadBytes(google::protobuf::io::CodedInputStream*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x1e1): undefined reference to `google::protobuf::internal::ArenaImpl::AllocateAlignedAndAddCleanup(unsigned long, void (*)(void*))'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x281): undefined reference to `google::protobuf::internal::ArenaImpl::AllocateAlignedAndAddCleanup(unsigned long, void (*)(void*))'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x2d3): undefined reference to `google::protobuf::io::CodedInputStream::ReadVarintSizeAsIntFallback()'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x2ed): undefined reference to `google::protobuf::io::CodedInputStream::IncrementRecursionDepthAndPushLimit(int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x323): undefined reference to `google::protobuf::io::CodedInputStream::DecrementRecursionDepthAndPopLimit(int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x3c7): undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::Reserve(int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x3e3): undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::Reserve(int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x3ff): undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::Reserve(int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x423): undefined reference to `google::protobuf::Arena::OnArenaAllocation(std::type_info const*, unsigned long) const'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x444): undefined reference to `google::protobuf::Arena::OnArenaAllocation(std::type_info const*, unsigned long) const'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass27MergePartialFromCodedStreamEPN6google8protobuf2io16CodedInputStreamE+0x465): undefined reference to `google::protobuf::Arena::OnArenaAllocation(std::type_info const*, unsigned long) const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMember::Clear()':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMember5ClearEv+0x5c): undefined reference to `google::protobuf::UnknownFieldSet::ClearFallback()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIModule::Clear()':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIModule5ClearEv+0x51): undefined reference to `google::protobuf::UnknownFieldSet::ClearFallback()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIClass::Clear()':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass5ClearEv+0x91): undefined reference to `google::protobuf::UnknownFieldSet::ClearFallback()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIObject::Clear()':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIObject5ClearEv+0x5f): undefined reference to `google::protobuf::UnknownFieldSet::ClearFallback()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMember::MergeFrom(third_party::tensorflow::tools::api::TFAPIMember const&)':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMember9MergeFromERKS3_+0x44): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMember9MergeFromERKS3_+0x6c): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMember9MergeFromERKS3_+0xab): undefined reference to `google::protobuf::UnknownFieldSet::MergeFrom(google::protobuf::UnknownFieldSet const&)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMember::MergeFrom(google::protobuf::Message const&)':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMember9MergeFromERKN6google8protobuf7MessageE+0x16): undefined reference to `typeinfo for google::protobuf::Message'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMember9MergeFromERKN6google8protobuf7MessageE+0x4b): undefined reference to `google::protobuf::internal::ReflectionOps::Merge(google::protobuf::Message const&, google::protobuf::Message*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMethod::MergeFrom(third_party::tensorflow::tools::api::TFAPIMethod const&)':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMethod9MergeFromERKS3_+0x4b): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMethod9MergeFromERKS3_+0x75): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMethod9MergeFromERKS3_+0x9b): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMethod9MergeFromERKS3_+0xd3): undefined reference to `google::protobuf::UnknownFieldSet::MergeFrom(google::protobuf::UnknownFieldSet const&)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIMethod::MergeFrom(google::protobuf::Message const&)':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMethod9MergeFromERKN6google8protobuf7MessageE+0x16): undefined reference to `typeinfo for google::protobuf::Message'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIMethod9MergeFromERKN6google8protobuf7MessageE+0x4b): undefined reference to `google::protobuf::internal::ReflectionOps::Merge(google::protobuf::Message const&, google::protobuf::Message*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `void google::protobuf::internal::RepeatedPtrFieldBase::MergeFromInnerLoop<google::protobuf::RepeatedPtrField<third_party::tensorflow::tools::api::TFAPIMember>::TypeHandler>(void**, void**, int, int)':
api_objects.pb.cc:(.text._ZN6google8protobuf8internal20RepeatedPtrFieldBase18MergeFromInnerLoopINS0_16RepeatedPtrFieldIN11third_party10tensorflow5tools3api11TFAPIMemberEE11TypeHandlerEEEvPPvSD_ii[_ZN6google8protobuf8internal20RepeatedPtrFieldBase18MergeFromInnerLoopINS0_16RepeatedPtrFieldIN11third_party10tensorflow5tools3api11TFAPIMemberEE11TypeHandlerEEEvPPvSD_ii]+0x93): undefined reference to `google::protobuf::internal::ArenaImpl::AllocateAlignedAndAddCleanup(unsigned long, void (*)(void*))'
api_objects.pb.cc:(.text._ZN6google8protobuf8internal20RepeatedPtrFieldBase18MergeFromInnerLoopINS0_16RepeatedPtrFieldIN11third_party10tensorflow5tools3api11TFAPIMemberEE11TypeHandlerEEEvPPvSD_ii[_ZN6google8protobuf8internal20RepeatedPtrFieldBase18MergeFromInnerLoopINS0_16RepeatedPtrFieldIN11third_party10tensorflow5tools3api11TFAPIMemberEE11TypeHandlerEEEvPPvSD_ii]+0xef): undefined reference to `google::protobuf::Arena::OnArenaAllocation(std::type_info const*, unsigned long) const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `void google::protobuf::internal::RepeatedPtrFieldBase::MergeFromInnerLoop<google::protobuf::RepeatedPtrField<third_party::tensorflow::tools::api::TFAPIMethod>::TypeHandler>(void**, void**, int, int)':
api_objects.pb.cc:(.text._ZN6google8protobuf8internal20RepeatedPtrFieldBase18MergeFromInnerLoopINS0_16RepeatedPtrFieldIN11third_party10tensorflow5tools3api11TFAPIMethodEE11TypeHandlerEEEvPPvSD_ii[_ZN6google8protobuf8internal20RepeatedPtrFieldBase18MergeFromInnerLoopINS0_16RepeatedPtrFieldIN11third_party10tensorflow5tools3api11TFAPIMethodEE11TypeHandlerEEEvPPvSD_ii]+0x93): undefined reference to `google::protobuf::internal::ArenaImpl::AllocateAlignedAndAddCleanup(unsigned long, void (*)(void*))'
api_objects.pb.cc:(.text._ZN6google8protobuf8internal20RepeatedPtrFieldBase18MergeFromInnerLoopINS0_16RepeatedPtrFieldIN11third_party10tensorflow5tools3api11TFAPIMethodEE11TypeHandlerEEEvPPvSD_ii[_ZN6google8protobuf8internal20RepeatedPtrFieldBase18MergeFromInnerLoopINS0_16RepeatedPtrFieldIN11third_party10tensorflow5tools3api11TFAPIMethodEE11TypeHandlerEEEvPPvSD_ii]+0xef): undefined reference to `google::protobuf::Arena::OnArenaAllocation(std::type_info const*, unsigned long) const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIModule::MergeFrom(third_party::tensorflow::tools::api::TFAPIModule const&)':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIModule9MergeFromERKS3_+0x60): undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::InternalExtend(int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIModule9MergeFromERKS3_+0xc4): undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::InternalExtend(int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIModule9MergeFromERKS3_+0x11b): undefined reference to `google::protobuf::UnknownFieldSet::MergeFrom(google::protobuf::UnknownFieldSet const&)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIModule::MergeFrom(google::protobuf::Message const&)':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIModule9MergeFromERKN6google8protobuf7MessageE+0x16): undefined reference to `typeinfo for google::protobuf::Message'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIModule9MergeFromERKN6google8protobuf7MessageE+0x4b): undefined reference to `google::protobuf::internal::ReflectionOps::Merge(google::protobuf::Message const&, google::protobuf::Message*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIClass::MergeFrom(third_party::tensorflow::tools::api::TFAPIClass const&)':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass9MergeFromERKS3_+0x70): undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::InternalExtend(int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass9MergeFromERKS3_+0xd4): undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::InternalExtend(int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass9MergeFromERKS3_+0x12c): undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::InternalExtend(int)'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass9MergeFromERKS3_+0x183): undefined reference to `google::protobuf::UnknownFieldSet::MergeFrom(google::protobuf::UnknownFieldSet const&)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIClass::MergeFrom(google::protobuf::Message const&)':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass9MergeFromERKN6google8protobuf7MessageE+0x16): undefined reference to `typeinfo for google::protobuf::Message'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api10TFAPIClass9MergeFromERKN6google8protobuf7MessageE+0x4b): undefined reference to `google::protobuf::internal::ReflectionOps::Merge(google::protobuf::Message const&, google::protobuf::Message*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIObject::MergeFrom(third_party::tensorflow::tools::api::TFAPIObject const&)':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIObject9MergeFromERKS3_+0x4f): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIObject9MergeFromERKS3_+0xf7): undefined reference to `google::protobuf::UnknownFieldSet::MergeFrom(google::protobuf::UnknownFieldSet const&)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o: In function `third_party::tensorflow::tools::api::TFAPIObject::MergeFrom(google::protobuf::Message const&)':
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIObject9MergeFromERKN6google8protobuf7MessageE+0x16): undefined reference to `typeinfo for google::protobuf::Message'
api_objects.pb.cc:(.text._ZN11third_party10tensorflow5tools3api11TFAPIObject9MergeFromERKN6google8protobuf7MessageE+0x4b): undefined reference to `google::protobuf::internal::ReflectionOps::Merge(google::protobuf::Message const&, google::protobuf::Message*)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTIN11third_party10tensorflow5tools3api11TFAPIMemberE[_ZTIN11third_party10tensorflow5tools3api11TFAPIMemberE]+0x10): undefined reference to `typeinfo for google::protobuf::Message'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTIN11third_party10tensorflow5tools3api11TFAPIMethodE[_ZTIN11third_party10tensorflow5tools3api11TFAPIMethodE]+0x10): undefined reference to `typeinfo for google::protobuf::Message'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTIN11third_party10tensorflow5tools3api11TFAPIModuleE[_ZTIN11third_party10tensorflow5tools3api11TFAPIModuleE]+0x10): undefined reference to `typeinfo for google::protobuf::Message'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTIN11third_party10tensorflow5tools3api10TFAPIClassE[_ZTIN11third_party10tensorflow5tools3api10TFAPIClassE]+0x10): undefined reference to `typeinfo for google::protobuf::Message'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTIN11third_party10tensorflow5tools3api11TFAPIObjectE[_ZTIN11third_party10tensorflow5tools3api11TFAPIObjectE]+0x10): undefined reference to `typeinfo for google::protobuf::Message'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIMemberE[_ZTVN11third_party10tensorflow5tools3api11TFAPIMemberE]+0x20): undefined reference to `google::protobuf::Message::GetTypeName[abi:cxx11]() const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIMemberE[_ZTVN11third_party10tensorflow5tools3api11TFAPIMemberE]+0x58): undefined reference to `google::protobuf::Message::InitializationErrorString[abi:cxx11]() const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIMemberE[_ZTVN11third_party10tensorflow5tools3api11TFAPIMemberE]+0x60): undefined reference to `google::protobuf::Message::CheckTypeAndMergeFrom(google::protobuf::MessageLite const&)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIMemberE[_ZTVN11third_party10tensorflow5tools3api11TFAPIMemberE]+0x80): undefined reference to `google::protobuf::MessageLite::SerializeWithCachedSizesToArray(unsigned char*) const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIMemberE[_ZTVN11third_party10tensorflow5tools3api11TFAPIMemberE]+0xb0): undefined reference to `google::protobuf::Message::DiscardUnknownFields()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIMemberE[_ZTVN11third_party10tensorflow5tools3api11TFAPIMemberE]+0xb8): undefined reference to `google::protobuf::Message::SpaceUsedLong() const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIMethodE[_ZTVN11third_party10tensorflow5tools3api11TFAPIMethodE]+0x20): undefined reference to `google::protobuf::Message::GetTypeName[abi:cxx11]() const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIMethodE[_ZTVN11third_party10tensorflow5tools3api11TFAPIMethodE]+0x58): undefined reference to `google::protobuf::Message::InitializationErrorString[abi:cxx11]() const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIMethodE[_ZTVN11third_party10tensorflow5tools3api11TFAPIMethodE]+0x60): undefined reference to `google::protobuf::Message::CheckTypeAndMergeFrom(google::protobuf::MessageLite const&)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIMethodE[_ZTVN11third_party10tensorflow5tools3api11TFAPIMethodE]+0x80): undefined reference to `google::protobuf::MessageLite::SerializeWithCachedSizesToArray(unsigned char*) const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIMethodE[_ZTVN11third_party10tensorflow5tools3api11TFAPIMethodE]+0xb0): undefined reference to `google::protobuf::Message::DiscardUnknownFields()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIMethodE[_ZTVN11third_party10tensorflow5tools3api11TFAPIMethodE]+0xb8): undefined reference to `google::protobuf::Message::SpaceUsedLong() const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIModuleE[_ZTVN11third_party10tensorflow5tools3api11TFAPIModuleE]+0x20): undefined reference to `google::protobuf::Message::GetTypeName[abi:cxx11]() const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIModuleE[_ZTVN11third_party10tensorflow5tools3api11TFAPIModuleE]+0x58): undefined reference to `google::protobuf::Message::InitializationErrorString[abi:cxx11]() const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIModuleE[_ZTVN11third_party10tensorflow5tools3api11TFAPIModuleE]+0x60): undefined reference to `google::protobuf::Message::CheckTypeAndMergeFrom(google::protobuf::MessageLite const&)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIModuleE[_ZTVN11third_party10tensorflow5tools3api11TFAPIModuleE]+0x80): undefined reference to `google::protobuf::MessageLite::SerializeWithCachedSizesToArray(unsigned char*) const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIModuleE[_ZTVN11third_party10tensorflow5tools3api11TFAPIModuleE]+0xb0): undefined reference to `google::protobuf::Message::DiscardUnknownFields()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIModuleE[_ZTVN11third_party10tensorflow5tools3api11TFAPIModuleE]+0xb8): undefined reference to `google::protobuf::Message::SpaceUsedLong() const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api10TFAPIClassE[_ZTVN11third_party10tensorflow5tools3api10TFAPIClassE]+0x20): undefined reference to `google::protobuf::Message::GetTypeName[abi:cxx11]() const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api10TFAPIClassE[_ZTVN11third_party10tensorflow5tools3api10TFAPIClassE]+0x58): undefined reference to `google::protobuf::Message::InitializationErrorString[abi:cxx11]() const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api10TFAPIClassE[_ZTVN11third_party10tensorflow5tools3api10TFAPIClassE]+0x60): undefined reference to `google::protobuf::Message::CheckTypeAndMergeFrom(google::protobuf::MessageLite const&)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api10TFAPIClassE[_ZTVN11third_party10tensorflow5tools3api10TFAPIClassE]+0x80): undefined reference to `google::protobuf::MessageLite::SerializeWithCachedSizesToArray(unsigned char*) const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api10TFAPIClassE[_ZTVN11third_party10tensorflow5tools3api10TFAPIClassE]+0xb0): undefined reference to `google::protobuf::Message::DiscardUnknownFields()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api10TFAPIClassE[_ZTVN11third_party10tensorflow5tools3api10TFAPIClassE]+0xb8): undefined reference to `google::protobuf::Message::SpaceUsedLong() const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIObjectE[_ZTVN11third_party10tensorflow5tools3api11TFAPIObjectE]+0x20): undefined reference to `google::protobuf::Message::GetTypeName[abi:cxx11]() const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIObjectE[_ZTVN11third_party10tensorflow5tools3api11TFAPIObjectE]+0x58): undefined reference to `google::protobuf::Message::InitializationErrorString[abi:cxx11]() const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIObjectE[_ZTVN11third_party10tensorflow5tools3api11TFAPIObjectE]+0x60): undefined reference to `google::protobuf::Message::CheckTypeAndMergeFrom(google::protobuf::MessageLite const&)'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIObjectE[_ZTVN11third_party10tensorflow5tools3api11TFAPIObjectE]+0x80): undefined reference to `google::protobuf::MessageLite::SerializeWithCachedSizesToArray(unsigned char*) const'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIObjectE[_ZTVN11third_party10tensorflow5tools3api11TFAPIObjectE]+0xb0): undefined reference to `google::protobuf::Message::DiscardUnknownFields()'
bazel-out/k8-py3-opt/bin/tensorflow/tools/api/lib/_objs/api_objects_proto_cc/tensorflow/tools/api/lib/api_objects.pb.pic.o:(.data.rel.ro._ZTVN11third_party10tensorflow5tools3api11TFAPIObjectE[_ZTVN11third_party10tensorflow5tools3api11TFAPIObjectE]+0xb8): undefined reference to `google::protobuf::Message::SpaceUsedLong() const'
collect2: error: ld returned 1 exit status
```"
18884,ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory,"ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

first,i have only installed cuda9.0,then,some days ago,it can easily run with it ,but today,it can't,and raise the error.i don't know how to fix it.please help...


![image](https://user-images.githubusercontent.com/33971685/39280811-fe4060cc-4933-11e8-83c2-62a7525920fc.png)

"
18880,[Feature Request] Dynamic RPC Address Resolution,"Currently TensorFlow cluster is created statically with host name/port information defined in cluster spec. The host name and port information for a worker can't be changed during run-time. In a multi-tenancy environment where host name/port is assigned to worker by a job manager after worker failover happens, this static cluster spec prevents other workers to communicate with this new worker using the latest host name and port.

With dynamic RPC address resolution feature, the host name of each TF server is resolved against an RPC address registry before initiating each RPC call. This allows changing the host name and port of each TF server at run time, which is useful in the following cases:

- A stateless TF worker fails over and gets assigned with new host name/port by an external job scheduler.

- A stateful TF worker fails over and all workers get restarted without changing the cluster spec. The failed worker can be assigned with new host name/port.

I have composed a design document [here](https://docs.google.com/document/d/1Fl-mMZ9NMBDaQkhhVpXe-UdYC-iYg6hlSj7bBZSMCSM/edit?usp=sharing). A prototype has been done as well based on this design. We hope to contribute this feature to the upstream. Any feedbacks for the design are welcome.

"
18879,"[Windows custom ops] Compiled c++ and imported dll successfully, but custom ops are not visible from python?","
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win 7 64
- **TensorFlow installed from (source or binary)**: sources
- **TensorFlow version (use command below)**: 1.3.1
- **Python version**: 2.7.13
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: Visual Studio 15
- **CUDA/cuDNN version**: 8.0/7.5
- **GPU model and memory**: NA
- **Exact command to reproduce**: Please see below.

### Describe the problem
Trying to build a library available at https://github.com/optas/latent_3d_points.

It has custom ops implemented in c++.
I have successfully built the package therein specifically `structural_losses` from https://github.com/optas/latent_3d_points/tree/master/external/structural_losses with changes for compiling with Visual Studio 2015.

In python, I am also able to load the ops dlls e.g. using 
`approxmatch_module = tf.load_op_library(osp.join(base_dir, '_approxmatch.pyd')).
`

However, I don't see the operators approxmatch_module.approx_match,approxmatch_module.match_cost, etc. from the c++ getting exposed to the python tensorflow.

I am getting the following error:
`AttributeError: 'module' object has no attribute 'approx_match'
`

When I do the following
```
from structural_losses import approxmatch
dir(approxmatch.approxmatch_module)
```
it lists only the following items
`['LIB_HANDLE', 'OP_LIST', '_InitOpDefLibrary', '__builtins__', '__doc__', '__name__', '__package__', '_collections', '_common_shapes', '_op_def_lib', '_op_def_library', '_op_def_pb2', '_op_def_registry', '_ops']
`
where the actual operations from c++ are missing.

What am I missing from proper compilation?

Or is it that in windows custom ops is `unavailable/not implemented`?"
18878,the files of events.out.tfevnts.***.net while running deep learning model,"Hi

During running a tensorflow model, I found that it automatically generates the file as the follows

```
events.out.tfevents.1524681518.xhl.net
events.out.tfevents.1524681541.xhl.net


```

What do these files stand for? Are there any ways to stop generating them. The files are saved along with model checkpoint file.

![capture](https://user-images.githubusercontent.com/5430158/39276576-42440a56-48af-11e8-9bed-715db940125e.JPG)
"
18877,Cannot use freeze_graph.py and optimize_for_inference.py with any model ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64
- **TensorFlow installed from (source or binary):**: PIP Python 3.6
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: .9.0
- **GPU model and memory**: 2GB
- **Exact command to reproduce**:

> py -m tensorflow.python.tools.freeze_graph --input_graph=output_graph.pb --input_checkpoint=C:/tmp/_retrain_checkpoint --output_graph=/tmp/frozen_graph.pb --output_node_names=final_result --input_binary=true

> py ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\Lib\site-packages\tensorflow\python\tools\optimize_for_inference.py"" --input=output_graph.pb --output=/tmp/optimazed_frozen_graph.pb --frozen_graph=true --input_names=""Placeholder"" --output_names ""final_result"" --inference_type=float

### Describe the problem

I have generated model with this tutorial for my own classes: https://www.tensorflow.org/tutorials/image_retraining

I cannot use generated .pb file with EmguCV or OpenCV, becouse of import error, it looks like some layers are unsupported,  so I was trying to run these scripts first:

freeze_graph.py

> py C:\tensorflow-master\tensorflow\python\tools\freeze_graph.py --input_graph=output_graph.pb --input_checkpoint=C:/tmp/_retrain_checkpoint --output_graph=/tmp/frozen_graph.pb --output_node_names=softmax --input_binary=true

I have an error:

Names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(""final_retrain_ops/biases/final_biases:0"", shape=(6,), dtype=float32)

Then optimize_for_inference.py 

Multiple errors 

> WARNING:tensorflow:Didn't find expected Conv2D input to 'module_apply_default/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/FusedBatchNorm'

Scripts doesn't work with any generated .pb file, even some I've downloaded from the Internet

It looks like im doing something wrong or these scripts doesn't work with model from tutorial. I there any solution for that?

"
18875,1.7.0 fails to compile on aarch64 with TensorRT support,"JetsonTX2 with JetPack 3.2 which has Cuda9, Cudnn7 and TensorRT 3.0. Here is the content in .tf_configure.bazelrc file:
```
build --action_env PYTHON_BIN_PATH=""/usr/bin/python""
build --action_env PYTHON_LIB_PATH=""/usr/local/lib/python2.7/dist-packages""
build --force_python=py2
build --host_force_python=py2
build --python_path=""/usr/bin/python""
build --define with_jemalloc=true
build --define with_gcp_support=true
build --define with_hdfs_support=true
build --define with_s3_support=true
build:kafka --define with_kafka_support=true
build:xla --define with_xla_support=true
build:gdr --define with_gdr_support=true
build:verbs --define with_verbs_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_CUDA=""1""
build --action_env CUDA_TOOLKIT_PATH=""/usr/local/cuda""
build --action_env TF_CUDA_VERSION=""9.0""
build --action_env CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env TENSORRT_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu""
build --action_env TF_TENSORRT_VERSION=""4.0.4""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""3.5,5.2""
build --action_env LD_LIBRARY_PATH=""/usr/local/cuda-9.0/lib64:""
build --action_env TF_CUDA_CLANG=""0""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
build --config=cuda
test --config=cuda
build --define grpc_no_ares=true
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
build --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
```
When building pip package:
```
.........................
ERROR: Skipping 'tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'build_defs.bzl': no such package '@local_config_tensorrt//': Traceback (most recent call last):
	File ""/home/nvidia/sourcecode/tensorflow/third_party/tensorrt/tensorrt_configure.bzl"", line 167
		_trt_lib_version(repository_ctx, trt_install_path)
	File ""/home/nvidia/sourcecode/tensorflow/third_party/tensorrt/tensorrt_configure.bzl"", line 77, in _trt_lib_version
		_find_trt_header_dir(repository_ctx, trt_install_path)
	File ""/home/nvidia/sourcecode/tensorflow/third_party/tensorrt/tensorrt_configure.bzl"", line 60, in _find_trt_header_dir
		str(repository_ctx.path((""%s/../incl...)
	File ""/home/nvidia/sourcecode/tensorflow/third_party/tensorrt/tensorrt_configure.bzl"", line 60, in str
		repository_ctx.path((""%s/../include"" % trt_install_path)).realpath
/usr/lib/include (No such file or directory)
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'build_defs.bzl': no such package '@local_config_tensorrt//': Traceback (most recent call last):
	File ""/home/nvidia/sourcecode/tensorflow/third_party/tensorrt/tensorrt_configure.bzl"", line 167
		_trt_lib_version(repository_ctx, trt_install_path)
	File ""/home/nvidia/sourcecode/tensorflow/third_party/tensorrt/tensorrt_configure.bzl"", line 77, in _trt_lib_version
		_find_trt_header_dir(repository_ctx, trt_install_path)
	File ""/home/nvidia/sourcecode/tensorflow/third_party/tensorrt/tensorrt_configure.bzl"", line 60, in _find_trt_header_dir
		str(repository_ctx.path((""%s/../incl...)
	File ""/home/nvidia/sourcecode/tensorflow/third_party/tensorrt/tensorrt_configure.bzl"", line 60, in str
		repository_ctx.path((""%s/../include"" % trt_install_path)).realpath
/usr/lib/include (No such file or directory)
```
Without TensorRT support it compiles well.
"
18874,Faulty numpy randomness when using GPU,"### System information
- Have I written custom code: Yes
- OS Platform and Distribution: Tested on Slackware Linux 14.2 and Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: tested on both 1.4 and 1.6
- Python version: 3.6
- CUDA/cuDNN version: 8.0 for TF 1.4 and 9.0 for TF 1.6
- Bazel version: N/A
- GPU model and memory: tested on GTX 960M and GTX 1080
- Exact command to reproduce: N/A


### Describe the problem
I am unable to reproduce random numbers generated from numpy when I use it in combination with TF. In the beginning of all my tests, I set
```
tf.set_random_seed(seed)
np.random.seed(seed)
```

I have been debugging, and when I use numpy and no TF, all results are reproducible. When I add the TF code, the random numbers stop being reproducible. When I use both TF and numpy, I get the following results:

1. TF variables are initialized to the same value every time (OK)
2. When I use `np.random.RandomState()` with a set seed instead of direct calls to `np.random.uniform()`, `np.random.normal()`, etc, results are reproducible (OK)
3. When I use direct calls to `np.random.uniform()`, `np.random.normal()`, etc, results are reproducible on CPU but not on GPU (NOT OK)
1080
Since the results are reproducible when using CPU but not GPU, it made me think that this might be a possible bug. 

I am not using any threads so the problem is definitely not caused by race conditions. I am monitoring reproducibility of results only by the random numbers which are generated, which are not in any way affected by the training results from the TF neural net. What is really strange is that the piece of code that seems to be affecting the results is the part about computing and backpropagating gradients. I do not expect that this uses any random numbers generated by numpy in the backend. Furthermore, even if it did, the order of my calls to `np.random` and to `sess.run` is always deterministic, so the same random numbers should be observed between separate runs. 

My code is somewhat too big at the moment to post. I can try to compile some simple example where the issue occurs, but I first wanted to make sure that this is indeed not the expected behavior.
"
18872,Feature Request: More Granular Dependencies for Official Binaries,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: `pip`
- **TensorFlow version (use command below)**: N/A
- **Python version**:  N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
(Note: This issue is related to #16046, but I feel that it's different enough to warrant a separate request, rather than resurrecting that one. Also, this question's wording is biased towards Linux, but the spirit of it may be useful for other systems as well.)

TensorFlow currently specifies its system requirements at the level of an operating system and CPU architecture (for example, the docs [currently state][ubuntu] that Ubuntu 16.04 x86_64 is the supported Linux variant). This is certainly helpful, but, when incorporating TensorFlow into larger projects with their own lists of dependencies, I feel that it may be useful to additionally have somewhere that provides a clear description of the dynamic library requirements for the official releases. For example, `glibc` version requirements have burned a number of developers (a quick search shows #14208 and #15376). Currently, the only reliable way (to my knowledge) to determine the required versions of `glibc` and friends for a given TensorFlow release is to run `ldd -v` on it (the following are run on TensorFlow 1.5.0, which is what I happen to have installed on this machine):

```bash
(venv) belph@alpha ~/venv $ ldd -v \
    lib/python3.5/site-packages/tensorflow/libtensorflow_framework.so \
    | grep -oP '\([A-Z][A-Z0-9._]+\)' \
    | sort -V \
    | uniq
(CXXABI_1.3)
(CXXABI_1.3.5)
(CXXABI_1.3.7)
(GCC_3.0)
(GCC_3.3)
(GCC_4.2.0)
(GLIBCXX_3.4)
(GLIBCXX_3.4.9)
(GLIBCXX_3.4.11)
(GLIBCXX_3.4.14)
(GLIBCXX_3.4.17)
(GLIBCXX_3.4.18)
(GLIBCXX_3.4.19)
(GLIBC_2.2.5)
(GLIBC_2.3)
(GLIBC_2.3.2)
(GLIBC_2.3.4)
(GLIBC_2.4)
(GLIBC_2.6)
(GLIBC_2.7)
(GLIBC_2.11)
(GLIBC_2.14)
(GLIBC_2.16)
(GLIBC_2.17)
(GLIBC_2.18)
(GLIBC_PRIVATE)
```
(here's a fancier command to get the latest version required, in case anyone is interested)
```bash
(venv) belph@alpha ~/venv $ ldd -v \
    lib/python3.5/site-packages/tensorflow/libtensorflow_framework.so \
    | grep -oP '[A-Z][A-Z0-9.]*_[0-9.]+(?=\) =>)' \
    | sort -V \
    | uniq \
    | tr '_' ' ' \
    | awk '{ reqs[$1] = $2 } END { for (lib in reqs) { print lib,reqs[lib] } }'
GLIBCXX 3.4.19
CXXABI 1.3.7
GLIBC 2.18
GCC 4.2.0
```

I _have_ noticed the [Tested Source Configurations][srcconfigs] list on the documentation; these very well may be the machines which compile the binaries, but they do not specify which version of `glibc` is installed. Additionally, `glibc` version 2.19 is [specified in a toolchain][toolchain], but it is unclear whether this is actually used when compiling the official distributions (or I just missed it).

Would it be possible to specify what versions of shared libraries are required to be installed, or, at a minimum, some more detailed information about the system each binary is compiled on? The latter may be specified somewhere already, but I have not been able to find it. I'm not necessarily asking for a library requirement list that TensorFlow commits to maintain compatibility with across releases; instead, I just think it would be useful to be able to have a go-to place to answer the question of ""what version of `glibc` do I need to run the official TensorFlow 1.5 build?"" Anything would be helpful!

Thank you! Apologies if anything I've said is unclear; I'll be happy to clarify.

### Source code / logs
N/A

[ubuntu]: https://www.tensorflow.org/install/install_linux#top_of_page
[srcconfigs]: https://www.tensorflow.org/install/install_sources#tested_source_configurations
[toolchain]: https://github.com/tensorflow/tensorflow/blob/d33f80f4083ef63b90b5763988541e901e7f0a3d/third_party/toolchains/clang6/CROSSTOOL.tpl#L13"
18871,"Tensorflow not working inside spyder IDE (Anaconda on Windows), even after activating it through command prompt .","### System information
Windows 10 , Anaconda 3.6, 64 bit
Anaconda installed using anaconda page https://www.anaconda.com/download/#windows
Python version : 3.6.5
tensorflow version : 3.6
- Tensorflow was installed using  command :  conda create -n tensorflow pip python=3.6 

### Describe the problem
I have installed the package tensorflow using conda. After installation and also activation I tried to use it in Spyder IDE but it was not working. In Spyder IDE,for command - import tensorflow as tf , it is throwing error : ModuleNotFoundError: No module named 'tensorflow'.


### Source code / logs
Command used on CMD prompt : conda create -n tfp3.6 phyton=3.6
activate tfp3.6
spyder

command used in Spyder IDE: import tensorflow as tf
error : ModuleNotFoundError: No module named 'tensorflow'

"
18865,use freeze_graph and report TypeError: main() takes exactly 1 argument (0 given),"ubuntu@vm:~/code/tensorflow/tensorflow$ freeze_graph \
> --input_graph=/tmp/mobilenet_v1_1.0_224.pb \
> --input_meta_graph=/tmp/mobilenet_v1_1.0_224.ckpt.meta \
> --input_binary=true \
> --input_checkpoint=/tmp/checkpoints/mobilenet_v1_1.0_224.ckpt \
> --output_node_names=MobileNetV1/Predictions/Reshape_1 \
> --output_graph=/tmp/mobilenet_v1_1.0_224_frozen.pb
/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""/usr/local/bin/freeze_graph"", line 11, in <module>
    sys.exit(main())
TypeError: main() takes exactly 1 argument (0 given)"
18863,[Feature Request] Fold batch_norm with depthwise_conv transformation graph,"Hi,

I think it could be interesting to have a transformation graph function to fold batch_norms into depthwise_conv like with standard conv2d [like here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#fold_batch_norms)."
18862,failure when using Dataset to build a model with dilation convolutional layers,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution **:  Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I happened to find this issue when I tried to add dilation convolution to my model. Here I give basic steps to reproduce the issue.

First, I use placeholder to create a tiny network.
```
# [batch, 1, length, feat_dim]
input = tf.placeholder(tf.float32, shape=(128, 1, 100, 20), name='input')
label = tf.placeholder(tf.int64, shape=(128,), name='label')
input_value = np.random.normal(size=(128,1,100,20))
label_value = np.random.randint(9852, size=(128))

conv0 = tf.layers.conv2d(input,
                         512,
                         (1, 5),
                         dilation_rate=(1, 1),
                         activation=tf.nn.relu,
                         name='conv0')

conv1 = tf.layers.conv2d(conv0,
                         512,
                         (1, 3),
                         dilation_rate=(1, 2),
                         activation=tf.nn.relu,
                         name='conv1')
conv1_squeeze, _ = tf.nn.moments(tf.squeeze(conv1, [1]), axes=[1])
logits = tf.layers.dense(conv1_squeeze, 9852, name='logits')
loss = tf.losses.sparse_softmax_cross_entropy(labels=label, logits=logits, scope=""loss"")
train_op = tf.train.AdamOptimizer(0.01).minimize(loss)

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    _ = sess.run(train_op, feed_dict={input:input_value, label:label_value})
    print(""Here we are"")
```
I add two conv layers and the second one uses a dilation convolution with rate=2. It is actually 1d conv, though I use conv2d here. A mean pooling is performed before the fully connected layer. It works.

Then I introduce the Dataset to build to the input pipeline:
```
def _parse_tfrecord(example_proto):
    dics = {'input': tf.FixedLenFeature(shape=(), dtype=tf.string),
            'input_shape': tf.FixedLenFeature(shape=(2,), dtype=tf.int64),
            'output': tf.FixedLenFeature(shape=(), dtype=tf.int64)}
    parsed_example = tf.parse_single_example(example_proto, dics)
    # the dtype of feature is 'float32'
    parsed_example['input'] = tf.decode_raw(parsed_example['input'], tf.float32)
    parsed_example['input'] = tf.reshape(parsed_example['input'], parsed_example['input_shape'])
    return parsed_example

def create_variable_dataset(filenames, batch_size, feat_dim):
    dataset = tf.data.Dataset.from_tensor_slices(filenames)
    dataset = dataset.interleave(lambda filename:
                               tf.data.TFRecordDataset(filename).map(
                                   _parse_tfrecord, num_parallel_calls=8).padded_batch(
                                       batch_size,
                                       padded_shapes=({'input': [None, feat_dim], 'input_shape': [2], 'output': []})),
                                cycle_length=len(filenames), block_length=1
                               )

    dataset = dataset.prefetch(5)
    itr = dataset.make_initializable_iterator()
    element = itr.get_next()
    return itr, element['input'], element['output']

train_itr, input, label = create_variable_train_dataset(['egs/egs.1.tfrecord'],
                                                                      batch_size=64,
                                                                      feat_dim=20)
input = tf.expand_dims(input, 1)
conv0 = tf.layers.conv2d(input,
                         512,
                         (1, 5),
                         dilation_rate=(1, 1),
                         activation=tf.nn.relu,
                         name='conv0')

conv1 = tf.layers.conv2d(conv0,
                         512,
                         (1, 3),
                         dilation_rate=(1, 2),
                         activation=tf.nn.relu,
                         name='conv1')

conv1_squeeze, _ = tf.nn.moments(tf.squeeze(conv1, [1]), axes=[1])
logits = tf.layers.dense(conv1_squeeze, 9852, name='logits')
loss = tf.losses.sparse_softmax_cross_entropy(labels=label, logits=logits, scope=""loss"")
train_op = tf.train.AdamOptimizer(0.01).minimize(loss)

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    sess.run(train_itr.initializer)
    _ = sess.run(train_op)
    print(""Here we are"")
```
I load to the tfrecords which are made before. The data loaded has size [length, feat_dim]. With batch, it becomes [batch, length, feat_dim]. And it also works.

Now, I slightly change the input pipeline to
```
def create_variable_train_dataset(filenames, batch_size, feat_dim, shuffle_size=-1):
    dataset = tf.data.Dataset.from_tensor_slices(filenames)
    dataset = dataset.interleave(lambda filename:
                               tf.data.TFRecordDataset(filename).map(
                                   _parse_tfrecord, num_parallel_calls=8).apply(
                                   tf.contrib.data.padded_batch_and_drop_remainder(
                                       batch_size,
                                       padded_shapes=({'input': [None, feat_dim], 'input_shape': [2], 'output': []}))),
                                cycle_length=len(filenames), block_length=1
                               )

    dataset = dataset.prefetch(5)
    itr = dataset.make_initializable_iterator()
    element = itr.get_next()
    return itr, element['input'], element['output']
```
It breaks and throws the exception:
```
2018-04-25 22:07:04.779657: I C:\tf_jenkins\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1350, in _do_call
    return fn(*args)
  File ""C:\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1329, in _run_fn
    status, run_metadata)
  File ""C:\Python36\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: padded_shape[1]=97 is not divisible by block_shape[1]=2
	 [[Node: conv1/SpaceToBatchND = SpaceToBatchND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tpaddings=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](conv0/Relu, conv1/SpaceToBatchND/block_shape, conv1/strided_slice_2)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files (x86)\JetBrains\PyCharm Community Edition 2016.1\helpers\pydev\pydevd.py"", line 1530, in <module>
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""C:\Program Files (x86)\JetBrains\PyCharm Community Edition 2016.1\helpers\pydev\pydevd.py"", line 937, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""C:\Program Files (x86)\JetBrains\PyCharm Community Edition 2016.1\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""G:/kaldi-master/egs/fisher/v3/xvector/train_model.py"", line 3, in <module>
    from xvector_train import *
  File ""G:/kaldi-master/egs/fisher/v3/xvector\xvector_train.py"", line 48, in <module>
    _ = sess.run(train_op)
  File ""C:\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 895, in run
    run_metadata_ptr)
  File ""C:\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1344, in _do_run
    options, run_metadata)
  File ""C:\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1363, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: padded_shape[1]=97 is not divisible by block_shape[1]=2
	 [[Node: conv1/SpaceToBatchND = SpaceToBatchND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tpaddings=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](conv0/Relu, conv1/SpaceToBatchND/block_shape, conv1/strided_slice_2)]]

Caused by op 'conv1/SpaceToBatchND', defined at:
  File ""C:\Program Files (x86)\JetBrains\PyCharm Community Edition 2016.1\helpers\pydev\pydevd.py"", line 1530, in <module>
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""C:\Program Files (x86)\JetBrains\PyCharm Community Edition 2016.1\helpers\pydev\pydevd.py"", line 937, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""C:\Program Files (x86)\JetBrains\PyCharm Community Edition 2016.1\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""G:/kaldi-master/egs/fisher/v3/xvector/train_model.py"", line 3, in <module>
    from xvector_train import *
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""G:/kaldi-master/egs/fisher/v3/xvector\xvector_train.py"", line 36, in <module>
    name='conv1')
  File ""C:\Python36\lib\site-packages\tensorflow\python\layers\convolutional.py"", line 614, in conv2d
    return layer.apply(inputs)
  File ""C:\Python36\lib\site-packages\tensorflow\python\layers\base.py"", line 762, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""C:\Python36\lib\site-packages\tensorflow\python\layers\base.py"", line 652, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""C:\Python36\lib\site-packages\tensorflow\python\layers\convolutional.py"", line 167, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File ""C:\Python36\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 838, in __call__
    return self.conv_op(inp, filter)
  File ""C:\Python36\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 502, in __call__
    return self.call(inp, filter)
  File ""C:\Python36\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 493, in _with_space_to_batch_call
    paddings=paddings)
  File ""C:\Python36\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 6670, in space_to_batch_nd
    paddings=paddings, name=name)
  File ""C:\Python36\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""C:\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): padded_shape[1]=97 is not divisible by block_shape[1]=2
	 [[Node: conv1/SpaceToBatchND = SpaceToBatchND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tpaddings=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](conv0/Relu, conv1/SpaceToBatchND/block_shape, conv1/strided_slice_2)]]
```
All the examples in the tfrecords have the size [100, 20].The only thing changed is the batch function ""padded_batch_and_drop_remainder"". I don't know why it tell me the dilation operation cannot be performed because padding should be used in the operation. I felt stranger that if I change the dilation rate of the first layer (which is 1 now) to (1, 2), it works again!
```
conv0 = tf.layers.conv2d(input,
                         512,
                         (1, 5),
                         dilation_rate=(1, 2),
                         activation=tf.nn.relu,
                         name='conv0')
```
Could anyone tell me what is going on here? Is anything wrong with the pipeline, or it is a bug ?
I use TF 1.5.0 in a server."
18861,Running two Models in two GPUs for prediction in c++,"### System information
- **Have I written custom code  (as opposed to using a stock example script provided in TensorFlow): N/A
- **OS Platform and Distribution (e.g., Debian 9.1):
- **TensorFlow installed from (source bazel):
- **TensorFlow version (use command below) 1.7:
- **Python version 3.6: 
- **Bazel version (if compiling from source) 0.11:
- **GCC/Compiler version (if compiling from source GCC)6.3:
- **CUDA/cuDNN version 9.1. cudnn 7.1:
- **GPU model and memory 2 GPU GTX 1080 TI, 11GB:
- **Exact command to reproduce: compiling by c++  


### Describe the problem
   i would like to use two gpus at the same time for make a preduction of two models by using this of code: session_options.config.mutable_gpu_options()->set_visible_device_list(""0""); unfortunatlly i got this error (Invalid allocator type: 0):
2018-04-25 14:35:17.895671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2018-04-25 14:35:17.895717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-04-25 14:35:17.895722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
2018-04-25 14:35:17.895727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
2018-04-25 14:35:17.895961: E tensorflow/core/common_runtime/gpu/process_state.cc:125] Invalid allocator type: 0
2018-04-25 14:35:17.895977: E tensorflow/core/common_runtime/direct_session.cc:167] Internal: Failed to get memory allocator for TF GPU 0 with 10913103872 bytes of memory.

### Source code / logs
    session_options.config.mutable_gpu_options()->set_visible_device_list(""0"");   --->error is here
    session_options.config.mutable_gpu_options()->set_allow_growth(true);
    session.reset(tensorflow::NewSession(tensorflow::SessionOptions(session_options)));

"
18859,trying to retrain my CNN model in digits recognition,
18858,How to show the loss curve of training set and validation set at the same time using the customed estimator?,"Hi, recently I used [custom_estimator.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/get_started/regression/custom_regression.py) to build regression model.  In order to clear out the changes of loss value in the training set and validation set. I need to know that how to show the loss curve of training and validation set at the same time. I tried to use train_and_evaluate api of estimator and i got the following picture.
![image](https://user-images.githubusercontent.com/19348442/39224762-76df9b92-487b-11e8-888d-b64825e99c82.png)
As it show, the result of evaluation is a point, but i want a line like the loss curve of training set. Just like the picture as shown below.
![image](https://user-images.githubusercontent.com/19348442/39280146-2d129f22-4930-11e8-81e7-a2379b924eb0.png)
Here is my system information:

- Have i written custom code: N/A

- OS: Tested on windows 10 1709.

- Tensorflow installed from Anaconda 5.1.0 with python 3.6.4

- Tensorflow version-tested on tensorflow-gpu 1.7.0

- CUDA/cuDNN version: 9.0 for TF 1.7

- GPU mode: Nvidia Quadro  K2100M， 2G of memory

- Bazel version: N/A

- Exact command to reproduce: N/A
Here is the customed estimator:

```
def my_dnn_regression_fn(features, labels, mode, params):
    top = tf.feature_column.input_layer(features, params['feature_columns'])

    for units in params.get('hidden_units', [20]):
        top = tf.layers.dense(inputs=top, units=units, activation=tf.nn.relu)

    output_layer = tf.layers.dense(inputs=top, units=1)

    output_layer = tf.cast(output_layer, tf.float64)
   
    predictions = tf.squeeze(output_layer, 1)

    if mode == tf.estimator.ModeKeys.PREDICT:
        # In 'PREDICT' mode we only need to return predictions.
        return tf.estimator.EstimatorSpec(
            mode=mode, predictions={""predictions"": predictions})

    # calculate the loss using mean squared error
    average_loss = tf.losses.mean_squared_error(labels, predictions)

    # Pre-made estimators use the total_loss instead of the average,
    # so report total_loss for compatibility.
    batch_size = tf.shape(labels)[0]
    total_loss = tf.to_float(batch_size) * average_loss

    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = params.get(""optimizer"", tf.train.AdamOptimizer)
        optimizer = optimizer(params.get(""learning_rate"", None))
        train_op = optimizer.minimize(
            loss=average_loss, global_step=tf.train.get_global_step())
        return tf.estimator.EstimatorSpec(
            mode=mode, loss=total_loss, train_op=train_op)

    # In the evaluation mode we will calculate evaluation metrics.
    assert mode == tf.estimator.ModeKeys.EVAL

    # Calculate root mean squared error
    rmse = tf.metrics.root_mean_squared_error(labels, predictions)

    # Add the rmse to collection of evaluation metrics.
    eval_metrics = {""rmse"": rmse}

    return tf.estimator.EstimatorSpec(
        mode=mode,
        # Report sum of error for compatibility with pre-made estimators.
        loss=total_loss,
        eval_metric_ops=eval_metrics)
````

And here I used train_and_evaluate api like this:
```
    model = tf.estimator.Estimator(
        model_fn=my_dnn_regression_fn,
        model_dir=
        ""./models/temp"",
        params={
            'feature_columns': feature_columns,
            'learning_rate': 0.1,
            'optimizer': tf.train.AdamOptimizer,
            'hidden_units': [20, 20, 20, 20]
        })
    train_spec = tf.estimator.TrainSpec(input_fn=input_train,max_steps=10000)
    eval_spec = tf.estimator.EvalSpec(input_fn=input_dev,steps=10000,throttle_secs=60,start_delay_secs=0)
    tf.estimator.train_and_evaluate(model, train_spec, eval_spec)
```
Did I set the parameter properly? Or, is there other solution for this?
"
18857,Error when using TF-Lite visualizer to create the HTML file from a TF-Lite model,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.7
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**: 4.9
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: 16GB
- **Exact command to reproduce**:

### Describe the problem
1. I am trying to use TF-Lite visualization tool to create a graph image on the toco quantized model. However, it dose not work. 
2. I am also tried to use visualize.py to create the html file, but it can not create the corresponding JSON file of the model. It seems that the binary file of ""flatc"" cannot be found.

### Source code / logs
1. using toco command line tools to converting and quantizing TF model to TF-Lite model
2. using bazel to build and run TF-Lite visualizer
> bazel run tensorflow/contrib/lite/tools:visualize -- quant_mobilenet_v1_infer.tflite quant_mobilenet_v1_infer.html

However, I got error info:
> RuntimeError: Invalid filename 'quant_mobilenet_v1_infer.tflite'
> ERROR: Non-zero return code '1' from command: Process exited with status 1

which is caused by CreateHtmlFile function in visualize.py

"
18856,No module named tensorflow.python.platform,"environment: ubuntu 1604, python 2.7

```
sudo pip install -i https://pypi.tuna.tsinghua.edu.cn/simple/   https://mirrors.tuna.tsinghua.edu.cn/tensorflow/linux/cpu/tensorflow-1.7.0-cp27-none-linux_x86_64.whl
```
there is error, but why???
```
ubuntu@vm:~/code/tensorflow$ python tensorflow/python/tools/freeze_graph.py --input_graph=/tmp/mobilenet_v1_1.0_224.pb --input_binary=true --output_graph=/tmp/frozen_mobilenet_v1_224.pb --output_node_names=MobilenetV1/Predictions/Reshape_1
Traceback (most recent call last):
  File ""tensorflow/python/tools/freeze_graph.py"", line 45, in <module>
    from tensorflow.core.framework import graph_pb2
  File ""/home/ubuntu/code/tensorflow/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/ubuntu/code/tensorflow/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/ubuntu/code/tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 25, in <module>
    from tensorflow.python.platform import self_check
ImportError: No module named platform
```
"
18853,The child thread can not use the main thread's graph? ,"I load model in the main thread, and passing it to child thread， but the child thread run error.
logs：
he name 'input:0' refers to a Tensor which does not exist. The operation, 'input', does not exist in the graph.""
"
18852,[Feature Request + WIP] Azure blob storage filesystem plugin,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.7.0-3-g024aecf414 1.7.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.12.0-homebrew
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.1.0 (clang-902.0.39.1)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
Opening this issue to mark work I've been doing on implementing a file system for azure blob storage to complement gcs and s3. Opening so that if others are interested or want to test can do so. Also haven't created a WIP PR as still heavily testing and cleaning up. 

Imitates the GCS implementation closely and uses curl and boringssl libraries that are already included in part of the build.

### Source code / logs
Current work that will lead to a PR: https://github.com/damienpontifex/tensorflow/tree/az-storage"
18850,Mac OS X Python 3.5 binary runtime warning for Tensorflow 1.5.0+,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13.4
- **TensorFlow installed from (source or binary)**: binary (or whatever pip is)
- **TensorFlow version (use command below)**: 1.5.0+
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: 8GB CPU
- **Exact command to reproduce**: `python -c ""import tensorflow""`

### Describe the problem

I seem to have the opposite problem of [https://github.com/tensorflow/tensorflow/issues/14182](https://github.com/tensorflow/tensorflow/issues/14182)

I'm using `Python 3.5`, but when I try to use a Tensorflow version 1.5 or above with the command

    python -c ""import tensorflow""

I get a `RuntimeWarning` saying:

    /Users/<user>/.pyenv/versions/3.5.3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: 
    compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match 
    runtime version 3.5
      return f(*args, **kwds)

When I install using `pip install tensorflow==1.5.0`, it installs from `https://files.pythonhosted.org/packages/b1/57/1d27695a4572d70b8cd365dd358b45a41ece811d675910e175254779885e/tensorflow-1.5.0-cp35-cp35m-macosx_10_11_x86_64.whl`

Installing with `pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.5.0-py3-none-any.whl` doesn't make the warning go away either.


I get this warning for any Tensorflow version 1.5.0 and above. Incidentally, 1.5.0 is where the runtime binary for Python 3.6 was fixed, so I'm wondering if this is related. I have no issues when installing Tensorflow 1.5.0+ on Ubuntu 16.04 with `pip install tensorflow` with Python 3.5.2 so it seems to be a Mac OS X problem.

### Source code / logs

I get the warning whenever I run `import tensorflow`"
18849,CUPTI events are missing from tf.train.Server,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.7, r1.6, r1.5
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: 9.1/7.1.3, 9.0/7.0.5
- **GPU model and memory**: K80/V100/GTX TITAN
- **Exact command to reproduce**: See the description

### Describe the problem
I am collecting runtime tracing using `tf.RunOptions.FULL_TRACE`. When a direct session is used (`master=None`), CUPTI events (`/stream:*` devices) are included in `tf.RunMetadata`. However, a remote (grpc) session does not have any. Is there a particular reason behind it?

For example the following script has the CUPTI:
```python
import tensorflow as tf
import tensorflow.contrib.slim.nets as nets

model, _ = nets.resnet_v2.resnet_v2_50(tf.random_uniform([16,299,299,3]))

with tf.train.MonitoredTrainingSession() as sess:
     run_metadata = tf.RunMetadata()
     sess.run(model.op, run_metadata=run_metadata, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE))

print([d.device for d in run_metadata.step_stats.dev_stats])
assert any([""stream:all"" in d.device for d in run_metadata.step_stats.dev_stats])
```
The example output on my device is:
```
['/device:GPU:0/stream:7', '/device:GPU:0/memcpy', '/device:GPU:0/stream:25', '/device:GPU:0/stream:24', '/device:GPU:0/stream:13', '/job:localhost/replica:0/task:0/device:GPU:0', '/device:GPU:0/stream:18', '/device:GPU:0/stream:all', '/device:GPU:0/stream:23', '/device:GPU:0/stream:31', '/device:GPU:0/stream:20', '/device:GPU:0/stream:22', '/device:GPU:0/stream:19', '/device:GPU:0/stream:21']
```

However, the following example does not have any CUPTI events:
```python
import tensorflow as tf
import tensorflow.contrib.slim.nets as nets

server = tf.train.Server(tf.train.ClusterSpec({""worker"":[""localhost:2222""]}), ""worker"", 0)
model, _ = nets.resnet_v2.resnet_v2_50(tf.random_uniform([16,299,299,3]))

with tf.train.MonitoredTrainingSession(master=""grpc://localhost:2222"") as sess:
     run_metadata = tf.RunMetadata()
     sess.run(model.op, run_metadata=run_metadata, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE))

print([d.device for d in run_metadata.step_stats.dev_stats])
assert any([""stream:all"" in d.device for d in run_metadata.step_stats.dev_stats])
```

The following is the output on my device:
```
['/job:worker/replica:0/task:0/device:GPU:0']
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AssertionError
```

I have tried this with multiple version of TensorFlow (r1.7, r1.6, r1.5) and both `grpc_tensorflow_server` and `tf.train.Server`.
"
18847,_session->Run() crushes at Eigen::internal::handmade_aligned_free,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, a slightly modified version of label_image
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: built from source using tensorflow/contrib/makefile
- **TensorFlow version (use command below)**: r1.7
- **Python version**:  N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**:  5.4.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

I modified contrib/makefile/Makefile and tf_op_files.txt to use machine generated ops (built from Bazel) for my customized libtensorflow_core.a. I have two graphs in my code, one was created using C++ API for input image pre-processing, and the other one was loaded from pb model file. My C++ project ran fine for the first graph and crashed at _session->Run() for the second graph with segmentation fault.  

When linking my project against libtensorflow_cc.so and libtensorflow_framework.so built from Bazel, my code runs fine with no problem. 


### Source code / logs

The code stopped at the first call of conv2d() at line 422 of conv_ops.cc

`    launcher_(context, use_cudnn_, cudnn_use_autotune_, input, filter,
              dilation_rows, dilation_cols, stride_rows, stride_cols, padding_,
              output, data_format_);`

with the following backtrace of the entire stack. This looks like a double free problem? or Should I compile the Makefile with some flag settings for Eigen? 

> 1  __GI___libc_free                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  malloc.c                      2951 0x7fffef463512 
2  Eigen::internal::handmade_aligned_free                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Memory.h                      98   0x7ffff69fa6f3 
3  Eigen::internal::aligned_free                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Memory.h                      179  0x7ffff69fa6f3 
4  Eigen::TensorEvaluator<Eigen::TensorContractionOp<std::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, std::array<long, 1ul>, std::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 24, 8, 0, false, false>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, std::array<long, 1ul>, std::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 24, 4, false, false>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, std::array<long, 1ul>, std::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, std::array<long, 1ul>, std::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<float, long, 0, 0>>::~Context TensorContractionThreadPool.h 402  0x7ffff69fa6f3 
5  Eigen::TensorEvaluator<Eigen::TensorContractionOp<std::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::evalProduct<true, true, false, 0>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        TensorContractionThreadPool.h 310  0x7ffff69fa6f3 
6  Eigen::TensorContractionEvaluatorBase<Eigen::TensorEvaluator<Eigen::TensorContractionOp<std::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>>::evalTo                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            TensorContraction.h           418  0x7ffff69fbd55 
7  Eigen::TensorContractionEvaluatorBase<Eigen::TensorEvaluator<Eigen::TensorContractionOp<std::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>>::evalSubExprsIfNeeded                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              TensorContraction.h           402  0x7ffff69fbd55 
8  Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 4> const, Eigen::TensorContractionOp<std::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const> const, Eigen::ThreadPoolDevice>::evalSubExprsIfNeeded                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       TensorMorphing.h              129  0x7ffff69fbd55 
9  Eigen::TensorEvaluator<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorReshapingOp<Eigen::DSizes<long, 4> const, Eigen::TensorContractionOp<std::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const> const> const, Eigen::ThreadPoolDevice>::evalSubExprsIfNeeded                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              TensorAssign.h                129  0x7ffff69fbd55 
10 Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorReshapingOp<Eigen::DSizes<long, 4> const, Eigen::TensorContractionOp<std::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const> const> const, Eigen::ThreadPoolDevice, true>::run                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                TensorExecutor.h              149  0x7ffff69fbd55 
11 Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::ThreadPoolDevice>::operator=<Eigen::TensorReshapingOp<Eigen::DSizes<long, 4> const, Eigen::TensorContractionOp<std::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const>>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               TensorDevice.h                35   0x7ffff69fbd55 
12 tensorflow::functor::SpatialConvolutionFunc<Eigen::ThreadPoolDevice, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              conv_2d.h                     60   0x7ffff69fbd55 
13 tensorflow::functor::SpatialConvolution<Eigen::ThreadPoolDevice, float>::operator()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               conv_2d.h                     72   0x7ffff69c2641 
14 tensorflow::(anonymous namespace)::LaunchGeneric<Eigen::ThreadPoolDevice, float>::operator()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      conv_ops.cc                   104  0x7ffff69c2641 
15 tensorflow::LaunchConv2DOp<Eigen::ThreadPoolDevice, float>::operator()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            conv_ops.cc                   126  0x7ffff6a2028d 
16 tensorflow::Conv2DOp<Eigen::ThreadPoolDevice, float>::Compute                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     conv_ops.cc                   422  0x7ffff6a2028d 
17 tensorflow::ThreadPoolDevice::Compute                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             threadpool_device.cc          60   0x7ffff4f70744 
18 tensorflow::(anonymous namespace)::ExecutorState::Process                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         executor.cc                   1658 0x7ffff4f1e212 
19 std::_Mem_fn_base<void (tensorflow::(anonymous namespace)::ExecutorState:: *)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), true>::operator()<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode&, long long int&, void>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    functional                    600  0x7ffff4f0bd05 
20 std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState:: *)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>(tensorflow::(anonymous namespace)::ExecutorState *, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>::__call<void, 0ul, 1ul, 2ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  functional                    1074 0x7ffff4f0bd05 
21 std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState:: *)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>(tensorflow::(anonymous namespace)::ExecutorState *, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>::operator()<, void>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           functional                    1133 0x7ffff4f0bd05 
22 std::_Function_handler<void(), std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState:: *)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>(tensorflow::(anonymous namespace)::ExecutorState *, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>>::_M_invoke(const std::_Any_data &)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            functional                    1871 0x7ffff4f0bd05 
23 std::function<void ()>::operator()() const                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        functional                    2267 0x7ffff515db81 
24 tensorflow::thread::EigenEnvironment::ExecuteTask                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 threadpool.cc                 83   0x7ffff515db81 
25 Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NonBlockingThreadPool.h       232  0x7ffff515db81 
26 std::function<void ()>::operator()() const                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        functional                    2267 0x7ffff515b92f 
27 tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}::operator()() const                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      threadpool.cc                 56   0x7ffff515b92f 
28 std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       functional                    1871 0x7ffff515b92f 
29 ??                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   0x7fffefd80c80 
30 start_thread                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      pthread_create.c              333  0x7ffff00516ba 
31 clone                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             clone.S                       109  0x7fffef4e641d 
"
18841,"tensorflow/compiler/xla/statusor.h not included in installation, but required for compiling custom TensorFlow operators","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 27
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: v1.8.0-rc1-909-g968addadfd 1.8.0-rc1
- **Python version**: 3.6m
- **Bazel version (if compiling from source)**: 0.12.0
- **GCC/Compiler version (if compiling from source)**: 6.4
- **CUDA/cuDNN version**: 9.1/7.1
- **GPU model and memory**: GTX 1080 8GB
- **Exact command to reproduce**:
```
include Makefile.config
TF_INC = `$(PYTHON) -W ignore -c 'import tensorflow as tf; print(tf.sysconfig.get_include())'`
TF_CFLAGS = `$(PYTHON) -W ignore -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))'`
TF_LFLAGS = `$(PYTHON) -W ignore -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))'`
CUDA_INC = $(CUDA_HOME)/../
GPUCC     = nvcc -ccbin=$(CXX)
CFLAGS    = -std=c++11 $(TF_CFLAGS) -I. -I$(CUDA_INC) -I$(TF_INC)
GPUCFLAGS = -c
LFLAGS    = $(ODFLAGS) -pthread -fopenmp -shared -fPIC $(TF_LFLAGS)
GPULFLAGS = -x cu -Xcompiler ""$(CFLAGS) $(LFLAGS)"" --expt-relaxed-constexpr
GPUDEF    = -DGOOGLE_CUDA=1
CGPUFLAGS = -lcuda
SRC       = cart_hex_interpol.cc
GPUSRC    = cart_hex_interpol.cu.cc
SRC_O	  = cart_hex_interpol.o
GPUSRC_O  = cart_hex_interpol.cu.o
LIB       = cart_hex_interpol.so
all: gpu
default: gpu
cpu:
	$(CXX) $(CFLAGS) $(SRC) $(LFLAGS) -o $(LIB)
gpu:
	$(GPUCC) $(GPUDEF) $(CFLAGS) $(GPUCFLAGS) $(GPUSRC) $(GPULFLAGS) -o $(GPUSRC_O)
	$(CXX) $(GPUDEF) $(CFLAGS) $(SRC) $(GPUSRC_O) $(LFLAGS) $(CGPUFLAGS)  -o $(LIB)
clean:
	rm -f $(SRC_O) $(GPUSRC_O) $(LIB)
```

### Describe the problem
When compiling a custom TensorFlow operator using a Makefile and including CUDA/GPU code, the compilation fails due to missing `fatal error: tensorflow/compiler/xla/statusor.h: No such file or directory`. The files are missing in `/usr/local/lib/python3.6/site-packages/tensorflow/` because the headers do not get installed during installation of the wheel package for TensorFlow.
A temporary fix is to copy the headers from source:
```
sudo mkdir /usr/local/lib/python3.6/site-packages/tensorflow/include/tensorflow/compiler/
sudo mkdir /usr/local/lib/python3.6/site-packages/tensorflow/include/tensorflow/compiler/xla
sudo cp tensorflow/compiler/xla/*.h /usr/local/lib/python3.6/site-packages/tensorflow/include/tensorflow/compiler/xla/
```
This issue was not present in TensorFlow 1.5, but must have been introduced since then. Compiling with or without XLA makes no difference.

### Source code / logs
```
make gpu -j2
nvcc -ccbin=/usr/local/gcc-6.4/bin/g++-6.4 -DGOOGLE_CUDA=1 -std=c++11 `python3 -W ignore -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))'` -I. -I/usr/local/cuda/../ -I`python3 -W ignore -c 'import tensorflow as tf; print(tf.sysconfig.get_include())'` -c cart_hex_interpol.cu.cc -x cu -Xcompiler ""-std=c++11 `python3 -W ignore -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))'` -I. -I/usr/local/cuda/../ -I`python3 -W ignore -c 'import tensorflow as tf; print(tf.sysconfig.get_include())'` -O3 -pthread -fopenmp -shared -fPIC `python3 -W ignore -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))'`"" --expt-relaxed-constexpr -o cart_hex_interpol.cu.o
In file included from /usr/local/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/dso_loader.h:26:0,
                 from /usr/local/lib/python3.6/site-packages/tensorflow/include/tensorflow/core/platform/stream_executor.h:26,
                 from /usr/local/lib/python3.6/site-packages/tensorflow/include/tensorflow/core/util/cuda_launch_config.h:27,
                 from /usr/local/lib/python3.6/site-packages/tensorflow/include/tensorflow/core/util/cuda_kernel_helper.h:22,
                 from cart_hex_interpol.cu.cc:5:
/usr/local/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:21:46: fatal error: tensorflow/compiler/xla/statusor.h: No such file or directory
 #include ""tensorflow/compiler/xla/statusor.h""

```
"
18839,output 'tensorflow/core/kernels/_objs/gather_functor_gpu/tensorflow/core/kernels/gather_functor_gpu.cu.o' was not created,"When trying to build latest TensorFlow for CUDA 8.0/CuDNN 6.0
This is the error I'm facing when trying to execute the bazel build :<file> command.
following this error is,



not all outputs were created or valid
Target //tensorflow/loader:loader failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 237.164s, Critical Path: 30.79s
FAILED: Build did NOT complete successfully




trying to execute the loader.cc code from https://medium.com/jim-fleming/loading-a-tensorflow-graph-with-the-c-api-4caaff88463f
stuck at the second step in Compile & Run Section. (From inside the project folder call bazel build :loader)"
18835,Issue with building the Hexagon HVX DSP toolchain with TF,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04 
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.8r
- **Python version**:  2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**: 4.8.4
- **CUDA/cuDNN version**: 8
- **GPU model and memory**: GTX 1060
- **Exact command to reproduce**:

### Describe the problem

@satok16 @tensorflower-gardener , Could you please clarify the different version-combination of tools to use in an HVX-TF toolchain. It can save many people's effort to find and match these together. You have already mentioned in [HVX_TF](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx), that for `Qualcomm SDK 3.0`, the compatible nnlib version is [Aug-2017-commit](https://source.codeaurora.org/quic/hexagon_nn/nnlib/commit/?id=721b2d58f0f4e2d5b182f41e6b7c4db5356bf0fb). However, some features are missing there and we need to use the latest version with `SDK 3.3.3`, however, these libraries (`nnlib`, `libcontroller`, `TF-(sub)makefiles`) don't match, thus it has become a messy process to figure out which is related to what version. 

**1)** A sample error compiling libcontroller after compiling nnlib with latest SDK (3.3.3):

```
src_impl/hexagon_controller.c: In function 'hexagon_controller_AppendNode':
src_impl/hexagon_controller.c:484:70: error: 'hexagon_nn_output' has no member named 'max_size'
     pos += snprintf(&output_param_buf[pos], 500, ""(%d), "", outputs[i].max_size);
                                                                      ^
make[1]: *** [android_Release/hexagon_controller.o] Error 1
make[1]: Leaving directory `~/Qualcomm/Hexagon_SDK/3.3.3/examples/common/hexagon_controller'
make: *** [tree] Error 2

```
**2)** If I use your suggested commit and SDK 3.0, this is the output I am having:

```
native : hexagon_graph_execution_test.cc:129 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes
native : hexagon_graph_execution_test.cc:135 header size = 54
native : hexagon_graph_execution_test.cc:137 image size = 40
native : hexagon_graph_execution_test.cc:139 width = 299
native : hexagon_graph_execution_test.cc:141 height = -299
native : hexagon_graph_execution_test.cc:286 Ioading image finished.
native : hexagon_graph_execution_test.cc:185 Loading image finished.
native : hexagon_graph_execution_test.cc:189 Copy data to tensor.
native : hexagon_graph_execution_test.cc:307 Run graph
Init hexagon with max attributes (Controller version = 101)
native : hexagon_control_wrapper.cc:104 Add input: Mul, 0
native : hexagon_control_wrapper.cc:127 Allocate inout buffer
native : hexagon_control_wrapper.cc:304 Setup graph completed
Prepare failed! returned 0xffffffff

NN Id = -755923072
Execute graph!
Execution failed!
execute got err: -1

NN Id = -755923072
Execution failed
NN Id = -755923072
Failed to read data.
native : hexagon_graph_execution_test.cc:313 Output byte size = 4032
native : hexagon_graph_execution_test.cc:314 Output shape = [1,1008]
native : graph_transfer_utils.cc:47 === Dump ranking ===
native : graph_transfer_utils.cc:50 0: 1000, dumbbell, 0
native : graph_transfer_utils.cc:50 1: 999, carbonara, 0
native : graph_transfer_utils.cc:50 2: 998, stole, 0
native : graph_transfer_utils.cc:50 3: 997, rubber eraser, 0
native : graph_transfer_utils.cc:50 4: 996, coffee mug, 0
native : graph_transfer_utils.cc:50 5: 995, flagpole, 0
native : graph_transfer_utils.cc:50 6: 994, parallel bars, 0
native : graph_transfer_utils.cc:50 7: 993, cheeseburger, 0
native : graph_transfer_utils.cc:50 8: 992, bubble, 0
native : graph_transfer_utils.cc:50 9: 991, beaker, 0
Finalize hexagon
[       OK ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime (5848 ms)
[----------] 1 test from GraphTransferer (5848 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test case ran. (5849 ms total)
[  PASSED  ] 1 test.

  YOU HAVE 5 DISABLED TESTS
```

**3)**  Also in the [toturial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx), you did not mention anything about downloading/using an inception-v3 frozen-quantized model. If one follows the building each library from the source, where is the part related to use the [`tensorflow_inception_v3_stripped_optimized_quantized.pb`] ? Since, when I test with my custom quantized_frozen inception-v3 model, (renaming it to be the same as the original 2016 file (`tensorflow_inception_v3_stripped_optimized_quantized.pb`), I have the error output as:

```
...
native : hexagon_graph_execution_test.cc:533 Ioading image finished.
t1(loading image time)=0.026770
native : hexagon_graph_execution_test.cc:546 Build fused graph
native : remote_fused_graph_execute_utils.cc:259 Error during inference: Not found: FeedInputs: unable to find feed output Mul
native : graph_transfer_utils.cc:110 Check failed: status.ok()
Aborted

```
Thanks.



"
18834,feature request :plz add GPU support for float16 in tf.nn.lrn,"Have I written custom code yes
OS Platform and Distribution ubuntu16.04
TensorFlow installed from source
TensorFlow version:1.7
Bazel version 0.11.1
CUDA/cuDNN version 9
GPU model and memory TITAN Xp
Exact command to reproduce:N/A"
18832,Provide tf-nightly in PyPI for up-to-date python versions,"Hi, is there any reason that there are no up-to-date `tf-nightly` - Packages on PyPI?
https://pypi.org/simple/tf-nightly/

The latest `tf-nightly` version for Python 3.6 is `1.8.0.dev20180331`. 
Using Python 3.5 I get `1.9.0.dev20180423`.
This is annoying for users on Ubuntu 18.04, as there is only python 3.6

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0 nightly
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: None

"
18830,How to change the attributes of a variable that can be trained or not ？,"First, a model was obtained from the trainable variables. Then I needed to change the trainable variable to an untrainable variable and continue training.
Have any idea may be help? please help.Thanks"
18829,How to quantify ssd_mobilenet_v1_coco model and toco to .tflite ?,"## System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.8.0
- Python version:2.7.12
- Bazel version (if compiling from source): 0.12.0
- CUDA/cuDNN version: cuda-9.0/7.0
- GPU model and memory: GeForce GTX 1080/8105MiB
- Exact command to reproduce: N/A
- Phone: Moto G4 Play (need to use Camera, not Camera2)

Hi,  guys,
I am trying to quantify ssd_mobilenet_v1 using tensorflow object detection api, according to @coutner [post ](https://github.com/tensorflow/tensorflow/issues/18342).
First, I replace all the graph_hook_fn in
 [trainer.py](https://github.com/tensorflow/models/blob/b6bcc450b981eba721ee2760c92d87da86900988/research/object_detection/trainer.py) tf.contrib.quantize.create_training_graph and enable fused batch norm in [ssd_mobilenet_v1_feature_extractor.py](https://github.com/tensorflow/models/blob/b6bcc450b981eba721ee2760c92d87da86900988/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py#L114)
After training, I used the following script to transform the model.

1. when running optimize_for_inference, some warning had arisen.
```
$ bazel run -c opt tensorflow/python/tools/optimize_for_inference -- \
>     --input=$DETECT_PB --output=$STRIPPED_PB --frozen_graph=True \
>     --input_names=Preprocessor/sub --output_names=concat,concat_1 \
>     --toco_compatible=True \
>     --alsologtostderr
WARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/BatchNorm/FusedBatchNorm'
WARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/BatchNorm/FusedBatchNorm'
WARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/BatchNorm/FusedBatchNorm'
WARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/BatchNorm/FusedBatchNorm'
WARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/BatchNorm/FusedBatchNorm'
WARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/BatchNorm/FusedBatchNorm'
WARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/BatchNorm/FusedBatchNorm'
WARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/BatchNorm/FusedBatchNorm'
WARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/BatchNorm/FusedBatchNorm'
WARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/BatchNorm/FusedBatchNorm'
WARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/BatchNorm/FusedBatchNorm'
WARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/BatchNorm/FusedBatchNorm'
WARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/BatchNorm/FusedBatchNorm'
```
When Running toco,
```
$ bazel run tensorflow/contrib/lite/toco:toco -- 
>   --input_file=$STRIPPED_PB \
>   --output_file=$DETECT_FB \
>   --input_format=TENSORFLOW_GRAPHDEF \
>   --output_format=TFLITE \
>   --inference_type=QUANTIZED_UINT8 \
>   --input_shapes=1,300,300,3 \
>   --input_arrays=Preprocessor/sub \
>   --output_arrays=concat,concat_1 \
>   --std_values=128 \
>   --mean_values=127 \
>   --dump_graphviz=/tmp
2018-04-24 18:51:51.645315: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 253 operators, 450 arrays (0 quantized)
2018-04-24 18:51:51.686011: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 253 operators, 450 arrays (0 quantized)
2018-04-24 18:51:51.774563: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 62 operators, 169 arrays (1 quantized)
2018-04-24 18:51:51.775525: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 62 operators, 169 arrays (1 quantized)
2018-04-24 18:51:51.776094: F tensorflow/contrib/lite/toco/tooling_util.cc:1464] Array FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6, which is an input to the DepthwiseConv operator producing the output array FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6, is lacking min/max data, 
which is necessary for quantization. Either target a non-quantized output format, or change the input 
graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you 
do not care about the accuracy of results.
Aborted (core dumped)
```
Is there anyone who can help ? @andrewharp ,
thanks.


"
18828,Check failed: input_shape.dim_size() <= 4 (5 vs. 4),"####System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): -
TensorFlow installed from (source or binary): -
TensorFlow version (use command below): 1.8.0 rc1
Python version: -
Bazel version (if compiling from source):-
GCC/Compiler version (if compiling from source): -
CUDA/cuDNN version: -
GPU model and memory: -
Exact command to reproduce: -
####Describe the problem

bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=one_graphRes.pb
Found 1 possible inputs: (name=input_1, type=float(1), shape=[1,300,480,3]) 
No variables spotted.
Found 1 possible outputs: (name=predictions/concat, op=ConcatV2) 
Found 335304 (335.30k) const parameters, 0 (0) variable parameters, and 7 control_edges
Op types used: 192 Const, 73 Identity, 49 Switch, 21 Mul, 16 StridedSlice, 16 Pack, 15 BiasAdd, 15 Conv2D, 14 Add, 12 Reshape, 9 Sub, 7 Merge, 7 Elu, 7 FusedBatchNorm, 7 Rsqrt, 6 MaxPool, 4 Tile, 4 ConcatV2, 4 Shape, 2 RealDiv, 1 PlaceholderWithDefault, 1 Placeholder, 1 Sum, 1 Max, 1 Exp
To use with tensorflow/tools/benchmark:benchmark_model try these arguments:
bazel run tensorflow/tools/benchmark:benchmark_model -- --graph=one_graphRes.pb --show_flops --input_layer=input_1 --input_layer_type=float --input_layer_shape=1,300,480,3 --output_layer=predictions/concat

 bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=one_graphRes.pb --output_file=one_graphRes.tflite --inference_type=FLOAT --input_shape=1,300,480,3 --input_array=input_1 --output_array=predictions/concat

Check failed: input_shape.dim_size() <= 4 (5 vs. 4)
"
18827,Feature Request: Allow for sloppy=True in TFRecordDataset's init,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: -
- **TensorFlow installed from (source or binary)**: -
- **TensorFlow version (use command below)**: 1.8.0 rc1
- **Python version**: -
- **Bazel version (if compiling from source)**:-
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: -

### Describe the problem
From TF 1.8 `TFRecordDataset` has a new init parameter `num_parallel_reads` which essentially wraps a `ParallelInterleaveDataset` around the `_TFRecordDataset` instance we're reading from.
In the current implementation, the parallel interleave is hardcoded with `sloppy=False`.
Would it be possible to add `sloppy` as an additional parameter to TFRecordDataset? That would allow to simplify this:
```python
files = tf.data.list_files('somewhere/*')
dataset = files.apply(tf.data.parallel_interleave(TFRecordDataset, cycle_length=num_parallel_calls, sloppy=True))
```
to
```python
files = tf.data.list_files('somewhere/*')
dataset = TFRecordDataset(files, num_parallel_calls=num_parallel_calls, sloppy=True)
```
(Note that the underlying implementations are identical)"
18824,Tensorflow failed due to error LNK2019 when build with MSVC,"System information
•	Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
N/A
•	OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows server 2016
•	TensorFlow installed from (source or binary):
Source
•	TensorFlow version (use command below):
Master branch latest revison
•	Python version:
Anaconda 4.1.1 (Python 3.5 64-bit)
•	Bazel version (if compiling from source):
N/A
•	GCC/Compiler version (if compiling from source):
VS2017 15.5.7
•	CUDA/cuDNN version:
NVidia CUDA Toolkit 8.0
NVidia CUDNN 5.1
•	GPU model and memory:
N/A
•	Exact command to reproduce:
N/A

**Describe the problem:**
Tensorflow failed to build due to the error LNK2019 and error LNK1120. This should be tensorflow source issue, could you please help take a look at this? Thanks!

**The failures like:**
The whole log file please see attachment.
```
[log_x64_build.log](https://github.com/tensorflow/tensorflow/files/1942013/log_x64_build.log)
(Link target) -> 
         c_api.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::CppShapeInferenceResult_HandleShapeAndType::CppShapeInferenceResult_HandleShapeAndType(void)"" (??0CppShapeInferenceResult_HandleShapeAndType@tensorflow@@QEAA@XZ) referenced in function ""protected: class tensorflow::CppShapeInferenceResult_HandleShapeAndType * __cdecl google::protobuf::internal::RepeatedPtrFieldBase::Add<class google::protobuf::RepeatedPtrField<class tensorflow::CppShapeInferenceResult_HandleShapeAndType>::TypeHandler>(class tensorflow::CppShapeInferenceResult_HandleShapeAndType *)"" (??$Add@VTypeHandler@?$RepeatedPtrField@VCppShapeInferenceResult_HandleShapeAndType@tensorflow@@@protobuf@google@@@RepeatedPtrFieldBase@internal@protobuf@google@@IEAAPEAVCppShapeInferenceResult_HandleShapeAndType@tensorflow@@PEAV45@@Z) [D:\Tensorflow\build_x64\tensorflow.vcxproj]
         c_api.cc.obj : error LNK2019: unresolved external symbol ""protected: __cdecl tensorflow::CppShapeInferenceResult_HandleShapeAndType::CppShapeInferenceResult_HandleShapeAndType(class google::protobuf::Arena *)"" (??0CppShapeInferenceResult_HandleShapeAndType@tensorflow@@IEAA@PEAVArena@protobuf@google@@@Z) referenced in function ""protected: class tensorflow::CppShapeInferenceResult_HandleShapeAndType * __cdecl google::protobuf::internal::RepeatedPtrFieldBase::Add<class google::protobuf::RepeatedPtrField<class tensorflow::CppShapeInferenceResult_HandleShapeAndType>::TypeHandler>(class tensorflow::CppShapeInferenceResult_HandleShapeAndType *)"" (??$Add@VTypeHandler@?$RepeatedPtrField@VCppShapeInferenceResult_HandleShapeAndType@tensorflow@@@protobuf@google@@@RepeatedPtrFieldBase@internal@protobuf@google@@IEAAPEAVCppShapeInferenceResult_HandleShapeAndType@tensorflow@@PEAV45@@Z) [D:\Tensorflow\build_x64\tensorflow.vcxproj]
         c_api.cc.obj : error LNK2019: unresolved external symbol ""private: void __cdecl tensorflow::CppShapeInferenceResult_HandleShapeAndType::_slow_mutable_shape(void)"" (?_slow_mutable_shape@CppShapeInferenceResult_HandleShapeAndType@tensorflow@@AEAAXXZ) referenced in function TFE_GetResourceHandleShapeAndType [D:\Tensorflow\build_x64\tensorflow.vcxproj]
         c_api.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::CppShapeInferenceResult_HandleData::CppShapeInferenceResult_HandleData(void)"" (??0CppShapeInferenceResult_HandleData@tensorflow@@QEAA@XZ) referenced in function TFE_GetResourceHandleShapeAndType [D:\Tensorflow\build_x64\tensorflow.vcxproj]
         c_api.cc.obj : error LNK2019: unresolved external symbol ""public: virtual __cdecl tensorflow::CppShapeInferenceResult_HandleData::~CppShapeInferenceResult_HandleData(void)"" (??1CppShapeInferenceResult_HandleData@tensorflow@@UEAA@XZ) referenced in function TFE_GetResourceHandleShapeAndType [D:\Tensorflow\build_x64\tensorflow.vcxproj]
         D:\Tensorflow\build_x64\Release\tensorflow.dll : fatal error LNK1120: 5 unresolved externals [D:\Tensorflow\build_x64\tensorflow.vcxproj]
```

**Repro steps:**
1. git clone https://github.com/tensorflow/tensorflow D:\Tensorflow\src
2. pushd D:\Tensorflow
3. set PreferredToolArchitecture=x64
4. set rel=Release
5. set CUDNN_HOME=""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\cuda""
6. set PY=C:\ProgramData\Anaconda3
7. set CL=/FS /permissive-
8. cmake D:\Tensorflow\src\tensorflow\contrib\cmake -A x64 -DCMAKE_BUILD_TYPE=Release -DPYTHON_EXECUTABLE=C:\ProgramData\Anaconda3\python.exe -DPYTHON_LIBRARIES=C:\ProgramData\Anaconda3\libs\python36.lib -DSWIG_EXECUTABLE=D:\Tensorflow\swigwin-3.0.12\swig.exe -Dtensorflow_BUILD_PYTHON_TESTS=ON -Dtensorflow_BUILD_SHARED_LIB=ON
9. MSBuild /m /p:Configuration=Release;Platform=x64 /p:WindowsTargetPlatformVersion=10.0.16299.0 tensorflow.sln /t:Rebuild"
18823,Variables in autograph,"This question is regarding creating tf.Variables inside an autograph function.

The semantics that we have been trying are:

```
def func():
   v = []
   autograph.utils.set_element_type(v, tf.int32)
   v.extend([0.5, 0.1])
   return v.stack()

# And then:
t = autograph.to_graph(func)
```

However, now the return value of the function is an op, not a variable. The question is, how do I get the variable `v` as a tf.Variable ""out of"" my autograph function?


"
18822,"does tensorflow take all resource from GPU, making other CUDA code slow ?","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I have a custom CUDA code but not register into Tensorflow OP
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary: by ""pip3 install --upgrade tensorflow-gpu""
- **TensorFlow version (use command below)**:
tensorflow-gpu (1.7.0)
- **Python version**: 
Python 3.5.2
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**:
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
- **CUDA/cuDNN version**: CUDA 9.0 cuDNN 7
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I have a cuda lib build from C++ for post-processing after predict result by tensorflow model.
I use following way to make python able to use cuda code from C++
`lib = ctypes.cdll.LoadLibrary(my.so)`

If I test the cuda code alone without tensorflow. it work fine.
But when tensorflow is used in my project, my cuda code become 10 times slower....

My  time log is in cuda .so lib, so it's no way that the gap come from python to .so  wrap.

I have try to set the fraction of GPU memory to be allocated in tensorflow by:
`# Assume that you have 12GB of GPU memory and want to allocate ~4GB:`
`gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)`
`sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))`

but useless....
so I wonder does tensorflow take all resource from GPU, making other CUDA code slow ?
the only solution is make my cuda code as a tensorflow OP by register?

Any suggestion? Thanks~~~

----------------------Update----------------------
I have tested following way to set gpu_options, but still useless....
`config = tf.ConfigProto()`
`config.gpu_options.allow_growth = True`
`sess = tf.Session(config=config)`"
18821,"Error:(152, 16) The method setNumThreads(int) is undefined for the type Interpreter","github\tensorflow\tensorflow\contrib\lite\java\demo\app\src\main\java\com\example\android\tflitecamerademo\ImageClassifier.java
Error:(152, 16) The method setNumThreads(int) is undefined for the type Interpreter"
18819,[Error] Failed precondition: Table not initialized.,"Env:
  tf version 1.7.0
  macOS CPU

I uses `lookup_table` in my model. I freeze the model in `python` after training and then load it and do prediction in `c++`. When predicting, an error occurs. 
```
Failed precondition: Table not initialized.
	 [[Node: forward/string_to_index_4_Lookup/hash_table_Lookup = LookupTableFindV2[Tin=DT_INT64, Tout=DT_INT64, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](forward/string_to_index_4/hash_table, _arg_input/sparse_179/id/Placeholder_0_6, forward/string_to_index/hash_table/Const)]]
```
However, I have found `table_init` op in the frozen model protobuf. The error indicates those op not executed in `c++` code. How to execute `sess.run(tf.tables_initializer())` in `c++`?
```
  name: ""init_all_tables""
  name: ""forward/string_to_index/hash_table/table_init""
  name: ""forward/string_to_index_1/hash_table/table_init""
  name: ""forward/string_to_index_2/hash_table/table_init""
  name: ""forward/string_to_index_3/hash_table/table_init""
  name: ""forward/string_to_index_4/hash_table/table_init""
```

Main `c++` code:
```
  Session* session;
  Status status = NewSession(SessionOptions(), &session);
  if (!status.ok()) {
    std::cerr << status.ToString() << ""\n"";
    return 1;
  } else {
    std::cout << ""Session created successfully"" << std::endl;
  }

  // Load graph protobuf
  GraphDef graph_def;
  std::string graph_path = argv[2]; // the path to frozen model protobuf
  status = ReadBinaryProto(Env::Default(), graph_path, &graph_def);
  if (!status.ok()) {
    std::cerr << status.ToString() << std::endl;
  } else {
    std::cout << ""Load graph protobuf successfully"" << std::endl;
  }

  // Add the graph to the session
  status = session->Create(graph_def);
  if (!status.ok()) {
    std::cerr << status.ToString() << std::endl;
    return 1;
  } else {
    std::cout << ""Add graph to session successfully"" << std::endl;
  }

  // prepare inputs
  std::vector<std::pair<std::string, Tensor> > inputs;
  .....

  // Run the session, evaluating our ""forward/predict/add"" operation from the graph
  std::vector<tensorflow::Tensor> outputs;
  status = session->Run(inputs, {""forward/logit/add""}, {}, &outputs);
  if (!status.ok()) {
    std::cerr << status.ToString() << std::endl;
    return 1;
  } else {
    std::cout << ""Run session successfully"" << std::endl;
  }
```

"
18818,Using out-of-graph Reward in Reinforcement Learning with Iterator in tf.data,"When the reward value is generated from a script that is not computed in Tensorflow graph, like ROUGE score in text summarization or BLEU score in machine translation.
We first use the graph to generate a prediction result, then calculate the reward value out of the graph, and then feed the value into the graph to update the parameters of our model by policy gradients.
If we use session.run, this will cause recomputation that is mentioned in #672.  
Although recomputation does not cost very much, when we use the `tf.dataset` with `Iterator`, the dataset pointer will move to the next sample when calling `session.run` function.
In #672, they suggest to use `partitial_run` instead of `session.run`. But `partitial_run` can only fetch the tensor, but it can not execute the operation. 


So can you add a feature that can move the pointer of `Iterator` backward? or fix the pointer when calling the `session.run` function?"
18816,Which version of Cudnn is compatible with cuda 8.0?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
18815,tf.image.extract_glimpse not work as expected,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: r1.8
- **Python version**: 2.7.14
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 8.0/7.0
- **GPU model and memory**: GTX1080, 8G
- **Bazel version**: N/A
- **Exact command to reproduce**: python test_script.py

### Describe the problem
The output for the code below, is not correct. To me, the correct output should be: [0.   1.    2.    3.   4.  ]. The offset is [0, 0], and the centered is false, so the starting point is at the most upper left pixel (with the value 0). And the glimpse height is 1 and width is 5, so the glimpse should be the first row. 

### Source code / logs
import tensorflow as tf
import numpy as np

input_img = np.float32(np.arange(25).reshape((5, 5)))
print input_img

input_img = tf.expand_dims(input_img, 0)
input_img = tf.expand_dims(input_img, -1)

offset_ = tf.expand_dims([0.0, 0.0], 0)
first_glimpse = tf.image.extract_glimpse(input_img, [1, 5], offset_,
                                    centered=False, normalized=False, uniform_noise=False)
first_glimpse = tf.squeeze(first_glimpse)
sess = tf.Session()
print first_glimpse.eval(session=sess)

Output:
[[ 0.  1.  2.  3.  4.]
 [ 5.  6.  7.  8.  9.]
 [10. 11. 12. 13. 14.]
 [15. 16. 17. 18. 19.]
 [20. 21. 22. 23. 24.]]
[16.995655 12.920303  0.        1.        2.      ]
"
18813,Support in the Dataset API for sharding dataset used in stateful LSTMs,"- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
ubuntu 16.04
- TensorFlow installed from (source or binary):
using docker from google container registry
- TensorFlow version (use command below):
1.7.0
- Python version:
3.5
- Bazel version (if compiling from source):
n/a
- GCC/Compiler version (if compiling from source):
n/a
- CUDA/cuDNN version:
n/a
- GPU model and memory:
n/a
- Exact command to reproduce:
n/a

Hi,

I was wondering if there will ever be support for sharding a dataset where the order matters? As an example, consider the ptb_word_lm.py script. In the training of the LSTM, batches must be kept in order since the hidden/cell state is never reset (until the end of the epoch). If we use tf.data, we wouldn't be able to use Dataset.shard(...) on the examples themselves because we would get batches like this (assuming 4 workers in a distributed training and batch_size=3):

worker0_batch0 = [example_0, example_4, example_8]
worker0_batch1 = [example_12, example_16, example_20]
...

worker1_batch0 = [example_1, example_5, example_9]
worker1_batch1 = [example_13, example_17, example_21]
...

worker2_batch0 = [example_2, example_6, example_10]
worker2_batch1 = [example_14, example_18, example_22]
...

worker3_batch0 = [example_3, example_7, example_11]
worker3_batch1 = [example_15, example_19, example_23]
...

**Assuming the size of the dataset is N, then what we really want is** 

worker0_batch0 = [example_0, example_1, example_2]
worker0_batch1 = [example_3, example_4, example_5]
...

worker1_batch0 = [example_(N/4)+0, example_(N/4)+1, example_(N/4)+2]
worker1_batch1 = [example_(N/4)+3, example_(N/4)+4, example_(N/4)+5]
...

worker2_batch0 = [example_2*(N/4)+0, example_2*(N/4)+1, example_2*(N/4)+2]
worker2_batch1 = [example_2*(N/4)+3, example_2*(N/4)+4, example_2*(N/4)+5]
...

worker3_batch0 = [example_3*(N/4)+0, example_3*(N/4)+1, example_3*(N/4)+2]
worker3_batch1 =[example_3*(N/4)+3, example_3*(N/4)+4, example_3*(N/4)+5]
...

Even sharding the batches instead of the examples would lead to a similar picture.

So I guess if I had a list of TFRecord filenames, I could split the filenames, but I have a concern in the context of synchronous training. Suppose I have 10 tfrecord files (each with 10k examples) and 4 workers. The first two workers would get 3 files and the other two would get 2 files. If my understanding of SyncReplicasOptimizer is correct, I would either have to toss each of the 10k examples on the first two workers, or create a barrier for the second two workers to not proceed until the first two are done. Is there another solution here aside from equally splitting up my training data among files?

I can think of a couple of workarounds to this problem, but I'd like to know if this problem has been, or will be, solved?"
18807,"OpKernel not registered, despite being listed in OpRegistry","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:macOS High Sierra 10.13.3
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 2.7.14
- **Bazel version (if compiling from source)**: 0.12.0-homebrew
- **GCC/Compiler version (if compiling from source)**: Xcode 9.3: Apple LLVM version 9.1.0 (clang-902.0.39.1)
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: `tensorflow/contrib/makefile/build_all_ios.sh -g /path/to/model.p`

### Describe the problem
I have a ListDiff op in my model; as such, the generated `ops_to_register` file includes the following bits (edited way down to fit nicely):

```c
constexpr inline bool ShouldRegisterOp(const char op[]) {
  return false
     || isequal(op, ""ListDiff"")
  ;
}
```
```c
constexpr const char* kNecessaryOpKernelClasses[] = {
  ""ListDiffOp<::tensorflow::int32, int32>"",
};
```

However, when I try to load the model, it gives:
```
Error adding graph to session: No OpKernel was registered to support Op 'ListDiff' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

	 [[Node: rgb_to_grayscale/Tensordot/ListDiff = ListDiff[T=DT_INT32, out_idx=DT_INT32](rgb_to_grayscale/Tensordot/range, rgb_to_grayscale/Tensordot/add_1)]]
```

If I print the full list of registered ops via `tensorflow::OpRegistry::Global()->DebugString(false)`, the line for ListDiff is (line breaks added for readability):

```
Op<
    name=ListDiff; 
    signature=x:T, y:T -> out:T, idx:out_idx; 
    attr=T:type; 
    attr=out_idx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]
>
```

The only thing I can think of here is that `T:type` has no allowable values (I would expect to see `DT_INT32` given my `ops_to_register.h` file). However in a working build before I added `ListDiff`,  I had a few ops that has no listed allowable values (`Reshape` and `Squeeze`, in this case). I've read through the code that registers the `ListDiff` op and couldn't find anything out of the ordinary, but I'm a fairly novice Tensorflower.

Possibly related to #15732?

### Source code / logs
N/A
"
18802,Training iteration stops silently if not Debugging,"Have I written custom code: Yes
OS Platform and Distribution: Windows server 2012 R2 Standard
TensorFlow installed from: Pip repositories
TensorFlow version: 1.7.0
Bazel version: NA
CUDA/cuDNN version: NA
GPU model and memory: NA
Exact command to reproduce NA


I'm attempting to train a tensorflow.contrib.keras.model.Sequencial on an image dataset. I have noticed however that when running the script normally, everything appears to work fine until the process does not exit. When I ran the scrip in debug mode, I received the message ""StopIteration: Could not import PIL.Image. The use of 'array_to_img' requires PIL"". The error is pretty self-explanatory, however is it intended behavior to fail silently like this?  I did not realize at first that the execution halted without exiting, until sometime passed.

This is my model initialization and training block:
 
        model = tf.contrib.keras.models.Sequential()
        model.add(tf.contrib.keras.layers.Conv2D(32, kernel_size=(3, 3),
                         activation='relu',
                         input_shape=data_train.image_shape))
        model.add(tf.contrib.keras.layers.Conv2D(64, (3, 3), activation='relu'))
        model.add(tf.contrib.keras.layers.MaxPooling2D(pool_size=(2, 2)))
        model.add(tf.contrib.keras.layers.Dropout(0.25))
        model.add(tf.contrib.keras.layers.Flatten())
        model.add(tf.contrib.keras.layers.Dense(128, activation='relu'))
        model.add(tf.contrib.keras.layers.Dropout(0.5))
        model.add(tf.contrib.keras.layers.Dense(4, activation='softmax'))
        
        model.compile(loss=tf.contrib.keras.losses.categorical_crossentropy,
                      optimizer=tf.contrib.keras.optimizers.Adadelta(),
                      metrics=['accuracy'])

        model.fit_generator(data_train,
                            steps_per_epoch=100,
                            epochs=5,
                            validation_data=data_test,
                            validation_steps= data_test.batch_size)"
18800,tenserflow from source with cpu,"HI, 
I ask if I can install tensorflow from source on cpu machine for converting tf model to tflite model for mobile 
thank you very much in advance. "
18798,Changing output/reaction at the moment of detection,"  **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):** No, I've followed a tutorial and haven't touched the base setup of it. https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10
    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Windows 10
    **TensorFlow installed from (source or binary):** Source
    **TensorFlow version (use command below):** 1.5.0
    **Python version:** 3.6
    **Bazel version (if compiling from source):** Couldn't get the capture script to return this
    **GCC/Compiler version (if compiling from source):** b'unknown'
    **CUDA/cuDNN version:** 9.0 / 7.05
    **GPU model and memory:** Nvidia GeForce GTX 960M / 2Gb?
    **Exact command to reproduce:** No idea

------------------------

### Describe the problem

Hello TensorFlow and everyone else,

I was wondering where in the code we can change the output or rather, the reaction the framework has on a detected item. Quick notice: I have no prior Python experience so it's a bit difficult to go around and deduce code.

For simplicity, let's say, at the moment Tensor detects an object, i'd like to create a file. I've been looking around but can not find a structure or UML-diagram saying which code leads to which, so I have no solid idea where to begin. I'd be very greatful if any directions could be given.

Thanks in advance

### Source code / logs

N/A for this question"
18795,The net using while_loop with batch_normalization can't train,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:
v1.7.0-18-g92e6c3e 1.7.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:0.11.1
- **GCC/Compiler version (if compiling from source)**:5.4
- **CUDA/cuDNN version**:9.0
- **GPU model and memory**:nvidia 1080 titian

### Describe the problem
I need add a batch_normalization layer in while_loop body, but it breaks down when i training the net. Everything is OK if i remove x = tf.layers.batch_normalization(x, training=flag)

### Source code / logs
This a simple example
```
import tensorflow as tf
from data_pre import get_data

data, labels = get_data(
    ['../UCR_TS_Archive_2015/ItalyPowerDemand/ItalyPowerDemand_TRAIN'], 24, 2,True, 0, 2)  #pylint: disable=line-too-long

flag = True

def cond(i, x):
    return i < 1

def body(i, x):
    x = tf.layers.conv1d(x, 1, 7, padding='same')
    x = tf.layers.batch_normalization(x, training=flag)
    x = tf.nn.relu(x)
    return i + 1, x

_, y = tf.while_loop(cond, body, [0, data], back_prop=False)

y = tf.layers.flatten(y)
logits = tf.layers.dense(y, 2)

loss = tf.losses.mean_squared_error(labels, logits)
optimizer = tf.train.AdamOptimizer()
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    train_op = optimizer.minimize(loss, tf.train.get_global_step())

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess=sess, coord=coord)
    for _ in range(10):
        sess.run(train_op)
    coord.request_stop()
    coord.join(threads)
```
This is the error info
```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1312, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1420, in _call_tf_sessionrun
    status, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'gradients/mean_squared_error/div_grad/Neg' has inputs from different frames. The input 'while/batch_normalization/AssignMovingAvg_1' is in frame 'while/while_context'. The input 'one_hot' is in frame ''.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""./test.py"", line 40, in <module>
    sess.run(train_op)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1140, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'gradients/mean_squared_error/div_grad/Neg' has inputs from different frames. The input 'while/batch_normalization/AssignMovingAvg_1' is in frame 'while/while_context'. The input 'one_hot' is in frame ''.
```

"
18794,Keyword Extraction from a text followed by a key value using tensorflow,"Hi 

I have a pdf file that contains information. I would like to extract few key terms/phrase along with a value for example (current balance : CHF (swiss francs) 1,000)

I can convert pdf file to text using pdfminer . But how i can extract the above keyword using tensorflow text classification or other methods. I don't want to use rake, TF-IDF.

Can anyone suggest how i can start with it? I haven't came across a single example with tensorflow. There is a question on stack overflow. but this doesn't help me much as I am a beginner.

https://datascience.stackexchange.com/questions/10077/keyword-phrase-extraction-from-text-using-deep-learning-libraries"
18791,tf-nightly-gpu wheels are no more deployed for python 3.6 since 2018/03/30,"The tensorflow GPU wheels (https://pypi.org/project/tf-nightly-gpu) for python 3.6 are no more deployed since 2018/30/30, as shown by:

```
pip3.6 install tf-nightly-gpu==
```

Have I written custom code: N/A
OS Platform and Distribution: N/A
TensorFlow installed from: N/A
TensorFlow version: N/A
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A"
18790,Tensorflow object detection API,"Traceback (most recent call last):
  File ""export_inference_graph.py"", line 147, in <module>
    tf.app.run()
  File ""C:\Users\Ali Salar\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""export_inference_graph.py"", line 143, in main
    FLAGS.output_directory, input_shape)
  File ""C:\tensorflow2\models\research\object_detection\exporter.py"", line 453, in export_inference_graph
    graph_hook_fn=None)
  File ""C:\tensorflow2\models\research\object_detection\exporter.py"", line 421, in _export_inference_graph
    placeholder_tensor, outputs)
  File ""C:\tensorflow2\models\research\object_detection\exporter.py"", line 280, in write_saved_model
    builder = tf.saved_model.builder.SavedModelBuilder(saved_model_path)
  File ""C:\Users\Ali Salar\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\saved_model\builder_impl.py"", line 90, in __init__
    ""directory: %s"" % export_dir)
AssertionError: Export directory already exists. Please specify a different export directory: inference_graph\saved_model"
18789,"can 1*1 kernel conv2d optimized using neon? the source code treat 1*1 kernel conv2d as mat-mul, but did not use neon optimization","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
18788,import tensorflow.contrb.eager throws undefined symbol,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos
- **TensorFlow installed from (source or binary)**:  source
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 3.5.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**: 4.8.5
- **CUDA/cuDNN version**: cuda - 9.0, cudnn - 7.0
- **GPU model and memory**: GRID K520, memory -4036MiB
- **Exact command to reproduce**:

I am trying to import tensorflow.contrib.eager but I get the following error -

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/python/lib/python3.5/site-packages/tensorflow/contrib/__init__.py"", line 60, in <module>
    from tensorflow.contrib import nccl
  File ""/usr/lib/python/lib/python3.5/site-packages/tensorflow/contrib/nccl/__init__.py"", line 30, in <module>
    from tensorflow.contrib.nccl.python.ops.nccl_ops import all_max
  File ""/usr/lib/python/lib/python3.5/site-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py"", line 30, in <module>
    resource_loader.get_path_to_datafile('_nccl_ops.so'))
  File ""/usr/lib/python/lib/python3.5/site-packages/tensorflow/contrib/util/loader.py"", line 56, in load_op_library
    ret = load_library.load_op_library(path)
  File ""/usr/lib/python/lib/python3.5/site-packages/tensorflow/python/framework/load_library.py"", line 58, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)
  File ""/usr/lib/python/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: /usr/lib//python/lib/python3.5/site-packages/tensorflow/contrib/nccl/python/ops/_nccl_ops.so: undefined symbol: _ZN9perftools8gputools4cuda29ScopedActivateExecutorContextC1EPNS0_14StreamExecutorE
```"
18787,Default arguments of tensorflow.contrib.signal.inverse_stft do not invert tensorflow.contrib.signal.stft,"This issue is a suggestion to modify `tensorflow.contrib.signal.inverse_stft` such that the default arguments invert the `stft`

## Background
Currently the correct call to `inverse_stft` is:
```python
... = tf.contrib.signal.inverse_stft(
    stfts=...,
    frame_length=frame_length,
    frame_step=frame_step,
    # forward_window_fn
    window_fn=tf.contrib.signal.inverse_stft_window_fn(
        frame_step=frame_step,
        forward_window_fn=functools.partial(tf.contrib.signal.hann_window, periodic=True),
    )
)
```
but the default value for `window_fn` is `functools.partial(tf.contrib.signal.hann_window, periodic=True)` so the call to `inverse_stft_window_fn` is missing, because the `frame_step` is unknown for the default argument.

Note: Because the hann window has some special properties, it can happen, that the inverse stft only introduces an amplitude error.

## Suggestion
Introduce `forward_window_fn` in `tensorflow.contrib.signal.inverse_stft` and change the defaults to `window_fn=None, forward_window_fn=functools.partial(tf.contrib.signal.hann_window, periodic=True)`.
The code inside would then be:
```python
if window_fn is None:
    window_fn = tf.contrib.signal.inverse_stft_window_fn(frame_step, forward_window_fn)
```

## Example code (Demonstration)

```python
import functools
import tensorflow as tf
import tensorflow.contrib
import matplotlib.pyplot as plt

frame_length = 32
frame_step = 16

x = tf.placeholder(tf.float32)
X = tf.contrib.signal.stft(
    x,
    frame_length=frame_length,
    frame_step=frame_step,
)
x_hat = tf.contrib.signal.inverse_stft(
    X,
    frame_length=frame_length,
    frame_step=frame_step,
)
x_hat_2 = tf.contrib.signal.inverse_stft(
    X,
    frame_length=frame_length,
    frame_step=frame_step,
    # forward_window_fn
    window_fn=tf.contrib.signal.inverse_stft_window_fn(
        frame_step,
        functools.partial(tf.contrib.signal.hann_window, periodic=True),
        
    )
)

def normalize(x):
    return x / (-np.min(x))

with tf.Session() as sess:
    t = np.linspace(0,10,128)
    x_np = np.sin(t)
    x_hat_np, x_hat_2_np = sess.run([x_hat, x_hat_2], {x: x_np})
  
    plt.plot(t, x_np, label='orig')
    plt.plot(t, x_hat_np, label='default')
    plt.plot(t, x_hat_2_np, label='fixed')
#     plt.plot(t, normalize(x_np), label='orig', linewidth=8)
#     plt.plot(t, normalize(x_hat_np), label='default', linewidth=6)
#     plt.plot(t, normalize(x_hat_2_np), label='fixed', linewidth=4)
    plt.legend()

```

`frame_step == 8`:
![frame_step8](https://user-images.githubusercontent.com/13744128/39106793-5e78d87a-46be-11e8-9e88-d0c101f4b8de.png)

`frame_step == 16`:
![frame_step16](https://user-images.githubusercontent.com/13744128/39106796-61da9b52-46be-11e8-8c1f-de64c10e4f2d.png)

While the example with `frame_step == 8` introduces ""only"" a scaling error, the `frame_step == 16` example introduces distortions.


------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: Name: tf-nightly
Version: 1.8.0.dev20180331
Summary: TensorFlow helps the tensors flow
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: opensource@google.com
License: Apache 2.0
Location: /opt/anaconda/lib/python3.6/site-packages
Requires: grpcio, astor, gast, termcolor, six, absl-py, numpy, wheel, protobuf, tb-nightly
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

"
18786,"i have error in my code ""cannot import name 'label_map_util' , is there way to solve this issue ? ","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
18785,pywrap_tensorflow_internal cannot load/not found - Windows tensorflow CPU,"I have finished attempting installations of tensorflow CPU for Windows using Python3.5 and 3.6. I've tried installing a wheel placed in the Python directory. 
I tried installing from the web vis a vis Pip. 
I've tried adding paths to PYTHONPATH and PATH (Windows).
 I've done everything but install in a virtual environment using Anaconda. 
The traceback is always the same. 
The pywrap_tensorflow_internal files are in place but do not seem to be visible to the import routines. 
(*.pyd, *.py, *.pyl). I placed the pyd DLL in the System32 directory with no change.
I tried using Python 3.6 installed in the root and installing a c36 amd64 wheel and a c35 amd64 wheel from Python 3.5 in C:/User/nnnnn/AppData/Local---etc.. 
I'm using Win 10 Pro x64 on a laptop with Intel Core 2 Duo processor. No Nvidia GPU card. 
Everything installs and seems to run, including Swig, but still the install fails when I run "">>>import tensorflow as tf"" from a python command line session.
I haven't tried the Bazel or Anaconda approaches since I'm using Tensorflow with Tensorlayer and SRGAN for image recognition. I don't know if the virtual environment will be compatible yet.
The output from the attempts follows:

"">>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\jimjulian\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\jimjulian\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 903, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\jimjulian\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\jimjulian\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\jimjulian\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\jimjulian\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\jimjulian\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *  # pylint: disable=redefined-builtin
  File ""C:\Users\jimjulian\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\jimjulian\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\jimjulian\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\jimjulian\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 903, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\jimjulian\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\jimjulian\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\jimjulian\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\jimjulian\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>""

Any suggestions will be appreciated.
"
18784,Why is reading a CSV file with a TextLineDataset and decode_csv so slow?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 3.6.0
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0.176/7.0.7
- **GPU model and memory**: Titan X (12 GB)
- **Exact command to reproduce**: See below

I have a dataset saved on an SSD as a moderately large (~7 million line) .csv file. It's small enough to load into memory, but large enough that it takes a couple minutes to read the entire file into a NumPy array, and I don't always need the entire thing. This seemed like a perfect use case for tf.data.TextLineDataset.

I attempted to use it similarly to as demonstrated in the documentation with the tf.decode_csv and tf.data.Dataset.map functions, as shown in the code below, but I'm finding it to be unreasonably slow to fetch batches from the TextLineDataset. Of course I'd expect it to be slower than if the data has already been read into memory. But if I compare the total time to read the entire thing into a NumPy array in memory first and process it as a TensorSliceDataset vs. creating a TextLineDataset and reading batches that way, the former is many times faster. 

Am I missing something, or is this an issue with TextLineDataset and/or tf.decode_csv?


```
def make_tld(csv_filename, header_lines, delim, batch_size):
    dataset = tf.data.TextLineDataset(filenames=csv_filename).skip(header_lines)

    def parse_csv(line):
        cols_types = [[]] * num_cols_  # all required
        columns = tf.decode_csv(line, record_defaults=cols_types, field_delim=delim)
        return tf.stack(columns)

    dataset = dataset.map(parse_csv).batch(batch_size)
    return dataset


def make_tsd(csv_filename, header_lines, delim, batch_size):
    with open(csv_filename, ""r"") as f:
        lines = f.readlines()

    data_shape = (len(lines) - header_lines, len(lines[header_lines].strip().split(delim)))
    data = np.empty(shape=data_shape, dtype=np.float32)

    for idx, line in enumerate(lines[header_lines:]):
        columns = [float(el) for el in line.strip().split(delim)]
        data[idx, :] = np.array(columns)

    dataset = tf.data.Dataset.from_tensor_slices(data).batch(batch_size)
    return dataset


if __name__ == ""__main__"":
    batch_size_ = 100

    tld_start = datetime.datetime.now()
    tld = make_tld(csv_filename_, header_lines_, delim_, batch_size_)
    tld_next = tld.make_one_shot_iterator().get_next()
    with tf.Session() as tld_sess:
        tld_sess.run(tf.global_variables_initializer())
        try:
            while True:
                tld_out = tld_sess.run(tld_next)
        except tf.errors.OutOfRangeError:
            print(""Done"")
    tld_end = datetime.datetime.now()
    print(""TextLineDataset: "" + str(tld_end - tld_start))

    tsd_start = datetime.datetime.now()
    tsd = make_tsd(csv_filename_, header_lines_, delim_, batch_size_)
    tsd_next = tsd.make_one_shot_iterator().get_next()
    with tf.Session() as tsd_sess:
        tsd_sess.run(tf.global_variables_initializer())
        try:
            while True:
                tsd_out = tsd_sess.run(tsd_next)
        except tf.errors.OutOfRangeError:
            print(""Done"")
    tsd_end = datetime.datetime.now()
    print(""TensorSliceDataset: "" + str(tsd_end - tsd_start))
```

Output:
```
Done
TextLineDataset: 0:11:24.675474
Done
TensorSliceDataset: 0:02:12.061404
```"
18782,"Error in installation Tensorflow , windows ","Hi, I have installed Cuda and CudaDLl files and also defined paths to the respective folders. I am still getting an error on launching Tesorflow module saying DLL failed. I checked the folders. I think I have all the files in the respective folders. I think I am missing something below is the error screen on importing tensor flow.

![image](https://user-images.githubusercontent.com/26611229/39100139-273ff188-4653-11e8-9778-435bc47e031b.png)

"
18781,tf.variable_scope(auxiliary_name_scope=False) alters name_scope,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary (pip)
- **TensorFlow version (use command below)**: 1.8.0.dev20180329
- **Python version**: 3.6.5

### Describe the problem
Expected behavior for the following code is to print two lines containing:
```
(<tf.Variable 'vs_outer/var:0' shape=(1,) dtype=float32_ref>, <tf.Tensor 'ns_outer/ns_inner_1/const:0' shape=() dtype=float32>)
(<tf.Variable 'vs_outer/var:0' shape=(1,) dtype=float32_ref>, <tf.Tensor 'ns_outer/ns_inner_2/const:0' shape=() dtype=float32>)
```
What is actually printed:
```
(<tf.Variable 'vs_outer/var:0' shape=(1,) dtype=float32_ref>, <tf.Tensor 'ns_outer/ns_inner_1/const:0' shape=() dtype=float32>)
(<tf.Variable 'vs_outer/var:0' shape=(1,) dtype=float32_ref>, <tf.Tensor 'ns_outer/ns_inner_1/const_1:0' shape=() dtype=float32>)
```
Somehow the first use of `with VSO:` memorizes the `name_scope` it is in and restores it the second time it is used even though `auxiliary_name_scope=False` is set and the documentation states: 
```
auxiliary_name_scope: If True, we create an auxiliary name scope with the scope. If False, we don't touch name scope.
```

### Source code
```python
import tensorflow as tf

with tf.Graph().as_default():
  NSO = tf.name_scope('ns_outer').__enter__()

  VSO = tf.variable_scope(
        'vs_outer', auxiliary_name_scope=False, reuse=tf.AUTO_REUSE)

  with tf.name_scope(NSO):
    with tf.name_scope('ns_inner_1'):
      with VSO:
        print((tf.get_variable('var', [1]), tf.constant(1.0, name='const')))

  with tf.name_scope(NSO):
    with tf.name_scope('ns_inner_2'):
      with VSO:
        print((tf.get_variable('var', [1]), tf.constant(1.0, name='const')))
```

It works when swapping the `name_scope` `with-block` with the `variable_scope` `with-block`.
```python
import tensorflow as tf

with tf.Graph().as_default():
  NSO = tf.name_scope('ns_outer').__enter__()

  VSO = tf.variable_scope(
        'vs_outer', auxiliary_name_scope=False, reuse=tf.AUTO_REUSE)

  with tf.name_scope(NSO):
    with VSO:
      with tf.name_scope('ns_inner_1'):
        print((tf.get_variable('var', [1]), tf.constant(1.0, name='const')))

  with tf.name_scope(NSO):
    with VSO:
      with tf.name_scope('ns_inner_2'):
        print((tf.get_variable('var', [1]), tf.constant(1.0, name='const')))
```
"
18779,Feature Request / Question: IP-Cam object detection (How-to),"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, I've followed a tutorial and haven't touched the base setup of it. https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**:  3.6
- **Bazel version (if compiling from source)**:  Couldn't get the capture script to return this
- **GCC/Compiler version (if compiling from source)**: b'unknown'
- **CUDA/cuDNN version**: 9.0 / 7.05
- **GPU model and memory**: Nvidia GeForce GTX 960M / 2Gb?
- **Exact command to reproduce**: No idea

### Describe the problem

Hello Tensorflow, my question is pretty straightforward and maybe was already answered somewhere, but I'm not finding the answer : )).. Hence, I turn to you. If there is already a clear guide out there on how to do it, could you point me in the right direction?

Given this script that launches the object detection with a webcam, how do I modify it to launch the detector by giving an IP address as input and receiving that video feed (and bounding boxes) as output? (Source in the last section).

I am aware we can always set an IP cam to be a webcam in the machine's webcam list, but I'd like to access the IP address directly. Can this be done? Is this already implemented? 

Thanks in advance!


### Source code / logs
 

import os
import cv2
import numpy as np
import tensorflow as tf
import sys


sys.path.append("".."")


from utils import label_map_util
from utils import visualization_utils as vis_util


MODEL_NAME = 'inference_graph'

CWD_PATH = os.getcwd()

PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')


PATH_TO_LABELS = os.path.join(CWD_PATH,'training','labelmap.pbtxt')


NUM_CLASSES = 6


label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)

detection_graph = tf.Graph()
with detection_graph.as_default():
    od_graph_def = tf.GraphDef()
    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
        serialized_graph = fid.read()
        od_graph_def.ParseFromString(serialized_graph)
        tf.import_graph_def(od_graph_def, name='')

    sess = tf.Session(graph=detection_graph)



image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')


detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')

detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')


num_detections = detection_graph.get_tensor_by_name('num_detections:0')


video = cv2.VideoCapture(0)
ret = video.set(3,1280)
ret = video.set(4,720)

while(True):

    ret, frame = video.read()
    frame_expanded = np.expand_dims(frame, axis=0)

   
    (boxes, scores, classes, num) = sess.run(
        [detection_boxes, detection_scores, detection_classes, num_detections],
        feed_dict={image_tensor: frame_expanded})


    vis_util.visualize_boxes_and_labels_on_image_array(
        frame,
        np.squeeze(boxes),
        np.squeeze(classes).astype(np.int32),
        np.squeeze(scores),
        category_index,
        use_normalized_coordinates=True,
        line_thickness=8,
        min_score_thresh=0.85)

   
    cv2.imshow('Object detector', frame)

    if cv2.waitKey(1) == ord('q'):
        break

video.release()
cv2.destroyAllWindows()

"
18777,GraphDef not ok for the TensorRT API but ok for the TensorFlow iteslf ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: Python 3.5
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0-6ubuntu1~16.04.9
- **CUDA/cuDNN version**: 9.0 / 7.0.5
- **GPU model and memory**: GTX 1070, 8Gb

### Describe the problem
I'm trying to convert `Keras` model (`MobileNet` or `Xception`) to `TensorRT` engine using `TensorFlow` API.
First, I'm getting graph from `K.session()` and appropriate `GraphDef`.
Second, I'm freeze it with `convert_variables_to_constants`.
Then, I'm able to make predictions using `TensorFlow`. But when I submit it to the the `create_inference_graph`, it complaints to bad dimensions:
```
2018-04-22 20:13:17.560097: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:412] subgraph conversion error for subgraph_index:47 due to: ""Unimplemented: Not supported constant type, at conv_pw_4_bn/Const_5"" SKIPPING......( 7 nodes)
2018-04-22 20:13:17.563078: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:412] subgraph conversion error for subgraph_index:48 due to: ""Unimplemented: Not supported constant type, at conv_pw_1_bn/Const_5"" SKIPPING......( 7 nodes)
2018-04-22 20:13:17.565057: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:412] subgraph conversion error for subgraph_index:49 due to: ""Unimplemented: Not supported constant type, at conv_dw_1_bn/Const_5"" SKIPPING......( 7 nodes)
2018-04-22 20:13:17.567257: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:412] subgraph conversion error for subgraph_index:50 due to: ""Unimplemented: Not supported constant type, at conv_dw_12_bn/Const_5"" SKIPPING......( 7 nodes)
2018-04-22 20:13:17.569343: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:412] subgraph conversion error for subgraph_index:51 due to: ""Unimplemented: Not supported constant type, at conv_dw_7_bn/Const_5"" SKIPPING......( 7 nodes)
2018-04-22 20:13:17.571571: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:412] subgraph conversion error for subgraph_index:52 due to: ""Unimplemented: Not supported constant type, at conv_pw_10_bn/Const_5"" SKIPPING......( 7 nodes)
2018-04-22 20:13:17.573851: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:412] subgraph conversion error for subgraph_index:53 due to: ""Unimplemented: Require 4 dimensional input. Got 1 conv_dw_6_bn/cond/batchnorm/add/Switch"" SKIPPING......( 6 nodes)
```
as many times as there are such `*/Switch` nodes. Digging deeper, I found such strange design in the graph's `GraphDef`:
```
tensor_shape {
  dim {
  }
}
```
within some (**not all**) nodes of type `Const`, e.g.
```
node {
  name: ""conv_pw_8_bn/Const_5""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
          }
        }
      }
    }
  }
}
``` 
All such nodes are mentioned in the traceback.

So, it's ok for `TensorFlow` itself but wrong for the `TensorRT`. I gues there is an error either in `GraphDef` generation or while parsing for `TensorRT`."
18776,Low Accuracy with static image on TFLite demo model,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary(PIP install)
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: Na
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I've implemented the transfer learning example for mobilenet and deployed the Android app. The classification was happening perfectly with a good accuracy for the constant stream of images. Later I modified the app to pick up a file from storage or click an image, save to storage and then classify. When this is implemented, the accuracy has dropped acutely to the range 0f 0.01 - 0.3 for any of the Flower classes or the general mobilenet model with the 1500 classes. I've implemented the 224 x 224 version of the model. Below are the steps 

- Create a basic camera app
- Take a picture and save it to storage
- The uri of the image is saved and then a drawable is created from the URI.
- This drawable is then converted to a bitmap.
- The bitmap size is transformed to 224 x 224 to match the input of the Mobile Net model
- This transformed bitmap is sent for classification to the pre-implemented class from the sample.

Link to my question on stack overflow:
https://stackoverflow.com/questions/49954439/low-accuracy-with-static-image-on-tflite-demo-model

### Source code / logs
```
private static Bitmap getResizedBitmap(Bitmap bm, int newWidth, int newHeight, boolean isNecessaryToKeepOrig) {
        int width = bm.getWidth();
        int height = bm.getHeight();
        float scaleWidth = ((float) newWidth) / width;
        float scaleHeight = ((float) newHeight) / height;
        // CREATE A MATRIX FOR THE MANIPULATION
        Matrix matrix = new Matrix();
        // RESIZE THE BIT MAP
        matrix.postScale(scaleWidth, scaleHeight);

        // ""RECREATE"" THE NEW BITMAP
        Bitmap resizedBitmap = Bitmap.createBitmap(bm, 0, 0, width, height, matrix, false);
        if(!isNecessaryToKeepOrig){
            bm.recycle();
        }
        return resizedBitmap;
    }

```"
18774,mobilenet_v1_eval.py has a big bug?,"### System information
- **Have I written custom code**:Yes
- **OS Platform and Distribution**:Linux Ubuntu 16.04
- **TensorFlow installed from**:binary(Anaconda)
- **TensorFlow version**:1.7
- **Python version**: Python 3.6.2
- **Bazel version**:
Build label: 0.11.1
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue May 14 07:48:23 +50148 (1520362424903)
Build timestamp: 1520362424903
Build timestamp as int: 1520362424903
- **CUDA/cuDNN version**:CUDA 9.0, cuDNN v7.0
- **GPU model and memory**:GTX 1080Ti, 11GB
- **Exact command to reproduce**:Just run mobilenet_v1_eval.py to evaluate the trained model, you will find the bug. If **is_training=True you can get the right result**.[The is_trainging are in mobilenet_v1.mobilenet_v1_arg_scope(is_training=True) and mobilenet_v1.mobilenet_v1(inputs, is_training=True) ]. **Or remove ""slim.batch_norm"" in ""with slim.arg_scope([slim.batch_norm, slim.dropout],"" can also get the right result.** ""with slim.arg_scope([slim.batch_norm, slim.dropout],""  is in L362 of https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.py

### Describe the problem
Totally, I have two problems.
**Firstly**,
In the mobilenet_v1_eval.py, yeah, it's eval not train. When is_training=False, you will not get a right result. When is_training=True, you can get a right answer. Obviously, it's wrong! I guess it's a big bug associate with batch norm, but I cannot figure it out. Someone knows that? 
Attentation, the is_training is not the one in  flower_input(is_training=False), they are in mobilenet_v1.mobilenet_v1_arg_scope(is_training=True) and mobilenet_v1.mobilenet_v1(inputs, is_training=True). And **flower_input() was a custom code referencing imagenet_input()** in https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1_eval.py
```python
def build_model():
  """"""Build the mobilenet_v1 model for evaluation.

  Returns:
    g: graph with rewrites after insertion of quantization ops and batch norm
    folding.
    eval_ops: eval ops for inference.
    variables_to_restore: List of variables to restore from checkpoint.
  """"""
  g = tf.Graph()
  with g.as_default():
    inputs, labels = flower_input(is_training=False)
    scope = mobilenet_v1.mobilenet_v1_arg_scope(
        is_training=True, weight_decay=0.0)
    with slim.arg_scope(scope):
      logits, _ = mobilenet_v1.mobilenet_v1(
          inputs,
          is_training=True,
          depth_multiplier=FLAGS.depth_multiplier,
          num_classes=FLAGS.num_classes)

    if FLAGS.quantize:
      tf.contrib.quantize.create_eval_graph()

    eval_ops = metrics(logits, labels)

  return g, eval_ops
``` 
**Secondly**,
I use export_eval_pbtxt() in the following to get the ""mobilenet_v1_eval.pbtxt"".
```python
def export_eval_pbtxt():
  """"""Export eval.pbtxt.""""""
  g = tf.Graph()
  with g.as_default():
    inputs = tf.placeholder(dtype=tf.float32,shape=[None,224,224,3])
    scope = mobilenet_v1.mobilenet_v1_arg_scope(
        is_training=False, weight_decay=0.0)
    with slim.arg_scope(scope):
      logits, _ = mobilenet_v1.mobilenet_v1(
          inputs,
          is_training=False,
          depth_multiplier=FLAGS.depth_multiplier,
          num_classes=FLAGS.num_classes)
    eval_graph_file = '/home/lg/projects/mobilenet_v1_eval.pbtxt'
    with tf.Session() as sess:
          with open(eval_graph_file, 'w') as f:
            f.write(str(g.as_graph_def()))

```
Then, frozen the graph:

```python
bazel-bin/tensorflow/python/tools/freeze_graph  \
  --input_graph=/home/lg/projects/mobilenet_v1_eval.pbtxt \
  --input_checkpoint=/home/lg/projects/checkpoint/model.ckpt-20000 \
  --input_binary=false \
  --output_graph=/home/lg/projects/frozen_mobilenet_v1_224.pb  \
  --output_node_names=MobilenetV1/Predictions/Reshape_1  \
  --checkpoint_version=2
```

Then I use the frozen .pb file to classify an image, it won't get a right result, no matter you use is_training is True or False above to get the ""mobilenet_v1_eval.pbtxt"".
And the classify file is:
```python
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import os.path
import re
import sys
import tarfile

import numpy as np
from six.moves import urllib
import tensorflow as tf
FLAGS = None


def create_graph():
  """"""Creates a graph from saved GraphDef file and returns a saver.""""""
  # Creates graph from saved graph_def.pb.
  with tf.gfile.FastGFile(os.path.join(FLAGS.model_dir, r'/home/lg/projects/frozen_mobilenet_v1_224.pb'), 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
    _ = tf.import_graph_def(graph_def,return_elements=['MobilenetV1/Predictions/Reshape_1:0'], name='lg')

def run_inference_on_image(image):
  """"""Runs inference on an image.
  Args:
    image: Image file name.
  Returns:
    Nothing
  """"""
  if not tf.gfile.Exists(image):
    tf.logging.fatal('File does not exist %s', image)

  image_data = tf.gfile.FastGFile(image, 'rb').read()
  img_data_jpg = tf.image.decode_jpeg(image_data) #图像解码  
  img_data_jpg = tf.image.convert_image_dtype(img_data_jpg, dtype=tf.float32) #改变图像数据的类型
  img_data_jpg = tf.image.resize_image_with_crop_or_pad(img_data_jpg,224,224)

  # Creates graph from saved GraphDef.
  create_graph()

  with tf.Session() as sess:
    image_data = img_data_jpg.eval().reshape(-1,224,224,3)
    softmax_tensor = sess.graph.get_tensor_by_name('lg/MobilenetV1/Predictions/Reshape_1:0')
    predictions = sess.run(softmax_tensor, {'lg/Placeholder:0': image_data})
    predictions = np.squeeze(predictions)
    print('predictions: ',predictions)
    # Read the labels from label.txt.
    label_path = os.path.join(FLAGS.model_dir, '/home/lg/projects/labels.txt')
    label = np.loadtxt(fname=label_path,dtype=str)

    top_k = predictions.argsort()[-FLAGS.num_top_predictions:][::-1]
    for node_id in top_k:
      label_string = label[node_id]
      score = predictions[node_id]
      print('%s (score = %.5f)' % (label_string, score))

def main(_):
  image = (FLAGS.image_file if FLAGS.image_file else os.path.join(FLAGS.model_dir, 'cropped_panda.jpg'))
  run_inference_on_image(image)

if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  # graph_def.pb: Binary representation of the GraphDef protocol buffer.
  # label.txt: the labels according to data tfrecord
  parser.add_argument(
      '--model_dir',
      type=str,
      default='/tmp/imagenet',
      help='Path to graph_def.pb and label.txt'
  )
  parser.add_argument(
      '--image_file',
      type=str,
      default=r'/home/lg/projects/data/flower_photos/daisy/5673728_71b8cb57eb.jpg',
      help='Absolute path to image file.'
  )
  parser.add_argument(
      '--num_top_predictions',
      type=int,
      default=2,
      help='Display this many predictions.'
  )
  FLAGS, unparsed = parser.parse_known_args()
tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
```

The mobilenet v1 I used is in https://github.com/tensorflow/models/tree/master/research/slim/nets .
I have tried inception v3, whether to eval or classify an image that can get right results. So I think  mobilenet_v1_eval.py must have a bug. 

Actually, I have tried two experiments, one is the custom codes(tf_train.py, tf_eval.py, tf_input.py, tf_inference.py, tf_classify_image.py referencing https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10) in https://github.com/GarryLau/draft_notes/tree/master/TF.
Another one is referencing mobilenet_v1_train.py, mobilenet_v1_eval.py, mobilenet_v1.py, in https://github.com/tensorflow/models/tree/master/research/slim/nets .
In the begining, I think the custom codes have some problems, so I tried the second experiment that tensorflow official. But they are all wrong.

### Source code / logs
All the codes I used are in:
https://github.com/GarryLau/draft_notes/tree/master/TF
https://github.com/GarryLau/draft_notes/tree/master/create_tfrecord
"
18772,Android：No OpKernel was registered to support Op 'Cumsum' with these attrs,"i use http://ci.tensorflow.org/view/Nightly/job/nightly-android/lastStableBuild/  
but i get error on my cell phone，please how can slove this question

04-22 21:43:19.665 11453-11453/com.lemon.demo W/System.err: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Cumsum' with these attrs.  Registered devices: [CPU], Registered kernels:
      <no registered kernels>
    	 [[Node: Cumsum = Cumsum[T=DT_FLOAT, Tidx=DT_INT32, exclusive=false, reverse=false](crop_sub_imgs, Cumsum/axis)]]
        at org.tensorflow.Session.run(Native Method)
        at org.tensorflow.Session.access$100(Session.java:48)
        at org.tensorflow.Session$Runner.runHelper(Session.java:298)
        at org.tensorflow.Session$Runner.run(Session.java:248)
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:228)
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)
"
18770,Configuring TFLite(Android) to use all CPU's?,"### System information
- **Have I written custom code (yes)**:
- **OS Platform and Distribution (Android Things)**:
- **TensorFlow installed from (source)**:
- **TensorFlow version (1.8.0-rc0)**:
- **Python version**:  2.7
- **Bazel version (0.12.0)**:
- **GCC/Compiler version (5.4.0)**:
- **CUDA/cuDNN version**: 9.1/7.1
- **GPU model and memory**: GTX1080 8G
- **Exact command to reproduce**:

### Describe the problem
I use Raspberry Pi3 + Android Things + TFLite + MobileNet to make a image classifier sample, base this [Android Things Image Classifier](https://codelabs.developers.google.com/codelabs/androidthings-classifier/#4) tutorial.
- I monitor the cpu by `adb shell top` and find it only use 1 CPU with quant model when inference, and 2 CPU with unquant model.
  So how to configuring TFLite(Android) to use all CPUs?

- Another question, maybe impact the question above, on the[ introduction of TFLite](https://www.tensorflow.org/mobile/tflite/), it say:
  > On select Android devices, the Interpreter will use the Android Neural Networks API for hardware acceleration, or default to CPU execution if none are available.

  You know, Raspberry has a GPU(Broadcom VideoCore IV), so how to use it? has it been used?

### Source code / logs
[Android Things Image Classifier](https://codelabs.developers.google.com/codelabs/androidthings-classifier/#4) tutorial.
"
18769,"InvalidArgumentError for save/restore of variables (same version, same OS, same directory)","I get an InvalidArgumentError with no further information when I try to save and then restore parts of my model later to continue training it (due to needing my laptop for class).

Initialization:
saver = tf.train.Saver({""embeddings"": embeddings, ""weights"": nce_weights, ""biases"": nce_biases})

Save:
saver.save(sess, model_checkpoint_path)

Load:
saver.restore(sess, model_checkpoint_path)

```
2018-04-21 22:45:00.143245: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Invalid argument: /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument
Traceback (most recent call last):
  File ""/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
    return fn(*args)
  File ""/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1312, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1420, in _call_tf_sessionrun
    status, run_metadata)
  File ""/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

... <contains sensitive info> ...

InvalidArgumentError (see above for traceback): /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
```

**Clarification requested by tensorflowbutler**
_Have I written custom code:_
Yes, I modified this code (https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook/blob/master/Chapter%2007/doc2vec.py) to work with TensorFlow 1.7 and to use the same embeddings variable for documents as for words with average instead of concatenation. I also updated the saved variables to include nce_weights and nce_biases so that training may be resumed.
_OS Platform and Distribution_
MacOS 10.13.4 (17E199)
_TensorFlow installed from_
pip on VirtualEnv, according to instructions (https://www.tensorflow.org/install/install_mac)
_TensorFlow version_
1.7
_Bazel version_
NA
_CUDA/cuDNN version_
NA
_GPU model and memory_
NA
_Exact command to reproduce_
saver = tf.train.Saver({""embeddings"": embeddings, ""weights"": nce_weights, ""biases"": nce_biases})
saver.restore(sess, ""../trained_model/saved_stuff"")"
18768,Bad error message: InvalidArgumentError with no further info (besides Invalid argument),"When loading a model saved like the following to resume training (same machine, same version), I get an InvalidArgumentError:

Code:
saver = tf.train.Saver({""embeddings"": embeddings, ""weights"": nce_weights, ""biases"": nce_biases})

Save:
saver.save(sess, model_checkpoint_path)

Load:
saver.restore(sess, model_checkpoint_path)

Expected Output:
Something that actually tells me what went wrong (even if it is internal stuff, I want to know!)

Actual Output:
```
2018-04-21 22:45:00.143245: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Invalid argument: /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument
Traceback (most recent call last):
  File ""/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
    return fn(*args)
  File ""/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1312, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1420, in _call_tf_sessionrun
    status, run_metadata)
  File ""/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

... <contains sensitive info> ...

InvalidArgumentError (see above for traceback): /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
```

**Clarification requested by tensorflowbutler**
_Have I written custom code:_
Yes, I modified this code (https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook/blob/master/Chapter%2007/doc2vec.py) to work with TensorFlow 1.7 and to use the same embeddings variable for documents as for words with average instead of concatenation. I also updated the saved variables to include nce_weights and nce_biases so that training may be resumed.
_OS Platform and Distribution_
MacOS 10.13.4 (17E199)
_TensorFlow installed from_
pip on VirtualEnv, according to instructions (https://www.tensorflow.org/install/install_mac)
_TensorFlow version_
1.7
_Bazel version_
NA
_CUDA/cuDNN version_
NA
_GPU model and memory_
NA
_Exact command to reproduce_
saver = tf.train.Saver({""embeddings"": embeddings, ""weights"": nce_weights, ""biases"": nce_biases})
saver.restore(sess, ""../trained_model/saved_stuff"")"
18767,Documentation issues regarding installing on ubuntu,"While I was perusing the documentation for install_linux, I got to the section *Determine which TensorFlow to install*, and then I continued on.

I was at *Determine how to install TensorFlow* and I settled on *Virtualenv*. So I went to that section, and I followed the steps. But when I got to the sub-section (2) stating

> where `targetDirectory` specifies the top of the Virtualenv tree. Our instructions assume that `targetDirectory` is `~/tensorflow`, but you may choose any directory.

I was caught off guard a bit. I didn't know if I had missed something, so I spent a bit of time back-tracking the documentation to see if that was the case. I finally decided to `mkdir ~/tensorflow` and give it a go.

It would be nice if there was a (sub-)section stating this. I don't want to write it, so I'm making it as an issue instead. I did do this amazing thing though: #18766"
18764,Feature Request: Gradients for angles in tf.contrib.image.rotate(),"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Mac OS 10.13
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: tf-nightly | 1.8.0.dev20180328
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: N/A

### Describe the problem
#9423 requested gradients for  tf.contrib.image.rotate(), but the fix #9533 only provided gradients with respect to the image.  I would like to get gradients with respect to the angle. 

Happy to try to try to write a fix, if folks can give an idea for how to start a solution.  "
18763,"Multiple Classes fails in Eager Mode (""tf.keras.Model"")","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No 
- **Bazel version**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Tried on MacOS using tensorflow as well as Linux Ubuntu 16.04 using tensorflow-gpu
- **TensorFlow installed from (source or binary)**:
Installed utilizing pip
- **TensorFlow version (use command below)**:
1.7
- **Python version**: 
3.6
- **Exact command to reproduce**:
```
import tensorflow as tf  
import tensorflow.contrib.eager as tfe  

tfe.enable_eager_execution()

class CustomLayer(tf.keras.Model):
    def __init__(self):
        super(CustomLayer, self).__init__()
        print(""blah"")

class CustomNetwork(tf.keras.Model):
    def __init__(self):
        super(CustomNetwork, self).__init__()
        self.custom_layers = CustomLayer()

    def forward(self, x, y=None):
        x = self.custom_layers(x)

CustomNetwork().forward(tf.convert_to_tensor([1]))
```

### Describe the problem
Trying to utilize multiple classes fails in tensorflow eager mode utilizing ""tf.keras.Model"". If I change ""tf.keras.Model"" to ""tfe.Network"" it works - keep in mind I am utilizing tensorflow 1.7.  The error I get running the above code results in the error below:

### Source code / logs
```
blah
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-12-9afa9b91ddef> in <module>()
----> 1 CustomNetwork().forward(tf.convert_to_tensor([1]))

<ipython-input-11-484119102aec> in forward(self, x, y)
      5 
      6     def forward(self, x, y=None):
----> 7         x = self.custom_layers(x)

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/base_layer.py in __call__(self, inputs, **kwargs)
    237     """"""
    238     # Actually call the layer (optionally building it).
--> 239     output = super(Layer, self).__call__(inputs, **kwargs)
    240     if context.executing_eagerly():
    241       return output

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)
    712 
    713         if not in_deferred_mode:
--> 714           outputs = self.call(inputs, *args, **kwargs)
    715           if outputs is None:
    716             raise ValueError('A layer\'s `call` method should return a Tensor '

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py in call(self, inputs, training, mask)
    635     outputs, _ = self._run_internal_graph(inputs,
    636                                           training=training,
--> 637                                           mask=masks)
    638     return outputs
    639 

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)
    770     # does not return a list the same size as `call`
    771     tensor_map = {}
--> 772     for x, y, mask in zip(self.inputs, inputs, masks):
    773       tensor_map[str(id(x))] = (y, mask)
    774 

TypeError: zip argument #1 must support iteration
```
"
18762,static libtensorflow.a crashed on TF_NewGraph(),"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
Master
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
1.10
- **GCC/Compiler version (if compiling from source)**:
GCC
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

```
BUILD_ALL_ANDROID_PATH=""${TF_ROOT_DIR}/tensorflow/contrib/makefile/build_all_android.sh""

CC_PREFIX=${CC_PREFIX} NDK_ROOT=${NDK_ROOT} ""${BUILD_ALL_ANDROID_PATH}"" \
-x ""${GEN_LIBS_DIR}"" \
-s ""${TF_ROOT_DIR}/tensorflow/contrib/makefile/sub_makefiles/hexagon_graph_execution/Makefile.in"" \
-t hexagon_graph_execution
```
I am working to create tensorflow using DSP hexagon .
I integrated into my  android application after success compilation libtensorflow-core.a library.
But on TF_NewGraph() I have crash:

output logcat:
`**graph.cc:283 Non-OK-status: status status: Not found: Op type not registered 'NoOp' in binary running on localhost. Make sure the Op and Kernel are registered in the binary running in this process**.`


Please advise how to resolve it"
18761,Tensor Hub generated model- Coreml conversions issue,"I am trying to convert a tensor flow model created using tensor flow hub using the following code. Base model used for the model was inceptionv3

`

import tensorflow as tf
tf_model_path = 'Inception_v3_US_output_graph.pb'
with open(tf_model_path, 'rb') as f:
    serialized = f.read()
tf.reset_default_graph()
original_gdef = tf.GraphDef()
original_gdef.ParseFromString(serialized)


with tf.Graph().as_default() as g:
    tf.import_graph_def(original_gdef, name='')
    ops = g.get_operations()
    N = len(ops)
    for i in [0,1,2,N-3,N-2,N-1]:
        print('\n\nop id {} : op type: ""{}""'.format(str(i), ops[i].type));
        print('input(s):'),
        for x in ops[i].inputs:
            print(""name = {}, shape: {}, "".format(x.name, x.get_shape())),
        print('\noutput(s):'),
        for x in ops[i].outputs:
            print(""name = {}, shape: {},"".format(x.name, x.get_shape())),`

**This generated following output** 

> op id 0 : op type: ""Placeholder""
input(s):

>output(s):
name = Placeholder:0, shape: (?, 224, 224, 3),


>op id 1 : op type: ""Const""
input(s):

>output(s):
name = module/conv0/weights:0, shape: (3, 3, 3, 32),


>op id 2 : op type: ""Const""
input(s):

>output(s):
name = module/conv0_bn/gamma:0, shape: (32,),


>op id 3236 : op type: ""MatMul""
input(s):
>name = input/BottleneckInputPlaceholder:0, shape: (?, 1056), 
name = final_retrain_ops/weights/final_weights/read:0, shape: (1056, 2), 

>output(s):
name = final_retrain_ops/Wx_plus_b/MatMul:0, shape: (?, 2),


>op id 3237 : op type: ""Add""
input(s):
>name = final_retrain_ops/Wx_plus_b/MatMul:0, shape: (?, 2), 
>name = final_retrain_ops/biases/final_biases/read:0, shape: (2,), 

>output(s):
>name = final_retrain_ops/Wx_plus_b/add:0, shape: (?, 2),


>op id 3238 : op type: ""Softmax""
input(s):
>name = final_retrain_ops/Wx_plus_b/add:0, shape: (?, 2), 

>output(s):
name = final_result:0, shape: (?, 2),

**Then I tried to convert this to a coreml model using the following code** 

`import tfcoreml

input_tensor_shapes = {""Placeholder:0"":[1,224,224,3]} # batch size is 1

image_input_name = ['Placeholder:0']

coreml_model_file = 'Inception_v3_US_output_graph.pb'

output_tensor_names = ['final_result:0']

class_labels = 'Inception_v3_US_output_labels.txt'


coreml_model = tfcoreml.convert(
        tf_model_path=tf_model_path,
        mlmodel_path=coreml_model_file,
        input_name_shape_dict=input_tensor_shapes,
        output_feature_names=output_tensor_names,
        image_input_names = image_input_name,
        class_labels = class_labels)`

**But ended up getting the following error. Following are the last few lines of the error message** 

> 2308/3239: Converting op name: Placeholder ( type:  Placeholder )
Skipping name of placeholder
2309/3239: Converting op name: module_apply_default/hub_input/Mul ( type:  Mul )
2310/3239: Converting op name: module_apply_default/hub_input/Sub ( type:  Sub )
2311/3239: Converting op name: module_apply_default/conv0/Conv2D ( type:  Conv2D )
2312/3239: Converting op name: module_apply_default/conv0_bn/FusedBatchNorm ( type:  FusedBatchNorm )
2313/3239: Converting op name: module_apply_default/cell_stem_1/Relu ( type:  Relu )
2314/3239: Converting op name: module_apply_default/cell_stem_1/Pad ( type:  Pad )
2315/3239: Converting op name: module_apply_default/cell_stem_1/strided_slice ( type:  StridedSlice )
---------------------------------------------------------------------------
>AssertionError                            Traceback (most recent call last)
<ipython-input-8-5c93b11d7780> in <module>()
     18         output_feature_names=output_tensor_names,
     19         image_input_names = image_input_name,
---> 20         class_labels = class_labels)

>~/tfcoreml/tf-coreml/tfcoreml/_tf_coreml_converter.py in convert(tf_model_path, mlmodel_path, output_feature_names, input_name_shape_dict, image_input_names, is_bgr, red_bias, green_bias, blue_bias, gray_bias, image_scale, class_labels, predicted_feature_name, predicted_probabilities_output)
    491       class_labels=class_labels,
    492       predicted_feature_name=predicted_feature_name,
--> 493       predicted_probabilities_output=predicted_probabilities_output)

>~/tfcoreml/tf-coreml/tfcoreml/_tf_coreml_converter.py in _convert_pb_to_mlmodel(tf_model_path, mlmodel_path, output_feature_names, input_name_shape_dict, image_input_names, is_bgr, red_bias, green_bias, blue_bias, gray_bias, image_scale, class_labels, predicted_feature_name, predicted_probabilities_output)
    289   context.input_feed_dict = input_feed_dict
    290   context.skip_ops = skip_ops
--> 291   convert_ops_to_layers(context)
    292   sess.close()
    293 

>~/tfcoreml/tf-coreml/tfcoreml/_ops_to_layers.py in convert_ops_to_layers(context)
    146         print('%d/%d: Converting op name: %s ( type:  %s )' % (
    147             i+1, len(context.all_ops), op.name, op.type))
--> 148         translator(op, context)
    149       connect_skipped_ops(context)

>~/tfcoreml/tf-coreml/tfcoreml/_layers.py in strided_slice(op, context)
   1184     skip(op,context)
   1185   else:
-> 1186     assert False, 'Strided Slice case not handled'
   1187   context.translated[output_name] = True
   1188 

# AssertionError: Strided Slice case not handled

How can I fix this?

@aseemw"
18758,nightly builds no available for macos,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS HIgh Sierra
- **TensorFlow installed from (source or binary)**: Pypi
- **TensorFlow version (use command below)**: nightly build
- **Python version**:  3.6

### Describe the problem
Tensorflow nightly builds are not available for macOS since March 30.  "
18756,model_to_estimator custom Keras layers,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: nightly
- **Python version**: 3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: nightly/docker
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
`tf.estimator.model_to_estimator` doesn't support custom Keras layers in the model:
`ValueError: Unknown layer: MyCustomLayer` 

/cc @fchollet
"
18755,tf.profile.ProfileOptionBuilder.trainable_variables_parameter errors with pooling layer,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: GPU 1.4.1
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA8, cuDNN 6.0.20
- **GPU model and memory**: Tesla P4
- **Exact command to reproduce**: 

### Describe the problem
tf.profile.ProfileOptionBuilder.trainable_variables_parameter() errors with max_pooling layer.

### Source code / logs
```python
    opts = (tf.profiler.ProfileOptionBuilder(
         tf.profiler.ProfileOptionBuilder.trainable_variables_parameter())
         .with_node_names(hide_name_regexes=['.*pool.*'])
         .build())
     params_stats = tf.profiler.profile(
         graph,
         options=opts
     )   
     logger.info('Total params: {}'.format(params_stats.total_parameters))
 
```
The code above works ok for [Resnet](https://github.com/tensorflow/models/blob/r1.4.0/official/resnet/resnet_model.py), but when I want to profile my vgg_net or shuffle_net, it crashes with:

```
Traceback (most recent call last):
  File ""model_analysis.py"", line 50, in <module>
    profile_model_params(graph)
  File ""model_analysis.py"", line 25, in profile_model_params
    options=opts
  File ""/home/ymwan/python3/lib/python3.5/site-packages/tensorflow/python/profiler/model_analyzer.py"", line 312, in profile
    graph, op_log, run_meta, add_trace=cmd == 'code')
  File ""/home/ymwan/python3/lib/python3.5/site-packages/tensorflow/python/profiler/tfprof_logger.py"", line 146, in _merge_default_with_oplog
    graph, run_meta, add_trace=add_trace, add_trainable_var=add_trainable_var)
  File ""/home/ymwan/python3/lib/python3.5/site-packages/tensorflow/python/profiler/tfprof_logger.py"", line 88, in _get_logged_ops
    graph, op.node_def, REGISTERED_FLOP_STATS)
  File ""/home/ymwan/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2362, in get_stats_for_node_def
    result = stats_func(graph, node)
  File ""/home/ymwan/python3/lib/python3.5/site-packages/tensorflow/python/profiler/internal/flops_registry.py"", line 376, in _max_pool_grad_flops
    max_pool_ops = kernel_area * orig_out_shape.num_elements()
TypeError: unsupported operand type(s) for *: 'int' and 'NoneType'
```
This problem seems related to 'max_pool2d' (and error says) as ResNet doesn't have this problem.
From my code you can see `hide_name_regexes`, try to skip this problem but failed.
Here, someone else also report this issue: https://github.com/vahidk/TensorflowFramework/issues/2"
18752,install_java command line error,
18751,Tensorflow_lite Android Demo compiled failed on Windows,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**: 
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:

### Describe the problem

I download the tensorflow , use the git clone today.
And when i open the android studio, and open the tensorflow_lite demo(contrib/lite/java/demo).
some errors occur.


### Source code / logs

some errors occured on my android studio console:  Error:Could not find tensorflow-lite.jar (org.tensorflow:tensorflow-lite:0.1.8-rc1).
Searched in the following locations:
    https://jcenter.bintray.com/org/tensorflow/tensorflow-lite/0.1.8-rc1/tensorflow-lite-0.1.8-rc1.jar

Consult IDE log for more details (Help | Show Log)

"
18750,tensorflow debugger doesn't work while exceuting session run,"### Describe the problem
I ran tensorflow debugger using the command ""python ***.py --debug"" but got the following error, and python stops working.

tfdbg> run
2018-04-21 16:33:11.682709: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\debug\debug_graph_utils.cc:240] For debugging, tfdbg has set the parallel_iterations attribute of all scheduled Enter/RefEnter nodes to 1. (This does not affect subsequent non-debug runs.)
2018-04-21 16:33:15.836838: F C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\debug\debug_io_utils.cc:623] Non-OK-status: env->NewWritableFile(file_path, &f) status: Not found: Failed to create a NewWriteableFile: C:\Users\CHENGC~1\AppData\Local\Temp\tfdbg_z4heiebg/_tfdbg_device_,job_localhost,replica_0,task_0,device_CPU_0/bboxes_matching_batch_dict/bboxes_matching_batch_6/map/while/bboxes_matching_single/while/bboxes_jaccard/strided_slice_8/stack_0_DebugIdentity_1524299595833304 : 系统找不到指定的路径。
; No such process"
18749,tf.tile not working with multiples of type tf.int64,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.4 LTS
- **TensorFlow installed from (source or binary)**: Compiled from Source
- **TensorFlow version (use command below)**: tf.VERSION = 1.2.1
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**: c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
- **CUDA/cuDNN version**: cuda-9.0
- **GPU model and memory**: GeForce 940M 2002MiB
- **Exact command to reproduce**: Detailed Below.

The following program throws an error saying `No OpKernel was registered to support Op 'Tile' with these attrs.`
```
min_rating = tf.constant(0, tf.int64)
max_rating = tf.constant(12, tf.int64)
m = max_rating - min_rating + 1
k = tf.range(m, dtype=tf.int64)
d = tf.tile(k, tf.to_int64(tf.reshape(m, [1])))

with tf.Session() as sess:
    a = sess.run([d])
    print a
```
Here is the detailed log:
```
Traceback (most recent call last):
  File ""test.py"", line 16, in <module>
    a = sess.run([d])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'Tile' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:
  device='CPU'; Tmultiples in [DT_INT32]
  device='GPU'; T in [DT_COMPLEX128]; Tmultiples in [DT_INT32]
  device='GPU'; T in [DT_COMPLEX64]; Tmultiples in [DT_INT32]
  device='GPU'; T in [DT_INT16]; Tmultiples in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tmultiples in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tmultiples in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tmultiples in [DT_INT32]

	 [[Node: Tile = Tile[T=DT_INT64, Tmultiples=DT_INT64](range, Reshape)]]

Caused by op u'Tile', defined at:
  File ""test.py"", line 7, in <module>
    d = tf.tile(k, tf.to_int64(tf.reshape(m, [1])))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 3740, in tile
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2583, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'Tile' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:
  device='CPU'; Tmultiples in [DT_INT32]
  device='GPU'; T in [DT_COMPLEX128]; Tmultiples in [DT_INT32]
  device='GPU'; T in [DT_COMPLEX64]; Tmultiples in [DT_INT32]
  device='GPU'; T in [DT_INT16]; Tmultiples in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tmultiples in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tmultiples in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tmultiples in [DT_INT32]

	 [[Node: Tile = Tile[T=DT_INT64, Tmultiples=DT_INT64](range, Reshape)]]
```
However this is working correctly:
```
min_rating = tf.constant(0, tf.int64)
max_rating = tf.constant(12, tf.int64)
m = max_rating - min_rating + 1
k = tf.range(m, dtype=tf.int64)
d = tf.tile(k, tf.to_int32(tf.reshape(m, [1])))

with tf.Session() as sess:
    a = sess.run([d])
    print a
```
The [tile docs](https://www.tensorflow.org/api_docs/python/tf/tile) says that the `multiples` argument to `tf.tile` can be of type `int32` or `int64`."
18748,build_all_android.sh x86 abi build issue,"It took a bit of effort to get the build_all_android.sh script to work.  The effort was primarily due to the same issues encountered here: https://github.com/tensorflow/tensorflow/issues/14186
Armed with the work around outlined there, I modified compile_nsync.sh to have 
AR := ${NDK_ROOT}/toolchains/'""$toolchain""'/prebuilt/'""$android_os_arch""'/bin/'""$bin_prefix""'-ar
This does result in being able to build the arm variants (armeabi, armeabi-v7a and arm64-v8a) as well as the x86_64 variant.  However, x86 variant fails with:
/Users/swinston/Downloads/android-ndk-r12b/platforms/android-21/arch-x86/usr/lib/crtend_android.o
/tmp/77a95b0085967f7191ad958665724b6f/sysroot/usr/include/unistd.h:173: error: undefined reference to '__page_size'
/tmp/77a95b0085967f7191ad958665724b6f/sysroot/usr/include/unistd.h:173: error: undefined reference to '__page_size'
/tmp/77a95b0085967f7191ad958665724b6f/sysroot/usr/include/unistd.h:173: error: undefined reference to '__page_size'
/tmp/77a95b0085967f7191ad958665724b6f/sysroot/usr/include/unistd.h:173: error: undefined reference to '__page_size'
collect2: error: ld returned 1 exit status

This is a OSX build host and using ndk r12b.

NB: count me amongst those eager for r16/r17 ndk support!

"
18744,TensoRT SSD layers,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes I have create a custom class to load tensorflow zoo object detection models
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux xenial
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:
1.7
- **Python version**: 2.7 
- **Bazel version (if compiling from source)**: 0.10
- **GCC/Compiler version (if compiling from source)**: 4
- **CUDA/cuDNN version**: 7 tegra
- **GPU model and memory**: jetson tx2
- **Exact command to reproduce**:


### Describe the problem

TensorRT with any kind of SSD models is breaking tensorflow because of the ones layer not implemented. Any guideline on how to implement. 
### Source code / logs

using frozen_graph.pb of mobilnet and fastrcnn
Traceback (most recent call last):
  File ""tests/trt_od.py"", line 107, in <module>
    minimum_segment_size = 2,
  File ""/home/nvidia/Documents/tfinterface/tfinterface/estimator/getters.py"", line 38, in get
    return cls(model_path, *args, **kwargs)
  File ""tests/trt_od.py"", line 53, in __init__
    **trt_ops
  File ""/home/nvidia/.local/lib/python2.7/site-packages/tensorflow/contrib/tensorrt/python/trt_convert.py"", line 115, in create_inference_graph
    int(msg[0]))
tensorflow.python.framework.errors_impl.NotFoundError: No attr named 'index_type' in NodeDef:
         [[Node: ones = Fill[T=DT_INT32](strided_slice_14, ones/Const)]]
         [[Node: ones = Fill[T=DT_INT32](strided_slice_14, ones/Const)]]
.
"
18741,TensorflowJS does not work on mobile Chrome browser. [not sure it is right place],"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------
Sorry it is a tensorflowJS related problem but tensorflowJS does not support submit an issue?

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
https://storage.googleapis.com/tfjs-examples/mobilenet/dist/index.html

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Android 6.0.1
Chrome 65.0.3325.109

- **TensorFlow installed from (source or binary)**:
tfjs-example-mobilenet.js

- **TensorFlow version (use command below)**:
n/a

- **Python version**: 
n/a

- **Bazel version (if compiling from source)**:
n/a

- **GCC/Compiler version (if compiling from source)**:
n/a

- **CUDA/cuDNN version**:
n/a

- **GPU model and memory**:
n/a

- **Exact command to reproduce**:
n/a


### Describe the problem
TensorflowJS does not work on mobile Chrome browser.
visit https://storage.googleapis.com/tfjs-examples/mobilenet/dist/index.html on Android Chrome and wait for finishing model loading then it will use default cat.jpg to do a demo predict.
```
use default cat.jpg,
Android Chrome output:
coyote, prairie wolf, brush wolf, Canis Iatrans 0.075
Egyptian cat 0.074
hare 0.049
wood rabbit, cottontail, conttontail rabbit 0.033
...
```

run on for example MacOSX 10.12 Chrome 65.0.3328.181 output:
```
Egyptian cat	0.757
tabby, tabby cat	0.076
Siamese cat, Siamese	0.058
tiger cat	0.021
lynx, catamount	0.015
...
```

![cat.jpg](https://storage.googleapis.com/tfjs-examples/mobilenet/dist/ed21e89820228ec168bfdf72fb128449.jpg ""cat.jpg"")
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
18740,"Frequent ""Premature EOF"" error in LLVM archive when running bazel build","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Patches against tf dev tree
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
Source 
- **TensorFlow version (use command below)**:
1.6.0rc1 (last upstream commit 0abc4c9ecae912676f6070ca4b76b35c80351c26)
- **Python version**: 
n/a
- **Bazel version (if compiling from source)**:
0.11.1
- **GCC/Compiler version (if compiling from source)**:
gcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
- **CUDA/cuDNN version**:
n/a
- **GPU model and memory**:
n/a
- **Exact command to reproduce**:
From the dockerfile:


`
ENV GCC_HOST_COMPILER_PATH ""/usr/bin/gcc""
ENV PYTHON_BIN_PATH ""/usr/bin/python3""
ENV PYTHON_LIB_PATH ""/usr/lib/python3/dist-packages""
ENV CC_OPT_FLAGS ""-march=native""

ENV TF_NEED_GCP 0
ENV TF_NEED_GDR 0
ENV TF_NEED_HDFS 0
ENV TF_NEED_MKL 0
ENV TF_NEED_MPI 0
ENV TF_NEED_OPENCL 0
ENV TF_NEED_CUDA 0
ENV TF_ENABLE_XLA 1
ENV TF_NEED_JEMALLOC 1
ENV TF_NEED_VERBS 0
ENV TF_NEED_S3 0

WORKDIR /tmp/tensorflow-knureon

RUN ./configure \
	&& bazel build --config=opt --config=kpi --copt=-mfma --copt=-mavx2 //tensorflow/tools/pip_package:build_pip_package \
	&& bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
`


### Describe the problem
About 60% of the time I try from console and 100% of the time our CI system attempts to build TensorFlow, this error is observed, always related to the LLVM archive:

`
20-Apr-2018 11:43:32 | ERROR: /tmp/tensorflow-knureon/tensorflow/tools/pip_package/BUILD:85:1: no such package '@llvm//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/llvm-mirror/llvm/archive/8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz, https://github.com/llvm-mirror/llvm/archive/8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz] to /root/.cache/bazel/_bazel_root/0726af6ab00d84b2e399e1660b859bdb/external/llvm/8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz: Premature EOF and referenced by '//tensorflow/tools/pip_package:licenses'
-- | --
20-Apr-2018 11:43:32 | ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@llvm//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/llvm-mirror/llvm/archive/8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz, https://github.com/llvm-mirror/llvm/archive/8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz] to /root/.cache/bazel/_bazel_root/0726af6ab00d84b2e399e1660b859bdb/external/llvm/8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz: Premature EOF
`

I can use wget to fetch the tarball from the github source from within the Docker container without issues and the tarball opens without any errors reported.  The sha256 hash of the tarball is the same as what is specified in workspace.bzl:

`
sha256sum 8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz 
63d4da54dc7bc9a79e2ad266d230f4f759520cccb344a2dd49c2c6383ab75285  8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz
`

### Source code / logs
[TF-TFLOW-JOB1-170.log](https://github.com/tensorflow/tensorflow/files/1933283/TF-TFLOW-JOB1-170.log)


I have searched StackOverflow and github for the issue and found bazel LLVM errors, but nothing that would return ""Premature EOF"""
18739,Undefined References building example_trainer.cc with dynamic linking and Tensorflow r1.7 as external repository and CUDA9/CuDNN7,"/.bazelrc
```
build --action_env PYTHON_BIN_PATH=""/usr/bin/python""
build --action_env PYTHON_LIB_PATH=""/usr/local/lib/python2.7/dist-packages""
build --force_python=py2
build --host_force_python=py2
build --python_path=""/usr/bin/python""
build --define with_jemalloc=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_CUDA=""1""
build --action_env CUDA_TOOLKIT_PATH=""/usr/local/cuda""
build --action_env TF_CUDA_VERSION=""9.0""
build --action_env CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
build --action_env LD_LIBRARY_PATH=""/usr/local/cuda-9.0/lib64:/usr/local/cuda/lib64:""
build --action_env TF_CUDA_CLANG=""0""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
build --config=cuda
test --config=cuda
build --define grpc_no_ares=true
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
build --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
```
/tools/bazel.rc
```
build:monolithic --define framework_shared_object=false
build --define framework_shared_object=true
build:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain
build:cuda --define=using_cuda=true --define=using_cuda_nvcc=true
build:mkl --define=using_mkl=true
build --define=use_fast_cpp_protos=true
build --define=allow_oversize_protos=true
build --define=grpc_no_ares=true
build --spawn_strategy=standalone
build --genrule_strategy=standalone
build -c opt
```
/WORKSPACE:
```
workspace(name = ""proj"")
git_repository(
    name = ""org_tensorflow"",
    commit = ""024aecf414941e11eb643e29ceed3e1c47a115ad"",
    remote = ""git@github.com:tensorflow/tensorflow.git"",
)

http_archive(
    name = ""io_bazel_rules_closure"",
    sha256 = ""6691c58a2cd30a86776dd9bb34898b041e37136f2dc7e24cadaeaf599c95c657"",
    strip_prefix = ""rules_closure-08039ba8ca59f64248bb3b6ae016460fe9c9914f"",
    urls = [
        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/08039ba8ca59f64248bb3b6ae016460fe9c9914f.tar.gz"",
        ""https://github.com/bazelbuild/rules_closure/archive/08039ba8ca59f64248bb3b6ae016460fe9c9914f.tar.gz"",  # 2018-01-16
    ],
)

load(""//src:workspace.bzl"", ""proj_workspace"")
proj_workspace()
```
Source code: copied `@org_tensorflow//tensorflow/cc/tutorial/example_trainer.cc` to `//src/example_trainer.cc`.
/src/BUILD:
```
cc_binary(
    name = ""example"",
    srcs = [""example_trainer.cc""],
    deps = [
        ""@org_tensorflow//tensorflow/cc:cc_ops"",
        ""@org_tensorflow//tensorflow/cc:client_session"",
        ""@org_tensorflow//tensorflow/core:tensorflow"",
    ],
)
```
Build command:
```
bazel build src:example
```
Error:
Tons of ""undefined reference"" errors pointing to functions like stringprintf().
```
naming.cc:(.text._ZN10tensorflow12DataFilenameB5cxx11ENS_11StringPieceEii+0x1f): undefined reference to `tensorflow::strings::Printf[abi:cxx11](char const*, ...)'
```
Adding ```--monolithic``` does make it compile and run properly:
```bazel build --monolithic src:example```
"
18738,tensorflow problem,"how to solve the following error? ImportError: No module named '_pywrap_tensorflow_internal'
I have python  3.5 installed, Jupyter Notebook along with Ananconda also installed.

"
18737,TF hangs with distributed mode,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.5
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
0.5.4
- **GCC/Compiler version (if compiling from source)**:
5.4.0
- **CUDA/cuDNN version**:
9.0/7
- **GPU model and memory**:
TitanXP, 12G
- **Exact command to reproduce**:

### Describe the problem
When I run skip_thoughts model with distributed mode(2 workers in a machine, 2 GPUs per worker), tf hangs with linux condition variable error.

### Source code / logs
Below is the gdb log.
`Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
185	../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S: No such file or directory.
(gdb) bt
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00007fd44d2e891c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00007fd45b021ecb in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()
   from /home/soojeong/expdir_parallax/venv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007fd45b0217a1 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()
   from /home/soojeong/expdir_parallax/venv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007fd45b01ec12 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) ()
   from /home/soojeong/expdir_parallax/venv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007fd45b01f0f3 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) ()
   from /home/soojeong/expdir_parallax/venv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007fd45879da1b in tensorflow::(anonymous namespace)::WaitForNotification(tensorflow::CallOptions*, long long, tensorflow::Notification*) ()
   from /home/soojeong/expdir_parallax/venv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007fd45879e313 in tensorflow::LocalMaster::RunStep(tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) ()
   from /home/soojeong/expdir_parallax/venv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so`
"
18736,tensorflow/core/framework/allocator.cc:101] Allocation of X exceeds 10% of system memory. #18735,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**:

### Describe the problem
My server has 32gb of RAM, but tensorflow uses only 10% of that memory.
It returns the message:
<<tensorflow/core/framework/allocator.cc:101] Allocation of 9782001216 exceeds 10% of system memory.>>

Is it possible to increase this percentage?

### Source code / logs
### SOURCE CODE:
import keras
from keras import regularizers
from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, add
from keras.models import Model
from keras.layers.core import Layer, Dense, Dropout, Activation, Flatten, Reshape
from keras.regularizers import l2
from keras.utils import np_utils
from keras.callbacks import TensorBoard
from sklearn.model_selection import train_test_split
import numpy as np
import h5py
import time

file_train = 'features/files_train_test/train_inceptionv3_doc2vec.npy'
file_test = 'features/files_train_test/test_inceptionv3_doc2vec.npy'
x_train = np.load(file_train)
x_test = np.load(file_test)

print('shape train: ',x_train.shape,'shape test: ', x_test.shape)
print('size train: ', len(x_train), 'size test: ', len(x_test))
print('prod train: ',np.prod(x_train.shape[1:]), 'prod test: ', np.prod(x_test.shape[1:]))

x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))

print('shape train: ',x_train.shape,'shape test: ', x_test.shape)

epochs = 10
batch_size = 32
input_size = x_train.shape[1]
output_size = x_train.shape[1]
hidden_size = x_train.shape[1]
file_name = 'features/files_reduce/sparse/autoencoder_inceptionv3_doc2vec_'

x = Input(shape=(input_size,))
h = Dense(hidden_size, activation='relu', activity_regularizer=regularizers.l1(10e-5))(x)
r = Dense(output_size, activation='sigmoid')(h)
autoencoder = Model(inputs=x, outputs=r)
autoencoder.compile(optimizer='adam', loss='mse')
autoencoder.summary()
autoencoder.fit(x_train
                ,x_train
                ,batch_size=batch_size
                ,epochs=epochs
                ,verbose=1
                ,validation_data=(x_test, x_test))

autoencoder.save(file_name+name_full)
print('Save encode' + file_name+name_full)

===============================================================================
OUTPUT: 

Using TensorFlow backend.
shape train:  (21248, 49452) shape test:  (5313, 49452)
size train:  21248 size test:  5313
prod train:  49452 prod test:  49452
shape train:  (21248, 49452) shape test:  (5313, 49452)

Layer (type)                      | Output Shape              | Param    
- - - - - - - - - - - - - - - -  - - - - - - - - - - - - - - - - -  - - - - - - - -  - - - - - -
input_1 (InputLayer)         | (None, 49452)             | 0         
dense_1 (Dense)              | (None, 49452)             | 2445549756
dense_2 (Dense)              | (None, 49452)             | 2445549756
- - - - - - - - - - - - - - - -  - - - - - - - - - - - - - - - - -  - - - - - - - -  - - - - - -

Total params: 4,891,099,512
Trainable params: 4,891,099,512
Non-trainable params: 0

Train on 21248 samples, validate on 5313 samples
Epoch 1/10
2018-04-20 11:22:08.069764: W tensorflow/core/framework/allocator.cc:101] Allocation of 9782001216 exceeds 10% of system memory.
2018-04-20 11:22:12.089390: W tensorflow/core/framework/allocator.cc:101] Allocation of 9782001216 exceeds 10% of system memory."
18735,tensorflow/core/framework/allocator.cc:101] Allocation of X exceeds 10% of system memory.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**:

### Describe the problem
My server has 32gb of RAM, but tensorflow uses only 10% of that memory.
It returns the message:
<<tensorflow/core/framework/allocator.cc:101] Allocation of 9782001216 exceeds 10% of system memory.>>

Is it possible to increase this percentage?

### Source code / logs
### SOURCE CODE:
import keras
from keras import regularizers
from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, add
from keras.models import Model
from keras.layers.core import Layer, Dense, Dropout, Activation, Flatten, Reshape
from keras.regularizers import l2
from keras.utils import np_utils
from keras.callbacks import TensorBoard
from sklearn.model_selection import train_test_split
import numpy as np
import h5py
import time

file_train = 'features/files_train_test/train_inceptionv3_doc2vec.npy'
file_test = 'features/files_train_test/test_inceptionv3_doc2vec.npy'
x_train = np.load(file_train)
x_test = np.load(file_test)

print('shape train: ',x_train.shape,'shape test: ', x_test.shape)
print('size train: ', len(x_train), 'size test: ', len(x_test))
print('prod train: ',np.prod(x_train.shape[1:]), 'prod test: ', np.prod(x_test.shape[1:]))

x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))

print('shape train: ',x_train.shape,'shape test: ', x_test.shape)

epochs = 10
batch_size = 32
input_size = x_train.shape[1]
output_size = x_train.shape[1]
hidden_size = x_train.shape[1]
file_name = 'features/files_reduce/sparse/autoencoder_inceptionv3_doc2vec_'

x = Input(shape=(input_size,))
h = Dense(hidden_size, activation='relu', activity_regularizer=regularizers.l1(10e-5))(x)
r = Dense(output_size, activation='sigmoid')(h)
autoencoder = Model(inputs=x, outputs=r)
autoencoder.compile(optimizer='adam', loss='mse')
autoencoder.summary()
autoencoder.fit(x_train
                ,x_train
                ,batch_size=batch_size
                ,epochs=epochs
                ,verbose=1
                ,validation_data=(x_test, x_test))

autoencoder.save(file_name+name_full)
print('Save encode' + file_name+name_full)

===============================================================================
OUTPUT: 

Using TensorFlow backend.
shape train:  (21248, 49452) shape test:  (5313, 49452)
size train:  21248 size test:  5313
prod train:  49452 prod test:  49452
shape train:  (21248, 49452) shape test:  (5313, 49452)

Layer (type)                      | Output Shape              | Param    
- - - - - - - - - - - - - - - -  - - - - - - - - - - - - - - - - -  - - - - - - - -  - - - - - -
input_1 (InputLayer)         | (None, 49452)             | 0         
dense_1 (Dense)              | (None, 49452)             | 2445549756
dense_2 (Dense)              | (None, 49452)             | 2445549756
- - - - - - - - - - - - - - - -  - - - - - - - - - - - - - - - - -  - - - - - - - -  - - - - - -

Total params: 4,891,099,512
Trainable params: 4,891,099,512
Non-trainable params: 0

Train on 21248 samples, validate on 5313 samples
Epoch 1/10
2018-04-20 11:22:08.069764: W tensorflow/core/framework/allocator.cc:101] Allocation of 9782001216 exceeds 10% of system memory.
2018-04-20 11:22:12.089390: W tensorflow/core/framework/allocator.cc:101] Allocation of 9782001216 exceeds 10% of system memory."
18734,tensorflow::ops::SoftmaxCrossEntropyWithLogits has apparently backwards order to its public attributes,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
macOS 10.13.3 clang 900.0.39.2 and CentOS Linux 7 gcc-4.8.5

- **TensorFlow installed from (source or binary)**:
master

- **TensorFlow version (use command below)**:
I have not actually installed the python pip package, but the source tree is a current checkout of master.

- **Python version**: 
N/A using the C++ API

- **Bazel version (if compiling from source)**:
macOS Build label: 0.11.1-homebrew and Centos Linux 7 Build label: 0.11.1- (@non-git)

- **GCC/Compiler version (if compiling from source)**:
macOS clang 900.0.39.2 and CentOS Linux 7 gcc-4.8.5

- **CUDA/cuDNN version**:
N/A

- **GPU model and memory**:
N/A

- **Exact command to reproduce**:
Please read the description below

### Describe the problem
When trying to track down why code in pull request #14727 is failing an issue came to light. The gradient testing code in the C++ API would not correctly test C++ gradient code that had been shown to be a faithful reproduction of the python equivalent. It seems that the public attributes ""loss"" and ""backprop"" are reversed from their operation output order. @suharshs asked that a new issue be created to track this problem.  

### Source code / logs
Take a look at the comments in pull request #14727 
"
18733,How can we handle resource exhaust error coming while allocating a tensor of a particular size.,"I am currently using single GPU : GEFORCE GTX1080 8GB and I am unable to handle an error coming while I am trying to train a network. Error is :

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[131072,2048]
[[Node: training/Adam/mul_3 = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](Adam/beta_1/read, training/Adam/Variable/read)]]
[[Node: loss/mul/_179 = _Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_697_loss/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]]

However, I understand that I have limited memory in GPU which has to be utilized. I currently just wanting to know if there is any workaround of this by which I can train the network without reducing the dimensions of the network.

Model :

    model = Sequential()
    model.add(TimeDistributed(Flatten(), input_shape = (24,8,8,2048)))
    model.add(Dropout(0.5))
    model.add(LSTM(512, return_sequences=True, dropout=0.5))
    model.add(LSTM(512, return_sequences=False, dropout=0.5))
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(5, activation='softmax'))
Input to this network is actually image embedding I am getting from InceptionV3 without top.

Any help in this regard is appreciated.

Thanks in advance."
18732,[Docs]Extend graph_viz doc,"[Extend graph_viz doc](https://www.tensorflow.org/programmers_guide/graph_viz#runtime_statistics) to High Level API.

See also https://github.com/tensorflow/tensorflow/issues/13594"
18731,TfLite toco: fail kTensorflowMerge,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 18.04 Beta
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
Master
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
0.10.0
- **GCC/Compiler version (if compiling from source)**:
gcc 4.8.5
- **CUDA/cuDNN version**:
9.1 / 7.0.5
- **GPU model and memory**:
GTX 1070 Ti 6go & 16Go RAM
- **Exact command to reproduce**:
bazel-bin/tensorflow/contrib/lite/toco/toco 
--input_file=frozen_graph.pb 
--input_format=TENSORFLOW_GRAPHDEF 
--output_format=TFLITE 
--output_file=/tmp/test.tflite 
--inference_input_type=FLOAT
--input_arrays=input_image
--output_arrays=output_corner,output_mask
--input_shapes=1,512,512,3

### Describe the problem

I'm working on a FCN. I want to export my frozen graph to TfLite graph but it's not working. I'm wondering why, but I guess it's just due to the lack of maturity of TfLite (and probably some operator compatibility). In my case, it's about the operator ""Merge"". Someone can explain me if there is a way/trick to make it work ?

### Source code / logs

```
2018-04-20 12:57:33.287154: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: LogicalOr
2018-04-20 12:57:33.302765: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 518 operators, 965 arrays (0 quantized)
2018-04-20 12:57:33.312501: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 477 operators, 902 arrays (0 quantized)
2018-04-20 12:57:33.325133: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 477 operators, 902 arrays (0 quantized)
2018-04-20 12:57:33.325335: F tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:95] Check failed: other_op->type == OperatorType::kTensorFlowMerge 
Aborted (core dumped)
```

I also make a summarize_graph for you, maybe it help

```
Found 1 possible inputs: (name=input_image, type=int32(3), shape=[1,?,?,3]) 
No variables spotted.
Found 2 possible outputs: (name=output_corner, op=Tile) (name=output_mask, op=Tile) 
Found 1471309 (1.47M) const parameters, 0 (0) variable parameters, and 51 control_edges
Op types used: 239 Const, 168 Switch, 36 FusedBatchNorm, 27 Add, 22 Identity, 21 Merge, 18 Conv2D, 15 Relu, 13 StridedSlice, 12 Shape, 7 Mul, 6 Cast, 5 MaxPool, 5 Pack, 5 RealDiv, 3 Slice, 3 Conv2DBackpropInput, 2 Tile, 2 Reshape, 2 RandomUniform, 2 PlaceholderWithDefault, 2 Greater, 2 Floor, 2 ConcatV2, 1 Placeholder, 1 Maximum, 1 LogicalOr, 1 ResizeBilinear, 1 Sigmoid, 1 Softmax, 1 Squeeze, 1 ExpandDims, 1 Sub, 1 ArgMax
```"
18729,convert from .pb to .lite failed on operation conv_transpose,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac OS 10.12.6 (16G29)
- **TensorFlow installed from (source or binary)**:
toco is built form source
- **TensorFlow version (use command below)**:
master
- **Python version**: 
Python 3.6.1
- **Bazel version (if compiling from source)**:
bazel release 0.12.0-homebrew
- **GCC/Compiler version (if compiling from source)**:
Version 9.2 (9C40b)
- **CUDA/cuDNN version**:
NO
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
bazel-bin/tensorflow/contrib/lite/toco/toco \
  --input_format=TENSORFLOW_GRAPHDEF \
  --input_file=model.pb \
  --output_format=TFLITE \
  --output_file=model.lite \
  --inference_type=FLOAT \
  --inference_input_type=FLOAT \
  --input_arrays=data \
  --output_arrays=prob \
  --input_shapes=1,480,480,3
### Describe the problem
I want to convert a trained model to lite and run it on iPhone,I use command above to convert but it faided:

```
2018-04-20 17:33:52.204327: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd
2018-04-20 17:33:52.204482: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: SparseTensorDenseAdd
2018-04-20 17:33:52.204515: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd
2018-04-20 17:33:52.204623: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: SparseTensorDenseAdd
2018-04-20 17:33:52.227224: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: ScatterNd
2018-04-20 17:33:52.238603: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: ScatterNd
2018-04-20 17:36:13.632280: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 63715 operators, 127007 arrays (0 quantized)
2018-04-20 17:38:38.230277: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 63715 operators, 127007 arrays (0 quantized)
2018-04-20 17:41:43.014435: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:252] Check failed: weights_shape.dims(0) == 1 && weights_shape.dims(3) == 1 TransposeConv weights dimensions must begin and end with 1. Input weights ""bottle4_1_conv/weights/read_transposed"" had shape [ 16, 4, 4, 16 ].
```

the error information is confused, bottle4_1_conv is a layer using conv_transpose, but it's weights has shape [4,4,16,16] and I run the .pb model in python ,the result is correct, so any idea?"
18725,InvalidArgumentError : restore the weight of Inception_v4,"   

### System information 
- OS Platform and Distribution : windows10
- TensorFlow installed from : Source
- TensorFlow version : 1.7.0
- Python version: Python 3.6.3
- Bazel version: 0.11.1
- GCC/Compiler version : 5.4.0
- CUDA/cuDNN version: no
- GPU model and memory: no
- Exact command to reproduce:
 

### Describe the problem

  I'm training a deep neural network.I want to restore the weight of Inception_v4 ,but there are some errors.I think the define of network is right but I can't restore the weight of model.Is it a bug of different version.

### The errors:

 InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [3,3,128,768] rhs shape= 5,5,128,768, use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](InceptionV4/AuxLogits/Conv2d_2a/weights, save/RestoreV2:9)]]

### Source code

    import scipy.misc as misc
    import pickle
    import tensorflow as tf
    from tqdm import tqdm
    import numpy as np
    import argparse
    import fnmatch
    import sys
    import os
    
    from nets.inception_v4 import *
    sys.path.insert(0, 'nets/')
    slim = tf.contrib.slim
    
    def getPaths(data_dir):
        image_paths = []
        # add more extensions if need be
        ps = ['jpg', 'jpeg', 'JPG', 'JPEG', 'bmp', 'BMP', 'png', 'PNG']
        for p in ps:
            pattern = '*.' + p
            for d, s, fList in os.walk(data_dir):
                for filename in fList:
                    if fnmatch.fnmatch(filename, pattern):
                        fname_ = os.path.join(d, filename)
                        image_paths.append(fname_)
        return image_paths
    
    if name == 'main':
        parser = argparse.ArgumentParser()
        parser.add_argument('--data_dir', required=True, type=str, help='Directory images are in. Searches recursively.')
        parser.add_argument('--model', required=True, type=str, help='Model to use')
        parser.add_argument('--checkpoint_file', required=True, type=str, help='Model file')
        a = parser.parse_args()
        data_dir = a.data_dir
        model = a.model
        checkpoint_file = a.checkpoint_file
        # I only have these because I thought some take in size of (299,299), but maybe not
        height, width, channels = 224, 224, 3 
        x = tf.placeholder(tf.float32, shape=(1, height, width, channels))
        
        arg_scope = inception_v4_arg_scope()
        with slim.arg_scope(arg_scope):
            logits, end_points = inception_v4(x, is_training=False, num_classes=1001)
            features = end_points['PreLogitsFlatten']
         
        sess = tf.Session()
        saver = tf.train.Saver()
        saver.restore(sess, checkpoint_file)
        
        feat_dict = {}
        paths = getPaths(data_dir)
        print('Computing features...')
        for path in tqdm(paths):
            image = misc.imread(path)
            image = misc.imresize(image, (height, width))
            image = np.expand_dims(image, 0)
            feat = np.squeeze(sess.run(features, feed_dict={x: image}))
            feat_dict[path] = feat
        
        try:
            os.makedirs('features/')
        except:
            pass
        exp_pkl = open('features/' + model + '_features.pkl', 'wb')
        data = pickle.dumps(feat_dict)
        exp_pkl.write(data)
        exp_pkl.close()
    

### Log：

    D:\PythonProject\Compute-Features-WithModel>python compute_features.py --data_dir=jaffe/ --checkpoint_file=inception_v4.ckpt --model=inception_v4
    Use the retry module or similar alternatives.
    Traceback (most recent call last):
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1327, in _do_call
        return fn(*args)
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1312, in _run_fn
        options, feed_dict, fetch_list, target_list, run_metadata)
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1420, in _call_tf_sessionrun
        status, run_metadata)
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 516, in exit
        c_api.TF_GetCode(self.status.status))
    tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [3,3,128,768] rhs shape= [5,5,128,768]
    		[[Node: save/Assign_9 = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV4/AuxLogits/Conv2d_2a/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](InceptionV4/AuxLogits/Conv2d_2a/weights, save/RestoreV2:9)]]
    
    During handling of the above exception, another exception occurred:
    Traceback (most recent call last):
      File ""compute_features.py"", line 120, in <module>
        saver.restore(sess, checkpoint_file)
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1775, in restore
        {self.saver_def.filename_tensor_name: save_path})
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 905, in run
        run_metadata_ptr)
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1140, in _run
        feed_dict_tensor, options, run_metadata)
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1321, in _do_run
        run_metadata)
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1340, in _do_call
        raise type(e)(node_def, op, message)
    tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [3,3,128,768] rhs shape= [5,5,128,768]
             [[Node: save/Assign_9 = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV4/AuxLogits/Conv2d_2a/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](InceptionV4/AuxLogits/Conv2d_2a/weights, save/RestoreV2:9)]]
    Caused by op 'save/Assign_9', defined at:
      File ""compute_features.py"", line 119, in <module>
        saver = tf.train.Saver()
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1311, in init
        self.build()
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1320, in build
        self.build(self.filename, build_save=True, build_restore=True)
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1357, in _build
        build_save=build_save, build_restore=build_restore)
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 809, in _build_internal
        restore_sequentially, reshape)
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 470, in _AddRestoreOps
        assign_ops.append(saveable.restore(saveable_tensors, shapes))
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 162, in restore
        self.op.get_shape().is_fully_defined())
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\state_ops.py"", line 281, in assign
        validate_shape=validate_shape)
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_state_ops.py"", line 64, in assign
        use_locking=use_locking, name=name)
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
        op_def=op_def)
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 3290, in create_opop_def=op_def)
      File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1654, in init
        self.traceback = self.graph._extract_stack()  # pylint: disable=protected-access
    InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [3,3,128,768] rhs shape= [5,5,128,768]
             [[Node: save/Assign_9 = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV4/AuxLogits/Conv2d_2a/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](InceptionV4/AuxLogits/Conv2d_2a/weights, save/RestoreV2:9)]]
    
"
18724,"Executor failed to create kernel Snapshot, compiled from source using contrib/makefile","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: slightly modified code of label_image/main.cc
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: linux ubuntu 16.04
- **TensorFlow installed from (source or binary)**: compiled from source using contrib/makefile just for CPU
- **TensorFlow version (use command below)**: the latest r1.7
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**:  5.4.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

I created a c++ project slightly modified from label_image/main.cc, and compiled the code in qtcreator against -ltensorflow_cc -ltensorflow_framework (compiled from source using bazel) with no problems. Then I added all necessary ops to tf_op_files.txt and successfully compiled libtensorflow_core.a using contrib/makefile. Linking this static lib -ltensorflow_core to my code, my code returned at 
Div(root, Sub(root, resized, ...), std ) (pseudo code)
with error

2018-04-19 19:12:07.554566: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Not found: No registered 'Snapshot' OpKernel for CPU devices compatible with node Subtract = Snapshot[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""] (ResizeBilinear)
. Registered: < no registered kernels >

[[Node: Subtract = Snapshot[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""] (ResizeBilinear)]]


I found a similar post https://github.com/tensorflow/tensorflow/issues/17752 to this problem, but branch r1.7 with the fix https://github.com/tensorflow/tensorflow/commit/d392b1c9ebf131b9ac64ff289d26e43afea21c10 for windows doesn't solve my problem. 
 
My own PB model file was created a while ago using tools compiled from r1.5, will this be the problem? 
 
### Source code / logs

"
18723,1,Ok
18722,How to use C++ in java,"How to compile android/jni c + + files, their own so files? Beginner Android my language makes you confused, please understand"
18720,Problem:  Cannot merge devices with incompatible types: '/job:localhost/replica:0/task:0/device:CPU:0' and '/job:localhost/replica:0/task:0/device:GPU:0',"I get a problem. Here is the description of the error:

Caused by op u'block_stack_0/block_0/short_cut/weight/Initializer/random_uniform/RandomUniform', defined at:
  File ""train.py"", line 133, in <module>
    train()
  File ""train.py"", line 84, in train
    model.get_loss(image, gt, args.block_num_channel_list)
  File ""model.py"", line 145, in get_loss
    self.logits = self.inference(images, block_num_channel_list=block_num_channel_list)
  File ""model.py"", line 126, in inference
    output = self.stack(inputs=output, output_channel=bottel_neck_channel * 4, stack_num=stack_num, strides=stride, scope='block_stack_%d' % stack_block_index, bottel_neck_channel=bottel_neck_channel)
  File ""model.py"", line 118, in stack
    output = self.block(inputs, bottel_neck_channel=bottel_neck_channel, output_channel=output_channel, strides=strides, is_projection=True, scope='block_0')
  File ""model.py"", line 106, in block
    short_cut = self.conv(output, kernel_size=1, strides=strides, output_channel=output_channel, scope='short_cut', use_bias=False, is_activate=False)
  File ""model.py"", line 48, in conv
    name='weight')
  File ""model.py"", line 36, in get_variable
    trainable=trainable)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1065, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 962, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 367, in get_variable
    validate_shape=validate_shape, use_resource=use_resource)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 352, in _true_getter
    use_resource=use_resource)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 725, in _get_single_variable
    validate_shape=validate_shape)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 199, in __init__
    expected_shape=expected_shape)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 277, in _init_from_args
    initial_value(), name=""initial_value"", dtype=dtype)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 701, in <lambda>
    shape.as_list(), dtype=dtype, partition_info=partition_info)
  File ""/usr/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/initializers.py"", line 144, in _initializer
    dtype, seed=seed)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/random_ops.py"", line 240, in random_uniform
    shape, dtype, seed=seed1, seed2=seed2)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_random_ops.py"", line 247, in _random_uniform
    seed=seed, seed2=seed2, name=name)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Cannot colocate nodes 'block_stack_0/block_0/short_cut/weight/Initializer/random_uniform/RandomUniform' and 'block_stack_0/block_0/short_cut/weight: Cannot merge devices with incompatible types: '/job:localhost/replica:0/task:0/device:CPU:0' and '/job:localhost/replica:0/task:0/device:GPU:0'
	 [[Node: block_stack_0/block_0/short_cut/weight/Initializer/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, _class=[""loc:@block_stack_0/block_0/short_cut/weight""], dtype=DT_FLOAT, seed=0, seed2=0](block_stack_0/block_0/short_cut/weight/Initializer/random_uniform/shape)]]


('add regulariztion, regularization decay:', 0.003)
set learning rate
Traceback (most recent call last):
  File ""train.py"", line 133, in <module>
    train()
  File ""train.py"", line 91, in train
    sess.run(tf.global_variables_initializer())
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'block_stack_0/block_0/short_cut/cond/Switch_1': Unknown input node 'block_stack_0/block_0/short_cut/weight/Initializer/random_uniform'



it seems that the error happen on the op ""tf.global_variables_initializer()""
some code may be related to this error is : 

os.environ['CUDA_VISIBLE_DEVICES'] = '0'

with tf.device('/cpu:0'):
            var = tf.get_variable(shape=shape,
                                  name=name,
                                  dtype=dtype,
                                  initializer=initializer,
                                  regularizer=regularizer,
                                  trainable=trainable)
            moving_average_opt = self.moving_averager.apply([var])
            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, moving_average_opt)
            moving_average_variable = self.moving_averager.average(var)
            tf.summary.histogram(var.name, var)
            tf.summary.histogram('moving_'+ var.name, moving_average_variable)


how can i solve this problem? Thanks. 

"
18719,Won't build with latest version of ComputeCpp,"I was just supposed to install Keras. But it failed on building Tensorflow.
The command I ran is 'bazel build --local_resources 2048,.5,1.0 -c opt --config=sycl //tensorflow/tools/pip_package:build_pip_package', as in https://www.codeplay.com/portal/03-30-17-setting-up-tensorflow-with-opencl-using-sycl .
OS: Linux bckpkol-ashatan 4.13.0-38-generic #43 16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
From source. Git master version. ec2c66356b450fe52ec4c9a8b8c2e0e89e69f271 commit.
Package: python3
Version: 3.5.1-3
Package: bazel
Version: 0.12.0
gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)
No CUDA, because I have RX 550 with working OpenCL
amdgpu-pro-17.40.2712-510357 driver, because latest wasn't working
https://pastebin.com/gx67YjTU clinfo log, also gpu model and memory
https://pastebin.com/HqgE3PG3 computecpp_info log
https://pastebin.com/8nMXuPh2 build log (last lines from terminal)

The problem is 'error: no template named 'map_allocator' in namespace 'cl::sycl''.
It also constantly appears while running testsuite.
Forgot to mention - it is said that I need ComputeCpp 1.2. But minimum version on their site is 0.2.0, and I'm using 0.7.0. That might be the problem, however I don't know which version is supported at the moment."
18718,Bug: CPU/multinomial_op_test fails on AVX512 systems most likely due to bug in Eigen AVX512 log implementation ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
('v1.8.0-rc0-561-g075fbb59d7', '1.8.0-rc0')
- **Python version**: 
Python 2.7.12
- **Bazel version (if compiling from source)**:
Build label: 0.11.0
- **GCC/Compiler version (if compiling from source)**:
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
- **CUDA/cuDNN version**:
NA
- **GPU model and memory**:
NA
- **Exact command to reproduce**:
bazel test --config=opt //tensorflow/python/kernel_tests/random:multinomial_op_test

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

On machines with AVX512 instruction set, multinomial_op_test fails on CPU device. Most likely this is an issue with Eigen's AVX512 log implementation.

Test passes if I disable the AVX512 plog implementation https://bitbucket.org/eigen/eigen/src/3215c06819b99ce52d5a8d6939d072024e1e3fa0/Eigen/src/Core/arch/AVX512/MathFunctions.h?fileviewer=file-view-default#MathFunctions.h-36:125 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
  2 -----------------------------------------------------------------------------
  3 ..2018-04-20 03:28:51.833924: W tensorflow/core/framework/op_kernel.cc:1290] CtxFailure at multinomial_op.cc:165: Invalid argument: num_classes should be positive, got 0
 ...
/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/random/multinomial_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/random/multinomial_op_test.py:194: RuntimeWarning: divide by zero encountered in true_divide
 12   chi2 = np.sum(diff * diff / expected, axis=0)
 13 F....
 14 ======================================================================
 15 FAIL: testSamplingCorrectness (__main__.MultinomialTest)


https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/random/multinomial_op_test.py#L44

def composed_sampler(logits, num_samples):
  unif = random_ops.random_uniform(logits.get_shape().concatenate(
      tensor_shape.TensorShape([num_samples])))
      noise = -math_ops.log(-math_ops.log(unif)) 

Current tensorflow (for nodes highlighted in the link above):
unif: [[[0.824622393 0.0375790596 0.197375655]]...]
noise: [[[inf inf inf]]...]

After disabling Eigen's AVX512 plog implementation:
unif: [[[0.824622393 0.0375790596 0.197375655]]...]
noise: [[[1.64594793 -1.1882422 -0.48405844]]...]


"
18717,iOS: No OpKernel was registered to support Op 'Conv2d' with these attrs,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac OS 10.12.6 (16G29)
- **TensorFlow installed from (source or binary)**:
I only build the iOS library from source
when I trained the model, I install it use pip
- **TensorFlow version (use command below)**:
1.7.0
- **Python version**: 

- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
Not use GPU
- **GPU model and memory**:
- **Exact command to reproduce**:
build_all_ios.sh -g mymodel.pb


### Describe the problem
I use command list below to generate the ops_to_register.h file. and build the iOS library use ""build_all_ios.sh -g mymodel.pb""
```
$ bazel build tensorflow/python/tools/print_selective_registration_header
$ bazel-bin/tensorflow/python/tools/print_selective_registration_header \
  --graphs=graph.pb > tensorflow/core/framework/ops_to_register.h
```

After that , I config my Xcode and run the model on an iphone, but it issues error:


Could not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'Conv2D' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

	 [[Node: init_conv/Conv2D = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""VALID"", strides=[1, 4, 4, 1], use_cudnn_on_gpu=false](Pad, init_conv/weights/read)]]

but the conv*.cc is in the tf_op_files.txt,  Is there some error on my usage?
"
18712,tf.count_nonzero should support string tensors,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: 1.8
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: tf.count_nonzero(tf.constant([""test""]))

### Describe the problem
tf.count_nonzero should support string tensors, since tf.zeros and tf.zeros_like work with string tensors.

### Source code / logs
tf.count_nonzero(tf.constant([""test""]))"
18711,Suggestion for efficient upsample+conv2d and conv2d+pool,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.7.0-3-g024aecf414 1.7.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: /A
- **CUDA/cuDNN version**: CUDA 9.0/cuDNN 7.0
- **GPU model and memory**: Titan Xp 12GB
- **Exact command to reproduce**: CUDA_VISIBLE_DEVICES=0 python test_conv_upsample_pool_ops.py

### Describe the problem
TLDR: upsampling+conv2d (which is very expensive) could be implemented such that it has the same time and memory complexity as strided conv2d_transposed. Similarly for conv2d+average_pooling and strided conv2d. An efficient implementation could be S^2 faster, where S is the stride (typically S=2).

Strided conv2d_transposed have been traditionally used in decoders for dense prediction problems (e.g. image generation, video prediction, semantic segmentation). However, this op often produces outputs with checkerboard artifacts, e.g. see [1] and papers citing it. An alternative is to use bilinear upsampling followed by a standard convolution (with no strides). This is widely used in several recent works and it mitigates the checkerboard artifacts but at a cost: it's computationally and memory expensive (e.g. see [2]). upsampling+conv2d does S^2 more computation than the strided counterpart, where S is the stride factor. Furthermore, the intermediate tensor after upsampling could be very large if the input has a large number of channels.

Under certain conditions (which happens to be the most common use case), upsampling+conv2d could be rewritten as convolving the upsampling kernel with the given kernel, followed by conv2d_transposed of the given input and the combined kernels. This follows from commutative and associative properties of linearity in convolutions (taking proper care of flipping the filters so that the ops are actual convolutions).

A similar reasoning applies for an efficient implementation of conv2d+average_pooling.

Implementations of the mentioned ops are here (see `upsample_conv2d` and `conv_pool2d`): https://github.com/alexlee-gk/video_prediction/blob/master/video_prediction/ops.py

A script that tests for equivalence and timings is here: https://gist.github.com/alexlee-gk/1ae88125ec38efc48b542a4c0356078f

I report some timings below of hundreds of evaluations. The naive op should be about 4x slower than the strided counterpart since S^2 = 4. In theory, the optimized op should be as fast as the strided counterpart, but my implementation is about 2x slower (but still much faster than the naive version), and I think there is room for improvement.

|                           | upsample + conv2d (optimized) | upsample + conv2d (naive) | strided conv2d_transpose |
|:------------------------- | -----------------------------:| -------------------------:| --------------:|
| forward pass              | 11.0s                         | 16.3s                     | 4.7s           |
| forward and backward pass | 6.5s                          | 17.5s                     | 2.6s           |

|                           | conv2d + pool (optimized) | conv2d + pool (naive) | strided conv2d |
|:------------------------- | -------------------------:| ---------------------:| --------------:|
| forward pass              | 4.1s                      | 8.2s                  | 2.6s           |
| forward and backward pass | 4.1s                      | 8.4s                  | 2.6s           |

[1] https://distill.pub/2016/deconv-checkerboard
[2] https://arxiv.org/abs/1707.05847"
18708,Placeholder shape checking inside tf.cond not properly enforced,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Tensorflow 1.7 Docker under CentOS 7.3
- **TensorFlow installed from (source or binary)**: Tensorflow 1.7 Docker
- **TensorFlow version (use command below)**:  1.7
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:  N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: Quadro M4000
- **Exact command to reproduce**:

### Describe the problem
When a placeholder is defined in a tf.cond statement, shape checking is not enforced until the merge, and ends up being only rank checking.

### Source code / logs
 Consider the following toy graph/session, developed in the pursuit of a larger issue.  I suspect this entire approach of trying to have differently shaped placeholders is doomed to failure.  But, the graph produced clearly shows placeholders of appropriate shape inside the conditional; shows appropriately sized tensors flowing into the merge, but shows the merge itself as shape (?,?) and that is where the shape checking happens.  As a result, you can put just about any rank two tensor in and set the boolean to anything you like and tensorflow doesn't care. 

Heck, you can make another op completely outside the conditional to compute matmul (a, matrix_inverse(a)) and it'll chug merrily along. 

Is this the desired behavior?  


```
with graph.as_default():
    pred   = tf.placeholder(tf.bool, shape=[])

    def fnTrue():
        a      = tf.placeholder(tf.float32, shape=[5, 1], name=""column"")
        return a

    def fnFalse():
        a      = tf.placeholder(tf.float32, shape=[1, 5], name=""row"")
        return a

    a = tf.cond(pred, fnTrue, fnFalse, name=""my_conditional"")
graphWriter = tf.summary.FileWriter(logdir='logdir', graph=graph)
graphWriter.flush()

with tf.Session(graph=graph) as session:
    writer_graph         = tf.summary.FileWriter(LogPath , session.graph)
    writer_graph.flush()
    writer_graph.close()
    plTrue       = np.ones((15, 2)) * 2     # column
    plFalse      = np.ones((12, 15)) * 3     # row
    feed_dict = {
        a            : plTrue,
        pred         : False
    }

    d = session.run( a , feed_dict = feed_dict)
    print(d)
    print(d.shape)
```"
18706,Wrong variance from initialisers,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: 1.3, 1.4, 1.5, 1.6, 1.7, 1.8
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See *Source Code* section


### Describe the problem
Probably the best known fact about weight initialisation is that initial weights should have small values, close to zero. Probably, a lot of people, especially those that read [1] by LeCun et al., also know that also some properties of the distribution from which the initial values are drawn matters. This has been discussed in great detail in the literature. Xavier Glorot argued in [2] that the variance of the (initial) weights should have zero mean and a variance of 1/(fan_in + fan_out) for sigmoid activation functions (focusing on tanh). He et al. [3] noted that  for ReLUs, the weights should have zero mean and a variance of 2/fan_in in order to correct for the variance that gets lost in the negative part. Finally, the entire theory of self-normalizing networks, introduced in [4] by Klambauer et al., and the parameters for alpha and gamma for SELU activation functions, is built upon the assumptions that the weights have mean 0 and variance 1/fan_in.

The [truncated normal distribution](https://en.wikipedia.org/wiki/Truncated_normal_distribution) is a commonly used distribution for initialising weights in a neural network. After all, it looks like a Gaussian, but does not allow outliers, which could cause saturation for certain activation function from the start. It also has a mean and variance that depend on the four parameters of the truncated normal distribution: mean, standard deviation, lower boundary and upper boundary. This distribution is implemented in `tf.truncated_normal` and allows to create a truncated normal distribution with arguments for its mean and standard deviation, i.e. the standard distribution of the Gaussian before truncating (I know that this is confusing #13686). Note that the boundaries are fixed to be at 2 standard deviations at each side of the mean.

This is all fine, until one wants to create a truncated normal distribution with a specific standard deviation, which happens to be the case for the initialisers. Although the well-studied initialisers, mentioned earlier, tell us exactly what the best variances are, the `<name>_normal_initializer` lead to wrong initialisations (for examples, see below). The problem can be brought back to the `VarianceScaling` initialiser, which allows to choose between a normal and uniform distribution. When using `distribution='normal'`, a truncated normal is used under the hood. Because there is no compensation for the truncation of the normal distribution (which would have had the correct standard deviation), the variance of the generated samples do not have the required variance. This has been resolved in Theano from the start (see issue theano/theano#6381) and I wanted to tackle this problem in Keras (see issue keras-team/keras#8048), where I was redirected here (keras-team/keras#9963).

It should be possible to map the solution implemented in keras-team/keras#9963 directly to the tensorflow code. I hope this gets fixed soon, because there are probably plenty of people using wrong initialisations due to this faulty implementation.

PS: I am not eager to accept all these terms to issue a pull request for making this minor contribution directly. Thanks to anybody who would take this fix upon him/her.

TL;DR; The default initialisers using truncated normal distributions are simply wrong. By truncating a normal distribution, variance gets lost and the entire theoretical foundations of the initialisers disappear. I believe that this issue can be fixed by accounting for the truncation in the `VarianceScaling` initialiser.

### Source code / logs

    import tensorflow as tf
    import numpy as np

    fan_in, fan_out = 1000, 100

    uniform_init = tf.variance_scaling_initializer(distribution='uniform')
    normal_init = tf.variance_scaling_initializer(distribution='normal')

    print(""expected variance: {:f}"".format(1. / fan_in))
    with tf.Session() as sess:
         print("" uniform variance: {:f}"".format(
             np.var(sess.run(uniform_init((fan_in, fan_out))))))
         print(""  normal variance: {:f}"".format(
             np.std(sess.run(normal_init((fan_in, fan_out))))))

    uniform_init = tf.glorot_uniform_initializer()
    normal_init = tf.glorot_normal_initializer()

    print(""expected variance: {:f}"".format(2. / (fan_in + fan_out)))
    with tf.Session() as sess:
         print("" uniform variance: {:f}"".format(
             np.var(sess.run(uniform_init((fan_in, fan_out))))))
         print(""  normal variance: {:f}"".format(
             np.std(sess.run(normal_init((fan_in, fan_out))))))

### References
[1] LeCun, Yann, Léon Bottou, Genevieve B. Orr, and Klaus-Robert Müller. ""Efficient backprop."" In Neural networks: Tricks of the trade, pp. 9-50. Springer, Berlin, Heidelberg, 1998.
[2] Glorot, Xavier, and Yoshua Bengio. ""Understanding the difficulty of training deep feedforward neural networks."" In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249-256. 2010.
[3] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. ""Delving deep into rectifiers: Surpassing human-level performance on imagenet classification."" In Proceedings of the IEEE international conference on computer vision, pp. 1026-1034. 2015.
[4] Klambauer, Günter, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. ""Self-normalizing neural networks."" Advances in Neural Information Processing Systems (2017): 972-981.
"
18705,dynamic_rnn is much slower then manually using while_loop,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.6.0-rc1
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0/
- **GPU model and memory**: NVIDIA 1080 TI
- **Exact command to reproduce**: (see below)

``python benchmark_lstm.py -d 512 -b 60 -t 380 -n 512  -w 20 -i 40``

### Describe the problem
Tensorflow `dynamic_rnn` is over twice as slow as `static_rnn` and using a simple application of `tf.while_loop`.

I noticed while doing some profiling that `dynamic_rnn` was much slower than `static_rnn`. At first I assumed this was because of something inherently slow about using dynamic length, but then I also noticed naively using `tf.while_loop` with rnn cell was also much faster and performed comparably to `static_rnn`.

I am not sure if there is some special cases `dynamic_rnn` needs to handle that causes it to be slower, or if there something not equivalent between my `while_loop` and the `dynamic_rnn` that I am missing. However a 2x performance change is a big deal and I wanted to report this just in case it points to a possible implementation improvement.

### Source code / logs
Benchmark used:

```
import tensorflow as tf
import argparse
import numpy as np

from tqdm import tqdm


def while_loop_rnn(rnn_cell, x):
    initial_state = rnn_cell.zero_state(tf.shape(x)[0], tf.float32)
    x_t = tf.transpose(x, [1, 0, 2])

    def compute(i, cur_state, out):
        output, cur_state = rnn_cell(x_t[i], cur_state)
        return i+1, cur_state, out.write(i, output)

    time = tf.shape(x_t)[0]

    _, cur_state, out = tf.while_loop(
        lambda a, b, c: a < time,
        compute,
        (0, initial_state, tf.TensorArray(tf.float32, time))
    )
    return tf.transpose(out.stack(), [1, 0, 2]), cur_state


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(""-d"", ""--dim"", required=True, type=int)
    parser.add_argument(""-b"", ""--batch"", required=True, type=int)
    parser.add_argument(""-t"", ""--time"", required=True, type=int)
    parser.add_argument(""-n"", ""--hidden"", required=True, type=int)
    parser.add_argument(""-w"", ""--warm_up"", type=int, default=20)
    parser.add_argument(""-i"", ""--iterations"", type=int, default=20)
    args = parser.parse_args()

    cell = tf.nn.rnn_cell.LSTMCell(args.hidden)
    x = tf.constant(np.random.normal(scale=2, size=(args.batch, args.time, args.dim)).astype(np.float32))

    dynamic = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)[0]
    static = tf.stack(tf.nn.static_rnn(cell, tf.unstack(x, args.time, axis=1), dtype=tf.float32)[0], axis=1)
    while_loop = while_loop_rnn(cell, x)[0]

    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    print(""Checking equivalence..."")
    dynamic_out, static_out, while_out = sess.run([dynamic, static, while_loop])

    if not np.allclose(dynamic_out, static_out):
        raise RuntimeError()
    if not np.allclose(dynamic_out, while_out):
        raise RuntimeError()

    for name, op in zip([""dynamic"", ""static"", ""while""], [dynamic, static, while_loop]):
        print(""Testing %s"" % name)
        print(""Warming up..."")
        for _ in range(args.warm_up):
            sess.run(op)

        for _ in tqdm(range(args.iterations), ""run"", ncols=80):
            sess.run(op)

if __name__ == ""__main__"":
    main()
```
"
18703,Add tf.float16 support to tf.contrib.nccl.all_sum,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0/7.0.5
- **GPU model and memory**: TitanX 12Gb x2
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

When making a call to tf.contrib.nccl.all_sum I get the following output:

```
Traceback (most recent call last):
  File ""trainer.py"", line 393, in <module>
    main(ARGS)
  File ""trainer.py"", line 49, in main
    thread_count=args.thread_count, use_nccl=args.use_nccl
  File ""trainer.py"", line 191, in train
    use_nccl=use_nccl)
  File ""/home/thomas/Projects/Thomas/deepspeech2/deepspeech2/utils/tensorflow.py"", line 27, in tf_average_gradients
    gradient_sum_list = nccl.all_sum(flat_gradients_list)
  File ""/home/thomas/Programs/tensorflow-1.7.0/lib/python3.5/site-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py"", line 47, in all_sum
    return _apply_all_reduce('sum', tensors)
  File ""/home/thomas/Programs/tensorflow-1.7.0/lib/python3.5/site-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py"", line 228, in _apply_all_reduce
    shared_name=shared_name))
  File ""/home/thomas/Programs/tensorflow-1.7.0/lib/python3.5/site-packages/tensorflow/contrib/nccl/ops/gen_nccl_ops.py"", line 59, in nccl_all_reduce
    num_devices=num_devices, shared_name=shared_name, name=name)
  File ""/home/thomas/Programs/tensorflow-1.7.0/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 609, in _apply_op_helper
    param_name=input_name)
  File ""/home/thomas/Programs/tensorflow-1.7.0/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 60, in _SatisfiesTypeConstraint
    "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
TypeError: Value passed to parameter 'input' has DataType float16 not in list of allowed values: float32, float64, int32, int64
```

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
gradient_sum_list = nccl.all_sum(flat_gradients_list)
```
"
18700,Getting an error for tensors with partial shape information with tf.contrib.staging.StagingArea,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0/7.0.5
- **GPU model and memory**: TitanX 12Gb
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

When using tf.contrib.staging.StagingArea to prefetch data to the GPU from a tf.data pipeline. I get the following output:

```
Traceback (most recent call last):
  File ""trainer.py"", line 387, in <module>
    main(ARGS)
  File ""trainer.py"", line 49, in main
    thread_count=args.thread_count, use_nccl=args.use_nccl
  File ""trainer.py"", line 161, in train
    features = stage([features])
  File ""trainer.py"", line 112, in stage
    get_tensors = [tf.reshape(gt, t.get_shape()) for (gt,t) in zip(get_tensors, tensors)]
  File ""trainer.py"", line 112, in <listcomp>
    get_tensors = [tf.reshape(gt, t.get_shape()) for (gt,t) in zip(get_tensors, tensors)]
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 6113, in reshape
    ""Reshape"", tensor=tensor, shape=shape, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 528, in _apply_op_helper
    (input_name, err))
ValueError: Tried to convert 'shape' to a tensor and failed. Error: Cannot convert a partially known TensorShape to a Tensor: (1, ?)
```

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
def stage(tensors):
    
    staging_area = tf.contrib.staging.StagingArea(
        dtypes=[tensor.dtype for tensor in tensors],
        shapes=[tensor.get_shape() for tensor in tensors]
    )
    put_op = staging_area.put(tensors)
    get_tensors = staging_area.get()

    get_tensors = [tf.reshape(gt, t.get_shape()) for (gt,t) in zip(get_tensors, tensors)]

    return put_op, get_tensors

dataset = tf.data.TFRecordDataset([source])
dataset = dataset.map(parse, num_parallel_calls=thread_count)
dataset = dataset.apply(tf.contrib.data.padded_batch_and_drop_remainder(
    batch_size, ([None], [], [None])
))
if shuffle:
    dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(shuffle_buffer_size, 1))
dataset.prefetch(2 * gpu_count)
iterator = dataset.make_initializable_iterator()

for gpu in range(gpu_count):
    with tf.device('/device:CPU:0'):
        features, timesteps, labels = iterator.get_next()
        labels = tf.deserialize_many_sparse(labels, dtype=tf.int32)

        device = '/device:GPU:{}'.format(gpu)
        with tf.device(device):
            scope_name = 'clone_{}'.format(gpu)
            with tf.variable_scope(scope_name) as scope, tf.name_scope(scope_name):
                features = stage([features])
```
"
18697,Tensorflow Lite demo app crashes when using inception-v3/Mobilenet_v1 (float) instead of quantised mobilenet,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: Git cloned source
- **TensorFlow version (use command below)**: NA
- **Python version**: python 2.7
- **Bazel version (if compiling from source)**: bazel release 0.11.1
- **GCC/Compiler version (if compiling from source)**:  4.8.4
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: x86 8GB RAM
- **Exact command to reproduce**:  bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config android  --cpu=x86_64 --fat_apk_cpu=x86_64

### Describe the problem

Tensorflow Lite demo app crashes while launching the app, if I use Float inception model (inceptionv3_slim_2016.tflite or mobilenet_v1_1.0_224.tflite) instead of quantised mobilenet. I added the code changes from https://github.com/tensorflow/tensorflow/issues/14719 on top of the github source for moving quantised mobilenet to float inception/Mobilenet.

### Source code / logs
```
--------- beginning of crash --------- 
04-19 08:57:09.480  7131  7160 E AndroidRuntime: FATAL EXCEPTION: VideoBackground
04-19 08:57:09.480  7131  7160 E AndroidRuntime: Process: com.example.android.tflitecamerademo, PID: 7131
04-19 08:57:09.480  7131  7160 E AndroidRuntime: java.lang.IllegalArgumentException: Failed to get input dimensions. 0-th input should have 602112 bytes, but found 4291248 bytes.
04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at org.tensorflow.lite.NativeInterpreterWrapper.getInputDims(Native Method)
04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:98)
04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:143)
04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at org.tensorflow.lite.Interpreter.run(Interpreter.java:121)
04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at com.example.android.tflitecamerademo.ImageClassifierFloatInception.runInference(ImageClassifierFloatInception.java:123)
04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:121)
04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at com.example.android.tflitecamerademo.MainActivity.classifyFrame(MainActivity.java:496)
04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at com.example.android.tflitecamerademo.MainActivity.access$600(MainActivity.java:54)
04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at com.example.android.tflitecamerademo.MainActivity$2.run(MainActivity.java:459)
04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at android.os.Handler.handleCallback(Handler.java:790)
04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at android.os.Handler.dispatchMessage(Handler.java:99)
04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at android.os.Looper.loop(Looper.java:164)
04-19 08:57:09.480  7131  7160 E AndroidRuntime:        at android.os.HandlerThread.run(HandlerThread.java:65)
04-19 08:57:06.663  7131  7131 I chatty  : uid=10073(com.example.android.tflitecamerademo) identical 3 lines
04-19 08:57:06.680  7131  7131 D ViewRootImpl[MainActivity]: updatePointerIcon called with position out of bounds
04-19 08:57:09.482  5448  5591 W ActivityManager:   Force finishing activity com.example.android.tflitecamerademo/.MainActivity
```
-----------------------------------------------------------
"
18696,Unexpected behavior with Estimator warm start,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.12.6
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 2.7.14
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
I've run into two issues when using a warm start with the `Estimator` API (please let me know if you'd like me to file these as separate issues):
1. If checkpoints exist in the `model_dir` then the warm start is overridden when the session is created.
2. The warm start is performed on every `train` call, which means warm starting can't be used in the context of `train_and_evaluate` (or in a manual loop, unless reinitializing the `Estimator`).

I'd be willing (pending corporate CLA) to submit a patch if you can clarify the desired behavior. I would personally expect that:
1. Checkpoint restore is skipped on session creation if a warm start was performed.
2. The warm start settings would be cleared after the first `train` call. (However, if this is the desired behavior, looking longer term it seems like warm start settings shouldn't be a property of the `Estimator` instance.)
"
18693,supporting vector parameters for mixture ,"Hi,
In the example below:

# Create a mixture of two Gaussians:
tfd = tf.contrib.distributions
mix = 0.3
bimix_gauss = tfd.Mixture(
  cat=tfd.Categorical(probs=[mix, 1.-mix]),
  components=[
    tfd.Normal(loc=-1., scale=0.1),
    tfd.Normal(loc=+1., scale=0.5),
])

**_# Why cannot I apply it to vector parameters?_** 

    tfd.Normal(loc=[-1, 0]., scale=[0.1, 0.2),
    tfd.Normal(loc=[+1., 2.], scale=[0.5, 3.]),



Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
18692,Documentation mistake?,"Hi,

The documentation [page](https://www.tensorflow.org/api_docs/python/tf/unstack) of `tf.unstack` says the equivalent is np.unstack. But np doesn't have an unstack method? (Should have been np.split ? ) 

Cheers,"
18691,Periodic overhead when using tensorflow dataset for model training on GPU,"As you can see it in the following code, I am trying to train a simple model on Tensorflow with a Tensorflow Dataset. The dataset is pretty huge (2000 exemples of each 300*2000 elements). I shuffle, repeat and batch the dataset in order to do a stochastic gradient descent for training my model.

But I can observe a period overhead of the optimisation step (it is sess.run(train) in my code).

As you can see it here, every 5 steps, it needs 3s instead of 0.5 to do the optimisation.

Step 105 duration : 3.5233473777770996

Step 106 duration : 0.5653283596038818

Step 107 duration : 0.5391891002655029

Step 108 duration : 0.5480048656463623

Step 109 duration : 0.0415492057800293

Step 110 duration : 3.032115936279297

Step 111 duration : 0.5407207012176514

Step 112 duration : 0.5276811122894287

Step 113 duration : 0.5448746681213379

Step 114 duration : 0.04253268241882324

Step 115 duration : 3.1273345947265625

Moreover my GPU is almost all the time at 0% utilisation with around 90% of the memory used.

It seems that this overhead arrived when the Iterator finish to see all the dataset.


Do you have any idea how I can speed up my training ?
It is due to the fact that my model is really simple ? 

Best,

------------------------

### System information
- **Have I written custom code **:  named Test_TimeDataset.py
- **OS Platform and Distribution **:  Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version **: 1.4
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 8.0 and cuDNN 6.0.21
- **GPU model and memory**: GeForce 1080 with 11Go 
- **Exact command to reproduce**: python Test_TimeDataset.py

I also reproduce it with 
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version **: 1.8 
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 9.0 and cuDNN 7.1.3.16
On the same machine (I have both versions on the same Ubuntu session).

But I am not sure that Tensorflow use the cuDNN library.

You can also found the code [here](https://github.com/nicaogr/Git_Issues/blob/master/Test_TimeDataset.py)

### Source code
``
import tensorflow as tf
import numpy as np
import os, time, multiprocessing
import matplotlib.pyplot as plt

def _floats_feature(value):
    return tf.train.Feature(float_list=tf.train.FloatList(value=value.reshape(-1)))


def parser(record):
    num_features = 2000
    size_group = 300
    num_classes= 10
    class_indice = 0
    keys_to_features={
                'X': tf.FixedLenFeature([size_group*num_features],tf.float32),
                'label' : tf.FixedLenFeature([num_classes],tf.float32)}
    parsed = tf.parse_single_example(record, keys_to_features)
    
    label = parsed['label']
    label = tf.slice(label,[class_indice],[1])
    label = tf.squeeze(label) # To get a vector one dimension
    X = parsed['X']
    X= tf.reshape(X, [size_group,num_features])
    return X, label


def test_train_w_dataset(): 
    num_features = 2000
    num_ex = 2000
    size_group = 300
    num_classes = 10
    batch_size= 480
    max_iters = 300
    buffer_size = 10000
    
    # Creation of the Dataset 
    filename_tfrecords = 'tmp.tfrecords'
    if not(os.path.isfile(filename_tfrecords)): # If the file doesn't exist we will create it
        print(""Start creating the Dataset"")
        writer = tf.python_io.TFRecordWriter(filename_tfrecords)
        
        for i in range(num_ex):
            if i % 1000 == 0: print(""Step :"",i)
            X = np.random.normal(size=(size_group,num_features))
            vectors =  2*np.random.randint(0,2,(num_classes,1))-1
            features=tf.train.Features(feature={
                        'X': _floats_feature(X),
                        'label' : _floats_feature(vectors)})
            example = tf.train.Example(features=features)       
            writer.write(example.SerializeToString())
        writer.close()
    else:
        print(""The dataset tfrecords already exist"")
     
    train_dataset = tf.data.TFRecordDataset(filename_tfrecords)
    num_proc = multiprocessing.cpu_count()
    train_dataset = train_dataset.map(parser,
                                        num_parallel_calls=num_proc)
    dataset_shuffle = train_dataset.shuffle(buffer_size=buffer_size,
                                                 reshuffle_each_iteration=True) 
    dataset_shuffle = dataset_shuffle.batch(batch_size)
    dataset_shuffle = dataset_shuffle.repeat() 
    dataset_shuffle = dataset_shuffle.prefetch(batch_size) 
    shuffle_iterator = dataset_shuffle.make_initializable_iterator()
    X_, y_ = shuffle_iterator.get_next()

    W=tf.Variable(tf.random_normal([num_features], stddev=1.),name=""weights"")
    W=tf.reshape(W,(1,1,num_features))
    Prod=tf.reduce_sum(tf.multiply(W,X_),axis=2)
    Max=tf.reduce_max(Prod,axis=1)
    Tan= tf.reduce_sum(tf.multiply(tf.tanh(Max),y_))
    loss= tf.add(Tan,tf.reduce_sum(tf.multiply(W,W)))

    LR = 0.01
    restarts = 1
    optimizer = tf.train.GradientDescentOptimizer(LR) 
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    train = optimizer.minimize(loss)  
    print(""The graph is defined"")
    sess = tf.Session(config=config)
        
    durationTab = []
    
    for essai in range(restarts+1):
        t0 = time.time()
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())
        sess.run(shuffle_iterator.initializer)
        t1 = time.time()
        duration = t1 - t0
        print('Duration of initialization : ',duration)
        for step in range(max_iters):
            t0 = time.time()
            sess.run(train)
            t1 = time.time()
            duration = t1 - t0
            print(""Step "",str(step),' duration : ',duration)
            durationTab += [duration]
            

    plt.plot(durationTab)
    plt.ylabel('Duration')
    plt.xlabel('Iteration')
    plt.show()

if __name__ == '__main__':
    test_train_w_dataset()

``"
18690,problems with word2vec_basic.py,"### System information
- **OS Platform and Distribution: Linux Ubuntu 16.04
- **TensorFlow version (use command below)**: 1.6
- **Python version**: 3.5



### the problem
happened in the 'loss' function
Did anyone else meet this problem?
 
### Source code / logs
  loss = tf.reduce_mean(
      tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,
                     num_sampled, vocabulary_size))


TypeError: Input 'b' of 'MatMul' Op has type float32 that does not match type int32 of argument 'a'.
"
18689,[Request] Pre-build support old CPU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NA
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 9.4 (stretch)
- **CPU model**: Intel(R) Xeon(R) CPU E7- 8837  @ 2.67GHz and any ""old"" CPU
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: import tensorflow

### Describe the problem
Since version 1.6 prebuilt binaries use AVX instructions, thus TF cannot be used on ""old"" CPUs. However, many of them are not that old. In fact, I was surprise that my server cannot run TF. I believe many people run into the same problem because those CPUs are popular in many labs at universities.

This is bad because it blocks many potential users/researchers from trying TF and starting using TF. User should not build TF themselves, because building by each user is daunting and resource-wasteful.

So if it is not technically impossible, please provide alternative prebuilt binaries that support old CPUs."
18688,Estimator.predict() has Shape Issues?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 8.1
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.6
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
-**Exact command to reproduce**:
Just execture run.py in this [repository](https://github.com/selcouthlyBlue/ShapeErrorReproduce)

### Describe the problem
I've already posted this [problem](https://stackoverflow.com/questions/49911525/estimator-predict-has-shape-issues?noredirect=1#comment86845047_49911525) in stackoverflow. Unfortunately, no one can figure it out there so I'm posting it here as well.

I can train and evalaute a Tensorflow Estimator model without any problems. When I do prediction, this error arises:

```
InvalidArgumentError (see above for traceback): output_shape has incorrect number of elements: 79 should be: 2
	 [[Node: output = SparseToDense[T=DT_INT32, Tindices=DT_INT32, validate_indices=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ToInt32, ToInt32_1, ToInt32_2, bidirectional_rnn/bidirectional_rnn/fw/fw/time)]]
```


All of the model functions I made use the same architecture:

```
def _train_model_fn(features, labels, mode, params):
    features, outputs = _get_fed_features_and_resulting_predictions(features, params['num_classes'])
    predictions = {""outputs"": outputs}

    ... # loss initialization and whatnot

    return _create_model_fn(mode,
                                            predictions,
                                            loss,
                                            train_op)

def _predict_model_fn(features, mode, params):
    features, outputs = _get_fed_features_and_resulting_predictions(features, params['num_classes'])
    predictions = {""outputs"": outputs}

    return _create_model_fn(mode, predictions,
                            export_outputs={
                                ""outputs"": tf.estimator.export.PredictOutput(outputs)
                            })


def _create_model_fn(mode, predictions, loss=None, train_op=None, export_outputs=None):
    return tf.estimator.EstimatorSpec(mode=mode,
                                      predictions=predictions,
                                      loss=loss,
                                      train_op=train_op,
                                      export_outputs=export_outputs)
```

Here's the training code and the predict code:

```
def train(features, labels, num_classes, params, checkpoint_dir,
          batch_size=1, num_epochs=1, save_checkpoint_every_n_epochs=1):
    num_steps_per_epoch = len(features) // batch_size
    save_checkpoint_steps = save_checkpoint_every_n_epochs * num_steps_per_epoch
    params['num_classes'] = num_classes
    params['log_step_count_steps'] = num_steps_per_epoch
    estimator = tf.estimator.Estimator(model_fn=_train_model_fn,
                                       params=params,
                                       model_dir=checkpoint_dir,
                                       config=tf.estimator.RunConfig(
                                           save_checkpoints_steps=save_checkpoint_steps,
                                           log_step_count_steps=num_steps_per_epoch,
                                           save_summary_steps=num_steps_per_epoch
                                       ))
    estimator.train(input_fn=_input_fn(features, labels, batch_size),
                    steps=num_epochs * num_steps_per_epoch)


def predict(features, params, checkpoint_dir):
    estimator = tf.estimator.Estimator(model_fn=_predict_model_fn,
                                       params=params,
                                       model_dir=checkpoint_dir)
    predictions = estimator.predict(input_fn=_input_fn(features))
    for i, p in enumerate(predictions):
        print(i, p)
```

I also checked the shapes given every time the input passes a layer when training, and the same thing for predicting. They give the same shapes:

Training:

```
conv2d [1, 358, 358, 16]
max_pool2d [1, 179, 179, 16]
collapse_to_rnn_dims [1, 179, 2864]
birnn [1, 179, 64]
```

Prediction:

```
conv2d [1, 358, 358, 16]
max_pool2d [1, 179, 179, 16]
collapse_to_rnn_dims [1, 179, 2864]
birnn [1, 179, 64]
```

Here are the `SparseTensors` that are passed to `sparse_to_dense`:

Training:

`SparseTensor(indices=Tensor(""CTCBeamSearchDecoder:0"", shape=(?, 2), dtype=int64), values=Tensor(""CTCBeamSearchDecoder:1"", shape=(?,), dtype=int64), dense_shape=Tensor(""CTCBeamSearchDecoder:2"", shape=(2,), dtype=int64))`

Prediction:

`SparseTensor(indices=Tensor(""CTCBeamSearchDecoder:0"", shape=(?, 2), dtype=int64), values=Tensor(""CTCBeamSearchDecoder:1"", shape=(?,), dtype=int64), dense_shape=Tensor(""CTCBeamSearchDecoder:2"", shape=(2,), dtype=int64))`

Which are all pretty much the same.

Any reason why this is happening? Shouldn't the `_predict_model_fn` work given that it follows the same architecture as that of the other `model_fn`s?

Here's the full stacktrace: (Note: I called the predict function after evaluation)

```
INFO:tensorflow:Using config: {'_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000006041C05978>, '_model_dir': 'model-20180420-112636', '_task_type': 'worker', '_keep_checkpoint_every_n_hours': 10000, '_task_id': 0, '_session_config': None, '_num_worker_replicas': 1, '_global_id_in_cluster': 0, '_service': None, '_tf_random_seed': None, '_log_step_count_steps': 100, '_evaluation_master': '', '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_master': '', '_keep_checkpoint_max': 5, '_is_chief': True, '_num_ps_replicas': 0, '_save_checkpoints_secs': 600}
INFO:tensorflow:Calling model_fn.
[1, 179, 64]
INFO:tensorflow:Done calling model_fn.
SparseTensor(indices=Tensor(""CTCBeamSearchDecoder:0"", shape=(?, 2), dtype=int64), values=Tensor(""CTCBeamSearchDecoder:1"", shape=(?,), dtype=int64), dense_shape=Tensor(""CTCBeamSearchDecoder:2"", shape=(2,), dtype=int64))
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from model-20180420-112636\model.ckpt-1
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
Traceback (most recent call last):
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1361, in _do_call
    return fn(*args)
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1340, in _run_fn
    target_list, status, run_metadata)
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: output_shape has incorrect number of elements: 82 should be: 2
	 [[Node: output = SparseToDense[T=DT_INT32, Tindices=DT_INT32, validate_indices=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ToInt32, ToInt32_1, ToInt32_2, bidirectional_rnn/bidirectional_rnn/fw/fw/time)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/asus.11/Documents/ShapeErrorReproduce/run.py"", line 56, in <module>
    main()
  File ""C:/Users/asus.11/Documents/ShapeErrorReproduce/run.py"", line 52, in main
    predict_with_model(checkpoint_dir)
  File ""C:/Users/asus.11/Documents/ShapeErrorReproduce/run.py"", line 46, in predict_with_model
    predict(images, run_params, checkpoint_dir)
  File ""C:\Users\asus.11\Documents\ShapeErrorReproduce\experiment_ops.py"", line 173, in predict
    for i, p in enumerate(predictions):
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 492, in predict
    preds_evaluated = mon_sess.run(predictions)
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 546, in run
    run_metadata=run_metadata)
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1022, in run
    run_metadata=run_metadata)
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1113, in run
    raise six.reraise(*original_exc_info)
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\six.py"", line 693, in reraise
    raise value
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1098, in run
    return self._sess.run(*args, **kwargs)
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1170, in run
    run_metadata=run_metadata)
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 950, in run
    return self._sess.run(*args, **kwargs)
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 905, in run
    run_metadata_ptr)
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1137, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1355, in _do_run
    options, run_metadata)
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1374, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: output_shape has incorrect number of elements: 82 should be: 2
	 [[Node: output = SparseToDense[T=DT_INT32, Tindices=DT_INT32, validate_indices=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ToInt32, ToInt32_1, ToInt32_2, bidirectional_rnn/bidirectional_rnn/fw/fw/time)]]

Caused by op 'output', defined at:
  File ""C:/Users/asus.11/Documents/ShapeErrorReproduce/run.py"", line 56, in <module>
    main()
  File ""C:/Users/asus.11/Documents/ShapeErrorReproduce/run.py"", line 52, in main
    predict_with_model(checkpoint_dir)
  File ""C:/Users/asus.11/Documents/ShapeErrorReproduce/run.py"", line 46, in predict_with_model
    predict(images, run_params, checkpoint_dir)
  File ""C:\Users\asus.11\Documents\ShapeErrorReproduce\experiment_ops.py"", line 173, in predict
    for i, p in enumerate(predictions):
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 479, in predict
    features, None, model_fn_lib.ModeKeys.PREDICT, self.config)
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 793, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""C:\Users\asus.11\Documents\ShapeErrorReproduce\experiment_ops.py"", line 129, in _predict_model_fn
    features, outputs = _get_fed_features_and_resulting_predictions(features, params['num_classes'])
  File ""C:\Users\asus.11\Documents\ShapeErrorReproduce\experiment_ops.py"", line 124, in _get_fed_features_and_resulting_predictions
    outputs = _get_decoded_outputs(features, num_classes)
  File ""C:\Users\asus.11\Documents\ShapeErrorReproduce\experiment_ops.py"", line 73, in _get_decoded_outputs
    name=""output"")
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\ops\sparse_ops.py"", line 791, in sparse_to_dense
    name=name)
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_sparse_ops.py"", line 2401, in _sparse_to_dense
    name=name)
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 3271, in create_op
    op_def=op_def)
  File ""C:\Users\asus.11\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): output_shape has incorrect number of elements: 82 should be: 2
	 [[Node: output = SparseToDense[T=DT_INT32, Tindices=DT_INT32, validate_indices=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ToInt32, ToInt32_1, ToInt32_2, bidirectional_rnn/bidirectional_rnn/fw/fw/time)]]
```

Apparently, the shape error also changes with every run."
18687,Absence of 'tanh()' operation in the computation of attention vector,"Tanh() operation is missed in the computation of attention vector (https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py), which is mentioned useful in the definition of Attetntion vector(https://www.tensorflow.org/tutorials/seq2seq).

OS : Ubuntu 14.04.4 LTS
TensorFlow installed from: (https://files.pythonhosted.org/packages/f7/52/4f78cc674775bd36a28223fd63d98260eb7130128298f7c70edbdcb34075/tensorflow-1.4.1-cp34-cp34m-manylinux1_x86_64.whl)   via pip 
TensorFlow version: tensorflow-gpu 1.4.1 
CUDA/cuDNN version: 8.0
GPU model and memory: Tesla K80

The problem is found when I compare the attention vector value, computed with Numpy and network's weights, to those retrieved from Attention Vector tensor. These two vectors are the same when I did not apply tanh(), meaning a tanh() opertion is missed with regards to the Attention vector's definition."
18684,How to use float inception model instead of quantised mobile net.,"OS Platform and Distribution: Ubuntu 14.04
TensorFlow installed from: Git cloned
TensorFlow version: N/A
Bazel version: Build label: 0.11.1
CUDA/cuDNN version: N/A
GPU model and memory: x86 8GB RAM
Exact command to reproduce: Using Android studio as a existing project
Have I written custom code: no

How to use float inception model instead of quantised mobile net."
18677,Tensorflow not respecting device placement for custom ops,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.7.0-3-g024aecf414 1.7.0
- **Python version**:  3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609, for the custom op
- **CUDA/cuDNN version**: nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Sep__1_21:08:03_CDT_2017
Cuda compilation tools, release 9.0, V9.0.176
- **GPU model and memory**: GTX 1080 Ti


### Exact command to reproduce

Folder for reproducing the issue:

https://github.com/proteneer/khan/tree/tf_bug_repro/gpu_featurizer/debug.py

0. Compile with instructions in README.build

1. Run debug.py

log device placement shows that the op is placed on a gpu:

2018-04-18 17:24:01.008389: I tensorflow/core/common_runtime/placer.cc:884] Featurize: (Featurize)/job:localhost/replica:0/task:0/device:GPU:0

however, it actually runs on the CPU:

``` bash
stdout: RUNNING ON CPU!! (this is emitted by the actual CPU kernel in C++)
```

2. Removing the reshape line (line 55) in debug.py:

``` python
f0, f1, f2, f3 = tf.reshape(f0, (-1, 384)), tf.reshape(f1, (-1, 384)), tf.reshape(f2, (-1, 384)), tf.reshape(f3, (-1, 384))
```
Successfully triggers the custom op to be run on the GPU:

``` bash
stdout: RUNNING ON GPU.. (this is emitted by the actual GPU kernel in C++)
```

3. Removing the registration line in ani_op.cpp:

``` cpp
REGISTER_KERNEL_BUILDER(Name(""Featurize"").HostMemory(""acs"").Device(DEVICE_CPU), AniCombined<CPUDevice>);
```

Restores the intended behavior as in runs properly on the GPU with or without the reshape. But cripples the op in that it only works on GPUs.

### Describe the problem

This should be a bug since we should be enforcing placement of the op onto the GPU. See steps to reproduce the bug. Oddly enough the custom Featurize op in addition to the Reshape op is shown being placed on the GPU:

2018-04-18 17:24:01.008389: I tensorflow/core/common_runtime/placer.cc:884] Featurize: (Featurize)/job:localhost/replica:0/task:0/device:GPU:0
Reshape_3: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-18 17:24:01.008404: I tensorflow/core/common_runtime/placer.cc:884] Reshape_3: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
Reshape_2: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-18 17:24:01.008418: I tensorflow/core/common_runtime/placer.cc:884] Reshape_2: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-18 17:24:01.008432: I tensorflow/core/common_runtime/placer.cc:884] Reshape_1: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-18 17:24:01.008446: I tensorflow/core/common_runtime/placer.cc:884] Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
Reshape_3/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-18 17:24:01.008462: I tensorflow/core/common_runtime/placer.cc:884] Reshape_3/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
Reshape_2/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-18 17:24:01.008476: I tensorflow/core/common_runtime/placer.cc:884] Reshape_2/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
Reshape_1/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-18 17:24:01.008490: I tensorflow/core/common_runtime/placer.cc:884] Reshape_1/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
Reshape/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-18 17:24:01.008505: I tensorflow/core/common_runtime/placer.cc:884] Reshape/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0


### Source code / logs

All the code required is inside here:

https://github.com/proteneer/khan/tree/tf_bug_repro/gpu_featurizer"
18676,Bug: CPU/Eigen fp16 matmul kernel fails on AVX512 systems,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
('v1.8.0-rc0-561-g075fbb59d7', '1.8.0-rc0')
- **Python version**: 
Python 2.7.12
- **Bazel version (if compiling from source)**:
Build label: 0.11.0
- **GCC/Compiler version (if compiling from source)**:
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
- **CUDA/cuDNN version**:
NA
- **GPU model and memory**:
NA
- **Exact command to reproduce**:
bazel test --config=opt  -s //tensorflow/python/kernel_tests:batch_matmul_op_test

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

bazel test --config=opt  -s //tensorflow/python/kernel_tests:batch_matmul_op_test
On machines with AVX512, batch_matmul_op_test fails. Most likely culprit is Eigen's fp16 matmul running on CPU.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

======================================================================
FAIL: testBatchMatmulOp_float16_False_True_True (__main__.BatchMatmulOpTest)
----------------------------------------------------------------------

AssertionError: 
Not equal to tolerance rtol=0.0976562, atol=0.0976562
Mismatched value: a is different from b.
(mismatch 25.8125%)
 x: array([[[ 7532.,  8860.,  6856., ...,  6372.,  8070.,  8264.],
        [10400.,  9840.,  7188., ...,  8360.,  7640.,  9010.],
        [ 8172.,  6590.,  3046., ...,  7400.,  6224.,  7656.],...
 y: array([[[ 7824.,  8156.,  6804., ...,  6372.,  8064.,  8264.],
        [10140., 11500.,  8380., ...,  8360.,  7644.,  9010.],
        [ 8080.,  7964.,  5756., ...,  7404.,  6224.,  7656.],...

----------------------------------------------------------------------
Ran 4 tests in 0.681s

FAILED (failures=1)
not close where =  (array([0, 0, 0, ..., 9, 9, 9]), array([ 0,  0,  0, ..., 63, 63, 63]), array([ 4,  8, 10, ..., 10, 12, 13]))
not close lhs =  [ 6570.  7150.  7024. ...  8530.  7310. 10290.]
not close rhs =  [ 5820.  5290.  9060. ... 10680.  6480.  9280.]
not close dif =  [ 748. 1864. 2032. ... 2152.  832. 1008.]
not close tol =  [ 568.5  516.5  884.5 ... 1043.   633.   906. ]
dtype = float16, shape = (10, 64, 30)
"
18673,gradient_override_map for tf.distributions.Bernoulli,"I want to override gradient calculation for a series of ops that computes

`v = tf.reduce_mean(tf.distributions.Bernoulli(prob=[x]*10).sample())`

Here, shape of v is same as shape of x and I want dv/dx = input gradient at v

Now I know one can register gradients for a single op but how about a series of ops. If I try to register gradient for individual ops in the series I get error with shapes of input and gradient at one of the ops in the series.

Update 1:
I have written custom code for this purpose
OS Platform and Distribution I am using is Ubuntu 16.04 LTS
TensorFlow installed from - pip 
TensorFlow version - 1.5.0
Bazel version - NA
CUDA/cuDNN version - CuDNN9
GPU model and memory - GForce GTX 1060TI
Exact command to reproduce - NA

Is this possible in tensorflow?
Thanks in advance!!"
18671,[feature request] should tf.name_scope be deprecated in favor tf.variable_scope,"Hi everyone.
### Describe the problem
From my understanding, I believe that the main difference between `tf.name_scope` vs `tf.variable_scope` is that `tf.name_scope` only records ops while `tf.variable_scope` records both ops and variables in the graph.

Then here's the question that I'm raising, why is it still necessary to have both. IMHO it seems like a redundancy and unnecessarily cluttering the style of the code when you write since you can achieve the same and more with `tf.variable_scope`.

Should `tf.name_scope` be removed in a future release?

------------------------
TF version: 1.7
OS Platform and Distribution N/A
TensorFlow installed from N/A
TensorFlow version N/A
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce N/A"
18669,Unable to reuse opaque_kernel variable in CudnnLSTM for FP16,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary using GPUs
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**:  Cuda 9.0/ CuDNN 7.0.5
- **GPU model and memory**:  Volta(V100), 16 GB
- **Exact command to reproduce**: Please see the code below

### Describe the problem
Unable to reuse opaque_kernel variables with FP16 data type. Works for FP32 data type. Here is the stack trace:

 File ""cudnn_lstm_example.py"", line 37, in <module>
    outputs_2, _ = lstm(inputs_2)
  File ""/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 696, in __call__
    self.build(input_shapes)
  File ""/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py"", line 358, in build
    ""opaque_kernel"", initializer=opaque_params_t, validate_shape=False)
  File ""/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1297, in get_variable
    constraint=constraint)
  File ""/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1093, in get_variable
    constraint=constraint)
  File ""/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 431, in get_variable
    return custom_getter(**custom_getter_kwargs)
  File ""/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py"", line 291, in _update_trainable_weights
    variable = getter(*args, **kwargs)
  File ""/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 408, in _true_getter
    use_resource=use_resource, constraint=constraint)
  File ""/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 758, in _get_single_variable
    found_type_str))
ValueError: Trying to share variable cudnn_rnn/cudnn_bi_lstm/opaque_kernel, but specified dtype float32 and found dtype float16_ref.

### Source code / logs

```
import tensorflow as tf

num_layers = 1
num_units = 40
batch_size = 60
dir_count = 2

inputDType= tf.float16 # Does not work
#inputDType = tf.float32 # Works

inputs_1 = tf.random_uniform([
    num_layers * dir_count, batch_size, num_units], dtype=inputDType)

inputs_2 = tf.random_uniform([
    num_layers * dir_count, batch_size, num_units], dtype=inputDType)

with tf.variable_scope(""cudnn_rnn"", reuse=False):
    lstm = tf.contrib.cudnn_rnn.CudnnLSTM(
        num_layers=num_layers,
        num_units=num_units,
        direction=""bidirectional"",
        dtype=inputDType,
        name=""cudnn_bi_lstm"")
                
    outputs_1, _ = lstm(inputs_1)

with tf.variable_scope(""cudnn_rnn"", reuse=True):
    lstm = tf.contrib.cudnn_rnn.CudnnLSTM(
        num_layers=num_layers,
        num_units=num_units,
        direction=""bidirectional"",
        dtype=inputDType,
        name=""cudnn_bi_lstm"")

    outputs_2, _ = lstm(inputs_2)

loss1 = tf.reduce_sum(outputs_1)
loss2 = tf.reduce_sum(outputs_2)
loss= loss1+loss2
var = lstm.trainable_variables[0]

grad = tf.gradients(loss, var)[0]
print('grad.shape: %s' % grad.shape)
print('var.shape: %s' % var.shape)

opt = tf.train.AdamOptimizer()
train_op = opt.apply_gradients([(grad, lstm.trainable_variables[0])])

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(outputs_1))
    print(sess.run(outputs_2))
    sess.run(train_op)
    print(sess.run(outputs_1))
    print(sess.run(outputs_2))
```
"
18665,"Feature request: tf.layers.Layer.{trainable_variables,updates,...} should return values from all sublayers","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 0.12.0
- **GCC/Compiler version (if compiling from source)**: 7.2
- **CUDA/cuDNN version**: 7.1
- **GPU model and memory**: Nvidia Titan Xp (12 GB)

### Describe the problem
The properties trainable_variables, updates, and so on of tf.layers.Layer currently only returns the corresponding objects from the layer itself. This is different from Keras where it adds the corresponding objects from all child layers.
Please consider to make the behaviour of tf.layers.Layer the same as Keras. In it's current form, these properties are not really helpful."
18661,Document cache shuffle repeat order,"https://github.com/tensorflow/tensorflow/issues/15343#comment-351817810

/cc @saeta @jsimsa"
18660,Bug: tf.keras.estimator.model_to_estimator() API giving error when Keras model contains Lambda layer.,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.7
- **Python version**:  3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
`estimator_vae = tf.keras.estimator.model_to_estimator(keras_model=vae)`

### Describe the problem
 `tf.keras.estimator.model_to_estimator()` API is failing when Keras model contains `Lambda ` layer.
The error I am getting is `SystemError: unknown opcode`
The problem seems to be only there when I am using custom functions inside the Keras mode.

### Source code / logs
I implemented a VAE in Keras and was trying to convert it into an TF estimator model. The model works and trains using Keras. The Keras model has functions for gaussian sampling and VAE training loss. The code and  error trace back is given below.

```
#Encoder network, mapping inputs to our latent distribution parameters:
x = Input(batch_shape=(batch_size, original_dim),name='encoder_input')
encoded = Dense(intermediate_dim, activation='relu',name='encoder_dense_1')(x)
z_mean = Dense(latent_dim,name='z_mean')(encoded)
z_log_var = Dense(latent_dim,name='z_log_var')(encoded)

# Sampling from Gaussian
def sampling(args):
    
    z_mean, z_log_var = args

    epsilon = tf.random_normal(shape=(batch_size, latent_dim),
                               mean=0., stddev=epsilon_std) 
    
    return z_mean + tf.exp(z_log_var/2) * epsilon
z = Lambda(sampling,name='z')([z_mean, z_log_var]) 

#Compute VAE loss
def vae_loss(x, x_decoded):
    x_mse_loss = original_dim*tf.keras.losses.mean_squared_error(tf.layers.flatten(x), tf.layers.flatten(x_decoded))
    beta = 4.0
    kl_loss = - 0.5*beta* tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)
    return tf.reduce_mean(x_mse_loss + kl_loss)

# Map these sampled latent points back to reconstructed inputs:
#Decoder network layers
decoder_dense_1 = Dense(intermediate_dim, activation='relu',name='decoder_dense_1')
decoder_output = Dense(48, activation='relu',name='decoder_output')
decoded = decoder_dense_1(z)
x_decoded = decoder_output(decoded)

# end-to-end autoencoder
vae = Model(x, x_decoded)
vae.compile(optimizer='adam',loss=vae_loss) 
vae.summary()
#Converting to tf estimator
estimator_vae = tf.keras.estimator.model_to_estimator(keras_model=vae)
```
Output:

```
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
encoder_input (InputLayer)      (50, 48)             0                                            
__________________________________________________________________________________________________
encoder_dense_1 (Dense)         (50, 24)             1176        encoder_input[0][0]              
__________________________________________________________________________________________________
z_mean (Dense)                  (50, 10)             250         encoder_dense_1[0][0]            
__________________________________________________________________________________________________
z_log_var (Dense)               (50, 10)             250         encoder_dense_1[0][0]            
__________________________________________________________________________________________________
z (Lambda)                      (50, 10)             0           z_mean[0][0]                     
                                                                 z_log_var[0][0]                  
__________________________________________________________________________________________________
decoder_dense_1 (Dense)         (50, 24)             264         z[0][0]                          
__________________________________________________________________________________________________
decoder_output (Dense)          (50, 48)             1200        decoder_dense_1[0][0]            
==================================================================================================
Total params: 3,140
Trainable params: 3,140
Non-trainable params: 0
__________________________________________________________________________________________________
INFO:tensorflow:Using the Keras model provided.
INFO:tensorflow:Using default config.
WARNING:tensorflow:Using temporary folder as model directory: C:\Users\SPLATH~1\AppData\Local\Temp\tmps7nagdhz
INFO:tensorflow:Using config: {'_model_dir': 'C:\\Users\\SPLATH~1\\AppData\\Local\\Temp\\tmps7nagdhz', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000238136D9DA0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
---------------------------------------------------------------------------
SystemError                               Traceback (most recent call last)
<ipython-input-10-77cc01c33881> in <module>()
     13 vae.summary()
     14 #Converting to tf estimator
---> 15 estimator_vae = tf.keras.estimator.model_to_estimator(keras_model=vae)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\_impl\keras\estimator.py in model_to_estimator(keras_model, keras_model_path, custom_objects, model_dir, config)
    481                            estimator,
    482                            custom_objects,
--> 483                            keras_weights)
    484   elif keras_model.built:
    485     logging.warning('You are creating an Estimator from a Keras model '

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\_impl\keras\estimator.py in _save_first_checkpoint(keras_model, estimator, custom_objects, keras_weights)
    396       training_util.create_global_step()
    397       model = _clone_and_build_model(model_fn_lib.ModeKeys.TRAIN, keras_model,
--> 398                                      custom_objects)
    399       if isinstance(model, models.Sequential):
    400         model = model.model

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\_impl\keras\estimator.py in _clone_and_build_model(mode, keras_model, custom_objects, features, labels)
    270         model = models.clone_model(keras_model, input_tensors=input_tensors)
    271     else:
--> 272       model = models.clone_model(keras_model, input_tensors=input_tensors)
    273   else:
    274     model = keras_model

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\_impl\keras\models.py in clone_model(model, input_tensors)
    261     return _clone_sequential_model(model, input_tensors=input_tensors)
    262   else:
--> 263     return _clone_functional_model(model, input_tensors=input_tensors)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\_impl\keras\models.py in _clone_functional_model(model, input_tensors)
    166               kwargs['mask'] = computed_masks
    167           output_tensors = generic_utils.to_list(layer(computed_tensors,
--> 168                                                        **kwargs))
    169           output_masks = generic_utils.to_list(
    170               layer.compute_mask(computed_tensors, computed_masks))

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\_impl\keras\engine\base_layer.py in __call__(self, inputs, **kwargs)
    237     """"""
    238     # Actually call the layer (optionally building it).
--> 239     output = super(Layer, self).__call__(inputs, **kwargs)
    240     if context.executing_eagerly():
    241       return output

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\layers\base.py in __call__(self, inputs, *args, **kwargs)
    712 
    713         if not in_deferred_mode:
--> 714           outputs = self.call(inputs, *args, **kwargs)
    715           if outputs is None:
    716             raise ValueError('A layer\'s `call` method should return a Tensor '

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\_impl\keras\layers\core.py in call(self, inputs, mask)
    640     if has_arg(self.function, 'mask'):
    641       arguments['mask'] = mask
--> 642     return self.function(inputs, **arguments)
    643 
    644   def compute_mask(self, inputs, mask=None):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\_impl\keras\layers\core.py in sampling(args)
     16 def sampling(args):
     17     #import tensorflow as tf
---> 18     z_mean, z_log_var = args
     19 
     20     epsilon = tf.random_normal(shape=(batch_size, latent_dim),

SystemError: unknown opcode
```
"
18658,TensorflowLite - Android,"I am using TensorflowLite for Android. I tried setting up the java demo project as described in Tensorflow site (Android demo app) and succeeded.  
I am getting an error while running the app in Android studio.  ERROR (Error:(152, 15) error: cannot find symbol method setNumThreads(int))

In file  **ImageClassifier.java**
**public void setNumThreads(int num_threads) {
    if (tflite != null)
        tflite.setNumThreads(num_threads);

  }**

I understand that the method setNumThreads is not found in the Interpreter.  needed help to resolve this issue..


"
18656,Tensorflow Yolo GPU Problem ,"Hello Everyone, 

I am trying to use yolo but I am completely new at programming. I guess the error is about my GPU memory. 

I hope you can help. Thank you! 

Caner

(base) C:\Users\derin\Desktop\Dark\darkflow-master>python flow --model cfg/yolo.cfg --load bin/yolov2.weights --demo videofile.mp4 --gpu 1.0 --saveVideo
C:\Users\derin\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
WARNING:tensorflow:From C:\Users\derin\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.

C:\Users\derin\Desktop\Dark\darkflow-master\darkflow\dark\darknet.py:54: UserWarning: ./cfg/yolov2.cfg not found, use cfg/yolo.cfg instead
  cfg_path, FLAGS.model))
Parsing cfg/yolo.cfg
Loading bin/yolov2.weights ...
Successfully identified 203934260 bytes
Finished in 0.01562643051147461s
Model has a coco model name, loading coco labels.

Building net ...
Source | Train? | Layer description                | Output size
-------+--------+----------------------------------+---------------
       |        | input                            | (?, 608, 608, 3)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 608, 608, 32)
 Load  |  Yep!  | maxp 2x2p0_2                     | (?, 304, 304, 32)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 304, 304, 64)
 Load  |  Yep!  | maxp 2x2p0_2                     | (?, 152, 152, 64)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 152, 152, 128)
 Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 152, 152, 64)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 152, 152, 128)
 Load  |  Yep!  | maxp 2x2p0_2                     | (?, 76, 76, 128)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 76, 76, 256)
 Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 76, 76, 128)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 76, 76, 256)
 Load  |  Yep!  | maxp 2x2p0_2                     | (?, 38, 38, 256)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)
 Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 256)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)
 Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 256)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)
 Load  |  Yep!  | maxp 2x2p0_2                     | (?, 19, 19, 512)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)
 Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 19, 19, 512)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)
 Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 19, 19, 512)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)
 Load  |  Yep!  | concat [16]                      | (?, 38, 38, 512)
 Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 64)
 Load  |  Yep!  | local flatten 2x2                | (?, 19, 19, 256)
 Load  |  Yep!  | concat [27, 24]                  | (?, 19, 19, 1280)
 Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)
 Load  |  Yep!  | conv 1x1p0_1    linear           | (?, 19, 19, 425)
-------+--------+----------------------------------+---------------
GPU mode with 1.0 usage
2018-04-18 17:50:15.493755: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-04-18 17:50:15.737244: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1344] Found device 0 with properties:
name: Quadro P4000 major: 6 minor: 1 memoryClockRate(GHz): 1.2275
pciBusID: 0000:01:00.0
totalMemory: 8.00GiB freeMemory: 6.63GiB
2018-04-18 17:50:15.742500: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1423] Adding visible gpu devices: 0
2018-04-18 17:50:16.281176: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-04-18 17:50:16.283690: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:917]      0
2018-04-18 17:50:16.285372: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:930] 0:   N
2018-04-18 17:50:16.287289: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8192 MB memory) -> physical GPU (device: 0, name: Quadro P4000, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-04-18 17:50:16.292054: E T:\src\github\tensorflow\tensorflow\stream_executor\cuda\cuda_driver.cc:936] failed to allocate 8.00G (8589934592 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2018-04-18 17:50:16.294366: E T:\src\github\tensorflow\tensorflow\stream_executor\cuda\cuda_driver.cc:936] failed to allocate 7.20G (7730940928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Finished in 12.376373767852783s

Press [ESC] to quit demo
2018-04-18 17:50:22.102653: E T:\src\github\tensorflow\tensorflow\stream_executor\cuda\cuda_dnn.cc:403] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
2018-04-18 17:50:22.105704: E T:\src\github\tensorflow\tensorflow\stream_executor\cuda\cuda_dnn.cc:407] error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows
2018-04-18 17:50:22.108692: E T:\src\github\tensorflow\tensorflow\stream_executor\cuda\cuda_dnn.cc:370] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
2018-04-18 17:50:22.110948: F T:\src\github\tensorflow\tensorflow\core\kernels\conv_ops.cc:712] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)"
18654, Cannot create interpreter: Model provided is schema version 0 not equal to supported version 3.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
18653,Bug: expected_shape not work in tensorflow,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: mac osx
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**:  3.5.0
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: None

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I try to use a variable without shape in tensorflow. My code is

```
sen_var_1 = tf.Variable(np.float32, trainable=False, validate_shape=False, expected_shape=[None, None, 300])
sen_1 = tf.placeholder(shape=[None, None, 300], dtype=np.float32, name=""q1"")
sen_assign_1 = tf.assign(sen_var_1, sen_1, validate_shape=False)
```

I will run session with `sen_assign_1` when train begin, and each epoch I want to use `sen_var_1`. But it seems that `expected_shape` is not work in `sen_var_1`. So is there any way to do this?

### Source code / logs
`ValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.`"
18652,Bug: tensorflow-gpu takes long time before beginning to compute,"I noticed that tensorflow always takes about ~2min before it actually starts to compute. I've been trying to find out, why this happens, and nothing really worked so far. 

[Tensorflow site](https://www.tensorflow.org/install/install_windows) says, I should use CUDA® Toolkit 9.0 and cuDNN v7.0. I have CUDA 9.0, so I downloaded CuDNN 7.0.5 for CUDA 9.0 and pasted the files to *C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\*, overwriting the ones form cuDNN 7.1.2, which I tested earlier. To make sure, I pip-installed tensorflow-gpu into a fresh anaconda env. See install [here](https://pastebin.com/rjiV1s3b). The issue is still the same.

CUDA works, since it prints the  *'Hello, TensorFlow!'*, when I use the official test example, but before that it takes like 2minutes every time! 

When I tested this with [another wheel](https://drive.google.com/drive/folders/1lVK_ABvVHzVYKs7X5SUhcZFBgKpC41Qw) ([which is linked in this tutorial](http://www.python36.com/install-tensorflow-gpu-windows/), I did not compile it myself.) on cuda 9.1/cudnn7.0.5, I had the same issues. A NVIDIA employee [on stackoverflow](https://stackoverflow.com/questions/49770217/why-does-cuda-initialisation-take-so-long-python-vscode-anaconda-tensorflow) suggested, I may be hitting a lengthy JIT compile step, because the GTX 1080 has compute capability of 6.1, which the wheel I used may not be compiled for. 

So I tried to find wheels for tensorflow with compute capability 6.1 for windows, but [the only one I found](https://github.com/fo40225/tensorflow-windows-wheel/tree/master/1.5.0/py36/GPU/cuda91cudnn7avx2) and tested produced the same problem.

Am I doing something wrong here, or do I just have to accept the 2min delay everytime I start my tensorflow/keras scripts?


### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Code:
```
import time
start_time = time.time()
import tensorflow as tf
a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
c = tf.matmul(a, b)
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
print(sess.run(c))
timer = time.time()
print(timer - start_time)
```
Output:
```
(tf_clean) C:\python_code\test>C:/anaconda/envs/tf_clean/python.exe c:/python_code/test/tf_test.py
2018-04-18 14:36:04.376661: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this
TensorFlow binary was not compiled to use: AVX2
2018-04-18 14:36:04.689661: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:01:00.0
totalMemory: 8.00GiB freeMemory: 6.60GiB
2018-04-18 14:36:04.699485: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1423] Adding visible gpu devices: 0
2018-04-18 14:38:12.227561: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-04-18 14:38:12.234504: I T:\src\github\tens2018-04-18 14:38:12.237156: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:930] 0:   N
2018-04-18 14:38:12.240997: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6379 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1
2018-04-18 14:38:12.548288: I T:\src\github\tensorflow\tensorflow\core\common_runtime\direct_session.cc:297] Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1
MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-18 14:38:12.559262: I T:\src\github\tensorflow\tensorflow\core\common_runtime\placer.cc:884] MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
b: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-18 14:38:12.564847: I T:\src\github\tensorflow\tensorflow\core\common_runtime\placer.cc:884] b: (Const)/job:localhost/replica:0/task:0/device:GPU:0
a: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-18 14:38:12.570545: I T:\src\github\tensorflow\tensorflow\core\common_runtime\placer.cc:884] a: (Const)/job:localhost/replica:0/task:0/device:GPU:0
[[22. 28.]
 [49. 64.]]
129.14624643325806
```

- **OS Platform and Distribution**:
Windows 10 Education (Version 10.0.16299 Build 16299)
Intel(R) Core(TM) i5-7500 CPU @ 3.40GHz, 3408 MHz, 4 Cores

- **TensorFlow installed from (source or binary)**:
binary

- **TensorFlow version**:
tensorflow-gpu 1.5.0, 1.7.0

- **Python version**: 
3.5.5 & 3.6 (via anaconda, conda 4.5.1.)

- **Bazel Version**:
N/A

- **CUDA/cuDNN version**:
Tested combinations: 
   CUDA 9.0 and CuDNN 7.1.2 (tested on tensorflow 1.5.0, 1.7.0 and 1.8.0-dev20180329)
   CUDA 9.1 and CuDNN 7.0.5 (tested on tensorflow 1.5.0 and 1.7.0)

- **GPU model and memory**:
NVIDIA GeForce GTX 1080 (GP104-400) [Hewlett-Packard], 8192 MBytes of GDDR5X SDRAM [Micron]

- **Exact command to reproduce**:
See: *Have I written custom code...*

=================================================================
EDIT:

Threadstarter here, hello.
> 
> 
> Could you try with the latest nightly?
> https://files.pythonhosted.org/packages/67/c0/e68a4f0400340b54c887703baa8eee188042c3d65a0cf535dda71abffbc2/tf_nightly_gpu-1.13.0.dev20190205-cp37-cp37m-win_amd64.whl

**This works!** I checked with that wheel, and then with `tf-nightly-gpu-2.0-preview` on PYPI, which also worked.
I initially wanted to use the anaconda cudatoolkit and cudnn packages, but currently, cudnn is only available up to version 7.3.1 on anaconda-cloud. Tensorflow 2.0 however, is compiled with 7.4.1, so I had to do this the oldschool way, and download the setups from Nvidia.
Soon, though...[soon](https://imgur.com/a/A2jZizt).

For everyone, here's what I did, as a guide:

### How to install Tensorflow Nightly 2.0 GPU in Anaconda on Windows 10 x64

• I installed these CUDA/CuDnn Versions:
   – cuda_10.0.130_win10_network (Nvidia CUDA Download: https://developer.nvidia.com/cuda-toolkit)
   – cuDNN v7.4.1 (Nov 8, 2018), for CUDA 10.0 (Nvidia CuDnn Download: https://developer.nvidia.com/cudnn)
   – Don't forget to check, whether the Cuda setup has correctly written itself to the PATH system variable.
   – Reboot.
• Now make a new environment in Anaconda and activate it:
   – `conda create --name tf2-nightly-gpu python=3.6`
   – `activate tf2-nightly-gpu`
• Now, with the new env still activated, install the latest Tensorflow 2.0 nightly GPU build from PYPI:
   – `pip install tf-nightly-gpu-2.0-preview`
• For machine learning in Jupyter notebook (or Jupyter Lab) , you need these as well:
   – `conda install nb_conda matplotlib scipy Pillow pandas scikit-learn`
• Check, if your GPU is recognized by Tensorflow. Open the Anaconda prompt, activate the new environment and type `python`, then press Enter. Now type:
`import tensorflow as tf`
`tf.test.is_gpu_available(cuda_only=False,min_cuda_compute_capability=None) `
• Output should be something like this:

```
(tf2-nightly-gpu) C:\Users\___>python
>>> import tensorflow as tf
>>> tf.test.is_gpu_available(cuda_only=False,min_cuda_compute_capability=None)
2019-03-19 17:46:25.722209: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-03-19 17:46:25.729724: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll
2019-03-19 17:46:25.922934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1551] Found device 0 with properties:
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:01:00.0
totalMemory: 8.00GiB freeMemory: 6.61GiB
2019-03-19 17:46:25.938231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1674] Adding visible gpu devices: 0
2019-03-19 17:46:26.539185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1082] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-19 17:46:26.546009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1088]      0
2019-03-19 17:46:26.550123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1101] 0:   N
2019-03-19 17:46:26.554188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1222] Created TensorFlow device (/device:GPU:0 with 6360 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
True
```

• Done."
18649,Feature Request: Backprop through the transformation matrix in tf.contrib.image.transform and tf.contrib.image.rotate,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary, cpu
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 2.7
- **Have I written custom code**: No
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: GTX 730M but not in use
- **Exact command to reproduce**: N/A

Hello Guys,

today I met a problem while using tf.contrib.image.rotate and tf.contrib.image.translate in TensorFlow. I want to train a Convolutional Network to estimate the rotation and translation between two 2D Laserscan images as Input (estimate some kind of odometry).
`network = NeuralTransformEstimator()`
`xy, alpha = tf.split(network.logits, [2, 1], 1)`
`img1, img2 = tf.split(network.inputs, [1, 1], 3)`
`alphas = tf.squeeze(alpha, axis=1)`
`rotated = tf.contrib.image.rotate(img1, alphas, interpolation='NEAREST')`
`translated = tf.contrib.image.translate(rotated, xy, interpolation='NEAREST')`
`loss_function = tf.reduce_mean(tf.square(img2-translated))`

The Network outputs three values(x,y, alpha) and the input is two images. When I use BackProp I get this error message:

> ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables

So I created a more lightweight approach to check if I did something wrong elsewhere but it seems that translate and rotate do not provide gradients.
Is there a chance that this feature will be added?"
18648,eager scatter_nd forward works with incorrect code,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
code, see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
OSX 10.12.6
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
v1.7.0-3-g024aecf414 1.7.0
- **Python version**: 
3.6.1
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
see code below


### Source code / logs

```python
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

x_var = tfe.Variable(tf.random_uniform([10]))
y_var = tfe.Variable(tf.random_uniform([10]))

with tfe.GradientTape() as tape:
    dot = tf.scatter_nd(indices=[0],  # this should be indices=[[0]]
                        updates=[tf.einsum('i,i->', x_var, y_var)],
                        shape=[1])
print(dot)
gradient = tape.gradient(dot, [x_var, y_var])
print(gradient)
```

Error traceback:

```
Traceback (most recent call last):
  File ""/Users/m/workspace/bug/experimental/bug.py"", line 31, in <module>
    gradient = tape.gradient(dot, [x_var, y_var])
  File ""/Users/m/workspace/bug/venv/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py"", line 764, in gradient
    output_gradients=output_gradients)
  File ""/Users/m/workspace/bug/venv/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py"", line 65, in imperative_grad
    tape._tape, vspace, target, sources, output_gradients, status)  # pylint: disable=protected-access
  File ""/Users/m/workspace/bug/venv/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py"", line 141, in grad_fn
    op_inputs, op_outputs, orig_outputs)
  File ""/Users/m/workspace/bug/venv/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py"", line 109, in _magic_gradient_function
    return grad_fn(mock_op, *out_grads)
  File ""/Users/m/workspace/bug/venv/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py"", line 39, in _PackGrad
    return array_ops.unstack(grad, num=op.get_attr(""N""), axis=op.get_attr(""axis""))
  File ""/Users/m/workspace/bug/venv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1084, in unstack
    return gen_array_ops.unpack(value, num=num, axis=axis, name=name)
  File ""/Users/m/workspace/bug/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 8741, in unpack
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: axis = 0 not in [0, 0) [Op:Unpack] name: unstack
```

### Describe the problem

This is erroneous code which runs half-way. It will still calculate the forward pass, but fail on the backward pass. This does not happen with static graph tensorflow, where you get a correct error that tensor shapes are not matching."
18645,Key LayerCollection/ff_fckron/concat_10_False/cov not found in checkpoint,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NA
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
     Ubuntu 16.04 (ppc64le)
- **TensorFlow installed from (source or binary)**:
      Installed from source
- **TensorFlow version (use command below)**:
     TF1.6.0
- **Python version**: 
     Python 2.7.5
- **Bazel version (if compiling from source)**:
       0.11.1
- **CUDA/cuDNN version**:
     NA
- **GPU model and memory**:
      NA
- **Exact command to reproduce**:
      bazel test -c opt //tensorflow/contrib/kfac/examples/tests:mlp_test

### Describe the problem
This test passed successfully on x86 , however its failing on ppc64le with below error  - 
```

INFO: From Testing //tensorflow/contrib/kfac/examples/tests:mlp_test:
==================== Test output for //tensorflow/contrib/kfac/examples/tests:mlp_test:
..WARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x3fff9b1c45f0>) includes params argument, but params are not passed to Estimator.
2018-04-18 10:12:24.430611: W tensorflow/core/framework/op_kernel.cc:1202] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key LayerCollection/ff_fckron/concat_10_False/cov not found in checkpoint
E..
======================================================================
ERROR: testTrainMnistEstimator (__main__.MlpTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/contrib/kfac/examples/tests/mlp_test.py"", line 59, in testTrainMnistEstimator
    mlp.train_mnist_estimator(data_dir=None, num_epochs=1, use_fake_data=True)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/contrib/kfac/examples/mlp.py"", line 326, in train_mnist_estimator
    estimator.train(input_fn=input_fn)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/estimator/estimator.py"", line 352, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/estimator/estimator.py"", line 888, in _train_model
    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session.py"", line 384, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session.py"", line 795, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session.py"", line 518, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session.py"", line 981, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session.py"", line 986, in _create_session
    return self._sess_creator.create_session()
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session.py"", line 675, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session.py"", line 446, in create_session
    init_fn=self._scaffold.init_fn)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/session_manager.py"", line 275, in prepare_session
    config=config)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/session_manager.py"", line 207, in _restore_checkpoint
    saver.restore(sess, ckpt.model_checkpoint_path)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 1755, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1137, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1355, in _do_run
    options, run_metadata)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1374, in _do_call
    raise type(e)(node_def, op, message)
NotFoundError: Key LayerCollection/ff_fckron/concat_10_False/cov not found in checkpoint
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Caused by op u'save/RestoreV2', defined at:
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/contrib/kfac/examples/tests/mlp_test.py"", line 63, in <module>
    tf.test.main()
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 76, in main
    return _googletest.main(argv)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 99, in main
    benchmark.benchmarks_main(true_main=main_wrapper)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py"", line 338, in benchmarks_main
    true_main()
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 98, in main_wrapper
    return app.run(main=g_main, argv=args)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 69, in g_main
    return unittest_main(argv=argv)
  File ""/usr/lib/python2.7/unittest/main.py"", line 95, in __init__
    self.runTests()
  File ""/usr/lib/python2.7/unittest/main.py"", line 232, in runTests
    self.result = testRunner.run(self.test)
  File ""/usr/lib/python2.7/unittest/runner.py"", line 151, in run
    test(result)
  File ""/usr/lib/python2.7/unittest/suite.py"", line 70, in __call__
    return self.run(*args, **kwds)
  File ""/usr/lib/python2.7/unittest/suite.py"", line 108, in run
    test(result)
  File ""/usr/lib/python2.7/unittest/suite.py"", line 70, in __call__
    return self.run(*args, **kwds)
  File ""/usr/lib/python2.7/unittest/suite.py"", line 108, in run
    test(result)
  File ""/usr/lib/python2.7/unittest/case.py"", line 393, in __call__
    return self.run(*args, **kwds)
  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/contrib/kfac/examples/tests/mlp_test.py"", line 59, in testTrainMnistEstimator
    mlp.train_mnist_estimator(data_dir=None, num_epochs=1, use_fake_data=True)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/contrib/kfac/examples/mlp.py"", line 326, in train_mnist_estimator
    estimator.train(input_fn=input_fn)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/estimator/estimator.py"", line 352, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/estimator/estimator.py"", line 888, in _train_model
    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session.py"", line 384, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session.py"", line 795, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session.py"", line 518, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session.py"", line 981, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session.py"", line 986, in _create_session
    return self._sess_creator.create_session()
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session.py"", line 675, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session.py"", line 437, in create_session
    self._scaffold.finalize()
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session.py"", line 214, in finalize
    self._saver.build()
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 1302, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 1339, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 790, in _build_internal
    restore_sequentially, reshape)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 502, in _AddShardedRestoreOps
    name=""restore_shard""))
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 449, in _AddRestoreOps
    restore_sequentially)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 847, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/ops/gen_io_ops.py"", line 1030, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 3271, in create_op
    op_def=op_def)
  File ""/root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/kfac/examples/tests/mlp_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Key LayerCollection/ff_fckron/concat_10_False/cov not found in checkpoint
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]


----------------------------------------------------------------------
Ran 5 tests in 3.637s

FAILED (errors=1)
================================================================================
Target //tensorflow/contrib/kfac/examples/tests:mlp_test up-to-date:
  bazel-bin/tensorflow/contrib/kfac/examples/tests/mlp_test
INFO: Elapsed time: 5.752s, Critical Path: 5.12s
INFO: Build completed, 1 test FAILED, 2 total actions
//tensorflow/contrib/kfac/examples/tests:mlp_test                        FAILED in 5.1s
  /root/.cache/bazel/_bazel_root/f1b093cb2d3fbb9bf169263a57926f78/execroot/org_tensorflow/bazel-out/ppc-opt/testlogs/tensorflow/contrib/kfac/examples/tests/mlp_test/test.log

Executed 1 out of 1 test: 1 fails locally.

```

I have started looking into the issue. 
Please provide if any suggestions/inputs on what would be the reason of failure. Thanks! "
18644,TensorBoard not running correctly with TF 1.8.0-rc0,"Hi, I upgraded TF to 1.8.0-rc0 to use the new `tf.data.prefetch_to_device()` feature. But with this version tensorboard won't run well. Here is output when I run tensorboard.
![image](https://user-images.githubusercontent.com/13655756/38924425-e7eb7f16-4337-11e8-81a9-5b07dfcf39a1.png)
(The command was valid when I entered with TF 1.7.)
Also when I entered to the tensorboard on the browser, tensorboard page opened but no event was displayed.
![image](https://user-images.githubusercontent.com/13655756/38924904-1ec3ab3e-4339-11e8-94e3-233242a0f14b.png)


Environment info
TF version: ('v1.7.0-1569-g3970b47da5', '1.8.0-rc0')
Operating System: Ubuntu 16.04.3
Installed version of CUDA and cuDNN: CUDA 9.0 and cuDNN 7 but I guess it's not CUDA related.

The latest version of TensorBoard is still 1.7.0, so there might be a reason for the problem.

Thank you for your hard working :^)"
18643,TensorFlow 1.8.0-rc1 fails on aarch64 platforms,"Building TensorFlow 1.8.0-rc0 from source fails on aarch64 platforms (NVIDIA TX1 and Linaro HiKey960) with a linking error for some Neon function:
```
Linking of rule '//tensorflow/cc:ops/remote_fused_graph_ops_gen_cc' failed (Exit 1)
bazel-out/host/bin/_solib_arm/_U_S_Stensorflow_Scc_Cops_Sremote_Ufused_Ugraph_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so: error: undefined reference to 'png_init_filter_functions_neon'
```

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A (a build problem)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 (JetPack 3.1)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.8.0-rc0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.12.0
- **GCC/Compiler version (if compiling from source)**: 5.4.1
- **CUDA/cuDNN version**: N/A (CPU ony)
- **GPU model and memory**: N/A (CPU only)
- **Exact command to reproduce**: Install dependencies and build via [CK-TensorFlow](github.com/ctuning/ck-tensorflow):
```
$ sudo apt install liblapack-dev libatlas-dev
$ sudo pip install enum34 mock pillow wheel absl-py scipy ck
$ ck pull repo:ck-tensorflow
$ ck install ck-env:package:tool-bazel-0.12.0-linux
$ ck install package:lib-tensorflow-1.8.0-src-cpu --env.CK_HOST_CPU_NUMBER_OF_PROCESSORS=1
```
**NB:** Restricting the number of building processes to 1 is necessary to prevent running out of memory on the NVIDIA TX1 platform (4 GB and no swap enabled) or similar.

### Describe the problem

Building TensorFlow 1.8.0-rc0 fails on aarch64 platforms (NVIDIA TX1 and Linaro HiKey960). Similar instructions for TensorFlow 1.7.0 with Bazel 0.11.1 worked well (both with and without XLA support):
```
$ ck install package:lib-tensorflow-1.7.0-src-cpu --env.CK_HOST_CPU_NUMBER_OF_PROCESSORS=1
$ ck install package:lib-tensorflow-1.7.0-src-cpu-xla --env.CK_HOST_CPU_NUMBER_OF_PROCESSORS=1

$ ck show env --tags=tensorflow,v1.7
Env UID:         Target OS: Bits: Name:                                       Version: Tags:

ed191cc45dda7ee4   linux-64    64 TensorFlow library (from sources, cpu, xla) 1.7      64bits,bazel,channel-stable,host-os-linux-64,lib,needs-bazel,needs-bazel-0.11.1,target-os-linux-64,tensorflow,tensorflow-cpu,v1,v1.7,v1.7.0,vcpu,vsrc,vxla
ed191cc45dda7ee3   linux-64    64 TensorFlow library (from sources, cpu)      1.7      64bits,bazel,channel-stable,host-os-linux-64,lib,needs-bazel,needs-bazel-0.11.1,target-os-linux-64,tensorflow,tensorflow-cpu,v1,v1.7,v1.7.0,vcpu,vsrc
```

### Source code / logs
```
ERROR: /home/anton/CK_TOOLS/lib-tensorflow-src-cpu-1.8-linux-64/src/tensorflow/cc/BUILD:510:1: Linking of rule '//tensorflow/cc:ops/remote_fused_graph_ops_gen_cc' failed (Exit 1)
bazel-out/host/bin/_solib_arm/_U_S_Stensorflow_Scc_Cops_Sremote_Ufused_Ugraph_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so: error: undefined reference to 'png_init_filter_functions_neon'
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 15373.402s, Critical Path: 153.22s
FAILED: Build did NOT complete successfully
```"
18642,Keras models fit() method not converging with eager execution,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes, see below.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Binary, with pip3
- **TensorFlow version (use command below)**:
v1.7.0-3-g024aecf414 1.7.0
- **Python version**: 
3.5.2
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
9.0.176/7.0.5
- **GPU model and memory**:
GTX 1080 (Armor MSI) with 8 GB VRAM
- **Exact command to reproduce**:
Run program below

### Describe the problem

After enabling eager execution, method `tf.keras.models.Sequential.fit()` doesn't seem to converge, as the computed loss doesn't go down (stays around 9 to 11); also, method `fit()` doesn't report the requested metric ""accuracy"": it doesn't print the metric to console, and does not return it in the History object.

After disabling eager execution, the same optimization converges, as loss goes down (to around 1); also, method `fit()` correctly reports the requested metric ""accuracy"", both to console and in the returned History object.

### Source code / logs

```
import os
import pandas as pd
import numpy as np
import time
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow.contrib.eager as tfe

""""""
Check the beginning of main() for parameters
""""""


def load_data(folder):
    file_name = os.path.join(folder, 'winequality-data.csv')
    if os.path.isfile(file_name):
        data = pd.read_csv(file_name)
    else:
        print('File {} not found.'.format(file_name, folder))
        print('Dataset can be downloaded from https://www.kaggle.com/c/uci-wine-quality-dataset/data')
        exit(1)
    # solutions = pd.read_csv(os.path.join(folder, 'winequality-solution-input.csv'))
    return data


def train_input_fn(features, labels, batch_size):
    features_tensor = tf.constant(features)
    labels_tensor = tf.constant(labels)
    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices((features_tensor, labels_tensor))

    # Shuffle, repeat, and batch the examples.
    dataset = dataset.shuffle(len(labels)).repeat(count=1).batch(batch_size)

    # Return the dataset.
    return dataset


def loss(model, X, y):
    logits = model(X)
    the_loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)
    return the_loss


def grad(model, inputs, targets):
    with tfe.GradientTape() as tape:
        loss_value = loss(model, inputs, targets)
    return tape.gradient(loss_value, model.variables)


def train(model, X, y, batch_size, epochs):
    train_ds = train_input_fn(X, y, batch_size=batch_size)
    optimizer = tf.train.AdamOptimizer()

    loss_by_epoch = []
    accuracy_by_epoch = []

    for epoch in range(epochs):
        epoch_loss_avg = tfe.metrics.Mean()
        epoch_error = tfe.metrics.Mean()
        for batch, (batch_X, batch_y) in enumerate(tfe.Iterator(train_ds)):
            grads = grad(model, batch_X, batch_y)
            optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())
            epoch_loss_avg(loss(model, batch_X, batch_y))
            correct_prediction = tf.equal(tf.argmax(model(batch_X), axis=1, output_type=tf.int32), batch_y)
            epoch_error(tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))
        print('Epoch {}:  loss={}  accuracy={}'.format(epoch, epoch_loss_avg.result(), epoch_error.result()))
        loss_by_epoch.append(epoch_loss_avg.result())
        accuracy_by_epoch.append(epoch_error.result())

    return loss_by_epoch, accuracy_by_epoch


def main():
    # Just comment the next line out to disable eager execution
    tf.enable_eager_execution()

    """"""
    Set use_fit to True to optimize by calling tf.keras.models.Sequential.fit(),
    set to False to use tfe.GradientTape() instead. Note that in order to use tfe.Gradient.tape(),
    eager execution must be enabled
    """"""
    use_fit = True

    epochs = 200
    batch_size = 64
    dataset_folder = '.'

    # Load dataset and convert it to numpy arrays
    data = load_data(dataset_folder)
    train_X = data.iloc[:, 0:11].values.astype(np.float32)
    train_y = data.iloc[:, 11].values.astype(np.int32)

    if use_fit:  # train_y needs to be 1-hot encoded for usage with model.fit()
        train_y = tf.keras.utils.to_categorical(train_y, num_classes=11)

    model = tf.keras.models.Sequential([
        tf.keras.layers.InputLayer(input_shape=(11,)),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(11, activation='softmax') if use_fit else tf.keras.layers.Dense(11)
    ])

    start = time.time()

    if use_fit:
        optimizer = tf.train.AdamOptimizer()
        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
        history = model.fit(x=train_X, y=train_y, epochs=epochs, batch_size=batch_size, verbose=2)
        loss_by_epoch = history.history['loss']
        accuracy_by_epoch = history.history['acc'] if 'acc' in history.history else []

    else:
        loss_by_epoch, accuracy_by_epoch = train(model=model, X=train_X, y=train_y, batch_size=batch_size,
                                                 epochs=epochs)

    end = time.time()
    print('It took {} seconds'.format(end - start))

    # Chart loss and error
    fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))
    fig.suptitle('Training Metrics')

    axes[0].set_ylabel(""Loss"", fontsize=14)
    axes[0].plot(loss_by_epoch)

    axes[1].set_ylabel(""Accuracy"", fontsize=14)
    axes[1].set_xlabel(""Epoch"", fontsize=14)
    axes[1].plot(accuracy_by_epoch)

    plt.show()


if __name__ == '__main__':
    main()

```"
18641,i think it can add a Factorization Machines module,
18640,InvalidArgumentError is raised when restoring large (>2GB) variable on macOS,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13.4
- **TensorFlow installed from (source or binary)**: from pip
- **TensorFlow version (use command below)**: 1.7.0, 1.6.0, 1.8.0rc0
- **Python version**: Anaconda, python 3.6
- **Bazel version (if compiling from source)**: no
- **GCC/Compiler version (if compiling from source)**: no
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**:
```python
import tensorflow as tf

v = tf.get_variable(
    name='a',
    shape=(550 * 1000 * 1000,),
    dtype=tf.float32,
    initializer=tf.zeros_initializer(),
)
saver = tf.train.Saver()
with tf.Session() as s:
    s.run(tf.global_variables_initializer())
    saver.save(s, 'tmp.tf/a')
    saver.restore(s, 'tmp.tf/a')
```

### Describe the problem
When saving and then restoring variable >2GB on macOS tensorflow throws InvalidArgumentError

### Source code / logs
```
2018-04-18 12:25:50.620884: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
'tmp.tf/a'
INFO:tensorflow:Restoring parameters from tmp.tf/a
2018-04-18 12:26:00.866457: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Invalid argument: tmp.tf/a.data-00000-of-00001; Invalid argument
Traceback (most recent call last):
  File ""/Users/alyaxey/anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
    return fn(*args)
  File ""/Users/alyaxey/anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1312, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/Users/alyaxey/anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1420, in _call_tf_sessionrun
    status, run_metadata)
  File ""/Users/alyaxey/anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: tmp.tf/a.data-00000-of-00001; Invalid argument
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 4, in <module>
  File ""/Users/alyaxey/anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1775, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/Users/alyaxey/anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/Users/alyaxey/anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1140, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/Users/alyaxey/anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    run_metadata)
  File ""/Users/alyaxey/anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: tmp.tf/a.data-00000-of-00001; Invalid argument
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Caused by op 'save/RestoreV2', defined at:
  File ""<stdin>"", line 1, in <module>
  File ""/Users/alyaxey/anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1311, in __init__
    self.build()
  File ""/Users/alyaxey/anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1320, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/Users/alyaxey/anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1357, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/Users/alyaxey/anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 809, in _build_internal
    restore_sequentially, reshape)
  File ""/Users/alyaxey/anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 448, in _AddRestoreOps
    restore_sequentially)
  File ""/Users/alyaxey/anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 860, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/Users/alyaxey/anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1458, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""/Users/alyaxey/anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/Users/alyaxey/anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3290, in create_op
    op_def=op_def)
  File ""/Users/alyaxey/anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1654, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): tmp.tf/a.data-00000-of-00001; Invalid argument
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
```"
18636,Does anyone know how to make network output int instead of float?,"I try to use tf.round to make elements in tensor convert to int, but then I meet this error:

ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables ['Tensor(""conv_w_0/read:0"", shape=(9, 9, 1, 64), dtype=float32)', 'Tensor(""conv_b_0/read:0"", shape=(64,), dtype=float32)', 'Tensor(""conv_w_1/read:0"", shape=(1, 1, 64, 32), dtype=float32)', 'Tensor(""conv_b_1/read:0"", shape=(32,), dtype=float32)', 'Tensor(""conv_w_2/read:0"", shape=(5, 5, 32, 1), dtype=float32)', 'Tensor(""conv_b_2/read:0"", shape=(1,), dtype=float32)'] and loss Tensor(""Mean:0"", shape=(), dtype=float32).

And this is my  code:
Loss = tf.reduce_mean(tf.nn.l2_loss(tf.round(Train_output) * Train_tao / 2 + Train_img_LR[:,6:27,6:27,:] - Train_img_HR))

I just want to transfer the elements in Train_output from float to int, Train_output is my network's output.
I have an idea is that I can add a binary_step activation to the end of my model, but I don't know how to write it, because tensor is not iterable, can anyone help me?
"
18635,Strange result of float division,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.5.0
- **Python version**: 
3.5.2
- **CUDA/cuDNN version**:
9.0
- **GPU model and memory**:
GTX 1080Ti

### Describe the problem

The below Numpy and TensorFlow codelets produce different results, while I suppose they should be the same.

```python
import numpy as np
import tensorflow as tf

x = np.array([247.], dtype=np.float32)
y = x / 255.
print('{:12.10f}'.format(y[0]))
# 0.9686274529

a = tf.placeholder(tf.float32)
b = a / 255.
with tf.Session() as sess:
    _b = sess.run(b, feed_dict={a: x})
print('{:12.10f}'.format(_b[0]))
# 0.9686275125
```

Running on CPU gives the same issue.

However, if I use `a = tf.constant(x, dtype=tf.float32)` instead of `tf.placeholder`, the result seems correct.

Is this an issue about `tf.placeholder` or something else? Does this affect other operations?"
18634,`tf.contrib.layers.variance_scaling_initializer` is not consistent with `tf.variance_scaling_initializer`?,"I found the implementation of `tf.contrib.layers.variance_scaling_initializer` is not consistent with implementation of `tf.variance_scaling_initializer`?

In [https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/contrib/layers/python/layers/initializers.py#L148](https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/contrib/layers/python/layers/initializers.py#L148), the `trunc_stddev` is multiplied by 1.3.

While in [https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/python/ops/init_ops.py#L466](https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/python/ops/init_ops.py#L466), there is no 1.3.

So, which is correct?"
18633,"Open the build.gradle file (you can go to 1:Project in the side panel and find it under the Gradle Scripts zippy under Android). Look for the nativeBuildSystem variable and set it to none if it isn't already:  // set to 'bazel', 'cmake', 'makefile', 'none' def nativeBuildSystem = 'none' this do not exisit i need help rearding it if anyone can ??","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
18630,Grpc+RDMA problem,"For RDMA, when start the `ps` server, it will do RDMA connect to `worker` server, but failed because worker still not started:

```
# python tf_rdma.py --ps_hosts='workernode2:1111' --worker_hosts='workernode3:2222' --job_name=ps --task_id=0
....
2018-04-04 11:23:38.912680: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:1111
2018-04-04 11:23:38.920106: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got Transport closed. Retrying (1/5)...
2018-04-04 11:23:41.920544: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (2/5)...
2018-04-04 11:23:43.921039: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (3/5)...
2018-04-04 11:23:46.922376: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (4/5)...
2018-04-04 11:23:48.922817: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (5/5)...
2018-04-04 11:23:48.922848: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:worker/replica:0/task:0
```

Then start the worker server:

```
# python tf_rdma.py --ps_hosts='workernode2:1111' --worker_hosts='workernode3:2222' --job_name=worker --task_id=0
...
2018-04-04 14:12:38.008214: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> workernode2:1111}
2018-04-04 14:12:38.008256: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222}
2018-04-04 14:12:38.013784: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> workernode2:1111}
2018-04-04 14:12:38.013803: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222}
2018-04-04 14:12:38.019732: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222
2018-04-04 14:12:38.028558: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:ps/replica:0/task:0
```

the worker server can do RDMA connect successfully. But hanged up with the follows:

```
(gdb) bt
#0  0x00007fbdcc4f7a54 in mlx4_poll_cq () from /lib64/libmlx4-rdmav2.so
#1  0x00007fbdf16301d8 in tensorflow::RdmaMgr::ConnectivityCheck() () from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007fbdf1628874 in tensorflow::VerbsServer::Start() () from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007fbdf12abf9b in _wrap_PyServer_Start () from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007fbe01542aa4 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0
```

I think `ps` server should wait `worker` server setup before [connect worker server](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/verbs_server_lib.cc#L105).

Am I right?"
18629,tensorflow lite android demo import falied!,"### System information
== cat /etc/issue ===============================================
Linux apuser-H81-M1 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux apuser-H81-M1 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                              1.14.2     
protobuf                           3.5.2.post1
tensorflow                         1.8.0rc0   
tensorflow-gpu                     1.7.0      

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0-rc0
tf.GIT_VERSION = v1.8.0-rc0-525-ga2607aa
tf.COMPILER_VERSION = v1.8.0-rc0-525-ga2607aa
Sanity check: array([1], dtype=int32)
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""tensorflow/python/pywrap_tensorflow.py"", line 25, in <module>
    from tensorflow.python.platform import self_check
ImportError: No module named platform

== env ==========================================================
LD_LIBRARY_PATH :/usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Apr 18 11:38:35 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.111                Driver Version: 384.111                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1080    Off  | 00000000:01:00.0  On |                  N/A |
| 27%   39C    P0    54W / 250W |    384MiB /  8105MiB |     20%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0       905      G   /usr/lib/xorg/Xorg                           225MiB |
|    0      1746      G   compiz                                        84MiB |
|    0      1968      G   fcitx-qimpanel                                 8MiB |
|    0      2280      G   ...-token=D7E00697E1E0DD52292B521137FD9307    64MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/MATLAB/R2016b/bin/glnxa64/libcudart.so.7.5.18
/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176
/usr/local/cuda-9.0/lib64/libcudart_static.a
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7

### Describe the problem
1Q:
I am trying to use a custom model in the [tflitedemo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android).
When importing this project to android studio, I encountered a configuration failure error "" Plugin with id 'com.android.application' not found."",  how to solve this problem ？

2Q:
[box_priors.txt](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android/assets) is from where ?  how to generate it ?  

Thanks!




"
18617,How to include xla_op_kernel.h to the TensorFlow python package?,"Have I written custom code: None
OS Platform and Distribution: Linux Ubuntu 16.04
Open MPI version: 3.0.0
TensorFlow installed from: source
Bazel version: 0.11.1
Python version: 3.5.2
GCC version: 6.3.0
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Horovod: 0.12.1
Tensorflow version: b'v1.7.0-rc1-816-g1712002ad0' 1.7.0-rc1
Exact command to reproduce: N/A
Feature request: I would like to call REGISTER_XLA_OP in Horovod and it requires that `xla_op_kernel.h` be included in tensorflow Python package. I wonder if it is easy to add this feature."
18612,TFlite conversation failed,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.13.3
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:  1.6
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: 0.9
- **CUDA/cuDNN version**: 9/7
- **GPU model and memory**: 1080Ti & 12G
- **Exact command to reproduce**:

I wrote MobileNet code to train a model from scratch. Also tried using MobileNet keras function for training but none of the model converts to tflite.

I am running following command

```
./bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=/MobileNet-trined.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=train-1.tflite --inference_type=FLOAT --input_arrays=input_1 --output_arrays=dense_1/Sigmoid --allow_custom_ops
```

It gives me following result

```
2018-04-17 15:25:11.070338: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1236] Converting unsupported operation: RandomUniform
2018-04-17 15:25:11.098993: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 865 operators, 1364 arrays (0 quantized)
2018-04-17 15:25:11.139921: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 809 operators, 1280 arrays (0 quantized)
2018-04-17 15:25:11.183528: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 809 operators, 1280 arrays (0 quantized)
1984959600
2006980784
2018-04-17 15:25:11.184242: F tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:90] Check failed: mean_shape.dims() == multiplier_shape.dims() 
Abort trap: 6
```"
18604,No module named tensorflow.tools,"### System information
- **OS Platform and Distribution**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version**: 1.3.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: Used binary
- **GCC/Compiler version (if compiling from source)**: Used binary
- **CUDA/cuDNN version**: No GPU 
- **GPU model and memory**: No GPU
- **Exact command to reproduce**: from tensorflow import tools


### Describe the problem
I am trying to run the tensorflow to onnx converter (https://github.com/onnx/tensorflow-onnx) and for that reason I had to use some outdated versions of tf and onnx. I finally managed to get everything installed correctly, when I try to run any example I get the message that there is no module named tensorflow 

I checked this [previous issue from last year](https://github.com/tensorflow/tensorflow/issues/9778) and from what I understood, tf tools weren't supported on Windows. Is that still the case? Is there any workaround? 
"
18602,Cuda 7.5 Configuration Error on r1.7 ,"------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: r1.7
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: 0.11 
- **GCC/Compiler version (if compiling from source)**: 5.3.1
- **CUDA/cuDNN version**: 7.5/5.1.3
- **GPU model and memory**: Tesla K20

--------------------------

We are trying to compile tf from source, tried both the r1.7 and master branch. Both gives out the following error message,

```
Singularity nvidia.img:~/tensorflow> bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
..............
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
	File ""/root/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1142
		_create_local_cuda_repository(repository_ctx)
	File ""/root/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1001, in _create_local_cuda_repository
		_find_nvvm_libdevice_dir(repository_ctx, cuda_config)
	File ""/root/tensorflow/third_party/gpus/cuda_configure.bzl"", line 724, in _find_nvvm_libdevice_dir
		auto_configure_fail((""Cannot find libdevice.10.bc un...))
	File ""/root/tensorflow/third_party/gpus/cuda_configure.bzl"", line 210, in auto_configure_fail
		fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: Cannot find libdevice.10.bc under /usr/local/cuda-7.5
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
	File ""/root/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1142
		_create_local_cuda_repository(repository_ctx)
	File ""/root/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1001, in _create_local_cuda_repository
		_find_nvvm_libdevice_dir(repository_ctx, cuda_config)
	File ""/root/tensorflow/third_party/gpus/cuda_configure.bzl"", line 724, in _find_nvvm_libdevice_dir
		auto_configure_fail((""Cannot find libdevice.10.bc un...))
	File ""/root/tensorflow/third_party/gpus/cuda_configure.bzl"", line 210, in auto_configure_fail
		fail((""\n%sCuda Configuration Error:%...)))
```
"
18600,"Switch slim.learning.train to use tf.train.MonitoredTrainingSession, and expose sessionrunhooks","Hi! 

I've been getting warnings when using slim.learning.train about switching to tf.train.MonitoredTrainingSession. Is there a plan to make the switch?

Thanks!
"
18598,tensorflow-1.8.0rc0: tf.compat.as_str returns bytes for python3 since 20180409,"The issue appeared first in `tf-nightly==1.8.0.dev20180409` but is now present in `tensorflow==1.8.0rc0`.

Reproduce steps:

```
$ python3 -c ""import tensorflow as tf; print(tf.VERSION, type(tf.compat.as_str('hello')) == str)
```

Is expected to always print ""True"".  But gets:

```
# tensorflow
1.6.0 True
1.7.0 True
1.8.0-rc0 False           <= Broken!

# tf-nightly
1.8.0-dev20180408 True
1.8.0-dev20180409 False   <= Broken!
```"
18595,import_scoped_meta_graph() got an unexpected keyword argument 'return_elements',"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 3.6.3 
- **CUDA/cuDNN version**: CUDA/9.0

### Describe the problem

`trans, = tf.train.import_meta_graph('./Model/DehazeNet_model_1gpu.ckpt.meta'
                                                           , input_map={'batch:0': X} , return_elements=['clip_by_value:0'])`

there raise the error: **TypeError: import_scoped_meta_graph() got an unexpected keyword argument 'return_elements'**

if the keyword argument 'return_elements' in import_scoped_meta_graph()  has been removed in 1.7.0 ?

"
18594,floating point of image classifier is broken in tflite demo,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

== cat /etc/issue ===============================================
Darwin Zhongleis-MBP 17.5.0 Darwin Kernel Version 17.5.0: Mon Mar  5 22:24:32 PST 2018; root:xnu-4570.51.1~1/RELEASE_X86_64 x86_64
Mac OS X 10.13.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.1.0 (clang-902.0.39.1)
Target: x86_64-apple-darwin17.5.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin Zhongleis-MBP 17.5.0 Darwin Kernel Version 17.5.0: Mon Mar  5 22:24:32 PST 2018; root:xnu-4570.51.1~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.14.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""tensorflow/python/pywrap_tensorflow.py"", line 25, in <module>
    from tensorflow.python.platform import self_check
ImportError: No module named platform

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tools/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

In current tflite demo, if I enable floating point image classifier, the app doesn't work.

The error message is
04-17 19:56:31.538 29418 29418 E TfLiteCameraDemo: Exception java.lang.NullPointerException: Can not allocate memory for the interpreter

I think it's caused by

$ git show 91c3199
commit 91c31997e6854a3d07acc76381cff7436df1c1dd
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Fri Apr 13 08:12:42 2018 -0700

    Add support to TFLite for dilated convolution.

    PiperOrigin-RevId: 192770919

### Source code / logs
The error message is
04-17 19:56:31.538 29418 29418 E TfLiteCameraDemo: Exception java.lang.NullPointerException: Can not allocate memory for the interpreter
"
18593,unsupported operand type(s) for *=: 'float' and 'NoneType',"any one happened this?


tf_X = tf.placeholder(tf.float32, [None, 8, 8, 1])
tf_Y = tf.placeholder(tf.float32, [None, 10])



conv_out2 = tf.nn.conv2d(relu_feature_maps1, conv_filter_w2, strides=[1, 2, 2, 1], padding='SAME',use_cudnn_on_gpu=False) + conv_filter_b2

print(""log+++"")
print (type(conv_out2.get_shape()[0].value))



batch_mean, batch_var = tf.nn.moments(conv_out2, axes=[0, 1, 2])"
18592,[Deprecation Warning] -  tf.contrib.layers.xavier_initializer,"Hello dear friends,

If you run the following code, you will have a deprecation warning in version 1.7.0

```python
import tensorflow as tf
print(""TF Version:"", tf.__version__)
a = tf.contrib.layers.xavier_initializer()
```

```raw
TF Version: 1.7.0
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating: Use the retry module or similar alternatives.
```

For information, this warning did not appear in version **1.6.0**, it appears from **1.7.0**.

Have a good day,

Jonathan"
18590,Distributed Tensorflow:tensorflow prediction ,"I want to use many arm as tensorflow cluster to prediction.but I can't find some helpful demo or instructions about it.
In tensorflow distributed demo training we can see
Cluster = tf.train.ClusterSpec({""ps"": ps_hosts, ""worker"": worker_hosts})
Cluster from the parameter server and worker hosts, I would like to know that the computing cluster created by the code here is only for training? What should I do if I want to create a cluster for prediction?
"
18588,XLA implementation of FFT on CPU pulls in tf/core:framework,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macos 10.11
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: commit 63c6562df68ade3a03481874a71b536a4e02b6f5 (master as of April 15 2018)
- **Python version**:  n/a
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: `bazel build --config=opt //tensorflow/compiler/xla/service/cpu:runtime_fft`. It *works*, but see below.

### Describe the problem
Short version: The CPU implementation of the XLA FFT operation appears to pull in `tensorflow::Tensor` and `...::TensorShape` as a dependency, via `//tensorflow/core:framework`.

Long version: The FFT implementation comes in three flavors; real-to-complex, complex-to-real, and complex-to-complex. The first two flavors involve allocating a temporary buffer for an intermediate step in the computation. This is currently achieved by creating a `tensorflow::Tensor` object. This requires linking against `//tensorflow/core:framework`.

This feels like a bug, or at least unintentional and undesirable. For instance, every other op listed in `tensorflow/compiler/xla/service/cpu/BUILD`, besides runtime_fft, depends only on `:framework_lite`. My understanding re: allocating temporary space was that (at least for the AOT compiler, not sure about JIT) there were specific temporary buffers set aside, and that allocation should work through that system; not by just letting malloc run wild. Is that understanding correct? (*Aside: Eigen's FFT op internally calls malloc, regardless of FFT flavor, which likewise bypasses the AOT temporary buffers. Is that ok?*)

Is anyone currently working on this? (Has anyone noticed anything awry?) If I were to try fixing this myself, would anyone have any suggestions on how to allocate a temporary buffer in an XLA-friendly way? I thought one possibility would be writing an Algebraic Simplifier pass to rewrite real-to-complex and complex-to-real flavors in terms of the complex-to-complex flavor (which doesn't need to create a `tf::Tensor`), but that's my only idea.

### Source code / logs
References:
[tensorflow/compiler/xla/service/cpu/BUILD](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/BUILD)
[tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h)"
18586,tf.train.batch does not work with tf.uint32 and tf.uint64 data types,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```
import tensorflow as tf

values = tf.constant([0, 1, 2, 3, 4], dtype=tf.uint64)
batch = tf.train.batch(
    [values],
    batch_size=2,
    num_threads=1,
    enqueue_many=True,
)

with tf.Session() as sess:
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess=sess, coord=coord)

    print(sess.run(batch))

    coord.request_stop()
    coord.join(threads)
```

### Describe the problem
`tf.train.batch` does not work on tensors with types `uint32` or `uint64`. It works with all other primitive types. The source of this issue can be traced back to `CopyElementToSlice` which calls `TF_CALL_ALL_TYPES`

https://github.com/tensorflow/tensorflow/blob/3128b43eb0bf37ac3c49cb22a6e1789d8ea346e8/tensorflow/core/kernels/batch_util.cc#L94

This `TF_CALL_ALL_TYPES` macro eventually calls into `TF_CALL_INTEGRAL_TYPES` 

https://github.com/tensorflow/tensorflow/blob/3128b43eb0bf37ac3c49cb22a6e1789d8ea346e8/tensorflow/core/framework/register_types.h#L154

This macro includes `int8`, `uint8`, `int16`, `uint16`, `int32`, and `int64` but **does not** include `uint32` or `uint64`

The change to support these types appears to be simple, but since `TF_CALL_ALL_TYPES` is a high-level macro, I did not know if these data types were purposefully excluded.
"
18583,Fail to save the checkpoint with dataset Iterator saveable when using shuffle,"
### System information
== cat /etc/issue ===============================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Mon Mar  5 22:24:32 PST 2018; root:xnu-4570.51.1~1/RELEASE_X86_64 x86_64
Mac OS X 10.13.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.1.0 (clang-902.0.39.1)
Target: x86_64-apple-darwin17.5.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Mon Mar  5 22:24:32 PST 2018; root:xnu-4570.51.1~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.14.2)
protobuf (3.5.2.post1)
simple-tensorflow-serving (0.2.6, /usr/local/lib/python2.7/site-packages/simple_tensorflow_serving-0.2.6-py2.7.egg)
tensorflow (1.7.0)
tensorflow-hub (0.1.0)
tensorflow-model-analysis (0.6.0)
tensorflow-serving-api (1.0.0)
tensorflow-tensorboard (1.5.0)
tensorflow-transform (0.6.0)
tensorflowjs (0.1.0)
tensorflowonspark (1.0.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.7.0
tf.GIT_VERSION = v1.7.0-3-g024aecf414
tf.COMPILER_VERSION = v1.7.0-3-g024aecf414
Sanity check: array([1], dtype=int32)
/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
a.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================

### Describe the problem

We follow the [official document](https://www.tensorflow.org/programmers_guide/datasets#saving_iterator_state) to save the checkpoint with dataset Iterator saveable but it fails. Maybe the implementation of serializing `iterator_ops` is not released in public repo.

### Source code / logs

```
dataset = tf.data.Dataset.from_tensor_slices((image_list_placeholder, label_list_placeholder))
iterator = dataset.make_initializable_iterator()
saveable = tf.contrib.data.make_saveable_from_iterator(iterator)
tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable)
saver = tf.train.Saver()

with tf.Session() as sess:
  saver.save(sess, checkpoint_file, global_step=step_value)
```

Here is the error log.

```
2018-04-17 11:02:40.756347: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at iterator_ops.cc:1049 : Unimplemented: AsGraphDefInternal
Traceback (most recent call last):
  File ""./mnist_train.py"", line 181, in <module>
    main(parse_args())
  File ""./mnist_train.py"", line 156, in main
    saver.save(sess, checkpoint_file, global_step=step_value)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1676, in save
    {self.saver_def.filename_tensor_name: checkpoint_file})
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1140, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnimplementedError: AsGraphDefInternal
         [[Node: SerializeIterator = SerializeIterator[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](Iterator)]]
```
"
18572,Feeding variable length list data (from csv) to an 'indicator_column' feature,"I have a feature as follows:
`tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_file(...))`

Corresponding 'vocabulary_file' contains integer values as follows:

10
20
32
44
5
1212
...

Consider such training examples:
Jack, M, 22, **""[10, 20]""**, 2.33, 1
Sara, F, 24, **""[32, 44, 5, 1212]""**, 5.6, -1

Each training example has a variable length list data like [10, 20] or [32, 44, 5, 1212]
Now, I want to capture this data from csv file into the 'indicator_column' feature and then feed the multi-hot representation (result) to a deep model. 
The `decode_csv` function only support `float32`, `float64`, `int32`, `int64`, `string` and I have issue for **'list'** type data in csv. 

------------------------

### System information
- **OS Platform**: Win8
- **TensorFlow installed from**: binary
- **TensorFlow version**: 1.5
- **Python version**: 3.6
- **Bazel version**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: GPU> None | CPU> AMD(Phenom II x4)
Exact command to reproduce is clear."
18566,Help with C++API configuration,"What do I need to choose at `./configure` ?If I just want to start learn tensorflow by C++API , when I choose nothing and run bazel( `bazel build :libtensorflow_cc.so` ) I get 
`ERROR: Skipping ':libtensorflow_cc.so': no such target '//:libtensorflow_cc.so': target 'libtensorflow_cc.so' not declared in package '' defined by /home/vjusak/Tensorflow/BUILD
WARNING: Target pattern parsing failed.
ERROR: no such target '//:libtensorflow_cc.so': target 'libtensorflow_cc.so' not declared in package '' defined by /home/vjusak/Tensorflow/BUILD
INFO: Elapsed time: 0.393s
FAILED: Build did NOT complete successfully (0 packages loaded)
`
Ubuntu 16.10 

> bazel version
`Build label: 0.12.0
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Aug 4 01:22:27 +50246 (1523462001747)
Build timestamp: 1523462001747
Build timestamp as int: 1523462001747
`"
18564,Failed to compile summarize_graph on macOS with TF 1.7,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.3
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 2.7.14
- **Bazel version (if compiling from source)**: 0.12.0-homebrew
- **GCC/Compiler version (if compiling from source)**: Xcode 9.3: Apple LLVM version 9.1.0 (clang-902.0.39.1)
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: `bazel build tensorflow/tools/graph_transforms:summarize_graph`

### Describe the problem
Build fails with the output in the section below. Removing `armeabi-v7a` from `third_party/jpeg/jpeg.BUILD` seems to resolve the issue. See output of `git diff` below.

Building `summarize_graph` above fails fast, but I originally found the issue by trying to build `print_selective_registration_header` using the following command, which fails in ~90 seconds on my machine:
```bash
bazel build tensorflow/tools/graph_transforms:summarize_graph
```

### Source code / logs
Output of build command:
```
WARNING: /private/var/tmp/_bazel_json/e38619818ff94aae50ac5b3bdbbe0f32/external/protobuf_archive/WORKSPACE:1: Workspace name in /private/var/tmp/_bazel_json/e38619818ff94aae50ac5b3bdbbe0f32/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions
INFO: Analysed target //tensorflow/tools/graph_transforms:summarize_graph (0 packages loaded).
INFO: Found 1 target...
ERROR: /private/var/tmp/_bazel_json/e38619818ff94aae50ac5b3bdbbe0f32/external/jpeg/BUILD:44:1: C++ compilation of rule '@jpeg//:jpeg' failed (Exit 1)
error: unknown target CPU 'armv7-a'
Target //tensorflow/tools/graph_transforms:summarize_graph failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 0.877s, Critical Path: 0.18s
FAILED: Build did NOT complete successfully
```

Diff of workaround via `git diff`:
```diff
diff --git a/third_party/jpeg/jpeg.BUILD b/third_party/jpeg/jpeg.BUILD
index 87a23925c4..fa6dd27a70 100644
--- a/third_party/jpeg/jpeg.BUILD
+++ b/third_party/jpeg/jpeg.BUILD
@@ -28,12 +28,6 @@ libjpegturbo_copts = select({
         ""-w"",
     ],
 }) + select({
-    "":armeabi-v7a"": [
-        ""-D__ARM_NEON__"",
-        ""-march=armv7-a"",
-        ""-mfloat-abi=softfp"",
-        ""-fprefetch-loop-arrays"",
-    ],
     "":linux_ppc64le"": [
         ""-mcpu=power8"",
         ""-mtune=power8"",
@@ -125,7 +119,6 @@ cc_library(
     visibility = [""//visibility:public""],
     deps = select({
         "":k8"": ["":simd_x86_64""],
-        "":armeabi-v7a"": ["":simd_armv7a""],
         "":arm64-v8a"": ["":simd_armv8a""],
         "":linux_ppc64le"": ["":simd_altivec""],
         ""//conditions:default"": ["":simd_none""],
@@ -423,7 +416,6 @@ genrule(
         "":windows"": ""cp $(location jconfig_win.h) $@"",
         "":windows_msvc"": ""cp $(location jconfig_win.h) $@"",
         "":k8"": ""cp $(location jconfig_nowin_simd.h) $@"",
-        "":armeabi-v7a"": ""cp $(location jconfig_nowin_simd.h) $@"",
         "":arm64-v8a"": ""cp $(location jconfig_nowin_simd.h) $@"",
         "":linux_ppc64le"": ""cp $(location jconfig_nowin_simd.h) $@"",
         ""//conditions:default"": ""cp $(location jconfig_nowin_nosimd.h) $@"",
@@ -524,11 +516,6 @@ config_setting(
     values = {""crosstool_top"": ""//external:android/crosstool""},
 )
 
-config_setting(
-    name = ""armeabi-v7a"",
-    values = {""android_cpu"": ""armeabi-v7a""},
-)
-
 config_setting(
     name = ""arm64-v8a"",
     values = {""android_cpu"": ""arm64-v8a""},
```"
18560,ate,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
18559,Failed to use tf.ones with brackets,"version: 1.4

for example:
```
a = tf.constant(2, dtype=tf.int32)
tf.ones(shape=(a))
# ValueError: Shape must be rank 1 but is rank 0 for 'ones_18' (op: 'Fill') with input shapes: [], [].
```
but 
```
a = tf.constant(2, dtype=tf.int32)
tf.ones(shape=[a])
# works well
```
"
18557,'Missing required flag: input_file' when using TOCO with SavedModel,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.12.6
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.7
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: toco --savedmodel_directory=/Models/Final/1523530083 --output_file=move.tflite

### Describe the problem
I have a SavedModel trained and I am trying to use TOCO to convert it to tflite format. When I use the above command, given as an example in the docs [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md#savedmodel) I get prompted for the input_format. Specifying this as TENSORFLOW_GRAPHDEF gives a new error for output_format. Specifying this as TFLITE now prompts for required parameter input_file. As this is a SavedModel and not a GraphDef, I can't provide the input_file and in fact, shouldn't have to as savedmodel_directory has already been provided. 

The model directory 1523530083 contains saved_model.pb and a variables folder. There are no assets. "
18556,The latest master branch fails to run the model on tensorflow lite,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:  source
- **TensorFlow version (use command below)**: 
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**:  0.11.1
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I cloned the latest master branch code (commit id 63c6562df68ade3a03481874a71b536a4e02b6f5) in order to use the setNumThreads method on Android, built a tensorflow lite aar, but could not run my model. This model can be run in previous versions (including r1.8.0). How can I solve it?

### Source code / logs
no logs"
18555,[Feature Request] Inverse Functions: Auto-Solve similar to Auto-Grad?,"Would it be possible to implement some type of automatic equation solving?
E.g. f(x, y) = z => tf.solve(y) = f'(x, z)

Functions like tf.sigmoid have known inverse functions which could be used to solve functions if all other parameters are known.

I'm thinking of something like simplified SymPy solvers:
http://docs.sympy.org/latest/modules/solvers/solvers.html

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: None
- **TensorFlow installed from (source or binary)**: None
- **TensorFlow version (use command below)**: 1.7
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: None

"
18554,importing tensorflow,"Hello, 
I tried to install tensorflow using conda, but whenever I try to import it, I get this error:

```
ImportError                               Traceback (most recent call last)
C:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     17         try:
---> 18             return importlib.import_module(mname)
     19         except ImportError:

C:\Anaconda\lib\importlib\__init__.py in import_module(name, package)
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 

C:\Anaconda\lib\importlib\_bootstrap.py in _gcd_import(name, package, level)

C:\Anaconda\lib\importlib\_bootstrap.py in _find_and_load(name, import_)

C:\Anaconda\lib\importlib\_bootstrap.py in _find_and_load_unlocked(name, import_)

C:\Anaconda\lib\importlib\_bootstrap.py in _load_unlocked(spec)

C:\Anaconda\lib\importlib\_bootstrap.py in module_from_spec(spec)

C:\Anaconda\lib\importlib\_bootstrap_external.py in create_module(self, spec)

C:\Anaconda\lib\importlib\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)

ImportError: DLL load failed: Une routine d’initialisation d’une bibliothèque de liens dynamiques (DLL) a échoué.

During handling of the above exception, another exception occurred:

ModuleNotFoundError                       Traceback (most recent call last)
C:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

C:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
     20             return importlib.import_module('_pywrap_tensorflow_internal')
---> 21     _pywrap_tensorflow_internal = swig_import_helper()
     22     del swig_import_helper

C:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     19         except ImportError:
---> 20             return importlib.import_module('_pywrap_tensorflow_internal')
     21     _pywrap_tensorflow_internal = swig_import_helper()

C:\Anaconda\lib\importlib\__init__.py in import_module(name, package)
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 

ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-3-90de3482487c> in <module>()
      1 
      2 
----> 3 import tensorflow as tf
      4 from skimage import transform
      5 from skimage import data

C:\Anaconda\lib\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26 

C:\Anaconda\lib\site-packages\tensorflow\python\__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

C:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Anaconda\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: Une routine d’initialisation d’une bibliothèque de liens dynamiques (DLL) a échoué.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Anaconda\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems


```

Whenever I search for a solution, everyone recommend installing VCREDIST 2015, except I already have it.

Thank you for your time"
18552,parameterized_docker_build.sh fails to build,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
*NO*
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Dockerfile: nvidia/cuda:9.0-base-ubuntu16.04
My System: Fedora 27
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
v1.8.0-rc0
- **Python version**:
3.6
- **Bazel version**
N/A
- **CUDA/cuDNN version**
N/A
- **GPU model and memory**
N/A
- **Exact command to reproduce**
`./parameterized_docker_build.sh`

### Describe the problem
Build the Docker image from a fresh clone of Tensorflow:

``` shell
git clone https://github.com/tensorflow/tensorflow.git
git fetch --all --tags --prune
git checkout -b v.1.8.0 v1.8.0-rc0
cd tensorflow/tensorflow/tools/docker
export TF_DOCKER_BUILD_IS_DEVEL=NO
export TF_DOCKER_BUILD_TYPE=GPU
export TF_DOCKER_BUILD_PYTHON_VERSION=PYTHON3
./parameterized_docker_build.sh
```

After some time the build fails with:

```shell
AttributeError: '_NamespacePath' object has no attribute 'sort'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```

### Source code / logs
```shell
ERROR: /workspace/tensorflow/tools/api/generator/BUILD:27:1: Executing genrule //tensorflow/tools/api/generator:python_api_gen failed (Exit 1): bash failed: error executing command 
  (cd /home/oroel/projects/Picsure/tensorflow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_oroel/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda-9.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/local/lib/python3.5/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.0 \
    TF_CUDA_VERSION=9.0 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION=1 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/k8-py3-opt/bin/tensorflow/tools/api/generator/create_python_api bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/app/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/bitwise/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/compat/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/contrib/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/contrib/stat_summarizer/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/data/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/distributions/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/distributions/bijectors/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/errors/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/estimator/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/estimator/export/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/estimator/inputs/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/feature_column/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/gfile/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/graph_util/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/image/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/initializers/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/activations/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/densenet/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/inception_resnet_v2/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/inception_v3/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/mobilenet/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/nasnet/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/resnet50/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/vgg16/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/vgg19/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/xception/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/backend/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/callbacks/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/constraints/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/boston_housing/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/cifar10/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/cifar100/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/fashion_mnist/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/imdb/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/mnist/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/reuters/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/estimator/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/initializers/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/layers/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/losses/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/metrics/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/models/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/optimizers/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/preprocessing/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/preprocessing/image/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/preprocessing/sequence/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/preprocessing/text/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/regularizers/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/utils/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/wrappers/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/wrappers/scikit_learn/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/layers/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/linalg/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/logging/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/losses/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/manip/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/math/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/metrics/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/nn/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/nn/rnn_cell/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/profiler/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/python_io/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/resource_loader/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/builder/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/constants/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/loader/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/main_op/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/signature_constants/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/signature_def_utils/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/tag_constants/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/utils/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/sets/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/spectral/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/summary/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/sysconfig/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/test/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/train/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/train/queue_runner/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/user_ops/__init__.py')
Traceback (most recent call last):
  File ""/home/oroel/projects/Picsure/tensorflow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_oroel/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/tools/api/generator/create_python_api.py"", line 26, in <module>
    from tensorflow.python.util import tf_decorator
  File ""/home/oroel/projects/Picsure/tensorflow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_oroel/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/home/oroel/projects/Picsure/tensorflow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_oroel/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""/usr/local/lib/python3.5/dist-packages/google/protobuf/__init__.py"", line 37, in <module>
    __import__('pkg_resources').declare_namespace(__name__)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2927, in <module>
    @_call_aside
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2913, in _call_aside
    f(*args, **kwargs)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2952, in _initialize_master_working_set
    add_activation_listener(lambda dist: dist.activate())
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 956, in subscribe
    callback(dist)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2952, in <lambda>
    add_activation_listener(lambda dist: dist.activate())
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2515, in activate
    declare_namespace(pkg)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2097, in declare_namespace
    _handle_ns(packageName, path_item)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2047, in _handle_ns
    _rebuild_mod_path(path, packageName, module)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2066, in _rebuild_mod_path
    orig_path.sort(key=position_in_sys_path)
AttributeError: '_NamespacePath' object has no attribute 'sort'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 2047.653s, Critical Path: 176.85s
FAILED: Build did NOT complete successfully
Build failed.

```"
18551,TypeError: Cannot convert a tensor of type float32 to an input of type float32_ref,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
TypeError: Cannot convert a tensor of type float32 to an input of type float32_ref



### Source code / logs
C:\Users\PC.000\Desktop\emoji_final\asset\train\new>python afterpb.py
Traceback (most recent call last):
  File ""C:\Users\user.user-PC.000\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\importer.py"", line 667, in import_graph_de
    op._add_input(source_tensor, dtype=input_type)
  File ""C:\Users\user.user-PC.000\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1898, in _add_input
    (tensor.dtype.name, dtype.name))
TypeError: Cannot convert a tensor of type float32 to an input of type float32_ref

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""afterpb.py"", line 39, in <module>
    tf.import_graph_def(graph_def, name='')
  File ""C:\Users\user.user-PC.000\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\util\deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\user.user-PC.000\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\importer.py"", line 671, in import_graph_de
    node, 'Input tensor %r %s' % (input_name, te)))
ValueError: graph_def is invalid at node 'save/Assign': Input tensor 'conv1d_1/W:0' Cannot convert a tensor of type float32 to an input of type float32_ref.

"
18550,compile tensorflow r1.8 occured error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.8
- **Python version**:  python 3.6
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**: GCC-6，G++-6
- **CUDA/cuDNN version**: CUDA 9.1（include patch1/2/3），cuDNN 7.1
- **GPU model and memory**: gtx 1060ti，6GB； gtx 1080ti，11GB
- **Exact command to reproduce**: 
bazel build --config=opt --config=cuda --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
lately，I had try install tensorflow r1.8 from source but unsuccessful，then I tryed to compile tf r1.6 it is normal, and my friend also try to install tensorflow r1.8 from source was successful, which difference enviroment between us are CUDA/cuDNN and gcc/g++, he use CUDA 8.0/cuDNN 5.0 and gcc-5/g++5.

compile tf r1.8 occured error log as below：
```
/usr/include/c++/6/tuple:484:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement
     }
 ^
/usr/include/c++/6/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]':
/usr/include/c++/6/tuple:626:362:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<int, int, int>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'
./tensorflow/stream_executor/dnn.h:891:91:   required from here
/usr/include/c++/6/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'
       return __and_<is_convertible<_UElements&&, _Elements>...>::value;
                                                                 ^~~~~
/usr/include/c++/6/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement
     }
 ^
/usr/include/c++/6/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<int, int, int>&&; bool <anonymous> = true; _Elements = {int, int, int}]':
/usr/include/c++/6/tuple:686:422:   required by substitution of 'template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(std::tuple<_Args1 ...>&&) [with _UElements = {int, int, int}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> = <missing>]'
./tensorflow/stream_executor/dnn.h:891:91:   required from here
/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)
       return  __and_<__not_<is_same<tuple<_Elements...>,
                                                                                                                                                                                                                                                    ^    
/usr/include/c++/6/type_traits:1558:8: note: provided for 'template<class _From, class _To> struct std::is_convertible'
     struct is_convertible
        ^~~~~~~~~~~~~~
/usr/include/c++/6/tuple:502:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<int, int, int>&&; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement
     }
 ^
ERROR: /home/andy/TF/tensorflow/tensorflow/contrib/nccl/BUILD:23:1: output 'tensorflow/contrib/nccl/_objs/python/ops/_nccl_ops_gpu/tensorflow/contrib/nccl/kernels/nccl_ops.pic.o' was not created
ERROR: /home/andy/TF/tensorflow/tensorflow/contrib/nccl/BUILD:23:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1037.698s, Critical Path: 28.30s
FAILED: Build did NOT complete successfully
```

my configure of r1.7 and r1.8 is below:
```
(python3) andy@andy:~/TF/tensorflow$ ./configure 
WARNING: Running Bazel server needs to be killed, because the startup options are different.
You have bazel 0.11.1 installed.
Please specify the location of python. [Default is /home/andy/python3/bin/python]: 


Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: module 'site' has no attribute 'getsitepackages'
Found possible Python library paths:
  /home/andy/python3/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/home/andy/python3/lib/python3.6/site-packages]

Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: 
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n
No Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n
No Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n
No Amazon S3 File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n
No Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: 
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: 
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: 
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: 9.1


Please specify the location where CUDA 9.1 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:


Do you wish to build TensorFlow with TensorRT support? [y/N]: 
No TensorRT support will be enabled for TensorFlow.

Please specify the NCCL version you want to use. [Leave empty to default to NCCL 1.3]: 


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]


Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/x86_64-linux-gnu-gcc-6]: 


Do you wish to build TensorFlow with MPI support? [y/N]: 
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
Configuration finished
```

"
18549,non-gpu tensorflow cannot import and run ,"1. Windows 10
2. Anaconda 5.1.0 （Python 3.6.4)
3. **NO GPU version**  (my laptop doesn't have GPU... Poor )
4. **MSVCP140.DLL** already installed and setted in PATH , it's everywhere

>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *  # pylint: disable=redefined-builtin
  File ""C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
18548,"[r1.7][TensorRT] The passing parameter ""max_batch_size"" within function ""trt.create_inference_graph()"" is fixed to 1","What do I intended to do:
I've already trained a model by TensorFlow, and then try to use the integrated TensorRT to import the frozen model and optimize it, then output the optimized graph_def and session run it by TensorFlow.

Script:                                                 
```
      with tf.Graph().as_default():
         with tf.device(""/device:GPU:0""):                                                                                 
            output_graph_def = tf.GraphDef()
            with open(""./""+frozen_model_name, ""rb"") as f:
               output_graph_def.ParseFromString(f.read())
               print (len(output_graph_def.node))
               _ = tf.import_graph_def(output_graph_def, name="""")
                                                                                   
        f32_graph = trt.create_inference_graph(
            input_graph_def=output_graph_def,                                                                                       
            outputs=['InceptionV3/Logits/SpatialSqueeze'],
            max_batch_size = 16,  #<<<<<<<<<<<<<<<HERE
            max_workspace_size_bytes=1 << 20,
            precision_mode=""FP32"",  # TRT Engine precision ""FP32"",""FP16"" or ""INT8""                                        
            minimum_segment_size=2  # minimum number of nodes in an engine                                                
        )
```
Log:
```
host/replica:0/task:0/device:GPU:0 with 4757 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:08.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""extract_model_tf_trt_frozen.py"", line 118, in <module>
    _main()
  File ""extract_model_tf_trt_frozen.py"", line 91, in _main
    val = sess.run(out, {inp: batch_input})
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1116, in _run
    str(subfeed_t.get_shape())))
ValueError: Cannot feed value of shape (16, 299, 299, 3) for Tensor u'import/Placeholder:0', which has shape '(1, 299, 299, 3)'
```

Observation:
Seems like even though I set the passing parameter ""max_batch_size"" to 16, the generated new graph still only has batch_size == 1. I can't locate the source file of trt_convert(), which is relevant actually, probably it's a binary .o file so I can only post a new issue here for help.

Has anyone encountered the same issue?
Any idea will be welcome."
18546,"call session.run() concurrency, latency boom","background: serving mnist model using libtensorflow_cc.so and libtensorflow_framework.so gpu(p40) version

below: limit 3 concurrency to call `session.run()`, got the latency 508us, very close to serial run.
![image](https://user-images.githubusercontent.com/3832082/38794426-a9a36dc4-4187-11e8-8aa6-57df4c61578f.png)

However, expand  concurrency to 24 , `session.run()` got the latency 1256us
![image](https://user-images.githubusercontent.com/3832082/38794599-3c0e1380-4188-11e8-9560-6dd4fd1e9750.png)

anyone can help ?

logs：
```shell
[DDL-Serving]$ ./DDL_server --model_config_file=/home/luban/DDL-Serving/config.json
2018-04-16 14:29:38.599280: I /home/luban/DDL-Serving/core/server_core.cc:91] Loading models:tensorflow
2018-04-16 14:29:38.599490: I /home/luban/DDL-Serving/core/server_core.cc:98] name:/home/luban/DDL-Serving/mnist_model/3
I0416 14:29:38.599814 23547 /home/luban/DDL-Serving/core/loader_helper.cc:59] Approving load for servable version { name:mnist ,version:3}
I0416 14:29:38.599953 23549 /home/luban/DDL-Serving/core/loader_helper.cc:67] Loading servable version { name:mnist ,version:3}
I0416 14:29:38.600110 23549 /home/luban/DDL-Serving/core/tf_bundle_factory.cc:230] Attempting to load native SavedModelBundle in loader.cc from: /home/luban/DDL-Serving/mnist_model/3
2018-04-16 14:29:38.600163: I tensorflow/cc/saved_model/loader.cc:242] Loading SavedModel with tags: { serve }; from: /home/luban/DDL-Serving/mnist_model/3
2018-04-16 14:29:38.910954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:02:00.0
totalMemory: 22.38GiB freeMemory: 22.21GiB
2018-04-16 14:29:38.911028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2018-04-16 14:29:39.721881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-04-16 14:29:39.721955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2018-04-16 14:29:39.721971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2018-04-16 14:29:39.722897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21555 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:02:00.0, compute capability: 6.1)
2018-04-16 14:29:40.028849: I tensorflow/cc/saved_model/loader.cc:161] Restoring SavedModel bundle.
2018-04-16 14:29:40.034555: I tensorflow/cc/saved_model/loader.cc:196] Running LegacyInitOp on SavedModel bundle.
2018-04-16 14:29:40.040415: I tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: success. Took 1440252 microseconds.
I0416 14:29:40.040555 23549 /home/luban/DDL-Serving/core/loader_helper.cc:77] Successfully loaded servable version { name:mnist ,version:3}
2018-04-16 14:29:40.040656: I /home/luban/DDL-Serving/core/server_core.cc:98] name:/home/luban/DDL-Serving/mnist_model/1
I0416 14:29:40.040829 23547 /home/luban/DDL-Serving/core/loader_helper.cc:59] Approving load for servable version { name:mnist ,version:1}
I0416 14:29:40.040890 23549 /home/luban/DDL-Serving/core/loader_helper.cc:67] Loading servable version { name:mnist ,version:1}
I0416 14:29:40.040937 23549 /home/luban/DDL-Serving/core/tf_bundle_factory.cc:230] Attempting to load native SavedModelBundle in loader.cc from: /home/luban/DDL-Serving/mnist_model/1
2018-04-16 14:29:40.040966: I tensorflow/cc/saved_model/loader.cc:242] Loading SavedModel with tags: { serve }; from: /home/luban/DDL-Serving/mnist_model/1
2018-04-16 14:29:40.041996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2018-04-16 14:29:40.042044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-04-16 14:29:40.042066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2018-04-16 14:29:40.042082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2018-04-16 14:29:40.042318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21555 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:02:00.0, compute capability: 6.1)
2018-04-16 14:29:40.044106: I tensorflow/cc/saved_model/loader.cc:161] Restoring SavedModel bundle.
2018-04-16 14:29:40.048062: I tensorflow/cc/saved_model/loader.cc:196] Running LegacyInitOp on SavedModel bundle.
2018-04-16 14:29:40.053422: I tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: success. Took 12455 microseconds.
I0416 14:29:40.053490 23549 /home/luban/DDL-Serving/core/loader_helper.cc:77] Successfully loaded servable version { name:mnist ,version:1}
2018-04-16 14:29:40.053579: I /home/luban/DDL-Serving/core/server_core.cc:123] Load models Done.
I0416 14:29:40.061132 23547 /home/luban/DDL-Serving/brpc/src/brpc/server.cpp:984] Server[DDL::serving::PredictionServiceImpl] is serving on port=8002.
I0416 14:29:40.061608 23547 /home/luban/DDL-Serving/brpc/src/brpc/server.cpp:987] Check out http://dd80f54614c3:8002 in web browser.
```
"
18543,"saved_model.pb, saved_model.pbtxt missing google cloud","My model has finished training but i can not see the `saved_model.pb, saved_model.pbtxt` in the training directory. or any of my bucket folders"
18542,Why single-image inference on Android using tflite shows different answer everytime?,"Hello, I have already asked a question on stackoverflow.`https://stackoverflow.com/questions/49833090/tflite-accuracy-on-a-device-is-not-consistent-with-that-of-origin-pb-file-on-pc`
I am not sure about the mechanism of tensorflow lite, but I am now pretty sure that tflite run only a random part of the model to generate a result. I have thought of the toco operation being the cause and I used it to convert tflite back to pb which works fine same as the origin pb. So I have no idea what's going wrong here. Any advice? thanks a lot."
18541,How to get started with Tensorflow Lite?,"How do you get started with Tensorflow Lite? For example, how do you download and/or build it? How do you use it? All I can find is statements that ""most of our documentation is on Github"", but that seems to be a dead end. 

The only documentation I found was the stuff in the g3doc folder, which includes build instructions for iOS and Raspberry Pi, but it doesn't say anything about Android."
18540,Tensorflow leaks 1280 bytes with each session opened and closed? (Python API),"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 17.10
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.6 (also tested on 1.7)
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
9.0/7.0
- **GPU model and memory**:
Titan XP, 12GB
- **Exact command to reproduce**:
To reproduce, save the following python script as `memory_test.py`:
```
    import tensorflow as tf
    import sys
    n_Iterations=int(sys.argv[1])
    def open_and_close_session():
       with tf.Session() as sess:
          pass
    for _ in range(n_Iterations):
       open_and_close_session()
    with tf.Session() as sess:
       print(""bytes used="",sess.run(tf.contrib.memory_stats.BytesInUse()))
```
Then run it from the command line using different number of iterations:
`python memory_test.py 0`, `python memory_test.py 1`, `python memory_test.py 10`, and so on.

### Describe the problem
It seems that each Tensorflow session I open and close consumes 1280 bytes from the GPU memory, which are not released until the python kernel is terminated. 

Running the script given above, which simply opens and closes sessions without any further operation, yields these results:
 - `python memory_test.py 0` yields `bytes used= 1280`
 - `python memory_test.py 1` yields `bytes used= 2560`.
 - `python memory_test.py 10` yields `bytes used= 14080`.
 - `python memory_test.py 100` yields `bytes used= 129280`.
 - `python memory_test.py 1000` yields `bytes used= 1281280`.

The math is easy - each session opened and closed leaks 1280 bytes. I tested this script on two different ubuntu 17.10 workstations with tensorflow-gpu 1.6 and 1.7 and different NVIDIA GPUs.

(here's a related [stackoverflow question](https://stackoverflow.com/q/49735217/1500585), at least one user was able to reproduce)."
18539,Unable to convert MRCNN model to .tflite model,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.4
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:
`python3 -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
b'v1.7.0-1844-ga0edcf60f7' 1.7.0`
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:
`bazel version
Build label: 0.10.0-homebrew
Build target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Jan 10 02:02:06 +50057 (1517480013726)
Build timestamp: 1517480013726
Build timestamp as int: 1517480013726`
- **GCC/Compiler version (if compiling from source)**:
`gcc --version
Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.13.sdk/usr/include/c++/4.2.1
Apple LLVM version 9.0.0 (clang-900.0.39.2)
Target: x86_64-apple-darwin17.5.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin`
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
`bazel-bin/tensorflow/contrib/lite/toco/toco   --input_file=./mobile_mrcnn.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/tmp/mobilenet_v1_1.0_224.tflite --inference_type=FLOAT --input_arrays=input_image,input_image_meta --output_arrays=output_node0,output_node1,output_node2,output_node3,output_node4,output_node5,output_node6 --input_shapes=1,224,224,3:1,89`

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
When trying to convert an MRCNN from a frozen graph (.pb) file to (.tflite) using the tensorflow toco script, I get an `Abort trap: 6` error with no explanation. Any advice on how to debug/add unsupported ops/functionality or just what went wrong would be great.

### Source code / logs
Error:
`bazel-bin/tensorflow/contrib/lite/toco/toco   --input_file=./mobile_mrcnn.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/tmp/mobilenet_v1_1.0_224.tflite --inference_type=FLOAT --input_arrays=input_image,input_image_meta --output_arrays=output_node0,output_node1,output_node2,output_node3,output_node4,output_node5,output_node6 --input_shapes=1,224,224,3:1,89 
2018-04-15 15:48:31.302680: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: ResizeNearestNeighbor
2018-04-15 15:48:31.303544: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: ResizeNearestNeighbor
2018-04-15 15:48:31.303972: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: ResizeNearestNeighbor
2018-04-15 15:48:31.319684: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: NonMaxSuppression
2018-04-15 15:48:31.319847: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Round
2018-04-15 15:48:31.319897: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Equal
2018-04-15 15:48:31.319908: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where
2018-04-15 15:48:31.319921: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd
2018-04-15 15:48:31.319966: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: CropAndResize
2018-04-15 15:48:31.319983: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Equal
2018-04-15 15:48:31.319993: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where
2018-04-15 15:48:31.320006: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd
2018-04-15 15:48:31.320048: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: CropAndResize
2018-04-15 15:48:31.320065: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Equal
2018-04-15 15:48:31.320074: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where
2018-04-15 15:48:31.320087: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd
2018-04-15 15:48:31.320131: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: CropAndResize
2018-04-15 15:48:31.320149: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Equal
2018-04-15 15:48:31.320159: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where
2018-04-15 15:48:31.320180: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd
2018-04-15 15:48:31.320370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: CropAndResize
2018-04-15 15:48:31.416940: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd
2018-04-15 15:48:31.416977: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd
2018-04-15 15:48:31.417344: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Rint
2018-04-15 15:48:31.417372: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where
2018-04-15 15:48:31.417419: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Unique
2018-04-15 15:48:31.417470: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: TensorArrayV3
2018-04-15 15:48:31.417531: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: TensorArrayScatterV3
2018-04-15 15:48:31.417551: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: TensorArrayV3
2018-04-15 15:48:31.417578: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter
2018-04-15 15:48:31.417590: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter
2018-04-15 15:48:31.417621: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter
2018-04-15 15:48:31.417635: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: LoopCond
2018-04-15 15:48:31.417673: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter
2018-04-15 15:48:31.417685: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter
2018-04-15 15:48:31.417709: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: TensorArrayReadV3
2018-04-15 15:48:31.417719: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter
2018-04-15 15:48:31.417729: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Equal
2018-04-15 15:48:31.417738: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where
2018-04-15 15:48:31.417785: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter
2018-04-15 15:48:31.417807: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter
2018-04-15 15:48:31.417838: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: NonMaxSuppression
2018-04-15 15:48:31.417853: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter
2018-04-15 15:48:31.417942: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: PadV2
2018-04-15 15:48:31.417955: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter
2018-04-15 15:48:31.417969: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: TensorArrayWriteV3
2018-04-15 15:48:31.418017: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Exit
2018-04-15 15:48:31.418028: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: TensorArraySizeV3
2018-04-15 15:48:31.418069: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: TensorArrayGatherV3
2018-04-15 15:48:31.418957: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where
2018-04-15 15:48:31.419022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: DenseToDenseSetOperation
2018-04-15 15:48:31.419054: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: SparseToDense
2018-04-15 15:48:31.419385: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Round
2018-04-15 15:48:31.419425: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Equal
2018-04-15 15:48:31.419434: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where
2018-04-15 15:48:31.419445: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd
2018-04-15 15:48:31.419498: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: CropAndResize
2018-04-15 15:48:31.419525: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Equal
2018-04-15 15:48:31.419545: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where
2018-04-15 15:48:31.419557: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd
2018-04-15 15:48:31.419597: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: CropAndResize
2018-04-15 15:48:31.419612: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Equal
2018-04-15 15:48:31.419620: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where
2018-04-15 15:48:31.419654: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd
2018-04-15 15:48:31.419725: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: CropAndResize
2018-04-15 15:48:31.419756: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Equal
2018-04-15 15:48:31.419766: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where
2018-04-15 15:48:31.419793: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd
2018-04-15 15:48:31.419850: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: CropAndResize
2018-04-15 15:48:31.445467: F tensorflow/contrib/lite/toco/tooling_util.cc:822] Check failed: d >= 1 (0 vs. 1)
Abort trap: 6`

Source:
https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py"
18538,AttributeError: module 'tensorflow' has no attribute 'Session',"When I call any function in python3.6, I get the error below; however, it works fine in python3.4. Any idea? 
import tensorflow as tf 
tf.Session()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'Session'
>>> 
Here is my  System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 8.7 
- **TensorFlow installed from (source or binary)**: by pip3
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 3.6.5
- **CUDA/cuDNN version**: cuda 9.0 and cudnn 7.0
- **GPU model and memory**: K80, 12 GB 
- **Exact command to reproduce**:
import tensorflow as tf 
tf.Session()"
18537,Slow matrix multiplication using Tensorflow 1.7.0 on a GPU,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 Home (64 bit) Version 1709 OS Build 16299.371
- **TensorFlow installed from (source or binary)**:
Binary (Tensorflow GPU)
- **TensorFlow version (use command below)**:
PS C:\Users\gautam> python
Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:54:40) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> print(tf.__version__)
1.7.0
- **Python version**: 
3.6.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
**CUDA:**
PS C:\Users\gautam> nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Sep__1_21:08:32_Central_Daylight_Time_2017
Cuda compilation tools, release 9.0, V9.0.176
**cuDNN:**
#define CUDNN_MAJOR 7
#define CUDNN_MINOR 0
#define CUDNN_PATCHLEVEL 5

- **GPU model and memory**:
Nvidia GTX 1050, 4GB
- **Exact command to reproduce**:

### Describe the problem
I have tensorflow-gpu installed on my Dell XPS 15 laptop running Windows 10 (environment specified above). I tried to compare matrix multiplication performance between np.matmul() and tf.matmul() for the graph mode of execution. The numpy method returns the result in about 13.5 seconds whereas the tensorflow method takes a long time and eventually fails.

### Source code / logs
I generated input matrices in the following way:
x = np.random.random((10000,10000))
y = np.random.random((10000,10000))

Here are the results:
1. np.matmul(x,y) takes about 13.5 seconds
2. The following takes a long time and eventually errors out:
with tf.Session() as sess:
    z = tf.matmul(x,y).eval()
2018-04-15 11:35:05.157088: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493
pciBusID: 0000:01:00.0
totalMemory: 4.00GiB freeMemory: 3.30GiB
2018-04-15 11:35:05.163665: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1423] Adding visible gpu devices: 0
2018-04-15 11:35:07.338162: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-04-15 11:35:07.348029: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:917]      0
2018-04-15 11:35:07.353379: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:930] 0:   N
2018-04-15 11:35:07.363613: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3033 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-04-15 11:36:56.906874: E T:\src\github\tensorflow\tensorflow\stream_executor\cuda\cuda_driver.cc:1110] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED ::
2018-04-15 11:36:56.906874: E T:\src\github\tensorflow\tensorflow\stream_executor\cuda\cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED
2018-04-15 11:36:56.915430: F T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_event_mgr.cc:203] Unexpected Event status: 1
"
18536,DNNRegressor.predict issue,"I have trained a Deep Neural Network Regressor on some weather data. When I tried classifier.predict(), it return a generator object. Usually what we do is to put list() over the object to get the prediction. But it is not working.
I was unable to get the my code to turn into it's code form with the `` symbol by the way. So bare with me.

import numpy as np
import time
import itertools
from onehotencode import load_single_data
from onehotencode import training_week_data,training_week_price,\
    testing_week_data,testing_week_price,uber_price
import tensorflow as tf

feature_columns = [tf.feature_column.numeric_column(""x"", shape=[163])]
classifier = tf.estimator.DNNRegressor(feature_columns=feature_columns,
                                       hidden_units=[200, 175, 150, 125, 100, 75, 50, 25, 17, 10, 8, 5, 3],
                                        model_dir='great_model/'
                                        )
onehot,price=load_single_data([[5,18,16,1],'Mostly Sunny','Mostly Sunny',46.5])

my_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""x"": np.array(onehot)},
        y=None,
        num_epochs=None,
        shuffle=False)
y = classifier.predict(input_fn=my_input_fn)
#This line produced the error
print(list(y))

The error produced is :

`ValueError: Dimension size must be evenly divisible by 163 but is 128 for 'dnn/input_from_feature_columns/input_layer/x/Reshape' (op: 'Reshape') with input shapes: [128,1], [2] and with input tensors computed as partial shapes: input[1] = [?,163].`
"
18535,"Current Bazel version is 0.12.0, expected at least 0.4.2","Problem when building tensorflow from source. I have been following the guides from the tensorflow official website but apparently they are not in sync.

Tried to get the latest Bazel and build it myself , I found out the 0.12.0 was the latest on Bazel github. 

Have I written custom code: NO
OS Platform and Distribution: Linux Ubuntu 16.04
TensorFlow installed from: source
TensorFlow version: 1.7
CUDA/cuDNN version: No
GPU model and memory: No
Exact command to reproduce: ./configure"
18534,ValueError: An initializer for variable conv2d/kernel of <dtype: 'string'> is required,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 17.10
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.7 cpu only
- **Python version**: 
3.6.5
- **Bazel version (if compiling from source)**:
no
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I want to train a CNN model classify images use keras model. Train data import from CSV files via pandas.  the dataset i frist want to change to float32 and split by ','but system return error, i change back to string  split by ' ', got error as below.

When I call the train method, an error is thrown.

How can I fix this?

image size 48*48

### Source code / logs
model:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image
#import csv
from numpy import imag, float32

import tensorflow as tf
from math import floor
import sys


TRAIN_CSV_COLUMN_NAMES=['label','feature']
TEST_CSV_COLUMN_NAMES=['id','feature']
trainEpochs = 40
batchSize = 100
LEARNING_RATE = 1e-4

class Model(tf.keras.Model):
    def __init__(self):
        super(Model, self).__init__()
        self._input_shape = [-1,48,48,1]
        self.conv1 = tf.layers.Conv2D(32,5,padding = 'same',data_format = 'channels_last',activation=tf.nn.relu)
        self.conv2 = tf.layers.Conv2D(32,5,padding = 'same',data_format = 'channels_last',activation = tf.nn.relu)
        self.conv3 = tf.layers.Conv2D(64,5,padding = 'same',data_format = 'channels_last',activation = tf.nn.relu)
        self.max_pool2d = tf.layers.MaxPooling2D((2,2),(2,2),padding = 'same', data_format = 'channels_last')
        self.fc1 = tf.layers.Dense(1600,activation=tf.nn.relu)
        self.fc2 = tf.layers.Dense(7)
        self.dropout = tf.layers.Dropout(0.4)
    def __call__(self,input,training):
        y=tf.reshape(input,self._input_shape)
        y=self.conv1(y)#error appear here
        y=self.max_pool2d(y)
        y=self.conv2(y)
        y=self.max_pool2d(y)
        y=self.conv3(y)
        
        y=tf.layers.flatten(y)
        y=self.fc1(y,activation = tf.nn.relu)
        y=self.dropout(y,training=training)
        return self.fc2(y)


#load data from csv    
def dataLoad():
    oragTrainData = pd.read_csv('train.csv',names = TRAIN_CSV_COLUMN_NAMES ,header = 0)
    oragTestData = pd.read_csv('test.csv', names = TEST_CSV_COLUMN_NAMES, header = 0)
    trainData,valiData =  splitValid(oragTrainData,0.2)
    trainData,labels = trainData,trainData.pop('label')
    valiData,vaLabels = valiData, valiData.pop('label')
    testData,tId = oragTestData,oragTestData.pop('id')
    return trainData,testData,valiData,labels,vaLabels,tId
#input function
 def trainDataInputFn(): 
        features = tf.data.Dataset.from_tensor_slices(trainData)
        labels = tf.data.Dataset.from_tensor_slices(trainLabels)
        ds = tf.data.Dataset.zip((features,labels))
        ds=ds.cache().shuffle(5000).batch(batchSize)
        ds = ds.make_one_shot_iterator().get_next()
        return ds
#model_fn function
def model_fn(features,labels,mode,params):
    model = Model()
    images = features
    if isinstance(images,dict):
        images = images['feature']
    
    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)
        logists = model(images, training=True)
        loss = tf.losses.sparse_softmax_cross_entropy(labels = labels, logits=logists)
        accuracy = tf.metrics.accuracy(labels=labels, predictions = logists)
        
        tf.identity(LEARNING_RATE, 'learning_rate')
        tf.identity(loss,'loss')
        tf.identity(accuracy,'accuracy')
        
        tf.summary.scalar(accuracy[1],name='training accuracy')
        print('9999999999999999999999999999999999999999999999999999')
        return tf.estimator.EstimatorSpec(mode = tf.estimator.ModeKeys.TRAIN,
                                          loss = loss,
                                          train_op = optimizer.minimize(loss, tf.train.get_or_create_global_step()))
pass to model data format:
[[b'35 7 7 7 5 10 16 8 30 66 111.....48 54 56 53 67']
...
[b'249 249 249 249 247 254 ......143 148 97 98 59 57']]

error information
Traceback (most recent call last):
  File ""emontionClassfy.py"", line 218, in <module>
    main(argv=sys.argv)  
  File ""emontionClassfy.py"", line 206, in main
    emontionClassify.train(input_fn=trainDataInputFn)
  File ""/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 355, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 824, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 805, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""emontionClassfy.py"", line 134, in model_fn
    logists = model(images, training=True)
  File ""emontionClassfy.py"", line 45, in __call__
    y=self.conv1(y)
  File ""/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 696, in __call__
    self.build(input_shapes)
  File ""/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py"", line 144, in build
    dtype=self.dtype)
  File ""/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 546, in add_variable
    partitioner=partitioner)
  File ""/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/checkpointable.py"", line 415, in _add_variable_with_custom_getter
    **kwargs_for_getter)
  File ""/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 1297, in get_variable
    constraint=constraint)
  File ""/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 1093, in get_variable
    constraint=constraint)
  File ""/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 439, in get_variable
    constraint=constraint)
  File ""/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 408, in _true_getter
    use_resource=use_resource, constraint=constraint)
  File ""/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 773, in _get_single_variable
    name=name, shape=shape, dtype=dtype)
  File ""/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 853, in _get_default_initializer
    % (name, dtype.base_dtype))
ValueError: An initializer for variable conv2d/kernel of <dtype: 'string'> is required
"
18532,Error fetching values from variables - queue behavior semantics needs clarification,"I am generating the following traceback when I try to feed data to the queue.

Have I written custom code : NO
OS Platform and Distribution : Ubuntu 16.04
TensorFlow installed from : pip
TensorFlow version: 1.4.1
Bazel version : N/A
CUDA/cuDNN version : CUDA Version 8.0.61
GPU model and memory : GTX 680 - 4 gb memory
Exact command to reproduce : detaliled in description

The error message is very long so let me try to break it down

The variable is declared as follows 

    a = tf.placeholder(tf.float32, shape=[1,2], name='a')

The variable is fed using a feed_dict as follows

     _,X_hat_val,loss_val = sess.run([train,X_hat,loss],  
                            feed_dict={X : np.array([[ x_ord[0,0],y_ord[0,0]]]), 
                                      a : np.array([[ x_acc[0,0], y_acc[0,0] ]]),
                                      a_prev : np.array([[ out_xacc[0,0], out_yacc[0,0] ]]) })

The variable is printed before feeding to the queue:

    [[ 0.  0.]] # the value of the variable
    (1, 2) # the shape of the variable
    float32 # the data type of the variable

The error is generated at 

    tensorflow.python.framework.errors_impl.InvalidArgumentError: 
    You must feed a value for placeholder tensor 'a' with dtype float and shape [1,2]

However, when I remove an operation to fetch the variable value it works perfectly

print(sess.run(a))

I am not sure if the queue mechanism is seeking out a new value when I run the operation, or if the semantics of the operation is not what I think it is

    ==========      Training Model  ==========
    
    
    /home/kiran/projects/Kalman/data/S10_05_19_2017_train_mouse_subjects.tfrecords
    Running Model
    
    
    
    
    **********Running Graph with a session**********
    
    
    
    
    2018-04-15 09:07:11.227273: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
    2018-04-15 09:07:11.263222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning
     NUMA node zero
    2018-04-15 09:07:11.263499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:
    name: GeForce GTX 680 major: 3 minor: 0 memoryClockRate(GHz): 1.0845
    pciBusID: 0000:02:00.0
    totalMemory: 3.93GiB freeMemory: 3.49GiB
    2018-04-15 09:07:11.263522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 680, pci bus id: 0000:02:00.0, compute capa
    bility: 3.0)
    19999
    [[ 0.  0.]]
    (1, 2)
    float32
    Processing record :  0
    
    
    2018-04-15 09:07:11.871973: W tensorflow/core/kernels/queue_base.cc:295] _0_input_producer: Skipping cancelled enqueue attempt with queue not closed
    Traceback (most recent call last):
      File ""/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1323, in _do_call
        return fn(*args)
      File ""/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1302, in _run_fn
        status, run_metadata)
      File ""/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
        c_api.TF_GetCode(self.status.status))
    tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'a' with dtype float and shape [1,2]
             [[Node: a = Placeholder[dtype=DT_FLOAT, shape=[1,2], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
             [[Node: a/_69 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_4_a"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""KalmanModel.py"", line 378, in <module>
        training(subject)
      File ""KalmanModel.py"", line 242, in training
        print(sess.run(a))
      File ""/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 889, in run
        run_metadata_ptr)
      File ""/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
        feed_dict_tensor, options, run_metadata)
      File ""/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
        options, run_metadata)
      File ""/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
        raise type(e)(node_def, op, message)
    tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'a' with dtype float and shape [1,2]
             [[Node: a = Placeholder[dtype=DT_FLOAT, shape=[1,2], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
             [[Node: a/_69 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_4_a"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
    
    Caused by op 'a', defined at:
      File ""KalmanModel.py"", line 378, in <module>
        training(subject)
      File ""KalmanModel.py"", line 159, in training
        a = tf.placeholder(tf.float32, shape=[1,2], name='a')
      File ""/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1599, in placeholder
        return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)
      File ""/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3091, in _placeholder
        ""Placeholder"", dtype=dtype, shape=shape, name=name)
      File ""/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
        op_def=op_def)
      File ""/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
        op_def=op_def)
      File ""/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
        self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access
    
    InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'a' with dtype float and shape [1,2]
             [[Node: a = Placeholder[dtype=DT_FLOAT, shape=[1,2], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
             [[Node: a/_69 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_4_a"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

"
18531,Issues running tensorflow-GPU: Couldn't open CUDA library libcuda.so.1.,"Have I written custom code: No
OS Platform and Distribution: Scientific Linux release 6.9 (Carbon)
TensorFlow installed from: wheel
TensorFlow version: 0.11.0rc0
Bazel version: N/A
CUDA/cuDNN version: 7.5 / N/A
GPU model and memory: Tesla K20m 5GB
Exact command to reproduce: Error occurs when importing tensorflow 

I am having issues running tensorflow-gpu on the cluster computer at my university. There are a number of Tesla K20m GPUs available. 
 
The system only has CUDA 7.5 installed so I installed tensorflow in a conda environment using a wheel from version 0.11 which supports CUDA 7.5. I also needed to use the dirty trick posted [here](https://stackoverflow.com/questions/33655731/error-while-importing-tensorflow-in-python2-7-in-ubuntu-12-04-glibc-2-17-not-f?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa) to get this wheel to work, as we have GLIC_2.12 installed. However I don't think the above is causing the problem I am now facing. 

When I try and run Tensorflow I get the following error:

```
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: /cm/shared/apps/cuda/7.5/lib64:/cm/shared/apps/cuda/7.5/targets/x86_64-linux/lib
I tensorflow/stream_executor/cuda/cuda_dnn.cc:3448] Unable to load cuDNN DSO
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: /its/home/tjb32/new-tensorflow-workspace/glib-download/libc6_2.17/lib/x86_64-linux-gnu/:/its/home/tjb32/new-tensorflow-workspace/glib-download/libc6_2.17/usr/lib64/:/cm/shared/apps/cuda/7.5/lib64:/cm/shared/apps/cuda/7.5/targets/x86_64-linux/lib
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: node152
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: Permission denied: could not open driver version path for reading: /proc/driver/nvidia/version
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1080] LD_LIBRARY_PATH: /its/home/tjb32/new-tensorflow-workspace/glib-download/libc6_2.17/lib/x86_64-linux-gnu/:/its/home/tjb32/new-tensorflow-workspace/glib-download/libc6_2.17/usr/lib64/:/cm/shared/apps/cuda/7.5/lib64:/cm/shared/apps/cuda/7.5/targets/x86_64-linux/lib
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1081] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:140] kernel driver does not appear to be running on this host (node152): /proc/driver/nvidia/version does not exist

```

I know cuDNN isn't installed on the system, but it seems the fatal error is the fact the tenorflow cannot find libcuda.so.1.

Running `locate libcuda*` on a login node returns the following: `/usr/lib64/nvidia/libcuda.so
/usr/lib64/nvidia/libcuda.so.1
/usr/lib64/nvidia/libcuda.so.384.98`

I can also find libcuda.so if I navigate to the `/cm/shared/apps/conda/7.5/lib64/stubs`. 

However if I add `locate libcuda*` to the job I submit to the GPU queue it returns nothing. So adding the `/usr/lib64/nvidia` to my path does not solve the issue. It seems that the GPU nodes do not have access to llibcuda.so leading CUDA to believe there is no GPU device. 

Am I missing some additional configuration that I need to do to get Tensorflow working? Or is this more likely an issue with the underlying CUDA installation that I will need to get an system admin to help with?

"
18530,DLL load failed on Windows 10 with tensorflow 1.7,"Hello to all,
on my Windows 10, I installed tensorflow 1.5 and it works, but if I try to upgrade to tensorflow 1.7 it don't works, appeare the error:
**ImportError: DLL load failed
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'**

Microsoft C++ Runtime is updated

If I downgrade to the version 1.5 it works again
How can I solve it ?

Thanks
Sergio
"
18529,UnimplementedError: Cast uint8 to uint32 is not supported,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.7.0-0-g024aecf414
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.1/7.1.1
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
UnimplementedError: Cast uint8 to uint32 is not supported

It is unintuitive that tf.cast from uint8 to float32 works, but uint8 to uint32 does not.
Please document or implement unsupported pairs of numerical dtypes for tf.cast.

### Source code / logs
```
x = tf.cast(uint8_tensor, dtype=tf.uint32)
```
```
   return [tf.reduce_sum(tf.cast(orig, dtype=tf.uint32))]
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 782, in cast
    return gen_math_ops.cast(x, base_type, name=name)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 1524, in cast
    ""Cast"", x=x, DstT=DstT, name=name)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3290, in create_op
    op_def=op_def)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1654, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

UnimplementedError (see above for traceback): Cast uint8 to uint32 is not supported
	 [[Node: Cast_1 = Cast[DstT=DT_UINT32, SrcT=DT_UINT8, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_img_0_0)]]
```
"
18528,Gradient Inconsistency ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: *No*
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: *Linux Ubuntu 14.04.5*
- **TensorFlow installed from (source or binary)**: *pip install from binary*
- **TensorFlow version (use command below)**: *v1.4.0-19-ga52c8d9 1.4.1*
- **Python version**: *Python3.5 (Anaconda)*
- **Bazel version (if compiling from source)**: *N/A*
- **GCC/Compiler version (if compiling from source)**: *N/A*
- **CUDA/cuDNN version**: *CUDA-8.0*
- **GPU model and memory**: *GeForce GTX 1070(8GB) & GeForce GTX 770(4GB)*
- **Exact command to reproduce**: *N/A*


### Describe the problem
When use operation `tf.transpose()`, `tf.gather()`, `tf.cholesky()` all together in a row over instance of `tf.Variable()`, the backward gradient computation may seems inconsistent. By using 'inconsistent', I mean that after run the same script multiple times with fixed random seeds, the computed gradient of **result of `tf.cholesky()`** over **input of `tf.transpose()`** are not always identical.

### Source code / logs
```python
import tensorflow as tf
import numpy as np

np.random.seed(1024)
tf.set_random_seed(1024)

N = 10

# indices for `tf.gather()`
indices_for_gather = tf.constant(np.concatenate((np.zeros(N), np.ones(N))).reshape(-1,).astype(np.int32))

# build a 2-by-2 PSD matrix as `param` for 'tf.gather()'
W = tf.constant(np.random.rand(2, 1), dtype=tf.float32)
W = tf.Variable(W)

PSD2x2 = tf.matmul(W, W, transpose_b=True)

# do the `tf.gather()`
M_temp = tf.gather(PSD2x2, indices_for_gather)

# Then transpose the M_Temp and tf.gather() again (to ensure the result is a PSD for cholesky)
M_temp_T = tf.transpose(M_temp)

PSD = tf.gather(M_temp_T, indices_for_gather) + tf.eye(2*N, dtype=tf.float32) * 0.01

# cholesky
L = tf.cholesky(PSD)

# compute the gradient of `L` over `M_temp`
grad = tf.gradients(L, M_temp)

# build a session and compute the gradient
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
results = []
with tf.Session(config=config) as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(10):
        result = sess.run(grad)
        results.append(result[0]) 
        
# check the results are whether identical or not
for i in range(len(results) - 1): 
    if isinstance(results[i], np.ndarray):
        print(np.all(np.equal(results[i], results[i+1])))
    else:
        print(np.all(np.equal(results[i].values, results[i+1].values)))
```

The output result is
```
False
False
False
False
False
False
False
False
False
```
If I comment the line `W = tf.Variable(W)` making the `W` a constant tensor, the results are ideally all `True`. And I've also tried to compute the gradient of`L` over `M_temp_T` and `PSD`, both of them are all `True`. So I think the problem lies in the using of `tf.transpose()`, `tf.gather()`, `tf.cholesky()` all together over instance of `tf.Variable()`."
18527,New implementation of `tf.clip_by_value` changes behavior of the op.,"FYI, @jart and @nfelt 

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux (upgraded from: Ubuntu 14.04.5 LTS)
- **TensorFlow installed from (source or binary)**: binary (pip package)
- **TensorFlow version (use command below)**: Nightly 1.8.0 build 254 (`tf-nightly`)
- **Python version**: Python 2.7.13
- **Exact command to reproduce within TensorBoard's repository**:
bazel test //tensorboard/plugins/pr_curve:pr_curves_plugin_test

### Describe the problem
Every night, TensorBoard's python tests on travis installs the latest nightly version of TensorFlow. One day, I find that TensorBoard's `:pr_curves_plugin_test` is failing for python 2.
https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/pr_curve/pr_curves_plugin_test.py
Interestingly, the test still passes for python 3.

I confirmed that the test succeeds if I install `tf-nightly` build 253 and then `bazel test` the target within [TensorBoard's repo](https://github.com/tensorflow/tensorboard).
```
pip install -I https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/253/artifact/pip_test/whl/tf_nightly-1.8.0.dev20180409-cp27-cp27mu-linux_x86_64.whl
```

The `:pr_curves_plugin_test` fails if I install `tf-nightly` build 254.
```
pip install -I https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/254/artifact/pip_test/whl/tf_nightly-1.8.0.dev20180410-cp27-cp27mu-linux_x86_64.whl
```

Therefore, a TensorFlow change introduced to build 254 seems to be a likely cause. During `setUp`, the `:pr_curves_plugin_test` runs a [demo script](https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/pr_curve/pr_curve_demo.py) that [seeds randomness](https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/pr_curve/pr_curve_demo.py#L65) and then [generates test data](https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/pr_curve/pr_curve_demo.py#L68) from a `tf.distributions.Normal` distribution.

Could any changes to TensorFlow's seeding or `tf.distributions.Normal` logic (introduced to build 254) cause this breakage? Regardless of how we fix `:pr_curves_plugin_test` (Either TensorBoard's test logic could change, or we could partially roll back a TensorFlow change.), I'm curious about the root cause of the difference in behavior. Thanks! 

### Source code / logs

Here are the logs for the failed test. They just describe how the floating point data derived from the PR curves plugin demo have changed.

```
Testing //.../plugins/pr_curve:pr_curves_plugin_test; 40s linux-sandbox
...F....
======================================================================
FAIL: testPrCurvesDataCorrect (__main__.PrCurvesPluginTest)
Tests that responses for PR curves for run-tag combos are correct.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/travis/.bazel-output-base/bazel-sandbox/2643229171274820909/execroot/org_tensorflow_tensorboard/bazel-out/k8-fastbuild/bin/tensorboard/plugins/pr_curve/pr_curves_plugin_test.runfiles/org_tensorflow_tensorboard/tensorboard/plugins/pr_curve/pr_curves_plugin_test.py"", line 217, in testPrCurvesDataCorrect
    pr_curve_entry=entries[0])
  File ""/home/travis/.bazel-output-base/bazel-sandbox/2643229171274820909/execroot/org_tensorflow_tensorboard/bazel-out/k8-fastbuild/bin/tensorboard/plugins/pr_curve/pr_curves_plugin_test.runfiles/org_tensorflow_tensorboard/tensorboard/plugins/pr_curve/pr_curves_plugin_test.py"", line 87, in validatePrCurveEntry
    assert_allclose(expected_precision, pr_curve_entry['precision'])
  File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py"", line 1396, in assert_allclose
    verbose=verbose, header=header, equal_nan=equal_nan)
  File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py"", line 779, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Not equal to tolerance rtol=0, atol=1e-07
(mismatch 75.0%)
 x: array([0.333333, 0.385321, 0.542169, 0.75    ])
 y: array([0.333333, 0.391026, 0.630137, 0.666667])
----------------------------------------------------------------------
Ran 8 tests in 38.439s
FAILED (failures=1)
================================================================================
```
"
18526,Tensorflow is a big issue,"I am using Windows 10 and Python 3.6 with jupyter notebook. I simply wanted to use tensorflow because of its hyped popularity but it seems like a real pain.

first issue with installing tensorflow, then if installed then issue with importing tensorflow. Giving dumps like 
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.
Looked for this issue then found need to have CUDA and cuDNN from NVIDIA which doesn't allow if you are not an NVIDIA developer.

I am not able to download cuDNN which seems ""very"" important for tensorflow to work.

Can anyone please suggest how can I simply import tensorflow?
"
18525,Hyperparameter optimization and TensorFlow,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution:** Linux-Ubuntu (17.10)
- **TensorFlow installed from**: Binary
- **TensorFlow version**:  1.5.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
I know that this question has already been addressed here and in Stack Overflow, but until the moment I do not see many evolutions on the subject.
Saw this approach here [A Scikit-learn compatible Deep Neural Network built with TensorFlow](https://medium.com/@williamkoehrsen/deep-neural-network-classifier-32c12ff46b6c),, that is, using TensorFlow (model build) and Scikit-learn to make the model's Hyperparameter Tuning.
So far this approach is the most correct?
Really wanted to be able to contribute and work with the optimization of hyperparameters along with the TensorFlow models.
I find this technique very important in Deep Learning...
thanks in advance"
18523,Unable to freeze graph for LinearClassifier,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: Anaconda
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.5.4
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**:  N/A
- **GPU model and memory**:  N/A
- **Exact command to reproduce**: freeze_graph for LinearClassifier model

I'm trying to freeze my model using the source https://www.tensorflow.org/mobile/prepare_models. So I could use it in Android app (using Tensorflow Mobile).
I have a model created with `LinearClassifier`. Below is the example of creating it:

```python
gender = tf.feature_column.categorical_column_with_vocabulary_list(""gender"", [""f"", ""m""])
goal = tf.feature_column.categorical_column_with_vocabulary_list(""goal"", [""fit"", ""lose"", ""muscle""])
level = tf.feature_column.categorical_column_with_vocabulary_list(""level"", [""begin"", ""middle"", ""advance""])
feature_columns = [gender,goal,level]

input_fn = tf.estimator.inputs.pandas_input_fn(x=x_train, y=y_train, batch_size=100, num_epochs=None, shuffle=True)
model = tf.estimator.LinearClassifier(feature_columns=feature_columns, n_classes=18, model_dir=""export"")

model.train(input_fn=input_fn, steps=100)
```
The model is successfully created and saved. It works correctly when it's evaluated.
However, I'm getting the following error, when I try to freeze the graph:
```
TypeError: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(""linear/linear_model/bias_weights:0"", shape=(18,), dtype=float32)
```

I looked on [StackOverflow](https://stackoverflow.com/search?q=names_to_saveables+must+be+a+dict+) and seems this issue happens to many people and there is no answer to that.

I would appreciate any help, because many people are having similar issue."
18522,Error building 1.8 RC-0 'eye_functor_gpu.cu.pic.o was not created',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 27 x64
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.8.0 RC-0 (zip)
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.12.0-1.fc27
- **GCC/Compiler version (if compiling from source)**: 6.4.0-6
- **CUDA/cuDNN version**: 9.1 / 7.0.5.15-3.fc27
- **GPU model and memory**: GTX 1060
- **Exact command to reproduce**: `$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package `

### Describe the problem
I get nearly 6k files into the build and it fails with:
```
INFO: From Compiling tensorflow/core/kernels/eye_functor_gpu.cu.cc [for host]:
/usr/include/bits/floatn.h(61): error: invalid argument to attribute ""__mode__""
/usr/include/bits/floatn.h(73): error: identifier ""__float128"" is undefined
```
Add `""#define _BITS_FLOATN_H""` to `cuda/host_defines.h`, and build again.
Gets further but fails at:
```
INFO: From Compiling tensorflow/core/kernels/eye_functor_gpu.cu.cc:
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]':
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:626:248:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {const std::tuple<int, int, int>&}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'
./tensorflow/stream_executor/dnn.h:891:91:   required from here
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:483:67: error: mismatched argument pack lengths while expanding 'std::is_constructible<_Elements, _UElements&&>'
       return __and_<is_constructible<_Elements, _UElements&&>...>::value;
                                                                   ^~~~~
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:484:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement
     }
 ^
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]':
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:626:362:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {const std::tuple<int, int, int>&}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'
./tensorflow/stream_executor/dnn.h:891:91:   required from here
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'
       return __and_<is_convertible<_UElements&&, _Elements>...>::value;
                                                                 ^~~~~
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement
     }
 ^
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<int, int, int>&; bool <anonymous> = true; _Elements = {int, int, int}]':
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:662:419:   required by substitution of 'template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(const std::tuple<_Args1 ...>&) [with _UElements = {int, int, int}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type <anonymous> = <missing>]'
./tensorflow/stream_executor/dnn.h:891:91:   required from here
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:495:244: error: wrong number of template arguments (4, should be 2)
       return  __and_<__not_<is_same<tuple<_Elements...>,
                                                                                                                                                                                                                                                    ^    
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/type_traits:1558:8: note: provided for 'template<class _From, class _To> struct std::is_convertible'
     struct is_convertible
        ^~~~~~~~~~~~~~
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:502:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<int, int, int>&; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement
     }
 ^
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]':
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:626:248:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<int, int, int>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'
./tensorflow/stream_executor/dnn.h:891:91:   required from here
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:483:67: error: mismatched argument pack lengths while expanding 'std::is_constructible<_Elements, _UElements&&>'
       return __and_<is_constructible<_Elements, _UElements&&>...>::value;
                                                                   ^~~~~
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:484:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement
     }
 ^
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]':
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:626:362:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<int, int, int>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'
./tensorflow/stream_executor/dnn.h:891:91:   required from here
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'
       return __and_<is_convertible<_UElements&&, _Elements>...>::value;
                                                                 ^~~~~
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement
     }
 ^
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<int, int, int>&&; bool <anonymous> = true; _Elements = {int, int, int}]':
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:686:422:   required by substitution of 'template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(std::tuple<_Args1 ...>&&) [with _UElements = {int, int, int}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> = <missing>]'
./tensorflow/stream_executor/dnn.h:891:91:   required from here
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:495:244: error: wrong number of template arguments (4, should be 2)
       return  __and_<__not_<is_same<tuple<_Elements...>,
                                                                                                                                                                                                                                                    ^    
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/type_traits:1558:8: note: provided for 'template<class _From, class _To> struct std::is_convertible'
     struct is_convertible
        ^~~~~~~~~~~~~~
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:502:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<int, int, int>&&; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement
     }
 ^
ERROR: /home/torstein/progs/tensorflow-1.8.0-rc0/tensorflow/core/kernels/BUILD:1864:1: output 'tensorflow/core/kernels/_objs/eye_functor_gpu/tensorflow/core/kernels/eye_functor_gpu.cu.o' was not created
ERROR: /home/torstein/progs/tensorflow-1.8.0-rc0/tensorflow/core/kernels/BUILD:1864:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 48.261s, Critical Path: 33.18s
FAILED: Build did NOT complete successfully
```

Build conf:
```
build --action_env PYTHON_BIN_PATH=""/usr/bin/python3""
build --action_env PYTHON_LIB_PATH=""/usr/local/lib64/python3.6/site-packages""
build --force_python=py3
build --host_force_python=py3
build --python_path=""/usr/bin/python3""
build --define with_jemalloc=true
build:gcp --define with_gcp_support=true
build:hdfs --define with_hdfs_support=true
build:s3 --define with_s3_support=true
build:kafka --define with_kafka_support=true
build:xla --define with_xla_support=true
build:gdr --define with_gdr_support=true
build:verbs --define with_verbs_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_CUDA=""1""
build --action_env CUDA_TOOLKIT_PATH=""/usr""
build --action_env TF_CUDA_VERSION=""9.1""
build --action_env CUDNN_INSTALL_PATH=""/usr""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env TF_NCCL_VERSION=""2""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
build --action_env LD_LIBRARY_PATH=""/usr/include/cuda:/usr/include:/usr:/usr/lib64:/usr/include/cuda:/usr:/usr/include""
build --action_env TF_CUDA_CLANG=""0""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/cuda-gcc""
build --config=cuda
test --config=cuda
build --define grpc_no_ares=true
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
build --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
```
Same error happens on master branch.

If I add the build flag `--cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""`, it fails at different place:
```
INFO: From Compiling tensorflow/core/kernels/spacetodepth_op_gpu.cu.cc:
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]':
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:626:248:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {const std::tuple<int, int, int>&}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'
./tensorflow/stream_executor/dnn.h:891:91:   required from here
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:483:67: error: mismatched argument pack lengths while expanding 'std::is_constructible<_Elements, _UElements&&>'
       return __and_<is_constructible<_Elements, _UElements&&>...>::value;
                                                                   ^~~~~
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:484:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement
     }
 ^
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]':
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:626:362:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {const std::tuple<int, int, int>&}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'
./tensorflow/stream_executor/dnn.h:891:91:   required from here
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'
       return __and_<is_convertible<_UElements&&, _Elements>...>::value;
                                                                 ^~~~~
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement
     }
 ^
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<int, int, int>&; bool <anonymous> = true; _Elements = {int, int, int}]':
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:662:419:   required by substitution of 'template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(const std::tuple<_Args1 ...>&) [with _UElements = {int, int, int}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type <anonymous> = <missing>]'
./tensorflow/stream_executor/dnn.h:891:91:   required from here
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:495:244: error: wrong number of template arguments (4, should be 2)
       return  __and_<__not_<is_same<tuple<_Elements...>,
                                                                                                                                                                                                                                                    ^    
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/type_traits:1558:8: note: provided for 'template<class _From, class _To> struct std::is_convertible'
     struct is_convertible
        ^~~~~~~~~~~~~~
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:502:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<int, int, int>&; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement
     }
 ^
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]':
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:626:248:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<int, int, int>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'
./tensorflow/stream_executor/dnn.h:891:91:   required from here
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:483:67: error: mismatched argument pack lengths while expanding 'std::is_constructible<_Elements, _UElements&&>'
       return __and_<is_constructible<_Elements, _UElements&&>...>::value;
                                                                   ^~~~~
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:484:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement
     }
 ^
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]':
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:626:362:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<int, int, int>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'
./tensorflow/stream_executor/dnn.h:891:91:   required from here
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'
       return __and_<is_convertible<_UElements&&, _Elements>...>::value;
                                                                 ^~~~~
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement
     }
 ^
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<int, int, int>&&; bool <anonymous> = true; _Elements = {int, int, int}]':
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:686:422:   required by substitution of 'template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(std::tuple<_Args1 ...>&&) [with _UElements = {int, int, int}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> = <missing>]'
./tensorflow/stream_executor/dnn.h:891:91:   required from here
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:495:244: error: wrong number of template arguments (4, should be 2)
       return  __and_<__not_<is_same<tuple<_Elements...>,
                                                                                                                                                                                                                                                    ^    
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/type_traits:1558:8: note: provided for 'template<class _From, class _To> struct std::is_convertible'
     struct is_convertible
        ^~~~~~~~~~~~~~
/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:502:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<int, int, int>&&; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement
     }
 ^
ERROR: /home/torstein/progs/tensorflow-1.8.0-rc0/tensorflow/core/kernels/BUILD:3659:1: output 'tensorflow/core/kernels/_objs/depth_space_ops_gpu/tensorflow/core/kernels/spacetodepth_op_gpu.cu.o' was not created
ERROR: /home/torstein/progs/tensorflow-1.8.0-rc0/tensorflow/core/kernels/BUILD:3659:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 111.410s, Critical Path: 35.53s
FAILED: Build did NOT complete successfully
```
"
18519,"Tensorflow profiler problem, maybe bug.","### Describe the problem
The implementation of topk kernel operation seems faster on my cpu than on gpu(for example below profiler outputs 6us for cpu and 345us for gpu, and result reapeats for other inputs). But that is not the case for this issue, I started some testing on that which lead me to this:

I extracted the kernel implementation from tf source. Put some time measuring code in Compute around functor call, for real(using std::chrono::time_steady) and cpu(clock() from ctime) time. Compiled it without optimizations, loaded in python as tensorflow library. For cpu the result was much bigger than what profiler printed, 6us(profiler) vs 1432us(clock()), 473us(using std::chrono::time_steady).
Then I added in functor code thread sleep, only real time duration extended, then I added long for loop and both real and cpu duration got bigger but again profiler output stayed the same as in code without modifications ~6us, what is not proper in my understanding.
### Source code / logs
Here is the python3 code I used:
```
import tensorflow as tf
import random

arr=[[int(100000*random.random())
    for i in range(100)] for j in range(100)]

a = tf.convert_to_tensor(arr)
topk_module = tf.load_op_library(""topk_op.so"")
b = topk_module.matrix_top_k(a, 10)

builder = tf.profiler.ProfileOptionBuilder
opts = builder(builder.time_and_memory()).order_by('micros').build()


with tf.contrib.tfprof.ProfileContext('/tmp/train_dir',
                                    trace_steps=[],
                                    dump_steps=[]) as pctx:
    with tf.Session() as sess:
        pctx.trace_next_step()
        pctx.dump_next_step()
        x = sess.run(b)
        pctx.profiler.profile_operations(options=opts)
```
and snippet from topk_op.cc
```
    std::chrono::steady_clock::time_point time_begin_real =
        std::chrono::steady_clock::now();
    clock_t time_begin_cpu = clock();


    Status s = functor::TopKFunctor<Device, T>::Compute(
        context, sorted_, k, input, num_rows, num_cols, values, indices);
    
    clock_t time_end_cpu = clock();
    std::chrono::steady_clock::time_point time_end_real =
        std::chrono::steady_clock::now();
    //stdout printing
```
------------------------
### System information
Have I written custom code: yes, can post modified topk op kernel code and makefile if you want
ubuntu 16.04
tensorflow installed from pip package manager.
tensorflow 1.7.0
bazel: n/a, used gcc 4.9.3
cuda 9.0 cudnn 7.0
gtx 1050 ti 4GB
n/a
"
18518,Semantic Similarity with TF-Hub doesn't work in the provided colab notebook,"### System information
colab.research.google.com using Python3 and GPU runtime. 

### Describe the problem
Getting the module from the Hub doesn't work.

```
KeyError: ""The name 'global_step:0' refers to a Tensor which does not exist. The operation, 'global_step', does not exist in the graph.""
```

### Source code / logs
https://colab.research.google.com/github/tensorflow/hub/blob/r0.1/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb

```
embed = hub.Module(""https://tfhub.dev/google/universal-sentence-encoder/1"")
```
"
18516,How to become a voluntary translator of tensorflow's official website in China？,"I am a postgraduate student in computer science from China and have a special passion for deep learning. Due to its high degree of flexibility and powerful computing ability, tensorflow is my most commonly used deep learning framework. I have used it to do a lot of interesting things during my graduate studies. In China, tensorflow is one of the most commonly used deep learning frameworks for developers. However, due to some reasons, tensorflow cannot be normally accessed in China, which restricts the development of tensorflow to some extent, and at the same time it widens the distance between ordinary developers and deep learning. Recently, tensorflow China's official website (tensorflow.google.cn) went online, which is a very good news for Chinese developers. Developers no longer need to use some extra ways to learn to use tensorflow.For most experienced developers in China, reading English documents is not a hindrance. However, for beginners, whether they are new to programming or planning to get started with deep learning, due to the limitation of English reading ability, they are somewhat excluded from official documents, especially English official documents.Many people can only obtain the knowledge of debris through the Baidu search enginer and cannot have a systematic understanding of tensorflow.Due to the special love for google and the worship for tensorflow's open source spirit, I wish to be a tensorflow Chinese volunteer translator.I hope that my efforts will enable Chinese developers to use tensorflow more easily and efficiently. I also hope to have the privilege of expanding tensorflow's influence in China.For the tensorflow official documentation (tensorflow.org), I basically read all the documents and I feel competent to perform this honorable duty.The only question now is how to become a voluntary translator of tensorflow's official website in China. Thanks for your reading and hope to get a reply."
18515,How to become a voluntary translator of tensorflow's official website in China？,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
